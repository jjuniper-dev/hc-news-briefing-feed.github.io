‚úÖ Morning News Briefing ‚Äì November 11, 2025 10:48

üìÖ Date: 2025-11-11 10:48
üè∑Ô∏è Tags: #briefing #ai #publichealth #digitalgov

‚∏ª

üßæ Weather
‚Ä¢ No watches or warnings in effect, Pembroke
  No watches or warnings in effect. No warnings or watches or watches in effect . Watch or warnings are no longer in effect in the U.S. No watches, warnings are in effect for the rest of the day . No watches and warnings are still in effect, but no watches are in place for the day's events . The weather is not expected to be affected by the weather .
‚Ä¢ Current Conditions: Light Snow, -4.6¬∞C
  Temperature: -4.6&deg;C Pressure: 100.3 kPa Visibility: 10 km Humidity: 89 % Wind Chill: -10%. Dewpoint: -6.2&deg:C Wind: WNW 15 km/h Air Quality Health Index: n/a . Air Quality: "N/A" Air Quality health index: "Health Index
‚Ä¢ Tuesday: Flurries. High plus 1.
  Flurries ending this morning then cloudy then cloudy with 40 percent chance of flurries . Wind northwest 30 km/h gusting to 50 becoming light early this morning . Wind chill minus 13 this morning. UV index 1 or low. High plus 1.50¬∞C this morning, with wind chill at minus 13¬∞¬∞¬∞F . Forecast issued 5:00 AM EST

üåç International News
No updates.

üçÅ Canadian News
No updates.

üá∫üá∏ U.S. Top Stories
‚Ä¢ Trump floats tariff 'dividends' even while plan shows major flaws
  President Trump says the government will distribute checks to Americans from tariff revenue . Here's what that could mean for the rest of the country . President Trump said the government would distribute checks from tariffs from the proceeds of the proceeds from the sales of U.S. sales of the U.K. Here's how the checks will be distributed to Americans . Trump says tariffs will be paid back to
‚Ä¢ 'No turning back': More and more Ukrainian women join the army to fight Russia
  An increasing number of women are joining the Ukrainian military, with thousands serving in front line roles . Russia's war on Ukraine nears its fourth year ‚Äî with no peace in sight . Thousands of women serve in front-line roles in Ukraine's conflict with Russia . Russia has been fighting Ukraine for more than two years . Ukraine's government has been blamed for the collapse of Ukraine's pro-
‚Ä¢ Sen. Fetterman slams Democrats for shutting down government
  Sen. John Fetterman (D-Pa.) has a reputation for going against his party . He spoke to NPR Monday ahead of his new book release, "Unfettered" He has been doing so by voting to reopen the government and voted to do so in favor of his own party in the Senate . He also spoke ahead of the release of his book, 'Unfet
‚Ä¢ Climate negotiations have started. Here's how far countries need to go
  Nations have begun climate negotiations at the COP30 summit in Brazil . Studies show the world is not on track to avoid the most damaging impacts of climate change . Nations are meeting in Brazil for the first time this year to discuss the world's response to the climate change crisis . Nations have already agreed a deal to tackle climate change in the form of carbon dioxide emissions in the past decade, but
‚Ä¢ Is hormone therapy for menopause right for you? 6 things to know
  The science around hormone therapy to treat menopause has changed a lot since the FDA issued warning labels 20 years ago . Now the labels are being removed, here are 6 things to consider: Hormone therapy is being removed from the market . The science behind hormone therapy has changed since the warning labels were issued . Here are six things you should consider: Menopause and menop

üß† Artificial Intelligence
No updates.

üíª Digital Strategy
‚Ä¢ UK's Ajax fighting vehicle arrives ‚Äì years late and still sending crew to hospital
  The British Army just received its first new armored fighting vehicle (AFV) for nearly three decades . It is years late, hit by rising costs, is still reportedly injuring its crew, and there are questions about whether it remains relevant in the age of drone warfare . The AFV is still being injured by the terrain, and it is still reported to be in danger of injuring crew .
‚Ä¢ AI isn't throttling HPC. It is HPC
  In recent discussions with industry vendor sales/marketing types, I've been hearing that HPC demand is falling off while AI system demand is continuing to increase . I've also seen articles implying that AI is somehow displacing HPC . Huh? Huh?‚Ä¶‚Ä¶ Huh? I've seen articles imply that AI . is somehow displaced by HPC. Huh? ... Huh?... ...
‚Ä¢ Intel CTO and AI boss quits to join OpenAI after just six months in the job
  Sachin Katti was one of new Intel CEO Lip Bu Tan's first appointments . He was promoted to chief technology and AI officer in April . He will leave the x86 giant to join OpenAI after just six months in the job . Katti will join the AI firm OpenAI in the next round of the open AI race . He is one of the founders of OpenAI,
‚Ä¢ Superintelligence probably not happening, but AI will still reshape society, expert panel says
  Experts share the view that machine learning models will have a significant effect on society . Ask 339 people, get 339 answers from around the world to find out what they think about AI . Experts say they are skeptical of corporate AI hype to varying degrees, but they share a view that it will have an impact on society in the long run of coming years . For more information, visit CNN Tech
‚Ä¢ LLM side-channel attack could allow snoops to guess what you're talking about
  Microsoft researchers say models from some providers, including Anthropic, AWS, DeepSeek, and Google, haven't been fixed . Side-channel attacks can guess the subjects being discussed with LLMs using a side-channel attack, the researchers say . They told The Register that some models of some providers have not been fixed, putting both personal users and enterprise communications at risk.‚Ä¶‚Ä¶

üè• Public Health
No updates.

üî¨ Science
‚Ä¢ Optimized loading effects on pressure steam sterilization of loaned surgical instruments using product family categorization
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Author Correction: Global burden of chikungunya virus infections and the potential benefit of vaccination campaigns
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Greater benefits of immediate nirmatrelvir-ritonavir initiation for post-COVID outcomes: a population-based retrospective cohort study
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Validating the Balanced Inventory of Desirable Reporting in a low literacy adolescent population in Burkina Faso
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Genome sequencing for prevention of health-care-associated bacterial infections
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations

üßæ Government & Policy
No updates.

üèõÔ∏è Enterprise Architecture & IT Governance
No updates.

ü§ñ AI & Emerging Tech
‚Ä¢ The State of AI: Energy is king, and the US is falling behind
  Welcome back to¬†The State of AI, a new collaboration between the Financial Times and MIT Technology Review. Every Monday, writers from both publications debate one aspect of the generative AI revolution and how it is reshaping global power.



This week, Casey Crownhart, senior reporter for energy at MIT Technology Review and Pilita Clark, FT&#8217;s columnist, consider how China&#8217;s rapid renewables buildout could help it leapfrog on AI progress.







Casey Crownhart writes:



In the age of AI, the biggest barrier to progress isn‚Äôt money but energy. That should be particularly worrying here in the US, where massive data centers are waiting to come online, and it doesn‚Äôt look as if the country will build the steady power supply or infrastructure needed to serve them all.



It wasn‚Äôt always like this. For about a decade before 2020, data centers were able to offset increased demand with efficiency improvements. Now, though, electricity demand is ticking up in the US, with billions of queries to popular AI models each day‚Äîand efficiency gains aren‚Äôt keeping pace. With too little new power capacity coming online, the strain is starting to show: Electricity bills are ballooning for people who live in places where data centers place a growing load on the grid.



If we want AI to have the chance to deliver on big promises without driving electricity prices sky-high for the rest of us, the US needs to learn some lessons from the rest of the world on energy abundance. Just look at China.



China installed 429 GW of new power generation capacity in 2024, more than six times the net capacity added in the US during that time.



China still generates much of its electricity with coal, but that makes up a declining share of the mix. Rather, the country is focused on installing solar, wind, nuclear, and gas at record rates.



The US, meanwhile, is focused on reviving its ailing coal industry. Coal-fired power plants are polluting and, crucially, expensive to run. Aging plants in the US are also less reliable than they used to be, generating electricity just 42% of the time, compared with a 61% capacity factor in 2014.



It‚Äôs not a great situation. And unless the US changes something, we risk becoming consumers as opposed to innovators in both energy and AI tech. Already, China earns more from exporting renewables than the US does from oil and gas exports.&nbsp;



Building and permitting new renewable power plants would certainly help, since they‚Äôre currently the cheapest and fastest to bring online. But wind and solar are politically unpopular with the current administration. Natural gas is an obvious candidate, though there are concerns about delays with key equipment.



One quick fix would be for data centers to be more flexible. If they agreed not to suck electricity from the grid during times of stress, new AI infrastructure might be able to come online without any new energy infrastructure.





One study from Duke University found that if data centers agree to curtail their consumption just 0.25% of the time (roughly 22 hours over the course of the year), the grid could provide power for about 76 GW of new demand. That‚Äôs like adding about 5% of the entire grid‚Äôs capacity without needing to build anything new.



But flexibility wouldn‚Äôt be enough to truly meet the swell in AI electricity demand. What do you think, Pilita? What would get the US out of these energy constraints? Is there anything else we should be thinking about when it comes to AI and its energy use?&nbsp;



Pilita Clark responds:



I agree. Data centers that can cut their power use at times of grid stress should be the norm, not the exception. Likewise, we need more deals like those giving cheaper electricity to data centers that let power utilities access their backup generators. Both reduce the need to build more power plants, which makes sense regardless of how much electricity AI ends up using.



This is a critical point for countries across the world, because we still don‚Äôt know exactly how much power AI is going to consume.&nbsp;



Forecasts for what data centers will need in as little as five years‚Äô time vary wildly, from less than twice today‚Äôs rates to four times as much.



This is partly because there‚Äôs a lack of public data about AI systems‚Äô energy needs. It‚Äôs also because we don‚Äôt know how much more efficient these systems will become. The US chip designer Nvidia said last year that its specialized chips had become 45,000 times more energy efficient over the previous eight years.&nbsp;



Moreover, we have been very wrong about tech energy needs before. At the height of the dot-com boom in 1999, it was erroneously claimed that the internet would need half the US‚Äôs electricity within a decade‚Äînecessitating a lot more coal power.



Still, some countries are clearly feeling the pressure already. In Ireland, data centers chew up so much power that new connections have been restricted around Dublin to avoid straining the grid.



Some regulators are eyeing new rules forcing tech companies to provide enough power generation to match their demand. I hope such efforts grow. I also hope AI itself helps boost power abundance and, crucially, accelerates the global energy transition needed to combat climate change. OpenAI‚Äôs Sam Altman said in 2023 that ‚Äúonce we have a really powerful super intelligence, addressing climate change will not be particularly difficult.‚Äù&nbsp;



The evidence so far is not promising, especially in the US, where renewable projects are being axed. Still, the US may end up being an outlier in a world where ever cheaper renewables made up more than 90% of new power capacity added globally last year.&nbsp;



Europe is aiming to power one of its biggest data centers predominantly with renewables and batteries. But the country leading the green energy expansion is clearly China.



The 20th century was dominated by countries rich in the fossil fuels whose reign the US now wants to prolong. China, in contrast, may become the world‚Äôs first green electrostate. If it does this in a way that helps it win an AI race the US has so far controlled, it will mark a striking chapter in economic, technological, and geopolitical history.



Casey Crownhart replies:



I share your skepticism of tech executives‚Äô claims that AI will be a groundbreaking help in the race to address climate change. To be fair, AI is progressing rapidly. But we don‚Äôt have time to wait for technologies standing on big claims with nothing to back them up.&nbsp;



When it comes to the grid, for example, experts say there‚Äôs potential for AI to help with planning and even operating, but these efforts are still experimental.&nbsp;&nbsp;



Meanwhile, much of the world is making measurable progress on transitioning to newer, greener forms of energy. How that will affect the AI boom remains to be seen. What is clear is that AI is changing our grid and our world, and we need to be clear-eyed about the consequences.&nbsp;



Further reading&nbsp;



MIT Technology Review reporters did the math on the energy needs of an AI query.



There are still a few reasons to be optimistic about AI‚Äôs energy demands.&nbsp;&nbsp;



The FT‚Äôs visual data team take a look inside the relentless race for AI capacity.



And global FT reporters ask whether data centers can ever truly be green.
‚Ä¢ Reimagining cybersecurity in the era of AI and quantum
  AI and quantum technologies are dramatically reconfiguring how cybersecurity functions, redefining the speed and scale with which digital defenders and their adversaries can operate.







The weaponization of AI tools for cyberattacks is already proving a worthy opponent to current defenses. From reconnaissance to ransomware, cybercriminals can automate attacks faster than ever before with AI. This includes using generative AI to create social engineering attacks at scale, churning out tens of thousands of tailored phishing emails in seconds, or accessing widely available voice cloning software capable of bypassing security defenses for as little as a few dollars. And now, agentic AI raises the stakes by introducing autonomous systems that can reason, act, and adapt like human adversaries.



But AI isn‚Äôt the only force shaping the threat landscape. Quantum computing has the potential to seriously undermine current encryption standards if developed unchecked. Quantum algorithms can solve the mathematical problems underlying most modern cryptography, particularly public-key systems like RSA and Elliptic Curve, widely used for secure online communication, digital signatures, and cryptocurrency.



‚ÄúWe know quantum is coming. Once it does, it will force a change in how we secure data across everything, including governments, telecoms, and financial systems,‚Äù says Peter Bailey, senior vice president and general manager of Cisco‚Äôs security business.



‚ÄúMost organizations are understandably focused on the immediacy of AI threats,&#8221; says Bailey. ‚ÄúQuantum might sound like science fiction, but those scenarios are coming faster than many realize. It‚Äôs critical to start investing now in defenses that can withstand both AI and quantum attacks.‚Äù



Critical to this defense is a zero trust approach to cybersecurity, which assumes no user or device can be inherently trusted. By enforcing continuous verification, zero trust enables constant monitoring and ensures that any attempts to exploit vulnerabilities are quickly detected and addressed in real time. This approach is technology-agnostic and creates a resilient framework even in the face of an ever-changing threat landscape.



Putting up AI defenses&nbsp;



AI is lowering the barrier to entry for cyberattacks, enabling hackers even with limited skills or resources to infiltrate, manipulate, and exploit the slightest digital vulnerability.



Nearly three-quarters (74%) of cybersecurity professionals say AI-enabled threats are already having a significant impact on their organization, and 90% anticipate such threats in the next one to two years.&nbsp;



‚ÄúAI-powered adversaries have advanced techniques and operate at machine speed,‚Äù says Bailey. ‚ÄúThe only way to keep pace is to use AI to automate response and defend at machine speed.‚Äù



To do this, Bailey says, organizations must modernize systems, platforms, and security operations to automate threat detection and response‚Äîprocesses that have previously relied on human rule-writing and reaction times. These systems must adapt dynamically as environments evolve and criminal tactics change.



At the same time, companies must strengthen the security of their AI models and data to reduce exposure to manipulation from AI-enabled malware. Such risks could include, for instance, prompt injections, where a malicious user crafts a prompt to manipulate an AI model into performing unintended actions, bypassing its original instructions and safeguards.



Agentic AI further ups the ante, with hackers able to use AI agents to automate attacks and make tactical decisions without constant human oversight. ‚ÄúAgentic AI has the potential to collapse the cost of the kill chain,‚Äù says Bailey. ‚ÄúThat means everyday cybercriminals could start executing campaigns that today only well-funded espionage operations can afford.‚Äù



Organizations, in turn, are exploring how AI agents can help them stay ahead. Nearly 40% of companies expect agentic AI to augment or assist teams over the next 12 months, especially in cybersecurity, according to Cisco‚Äôs 2025 AI Readiness Index. Use cases include AI agents trained on telemetry, which can identify anomalies or signals from machine data too disparate and unstructured to be deciphered by humans.&nbsp;



Calculating the quantum threat



As many cybersecurity teams focus on the very real AI-driven threat, quantum is waiting on the sidelines. Almost three-quarters (73%) of US organizations surveyed by KPMG say they believe it is only a matter of time before cybercriminals are using quantum to decrypt and disrupt today‚Äôs cybersecurity protocols. And yet, the majority (81%) also admit they could do more to ensure that their data remains secure.



Companies are right to be concerned. Threat actors are already carrying out harvest now, decrypt later attacks, stockpiling sensitive encrypted data to crack once quantum technology matures. Examples include state-sponsored actors intercepting government communications and cybercriminal networks storing encrypted internet traffic or financial records.&nbsp;



Large technology companies are among the first to roll out quantum defenses. For example, Apple is using cryptography protocol PQ3 to defend against harvest now, decrypt later attacks on its iMessage platform. Google is testing post-quantum cryptography (PQC)‚Äîwhich is resistant to attacks from both quantum and classical computers‚Äîin its Chrome browser. And Cisco ‚Äúhas made significant investments in quantum-proofing our software and infrastructure,‚Äù says Bailey. ‚ÄúYou‚Äôll see more enterprises and governments taking similar steps over the next 18 to 24 months,‚Äù he adds.&nbsp;



As regulations like the US Quantum Computing Cybersecurity Preparedness Act lay out requirements for mitigating against quantum threats, including standardized PQC algorithms by the National Institute of Standards and Technology, a wider range of organizations will start preparing their own quantum defenses.&nbsp;



For organizations beginning that journey, Bailey outlines two key actions. First, establish visibility. ‚ÄúUnderstand what data you have and where it lives,‚Äù he says. ‚ÄúTake inventory, assess sensitivity, and review your encryption keys, rotating out any that are weak or outdated.‚Äù



Second, plan for migration. ‚ÄúNext, assess what it will take to support post-quantum algorithms across your infrastructure. That means addressing not just the technology, but also the process and people implications,‚Äù Bailey says.



Adopting proactive defense&nbsp;



Ultimately, the foundation for building resilience against both AI and quantum is a zero trust approach, says Bailey. By embedding zero trust access controls across users, devices, business applications, networks, and clouds, this approach grants only the minimum access required to complete a task and enables continuous monitoring. It can also minimize the attack surface by confining a potential threat to an isolated zone, preventing it from accessing other critical systems.



Into this zero trust architecture, organizations can integrate specific measures to defend against AI and quantum risks. For instance, quantum-immune cryptography and AI-powered analytics and security tools can be used to identify complex attack patterns and automate real-time responses.&nbsp;



‚ÄúZero trust slows down attacks and builds resilience,‚Äù Bailey says. ‚ÄúIt ensures that even if a breach occurs, the crown jewels stay protected and operations can recover quickly.‚Äù



Ultimately, companies should not wait for threats to emerge and evolve. They must get ahead now. ‚ÄúThis isn‚Äôt a what-if scenario; it‚Äôs a when,‚Äù says Bailey. ‚ÄúOrganizations that invest early will be the ones setting the pace, not scrambling to catch up.‚Äù



This content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review‚Äôs editorial staff. It was researched, designed, and written by human writers, editors, analysts, and illustrators. This includes the writing of surveys and collection of data for surveys. AI tools that may have been used were limited to secondary production processes that passed thorough human review.
‚Ä¢ The Download: busting weather myths, and AI heart attack prediction
  This is today&#8217;s edition of¬†The Download,¬†our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



Why it‚Äôs so hard to bust the weather control conspiracy theory



It was October 2024, and Hurricane Helene had just devastated the US Southeast. Representative Marjorie Taylor Greene of Georgia found an abstract target on which to pin the blame: ‚ÄúYes they can control the weather,‚Äù she posted on X. ‚ÄúIt‚Äôs ridiculous for anyone to lie and say it can‚Äôt be done.‚ÄùShe was repeating what‚Äôs by now a pretty familiar and popular conspiracy theory: that shadowy forces are out there, wielding technology to control the weather and wreak havoc on their enemies. This preposterous claim has grown louder and more common in recent years, especially after extreme weather strikes.But here‚Äôs the thing: While Greene and other believers are not correct, this conspiracy theory‚Äîlike so many others‚Äîholds a kernel of much more modest truth. Read the full story.



‚ÄîDave Levitan



This story is part of MIT Technology Review‚Äôs series ‚ÄúThe New Conspiracy Age,‚Äù on how the present boom in conspiracy theories is reshaping science and technology. Check out the rest of the series here.







AI could predict who will have a heart attack&nbsp;



For all the modern marvels of cardiology, we struggle to predict who will have a heart attack. Many people never get screened at all. Now, startups are applying AI algorithms to screen millions of CT scans for early signs of heart disease.This technology could be a breakthrough for public health, applying an old tool to uncover patients whose high risk for a heart attack is hiding in plain sight. But it remains unproven at scale, while raising thorny questions about implementation and even how we define disease. Read the full story.



‚ÄîVishal Khetpal



This story is from the latest print issue of MIT Technology Review magazine, which is full of fascinating stories about the body. If you haven‚Äôt already, subscribe now to receive future issues once they land.







The must-reads



I‚Äôve combed the internet to find you today‚Äôs most fun/important/scary/fascinating stories about technology.



1 Spending on AI may be to blame for all those tech layoffsAI isn‚Äôt necessarily replacing jobs, but spending on it is gobbling up budgets. (Fast Company $)+ Junior roles are likely to be the first on the chopping block. (FT $)+ Are the crazy sums that businesses are sinking into AI sustainable? (WP $)+ People are worried that AI will take everyone‚Äôs jobs. We‚Äôve been here before. (MIT Technology Review)



2 Anti-vaccine activists gathered in Austin over the weekendThey celebrated RFK Jr‚Äôs rise and outlined their goals‚Äîincluding eliminating school vaccine mandates. (WP $)+ We‚Äôre on the verge of stopping the next pandemic. But will we? (Vox)+ How conspiracy theories infiltrated the doctor‚Äôs office. (MIT Technology Review)



3 People who‚Äôve experienced AI-induced delusions are forming a movementThey‚Äôre pushing for legal action against chatbot makers. (Bloomberg $)+ The looming crackdown on AI companionship. (MIT Technology Review)



4 AI-generated clips of women being strangled are flooding social mediaMany of them appear to have been created using OpenAI‚Äôs Sora 2. (404 Media)5 Tech leaders are obsessed with bioengineering babiesThey‚Äôre not allowed to, but they‚Äôre not letting a little thing like ethics get in the way. (WSJ $)+ The race to make the perfect baby is creating an ethical mess. (MIT Technology Review)6 Apple has removed two popular gay dating apps in China¬†The country ordered it to take down Blued and Finka from its app. (Wired $)



7 The UK government is worried China could turn off its buses remotelyIt fears hundreds of Chinese-made electric buses on British roads could be at risk. (FT $)



8 How AI is changing the world‚Äôs newsrooms It‚Äôs brilliant at analyzing large data sets‚Äîbut shouldn‚Äôt be used to write stories. (NYT $)



9 How to contain an invasive speciesExperts argue that too much red tape is getting in the way. (Undark)+ The weeds are winning. (MIT Technology Review)10 The world‚Äôs largest electric ship is charging up Once it‚Äôs ready to go, it‚Äôll serve as a ferry in 90 minute bursts. (IEEE Spectrum)







Quote of the day



‚ÄúWe would move heaven and Earth, pun intended, to try to get to the Moon sooner.‚Äù&nbsp;



‚ÄîDave Limp, CEO of Blue Origin, says the company is raring to work with NASA to get humans back on the Moon, Ars Technica reports.







One more thing







Design thinking was supposed to fix the world. Where did it go wrong?In the 1990s, a six-step methodology for innovation called design thinking started to grow in popularity. Key to its spread was its replicable aesthetic, represented by the Post-it note: a humble square that anyone can use in infinite ways.But in recent years, for a number of reasons, the shine of design thinking has been wearing off. Critics have argued that its short-term focus on novel and naive ideas results in unrealistic and ungrounded recommendations.Today, some groups are working to reform both design thinking‚Äôs principles and its methodologies. These new efforts seek a set of design tools capable of equitably serving diverse communities and solving diverse problems well into the future. It‚Äôs a much more daunting‚Äîand crucial‚Äîtask than design thinking‚Äôs original remit. Read the full story.



‚ÄîRebecca Ackermann







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)



+ These tree-dwelling toads give birth to live young‚Äîwho knew?!+ Now‚Äôs the time to practice your baking skills ahead of Thanksgiving.+ Younguk Yi‚Äôs glitching paintings are a lot of fun.+ Place your bets! This fun game follows three balls in a race to the bottom, but who will win?
‚Ä¢ The Download: a new home under the sea, and cloning pets
  This is today&#8217;s edition of¬†The Download,¬†our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



The first new subsea habitat in 40 years is about to launch



Vanguard feels and smells like a new RV. It has long, gray banquettes that convert into bunks, a microwave cleverly hidden under a counter, a functional steel sink with a French press and crockery above. A weird little toilet hides behind a curtain.



But you can‚Äôt just fire up Vanguard‚Äôs engine and roll off the lot. Once it is sealed and moved to its permanent home beneath the waves of the Florida Keys National Marine Sanctuary early next year, Vanguard will be the world‚Äôs first new subsea habitat in nearly four decades.Teams of four scientists will live and work on the seabed for a week at a time, entering and leaving the habitat as scuba divers. Read our story about some of their potential missions.



‚ÄîMark Harris







Cloning isn‚Äôt just for celebrity pets like Tom Brady‚Äôs dog



This week, we heard that Tom Brady had his dog cloned. The former quarterback revealed that his Junie is actually a clone of Lua, a pit bull mix that died in 2023.



Brady‚Äôs announcement follows those of celebrities like Paris Hilton and Barbra Streisand, who also famously cloned their pet dogs. But some believe there are better ways to make use of cloning technologies, such as diversifying the genetic pools of inbred species, or potentially bringing other animals back from the brink of extinction. Read the full story.



‚ÄîJessica Hamzelou



This article first appeared in The Checkup, MIT Technology Review‚Äôs weekly biotech newsletter. To receive it in your inbox every Thursday, and read articles like this first, sign up here.







The must-reads



I‚Äôve combed the internet to find you today‚Äôs most fun/important/scary/fascinating stories about technology.



1 OpenAI is facing a wave of new lawsuits¬†The cases concern wrongful death complaints, and claims ChatGPT caused mental breakdowns. (NYT $)+ One family claims ChatGPT ‚Äúgoaded‚Äù their son into taking his own life. (CNN)+ The looming crackdown on AI companionship. (MIT Technology Review)



2 Tesla shareholders approved Elon Musk‚Äôs $1 trillion pay packageMore than 75% of voters backed it. (WSJ $)+ Musk had hinted he‚Äôd leave Tesla if the deal wasn‚Äôt greenlit. (Axios)+ Tesla has to hit its ambitious targets before he can get his hands on the cash. (Wired $)3 The EU is poised to water down the AI actAfter pressure from Big Tech and the US government. (FT $)+ While the legislation was passed last year, many provisions haven‚Äôt kicked in yet. (Reuters)



4 Meta is earning a colossal amount of money from scam adsThey accounted for 10% of its revenue last year. (Reuters)+ Meta claims it ‚Äúaggressively‚Äù addresses scam ads on its platform. (CNBC)



5 The Chan Zuckerberg Initiative is pivoting to AIIt‚Äôs shifting its philanthropic focus from social justice programs to curing disease. (WP $)+ To achieve its goals, the charity will need extra computing power. (NYT $)



6 Unesco has adopted global standards on neurotechnologyExperts were increasingly concerned that a lack of guardrails could give rise to unethical practices. (The Guardian)+ Meet the other companies developing brain-computer interfaces. (MIT Technology Review)



7 Benchmarks hugely oversell AI performanceA new study questions their reliability and the validity of their results. (NBC News)+ How to build a better AI benchmark. (MIT Technology Review)



8 Kim Kardashian blames ChatGPT for failing her law examsIt‚Äôs almost like she shouldn‚Äôt have been consulting it for legal expertise in the first place. (Hollywood Reporter)+ AI and social media is worsening brain rot. (NYT $)+ How AI is introducing errors into courtrooms. (MIT Technology Review)



9 Hyundai is using robot dogs to inspect its EV production lineAnd they may soon be joined by a bipedal master. (IEEE Spectrum)



10 Grand Theft Auto VI has been delayed yet againThe highly anticipated video game has big, big shoes to fill. (Bloomberg $)+ It‚Äôll land a full 13 years after its previous incarnation‚Äîor will it? (BBC)







Quote of the day



‚ÄúThis is what oligarchy looks like.‚Äù



‚ÄîSenator Bernie Sanders reacts to Tesla shareholders‚Äô decision to award Elon Musk a $1 trillion pay package in a post on X.







One more thing







Finding forgotten Indigenous landscapes with electromagnetic technology



The fertile river valleys of the American Midwest hide tens of thousands of Indigenous earthworks, according to experts: geometric structures consisting of walls, mounds, ditches, and berms, some dating back nearly 3,000 years.Archaeologists now believe that the earthworks functioned as religious gathering places, tombs for culturally important clans, and annual calendars, perhaps all at the same time. They can take the form of giant circles and squares, cloverleafs and octagons, complex S-curves and simple mounds.Until recently, it seemed as if much of the continent‚Äôs pre-European archaeological heritage had been carelessly wiped out, uprooted, and lost for good. But traces remain: electromagnetic remnants in the soil that can be detected using specialty surveying equipment. And archaeologists and tribal historians are working together to uncover them. Read the full story.



‚ÄîGeoff Manaugh







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)



+ If you‚Äôre a wildlife fan, take a look at this compilation of the best places to catch a glimpse of unusual animals.+ El Salvador‚Äôs annual fireball festival is a completely unhinged celebration of all things volcanic.+ The most influential Bostonians of 2025 have been announced.+ Get me in a potato bed, stat.
‚Ä¢ The first new subsea habitat in 40 years is about to launch
  Vanguard feels and smells like a new RV. It has long, gray banquettes that convert into bunks, a microwave cleverly hidden under a counter, a functional steel sink with a French press and crockery above. A weird little toilet hides behind a curtain.



But some clues hint that you can‚Äôt just fire up Vanguard‚Äôs engine and roll off the lot. The least subtle is its door, a massive disc of steel complete with a wheel that spins to lock.



COURTESY MARK HARRIS




Once it is sealed and moved to its permanent home beneath the waves of the Florida Keys National Marine Sanctuary early next year, Vanguard will be the world‚Äôs first new subsea habitat in nearly four decades. Teams of four scientists will live and work on the seabed for a week at a time, entering and leaving the habitat as scuba divers. Their missions could include reef restoration, species surveys, underwater archaeology, or even astronaut training.&nbsp;



One of Vanguard‚Äôs modules, unappetizingly named the ‚Äúwet porch,‚Äù has a permanent opening in the floor (a.k.a. a ‚Äúmoon pool‚Äù) that doesn‚Äôt flood because Vanguard‚Äôs air pressure is matched to the water around it.&nbsp;



It is this pressurization that makes the habitat so useful. Scuba divers working at its maximum operational depth of 50 meters would typically need to make a lengthy stop on their way back to the surface to avoid decompression sickness. This painful and potentially fatal condition, better known as the bends, develops if divers surface too quickly. A traditional 50-meter dive gives scuba divers only a handful of minutes on the seafloor, and they can make only a couple of such dives a day. With Vanguard‚Äôs atmosphere at the same pressure as the water, its aquanauts need to decompress only once, at the end of their stay. They can potentially dive for many hours every day.



That could unlock all kinds of new science and exploration. ‚ÄúMore time in the ocean opens a world of possibility, accelerating discoveries, inspiration, solutions,‚Äù said Kristen Tertoole, Deep‚Äôs chief operating officer, at Vanguard‚Äôs unveiling in Miami in October. ‚ÄúThe ocean is Earth‚Äôs life support system. It regulates our climate, sustains life, and holds mysteries we‚Äôve only begun to explore, but it remains 95% undiscovered.‚Äù



COURTESY DEEP




Subsea habitats are not a new invention. Jacques Cousteau (naturally) built the first in 1962, although it was only about the size of an elevator. Larger habitats followed in the 1970s and ‚Äô80s, maxing out at around the size of Vanguard.



But the technology has come a long way since then. Vanguard uses a tethered connection to a buoy above, known as the ‚Äúsurface expression,‚Äù that pipes fresh air and water down to the habitat. It also hosts a diesel generator to power a Starlink internet connection and a tank to hold wastewater. Norman Smith, Deep‚Äôs chief technology officer, says the company modeled the most severe hurricanes that Florida expects over the next 20 years and designed the tether to withstand them. Even if the worst happens and the link is broken, Deep says, Vanguard has enough air, water, and energy storage to support its crew for at least 72 hours.



That number came from DNV, an independent classification agency that inspects and certifies all types of marine vessels so that they can get commercial insurance. Vanguard will be the first subsea habitat to get a DNV classification. ‚ÄúThat means you have to deal with the rules and all the challenging, frustrating things that come along with it, but it means that on a foundational level, it‚Äôs going to be safe,‚Äù says Patrick Lahey, founder of Triton Submarines, a manufacturer of classed submersibles.



JASON KOERNER/GETTY IMAGES FOR DEEP




Although Deep hopes Vanguard itself will enable decades of useful science, its prime function for the company is to prove out technologies for its planned successor, an advanced modular habitat called Sentinel. Sentinel modules will be six meters wide, twice the diameter of Vanguard, complete with sweeping staircases and single-occupant cabins. A small deployment might have a crew of eight, about the same as the International Space Station. A big Sentinel system could house 50, up to 225 meters deep. Deep claims that Sentinel will be launched at some point in 2027.



Ultimately, according to its mission statement, Deep seeks to ‚Äúmake humans aquatic,‚Äù an indication that permanent communities are on its long-term road map.&nbsp;



Deep has not publicly disclosed the identity of its principal funder, but business records in the UK indicate that as of January 31, 2025 a Canadian man, Robert MacGregor, owned at least 75% of its holding company. According to a Reuters investigation, MacGregor was once linked with Craig Steven Wright, a computer scientist who claimed to be Satoshi Nakamoto, as bitcoin‚Äôs elusive creator is pseudonymously known. However, Wright‚Äôs claims to be Nakamoto later collapsed.&nbsp;



MacGregor has kept a very low public profile in recent years. When contacted for comment, Deep spokesperson Mike Bohan refused to comment on the link with Wright, only to say it was inaccurate, but said: ‚ÄúRobert MacGregor started his career as an IP lawyer in the dot-com era, moving into blockchain technology and has diverse interests including philanthropy, real estate, and now Deep.‚Äù



In any case, MacGregor could find keeping that low profile more difficult if Vanguard is successful in reinvigorating ocean science and exploration as the company hopes. The habitat is due to be deployed early next year, following final operational tests at Triton‚Äôs facility in Florida. It will welcome its first scientists shortly after.&nbsp;



‚ÄúThe ocean is not just our resource; it is our responsibility,‚Äù says Tertoole. ‚ÄúDeep is more than a single habitat. We are building a full-stack capability for human presence in the ocean.‚Äù



JASON KOERNER/GETTY IMAGES FOR DEEP

üîí Cybersecurity & Privacy
‚Ä¢ Drilling Down on Uncle Sam‚Äôs Proposed TP-Link Ban
  The U.S. government is reportedly preparing to ban the sale of wireless routers and other networking gear from TP-Link Systems, a tech company that currently enjoys an estimated 50% market share among home users and small businesses. Experts say while the proposed ban may have more to do with TP-Link&#8217;s ties to China than any specific technical threats, much of the rest of the industry serving this market also sources hardware from China and ships products that are insecure fresh out of the box.
A TP-Link WiFi 6 AX1800 Smart WiFi Router (Archer AX20).
The Washington Post recently reported that more than a half-dozen federal departments and agencies were backing a proposed ban on future sales of TP-Link devices in the United States. The story said U.S. Department of Commerce officials concluded TP-Link Systems products pose a risk because the U.S.-based company‚Äôs products handle sensitive American data and because the officials believe it remains subject to jurisdiction or influence by the Chinese government.
TP-Link Systems denies that, saying that it fully split from the Chinese TP-Link Technologies over the past three years, and that its critics have vastly overstated the company&#8217;s market share (TP-Link puts it at around 30 percent). TP-Link says it has headquarters in California, with a branch in Singapore, and that it manufactures in Vietnam. The company says it researches, designs, develops and manufactures everything except its chipsets in-house.
TP-Link Systems told The Post it has sole ownership of some engineering, design and manufacturing capabilities in China that were once part of China-based TP-Link Technologies, and that it operates them without Chinese government supervision.
&#8220;TP-Link vigorously disputes any allegation that its products present national security risks to the United States,&#8221; Ricca Silverio, a spokeswoman for TP-Link Systems, said in a statement. &#8220;TP-Link is a U.S. company committed to supplying high-quality and secure products to the U.S. market and beyond.&#8221;
Cost is a big reason TP-Link devices are so prevalent in the consumer and small business market: As this February 2025 story from Wired observed regarding the proposed ban, TP-Link has long had a reputation for flooding the market with devices that are considerably cheaper than comparable models from other vendors. That price point (and consistently excellent performance ratings) has made TP-Link a favorite among Internet service providers (ISPs) that provide routers to their customers.
In August 2024, the chairman and the ranking member of the House Select Committee on the Strategic Competition Between the United States and the Chinese Communist Party called for an investigation into TP-Link devices, which they said were found on U.S. military bases and for sale at exchanges that sell them to members of the military and their families.
‚ÄúTP-Link‚Äôs unusual degree of vulnerabilities and required compliance with PRC law are in and of themselves disconcerting,&#8221; the House lawmakers warned in a letter (PDF) to the director of the Commerce Department. &#8220;When combined with the PRC government‚Äôs common use of SOHO [small office/home office] routers like TP-Link to perpetrate extensive cyberattacks in the United States, it becomes significantly alarming.‚Äù
The letter cited a May 2023 blog post by Check Point Research about a Chinese state-sponsored hacking group dubbed &#8220;Camaro Dragon&#8221; that used a malicious firmware implant for some TP-Link routers to carry out a sequence of targeted cyberattacks against European foreign affairs entities. Check Point said while it only found the malicious firmware on TP-Link devices, &#8220;the firmware-agnostic nature of the implanted components indicates that a wide range of devices and vendors may be at risk.&#8221;
In a report published in October 2024, Microsoft said it was tracking a network of compromised TP-Link small office and home office routers that has been abused by multiple distinct Chinese state-sponsored hacking groups since 2021. Microsoft found the hacker groups were leveraging the compromised TP-Link systems to conduct &#8220;password spraying&#8221; attacks against Microsoft accounts. Password spraying involves rapidly attempting to access a large number of accounts (usernames/email addresses) with a relatively small number of commonly used passwords.
TP-Link rightly points out that most of its competitors likewise source components from China. The company also correctly notes that advanced persistent threat (APT) groups from China and other nations have leveraged vulnerabilities in products from their competitors, such as Cisco and Netgear.
But that may be cold comfort for TP-Link customers who are now wondering if it&#8217;s smart to continue using these products, or whether it makes sense to buy more costly networking gear that might only be marginally less vulnerable to compromise.
Almost without exception, the hardware and software that ships with most consumer-grade routers includes a number of default settings that need to be changed before the devices can be safely connected to the Internet. For example, bring a new router online without changing the default username and password and chances are it will only take a few minutes before it is probed and possibly compromised by some type of Internet-of-Things botnet. Also, it is incredibly common for the firmware in a brand new router to be dangerously out of date by the time it is purchased and unboxed.
Until quite recently, the idea that router manufacturers should make it easier for their customers to use these products safely was something of anathema to this industry. Consumers were largely left to figure that out on their own, with predictably disastrous results.
But over the past few years, many manufacturers of popular consumer routers have begun forcing users to perform basic hygiene &#8212; such as changing the default password and updating the internal firmware &#8212; before the devices can be used as a router. For example, most brands of &#8220;mesh&#8221; wireless routers &#8212; like Amazon&#8217;s Eero, Netgear&#8217;s Orbi series, or Asus&#8217;s ZenWifi &#8212; require online registration that automates these critical steps going forward (or at least through their stated support lifecycle).
For better or worse, less expensive, traditional consumer routers like those from Belkin and Linksys also now automate this setup by heavily steering customers toward installing a mobile app to complete the installation (this often comes as a shock to people more accustomed to manually configuring a router). Still, these products tend to put the onus on users to check for and install available updates periodically. Also, they&#8217;re often powered by underwhelming or else bloated firmware, and a dearth of configurable options.
Of course, not everyone wants to fiddle with mobile apps or is comfortable with registering their router so that it can be managed or monitored remotely in the cloud. For those hands-on folks &#8212; and for power users seeking more advanced router features like VPNs, ad blockers and network monitoring &#8212; the best advice is to check if your router&#8217;s stock firmware can be replaced with open-source alternatives, such as OpenWrt¬†or DD-WRT.
These open-source firmware options are compatible with a wide range of devices, and they generally offer more features and configurability. Open-source firmware can even help extend the life of routers years after the vendor stops supporting the underlying hardware, but it still requires users to manually check for and install any available updates.
Happily, TP-Link users spooked by the proposed ban may have an alternative to outright junking these devices, as many TP-Link routers also support open-source firmware options like OpenWRT. While this approach may not eliminate any potential hardware-specific security flaws, it could serve as an effective hedge against more common vendor-specific vulnerabilities, such as undocumented user accounts, hard-coded credentials, and weaknesses that allow attackers to bypass authentication.
Regardless of the brand, if your router is more than four or five years old it may be worth upgrading for performance reasons alone &#8212; particularly if your home or office is primarily accessing the Internet through WiFi.
NB: The Post&#8217;s story notes that a substantial portion of TP-Link routers and those of its competitors are purchased or leased through ISPs. In these cases, the devices are typically managed and updated remotely by your ISP, and equipped with custom profiles responsible for authenticating your device to the ISP&#8217;s network. If this describes your setup, please do not attempt to modify or replace these devices without first consulting with your Internet provider.

üéì University AI
No updates.

üè¢ Corporate AI
‚Ä¢ When industry knowledge meets PIKE-RAG: The innovation behind Signify‚Äôs customer service boost
  As a world leader in connected LED lighting products, systems, and services, Signify (formerly Philips Lighting) serves not only everyday consumers but also a large number of professional users who have stringent requirements for technical specifications and engineering compatibility. Faced with thousands of product models, complex component parameters, and technical documentation spanning multiple versions, delivering accurate, professional answers efficiently has become a core challenge for Signify‚Äôs knowledge management system.



To address this challenge, Signify (opens in new tab) collaborated with Microsoft Research Asia on a proof-of-concept (PoC) using PIKE-RAG technology, integrating it into their upgraded knowledge management system built on Microsoft Azure. The result: a 12% improvement in answer accuracy.



Challenges of applying RAG in lighting



In an era where AI is rapidly transforming how enterprises manage information, Signify recognized the strategic importance of precise and efficient knowledge systems. It adopted large AI models and retrieval-augmented generation (RAG) techniques to better support its wide range of customer inquiries.



Yet applying RAG to lighting scenarios involving professional users presented unique challenges. Product data spanned multimodal documents, unstructured tables, and complex product parameters, demanding continuous customization that slowed development and limited scalability. Despite improvements through keyword tuning, system optimization, and refined prompts, Signify sought more advanced approaches to further raise accuracy and reliability.



Seeking to unlock greater value from its knowledge management system, Signify began exploring more suitable technical solutions that are better aligned with their professional use cases. Upon learning that PIKE-RAG had been successfully applied in domains like healthcare and law, significantly improving information accuracy, Signify worked with Microsoft Research Asia on a PoC of PIKE-RAG on Microsoft Azure.



How PIKE-RAG addressed Signify‚Äôs pain points



Compared to traditional RAG, PIKE-RAG efficiently retrieves textual information and also understands multimodal content like charts and tables. Its built-in domain adaptation module quickly learns reasoning patterns aligned with specific domains to generate responses that are consistent with engineering contexts. These differentiated advantages stem from PIKE-RAG‚Äôs unique approach to understanding and processing professional knowledge. In Signify‚Äôs use case, this manifests in three key areas:



Multimodal document parsing and learning of industry-specific reasoning patterns



Signify‚Äôs product documentation includes diverse formats, such as nonstandard tables (e.g., comparison charts of voltage ranges under different currents) and circuit diagrams (e.g., driver power limits). Traditional systems often fail to process this information effectively‚Äîeither ignoring it or extracting disorganized text fragments.



PIKE-RAG integrates Microsoft Research Asia‚Äôs Document Intelligence technology with Microsoft Azure OpenAI models to accurately identify table structures and parse key parameters in circuit diagrams. For example, when a customer service agent queries, ‚ÄúWhat is the output voltage of a specific driver model at 0.15A current,‚Äù the system automatically locates the curve chart in the document and infers a range of 40‚Äì54V based on the current interval‚Äîan area where traditional systems frequently err, due to their inability to ‚Äúread‚Äù diagrams.



End-to-end knowledge loop, eliminating reliance on erroneous data sources



Enterprise knowledge systems often integrate data from multiple sources, which can lead to discrepancies, especially when database updates are not fully synchronized. PIKE-RAG captures diverse information sources and establishes citation relationships, supporting complex reasoning tasks that rely on multi-source data.



In other words, PIKE-RAG can directly use original documents as data sources, efficiently parsing and understanding product manuals and PDF charts. By extracting key information from these text- and graphic-rich documents, PIKE-RAG enables more efficient and trustworthy knowledge retrieval.



Dynamic task decomposition and multi-hop reasoning for precise answers to complex questions



Traditional RAG systems typically follow a ‚Äúone question, one answer‚Äù model and struggle with multi-step reasoning. In Signify‚Äôs lighting domain, customer inquiries often involve multi-level associations. PIKE-RAG dynamically decomposes user questions into executable subtasks and solves them through multi-hop reasoning. For example, when asked, ‚ÄúList all bases compatible with the G8 series lamps,‚Äù if no document directly provides the answer, PIKE-RAG‚Äôs reasoning proceeds as follows:



Step 1: The system identifies implicit knowledge. One document notes that the G7 and G8 series have identical dimensions and that all bases compatible with the G7 series are also compatible with the G8 series.&nbsp;



Step 2: Based on this, the system retrieves the base list for the G7 series.&nbsp;



Step 3: Since the list uses abbreviations, the system searches for a table that maps abbreviations to full names and generates a complete list of G8-compatible bases.&nbsp;



Through this automated multi-hop reasoning, the system delivers accurate and complete answers.



Figure 1: PIKE-RAG orchestrates and integrates heterogeneous information in multi-source and multimodal environments. 



Testing showed that the PIKE-RAG-powered knowledge management platform provided a significant advantage. It achieved a 12% improvement in performance compared with the original system.



These results were achieved without any question-specific customization, only algorithmic optimization, demonstrating precise knowledge matching and generation. As the system continues to learn and integrate Signify‚Äôs proprietary knowledge, accuracy is expected to improve further.



‚ÄúIn the PoC for our product specification insight tool, PIKE-RAG helped us significantly improve the original system‚Äôs performance. This will enhance overall customer satisfaction. We‚Äôre currently evaluating PIKE-RAG‚Äôs application path from multiple angles, including technical implementation, cost control, and future adaptability, and we look forward to deepening our collaboration with Microsoft Research Asia to drive further innovation,‚Äù said Haitao Liu, head of Signify Research China.



‚ÄúIt‚Äôs also worth noting that the researchers at Microsoft Research Asia demonstrated strong industry knowledge and rigorous scientific methodology. They proactively studied and analyzed the issues, tracing and clarifying the root causes of our issues to make PIKE-RAG better suited to Signify‚Äôs real-world needs.‚Äù



Beyond lighting: Generalization across industries



In Signify‚Äôs successful test, PIKE-RAG demonstrated strong generalization capabilities in complex industrial scenarios, enabling rapid cross-domain adaptation. Its three core strengths are:




Support for self-evolution and continuous learning: PIKE-RAG continuously analyzes error cases in interaction logs and uses evolutionary algorithms to automatically optimize knowledge extraction strategies, such as trying different table parsing methods or adjusting multimodal content weights. Validated strategies are then solidified for future Q&amp;A, allowing the system to adapt to new knowledge types without manual intervention.&nbsp;



Modular architecture driven by capability needs: PIKE-RAG flexibly combines modules for document parsing, knowledge extraction, storage, retrieval, organization, knowledge-centered reasoning, and task decomposition. It dynamically adjusts focus areas based on scenario needs (e.g., fact retrieval, multi-hop reasoning, innovative generation) and flexibly builds RAG methods that adapt to real-world applications, efficiently handling various complex tasks.&nbsp;



Strong adaptation to domain-specific reasoning patterns: With dynamic updates through the Domain Tips feature, enterprises can add domain-specific logic (e.g., ‚Äúthe maximum output voltage of an LED driver should be the maximum of the operating range, not the spec sheet‚Äôs max output‚Äù) in real time, enabling the system to process information according to professional engineering standards and follow industry conventions.&nbsp;




Figure 2: Overview of the PIKE-RAG framework



PIKE-RAG‚Äôs generalization capabilities have been validated not only in Signify‚Äôs knowledge management platform but also in pilot applications across industries like manufacturing, mining, and pharmaceuticals‚Äîsignificantly improving Q&amp;A system accuracy.



‚ÄúA leader in lighting, Signify presents a complex industrial knowledge system with a highly challenging real-world scenario for PIKE-RAG. Through this collaboration, we validated that PIKE-RAG‚Äôs general approach can greatly improve the accuracy of professional knowledge Q&amp;A and accelerate scenario customization. Our researchers also gained valuable experience in handling domain-specific data,‚Äù explained Jiang Bian, partner research manager at Microsoft Research Asia.



‚ÄúOur goal isn‚Äôt to build a universal chatbot but to create a professional assistant that aligns with domain-specific logic and performs rigorous knowledge reasoning. That‚Äôs the true driving force behind intelligent transformation in industrial knowledge management.‚Äù
Opens in a new tabThe post When industry knowledge meets PIKE-RAG: The innovation behind Signify‚Äôs customer service boost appeared first on Microsoft Research.
‚Ä¢ Fine-tune VLMs for multipage document-to-JSON with SageMaker AI and SWIFT
  Extracting structured data from documents like invoices, receipts, and forms is a persistent business challenge. Variations in format, layout, language, and vendor make standardization difficult, and manual data entry is slow, error-prone, and unscalable. Traditional optical character recognition (OCR) and rule-based systems often fall short in handling this complexity. For instance, a regional bank might need to process thousands of disparate documents‚Äîloan applications, tax returns, pay stubs, and IDs‚Äîwhere manual methods create bottlenecks and increase the risk of error. Intelligent document processing (IDP) aims to solve these challenges by using AI to classify documents, extract or derive relevant information, and validate the extracted data to use it in business processes. One of its core goals is to convert unstructured or semi-structured documents into usable, structured formats such as JSON, which then contain specific fields, tables, or other structured target information. The target structure needs to be consistent, so that it can be used as part of workflows or other downstream business systems or for reporting and insights generation. The following figure shows the workflow, which involves ingesting unstructured documents (for example, invoices from multiple vendors with varying layouts) and extracting relevant information. Despite differences in keywords, column names, or formats across documents, the system normalizes and outputs the extracted data into a consistent, structured JSON format. 
 
Vision language models (VLMs) mark a revolutionary advancement in IDP. VLMs integrate large language models (LLMs) with specialized image encoders, creating truly multi-modal AI capabilities of both textual reasoning and visual interpretation. Unlike traditional document processing tools, VLMs process documents more holistically‚Äîsimultaneously analyzing text content, document layout, spatial relationships, and visual elements in a manner that more closely resembles human comprehension. This approach enables VLMs to extract meaning from documents with unprecedented accuracy and contextual understanding. For readers interested in exploring the foundations of this technology, Sebastian Raschka‚Äôs post‚ÄîUnderstanding Multimodal LLMs‚Äîoffers an excellent primer on multimodal LLMs and their capabilities. 
This post has four main sections that reflect the primary contributions of our work and include: 
 
 An overview of the various IDP approaches available, including the option (our recommended solution) for fine-tuning as a scalable approach. 
 Sample code for fine-tuning VLMs for document-to-JSON conversion using Amazon SageMaker AI and the SWIFT framework, a lightweight toolkit for fine-tuning various large models. 
 Developing an evaluation framework to assess performance processing structured data. 
 A discussion of the possible deployment options, including an explicit example for deploying the fine-tuned adapter. 
 
SageMaker AI is a fully managed service to build, train and deploy models at scale. In this post, we use SageMaker AI to fine-tune the VLMs and deploy them for both batch and real-time inference. 
Prerequisites 
Before you begin, make sure you have the following set up so that you can successfully follow the steps outlined in this post and the accompanying GitHub repository: 
 
 AWS account: You need an active AWS account with permissions to create and manage resources in SageMaker AI, Amazon Simple Storage Service (Amazon S3), and Amazon Elastic Container Registry (Amazon ECR). 
 IAM permissions: Your IAM user or role must have sufficient permissions. For production setups, follow the principle of least privilege as described in security best practices in IAM. For a sandbox setup we suggest the following roles: 
   
   Full access to Amazon SageMaker AI (for example, AmazonSageMakerFullAccess). 
   Read/write access to S3 buckets for storing datasets and model artifacts. 
   Permissions to push and pull Docker images from Amazon ECR (for example, AmazonEC2ContainerRegistryPowerUser). 
   If using specific SageMaker instance types, make sure your service quotas are sufficient. 
    
 GitHub repository: Clone or download the project code from our GitHub repository. This repository contains the notebooks, scripts, and Docker artifacts referenced in this post. 
   
    
     
     git clone https://github.com/aws-samples/sample-for-multi-modal-document-to-json-with-sagemaker-ai.git 
      
    
 Local environment set up: 
   
   Python: Python 3.10 or higher is recommended. 
   AWS CLI: Make sure the AWS Command Line Interface (AWS CLI) is installed and configured with credentials that have the necessary permissions. 
   Docker: Docker must be installed and running on your local machine if you plan to build the custom Docker container for deployment. 
   Jupyter Notebook and Lab: To run the provided notebooks. 
   Install the required Python packages by running pip install -r requirements.txt from the cloned repository‚Äôs root directory. 
    
 Familiarity (recommended): 
   
   Basic understanding of Python programming. 
   Familiarity with AWS services, particularly SageMaker AI. 
   Conceptual knowledge of LLMs, VLMs, and the container technology will be beneficial. 
    
 
Overview of document processing and generative AI approaches 
There are varying degrees of autonomy in intelligent document processing. On one end of the spectrum are fully manual processes: Humans manually reading documents and entering the information into a form using a computer system. Most systems today are semi-autonomous document processing solutions. For example, a human taking a picture of a receipt and uploading it to a computer system that automatically extracts part of the information. The goal is to get to fully autonomous intelligent document processing systems. This means reducing the error rate and assessing the use case specific risk of errors.&nbsp;AI is significantly transforming document processing by enabling greater levels of automation. A variety of approaches exist, ranging in complexity and accuracy‚Äîfrom specialized models for OCR, to generative AI. 
Specialized OCR models that don‚Äôt rely on generative AI are designed as pre-trained, task-specific ML models that excel at extracting structured information such as tables, forms, and key-value pairs from common document types like invoices, receipts, and IDs. Amazon Textract is one example of this type of service. This service offers high accuracy out of the box and requires minimal setup, making it well-suited for workloads where basic text extraction is required, and documents don‚Äôt vary significantly in structure or contain images. 
However, as you increase the complexity and variability of documents, in addition to adding multimodality, using generative AI can help improve document processing pipelines. 
While powerful, applying general-purpose VLMs or LLMs to document processing isn‚Äôt straightforward. Effective prompt engineering is important to guide the model. Processing large volumes of documents (scaling) requires efficient batching and infrastructure. Because LLMs are stateless, providing historical context or specific schema requirements for every document can be cumbersome. 
Approaches to intelligent document processing that use LLMs or VLMs fall into four categories: 
 
 Zero-shot prompting: the foundation model (FM) receives the result of previous OCR or a PDF and the instructions to perform the document processing task. 
 Few-shot prompting: the FM receives the result of previous OCR or a PDF, the instructions to perform the document processing task, and some examples. 
 Retrieval-augmented few-shot prompting: similar to the preceding strategy, but the examples sent to the model are selected dynamically using Retrieval Augmented Generation (RAG). 
 Fine-tuning VLMs 
 
In the following, you can see the relationship between increasing effort and complexity and task accuracy, demonstrating how different techniques‚Äîfrom basic prompt engineering to advanced fine-tuning‚Äîimpact the performance of large and small base models compared to a specialized solution (inspired by the blog post Comparing LLM fine-tuning methods) 
 
As you move across the horizontal axis, the strategies grow in complexity, and as you move up the vertical axis, you improve overall accuracy. In general, large base models provide better performance than small base models in the strategies that require prompt engineering, however as we explain in the results of this post, fine-tuning small base models can deliver similar results as fine-tuning large base models for a specific task. 
Zero-shot prompting 
Zero-shot prompting is a technique to use language models where the model is given a task without prior examples or fine-tuning. Instead, it relies solely on the prompt‚Äôs wording and its pre-trained knowledge to generate a response.&nbsp;In document processing, this approach involves giving the model either an image of a PDF document, the OCR-extracted text from the PDF, or a structured markdown representation of the document and providing instructions to perform the document processing task, in addition to the desired output format. 
Amazon Bedrock Data Automation&nbsp;uses zero-shot prompting with generative AI to perform IDP. You can use Bedrock Data Automation to automate the transformation of multi-modal data‚Äîincluding documents containing text and complex structures, such as tables, charts and images‚Äîinto structured formats. You can benefit from customization capabilities through the creation of blueprints that specify output requirements using natural language or a schema editor. Bedrock Data Automation can also extract bounding boxes for the identified entities and route documents appropriately to the correct blueprint. These features can be configured and used through a single API, making it significantly more powerful than a basic zero-shot prompting approach. 
While out-of-the-box VLMs can handle general OCR tasks effectively, they often struggle with the unique structure and nuances of custom documents‚Äîsuch as invoices from diverse vendors. Although crafting a prompt for a single document might be straightforward, the variability across hundreds of vendor formats makes prompt iteration a labor-intensive and time-consuming process. 
Few-shot prompting 
Moving to a more complex approach, you have few-shot prompting, a technique used with LLMs where a small number of examples are provided within the prompt to guide the model in completing a specific task. Unlike zero-shot prompting, which relies solely on natural language instructions, few-shot prompting improves accuracy and consistency by demonstrating the desired input-output behavior through examples. 
One alternative is to use the Amazon Bedrock Converse API to perform few shot prompting. Converse API&nbsp;provides a consistent way to access LLMs using Amazon Bedrock. It supports turn-based messages between the user and the generative AI model, and allows including documents as part of the content. Another option is using Amazon SageMaker Jumpstart, which you can use to deploy models from providers like HuggingFace. 
However, most likely your business needs to process different types of documents (for example, invoices, contracts and hand written notes) and even within one document type there are many variations, for example, there is not one standardized invoice layout and instead each vendor has their own layout that you cannot control. Finding a single or a few examples that cover all the different documents you want to process is challenging. 
Retrieval-augmented few-shot prompting 
One way to address the challenge of finding the right examples is to dynamically retrieve previously processed documents as examples and add them to the prompt at runtime (RAG). 
You can store a few annotated samples in a vector store and retrieve them based on the document that needs to be processed. Amazon Bedrock Knowledge Bases helps you implement the entire RAG workflow from ingestion to retrieval and prompt augmentation without having to build custom integrations to data sources and manage data flows. 
This turns the intelligent document processing problem into a search problem, which comes with its own challenges on how to improve the accuracy of the search. In addition to how to scale for multiple types of documents, the few-shot approach is costly because every document processed requires a longer prompt with examples. This results in an increased number of input tokens. 
 
As shown in the preceding figure, the prompt context will vary based on the strategy selected (zero-shot, few-shot or few-shot with RAG), which will overall change the results obtained. 
Fine-tuning VLMs 
At the end of the spectrum, you have the option to fine-tune a custom model to perform document processing.&nbsp;This is our recommended approach and what we focus on in this post. Fine-tuning is a method where a pre-trained LLM is further trained on a specific dataset to specialize it for a particular task or domain. In the context of document processing, fine-tuning involves using labeled examples‚Äîsuch as annotated invoices, contracts, or insurance forms‚Äîto teach the model exactly how to extract or interpret relevant information. Usually, the labor-intensive part of fine-tuning is acquiring a suitable, high-quality dataset. In the case of document processing, your company probably already has a historic dataset in its existing document processing system. You can export this data from your document processing system (for example from your enterprise resource planning (ERP) system) and use it as the dataset for fine-tuning. This fine-tuning approach is what we focus on in this post as a scalable, high accuracy, and cost-effective approach for intelligent document processing. 
The preceding approaches represent a spectrum of strategies to improve LLM performance along two axes: LLM optimization (shaping model behavior through prompt engineering or fine-tuning) and context optimization (enhancing what the model knows at inference through techniques such as few-shot learning or RAG). These methods can be combined‚Äîfor example, using RAG with few-shot prompts or incorporating retrieved data into fine-tuning‚Äîto maximize accuracy. 
Fine-tuning VLMs for document-to-JSON conversion 
Our approach‚Äîthe recommended solution for cost-effective document-to-JSON conversion‚Äîuses a VLM and fine-tunes it using a dataset of historical documents paired with their corresponding ground-truth JSON that we consider as annotations. This allows the model to learn the specific patterns, fields, and output structure relevant to your historic data, effectively teaching it to read your documents and extract information according to your desired schema. 
The following figure shows a high-level architecture of the document-to-JSON conversion process for fine-tuning VLMs by using historic data. This allows the VLM to learn from high data variations and helps ensure that the structured output matches the target system structure and format.&nbsp; 
 
Fine-tuning offers several advantages over relying solely on OCR or general VLMs: 
 
 Schema adherence: The model learns to output JSON matching a specific target structure, which is vital for integration with downstream systems like ERPs. 
 Implicit field location: Fine-tuned VLMs often learn to locate and extract fields without explicit bounding box annotations in the training data, simplifying data preparation significantly. 
 Improved text extraction quality: The model becomes more accurate at extracting text even from visually complex or noisy document layouts. 
 Contextual understanding: The model can better understand the relationships between different pieces of information on the document. 
 Reduced prompt engineering: Post fine-tuning, the model requires less complex or shorter prompts because the desired extraction behavior is built into its weights. 
 
For our fine-tuning process, we selected the Swift framework. Swift provides a comprehensive, lightweight toolkit for fine-tuning various large language models, including VLMs like Qwen-VL and Llama-Vision. 
Data preparation 
To fine-tune the VLMs, you will use the Fatura2 dataset, a multi-layout invoice image dataset comprising 10,000 invoices with 50 distinct layouts. 
The Swift framework expects training data in a specific JSONL (JSON Lines) format. Each line in the file is a JSON object representing a single training example. For multimodal tasks, this JSON object typically includes: 
 
 messages: A list of conversational turns (for example, system, user, assistant). The user turn contains placeholders for images (for example, &lt;image&gt;) and the text prompt that guides the model. The assistant turn contains the target output, which in this case is the ground-truth JSON string. 
 images: A list of relative paths‚Äîwithin the dataset directory structure‚Äîto the document page images (JPG files) relevant to this training example. 
 
As with standard ML practice, the dataset is split into training, development (validation), and test sets to effectively train the model, tune hyperparameters, and evaluate its final performance on unseen data. Each document (which could be single-page or multi-page) paired with its corresponding ground-truth JSON annotation constitutes a single row or example in our dataset. In our use case, one training sample is the invoice image (or multiple images of document pages) and the corresponding detailed JSON extraction. This one-to-one mapping is essential for supervised fine-tuning. 
The conversion process, detailed in the dataset creation notebook from the associated GitHub repo, involves several key steps: 
 
 Image handling: If the source document is a PDF, each page is rendered into a high-quality PNG image. 
 Annotation processing (fill missing values):&nbsp;We apply light pre-processing to the raw JSON annotation. Fine-tuning multiple models on an open source dataset, we observed that the performance increases when all keys are present in every JSON sample. To maintain this consistency, the target JSONs in the dataset are made to include the same set of top-level keys (derived from the entire dataset). If a key is missing for a particular document, it‚Äôs added with a null value. 
 Key ordering: The keys within the processed JSON annotation are sorted alphabetically. This consistent ordering helps the model learn a stable output structure. 
 Prompt construction: A user prompt is constructed. This prompt includes &lt;image&gt; tags (one for each page of the document) and explicitly lists the JSON keys the model is expected to extract. Including the JSON keys in the prompts improves the fine-tuned model‚Äôs performance. 
 Swift formatting: These components (prompt, image paths, target JSON) are assembled into the Swift JSONL format. Swift datasets support multimodal inputs, including images, videos and audios. 
 
The following is an example structure of a single training instance in Swift‚Äôs JSONL format, demonstrating how multimodal inputs are organized. This includes conversational messages, paths to images, and objects containing bounding box (bbox) coordinates for visual references within the text. For more information about how to create a custom dataset for Swift, see the Swift documentation. 
 
  {
  "messages": [
    {"role": "system", "content": "Task definition"},
    {"role": "user", "content": "&lt;image&gt;&lt;image&gt;... + optional text prompt"},
    {"role": "assistant", "content": "JSON or text output with extracted data with &lt;bbox&gt; references."}
  ],
  "images": ["path/to/image1.png", "path/to/image2.png"]
  "objects": {"ref": [], "bbox": [[90.9, 160.8, 135, 212.8], [360.9, 480.8, 495,   532.8]]} #Optional
 }
 
 
Fine-tuning frameworks and resources 
In our evaluation of fine-tuning frameworks for use with SageMaker AI, we considered several prominent options highlighted in the community and relevant to our needs. These included Hugging Face Transformers, Hugging Face Autotrain, Llama Factory, Unsloth, Torchtune, and ModelScope SWIFT (referred to simply as SWIFT in this post, aligning with the SWIFT 2024 paper by Zhao and others.). 
After experimenting with these, we decided to use SWIFT because of its lightweight nature, comprehensive support for various Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA and DoRA, and its design tailored for efficient training of a wide array of models, including the VLMS used in this post (for example, Qwen-VL 2.5). Its scripting approach integrates seamlessly with SageMaker AI training jobs, allowing for scalable and reproducible fine-tuning runs in the cloud. 
There are several strategies for adapting pre-trained models: full fine-tuning, where all model parameters are updated, PEFT, which offers a more efficient alternative by updating only a small new number of parameters (adapters), and quantization, a technique that reduces model size and speeds up inference using lower-precision formats (see Sebastian Rashka‚Äôs post on fine-tuning to learn more about each technique). 
Our project uses LoRA and DoRA, as configured in the fine-tuning notebook. 
The following is an example of configuring and running a fine-tuning job (LoRA) as a SageMaker AI training job using SWIFT and remote function. When executing this function, the fine-tuning will be executed remotely as a SageMaker AI training job. 
 
 from sagemaker.remote_function import remote 
import json 
import os
@remote (instance_type="ml.g6e.12xlarge", volume_size=200, use_spot_instances=True)
def fine_tune_document (training_data_s3, train_data_path="train.jsonl" , validation_data_path="validation.jsonl"):
    from swift.llm.sft import lim_sft, get_sft_main 
    from swift.llm import sft_main
    
    ## copy the training data from input source to local directory
        ...
    train_data_local_path = ...
    validation_data_local_path = ...
    # set and run the fine-tuning using ms-swift framework
    os.environ["SIZE_FACTOR"] = json.dumps(8)# can be increase but requires more GPU memory
    os.environ["MAX_PIXELS"]= json.dumps (602112) # can be increase but requires more GPU memory os. environ ["CUDA_VISIBLE_DEVICES"]="0,1,2,3" # GPU devices to be used os. environ ["NPROC_PER_NODE"]="4" # we have 4 GPUs on on instance
    os.environ["USE_H_TRANSFER"] = json.dumps (1)
    argv = ['‚Äîmodel_type', 'qwen2_5_vl',
    '-model_id_or_path', 'Qwen/Qwen2.5-VL-3B-Instruct'
    '--train_type', 'lora'
    '--use_dora', 'true'
    '-output_dir', checkpoint_dir,
    '‚Äîmax_length', '4096'
    '-dataset', train_data_local_path,
    '--val_dataset', validation_data_local_path,
	...
    ]
    
    sft_main (argv)
## potentially evaluate inference on test dataset return "done" 
 
Fine-tuning VLMs typically requires GPU instances because of their computational demands. For models like Qwen2.5-VL 3B, an instance such as an Amazon SageMaker AI ml.g5.2xlarge or ml.g6.8xlarge can be suitable. Training time is a function of dataset size, model size, batch size, number of epochs, and other hyperparameters. For instance, as noted in our project readme.md, fine-tuning Qwen2.5 VL 3B on 300 Fatura2 samples took approximately 2,829 seconds (roughly 47 minutes) on an ml.g6.8xlarge instance using Spot pricing. This demonstrates how smaller models, when fine-tuned effectively, can deliver exceptional performance cost-efficiently. Larger models like Llama-3.2-11B-Vision would generally require more substantial GPU resources (for example, ml.g5.12xlarge or larger) and longer training times. 
Evaluation and visualization of structured outputs (JSON) 
A key aspect of any automation or machine learning project is evaluation. Without evaluating your solution, you don‚Äôt know how well it performs at solving your business problem. We wrote an evaluation notebook that you can use as a framework.&nbsp;Evaluating the performance of document-to-JSON models involves comparing the model-generated JSON outputs for unseen input documents (test dataset) against the ground-truth JSON annotations. 
Key metrics employed in our project include: 
 
 Exact match (EM) ‚Äì accuracy: This metric measures whether the extracted value for a specific field is an exact character-by-character match to the ground-truth value. It‚Äôs a strict metric, often reported as a percentage. 
 Character error rate (CER) ‚Äì edit distance: calculates the minimum number of single-character edits (insertions, deletions, or substitutions) required to change the model‚Äôs predicted string into the ground-truth string, typically normalized by the length of the ground-truth string. A lower CER indicates better performance. 
 Recall-Oriented Understudy for Gisting Evaluation (ROGUE): This is a suite of metrics that compare n-grams (sequences of words) and the longest common subsequence between the predicted output and the reference. While traditionally used for text summarization, ROUGE scores can also provide insights into the overall textual similarity of the generated JSON string compared to the ground truth. 
 
Visualizations are helpful for understanding model performance nuances. The following edit distance heatmap image provides a granular view, showing how closely the predictions match the ground truth (green means the model‚Äôs output exactly matches the ground truth, and shades of yellow, orange, and red depict increasing deviations). Each model has its own bar chart, allowing quick comparison across models.&nbsp;The X-axis is the number of sample documents. In this case, we ran inference on 250 unseen samples from the Fatura2 dataset. The Y-axis shows the JSON keys that we asked the model to extract; which will be different for you depending on what structure your downstream system requires. 
In the image, you can see the performance of three different models on the Fatura2 dataset. From left to right: Qwen2.5 VL 3B fine-tuned on 300 samples from the Fatura2 dataset, in the middle Qwen2.5 VL 3B without fine-tuning (labeled vanilla), and Llama 3.2 11B vision fine-tuned on 1,000 samples. 
The grey color shows the samples for which the Fatura2 dataset doesn‚Äôt contain any ground truth, which is why those are the same across the three models. 
For a detailed, step-by-step walk-through of how the evaluation metrics are calculated, the specific Python code used, and how the visualizations are generated, see the comprehensive evaluation notebook in our project. 
 
The image shows that Qwen2.5 vanilla is only decent at extracting the Title and Seller Name from the document. For the other keys it makes more than six character edit mistakes. However, out of the box Qwen2.5 is good at adhering to the JSON schema with only a few predictions where the key is missing (dark blue color) and no predictions of JSON that couldn‚Äôt be parsed (for example, missing quotation marks, missing parentheses, or a missing comma). Examining the two fine-tuned models, you can see improvement in performance with most samples, exactly matching the ground truth on all keys. There are only slight differences between fine-tuned Qwen2.5 and fine-tuned Llama 3.2, for example fine-tuned Qwen2.5 slightly outperforms fine-tuned Llama 3.2 on Total, Title, Conditions, and Buyer; whereas fine-tuned Llama 3.2 slightly outperforms fine-tuned Qwen2.5 on Seller Address, Discount, Tax, and Discount. 
The goal is to input a document into your fine-tuned model and receive a clean, structured JSON object that accurately maps the extracted information to predefined fields. JSON-constrained decoding enforces adherence to a specified JSON schema during inference and is useful to make sure the output is valid JSON. For the Fatura2 dataset, this approach was not necessary‚Äîour fine-tuned Qwen 2.5 model consistently produced valid JSON outputs without additional constraints. However, incorporating constrained decoding remains a valuable safeguard, particularly for production environments where output reliability is critical. 
Notebook 07 visualizes the input document and the extracted JSON data side-by-side. 
Deploying the fine-tuned model 
After you fine-tune a model and evaluate it on your dataset, you will want to deploy it to run inference to process your documents. Depending on your use case, a different deployment option might be more suitable. 
Option a: vLLM container extended for SageMaker 
To deploy our fine-tuned model for real-time inference, we use SageMaker endpoints. SageMaker endpoints provide fully managed hosting for real-time inference for FMs, deep learning, and other ML models and allows managed autoscaling and cost optimal deployment techniques. The process, detailed in our deploy model notebook, involves building a custom Docker container. This container packages the vLLM serving engine, highly optimized for LLM and VLM inference, along with the Swift framework components needed to load our specific model and adapter. vLLM provides an OpenAI-compatible API server by default, suitable for handling document and image inputs with VLMs. Our custom docker-artifacts and Dockerfile adapts this vLLM base for SageMaker deployment. Key steps include: 
 
 Setting up the necessary environment and dependencies. 
 Configuring an entry point that initializes the vLLM server. 
 Making sure the server can load the base VLM and dynamically apply our fine-tuned LoRA adapter. The Amazon S3 path to the adapter (model.tar.gz) is passed using the ADAPTER_URI environment variable when creating the SageMaker model. 
 The container, after being built and pushed to Amazon ECR, is then deployed to a SageMaker endpoint, which listens for invocation requests and routes them to the vLLM engine inside the container. 
 
The following image shows a SageMaker vLLM deployment architecture, where a custom Docker container from Amazon ECR is deployed to a SageMaker endpoint. The container uses vLLM‚Äôs OpenAI-compatible API and Swift to serve a base VLM with a fine-tuned LoRA adapter dynamically loaded from Amazon S3. 
 
Option b (optional): Inference components on SageMaker 
For more complex inference workflows that might involve sophisticated pre-processing of input documents, post-processing of the extracted JSON, or even chaining multiple models (for example, a classification model followed by an extraction model), Amazon SageMaker inference components offer enhanced flexibility. You can use them to build a pipeline of multiple containers or models within a single endpoint, each handling a specific part of the inference logic. 
Option c: Custom model inference in Amazon Bedrock 
You can now import your custom models in Amazon Bedrock and then use Amazon Bedrock features to make inference calls to the model. Qwen 2.5 architecture is supported (see Supported Architectures). For more information, see Amazon Bedrock Custom Model Import now generally available. 
Clean up 
To avoid ongoing charges, it‚Äôs important to remove the AWS resources created for this project when you‚Äôre finished. 
 
 SageMaker endpoints and models: 
   
   In the AWS Management Console for SageMaker AI, go to Inference and then Endpoints. Select and delete endpoints created for this project. 
   Then, go to Inference and then Models and delete the associated models. 
    
 Amazon S3 data: 
   
   Navigate to the Amazon S3 console. 
   Delete the S3 buckets or specific folders or prefixes used for datasets, model artifacts (for example, model.tar.gz from training jobs), and inference results. Note: Make sure you don‚Äôt delete data needed by other projects. 
    
 Amazon ECR images and repositories: 
   
   In the Amazon ECR console, delete Docker images and the repository created for the custom vLLM container if you deployed one. 
    
 CloudWatch logs (optional): 
   
   Logs from SageMaker activities are stored in Amazon CloudWatch. You can delete relevant log groups (for example, /aws/sagemaker/TrainingJobsand /aws/sagemaker/Endpoints) if desired, though many have automatic retention policies. 
    
 
Important: Always verify resources before deletion. If you experimented with Amazon Bedrock custom model imports, make sure those are also cleaned up. Use AWS Cost Explorer to monitor for unexpected charges. 
Conclusion and future outlook 
In this post, we demonstrated that fine-tuning VLMs provides a powerful and flexible approach to automate and significantly enhance document understanding capabilities. We have also demonstrated that using focused fine-tuning allows smaller, multi-modal models to compete effectively with much larger counterparts (98% accuracy with Qwen2.5 VL 3B). The project also highlights that fine-tuning VLMs for document-to-JSON processing can be done cost-effectively by using Spot instances and PEFT methods (approximately $1 USD to fine-tune a 3 billion parameter model on around 200 documents). 
The fine-tuning task was conducted using Amazon SageMaker training jobs and the Swift framework, which proved to be a versatile and effective toolkit for orchestrating this fine-tuning process. 
The potential for enhancing and expanding this work is vast. Some exciting future directions include deploying structured document models on CPU-based, serverless compute like AWS Lambda or Amazon SageMaker Serverless Inference using tools like llama.cpp or vLLM. Using quantized models can enable low-latency, cost-efficient inference for sporadic workloads. Another future direction includes improving evaluation of structured outputs by going beyond field-level metrics. This includes validating complex nested structures and tables using methods like tree edit distance for tables (TEDS). 
The complete code repository, including the notebooks, utility scripts, and Docker artifacts, is&nbsp;available on GitHub to help you get started unlocking insights from your documents. For a similar approach, using Amazon Nova, please refer to this AWS blog for optimizing document AI and structured outputs by fine-tuning Amazon Nova Models and on-demand inference. 
 
About the Authors 
Arlind Nocaj is a GTM Specialist Solutions Architect for AI/ML and Generative AI for Europe central based in AWS Zurich Office, who guides enterprise customers through their digital transformation journeys. With a PhD in network analytics and visualization (Graph Drawing) and over a decade of experience as a research scientist and software engineer, he brings a unique blend of academic rigor and practical expertise to his role. His primary focus lies in using the full potential of data, algorithms, and cloud technologies to drive innovation and efficiency. His areas of expertise include Machine Learning, Generative AI and in particular Agentic systems with Multi-modal LLMs for document processing and structured insights. 
Malte Reimann is a Solutions Architect based in Zurich, working with customers across Switzerland and Austria on their cloud initiatives. His focus lies in practical machine learning applications‚Äîfrom prompt optimization to fine-tuning vision language models for document processing. The most recent example, working in a small team to provide deployment options for Apertus on AWS. An active member of the ML community, Malte balances his technical work with a disciplined approach to fitness, preferring early morning gym sessions when it‚Äôs empty. During summer weekends, he explores the Swiss Alps on foot and enjoying time in nature. His approach to both technology and life is straightforward: consistent improvement through deliberate practice, whether that‚Äôs optimizing a customer‚Äôs cloud deployment or preparing for the next hike in the clouds. 
 Nick McCarthy is a Senior Generative AI Specialist Solutions Architect on the Amazon Bedrock team, focused on model customization. He has worked with AWS clients across a wide range of industries ‚Äî including healthcare, finance, sports, telecommunications, and energy ‚Äî helping them accelerate business outcomes through the use of AI and machine learning. Outside of work, Nick loves traveling, exploring new cuisines, and reading about science and technology. He holds a Bachelor‚Äôs degree in Physics and a Master‚Äôs degree in Machine Learning. 
Irene Marban Alvarez is a Generative AI Specialist Solutions Architect at Amazon Web Services (AWS), working with customers in the United Kingdom and Ireland. With a background in Biomedical Engineering and Masters in Artificial Intelligence, her work focuses on helping organizations leverage the latest AI technologies to accelerate their business. In her spare time, she loves reading and cooking for her friends.
‚Ä¢ How Clario automates clinical research analysis using generative AI on AWS
  Clinical outcome assessment (COA) interviews are important instruments in clinical trials for evaluating the efficacy and safety of treatments. In studies of psychosis, anxiety, and mood disorders, these assessments often determine the success or failure of the trial, highlighting the importance of data quality and reliability. The traditional approach to evaluating the quality of these outcomes is complex and involves time-consuming, logistically challenging reviews of audio-video recordings in near real time. Interview evaluation variability, poor assessment technique, and other factors can introduce noise, leading to unreliable results and potentially to study failure. 
About Clario 
Clario&nbsp;is a leading provider of endpoint data solutions for systematic collection, management, and analysis of specific, pre-defined outcomes (endpoints) to evaluate a treatment‚Äôs safety and effectiveness in the clinical trials industry. Clario generates high-quality clinical evidence for life sciences companies seeking to bring new therapies to patients. Since its founding over 50 years ago, Clario has deployed endpoint data solutions over 30,000 times, supporting over 710 novel drug regulatory approvals across more than 100 countries. 
In this post, we demonstrate how Clario has used Amazon Bedrock and other AWS services to build an AI-powered solution that automates and improves the analysis of COA interviews. We discuss how Clario: 
 
 implemented speaker diarization, multi-lingual transcription, and large language models (LLMs) 
 used vector databases and semantic search to evaluate interview quality 
 incorporated automation into complex assessment reviews while maintaining regulatory compliance 
 
Business challenge 
Clario sought to transform their COA review methodology to enhance operational effectiveness while also increasing data quality. The company required a system that could address the critical challenges of standardized review of multi-lingual data at a global scale, while reducing natural variation between different expert reviewers, and maintaining uniform assessment quality across the complex COA interview process. The solution also needed to efficiently manage large volumes of audio recordings while meeting strict regulatory and privacy requirements. Clario sought capabilities that could automatically analyze speech and dialogue in near real time during COA interviews to potentially enable: 
 
 Reduced subjectivity and variability ‚Äì Delivering more consistent and reliable behavioral health assessments, minimizing site and rater bias. 
 Enhanced data quality and credibility ‚Äì Improving the robustness of trial outcomes with objective, standardized, and repeatable interview evaluations. 
 Streamlined operations ‚Äì Automated complex assessment review and scoring could save time and resources for geographically dispersed sites and sponsor-level clinical teams. 
 Accelerated decision-making ‚Äì Gaining clearer insights earlier could support faster, evidence-based go or no-go decisions for the trial sponsors. 
 
Solution 
To address this challenge, Clario chose AWS for its comprehensive artificial intelligence and machine learning (AI/ML) capabilities, proven ability to deploy HIPAA-compliant services at a global scale. Clario used the power of generative AI and Amazon Bedrock, a fully managed service that provides access to a diverse range of high-performing foundation models, to offer several key advantages: 
 
 No infrastructure management&nbsp;‚Äì Alleviate the operational overhead of managing AI model infrastructure and updates 
 Multiple model access&nbsp;‚Äì Compare and select from leading foundation models to optimize performance for their specific COA analysis needs 
 Built-in compliance features&nbsp;‚Äì Native support for data governance, audit trails, and regulatory requirements essential for clinical research 
 Rapid prototyping and deployment&nbsp;‚Äì Accelerated time-to-market through serverless architecture and pre-built integrations 
 Seamless AWS system integration&nbsp;‚Äì Native compatibility with existing AWS services for data storage, processing, and analytics 
 Enterprise security and privacy controls&nbsp;‚Äì Advanced encryption, access controls, and data residency options to help meet stringent industry standards 
 Continuous model improvements&nbsp;‚Äì Automatic access to model updates and new capabilities, reducing migration complexity 
 
This comprehensive approach enabled Clario to focus on their core competency‚Äîclinical research excellence‚Äîwhile using cutting-edge AI capabilities through a trusted, compliance-aligned system. 
The solution integrates advanced AI capabilities, including speaker diarization, multi-lingual transcription, semantic search, and agentic AI, to automatically review the quality of complex COA interviews in a manner similar to expert human central reviewers. The workflow orchestrates multiple steps where audio data is first analyzed to identify the unique speakers in the interview based on their voice, followed by speech-to-text conversion, and speaker role attribution to determine which speech corresponds to the interviewer and the study participant. 
This information is segmented into semantically meaningful chunks based on speaker turns and natural conversation boundaries, with each segment maintaining crucial metadata. Examples of metadata include timestamps, speaker role, and positional context. These chunks are then vectorized and stored in an Amazon OpenSearch vector database, enabling the system to overcome the context window limitations of foundation models when processing lengthy interviews. The solution implements a sophisticated retrieval strategy where: 
 
 Overlapping windows makes sure that contextual information is not lost at segment boundaries 
 Targeted semantic searches identify specific dialogue segments relevant to each assessment criterion 
 A hierarchical approach preserves both local conversational flow and global interview context through interview-level summaries and speaker roles 
 Rolling context windows can be dynamically assembled when evaluating criteria that span multiple segments 
 
This architecture allows the system to efficiently handle multiple queries against the same interview data while maintaining contextual relationships throughout the conversation. The system uses this semantic retrieval capability to analyze the content of the dialogue between the interviewer and the participant, evaluating it against a structured interview guide and central review checklist. The output of the workflow includes a quality rating for the interview, along with structured feedback for each checklist item, specifying where the interview diverges from the established standards. The overall system provides near real-time insights into the quality and reliability of the COA interview, supporting faster evidence-based go or no-go decisions for sponsors of clinical trials. 
Solution architecture 
The following architecture diagram illustrates the solution implementation: 
 
The workflow consists of the following steps: 
 
 The COA interview recordings (audio and video files) from the interviews are collected on premises (1) using a recording application. The files are uploaded using&nbsp;AWS Direct Connect&nbsp;with encryption in transit to Amazon Simple Storage Service (Amazon S3)(2). The uploaded documents are then automatically stored with server-side object-level encryption. 
 After the files are uploaded, Clario‚Äôs AI Orchestration Engine (3) extracts the audio and identifies speech segments of unique speakers using a custom speaker diarization model on Amazon SageMaker (4). 
 The Orchestration Engine also invokes the Amazon Bedrock API for automated audio transcription. Clario uses the Whisper model from the Amazon Bedrock Marketplace (5) to generate near real-time transcriptions of the COA interview recordings. The transcriptions are then annotated with speaker information and timecodes, and then vectorized using an embedding model (Amazon Titan Text Embeddings v2 model) and stored into Amazon OpenSearch (7) for semantic retrieval. 
 After the information has been vectorized and stored, Clario‚Äôs AI Orchestration Engine executes a graph-based agent system running on Amazon Elastic Kubernetes Service (Amazon EKS)(3) for automated COA interview review. The agent implements a multi-step workflow that: (1) retrieves the assessment‚Äôs structured interview guide from configuration, (2) loads the corresponding central review checklist criteria, and (3) systematically queries Amazon OpenSearch (7) to extract relevant interview segments. Using the pre-configured graph structure for the task at hand, the agent traverses predefined decision nodes to compare interview responses against standardized assessment criteria, identify gaps or inconsistencies, and generate structured findings with supporting evidence citations. 
 The agent uses advanced large language models (LLMs), such as Anthropic Claude 3.7 Sonnet from Amazon Bedrock (6), to classify the speech segment as interviewer or participant, and to determine if each interview turn meets the interview quality criteria. 
 Clario‚Äôs AI Orchestration Engine then compiles the overall review of the interview and persists the information in Amazon Relational Database Service (Amazon RDS)(8). 
 Results of the AI-powered automated review can be retrieved by a client application (9) by invoking a Rest API using Amazon API Gateway endpoints (10). 
 
Benefits and results 
The initial implementation of this AI-powered solution is showing promise in improving Clario‚Äôs clinical trial processes: 
 
 Operational efficiency 
   
   Potential to decrease manual review effort by over 90%. 
    
 Quality improvements 
   
   Up to 100% data coverage through automated review versus human-only review of a smaller subset of recordings to spot check quality. 
   Highly targeted interventions might be enabled with rapid turnaround, focusing only on those raters and sites that require remediation. 
    
 Business impact 
   
   Potential to shorten turn-around time by decreasing central review time from weeks to hours. 
   Enhanced data reliability for regulatory submissions. 
   Reduced risk of study failure and uninterpretable results. 
   Improved scalability of clinical trial operations. 
    
 
Lessons learned and best practices 
Throughout the development and deployment of this solution, Clario has gained valuable insights and lessons learned that can benefit other organizations looking to implement similar AI-powered systems: 
 
 Importance of responsible AI development and use ‚Äì During initial testing, Clario discovered that LLMs would occasionally generate plausible sounding but inaccurate summaries. This critical finding reinforced the importance of responsible AI practices in healthcare applications. This led Clario to implement a validation system where AI outputs are cross-checked against source documents for factual accuracy before human review. 
 Continuous model evaluation ‚Äì Clario adopted a rigorous model evaluation process to maintain the highest standards of quality and reliability in their AI-powered COA interview analysis solution. Clario regularly assessed the performance and accuracy of their AI models through multiple approaches, including comparative studies on custom datasets, across multiple models and configurations. 
 Scalable and more secure architecture ‚Äì The serverless, cloud-based architecture of the solution‚Äìusing services like Amazon Bedrock, Amazon S3, and AWS Lambda‚Äìhelped Clario to scale their solution effectively while prioritizing data security and compliance. 
 
Next steps and conclusion 
Clario‚Äôs innovative solution has the potential to transform the way COAs are reviewed and rated, significantly improving the reliability of clinical trial data and reducing the time and effort required for manual review. As Clario continues to refine and expand the capabilities of this AI-powered system, Clario is exploring additional use cases in neuroscience studies that rely on clinical interviews for evaluating the safety and efficacy of treatments. 
By using generative AI and the robust features of Amazon Bedrock, Clario has set a new standard for clinical trial data analysis. This empowers their customers to make more informed decisions and accelerate the development of life-changing therapies. 
 
About the authors 
Alex Boudreau is the Director of AI at Clario. He leads the company‚Äôs innovative Generative AI department and oversees the development of the company‚Äôs advanced multi-modal GenAI Platform, which encompasses cutting-edge cloud engineering, AI engineering, and foundational AI research. Alex previously pioneered Deep Learning speech analysis systems for automotive applications, led cloud-based enterprise fraud detection solutions, advanced conversational AI technologies, and groundbreaking projects in medical image analysis. His expertise in leading high-impact initiatives positions him uniquely to drive forward the boundaries of AI technology in the business world. 
Cuong Lai is the Technical Team Lead for the Generative AI team at Clario, where he helps to drive the development and scaling of the company‚Äôs generative AI platform. With over eight years of software engineering experience, he specializes in web development, API design, and architecting cloud-native solutions. Cuong has extensive experience leveraging AWS services to build secure, reliable, and high-performance systems that support large-scale AI workloads. He is passionate about advancing generative AI technologies and delivering innovative, production-ready AI solutions. 
Praveen Haranahalli is a Senior Solutions Architect at Amazon Web Services (AWS), where he architects secure, scalable cloud solutions and provides strategic guidance to diverse enterprise customers. With nearly two decades of IT experience, Praveen has delivered transformative implementations across multiple industries. As a trusted technical advisor, he partners with customers to implement robust DevSecOps pipelines, establish comprehensive security guardrails, and develop innovative AI/ML solutions. He is passionate about solving complex business challenges through cutting-edge cloud architectures and empowering organizations to achieve successful digital transformations powered by artificial intelligence and machine learning.
‚Ä¢ Democratizing AI: How Thomson Reuters Open Arena supports no-code AI for every professional with Amazon Bedrock
  This post is cowritten by Laura Skylaki, Vaibhav Goswami, Ramdev Wudali and Sahar El Khoury from Thomson Reuters. 
Thomson Reuters (TR) is a leading AI and technology company dedicated to delivering trusted content and workflow automation solutions. With over 150 years of expertise, TR provides essential solutions across legal, tax, accounting, risk, trade, and media sectors in a fast-evolving world. 
TR recognized early that AI adoption would fundamentally transform professional work. According to TR‚Äôs 2025 Future of Professionals Report, 80% of professionals anticipate AI significantly impacting their work within five years, with projected productivity gains of up to 12 hours per week by 2029. To unlock this immense potential, TR needed a solution to democratize AI creation across its organization. 
In this blog post, we explore how TR addressed key business use cases with Open Arena, a highly scalable and flexible no-code AI solution powered by Amazon Bedrock and other AWS services such as Amazon OpenSearch Service, Amazon Simple Storage Service (Amazon S3), Amazon DynamoDB, and AWS Lambda. We‚Äôll explain how TR used AWS services to build this solution, including how the architecture was designed, the use cases it solves, and the business profiles that use it. The system demonstrates TR‚Äôs successful approach of using existing TR services for rapid launches while supporting thousands of users, showcasing how organizations can democratize AI access and support business profiles (for example, AI explorers and SMEs) to create applications without coding expertise. 
Introducing Open Arena: No-code AI for all 
TR introduced Open Arena to non-technical professionals to create their own customized AI solutions. With Open Arena users can use cutting-edge AI powered by Amazon Bedrock in a no-code environment, exemplifying TR‚Äôs commitment to democratizing AI access. 
Today, Open Arena supports: 
 
 High adoption: ~70% employee adoption, with&nbsp;19,000 monthly active users. 
 Custom solutions: Thousands of customized AI solutions created&nbsp;without coding, used for internal workflows or integrated into TR products for customers. 
 Self-served functionality: 100% self-served functionality, so that users, irrespective of technical background, can develop, evaluate, and deploy generative AI solutions. 
 
The Open Arena journey: From prototype to enterprise solution 
Conceived as a rapid prototype, Open Arena was developed in under six weeks at the onset of the generative AI boom in early 2023 by TR Labs ‚Äì TR‚Äôs dedicated applied research division focused on the research, development, and application of AI and emerging trends in technologies. The goal was to support internal team exploration of large language models (LLMs) and discover unique use cases by merging LLM capabilities with TR company data. 
Open Arena‚Äôs introduction significantly increased AI awareness, fostered developer-SME collaboration for groundbreaking concepts, and accelerated AI capability development for TR products. The rapid success and demand for new features quickly highlighted Open Arena‚Äôs potential for AI democratization, so TR developed an enterprise version of Open Arena. Built on the&nbsp;TR AI Platform, Open Arena enterprise version offers secure, scalable, and standardized services covering the entire AI development lifecycle, significantly accelerating time to production. 
The Open Arena enterprise version uses existing system capabilities for enhanced data access controls, standardized service access, and compliance with TR‚Äôs governance and ethical standards. This version introduced self-served capabilities so that every user, irrespective of their technical ability, can create, evaluate, and deploy customized AI solutions in a no-code environment. 

 ‚ÄúThe foundation of the AI Platform has always been about empowerment; in the early days it was about empowering Data Scientists but with the rise of Gen AI, the platform adapted and evolved on empowering users of any background to leverage and create AI Solutions.‚Äù 
 ‚Äì Maria Apazoglou, Head of AI Engineering, CoCounsel
 
As of July 2025, the TR Enterprise AI Platform consists of 15 services spanning the entire AI development lifecycle and user personas. Open Arena remains one of its most popular, serving 19,000 users each month, with increasing monthly usage. 
Addressing key enterprise AI challenges across user types 
Using the TR Enterprise AI Platform, Open Arena helped thousands of professionals transition into using generative AI. AI-powered innovation is now readily in the hands of everyone, not just AI scientists. 
Open Arena successfully addresses four critical enterprise AI challenges: 
 
 Enablement:&nbsp;Delivers AI solution building with consistent LLM and service provider experience and support for various user personas, including non-technical. 
 Security and quality:&nbsp;Streamlines AI solution quality tracking using evaluation and monitoring services, whilst complying with data governance and ethics policies. 
 Speed and reusability:&nbsp;Automates workflows and uses existing AI solutions and prompts. 
 Resources and cost management:&nbsp;Tracks and displays generative AI solution resource consumption, supporting transparency and efficiency. 
 
The solution currently supports several AI experiences, including tech support, content creation, coding assistance, data extraction and analysis, proof reading, project management, content summarization, personal development, translation, and problem solving, catering to different user needs across the organization. 
 
 
Figure 1. Examples of Open Arena use cases. 
AI explorers use Open Arena to speed up day-to-day tasks, such as summarizing documents, engaging in LLM chat, building custom workflows, and comparing AI models. AI creators and Subject Matter Experts (SMEs) use Open Arena to build custom AI workflows and experiences and to evaluate solutions without requiring coding knowledge. Meanwhile, developers can develop and deploy new AI solutions at speed, training models, creating new AI skills, and deploying AI capabilities. 
Why Thomson Reuters selected AWS for Open Arena 
TR strategically chose AWS as a primary cloud provider for Open Arena based on several critical factors: 
 
 Comprehensive AI/ML capabilities:&nbsp;Amazon Bedrock offers easy access to a choice of high-performing foundation models from leading AI companies like AI21 Labs, Anthropic, Cohere, DeepSeek, Luma AI, Meta, Mistral AI, OpenAI, Qwen, Stability AI, TwelveLabs, Writer, and Amazon. It supports simple chat and complex RAG workflows, and integrates seamlessly with TR‚Äôs existing Enterprise AI Platform. 
 Enterprise-grade security and governance:&nbsp;Advanced security controls, model access using RBAC, data handling with enhanced security features, single sign-on (SSO) enabled, and clear operational and user data separation across AWS accounts. 
 Scalable infrastructure:&nbsp;Serverless architecture for automatic scaling, pay-per-use pricing for cost optimization, and global availability with low latency. 
 Existing relationship and expertise:&nbsp;Strong, established relationship between TR and AWS, existing Enterprise AI Platform on AWS, and deep AWS expertise within TR‚Äôs technical teams. 
 

 ‚ÄúOur long-standing partnership with AWS and their robust, flexible and innovative services made them the natural choice to power Open Arena and accelerate our AI initiatives.‚Äù  
 ‚Äì Maria Apazoglou, Head of AI Engineering, CoCounsel
 
Open Arena architecture: Scalability, extensibility, and security 
Designed for a broad enterprise audience, Open Arena prioritizes scalability, extensibility and security while maintaining simplicity for non-technical users to create and deploy AI solutions. The following diagram illustrates the architecture of Open Arena. 
 
Figure 2. Architecture design of Open Arena. 
The architecture design facilitates enterprise-grade performance with clear separation between capability and usage, aligning with TR‚Äôs enterprise cost and usage tracking requirements. 
The following are key components of the solution architecture: 
 
 No-code interface:&nbsp;Intuitive UI, visual workflow builder, pre-built templates, drag-and-drop functionality. 
 Enterprise integration:&nbsp;Seamless integration with TR‚Äôs Enterprise AI Platform, SSO enabled, data handling with enhanced security, clear data separation. 
 Solution management:&nbsp;Searchable repository, public/private sharing, version control, usage analytics. 
 
TR developed Open Arena using AWS services such as Amazon Bedrock, Amazon OpenSearch, Amazon DynamoDB, Amazon API Gateway, AWS Lambda, and AWS Step Functions. It uses Amazon Bedrock for foundational model interactions, supporting simple chat and complex Retrieval-Augmented Generation (RAG) tasks. Open Arena uses Amazon Bedrock Flows as the custom workflow builder where users can drag-and-drop components like prompts, agents, knowledge bases and Lambda functions to create sophisticated AI workflows without coding. The system also integrates with AWS OpenSearch for knowledge bases and external APIs for advanced agent capabilities. 
For data separation, orchestration is managed using the Enterprise AI Platform AWS account, capturing operational data. Flow instances and user-specific data reside in the user‚Äôs dedicated AWS account, stored in a database. Each user‚Äôs data and workflow executions are isolated within their respective AWS accounts, which is required for complying with Thomson Reuters data sovereignty and enterprise security policies with strict regional controls. The system integrates with Thomson Reuters SSO solution to automatically identify users and grant secure, private access to foundational models. 
The orchestration layer, centrally hosted within the Enterprise AI Platform AWS account, manages AI workflow activities, including scheduling, deployment, resource provisioning, and governance across user environments. 
The system features fully automated provisioning of&nbsp; Amazon Bedrock Flows directly within each user‚Äôs AWS account, avoiding manual setup and accelerating time to value. Using AWS Lambda for serverless compute and DynamoDB for scalable, low-latency storage, the system dynamically allocates resources based on real-time demand. This architecture makes sure prompt flows and supporting infrastructure are deployed and scaled to match workload fluctuations, optimizing performance, cost, and user experience. 

 ‚ÄúOur decision to adopt a cross-account architecture was driven by a commitment to enterprise security and operational excellence. By isolating orchestration from execution, we make sure that each user‚Äôs data remains private and secure within their own AWS account, while still delivering a seamless, centrally-managed experience. This design empowers organizations to innovate rapidly without compromising compliance or control.‚Äù 
 ‚Äì Thomson Reuters‚Äô architecture team
 
Evolution of Open Arena: From classic to Amazon Bedrock Flows-powered chain builder 
Open Arena has evolved to cater to varying levels of user sophistication: 
 
 Open Arena v1 (Classic):&nbsp;Features a form-based interface for simple prompt customization and basic AI workflow deployment within a single AWS account. Its simplicity appeals to novice users for straightforward use cases, though with limited advanced capabilities. 
 Open Arena v2 (Chain Builder):&nbsp;Introduces a robust, visual workflow builder interface, enabling users to design complex, multi-step AI workflows using drag-and-drop components. With support for advanced node types, parallel execution, and seamless cross-account deployment, Chain Builder dramatically expands the system‚Äôs capabilities and accessibility for non-technical users. 
 
Thomson Reuters uses Amazon Bedrock Flows as a core feature of Chain Builder. Users can define, customize, and deploy AI-driven workflows using Amazon Bedrock models. Bedrock Flows supports advanced workflows combining multiple prompt nodes, incorporating AWS Lambda functions, and supporting sophisticated RAG pipelines. Operating seamlessly across user AWS accounts, Bedrock Flows facilitates secure, scalable execution of personalized AI solutions, serving as the fundamental engine for the Chain Builder workflows and driving TR‚Äôs ability to deliver robust, enterprise-grade automation and innovation. 
What‚Äôs next? 
TR continues to expand Open Arena‚Äôs capabilities through the strategic partnership with AWS, focusing on: 
 
 Driving further adoption of Open Arena‚Äôs DIY capabilities. 
 Enhancing flexibility for workflow creation in Chain Builder with custom components, such as inline scripts. 
 Developing new templates to represent common tasks and workflows. 
 Enhancing collaboration features within Open Arena. 
 Extending multimodal capabilities and model integration. 
 Expanding into new use cases across the enterprise. 
 

 ‚ÄúFrom innovating new product ideas to reimagining daily tasks for Thomson Reuters employees, we continue to push the boundaries of what‚Äôs possible with Open Arena.‚Äù 
 ‚Äì Maria Apazoglou, Head of AI Engineering, CoCounsel
 
Conclusion 
In this blog post, we explored how Thomson Reuters‚Äô Open Arena demonstrates the successful democratization of AI across an enterprise by using AWS services, particularly Amazon Bedrock and Bedrock Flows. With 19,000 monthly active users and 70% employee adoption, the system proves that no-code AI solutions can deliver enterprise-scale impact while maintaining security and governance standards. 
By combining the robust infrastructure of AWS with innovative architecture design, TR has created a blueprint for AI democratization that empowers professionals across technical skill levels to harness generative AI for their daily work. 
As Open Arena continues to evolve, it exemplifies how strategic cloud partnerships can accelerate AI adoption and transform how organizations approach innovation with generative AI. 
 
About the authors 
Laura Skylaki, PhD, leads the Enterprise AI Platform at Thomson Reuters, driving the development of GenAI services that accelerate the creation, testing and deployment of AI solutions, enhancing product value. A recognized expert with a doctorate in stem cell bioinformatics, her extensive experience in AI research and practical application spans legal, tax, and biotech domains. Her machine learning work is published in leading academic journals, and she is a frequent speaker on AI and machine learning 
Vaibhav Goswami is a Lead Software Engineer on the AI Platform team at Thomson Reuters, where he leads the development of the Generative AI Platform that empowers users to build and deploy generative AI solutions at scale. With expertise in building production-grade AI systems, he focuses on creating tools and infrastructure that democratize access to cutting-edge AI capabilities across the enterprise. 
Ramdev Wudali is a Distinguished Engineer, helping architect and build the AI/ML Platform to enable the Enterprise user, data scientists and researchers to develop Generative AI and machine learning solutions by democratizing access to tools and LLMs. In his spare time, he loves to fold paper to create origami tessellations, and wearing irreverent T-shirts 
As the director of AI Platform Adoption and Training, Sahar El Khoury guides users to seamlessly onboard and successfully use the platform services, drawing on her experience in AI and data analysis across robotics (PhD), financial markets, and media. 
Vu San Ha Huynh is a Solutions Architect at AWS with a PhD in Computer Science. He helps large Enterprise customers drive innovation across different domains with a focus on AI/ML and Generative AI solutions. 
Paul Wright is a Senior Technical Account Manager, with over 20 years experience in the IT industry and over 7 years of dedicated cloud focus. Paul has helped some of the largest enterprise customers grow their business and improve their operational excellence. In his spare time Paul is a huge football and NFL fan. 
Mike Bezak is a Senior Technical Account Manager in AWS Enterprise Support. He has over 20 years of experience in information technology, primarily disaster recovery and systems administration. Mike‚Äôs current focus is helping customers streamline and optimize their AWS Cloud journey. Outside of AWS, Mike enjoys spending time with family &amp; friends.
‚Ä¢ Introducing structured output for Custom Model Import in Amazon Bedrock
  With Amazon Bedrock Custom Model Import, you can deploy and scale fine-tuned or proprietary foundation models in a fully managed, serverless environment. You can bring your own models into Amazon Bedrock, scale them securely without managing infrastructure, and integrate them with other Amazon Bedrock capabilities. 
Today, we are excited to announce the addition of structured output to Custom Model Import. Structured output constrains a model‚Äôs generation process in real time so that every token it produces conforms to a schema you define. Rather than relying on prompt-engineering tricks or brittle post-processing scripts, you can now generate structured outputs directly at inference time. 
For certain production applications, the predictability of model outputs is more important than their creative flexibility. A customer service chatbot might benefit from varied, natural-sounding responses, but an order processing system needs exact, structured data that conforms to predefined schemas. Structured output bridges this gap by maintaining the intelligence of foundation models while verifying their outputs meet strict formatting requirements. 
This represents a shift from free-form text generation to outputs that are consistent, machine-readable, and designed for seamless integration with enterprise systems. While free-form text excels for human consumption, production applications require more precision. Businesses can‚Äôt afford the ambiguity of natural language variations when their systems depend on structured outputs to reliably interface with APIs, databases, and automated workflows. 
In this post, you will learn how to implement structured output for Custom Model Import in Amazon Bedrock. We will cover what structured output is, how to enable it in your API calls, and how to apply it to real-world scenarios that require structured, predictable outputs. 
Understanding structured output 
Structured output, also known as constrained decoding, is a method that directs LLM outputs to conform to a predefined schema, such as valid JSON. Rather than allowing the model to freely select tokens based on probability distributions, it introduces constraints during generation that limit choices to only those that maintain structural validity. If a particular token would violate the schema by producing invalid JSON, inserting stray characters, or using an unexpected field name the structured output rejects it and requires the model to select another allowed option. This real-time validation helps keep the final output consistent, machine readable, and immediately usable by downstream applications without the need for additional post-processing. 
Without structured output, developers often attempt to enforce structure through prompt instructions like ‚ÄúRespond only in JSON.‚Äù While this approach sometimes works, it remains unreliable due to the inherently probabilistic nature of LLMs. These models generate text by sampling from probability distributions, introducing natural variability that makes responses feel human but creates significant challenges for automated systems. 
Consider a customer support application that classifies tickets: if responses vary between ‚ÄúThis seems like a billing issue,‚Äù ‚ÄúI‚Äôd classify this as: Billing,‚Äù and ‚ÄúCategory = BILLING,‚Äù downstream code cannot reliably interpret the results. What production systems require instead is predictable, structured output. For example: 
 
 {
  "category": "billing",
  "priority": "high",
  "sentiment": "negative"
}
 
 
With a response like this, your application can automatically route tickets, trigger workflows, or update databases without human intervention. By providing predictable, schema-aligned responses, structured output transforms LLMs from conversational tools into reliable system components that can be integrated with databases, APIs, and business logic. This capability opens new possibilities for automation while maintaining the intelligent reasoning that underpin the value of these models. 
Beyond improving reliability and simplifying post-processing, structured output offers additional benefits that strengthens performance, security and safety in production environments. 
 
 Lower token usage and faster responses: By constraining generation to a defined schema, structured output removes unnecessary verbose, free-form text, resulting in reduced token count. Because token generation is sequential, shorter outputs directly translate to faster responses and lower latency, improving overall performance and cost efficiency. 
 Enhanced security against prompt injection: Structured output narrows the model‚Äôs expression space and helps prevent it from producing arbitrary or unsafe content. Bad actors cannot inject instructions, code or unexpected text outside the defined structure. Each field must match its expected type and format, making sure outputs remain within safe boundaries. 
 Safety and policy controls: Structured output enables you to design schemas that inherently help prevent harmful, toxic, or policy-violating content. By limiting fields to approved values, enforcing patterns, and restricting free-form text, schemas make sure outputs align with regulatory requirements. 
 
In the next section, we will explore how structured output works with Custom Model Import in Amazon Bedrock and walks through an example of enabling it in your API calls. 
Using structured output with Custom Model Import in Amazon Bedrock 
Let‚Äôs start by assuming you have already imported a Hugging Face model into Amazon Bedrock using the Custom Model Import feature. 
Prerequisites 
Before proceeding, make sure you have: 
 
 An active AWS account with access to Amazon Bedrock 
 A custom model created in Amazon Bedrock using the Custom Model Import feature 
 Appropriate AWS Identity and Access Management (IAM) permissions to invoke models through the Amazon Bedrock Runtime 
 
With these prerequisites in place, let‚Äôs explore how to implement structured output with your imported model. 
To start using structured output with a Custom Model Import in Amazon Bedrock, begin by configuring your environment. In Python, this involves creating a Bedrock Runtime client and initializing a tokenizer from your imported Hugging Face model. 
The Bedrock Runtime client provides access to your imported model using the Bedrock InvokeModel API. The tokenizer applies the correct chat template that aligns with the imported model, which defines how user, system, and assistant messages are combined into a single prompt, how the role markers (for example, &lt;|user|&gt;, &lt;|assistant|&gt;) are inserted, and where the model‚Äôs response should begin. 
By calling tokenizer.apply_chat_template(messages, tokenize=False) you can generate a prompt that matches the exact input format your model expects, which is essential for consistent and reliable inference, especially when structured encoding is enabled. 
 
 import boto3
from transformers import AutoTokenizer
from botocore.config import Config

# HF model identifier imported into Bedrock
hf_model_id = "&lt;&lt;huggingface_model_id&gt;&gt;" # Example: "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B
model_arn = "arn:aws:bedrock:&lt;&lt;aws-region&gt;&gt;:&lt;&lt;account-id&gt;&gt;:imported-model/your-model-id"
region      = "&lt;&lt;aws-region&gt;&gt;"

# Initialize tokenizer aligned with your imported model 
tokenizer = AutoTokenizer.from_pretrained(hf_model_id)

# Initialize Bedrock client
bedrock_runtime = boto3.client(
    service_name="bedrock-runtime",
    region_name=region) 
 
Implementing structured output 
When you invoke a custom model on Amazon‚ÄØBedrock, you have the option to enable structured output by adding a response_format block to the request payload. This block accepts a JSON schema that defines the structured of the model‚Äôs response. During inference, the model enforces this schema in real-time, making sure that each generated token conforms to the defined structure. Below is a walkthrough demonstrating how to implement structured output using a simple address extraction task. 
Step 1: Define the data structure 
You can define your expected output using a Pydantic model, which serves as a typed contract for the data you want to extract. 
 
 from pydantic import BaseModel, Field

class Address(BaseModel):
    street_number: str = Field(description="Street number")
    street_name: str = Field(description="Street name including type (Ave, St, Rd, etc.)")
    city: str = Field(description="City name")
    state: str = Field(description="Two-letter state abbreviation")
    zip_code: str = Field(description="5-digit ZIP code") 
 
Step 2: Generate the JSON schema 
Pydantic can automatically convert your data model into a JSON schema: 
 
 schema = Address.model_json_schema()
address_schema = {
    "name": "Address",
    "schema": schema
} 
 
This schema defines each field‚Äôs type, description, and requirement, creating a blueprint that the model will follow during generation. 
Step 3: Prepare your input messages 
Format your input using the chat format expected by your model: 
 
 messages = [{
    "role": "user",
    "content": "Extract the address: 456 Tech Boulevard, San Francisco, CA 94105"
}] 
 
Step 4: Apply the chat template 
Use your model‚Äôs tokenizer to generate the formatted prompt: 
 
 prompt = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
) 
 
Step 5: Build the request payload 
Create your request body, including the response_format that references your schema: 
 
 request_body = {
    'prompt': prompt,
    'temperature': 0.1,
    'max_gen_len': 1000,
    'top_p': 0.9,
    'response_format': {
        "type": "json_schema",
        "json_schema": address_schema
    }
} 
 
Step 6: Invoke the model 
Send the request using the InvokeModel API: 
 
 response = bedrock_runtime.invoke_model(
    modelId=model_arn,
    body=json.dumps(request_body),
    accept="application/json",
    contentType="application/json"
) 
 
Step 7: Parse the response 
Extract the generated text from the response: 
 
 result = json.loads(response['body'].read().decode('utf-8'))
raw_output = result['choices'][0]['text']
print(raw_output) 
 
Because the schema defines required fields, the model‚Äôs response will contain them: 
 
 {
"street_number": "456",
"street_name": "Tech Boulevard",
"city": "San Francisco",
"state": "CA",
"zip_code": "94105"
} 
 
The output is clean, valid JSON that can be consumed directly by your application with no extra parsing, filtering, or cleanup required. 
Conclusion 
Structured output with Custom Model Import in Amazon Bedrock provides an effective way to generate structures, schema-aligned outputs from your models. By shifting validation into the model inference itself, structured output reduce the need for complex post-processing workflows and error handling code. 
Structured output generates outputs that are predictable and straightforward to integrate into your systems and supports a variety of use cases, for example, building financial applications that require precise data extraction, healthcare systems that need structured clinical documentation, or customer service systems that demand consistent ticket classification. 
Start experimenting with structured output with your Custom Model Import today and transform how your AI applications deliver consistent, production-ready results. 
 
About the authors 
Manoj Selvakumar is a Generative AI Specialist Solutions Architect at AWS, where he helps organizations design, prototype, and scale AI-powered solutions in the cloud. With expertise in deep learning, scalable cloud-native systems, and multi-agent orchestration, he focuses on turning emerging innovations into production-ready architectures that drive measurable business value. He is passionate about making complex AI concepts practical and enabling customers to innovate responsibly at scale‚Äîfrom early experimentation to enterprise deployment. Before joining AWS, Manoj worked in consulting, delivering data science and AI solutions for enterprise clients, building end-to-end machine learning systems supported by strong MLOps practices for training, deployment, and monitoring in production. 
Yanyan Zhang&nbsp;is a Senior Generative AI Data Scientist at Amazon Web Services, where she has been working on cutting-edge AI/ML technologies as a Generative AI Specialist, helping customers use generative AI to achieve their desired outcomes. Yanyan graduated from Texas A&amp;M University with a PhD in Electrical Engineering. Outside of work, she loves traveling, working out, and exploring new things. 
Lokeshwaran Ravi is a Senior Deep Learning Compiler Engineer at AWS, specializing in ML optimization, model acceleration, and AI security. He focuses on enhancing efficiency, reducing costs, and building secure ecosystems to democratize AI technologies, making cutting-edge ML accessible and impactful across industries. 
Revendra Kumar is a Senior Software Development Engineer at Amazon Web Services. In his current role, he focuses on model hosting and inference MLOps on Amazon Bedrock. Prior to this, he worked as an engineer on hosting Quantum computers on the cloud and developing infrastructure solutions for on-premises cloud environments. Outside of his professional pursuits, Revendra enjoys staying active by playing tennis and hiking. 
Muzart Tuman is a software engineer utilizing his experience in fields like deep learning, machine learning optimization, and AI-driven applications to help solve real-world problems in a scalable, efficient, and accessible manner. His goal is to create impactful tools that not only advance technical capabilities but also inspire meaningful change across industries and communities.

‚∏ª