âœ… Morning News Briefing â€“ July 11, 2025 10:56

ğŸ“… Date: 2025-07-11 10:56
ğŸ·ï¸ Tags: #briefing #ai #publichealth #digitalgov

â¸»

ğŸ§¾ Weather
â€¢ No watches or warnings in effect, Pembroke
  No watches or warnings in effect. No warnings or watches or watches in effect . Watch or warnings are no longer in effect in the U.S. No watches, warnings are in effect for the rest of the day . No watches and warnings are still in effect, but no watches are in place for the day's events . The weather is not expected to be affected by the weather .
â€¢ Current Conditions:  16.7Â°C
  Temperature: 16.7&deg;C Pressure / Tendency: 101.4 kPa rising Humidity: 97 % Humidity : 97 % Dewpoint: 16 .2&deg:C Wind: NNW 5 km/h . Air Quality Health Index: n/a . Pembroke 6:00 AM EDT Friday 11 July 2025 Temperature: . 16.2
â€¢ Friday: Chance of showers. High 29. POP 30%
  Sunny this morning then a mix of sun and cloud with 30 percent chance of showers this afternoon . Risk of a thunderstorm this afternoon. High 29. Humidex 34. UV index 9 or very high. Chance of rain this morning, risk of thunderstorm today afternoon; high of 29.50C . Chance of showers, thunderstorms, high of 40C in the morning .

ğŸŒ International News
No updates.

ğŸ Canadian News
No updates.

ğŸ‡ºğŸ‡¸ U.S. Top Stories
â€¢ How 3 Muslim sisters helped change the rules of American women's wrestling
  Jamilah, Zaynah and Latifah McBryde grew up wrestling one another in Buffalo, N.Y. Coaches recognized their talent, but they couldn't wear the required wrestling singlet due to their faith . The three girls were able to wrestle one another, but were not allowed to wear a singlet because of their religious beliefs . Jamilsah,
â€¢ New Hampshire judge blocks Trump birthright citizenship executive order nationwide
  A federal judge in New Hampshire blocks President Trump's executive order that attempted to end birthright citizenship . The executive order is blocked from taking effect anywhere in the U.S. Judge: The order would be illegal in the United States, but it would not be allowed to take effect anywhere else in the country . Judge: It's a violation of the Constitution and the Constitution of American Constitution
â€¢ As Democrats spoil for a fight, a new face in the House is leading them on oversight
  Rep. Robert Garcia is the new top Democrat on the House Oversight Committee . Garcia says he's ready to lean into the fray with President Trump . Garcia: "I'm looking forward to more confrontation with the White House" Garcia says his party is craving more confrontational with the president, but he's not ready to get involved in the fray . Garcia is a Democrat on Oversight Committee,
â€¢ What AI bot started referring to itself as 'MechaHitler'? Find out in the quiz
  Elon Musk and his AI have been busy . So has the TSA and Amazon. And Amazon.com? Were you paying attention? Do you pay attention to the TSA? Have you been paying attention to it? Share your thoughts with CNN iReport.com/Tunami tonight at 9 p.m. ET . Back to the page you came from: http://www.mail
â€¢ How flood sirens could have saved lives in Texas
  In the wake of the deadly flash floods in Texas, state leaders are exploring whether to install more flood warning sirens . Such sirens can save lives if they're part of a larger warning system, experts say . Texas is considering installing more sirens in response to the deadly floods in the state . The state is also looking at installing more flood warnings sirens as part of the warning

ğŸ§  Artificial Intelligence
No updates.

ğŸ’» Digital Strategy
â€¢ UK Online Safety Act 'not up to scratch' on misinformation, warn MPs
  Online Safety Act fails to tackle online misinformation, report from MPs finds . UK in need of further regulation to curb viral spread of false content, report finds . Last summer's riots show how some content can be harmful but not illegal . Report: UK needs regulation to crack down on false content to curb the spread of misinformation . The UK needs to be regulated by the UK to prevent the spread
â€¢ Microsoft offers EU cloud providers fresh commercial terms, staves off risk of litigation
  Microsoft has tabled a fresh set of commercial terms for an association of cloud providers in Europe . CISPE earlier filed a complaint with antitrust authorities in the trading bloc over allegations of anti-competitive licensing practices . Agreement or otherwise expected from CISPE top brass before August Exclusive: Microsoft tabled new terms for CISPE association in Europe that earlier filed antitrust complaint with EU authorities over alleged anti-
â€¢ Security company hired a used car salesman to build a website, and it didn't end well
  On Call, The Register's Friday column that shares your stories of tech support terror and triumph . First came the dodgy lawyer, then the angry HR person, leaving a whistleblower techie to save his career . On Call is a weekly column from The Register that features tech support support support from tech support teams around the world . Back to the page you came from, read it here .
â€¢ French cops cuff Russian pro basketball player on ransomware charges
  A Russian professional basketball player is cooling his heels in a French detention center after being arrested and accused of acting as a negotiator for a ransomware gang . 'He's useless with computers and can't even install an application' says lawyer . â€˜Heâ€™s useless with computer and canâ€™t even install a computer application,' lawyer says of Russian pro basketball player . â€œHe
â€¢ Chinese censorship-busters claim Tencent is trying to kill its WeChat archive
  Group-IB accused of helping Chinese web giant Tencent to quell its activities . Alleges Singaporean infosec outfit sent feeble legal demands to hosting company, which caved . GreatFire.org has accused Singapore group-IB of helping Tencent quell their activities . Tencent has been accused of encouraging censorship in China by its web giant, Tencent, in China .

ğŸ¥ Public Health
No updates.

ğŸ”¬ Science
â€¢ Premenstrual disorders and risk of cardiovascular diseases
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
â€¢ Leveraging retinal vascular features in non-invasive, early diagnosis of preeclampsia
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
â€¢ Effects of long-term very high-altitude exposure on cardiopulmonary function of healthy adults in plain areas
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
â€¢ Childhood antecedents of adult place satisfaction in 22 countries
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
â€¢ An assessment of written leaflets for enhancing patient education and counselling on asthma and chronic obstructive pulmonary disease in the implementation of a new medicine service in Poland
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations

ğŸ§¾ Government & Policy
No updates.

ğŸ›ï¸ Enterprise Architecture & IT Governance
No updates.

ğŸ¤– AI & Emerging Tech
â€¢ Cybersecurityâ€™s global alarm system is breaking down
  Every day, billions of people trust digital systems to run everything from communication to commerce to critical infrastructure. But the global early warning system that alerts security teams to dangerous software flaws is showing critical gaps in coverageâ€”and most users have no idea their digital lives are likely becoming more vulnerable.



Over the past eighteen months, two pillars of global cybersecurity have flirted with apparent collapse. In February 2024, the US-backed National Vulnerability Database (NVD)â€”relied on globally for its free analysis of security threatsâ€”abruptly stopped publishing new entries, citing a cryptic â€œchange in interagency support.â€ Then, in April of this year, the Common Vulnerabilities and Exposures (CVE) program, the fundamental numbering system for tracking software flaws, seemed at similar risk: A leaked letter warned of an imminent contract expiration.



Cybersecurity practitioners have since flooded Discord channels and LinkedIn feeds with emergency posts and memes of â€œNVDâ€ and â€œCVEâ€ engraved on tombstones. Unpatched vulnerabilities are the second most common way cyberattackers break in, and they have led to fatal hospital outages and critical infrastructure failures. In a social media post, Jen Easterly, a US cybersecurity expert, said: â€œLosing [CVE] would be like tearing out the card catalog from every library at onceâ€”leaving defenders to sort through chaos while attackers take full advantage.â€ If CVEs identify each vulnerability like a book in a card catalog, NVD entries provide the detailed review with context around severity, scope, and exploitability.&nbsp;



In the end, the Cybersecurity and Infrastructure Security Agency (CISA) extended funding for CVE another year, attributing the incident to a â€œcontract administration issue.â€ But the NVDâ€™s story has proved more complicated. Its parent organization, the National Institute of Standards and Technology (NIST), reportedly saw its budget cut roughly 12% in 2024, right around the time that CISA pulled its $3.7 million in annual funding for the NVD. Shortly after, as the backlog grew, CISA launched its own â€œVulnrichmentâ€ program to help address the analysis gap, while promoting a more distributed approach that allows multiple authorized partners to publish enriched data.&nbsp;



â€œCISA continuously assesses how to most effectively allocate limited resources to help organizations reduce the risk of newly disclosed vulnerabilities,â€ says Sandy Radesky, the agencyâ€™s associate director for vulnerability management. Rather than just filling the gap, she emphasizes that Vulnrichment was established to provide unique additional information, like recommended actions for specific stakeholders, and to â€œreduce dependency of the federal governmentâ€™s role to be the sole provider of vulnerability enrichment.â€



Meanwhile, NIST has scrambled to hire contractors to help clear the backlog. Despite a return to pre-crisis processing levels, a boom in vulnerabilities newly disclosed to the NVD has outpaced these efforts. Currently, over 25,000 vulnerabilities await processing â€“ nearly 10 times the previous high in 2017, according to data from software company Anchore. Before that, the NVD largely kept pace with CVE publications, maintaining a minimal backlog.



â€œThings have been disruptive, and weâ€™ve been going through times of change across the board,â€ Matthew Scholl, then chief of the computer security division in NISTâ€™s Information Technology Laboratory, said at an industry event in April. â€œLeadership has assured me and everyone that NVD is and will continue to be a mission priority for NIST, both in resourcing and capabilities.â€ Scholl left NIST in May after 20 years at the agency, and NIST declined to comment on the backlog.&nbsp;



The situation has now prompted multiple government actions, with the Department of Commerce launching an audit of the NVD in May and House Democrats calling for a broader probe of both programs in June. But the damage to trust is already transforming geopolitics and supply chains as security teams prepare for a new era of cyber risk. â€œItâ€™s left a bad taste, and people are realizing they canâ€™t rely on this,â€ says Rose Gupta, who builds and runs enterprise vulnerability management programs. â€œEven if they get everything together tomorrow with a bigger budget, I donâ€™t know that this wonâ€™t happen again. So I have to make sure I have other controls in place.â€



As these public resources falter, organizations and governments are confronting a critical weakness in our digital infrastructure: Essential global cybersecurity services depend on a complex web of US agency interests and government funding that can be cut or redirected at any time.



Security haves and have-nots



What began as a trickle of software vulnerabilities in the early Internet era has become an unstoppable avalanche, and the free databases that have tracked them for decades have struggled to keep up. In early July, the CVE database crossed over 300,000 catalogued vulnerabilities. Numbers jump unpredictably each year, sometimes by 10% or much more. Even before its latest crisis, the NVD was notorious for delayed publication of new vulnerability analyses, often trailing private security software and vendor advisories by weeks or months.



Gupta has watched organizations increasingly adopt commercial vulnerability management (VM) software that includes its own threat intelligence services. â€œWeâ€™ve definitely become over-reliant on our VM tools,â€ she notes, describing security teamsâ€™ growing dependence on vendors like Qualys, Rapid7, and Tenable to supplement or replace unreliable public databases. These platforms combine their own research with various data sources to create proprietary risk scores that help teams prioritize fixes. But not all organizations can afford to fill the NVDâ€™s gap with premium security tools. â€œSmaller companies and startups, already at a disadvantage, are going to be more at risk,â€ she explains.&nbsp;



Komal Rawat, a security engineer in New Delhi whose mid-stage cloud startup has a limited budget, describes the impact in stark terms: â€œIf NVD goes, there will be a crisis in the market. Other databases are not that popular, and to the extent they are adopted, they are not free. If you donâ€™t have recent data, youâ€™re exposed to attackers who do.â€





The growing backlog means new devices could be more likely to have vulnerability blind spotsâ€”whether thatâ€™s a Ring doorbell at home or an office buildingâ€™s â€œsmartâ€ access control system. The biggest risk may be â€œone-offâ€ security flaws that fly under the radar. â€œThere are thousands of vulnerabilities that will not affect the majority of enterprises,â€ says Gupta. â€œThose are the ones that weâ€™re not getting analysis on, which would leave us at risk.â€



NIST acknowledges it has limited visibility into which organizations are most affected by the backlog. â€œWe donâ€™t track which industries use which products and therefore cannot measure impact to specific industries,â€ a spokesperson says. Instead, the team prioritizes vulnerabilities on the basis of CISAâ€™s known exploits list and those included in vendor advisories like Microsoft Patch Tuesday.



The biggest vulnerability



Brian Martin has watched this system evolveâ€”and deteriorateâ€”from the inside. A former CVE board member and an original project leader behind the Open Source Vulnerability Database, he has built a combative reputation over the decades as a leading historian and practitioner. Martin says his current project, VulnDB (part of Flashpoint Security), outperforms the official databases he once helped oversee. â€œOur team processes more vulnerabilities, at a much faster turnaround, and we do it for a fraction of the cost,&#8221; he says, referring to the tens of millions in government contracts that support the current system.&nbsp;



When we spoke in May, Martin said his database contains more than 112,000 vulnerabilities with no CVE identifiersâ€”security flaws that exist in the wild but remain invisible to organizations that rely solely on public channels. â€œIf you gave me the money to triple my team, that non-CVE number would be in the 500,000 range,â€ he said.





In the US, official vulnerability management duties are split between a web of contractors, agencies, and nonprofit centers like the Mitre Corporation. Critics like Martin say that creates potential for redundancy, confusion, and inefficiency, with layers of middle management and relatively few actual vulnerability experts. Others defend the value of this fragmentation. â€œThese programs build on or complement each other to create a more comprehensive, supportive, and diverse community,â€ CISA said in a statement. â€œThat increases the resilience and usefulness of the entire ecosystem.â€



As American leadership wavers, other nations are stepping up. China now operates multiple vulnerability databases, some surprisingly robust but tainted by the possibility that they are subject to state control. In May, the European Union accelerated the launch of its own database, as well as a decentralized â€œGlobal CVEâ€ architecture. Following social media and cloud services, vulnerability intelligence has become another front in the contest for technological independence.&nbsp;



That leaves security professionals to navigate multiple, potentially conflicting sources of data. â€œItâ€™s going to be a mess, but I would rather have too much information than none at all,â€ says Gupta, describing how her team monitors multiple databases despite the added complexity.&nbsp;



Resetting software liability



As defenders adapt to the fragmenting landscape, the tech industry faces another reckoning: Why donâ€™t software vendors carry more responsibility for protecting their customers from security issues? Major vendors routinely discloseâ€”but donâ€™t necessarily patchâ€”thousands of new vulnerabilities each year. A single exposure could crash critical systems or increase the risks of fraud and data misuse.&nbsp;



For decades, the industry has hidden behind legal shields. â€œShrink-wrap licensesâ€ once forced consumers to broadly waive their right to hold software vendors liable for defects. Todayâ€™s end-user license agreements (EULAs), often delivered in pop-up browser windows, have evolved into incomprehensibly long documents. Last November, a lab project called â€œEULAS of Despairâ€ used the length of War and Peace (587,287 words) to measure these sprawling contracts. The worst offender? Twitter, at 15.83 novelsâ€™ worth of fine print.



â€œThis is a legal fiction that weâ€™ve created around this whole ecosystem, and itâ€™s just not sustainable,â€ says Andrea Matwyshyn, a US special advisor and technology law professor at Penn State University, where she directs the Policy Innovation Lab of Tomorrow. â€œSome people point to the fact that software can contain a mix of products and services, creating more complex facts. But just like in engineering or financial litigation, even the most messy scenarios can be resolved with the assistance of experts.â€





This liability shield is finally beginning to crack. In July 2024, a faulty security update in CrowdStrikeâ€™s popular endpoint detection software crashed millions of Windows computers worldwide and caused outages at everything from airlines to hospitals to 911 systems. The incident led to billions in estimated damages, and the city of Portland, Oregon, even declared a â€œstate of emergency.â€ Now, affected companies like Delta Airlines have hired high-priced attorneys to pursue major damagesâ€”a signal opening of the floodgates to litigation.



Despite the soaring number of vulnerabilities, many fall into long-established categories, such as SQL injections that interfere with database queries and buffer memory overflows that enable code to be executed remotely. Matwyshyn advocates for a mandatory â€œsoftware bill of materials,â€ or S-BOMâ€”an ingredients list that would let organizations understand what components and potential vulnerabilities exist throughout their software supply chains. One recent report found 30% of data breaches stemmed from the vulnerabilities of third-party software vendors or cloud service providers.



She adds: â€œWhen you canâ€™t tell the difference between the companies that are cutting corners and a company that has really invested in doing right by their customers, that results in a market where everyone loses.â€



CISA leadership shares this sentiment, with a spokesperson emphasizing its â€œsecure-by-design principles,â€ such as â€œmaking essential security features available without additional cost, eliminating classes of vulnerabilities, and building products in a way that reduces the cybersecurity burden on customers.â€



Avoiding a digital â€˜dark ageâ€™



It will likely come as no surprise that practitioners are looking to AI to help fill the gap, while at the same time preparing for a coming swarm of cyberattacks by AI agents. Security researchers have used an OpenAI model to discover new â€œzero-dayâ€ vulnerabilities. And both the NVD and CVE teams are developing â€œAI-powered toolsâ€ to help streamline data collection, identification, and processing. NIST says that â€œup to 65% of our analysis time has been spent generating CPEsâ€â€”product information codes that pinpoint affected software. If AI can solve even part of this tedious process, it could dramatically speed up the analysis pipeline.



But Martin cautions against optimism around AI, noting that the technology remains unproven and often riddled with inaccuraciesâ€”which, in security, can be fatal. â€œRather than AI or ML [machine learning], there are ways to strategically automate bits of the processing of that vulnerability data while ensuring 99.5% accuracy,â€ he says.&nbsp;





AI also fails to address more fundamental challenges in governance. The CVE Foundation, launched in April 2025 by breakaway board members, proposes a globally funded nonprofit model similar to that of the internetâ€™s addressing system, which transitioned from US government control to international governance. Other security leaders are pushing to revitalize open-source alternatives like Googleâ€™s OSV Project or the NVD++ (maintained by VulnCheck), which are accessible to the public but currently have limited resources.



As these various reform efforts gain momentum, the world is waking up to the fact that vulnerability intelligenceâ€”like disease surveillance or aviation safetyâ€”requires sustained cooperation and public investment. Without it, a patchwork of paid databases will be all that remains, threatening to leave all but the richest organizations and nations permanently exposed.



Matthew King is a technology and environmental journalist based in New York. He previously worked for cybersecurity firm Tenable.
â€¢ The first babies have been born following â€œsimplifiedâ€ IVF in a mobile lab
  This week Iâ€™m sending congratulations to two sets of parents in South Africa. Babies Milayah and Rossouw arrived a few weeks ago. All babies are special, but these two set a new precedent. Theyâ€™re the first to be born following &#8220;simplified&#8221; IVF performed in a mobile lab.



This new mobile lab is essentially a trailer crammed with everything an embryologist needs to perform IVF on a shoestring. It was designed to deliver reproductive treatments to people who live in rural parts of low-income countries, where IVF can be prohibitively expensive or even nonexistent. And it seems to work!





While IVF is increasingly commonplace in wealthy countriesâ€”around 12% of all births in Spain result from such proceduresâ€”it remains expensive and isnâ€™t always covered by insurance or national health providers. And itâ€™s even less accessible in low-income countriesâ€”especially for people who live in rural areas.



People often assume that countries with high birth rates donâ€™t need access to fertility treatments, says Gerhard Boshoff, an embryologist at the University of Pretoria in South Africa. Sub-Saharan African countries like Niger, Angola, and Benin all have birth rates above 40 per 1,000 people, which isÂ over four times the rates in Italy and Japan, for example.



But that doesnâ€™t mean people in Sub-Saharan Africa donâ€™t need IVF. Globally, aroundÂ one in six adults experience infertility at some point in their lives, according to the World Health Organization. Research by the organization suggests that infertility rates are similar in high-income and low-income countries. AsÂ the WHOâ€™s director general Tedros Adhanom Ghebreyesus puts it: â€œInfertility does not discriminate.â€



For many people in rural areas of low-income countries, IVF clinics simply donâ€™t exist. South Africa is considered a â€œreproductive hubâ€ of the African continent, but even in that country thereÂ are fewer than 30 clinics for a population of over 60 million. AÂ recent study found there were no such clinics in Angola or Malawi.Â Â 



Willem Ombelet, a retired gynecologist, first noticed these disparities back in the 1980s, while he was working at an IVF lab in Pretoria. â€œI witnessed that infertility was [more prevalent] in the black population than the white populationâ€”but they couldnâ€™t access IVF because of apartheid,â€ he says. The experience spurred him to find ways to make IVF accessible for everyone. In the 1990s, he launchedÂ The Walking Eggâ€”a science and art project with that goal.



In 2008, Ombelet met Jonathan Van Blerkom, a reproductive biologist and embryologist who had already been experimenting with a simplified version of IVF. Typically, embryos are cultured in an incubator that provides a sterile mix of gases. Van Blerkomâ€™s approach was to preload tubes with the required gases and seal them with a rubber stopper. â€œWe donâ€™t need a fancy lab,â€ says Ombelet.



COURTESY OF GERHARD BOSHOFF




Eggs and sperm can be injected into the tubes through the stoppers, and the resulting embryos can be grown inside. All you really need is a good microscope and a way to keep the tube warm, says Ombelet. Once the embryos are around five days old, they can be transferred to a personâ€™s uterus or frozen. â€œThe cost is one tenth or one twentieth of a normal lab,â€ says Ombelet.



Ombelet, Van Blerkom, and their colleagues found that this approach appeared to work as well as regular IVF. The team ran their first pilot trial at a clinic in Belgium in 2012. The first babies conceived with the simplified IVF processÂ were born later that year.





More recently, Boshoff wondered if the team could take the show on the road. Making IVF simpler and cheaper is one thing, but getting it to people who donâ€™t have access to IVF care is another. What if the team could pack the simplified IVF lab into a trailer and drive it around rural South Africa?



â€œWe just needed to figure out how to have everything in a very confined space,â€ says Boshoff. As part of the Walking Egg project, he and his colleagues found a way to organize the lab equipment and squeeze in air filters. He then designed a â€œfold-out systemâ€ that allowed the team to create a second room when the trailer was parked. This provides some privacy for people who are having embryos transferred, he says.



People who want to use the mobile IVF lab will first have to undergo treatment at a local medical facility, where they will take drugs that stimulate their ovaries to release eggs, and then have those eggs collected. The rest of the process can be done in the mobile lab, says Boshoff, who presented his work atÂ the European Society of Human Reproduction and Embryologyâ€™s annual meeting in Paris earlier this month.



The first trial started last year. The team partnered with one of the few existing fertility clinics in rural South Africa, which put them in touch with 10 willing volunteers. Five of the 10 women got pregnant following their simplified IVF in the mobile lab. One miscarried, but four pregnancies continued. On June 18, baby Milayah arrived. Two days later, another mother welcomed baby Rossouw. The other babies could come any day now.



â€œWeâ€™ve proven that a very cheap and easy [IVF] method can be used even in a mobile unit and have comparable results to regular IVF,â€ says Ombelet, who says his team is planning similar trials in Egypt and Indonesia. â€œThe next step is to roll it out all over the world.â€



This article first appeared in The Checkup,Â MIT Technology Reviewâ€™sÂ weekly biotech newsletter. To receive it in your inbox every Thursday, and read articles like this first,Â sign up here.
â€¢ The Download: flaws in anti-AI protections for art, and an AI regulation vibe shift
  This is today&#8217;s edition of&nbsp;The Download,&nbsp;our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



This tool strips away anti-AI protections from digital art



The news: A new technique called LightShed will make it harder for artists to use existing protective tools to stop their work from being ingested for AI training. Itâ€™s the next step in a cat-and-mouse gameâ€”across technology, law, and cultureâ€”that has been going on between artists and AI proponents for years.&nbsp;



How it works: Protective tools like Glaze and Nightshade change enough pixels to affect an image, so if itâ€™s scraped up by AI models, they see it as something itâ€™s not. LightShed essentially works by spotting just the â€œpoisonâ€ on poisoned images. To be clear, the researchers behind it arenâ€™t trying to steal artistsâ€™ work. They just donâ€™t want people to get a false sense of security. &nbsp;Read the full story.



â€”Peter Hall







Why the AI moratoriumâ€™s defeat may signal a new political era



The â€œBig, Beautiful Billâ€ that President Donald Trump signed into law on July 4 was chock full of controversial policies. But one highly contested provision was missing. Just days earlier, during a late-night voting session, the Senate had killed the billâ€™s 10-year moratorium on state-level AI regulation.&nbsp;



The bipartisan vote was seen as a victory by many, and may signal a bigger political shift, with a broader and more diverse coalition in favor of AI regulation starting to form. After years of relative inaction, politicians are getting concerned about the risks of unregulated artificial intelligence. Read the full story.&nbsp;



â€”Grace Huckins







Chinaâ€™s energy dominance in three charts



China is the dominant force in next-generation energy technologies today. Itâ€™s pouring hundreds of billions of dollars into putting renewable sources like wind and solar, manufacturing millions of electric vehicles, and building out capacity for energy storage, nuclear power, and more. This investment has been transformational for the countryâ€™s economy and has contributed to establishing China as a major player in global politics.Â 



So while we all try to get our heads around whatâ€™s next for climate tech in the US and beyond, letâ€™s look at just how dominant China is when it comes to clean energy, as documented in three charts. Read the full story.



â€”Casey Crownhart



This article is from The Spark, MIT Technology Reviewâ€™s weekly climate newsletter. To receive it in your inbox every Wednesday, sign up here.







The must-reads



Iâ€™ve combed the internet to find you todayâ€™s most fun/important/scary/fascinating stories about technology.



1 Linda Yaccarino is stepping down as CEO of XShe managed to last almost exactly two years reporting to owner Elon Musk.&nbsp; (Axios)+&nbsp;She was planning to leave before Grokâ€™s anti-Semitic rants, apparently.&nbsp;(NYT&nbsp;$)+&nbsp;Turkey has banned Grok after it insulted President ErdoÄŸan. (Politico)2 OpenAI is planning to release its own web browserIf it works out, itâ€™ll give it the same advantage as Google: direct ownership over usersâ€™ data. (Reuters&nbsp;$)+&nbsp;AI means the end of internet search as weâ€™ve known it.&nbsp;(MIT Technology Review)3 McDonaldâ€™s hiring chatbot exposed millions of applicantsâ€™ data to hackersAdding the insult of carelessness to an already pretty dystopian process! (Wired&nbsp;$)



4 AI-generated images of child sexual abuse are proliferating onlineThis is going to make an already very hard job for law enforcement even harder. (NYT&nbsp;$)5 Autonomous fighter jets are on the horizonEuropean defense start-up Helsing just completed two successful test flights. (FT&nbsp;$)+&nbsp;Generative AI is learning to spy for the US military. (MIT Technology Review)6 What happened to all the human bird flu cases?Since February, the CDC has not recorded a single new case in the US. (Undark)7 An interstellar object is cruising through the solar systemAnd itâ€™s giving astronomers a chance to test out early theories of interstellar-object-ology (yes, thatâ€™s what itâ€™s called!) (The Economist&nbsp;$)+&nbsp;Inside the most dangerous asteroid hunt ever.&nbsp;(MIT Technology Review)8 Apple is planning its first upgrade to its Vision Pro headsetBut no matter what upgrades itâ€™s got, itâ€™s going to be a real struggle to revive its flagging fortunes. (Bloomberg&nbsp;$)9 Where have all the mundane social media posts gone?Normies used to be what made social media good. We miss them and their photos of their breakfasts. (New Yorker&nbsp;$)+&nbsp;Itâ€™s heartening to see that â€˜missed connectionâ€™ posts are making a comeback, though. (The Guardian)10 A global shortage is turning MatchaTok sourBut itâ€™s pretty easy to explain why itâ€™s in short supply: the whole worldâ€™s started going mad for it. (WSJ&nbsp;$)







Quote of the day



Â â€œYouâ€™ll be hard pressed to find someone that really believes in our AI mission. To most, itâ€™s not even clear what our mission is.â€



â€”Tijmen Blankevoort, an AI researcher at Meta, explains why he thinks expensive hires alone might not cure the companyâ€™s woes, The Information reports.







One more thing



MIKE MCQUADE




The race to save our online lives from a digital dark ageThere is a photo of my daughter that I love. She is sitting, smiling, in our old back garden, chubby hands grabbing at the cool grass. It was taken on a digital camera in 2013, when she was almost one, but now lives on Google Photos.But what if, one day, Google ceased to function? What if I lost my treasured photos forever? For many archivists, alarm bells are ringing. Across the world, they are scraping up defunct websites or at-risk data collections to save as much of our digital lives as possible. Others are working on ways to store that data in formats that will last hundreds, perhaps even thousands, of years.The endeavor raises complex questions. What is important to us? How and why do we decide what to keepâ€”and what do we let go? And how will future generations make sense of what weâ€™re able to save? Read the full story.



â€”Niall Firth







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)



+ Why Hollywood is so hell-bent on making sequels.+ I love this sweet little town building program.+ What makes Severanceâ€™s opening credits so darn good?+ This ranking of HBOâ€™s finest shows is fun.
â€¢ Chinaâ€™s energy dominance in three charts
  China is the dominant force in next-generation energy technologies today . Itâ€™s pouring billions of dollars into renewable sources like wind and solar on its grid, manufacturing millions of electric vehicles, and building out capacity for energy storage, nuclear power, and more . In the US, a massive new tax and spending bill just cut hundreds of billions in credits, grants, and loans for clean energy technologies . The question is, will that change anytime soon?
â€¢ This tool strips away anti-AI protections from digital art
  A new technique called LightShed will make it harder for artists to use existing protective tools to stop their work from being ingested for AI training. Itâ€™s the next step in a cat-and-mouse gameâ€”across technology, law, and cultureâ€”that has been going on between artists and AI proponents for years.&nbsp;



Generative AI models that create images need to be trained on a wide variety of visual material, and data sets that are used for this training allegedly include copyrighted art without permission. This has worried artists, who are concerned that the models will learn their style, mimic their work, and put them out of a job.




These artists got some potential defenses in 2023, when researchers created tools like Glaze and Nightshade to protect artwork by â€œpoisoningâ€ it against AI training (Shawn Shan was even named MIT Technology Reviewâ€™s Innovator of the Year last year for his work on these). LightShed, however, claims to be able to subvert these tools and others like them, making it easy for the artwork to be used for training once again.







To be clear, the researchers behind LightShed arenâ€™t trying to steal artistsâ€™ work. They just donâ€™t want people to get a false sense of security. â€œYou will not be sure if companies have methods to delete these poisons but will never tell you,â€ says Hanna Foerster, a PhD student at the University of Cambridge and the lead author of a paper on the work. And if they do, it may be too late to fix the problem.




AI models work, in part, by implicitly creating boundaries between what they perceive as different categories of images. Glaze and Nightshade change enough pixels to push a given piece of art over this boundary without affecting the imageâ€™s quality, causing the model to see it as something itâ€™s not. These almost imperceptible changes are called perturbations, and they mess up the AI modelâ€™s ability to understand the artwork.




Glaze makes models misunderstand style (e.g., interpreting a photorealistic painting as a cartoon). Nightshade instead makes the model see the subject incorrectly (e.g., interpreting a cat in a drawing as a dog). Glaze is used to defend an artistâ€™s individual style, whereas Nightshade is used to attack AI models that crawl the internet for art.




Foerster worked with a team of researchers from the Technical University of Darmstadt and the University of Texas at San Antonio to develop LightShed, which learns how to see where tools like Glaze and Nightshade splash this sort of digital poison onto art so that it can effectively clean it off. The group will present its findings at the Usenix Security Symposium, a leading global cybersecurity conference, in August.&nbsp;



The researchers trained LightShed by feeding it pieces of art with and without Nightshade, Glaze, and other similar programs applied. Foerster describes the process as teaching LightShed to reconstruct â€œjust the poison on poisoned images.â€ Identifying a cutoff for how much poison will actually confuse an AI makes it easier to â€œwashâ€ just the poison off.&nbsp;




LightShed is incredibly effective at this. While other researchers have found simple ways to subvert poisoning, LightShed appears to be more adaptable. It can even apply what itâ€™s learned from one anti-AI toolâ€”say, Nightshadeâ€”to others like Mist or MetaCloak without ever seeing them ahead of time. While it has some trouble performing against small doses of poison, those are less likely to kill the AI modelsâ€™ abilities to understand the underlying art, making it a win-win for the AIâ€”or a lose-lose for the artists using these tools.




Around 7.5 million people, many of them artists with small and medium-size followings and fewer resources, have downloaded Glaze to protect their art. Those using tools like Glaze see it as an important technical line of defense, especially when the state of regulation around AI training and copyright is still up in the air. The LightShed authors see their work as a warning that tools like Glaze are not permanent solutions. â€œIt might need a few more rounds of trying to come up with better ideas for protection,â€ says Foerster.



The creators of Glaze and Nightshade seem to agree with that sentiment: The website for Nightshade warned the tool wasnâ€™t future-proof before work on LightShed ever began. And Shan, who led research on both tools, still believes defenses like his have meaning even if there are ways around them.&nbsp;





â€œItâ€™s a deterrent,â€ says Shanâ€”a way to warn AI companies that artists are serious about their concerns. The goal, as he puts it, is to put up as many roadblocks as possible so that AI companies find it easier to just work with artists. He believes that â€œmost artists kind of understand this is a temporary solution,â€ but that creating those obstacles against the unwanted use of their work is still valuable.



Foerster hopes to use what she learned through LightShed to build new defenses for artists, including clever watermarks that somehow persist with the artwork even after itâ€™s gone through an AI model. While she doesnâ€™t believe this will protect a work against AI forever, she thinks this could help tip the scales back in the artistâ€™s favor once again.

ğŸ”’ Cybersecurity & Privacy
â€¢ UK Arrests Four in â€˜Scattered Spiderâ€™ Ransom Group
  Authorities in the United Kingdom this week arrested four people aged 17 to 20 in connection with recent data theft and extortion attacks against the retailers Marks &amp; Spencer and Harrods, and the British food retailer Co-op Group. The breaches have been linked to a prolific but loosely-affiliated cybercrime group dubbed &#8220;Scattered Spider,&#8221; whose other recent victims include multiple airlines.
The U.K.&#8217;s National Crime Agency (NCA) declined verify the names of those arrested, saying only that they included two males aged 19, another aged 17, and 20-year-old female.
Scattered Spider is the name given to an English-speaking cybercrime group known for using social engineering tactics to break into companies and steal data for ransom, often impersonating employees or contractors to deceive IT help desks into granting access. The FBI warned last month that Scattered Spider had recently shifted to targeting companies in the retail and airline sectors.
KrebsOnSecurity has learned the identities of two of the suspects. Multiple sources close to the investigation said those arrested include Owen David Flowers, a U.K. man alleged to have been involved in the cyber intrusion and ransomware attack that shut down several MGM Casino properties in September 2023. Those same sources said the woman arrested is or recently was in a relationship with Flowers.
Sources told KrebsOnSecurity that Flowers, who allegedly went by the hacker handles &#8220;bo764,&#8221; &#8220;Holy,&#8221; and &#8220;Nazi,&#8221; was the group member who anonymously gave interviews to the media in the days after the MGM hack. His real name was omitted from a September 2024 story about the group because he was not yet charged in that incident.
The bigger fish arrested this week is 19-year-old Thalha Jubair,Â a U.K. man whose alleged exploits under various monikers have been well-documented in stories on this site. Jubair is believed to have used the nickname &#8220;Earth2Star,&#8221; which corresponds to a founding member of the cybercrime-focused Telegram channel &#8220;Star Fraud Chat.&#8221;
In 2023, KrebsOnSecurity published an investigation into the work of three different SIM-swapping groups that phished credentials from T-Mobile employees and used that access to offer a service whereby any T-Mobile phone number could be swapped to a new device. Star Chat was by far the most active and consequential of the three SIM-swapping groups, who collectively broke into T-Mobile&#8217;s network more than 100 times in the second half of 2022.
Jubair allegedly used the handles &#8220;Earth2Star&#8221; and &#8220;Star Ace,&#8221; and was a core member of a prolific SIM-swapping group operating in 2022. Star Ace posted this image to the Star Fraud chat channel on Telegram, and it lists various prices for SIM-swaps.
Sources tell KrebsOnSecurity that Jubair also was a core member of the LAPSUS$ cybercrime group that broke into dozens of technology companies in 2022, stealing source code and other internal data from tech giants including Microsoft, Nvidia, Okta, Rockstar Games, Samsung, T-Mobile, and Uber.
In April 2022, KrebsOnSecurity published internal chat records from LAPSUS$, and those chats indicated Jubair was using the nicknames Amtrak and Asyntax. At one point in the chats, Amtrak told the LAPSUS$ group leader not to share T-Mobile&#8217;s logo in images sent to the group because he&#8217;d been previously busted for SIM-swapping and his parents would suspect he was back at it again.
As shown in those chats, the leader of LAPSUS$ eventually decided to betray Amtrak by posting his real name, phone number, and other hacker handles into a public chat room on Telegram.
In March 2022, the leader of the LAPSUS$ data extortion group exposed Thalha Jubair&#8217;s name and hacker handles in a public chat room on Telegram.
That story about the leaked LAPSUS$ chats connected Amtrak/Asyntax/Jubair to the identity &#8220;Everlynn,&#8221; the founder of a cybercriminal service that sold fraudulent &#8220;emergency data requests&#8221; targeting the major social media and email providers. In such schemes, the hackers compromise email accounts tied to police departments and government agencies, and then send unauthorized demands for subscriber data while claiming the information being requested canâ€™t wait for a court order because it relates to an urgent matter of life and death.
The roster of the now-defunct &#8220;Infinity Recursion&#8221; hacking team, from which some member of LAPSUS$ hail.
Sources say Jubair also used the nickname &#8220;Operator,&#8221; and that until recently he was the administrator of the Doxbin, a long-running and highly toxic online community that is used to â€œdoxâ€ or post deeply personal information on people. In May 2024, several popular cybercrime channels on Telegram ridiculed Operator after it was revealed that he&#8217;d staged his own kidnapping in a botched plan to throw off law enforcement investigators.
In November 2024, U.S. authorities charged five men aged 20 to 25 in connection with the Scattered Spider group, which has long relied on recruiting minors to carry out its most risky activities. Indeed, many of the group&#8217;s core members were recruited from online gaming platforms like Roblox and Minecraft in their early teens, and have been perfecting their social engineering tactics for years.
&#8220;There is a clear pattern that some of the most depraved threat actors first joined cybercrime gangs at an exceptionally young age,&#8221; said Allison Nixon, chief research officer at the New York based security firm Unit 221B. &#8220;Cybercriminals arrested at 15 or younger need serious intervention and monitoring to prevent a years long massive escalation.&#8221;
â€¢ Microsoft Patch Tuesday, July 2025 Edition
  Microsoft releases updates to fix at least 137 security vulnerabilities in its Windows operating systems and supported software . None of the weaknesses addressed this month are known to be actively exploited, but 14 of the flaws earned Microsoftâ€™s most-dire &#8217;s most dire . The flaws could be exploited to seize control over vulnerable Windows PCs with little or no help from users . Microsoft also patched at least four critical, remote code execution flaws in Office .

ğŸ“ University AI
No updates.

ğŸ¢ Corporate AI
â€¢ How AI will accelerate biomedical research and discovery
  In November 2022, OpenAIâ€™s ChatGPT kick-started a new era in AI. This was followed less than a half year later by the release of GPT-4. In the months leading up to GPT-4&#8217;s public release, Peter Lee, president of Microsoft Research, cowrote a book full of optimism for the potential of advanced AI models to transform the world of healthcare. What has happened since? In this special podcast series, The AI Revolution in Medicine, Revisited, Lee revisits the book, exploring how patients, providers, and other medical professionals are experiencing and using generative AI today while examining what he and his coauthors got rightâ€”and what they didnâ€™t foresee.



In this episode, Daphne Koller (opens in new tab), Noubar Afeyan (opens in new tab), and Dr. Eric Topol (opens in new tab), leaders in AI-driven medicine, join Lee to explore the rapidly evolving role of AI across the biomedical and healthcare landscape. Koller, founder and CEO of Insitro, shares how machine learning is transforming drug discovery, especially target identification for complex diseases like ALS, by uncovering biological patterns across massive datasets. Afeyan, founder and CEO of Flagship Pioneering and co-founder and chairman of Moderna, discusses how AI is being applied across biotech research and development, from protein design to autonomous science platforms. Topol, executive vice president of Scripps Research and founder and director of the Scripps Research Translational Institute, highlights how AI can today help mitigate and prevent the core diseases that erode our health and the possibility of realizing a virtual cell. Through his conversations with the three, Lee investigates how AI is reshaping the discovery, deployment, and delivery of medicine.Â 








Learn more:




How Machine Learning Is Revolutionising Drug Discovery (opens in new tab) (Koller)&nbsp;WIRED Health talk | March 2025



Insitro and Lilly Enter Strategic Agreements to Advance Novel Treatments for Metabolic Diseases (opens in new tab)&nbsp;(Koller)&nbsp;Insitro release | October 2024&nbsp;



2025 Annual Letter: Polyintelligence (opens in new tab) (Afeyan)&nbsp;Flagship Pioneering | 2025



The ultimate in mind extenders (opens in new tab) (Afeyan)&nbsp;McGill Daily article (college newspaper) | October 1982&nbsp;



Super Agers: An Evidence-Based Approach to Longevity (opens in new tab) (Topol)&nbsp;Book | May 2025&nbsp;



Deep Medicine: How Artificial Intelligence Can Make Healthcare Human Again (opens in new tab) (Topol)&nbsp;Book | March 2019&nbsp;



The AI Revolution in Medicine: GPT-4 and BeyondBook | Peter Lee, Carey Goldberg, Isaac Kohane | April 2023










	
		
			Subscribe to the Microsoft Research Podcast:		
		
							
					
						  
						Apple Podcasts
					
				
			
							
					
						
						Email
					
				
			
							
					
						
						Android
					
				
			
							
					
						
						Spotify
					
				
			
							
					
						
						RSS Feed
					
				
					
	




	
		
			
				
					

Transcript&nbsp;



[MUSIC]



[BOOK PASSAGE]&nbsp;



PETER LEE: â€œCan GPT-4 indeed accelerate the progression of medicine â€¦ ? It seems like a tall order, but if I had been told six months ago that it could rapidly summarize any published paper, that alone would have satisfied me as a strong contribution to research productivity. â€¦ But now that I&#8217;ve seen what GPT-4 can do with the healthcare process, I expect a lot more in the realm of research.â€&nbsp;



[END OF BOOK PASSAGE]



[THEME MUSIC]



This is The AI Revolution in Medicine, Revisited. Iâ€™m your host, Peter Lee.



Shortly after OpenAI&#8217;s GPT-4 was publicly released, Carey Goldberg, Dr. Zak Kohane, and I published The AI Revolution in Medicine to help educate the world of healthcare and medical research about the transformative impact this new generative AI technology could have. But because we wrote the book when GPT-4 was still a secret, we had to speculate. Now, two years later, what did we get right, and what did we get wrong?



In this series, weâ€™ll talk to clinicians, patients, hospital administrators, and others to understand the reality of AI in the field and where we go from here.



[THEME MUSIC FADES]



The book passage I read at the top was from â€œChapter 8: Smarter Science,â€ which was written by Zak.



In writing the book, we were optimistic about AIâ€™s potential to accelerate biomedical research and help get new and much-needed treatments and drugs to patients sooner. One area we explored was generative AI as a designer of clinical trials. We looked at generative AIâ€™s adeptness at summarizing helping speed up pre-trial triage and research. We even went so far as to predict the arrival of a large language model that can serve as a central intellectual tool.&nbsp;



For a look at how AI is impacting biomedical research today, Iâ€™m excited to welcome Daphne Koller, Noubar Afeyan, and Eric Topol.&nbsp;



				
				
					



Daphne Koller is the CEO and founder of Insitro, a machine learning-driven drug discovery and development company that recently made news for its identification of a novel drug target for ALS and its collaboration with Eli Lilly to license Lilly&#8217;s biochemical delivery systems. Prior to founding Insitro, Daphne was the co-founder, co-CEO, and president of the online education platform Coursera.



Noubar Afeyan is the founder and CEO of Flagship Pioneering, which creates biotechnology companies focused on transforming human health and environmental sustainability. He is also co-founder and chairman of the messenger RNA company Moderna. An entrepreneur and biochemical engineer, Noubar has numerous patents to his name and has co-founded many startups in science and technology.



Dr. Eric Topol is the executive vice president of the biomedical research non-profit Scripps Research, where he founded and now directs the Scripps Research Translational Institute. One of the most cited researchers in medicine, Eric has focused on promoting human health and individualized medicine through the use of genomic and digital data and AI.&nbsp;



These three are likely to have an outsized influence on how drugs and new medical technologies soon will be developed.



[TRANSITION MUSIC]&nbsp;



Hereâ€™s my interview with Daphne Koller:



LEE: Daphne, I&#8217;m just thrilled to have you join us.&nbsp;



DAPHNE KOLLER: Thank you for having me, Peter. It&#8217;s a pleasure to be here.&nbsp;



LEE: Well, you know, you&#8217;re quite well-known across several fields. But maybe for some audience members of this podcast, they might not have encountered you before. So where I&#8217;d like to start is a question I&#8217;ve been asking all of our guests.



How would you describe what you do? And the way I kind of put it is, you know, how do you explain to someone like your parents what you do for a living?&nbsp;



KOLLER: So that answer obviously has shifted over the years.



What I would say now is that we are working to leverage the incredible convergence of very powerful technologies, of which AI is one but not the only one, to change the way in which we discover and develop new treatments for diseases for which patients are currently suffering and even dying.&nbsp;



LEE: You know, I think I&#8217;ve known you for a long time.&nbsp;



KOLLER: Longer than I think either of us care to admit.&nbsp;



LEE: [LAUGHS] In fact, I think I remember you even when you were still a graduate student. But of course, I knew you best when you took up your professorship at Stanford. And I always, in my mind, think of you as a computer scientist and a machine learning person. And in fact, you really made a big name for yourself in computer science research in machine learning.



But now you&#8217;re, you know, leading one of the most important biotech companies on the planet. How did that happen?



KOLLER: So people often think that this is a recent transition. That is, after I left Coursera, I looked around and said, â€œHmm. What should I do next? Oh, biotech seems like a good thing,â€ but that&#8217;s actually not the way it transpired.



This goes all the way back to my early days at Stanford, where, in fact, I was, you know, as a young faculty member in machine learning, because I was the first machine learning hire into Stanford&#8217;s computer science department, I was looking for really exciting places in which this technology could be deployed, and applications back then, because of scarcity of data, were just not that inspiring.



And so I looked around, and this was around the late â€™90s, and realized that there was interesting data emerging in biology and medicine. My first application actually was in, interestingly, in epidemiologyâ€”patient tracking and tuberculosis. You know, you can think of it as a tiny microcosm of the very sophisticated models that COVID then enabled in a much later stage.



LEE: Right.&nbsp;



KOLLER: And so initially, this was based almost entirely on just technical interest. It&#8217;s kind of like, oh, this is more interesting as a question to tackle than spam filtering. But then I became interested in biology in its own right, biology and medicine, and ended up having a bifurcated existence as a Stanford professor where half my lab continued to do core computer science research published in, you know, NeurIPS and ICML. And the other half actually did biomedical research that was published in, you know, Nature Cell [and] Science. So that was back in, you know, the early, early 2000s, and for most of my Stanford career, I continued to have both interests.



And then the Coursera experience kind of took me out of Stanford and put me in an industry setting for the first time in my life actually. But then when my time at Coursera came to an end, you know, I&#8217;d been there for five years. And if you look at the timeline, I left Stanford in early 2012, right as the machine learning revolution was starting. So I missed the beginning.



And it was only in like 2016 or so that, as I picked my head up over the trenches, like, â€œOh my goodness, this technology is going to change the world.â€ And I wanted to deploy that big thing towards places where it would have beneficial impact on the world, like to make the world a better place.



LEE:â€¯Yeah.â€¯



KOLLER: And so I decided that one of the areas where I could make a unique, differentiated impact was in really bringing AI and machine learning to the life sciences, having spent, you know, the majority of my career at the boundary of those two disciplines. And notice I say â€œboundaryâ€ with deliberation because there wasn&#8217;t very much of an intersection.



LEE: Right.&nbsp;



KOLLER: I felt like I could do something that was unique.&nbsp;



LEE: So just to stick on you for a little bit longer, you know, we have been sort of getting into your origin story about what we call AI todayâ€”but machine learning, so deep learning.&nbsp;



And, you know, there has always been a kind of an emotional response for people like you and me and now the general public about their first encounters with what we now call generative AI. Iâ€™d love to hear what your first encounter was with generative AI and how you reacted to this.&nbsp;



KOLLER: I think my first encounter was actually an indirect one. Because, you know, the earlier generations of generative AI didnâ€™t directly touch our work at Insitro (opens in new tab).&nbsp;



And yet at the same time, I had always had an interest in computer vision. That was a large part of my non-bio work when I was at Stanford.&nbsp;



And so some of my earlier even presentations, when I was trying to convey to people back in 2016 how this technology was going to transform the world, I was talking about the incredible progress in image recognition that had happened up until that point.&nbsp;



So my first interaction was actually in the generative AI for images, where you are able to go the other way â€¦&nbsp;



LEE: Yes.&nbsp;



KOLLER: â€¦ where you can take a verbal description of an image and createâ€”and this was back in the days when the images weren&#8217;t particularly photorealistic, but still a natural language description to an image was magic given that only two or three years before that, we were barely able to look at an image and write a short phrase saying, â€œThis is a dog on the beach.â€ And so that arc, that hockey curve, was just mind blowing to me.&nbsp;



LEE: Did you have moments of skepticism?&nbsp;



KOLLER: Yeah, I mean the early, you know, early versions of ChatGPT, where it was more like parlor tricks and poking it a little bit revealed all of the easy ways that one could break it and make it do really stupid things. I was like, yeah, OK, this is kind of cute, but is it going to actually make a difference? Is it going to solve a problem that matters?&nbsp;



And I mean, obviously, I think now everyone agrees that the answer is yes, although there are still people who are like, yeah, but maybe it&#8217;s around the edges. I&#8217;m not among them, by the way, but &#8230; yeah, so initially there were like, â€œYeah, this is cute and very impressive, but is it going to make a difference to a problem that matters?â€&nbsp;



LEE: Yeah. So now, maybe this is a good time to get into what you&#8217;ve been doing with ALS [amyotrophic lateral sclerosis]. You know, there&#8217;s a knee-jerk reaction from the technology side to focus on designing small molecules, on predicting, you know, their properties, you know, maybe binding affinity or aspects of ADME [absorption, distribution, metabolism, and excretion], you know, like absorption or dispersion or whatever.&nbsp;



And all of that is very useful, but if I understand the work on ALS, you went to a much harder place, which is to actually identify and select targets.&nbsp;



KOLLER: Thatâ€™s right.&nbsp;



LEE: So first off, just for the benefit of the standard listeners of this podcast, explain what that problem is in general.&nbsp;



KOLLER: No, for sure. And I think maybe I&#8217;ll start by just very quickly talking about the drug discovery and development arc, &#8230;



LEE: Yeah.



KOLLER: &#8230; which, by and large, consists of three main phases. That&#8217;s the standard taxonomy.&nbsp;The first is what&#8217;s called sometimes target discovery or identifying a therapeutic hypothesis, which looks like: if I modulate this target in this disease, something beneficial will happen.&nbsp;



Then, you have to take that target and turn it into a molecule that you can actually put into a person. It could be a small molecule. It could be a large molecule like an antibody, whatever. And then you have that construct, that molecule. And the last piece is you put it into a person in the context of a clinical trial, and you measure what has happened. And there&#8217;s been AI deployed towards each of those three stages in different ways.&nbsp;



The last one is mostly like an efficiency gain. You know, the trial is kind of already defined, and you want to deploy technology to make it more efficient and effective, which is great because those are expensive operations.&nbsp;



LEE: Yep.&nbsp;



KOLLER: The middle one is where I would say the vast majority of efforts so far has been deployed in AI because it is a nice, well-defined problem. It doesn&#8217;t mean it&#8217;s easy, but it&#8217;s one where you can define the problem. It is, I need to inhibit this protein by this amount, and the molecule needs to be soluble and whatever and go past the blood-brain barrier. And you know probably within a year and a half or so, or two, if you succeeded or not.&nbsp;



The first stage is the one where I would say the least amount of energy has gone because when you&#8217;re uncovering a novel target in the context of an indication, you don&#8217;t know that you&#8217;ve been successful until you go all the way to the end, which is the clinical trial, which is what makes this a long and risky journey. And not a lot of people have the appetite or the capital to actually do that.&nbsp;



However, in my opinion, and that of, I think, quite a number of others, it is where the biggest impact can be made. And the reason is that while pharma has its deficiencies, making good molecules is actually something they&#8217;re pretty good at.&nbsp;



It might take them longer than it should, maybe it&#8217;s not as efficient as it could be, but at the end of the day, if you tell them to drug A target, pharma is actually pretty good at generating those molecules. However, when you put those molecules into the clinic, 90% of them fail. And the reason they fail is not by and large because the molecule wasn&#8217;t good. In the majority of cases, it&#8217;s because the target you went after didn&#8217;t do anything useful in the context of the patient population in which you put it.&nbsp;



And so in order to fix the inefficiency of this industry, which is incredible inefficiency, you need to address the problem at the root, and the root is picking the right targets to go after. And so that is what we elected to do.&nbsp;



It doesn&#8217;t mean we don&#8217;t make molecules. I mean, of course, you can&#8217;t just end up with a target because a target is not actionable. You need to turn it into a molecule. And we absolutely do that. And by the way, the partnership with Lilly (opens in new tab) is actually one where they help us make a molecule.&nbsp;



LEE: Yes.&nbsp;



KOLLER: I mean, it&#8217;s our target. It&#8217;s our program. But Lilly is deploying its very state-of-the-art molecule-making capabilities to help us turn that target into a drug.&nbsp;



LEE: So let&#8217;s get now into the machine learning of this. Again, this just strikes me as such a difficult problem to solve.&nbsp;



KOLLER: Yeah.&nbsp;



LEE: So how does machine learning &#8230; how does AI help you?&nbsp;



KOLLER: So I think when you look at how people currently select targets, it&#8217;s a combination of oftentimes at this point, with an increasing respect for the power of human genetics, some search for a genetic association, oftentimes with a human-defined, highly subjective, highly noisy clinical outcome, like some ICD [International Classification of Diseases] code.&nbsp;



And those are often underpowered and very difficult to deconvolute the underlying biology. You combine that with some mechanistic interrogation in a highly reductionist model system looking at a small number of readouts, biochemical readouts, that a biologist thinks are relevant to the disease. Like does this make this, whatever, cholesterol go up or amyloid beta go down? Or whatever. And then you take that as the second stage, and you pick, based on typically human intuition about, Oh, this one looks good to me, and then you take that forward.&nbsp;



What we&#8217;re doing is an attempt to be as unbiased and holistic as possible. So, first of all, rather than rely on human-defined clinical endpoints, like this person has been diagnosed with diabetes or fatty liver, we try and measure as much as we can a holistic physiological state and then use machine learning to find structure, patterns in that human physiological readouts, imaging readouts, and omics readouts from blood, from tissue, different kinds of imaging, and say, these are different vectors that this disease takes, this group of individuals, and here&#8217;s a different group of individuals that maybe from a diagnostical perspective are all called the same thing, but they are actually exhibiting a very different biology underlying it.&nbsp;



And so that is something that doesn&#8217;t emerge when a human being takes a reductionist view to looking at this high-content data, and oftentimes, they don&#8217;t even look at it and produce an ICD code.&nbsp;



LEE: Right. Yep.&nbsp;



KOLLER: The same approach, actually even the same code base, is taken in the cellular data. So we don&#8217;t just say, â€œWell, the thing that matters is, you know, the total amount of lipid in the cell or whatever.â€ Rather, we say, â€œLet&#8217;s look at multiple readouts, multiple ways of looking at the cells, combine them using the power of machine learning.â€ And again, looking at imaging readouts where a human&#8217;s eyes just glaze over looking at even a few dozen cells, far less a few hundreds of millions of cells, and understand what are the different biological processes that are going on. What are the vectors that the disease might take you in this direction, in this group of cells, or in that direction?&nbsp;



And then importantly, we take all of that information from the human side, from the cellular side, across these different readouts, and we combine them using an integrative approach that looks at the combined weight of evidence and says, these are the targets that I have the greatest amount of conviction about by looking across all of that information. Whereas we know, and we know this, I&#8217;m sure you&#8217;ve seen this analysis done for clinicians, a human being typically is able to keep three or four things in their head at the same time.&nbsp;



LEE: Right.&nbsp;



KOLLER: A really good human being who&#8217;s really expert at what they do can maybe get to six to eight.&nbsp;



LEE: Yeah.&nbsp;



KOLLER: The machine learning has no problem doing a few hundred.&nbsp;



LEE: Right.&nbsp;



KOLLER: And so you put that together, and that allows you, to your earlier question, really select the targets around which you have the highest conviction. And then those are the ones that we then prioritize for interrogation in more expensive systems like mice and monkeys and then at the end of the day pick the small handful that one can afford to actually take into clinical trials.&nbsp;



LEE: So now, Insitro recently received $25 million in milestone payments from Bristol Myers Squibb (opens in new tab) after discovering and selecting a novel drug target for ALS. Can you tell us a little bit more about that?â€¯



KOLLER: We are incredibly excited about the first novel target, and there is a couple of others just behind it in line that seem, you know, quite efficacious, as well, that truly seem to reverse, albeit in a cellular system, what we now understand to be ALS pathology across multiple different dimensions. There&#8217;s been obviously many attempts made to try and address ALS, which by the way, horrible, horrible disease, worse than most cancers. It kills you almost inevitably in three to five years in a particularly horrific way.&nbsp;



And what we have in our hands is a target that seems to revert a lot of the pathologies that are associated with the disease, which we now understand has to do with the mis-splicing of multiple proteins within the cell and creating defective versions of those proteins that are just not operational. And we are seeing reversion of many of those.&nbsp;



So can I tell you for sure it&#8217;ll work in a human? No, there&#8217;s many steps between now and then. But we couldn&#8217;t be more excited about the opportunity to provide what we hope will be a disease-modifying intervention for these patients who really desperately need something.&nbsp;



LEE: Well, it&#8217;s certainly been making waves in the biotech and biomedical world.&nbsp;



KOLLER: Thank you.&nbsp;



LEE: So we&#8217;ll be really watching very closely.&nbsp;



So, you know, I think just reflecting on, you know, what we missed and what we got right in our book, I think in our book, we did have the insight that there would be an ability to connect, say, genotypic and phenotypic data and, you know, just broadly the kinds of clinical measurements that get made on real patients and that these things could be brought together. And I think the work that you&#8217;re doing really illustrates that in a very, very sophisticated, very ambitious way.&nbsp;



But the fact that this could be connected all the way down to the biology, to the biochemistry, I think we didn&#8217;t have any clue what would happen, at least not this quickly.&nbsp;



KOLLER: Well, I think the &#8230;&nbsp;



LEE: And I realize, you&#8217;ve been at this for quite a few years, but still, it&#8217;s quite amazing.&nbsp;



KOLLER: The thread that connects them is human genetics. And I think that has, to us, been, sort of, the, kind of, the connective tissue that allows you to translate across different systems and say, â€œWhat does this gene do? What does this gene do in this organ and in that organ? What does it do in this type of cell and in that type of cell?â€&nbsp;



And then use that as sort of the thread, if you will, that follows the impact of modulating this gene all the way from the simple systems where you can do the experiment to the complex systems where you can&#8217;t do the experiment until the very end, but you have the human genetics as a way of looking at the statistics and understanding what the impact might be.&nbsp;



LEE: So I&#8217;d like to now switch gears and take â€¦ I want to take two steps in the remainder of this conversation towards the future. So one step into that future, of course, we&#8217;re living through now, which is just all of the crazy pace of work and advancement in generative AI generally, you know, just the scale of transformers, of post-training, and now inference scale and reasoning models and so on. And where do you see all of that going with respect to the goals that you have and that Insitro has?&nbsp;



KOLLER: So I think first and foremost is the parallel, if you will, to the predictions that you focused on in your book, which is this will transform a lot of the core data processing tasks, the information tasks. And sure, the doctors and nurses is one thing. But if you just think of clinical trial operations or the submission of regulatory documents, these are all kind of simple data â€¦ they&#8217;re not simple, obviously, but they&#8217;re data processing tasks. They involve natural language. That&#8217;s not going to be our focus, but I hope that others will use that to make clinical trials faster, more efficient, less expensive.&nbsp;



There&#8217;s already a lot of progress that&#8217;s happening on the molecular design side of things and taking hypotheses and turning them quickly and effectively into molecules. As I said, this is part of our work that we absolutely do and we don&#8217;t talk about it very much, simply because it&#8217;s a very crowded landscape and a lot of companies are engaged on that. But I think it&#8217;s really important to be able to take biological insights and turn them into new molecules.&nbsp;



And then, of course, the transformer models and their likes play a very significant role in that sort of turning insights into molecules because you can have foundation models for proteins. There are increasing efforts to create foundation models for other categories of molecules. And so that will undoubtedly accelerate the process by which you can quickly generate different molecular hypotheses and test them and learn from what you did so that you can do fewer iterations â€¦&nbsp;



LEE: Right.&nbsp;



KOLLER: â€¦ before you converge on a successful molecule.&nbsp;



I do think that arguably the biggest impact as yet to be had is in that understanding of core human biology and what are the right ways to intervene in it. And that plays a role in a couple different ways. First of all, it certainly plays a role in which â€¦ if we are able to understand the human physiological state and, you know, the state of different systems all the way down to the cell level, that will inform our ability to pick hypotheses that are more likely to actually impact the right biologies underneath.&nbsp;



LEE: Yep. Yeah.&nbsp;



KOLLER: And the more data we&#8217;re able to collect about humans and about cells, the more successful our models will be at representing that human physiological state or the cell biological state and making predictions reliably on the impact of these interventions.&nbsp;



The other side of it, though, and this comes back, I think, to themes that were very much in your book, is this will impact not only the early stages of which hypotheses we interrogate, which molecules we move forward, but also hopefully at the end of the day, which molecule we prescribe to which patient.&nbsp;



LEE: Right.&nbsp;



KOLLER: And I think there&#8217;s been obviously so much narrative over the years about precision medicine, personalized medicine, and very little of that has come to fruition, with the exception of, you know, certain islands in oncology, primarily on genetically driven cancers.&nbsp;



But I think the opportunity is still there. We just haven&#8217;t been able to bring it to life because of the lack of the right kind of data. And I think with the increasing amount of human, kind of, foundational data that we&#8217;re able to acquire, things that are not sort of distilled through the eye of a clinician, for example, â€¦&nbsp;



LEE: Yes.&nbsp;



KOLLER: â€¦ but really measurements of human pathology, we can start to get to some of that precision, carving out of the human population and then get to a world where we can prescribe the right medicine to the right patient and not only in cancer but also in other diseases that are also not a single disease.&nbsp;



LEE: All right, so now to wrap up this time together, I always try to ask one more provocative last question. One of the dreams that comes naturally to someone like me or any of my colleagues, probably even to you, is this idea of, you know, wouldn&#8217;t it be possible someday to have a foundation model for biology or for human biology or foundation model for the human cell or something along these lines?&nbsp;



And in fact, there are, of course, you and I are both aware of people who are taking that idea seriously and chasing after it. I have people in our labs that think hard about this kind of thing. Is it a reasonable thought at all?&nbsp;



KOLLER: I have learned over the years to avoid saying the word never because technology proceeds in ways that you often don&#8217;t expect. And so will we at some point be able to measure the cell in enough different ways across enough different channels at the same time that you can piece together what a cell does? I think that is eminently feasible, not today, but over time.&nbsp;



I don&#8217;t think it&#8217;s feasible using today&#8217;s technology, although the efforts to get there may expose where the biggest opportunities lie to, you know, build that next layer. So I think it&#8217;s good that people are working on really hard problems. I would also point out that even if one were to solve that really challenging problem of creating a model of a cell, there is thousands of different types of cells within the human body.&nbsp;



They&#8217;re very different. They also talk to each other â€¦&nbsp;



LEE: Yep.&nbsp;



KOLLER: â€¦ both within the cell type and across different cell types. So the combinatorial complexity of that system is, I think, unfathomable to many people. I mean, I would say to all of us.&nbsp;



LEE: Yeah.&nbsp;



KOLLER: And so even from that very lofty goal, there is multiple big steps that would need to be taken to a mechanistic model of the full organism. So will we ever get there? Again, you know, I don&#8217;t see a reason why this is impossible to do. So I think over time, technology will get better and will allow us to build more and more elaborate models of more and more complex systems.&nbsp;



Patients can&#8217;t wait &#8230;



LEE: Right. Yeah.&nbsp;



KOLLER: â€¦ for that to happen in order for us to get them better medicines. So I think there is a great basic science initiative on that side of things. And, in parallel, we need to make do with the data that we have or can collect or can print. We print a lot of data in our internal wet labs and get to drugs that are effective even though they don&#8217;t benefit from having a full-blown mechanistic model.&nbsp;



LEE: Last question: where do you think we&#8217;ll be in five years?&nbsp;



KOLLER: Phew. If I had answered that question five years ago, I would have been very badly embarrassed at the inaccuracy of my answer. [LAUGHTER] So I will not answer it today either.&nbsp;



I will say that the thing about exponential curves is that they are very, very tricky, and they move in unexpected ways. I would hope that in five years, we will have made a sufficient investment in the generation of scientific data that we will be able to move beyond data that was generated entirely by humans and therefore insights that are derivative of what people already know to things that are truly novel discoveries.&nbsp;



And I think in order to do that in, you know, math, maybe because math is entirely conceptual, maybe you can do that today. Math is effectively a construct of the human mind. I don&#8217;t think biology is a construct of the human mind, and therefore one needs to collect enough data to really build those models that will give rise to those novel insights.&nbsp;



And that&#8217;s where I hope we will have made considerable progress in five years.&nbsp;



LEE: Well, I&#8217;m with you. I hope so, too. Well, you know, thank you, Daphne, so much for this conversation. I learn a lot talking to you, and it was great to, you know, connect again on this. And congratulations on all of this success. It&#8217;s really groundbreaking.&nbsp;



KOLLER: Thank you very much, Peter. It was a pleasure chatting with you, as well.&nbsp;



[TRANSITION MUSIC]&nbsp;



LEE: I still think of Daphne first and foremost as an AI researcher. And for sure, her research work in machine learning continues to be incredibly influential to this day. But it&#8217;s her work on AI-enhanced drug development that now is on the verge of making a really big difference on some of the most difficult diseases afflicting people today.&nbsp;



In our book, Carey, Zak, and I predicted that AI might be a meaningful accelerant in biomedical research, but I don&#8217;t know that we foresaw the incredible potential specifically in drug development.&nbsp;



Today, we&#8217;re seeing a flurry of activity at companies, universities, and startups on generative AI systems that aid and maybe even completely automate the design of new molecules as drug candidates. But now, in our conversation with Daphne, seeing AI go even further than that to do what one might reasonably have assumed to be impossible, to identify and select novel drug targets, especially for a neurodegenerative disease like ALS, it&#8217;s just, well, mind blowing.â€¯



Let&#8217;s continue our deep dive on AI and biomedical research with this conversation with Noubar Afeyan:&nbsp;



LEE: Noubar, thanks so much for joining. I&#8217;m really looking forward to this conversation.&nbsp;



NOUBAR AFEYAN: Peter, thanks. Thrilled to be here.&nbsp;



LEE: While I think most of the listeners to this podcast have heard of Flagship Pioneering (opens in new tab), it&#8217;s still worth hearing from you, you know, what is Flagship? And maybe a little bit about your background. And finally, you found a way to balance science and business creation. And so, you know, your approach and philosophy to all of that.&nbsp;



AFEYAN: Well, great. So maybe I&#8217;ll just start out by way of quick background. You know, my &#8230; and since we&#8217;re going talk about AI, I&#8217;ll also highlight my first contact with the topic of AI. So as an undergraduate in 1980 up at McGill University, I was an engineering student, but I was really captivated by, at that time, the talk on the campus around the expert system, heuristic-based, rule-based kind of programs.&nbsp;



LEE: Right.&nbsp;



AFEYAN: And so actually I had the dubious distinction of writing my one and only college newspaper article. [LAUGHTER] That was a short career. And it was all about how artificial intelligence would be impacting medicine, would be impacting, you know, speech capture, translation, and some of the ideas that were there that it&#8217;s interesting to see now 45 years later re-emerge with some of the new learning-based models.&nbsp;



My journey after college ended up taking me into biotechnology. In the early â€™80s, I came to MIT to do a PhD. At the time, the field was brand new. I ended up being the first PhD graduate from MIT in this combination biology and engineering degree. And since then, I&#8217;ve basically beenâ€”so since 1987â€”a founder, a technologist in the space of biotechnology for human health and as well for planetary health.&nbsp;



And then in 1999/2000 formed what is now Flagship Pioneering, which essentially was an attempt to bring together the three elements of what we know are important in startups. That is scientific capital, human capital, and financial capital. Right now, startups get that from different places. The science in our fields mostly come from academia, research hospitals. The human capital comes from other startups â€¦&nbsp;



LEE: Yeah.&nbsp;



AFEYAN: â€¦ or large companies or some academics leave. And then the financial capital is usually venture capital, but there&#8217;s also now more and more other deeper pockets of money.&nbsp;



What we thought was, what if all that existed in one entity and instead of having to convince each other how much they should believe the other if we just said, â€œLet&#8217;s use that power to go work on much further out thingsâ€? But in a way where nobody would believe it in the beginning, but we could give ourselves a little bit of time to do impactful big things.&nbsp;



Twenty-five years later, that&#8217;s the road we&#8217;ve stayed on.&nbsp;



LEE: OK. So let&#8217;s get into AI. Now, you know, what I&#8217;ve been asking guests is kind of an origin story. And there&#8217;s the origin story of contact with AI, you know, before the emergence of generative AI and afterwards. I don&#8217;t think there&#8217;s much of a point to asking you the pre-ChatGPT. But â€¦ so let&#8217;s focus on your first encounter with ChatGPT or generative AI. When did that happen, and what went through your head?&nbsp;



AFEYAN: Yeah. So, if you permit me, Peter, just for very briefly, let me actually say I had the interesting opportunity over the last 25 years to actually stay pretty close to the machine learning world â€¦&nbsp;



LEE: Yeah. Yeah.&nbsp;



AFEYAN: â€¦ because one, as you well know, among the most prolific users of machine learning has been the bioinformatics computational biology world because it&#8217;s been so data rich that anything that can be done, people have thrown at these problems because unlike most other things, we&#8217;re not working on man-made data. We&#8217;re looking at data that comes from nature, the complexity of which far exceeds our ability to comprehend.&nbsp;



So you could imagine that any approach to statistically reduce complexity, get signal out of scant dataâ€”that&#8217;s a problem that&#8217;s been around.&nbsp;



The other place where I&#8217;ve been exposed to this, which I&#8217;m going to come back to because that&#8217;s where it first felt totally different to me, is that some 25 years ago, actually the very first company we started was a company that attempted to use evolutionary algorithms to essentially iteratively evolve consumer-packaged goods online. Literally, we tried to, you know, consider features of products as genes and create little genomes of them. And by recombination and mutation, we could create variety. And then we could get people through panels onlineâ€”this was 2002/2003 timeframeâ€”we could essentially get people through iterative cycles of voting to create a survival of the fittest. And that&#8217;s a company that was called Affinnova.&nbsp;



The reason I say that is that I knew that thereâ€™s a much better way to do this if only: one, you can generate variety â€¦&nbsp;



LEE: Yeah.&nbsp;



AFEYAN: â€¦ without having to prespecify genes. We couldnâ€™t do that before. And, two, which weâ€™ve come back to nowadays, you can actually mimic how humans think about voting on things and just get rid of that element of it.&nbsp;



So then to your question of when does this kind of begin to feel different? So you could imagine that in biotechnology, you know, as an engineer by background, I always wanted to do CAD, and I picked the one field in which CAD doesn&#8217;t exist, which is biology. Computer-aided design is kind of a notional thing in that space. But boy, have we tried. For a long time, &#8230;



LEE: Yep.&nbsp;



AFEYAN: &#8230; people would try to do, you know, hidden Markov models of genomes to try to figure out what should be the next, you know, base that you may want to or where genes might be, etc. But the notion of generating in biology has been something we&#8217;ve tried for a while. And in the late teens, so kind of 2018, â€™17, â€™18, because we saw deep learning come along, and you could basically generate novelty with some of the deep learning models â€¦ and so we started asking, â€œCould you generate a protein basically by training a correspondence table, if you will, between protein structures and their underlying DNA sequence?â€ Not their protein sequence, but their DNA sequence.&nbsp;



LEE: Yeah.&nbsp;



AFEYAN: So that&#8217;s a big leap. So â€™17/â€™18, we started this thing. It was called 56. It was FL56, Flagship Labs 56, our 56th project.&nbsp;



By the way, we started this parallel one called â€œ57â€ that did it in a very different way. So one of them did pure black box model-building. The other one said, you know what, we don&#8217;t want to do the kind of &#8230; at that time, AlphaFold was in its very early embodiments. And we said, â€œIs there a way we could actually take little, you know, multi amino acid kind of almost grammars, if you will, a little piece, and then see if we could compose a protein that way?â€ So we were experimenting.&nbsp;



And what we found was that actually, if you show enough instances and you could train a transformer modelâ€”back in the day, that&#8217;s what we were usingâ€”you could actually, say, predict another sequence that should have the same activity as the first one.&nbsp;



LEE: Yeah.&nbsp;



AFEYAN: So we trained on green fluorescent proteins. Now, we&#8217;re talking about seven years ago. We trained on enzymes, and then we got to antibodies.&nbsp;



With antibodies, we started seeing that, boy, this could be a pretty big deal because it has big market impact. And we started bringing in some of the diffusion models that were beginning to come along at that time. And so we started getting much more excited. This was all done in a company that subsequently got renamed from FL56 to Generate:Biomedicines (opens in new tab), â€¦&nbsp;



LEE: Yep, yep.&nbsp;



AFEYAN: â€¦ which is one of the leaders in protein design using the generative techniques. It was interesting because Generate:Biomedicines is a company that was called that before generative AI was a thing, [LAUGHTER] which was kind of very ironic.&nbsp;



And, of course, that team, which operates today very, very kind of at the cutting edge, has published their models. They came up with this first Chroma (opens in new tab) model, which is a diffusion-based model, and then started incorporating a lot of the LLM capabilities and fusing them.&nbsp;



Now we&#8217;re doing atomistic models and many other things. The point being, that gave us a glimpse of how quickly the capability was gaining, â€¦&nbsp;



LEE: Yeah. Yeah.&nbsp;



AFEYAN: â€¦ just like evolution shows you. Sometimes evolution is super silent, and then all of a sudden, all hell breaks loose. And that&#8217;s what we saw.&nbsp;



LEE: Right. One of the things that I reflect on just in my own journey through this is there are other emotions that come up. One that was prominent for me early on was skepticism. Were there points when even in your own work, transformer-based work on this early on, that you had doubts or skepticism that these transformer architectures would be or diffusion-based approaches would be worth anything?&nbsp;



AFEYAN: You know, it&#8217;s interesting, I think that, I&#8217;m going to say this to you in a kind of a friendly way, but you&#8217;ll understand what I mean. In the world I live in, it&#8217;s kind of like the slums of innovation, [LAUGHTER] kind of like just doing things that are not supposed to work. The notion of skepticism is a luxury, right. I assume everything we do won&#8217;t work. And then once in a while I&#8217;m wrong.&nbsp;



And so I don&#8217;t actually try to evaluate whether before I bring something in, like just think about it. We, some hundred or so times a year, ask â€œwhat ifâ€ questions that lead us to totally weird places of thought. We then try to iterate, iterate, iterate to come up with something that&#8217;s testable. Then we go into a lab, and we test it.&nbsp;



So in that world, right, sitting there going, like, â€œHow do I know this transformer is going to work?â€ The answer is, â€œFor what?â€ Like, it&#8217;s going to work. To make something up &#8230; well, guess what? We knew early on with LLMs that hallucination was a feature, not a bug for what we wanted to do.&nbsp;



So it&#8217;s just such a different use that, of course, I have trained scientific skepticism, but it&#8217;s a little bit like looking at a competitive situation in an ecology and saying, â€œI bet that thing&#8217;s going to die.â€ Well, you&#8217;d be rightâ€”most of the time, you&#8217;d be right. [LAUGHTER]&nbsp;



So I just don&#8217;t â€¦ like, it â€¦ and that&#8217;s whyâ€”I guess, call me an early adopterâ€”for us, things that could move the needle even a little, but then upon repetition a lot, let alone this, â€¦&nbsp;



LEE: Yeah.&nbsp;



AFEYAN: â€¦ you have to embrace. You can&#8217;t wait there and say, I&#8217;ll embrace it once it&#8217;s ready. And so that&#8217;s what we did.&nbsp;



LEE: Hmm. All right. So let&#8217;s get into some specifics and what you are seeing either in your portfolio companies or in the research projects or out in the industry. What is going on today with respect to AI really being used for something meaningful in the design and development of drugs?&nbsp;



AFEYAN: In companies that are doing as diverse things asâ€”let me give you a few examplesâ€”a project that&#8217;s now become a named company called ProFound Therapeutics (opens in new tab) that literally discovered three, four years ago, and would not have been able to without some of the big data-model-building capabilities, that our cells make literally thousands, if not tens of thousands, of more proteins than we were aware of, full stop.&nbsp;



We had done the human genome sequence, there was 20,000 genes, we thought that there was â€¦&nbsp;



LEE: Wow.&nbsp;



AFEYAN: â€¦ maybe 70-80,000, 100,000 proteins, and that&#8217;s that. And it turns out that our cells have a penchant to express themselves in the form of proteins, and they have many other ways than we knew to do that.&nbsp;



Now, so what does that mean? That means that we have generated a massive amount of data, the interpretation of which, the use of which to guide what you do and what these things might be involved with is purely being done using the most cutting-edge data-trained models that allow you to navigate such complexity.&nbsp;



LEE: Wow. Hmm.&nbsp;



AFEYAN: That&#8217;s just one example. Another example: a company called Quotient Therapeutics (opens in new tab), again three, four years old. I can talk about the ones that are three, four years old because we&#8217;ve kind of gotten to a place where we&#8217;ve decided that it&#8217;s not going to fail yet, [LAUGHTER] so we can talk about it.&nbsp;



You know, we discoveredâ€”our team discoveredâ€”that in our cells, right, so we know that when we get cancer, our cells have genetic mutations in them or DNA mutations that are correlated and often causal to the hyperproliferative stages of cancer. But what we assume is that all the other cells in our body, pretty much, have one copy of their genes from our mom, one copy from our dad, and that&#8217;s that.&nbsp;



And when very precise deep sequencing came along, we always asked the question, â€œHow much variation is there cell to cell?â€&nbsp;



LEE: Right.&nbsp;



AFEYAN: And the answer was it&#8217;s kind of noise, random variation. Well, our team said, â€œWell, what if it&#8217;s not really that random?â€ because upon cell division cycles, there&#8217;s selection happening on these cells. And so not just in cancer but in liver cells, in muscle cells, in skin cells â€¦&nbsp;



LEE: Oh, interesting.&nbsp;



AFEYAN: â€¦ can you imagine that there&#8217;s an evolutionary experiment that is favoring either compensatory mutations that are helping you avoid disease or disease-caused mutations that are gaining advantage as a way to understand the mechanism? Sure enoughâ€”I wouldn&#8217;t be telling you otherwiseâ€”with massive amount of single cell sequencing from individual patient samples, we&#8217;ve now discovered that the human genome is mutated on average in our bodies 10,000 times, like over every base, like, it&#8217;s huge numbers.&nbsp;



And we&#8217;re finding very interesting big signals come out of this massive amount of data. By the way, data of the sort that the human mind, if it tries to assign causal explanations to what&#8217;s happening â€¦&nbsp;



LEE: Right.&nbsp;



AFEYAN: â€¦ is completely inadequate.&nbsp;



LEE: When you think about a language model, we&#8217;re learning from human language, and the totality of human languageâ€”at least relative to what we&#8217;re able to compute today in terms of constructing a modelâ€”the totality of human language is actually pretty limited. And in fact, you know, as is always written about in click-baity titles, you know, the big model builders are actually starting to run short.&nbsp;



AFEYAN: Running out, running out, yes. [LAUGHTER]&nbsp;



LEE: But one of the things that perplexes me and maybe even worries meâ€”like these two examplesâ€”are generally in the realm of cellular biology and the complexity. Let&#8217;s just take the example of your company, ProFound. You know, the complexity of what&#8217;s going on and the potential genetic diversity is such that, can we ever have enough data? You know, because there just aren&#8217;t that many human beings. There just aren&#8217;t that many samples.&nbsp;



AFEYAN: Well, it depends on what you want to train, right. So if you want to train a de novo evolutionary model that could take you from bacteria to human mammalian cells and the like, there may not beâ€”and I&#8217;m not an expert in thatâ€”but that&#8217;s a question that we often kind of think about.&nbsp;



But if you&#8217;re trying to train a &#8230; like you know what the proteins we know about, how they interact with pathways and disease mechanisms and the like. Now all of a sudden you find out that there&#8217;s a whole continent of them missing in your explanations. But there are things you can reason, in quotations, through analogy, functional analogy, sequence analogy, homology. So there&#8217;s a lot of things that we could do to essentially make use of this, even though you may not have the totality of data needed to, kind of, predict, based on a de novo sequence, exactly what it&#8217;s going to do.&nbsp;



So I agree with the comparison. But &#8230; but you&#8217;re right. The complexity is â€¦ just keep in mind, on average, a protein may be interacting with 50 to 100 other proteins.&nbsp;



LEE: Right.&nbsp;



AFEYAN: So if you find thousands of proteins, you&#8217;ve found a massive interaction space through which information is being processed in a living cell.&nbsp;



LEE: But do you find in your AI companies that access to data ends up being a key challenge? Or, you know, how central is that?&nbsp;



AFEYAN: Access to data is a key challenge for the companies we have that are trying to build just models. But that&#8217;s the minority of things we do. The majority of things we do is to actually co-develop the data and the models. And as you know well, because you guys, you know, have given us some ideas around this space, that, you know, you could generate data and then think about what you&#8217;re to do with it, which is the way biotech is operated with bioinformatics.&nbsp;



LEE: Right, right.&nbsp;



AFEYAN: Or you could generate bespoke data that is used to train the model that&#8217;s quite separate from what you would have done in the natural course of biology. So we&#8217;re doing much more of the latter of late, and I think that&#8217;ll continue. So, but these things are proliferating.&nbsp;



I mean, it&#8217;s hard to find a place where we&#8217;re not using this. And the â€œthisâ€ is any and all data-driven model building, generative, LLM-based, but also every other technique to make progress.&nbsp;



LEE: Sure. So now moving away from the straight biochemistry applications, what about AI in the process of building a business, of making investment decisions, of actually running an operation? What are you seeing there?&nbsp;



AFEYAN: So, well, you know, Moderna, which is a company that I&#8217;m quite proud of being a founder and chairman of, has adopted a significant, significant amount of AI embedded into their operations in all aspects: from the manufacturing, quality control, the clinical monitoring, the designâ€”every aspect. And in fact, they&#8217;ve had a partnership that they&#8217;ve had for a little while here with OpenAI, and they&#8217;ve tried many different ways to stay at the cutting edge of that.&nbsp;



So we see that play out at some scale. Thatâ€™s a 5,000-, 6,000-person organization, and what they&#8217;re doing is a good example of what early adopters would do, at least in our kind of biotechnology company.&nbsp;



But then, you know, in our space, I would say the efficiency impact is kind of no different, than, you know, anywhere else in academia you might adopt it or in other kinds of companies. But where I find it an interesting kind of maybe segue is the degree to which it may fundamentally change the way we think about how to do science, which is a whole other use, right?&nbsp;



LEE: Right.&nbsp;



AFEYAN: So it&#8217;s not an efficiency gain per se, although it&#8217;s maybe an effectiveness gain when it comes to science, but can you just fundamentally train models to generate hypotheses?&nbsp;



LEE: Yep.&nbsp;



AFEYAN: And we have done that, and we&#8217;ve been doing this for the last three years. And now it&#8217;s getting better and better, the better these reasoning engines are getting and kind of being able to extrapolate and train for novelty. Can you convert that to the world&#8217;s best experimental protocol to very precisely falsify your hypothesis, on and on?&nbsp;



That closing of that loop, kind of what we call autonomous science, which we&#8217;ve been trying to do for the last two, three years and are making some progress in, that to me is another kind of bespoke use of these things, not to generate molecules in its chemistry, but to change the behavior of how science is done.&nbsp;



LEE: Yeah. So I always end with a couple of provocative questions, but I needâ€”before we do that, while we&#8217;re on this subjectâ€”to get your take on Lila Sciences (opens in new tab).&nbsp;



And there is a vision there that I think is very interesting. It&#8217;d be great to hear it described by you.&nbsp;



AFEYAN: Sure. So Lila, after operating for two to three years in kind of a preparatory kind of stealth mode, we&#8217;ve now had a little bit more visibility around, and essentially what we&#8217;re trying to do there is to create what we call automated science factories, and such a factory would essentially be able to take problems, either computationally specified or human-specified, and essentially do the experimental work in order to either make an optimization happen or enable something that just didnâ€™t exist. And itâ€™s really, at this point, weâ€™ve shown proof of concept in narrow areas.&nbsp;



LEE: Yep.&nbsp;



AFEYAN: But itâ€™s hard to say that if you can do this, you canâ€™t do some other things, so weâ€™re just expanding it that way. We donâ€™t think we need a complete proof or complete demonstration of it for every aspect.&nbsp;



LEE: Right.&nbsp;



AFEYAN: So we&#8217;re just kind of being opportunistic. The idea for Lila is to partner with a number of companies. The good news is, within Flagship, there&#8217;s 48 of them. And so there&#8217;s a whole lot of them they can partner with to get their learning cycles. But eventually they want to be a real alternative to every time somebody has an idea, having to kind of go into a lab and manually do this.&nbsp;



I do want to say one thing we touched on, Peter, though, just on that front, which is &#8230;&nbsp;



LEE: Yep.&nbsp;



AFEYAN: &#8230; if you say, like, â€œWhat problem is this going to solve?â€ It&#8217;s several but an important one is just the flat-out human capacity to reason on this much data and this much complexity that is real. Because nature doesn&#8217;t try to abstract itself in a human understandable form.&nbsp;



LEE: Right. Yeah.&nbsp;



AFEYAN: In biology, since it&#8217;s kind of like progress happens through evolutionary kind of selections, the evidence of which [has] long been lost, and so therefore, you just see what you have, and then it has a behavior. I really do think that there&#8217;s something to be said, and I want toâ€”just for your audienceâ€”lay out a provocative, at least, thought on all this, which Lila is a beginning embodiment of, which is that I really think that what&#8217;s going to happen over the next five, 10 years, even while we&#8217;re all fascinated with the impending arrival of AGI [artificial general intelligence] is really what I call poly-intelligence, which is the combination of human intelligence, machine intelligence, AI, and nature&#8217;s intelligence.&nbsp;



We&#8217;re all fascinated at the human-machine interface. We know the human-nature interface, but imagine the machine-nature interfaceâ€”that is, actually letting loose a digital kind of information processing life form through the algorithms that are being developed and the commensurately complex, maybe much more complex. We&#8217;ll see. And so now the question becomes, what does the human do?&nbsp;



And we&#8217;re living in a world which is human dominated, which means the humans say, â€œIf I don&#8217;t understand it, it&#8217;s not real, basically. And if I don&#8217;t understand it, I can&#8217;t regulate it.â€ And we&#8217;re going to have to make peace with the fact that we&#8217;re not going to be able to predictably affect things without necessarily understanding them the way we could if we just forced ourselves to only work on problems we can understand. And that world we&#8217;re not ready for at all.&nbsp;



LEE: Yeah. All right. So this one I predict is going to be a little harder for you because I think while you think about the future, you live very much in the present. But I&#8217;d like you to make some predictions about what the biotech and biopharmaceutical industries are going to be able to do two years from now, five years from now, 10 years from now.&nbsp;



AFEYAN: Yeah, well, it&#8217;s hard for me because you know my nature, which is that I think this is all emergent.&nbsp;



LEE: Right.&nbsp;



AFEYAN: And so I would be the conceit of predicting. So I would say with likelihood positive predictive value of less than 10%, I&#8217;m happy to answer your question. So I&#8217;m not trying to score high [LAUGHTER] because I really think that my job is to envision it, not to predict it. And that&#8217;s a little bit different, right?&nbsp;



LEE: Yeah, I actually was trying to pick what would be the hardest possible question I could ask you, [LAUGHTER] and this is what I came up with.&nbsp;



AFEYAN: Yeah, no, no, I&#8217;m kidding here. So now look, I think that we will cross this threshold of understandability. And of course you&#8217;re seeing that in a lot of LLM things today. And of course, people are trying to train for things that are explainers and all that whole, there&#8217;s a whole world of that. But I think at some point we&#8217;re going to have to kind of let go and get comfortable working on things that, you know â€¦&nbsp;



I sometimes tell people, you know, and I&#8217;m not the first, but scientists and engineers are different, it&#8217;s said, in that engineers work on things that they don&#8217;t wait until they get a full understanding of before they work with them. Well, now scientists are going to have to get used to that, too, right?&nbsp;



LEE: Yeah. Yeah.&nbsp;



AFEYAN: Because insisting that it&#8217;s only valid if it&#8217;s understandable. So, I would say, look, I hope that the time â€¦ for example, I think major improvements will be made in patient selection. If we can test drugs on patients that are more synchronized as to the stage of their disease â€¦&nbsp;



LEE: Yep.&nbsp;



AFEYAN: &#8230; I think the answer will be much better. We&#8217;re working on that. It&#8217;s a company called Etiome (opens in new tab), very, very early stage. It&#8217;s really beautiful data, very early data that shows that when we talk about MASH [metabolic dysfunction-associated steatohepatitis], liver disease, when we talk about Parkinson&#8217;s, there&#8217;s such a heterogeneity, not only of the subset type of the disease, but the stage of the disease, that this notion that you have stage one cancer, stage two cancer, again, nobody told nature there&#8217;s stages of that kind. It&#8217;s a continuum.&nbsp;



But if you can synchronize based on training, kind of, the ability to detect who are the patients that are in enough of a close proximity that should be treated so that the trialâ€”much smaller a trial sizeâ€”could give you a drug, then afterwards, you can prescribe it using these approaches.&nbsp;



Kind of we&#8217;re going to find that what we thought is one disease is more like 15 diseases. That&#8217;s bad news because we&#8217;re not going to be able to claim that we can treat everything which we can. It&#8217;s good news in that there&#8217;s going to be people who are going to start making much more specific solutions to things.&nbsp;



LEE: Right.&nbsp;



AFEYAN: So I can imagine that. I can imagine a generation of, kind of, students who are going to be able to play in this space without having 25 years of graduate education on the subject. So what is deemed knowledge sufficient to do creative things will change. I can go on and on, but I think all this is very close by and it&#8217;s very exciting.&nbsp;



LEE: Noubar, I just always have so much fun, and I learn really a lot. It&#8217;s high-density learning when I talk to you. And so I hope our listeners feel the same way. It&#8217;s something I really appreciate.&nbsp;



AFEYAN: Well, Peter, thanks for this. And I think your listeners know that if I was asking you questions, you would be answering them with equal if not more fascinating stuff. So, thanks for giving me the chance to do that today.&nbsp;



[TRANSITION MUSIC]&nbsp;



LEE: Iâ€™m always fascinated by Noubarâ€™s perspectives on fundamental research and how it connects to human health and the building of successful companies. I see him as a classic â€œsystems thinker,â€ and by that, I mean he builds impressive things like Flagship Pioneering itself, which he created as a kind of biomedical innovation system.&nbsp;



In our conversation, I was really struck by the fact that heâ€™s been thinking about the potential impact of transformersâ€”transformers being the fundamental building block of large language modelsâ€”as far back as 2017, when the first paper on the attention mechanism in transformers was published by Google.&nbsp;



But, you know, it isnâ€™t only about using AI to do things like understand and design molecules and antibodies faster. It&#8217;s interesting that he is also pushing really hard towards a future where AI might â€œclose the loopâ€ from hypothesis generation, to experiment design, to analysis, and so on.&nbsp;



Now, hereâ€™s my conversation with Dr. Eric Topol:&nbsp;



LEE: Eric, it&#8217;s really great to have you here.&nbsp;



ERIC TOPOL: Oh, Peter, I&#8217;m thrilled to be here with you here at Microsoft.&nbsp;



LEE: You&#8217;re a super famous person. Extremely well known to researchers even in computer science, as we have here at Microsoft Research.&nbsp;



But the question I&#8217;d like to ask is, how would you explain to your parents what you do every day?&nbsp;



TOPOL: [LAUGHS] That&#8217;s a good question. If I was just telling them I&#8217;m trying to come up with better ways to keep people healthy, that probably would be the easiest way to do it because if I ever got in deeper, I would lose them real quickly. They&#8217;re not around, but just thinking about what they could understand.&nbsp;



LEE: Right.&nbsp;



TOPOL: I think as long as they knew it was work centered on innovative paths to promoting and preserving human health, that would get to them, I think.&nbsp;



LEE: OK, so now, kind of the second topic, and then we let the conversation flow, is about origin stories with respect to AI. And with most of our guests, you know, I factor that into two pieces: the encounters with AI before ChatGPT and what we call generative AI and then the first contacts after.&nbsp;



And, of course, you have extensive contact with both now. But let&#8217;s start with how you got interested in machine learning and AI prior to ChatGPT. How did that happen?&nbsp;



TOPOL: Yeah, it was out of necessity. So back, you know, when I started at Scripps at the end of â€™06, we started accumulating, you know, massive datasets. First, it was whole genomes. We did one of the early big cohorts of 1,400 people of healthy aging. We called the Wellderly whole genome sequence (opens in new tab).&nbsp;



And then we started big in the sensor world, and then we started saying, what are we going to do with all this data, with electronic health records and all those sensors? And now we got whole genomes.&nbsp;



And basically, what we were doing, we were in hoarding mode. We didn&#8217;t have a way to meaningfully analyze it.&nbsp;



LEE: Right.&nbsp;



TOPOL: You would read about how, you know, data is the new oil and, you know, gold and whatnot. But we just didn&#8217;t have a way to extract the juice. And even when we wanted to analyze genomes, it was incredibly laborious.&nbsp;



LEE: Yeah.&nbsp;



TOPOL: And we weren&#8217;t extracting a lot of the important information. So that&#8217;s why &#8230; not having any training in computer science, when I was doing the &#8230; about three years of work to do the book Deep Medicine, I started really, first auto-didactic about, you know, machine learning. And then I started contacting a lot of the real top people in the field and hanging out with them, and learning from them, getting their views as to, you know, where we are today, what models are coming in the future.&nbsp;



And then I said, â€œYou know what? We are going to be able to fix this mess.â€ [LAUGHS] We&#8217;re going to get out of the hoarding phase, and we&#8217;re going to get into, you know, really making a difference.&nbsp;



So that&#8217;s when I embraced the future of AI. And I knew, you know, backâ€”that was six years ago when it was published and probably eight or nine years ago when I was doing the research, and I knew that we weren&#8217;t there yet.&nbsp;



You know, at the time, we were seeing the image interpretation. That was kind of the early promise. But really, the models that were transformative, the transformer models, they were incubating back in 2017. So people knew something was brewing.&nbsp;



LEE: Right. Yes.&nbsp;



TOPOL: And everyone said we&#8217;re going to get there.&nbsp;



LEE: So then, ChatGPT comes out November of 2022; thereâ€™s GPT-4 in 2023, and now a lot has happened. Do you remember what your first encounter with that technology was?&nbsp;



TOPOL: Oh, sure. First, ChatGPT. You know, in the last days of November â€™22, I was just blown away. I mean, I&#8217;m having a conversation. I&#8217;m having fun. And this is humanoid responding to me. I said, â€œWhat?â€ You know? So that was to me, a moment I&#8217;ll never forget. And so I knew that the world was, you know, at a very kind of momentous changing point.&nbsp;



Of course, knowing, too, that this is going to be built on, and built on quickly. Of course, I didn&#8217;t know how soon GPT-4 and all the others were going to come forward, but that was a wake-up call that the capabilities of AI had just made a humongous jump, which seemingly was all of a sudden, although I did know this had been percolating â€¦&nbsp;



LEE: Right.&nbsp;



TOPOL: â€¦ you know, for what, at least five years, that, you know, it really was getting into its position to do this.&nbsp;



LEE: I know one of the things that was challenging psychologically and emotionally for me is, it made me rethink a lot of things that were going on in Microsoft Research in areas like causal reasoning, natural language processing, speech processing, and so on.&nbsp;



I&#8217;m imagining you must have had some emotional struggles too because you have this amazing book, Deep Medicine. Did you have to â€¦ did it go through your mind to rethink what you wrote in Deep Medicine in light of this or, or, you know, how did that feel?&nbsp;



TOPOL: It&#8217;s funny you ask that because in this one chapter I have on the virtual health coach, I wrote a whole bunch of scenarios &#8230;&nbsp;



LEE: Yeah.&nbsp;



TOPOL: â€¦ that were very kind of futuristic. You know, about how the AI interacts with the person&#8217;s health and schedules their appointment for this and their scan and tells them what lab tests they should tell their doctor to have, and, you know, all these things. And I sent a whole bunch of these, thinking that they were a little too far-fetched.&nbsp;



LEE: Yes.&nbsp;



TOPOL: And I sent them to my editor when I wrote the book, and he says, â€œOh, these are great. You should put them all in.â€ [LAUGHTER] What I didn&#8217;t realize is they weren&#8217;t that, you know, they were all going to happen.&nbsp;



&nbsp;LEE: Yeah. They weren&#8217;t that far-fetched at all.&nbsp;



TOPOL: Not at all. If there&#8217;s one thing I&#8217;ve learned from all this, is our imagination isn&#8217;t big enough.&nbsp;



&nbsp;LEE: Yeah.&nbsp;



TOPOL: We think too small.&nbsp;



LEE: Now in our book that Carey, Zak, and I wrote, you know, we made, you know, we sort of guessed that GPT-4 might help biomedical researchers, but I don&#8217;t think that any of us had the thought in mind that the architecture around generative AI would be so directly applicable to, you know, say, protein structures or, you know, to clinical health records and so on.&nbsp;



And so a lot of that seems much more obvious today. But two years ago, it wasn&#8217;t. But we did guess that biomedical researchers would find this interesting and be helped along.&nbsp;



So as you reflect over the past two years, you know, do you have things that you think are very important, kind of, meaningful applications of generative AI in the kinds of research that Scripps does?&nbsp;



TOPOL: Yeah. I mean, I think for one, you pointed out how the term generative AI is a misnomer.&nbsp;



LEE: Yeah.&nbsp;



TOPOL: And so it really was prescient about how, you know, it had a pluripotent capability in every respect, you know, of editing and creating. So that was something that I think was telling us, an indicator that this is, you know, a lot bigger than how it&#8217;s being labeled. And our expectations can actually be more than what we had seen previously with the earlier version.&nbsp;



So I think what&#8217;s happened is that now, we keep jumping. It&#8217;s so quick that we can&#8217;t â€¦ you know, first we think, oh, well, weâ€™ve gone into the agentic era, and then we could pass that with reasoning. [LAUGHTER] And, you know, we just can&#8217;t â€¦&nbsp;



LEE: Right.&nbsp;



TOPOL: It&#8217;s just wild.&nbsp;



LEE: Yeah.&nbsp;



TOPOL: So I think so many of us now will put in prompts that will necessitate or ideally result in a not-immediate gratification, but rather one that requires, you know, quite a bit of combing through the corpus of knowledge &#8230;&nbsp;



LEE: Yeah.&nbsp;



TOPOL: â€¦ and getting, with all the citations, a report or a response. And I think now this has been a reset because to do that on our own, it takes, you know, many, many hours. And it&#8217;s usually incomplete.&nbsp;



But one of the things that was so different in the beginning was you would get the references from up to a year and a half previously.&nbsp;



LEE: Yep.&nbsp;



TOPOL: And that&#8217;s not good enough. [LAUGHS]&nbsp;



LEE: Right.&nbsp;



TOPOL: And now you get references, like, from the day before.&nbsp;



LEE: Yes. Yeah.&nbsp;



TOPOL: And so, you say, â€œWhy would you do a regular search for anything when you could do something like this?â€&nbsp;



LEE: Yeah.&nbsp;



TOPOL: And then, you know, the reasoning power. And a lot of people who are not using this enough still are talking about, â€œWell, there&#8217;s no reasoning.â€&nbsp;



LEE: Yeah.



TOPOL: Which you dealt with really well in the book. But what, of course, you couldn&#8217;t have predicted is the new dimensions.&nbsp;



LEE: Right.&nbsp;



TOPOL: I think you nailed it with GPT-4. But it&#8217;s all these just, kind of, stepwise progressions that have been occurring because of the velocity that&#8217;s unprecedented. I just can&#8217;t believe it.&nbsp;



LEE: We were aware of the idea of multi-modality, but we didn&#8217;t appreciate, you know, what that would mean. Like AlphaFold (opens in new tab) [protein structure database], you know, the ability for AI to understandâ€”or crystal structuresâ€”to really start understanding something more fundamental about biochemistry or medicinal chemistry.&nbsp;



I have to admit, when we wrote the book, we really had no idea.&nbsp;



TOPOL: Well, I feel the same way. I still today can&#8217;t get over it because the reason AlphaFold and Demis [Hassabis] and John Jumper [AlphaFoldâ€™s co-creators] were so successful is there was this protein databank.&nbsp;



LEE: Yes.&nbsp;



TOPOL: And it had been kept for decades. And so, they had the substrate to work with.&nbsp;



LEE: Right.&nbsp;



TOPOL: So, you say, â€œOK, we can do proteins.â€ But then how do you do everything else?&nbsp;



LEE: Right.&nbsp;



TOPOL: And so this whole, what I call, â€œlarge language of life modelâ€ work, which has gone into high gear like I&#8217;ve never seen.&nbsp;



LEE: Yeah.&nbsp;



TOPOL: You know, now to this holy grail of a virtual cell, and &#8230;&nbsp;



LEE: Yeah.&nbsp;



TOPOL: You know, it&#8217;s basically &#8230; it&#8217;s &#8230; it was inspired by proteins. But now it&#8217;s hitting on, you know, ligands and small molecules, cells. I mean, nothing is being held back here.&nbsp;



LEE: Yeah.&nbsp;



TOPOL: So how could anybody have predicted that?&nbsp;



LEE: Right.&nbsp;



TOPOL: I sure wouldn&#8217;t have thought it would be possible at this point.&nbsp;



LEE: Yeah. So just to challenge you, where do you think that is going to be two years from now? Five years from now? Ten years from now? Like, so you talk about a virtual cell. Is that achievable within 10 years, or is that still too far out?&nbsp;



TOPOL: No, I think within 10 years for sure. You know the group that got assembled that Steve Quake (opens in new tab) pulled together?&nbsp;



LEE: Right.&nbsp;



TOPOL: I think has 42 authors in a paper (opens in new tab) in Cell. The fact that he could get these 42 experts in life science and some in computer science to come together and all agree â€¦&nbsp;



LEE: Yeah.&nbsp;



TOPOL: â€¦ that not only is this a worthy goal, but it&#8217;s actually going to be realized, that was impressive.&nbsp;



I challenged him about that. How did you get these people all to agree? So many of them were naysayers. And by the time the workshop finished, they were fully convinced. I think that what we&#8217;re seeing is so much progress happening so quickly. And then all the different models, you know, across DNA, RNA, and everything are just zooming forward.&nbsp;



LEE: Yeah.&nbsp;



TOPOL: And it&#8217;s just a matter of pulling this together. Now when we have that, and I think it could easily be well before a decade and possibly, you know, between the five- and 10-year markâ€”that&#8217;s just a guessâ€”but then we&#8217;re moving into another era of life science because right now, you know, this whole buzz about drug discovery.&nbsp;



LEE: Yep.&nbsp;



TOPOL: It&#8217;s not&#8230; with the ability to do all these perturbations at a cellular level.&nbsp;



LEE: Right.&nbsp;



TOPOL: Or the cell of interest.&nbsp;



LEE: Yeah.&nbsp;



TOPOL: Or the cell-to-cell interactions or the intra-cell interaction. So once you nail that, yeah, it takes it to a kind of another predictive level that we haven&#8217;t really fathomed. So, yes, there&#8217;s going to be drug discovery that&#8217;s accelerated. But this would make that and also the underpinnings of diseases.&nbsp;



LEE: Yeah.&nbsp;



TOPOL: So the idea that there&#8217;s so many diseases we don&#8217;t understand now. And if you had virtual cell, â€¦&nbsp;



LEE: Yeah.&nbsp;



TOPOL: â€¦ you would probably get to that answer â€¦&nbsp;



LEE: Yeah.&nbsp;



TOPOL: â€¦ much more quickly. So whether it&#8217;s underpinnings of diseases or what it&#8217;s going to take to really come up with far better treatmentsâ€”preventionsâ€”I think that&#8217;s where virtual cell will get us.&nbsp;



LEE: There&#8217;s a technical question &#8230; I wonder if you have an opinion. You may or may not. There is sort of what I would refer to as ab initio approaches to this. You know, you start from the fundamental physics and chemistry, and we know the laws, we have the math and, you know, we can try to derive from there â€¦ in fact, we can even run simulations of that math to generate training data to build generative models and work up to a cell, or forget all of that and just take as many observations and measurements of, say, living cells as possible, and just have faith that hidden amongst all of the observational data, there is structure and language that can be derived.&nbsp;



So that&#8217;s sort of bottom-up versus top-down approaches. Do you have an opinion about which way?&nbsp;



TOPOL: Oh, I think you go after both. And clearly whenever you&#8217;re positing that you&#8217;ve got a virtual cell model that&#8217;s working, you&#8217;ve got to do the traditional methods as well to validate it, and â€¦ so all that. You know, I think if you&#8217;re going to go out after this seriously, you have to pull out all the stops. Both approaches, I think, are going to be essential.&nbsp;



LEE: You know, if what you&#8217;re saying is true, and it is amazing to hear the confidence, the one thing I tried to explain to someone nontechnical is that for a lot of problems in medicine, we just don&#8217;t have enough data in a really profound way. And the most profound way to say that is, since Adam and Eve, there have only been an estimated 106 billion people who have ever lived.&nbsp;



So even if we had the DNA of every human being, every individual of Homo sapiens, there are certain problems for which we would not have enough data.&nbsp;



TOPOL: Sure.&nbsp;



LEE: And so I think another thing that seems profound to me, if we can actually have a virtual cell, is we can actually make trillions of virtual â€¦&nbsp;



TOPOL: Yeah&nbsp;



LEE: â€¦ human beings. The true genetic diversity could be realized for our species.&nbsp;



TOPOL: I think you nailed it. The ability to have that type of data, no less synthetic data, I mean, itâ€™s just extraordinary.&nbsp;



LEE: Yeah.&nbsp;



TOPOL: We will get there someday. I&#8217;m confident of that. We may be wrong in projections. And I do think [science writer] Philip Ball won&#8217;t be right that it will never happen, though. [LAUGHTER] No, I think that if there&#8217;s a holy grail of biology, this is it.&nbsp;



LEE: Yeah.&nbsp;



TOPOL: And I think you&#8217;re absolutely right about where that will get us.&nbsp;



LEE: Yeah.&nbsp;



TOPOL: Transcending the beginning of the species.&nbsp;



LEE: Yeah.&nbsp;



TOPOL: Of our species.&nbsp;



LEE: Yeah. All right. So now, we&#8217;re starting to run short on time here. And so I wanted to ask you about, I&#8217;m in my 60s, so I actually think about this a lot more. [LAUGHTER] And I know you&#8217;ve been thinking a lot about longevity. And, of course, your new book, Super Agers.&nbsp;



And one of the reasons I&#8217;m so eager to read is it&#8217;s a topic very top of mind for me and actually for a lot of people. Where is this going? Because this is another area where you hear so much hype. At the same time, you see Nobel laureate scientists &#8230;&nbsp;



TOPOL: Yeah.&nbsp;



LEE: &#8230; working on this.&nbsp;



TOPOL: Yeah.&nbsp;



LEE: So, so what&#8217;s, what&#8217;s real there?&nbsp;



TOPOL: Yeah. Well, it&#8217;s really â€¦ the real deal is the science of aging is zooming forward.&nbsp;



And that&#8217;s exciting. But I see it bifurcating. On the one hand, all these new ideas, strategies to reverse aging are very ambitious. Like cell reprogramming and senolytics and, you know, the rejuvenation of our thymus gland, and it&#8217;s a long list.&nbsp;



LEE: Yeah.&nbsp;



TOPOL: And theyâ€™re really cool science, and it used to be the mouse lived longer. Now it&#8217;s the old mouse looks really young.&nbsp;



LEE: Yeah. Yeah.&nbsp;



TOPOL: All the different features. A blind mouse with cataracts is all of a sudden there&#8217;s no cataracts. I mean, so these things are exciting, but none of them are proven in people, and they all have significant risk, no less, you know, the expense that might be attached.&nbsp;



LEE: Right.&nbsp;



TOPOL: And some people are jumping the gun. They&#8217;re taking rapamycin, which can really knock out their immune system. So they all carry a lot of risk. And people are just getting a little carried away. We&#8217;re not there yet.&nbsp;



But the other side, which is what I emphasize in the book, which is exciting, is that we have all these new metrics that came out of the science of aging.&nbsp;



LEE: Yes.&nbsp;



TOPOL: So we have clocks of the body. Our biological clock versus our chronological clock, and we have organ clocks. So I can say, you know, Peter, we&#8217;ve assessed all your organs and your immune system. And guess what? Every one of them is either at or less than your actual age.&nbsp;



LEE: Right.&nbsp;



TOPOL: And that&#8217;s very reassuring. And by the way, your methylation clock is also â€¦ I don&#8217;t need to worry about you so much. And then I have these other tests that I can do now, like, for example, the brain. We have an amazing protein p-Tau217 that we can say over 20 years in advance of you developing Alzheimer&#8217;s, â€¦&nbsp;



LEE: Yeah.&nbsp;



TOPOL: â€¦ we can look at that, and it&#8217;s modifiable by lifestyle, bringing it down. It should be you can change the natural history. So what we&#8217;ve seen is an explosion of knowledge of metrics, proteins, no less, you know, our understanding at the gene level, the gut microbiome, the immune system. So that&#8217;s what&#8217;s so exciting. How our immune system ages. Immunosenescence. How we have more inflammationâ€”inflammagingâ€”with aging. So basically, we have three diseases that kill us, that take away our health: heart, cancer, and neurodegenerative.&nbsp;



LEE: Yep.&nbsp;



TOPOL: And they all take more than 20 years. They all have a defective immune system inflammation problem, and they&#8217;re all going to be preventable.&nbsp;



LEE: Yeah.&nbsp;



TOPOL: That&#8217;s what&#8217;s so exciting.&nbsp;So we don&#8217;t have to have reverse aging. We can actually work on â€¦&nbsp;



LEE: Just prevent aging in the first place.&nbsp;



TOPOL: â€¦ the age-related diseases. So basically, what it means is: I got to find out if you have a risk, if you&#8217;re in this high-risk group for this particular condition, because if you areâ€”and we have many levels, layers, orthogonal ways to checkâ€”we don&#8217;t just bank it all on one polygenic test. We&#8217;re going to have several ways, say this is the one we are going &#8230;&nbsp;



And then we go into high surveillance, where, let&#8217;s say if it&#8217;s your brain, we do more p-Tau, if we need to do brain imagingâ€”whatever it takes. And also, we do preventive treatments on top of the lifestyle [changes], that one of the problems we have today is a lot of people know generally, what are good lifestyle factors. Although, I go through a lot more than people generally acknowledge.&nbsp;



But they don&#8217;t incorporate them because they don&#8217;t know that they&#8217;re at risk and they could change their &#8230; extend their health span and prevent that disease. So what I at least put out there, a blueprint, is how we can use AI, because it&#8217;s multimodal AI, with all these layers of data, and then temporally, it&#8217;s like today you could say if you have two protein tests, not only are you going to have Alzheimer&#8217;s, but within a two-year time frame when &#8230;&nbsp;



LEE: Yep.&nbsp;



TOPOL: &#8230; and if you don&#8217;t change things, if we don&#8217;t gear up â€¦ you know, we can &#8230; we can completely prevent this, so â€¦ or at least defer it for a decade or more. So that&#8217;s why I&#8217;m excited, is that we made these strides in the science of aging. But we haven&#8217;t acknowledged the part that doesn&#8217;t require reversing aging. There&#8217;s this much less flashy, attainable, less risky approach &#8230;&nbsp;



LEE: Yeah.&nbsp;



TOPOL: &#8230; than the one that â€¦ when you reverse aging, you&#8217;re playing with the hallmarks of cancer. They are like, if you look at the hallmarks of cancer â€¦&nbsp;



LEE: That has been one of the primary challenges.&nbsp;



TOPOL: They&#8217;re lined up.&nbsp;



LEE: Yeah.&nbsp;



TOPOL: Theyâ€™re all the same, you know, whether it&#8217;s telomeres, or whether it&#8217;s &#8230; you know &#8230; so this is the problem. I actually say in the book, I do think one of theseâ€”we have so many shots on goalâ€”one of these reverse aging things will likely happen someday. But we&#8217;re nowhere close.&nbsp;



On the other hand, let&#8217;s gear up. Let&#8217;s do what we can do. Because we have these new metrics that&#8217;s &#8230; people don&#8217;t â€¦ like, when I read the organ clock paper (opens in new tab) from Tony Wyss-Coray from Stanford. It was published end of â€™23; it was the cover of Nature. It blew me away.&nbsp;



LEE: Yeah.&nbsp;



TOPOL: And I wrote a Substack (opens in new tab) [article] on it. And Tony said, â€œWell, that&#8217;s so nice of you.â€ I said, â€œSo nice? This is revolutionary, you know.â€ [LAUGHTER] So â€¦&nbsp;



LEE: By the way, what&#8217;s so interesting is, how these things, this kind of understanding and AI, are coming together.



TOPOL: Yes.&nbsp;



LEE: It&#8217;s almost eerie the timing of these things.&nbsp;



TOPOL: Absolutely. Because you couldn&#8217;t take all these layers of data, just like we were talking about data hoarding.



LEE: Yep.



TOPOL: Now we have data hoarding on individual with no way to be able to make these assessments of what level of risk, when, what are we going to do in this individual to prevent that? We can do that now.&nbsp;



We can do it today. And we could keep building on that. So I&#8217;m really excited about it. I think that, you know, when I wrote the last book on deep medicine, it was our overarching goal should be to bring back the patient-doctor relationship. I&#8217;m an old dog, and I know what it used to be when I got out of medical school.&nbsp;



It&#8217;s totally &#8230; you couldn&#8217;t imagine how much erosion from the â€™70s, â€™80s to now. But now I have a new overarching goal. I&#8217;m thinking that that still is really importantâ€”humanity in medicineâ€”but let&#8217;s prevent these three &#8230; big three diseases because it&#8217;s an opportunity that we&#8217;re not â€¦ you know, in medicine, all my life we&#8217;ve been hearing and talking about we need to prevent diseases.&nbsp;



Curing is much harder than prevention. And the economics. Oh my gosh. But we haven&#8217;t done it.&nbsp;



LEE: Yeah.&nbsp;



TOPOL: Now we can do it. Primary prevention. Weâ€™d do really well. Somebodyâ€™s had heart attack.&nbsp;



LEE: Yeah.&nbsp;



TOPOL: Oh, we&#8217;re going to get all over it. Why did they have a heart attack in the first place?&nbsp;



LEE: Well, the thing that makes so much sense in what you&#8217;re saying is that we understand we have an understanding both economically and medically that prevention is a good thing. And extending the concept of prevention to these age-related conditions, I think, makes all the sense in the world.&nbsp;



You know, Eric, maybe on that optimistic note, itâ€™s time to wrap up this conversation. Really appreciate you coming. Let me just brag in closing that I&#8217;m now the proud owner of an autographed copy of your latest book, and, really, thank you for that.&nbsp;



TOPOL: Oh, thank you. I could spend the rest of the day talking to you. I&#8217;ve really enjoyed it. Thanks.&nbsp;



[TRANSITION MUSIC]&nbsp;



LEE: For me, the biggest takeaway from our conversation was Ericâ€™s supremely optimistic predictions about what AI will allow us to do in much less than 10 years.&nbsp;



You know, for me personally, I started off several years ago with the typical techie naivete that if we could solve protein folding using machine learning, we would solve human biology. But as Iâ€™ve gotten smarter, Iâ€™ve realized that things are way, way more complicated than that, and so hearing Ericâ€™s techno-optimism on this is really both heartening and so interesting.&nbsp;



Another thing that really caught my attention are Ericâ€™s views on AI in medical diagnosis. That really stood out to me because within our labs here at Microsoft Research, we have been doing a lot of work on this, for example in creating foundation models for whole-slide digital pathology.&nbsp;



The bottom line, though, is that biomedical research and development is really changing and changing quickly. It&#8217;s something that we thought about and wrote briefly about in our book, but just hearing it from these three people gives me reason to believe that this is going to create tremendous benefits in the diagnosis and treatment of disease.&nbsp;



And in fact, I wonder now how regulators, such as the Food and Drug Administration here in the United States, will be able to keep up with what might become a really big increase in the number of animal and human studies that need to be approved. On this point, it&#8217;s clear that the FDA and other regulators will need to use AI to help process the likely rise in the pace of discovery and experimentation. And so stay tuned for more information about that.&nbsp;



[THEME MUSIC]â€¯



I&#8217;d like to thank Daphne, Noubar, and Eric again for their time and insights. And to our listeners, thank you for joining us. There are several episodes left in the series, including discussions on medical studentsâ€™ experiences with AI and AIâ€™s influence on the operation of health systems and public health departments. We hope you&#8217;ll continue to tune in.&nbsp;



Until next time.&nbsp;



[MUSIC FADES]â€¯

				
			
			
				Show more			
		
	





AI Revolution in Medicine podcast series

Opens in a new tabThe post How AI will accelerate biomedical research and discovery appeared first on Microsoft Research.
â€¢ AI Testing and Evaluation: Learnings from pharmaceuticals and medical devices
  Generative AI presents a unique challenge and opportunity to reexamine governance practices for the responsible development, deployment, and use of AI. To advance thinking in this space, Microsoft has tapped into the experience and knowledge of experts across domainsâ€”from genome editing to cybersecurityâ€”to investigate the role of testing and evaluation as a governance tool. AI Testing and Evaluation: Learnings from Science and Industry, hosted by Microsoft Researchâ€™s Kathleen Sullivan, explores what the technology industry and policymakers can learn from these fields and how that might help shape the course of AI development.



In this episode, Daniel Carpenter, the Allie S. Freed Professor of Government and chair of the department of government at Harvard University, explains how the US Food and Drug Administrationâ€™s rigorous, multi-phase drug approval process serves as a gatekeeper that builds public trust and scientific credibility, while Timo Minssen, professor of law and founding director of the Center for Advanced Studies in Bioscience Innovation Law at the University of Copenhagen, explores the evolving regulatory landscape of medical devices with a focus on the challenges of balancing innovation with public safety. Later, Microsoftâ€™s Chad Atalla, an applied scientist in responsible AI, discusses the sociotechnical nature of AI models and systems, their teamâ€™s work building an evaluation framework inspired by social science, and where AI researchers, developers, and policymakers might find inspiration from the approach to governance and testing in pharmaceuticals and medical devices.







Learn more:



Learning from other Domains to Advance AI Evaluation and Testing: The History and Evolution of Testing in Pharmaceutical RegulationCase study | January 2025&nbsp;



Learning from other Domains to Advance AI Evaluation and Testing: Medical Device Testing: Regulatory Requirements, Evolution and Lessons for AI GovernanceCase study | January 2025&nbsp;



Learning from other domains to advance AI evaluation and testing&nbsp;Microsoft Research Blog | June 2025â€¯â€¯&nbsp;



Evaluating Generative AI Systems is a Social Science Measurement Challenge&nbsp;Publication&nbsp;|&nbsp;November 2024â€¯&nbsp;



STAC: Sociotechnical Alignment Centerâ€¯



Responsible AI: Ethical policies and practices | Microsoft AI



AI and Microsoft Researchâ€¯








	
		
			Subscribe to the Microsoft Research Podcast:		
		
							
					
						  
						Apple Podcasts
					
				
			
							
					
						
						Email
					
				
			
							
					
						
						Android
					
				
			
							
					
						
						Spotify
					
				
			
							
					
						
						RSS Feed
					
				
					
	




	
		
			
				
					

Transcript



[MUSIC]



KATHLEEN SULLIVAN: Welcome to AI Testing and Evaluation: Learnings from Science and Industry. I&#8217;m your host, Kathleen Sullivan.



As generative AI continues to advance, Microsoft has gathered a range of expertsâ€”from genome editing to cybersecurityâ€”to share how their fields approach evaluation and risk assessment. Our goal is to learn from their successes and their stumbles to move the science and practice of AI testing forward. In this series, we&#8217;ll explore how these insights might help guide the future of AI development, deployment, and responsible use.



[MUSIC ENDS]



SULLIVAN: Today, I&#8217;m excited to welcome Dan Carpenter and Timo Minssen to the podcast to explore testing and risk assessment in the areas of pharmaceuticals and medical devices, respectively.



Dan Carpenter is chair of the Department of Government at Harvard University. His research spans the sphere of social and political science, from petitioning in democratic society to regulation and government organizations. His recent work includes the FDA Project, which examines pharmaceutical regulation in the United States.



Timo is a professor of law at the University of Copenhagen, where he is also director of the Center for Advanced Studies in Bioscience Innovation Law. He specializes in legal aspects of biomedical innovation, including intellectual property law and regulatory law. He&#8217;s exercised his expertise as an advisor to such organizations as the World Health Organization and the European Commission.



And after our conversations, we&#8217;ll talk to Microsoft&#8217;s Chad Atalla, an applied scientist in responsible AI, about how we should think about these insights in the context of AI.



Daniel, it&#8217;s a pleasure to welcome you to the podcast. I&#8217;m just so appreciative of you being here. Thanks for joining us today.



				
				
					



DANIEL CARPENTER:&nbsp;Thanks for having me.&nbsp;



SULLIVAN:&nbsp;Dan, before we dissect policy,&nbsp;let&#8217;s&nbsp;rewind the tape to your&nbsp;origin&nbsp;story. Can you take us to the moment that you first became fascinated with regulators rather than, say, politicians? Was there a spark that pulled you toward the FDA story?&nbsp;



CARPENTER:&nbsp;At one point during graduate school, I was studying a combination of American politics and political theory, and I did a summer interning at the Department of Housing and Urban Development. And I began to think, why don&#8217;t people study these administrators more and the rules they make, the, you know,&nbsp;inefficiencies, the efficiencies?&nbsp;Really more&nbsp;from,&nbsp;kind of,&nbsp;a descriptive standpoint, less from a normative standpoint.&nbsp;And I was reading a lot that summer about the Food and Drug Administration and some of the decisions it was making on AIDS drugs. That was&nbsp;a,&nbsp;sort of,&nbsp;a major, &#8230;



SULLIVAN: Right.&nbsp;



CARPENTER: &#8230; sort of, you know,&nbsp;moment in the news, in the global news as well as the national news during, I would say, what?&nbsp;The late&nbsp;â€™80s, early&nbsp;â€™90s? And&nbsp;so&nbsp;I began to&nbsp;look&nbsp;into&nbsp;that.



SULLIVAN:&nbsp;So now that we know what pulled you in,&nbsp;letâ€™s&nbsp;zoom out for our listeners. Give us&nbsp;the&nbsp;whirlwind tour. I think most of us know pharma involves years of trials, but&nbsp;whatâ€™s&nbsp;the part we&nbsp;donâ€™t&nbsp;know?



CARPENTER:&nbsp;So&nbsp;I think when most businesses develop a product, they all go through some phases of research and development and testing. And I think&nbsp;what&#8217;s&nbsp;different about the FDA is,&nbsp;sort of,&nbsp;two-&nbsp;or three-fold.



First, a lot of those tests are much more stringently specified and regulated by the government, and second, one of the reasons for that is that the FDA imposes not simply safety requirements upon drugs&nbsp;in particular but&nbsp;also efficacy requirements. The FDA wants you to prove not simply that&nbsp;it&#8217;s&nbsp;safe and non-toxic&nbsp;but also that&nbsp;it&#8217;s&nbsp;effective.&nbsp;And the final thing,&nbsp;I think, that&nbsp;makes the FDA different is that it stands as what I would call the&nbsp;â€œveto playerâ€&nbsp;over R&amp;D [research and development] to the marketplace.&nbsp;The FDA&nbsp;basically has,&nbsp;sort of,&nbsp;this control over entry&nbsp;to&nbsp;the marketplace.



And&nbsp;so&nbsp;what that involves is usually first, a set of human trials where people who have no disease take it. And&nbsp;you&#8217;re&nbsp;only looking&nbsp;for&nbsp;toxicity generally. Then&nbsp;there&#8217;s&nbsp;a set of Phase 2 trials, where they look more at safety and a little bit at efficacy, and&nbsp;you&#8217;re&nbsp;now examining people who have the disease that the drug claims to treat. And&nbsp;you&#8217;re&nbsp;also basically comparing people who get the drug,&nbsp;often&nbsp;with those who do not.



And then finally, Phase 3 involves a much more direct and large-scale attack, if you will, or assessment of efficacy, and&nbsp;that&#8217;s&nbsp;where you get the sort of large randomized clinical trials that are&nbsp;very expensive&nbsp;for pharmaceutical companies, biomedical companies to launch, to execute, to analyze. And those are often the sort of core evidence base for the decisions that the FDA makes about&nbsp;whether or not&nbsp;to approve a new drug for marketing in the United States.



SULLIVAN:&nbsp;Are there&nbsp;differences in how that process has, you know, changed through other countries and&nbsp;maybe just&nbsp;how&nbsp;that&#8217;s&nbsp;evolved as&nbsp;you&#8217;ve&nbsp;seen it play out?&nbsp;



CARPENTER:&nbsp;Yeah, for a long time, I would say that the United States had&nbsp;probably the&nbsp;most&nbsp;stringent regime&nbsp;of regulation for biopharmaceutical products until,&nbsp;I would say,&nbsp;about the 1990s and early 2000s. It used to be the case that a number of other countries, especially in Europe but around the world, basically waited for the FDA to mandate tests on a drug and only after the drug was approved in the United States would they deem it approvable and marketable in their own countries. And then after the formation of the European Union and the creation of the European Medicines Agency, gradually the European Medicines Agency began to get a bit more stringent.&nbsp;&nbsp;



But, you know,&nbsp;over the long run,&nbsp;there&#8217;s&nbsp;been a&nbsp;lot of,&nbsp;sort&nbsp;of,&nbsp;heterogeneity, a lot of variation over time and space, in the way that the FDA has approached these problems. And&nbsp;I&#8217;d&nbsp;say in the last 20 years, it&#8217;s begun to partially deregulate, namely,&nbsp;you know,&nbsp;trying to find all sorts of mechanisms or pathways for really innovative&nbsp;drugs for deadly diseases without a lot of treatments to&nbsp;basically get&nbsp;through the process at lower cost.&nbsp;For many people,&nbsp;that has not been sufficient.&nbsp;They&#8217;re&nbsp;concerned about the cost of the system.&nbsp;Of course, then the agency also gets criticized by those&nbsp;who believe&nbsp;it&#8217;s&nbsp;too lax. It is&nbsp;potentially letting&nbsp;ineffective and unsafe therapies on the market.



SULLIVAN:&nbsp;In your view, when does the structured model genuinely safeguard patients and where do you think it&nbsp;maybe slows&nbsp;or&nbsp;limits&nbsp;innovation?



CARPENTER:&nbsp;So&nbsp;I think&nbsp;the worry&nbsp;is that if you approach pharmaceutical approval as a world where only things can go wrong,&nbsp;then&nbsp;you&#8217;re&nbsp;really at a risk of limiting innovation. And even if you end up letting a lot of things through, if by your regulations you end up basically slowing down the development process or making it very, very costly, then there&#8217;s just a whole bunch of drugs that either come to market too slowly or they come to market not at all because&nbsp;they just aren&#8217;t worth the kind of cost-benefit or, sort of, profit analysis of the firm.&nbsp;You know, so&nbsp;that&#8217;s&nbsp;been a concern.&nbsp;And I think&nbsp;it&#8217;s&nbsp;been one of the reasons that the Food and Drug Administration as well as other world regulators have begun to&nbsp;basically try&nbsp;to smooth the process and accelerate the process at the margins.



The other thing is that&nbsp;they&#8217;ve&nbsp;started to&nbsp;basically make&nbsp;approvals&nbsp;on the basis of&nbsp;what are called&nbsp;surrogate endpoints. So the idea is that a cancer drug, we really want to know whether that drug saves lives, but if we wait to see whose lives are saved or prolonged by that drug, we might miss the opportunity to make judgments on the basis of, well, are we detecting tumors in the bloodstream? Or can we measure the size of those tumors&nbsp;in, say, a&nbsp;solid cancer? And then the further question is, is the size of the tumor&nbsp;basically a&nbsp;really good&nbsp;correlate&nbsp;or predictor of whether people will die or&nbsp;not, right?&nbsp;Generally, the&nbsp;FDA tends to be less stringent when&nbsp;you&#8217;ve&nbsp;got, you know, a remarkably innovative new&nbsp;therapy&nbsp;and the disease being treated is one that just&nbsp;doesn&#8217;t&nbsp;have a lot of available treatments,&nbsp;right.



The one thing that people often think about when&nbsp;they&#8217;re&nbsp;thinking about pharmaceutical regulation is they often contrast,&nbsp;kind of,&nbsp;speed versus safety &#8230;



SULLIVAN:&nbsp;Right.&nbsp;&nbsp;



CARPENTER:&nbsp;&#8230; right. And&nbsp;that&#8217;s&nbsp;useful as a tradeoff,&nbsp;but I often try to remind people that&nbsp;it&#8217;s&nbsp;not simply&nbsp;about whether the drug gets out&nbsp;there&nbsp;and&nbsp;it&#8217;s&nbsp;unsafe. You know, you and I as patients and even doctors have&nbsp;a hard time&nbsp;knowing whether something works and whether it should be prescribed. And the evidence for knowing whether something works&nbsp;isn&#8217;t&nbsp;just, well,&nbsp;you&nbsp;know, Sally took&nbsp;it&nbsp;or Dan took it or Kathleen took it, and they&nbsp;seem to get&nbsp;better or they&nbsp;didn&#8217;t&nbsp;seem to get better.&nbsp;&nbsp;



The really rigorous evidence comes from randomized clinical trials.&nbsp;And I think&nbsp;it&#8217;s&nbsp;fair to say that if you didn&#8217;t&nbsp;have the FDA there as a veto player, you&nbsp;wouldn&#8217;t&nbsp;get as many randomized clinical&nbsp;trials&nbsp;and the evidence&nbsp;probably&nbsp;wouldn&#8217;t&nbsp;be as rigorous for whether these things work. And as I like to put it,&nbsp;basically there&#8217;s&nbsp;a whole ecology of expectations and beliefs around the biopharmaceutical industry in the United States and globally,&nbsp;and to some extent,&nbsp;it&#8217;s&nbsp;undergirded by&nbsp;all of&nbsp;these tests that happen.&nbsp;&nbsp;



SULLIVAN:&nbsp;Right.&nbsp;&nbsp;



CARPENTER:&nbsp;And in part, that means&nbsp;it&#8217;s&nbsp;undergirded by regulation. Would there still be a market without regulation? Yes. But it would be a market in which people had far less information in and confidence about the drugs that are being taken. And&nbsp;so&nbsp;I think&nbsp;it&#8217;s&nbsp;important to recognize that kind of confidence-boosting potential of, kind of, a scientific regulation base.&nbsp;



SULLIVAN:&nbsp;Actually, if we could&nbsp;double-click&nbsp;on that for a minute, I&#8217;d love to hear your perspective on, testing&nbsp;has been completed;&nbsp;there&#8217;s results.&nbsp;Can you walk us through how those results actually shape the next steps and decisions of a particular drug and just,&nbsp;like,&nbsp;how regulators actually think about using that data to influence really what happens next with it?



CARPENTER:&nbsp;Right.&nbsp;So&nbsp;it&#8217;s&nbsp;important to understand that every drug is approved for&nbsp;what&#8217;s called&nbsp;an indication. It can have a first primary&nbsp;indication, which is the main disease that it treats, and then others can be added as more evidence is shown. But a drug is not something that just kind of exists out there in the ether.&nbsp;It has to have the right form of administration.&nbsp;Maybe it&nbsp;should be injected.&nbsp;Maybe it&nbsp;should be ingested.&nbsp;Maybe it&nbsp;should&nbsp;be administered only at a clinic&nbsp;because it needs to be&nbsp;kind of administered&nbsp;in just the right way. As doctors will tell you, dosage is everything, right.&nbsp;&nbsp;



And&nbsp;so&nbsp;one of the reasons that you want those trials is not simply a, you know, yes or no answer about whether the drug works,&nbsp;right.&nbsp;It&#8217;s&nbsp;not simply if-then.&nbsp;It&#8217;s&nbsp;literally what&nbsp;goes into what you might call the dose response curve.&nbsp;You know, how much of this drug do we need to&nbsp;basically, you know,&nbsp;get the benefit? At what point does that fall off significantly that we can&nbsp;basically say, we can stop there? All that evidence comes from&nbsp;trials. And&nbsp;that&#8217;s&nbsp;the kind of evidence that is&nbsp;required&nbsp;on the basis of&nbsp;regulation.&nbsp;&nbsp;



Because&nbsp;it&#8217;s&nbsp;not simply a drug&nbsp;that&#8217;s&nbsp;approved.&nbsp;It&#8217;s&nbsp;a drug and a&nbsp;frequency&nbsp;of administration. It&#8217;s&nbsp;a&nbsp;method of administration.&nbsp;And&nbsp;so&nbsp;the drug&nbsp;isn&#8217;t&nbsp;just,&nbsp;there&#8217;s&nbsp;something to be taken off the shelf and popped into your mouth. I mean, sometimes&nbsp;that&#8217;s&nbsp;what happens, but even then,&nbsp;we want to know what the dosage is,&nbsp;right.&nbsp;We want to know what to look for in terms of side effects, things like that.



SULLIVAN:&nbsp;Going back to that point, I&nbsp;mean,&nbsp;it sounds like&nbsp;we&#8217;re&nbsp;making a lot of progress from a regulation perspective&nbsp;in, you know, sort of speed and getting things approved but doing it in a&nbsp;really balanced&nbsp;way. I mean, any other kind of closing thoughts on the tradeoffs there or where&nbsp;you&#8217;re&nbsp;seeing that going?



CARPENTER:&nbsp;I think&nbsp;you&#8217;re&nbsp;going to see some move in the coming yearsâ€”there&#8217;s&nbsp;already been some of itâ€”to say, do we always need a&nbsp;really large&nbsp;Phase 3 clinical trial? And to what degree do we need the, like, you&nbsp;know,&nbsp;all the i&#8217;s dotted and the t&#8217;s crossed or a really,&nbsp;really large&nbsp;sample size?&nbsp;And&nbsp;I&#8217;m&nbsp;open to innovation there.&nbsp;I&#8217;m&nbsp;also open to the idea that we consider, again, things like accelerated approvals or pathways for looking at&nbsp;different kinds&nbsp;of surrogate endpoints.&nbsp;I do think, once we do that, then we also have to have some degree of follow-up.



SULLIVAN:&nbsp;So&nbsp;I know&nbsp;we&#8217;re&nbsp;getting&nbsp;close to&nbsp;out of time, but&nbsp;maybe just&nbsp;a quick rapid fire if&nbsp;youâ€™re&nbsp;open to it. Biggest myth about clinical trials?



CARPENTER:&nbsp;Well, some people tend to think that the FDA performs them.&nbsp;You know,&nbsp;it&#8217;s&nbsp;companies that do it. And the only other thing I would say is the company that does a lot of the testing and even the innovating is not always the company that takes the drug to market, and it tells you something about how powerful regulation is in our system, in our world,&nbsp;that you often need a company that has dealt with the FDA quite a bit and knows all the regulations and knows how to dot the i&#8217;s and cross the t&#8217;s in order to get a drug across the finish line.



SULLIVAN:&nbsp;If you had a magic wand,&nbsp;what&#8217;s&nbsp;the one thing&nbsp;you&#8217;d&nbsp;change in regulation today?



CARPENTER:&nbsp;I would like people to think a little bit less about just speed versus safety and,&nbsp;again, more about this basic issue of confidence. I think&nbsp;it&#8217;s&nbsp;fundamental to everything that happens in markets but especially in biopharmaceuticals.



SULLIVAN:&nbsp;Such a great point.&nbsp;This has been really fun.&nbsp;Just thanks so much for being here today. We&#8217;re really excited to share your thoughts&nbsp;out to&nbsp;our listeners. Thanks.



[TRANSITION MUSIC]&nbsp;



CARPENTER:&nbsp;Likewise.&nbsp;



SULLIVAN:&nbsp;Now&nbsp;to&nbsp;the world of medical devices,&nbsp;I&#8217;m&nbsp;joined by Professor Timo&nbsp;Minssen. Professor Minssen, it&#8217;s&nbsp;great to have you here. Thank you for joining us today.&nbsp;



TIMO&nbsp;MINSSEN:&nbsp;Yeah, thank you very much,&nbsp;it&#8217;s&nbsp;a pleasure.



SULLIVAN:&nbsp;Before getting into the regulatory world of medical devices, tell our audience a bit about your personal journey or your origin story, as&nbsp;we&#8217;re&nbsp;asking our guests. How did you land in regulation, and what&#8217;s kept you hooked in this space?



MINSSEN:&nbsp;So&nbsp;I started out as a patent expert in the biomedical area, starting with my PhD thesis on patenting biologics in Europe and in the US.&nbsp;So&nbsp;during that time, I was mostly interested in patent and trade secret questions.&nbsp;But at the same time, I also developed and taught courses in regulatory law and held talks on regulating advanced medical therapy medicinal products.&nbsp;I&nbsp;then&nbsp;started to lead large research projects on legal challenges in a wide variety of health and life science innovation frontiers. I also started to focus increasingly on AI-enabled medical devices and software as a medical device, resulting in several academic articles in this area&nbsp;and also&nbsp;in the regulatory area and a book on the future of medical device regulation.&nbsp;&nbsp;



SULLIVAN:&nbsp;Yeah,&nbsp;what&#8217;s&nbsp;kept you hooked in&nbsp;the space?



MINSSEN:&nbsp;It&#8217;s&nbsp;just incredibly exciting,&nbsp;in particular right&nbsp;now with everything that is going on, you know, in the software arena, in the marriage between AI and medical devices. And this is really challenging not only societies but also regulators and authorities in Europe and in the US.



SULLIVAN:&nbsp;Yeah,&nbsp;it&#8217;s&nbsp;a super exciting time to be in this space. You know, we talked to Daniel a little earlier and, you know, I think&nbsp;similar to&nbsp;pharmaceuticals, people have a general sense of what we mean when we say medical devices, but most listeners may&nbsp;picture&nbsp;like a stethoscope or a hip implant. The word &#8220;medical device&#8221;&nbsp;reaches&nbsp;much wider. Can you give us a quick, kind of, range from perhaps&nbsp;very simple&nbsp;to even, I don&#8217;t know, sci-fi and then your 90-second tour of how risk assessment works and why a framework is essential?



MINSSEN:&nbsp;Let me start out by saying that&nbsp;the WHO [World Health Organization] estimates that today there are approximately 2 million different kinds of medical devices on the world market, and as of the FDA&#8217;s latest update that I&#8217;m aware of, the FDA has authorized more than 1,000 AI-, machine learning-enabled medical devices, and that number is rising rapidly.



So in that context, I think it is important to understand that medical devices can be any instrument, apparatus, implement, machine, appliance, implant, reagent for in vitro use, software, material, or other similar or related articles that are&nbsp;intended&nbsp;by the manufacturer to be used alone or in combination for a medical purpose. And the spectrum of what constitutes a medical device can&nbsp;thus&nbsp;range from very simple devices such as tongue depressors, contact lenses, and thermometers to more complex devices such as blood pressure monitors, insulin pumps, MRI machines, implantable pacemakers, and even software as a medical device or AI-enabled monitors or drug device combinations, as well.



So&nbsp;talking about regulation,&nbsp;I think&nbsp;it&nbsp;is also&nbsp;very important&nbsp;to stress that medical devices are used in many diverse situations by&nbsp;very different&nbsp;stakeholders. And testing&nbsp;has to&nbsp;take this variety into consideration, and it is intrinsically tied to regulatory requirements across various&nbsp;jurisdictions.



During the pre-market phase, medical testing&nbsp;establishes&nbsp;baseline safety and effectiveness metrics through bench testing, performance standards, and clinical studies. And post-market testing ensures that real-world data informs ongoing compliance and safety improvements. So testing is indispensable in translating technological innovation into safe and effective medical devices. And while&nbsp;particular details&nbsp;of pre-market and post-market review procedures may slightly differ among countries, most developed&nbsp;jurisdictions regulate medical devices similarly to the US or European models.â€¯



So&nbsp;most&nbsp;jurisdictions&nbsp;with medical device regulation classify devices based on their risk profile, intended use, indications for use, technological characteristics,&nbsp;and the regulatory controls necessary to provide a reasonable assurance of safety and effectiveness.



SULLIVAN:&nbsp;So medical devices face a pretty prescriptive multi-level testing path before they hit the market. From your vantage point, what are some of the downsides of that system and when does it make the most sense?



MINSSEN:&nbsp;One primary drawback is, of course, the lengthy and expensive approval process. High-risk devices, for example, often undergo years of clinical trials,&nbsp;which can cost millions of dollars, and this can create a significant barrier for startups and small companies with limited resources.&nbsp;And even for moderate-risk devices, the regulatory burden can slow product development and time to the market.



And the approach can also limit flexibility. Prescriptive requirements may not accommodate emerging innovations like digital therapeutics or AI-based diagnostics in&nbsp;a feasible&nbsp;way. And in such cases, the framework can unintentionally [stiffen]&nbsp;innovation by discouraging creative solutions or iterative improvements, which as matter of fact can also&nbsp;put&nbsp;patients&nbsp;at risk when you&nbsp;don&#8217;t&nbsp;use&nbsp;new technologies and AI.&nbsp;And&nbsp;additionally, the same level of scrutiny may be applied to low-risk devices, where&nbsp;the extensive testing and documentation may also be disproportionate to the actual patient risk.



However, the prescriptive model is highly&nbsp;appropriate where&nbsp;we have high testing standards for high-risk medical devices, in my view, particularly those that are life-sustaining, implanted, or involve new materials or mechanisms.



I also wanted to say that I think that these higher compliance thresholds can be OK and necessary if you have a system where authorities and stakeholders also have the capacity and funding to enforce, monitor, and achieve compliance with such rules in a feasible, time-effective, and straightforward manner. And this, of course, requires resources, novel solutions,&nbsp;and investments.



SULLIVAN:&nbsp;A range of tests are undertaken across the life cycle of medical devices.&nbsp;How do these testing requirements vary across&nbsp;different stages&nbsp;of development and across various applications?



MINSSEN:&nbsp;Yes,&nbsp;that&#8217;s&nbsp;a good question.&nbsp;So&nbsp;I think first it&nbsp;is important to realize that testing is conducted by various entities, including manufacturers, independent third-party laboratories, and regulatory agencies. And it occurs throughout the device&nbsp;life&nbsp;cycle, beginning with iterative testing during the research and development stage, advancing to pre-market evaluations, and continuing into post-market monitoring. And the outcomes of&nbsp;these tests directly&nbsp;impact&nbsp;regulatory approvals, market access, and device design refinements, as well.&nbsp;So&nbsp;the testing results are typically shared with regulatory authorities and in some cases with healthcare providers and the broader public to enhance transparency and trust.



So&nbsp;if you talk about the&nbsp;different phases&nbsp;that play a role here â€¦ so&nbsp;let&#8217;s&nbsp;turn to the pre-market phase, where manufacturers must&nbsp;demonstrate&nbsp;that the device is conformed to safety and performance benchmarks defined by regulatory authorities. Pre-market evaluations include functional bench testing, biocompatibility, for example, assessments and software validation, all of which are integral components of a manufacturer&#8217;s submission.&nbsp;



But, yes, but, testing also, and we touched already up on that, extends into the post-market phase, where it continues to ensure device safety and efficacy, and post-market surveillance relies on testing to&nbsp;monitor real-world performance and&nbsp;identify&nbsp;emerging risks on the post-market phase. By integrating real-world evidence into ongoing assessments, manufacturers can address unforeseen issues, update devices as needed, and&nbsp;maintain compliance with evolving regulatory expectations. And&nbsp;I think this&nbsp;is particularly important in this new generation of medical devices that are AI-enabled or machine-learning enabled.



I think we have to understand that in this AI-enabled medical devices field, you know, the devices and the algorithms that are working with&nbsp;them, they&nbsp;can improve in the lifetime of a product.&nbsp;So actually, not&nbsp;only you could assess them and make sure that they&nbsp;maintain&nbsp;safe,&nbsp;you&nbsp;could also sometimes lower the risk category by finding evidence that these devices are&nbsp;actually becoming&nbsp;more precise and safer.&nbsp;So&nbsp;it can both, you know, heighten the risk&nbsp;category&nbsp;or lower the risk category, and&nbsp;that&#8217;s&nbsp;why&nbsp;this continuous testing is so important.



SULLIVAN:&nbsp;Given what you just said, how should regulators handle a device whose algorithm keeps updating itself after approval?



MINSSEN:&nbsp;Well, it&nbsp;has to&nbsp;be an iterative process that is&nbsp;feasible&nbsp;and straightforward and that is based on a very efficient, both time efficient and performance efficient, communication between the regulatory authorities and the medical device developers, right. We need to have&nbsp;the sensors&nbsp;in place that spot potential changes, and we need to have&nbsp;the mechanisms&nbsp;in place that allow us to quickly react to these changes both regulatory wise&nbsp;and also&nbsp;in&nbsp;the&nbsp;technological way.â€¯



So&nbsp;I think communication&nbsp;is important,&nbsp;and we need to have&nbsp;the pathways&nbsp;and&nbsp;the feedback&nbsp;loops in the regulation that quickly allow us to&nbsp;monitor&nbsp;these self-learning algorithms and devices.



SULLIVAN:&nbsp;It sounds like&nbsp;it&#8217;s&nbsp;just â€¦&nbsp;there&#8217;s&nbsp;such a delicate balance between advancing technology and really ensuring public safety. You know, if we clamp down too hard, we stifle that innovation. You already touched upon this a bit. But if&nbsp;we&#8217;re&nbsp;too lax, we risk unintended consequences. And&nbsp;I&#8217;d&nbsp;just love to hear how you think the field is balancing that and any learnings you can share.



MINSSEN:&nbsp;So&nbsp;this is&nbsp;very true, and&nbsp;you just touched upon a very central question also in our research and our writing. And this is also the&nbsp;reason why&nbsp;medical device regulation is so fascinating and continues to evolve in response to rapid advancements in technologies, particularly dual technologies&nbsp;regarding&nbsp;digital health, artificial intelligence, for example, and personalized medicine.



And finding the balance is tricky because also [a] related major future challenge relates to the increasing regulatory jungle and the complex interplay between evolving regulatory landscapes that regulate AI more generally.



We really need to make sure that the regulatory authorities that deal with this, that need to find the right balance to promote innovation and mitigate and prevent risks, need to have the&nbsp;capacity&nbsp;to do this.&nbsp;So&nbsp;this requires investments, and it also requires new ways to regulate this technology more flexibly, for example through regulatory sandboxes and so on.



SULLIVAN:&nbsp;Could you just expand upon that a bit and double-click on what it is&nbsp;you&#8217;re&nbsp;seeing there? What excites you about&nbsp;what&#8217;s&nbsp;happening in that space?



MINSSEN:&nbsp;Yes, well, the research of my group at the Center for Advanced Studies in Bioscience Innovation Law is&nbsp;very broad. I mean, we are looking into gene editing technologies. We are looking into new biologics. We are looking into medical&nbsp;devices,&nbsp;as well, obviously, but also other technologies&nbsp;in advanced medical computing.



And what we see across the line here is that there is an increasing demand for having more adaptive and flexible regulatory frameworks in these&nbsp;new technologies,&nbsp;in particular when&nbsp;they have new uses, regulations that are focusing more on the product rather than the process. And I have recently&nbsp;written&nbsp;a report, for example,&nbsp;for&nbsp;emerging biotechnologies and&nbsp;bio-solutions&nbsp;for the EU commission. And even in that area, regulatory sandboxes are increasingly important, increasingly considered.



So&nbsp;this idea of regulatory sandboxes has been developing originally in the financial sector, and it is now penetrating into&nbsp;other sectors, including synthetic biology, emerging biotechnologies, gene editing, AI, quantum technology, as&nbsp;well. This is&nbsp;basically creating&nbsp;an environment where actors can test&nbsp;new ideas&nbsp;in close collaboration and under the oversight of regulatory authorities.



But&nbsp;to implement&nbsp;this in the AI sector now also leads us to&nbsp;a&nbsp;lot of questions and challenges. For example, you need to have the&nbsp;capacities&nbsp;of authorities that are governing and&nbsp;monitoring&nbsp;and deciding&nbsp;on these regulatory sandboxes. There are issues relating to competition law, for example, which&nbsp;you&nbsp;call antitrust law in the US, because the question is, who can enter the sandbox and how may they compete after they exit the sandbox? And there are many questions relating to, how&nbsp;should we&nbsp;work with these sandboxes and how&nbsp;should we&nbsp;implement these sandboxes?



[TRANSITION MUSIC]&nbsp;



SULLIVAN:&nbsp;Well, Timo, it has just been such a pleasure to speak with you today.



MINSSEN:&nbsp;Yes, thank you very much.&nbsp;



And now&nbsp;I&#8217;m&nbsp;happy to introduce Chad Atalla.



Chad&nbsp;is&nbsp;senior applied scientist&nbsp;in&nbsp;Microsoft Research&nbsp;New York City&#8217;s&nbsp;Sociotechnical Alignment Center, where they contribute to foundational responsible AI research and practical responsible AI solutions for teams across Microsoft.



Chad, welcome!



CHAD ATALLA:&nbsp;Thank you.



SULLIVAN:&nbsp;So&nbsp;we&#8217;ll&nbsp;kick off with a couple questions just to dive right in.&nbsp;So&nbsp;tell me a little bit more about the&nbsp;Sociotechnical Alignment Center,&nbsp;or&nbsp;STAC? I know it was founded in&nbsp;2022.&nbsp;I&#8217;d&nbsp;love to just learn a little bit more about what the group does, how&nbsp;you&#8217;re&nbsp;thinking about evaluating AI, and&nbsp;maybe just&nbsp;give us a sense of some of the projects&nbsp;you&#8217;re&nbsp;working on.



ATALLA:&nbsp;Yeah, absolutely. The name is quite a mouthful.



SULLIVAN:&nbsp;It is!&nbsp;[LAUGHS]&nbsp;



ATALLA:&nbsp;So&nbsp;let&#8217;s&nbsp;start by breaking that down and seeing what that means.



SULLIVAN:&nbsp;Great.



ATALLA: So modern AI systems are sociotechnical systems, meaning that the social and technical aspects are deeply intertwined. And&nbsp;we&#8217;re interested in aligning the behaviors of these sociotechnical&nbsp;systems with some values.&nbsp;Those could be societal values;&nbsp;they could be regulatory values, organizational values, etc. And to make this alignment happen, we need the ability to evaluate the systems.



So&nbsp;my team is broadly working on an evaluation framework that acknowledges the sociotechnical nature of the technology and the often-abstract nature of the concepts&nbsp;we&#8217;re&nbsp;actually interested&nbsp;in evaluating. As you noted,&nbsp;it&#8217;s&nbsp;an applied science team, so we split our time between some fundamental research and time to bridge the work into real products across the company. And I also want to note that to power this sort of work, we have an interdisciplinary team drawing upon the social sciences, linguistics, statistics, and,&nbsp;of course, computer science.



SULLIVAN:&nbsp;Well,&nbsp;I&#8217;m&nbsp;eager to get into our takeaways from the conversation with&nbsp;both Daniel&nbsp;and Timo. But&nbsp;maybe just&nbsp;to double-click on this for a minute, can you talk a bit about some of the overarching goals of the AI evaluations that you noted?&nbsp;



ATALLA:&nbsp;So&nbsp;evaluation is really the act of making valuative judgments based on some evidence, and in the case of AI evaluation, that evidence might be from tests or measurements, right.&nbsp;And the goal of why&nbsp;we&#8217;re doing this in the first place is to make decisions and claims most often.



So&nbsp;perhaps I&nbsp;am going to make a claim about a model that&nbsp;I&#8217;m&nbsp;producing, and I want to say that&nbsp;it&#8217;s&nbsp;better than this other model. Or we are asking whether a certain product is safe to ship.&nbsp;All of these decisions need to be informed by good evaluation and therefore good measurement or testing.&nbsp;And&nbsp;I&#8217;ll&nbsp;also note that in&nbsp;the regulatory conversation, risk&nbsp;is often what we want to evaluate. So that is a goal in and of itself. And&nbsp;I&#8217;ll&nbsp;touch more on that later.



SULLIVAN:&nbsp;I read a recent&nbsp;paper that you had put out with some of our colleagues from Microsoft Research, from the University of Michigan, and Stanford, and you were arguing that evaluating generative AI is&nbsp;the&nbsp;social-science measurement challenge.&nbsp;Maybe for&nbsp;those who&nbsp;haven&#8217;t&nbsp;read the paper, what does this mean? And can you tell us a little bit more about what motivated you and your coauthors?&nbsp;



ATALLA:&nbsp;So the measurement tasks involved in evaluating generative AI systems are often abstract and contested. So that means they cannot be directly measured and must instead [be] indirectly measured via other observable phenomena. So this is very different than the older machine learning paradigm, where, let&#8217;s say, for example, I had a system that took a picture of a traffic light and told you whether it was green, yellow, or red at a given time.&nbsp;



If we wanted to evaluate that system, the task is much simpler. But with the modern generative AI systems that are also general purpose, they have open-ended output, and language in a whole chat or multiple paragraphs being outputted can have a lot of different properties. And as I noted, these are general-purpose systems, so we don&#8217;t know exactly what task they&#8217;re supposed to be carrying out.



So&nbsp;then the question becomes, if I want to make some decision or claimâ€”maybe I&nbsp;want to make a claim that this system has human-level reasoning capabilitiesâ€”well, what does that mean? Do I have the same impression of what that means as you do? And how do we know whether the downstream, you know, measurements and tests that&nbsp;I&#8217;m&nbsp;conducting&nbsp;actually will&nbsp;support my notion of what it means to have human-level reasoning,&nbsp;right?&nbsp;Difficult questions. But luckily, social scientists have been dealing with these exact sorts of challenges for multiple decades in fields like education, political science, and psychometrics. So&nbsp;we&#8217;re&nbsp;really&nbsp;attempting&nbsp;to avoid reinventing the wheel here and trying to learn from their past methodologies.



And so the rest of the paper goes on to delve into&nbsp;a four-level framework, a measurement framework, that&#8217;s grounded in the measurement theory from the quantitative social sciences that takes us all the way from these abstract and contested concepts through processes to get much clearer and eventually reach reliable and valid measurements that can power our evaluations.



SULLIVAN:&nbsp;I love that. I mean,&nbsp;that&#8217;s&nbsp;the whole point of this podcast,&nbsp;too,&nbsp;right.&nbsp;Is&nbsp;to really&nbsp;build&nbsp;on those other learnings and frameworks that&nbsp;we&#8217;re&nbsp;taking from industries that have been thinking about this for much longer.&nbsp;Maybe from&nbsp;your vantage point, what are some of the biggest day-to-day hurdles in building solid AI evaluations&nbsp;and,&nbsp;I&nbsp;don&#8217;t&nbsp;know, do we need more shared standards? Are there&nbsp;bespoke methods? Are those&nbsp;the way to go? I would love&nbsp;to just&nbsp;hear your thoughts on that.



ATALLA:&nbsp;So&nbsp;let&#8217;s&nbsp;talk about some of those practical challenges. And I want to briefly go back to what I mentioned about risk before, all right.&nbsp;Oftentimes,&nbsp;some of the regulatory environment&nbsp;is requiring practitioners to measure the&nbsp;risk&nbsp;involved in deploying one of their models or AI systems. Now, risk is importantly a&nbsp;concept that includes both event and impact,&nbsp;right.&nbsp;So&nbsp;there&#8217;s&nbsp;the probability of some event occurring. For the case of AI evaluation,&nbsp;perhaps this&nbsp;is us seeing a certain AI behavior&nbsp;exhibited. Then there&#8217;s also the severity of the&nbsp;impacts,&nbsp;and this is a complex chain of effects in the real world that&nbsp;happen&nbsp;to people, organizations, systems, etc., and&nbsp;it&#8217;s&nbsp;a lot more challenging to&nbsp;observe&nbsp;the impacts,&nbsp;right.



So&nbsp;if we&#8217;re saying that we need to measure risk, we have to measure both the event and the&nbsp;impacts. But realistically, right now, the field is not doing&nbsp;a very good&nbsp;job of&nbsp;actually measuring&nbsp;the impacts. This requires vastly different techniques and methodologies where if I just wanted to measure something about the event itself, I can, you know, do that in a technical sandbox&nbsp;environment&nbsp;and&nbsp;perhaps have&nbsp;some automated methods to detect whether a certain AI behavior is being&nbsp;exhibited. But if I want to measure the impacts? Now,&nbsp;we&#8217;re&nbsp;in the realm of needing to have real people involved, and&nbsp;perhaps a&nbsp;longitudinal study where you have interviews, questionnaires, and more qualitative evidence-gathering techniques to&nbsp;truly understand&nbsp;the long-term impacts. So&nbsp;that&#8217;s&nbsp;a significant challenge.



Another is that, you know,&nbsp;let&#8217;s&nbsp;say we forget about the impacts for&nbsp;now&nbsp;and we focus on the event side of things. Still, we need datasets, we need&nbsp;annotations,&nbsp;and we need&nbsp;metrics to make this whole thing work. When I say we need datasets, if I want to test whether my system has good mathematical reasoning, what questions should I ask? What are my set of inputs that are relevant? And then when I get&nbsp;the&nbsp;response from the system, how do I annotate them? How do I know if it was a good response that&nbsp;did demonstrate mathematical reasoning or if it was a mediocre response? And then once I have an annotation of&nbsp;all of these outputs from the AI system, how do I aggregate those all up into a single informative number?



SULLIVAN:&nbsp;Earlier in this episode, we heard Daniel and&nbsp;Timo walk&nbsp;through the regulatory frameworks in pharma and medical devices.&nbsp;I&#8217;d&nbsp;be curious what pieces of those mature systems are already showing up or at least may&nbsp;be bubbling up in AI governance.



ATALLA:&nbsp;Great question. You know, Timo was talking about the pre-market and post-market testing difference. Of course, this is similarly important in the AI evaluation space. But again, these have different methodologies and serve different purposes.



So&nbsp;within the pre-deployment phase, we&nbsp;don&#8217;t&nbsp;have evidence of how people are going to use the system. And when we have these general-purpose AI systems,&nbsp;to understand what the risks are, we really need to have a sense of what might happen and how they might be used.&nbsp;So&nbsp;there are&nbsp;significant challenges there where I think we can learn from other fields and how they do pre-market testing. And the difference in that pre- versus post-market testing also ties to testing at&nbsp;different stages&nbsp;in the life cycle.



For AI systems, we already see some regulations saying you need to start with the base model and do some evaluation of the base model, some basic attributes, some core attributes,&nbsp;of that base model before you start putting it into any real products. But once we have a product in mind, we have a user base in mind, we have a specific taskâ€”like maybe we&#8217;re going to integrate this model into Outlook and it&#8217;s going to help you write&nbsp;emailsâ€”now we suddenly have a much crisper picture of how the system will interact with the world around it. And again, at that stage, we need to think about another round of evaluation.



Another part that jumped out to me in what they were saying about pharmaceuticals is that sometimes approvals can be based on surrogate endpoints.&nbsp;So&nbsp;this is like&nbsp;we&#8217;re&nbsp;choosing some&nbsp;heuristic.&nbsp;Instead of measuring the long-term impact, which is what we&nbsp;actually care&nbsp;about,&nbsp;perhaps we&nbsp;have a proxy that we&nbsp;feel like&nbsp;is a good enough indicator of what that long-term impact might look like.&nbsp;&nbsp;



This is occurring in the AI evaluation space right now and is often perhaps even the default here since&nbsp;we&#8217;re not seeing that many studies of the long-term impact itself. We are seeing, instead, folks constructing these heuristics or proxies and saying if I see this behavior happen,&nbsp;I&#8217;m&nbsp;going to&nbsp;assume&nbsp;that it&nbsp;indicates&nbsp;this sort of impact will happen downstream. And&nbsp;that&#8217;s&nbsp;great.&nbsp;It&#8217;s&nbsp;one of the techniques that was used to speed up and reduce the barrier to innovation in&nbsp;the other&nbsp;fields. And I think&nbsp;it&#8217;s&nbsp;great that we are applying that in the AI evaluation space. But&nbsp;special care&nbsp;is,&nbsp;of course, needed to ensure that those heuristics and proxies you&#8217;re&nbsp;using are reasonable indicators of the greater outcome&nbsp;you&#8217;re&nbsp;looking for.



SULLIVAN:&nbsp;What are some of the promising ideas from&nbsp;maybe pharma&nbsp;or med device regulation that maybe haven&#8217;t&nbsp;made it to AI testing yet and&nbsp;maybe should? And where would you urge technologists, policymakers,&nbsp;and researchers to focus their energy next?



ATALLA:&nbsp;Well, one of the key things that jumped out to me in the discussion about pharmaceuticals was driving home the emphasis that there&nbsp;is&nbsp;a&nbsp;holistic&nbsp;focus on safety&nbsp;and&nbsp;efficacy. These go hand in hand&nbsp;and decisions must be made while considering both pieces of the picture. I would like to see that further emphasized in the AI evaluation space.



Often,&nbsp;we&nbsp;are seeing&nbsp;evaluations of risk being separated from evaluations of&nbsp;performance or quality&nbsp;or efficacy, but these two pieces of the puzzle really are not enough for us to make informed decisions independently.&nbsp;And that ties back into my desire to really also see us measuring the impacts.



So&nbsp;we see Phase 3 trials as something that occurs in the medical devices and pharmaceuticals field. That&#8217;s not something that we are doing an equivalent of in the AI evaluation space at this time.&nbsp;These are really&nbsp;cost intensive. They can last years and really involve careful monitoring of that holistic picture of safety and efficacy. And realistically, we are not going to be able to put that on the critical path to getting specific individual AI models or AI systems vetted before they&nbsp;go out&nbsp;into the world. However, I would love to see a world in which this sort of work is prioritized&nbsp;and funded or&nbsp;required. Think of how, with&nbsp;social media, it took quite a long time for us to understand that there are some long-term negative impacts on mental health, and we have the opportunity now, while the AI wave is still building,&nbsp;to start prioritizing and funding this sort of work. Let it run in the background and as soon as possible develop a good understanding of the subtle, long-term effects.



More broadly, I would love to see us focus on reliability and validity of the evaluations&nbsp;we&#8217;re&nbsp;conducting because trust in these decisions and claims is important. If we&nbsp;don&#8217;t&nbsp;focus on building reliable, valid, and trustworthy evaluations,&nbsp;we&#8217;re&nbsp;just going to continue to be flooded by a bunch of competing, conflicting, and&nbsp;largely meaningless&nbsp;AI evaluations.



SULLIVAN:&nbsp;In a number of the discussions we&#8217;ve had on this podcast, we talked about how it&#8217;s not just one entity that really needs to ensure safety across the board,&nbsp;and Iâ€™d&nbsp;just love to hear from you how you think about some of those ecosystem collaborations, and you know, from across &#8230; where we think about ourselves as more of a platform company or places that these AI models are being deployed more at the application level. Tell me a little bit about how you think about,&nbsp;sort&nbsp;of, stakeholders in that mix and where responsibility lies across the board.



ATALLA:&nbsp;It&#8217;s&nbsp;interesting. In this age of general-purpose AI technologies,&nbsp;we&#8217;re&nbsp;often&nbsp;seeing&nbsp;one company or organization&nbsp;being responsible for&nbsp;building the foundational model. And then many, many other people will take that model and build it into specific products that are designed for specific tasks and contexts.



Of course,&nbsp;in that, we already see that there is&nbsp;a responsibility&nbsp;of the owners of that foundational model to do some testing of the central model before they distribute it broadly. And then again, there is responsibility of all of the downstream individuals digesting that and turning it into products to consider the specific contexts that they are deploying into and how that may affect the risks we&#8217;re concerned with or the types of quality and safety and performance we need to evaluate.



Again, because that field of risks we may be concerned with is so broad, some of them also require an immense amount of&nbsp;expertise.&nbsp;Let&#8217;s&nbsp;think about whether AI systems can enable people to create dangerous chemicals or dangerous weapons at home. It&#8217;s not that every AI practitioner is going to have the knowledge to evaluate this, so in some of those cases, we really need third-party experts, people who are experts in chemistry, biology, etc., to come in and evaluate certain systems and models for those specific risks,&nbsp;as well.



So&nbsp;I think there&nbsp;are many reasons why multiple stakeholders need to be involved, partly from who owns what and&nbsp;is responsible for&nbsp;what and partly from the perspective of who has the&nbsp;expertise&nbsp;to meaningfully construct the evaluations that we need.



SULLIVAN:&nbsp;Well, Chad, this has just been great to connect, and in a few of our discussions,&nbsp;we&#8217;ve&nbsp;done a bit of a lightning round, so&nbsp;I&#8217;d&nbsp;love to just hear your&nbsp;30-second responses to a few of these questions. Perhaps&nbsp;favorite&nbsp;evaluation&nbsp;you&#8217;ve&nbsp;run so far this year?&nbsp;



ATALLA:&nbsp;So&nbsp;I&#8217;ve&nbsp;been involved in trying to evaluate some language models for whether they&nbsp;infer&nbsp;sensitive attributes about people. So&nbsp;perhaps&nbsp;you&#8217;re&nbsp;chatting with a&nbsp;chatbot,&nbsp;and it infers your religion or sexuality based on things&nbsp;you&#8217;re&nbsp;saying or how you sound,&nbsp;right.&nbsp;And in working to evaluate this, we&nbsp;encounter&nbsp;a lot of interesting questions. Or,&nbsp;like,&nbsp;what is a sensitive attribute? What makes these attributes sensitive, and what are the differences that make it inappropriate for an AI system to infer these things about a person? Whereas realistically, whenever I meet a person on the street, my&nbsp;brain is&nbsp;immediately&nbsp;forming&nbsp;first impressions and some assumptions about these people.&nbsp;So&nbsp;it&#8217;s&nbsp;a very interesting&nbsp;and thought-provoking evaluation to conduct and think about the norms that we place upon&nbsp;people&nbsp;interacting with other people and the norms we place upon&nbsp;AI systems&nbsp;interacting with other people.



SULLIVAN:&nbsp;Thatâ€™s&nbsp;fascinating!&nbsp;I&#8217;d&nbsp;love to hear the AI&nbsp;buzzword&nbsp;you&#8217;d&nbsp;retire tomorrow.&nbsp;[LAUGHTER]



ATALLA:&nbsp;I would love to see the term â€œbiasâ€ being&nbsp;used less when referring to fairness-related issues and systems. Bias happens to be a highly overloaded term in statistics and machine learning and has a lot of technical meanings and just&nbsp;fails to&nbsp;perfectly capture what we mean in the AI risk sense.



SULLIVAN:&nbsp;And last one. One metric&nbsp;we&#8217;re&nbsp;not tracking enough.



ATALLA:&nbsp;I would say over-blocking, and this comes into that connection between the holistic picture of safety and efficacy. It&#8217;s too easy to produce systems that throw safety to the wind and focus purely on utility or achieving some goal, but simultaneously, the other side of the picture is possible, where we can clamp down too hard and reduce the utility of our systems and block even benign and useful outputs just because they border on something sensitive.&nbsp;So&nbsp;it&#8217;s&nbsp;important for us to track that over-blocking and actively track that tradeoff between safety and efficacy.



SULLIVAN:&nbsp;Yeah, we talk a lot about this on the podcast,&nbsp;too,&nbsp;of how do you both make things safe but also ensure innovation can&nbsp;thrive,&nbsp;and&nbsp;I think you&nbsp;hit the nail on the head with that last piece.



[MUSIC]&nbsp;



Well, Chad, this was&nbsp;really terrific. Thanks for joining us and thanks for your work and your&nbsp;perspectives. And another big thanks to Daniel and Timo for setting the stage earlier in the podcast.



And to our listeners, thanks for tuning in. You can find resources related to this podcast in the show notes. And if you want to learn more about how Microsoft approaches AI governance, you can visit microsoft.com/RAI.â€¯



See you next time!â€¯



[MUSIC FADES]

				
			
			
				Show more			
		
	





AI Testing and Evaluation podcast series

Opens in a new tabThe post AI Testing and Evaluation: Learnings from pharmaceuticals and medical devices appeared first on Microsoft Research.
â€¢ New capabilities in Amazon SageMaker AI continue to transform how organizations develop AI models
  As AI models become increasingly sophisticated and specialized, the ability to quickly train and customize models can mean the difference between industry leadership and falling behind. That is why hundreds of thousands of customers use the fully managed infrastructure, tools, and workflows of Amazon SageMaker AI to scale and advance AI model development. Since launching in 2017, SageMaker AI has transformed how organizations approach AI model development by reducing complexity while maximizing performance. Since then, weâ€™ve continued to relentlessly innovate, adding more than 420 new capabilities since launch to give customers the best tools to build, train, and deploy AI models quickly and efficiently. Today, weâ€™re pleased to announce new innovations that build on the rich features of SageMaker AI to accelerate how customers build and train AI models. 
Amazon SageMaker HyperPod: The infrastructure of choice for developing AI models 
AWS launched Amazon SageMaker HyperPod in 2023 to reduce complexity and maximize performance and efficiency when building AI models. With SageMaker HyperPod, you can quickly scale generative AI model development across thousands of AI accelerators and reduce foundation model (FM) training and fine-tuning development costs by up to 40%. Many of todayâ€™s top models are trained on SageMaker HyperPod, including models from Hugging Face, Luma AI, Perplexity AI, Salesforce, Thomson Reuters, Writer, and Amazon. By training Amazon Nova FMs on SageMaker HyperPod, Amazon saved months of work and increased utilization of compute resources to more than 90%. 
 
To further streamline workflows and make it faster to develop and deploy models, a new command line interface (CLI) and software development kit (SDK) provides a single, consistent interface that simplifies infrastructure management, unifies job submission across training and inference, and supports both recipe-based and custom workflows with integrated monitoring and control. Today, we are also adding two capabilities to SageMaker HyperPod that can help you reduce training costs and accelerate AI model development. 
Reduce the time to troubleshoot performance issues from days to minutes with SageMaker HyperPod observability 
To bring new AI innovations to market as quickly as possible, organizations need visibility across AI model development tasks and compute resources to optimize training efficiency and detect and resolve interruptions or performance bottlenecks as soon as possible. For example, to investigate if a training or fine-tuning job failure was the result of a hardware issue, data scientists and machine learning (ML) engineers want to quickly filter to review the monitoring data of the specific GPUs that performed the job rather than manually browsing through the hardware resources of an entire cluster to establish the correlation between the job failure and a hardware issue. 
The new observability capability in SageMaker HyperPod transforms how you can monitor and optimize your model development workloads. Through a unified dashboard preconfigured in Amazon Managed Grafana, with the monitoring data automatically published to an Amazon Managed Service for Prometheus workspace, you can now see generative AI task performance metrics, resource utilization, and cluster health in a single view. Teams can now quickly spot bottlenecks, prevent costly delays, and optimize compute resources. You can define automated alerts, specify use case-specific task metrics and events, and publish them to the unified dashboard with just a few clicks. 
By reducing troubleshooting time from days to minutes, this capability can help you accelerate your path to production and maximize the return on your AI investments. 
 
DatologyAI builds tools to automatically select the best data on which to train deep learning models. 

 â€œWe are excited to use Amazon SageMaker HyperPodâ€™s one-click observability solution. Our senior staff members needed insights into how weâ€™re utilizing GPU resources. The pre-built Grafana dashboards will give us exactly what we needed, with immediate visibility into critical metricsâ€”from task-specific GPU utilization to file system (FSx for Lustre) performanceâ€”without requiring us to maintain any monitoring infrastructure. As someone who appreciates the power of the Prometheus Query Language, I like the fact that I can write my own queries and analyze custom metrics without worrying about infrastructure problems.â€ â€“Josh Wills, Member of Technical Staff at DatologyAI
 
â€“ 
 
Articul8 helps companies build sophisticated enterprise generative AI applications. 

 â€œWith SageMaker HyperPod observability, we can now deploy our metric collection and visualization systems in a single click, saving our teams days of otherwise manual setup and enhancing our cluster observability workflows and insights. Our data scientists can quickly monitor task performance metrics, such as latency, and identify hardware issues without manual configuration. SageMaker HyperPod observability will help streamline our foundation model development processes, allowing us to focus on advancing our mission of delivering accessible and reliable AI-powered innovation to our customers.â€ â€“Renato Nascimento, head of technology at Articul8
 
â€“ 
Deploy Amazon SageMaker JumpStart models on SageMaker HyperPod for fast, scalable inference 
After developing generative AI models on SageMaker HyperPod, many customers import these models to Amazon Bedrock, a fully managed service for building and scaling generative AI applications. However, some customers want to use their SageMaker HyperPod compute resources to speed up their evaluation and move models into production faster. 
Now, you can deploy open-weights models from Amazon SageMaker JumpStart, as well as fine-tuned custom models, on SageMaker HyperPod within minutes with no manual infrastructure setup. Data scientists can run inference on SageMaker JumpStart models with a single click, simplifying and accelerating model evaluation. This straightforward, one-time provisioning reduces manual infrastructure setup, providing a reliable and scalable inference environment with minimal effort. Large model downloads are reduced from hours to minutes, accelerating model deployments and shortening the time to market. 
â€“ 
 
H.AI exists to push the boundaries of superintelligence with agentic AI. 

 â€œWith Amazon SageMaker HyperPod, we used the same high-performance compute to build and deploy the foundation models behind our agentic AI platform. This seamless transition from training to inference streamlined our workflow, reduced time to production, and delivered consistent performance in live environments. SageMaker HyperPod helped us go from experimentation to real-world impact with greater speed and efficiency.â€ â€“Laurent Sifre, Co-founder &amp; CTO at H.AI
 
â€“ 
Seamlessly access the powerful compute resources of SageMaker AI from local development environments 
Today, many customers choose from the broad set of fully managed integrated development environments (IDEs) available in SageMaker AI for model development, including JupyterLab, Code Editor based on Code-OSS, and RStudio. Although these IDEs enable secure and efficient setups, some developers prefer to use local IDEs on their personal computers for their debugging capabilities and extensive customization options. However, customers using a local IDE, such as Visual Studio Code, couldnâ€™t easily run their model development tasks on SageMaker AI until now. 
With new remote connections to SageMaker AI, developers and data scientists can quickly and seamlessly connect to SageMaker AI from their local VS Code, maintaining access to the custom tools and familiar workflows that help them work most efficiently. Developers can build and train AI models using their local IDE while SageMaker AI manages remote execution, so you can work in your preferred environment while still benefiting from the performance, scalability, and security of SageMaker AI. You can now choose your preferred IDEâ€”whether that is a fully managed cloud IDE or VS Codeâ€”to accelerate AI model development using the powerful infrastructure and seamless scalability of SageMaker AI. 
â€“ 
 

 CyberArk is a leader in Identity Security, which provides a comprehensive approach centered on privileged controls to protect against advanced cyber threats. 
 â€œWith remote connections to SageMaker AI, our data scientists have the flexibility to choose the IDE that makes them most productive. Our teams can leverage their customized local setup while accessing the infrastructure and security controls of SageMaker AI. As a security first company, this is extremely important to us as it ensures sensitive data stays protected, while allowing our teams to securely collaborate and boost productivity.â€ â€“Nir Feldman, Senior Vice President of Engineering at CyberArk
 
â€“ 
Build generative AI models and applications faster with fully managed MLflow 3.0 
As customers across industries accelerate their generative AI development, they require capabilities to track experiments, observe behavior, and evaluate performance of models and AI applications. Customers such as Cisco, SonRai, and Xometry are already using managed MLflow on SageMaker AI to efficiently manage ML model experiments at scale. The introduction of fully managed&nbsp;MLflow 3.0 on SageMaker AI makes it straightforward to track experiments, monitor training progress, and gain deeper insights into the behavior of models and AI applications using a single tool, helping you accelerate generative AI development. 
Conclusion 
In this post, we shared some of the new innovations in SageMaker AI to accelerate how you can build and train AI models. 
To learn more about these new features, SageMaker AI, and how companies are using this service, refer to the following resources: 
 
 Accelerate foundation model development with one click observability in Amazon SageMaker HyperPod 
 Supercharge your AI workflows by connecting to SageMaker Studio from Visual Studio Code 
 Accelerating generative AI development with fully managed MLflow 3.0 on Amazon SageMaker AI 
 Amazon SageMaker HyperPod launches model deployments to accelerate the generative AI model development lifecycle 
 Amazon SageMaker AI 
 Amazon SageMaker AI customers 
 
 
 
About the author 
Ankur Mehrotra joined Amazon back in 2008 and is currently the General Manager of Amazon SageMaker AI. Before Amazon SageMaker AI, he worked on building Amazon.comâ€™s advertising systems and automated pricing technology.
â€¢ Accelerate foundation model development with one-click observability in Amazon SageMaker HyperPod
  Amazon SageMaker HyperPod now provides a comprehensive, out-of-the-box dashboard that delivers insights into foundation model (FM) development tasks and cluster resources. This unified observability solution automatically publishes key metrics to Amazon Managed Service for Prometheus and visualizes them in Amazon Managed Grafana dashboards, optimized specifically for FM development with deep coverage of hardware health, resource utilization, and task-level performance. 
With a one-click installation of the Amazon Elastic Kubernetes Service (Amazon EKS) add-on for SageMaker HyperPod observability, you can consolidate health and performance data from NVIDIA DCGM, instance-level Kubernetes node exporters, Elastic Fabric Adapter (EFA), integrated file systems, Kubernetes APIs, Kueue, and SageMaker HyperPod task operators. With this unified view, you can trace model development task performance to cluster resources with aggregation of resource metrics at the task level. The solution also abstracts management of collector agents and scrapers across clusters, offering automatic scalability of collectors across nodes as the cluster grows. The dashboards feature intuitive navigation across metrics and visualizations to help users diagnose problems and take action faster. They are also fully customizable, supporting additional PromQL metric imports and custom Grafana layouts. 
These capabilities save teams valuable time and resources during FM development, helping accelerate time-to-market and reduce the cost of generative AI innovations. Instead of spending hours or days configuring, collecting, and analyzing cluster telemetry systems, data scientists and machine learning (ML) engineers can now quickly identify training, tuning, and inference disruptions, underutilization of valuable GPU resources, and hardware performance issues. The pre-built, actionable insights of SageMaker HyperPod observability can be used in several common scenarios when operating FM workloads, such as: 
 
 Data scientists can monitor resource utilization of submitted training and inference tasks at the per-GPU level, with insights into GPU memory and FLOPs 
 AI researchers can troubleshoot sub-optimal time-to-first-token (TTFT) for their inferencing workloads by correlating the deployment metrics with the corresponding resource bottlenecks 
 Cluster administrators can configure customizable alerts to send notifications to multiple destinations such as Amazon Simple Notification Service (Amazon SNS), PagerDuty, and Slack when hardware falls outside of recommended health thresholds 
 Cluster administrators can quickly identify inefficient resource queuing patterns across teams or namespaces to reconfigure allocation and prioritization policies 
 
In this post, we walk you through installing and using the unified dashboards of the out-of-the-box observability feature in SageMaker HyperPod. We cover the one-click installation from the Amazon SageMaker AI console, navigating the dashboard and metrics it consolidates, and advanced topics such as setting up custom alerts. If you have a running SageMaker HyperPod EKS cluster, then this post will help you understand how to quickly visualize key health and performance telemetry data to derive actionable insights. 
Prerequisites 
To get started with SageMaker HyperPod observability, you first need to enable AWS IAM Identity Center to use Amazon Managed Grafana. If IAM Identity Center isnâ€™t already enabled in your account, refer to Getting started with IAM Identity Center. Additionally, create at least one user in the IAM Identity Center. 
SageMaker HyperPod observability is available for SageMaker HyperPod clusters with an Amazon EKS orchestrator. If you donâ€™t already have a SageMaker HyperPod cluster with an Amazon EKS orchestrator, refer to Amazon SageMaker HyperPod quickstart workshops for instructions to create one. 
Enable SageMaker HyperPod observability 
To enable SageMaker HyperPod observability, follow these steps: 
 
 On the SageMaker AI console, choose Cluster management in the navigation pane. 
 Open the cluster detail page from the SageMaker HyperPod clusters list. 
 On the Dashboard tab, in the HyperPod Observability section, choose Quick installation. 
 
SageMaker AI will create a new Prometheus workspace, a new Grafana workspace, and install the SageMaker HyperPod observability add-on to the EKS cluster. The installation typically completes within a few minutes. 
 
When the installation process is complete, you can view the add-on details and metrics available. 
 
 Choose Manage users to assign a user to a Grafana workspace. 
 Choose Open dashboard in Grafana to open the Grafana dashboard. 
 
 
 
 When prompted, sign in with IAM Identity Center with the user you configured as a prerequisite. 
 
 
After signing in successfully, you will see the SageMaker HyperPod observability dashboard on Grafana. 
SageMaker HyperPod observability dashboards 
You can choose from multiple dashboards, including Cluster, Tasks, Inference, Training, and File system. 
The Cluster dashboard shows cluster-level metrics such as Total Nodes and Total GPUs, and cluster node-level metrics such as GPU Utilization and Filesystem space available. By default, the dashboard shows metrics about entire cluster, but you can apply filters to show metrics only about a specific hostname or specific GPU ID. 
 
The Tasks dashboard is helpful if you want to see resource allocation and utilization metrics at the task level (PyTorchJob, ReplicaSet, and so on). For example, you can compare GPU utilization by multiple tasks running on your cluster and identify which task should be improved. 
You can also choose an aggregation level from multiple options (Namespace, Task Name, Task Pod), and apply filters (Namespace, Task Type, Task Name, Pod, GPU ID). You can use these aggregation and filtering capabilities to view metrics at the appropriate granularity and drill down into the specific issue you are investigating. 
 
The Inference dashboard shows inference application specific metrics such as Incoming Requests, Latency, and Time to First Byte (TTFB). The Inference dashboard is particularly useful when you use SageMaker HyperPod clusters for inference and need to monitor the traffic of the requests and performance of models. 
 
Advanced installation 
The Quick installation option will create a new workspace for Prometheus and Grafana and select default metrics. If you want to reuse an existing workspace, select additional metrics, or enable Pod logging to Amazon CloudWatch Logs, use the Custom installation option. For more information, see Amazon SageMaker HyperPod. 
Set up alerts 
Amazon Managed Grafana includes access to an updated alerting system that centralizes alerting information in a single, searchable view (in the navigation pane, choose Alerts to create an alert). Alerting is useful when you want to receive timely notifications, such as when GPU utilization drops unexpectedly, when a disk usage of your shared file system exceeds 90%, when multiple instances become unavailable at the same time, and so on. The HyperPod observability dashboard in Amazon Managed Grafana has pre-configured alerts for few of these key metrics. You can create additional alert rules based on metrics or queries and set up multiple notification channels, such as emails and Slack messages. For instructions on setting up alerts with Slack messages, see the Setting Up Slack Alerts for Amazon Managed Grafana GitHub page. 
The number of alerts is limited to 100 per Grafana workspace. If you need a more scalable solution, check out the alerting options in Amazon Managed Service for Prometheus. 
High-level overview 
The following diagram illustrates the architecture of the new HyperPod observability capability. 
 
Clean up 
If you want to uninstall the SageMaker HyperPod observability feature (for example, to reconfigure it), clean up the resources in the following order: 
 
 Remove the SageMaker HyperPod observability add-on, either using the SageMaker AI console or Amazon EKS console. 
 Delete the Grafana workspace on the Amazon Managed Grafana console. 
 Delete the Prometheus workspace on the Amazon Managed Service for Prometheus console. 
 
Conclusion 
This post provided an overview and usage instructions for SageMaker HyperPod observability, a newly released observability feature for SageMaker HyperPod. This feature reduces the heavy lifting involved in setting up cluster observability and provides centralized visibility into cluster health status and performance metrics. 
For more information about SageMaker HyperPod observability, see Amazon SageMaker HyperPod. Please leave your feedback on this post in the comments section. 
 
About the authors 
 Tomonori Shimomura is a Principal Solutions Architect on the Amazon SageMaker AI team, where he provides in-depth technical consultation to SageMaker AI customers and suggests product improvements to the product team. Before joining Amazon, he worked on the design and development of embedded software for video game consoles, and now he leverages his in-depth skills in Cloud side technology. In his free time, he enjoys playing video games, reading books, and writing software. 
Matt Nightingale is a Solutions Architect Manager on the AWS WWSO Frameworks team focusing on Generative AI Training and Inference. Matt specializes in distributed training architectures with a focus on hardware performance and reliability. Matt holds a bachelors degree from University of Virginia and is based in Boston, Massachusetts. 
Eric Saleh is a Senior GenAI Specialist at AWS, focusing on foundation model training and inference. He is partnering with top foundation model builders and AWS service teams to enable distributed training and inference at scale on AWS and lead joint GTM motions with strategic customers. Before joining AWS, Eric led product teams building enterprise AI/ML solutions, which included frontier GenAI services for fine-tuning, RAG, and managed inference. He holds a masterâ€™s degree in Business Analytics from UCLA Anderson. 
Piyush Kadam is a Senior Product Manager on the Amazon SageMaker AI team, where he specializes in LLMOps products that empower both startups and enterprise customers to rapidly experiment with and efficiently govern foundation models. With a Masterâ€™s degree in Computer Science from the University of California, Irvine, specializing in distributed systems and artificial intelligence, Piyush brings deep technical expertise to his role in shaping the future of cloud AI products. 
Aman Shanbhag is a Specialist Solutions Architect on the ML Frameworks team at Amazon Web Services (AWS), where he helps customers and partners with deploying ML training and inference solutions at scale. Before joining AWS, Aman graduated from Rice University with degrees in computer science, mathematics, and entrepreneurship. 
Bhaskar Pratap is a Senior Software Engineer with the Amazon SageMaker AI team. He is passionate about designing and building elegant systems that bring machine learning to peopleâ€™s fingertips. Additionally, he has extensive experience with building scalable cloud storage services. 
Gopi Sekar is an Engineering Leader for the Amazon SageMaker AI team. He is dedicated to assisting customers and developing products that simplify the adaptation of machine learning to address real-world customer challenges.
â€¢ Accelerating generative AI development with fully managed MLflow 3.0 on Amazon SageMaker AI
  Amazon SageMaker now offers fully managed support for MLflow 3.0 that streamlines AI experimentation and accelerates your generative AI journey from idea to production. This release transforms managed MLflow from experiment tracking to providing end-to-end observability, reducing time-to-market for generative AI development. 
As customers across industries accelerate their generative AI development, they require capabilities to track experiments, observe behavior, and evaluate performance of models and AI applications. Data scientists and developers struggle to effectively analyze the performance of their models and AI applications from experimentation to production, making it hard to find root causes and resolve issues. Teams spend more time integrating tools than improving the quality of their models or generative AI applications. 
With the launch of fully managed MLflow 3.0 on Amazon SageMaker AI, you can accelerate generative AI development by making it easier to track experiments and observe behavior of models and AI applications using a single tool. Tracing capabilities in fully managed MLflow 3.0 provide customers the ability to record the inputs, outputs, and metadata at every step of a generative AI application, so developers can quickly identify the source of bugs or unexpected behaviors. By maintaining records of each model and application version, fully managed MLflow 3.0 offers traceability to connect AI responses to their source components, which means developers can quickly trace an issue directly to the specific code, data, or parameters that generated it. With these capabilities, customers using Amazon SageMaker HyperPod to train and deploy foundation models (FMs) can now use managed MLflow to track experiments, monitor training progress, gain deeper insights into the behavior of models and AI applications, and manage their machine learning (ML) lifecycle at scale. This reduces troubleshooting time and enables teams to focus more on innovation. 
This post walks you through the core concepts of fully managed MLflow 3.0 on SageMaker and provides technical guidance on how to use the new features to help accelerate your next generative AI application development. 
Getting started 
You can get started with fully managed MLflow 3.0 on Amazon SageMaker to track experiments, manage models, and streamline your generative AI/ML lifecycle through the AWS Management Console, AWS Command Line Interface (AWS CLI), or API. 
Prerequisites 
To get started, you need: 
 
 An AWS account with billing enabled 
 An Amazon SageMaker Studio AI domain. To create a domain, refer to Guide to getting set up with Amazon SageMaker AI. 
 
Configure your environment to use SageMaker managed MLflow Tracking Server 
To perform the configuration, follow these steps: 
 
 In the SageMaker Studio UI, in the Applications pane, choose MLflow and choose Create. 
 
 
 
 Enter a unique name for your tracking server and specify the Amazon Simple Storage Service (Amazon S3) URI where your experiment artifacts will be stored. When youâ€™re ready, choose Create. By default, SageMaker will select version 3.0 to create the MLflow tracking server. 
 Optionally, you can choose Update&nbsp;to adjust settings such as server size, tags, or AWS Identity and Access Management (IAM) role. 
 
The server will now be provisioned and started automatically, typically within 25 minutes. After setup, you can launch the MLflow UI from SageMaker Studio to start tracking your ML and generative AI experiments. For more details on tracking server configurations, refer to Machine learning experiments using Amazon SageMaker AI with MLflow in the SageMaker Developer Guide. 
To begin tracking your experiments with your newly created SageMaker managed MLflow tracking server, you need to install both MLflow and the AWS SageMaker MLflow Python packages in your environment. You can use SageMaker Studio managed Jupyter Lab, SageMaker Studio Code Editor, a local integrated development environment (IDE), or other supported environment where your AI workloads operate to track with SageMaker managed MLFlow tracking server. 
To install both Python packages using pip:pip install mlflow==3.0 sagemaker-mlflow==0.1.0 
To connect and start logging your AI experiments, parameters, and models directly to the managed MLflow on SageMaker, replace the Amazon Resource Name (ARN) of your SageMaker MLflow tracking server: 
 
 import mlflow

# SageMaker MLflow ARN
tracking_server_arn = "arn:aws:sagemaker:&lt;Region&gt;:&lt;Account_id&gt;:mlflow-tracking-server/&lt;Name&gt;"&nbsp;# Enter ARN
mlflow.set_tracking_uri(tracking_server_arn)&nbsp;
mlflow.set_experiment("customer_support_genai_app") 
 
Now your environment is configured and ready to track your experiments with your SageMaker Managed MLflow tracking server. 
Implement generative AI application tracing and version tracking 
Generative AI applications have multiple components, including code, configurations, and data, which can be challenging to manage without systematic versioning. A LoggedModel entity in managed MLflow 3.0 represents your AI model, agent, or generative AI application within an experiment. It provides unified tracking of model artifacts, execution traces, evaluation metrics, and metadata throughout the development lifecycle. A trace is a log of inputs, outputs, and intermediate steps from a single application execution. Traces provide insights into application performance, execution flow, and response quality, enabling debugging and evaluation. With LoggedModel, you can track and compare different versions of your application, making it easier to identify issues, deploy the best version, and maintain a clear record of what was deployed and when. 
To implement version tracking and tracing with managed MLflow 3.0 on SageMaker, you can establish a versioned model identity using a Git commit hash, set this as the active model context so all subsequent traces will be automatically linked to this specific version, enable automatic logging for Amazon Bedrock interactions, and then make an API call to Anthropicâ€™s Claude 3.5 Sonnet that will be fully traced with inputs, outputs, and metadata automatically captured within the established model context. Managed MLflow 3.0 tracing is already integrated with various generative AI libraries and provides one-line automatic tracing experience for all the support libraries. For information about supported libraries, refer to Supported Integrations in the MLflow documentation. 
 
 # 1. Define your application version using the git commit
logged_model= "customer_support_agent"
logged_model_name = f"{logged_model}-{git_commit}"

# 2.Set the active model context - traces will be linked to this
mlflow.set_active_model(name=logged_model_name)


# 3.Set auto logging for your model provider
mlflow.bedrock.autolog()

# 4. Chat with your LLM provider
# Ensure that your boto3 client has the necessary auth information
bedrock = boto3.client(
 service_name="bedrock-runtime",
 region_name="&lt;REPLACE_WITH_YOUR_AWS_REGION&gt;",
)

model = "anthropic.claude-3-5-sonnet-20241022-v2:0"
messages = [{ "role": "user", "content": [{"text": "Hello!"}]}]
# All intermediate executions within the chat session will be logged
bedrock.converse(modelId=model, messages=messages) 
 
After logging this information, you can track these generative AI experiments and the logged model for the agent in the managed MLflow 3.0 tracking server UI, as shown in the following screenshot. 
 
In addition to the one-line auto tracing functionality, MLflow offers Python SDK for manually instrumenting your code and manipulating traces. Refer to the code sample notebook sagemaker_mlflow_strands.ipynb in the aws-samples GitHub repository, where we use MLflow manual instrumentation to trace Strands Agents. With tracing capabilities in fully managed MLflow 3.0, you can record the inputs, outputs, and metadata associated with each intermediate step of a request, so you can pinpoint the source of bugs and unexpected behaviors. 
These capabilities provide observability in your AI workload by capturing detailed information about the execution of the workload services, nodes, and tools that you can see under the Traces tab. 
 
You can inspect each trace, as shown in the following image, by choosing the request ID in the traces tab for the desired trace. 
 
Fully managed MLflow 3.0 on Amazon SageMaker also introduces the capability to tag traces. Tags are mutable key-value pairs you can attach to traces to add valuable metadata and context. Trace tags make it straightforward to organize, search, and filter traces based on criteria such as user session, environment, model version, or performance characteristics. You can add, update, or remove tags at any stageâ€”during trace execution using mlflow.update_current_trace() or after a trace is logged using the MLflow APIs or UI. Managed MLflow 3.0 makes it seamless to search and analyze traces, helping teams quickly pinpoint issues, compare agent behaviors, and optimize performance. The tracing UI and Python API both support powerful filtering, so you can drill down into traces based on attributes such as status, tags, user, environment, or execution time as shown in the screenshot below. For example, you can instantly find all traces with errors, filter by production environment, or search for traces from a specific request. This capability is essential for debugging, cost analysis, and continuous improvement of generative AI applications. 
The following screenshot displays the traces returned when searching for the tag â€˜Productionâ€™. 
 
The following code snippet shows how you can use search for all traces in production with a successful status: 
 
 # Search for traces in production environment with successful status 
traces = mlflow.search_traces( filter_string="attributes.status = 'OK' AND tags.environment = 'production'") 
 
Generative AI use case walkthrough with MLflow tracing 
Building and deploying generative AI agents such as chat-based assistants, code generators, or customer support assistants requires deep visibility into how these agents interact with large language models (LLMs) and external tools. In a typical agentic workflow, the agent loops through reasoning steps, calling LLMs and using tools or subsystems such as search APIs or Model Context Protocol (MCP) servers until it completes the userâ€™s task. These complex, multistep interactions make debugging, optimization, and cost tracking especially challenging. 
Traditional observability tools fall short in generative AI because agent decisions, tool calls, and LLM responses are dynamic and context-dependent. Managed MLflow 3.0 tracing provides comprehensive observability by capturing every LLM call, tool invocation, and decision point in your agentâ€™s workflow. You can use this end-to-end trace data to: 
 
 Debug agent behavior â€“ Pinpoint where an agentâ€™s reasoning deviates or why it produces unexpected outputs. 
 Monitor tool usage â€“ Discover how and when external tools are called and analyze their impact on quality and cost. 
 Track performance and cost â€“ Measure latency, token usage, and API costs at each step of the agentic loop. 
 Audit and govern â€“ Maintain detailed logs for compliance and analysis. 
 
Imagine a real-world scenario using the managed MLflow 3.0 tracing UI for a sample finance customer support agent equipped with a tool to retrieve financial data from a datastore. While youâ€™re developing a generative AI customer support agent or analyzing the agent behavior in production, you can observe how agent responses and the execution optionally call a product database tool for more accurate recommendations. For illustration, the first trace, shown in the following screenshot, shows the agent handling a user query without invoking any tools. The trace captures the prompt, agent response, and agent decision points. The agentâ€™s response lacks product-specific details. The trace makes it clear that no external tool was called, and you quickly identify the behavior in the agentâ€™s reasoning chain. 
 
The second trace, shown in the following screenshot, captures the same agent, but this time it decides to call the product database tool. The trace logs the tool invocation, the returned product data, and how the agent incorporates this information into its final response. Here, you can observe improved answer quality, a slight increase in latency, and additional API cost with higher token usage. 
 
By comparing these traces side by side, you can debug why the agent sometimes skips using the tool, optimize when and how tools are called, and balance quality against latency and cost. MLflowâ€™s tracing UI makes these agentic loops transparent, actionable, and seamless to analyze at scale. This postâ€™s sample agent and all necessary code is available on the aws-samples GitHub repository, where you can replicate and adapt it for your own applications. 
Cleanup 
After itâ€™s created, a SageMaker managed MLflow tracking server will incur costs until you delete or stop it. Billing for tracking servers is based on the duration the servers have been running, the size selected, and the amount of data logged to the tracking servers. You can stop tracking servers when theyâ€™re not in use to save costs, or you can delete them using API or the SageMaker Studio UI. For more details on pricing, refer to Amazon SageMaker pricing. 
Conclusion 
Fully managed MLflow 3.0 on Amazon SageMaker AI is now available. Get started with sample code in the aws-samples GitHub repository. We invite you to explore this new capability and experience the enhanced efficiency and control it brings to your ML projects. To learn more, visit Machine Learning Experiments using Amazon SageMaker with MLflow. 
For more information, visit the SageMaker Developer Guide and send feedback to AWS re:Post for SageMaker or through your usual AWS Support contacts. 
 
About the authors 
Ram Vittal&nbsp;is a Principal ML Solutions Architect at AWS. He has over 3 decades of experience architecting and building distributed, hybrid, and cloud applications. He is passionate about building secure, scalable, reliable AI/ML and big data solutions to help enterprise customers with their cloud adoption and optimization journey to improve their business outcomes. In his spare time, he rides motorcycle and walks with his three-year old sheep-a-doodle! 
Sandeep Raveesh is a GenAI Specialist Solutions Architect at AWS. He works with customer through their AIOps journey across model training, Retrieval-Augmented-Generation (RAG), GenAI Agents, and scaling GenAI use-cases. He also focuses on Go-To-Market strategies helping AWS build and align products to solve industry challenges in the GenerativeAI space. You can find Sandeep on&nbsp;LinkedIn. 
Amit Modi is the product leader for SageMaker AIOps and Governance, and Responsible AI at AWS. With over a decade of B2B experience, he builds scalable products and teams that drive innovation and deliver value to customers globally. 
Rahul Easwar is a Senior Product Manager at AWS, leading managed MLflow and Partner AI Apps within the SageMaker AIOps team. With over 15 years of experience spanning startups to enterprise technology, he leverages his entrepreneurial background and MBA from Chicago Booth to build scalable ML platforms that simplify AI adoption for organizations worldwide. Connect with Rahul on LinkedIn to learn more about his work in ML platforms and enterprise AI solutions.

â¸»