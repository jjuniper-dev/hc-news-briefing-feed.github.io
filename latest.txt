‚úÖ Morning News Briefing ‚Äì October 02, 2025 10:41

üìÖ Date: 2025-10-02 10:41
üè∑Ô∏è Tags: #briefing #ai #publichealth #digitalgov

‚∏ª

üßæ Weather
‚Ä¢ No watches or warnings in effect, Pembroke
  No watches or warnings in effect. No warnings or watches or watches in effect . Watch or warnings are no longer in effect in the U.S. No watches, warnings are in effect for the rest of the day . No watches and warnings are still in effect, but no watches are in place for the day's events . The weather is not expected to be affected by the weather .
‚Ä¢ Current Conditions:  -0.5¬∞C
  Observed at: Pembroke 6:00 AM EDT Thursday 2 October 2025 Temperature: -0.5&deg;C Pressure / Tendency: 103.2 kPa rising Humidity: 98 % Humidity : 98 % Dewpoint: - 0.8&deg:C Wind: W calm km/h . Air Quality Health Index: n/a . Air
‚Ä¢ Thursday: Sunny. High 19.
  Sunny. Sunny. High 19. UV index 5 or moderate . Sunny . Sunny. Humid cloudy. Sunny . Humid clouds . Sunny, breezy . Sunny morning . Sunny afternoon . Sunny night . Sunny day . Sunny and breezy evening . Sunny evening. Sunny day. Sunny night night. Sunny afternoon night. Humiliene afternoon. Sunny and sunny day for the rest

üåç International News
No updates.

üçÅ Canadian News
No updates.

üá∫üá∏ U.S. Top Stories
‚Ä¢ Research, curriculum and grading: new data sheds light on how professors are using AI
  More professors are using AI in the classroom . They say more guidance is needed on how to use the technology . Students say they want to be able to use AI to help students understand their subjects . Professor: More guidance needed to be given to students about how to take advantage of AI in classrooms . For more information, visit CNN iReport.com/professorshire.co.uk
‚Ä¢ From Madagascar to Morocco: Gen Z protests shake Africa
  Gen Z-led protests in two diverse African countries highlight frustration over years of poor governance . Fueled by social media, these youth movements are demanding accountability . The youth movement is demanding accountability in Africa's most populous countries . The protests are in response to the government's failure to meet Gen Z's demands for accountability in recent years . It is the latest in a series of social media protests
‚Ä¢ Furloughs, closures and mass firings threats: What's next in the shutdown fight
  On the first day of the government shutdown, Republicans and Democrats traded blame while a small bipartisan group of senators began to negotiate . The shutdown is the first of the year and the government is now in shutdown mode . The government shutdown began at 1.30am on January 1, 2013, with the shutdown set to last until January 31, 2013 . The federal government has been in shutdown since
‚Ä¢ Air traffic controllers helped end the last government shutdown, and may again
  A shortage of air traffic controllers may have played a role in ending the last government shutdown in 2019 . U.S. airlines are once again bracing for possible delays in commercial aviation . A shortage may have been a factor in the end of the government shutdown, according to CNN.com's Brandon Bell.com.com/Travel.com for more information about the shutdown.com .
‚Ä¢ What Mississippi's infant mortality crisis says about the risks of Medicaid cuts
  Mississippi recently declared a public health emergency because its infant mortality rate has surged . With Medicaid cuts coming, experts fear the crisis may worsen in other states . Experts fear that Medicaid cuts may worsen the crisis with other states also on the rise . Mississippi's infant mortality has surged in recent years, and experts fear that it will continue to rise in the coming years, as Medicaid cuts are coming .

üß† Artificial Intelligence
No updates.

üíª Digital Strategy
‚Ä¢ Avio bags ‚Ç¨40M ESA contract for reusable rocket stage, but don't hold your breath
  Italian rocket company Avio has signed a ‚Ç¨40 million contract with the European Space Agency to develop a reusable upper stage . The project is already drawing skepticism from industry watchers . Industry insiders whisper more about posturing than practical progress . The contract is expected to be worth around $100 million to Avio, which is worth around ‚Ç¨400 million, according to the company's website . The
‚Ä¢ Cybercrims claim raid on 28,000 Red Hat repos, say they have sensitive customer files
  Crimson Collective claims to have broken into Red Hat's GitHub repositories . 570GB of compressed data was exfiltrated, including sensitive documents belonging to customers . Red Hat says Crimson Collective broke into its private GitHub repositories to steal the data . The Crimson Collective says the data was stolen by the Crimson Collective . The group claims to be a hacker collective of hackers who broke into the company's GitHub
‚Ä¢ Ionos customers fume at mid-contract Plesk hike
  Web host blames partner's license fee increase, but users say notice was too short and terms unfair . Ionos is hiking the price of its server instances, blaming an increase in Plesk license costs . Customers have a month to accept the increase or else disable Plesk on their account . Users say notice of the increase is too short, terms unfair and unfair to the host host company .
‚Ä¢ Irony alert: UK.gov Work dept hires IBM to aid AI projects
  UK's pensions and benefits department has awarded IBM a contract that's worth up to ¬£27 million . IBM will explore, deploy and support AI technologies to enhance its services . AI technology promises employment extinction for humanity for humanity . Some Big Blue sky thinking needed for tech that promises employment elimination for humanity that promises jobs extinction for humans . IBM's contract is worth ¬£27million to explore and support
‚Ä¢ Lloyds Banking Group says 'digitization' will power more branch closures
  Group promises sandboxing of AI money management tools with 1,000 branches remaining . Lloyds Banking Group promises to continue to use ‚Äòdigitization‚Äô to power a program of branch closures . Bank says it will continue using ‚Äòdeteriorization‚Äù to power closing program of bank branches . Bank has ¬£18.67 billion turnover UK-based bank says it has

üè• Public Health
No updates.

üî¨ Science
‚Ä¢ The impact of confounders, spillovers and interactions on social distancing policy effects estimates
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Unmasking hidden cardiovascular risk: masked hypertension, aortic stiffness and cardiac remodeling in women with prior preeclampsia
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Monitoring changes in vitamin D levels during the COVID-19 pandemic with routinely-collected laboratory data
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Nurturing care group approach for improving animal faeces management in Ghana
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ The role of the patient in rheumatology
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations

üßæ Government & Policy
No updates.

üèõÔ∏è Enterprise Architecture & IT Governance
No updates.

ü§ñ AI & Emerging Tech
‚Ä¢ EV tax credits are dead in the US. Now what?
  On Wednesday, federal EV tax credits in the US officially came to an end.



Those credits, expanded and extended in the 2022 Inflation Reduction Act, gave drivers up to $7,500 in credits toward the purchase of a new electric vehicle. They‚Äôve been a major force in cutting the up-front costs of EVs, pushing more people toward purchasing them and giving automakers confidence that demand would be strong.





The tax credits‚Äô demise comes at a time when battery-electric vehicles still make up a small percentage of new vehicle sales in the country. And transportation is a major contributor to US climate pollution, with cars, trucks, ships, trains, and planes together making up roughly 30% of total greenhouse-gas emissions.



To anticipate what‚Äôs next for the US EV market, we can look to countries like Germany, which have ended similar subsidy programs. (Spoiler alert: It‚Äôs probably going to be a rough end to the year.)



When you factor in fuel savings, the lifetime cost of an EV can already be lower than that of a gas-powered vehicle today. But EVs can have a higher up-front cost, which is why some governments offer a tax credit or rebate that can help boost adoption for the technology.



In 2016, Germany kicked off a national incentive program to encourage EV sales. While the program was active, drivers could get grants of up to about ‚Ç¨6,000 toward the purchase of a new battery-electric or plug-in hybrid vehicle.



Eventually, the government began pulling back the credits. Support for plug-in hybrids ended in 2022, and commercial buyers lost eligibility in September 2023. Then the entire program came to a screeching halt in December 2023, when the government announced it would be ending the incentives with about one week‚Äôs notice.



Monthly sales data shows the fingerprints of those changes. In each case where there‚Äôs a contraction of public support, there‚Äôs a peak in sales just before a cutback, then a crash after. These short-term effects can be dramatic: There were about half as many battery-electric vehicles sold in Germany in January 2024 than there were in December 2023.¬†





We‚Äôre already seeing the first half of this sort of boom-bust cycle in the US: EV sales ticked up in August, making up about 10% of all new vehicle sales, and analysts say September will turn out to be a record-breaking month. People rushed to take advantage of the credits while they still could.



Next comes the crash‚Äîthe next few months will probably be very slow for EVs. One analyst predicted to the Washington Post that the figure could plummet to the low single digits, ‚Äúlike 1 or 2%.‚Äù



Ultimately, it‚Äôs not terribly surprising that there are local effects around these policy changes. ‚ÄúThe question is really how long this decline will last, and how slowly any recovery in the growth will be,‚Äù Robbie Andrew, a senior researcher at the CICERO Center for International Climate Research in Norway who collects EV sales data, said in an email.&nbsp;



When I spoke to experts (including Andrew) for a story last year, several told me that Germany‚Äôs subsidies were ending too soon, and that they were concerned about what cutting off support early would mean for the long-term prospects of the technology in the country. And Germany was much further along than the US, with EVs making up 20% of new vehicle sales‚Äîtwice the American proportion.



EV growth did see a longer-term backslide in Germany after the end of the subsidies. Battery-electric vehicles made up 13.5% of new registrations in 2024, down from 18.5% the year before, and the UK also passed Germany to become Europe‚Äôs largest EV market.¬†



Things have improved this year, with sales in the first half beating records set in 2023. But growth would need to pick up significantly for Germany to reach its goal of getting 15 million battery-electric vehicles registered in the country by 2030. As of January 2025, that number was just 1.65 million.¬†



According to early projections, the end of tax credits in the US could significantly slow progress on EVs and, by extension, on cutting emissions. Sales of battery-electric vehicles could be about 40% lower in 2030 without the credits than what we‚Äôd see with them, according to one analysis by Princeton University‚Äôs Zero Lab.



Some US states still have their own incentive programs for people looking to buy electric vehicles. But without federal support, the US is likely to continue lagging behind global EV leaders like China.¬†



As Andrew put it: ‚ÄúFrom a climate perspective, with road transport responsible for almost a quarter of US total emissions, leaving the low-hanging fruit on the tree is a significant setback.‚Äù&nbsp;



This article is from The Spark, MIT Technology Review‚Äôs weekly climate newsletter. To receive it in your inbox every Wednesday, sign up here.
‚Ä¢ Turning migration into modernization
  In late 2023, a long-trusted virtualization staple became the biggest open question on the enterprise IT roadmap . CIOs contending with pricing hikes and product roadmap opacity face a daunting choice: double‚Äëdown on a familiar but costlier stack, or use the disruption to rethink how‚Äîand where‚Äîcritical workloads should run . Forrester predicted that one in five large VMware customers would begin moving away from the platform in 2024 .
‚Ä¢ Roundtables: Trump‚Äôs Impact on the Next Generation of Innovators
  MIT Technology Review has honored dozens of young researchers on our Innovators Under 35 list . We checked back in with recent honorees to see how they‚Äôre faring amid sweeping changes . Learn about the complex realities of what life has been like for those aiming to build their labs and companies in today‚Äôs political climate . This was the third event in a special, three-part Roundtables series that also included: The Future of Birth Control Control .
‚Ä¢ Unlocking AI‚Äôs full potential requires operational excellence
  Talk of AI is inescapable. It‚Äôs often the main topic of discussion at board and executive meetings, at corporate retreats, and in the media. A record 58% of S&amp;P 500 companies mentioned AI in their second-quarter earnings calls, according to Goldman Sachs.







But it‚Äôs difficult to walk the talk. Just 5% of generative AI pilots are driving measurable profit-and-loss impact, according to a recent MIT study. That means 95% of generative AI pilots are realizing zero return, despite significant attention and investment.



Although we‚Äôre nearly three years past the watershed moment of ChatGPT‚Äôs public release, the vast majority of organizations are stalling out in AI. Something is broken. What is it?



Date from Lucid‚Äôs AI readiness survey sheds some light on the tripwires that are making organizations stumble. Fortunately, solving these problems doesn‚Äôt require recruiting top AI talent worth hundreds of millions of dollars, at least for most companies. Instead, as they race to implement AI quickly and successfully, leaders need to bring greater rigor and structure to their operational processes.



Operations are the gap between AI&#8217;s promise and practical adoption



I can‚Äôt fault any leader for moving as fast as possible with their implementation of AI. In many cases, the existential survival of their company‚Äîand their own employment‚Äîdepends on it. The promised benefits to improve productivity, reduce costs, and enhance communication are transformational, which is why speed is paramount.



But while moving quickly, leaders are skipping foundational steps required for any technology implementation to be successful. Our survey research found that more than 60% of knowledge workers believe their organization‚Äôs AI strategy is only somewhat to not at all well aligned with operational capabilities.



AI can process unstructured data, but AI will only create more headaches for unstructured organizations. As Bill Gates said, ‚ÄúThe first rule of any technology used in a business is that automation applied to an efficient operation will magnify the efficiency. The second is that automation applied to an inefficient operation will magnify the inefficiency.‚Äù



Where are the operations gaps in AI implementations? Our survey found that approximately half of respondents (49%) cite undocumented or ad-hoc processes impacting efficiency sometimes; 22% say this happens often or always.



The primary challenge of AI transformation lies not in the technology itself, but in the final step of integrating it into daily workflows. We can compare this to the &#8220;last mile problem&#8221; in logistics: The most difficult part of a delivery is getting the product to the customer, no matter how efficient the rest of the process is.



In AI, the &#8220;last mile&#8221; is the crucial task of embedding AI into real-world business operations. Organizations have access to powerful models but struggle to connect them to the people who need to use them. The power of AI is wasted if it&#8217;s not effectively integrated into business operations, and that requires clear documentation of those operations.



Capturing, documenting, and distributing knowledge at scale is critical to organizational success with AI. Yet our survey showed only 16% of respondents say their workflows are extremely well-documented. The top barriers to proper documentation are a lack of time, cited by 40% of respondents, and a lack of tools, cited by 30%.



The challenge of integrating new technology with old processes was perfectly illustrated in a recent meeting I had with a Fortune 500 executive. The company is pushing for significant productivity gains with AI, but it still relies on an outdated collaboration tool that was never designed for teamwork. This situation highlights the very challenge our survey uncovered: Powerful AI initiatives can stall if teams lack modern collaboration and documentation tools.



This disconnect shows that AI adoption is about more than just the technology itself. For it to truly succeed enterprise-wide, companies need to provide a unified space for teams to brainstorm, plan, document, and make decisions. The fundamentals of successful technology adoption still hold true: You need the right tools to enable collaboration and documentation for AI to truly make an impact.



Collaboration and change management are hidden blockers to AI implementation



A company&#8217;s approach to AI is perceived very differently depending on an employee&#8217;s role. While 61% of C-suite executives believe their company&#8217;s strategy is well-considered, that number drops to 49% for managers and just 36% for entry-level employees, as our survey found.



Just like with product development, building a successful AI strategy requires a structured approach. Leaders and teams need a collaborative space to come together, brainstorm, prioritize the most promising opportunities, and map out a clear path forward. As many companies have embraced hybrid or distributed work, supporting remote collaboration with digital tools becomes even more important.



We recently used AI to streamline a strategic challenge for our executive team. A product leader used it to generate a comprehensive preparatory memo in a fraction of the typical time, complete with summaries, benchmarks, and recommendations.



Despite this efficiency, the AI-generated document was merely the foundation. We still had to meet to debate the specifics, prioritize actions, assign ownership, and formally document our decisions and next steps.



According to our survey, 23% of respondents reported that collaboration is frequently a bottleneck in complex work. Employees are willing to embrace change, but friction from poor collaboration adds risk and reduces the potential impact of AI.



Operational readiness enhances your AI readiness



Operations lacking structure are preventing many organizations from implementing AI successfully. We asked teams about their top needs to help them adapt to AI. At the top of their lists were document collaboration (cited by 37% of respondents), process documentation (34%), and visual workflows (33%).



Notice that none of these requests are for more sophisticated AI. The technology is plenty capable already, and most organizations are still just scratching the surface of its full potential. Instead, what teams want most is ensuring the fundamentals around processes, documentation, and collaboration are covered.



AI offers a significant opportunity for organizations to gain a competitive edge in productivity and efficiency. But moving fast isn‚Äôt a guarantee of success. The companies best positioned for successful AI adoption are those that invest in operational excellence, down to the last mile.



This content was produced by Lucid Software. It was not written by MIT Technology Review‚Äôs editorial staff.
‚Ä¢ The Download: OpenAI‚Äôs caste bias problem, and how AI videos are made
  This is today&#8217;s edition of¬†The Download,¬†our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



OpenAI is huge in India. Its models are steeped in caste bias.



Caste bias is rampant in OpenAI‚Äôs products, including ChatGPT, according to an MIT Technology Review investigation. Though CEO Sam Altman boasted about India being its second-largest market during the launch of GPT-5 in August, we found that both this new model, which now powers ChatGPT, as well as Sora, OpenAI‚Äôs text-to-video generator, exhibit caste bias. This risks entrenching discriminatory views in ways that are currently going unaddressed.¬†



Mitigating caste bias in AI models is more pressing than ever. In contemporary India, many caste-oppressed Dalit people have escaped poverty and have become doctors, civil service officers, and scholars; some have even risen to become the president of India. But AI models continue to reproduce socioeconomic and occupational stereotypes that render Dalits as dirty, poor, and performing only menial jobs. Read the full story.



‚ÄîNilesh Christopher







MIT Technology Review Narrated: how do AI models generate videos?



It‚Äôs been a big year for video generation. The downside is that creators are competing with AI slop, and social media feeds are filling up with faked news footage. Video generation also uses up a huge amount of energy, many times more than text or image generation.With AI-generated videos everywhere, let&#8217;s take a moment to talk about the tech that makes them work.This is our latest story to be turned into a MIT Technology Review Narrated podcast, which we‚Äôre publishing each week on Spotify and Apple Podcasts. Just navigate to MIT Technology Review Narrated on either platform, and follow us to get all our new content as it‚Äôs released.







The must-reads



I‚Äôve combed the internet to find you today‚Äôs most fun/important/scary/fascinating stories about technology.



1 Taiwan has rejected America‚Äôs chip demandIt‚Äôs pushed back on a US request to move 50% of chip production to the States. (Bloomberg $)+ Taiwan said it never agreed to the commitment. (CNN)+ Taiwan‚Äôs ‚Äúsilicon shield‚Äù could be weakening. (MIT Technology Review)



2 Chatbots may not be eliminating jobs after allA new labor market study has found little evidence they‚Äôre putting humans out of work. (FT $)+ People are worried that AI will take everyone‚Äôs jobs. We‚Äôve been here before. (MIT Technology Review)



3 OpenAI has released a new Sora video appIt‚Äôs the latest in a long line of attempts to make AI a social experience. (Axios)+ Copyright holders will have to request the removal of their property. (WSJ $)



4 Scientists have made embryos from human skin cells for the first timeIt could allow people experiencing infertility and same-sex couples to have children. (BBC)+ How robots are changing the face of fertility science. (WP $)5 Elon Musk claims to be building a Wikipedia rivalWhich I‚Äôm sure will be entirely accurate and impartial. (Gizmodo)+ How AI and Wikipedia have sent vulnerable languages into a doom spiral. (MIT Technology Review)



6 America‚Äôs chips resurgence has been thrown into chaosAfter funding was yanked from the multi-billion dollar initiative designed to revive the industry. (Politico)



7 ICE wants to buy a phone location-tracking toolEven though it doesn‚Äôt have a warrant to do so. (404 Media)



8 The trouble with scaling up EV manufacturingSolid-state batteries are the holy grail‚Äîbut is full commercialization feasible? (Knowable Magazine)+ Why bigger EVs aren‚Äôt always better. (MIT Technology Review)



9 DoorDash‚Äôs food delivery robot is coming to Arizona‚Äôs roadsOthers before it have failed. Can Dot succeed? (TechCrunch)



10 What it‚Äôs like to give ChatGPT therapyIt‚Äôs very good at telling you what it thinks you want to hear. (New Yorker $)+ Therapists are secretly using ChatGPT. Clients are triggered. (MIT Technology Review)







Quote of the day



‚ÄúPlease treat adults like adults.&#8221;



‚ÄîAn X user reacts angrily to OpenAI‚Äôs moves to restrict the topics ChatGPT will discuss, Ars Technica reports.



¬†



One more thing







Africa fights rising hunger by looking to foods of the pastAfter falling steadily for decades, the prevalence of global hunger is now on the rise‚Äînowhere more so than in sub-Saharan Africa, thanks to conflicts, economic fallout from the covid-19 pandemic, and extreme weather events.Africa‚Äôs indigenous crops are often more nutritious and better suited to the hot and dry conditions that are becoming more prevalent, yet many have been neglected by science, which means they tend to be more vulnerable to diseases and pests and yield well below their theoretical potential.Now the question is whether researchers, governments, and farmers can work together in a way that gets these crops onto plates and provides Africans from all walks of life with the energy and nutrition that they need to thrive, whatever climate change throws their way. Read the full story.



‚ÄîJonathan W. Rosen







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)



+ The mighty Stonehenge is still keeping us guessing after all these years (4,600 of them).+ Bj√∂rk&#8217;s VR experience looks typically bonkers.+ We may finally have an explanation for the will-o‚Äô-the-wisp phenomenon.+ How to build your very own Commodore 64 Cartridge.

üîí Cybersecurity & Privacy
No updates.

üéì University AI
No updates.

üè¢ Corporate AI
‚Ä¢ How Hapag-Lloyd improved schedule reliability with ML-powered vessel schedule predictions using Amazon SageMaker
  This post is cowritten with Thomas Voss and Bernhard Hersberger from Hapag-Lloyd. 
Hapag-Lloyd is one of the world‚Äôs leading shipping companies with more than 308 modern vessels, 11.9 million TEUs (twenty-foot equivalent units) transported per year, and 16,700 motivated employees in more than 400 offices in 139 countries. They connect continents, businesses, and people through reliable container transportation services on the major trade routes across the globe. 
In this post, we share how Hapag-Lloyd developed and implemented a machine learning (ML)-powered assistant predicting vessel arrival and departure times that revolutionizes their schedule planning. By using Amazon SageMaker AI and implementing robust MLOps practices, Hapag-Lloyd has enhanced its schedule reliability‚Äîa key performance indicator in the industry and quality promise to their customers. 
For Hapag-Lloyd, accurate vessel schedule predictions are crucial for maintaining schedule reliability, where schedule reliability is defined as percentage of vessels arriving within 1 calendar day (earlier or later) of their estimated arrival time, communicated around 3 to 4 weeks before arrival. 
Prior to developing the new ML solution, Hapag-Lloyd relied on simple rule-based and statistical calculations, based on historical transit patterns for vessel schedule predictions. While this statistical method provided basic predictions, it couldn‚Äôt effectively account for real-time conditions such as port congestion, requiring significant manual intervention from operations teams. 
Developing a new ML solution to replace the existing system presented several key challenges: 
 
 Dynamic shipping conditions ‚Äì The estimated time of arrival (ETA) prediction model needs to account for numerous variables that affect journey duration, including weather conditions, port-related delays such as congestion, labor strikes, and unexpected events that force route changes. For example, when the Suez Canal was blocked by the Ever Given container ship in March 2021, vessels had to be rerouted around Africa, adding approximately 10 days to their journey times. 
 Data integration at scale ‚Äì The development of accurate models requires integration of large volumes of historical voyage data with external real-time data sources including port congestion information and vessel position tracking (AIS). The solution needs to scale across 120 vessel services or lines and 1,200 unique port-to-port routes. 
 Robust MLOps infrastructure ‚Äì A robust MLOps infrastructure is required to continuously monitor model performance and quickly deploy updates whenever needed. This includes capabilities for regular model retraining to adapt to changing patterns, comprehensive performance monitoring, and maintaining real-time inference capabilities for immediate schedule adjustments. 
 
Hapag-Llyod‚Äôs previous approach to schedule planning couldn‚Äôt effectively address these challenges. A comprehensive solution that could handle both the complexity of vessel schedule prediction and provide the infrastructure needed to sustain ML operations at global scale was needed. 
The Hapag-Lloyd network consists of over 308 vessels and many more partner vessels that continuously circumnavigate the globe on predefined service routes, resulting in more than 3,500 port arrivals per month. Each vessel operates on a fixed service line, making regular round trips between a sequence of ports. For instance, a vessel might repeatedly sail a route from Southampton to Le Havre, Rotterdam, Hamburg, New York, and Philadelphia before starting the cycle again. For each port arrival, an ETA must be provided multiple weeks in advance to arrange critical logistics, including berth windows at ports and onward transportation of containers by sea, land or air transport. The following table shows an example where a vessel travels from Southampton to New York through Le Havre, Rotterdam, and Hamburg. The vessel‚Äôs time until arrival at the New York port can be calculated as the sum of ocean to port time to Southampton, and the respective berth times and port-to-port times for the intermediate ports called while sailing to New York. If this vessel encounters a delay in Rotterdam, it affects its arrival in Hamburg and cascades through the entire schedule, impacting arrivals in New York and beyond as shown in the following table. This ripple effect can disrupt carefully planned transshipment connections and require extensive replanning of downstream operations. 
 
  
   
   Port 
   Terminal call 
   Scheduled arrival 
   Scheduled departure 
   
   
   SOUTHAMPTON 
   1 
   2025-07-29 07:00 
   2025-07-29 21:00 
   
   
   LE HAVRE 
   2 
   2025-07-30 16:00 
   2025-07-31 16:00 
   
   
   ROTTERDAM 
   3 
   2025-08-03 18:00 
   2025-08-05 03:00 
   
   
   HAMBURG 
   4 
   2025-08-07 07:00 
   2025-08-08 07:00 
   
   
   NEW YORK 
   5 
   2025-08-18 13:00 
   2025-08-21 13:00 
   
   
   PHILADELPHIA 
   6 
   2025-08-22 06:00 
   2025-08-24 16:30 
   
   
   SOUTHAMPTON 
   7 
   2025-09-01 08:00 
   2025-09-02 20:00 
   
  
 
When a vessel departs Rotterdam with a delay, new ETAs must be calculated for the remaining ports. For Hamburg, we only need to estimate the remaining sailing time from the vessel‚Äôs current position. However, for subsequent ports like New York, the prediction requires multiple components: the remaining sailing time to Hamburg, the duration of port operations in Hamburg, and the sailing time from Hamburg to New York. 
Solution overview 
As an input to the vessel ETA prediction, we process the following two data sources: 
 
 Hapag-Lloyd‚Äôs internal data, which is stored in a data lake. This includes detailed vessel schedules and routes, port and terminal performance information, real-time port congestion and waiting times, and vessel characteristics datasets. This data is prepared for model training using AWS Glue jobs. 
 Automatic Identification System (AIS) data, which provides streaming updates on the vessel movements. This AIS data ingestion is batched every 20 minutes using AWS Lambda and includes crucial information such as latitude, longitude, speed, and direction of vessels. New batches are processed using AWS Glue and Iceberg to update the existing AIS database‚Äîcurrently holding around 35 million observations. 
 
These data sources are combined to create training datasets for the ML models. We carefully consider the timing of available data through temporal splitting to avoid data leakage. Data leakage occurs when using information that wouldn‚Äôt be available at prediction time in the real world. For example, when training a model to predict arrival time in Hamburg for a vessel currently in Rotterdam, we can‚Äôt use actual transit times that were only known after the vessel reached Hamburg. 
A vessel‚Äôs journey can be divided into different legs, which led us to develop a multi-step solution using specialized ML models for each leg, which are orchestrated as hierarchical models to retrieve the overall ETA: 
 
 The Ocean to Port (O2P) model predicts the time needed for a vessel to reach its next port from its current position at sea. The model uses features such as remaining distance to destination, vessel speed, journey progress metrics, port congestion data, and historical sea leg durations. 
 The Port to Port (P2P) model forecasts sailing time between any two ports for a given date, considering key features such as ocean distance between ports, recent transit time trends, weather, and seasonal patterns. 
 The Berth Time model estimates how long a vessel will spend at port. The model uses vessel characteristics (such as tonnage and load capacity), planned container load, and historical port performance. 
 The Combined model takes as input the predictions from the O2P, P2P, and Berth Time models, along with the original schedule. Rather than predicting absolute arrival times, it computes the expected deviation from the original schedule by learning patterns in historical prediction accuracy and specific voyage conditions. These computed deviations are then used to update ETAs for the upcoming ports in a vessel‚Äôs schedule. 
 
 
All four models are trained using the XGBoost algorithm built into SageMaker, chosen for its ability to handle complex relationships in tabular data and its robust performance with mixed numerical and categorical features. Each model has a dedicated training pipeline in SageMaker Pipelines, handling data preprocessing steps and model training. The following diagram shows the data processing pipeline, which generates the input datasets for ML training. 
 
As an example, this diagram shows the training pipeline of the Berth model. The steps in the SageMaker training pipelines of the Berth, P2P, O2P, and Combined models are identical. Therefore, the training pipeline is implemented once as a blueprint and re-used across the other models, enabling a fast turn-around time of the implementation. 
 
Because the Combined model depends on outputs from the other three specialized models, we use AWS Step Functions to orchestrate the SageMaker pipelines for training. This helps ensure that the individual models are updated in the correct sequence and maintains prediction consistency across the system. The orchestration of the training pipelines is shown in the following pipeline architecture. 
 The individual workflow begins with a data processing pipeline that prepares the input data (vessel schedules, AIS data, port congestion, and port performance metrics) and splits it into dedicated datasets. This feeds into three parallel SageMaker training pipelines for our base models (O2P, P2P, and Berth), each following a standardized process of feature encoding, hyperparameter optimization, model evaluation, and registration using SageMaker Processing and hyperparameter turning jobs and SageMaker Model Registry. After training, each base model runs a SageMaker batch transform job to generate predictions that serve as input features for the combined model training. The performance of the latest Combined model version is tested on the last 3 months of data with known ETAs, and performance metrics (R¬≤, mean absolute error (MAE)) are computed. If the model‚Äôs performance is below a set MAE threshold, the entire training process fails and the model version is automatically discarded, preventing the deployment of models that don‚Äôt meet the minimum performance threshold. 
All four models are versioned and stored as separate model package groups in the SageMaker Model Registry, enabling systematic version control and deployment. This orchestrated approach helps ensure that our models are trained in the correct sequence using parallel processing, resulting in an efficient and maintainable training process.The hierarchical model approach helps further ensure that a degree of explainability comparable to the current statistical and rule-based solution is maintained‚Äîavoiding ML black box behavior. For example, it becomes possible to highlight unusually long berthing time predictions when discussing predictions results with business experts. This helps increase transparency and build trust, which in turn increases acceptance within the company. 
Inference solution walkthrough 
The inference infrastructure implements a hybrid approach combining batch processing with real-time API capabilities as shown in Figure 5. Because most data sources update daily and require extensive preprocessing, the core predictions are generated through nightly batch inference runs. These pre-computed predictions are complemented by a real-time API that implements business logic for schedule changes and ETA updates. 
 
 Daily batch Inference: 
   
   Amazon EventBridge triggers a Step Functions workflow every day. 
   The Step Functions workflow orchestrates the data and inference process: 
     
     Lambda copies internal Hapag-Lloyd data from the data lake to Amazon Simple Storage Service (Amazon S3). 
     AWS Glue jobs combine the different data sources and prepare inference inputs 
     SageMaker inference executes in sequence: 
       
       Fallback predictions are computed from historical averages and written to Amazon Relational Database Service (Amazon RDS). Fallback predictions are used in case of missing data or a downstream inference failure. 
       Preprocessing data for the four specialized ML models. 
       O2P, P2P, and Berth model batch transforms. 
       The Combined model batch transform generates final ETA predictions, which are written to Amazon RDS. 
       Input features and output files are stored in Amazon S3 for analytics and monitoring. 
        
      
   For operational reliability, any failures in the inference pipeline trigger immediate email notifications to the on-call operations team through Amazon Simple Email Service (Amazon SES). 
    
 Real-time API: 
   
   Amazon API Gateway receives client requests containing the current schedule and an indication for which vessel-port combinations an ETA update is required. By receiving the current schedule through the client request, we can take care of intraday schedule updates while doing daily batch transform updates. 
   The API Gateway triggers a Lambda function calculating the response. The Lambda function constructs the response by linking the ETA predictions (stored in Amazon RDS) with the current schedule using custom business logic, so that we can take care of short-term schedule changes unknown at inference time. Typical examples of short-term schedule changes are port omissions (for example, due to port congestion) and one-time port calls. 
    
 
This architecture enables millisecond response times to custom requests while achieving a 99.5% availability (a maximum 3.5 hours downtime per month). 
 
Conclusion 
Hapag Lloyd‚Äôs ML powered vessel scheduling assistant outperforms the current solution in both accuracy and response time. Typical API response times are in the order of hundreds of milliseconds, helping to ensure a real-time user experience and outperforming the current solution by more than 80%. Low response times are crucial because, in addition to fully automated schedule updates, business experts require low response times to work with the schedule assistant interactively. In terms of accuracy, the MAE of the ML-powered ETA predictions outperform the current solution by approximately 12%, which translates into climbing by two positions in the international ranking of schedule reliability on average. This is one of the key performance metrics in liner shipping, and this is a significant improvement within the industry. 
To learn more about architecting and governing ML workloads at scale on AWS, see the AWS blog post Governing the ML lifecycle at scale, Part 1: A framework for architecting ML workloads using Amazon SageMaker and the accompanying AWS workshop AWS Multi-Account Data &amp; ML Governance Workshop. 
Acknowledgement 
We acknowledge the significant and valuable work of Michal Papaj and Piotr Zielinski from Hapag-Lloyd in the data science and data engineering areas of the project. 
About the authors 
Thomas Voss Thomas Voss works at Hapag-Lloyd as a data scientist. With his background in academia and logistics, he takes pride in leveraging data science expertise to drive business innovation and growth through the practical design and modeling of AI solutions. 
Bernhard Hersberger Bernhard Hersberger works as a data scientist at Hapag-Lloyd, where he heads the AI Hub team in Hamburg. He is enthusiastic about integrating AI solutions across the company, taking comprehensive responsibility from identifying business issues to deploying and scaling AI solutions worldwide. 
Gabija Pasiunaite At AWS, Gabija Pasiunaite was a Machine Learning Engineer at AWS Professional Services based in Zurich. She specialized in building scalable ML and data solutions for AWS Enterprise customers, combining expertise in data engineering, ML automation and cloud infrastructure. Gabija has contributed to the AWS MLOps Framework used by AWS customers globally. Outside work, Gabija enjoys exploring new destinations and staying active through hiking, skiing, and running. 
Jean-Michel Lourier Jean-Michel Lourier is a Senior Data Scientist within AWS Professional Services. He leads teams implementing data driven applications side by side with AWS customers to generate business value out of their data. He‚Äôs passionate about diving into tech and learning about AI, machine learning, and their business applications. He is also an enthusiastic cyclist. 
Mousam Majhi Mousam Majhi is a Senior ProServe Cloud Architect focusing on Data &amp; AI within AWS Professional Services. He works with Manufacturing and Travel, Transportation &amp; Logistics customers in DACH to achieve their business outcomes by leveraging data and AI powered solutions. Outside of work, Mousam enjoys hiking in the Bavarian Alps.
‚Ä¢ Rox accelerates sales productivity with AI agents powered by Amazon Bedrock
  This post was co-written with Shriram Sridharan, Taeuk Kang, and Santhosh Kumar Manavasi Lakshminarayanan from Rox. 
Rox is building a new revenue operating system for the applied AI era. 
Modern revenue teams rely on more data than ever before, such as Customer Relationship Management (CRM) systems, marketing automation, finance systems, support tickets, and live product usage. Though each serves its role, together they create silos that slow sellers down and leave insights untapped. 
Rox addresses this by providing a revenue operating system: a unified layer that brings these signals together and equips AI agents to execute go-to-market (GTM) workflows. Instead of reconciling reports or updating fields, sellers get real-time intelligence and automation in their daily flow. 
Today, we‚Äôre excited to announce that Rox is generally available, with Rox infrastructure built on AWS and delivered across web, Slack, macOS, and iOS. In this post, we share how Rox accelerates sales productivity with AI agents powered by Amazon Bedrock. 
Solution overview 
As noted in Rox is transforming revenue teams with AI-driven integration powered by AWS, modern GTM teams need more than a static database. Revenue data spans dozens of systems, such as product usage, finance, and support, and teams require a system that unifies context and acts on it in real time. 
Rox delivers this through a layered architecture on AWS: 
 
 System of record ‚Äì A unified, governed knowledge graph consolidates CRM, finance, support, product telemetry, and web data 
 Agent swarms ‚Äì Intelligent, account-aware agents reason over the graph and orchestrate multi-step workflows like research, outreach, opportunity management, and proposal generation 
 Interfaces across surfaces ‚Äì Sellers engage these workflows where they work, such as web application, Slack, iOS, and macOS 
 
This converts the CRM from a passive system of record into an active system of action, so teams can act on their data immediately and intelligently. 
The following diagram illustrates the solution architecture. 
 
Benefits and features of ROX 
Now generally available, Rox extends from intelligence to full execution with Command, a new conversational interface that orchestrates multi-agent workflows. Command coordinates with multiple specialized agents running in parallel. A single request (for example, ‚Äúprep me for the ACME renewal and draft follow-ups‚Äù) expands into a plan: research usage and support signals, identify missing stakeholders, refresh enrichment, propose next-best actions, draft outreach, update the opportunity, and assemble a proposal. Each step is completed through tool calls into your systems and is subject to guardrail approvals. Our comprehensive safety architecture employs a sophisticated multi-layer guardrail system as the first line of defense against inappropriate, harmful, or malicious requests. Incoming requests undergo rigorous analysis through our advanced filtering mechanisms before reaching the inference layer. This preprocessing stage evaluates multiple dimensions of safety and appropriateness, such as legal compliance assessment and business relevance evaluation, to make sure only legitimate, safe, and contextually appropriate requests proceed to model execution. 
Command decomposes the request, routes steps to the right agents, sequences external tool invocations (CRM, calendar, enrichment, email), reconciles results into the system of context, and returns one coherent thread that‚Äôs ready for consumption on the web, Slack, iOS, or macOS. Every suggestion is explainable (sources and traces), reversible (audit logs), and policy-aware (role-based access control, rate limits, required approvals). 
How Amazon Bedrock powers Rox 
Command demands a model capable of reasoning across multiple steps, orchestrating tools, and adapting dynamically. 
To meet these needs, Rox chose Anthropic‚Äôs Claude Sonnet 4 on Amazon Bedrock. Anthropic‚Äôs Claude Sonnet 4 has consistently demonstrated unmatched tool-calling and reasoning performance, allowing Rox agents to sequence workflows like account research, enrichment, outreach, opportunity management, and proposal generation with reliability. 
Amazon Bedrock provides the foundation to deliver Rox at enterprise scale, offering security, flexibility to integrate with the latest models, and scalability to handle thousands of concurrent agents reliably. 
In addition to Command, Rox includes the following features: 
 
 Research ‚Äì Offers deep account and market research, grounded in unified context (carried over from private beta) 
 Meet ‚Äì Makes it possible to record, transcribe, summarize, and turn meetings into actions (carried over from private beta) 
 Outreach ‚Äì Provides personalized prospect engagement, contextualized by unified data (new) 
 Revenue ‚Äì Helps you track, update, and advance pipelines in the flow of work (new) 
 Auto-fill proposals ‚Äì Helps you assemble tailored proposals in seconds from account context (new) 
 Rox apps ‚Äì Offers modular extensions that add purpose-built workflows (dashboards, trackers) directly into the system (new) 
 iOS app ‚Äì Delivers notifications and meeting prep on the go (new) 
 Mac app ‚Äì Brings the ability to transcribe calls and add them to the system of context (new) 
 Regional expansion ‚Äì Now live in the AWS Middle East (Bahrain) AWS Region, aligning with data residency and sovereignty needs (new) 
 
Early customer impact 
In beta, enterprises saw immediate gains: 
 
 50% higher representative productivity 
 20% faster sales velocity 
 Twofold revenue per rep 
 
For example, real Rox customers were able to sharpen their focus on high-value opportunities, driving a 40‚Äì50% increase in average selling price. Another customer saw 90% reduction in rep prep time and faster closes, plus 15% more six-figure deals uncovered through Rox insights. Rox also shortens ramp time for new reps, with customers reporting 50% quicker ramp time using Rox. 
Try Rox today 
Our vision is for revenue teams to run with an always-on agent swarm that continuously researches accounts, engages stakeholders, and moves the pipeline forward. 
Rox is now generally available. Get started at rox.com or visit the AWS Marketplace. Together with AWS, we will continue to build the AI-based operating system for modern revenue teams. 
 
About the authors 
Shriram Sridharan&nbsp;is the Co-Founder/Engineering Head of Rox, a Sequoia backed AI company. Before Rox, Shriram led the data infrastructure team at Confluent responsible for making Kafka faster and cheaper across clouds. Prior to that he was one of the early engineers in Amazon Aurora (pre-launch) re-imagining databases for the cloud. Aurora was the fastest growing AWS Service and a recipient of the 2019 SIGMOD systems award. 
Taeuk Kang is a Founding Engineer at Rox, working across AI research and engineering. He studied Computer Science at Stanford. Prior to Rox, he built large language model agents and retrieval-augmented generation systems at X (formerly Twitter) and designed the distributed LLM infrastructure powering core product features and Trust &amp; Safety, improving overall platform health. Earlier at Stripe, he developed high-performance streaming and batch data processing pipelines integrating Apache Flink, Spark, Kafka, and AWS SQS. 
Santhosh Kumar Manavasi Lakshminarayanan&nbsp;leads Platform at Rox. Before Rox he was Director of Engineering at StreamSets, acquired by IBM leading StreamSets Cloud Platform making it seamless for big enterprises to run their data pipeline at scale on modern cloud providers. Before StreamSets, he was an senior engineer at Platform Metadata team at Informatica. 
Andrew Brown&nbsp;is an Account Executive for AI Startups at Amazon Web Services (AWS) in San Francisco, CA. With a strong background in cloud computing and a focus on supporting startups, Andrew specializes in helping companies scale their operations using AWS technologies. 
Santhan Pamulapati&nbsp;is a Sr. Solutions Architect for GenAI startups at AWS, with deep expertise in designing and building scalable solutions that drives customer growth. He has strong background in building HPC systems leveraging AWS services and worked with strategic customers to solve business challenges.
‚Ä¢ Modernize fraud prevention: GraphStorm v0.5 for real-time inference
  Fraud continues to&nbsp;cause significant financial damage globally, with U.S. consumers alone losing $12.5 billion in 2024‚Äîa 25% increase from the previous year according to the Federal Trade Commission. This surge stems not from more frequent attacks, but from fraudsters‚Äô increasing sophistication. As fraudulent activities become more complex and interconnected, conventional machine learning approaches fall short by analyzing transactions in isolation, unable to capture the networks of coordinated activities that characterize modern fraud schemes. 
Graph neural networks (GNNs) effectively address this challenge by modeling relationships between entities‚Äîsuch as users sharing devices, locations, or payment methods. By analyzing both network structures and entity attributes, GNNs&nbsp;are effective at identifying sophisticated fraud schemes where perpetrators mask individual suspicious activities but leave traces in their relationship networks. However, implementing GNN-based online fraud prevention in production environments presents unique challenges: achieving sub-second inference responses, scaling to billions of nodes and edges, and maintaining operational efficiency for model updates.&nbsp;In this post, we show you how to overcome these challenges using GraphStorm, particularly the new real-time inference capabilities of GraphStorm v0.5. 
Previous solutions required tradeoffs between capability and simplicity. Our initial DGL approach provided comprehensive real-time capabilities but demanded intricate service orchestration‚Äîincluding manually updating endpoint configurations and payload formats after retraining with new hyperparameters. This approach also lacked model flexibility, requiring customization of GNN models and configurations when using architectures beyond relational graph convolutional networks (RGCN). Subsequent in-memory DGL implementations reduced complexity but&nbsp;encountered scalability limitations with enterprise data volumes. We built GraphStorm to bridge this gap, by introducing distributed training and high-level APIs that help simplify GNN development at enterprise scale. 
In a recent blog post, we illustrated GraphStorm‚Äôs enterprise-scale GNN model training and offline inference capability and simplicity. While offline GNN fraud detection can identify fraudulent transactions after they occur‚Äîpreventing financial loss requires stopping fraud before it happens. GraphStorm v0.5&nbsp;makes this possible through native real-time inference support through Amazon SageMaker AI. GraphStorm v0.5&nbsp;delivers two innovations: streamlined endpoint deployment that reduces weeks of custom engineering‚Äîcoding SageMaker entry point files, packaging model artifacts, and calling SageMaker deployment APIs‚Äîto a single-command operation, and standardized payload specification that helps simplify client integration with real-time inference services. These capabilities enable sub-second node classification tasks like fraud prevention, empowering organizations to proactively counter fraud threat with scalable, operationally straightforward GNN solutions. 
To showcase these capabilities, this post presents a fraud prevention solution. Through this solution, we show how a data scientist can transition a trained GNN model to production-ready inference endpoints with minimal operational overhead. If you‚Äôre interested in implementing GNN-based models for real-time fraud prevention or similar business cases, you can adapt the approaches presented here to create your own solutions. 
Solution overview 
Our proposed solution is a 4-step pipeline as shown in the following figure. The pipeline starts at step 1 with transaction graph export from an online transaction processing (OLTP) graph database to scalable storage (Amazon Simple Storage Service (Amazon S3) or Amazon EFS), followed by distributed model training in step 2. Step 3 is GraphStorm v0.5‚Äôs simplified deployment process that creates SageMaker&nbsp;real-time inference endpoints with one command. After SageMaker AI has deployed the endpoint successfully, a client application integrates with the OLTP graph database that processes live transaction streams in step 4. By querying the graph database, the client prepares subgraphs around to-be predicted transactions, convert the subgraph into standardized payload format, and invoke deployed endpoint for real-time prediction. 
 
To provide concrete implementation details for each step in the real-time inference solution, we demonstrate the complete workflow using the publicly available IEEE-CIS fraud detection task. 
Note: This example uses a Jupyter notebook as the controller of the overall four-step pipeline for simplicity. For more production-ready design, see the architecture described in Build a GNN-based real-time fraud detection solution. 
Prerequisites 
To run this example, you need an AWS account&nbsp;that&nbsp;the example‚Äôs AWS Cloud Development Kit (AWS CDK) code uses to create required resources, including Amazon Virtual Private Cloud (Amazon VPC), an Amazon Neptune database, Amazon SageMaker AI,&nbsp;Amazon Elastic Container Registry (Amazon ECR), Amazon S3, and related roles and permission. 
Note: These resources incur costs during execution (approximately $6 per hour with default settings). Monitor usage carefully and review pricing pages for these services before proceeding. Follow cleanup instructions at the end to avoid ongoing charges. 
Hands-on example: Real-time fraud prevention with IEEE-CIS dataset 
All implementation code for this example, including Jupyter notebooks and supporting Python scripts, is available in our public repository. The repository provides a complete end-to-end implementation that you can directly execute and adapt for your own fraud prevention use cases. 
Dataset and task overview 
This example uses the IEEE-CIS fraud detection dataset, containing 500,000 anonymized transactions with approximately 3.5% fraudulent cases. The dataset includes 392 categorical and numerical features, with key attributes like card types, product types, addresses, and email domains forming the graph structure shown in the following figure. Each transaction (with an isFraud&nbsp;label) connects to Card Type, Location, Product Type, and Purchaser and Recipient email domain entities, creating a heterogeneous graph that enables GNN models to detect fraud patterns through entity relationships. 
 
Unlike our previous post that demonstrated GraphStorm plus Amazon Neptune Analytics for offline analysis workflows, this example uses&nbsp;a Neptune database as the OLTP graph store, optimized for the quick subgraph extraction required during real-time inference. Following the graph design, the tabular IEEE-CIS data is converted to a set CSV files compatible with Neptune database format, allowing direct loading into both the Neptune database and GraphStorm‚Äôs GNN model training pipeline with a single set of files. 
Step 0: Environment setup 
Step 0 establishes the running environment required for the four-step fraud prevention pipeline. Complete setup instructions are available in the implementation repository. 
To run the example solution, you need to deploy an AWS CloudFormation stack through the AWS CDK. This stack creates the Neptune DB instance, the VPC to place it in, and appropriate roles and security groups. It additionally creates a SageMaker AI notebook instance, from which you run the example notebooks&nbsp;that come with the repository. 
 
 git clone https://github.com/aws-samples/amazon-neptune-samples.git
cd neptune-database-graphstorm-online-inference/neptune-db-cdk
# Ensure you have CDK installed and have appropriate credentials set up
cdk deploy 
 
When deployment is finished (it takes approximately 10 minutes for required resources to be ready), the AWS CDK prints a few outputs, one of which is the name of the SageMaker notebook instance you use to run through the notebooks: 
 
 # Example output
NeptuneInfraStack.NotebookInstanceName = arn:aws:sagemaker:us-east-1:012345678912:notebook-instance/NeptuneNotebook-9KgSB9XXXXXX 
 
You can navigate to the SageMaker AI notebook UI, find the corresponding notebook instance, and select its Open Jupyterlab link to access the notebook. 
Alternatively, you can use the AWS Command Line Interface (AWS CLI) to get a pre-signed URL to access the notebook. You will need to replace the &lt;notebook-instance-name&gt; with the actual notebook instance name. 
 
 aws sagemaker create-presigned-notebook-instance-url --notebook-instance-name &lt;notebook-instance-name&gt; 
 
When you‚Äôre in the notebook instance web console, open the first notebook, 0-Data-Preparation.ipynb, to start going through the example. 
Step 1: Graph construction 
In the Notebook 0-Data-Preparation, you transform the tabular IEEE-CIS dataset into the heterogeneous graph structure shown in the figure at the start of this section. The provided Jupyter Notebook extracts entities from transaction features, creating Card Type nodes from card1‚Äìcard6 features, Purchaser and Recipient nodes from email domains, Product Type nodes from product codes, and Location nodes from geographic information. The transformation establishes relationships between transactions and these entities, generating graph data in Neptune import format for direct ingestion into the OLTP graph store. The create_neptune_db_data() function orchestrates this entity extraction and relationship creation process across all node types (which takes approximately 30 seconds). 
 
 GRAPH_NAME&nbsp;= "ieee-cis-fraud-detection"
PROCESSED_PREFIX&nbsp;= f"./{GRAPH_NAME}"
ID_COLS&nbsp;= "card1,card2,card3,card4,card5,card6,ProductCD,addr1,addr2,P_emaildomain,R_emaildomain"
CAT_COLS&nbsp;= "M1,M2,M3,M4,M5,M6,M7,M8,M9"
# Lists of columns to keep from each file
COLS_TO_KEEP&nbsp;= {
&nbsp;&nbsp; &nbsp;"transaction.csv": (
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;ID_COLS.split(",")
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;+ CAT_COLS.split(",")
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;+
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;# Numerical features without missing values
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;[f"C{idx}"&nbsp;for&nbsp;idx&nbsp;in&nbsp;range(1, 15)]
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;+ ["TransactionID", "TransactionAmt", "TransactionDT", "isFraud"]
&nbsp;&nbsp; &nbsp;),
&nbsp;&nbsp; &nbsp;"identity.csv": ["TransactionID", "DeviceType"],
}

create_neptune_db_data(
&nbsp;&nbsp; &nbsp;data_prefix="./input-data/",
&nbsp;&nbsp; &nbsp;output_prefix=PROCESSED_PREFIX,
&nbsp;&nbsp; &nbsp;id_cols=ID_COLS,
&nbsp;&nbsp; &nbsp;cat_cols=CAT_COLS,
&nbsp;&nbsp; &nbsp;cols_to_keep=COLS_TO_KEEP,
&nbsp;&nbsp; &nbsp;num_chunks=1,
) 
 
This notebook also generates the JSON configuration file required by GraphStorm‚Äôs GConstruct command and executes the graph construction process. This GConstruct command transforms the Neptune-formatted data into a distributed binary graph format optimized for GraphStorm‚Äôs training pipeline, which partitions the heterogeneous graph structure across compute nodes to enable scalable model training on industry-scale graphs (measured in billions of nodes and edges). For the IEEE-CIS data, the GConstruct command takes 90 seconds to complete. 
In the Notebook&nbsp;1-Load-Data-Into-Neptune-DB, you load the CSV data into the Neptune database instance (takes approximately 9 minutes), which makes them available for online inference. During online inference, after selecting a transaction node, you query the Neptune database to get the graph neighborhood of the target node, retrieving the features of every node in the neighborhood and the subgraph structure around the target. 
Step 2: Model training 
After you have converted the data into the distributed binary graph format, it‚Äôs time to train a GNN model. GraphStorm provides command-line scripts to train a model without writing code. In the Notebook 2-Model-Training, you&nbsp;train a GNN model using GraphStorm‚Äôs node classification command with configuration managed through YAML files. The baseline configuration defines a two-layer RGCN model with 128-dimensional hidden layers, training for 4 epochs with a 0.001 learning rate and 1024 batch size, which takes approximately 100 seconds for 1 epoch of model training and evaluation in an ml.m5.4xlarge instance. To improve fraud detection accuracy, the notebook provides more advanced model configurations like the command below. 
 
 !python -m&nbsp;graphstorm.run.gs_node_classification \
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; --workspace ./&nbsp;\
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; --part-config ieee_gs/ieee-cis.json \
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; --num-trainers 1&nbsp;\
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; --cf ieee_nc.yaml \
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; --eval-metric roc_auc \
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; --save-model-path ./model-simple/&nbsp;\
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; --topk-model-to-save 1&nbsp;\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;--imbalance-class-weights 0.1,1.0 
 
Arguments in this command address the dataset‚Äôs label imbalance challenge where only 3.5% of transactions are fraudulent by using AUC-ROC as the evaluation metric and using class weights. The command also saves the best-performing model along with essential configuration files required for endpoint deployment. Advanced configurations can further enhance model performance through techniques like HGT encoders, multi-head attention, and class-weighted cross entropy loss function, though these optimizations increase computational requirements. GraphStorm enables these changes through run time arguments and YAML configurations, reducing the need for code modifications. 
Step 3: Real-time endpoint deployment 
In the Notebook 3-GraphStorm-Endpoint-Deployment, you deploy the real-time endpoint through GraphStorm v0.5‚Äôs straightforward launch script. The deployment requires three model artifacts generated during training: the saved model file that contains weights, the updated graph construction JSON file with feature transformation metadata, and the runtime-updated training configuration YAML file. These artifacts enable GraphStorm to recreate the exact training configurations and model for consistent inference behavior. Notably, the updated graph construction JSON and training configuration YAML file contains crucial configurations that are essential for restoring the trained model on the endpoint and processing incoming request payloads. It is crucial to use the updated JSON and YAML files for endpoint deployment.GraphStorm uses SageMaker AI bring your own container (BYOC) to deploy a consistent inference environment. You need to build and push the GraphStorm real-time Docker image to Amazon ECR using the provided shell scripts. This containerized approach provides consistent runtime environments compatible with the SageMaker AI managed infrastructure. The Docker image contains the necessary dependencies for GraphStorm‚Äôs real-time inference capabilities on the deployment environment. 
To deploy the endpoint, you can use the GraphStorm-provided launch_realtime_endpoint.py script that helps you gather required artifacts and creates the necessary SageMaker AI resources to deploy an endpoint. The script accepts the Amazon ECR image URI, IAM role, model artifact paths, and S3 bucket configuration, automatically handling endpoint provisioning and configuration. By default, the script waits for endpoint deployment to be complete before exiting. When completed, it prints the name and AWS Region of the deployed endpoint for subsequent inference requests. You will need to replace the fields enclosed by &lt;&gt; with the actual values of your environment. 
 
 !python ~/graphstorm/sagemaker/launch/launch_realtime_endpoint.py \
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;--image-uri &lt;account_id&gt;.dkr.ecr.&lt;aws_region&gt;.amazonaws.com/graphstorm:sagemaker-endpoint-cpu \
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;--role arn:aws:iam::&lt;account_id&gt;:role/&lt;your_role&gt; \
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;--region &lt;aws_region&gt; \
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;--restore-model-path &lt;restore-model-path&gt;/models/epoch-1/ \
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;--model-yaml-config-file &lt;restore-model-path&gt;/models/GRAPHSTORM_RUNTIME_UPDATED_TRAINING_CONFIG.yaml \
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;--graph-json-config-file &lt;restore-model-path&gt;/models/data_transform_new.json \
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;--infer-task-type&nbsp;node_classification \
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;--upload-tarfile-s3 s3://&lt;cdk-created-bucket&gt; \
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;--model-name ieee-fraud-detect 
 
Step 4: Real-time inference 
In the Notebook 4-Sample-Graph-and-Invoke-Endpoint, you build a basic client application that integrates with the deployed GraphStorm endpoint to perform real-time fraud prevention on incoming transactions. The inference process accepts transaction data through standardized JSON payloads, executes node classification predictions in a few hundreds of milliseconds, and returns fraud probability scores that enable immediate decision-making. 
An end-to-end inference call for a node that already exists in the graph has three distinct stages: 
 
 Graph sampling from the Neptune database. For a given target node that already exists in the graph, retrieve its k-hop neighborhood with a fanout limit, that is, limiting the number of neighbors retrieved at each hop by a threshold. 
 Payload preparation for inference. Neptune returns graphs using GraphSON, a specialized JSON-like data format used to describe graph data. At this step, you need to convert the returned GraphSON to GraphStorm‚Äôs own JSON specification. This step is performed on the inference client, in this case a SageMaker notebook instance. 
 Model inference using a SageMaker endpoint. After the payload is prepared, you send an inference request to a SageMaker endpoint that has loaded a previously trained model snapshot. The endpoint receives the request, performs any feature transformations needed (such as converting categorical features to one-hot encoding), creates the binary graph representation in memory, and makes a prediction for the target node using the graph neighborhood and trained model weights. The response is encoded to JSON and sent back to the client. 
 
An example response from the endpoint would look like: 
 
 {'status_code': 200,
&nbsp;'request_uid': '877042dbc361fc33',
&nbsp;'message': 'Request&nbsp;processed&nbsp;successfully.',
&nbsp;'error': '',
&nbsp;'data': {
&nbsp;&nbsp; &nbsp;'results': [
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;{
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;'node_type': 'Transaction',
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;'node_id': '2991260',
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;'prediction': [0.995966911315918, 0.004033133387565613]
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;]
&nbsp;&nbsp; &nbsp;}
} 
 
The data of interest for the single transaction you made a prediction for are in the prediction key and corresponding node_id. The prediction gives you the raw scores the model produces for class 0 (legitimate) and class 1 (fraudulent) at the corresponding 0 and 1 indexes of the predictions list. In this example, the model marks the transaction as most likely legitimate. You can find the full GraphStorm response specification in the GraphStorm documentation. 
Complete implementation examples, including client code and payload specifications, are provided in the repository to guide integration with production systems. 
Clean up 
To stop accruing costs on your account, you need to delete the AWS resources that you created with the AWS CDK at the Environment Setup step. 
You must first delete the SageMaker endpoint created during the Step 3 for cdk destroy to complete.&nbsp;See the Delete Endpoints and Resources for more options to delete an endpoint. When done, you can run the following from the repository‚Äôs root: 
 
 cd&nbsp;neptune-database-graphstorm-online-inference/neptune-db-cdk
cdk destroy 
 
See the AWS CDK docs for more information about how to use cdk destroy, or see the CloudFormation docs for how to delete a stack from the console UI.&nbsp;By default, the cdk destroy command does not delete the model artifacts and processed graph data stored in the S3 bucket during the training and deployment process. You must remove them manually. See Deleting a general purpose bucket for information about how to empty and delete an S3 bucket the AWS CDK has created. 
Conclusion 
Graph neural networks address complex fraud prevention challenges by modeling relationships between entities that traditional machine learning approaches miss when analyzing transactions in isolation. GraphStorm v0.5&nbsp;helps simplify deployment of GNN real-time inference with one command for endpoint creation that previously required coordination of multiple services and a standardized payload specification that helps simplify client integration with real-time inference services. Organizations can now deploy enterprise-scale fraud prevention endpoints through streamlined commands that reduce custom engineering from weeks to single-command operations. 
To implement GNN-based fraud prevention with your own data: 
 
 Review the GraphStorm documentation for model configuration options and deployment specifications. 
 Adapt this IEEE-CIS example to your fraud prevention dataset by modifying the graph construction and feature engineering steps using the complete source code and tutorials available in our GitHub repository. 
 Access step-by-step implementation guidance to build production-ready fraud prevention solutions with GraphStorm v0.5‚Äôs enhanced capabilities using your enterprise data. 
 
 
 
About the authors 
Jian Zhang&nbsp;is a Senior Applied Scientist who has been using machine learning techniques to help customers solve various problems, such as fraud detection, decoration image generation, and more. He has successfully developed graph-based machine learning, particularly graph neural network, solutions for customers in China, the US, and Singapore. As an enlightener of AWS graph capabilities, Zhang has given many public presentations about GraphStorm, the GNN, the Deep Graph Library (DGL), Amazon Neptune, and other AWS services. 
Theodore Vasiloudis&nbsp;is a Senior Applied Scientist at AWS, where he works on distributed machine learning systems and algorithms. He led the development of GraphStorm Processing, the distributed graph processing library for GraphStorm and is a core developer for GraphStorm. He received his PhD in Computer Science from KTH Royal Institute of Technology, Stockholm, in 2019. 
Xiang Song&nbsp;is a Senior Applied Scientist at AWS AI Research and Education (AIRE), where he develops deep learning frameworks including GraphStorm, DGL, and DGL-KE. He led the development of Amazon Neptune ML, a new capability of Neptune that uses graph neural networks for graphs stored in graph database. He is now leading the development of GraphStorm, an open source graph machine learning framework for enterprise use cases. He received his PhD in computer systems and architecture at the Fudan University, Shanghai, in 2014. 
Florian Saupe&nbsp;is a Principal Technical Product Manager at AWS AI/ML research supporting science teams like the graph machine learning group, and ML Systems teams working on large scale distributed training, inference, and fault resilience. Before joining AWS, Florian lead technical product management for automated driving at Bosch, was a strategy consultant at McKinsey &amp; Company, and worked as a control systems and robotics scientist‚Äîa field in which he holds a PhD. 
Ozan Eken&nbsp;is a Product Manager at AWS, passionate about building cutting-edge Generative AI and Graph Analytics products. With a focus on simplifying complex data challenges, Ozan helps customers unlock deeper insights and accelerate innovation. Outside of work, he enjoys trying new foods, exploring different countries, and watching soccer.

‚∏ª