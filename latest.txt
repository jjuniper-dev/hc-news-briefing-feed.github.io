‚úÖ Morning News Briefing ‚Äì July 07, 2025 10:56

üìÖ Date: 2025-07-07 10:56
üè∑Ô∏è Tags: #briefing #ai #publichealth #digitalgov

‚∏ª

üßæ Weather
‚Ä¢ RAINFALL WARNING, Pembroke
  Persons in or near this area should be on the lookout for adverse weather conditions and take necessary safety precautions . People in or around this area are advised to take necessary precautions . Weather conditions are expected to worsen in the summer of 2025 . People should also be aware of the weather conditions in the area and take safety precautions if conditions worsen in this area . The weather is expected to deteriorate in
‚Ä¢ Current Conditions: Light Rainshower, 15.6¬∞C
  Light Rainshower was observed at Garrison Petawawa 6:00 AM EDT Monday 7 July 2025 . Temperature: 15.6&deg;C Pressure: 101.1 kPa Visibility: 3 km Humidity: 95 % Dewpoint: 14.8&deg:C Wind: NNW 4 km/h Air Quality Health Index: n/a . Rainfall:
‚Ä¢ Monday: Showers. High 21.
  Showers ending this afternoon then cloudy with 40 percent chance of showers . Risk of a thunderstorm this morning and early this afternoon . Local amount of rain expected to fall 10 to 20 mm. High 21. Humidex 27. UV index 5 or moderate. Showery weather expected to make it feel like it was in the middle of the day . Forecast issued 5:10

üåç International News
No updates.

üçÅ Canadian News
No updates.

üá∫üá∏ U.S. Top Stories
‚Ä¢ Why some see the dollar's drop as a sign America is losing its financial might
  The dollar posted its worst first-half of the year since 1973 . Investors wonder ‚Äî is it a sign that America is losing its financial standing? The dollar has posted its lowest first half of a year in more than 40 years . The dollar is now the world's largest dollar dollar in the world, according to the Wall Street Journal's Daniel Munoz . He asks: Is it a
‚Ä¢ 'The worst day of my life:' Texas' Hill Country reels as deaths rise due to floods
  Dozens of people have died in the Texas Hill Country . Scores of others are missing or unaccounted for . Rescue crews continue to search for victims . Those who survived are coping with the loss of those who have survived the tragedy . The area is in the midst of a deadly wildfire that swept across the country in the past two weeks . It is unclear how many people have been killed in the
‚Ä¢ Inside the evolution of Biosphere 2, from '90s punchline to scientific playground
  The venture, privately funded to start, is now run by the University of Arizona . Scientists are quietly plugging away at research they hope will help us all adapt to the Biosphere 1 ‚Äî that is Earth, and climate change we are causing to it . The venture is privately funded, and is now being funded by the university of Arizona, which runs the biosphere . The Biosphere
‚Ä¢ Are seed oils actually bad for your health? Here's the science behind the controversy
  Health Secretary RFK Jr. has said vegetable oils, like canola and soybean, are 'poisoning Americans' But many researchers say the evidence isn't there . So, what does the science say about seed oils? We ask you what you think the science says about the vegetable oils and the health of the plant is telling you to get rid of them . Have you ever
‚Ä¢ In the midst of grief, a woman is comforted by a stranger on the subway
  A few days after her sister died, Maureen Futtner was on the subway when a stranger asked how she was doing . The conversation that followed was one that she will forever cherish . Maureen will always remember her sister's death and her life with her family . She will always be able to share her story with CNN iReport.com . Back to the page you came from

üß† Artificial Intelligence
No updates.

üíª Digital Strategy
‚Ä¢ Ordnance Survey digs deep to prevent costly cable strikes
  Digital map of subterranean infrastructure promised in 2021 set to launch by year end . Ordnance Survey seeking tech supplier to help it obtain and manage data from utilities companies . Project aims to avoid damage to underground infrastructure, which costs around ¬£2.4 billion a year, which is worth ¬£1.4bn a year . Project is due to be completed by the end of the year .
‚Ä¢ TUPE or not TUPE? How AI and cloud are rewriting the rules of supplier transitions
  Few IT leaders or staffers realize just how much automation, AI, and cloud delivery are disrupting the legal and human frameworks that underpin outsourcing . The Transfer of Undertakings (Protection of Employment) Regulations 2006, better known as TUPE, is known as 'TUPE . Tips on who pays when staff don't transfer, when the regulations apply ... and when they don't
‚Ä¢ AI scores a huge own goal if you play up and play the game
  In human imagination, AIs have been good for two things: trying to take over, and loving a good game . A virtual environment makes a great de-hype advisor . IBM's Deep Blue beat Kasparov in 1997, disproving post-war AI thinkers' claim that true artificial intelligence would have arrived . AI thinkers took it almost for granted that once computers could beat humans at chess,
‚Ä¢ Yes, I wrote a very expensive bug. In my defense I was only seven years old at the time
  Monday morning brings many readers a return to the world of adults, which The Register marks by bringing you a new edition of Who, Me? It's the reader-contributed column in which you share stories of making mistakes for which you are somehow forgiven . Years later, deep into a great tech career, your fellow reader remains inspired by the forgiveness received after the error who you are forgiven
‚Ä¢ Airbus okays use of ‚ÄòTaxibot‚Äô to tow planes to the runway
  Airbus has certified a ‚ÄòTaxibot‚Äô to transport its single-aisle planes from stand to runway . Airlines get the chance to cool their jets rather than burn fuel on the ground . Airbus last week revealed it has certified it has a ‚ÄúTaxibat‚Äôs‚Äù to transport planes to the airport from stand-to-stand to runway.‚Ä¶‚Ä¶

üè• Public Health
No updates.

üî¨ Science
‚Ä¢ Effects of exercise on mobility, balance and gait in patients with the chronic stroke: a systematic review and meta-analysis
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ The predictive relationship between parents‚Äô perceptions of physical activity and children‚Äôs physical literacy
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Trends in meat consumption and its association with menarche timing: findings from CHNS 1997‚Äì2015
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Tocilizumab as a targeted immunomodulatory therapy in the management of severe respiratory illnesses: a multicenter cohort study of COVID-19 patients
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Explainable artificial intelligence driven insights into smoking prediction using machine learning and clinical parameters
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations

üßæ Government & Policy
No updates.

üèõÔ∏è Enterprise Architecture & IT Governance
No updates.

ü§ñ AI & Emerging Tech
‚Ä¢ The latest threat from the rise of Chinese manufacturing
  The findings a decade ago were, well, shocking. Mainstream economists had long argued that free trade was overall a good thing; though there might be some winners and losers, it would generally bring lower prices and widespread prosperity. Then, in 2013, a trio of academic researchers showed convincing evidence that increased trade with China beginning in the early 2000s and the resulting flood of cheap imports had been an unmitigated disaster for many US communities, destroying their manufacturing lifeblood.



The results of what in 2016 they called the ‚ÄúChina shock‚Äù were gut-wrenching: the loss of 1 million US manufacturing jobs and 2.4 million jobs in total by 2011. Worse, these losses were heavily concentrated in what the economists called ‚Äútrade-exposed‚Äù towns and cities (think furniture makers in North Carolina).



If in retrospect all that seems obvious, it‚Äôs only because the research by David Autor, an MIT labor economist, and his colleagues has become an accepted, albeit often distorted, political narrative these days: China destroyed all our manufacturing jobs! Though the nuances of the research are often ignored, the results help explain at least some of today&#8217;s political unrest. It‚Äôs reflected in rising calls for US protectionism, President Trump‚Äôs broad tariffs on imported goods, and nostalgia for the lost days of domestic manufacturing glory.



The impacts of the original China shock still scar much of the country. But Autor is now concerned about what he considers a far more urgent problem‚Äîwhat some are calling China shock 2.0. The US, he warns, is in danger of losing the next great manufacturing battle, this time over advanced technologies to make cars and planes as well as those enabling AI, quantum computing, and fusion energy.



Recently, I asked Autor about the lingering impacts of the China shock and the lessons it holds for today&#8217;s manufacturing challenges.



How are the impacts of the China shock still playing out? 



I have a recent paper looking at 20 years of data, from 2000 to 2019. We tried to ask two related questions. One, if you looked at the places that were most exposed, how have they adjusted? And then if you look to the people who are most exposed, how have they adjusted? And how do those two things relate to one anothe



It turns out you get two very different answers. If you look at places that were most exposed, they have been substantially transformed. Manufacturing, once it starts going down, never comes back. But after 2010, these trade-impacted local labor markets staged something of an employment recovery, such that employment has grown faster after 2010 in trade-exposed places than non-trade-exposed places because a lot of people have come in. But these are jobs mostly in low-wage sectors. They‚Äôre in K‚Äì12 education and non-traded health services. They‚Äôre in warehousing and logistics. They‚Äôre in hospitality and lodging and recreation, and so they‚Äôre lower-wage, non-manufacturing jobs. And they‚Äôre done by a really different set of people.



The growth in employment is among women, among native-born Hispanics, among foreign-born adults and a lot of young people. The recovery is staged by a very different group from the white and black men, but especially white men, who were most represented in manufacturing. They have not really participated in this renaissance.



Employment is growing, but are these areas prospering?



They have a lower wage structure: fewer high-wage jobs, more low-wage jobs. So they‚Äôre not, if your definition of prospering is rapidly rising incomes. But there‚Äôs a lot of employment growth. They‚Äôre not like ghost towns. But then if you look at the people who were most concentrated in manufacturing‚Äîmostly white, non-college, native-born men‚Äîthey have not prospered. Most of them have not transitioned from manufacturing to non-manufacturing.



One of the great surprises is everyone had believed that people would pull up stakes and move on. In fact, we find the opposite. People in the most adversely exposed places become less likely to leave. They have become less mobile. The presumption was that they would just relocate to find higher ground. And that is not at all what occurred.



What happened to the total number of manufacturing jobs?



There‚Äôs been no rebound. Once they go, they just keep going. If there is going to be new manufacturing, it won‚Äôt be in the sectors that were lost to China. Those were basically labor-intensive jobs, the kind of low-tech sectors that we will not be getting back. You know‚Äîcommodity furniture and assembly of things, shoes, construction material. The US wasn‚Äôt going to keep them forever, and once they‚Äôre gone, it‚Äôs very unlikely to get them back.



I know you‚Äôve written about this, but it‚Äôs not hard to draw a connection between the dynamics you‚Äôre describing‚Äîwhite-male manufacturing jobs going away and new jobs going to immigrants‚Äîand today‚Äôs political turmoil.



We have a paper about that called ‚ÄúImporting Political Polarization?‚Äù



How big a factor would you say it is in today‚Äôs political unrest?



I don‚Äôt want to say it‚Äôs the factor. The China trade shock was a catalyst, but there were lots of other things that were happening. It would be a vast oversimplification to say that it was the sole cause.



But most people don‚Äôt work in manufacturing anymore. Aren‚Äôt these impacts that you‚Äôre talking about, including the political unrest, disproportionate to the actual number of jobs lost?



These are jobs in places where manufacturing is the anchor activity. Manufacturing is very unevenly distributed. It‚Äôs not like grocery stores and hospitals that you find in every county. The impact of the China trade shock on these places was like dropping an economic bomb in the middle of downtown. If the China trade shock cost us a few million jobs, and these were all‚Äîyou know‚Äîpeople in groceries and retail and gas stations, in hospitality and in trucking, you wouldn‚Äôt really notice it that much. We lost lots of clerical workers over the last couple of decades. Nobody talks about a clerical shock. Why not? Well, there was never a clerical capital of America. Clerical workers are everywhere. If they decline, it doesn‚Äôt wipe out the entire basis of a place.



So it goes beyond the jobs. These places lost their identity.



Maybe. But it‚Äôs also the jobs. Manufacturing offered relatively high pay to non-college workers, especially non-college men. It was an anchor of a way of life.



And we‚Äôre still seeing the damage.



Yeah, absolutely. It‚Äôs been 20 years. What‚Äôs amazing is the degree of stasis among the people who are most exposed‚Äînot the places, but the people. Though it‚Äôs been 20 years, we‚Äôre still feeling the pain and the political impacts from this transition.



Clearly, it has now entered the national psyche. Even if it weren‚Äôt true, everyone now believes it to have been a really big deal, and they‚Äôre responding to it. It continues to drive policy, political resentments, maybe even out of proportion to its economic significance. It certainly has become mythological.



What worries you now?



We‚Äôre in the midst of a totally different competition with China now that‚Äôs much, much more important. Now we‚Äôre not talking about commodity furniture and tube socks. We‚Äôre talking about semiconductors and drones and aviation, electric vehicles, shipping, fusion power, quantum, AI, robotics. These are the sectors where the US still maintains competitiveness, but they‚Äôre extremely threatened. China‚Äôs capacity for high-tech, low-cost, incredibly fast, innovative manufacturing is just unbelievable. And the Trump administration is basically fighting the war of 20 years ago. The loss of those jobs, you know, was devastating to those places. It was not devastating to the US economy as a whole. If we lose Boeing, GM, and Apple and Intel‚Äîand that‚Äôs quite possible‚Äîthen that will be economically devastating.



I think some people are calling it China shock 2.0.



Yeah. And it‚Äôs well underway.



When we think about advanced manufacturing and why it‚Äôs important, it‚Äôs not so much about the number of jobs anymore, is it? Is it more about coming up with the next technologies?



It does create good jobs, but it‚Äôs about economic leadership. It‚Äôs about innovation. It‚Äôs about political leadership, and even standard setting for how the rest of the world works.



Should we just accept that manufacturing as a big source of jobs is in the past and move on?



No. It‚Äôs still 12 million jobs, right? Instead of the fantasy that we‚Äôre going to go back to 18 million or whatever‚Äîwe had, what, 17.7 million manufacturing jobs in 1999‚Äîwe should be worried about the fact that we‚Äôre going to end up at 6 million, that we‚Äôre going to lose 50% in the next decade. And that‚Äôs quite possible. And the Trump administration is doing a lot to help that process of loss along.



We have a labor market of over 160 million people, so it‚Äôs like 8% of employment. It‚Äôs not zero. So you should not think of it as too small to worry about it. It‚Äôs a lot of people; it‚Äôs a lot of jobs. But more important, it‚Äôs a lot of what has helped this country be a leader. So much innovation happens here, and so many of the things in which other countries are now innovating started here. It‚Äôs always been the case that the US tends to innovate in sectors and then lose them after a while and move on to the next thing. But at this point, it‚Äôs not clear that we‚Äôll be in the frontier of a lot of these sectors for much longer.



So we want to revive manufacturing, but the right kind‚Äîadvanced manufacturing?



The notion that we should be assembling iPhones in the United States, which Trump wants, is insane. Nobody wants to do that work. It‚Äôs horrible, tedious work. It pays very, very little. And if we actually did it here, it would make the iPhones 20% more expensive or more. Apple may very well decide to pay a 25% tariff rather than make the phones here. If Foxconn started doing iPhone assembly here, people would not be lining up for that job.



But at the same time, we do need new people coming into manufacturing.



But not that manufacturing. Not tedious, mind-numbing, eyestrain-inducing assembly.



We need them to do high-tech work. Manufacturing is a skilled activity. We need to build airplanes better. That takes a ton of expertise. Assembling iPhones does not.



What are your top priorities to head off China shock 2.0?



I would choose sectors that are important, and I would invest in them. I don‚Äôt think that tariffs are never justified, or industrial policies are never justified. I just don‚Äôt think protecting phone assembly is smart industrial policy. We really need to improve our ability to make semiconductors. I think that‚Äôs important. We need to remain competitive in the automobile sector‚Äîthat‚Äôs important. We need to improve aviation and drones. That‚Äôs important. We need to invest in fusion power. That‚Äôs important. We need to adopt robotics at scale and improve in that sector. That‚Äôs important. I could come up with 15 things where I think public money is justified, and I would be willing to tolerate protections for those sectors.



What are the lasting lessons of the China shock and the opening up of global trade in the 2000s?



We did it too fast. We didn‚Äôt do enough to support people, and we pretended it wasn‚Äôt going on.



When we started the China shock research back around 2011, we really didn‚Äôt know what we‚Äôd find, and so we were as surprised as anyone. But the work has changed our own way of thinking and, I think, has been constructive‚Äînot because it has caused everyone to do the right thing, but it at least caused people to start asking the right questions.



What do the findings tell us about China shock 2.0?



I think the US is handling that challenge badly. The problem is much more serious this time around. The truth is, we have a sense of what the threats are. And yet we‚Äôre not seemingly responding in a very constructive way. Although we now know how seriously we should take this, the problem is that it doesn‚Äôt seem to be generating very serious policy responses. We‚Äôre generating a lot of policy responses‚Äîthey‚Äôre just not serious ones.
‚Ä¢ The Download: India‚Äôs AI independence, and predicting future epidemics
  This is today&#8217;s edition of¬†The Download,¬†our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



Inside India‚Äôs scramble for AI independence



Despite its status as a global tech hub, India lags far behind the likes of the US and China when it comes to homegrown AI.That gap has opened largely because India has chronically underinvested in R&amp;D, institutions, and invention. Meanwhile, since no one native language is spoken by the majority of the population, training language models is far more complicated than it is elsewhere.



So when the open-source foundation model DeepSeek-R1 suddenly outperformed many global peers, it struck a nerve. This launch by a Chinese startup prompted Indian policymakers to confront just how far behind the country was in AI infrastructure‚Äîand how urgently it needed to respond. Read the full story.



‚ÄîShadma Shaikh







Job titles of the future: Pandemic oracle



Officially, Conor Browne is a biorisk consultant. Based in Belfast, Northern Ireland, he has advanced degrees in security studies and medical and business ethics, along with United Nations certifications in counterterrorism and conflict resolution.Early in the emergence of SARS-CoV-2, international energy conglomerates seeking expert guidance on navigating the potential turmoil in markets and transportation became his main clients.&nbsp;



Having studied the 2002 SARS outbreak, he predicted the exponential spread of the new airborne virus. In fact, he forecast the epidemic‚Äôs broadscale impact and its implications for business so accurately that he has come to be seen as a pandemic oracle. Read the full story.



‚ÄîBritta Shoot



This story is from the most recent print edition of MIT Technology Review, which explores power‚Äîwho has it, and who wants it. Subscribe here to receive future copies once they drop.







The must-reads



I‚Äôve combed the internet to find you today‚Äôs most fun/important/scary/fascinating stories about technology.



1 Donald Trump‚Äôs ‚Äòbig beautiful bill‚Äô has passed&nbsp;



Which is terrible news for the clean energy industry. (Vox)+ An energy-affordability crisis is looming in the US. (The Atlantic $)+ The President struck deals with House Republican holdouts to get it over the line. (WSJ $)+ The Trump administration has shut down more than 100 climate studies. (MIT Technology Review)



2 Daniel Gross is joining Meta‚Äôs superintelligence lab¬†He‚Äôs jumping ship from the startup he co-founded with Ilya Sutskever. (Bloomberg $)+ Sutskever is stepping into the CEO role in his absence. (TechCrunch)+ Here‚Äôs what we can infer from Meta‚Äôs recent hires. (Semafor)3 AI‚Äôs energy demands could destabilize the global supplyThat‚Äôs according to the head of the world‚Äôs largest transformer maker. (FT $)+ We did the math on AI‚Äôs energy footprint. Here‚Äôs the story you haven‚Äôt heard. (MIT Technology Review)4 Elon Musk is threatening to start his own political partyWould anyone vote for him, though? (WP $)+ You‚Äôd think his bruising experience in the White House would have put him off. (NY Mag $)



5 The US has lifted exports on chip design software to ChinaIt suggests that frosty relations between the nations may be thawing. (Reuters)



6 Trump officials are going after this ICE warning appBut lawyers say there‚Äôs nothing illegal about it. (Wired $)+ Downloads of ICEBlock are rising. (NBC News)



7 Wildfires are making it harder to monitor air pollutantsCurrent tracking technology isn‚Äôt built to accommodate shifting smoke. (Undark)+ How AI can help spot wildfires. (MIT Technology Review)



8 Apple‚Äôs iOS 26 software can detect nudity on FaceTime callsThe feature will pause the call and ask if you want to continue. (Gizmodo)



9 Threads has finally launched DMsBut users are arguing there should be a way to opt out of them entirely. (TechCrunch)



10 You can hire a robot to write a handwritten note Or, y‚Äôknow, pick up a pen and write it yourself. (Insider $)







Quote of the day



‚ÄúIt‚Äôs almost like we never even spoke.‚Äù



Richard Wilson, an online dater who is convinced his most recent love interest used a chatbot to converse with him online before they awkwardly met in person, tells the Washington Post about his disappointment.







One more thing







Deepfakes of your dead loved ones are a booming Chinese businessOnce a week, Sun Kai has a video call with his mother, and they discuss his day-to-day life. But Sun‚Äôs mother died five years ago, and the person he‚Äôs talking to isn‚Äôt actually a person, but a digital replica he made of her.There are plenty of people like Sun who want to use AI to preserve, animate, and interact with lost loved ones as they mourn and try to heal. The market is particularly strong in China, where at least half a dozen companies are now offering such technologies and thousands of people have already paid for them.But some question whether interacting with AI replicas of the dead is truly a healthy way to process grief, and it‚Äôs not entirely clear what the legal and ethical implications of this technology may be. Read the full story.



‚ÄîZeyi Yang







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)+ There‚Äôs nothing cooler than wooden interiors right now.+ Talented artist Ian Robinson creates beautiful paintings of people‚Äôs vinyl collections.+ You‚Äôll find me in every one of Europe‚Äôs top wine destinations this summer.+ Here‚Äôs everything you need to remember before Stranger Things returns this fall.
‚Ä¢ Inside India‚Äôs scramble for AI independence
  In Bengaluru, India, Adithya Kolavi felt a mix of excitement and validation as he watched DeepSeek unleash its disruptive language model on the world earlier this year. The Chinese technology rivaled the best of the West in terms of benchmarks, but it had been built with far less capital in far less time.&nbsp;



‚ÄúI thought: ‚ÄòThis is how we disrupt with less,‚Äô‚Äù says Kolavi, the 20-year-old founder of the Indian AI startup CognitiveLab. ‚ÄúIf DeepSeek could do it, why not us?‚Äù&nbsp;



But for Abhishek Upperwal, founder of Soket AI Labs and architect of one of India‚Äôs earliest efforts to develop a foundation model, the moment felt more bittersweet.&nbsp;



Upperwal‚Äôs model, called Pragna-1B, had struggled to stay afloat with tiny grants while he watched global peers raise millions. The multilingual model had a relatively modest 1.25 billion parameters and was designed to reduce the ‚Äúlanguage tax,‚Äù the extra costs that arise because India‚Äîunlike the US or even China‚Äîhas a multitude of languages to support. His team had trained it, but limited resources meant it couldn‚Äôt scale. As a result, he says, the project became a proof of concept rather than a product.&nbsp;



‚ÄúIf we had been funded two years ago, there‚Äôs a good chance we‚Äôd be the ones building what DeepSeek just released,‚Äù he says.



Kolavi‚Äôs enthusiasm and Upperwal‚Äôs dismay reflect the spectrum of emotions among India‚Äôs AI builders. Despite its status as a global tech hub, the country lags far behind the likes of the US and China when it comes to homegrown AI. That gap has opened largely because India has chronically underinvested in R&amp;D, institutions, and invention. Meanwhile, since no one native language is spoken by the majority of the population, training language models is far more complicated than it is elsewhere.&nbsp;





Historically known as the global back office for the software industry, India has a tech ecosystem that evolved with a services-first mindset. Giants like Infosys and TCS built their success on efficient software delivery, but invention was neither prioritized nor rewarded. Meanwhile, India‚Äôs R&amp;D spending hovered at just 0.65% of GDP ($25.4 billion) in 2024, far behind China‚Äôs 2.68% ($476.2 billion) and the US‚Äôs 3.5% ($962.3 billion). The muscle to invent and commercialize deep tech, from algorithms to chips, was just never built.



Isolated pockets of world-class research do exist within government agencies like the DRDO (Defense Research &amp; Development Organization) and ISRO (Indian Space Research Organization), but their breakthroughs rarely spill into civilian or commercial use. India lacks the bridges to connect risk-taking research to commercial pathways, the way DARPA does in the US. Meanwhile, much of India‚Äôs top talent migrates abroad, drawn to ecosystems that better understand and, crucially, fund deep tech.So when the open-source foundation model DeepSeek-R1 suddenly outperformed many global peers, it struck a nerve. This launch by a Chinese startup prompted Indian policymakers to confront just how far behind the country was in AI infrastructure, and how urgently it needed to respond.



India responds



In January 2025, 10 days after DeepSeek-R1‚Äôs launch, the Ministry of Electronics and Information Technology (MeitY) solicited proposals for India‚Äôs own foundation models, which are large AI models that can be adapted to a wide range of tasks. Its public tender invited private-sector cloud and data‚Äëcenter companies to reserve GPU compute capacity for government‚Äëled AI research.&nbsp;



Providers including Jio, Yotta, E2E Networks, Tata, AWS partners, and CDAC responded. Through this arrangement, MeitY suddenly had access to nearly 19,000 GPUs at subsidized rates, repurposed from private infrastructure and allocated specifically to foundational AI projects. This triggered a surge of proposals from companies wanting to build their own models.&nbsp;



Within two weeks, it had 67 proposals in hand. That number tripled by mid-March.&nbsp;



In April, the government announced plans to develop six large-scale models by the end of 2025, plus 18 additional AI applications targeting sectors like agriculture, education, and climate action. Most notably, it tapped Sarvam AI to build a 70-billion-parameter model optimized for Indian languages and needs.&nbsp;



For a nation long restricted by limited research infrastructure, things moved at record speed, marking a rare convergence of ambition, talent, and political will.



‚ÄúIndia could do a Mangalyaan in AI,‚Äù said Gautam Shroff of IIIT-Delhi, referencing the country‚Äôs cost-effective, and successful, Mars orbiter mission.&nbsp;



Jaspreet Bindra, cofounder of AI&amp;Beyond, an organization focused on teaching AI literacy, captured the urgency: ‚ÄúDeepSeek is probably the best thing that happened to India. It gave us a kick in the backside to stop talking and start doing something.‚Äù



The language problem



One of the most fundamental challenges in building foundational AI models for India is the country‚Äôs sheer linguistic diversity. With 22 official languages, hundreds of dialects, and millions of people who are multilingual, India poses a problem that few existing LLMs are equipped to handle.



Whereas a massive amount of high-quality web data is available in English, Indian languages collectively make up less than 1% of online content. The lack of digitized, labeled, and cleaned data in languages like Bhojpuri and Kannada makes it difficult to train LLMs that understand how Indians actually speak or search.



Global tokenizers, which break text into units a model can process, also perform poorly on many Indian scripts, misinterpreting characters or skipping some altogether. As a result, even when Indian languages are included in multilingual models, they‚Äôre often poorly understood and inaccurately generated.



And unlike OpenAI and DeepSeek, which achieved scale using structured English-language data, Indian teams often begin with fragmented and low-quality data sets encompassing dozens of Indian languages. This makes the early steps of training foundation models far more complex.





Nonetheless, a small but determined group of Indian builders is starting to shape the country‚Äôs AI future.



For example, Sarvam AI has created OpenHathi-Hi-v0.1, an open-source Hindi language model that shows the Indian AI field‚Äôs growing ability to address the country‚Äôs vast linguistic diversity. The model, built on Meta‚Äôs Llama 2 architecture, was trained on 40 billion tokens of Hindi and related Indian-language content, making it one of the largest open-source Hindi models available to date.



Pragna-1B, the multilingual model from Upperwal, is more evidence that India could solve for its own linguistic complexity. Trained on 300 billion tokens for just $250,000, it introduced a technique called ‚Äúbalanced tokenization‚Äù to address a unique challenge in Indian AI, enabling a 1.25-billion-parameter model to behave like a much larger one.The issue is that Indian languages use complex scripts and agglutinative grammar, where words are formed by stringing together many smaller units of meaning using prefixes and suffixes. Unlike English, which separates words with spaces and follows relatively simple structures, Indian languages like Hindi, Tamil, and Kannada often lack clear word boundaries and pack a lot of information into single words. Standard tokenizers struggle with such inputs. They end up breaking Indian words into too many tokens, which bloats the input and makes it harder for models to understand the meaning efficiently or respond accurately.



With the new technique, however, ‚Äúa billion-parameter model was equivalent to a 7 billion one like Llama 2,‚Äù Upperwal says. This performance was particularly marked in Hindi and Gujarati, where global models often underperform because of limited multilingual training data. It was a reminder that with smart engineering, small teams could still push boundaries.Upperwal eventually repurposed his core tech to build speech APIs for 22 Indian languages, a more immediate solution better suited to rural users who are often left out of English-first AI experiences.



‚ÄúIf the path to AGI is a hundred-step process, training a language model is just step one,‚Äù he says.&nbsp;



At the other end of the spectrum are startups with more audacious aims. Krutrim-2, for instance, is a 12-billion-parameter multilingual language model optimized for English and 22 Indian languages.&nbsp;



Krutrim-2 is attempting to solve India‚Äôs specific problems of linguistic diversity, low-quality data, and cost constraints. The team built a custom Indic tokenizer, optimized training infrastructure, and designed models for multimodal and voice-first use cases from the start, crucial in a country where text interfaces can be a problem.



Krutrim‚Äôs bet is that its approach will not only enable Indian AI sovereignty but also offer a model for AI that works across the Global South.



Besides public funding and compute infrastructure, India also needs the institutional support of talent, the research depth, and the long-horizon capital that produce globally competitive science.



While venture capital still hesitates to bet on research, new experiments are emerging. Paras Chopra, an entrepreneur who previously built and sold the software-as-a-service company Wingify, is now personally funding Lossfunk, a Bell Labs‚Äìstyle AI residency program designed to attract independent researchers with a taste for open-source science.&nbsp;



‚ÄúWe don‚Äôt have role models in academia or industry,‚Äù says Chopra. ‚ÄúSo we‚Äôre creating a space where top researchers can learn from each other and have startup-style equity upside.‚Äù



Government-backed bet on sovereign AI



The clearest marker of India‚Äôs AI ambitions came when the government selected Sarvam AI to develop a model focused on Indian languages and voice fluency.



The idea is that it would not only help Indian companies compete in the global AI arms race but benefit the wider population as well. ‚ÄúIf it becomes part of the India stack, you can educate hundreds of millions through conversational interfaces,‚Äù says Bindra.&nbsp;



Sarvam was given access to 4,096 Nvidia H100 GPUs for training a 70-billion-parameter Indian language model over six months. (The company previously released a 2-billion-parameter model trained in 10 Indian languages, called Sarvam-1.)



Sarvam‚Äôs project and others are part of a larger strategy called the IndiaAI Mission, a $1.25 billion national initiative launched in March 2024 to build out India‚Äôs core AI infrastructure and make advanced tools more widely accessible. Led by MeitY, the mission is focused on supporting AI startups, particularly those developing foundation models in Indian languages and applying AI to key sectors such as health care, education, and agriculture.



Under its compute program, the government is deploying more than 18,000 GPUs, including nearly 13,000 high-end H100 chips, to a select group of Indian startups that currently includes Sarvam, Upperwal‚Äôs Soket Labs, Gnani AI, and Gan AI.&nbsp;



The mission also includes plans to launch a national multilingual data set repository, establish AI labs in smaller cities, and fund deep-tech R&amp;D. The broader goal is to equip Indian developers with the infrastructure needed to build globally competitive AI and ensure that the results are grounded in the linguistic and cultural realities of India and the Global South.According to Abhishek Singh, CEO of IndiaAI and an officer with MeitY, India‚Äôs broader push into deep tech is expected to raise around $12 billion in research and development investment over the next five years.&nbsp;



This includes approximately $162 million through the IndiaAI Mission, with about $32 million earmarked for direct startup funding. The National Quantum Mission is contributing another $730 million to support India‚Äôs ambitions in quantum research. In addition to this, the national budget document for 2025-26 announced a $1.2 billion Deep Tech Fund of Funds aimed at catalyzing early-stage innovation in the private sector.



The rest, nearly $9.9 billion, is expected to come from private and international sources including corporate R&amp;D, venture capital firms, high-net-worth individuals, philanthropists, and global technology leaders such as Microsoft.&nbsp;



IndiaAI has now received more than 500 applications from startups proposing use cases in sectors like health, governance, and agriculture.&nbsp;



‚ÄúWe‚Äôve already announced support for Sarvam, and 10 to 12 more startups will be funded solely for foundational models,‚Äù says Singh. Selection criteria include access to training data, talent depth, sector fit, and scalability.



Open or closed?



The IndiaAI program, however, is not without controversy. Sarvam is being built as a closed model, not open-source, despite its public tech roots. That has sparked debate about the proper balance between private enterprise and the public good.&nbsp;



‚ÄúTrue sovereignty should be rooted in openness and transparency,‚Äù says Amlan Mohanty, an AI policy specialist. He points to DeepSeek-R1, which despite its 236-billion parameter size was made freely available for commercial use.&nbsp;





Its release allowed developers around the world to fine-tune it on low-cost GPUs, creating faster variants and extending its capabilities to non-English applications.



‚ÄúReleasing an open-weight model with efficient inference can democratize AI,‚Äù says Hancheng Cao, an assistant professor of information systems and operations management at Emory University. ‚ÄúIt makes it usable by developers who don‚Äôt have massive infrastructure.‚Äù



IndiaAI, however, has taken a neutral stance on whether publicly funded models should be open-source.&nbsp;



‚ÄúWe didn‚Äôt want to dictate business models,‚Äù says Singh. ‚ÄúIndia has always supported open standards and open source, but it‚Äôs up to the teams. The goal is strong Indian models, whatever the route.‚Äù



There are other challenges as well. In late May, Sarvam‚ÄØAI unveiled Sarvam‚ÄëM, a 24-billion-parameter multilingual LLM fine-tuned for 10 Indian languages and built on top of Mistral‚ÄØSmall, an efficient model developed by the French company Mistral AI. Sarvam‚Äôs cofounder Vivek‚ÄØRaghavan called the model ‚Äúan important stepping stone on our journey to build sovereign AI for India.‚Äù But its download numbers were underwhelming, with only 300 in the first two days. The venture capitalist Deedy Das called the launch ‚Äúembarrassing.‚ÄùAnd the issues go beyond the lukewarm early reception. Many developers in India still lack easy access to GPUs and the broader ecosystem for Indian-language AI applications is still nascent.&nbsp;



The compute question



Compute scarcity is emerging as one of the most significant bottlenecks in generative AI, not just in India but across the globe. For countries still heavily reliant on imported GPUs and lacking domestic fabrication capacity, the cost of building and running large models is often prohibitive.&nbsp;



India still imports most of its chips rather than producing them domestically, and training large models remains expensive. That‚Äôs why startups and researchers alike are focusing on software-level efficiencies that involve smaller models, better inference, and fine-tuning frameworks that optimize for performance on fewer GPUs.



‚ÄúThe absence of infrastructure doesn‚Äôt mean the absence of innovation,‚Äù says Cao. ‚ÄúSupporting optimization science is a smart way to work within constraints.‚Äù&nbsp;



Yet Singh of IndiaAI argues that the tide is turning on the infrastructure challenge thanks to the new government programs and private-public partnerships. ‚ÄúI believe that within the next three months, we will no longer face the kind of compute bottlenecks we saw last year,‚Äù he says.



India also has a cost advantage.According to Gupta, building a hyperscale data center in India costs about $5 million, roughly half what it would cost in markets like the US, Europe, or Singapore. That‚Äôs thanks to affordable land, lower construction and labor costs, and a large pool of skilled engineers.&nbsp;



For now, India‚Äôs AI ambitions seem less about leapfrogging OpenAI or DeepSeek and more about strategic self-determination. Whether its approach takes the form of smaller sovereign models, open ecosystems, or public-private hybrids, the country is betting that it can chart its own course.&nbsp;



While some experts argue that the government‚Äôs action, or reaction (to DeepSeek), is performative and aligned with its nationalistic agenda, many startup founders are energized. They see the growing collaboration between the state and the private sector as a real opportunity to overcome India&#8217;s long-standing structural challenges in tech innovation.



At a Meta summit held in Bengaluru last year, Nandan Nilekani, the chairman of Infosys, urged India to resist chasing a me-too AI dream.&nbsp;



‚ÄúLet the big boys in the Valley do it,‚Äù he said of building LLMs. ‚ÄúWe will use it to create synthetic data, build small language models quickly, and train them using appropriate data.‚Äù&nbsp;



His view that India should prioritize strength over spectacle had a divided reception. But it reflects a broader growing consensus on whether India should play a different game altogether.



‚ÄúTrying to dominate every layer of the stack isn‚Äôt realistic, even for China,‚Äù says Shobhankita Reddy, a researcher at the Takshashila Institution, an Indian public policy nonprofit. ‚ÄúDominate one layer, like applications, services, or talent, so you remain indispensable.‚Äù¬†



Correction: We amended Reddy&#8217;s name
‚Ä¢ The Download: AI agents hype, and Google‚Äôs electricity plans
  This is today&#8217;s edition of¬†The Download,¬†our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



Don‚Äôt let hype about AI agents get ahead of reality



‚ÄîYoav Shoham is a professor emeritus at Stanford University and cofounder of AI21 Labs.



At Google‚Äôs I/O 2025 event in May, the company showed off a digital assistant that didn‚Äôt just answer questions; it helped work on a bicycle repair by finding a matching user manual, locating a YouTube tutorial, and even calling a local store to ask about a part, all with minimal human nudging. Such capabilities could soon extend far outside the Google ecosystem.The vision is exciting: Intelligent software agents that act like digital coworkers, booking your flights, rescheduling meetings, filing expenses, and talking to each other behind the scenes to get things done.But if we‚Äôre not careful, we‚Äôre going to derail the whole idea before it has a chance to deliver real benefits. As with many tech trends, there‚Äôs a risk of hype racing ahead of reality. And when expectations get out of hand, a backlash isn‚Äôt far behind. Read the full story.







Google‚Äôs electricity demand is skyrocketing



We got two big pieces of energy news from Google this week. The company announced that it‚Äôs signed an agreement to purchase electricity from a fusion company‚Äôs forthcoming first power plant. Google also released its latest environmental report, which shows that its energy use from data centers has doubled since 2020.



Taken together, these two bits of news offer a fascinating look at just how desperately big tech companies are hunting for clean electricity to power their data centers as energy demand and emissions balloon in the age of AI. Of course, we don‚Äôt know exactly how much of this pollution is attributable to AI because Google doesn‚Äôt break that out. (Also a problem!) So, what‚Äôs next and what does this all mean?



‚ÄîCasey Crownhart



This article is from The Spark, MIT Technology Review‚Äôs weekly climate newsletter. To receive it in your inbox every Wednesday, sign up here.+ To read more about whether nuclear energy is really a viable way to power the AI boom, check out Casey‚Äôs recent article, which is part of Power Hungry: AI and our energy future‚Äîour new series shining a light on the energy demands and carbon costs of the artificial intelligence revolution. You can take a look at the rest of the package here.







The must-reads



I‚Äôve combed the internet to find you today‚Äôs most fun/important/scary/fascinating stories about technology.



1 Meta‚Äôs climate tool was ‚Äòtrained using faulty data‚ÄôScientists claim it raised false hopes about the feasibility of removing carbon dioxide from the atmosphere. (FT $)+ xAI‚Äôs gas turbines have been greenlit, despite community backlash. (Wired $)+ Why we need to shoot carbon dioxide thousands of feet underground. (MIT Technology Review)



2 We don‚Äôt know whether US insurers will cover vaccines for kidsMajor insurers haven‚Äôt confirmed whether they‚Äôll keep covering the costs of shots. (Wired $)+ What‚Äôs next for the Gates Foundation‚Äôs global health initiatives? (Undark)+ How measuring vaccine hesitancy could help health professionals tackle it. (MIT Technology Review)



3 The Trump administration wants to gut Biden‚Äôs climate lawThe Inflation Reduction Act‚Äôs green energy tax incentives are hanging in the balance. (WP $)+ It‚Äôs bad news for one of the US economy‚Äôs biggest growth sectors. (Vox)+ How are we going to feed the world without making climate change worse? (New Yorker $)+ The President threatened to unravel the landmark law long before he was elected. (MIT Technology Review)



4 There are certain tells a scientific study abstract has been written by AIUse of hundreds of words has shot up since ChatGPT was made public. (NYT $)+ Beware over-reliance on AI-text detection tools, though. (MIT Technology Review)



5 Elon Musk doesn‚Äôt care about cars any moreWhich is terrible news for Tesla and its investors. (WSJ $)+ Things aren‚Äôt looking too hot for Rivian, either. (Insider $)



6 America‚Äôs weather forecasting is getting worseJust a year ago, US storm forecasting was the best it had ever been. Now, its accuracy is rapidly declining. (The Atlantic $)



7 Brazil has sustainable data center ambitionsEnvironmentalists aren‚Äôt convinced, however. (Rest of World)



8 A mysterious object has been spotted passing through the solar systemAnd we‚Äôve got good reason to believe it originated outside our system. (Ars Technica)



9 A rising band on Spotify is probably AI-generatedBut no one seems able to say for sure. (Vice)



10 The homes float in flood waterIt‚Äôs one solution to building homes on known flood plains. (Fast Company $)+ How to stop a state from sinking. (MIT Technology Review)







Quote of the day



‚ÄúAI doesn‚Äôt know what an orgasm sounds like.‚Äù&nbsp;



‚ÄîAnnabelle Tudor, an audiobook narrator, tells the Guardian why she‚Äôs not convinced by the industry‚Äôs plans to have AI narrate audiobooks.







One more thing







Who gets to decide who receives experimental medical treatments?



There has been a trend toward lowering the bar for new medicines, and it is becoming easier for people to access treatments that might not help them‚Äîand could even harm them. Anecdotes appear to be overpowering evidence in decisions on drug approval. As a result, we‚Äôre ending up with some drugs that don‚Äôt work.



We urgently need to question how these decisions are made. Who should have access to experimental therapies? And who should get to decide? Such questions are especially pressing considering how quickly biotechnology is advancing. We‚Äôre not just improving on existing classes of treatments‚Äîwe‚Äôre creating entirely new ones. Read the full story.



‚ÄîJessica Hamzelou







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)



+ These aerial shots of Glastonbury festival are crazy.+ Our oceans really are amazing places‚Äîtake a moment to appreciate them.+ How to be truly cool, according to science.+ Happy 62nd birthday to Tracey Emin, still an enfant terrible after all these years.
‚Ä¢ Google‚Äôs electricity demand is skyrocketing
  We got two big pieces of energy news from Google this week. The company announced that it‚Äôs signed an agreement to purchase electricity from a fusion company‚Äôs forthcoming first power plant. Google also released its latest environmental report, which shows that its energy use from data centers has doubled since 2020.



Taken together, these two bits of news offer a fascinating look at just how desperately big tech companies are hunting for clean electricity to power their data centers as energy demand and emissions balloon in the age of AI. Of course, we don‚Äôt know exactly how much of this pollution is attributable to AI because Google doesn‚Äôt break that out. (Also a problem!) So, what‚Äôs next and what does this all mean?&nbsp;





Let‚Äôs start with fusion: Google‚Äôs deal with Commonwealth Fusion Systems is intended to provide the tech giant with 200 megawatts of power. This will come from Commonwealth‚Äôs first commercial plant, a facility planned for Virginia that the company refers to as the Arc power plant. The agreement represents half its capacity.



What‚Äôs important to note here is that this power plant doesn‚Äôt exist yet. In fact, Commonwealth still needs to get its Sparc demonstration reactor, located outside Boston, up and running. That site, which I visited in the fall, should be completed in 2026.



(An aside: This isn‚Äôt the first deal between Big Tech and a fusion company. Microsoft signed an agreement with Helion a couple of years ago to buy 50 megawatts of power from a planned power plant, scheduled to come online in 2028. Experts expressed skepticism in the wake of that deal, as my colleague James Temple reported.)



Nonetheless, Google‚Äôs announcement is a big moment for fusion, in part because of the size of the commitment and also because Commonwealth, a spinout company from MIT‚Äôs Plasma Science and Fusion Center, is seen by many in the industry as a likely candidate to be the first to get a commercial plant off the ground. (MIT Technology Review is owned by MIT but is editorially independent.)



Google leadership was very up-front about the length of the timeline. ‚ÄúWe would certainly put this in the long-term category,‚Äù said Michael Terrell, Google‚Äôs head of advanced energy, in a press call about the deal.



The news of Google‚Äôs foray into fusion comes just days after the tech giant‚Äôs release of its latest environmental report. While the company highlighted some wins, some of the numbers in this report are eye-catching, and not in a positive way.



Google‚Äôs emissions have increased by over 50% since 2019, rising 6% in the last year alone. That‚Äôs decidedly the wrong direction for a company that‚Äôs set a goal to reach net-zero greenhouse-gas emissions by the end of the decade.



It‚Äôs true that the company has committed billions to clean energy projects, including big investments in next-generation technologies like advanced nuclear and enhanced geothermal systems. Those deals have helped dampen emissions growth, but it‚Äôs an arguably impossible task to keep up with the energy demand the company is seeing.



Google‚Äôs electricity consumption from data centers was up 27% from the year before. It‚Äôs doubled since 2020, reaching over 30 terawatt-hours. That‚Äôs nearly the annual electricity consumption from the entire country of Ireland.



As an outsider, it‚Äôs tempting to point the finger at AI, since that technology has crashed into the mainstream and percolated into every corner of Google‚Äôs products and business. And yet the report downplays the role of AI. Here‚Äôs one bit that struck me:



‚ÄúHowever, it‚Äôs important to note that our growing electricity needs aren‚Äôt solely driven by AI. The accelerating growth of Google Cloud, continued investments in Search, the expanding reach of YouTube, and more, have also contributed to this overall growth.‚Äù



There is enough wiggle room in that statement to drive a large electric truck through. When I asked about the relative contributions here, company representative Mara Harris said via email that they don‚Äôt break out what portion comes from AI. When I followed up asking if the company didn‚Äôt have this information or just wouldn‚Äôt share it, she said she‚Äôd check but didn‚Äôt get back to me.



I‚Äôll make the point here that we‚Äôve made before, including in our recent package on AI and energy: Big companies should be disclosing more about the energy demands of AI. We shouldn‚Äôt be guessing at this technology‚Äôs effects.



Google has put a ton of effort and resources into setting and chasing ambitious climate goals. But as its energy needs and those of the rest of the industry continue to explode, it‚Äôs obvious that this problem is getting tougher, and it‚Äôs also clear that more transparency is a crucial part of the way forward.



This article is from The Spark, MIT Technology Review‚Äôs weekly climate newsletter. To receive it in your inbox every Wednesday, sign up here.

üîí Cybersecurity & Privacy
‚Ä¢ Big Tech‚Äôs Mixed Response to U.S. Treasury Sanctions
  In May 2025, the U.S. government sanctioned a Chinese national for operating a cloud provider linked to the majority of virtual currency investment scam websites reported to the FBI. But a new report finds the accused continues to operate a slew of established accounts at American tech companies &#8212; including Facebook, Github, PayPal and Twitter/X.
On May 29, the U.S. Department of the Treasury¬†announced economic sanctions against Funnull Technology Inc., a Philippines-based company alleged to provide infrastructure for hundreds of thousands of websites involved in virtual currency investment scams known as &#8220;pig butchering.&#8221; In January 2025, KrebsOnSecurity detailed how Funnull was designed as a content delivery network that catered to foreign cybercriminals seeking to route their traffic through U.S.-based cloud providers.

The Treasury also sanctioned Funnull&#8217;s alleged operator, a 40-year-old Chinese national named Liu &#8220;Steve&#8221; Lizhi. The government says Funnull directly facilitated financial schemes resulting in more than $200 million in financial losses by Americans, and that the company&#8217;s operations were linked to the majority of pig butchering scams reported to the FBI.
It is generally illegal for U.S. companies or individuals to transact with people sanctioned by the Treasury. However, as Mr. Lizhi&#8217;s case makes clear, just because someone is sanctioned doesn&#8217;t necessarily mean big tech companies are going to suspend their online accounts.
The government says Lizhi was born November 13, 1984, and used the nicknames &#8220;XXL4&#8221; and &#8220;Nice Lizhi.&#8221; Nevertheless, Steve Liu&#8217;s 17-year-old account on LinkedIn (in the name &#8220;Liulizhi&#8221;) had hundreds of followers (Lizhi&#8217;s LinkedIn profile helpfully confirms his birthday) until quite recently: The account was deleted this morning, just hours after KrebsOnSecurity sought comment from LinkedIn.
Mr. Lizhi&#8217;s LinkedIn account was suspended sometime in the last 24 hours, after KrebsOnSecurity sought comment from LinkedIn.
In an emailed response, a LinkedIn spokesperson said the company&#8217;s &#8220;Prohibited countries policy&#8221; states that LinkedIn &#8220;does not sell, license, support or otherwise make available its Premium accounts or other paid products and services to individuals and companies sanctioned by the U.S. government.&#8221; LinkedIn declined to say whether the profile in question was a premium or free account.
Mr. Lizhi also maintains a working PayPal account under the name Liu Lizhi and username &#8220;@nicelizhi,&#8221; another nickname listed in the Treasury sanctions. PayPal did not respond to a request for comment. A 15-year-old Twitter/X account named &#8220;Lizhi&#8221; that links to Mr. Lizhi&#8217;s personal domain remains active, although it has few followers and hasn&#8217;t posted in years.
These accounts and many others were flagged by the security firm Silent Push, which has been tracking Funnull&#8217;s operations for the past year and calling out U.S. cloud providers like Amazon and Microsoft for failing to more quickly sever ties with the company.
Liu Lizhi&#8217;s PayPal account.
In a report released today, Silent Push found Lizhi still operates numerous Facebook accounts and groups, including a private Facebook account under the name Liu Lizhi. Another Facebook account clearly connected to Lizhi is a tourism page for Ganzhou, China called &#8220;EnjoyGanzhou&#8221; that was named in the Treasury Department sanctions.
&#8220;This guy is the technical administrator for the infrastructure that is hosting a majority of scams targeting people in the United States, and hundreds of millions have been lost based on the websites he&#8217;s been hosting,&#8221; said Zach Edwards, senior threat researcher at Silent Push. &#8220;It&#8217;s crazy that the vast majority of big tech companies haven&#8217;t done anything to cut ties with this guy.&#8221;
The FBI says it received nearly 150,000 complaints last year involving digital assets and $9.3 billion in losses &#8212; a 66 percent increase from the previous year. Investment scams were the top crypto-related crimes reported, with $5.8 billion in losses.
In a statement, a Meta spokesperson said the company continuously takes steps to meet its legal obligations, but that sanctions laws are complex and varied. They explained that sanctions are often targeted in nature and don&#8217;t always prohibit people from having a presence on its platform. Nevertheless, Meta confirmed it had removed the account, unpublished Pages, and removed Groups and events associated with the user for violating its policies.
Attempts to reach Mr. Lizhi via his primary email addresses at Hotmail and Gmail bounced as undeliverable. Likewise, his 14-year-old YouTube channel appears to have been taken down recently.
However, anyone interested in viewing or using Mr. Lizhi&#8217;s 146 computer code repositories will have no problem finding GitHub accounts for him, including one registered under the NiceLizhi and XXL4 nicknames mentioned in the Treasury sanctions.
One of multiple GitHub profiles used by Liu &#8220;Steve&#8221; Lizhi, who uses the nickname XXL4 (a moniker listed in the Treasury sanctions for Mr. Lizhi).
Mr. Lizhi also operates a GitHub page for an open source e-commerce platform called NexaMerchant, which advertises itself as a payment gateway working with numerous American financial institutions. Interestingly, this profile&#8217;s &#8220;followers&#8221; page shows several other accounts that appear to be Mr. Lizhi&#8217;s. All of the account&#8217;s followers are tagged as &#8220;suspended,&#8221; even though that suspended message does not display when one visits those individual profiles.
In response to questions, GitHub said it has a process in place to identify when users and customers are Specially Designated Nationals or other denied or blocked parties, but that it locks those accounts instead of removing them. According to its policy, GitHub takes care that users and customers aren&#8217;t impacted beyond what is required by law.
All of the follower accounts for the XXL4 GitHub account appear to be Mr. Lizhi&#8217;s, and have been suspended by GitHub, but their code is still accessible.
&#8220;This includes keeping public repositories, including those for open source projects, available and accessible to support personal communications involving developers in sanctioned regions,&#8221; the policy states. &#8220;This also means GitHub will advocate for developers in sanctioned regions to enjoy greater access to the platform and full access to the global open source community.&#8221;
Edwards said it&#8217;s great that GitHub has a process for handling sanctioned accounts, but that the process doesn&#8217;t seem to communicate risk in a transparent way, noting that the only indicator on the locked accounts is the message, &#8220;This repository has been archived by the owner. It is not read-only.&#8221;
&#8220;It&#8217;s an odd message that doesn&#8217;t communicate, &#8216;This is a sanctioned entity, don&#8217;t fork this code or use it in a production environment&#8217;,&#8221; Edwards said.
Mark Rasch is a former federal cybercrime prosecutor who now serves as counsel for the New York City based security consulting firm Unit 221B. Rasch said when Treasury&#8217;s Office of Foreign Assets Control (OFAC) sanctions a person or entity, it then becomes illegal for businesses or organizations to transact with the sanctioned party.
Rasch said financial institutions have very mature systems for severing accounts tied to people who become subject to OFAC sanctions, but that tech companies may be far less proactive &#8212; particularly with free accounts.
&#8220;Banks have established ways of checking [U.S. government sanctions lists] for sanctioned entities, but tech companies don&#8217;t necessarily do a good job with that, especially for services that you can just click and sign up for,&#8221; Rasch said. &#8220;It&#8217;s potentially a risk and liability for the tech companies involved, but only to the extent OFAC is willing to enforce it.&#8221;
Liu Lizhi operates numerous Facebook accounts and groups, including this one for an entity specified in the OFAC sanctions: The &#8220;Enjoy Ganzhou&#8221; tourism page for Ganzhou, China. Image: Silent Push.
In July 2024, Funnull purchased the domain polyfill[.]io, the longtime home of a legitimate open source project that allowed websites to ensure that devices using legacy browsers could still render content in newer formats. After the Polyfill domain changed hands, at least 384,000 websites were caught in a supply-chain attack that redirected visitors to malicious sites. According to the Treasury, Funnull used the code to redirect people to scam websites and online gambling sites, some of which were linked to Chinese criminal money laundering operations.
The U.S. government says Funnull provides domain names for websites on its purchased IP addresses, using domain generation algorithms (DGAs) ‚Äî programs that generate large numbers of similar but unique names for websites ‚Äî and that it sells web design templates to cybercriminals.
&#8220;These services not only make it easier for cybercriminals to impersonate trusted brands when creating scam websites, but also allow them to quickly change to different domain names and IP addresses when legitimate providers attempt to take the websites down,&#8221; reads a Treasury statement.
Meanwhile, Funnull appears to be morphing nearly all aspects of its business in the wake of the sanctions, Edwards said.
&#8220;Whereas before they might have used 60 DGA domains to hide and bounce their traffic, we&#8217;re seeing far more now,&#8221; he said. &#8220;They&#8217;re trying to make their infrastructure harder to track and more complicated, so for now they&#8217;re not going away but more just changing what they&#8217;re doing. And a lot more organizations should be holding their feet to the fire.&#8221;
Update, 2:48 PM ET: Added response from Meta, which confirmed it has closed the accounts and groups connected to Mr. Lizhi.

üéì University AI
No updates.

üè¢ Corporate AI
‚Ä¢ Transforming network operations with AI: How Swisscom built a network assistant using Amazon Bedrock
  In the telecommunications industry, managing complex network infrastructures requires processing vast amounts of data from multiple sources. Network engineers often spend considerable time manually gathering and analyzing this data, taking away valuable hours that could be spent on strategic initiatives. This challenge led Swisscom, Switzerland‚Äôs leading telecommunications provider, to explore how AI can transform their network operations. 
Swisscom‚Äôs Network Assistant, built on Amazon Bedrock, represents a significant step forward in automating network operations. This solution combines generative AI capabilities with a sophisticated data processing pipeline to help engineers quickly access and analyze network data. Swisscom used AWS services to create a scalable solution that reduces manual effort and provides accurate and timely network insights. 
In this post, we explore how Swisscom developed their Network Assistant. We discuss the initial challenges and how they implemented a solution that delivers measurable benefits. We examine the technical architecture, discuss key learnings, and look at future enhancements that can further transform network operations. We highlight best practices for handling sensitive data for Swisscom to comply with the strict regulations governing the telecommunications industry. This post provides telecommunications providers or other organizations managing complex infrastructure with valuable insights into how you can use AWS services to modernize operations through AI-powered automation. 
The opportunity: Improve network operations 
Network engineers at Swisscom faced the daily challenge to manage complex network operations and maintain optimal performance and compliance. These skilled professionals were tasked to monitor and analyze vast amounts of data from multiple and decoupled sources. The process was repetitive and demanded considerable time and attention to detail. In certain scenarios, fulfilling the assigned tasks consumed more than 10% of their availability. The manual nature of their work presented several critical pain points. The data consolidation process from multiple network entities into a coherent overview was particularly challenging, because engineers had to navigate through various tools and systems to retrieve telemetry information about data sources and network parameters from extensive documentation, verify KPIs through complex calculations, and identify potential issues of diverse nature. This fragmented approach consumed valuable time and introduced the risk of human error in data interpretation and analysis. The situation called for a solution to address three primary concerns: 
 
 Efficiency in data retrieval and analysis 
 Accuracy in calculations and reporting 
 Scalability to accommodate growing data sources and use cases 
 
The team required a streamlined approach to access and analyze network data, maintain compliance with defined metrics and thresholds, and deliver fast and accurate responses to events while maintaining the highest standards of data security and sovereignty. 
Solution overview 
Swisscom‚Äôs approach to develop the Network Assistant was methodical and iterative. The team chose Amazon Bedrock as the foundation for their generative AI application and implemented a Retrieval Augmented Generation (RAG) architecture using Amazon Bedrock Knowledge Bases to enable precise and contextual responses to engineer queries. The RAG approach is implemented in three distinct phases: 
 
 Retrieval ‚Äì User queries are matched with relevant knowledge base content through embedding models 
 Augmentation ‚Äì The context is enriched with retrieved information 
 Generation ‚Äì The large language model (LLM) produces informed responses 
 
The following diagram illustrates the solution architecture. 
 
The solution architecture evolved through several iterations. The initial implementation established basic RAG functionality by feeding the Amazon Bedrock knowledge base with tabular data and documentation. However, the Network Assistant struggled to manage large input files containing thousands of rows with numerical values across multiple parameter columns. This complexity highlighted the need for a more selective approach that could identify only the rows relevant for specific KPI calculations. At that point, the retrieval process wasn‚Äôt returning the precise number of vector embeddings required to calculate the formulas, prompting the team to refine the solution for greater accuracy. 
Next iterations enhanced the assistant with agent-based processing and action groups. The team implemented AWS Lambda functions using Pandas or Spark for data processing, facilitating accurate numerical calculations retrieval using natural language from the user input prompt. 
A significant advancement was introduced with the implementation of a multi-agent approach, using Amazon Bedrock Agents, where specialized agents handle different aspects of the system: 
 
 Supervisor agent ‚Äì Orchestrates interactions between documentation management and calculator agents to provide comprehensive and accurate responses. 
 Documentation management agent ‚Äì Helps the network engineers access information in large volumes of data efficiently and extract insights about data sources, network parameters, configuration, or tooling. 
 Calculator agent ‚Äì Supports the network engineers to understand complex network parameters and perform precise data calculations out of telemetry data. This produces numerical insights that help perform network management tasks; optimize performance; maintain network reliability, uptime, and compliance; and assist in troubleshooting. 
 
This following diagram illustrates the enhanced data extract, transform, and load (ETL) pipeline interaction with Amazon Bedrock. 
 
To achieve the desired accuracy in KPI calculations, the data pipeline was refined to achieve consistent and precise performance, which leads to meaningful insights. The team implemented an ETL pipeline with Amazon Simple Storage Service (Amazon S3) as the data lake to store input files following a daily batch ingestion approach, AWS Glue for automated data crawling and cataloging, and Amazon Athena for SQL querying. At this point, it became possible for the calculator agent to forego the Pandas or Spark data processing implementation. Instead, by using Amazon Bedrock Agents, the agent translates natural language user prompts into SQL queries. In a subsequent step, the agent runs the relevant SQL queries selected dynamically through analysis of various input parameters, providing the calculator agent an accurate result. This serverless architecture supports scalability, cost-effectiveness, and maintains high accuracy in KPI calculations. The system integrates with Swisscom‚Äôs on-premises data lake through daily batch data ingestion, with careful consideration of data security and sovereignty requirements. 
To enhance data security and appropriate ethics in the Network Assistant responses, a series of guardrails were defined in Amazon Bedrock. The application implements a comprehensive set of data security guardrails to protect against malicious inputs and safeguard sensitive information. These include content filters that block harmful categories such as hate, insults, violence, and prompt-based threats like SQL injection. Specific denied topics and sensitive identifiers (for example, IMSI, IMEI, MAC address, or GPS coordinates) are filtered through manual word filters and pattern-based detection, including regular expressions (regex). Sensitive data such as personally identifiable information (PII), AWS access keys, and serial numbers are blocked or masked. The system also uses contextual grounding and relevance checks to verify model responses are factually accurate and appropriate. In the event of restricted input or output, standardized messaging notifies the user that the request can‚Äôt be processed. These guardrails help prevent data leaks, reduce the risk of DDoS-driven cost spikes, and maintain the integrity of the application‚Äôs outputs. 
Results and benefits 
The implementation of the Network Assistant is set to deliver substantial and measurable benefits to Swisscom‚Äôs network operations. The most significant impact is time savings. Network engineers are estimated to experience 10% reduction in time spent on routine data retrieval and analysis tasks. This efficiency gain translates to nearly 200 hours per engineer saved annually, and represents a significant improvement in operational efficiency. The financial impact is equally impressive. The solution is projected to provide substantial cost savings per engineer annually, with minimal operational costs at less than 1% of the total value generated. The return on investment increases as additional teams and use cases are incorporated into the system, demonstrating strong scalability potential. 
Beyond the quantifiable benefits, the Network Assistant is expected to transform how engineers interact with network data. The enhanced data pipeline supports accuracy in KPI calculations, critical for network health tracking, and the multi-agent approach provides orchestrated and comprehensive responses to complex queries out of user natural language. 
As a result, engineers can have instant access to a wide range of network parameters, data source information, and troubleshooting guidance from an individual personalized endpoint with which they can quickly interact and obtain insights through natural language. This enables them to focus on strategic tasks rather than routine data gathering and analysis, leading to a significant work reduction that aligns with Swisscom SRE principles. 
Lessons learned 
Throughout the development and implementation of the Swisscom Network Assistant, several learnings emerged that shaped the solution. The team needed to address data sovereignty and security requirements for the solution, particularly when processing data on AWS. This led to careful consideration of data classification and compliance with applicable regulatory requirements in the telecommunications sector, to make sure that sensitive data is handled appropriately. In this regard, the application underwent a strict threat model evaluation, verifying the robustness of its interfaces against vulnerabilities and acting proactively towards securitization. The threat model was applied to assess doomsday scenarios, and data flow diagrams were created to depict major data flows inside and beyond the application boundaries. The AWS architecture was specified in detail, and trust boundaries were set to indicate which portions of the application trusted each other. Threats were identified following the STRIDE methodology (Spoofing, Tampering, Repudiation, Information disclosure, Denial of service, Elevation of privilege), and countermeasures, including Amazon Bedrock Guardrails, were defined to avoid or mitigate threats in advance. 
A critical technical insight was that complex calculations involving significant data volume management required a different approach than mere AI model interpretation. The team implemented an enhanced data processing pipeline that combines the contextual understanding of AI models with direct database queries for numerical calculations. This hybrid approach facilitates both accuracy in calculations and richness in contextual responses. 
The choice of a serverless architecture proved to be particularly beneficial: it minimized the need to manage compute resources and provides automatic scaling capabilities. The pay-per-use model of AWS services helped keep operational costs low and maintain high performance. Additionally, the team‚Äôs decision to implement a multi-agent approach provided the flexibility needed to handle diverse types of queries and use cases effectively. 
Next steps 
Swisscom has ambitious plans to enhance the Network Assistant‚Äôs capabilities further. A key upcoming feature is the implementation of a network health tracker agent to provide proactive monitoring of network KPIs. This agent will automatically generate reports to categorize issues based on criticality, enable faster response time, and improve the quality of issue resolution to potential network issues. The team is also exploring the integration of Amazon Simple Notification Service (Amazon SNS) to enable proactive alerting for critical network status changes. This can include direct integration with operational tools that alert on-call engineers, to further streamline the incident response process. The enhanced notification system will help engineers address potential issues before they critically impact network performance and obtain a detailed action plan including the affected network entities, the severity of the event, and what went wrong precisely. 
The roadmap also includes expanding the system‚Äôs data sources and use cases. Integration with additional internal network systems will provide more comprehensive network insights. The team is also working on developing more sophisticated troubleshooting features, using the growing knowledge base and agentic capabilities to provide increasingly detailed guidance to engineers. 
Additionally, Swisscom is adopting infrastructure as code (IaC) principles by implementing the solution using AWS CloudFormation. This approach introduces automated and consistent deployments while providing version control of infrastructure components, facilitating simpler scaling and management of the Network Assistant solution as it grows. 
Conclusion 
The Network Assistant represents a significant advancement in how Swisscom can manage its network operations. By using AWS services and implementing a sophisticated AI-powered solution, they have successfully addressed the challenges of manual data retrieval and analysis. As a result, they have boosted both accuracy and efficiency so network engineers can respond quickly and decisively to network events. The solution‚Äôs success is aided not only by the quantifiable benefits in time and cost savings but also by its potential for future expansion. The serverless architecture and multi-agent approach provide a solid foundation for adding new capabilities and scaling across different teams and use cases.As organizations worldwide grapple with similar challenges in network operations, Swisscom‚Äôs implementation serves as a valuable blueprint for using cloud services and AI to transform traditional operations. The combination of Amazon Bedrock with careful attention to data security and accuracy demonstrates how modern AI solutions can help solve real-world engineering challenges. 
As managing network operations complexity continues to grow, the lessons from Swisscom‚Äôs journey can be applied to many engineering disciplines. We encourage you to consider how Amazon Bedrock and similar AI solutions might help your organization overcome its own comprehension and process improvement barriers. To learn more about implementing generative AI in your workflows, explore Amazon Bedrock Resources or contact AWS. 
Additional resources 
For more information about Amazon Bedrock Agents and its use cases, refer to the following resources: 
 
 Generative AI for telecom 
 Best practices for building robust generative AI applications with Amazon Bedrock Agents ‚Äì Part 1 
 Best practices for building robust generative AI applications with Amazon Bedrock Agents ‚Äì Part 2 
 
 
 
About the authors 
Pablo Garc√≠a Benedicto is an experienced Data &amp; AI Cloud Engineer with strong expertise in cloud hyperscalers and data engineering. With a background in telecommunications, he currently works at Swisscom, where he leads and contributes to projects involving Generative AI applications and agents using Amazon Bedrock. Aiming for AI and data specialization, his latest projects focus on building intelligent assistants and autonomous agents that streamline business information retrieval, leveraging cloud-native architectures and scalable data pipelines to reduce toil and drive operational efficiency. 
Rajesh Sripathi is a Generative AI Specialist Solutions Architect at AWS, where he partners with global Telecommunication and Retail &amp; CPG customers to develop and scale generative AI applications. With over 18 years of experience in the IT industry, Rajesh helps organizations use cutting-edge cloud and AI technologies for business transformation. Outside of work, he enjoys exploring new destinations through his passion for travel and driving. 
Ruben Merz Ruben Merz is a Principal Solutions Architect at AWS. With a background in distributed systems and networking, his work with customers at AWS focuses on digital sovereignty, AI, and networking. 
Jordi Montoliu Nerin is a Data &amp; AI Leader currently serving as Senior AI/ML Specialist at AWS, where he helps worldwide telecommunications customers implement AI strategies after previously driving Data &amp; Analytics business across EMEA regions. He has over 10 years of experience, where he has led multiple Data &amp; AI implementations at scale, led executions of data strategy and data governance frameworks, and has driven strategic technical and business development programs across multiple industries and continents. Outside of work, he enjoys sports, cooking and traveling.
‚Ä¢ End-to-End model training and deployment with Amazon SageMaker Unified Studio
  Although rapid generative AI advancements are revolutionizing organizational natural language processing tasks, developers and data scientists face significant challenges customizing these large models. These hurdles include managing complex workflows, efficiently preparing large datasets for fine-tuning, implementing sophisticated fine-tuning techniques while optimizing computational resources, consistently tracking model performance, and achieving reliable, scalable deployment.The fragmented nature of these tasks often leads to reduced productivity, increased development time, and potential inconsistencies in the model development pipeline. Organizations need a unified, streamlined approach that simplifies the entire process from data preparation to model deployment. 
To address these challenges, AWS has expanded Amazon SageMaker with a comprehensive set of data, analytics, and generative AI capabilities. At the heart of this expansion is Amazon SageMaker Unified Studio, a centralized service that serves as a single integrated development environment (IDE). SageMaker Unified Studio streamlines access to familiar tools and functionality from purpose-built AWS analytics and artificial intelligence and machine learning (AI/ML) services, including Amazon EMR, AWS Glue, Amazon Athena, Amazon Redshift, Amazon Bedrock, and Amazon SageMaker AI. With SageMaker Unified Studio, you can discover data through Amazon SageMaker Catalog, access it from Amazon SageMaker Lakehouse, select foundation models (FMs) from Amazon SageMaker JumpStart or build them through JupyterLab, train and fine-tune them with SageMaker AI training infrastructure, and deploy and test models directly within the same environment. SageMaker AI is a fully managed service to build, train, and deploy ML models‚Äîincluding FMs‚Äîfor different use cases by bringing together a broad set of tools to enable high-performance, low-cost ML. It‚Äôs available as a standalone service on the AWS Management Console, or through APIs. Model development capabilities from SageMaker AI are available within SageMaker Unified Studio. 
In this post, we guide you through the stages of customizing large language models (LLMs) with SageMaker Unified Studio and SageMaker AI, covering the end-to-end process starting from data discovery to fine-tuning FMs with SageMaker AI distributed training, tracking metrics using MLflow, and then deploying models using SageMaker AI inference for real-time inference. We also discuss best practices to choose the right instance size and share some debugging best practices while working with JupyterLab notebooks in SageMaker Unified Studio. 
Solution overview 
The following diagram illustrates the solution architecture. There are three personas: admin, data engineer, and user, which can be a data scientist or an ML engineer. 

 
 AWS SageMaker Unified Studio ML workflow showing data processing, model training, and deployment stages
 
Setting up the solution consists of the following steps: 
 
 The admin sets up the SageMaker Unified Studio domain for the user and sets the access controls. The admin also publishes the data to SageMaker Catalog in SageMaker Lakehouse. 
 Data engineers can create and manage extract, transform, and load (ETL) pipelines directly within Unified Studio using Visual ETL. They can transform raw data sources into datasets ready for exploratory data analysis. The admin can then manage the publication of these assets to the SageMaker Catalog, making them discoverable and accessible to other team members or users such as data engineers in the organization. 
 Users or data engineers can log in to the Unified Studio web-based IDE using the login provided by the admin to create a project and create a managed MLflow server for tracking experiments. Users can discover available data assets in the SageMaker Catalog and request a subscription to an asset published by the data engineer. After the data engineer approves the subscription request, the user performs an exploratory data analysis of the content of the table with the query editor or with a JupyterLab notebook, then prepares the dataset by connecting with SageMaker Catalog through an AWS Glue or Athena connection. 
 You can explore models from SageMaker JumpStart, which hosts over 200 models for various tasks, and fine-tune directly with the UI, or develop a training script for fine-tuning the LLM in the JupyterLab IDE. SageMaker AI provides distributed training libraries and supports various distributed training options for deep learning tasks. For this post, we use the PyTorch framework and use Hugging Face open source FMs for fine-tuning. We will show you how you can use parameter efficient fine-tuning (PEFT) with Low-Rank Adaptation (LoRa), where you freeze the model weights, train the model with modifying weight metrics, and then merge these LoRa adapters back to the base model after distributed training. 
 You can track and monitor fine-tuning metrics directly in SageMaker Unified Studio using MLflow, by analyzing metrics such as loss to make sure the model is correctly fine-tuned. 
 You can deploy the model to a SageMaker AI endpoint after the fine-tuning job is complete and test it directly from SageMaker Unified Studio. 
 
Prerequisites 
Before starting this tutorial, make sure you have the following: 
 
 An AWS account with permissions to create SageMaker resources. For setup instructions, see Set up an AWS account and create an administrator user. 
 Familiarity with Python and PyTorch for distributed training and model customization. 
 
Set up SageMaker Unified Studio and configure user access 
SageMaker Unified Studio is built on top of Amazon DataZone capabilities such as domains to organize your assets and users, and projects to collaborate with others users, securely share artifacts, and seamlessly work across compute services. 
To set up Unified Studio, complete the following steps: 
 
 As an admin, create a SageMaker Unified Studio domain, and note the URL. 
 On the domain‚Äôs details page, on the User management tab, choose Configure SSO user access. For this post, we recommend setting up using single sign-on (SSO) access using the URL. 
 
For more information about setting up user access, see Managing users in Amazon SageMaker Unified Studio. 
Log in to SageMaker Unified Studio 
Now that you have created your new SageMaker Unified Studio domain, complete the following steps to access SageMaker Unified Studio: 
 
 On the SageMaker console, open the details page of your domain. 
 Choose the link for the SageMaker Unified Studio URL. 
 Log in with your SSO credentials. 
 
Now you‚Äôre signed in to SageMaker Unified Studio. 
Create a project 
The next step is to create a project. Complete the following steps: 
 
 In SageMaker Unified Studio, choose Select a project on the top menu, and choose Create project. 
 For Project name, enter a name (for example, demo). 
 For Project profile, choose your profile capabilities. A project profile is a collection of blueprints, which are configurations used to create projects. For this post, we choose All capabilities, then choose Continue. 
 

 
 Creating a project in Amazon SageMaker Unified Studio
 
Create a compute space 
SageMaker Unified Studio provides compute spaces for IDEs that you can use to code and develop your resources. By default, it creates a space for you to get started with you project. You can find the default space by choosing Compute in the navigation pane and choosing the Spaces tab. You can then choose Open to go to the JuypterLab environment and add members to this space. You can also create a new space by choosing Create space on the Spaces tab. 
 
To use SageMaker Studio notebooks cost-effectively, use smaller, general-purpose instances (like the T or M families) for interactive data exploration and prototyping. For heavy lifting like training or large-scale processing or deployment, use SageMaker AI training jobs and SageMaker AI prediction to offload the work to separate and more powerful instances such as the P5 family. We will show you in the notebook how you can run training jobs and deploy LLMs in the notebook with APIs. It is not recommended to run distributed workloads in notebook instances. The chances of kernel failures is high because JupyterLab notebooks should not be used for large distributed workloads (both for data and ML training). 
The following screenshot shows the configuration options for your space. You can change your instance size from default (ml.t3.medium) to (ml.m5.xlarge) for the JupyterLab IDE. You can also increase the Amazon Elastic Block Store (Amazon EBS) volume capacity from 16 GB to 50 GB for training LLMs. 

 
 Canfigure space in Amazon SageMaker Unified Studio
 
Set up MLflow to track ML experiments 
You can use MLflow in SageMaker Unified Studio to create, manage, analyze, and compare ML experiments. Complete the following steps to set up MLflow: 
 
 In SageMaker Unified Studio, choose Compute in the navigation pane. 
 On the MLflow Tracking Servers tab, choose Create MLflow Tracking Server. 
 Provide a name and create your tracking server. 
 Choose Copy ARN to copy the Amazon Resource Name (ARN) of the tracking server. 
 
 
You will need this MLflow ARN in your notebook to set up distributed training experiment tracking. 
Set up the data catalog 
For model fine-tuning, you need access to a dataset. After you set up the environment, the next step is to find the relevant data from the SageMaker Unified Studio data catalog and prepare the data for model tuning. For this post, we use the Stanford Question Answering Dataset (SQuAD) dataset. This dataset is a reading comprehension dataset, consisting of questions posed by crowd workers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable. 
Download the SQuaD dataset and upload it to SageMaker Lakehouse by following the steps in Uploading data. 

 
 Adding data to Catalog in Amazon SageMaker Unified Studio
 
To make this data discoverable by the users or ML engineers, the admin needs to publish this data to the Data Catalog. For this post, you can directly download the SQuaD dataset and upload it to the catalog. To learn how to publish the dataset to SageMaker Catalog, see Publish assets to the Amazon SageMaker Unified Studio catalog from the project inventory. 
Query data with the query editor and JupyterLab 
In many organizations, data preparation is a collaborative effort. A data engineer might prepare an initial raw dataset, which a data scientist then refines and augments with feature engineering before using it for model training. In the SageMaker Lakehouse data and model catalog, publishers set subscriptions for automatic or manual approval (wait for admin approval). Because you already set up the data in the previous section, you can skip this section showing how to subscribe to the dataset. 
To subscribe to another dataset like SQuAD, open the data and model catalog in Amazon SageMaker Lakehouse, choose SQuAD, and subscribe. 

 
 Subscribing to any asset or dataset published by Admin
 
Next, let‚Äôs use the data explorer to explore the dataset you subscribed to. Complete the following steps: 
 
 On the project page, choose Data. 
 Under Lakehouse, expand AwsDataCatalog. 
 Expand your database starting from glue_db_. 
 Choose the dataset you created (starting with squad) and choose Query with Athena. 
 

 
 Querying the data using Query Editor in Amazon SageMaker Unfied Studio
 
Process your data through a multi-compute JupyterLab IDE notebook 
SageMaker Unified Studio provides a unified JupyterLab experience across different languages, including SQL, PySpark, Python, and Scala Spark. It also supports unified access across different compute runtimes such as Amazon Redshift and Athena for SQL, Amazon EMR Serverless, Amazon EMR on EC2, and AWS Glue for Spark. 
Complete the following steps to get started with the unified JupyterLab experience: 
 
 Open your SageMaker Unified Studio project page. 
 On the top menu, choose Build, and under IDE &amp; APPLICATIONS, choose JupyterLab. 
 Wait for the space to be ready. 
 Choose the plus sign and for Notebook, choose Python 3. 
 Open a new terminal and enter git clonehttps://github.com/aws-samples/amazon-sagemaker-generativeai. 
 Go to the folder amazon-sagemaker-generativeai/3_distributed_training/distributed_training_sm_unified_studio/ and open the distributed training in unified studio.ipynb notebook to get started. 
 Enter the MLflow server ARN you created in the following code: 
 
 
 import&nbsp;os
os.environ["mlflow_uri"]&nbsp;=&nbsp;""
os.environ["mlflow_experiment_name"]&nbsp;=&nbsp;"deepseek-r1-distill-llama-8b-sft" 
 
Now you an visualize the data through the notebook. 
 
 On the project page, choose Data. 
 Under Lakehouse, expand AwsDataCatalog. 
 Expand your database starting from glue_db, copy the name of the database, and enter it in the following code: 
 
 
 db_name&nbsp;=&nbsp;"&lt;enter your db name&gt;"
table&nbsp;=&nbsp;"sqad" 
 
 
 You can now access the entire dataset directly by using the in-line SQL query capabilities of JupyterLab notebooks in SageMaker Unified Studio. You can follow the data preprocessing steps in the notebook. 
 
 
 %%sql project.athena
SELECT * FROM "&lt;DATABASE_NAME&gt;"."sqad"; 
 
The following screenshot shows the output. 
 
We are going to split the dataset into a test set and training set for model training. When the data processing in done and we have split the data into test and training sets, the next step is to perform fine-tuning of the model using SageMaker Distributed Training. 
Fine-tune the model with SageMaker Distributed training 
You‚Äôre now ready to fine-tune your model by using SageMaker AI capabilities for training. Amazon SageMaker Training is a fully managed ML service offered by SageMaker that helps you efficiently train a wide range of ML models at scale. The core of SageMaker AI jobs is the containerization of ML workloads and the capability of managing AWS compute resources. SageMaker Training takes care of the heavy lifting associated with setting up and managing infrastructure for ML training workloads 
We select one model directly from the Hugging Face Hub, DeepSeek-R1-Distill-Llama-8B, and develop our training script in the JupyterLab space. Because we want to distribute the training across all the available GPUs in our instance, by using PyTorch Fully Sharded Data Parallel (FSDP), we use the Hugging Face Accelerate library to run the same PyTorch code across distributed configurations. You can start the fine-tuning job directly in your JupyterLab notebook or use the SageMaker Python SDK to start the training job. We use the Trainer from transfomers to fine-tune our model. We prepared the script train.py, which loads the dataset from disk, prepares the model and tokenizer, and starts the training. 
For configuration, we use TrlParser, and provide hyperparameters in a YAML file. You can upload this file and provide it to SageMaker similar to your datasets. The following is the config file for fine-tuning the model on ml.g5.12xlarge. Save the config file as args.yaml and upload it to Amazon Simple Storage Service (Amazon S3). 
 
 cat&nbsp;&gt;&nbsp;./args.yaml&nbsp;&lt;&lt;EOF
model_id: "deepseek-ai/DeepSeek-R1-Distill-Llama-8B" &nbsp; &nbsp; &nbsp; # Hugging Face model id
mlflow_uri: "${mlflow_uri}"
mlflow_experiment_name: "${mlflow_experiment_name}"
# sagemaker specific parameters
output_dir: "/opt/ml/model" &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; # path to where SageMaker will upload the model 
train_dataset_path: "/opt/ml/input/data/train/" &nbsp; # path to where FSx saves train dataset
test_dataset_path: "/opt/ml/input/data/test/" &nbsp; &nbsp; # path to where FSx saves test dataset
# training parameters
lora_r: 8
lora_alpha: 16
lora_dropout: 0.1 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 
learning_rate: 2e-4 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# learning rate scheduler
num_train_epochs: 1 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# number of training epochs
per_device_train_batch_size: 2 &nbsp; &nbsp; &nbsp; &nbsp; # batch size per device during training
per_device_eval_batch_size: 1 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# batch size for evaluation
gradient_accumulation_steps: 2 &nbsp; &nbsp; &nbsp; &nbsp; # number of steps before performing a backward/update pass
gradient_checkpointing: true &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; # use gradient checkpointing
bf16: true &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; # use bfloat16 precision
tf32: false &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# use tf32 precision
fsdp: "full_shard auto_wrap offload"
fsdp_config: 
&nbsp;&nbsp; &nbsp;backward_prefetch: "backward_pre"
&nbsp;&nbsp; &nbsp;cpu_ram_efficient_loading: true
&nbsp;&nbsp; &nbsp;offload_params: true
&nbsp;&nbsp; &nbsp;forward_prefetch: false
&nbsp;&nbsp; &nbsp;use_orig_params: true
merge_weights: true &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# merge weights in the base model
EOF 
 
Use the following code to use the native PyTorch container image, pre-built for SageMaker: 
 
 image_uri = sagemaker.image_uris.retrieve(
&nbsp;&nbsp; &nbsp;framework="pytorch",
&nbsp;&nbsp; &nbsp;region=sagemaker_session.boto_session.region_name,
&nbsp;&nbsp; &nbsp;version="2.6.0",
&nbsp;&nbsp; &nbsp;instance_type=instance_type,
&nbsp;&nbsp; &nbsp;image_scope="training"
)

image_uri 
 
Define the trainer as follows: 
 
 Define the ModelTrainer
model_trainer = ModelTrainer(
&nbsp;&nbsp; &nbsp;training_image=image_uri,
&nbsp;&nbsp; &nbsp;source_code=source_code,
&nbsp;&nbsp; &nbsp;base_job_name=job_name,
&nbsp;&nbsp; &nbsp;compute=compute_configs,
&nbsp;&nbsp; &nbsp;distributed=Torchrun(),
&nbsp;&nbsp; &nbsp;stopping_condition=StoppingCondition(
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;max_runtime_in_seconds=7200
&nbsp;&nbsp; &nbsp;),
&nbsp;&nbsp; &nbsp;hyperparameters={
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"config": "/opt/ml/input/data/config/args.yaml" # path to TRL config which was uploaded to s3
&nbsp;&nbsp; &nbsp;},
&nbsp;&nbsp; &nbsp;output_data_config=OutputDataConfig(
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;s3_output_path=output_path
&nbsp;&nbsp; &nbsp;),
) 
 
Run the trainer with the following: 
 
 # starting the train job with our uploaded datasets as input
model_trainer.train(input_data_config=data,&nbsp;wait=True) 
 
You can follow the steps in the notebook. 
You can explore the job execution in SageMaker Unified Studio. The training job runs on the SageMaker training cluster by distributing the computation across the four available GPUs on the selected instance type ml.g5.12xlarge. We choose to merge the LoRA adapter with the base model. This decision was made during the training process by setting the merge_weights parameter to True in our train_fn() function. Merging the weights provides a single, cohesive model that incorporates both the base knowledge and the domain-specific adaptations we‚Äôve made through fine-tuning. 
Track training metrics and model registration using MLflow 
You created an MLflow server in an earlier step to track experiments and registered models, and provided the server ARN in the notebook. 
You can log MLflow models and automatically register them with Amazon SageMaker Model Registry using either the Python SDK or directly through the MLflow UI. Use mlflow.register_model() to automatically register a model with SageMaker Model Registry during model training. You can explore the MLflow tracking code in train.py and the notebook. The training code tracks MLflow experiments and registers the model to the MLflow model registry. To learn more, see Automatically register SageMaker AI models with SageMaker Model Registry. 
To see the logs, complete the following steps: 
 
 Choose Build, then choose Spaces. 
 Choose Compute in the navigation pane. 
 On the MLflow Tracking Servers tab, choose Open to open the tracking server. 
 
You can see both the experiments and registered models. 
 
Deploy and test the model using SageMaker AI Inference 
When deploying a fine-tuned model on AWS, SageMaker AI Inference offers multiple deployment strategies. In this post, we use SageMaker real-time inference. The real-time inference endpoint is designed for having full control over the inference resources. You can use a set of available instances and deployment options for hosting your model. By using the SageMaker built-in container DJL Serving, you can take advantage of the inference script and optimization options available directly in the container. In this post, we deploy the fine-tuned model to a SageMaker endpoint for running inference, which will be used for testing the model. 
In SageMaker Unified Studio, in JupyterLab, we create the Model object, which is a high-level SageMaker model class for working with multiple container options. The image_uri parameter specifies the container image URI for the model, and model_data points to the Amazon S3 location containing the model artifact (automatically uploaded by the SageMaker training job). We also specify a set of environment variables to configure the specific inference backend option (OPTION_ROLLING_BATCH), the degree of tensor parallelism based on the number of available GPUs (OPTION_TENSOR_PARALLEL_DEGREE), and the maximum allowable length of input sequences (in tokens) for models during inference (OPTION_MAX_MODEL_LEN). 
 
 model = Model(
&nbsp;&nbsp; &nbsp;image_uri=image_uri,
&nbsp;&nbsp; &nbsp;model_data=f"s3://{bucket_name}/{job_prefix}/{job_name}/output/model.tar.gz",
&nbsp;&nbsp; &nbsp;role=get_execution_role(),
&nbsp;&nbsp; &nbsp;env={
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;'HF_MODEL_ID': "/opt/ml/model",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;'OPTION_TRUST_REMOTE_CODE': 'true',
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;'OPTION_ROLLING_BATCH': "vllm",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;'OPTION_DTYPE': 'bf16',
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;'OPTION_TENSOR_PARALLEL_DEGREE': 'max',
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;'OPTION_MAX_ROLLING_BATCH_SIZE': '1',
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;'OPTION_MODEL_LOADING_TIMEOUT': '3600',
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;'OPTION_MAX_MODEL_LEN': '4096'
&nbsp;&nbsp; &nbsp;}
) 
 
After you create the model object, you can deploy it to an endpoint using the deploy method. The initial_instance_count and instance_type parameters specify the number and type of instances to use for the endpoint. We selected the ml.g5.4xlarge instance for the endpoint. The container_startup_health_check_timeout and model_data_download_timeout parameters set the timeout values for the container startup health check and model data download, respectively. 
 
 model_id&nbsp;=&nbsp;"deepseek-ai/DeepSeek-R1-Distill-Llama-8B"
endpoint_name&nbsp;=&nbsp;f"{model_id.split('/')[-1].replace('.',&nbsp;'-')}-sft-djl"
predictor = model.deploy(
&nbsp;&nbsp; &nbsp;initial_instance_count=instance_count,
&nbsp;&nbsp; &nbsp;instance_type=instance_type,
&nbsp;&nbsp; &nbsp;container_startup_health_check_timeout=1800,
&nbsp;&nbsp; &nbsp;model_data_download_timeout=3600
) 
 
It takes a few minutes to deploy the model before it becomes available for inference and evaluation. You can test the endpoint invocation in JupyterLab, by using the AWS SDK with the boto3 client for sagemaker-runtime, or by using the SageMaker Python SDK and the predictor previously created, by using the predict API. 
 
 base_prompt = f"""&lt;s&gt; [INST] {{question}} [/INST] """

prompt = base_prompt.format(
&nbsp; &nbsp; question="What statue is in front of the Notre Dame building?"
)

predictor.predict({
&nbsp;&nbsp; &nbsp;"inputs": prompt,
&nbsp;&nbsp; &nbsp;"parameters": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"max_new_tokens": 300,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"temperature": 0.2,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"top_p": 0.9,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"return_full_text": False,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"stop": ['&lt;/s&gt;']
&nbsp;&nbsp; &nbsp;}
}) 
 
You can also test the model invocation in SageMaker Unified Studio, on the Inference endpoint page and Text inference tab. 
Troubleshooting 
You might encounter some of the following errors while running your model training and deployment: 
 
 Training job fails to start ‚Äì If a training job fails to start, make sure your IAM role AmazonSageMakerDomainExecution has the necessary permissions, verify the instance type is available in your AWS Region, and check your S3 bucket permissions. This role is created when an admin creates the domain, and you can ask the admin to check your IAM access permissions associated with this role. 
 Out-of-memory errors during training ‚Äì If you encounter out-of-memory errors during training, try reducing the batch size, use gradient accumulation to simulate larger batches, or consider using a larger instance. 
 Slow model deployment ‚Äì For slow model deployment, make sure model artifacts aren‚Äôt excessively large, and use appropriate instance types for inference and capacity available for that instance in your Region. 
 
For more troubleshooting tips, refer to Troubleshooting guide. 
Clean up 
SageMaker Unified Studio by default shuts down idle resources such as JupyterLab spaces after 1 hour. However, you must delete the S3 bucket and the hosted model endpoint to stop incurring costs. You can delete the real-time endpoints you created using the SageMaker console. For instructions, see Delete Endpoints and Resources. 
Conclusion 
This post demonstrated how SageMaker Unified Studio serves as a powerful centralized service for data and AI workflows, showcasing its seamless integration capabilities throughout the fine-tuning process. With SageMaker Unified Studio, data engineers and ML practitioners can efficiently discover and access data through SageMaker Catalog, prepare datasets, fine-tune models, and deploy them‚Äîall within a single, unified environment. The service‚Äôs direct integration with SageMaker AI and various AWS analytics services streamlines the development process, alleviating the need to switch between multiple tools and environments. The solution highlights the service‚Äôs versatility in handling complex ML workflows, from data discovery and preparation to model deployment, while maintaining a cohesive and intuitive user experience. Through features like integrated MLflow tracking, built-in model monitoring, and flexible deployment options, SageMaker Unified Studio demonstrates its capability to support sophisticated AI/ML projects at scale. 
To learn more about SageMaker Unified Studio, see An integrated experience for all your data and AI with Amazon SageMaker Unified Studio. 
If this post helps you or inspires you to solve a problem, we would love to hear about it! The code for this solution is available on the GitHub repo for you to use and extend. Contributions are always welcome! 
 
About the authors 
Mona Mona currently works as a Sr World Wide Gen AI Specialist Solutions Architect at Amazon focusing on Gen AI Solutions. She was a Lead Generative AI specialist in Google Public Sector at Google before joining Amazon. She is a published author of two books ‚Äì Natural Language Processing with AWS AI Services and Google Cloud Certified Professional Machine Learning Study Guide. She has authored 19 blogs on AI/ML and cloud technology and a co-author on a research paper on CORD19 Neural Search which won an award for Best Research Paper at the prestigious AAAI (Association for the Advancement of Artificial Intelligence) conference. 
Bruno Pistone is a Senior Generative AI and ML Specialist Solutions Architect for AWS based in Milan. He works with large customers helping them to deeply understand their technical needs and design AI and Machine Learning solutions that make the best use of the AWS Cloud and the Amazon Machine Learning stack. His expertise include: Machine Learning end to end, Machine Learning Industrialization, and Generative AI. He enjoys spending time with his friends and exploring new places, as well as travelling to new destinations. 
Lauren Mullennex is a Senior GenAI/ML Specialist Solutions Architect at AWS. She has a decade of experience in DevOps, infrastructure, and ML. Her areas of focus include MLOps/LLMOps, generative AI, and computer vision.
‚Ä¢ Optimize RAG in production environments using Amazon SageMaker JumpStart and Amazon OpenSearch Service
  Generative AI has revolutionized customer interactions across industries by offering personalized, intuitive experiences powered by unprecedented access to information. This transformation is further enhanced by Retrieval Augmented Generation (RAG), a technique that allows large language models (LLMs) to reference external knowledge sources beyond their training data. RAG has gained popularity for its ability to improve generative AI applications by incorporating additional information, often preferred by customers over techniques like fine-tuning due to its cost-effectiveness and faster iteration cycles. 
The RAG approach excels in grounding language generation with external knowledge, producing more factual, coherent, and relevant responses. This capability proves invaluable in applications such as question answering, dialogue systems, and content generation, where accuracy and informative outputs are crucial. For businesses, RAG offers a powerful way to use internal knowledge by connecting company documentation to a generative AI model. When an employee asks a question, the RAG system retrieves relevant information from the company‚Äôs internal documents and uses this context to generate an accurate, company-specific response. This approach enhances the understanding and usage of internal company documents and reports. By extracting relevant context from corporate knowledge bases, RAG models facilitate tasks like summarization, information extraction, and complex question answering on domain-specific materials, enabling employees to quickly access vital insights from vast internal resources. This integration of AI with proprietary information can significantly improve efficiency, decision-making, and knowledge sharing across the organization. 
A typical RAG workflow consists of four key components: input prompt, document retrieval, contextual generation, and output. The process begins with a user query, which is used to search a comprehensive knowledge corpus. Relevant documents are then retrieved and combined with the original query to provide additional context for the LLM. This enriched input allows the model to generate more accurate and contextually appropriate responses. RAG‚Äôs popularity stems from its ability to use frequently updated external data, providing dynamic outputs without the need for costly and compute-intensive model retraining. 
To implement RAG effectively, many organizations turn to platforms like Amazon SageMaker JumpStart. This service offers numerous advantages for building and deploying generative AI applications, including access to a wide range of pre-trained models with ready-to-use artifacts, a user-friendly interface, and seamless scalability within the AWS ecosystem. By using pre-trained models and optimized hardware, SageMaker JumpStart enables rapid deployment of both LLMs and embedding models, minimizing the time spent on complex scalability configurations. 
In the previous post, we showed how to build a RAG application on SageMaker JumpStart using Facebook AI Similarity Search (Faiss). In this post, we show how to use Amazon OpenSearch Service as a vector store to build an efficient RAG application. 
Solution overview 
To implement our RAG workflow on SageMaker, we use a popular open source Python library known as LangChain. With LangChain, the RAG components are simplified into independent blocks that you can bring together using a chain object that will encapsulate the entire workflow. The solution consists of the following key components: 
 
 LLM (inference) ‚Äì We need an LLM that will do the actual inference and answer the end-user‚Äôs initial prompt. For our use case, we use Meta Llama3 for this component. LangChain comes with a default wrapper class for SageMaker endpoints with which we can simply pass in the endpoint name to define an LLM object in the library. 
 Embeddings model ‚Äì We need an embeddings model to convert our document corpus into textual embeddings. This is necessary for when we‚Äôre doing a similarity search on the input text to see what documents share similarities or contain the information to help augment our response. For this post, we use the BGE Hugging Face Embeddings model available in SageMaker JumpStart. 
 Vector store and retriever ‚Äì To house the different embeddings we have generated, we use a vector store. In this case, we use OpenSearch Service, which allows for similarity search using k-nearest neighbors (k-NN) as well as traditional lexical search. Within our chain object, we define the vector store as the retriever. You can tune this depending on how many documents you want to retrieve. 
 
The following diagram illustrates the solution architecture. 
 
In the following sections, we walk through setting up OpenSearch, followed by exploring the notebook that implements a RAG solution with LangChain, Amazon SageMaker AI, and OpenSearch Service. 
Benefits of using OpenSearch Service as a vector store for RAG 
In this post, we showcase how you can use a vector store such as OpenSearch Service as a knowledge base and embedding store. OpenSearch Service offers several advantages when used for RAG in conjunction with SageMaker AI: 
 
 Performance ‚Äì Efficiently handles large-scale data and search operations 
 Advanced search ‚Äì Offers full-text search, relevance scoring, and semantic capabilities 
 AWS integration ‚Äì Seamlessly integrates with SageMaker AI and other AWS services 
 Real-time updates ‚Äì Supports continuous knowledge base updates with minimal delay 
 Customization ‚Äì Allows fine-tuning of search relevance for optimal context retrieval 
 Reliability ‚Äì Provides high availability and fault tolerance through a distributed architecture 
 Analytics ‚Äì Provides analytical features for data understanding and performance improvement 
 Security ‚Äì Offers robust features such as encryption, access control, and audit logging 
 Cost-effectiveness ‚Äì Serves as an economical solution compared to proprietary vector databases 
 Flexibility ‚Äì Supports various data types and search algorithms, offering versatile storage and retrieval options for RAG applications 
 
You can use SageMaker AI with OpenSearch Service to create powerful and efficient RAG systems. SageMaker AI provides the machine learning (ML) infrastructure for training and deploying your language models, and OpenSearch Service serves as an efficient and scalable knowledge base for retrieval. 
OpenSearch Service optimization strategies for RAG 
Based on our learnings from the hundreds of RAG applications deployed using OpenSearch Service as a vector store, we‚Äôve developed several best practices: 
 
 If you are starting from a clean slate and want to move quickly with something simple, scalable, and high-performing, we recommend using an Amazon OpenSearch Serverless vector store collection. With OpenSearch Serverless, you benefit from automatic scaling of resources, decoupling of storage, indexing compute, and search compute, with no node or shard management, and you only pay for what you use. 
 If you have a large-scale production workload and want to take the time to tune for the best price-performance and the most flexibility, you can use an OpenSearch Service managed cluster. In a managed cluster, you pick the node type, node size, number of nodes, and number of shards and replicas, and you have more control over when to scale your resources. For more details on best practices for operating an OpenSearch Service managed cluster, see Operational best practices for Amazon OpenSearch Service. 
 OpenSearch supports both exact k-NN and approximate k-NN. Use exact k-NN if the number of documents or vectors in your corpus is less than 50,000 for the best recall. For use cases where the number of vectors is greater than 50,000, exact k-NN will still provide the best recall but might not provide sub-100 millisecond query performance. Use approximate k-NN in use cases above 50,000 vectors for the best performance. 
 OpenSearch uses algorithms from the NMSLIB, Faiss, and Lucene libraries to power approximate k-NN search. There are pros and cons to each k-NN engine, but we find that most customers choose Faiss due to its overall performance in both indexing and search as well as the variety of different quantization and algorithm options that are supported and the broad community support. 
 Within the Faiss engine, OpenSearch supports both Hierarchical Navigable Small World (HNSW) and Inverted File System (IVF) algorithms. Most customers find HNSW to have better recall than IVF and choose it for their RAG use cases. To learn more about the differences between these engine algorithms, see Vector search. 
 To reduce the memory footprint to lower the cost of the vector store while keeping the recall high, you can start with Faiss HNSW 16-bit scalar quantization. This can also reduce search latencies and improve indexing throughput when used with SIMD optimization. 
 If using an OpenSearch Service managed cluster, refer to Performance tuning for additional recommendations. 
 
Prerequisites 
Make sure you have access to one ml.g5.4xlarge and ml.g5.2xlarge instance each in your account. A secret should be created in the same region as the stack is deployed.Then complete the following prerequisite steps to create a secret using AWS Secrets Manager: 
 
 On the Secrets Manager console, choose Secrets in the navigation pane. 
 Choose Store a new secret. 
 
 
 
 For Secret type, select Other type of secret. 
 For Key/value pairs, on the Plaintext tab, enter a complete password. 
 Choose Next. 
 
 
 
 For Secret name, enter a name for your secret. 
 Choose Next. 
 
 
 
 Under Configure rotation, keep the settings as default and choose Next. 
 
 
 
 Choose Store to save your secret. 
 
 
 
 On the secret details page, note the secret Amazon Resource Name (ARN) to use in the next step. 
 
 
Create an OpenSearch Service cluster and SageMaker notebook 
We use AWS CloudFormation to deploy our OpenSearch Service cluster, SageMaker notebook, and other resources. Complete the following steps: 
 
 Launch the following CloudFormation template. 
 Provide the ARN of the secret you created as a prerequisite and keep the other parameters as default. 
 
 
 
 Choose Create to create your stack, and wait for the stack to complete (about 20 minutes). 
 When the status of the stack is CREATE_COMPLETE, note the value of OpenSearchDomainEndpoint on the stack Outputs tab. 
 Locate SageMakerNotebookURL in the outputs and choose the link to open the SageMaker notebook. 
 
Run the SageMaker notebook 
After you have launched the notebook in JupyterLab, complete the following steps: 
 
 Go to genai-recipes/RAG-recipes/llama3-RAG-Opensearch-langchain-SMJS.ipynb. 
 
You can also clone the notebook from the GitHub repo. 
 
 
 Update the value of&nbsp;OPENSEARCH_URL&nbsp;in the notebook with the value copied from&nbsp;OpenSearchDomainEndpoint in the previous step (look for os.environ['OPENSEARCH_URL'] = "").&nbsp; The port needs to be 443. 
 Run the cells in the notebook. 
 
The notebook provides a detailed explanation of all the steps. We explain some of the key cells in the notebook in this section. 
For the RAG workflow, we deploy the huggingface-sentencesimilarity-bge-large-en-v1-5 embedding model and meta-textgeneration-llama-3-8b-instruct LLM from Hugging Face. SageMaker JumpStart simplifies this process because the model artifacts, data, and container specifications are all prepackaged for optimal inference. These are then exposed using the SageMaker Python SDK high-level API calls, which let you specify the model ID for deployment to a SageMaker real-time endpoint: 
 
 
&nbsp;sagemaker.jumpstart.model&nbsp;&nbsp;JumpStartModel

model_id&nbsp;&nbsp;"meta-textgeneration-llama-3-8b-instruct"
accept_eula&nbsp;&nbsp;
model&nbsp;&nbsp;JumpStartModel(model_idmodel_id)
llm_predictor&nbsp;&nbsp;modeldeploy(accept_eulaaccept_eula)

model_id&nbsp;&nbsp;"huggingface-sentencesimilarity-bge-large-en-v1-5"
text_embedding_model&nbsp;&nbsp;JumpStartModel(model_idmodel_id)
embedding_predictor&nbsp;&nbsp;text_embedding_modeldeploy() 
 
Content handlers are crucial for formatting data for SageMaker endpoints. They transform inputs into the format expected by the model and handle model-specific parameters like temperature and token limits. These parameters can be tuned to control the creativity and consistency of the model‚Äôs responses. 
 
 class Llama38BContentHandler(LLMContentHandler):
&nbsp;&nbsp; &nbsp;content_type = "application/json"
&nbsp;&nbsp; &nbsp;accepts = "application/json"

&nbsp;&nbsp; &nbsp;def transform_input(self, prompt: str, model_kwargs: dict) -&gt; bytes:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;payload = {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"inputs": prompt,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"parameters": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"max_new_tokens": 1000,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"top_p": 0.9,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"temperature": 0.6,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"stop": ["&lt;|eot_id|&gt;"],
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;},
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;input_str = json.dumps(
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;payload,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;)
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;#print(input_str)
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;return input_str.encode("utf-8") 
 
We use PyPDFLoader from LangChain to load PDF files, attach metadata to each document fragment, and then use RecursiveCharacterTextSplitter to break the documents into smaller, manageable chunks. The text splitter is configured with a chunk size of 1,000 characters and an overlap of 100 characters, which helps maintain context between chunks. This preprocessing step is crucial for effective document retrieval and embedding generation, because it makes sure the text segments are appropriately sized for the embedding model and the language model used in the RAG system. 
 
 import&nbsp;numpy as&nbsp;np
from&nbsp;langchain_community.document_loaders import&nbsp;PyPDFLoader
from&nbsp;langchain.text_splitter import&nbsp;RecursiveCharacterTextSplitter
documents&nbsp;=&nbsp;[]
for&nbsp;idx, file&nbsp;in&nbsp;enumerate(filenames):
&nbsp;&nbsp; &nbsp;loader&nbsp;=&nbsp;PyPDFLoader(data_root&nbsp;+&nbsp;file)
&nbsp;&nbsp; &nbsp;document&nbsp;=&nbsp;loader.load()
&nbsp;&nbsp; &nbsp;for&nbsp;document_fragment&nbsp;in&nbsp;document:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;document_fragment.metadata =&nbsp;metadata[idx]
&nbsp;&nbsp; &nbsp;documents&nbsp;+=&nbsp;document
# - in our testing Character split works better with this PDF data set
text_splitter&nbsp;=&nbsp;RecursiveCharacterTextSplitter(
&nbsp;&nbsp; &nbsp;# Set a really small chunk size, just to show.
&nbsp;&nbsp; &nbsp;chunk_size=1000,
&nbsp;&nbsp; &nbsp;chunk_overlap=100,
)
docs&nbsp;=&nbsp;text_splitter.split_documents(documents)
print(docs[100]) 
 
The following block initializes a vector store using OpenSearch Service for the RAG system. It converts preprocessed document chunks into vector embeddings using a SageMaker model and stores them in OpenSearch Service. The process is configured with security measures like SSL and authentication to provide secure data handling. The bulk insertion is optimized for performance with a sizeable batch size. Finally, the vector store is wrapped with VectorStoreIndexWrapper, providing a simplified interface for operations like querying and retrieval. This setup creates a searchable database of document embeddings, enabling quick and relevant context retrieval for user queries in the RAG pipeline. 
 
 from&nbsp;langchain.indexes.vectorstore import&nbsp;VectorStoreIndexWrapper
# Initialize OpenSearchVectorSearch
vectorstore_opensearch&nbsp;=&nbsp;OpenSearchVectorSearch.from_documents(
&nbsp;&nbsp; &nbsp;docs,
&nbsp;&nbsp; &nbsp;sagemaker_embeddings,
&nbsp;&nbsp; &nbsp;http_auth=awsauth, &nbsp;# Auth will use the IAM role
&nbsp;&nbsp; &nbsp;use_ssl=True,
&nbsp;&nbsp; &nbsp;verify_certs=True,
&nbsp;&nbsp; &nbsp;connection_class=RequestsHttpConnection,
&nbsp;&nbsp; &nbsp;bulk_size=2000&nbsp;&nbsp;# Increase this to accommodate the number of documents you have
)
# Wrap the OpenSearch vector store with the VectorStoreIndexWrapper
wrapper_store_opensearch&nbsp;=&nbsp;VectorStoreIndexWrapper(vectorstore=vectorstore_opensearch) 
 
Next, we use the wrapper from the previous step along with the prompt template. We define the prompt template for interacting with the Meta Llama 3 8B Instruct model in the RAG system. The template uses specific tokens to structure the input in a way that the model expects. It sets up a conversation format with system instructions, user query, and a placeholder for the assistant‚Äôs response. The PromptTemplate class from LangChain is used to create a reusable prompt with a variable for the user‚Äôs query. This structured approach to prompt engineering helps maintain consistency in the model‚Äôs responses and guides it to act as a helpful assistant. 
 
 prompt_template&nbsp;=&nbsp;"""&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;
You are a helpful assistant.
&lt;|eot_id|&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;
{query}
&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;
"""
PROMPT&nbsp;=&nbsp;PromptTemplate(
&nbsp;&nbsp; &nbsp;template=prompt_template, input_variables=["query"]
)
query&nbsp;=&nbsp;"How did AWS perform in 2021?"

answer&nbsp;=&nbsp;wrapper_store_opensearch.query(question=PROMPT.format(query=query), llm=llm)
print(answer) 
 
Similarly, the notebook also shows how to use Retrieval QA, where you can customize how the documents fetched should be added to prompt using the chain_type parameter. 
Clean up 
Delete your SageMaker endpoints from the notebook to avoid incurring costs: 
 
 # Delete resources
llm_predictor.delete_model()
llm_predictor.delete_endpoint()
embedding_predictor.delete_model()
embedding_predictor.delete_endpoint() 
 
Next, delete your OpenSearch cluster to stop incurring additional charges:aws cloudformation delete-stack --stack-name rag-opensearch 
Conclusion 
RAG has revolutionized how businesses use AI by enabling general-purpose language models to work seamlessly with company-specific data. The key benefit is the ability to create AI systems that combine broad knowledge with up-to-date, proprietary information without expensive model retraining. This approach transforms customer engagement and internal operations by delivering personalized, accurate, and timely responses based on the latest company data. The RAG workflow‚Äîcomprising input prompt, document retrieval, contextual generation, and output‚Äîallows businesses to tap into their vast repositories of internal documents, policies, and data, making this information readily accessible and actionable. For businesses, this means enhanced decision-making, improved customer service, and increased operational efficiency. Employees can quickly access relevant information, while customers receive more accurate and personalized responses. Moreover, RAG‚Äôs cost-efficiency and ability to rapidly iterate make it an attractive solution for businesses looking to stay competitive in the AI era without constant, expensive updates to their AI systems. By making general-purpose LLMs work effectively on proprietary data, RAG empowers businesses to create dynamic, knowledge-rich AI applications that evolve with their data, potentially transforming how companies operate, innovate, and engage with both employees and customers. 
SageMaker JumpStart has streamlined the process of developing and deploying generative AI applications. It offers pre-trained models, user-friendly interfaces, and seamless scalability within the AWS ecosystem, making it straightforward for businesses to harness the power of RAG. 
Furthermore, using OpenSearch Service as a vector store facilitates swift retrieval from vast information repositories. This approach not only enhances the speed and relevance of responses, but also helps manage costs and operational complexity effectively. 
By combining these technologies, you can create robust, scalable, and efficient RAG systems that provide up-to-date, context-aware responses to customer queries, ultimately enhancing user experience and satisfaction. 
To get started with implementing this Retrieval Augmented Generation (RAG) solution using Amazon SageMaker JumpStart and Amazon OpenSearch Service, check out the example notebook on GitHub. You can also learn more about Amazon OpenSearch Service in the developer guide. 
 
About the authors 
Vivek Gangasani is a Lead Specialist Solutions Architect for Inference at AWS. He helps emerging generative AI companies build innovative solutions using AWS services and accelerated compute. Currently, he is focused on developing strategies for fine-tuning and optimizing the inference performance of large language models. In his free time, Vivek enjoys hiking, watching movies, and trying different cuisines. 
Harish Rao is a Senior Solutions Architect at AWS, specializing in large-scale distributed AI training and inference. He empowers customers to harness the power of AI to drive innovation and solve complex challenges. Outside of work, Harish embraces an active lifestyle, enjoying the tranquility of hiking, the intensity of racquetball, and the mental clarity of mindfulness practices. 
Raghu Ramesha is an ML Solutions Architect. He specializes in machine learning, AI, and computer vision domains, and holds a master‚Äôs degree in Computer Science from UT Dallas. In his free time, he enjoys traveling and photography. 
Sohaib Katariwala is a Sr. Specialist Solutions Architect at AWS focused on Amazon OpenSearch Service. His interests are in all things data and analytics. More specifically he loves to help customers use AI in their data strategy to solve modern day challenges. 
Karan Jain is a Senior Machine Learning Specialist at AWS, where he leads the worldwide Go-To-Market strategy for Amazon SageMaker Inference. He helps customers accelerate their generative AI and ML journey on AWS by providing guidance on deployment, cost-optimization, and GTM strategy. He has led product, marketing, and business development efforts across industries for over 10 years, and is passionate about mapping complex service features to customer solutions.
‚Ä¢ Advancing AI agent governance with Boomi and AWS: A unified approach to observability and compliance
  Just as APIs became the standard for integration, AI agents are transforming workflow automation through intelligent task coordination. AI agents are already enhancing decision-making and streamlining operations across enterprises. But as adoption accelerates, organizations face growing complexity in managing them at scale. Organizations struggle with observability and lifecycle management, finding it difficult to monitor performance and manage versions effectively. Governance and security concerns arise as these agents process sensitive data, which requires strict compliance and access controls. Perhaps most concerningly, without proper management, organizations face the risk of agent sprawl‚Äîthe unchecked proliferation of AI agents leading to inefficiency and security vulnerabilities. 
Boomi and AWS have collaborated to address the complexity surrounding AI agents with Agent Control Tower, an AI agent management solution developed by Boomi and tightly integrated with Amazon Bedrock. Agent Control Tower, part of the Boomi Agentstudio solution, provides the governance framework to manage this transformation, with capabilities that address both current and emerging compliance needs. 
As a leader in enterprise iPaaS per Gartner‚Äôs Magic Quadrant, based on Completeness of Vision and Ability to Execute, Boomi serves over 20,000 enterprise customers, with three-quarters of these customers operating on AWS. This includes a significant presence among Fortune 500 and Global 2000 organizations across critical sectors such as healthcare, finance, technology, and manufacturing. Boomi is innovating with generative AI, with more than 2,000 customers using its AI agents. The convergence of capabilities that Boomi provides‚Äîspanning AI, integration, automation, API management, and data management‚Äîwith AWS and its proven track record in reliability, security, and AI innovation creates a compelling foundation for standardized AI agent governance at scale. In this post, we share how Boomi partnered with AWS to help enterprises accelerate and scale AI adoption with confidence using Agent Control Tower. 
A unified AI management solution 
Built on AWS, Agent Control Tower uniquely delivers a single control plane for managing AI agents across multiple systems, including other cloud providers and on-premises environments. At its core, it offers comprehensive observability and monitoring, providing real-time performance tracking and deep visibility into agent decision-making and behavior. 
The following screenshot showcases how users can view summary data across agent providers and add or manage providers. 
 
The following screenshot shows an example of the Monitoring and Compliance dashboard. 
 
Agent Control Tower also provides a single pane of glass for visibility into the tools used by each agent, as illustrated in the following screenshot. 
 
Agent Control Tower provides key governance and security controls such as centralized policy enforcement and role-based access control, and enables meeting regulatory compliance with frameworks like GDPR and HIPAA. Furthermore, its lifecycle management capabilities enable automated agent discovery, version tracking, and operational control through features such as pause and resume functionality. Agent Control Tower is positioned as one of the first, if not the first, unified solutions that provides full lifecycle AI agent management with integrated governance and orchestration features. Although many vendors focus on releasing AI agents, there are few that focus on solutions for managing, deploying, and governing AI agents at scale. 
The following screenshot shows an example of how users can review agent details and disable or enable an agent. 
 
As shown in the following screenshot, users can drill down into details for each part of the agent. 
 
Amazon Bedrock: Enabling and enhancing AI governance 
Using Amazon Bedrock, organizations can implement security guardrails and content moderation while maintaining the flexibility to select and switch between AI models for optimized performance and accuracy. Organizations can create and enable access to curated knowledge bases and predefined action groups, enabling sophisticated multi-agent collaboration. Amazon Bedrock also provides comprehensive metrics and trace logs for agents to help facilitate complete transparency and accountability in agent operations. Through deep integration with Amazon Bedrock, Boomi‚Äôs Agent Control Tower enhances agent transparency and governance, offering a unified, actionable view of agent configurations and activities across environments. 
The following diagram illustrates the Agent Control Tower architecture on AWS. 
 
Business impact: Transforming enterprise AI operations 
Consider a global manufacturer using AI agents for supply chain optimization. With Agent Control Tower, they can monitor agent performance across regions in real time, enforce consistent security policies, and enable regulatory compliance. When issues arise, they can quickly identify and resolve them while maintaining the ability to scale AI operations confidently. With this level of control and visibility, organizations can deploy AI agents more effectively while maintaining robust security and compliance standards. 
Conclusion 
Boomi customers have already deployed more than 33,000 agents and are seeing up to 80% less time spent on documentation and 50% faster issue resolution. With Boomi and AWS, enterprises can accelerate and scale AI adoption with confidence, backed by a product that puts visibility, governance, and security first. Discover how Agent Control Tower can help your organization manage AI agent sprawl and take advantage of scalable, compliance-aligned innovation. Take a guided tour and learn more about Boomi Agent Control Tower and Amazon Bedrock integration. Or, you can get started today with AI FastTrack. 
 
About the authors 
 Deepak Chandrasekar is the VP of Software Engineering &amp; User Experience and leads multidisciplinary teams at Boomi. He oversees flagship initiatives like Boomi‚Äôs Agent Control Tower, Task Automation, and Market Reach, while driving a cohesive and intelligent experience layer across products. Previously, Deepak held a key leadership role at Unifi Software, which was acquired by Boomi. With a passion for building scalable, and intuitive AI-powered solutions, he brings a commitment to engineering excellence and responsible innovation. 
 Sandeep Singh is Director of Engineering at Boomi, where he leads global teams building solutions that enable enterprise integration and automation at scale. He drives initiatives like Boomi Agent Control Tower, Marketplace, and Labs, empowering partners and customers with intelligent, trusted solutions. With leadership experience at GE and Fujitsu, Sandeep brings expertise in API strategy, product engineering, and AI/ML solutions. A former solution architect, he is passionate about designing mission-critical systems and driving innovation through scalable, intelligent solutions. 
 Santosh Ameti is a seasoned Engineering leader in the Amazon Bedrock team and has built Agents, Evaluation, Guardrails, and Prompt Management solutions. His team continuously innovates in the agentic space, delivering one of the most secure and managed agentic solutions for enterprises. 
 Greg Sligh is a Senior Solutions Architect at AWS with more than 25 years of experience in software engineering, software architecture, consulting, and IT and Engineering leadership roles across multiple industries. For the majority of his career, he has focused on creating and delivering distributed, data-driven applications with particular focus on scale, performance, and resiliency. Now he helps ISVs meet their objectives across technologies, with particular focus on AI/ML. 
 Padma Iyer is a Senior Customer Solutions Manager at Amazon Web Services, where she specializes in supporting ISVs. With a passion for cloud transformation and financial technology, Padma works closely with ISVs to guide them through successful cloud transformations, using best practices to optimize their operations and drive business growth. Padma has over 20 years of industry experience spanning banking, tech, and consulting.

‚∏ª