âœ… Morning News Briefing â€“ September 23, 2025 10:43

ðŸ“… Date: 2025-09-23 10:43
ðŸ·ï¸ Tags: #briefing #ai #publichealth #digitalgov

â¸»

ðŸ§¾ Weather
â€¢ No watches or warnings in effect, Pembroke
  No watches or warnings in effect. No warnings or watches or watches in effect . Watch or warnings are no longer in effect in the U.S. No watches, warnings are in effect for the rest of the day . No watches and warnings are still in effect, but no watches are in place for the day's events . The weather is not expected to be affected by the weather .
â€¢ Current Conditions:  15.6Â°C
  Temperature: 15.6&deg;C Pressure / Tendency: 101.3 kPa rising Humidity: 100% Humidity is 100% Dewpoint: 15 .6&degree . Wind: ESE 3 km/h . Air Quality Health Index: n/a . Pembroke 6:00 AM EDT Tuesday 23 September 2025 . Weather forecast: 20,000
â€¢ Tuesday: Chance of showers. High 21. POP 40%
  40 percent chance of showers this afternoon. Risk of thunderstorm late this afternoon . Fog patches dissipating this morning. High 21. Humidex 26. UV index 3 or moderate. Chance of rain in the morning. Showery cloudy with rain in mid-afternoon. Rainy with risk of thunderstorms late afternoon. High risk of rain, thunderstorm possible late afternoon

ðŸŒ International News
No updates.

ðŸ Canadian News
No updates.

ðŸ‡ºðŸ‡¸ U.S. Top Stories
â€¢ Did Amazon trick people into paying for Prime? Federal case goes to trial
  The U.S. government says Amazon manipulated people into signing up for Prime memberships that were purposefully hard to cancel . The company says its designs and disclosures follow industry standards . Amazon says it is not breaking any laws, but the company says it has complied with industry standards and that the company's designs follow industry guidelines . The government says the Prime membership program was designed to be hard to
â€¢ How to reintegrate over a million veterans? Groups in Ukraine are working on it
  Ukrainian war veterans have physical and mental trauma, and struggle to return to civilian life . Here is a look at some groups trying to help ease them back into the community . Many Ukrainian war vets struggle to cope with the trauma they have suffered from, such as physical, mental and physical trauma, they struggle to get back on their feet and get back to life in civilian life. Here is
â€¢ 5 takeaways from Kamala Harris' new book about her sprint for the presidency
  Former Vice President Kamala Harris writes about her brief run for the presidency last year and her relationship with former President Joe Biden in her new book . Harris' new book, out Tuesday, is out on Tuesday at 8 p.m. ET on CNN.com.com/Heroes, Tuesday at 9 p a.m., Tuesday at 10 p a p.a.com .
â€¢ Georgia senators demand answers on more than a dozen deaths in immigration detention
  Sens. Jon Ossoff and Raphael Warnock are asking Homeland Security Secretary Kristi Noem for answers . More than a dozen people died in immigration detention, as the department rushes to expand . The department is rushing to expand its immigration detention facilities, as a result of the department's rapid expansion of detention facilities . Noem is being asked to explain the deaths of those who died in
â€¢ Judge orders Trump administration to restore $500 million in grant funding to UCLA
  A federal judge has ordered the Trump administration to restore $500 million in federal grant funding that it froze at the University of California, Los Angeles . A judge ordered the federal government to restore the funds . The University of Los Angeles is a major university in the U.S. state of California . The U.L.A. law school was founded in 1964 and won't accept federal

ðŸ§  Artificial Intelligence
No updates.

ðŸ’» Digital Strategy
â€¢ Linux's love-to-hate projects drop fresh versions: systemd 258 and GNOME 49
  Init system update arrives behind schedule while desktop overhaul adds app and HDR polish . There are fresh new releases of two of the more controversial and divisive projects in the Linux world for everyone to argue aboutâ€¦ and then adopt anyway . The new versions of the Init system and desktop overhauls the Linux desktop are among the most controversial projects in Linux world to be released in the last few months of the
â€¢ UK.gov ditching 'Red' risk data sharing project after slashing Â£0.5B budget in half
  A flagship Office for National Statistics project to share data across the UK government appears to be ending several years before its time after failing to make enough progress . Meanwhile Lotus Notes still lurks in some Office of National Statistics systems, for now . The project was given a "Red" risk rating two years in a row, and never appointed a program director . Lotus Notes is still in the UK
â€¢ UK chancellor Putin the blame on Russia for cyber chaos, but evidence says otherwise
  UK chancellor Rachel Reeves points finger at Moscow in interview when authorities reckon it's local lads . UK authorities reckon local hackers are responsible for Britain's latest cyber woes . But the trail of evidence points to attackers much closer to home, authorities say they're local . Reeves: Moscow is responsible for the UK's cyber woes, but it's not Moscow's, it's a local attack .
â€¢ HCL stretches support window for Domino v9/v10 despite repeated end-of-life deadlines
  Domino and Notes versions 9.0.x and 10.0 .x are now set to limp on until the end of this decade . Lotus Notes software is more difficult to kill than a horror movie villain, it seems, as it seems . Domino will be available to download again in the next few months, with the software set to be released in September 2015, with a
â€¢ Node4's Â£45M Tisski takeover ends in tears â€“ and Â£2.4M in damages
  Node4 has won a Â£2.4 million (c $3.2 million) damages award against the founder of Microsoft Dynamics consultancy Tisski . The High Court ruled the company was sold with problematic contracts that were collapsing as the deal was being finalized . The case details a perfect storm of NAO, MoD, and Aquila contract failures in a bid to sell Node4

ðŸ¥ Public Health
No updates.

ðŸ”¬ Science
â€¢ US investment in African food systems will have global benefits
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
â€¢ Trump links autism and Tylenol: is there any truth to it?
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
â€¢ Heat-related mortality in Europe during 2024 and health emergency forecasting to reduce preventable deaths
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
â€¢ Hotly anticipated US vaccine meeting ends with confusion â€” and a few decisions
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations

ðŸ§¾ Government & Policy
No updates.

ðŸ›ï¸ Enterprise Architecture & IT Governance
No updates.

ðŸ¤– AI & Emerging Tech
â€¢ AI models are using material from retracted scientific papers
  Some AI chatbots rely on flawed research from retracted scientific papers to answer questions, according to recent studies. The findings, confirmed by MIT Technology Review, raise questions about how reliable AI tools are at evaluating scientific research and could complicate efforts by countries and industries seeking to invest in AI tools for scientists.



AI search tools and chatbots are already known to fabricate links and references. But answers based on the material from actual papers can mislead as well if those papers have been retracted.&nbsp; The chatbot is â€œusing a real paper, real material, to tell you something,â€ says Weikuan Gu, a medical researcher at the University of Tennessee in Memphis and an author of one of the recent studies. But, he says, if people only look at the content of the answer and do not click through to the paper and see that itâ€™s been retracted, thatâ€™s really a problem.&nbsp;



Gu and his team asked OpenAIâ€™s ChatGPT, running on the GPT-4o model, questions based on information from 21 retracted papers on medical imaging. The chatbotâ€™s answers referenced retracted papers in five cases but advised caution in only three. While it cited non-retracted papers for other questions, the authors note it may not have recognized the retraction status of the articles. In a study from August, a different group of researchers used ChatGPT-4o mini to evaluate the quality of 217 retracted and low-quality papers from different scientific fields; they found that none of the chatbotâ€™s responses mentioned retractions or other concerns. (No similar studies have been released on GPT-5, which came out this August.)



The public uses AI chatbots to ask for medical advice and diagnose health conditions. Students and scientists increasingly use science-focused AI tools to review existing scientific literature and summarize papers. That kind of usage is likely to increase. The US National Science Foundation, for instance, invested $75 million in building AI models for science research this August.





â€œIf [a tool is] facing the general public, then using retraction as a kind of quality indicator is very important,â€ says Yuanxi Fu, an information science researcher at the University of Illinois Urbana-Champaign. Thereâ€™s â€œkind of an agreement that retracted papers have been struck off the record of science,â€ she says, â€œand the people who are outside of scienceâ€”they should be warned that these are retracted papers.â€ OpenAI did not provide a response to a request for comment about the paper results.



The problem is not limited to ChatGPT. In June, MIT Technology Review tested AI tools specifically advertised for research work, such as Elicit, Ai2 ScholarQA (now part of the Allen Institute for Artificial Intelligenceâ€™s Asta tool), Perplexity, and Consensus, using questions based on the 21 retracted papers in Guâ€™s study. Elicit referenced five of the retracted papers in its answers, while Ai2 ScholarQA referenced 17, Perplexity 11, and Consensus 18â€”all without noting the retractions.



Some companies have since made moves to correct the issue. â€œUntil recently, we didnâ€™t have great retraction data in our search engine,â€ says Christian Salem, cofounder of Consensus. His company has now started using retraction data from a combination of sources, including publishers and data aggregators, independent web crawling, and Retraction Watch, which manually curates and maintains a database of retractions. In a test of the same papers in August, Consensus cited only five retracted papers.&nbsp;



Elicit told MIT Technology Review that it removes retracted papers flagged by the scholarly research catalogue OpenAlex from its database and is â€œstill working on aggregating sources of retractions.â€ Ai2 told us that its tool does not automatically detect or remove retracted papers currently. Perplexity said that it â€œ[does] not ever claim to be 100% accurate.â€&nbsp;



However, relying on retraction databases may not be enough. Ivan Oransky, the cofounder of Retraction Watch, is careful not to describe it as a comprehensive database, saying that creating one would require more resources than anyone has: â€œThe reason itâ€™s resource intensive is because someone has to do it all by hand if you want it to be accurate.â€



Further complicating the matter is that publishers donâ€™t share a uniform approach to retraction notices. â€œWhere things are retracted, they can be marked as such in very different ways,â€ says Caitlin Bakker from University of Regina, Canada, an expert in research and discovery tools. â€œCorrection,â€ â€œexpression of concern,â€ â€œerratum,â€ and â€œretractedâ€ are among some labels publishers may add to research papersâ€”and these labels can be added for many reasons, including concerns about the content, methodology, and data or the presence of conflicts of interest.&nbsp;



Some researchers distribute their papers on preprint servers, paper repositories, and other websites, causing copies to be scattered around the web. Moreover, the data used to train AI models may not be up to date. If a paper is retracted after the modelâ€™s training cutoff date, its responses might not instantaneously reflect what&#8217;s going on, says Fu. Most academic search engines donâ€™t do a real-time check against retraction data, so you are at the mercy of how accurate their corpus is, says Aaron Tay, a librarian at Singapore Management University.



Oransky and other experts advocate making more context available for models to use when creating a response. This could mean publishing information that already exists, like peer reviews commissioned by journals and critiques from the review site PubPeer, alongside the published paper.&nbsp;&nbsp;



Many publishers, such as Nature and the BMJ, publish retraction notices as separate articles linked to the paper, outside paywalls. Fu says companies need to effectively make use of such information, as well as any news articles in a modelâ€™s training data that mention a paperâ€™s retraction.&nbsp;



The users and creators of AI tools need to do their due diligence. â€œWe are at the very, very early stages, and essentially you have to be skeptical,â€ says Tay.



Ananya is a freelance science and technology journalist based in Bengaluru, India.
â€¢ The Download: the LLM will see you now, and a new fusion power deal
  This is today&#8217;s edition ofÂ The Download,Â our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



This medical startup uses LLMs to run appointments and make diagnoses



Patients at a small number of clinics in Southern California run by the medical startup Akido Labs are spending relatively little time, or even no time at all, with their doctors. Instead, they see a medical assistant, who can lend a sympathetic ear but has limited clinical training.The job of formulating diagnoses and concocting a treatment plan is done by an LLM-based system called ScopeAI that transcribes and analyzes the dialogue between patient and assistant. A doctor then approves, or corrects, the AI systemâ€™s recommendations.According to Akidoâ€™s CEO, this approach allows doctors to see four to five times as many patients as they could previously. But experts arenâ€™t convinced that displacing so much of the cognitive work of medicine onto AI is the right way to remedy the doctor shortage. Read the full story.â€”Grace Huckins







An oil and gas giant signed a $1 billion deal with Commonwealth Fusion Systems



Eni, one of the worldâ€™s largest oil and gas companies, just agreed to buy $1 billion in electricity from a power plant being built by Commonwealth Fusion Systems. The deal is the latest to illustrate just how much investment Commonwealth and other fusion companies are courting as they attempt to take fusion power from the lab to the power grid.The agreement will see Eni purchase electricity from Commonwealthâ€™s first commercial fusion power plant, in Virginia. The facility is still in the planning stages but is scheduled to come online in the early 2030s. Read the full story.



â€”Casey Crownhart







The must-reads



Iâ€™ve combed the internet to find you todayâ€™s most fun/important/scary/fascinating stories about technology.



1 Trump officials are expected to link Tylenol to autismTheyâ€™re also likely to tout a lesser-known drug called leucovorin as a potential treatment. (WP $)+ Theyâ€™ll warn women in the early stages of pregnancy that they should only take Tylenol to treat high fevers. (Politico)+ But a huge study found no connection last year. (Axios)



2 Trump wants to charge skilled foreign workers $100,000 for H-1B visasThe decision is highly likely to harm US growth, especially in its tech sector. (The Guardian)+ The visa has been a lifeline for hundreds of thousands of tech workers. (BBC)+ Indian outsourcing companies are struggling to pivot. (Bloomberg $)+ Tech firms are sending memos to their workers on the visa. (Insider $)



3 The European Commission wants to ax cookie consent bannersA 2009 law triggered an influx in pesky pop-ups that the EU now wants to get rid of. (Politico)



4 The Murdochs and Michael Dell are among TikTokâ€™s potential buyersThe media mogul family and Dell founder are interested in shares, Trump says. (CNN)



5 Inside Chinaâ€™s plan to put its data centers to workA mega-cluster of centers is springing up in the city of Wuhu. (FT $)+ China built hundreds of AI data centers to catch the AI boom. Now many stand unused. (MIT Technology Review)



6 Seattleâ€™s tech scene is in troubleWhen its biggest firms slash their workforces, where does that leave everyone else? (WSJ $)



7 Innocent people are being scammed into scammingChinese gangs are imprisoning trafficking victims in compounds on the Myanmar-Thai border. (Reuters)+ Inside a romance scam compoundâ€”and how people get tricked into being there. (MIT Technology Review)



8 Europeâ€™s reusable rocket dream isnâ€™t entirely deadBut progress has been a lot slower than it should be. (Ars Technica)+ Elon Muskâ€™s utter dominance of space tech is hard to overestimate. (Wired $)+ Europe is finally getting serious about commercial rockets. (MIT Technology Review)



9 How ChatGPT fares as a financial stock pickerBe prepared to roll the dice. (Fast Company $)



10 Silicon Valley is ditching dating appsAnd turning to elite matchmakers instead. (The Information $)







Quote of the day



&#8220;I didn&#8217;t sleep all night. I kept thinking: What if I get stuck outside the US?&#8221;



â€”Akaash Hazarika, a Salesforce engineer, tells Insider he was forced to cut his vacation to Toronto short and rush back to America after the Trump administration announced changes to the H-1B skilled foreign worker visa.







One more thing







The quest to figure out farming on MarsOnce upon a time, water flowed across the surface of Mars. Waves lapped against shorelines, strong winds gusted and howled, and driving rain fell from thick, cloudy skies. It wasnâ€™t really so different from our own planet 4 billion years ago, except for one crucial detailâ€”its size. Mars is about half the diameter of Earth, and thatâ€™s where things went wrong.The Martian core cooled quickly, soon leaving the planet without a magnetic field. This, in turn, left it vulnerable to the solar wind, which swept away much of its atmosphere. Without a critical shield from the sunâ€™s ultraviolet rays, Mars could not retain its heat. Some of the oceans evaporated, and the subsurface absorbed the rest, with only a bit of water left behind and frozen at its poles. If ever a blade of grass grew on Mars, those days are over.But could they begin again? And what would it take to grow plants to feed future astronauts on Mars? Read the full story.



â€”David W. Brown







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)



+&nbsp; These abandoned blogs are a relic of the bygone internet (bring them back!)+ How to strengthen your bond with your reluctant cat + How Metal Gear Solid inspired the video to one of the greatest hits of the late 90s.+ If I had to explain British culture to someone, Iâ€™d just send them this video.
â€¢ An oil and gas giant signed a $1 billion deal with Commonwealth Fusion Systems
  Eni, one of the worldâ€™s largest oil and gas companies, just agreed to buy $1 billion in electricity from a power plant being built by Commonwealth Fusion Systems. The deal is the latest to illustrate just how much investment Commonwealth and other fusion companies are courting as they attempt to take fusion power from the lab to the power grid.&nbsp;





â€œThis is showing in concrete terms that people that use large amounts of energy, that know the energy marketâ€”they want fusion power, and theyâ€™re willing to contract for it and to pay for it,â€ said Bob Mumgaard, cofounder and CEO of Commonwealth, on a press call about the deal.&nbsp;&nbsp;&nbsp;



The agreement will see Eni purchase electricity from Commonwealthâ€™s first commercial fusion power plant, in Virginia. The facility is still in the planning stages but is scheduled to come online in the early 2030s.



The news comes a few weeks after Commonwealth announced a $863 million funding round, bringing its total funding raised to date to nearly $3 billion. The fusion company also announced earlier this year that Google would be its first commercial power customer for the Virginia plant.



Commonwealth, a spinout from MITâ€™s Plasma Science and Fusion Center, is widely considered one of the leading companies in fusion power. Investment in the company represents nearly one-third of the total global investment in private fusion companies. (MIT Technology Review is owned by MIT but is editorially independent.)



Eni has invested in Commonwealth since 2018 and participated in the latest fundraising round. The vast majority of the companyâ€™s business is in oil and gas, but in recent years itâ€™s made investments in technologies like biofuels and renewables.



â€œA company like usâ€”we cannot stay and wait for things to happen,â€ says Lorenzo Fiorillo, Eniâ€™s director of technology, research and development, and digital.&nbsp;



One open question is what, exactly, Eni plans to do with this electricity. When asked about it on the press call, Fiorillo referenced wind and solar plants that Eni owns and said the plan â€œis not different from what we do in other areas in the US and the world.â€ (Eni sells electricity from power plants that it owns, including renewable and fossil-fuel plants.)



Commonwealth is building tokamak fusion reactors that use superconducting magnets to hold plasma in place. That plasma is where fusion reactions happen, forcing hydrogen atoms together to release large amounts of energy.



The companyâ€™s first demonstration reactor, which it calls Sparc, is over 65% complete, and the team is testing components and assembling them. The plan is for the reactor, which is located outside Boston, to make plasma within two years and then demonstrate that it can generate more energy than is required to run it.



While Sparc is still under construction, Commonwealth is working on plans for Arc, its first commercial power plant. That facility should begin construction in 2027 or 2028 and generate electricity for the grid in the early 2030s, Mumgaard says.



Despite the billions of dollars Commonwealth has already raised, the company still needs more money to build its Arc power plantâ€”that will be a multibillion-dollar project, Mumgaard said on a press call in August about the companyâ€™s latest fundraising round.&nbsp;



The latest commitment from Eni could help Commonwealth secure the funding it needs to get Arc built. â€œThese agreements are a really good way to create the right environment for building up more investment,â€ says Paul Wilson, chair of the department of nuclear engineering and engineering physics at the University of Wisconsin, Madison.





Even though commercial fusion energy is still years away at a minimum, investors and big tech companies have pumped money into the industry and signed agreements to buy power from plants once theyâ€™re operational.&nbsp;



Helion, another leading fusion startup, has plans to produce electricity from its first reactor in 2028 (an aggressive timeline that has some experts expressing skepticism). That facility will have a full generating capacity of 50 megawatts, and in 2023 Microsoft signed an agreement to purchase energy from the facility in order to help power its data centers.



As billions of dollars pour into the fusion industry, there are still many milestones ahead. To date, only the National Ignition Facility at Lawrence Livermore National Laboratory has demonstrated that a fusion reactor can generate more energy than the amount put into the reaction. No commercial project has achieved that yet.&nbsp;



â€œThereâ€™s a lot of capital going out now to these startup companies,â€ says Ed Morse, a professor of nuclear engineering at the University of California, Berkeley. â€œWhat Iâ€™m not seeing is a peer-reviewed scientific article that makes me feel like, boy, we really turned the corner with the physics.â€



But others are taking major commercial deals from Commonwealth and others as reasons to be optimistic. â€œFusion is moving from the lab to be a proper industry,â€ says Sehila Gonzalez de Vicente, global director of fusion energy at the nonprofit Clean Air Task Force. â€œThis is very good for the whole sector to be perceived as a real source of energy.â€
â€¢ This medical startup uses LLMs to run appointments and make diagnoses
  Imagine this: Youâ€™ve been feeling unwell, so you call up your doctorâ€™s office to make an appointment. To your surprise, they schedule you in for the next day. At the appointment, you arenâ€™t rushed through describing your health concerns; instead, you have a full half hour to share your symptoms and worries and the exhaustive details of your health history with someone who listens attentively and asks thoughtful follow-up questions. You leave with a diagnosis, a treatment plan, and the sense that, for once, youâ€™ve been able to discuss your health with the care that it merits.





The catch? You might not have spoken to a doctor, or other licensed medical practitioner, at all.



This is the new reality for patients at a small number of clinics in Southern California that are run by the medical startup Akido Labs. These patientsâ€”some of whom are on Medicaidâ€”can access specialist appointments on short notice, a privilege typically only afforded to the wealthy few who patronize concierge clinics.



The key difference is that Akido patients spend relatively little time, or even no time at all, with their doctors. Instead, they see a medical assistant, who can lend a sympathetic ear but has limited clinical training. The job of formulating diagnoses and concocting a treatment plan is done by a proprietary, LLM-based system called ScopeAI that transcribes and analyzes the dialogue between patient and assistant. A doctor then approves, or corrects, the AI systemâ€™s recommendations.



â€œOur focus is really on what we can do to pull the doctor out of the visit,â€ says Jared Goodner, Akidoâ€™s CTO.&nbsp;



According to Prashant Samant, Akidoâ€™s CEO, this approach allows doctors to see four to five times as many patients as they could previously. Thereâ€™s good reason to want doctors to be much more productive. Americans are getting older and sicker, and many struggle to access adequate health care. The pending 15% reduction in federal funding for Medicaid will only make the situation worse.



But experts arenâ€™t convinced that displacing so much of the cognitive work of medicine onto AI is the right way to remedy the doctor shortage. Thereâ€™s a big gap in expertise between doctors and AI-enhanced medical assistants, says Emma Pierson, a computer scientist at UC Berkeley.&nbsp; Jumping such a gap may introduce risks. â€œI am broadly excited about the potential of AI to expand access to medical expertise,â€ she says. â€œItâ€™s just not obvious to me that this particular way is the way to do it.â€



AI is already everywhere in medicine. Computer vision tools identify cancers during preventive scans, automated research systems allow doctors to quickly sort through the medical literature, and LLM-powered medical scribes can take appointment notes on a clinicianâ€™s behalf. But these systems are designed to support doctors as they go about their typical medical routines.



What distinguishes ScopeAI, Goodner says, is its ability to independently complete the cognitive tasks that constitute a medical visit, from eliciting a patientâ€™s medical history to coming up with a list of potential diagnoses to identifying the most likely diagnosis and proposing appropriate next steps.



Under the hood, ScopeAI is a set of large language models, each of which can perform a specific step in the visitâ€”from generating appropriate follow-up questions based on what a patient has said to to populating a list of likely conditions. For the most part, these LLMs are fine-tuned versions of Metaâ€™s open-access Llama models, though Goodner says that the system also makes use of Anthropicâ€™s Claude models.&nbsp;



During the appointment, assistants read off questions from the ScopeAI interface, and ScopeAI produces new questions as it analyzes what the patient says. For the doctors who will review its outputs later, ScopeAI produces a concise note that includes a summary of the patientâ€™s visit, the most likely diagnosis, two or three alternative diagnoses, and recommended next steps, such as referrals or prescriptions. It also lists a justification for each diagnosis and recommendation.



ScopeAI is currently being used in cardiology, endocrinology, and primary care clinics and by Akidoâ€™s street medicine team, which serves the Los Angeles homeless population. That teamâ€”which is led by Steven Hochman, a doctor who specializes in addiction medicineâ€”meets patients out in the community to help them access medical care, including treatment for substance use disorders.&nbsp;



Previously, in order to prescribe a drug to treat an opioid addiction, Hochman would have to meet the patient in person; now, caseworkers armed with ScopeAI can interview patients on their own, and Hochman can approve or reject the systemâ€™s recommendations later. â€œIt allows me to be in 10 places at once,â€ he says.



Since they started using ScopeAI, the team has been able to get patients access to medications to help treat their substance use within 24 hoursâ€”something that Hochman calls â€œunheard of.â€



This arrangement is only possible because homeless patients typically get their health insurance from Medicaid, the public insurance system for low-income Americans. While Medicaid allows doctors to approve ScopeAI prescriptions and treatment plans asynchronously, both for street medicine and clinic visits, many other insurance providers require that doctors speak directly with patients before approving those recommendations. Pierson says that discrepancy raises concerns. â€œYou worry about that exacerbating health disparities,â€ she says.



Samant is aware of the appearance of inequity, and he says the discrepancy isnâ€™t intentionalâ€”itâ€™s just a feature of how the insurance plans currently work. He also notes that being seen quickly by an AI-enhanced medical assistant may be better than dealing with long wait times and limited provider availability, which is the status quo for Medicaid patients. And all Akido patients can opt for traditional doctorâ€™s appointments, if they are willing to wait for them, he says.



Part of the challenge of deploying a tool like ScopeAI is navigating a regulatory and insurance landscape that wasnâ€™t designed for AI systems that can independently direct medical appointments. Glenn Cohen, a professor at Harvard Law School, says that any AI system that effectively acts as a â€œdoctor in a boxâ€ would likely need to be approved by the FDA and could run afoul of medical licensure laws, which dictate that only doctors and other licensed professionals can practice medicine.



The California Medical Practice Act says that AI can&#8217;t replace a doctorâ€™s responsibility to diagnose and treat a patient, but doctors are allowed to use AI in their work, and they donâ€™t need to see patients in-person or in real-time before diagnosing them. Neither the FDA nor the Medical Board of California were able to say whether or not ScopeAI was on solid legal footing based only on a written description of the system.



But Samant is confident that Akido is in compliance, as ScopeAI was intentionally designed to fall short of being a â€œdoctor in a box.â€ Because the system requires a human doctor to review and approve of all of its diagnostic and treatment recommendations, he says, it doesnâ€™t require FDA approval.&nbsp;



At the clinic, this delicate balance between AI and doctor decision making happens entirely behind the scenes. Patients donâ€™t ever see the ScopeAI interface directlyâ€”instead, they speak with a medical assistant who asks questions in the way that a doctor might in a typical appointment. That arrangement might make patients feel more comfortable. But Zeke Emanuel, a professor of medical ethics and health policy at the University of Pennsylvania who served in the Obama and Biden administrations, worries that this comfort could be obscuring from patients the extent to which an algorithm is influencing their care.



Pierson agrees. â€œThat certainly isnâ€™t really what was traditionally meant by the human touch in medicine,â€ she says.



DeAndre Siringoringo, a medical assistant who works at Akidoâ€™s cardiology office in Rancho Cucamonga, says that while he tells the patients he works with that an AI system will be listening to the appointment in order to gather information for their doctor, he doesnâ€™t inform them about the specifics of how ScopeAI works, including the fact that it makes diagnostic recommendations to doctors.&nbsp;



Because all ScopeAI recommendations are reviewed by a doctor, that might not seem like such a big dealâ€”itâ€™s the doctor who makes the final diagnosis, not the AI. But itâ€™s been widely documented that doctors using AI systems tend to go along with the systemâ€™s recommendations more often than they should, a phenomenon known as automation bias.&nbsp;



At this point, itâ€™s impossible to know whether automation bias is affecting doctorsâ€™ decisions at Akido clinics, though Pierson says itâ€™s a riskâ€”especially when doctors arenâ€™t physically present for appointments. â€œI worry that it might predispose you to sort of nodding along in a way that you might not if you were actually in the room watching this happen,â€ she says.



An Akido spokesperson says that automation bias is a valid concern for any AI tool that assists a doctorâ€™s decision-making and that the company has made efforts to mitigate that bias. â€œWe designed ScopeAI specifically to reduce bias by proactively countering blind spots that can influence medical decisions, which historically lean heavily on physician intuition and personal experience,â€ she says. â€œWe also train physicians explicitly on how to use ScopeAI thoughtfully, so they retain accountability and avoid over-reliance.â€



Akido evaluates ScopeAIâ€™s performance by testing it on historical data and monitoring how often doctors correct its recommendations; those corrections are also used to further train the underlying models. Before deploying ScopeAI in a given specialty, Akido ensures that when tested on historical data sets, the system includes the correct diagnosis in its top three recommendations at least 92% of the time.



But Akido hasnâ€™t undertaken more rigorous testing, such as studies that compare ScopeAI appointments with traditional in-person or telehealth appointments, in order to determine whether the system improvesâ€”or at least maintainsâ€”patient outcomes. Such a study could help indicate whether automation bias is a meaningful concern.



â€œMaking medical care cheaper and more accessible is a laudable goal,â€ Pierson says. â€œBut I just think itâ€™s important to conduct strong evaluations comparing to that baseline.â€
â€¢ The Download: the CDCâ€™s vaccine chaos
  This is today&#8217;s edition ofÂ The Download,Â our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



A pivotal meeting on vaccine guidance is underwayâ€”and former CDC leaders are alarmed



This week has been an eventful one for Americaâ€™s public health agency. Two former leaders of the US Centers for Disease Control and Prevention explained why they suddenly departed in a Senate hearing. They also described how CDC employees are being instructed to turn their backs on scientific evidence.They painted a picture of a health agency in turmoilâ€”and at risk of harming the people it is meant to serve. And, just hours afterwards, a panel of CDC advisers voted to stop recommending the MMRV vaccine for children under four. Read the full story.



â€”Jessica Hamzelou



This article first appeared in The Checkup, MIT Technology Reviewâ€™s weekly biotech newsletter. To receive it in your inbox every Thursday, and read articles like this first, sign up here.



If youâ€™re interested in reading more about US vaccine policy, check out:



+ Read our profile of Jim Oâ€™Neill, the deputy health secretary and current acting CDC director.+ Why US federal health agencies are abandoning mRNA vaccines. Read the full story.+ Why childhood vaccines are a public health success story. No vaccine is perfect, but these medicines are still saving millions of lives. Read the full story.&nbsp;



+ The FDA plans to limit access to covid vaccines. Hereâ€™s why thatâ€™s not all bad.







Meet Sneha Goenka: our 2025 Innovator of the Year



Every year, MIT Technology Review selects one individual whose work we admire to recognize as Innovator of the Year. For 2025, we chose Sneha Goenka, who designed the computations behind the worldâ€™s fastest whole-genome sequencing method.&nbsp;



Thanks to her work, physicians can now sequence a patientâ€™s genome and diagnose a genetic condition in less than eight hoursâ€”an achievement that could transform medical care.



Register here to join an exclusive subscriber-only Roundtable conversation with Goenka, Leilani Battle, assistant professor at the University of Washington, and our editor in chief Mat Honan at 1pm ET next Tuesday September 23.







The must-reads



Iâ€™ve combed the internet to find you todayâ€™s most fun/important/scary/fascinating stories about technology.



1 The CDC voted against giving some children a combined vaccineÂ If accepted, the agency will stop recommending the MMRV vaccine for children under 4. (CNN)+ Its vote on hepatitis B vaccines for newborns is expected today too. (The Atlantic $)+ RFK JRâ€™s allies are closing ranks around him. (Politico)2 Russia is using Charlie Kirkâ€™s murder to sow division in the USIt&#8217;s using the momentum to push pro-Kremlin narratives and divide Americans. (WP $)+ The complicated phenomenon of political violence. (Vox)+ We donâ€™t know what being â€˜terminally onlineâ€™ means any more. (Wired $)3 Nvidia will invest $5 billion in IntelThe partnership allows Intel to develop custom CPUs to work with Nvidiaâ€™s chips. (WSJ $)+ Itâ€™s a much-needed financial shot in the arm for Intel. (WP $)+ Itâ€™s also great news for Intelâ€™s Asian suppliers. (Bloomberg $)



4 Medical AI tools downplay symptoms in women and ethnic minoritiesExperts fear that LLM-powered tools could lead to worse health outcomes. (FT $)+ Artificial intelligence is infiltrating health care. We shouldnâ€™t let it make all the decisions. (MIT Technology Review)



5 AI browsers have hit the mainstreamWhereâ€™s the off switch? (Wired $)+ AI means the end of internet search as weâ€™ve known it. (MIT Technology Review)



6 China has entered the global brain interface raceIts ambitious government-backed startups are primed to challenge Neuralink. (Bloomberg $)+ This patientâ€™s Neuralink brain implant gets a boost from generative AI. (MIT Technology Review)



7 What makes humans unique in the age of AI?Defining the distinctions between us and machines isnâ€™t as easy as it used to be. (New Yorker $)+ How AI can help supercharge creativity. (MIT Technology Review)



8 This ship helps to reconnect Africaâ€™s internetAI needs high speed internet, which needs undersea cables. (Rest of World)+ What Africa needs to do to become a major AI player. (MIT Technology Review)



9 Hundreds of people queued in Beijing to buy Appleâ€™s new iPhoneDesire for Apple products in the country appears to be alive and well. (Reuters)



10 San Franciscoâ€™s idea of a great night out? A robot cage fightItâ€™s certainly one way to have a good time. (NYT $)







Quote of the day



&#8220;Get off the iPad!&#8221;



â€”An irate air traffic controller tells the pilots of a Spirit Airlines flight to pay attention to avoid potentially colliding with Donald Trumpâ€™s Air Force One aircraft, Ars Technica reports.







One more thing







We used to get excited about technology. What happened?As a philosopher who studies AI and data, Shannon Vallorâ€™s Twitter feed is always filled with the latest tech news. Increasingly, sheâ€™s realized that the constant stream of information is no longer inspiring joy, but a sense of resignation.Joy is missing from our lives, and from our technology. Its absence is feeding a growing unease being voiced by many who work in tech or study it. Fixing it depends on understanding how and why the priorities in our tech ecosystem have changed. Read the full story.



We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)



+ Would you go about your daily business with a soft toy on your shoulder? This intrepid reporter gave it a go.+ How dying dinosaurs shaped the landscapes around us.+ I canâ€™t believe I missed Pythagorean Theorem day earlier this week.+ Inside the rise in popularity of the no-water yard.

ðŸ”’ Cybersecurity & Privacy
No updates.

ðŸŽ“ University AI
No updates.

ðŸ¢ Corporate AI
â€¢ Using AI to assist in rare disease diagnosis
  In the promising and rapidly evolving field of genetic analysis, the ability to accurately interpret whole genome sequencing data is crucial for diagnosing and improving outcomes for people with rare genetic diseases. Yet despite technological advancements, genetic professionals face steep challenges in managing and synthesizing the vast amounts of data required for these analyses. Fewer than 50% of&nbsp;initial&nbsp;cases yield a diagnosis, and while reanalysis can lead to new findings, the process remains&nbsp;time-consuming and complex.&nbsp;



To better understand and address these challenges, Microsoft Researchâ€”in collaboration with Drexel University and the Broad Instituteâ€‹â€‹â€”conducted a comprehensive study titledÂ AI-Enhanced Sensemaking: Exploring the Design of a Generative AI-Based Assistant to Support Genetic Professionals (opens in new tab).Â The study was recently published in a special edition ofÂ ACM Transactions on Interactive Intelligent SystemsÂ journal focused on generative AI.Â Â 



The study focused on integrating generative AI to support the complex, time-intensive, and information-dense sensemaking tasks inherent in whole genome sequencing analysis. Through detailed empirical research and collaborative design sessions with experts in the field, we identified key obstacles genetic professionals face and proposed AI-driven solutions to enhance their workflows.&nbsp;â€‹&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;â€‹We&nbsp;developed strategies for how generative AI can help synthesize biomedical data, enabling AI-expert collaboration to increase the diagnoses of previously unsolved rare diseasesâ€”ultimately aiming to improve patientsâ€™ quality of life and life expectancy.



Whole genome sequencing in rare disease diagnosis



Rare diseases affect up to half a billion people globally and obtaining a diagnosis can take multiple years. These diagnoses often involve specialist consultations, laboratory tests, imaging studies, and invasive procedures. Whole genome sequencing is used to identify genetic variants responsible for these diseases by comparing a patientâ€™s DNA sequence to reference genomes.&nbsp;â€‹â€‹Genetic professionals use bioinformatics tools such as&nbsp;seqr,&nbsp;an open-source, web-based tool for rare disease case analysis and project management to assist them in filtering and prioritizing&nbsp; > 1 million variants to determine their potential role in disease.&nbsp;A critical component of&nbsp;their&nbsp;work is sensemaking: the process of searching, filtering, and synthesizing data to build, refine, and present models from complex sets of gene and variant information.&nbsp;&nbsp;



â€‹â€‹The multi-step sequencing processâ€‹â€‹â€‹&nbsp;typically takes three to 12 weeks and requires extensive amounts of evidence and time to synthesize and aggregate information&nbsp;â€‹â€‹to understand the gene and variant effects for the patient.&nbsp;If a patient&#8217;s case goes unsolved, their whole genome sequencing data is set aside until enough time has passed to warrant a reanalysisâ€‹â€‹. This creates a backlog of patient casesâ€‹â€‹. The ability to easily&nbsp;identify&nbsp;when new scientific evidence&nbsp;emerges&nbsp;and when to reanalyze an unsolved patient case is key to shortening the time patients suffer with an unknown rare disease diagnosis.&nbsp;



The promise of AI systems to assist with complex human tasks



Approximately 87% of AI systems never reach deployment&nbsp;â€‹simply because they solveâ€‹â€‹â€‹&nbsp;the wrong problems.&nbsp;â€‹â€‹Understanding the AI support desired by different types of professionals, their current workflows, and AI capabilities is critical to successful AI system deployment and use. Matching technology capabilities with user tasks is particularly challenging in AI design because AI models can generate numerous outputs, and their capabilities can be unclear.&nbsp;â€‹To design an effectiveâ€‹â€‹â€‹&nbsp;AI-based systemâ€‹, one needs to identifyâ€‹&nbsp;â€‹â€‹tasks AI can support,&nbsp;â€‹â€‹determineâ€‹â€‹â€‹â€‹â€‹â€‹&nbsp;the appropriate level of AI involvement, and&nbsp;â€‹â€‹designâ€‹â€‹â€‹â€‹â€‹â€‹&nbsp;user-AI interactions. This necessitates considering how humans interact with technology and how&nbsp;â€‹â€‹AI&nbsp;can best be incorporated into workflows and tools.



	
		

		
		Spotlight: Microsoft research newsletter
	
	
	
						
				
					
				
			
			
			

									Microsoft Research Newsletter
				
								Stay connected to the research community at Microsoft.
				
								
					
						
							Subscribe today						
					
				
							
	
Opens in a new tab	
	


Study objectives and co-designing a genetic AI assistant



Our study aimed to understand the current challenges and needs of genetic professionals performing whole genome sequencing analyses and explore the tasks where they want an AI assistant to support them in their work. The first phase of our study involved interviews with 17 genetics professionals to better understand their workflows, tools, and challenges. They included genetic analysts directly involved in interpreting data, as well as other roles participating in whole genome sequencing. In the second phase of our study, we conducted co-design sessions with study participants on how an AI assistant could support their workflows. We then developed a prototype of an AI assistant, which was further tested and refined with study participants in follow-up design walk-through sessions.



Identifying challenges in whole genome sequencing analysis



Through our in-depth interviews with genetic professionals, our study uncovered three critical challenges in whole genome sequencing analysis:




Information Overload: Genetic analysts need to gather and synthesize vast amounts of data from multiple sources. This task is incredibly time-consuming and prone to human error.



Collaborative Sharing: Sharing findings with others in the field can be cumbersome and inefficient, often relying on outdated methods that slow the collaborative analysis process.



Prioritizing Reanalysis: Given the continuous influx of new scientific discoveries, prioritizing unsolved cases to reanalyze is a daunting challenge. Analysts need a systematic approach to identify cases that might benefit most from reanalysis.




Genetic professionals highlighted the time-consuming nature of gathering and synthesizing information about genes and variants from different data sources. Other genetic professionals may have insights into certain genes and variants, but sharing and interpreting information with others for collaborative sensemaking requires significant time and effort. Although new scientific findings could affect unsolved cases through reanalysis, prioritizing cases based on new findings was challenging given the number of unsolved cases and limited time of genetic professionals.



Co-designing with experts and AI-human sensemaking tasks



Our study participants prioritized two potential tasks of an AI assistant. The first task was flagging cases for reanalysis based on new scientific findings. The assistant would alert analysts to unsolved cases that could benefit from new research, providing relevant updates drawn from recent publications. The second task focused on aggregating and synthesizing information about genes and variants from the scientific literature. This feature would compile essential information from numerous scientific papers about genes and variants, presenting it in a user-friendly format and saving analysts significant time and effort. Participants emphasized the need to balance selectivity with comprehensiveness in the evidence they review. They also envisioned collaborating with other genetic professionals to interpret, edit, and verify artifacts generated by the AI assistant.



Genetic professionals require both broad and focused evidence at different stages of their workflow. The AI assistant prototypes were designed to allow flexible filtering and thorough evidence aggregation, ensuring users can delve into comprehensive data or selectively focus on pertinent details. The prototypes included features for collaborative sensemaking, enabling users to interpret, edit, and verify AI-generated information collectively. This&nbsp;â€‹â€‹approach not only&nbsp;â€‹underscoresâ€‹â€‹â€‹&nbsp;the trustworthiness of AI outputs, but also facilitates shared understanding and decision-making among genetic professionals.



Design implications for expert-AI sensemaking



In the&nbsp;shifting frontiers of genome sequence analysis,&nbsp;leveraging generative AI to enhance sensemaking offers intriguing possibilitiesâ€‹â€‹. The task of staying&nbsp;â€‹â€‹currentâ€‹â€‹â€‹â€‹â€‹â€‹, synthesizing information from diverse sources, and making informed decisions&nbsp;â€‹â€‹is challengingâ€‹â€‹â€‹â€‹â€‹â€‹.&nbsp;&nbsp;



Our study participants emphasized the hurdles in integrating data from multiple sources without losing critical components, documenting decision rationales, and fostering collaborative environments. Generative AI models, with their advanced capabilities, have started to address these challenges by automatically generating interactive artifacts to support sensemaking. However, the effectiveness of such systems hinges on careful design considerations,&nbsp;â€‹â€‹particularly in how they facilitate distributed sensemaking, support both initial and ongoing sensemaking, and combine evidence from multiple modalities. We next discuss three design considerations for using generative AI models to support sensemaking.



Distributed expert-AI sensemaking design



Generative AI models can create artifacts that aid an individual user&#8217;s sensemaking process; however, the true potential lies in sharing these artifacts among users to foster collective understanding and efficiency. Participants in our study emphasized the importance of explainability, feedback, and trust when interacting with AI-generated content.&nbsp;â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹Trust is gained byâ€‹â€‹â€‹â€‹â€‹â€‹&nbsp;viewing portions of artifacts marked as correct by other users, or observing edits made to AI-generated informationâ€‹â€‹.&nbsp;â€‹â€‹Someâ€‹â€‹â€‹â€‹â€‹â€‹&nbsp;usersâ€‹, however,â€‹&nbsp;cautioned against over-reliance on AI, which could obscure underlying inaccuracies. Thus, design strategies should ensure that any corrections are clearly marked&nbsp;â€‹â€‹and annotatedâ€‹â€‹â€‹â€‹â€‹â€‹. Furthermore, to enhance distributed sensemaking, visibility of others&#8217; notes and context-specific synthesis through AI can streamline the processâ€‹â€‹.&nbsp;



Initial expert-AI sensemaking and re-sensemaking design



In our fast-paced, information-driven world,&nbsp;â€‹â€‹it is essential to understand a situation both&nbsp;initially&nbsp;and again when new information arises.â€‹â€‹&nbsp;â€‹â€‹Sensemaking is inherently temporal, reflecting and shaping our understanding of time as we revisit tasks to reevaluate past decisions or incorporate new information. Generative AI plays a pivotal role here by transforming static data into dynamic artifacts that evolve, offering a comprehensive view of past rationales. Such AI-generated artifacts provide continuity, allowing usersâ€”both&nbsp;original decision-makers or new individualsâ€”to access the rationale behind decisions made in earlier task instances. By continuously editing and updating these artifacts, generative AI highlights new information since the last review, supporting ongoing understanding and decision-making.&nbsp;Moreover, AI systems enhance&nbsp;â€‹â€‹transparencyâ€‹â€‹â€‹â€‹â€‹â€‹&nbsp;by summarizing previous notes and questions, offering insights into earlier thought processes and facilitating a deeper understanding of how conclusions were drawn. This reflective capability not only can reinforce initial sensemaking efforts but also equips users with the clarity needed for informed re-sensemaking as new data emerges.&nbsp;



Combining evidence from multiple modalities to enhance AI-expert sensemaking



â€‹â€‹â€‹Theâ€‹â€‹â€‹â€‹â€‹â€‹&nbsp;ability to combine evidence from multiple modalities is essential for effective sensemaking. Users often need to integrate diverse types of dataâ€”text, images, spatial coordinates, and moreâ€”into a coherent narrative to make informed decisions. Consider the case of search and rescue operations, where workers must rapidly synthesize information from texts, photographs, and GPS data to strategize their efforts. Recent advancements in multimodal generative AI models have empowered users by incorporating and synthesizing these varied inputs into a unified, comprehensive view. For instance, a participant in our study illustrated this capability by using a generative AI model to merge text from scientific publications with a visual gene structure depiction. This integration&nbsp;â€‹â€‹could createâ€‹â€‹â€‹â€‹â€‹â€‹&nbsp;an image that contextualizes an individual&#8217;s genetic variant within the&nbsp;â€‹â€‹contextâ€‹â€‹â€‹â€‹â€‹â€‹&nbsp;of documented variants. Such advanced synthesis enables users to capture complex relationships and insights briefly, streamlining decision-making and expanding the potential for innovative solutions across diverse fields.&nbsp;



Sensemaking Process with AI Assistant



Figure: Sensemaking process when interpreting variants with the introduction of prototype AI assistant. Gray boxes represent sensemaking activities which are currently performed by an analyst but are human-in-the-loop processes with involvement of our prototype AI assistant. Non-gray boxes represent activities reserved for analyst completion without assistance by our AI assistant prototype. Within the foraging searching and synthesizing processes, examples of data sources and data types for each, respectively, are connected by dotted lines.



Conclusion



We explored the potential of generative AI&nbsp;to supportâ€‹â€‹ genetic professionalsâ€‹&nbsp;â€‹in diagnosing rare diseasesâ€‹â€‹. By designing an AI-based assistant, we aim to streamline whole genome sequencing analysis, helping professionals diagnose rare genetic diseases more efficiently. Our study unfolded in two key phases:&nbsp;â€‹pinpointingâ€‹â€‹â€‹&nbsp;existing challenges in analysis, and design ideation, where we crafted a prototype AI assistant. This tool is designed to boost diagnostic yield and cut down diagnosis time by flagging cases for reanalysis and synthesizing crucial gene and variant data. Despite valuable findings, more research is neededâ€‹â€‹. Future research will involve testing the AI assistant in real-time, task-based user testing with genetic professionals to assess the AI&#8217;s impact on their workflow. The promise of AI advancements lies in solving the right user problems and building the appropriate solutions, achieved through collaboration among model developers, domain experts, system designers, and HCI researchers. By fostering these collaborations, we aim to develop robust, personalized AI assistants tailored to specific domains.&nbsp;



Join the conversation



Join us as we continue to explore the transformative potential of generative AI in genetic analysis, and please read the full text publication&nbsp;here (opens in new tab). Follow us on social media, share this post with your network, and let us know your thoughts on how AI can transform genetic research. If interested in our other related research work, check out&nbsp;Evidence Aggregator: AI reasoning applied to rare disease diagnosis. (opens in new tab)&nbsp;&nbsp;




Opens in a new tabThe post Using AI to assist in rare disease diagnosis appeared first on Microsoft Research.
â€¢ Rapid ML experimentation for enterprises with Amazon SageMaker AI and Comet
  This post was written with Sarah Ostermeier from Comet. 
As enterprise organizations scale their machine learning (ML) initiatives from proof of concept to production, the complexity of managing experiments, tracking model lineage, and managing reproducibility grows exponentially. This is primarily because data scientists and ML engineers constantly explore different combinations of hyperparameters, model architectures, and dataset versions, generating massive amounts of metadata that must be tracked for reproducibility and compliance. As the ML model development scales across multiple teams and regulatory requirements intensify, tracking experiments becomes even more complex. With increasing AI regulations, particularly in the EU, organizations now require detailed audit trails of model training data, performance expectations, and development processes, making experiment tracking a business necessity and not just a best practice. 
Amazon SageMaker AI provides the managed infrastructure enterprises need to scale ML workloads, handling compute provisioning, distributed training, and deployment without infrastructure overhead. However, teams still need robust experiment tracking, model comparison, and collaboration capabilities that go beyond basic logging. 
Comet is a comprehensive ML experiment management platform that automatically tracks, compares, and optimizes ML experiments across the entire model lifecycle. It provides data scientists and ML engineers with powerful tools for experiment tracking, model monitoring, hyperparameter optimization, and collaborative model development. It also offers Opik, Cometâ€™s open source platform for LLM observability and development. 
Comet is available in SageMaker AI as a Partner AI App, as a fully managed experiment management capability, with enterprise-grade security, seamless workflow integration, and a straightforward procurement process through AWS Marketplace. 
The combination addresses the needs of an enterprise ML workflow end-to-end, where SageMaker AI handles infrastructure and compute, and Comet provides the experiment management, model registry, and production monitoring capabilities that teams require for regulatory compliance and operational efficiency. In this post, we demonstrate a complete fraud detection workflow using SageMaker AI with Comet, showcasing reproducibility and audit-ready logging needed by enterprises today. 
Enterprise-ready Comet on SageMaker AI 
Before proceeding to setup instructions, organizations must identify their operating model and based on that, decide how Comet is going to be set up. We recommend implementing Comet using a federated operating model. In this architecture, Comet is centrally managed and hosted in a shared services account, and each data science team maintains fully autonomous environments. Each operating model comes with their own sets of benefits and limitations. For more information, refer to SageMaker Studio Administration Best Practices. 
Letâ€™s dive into the setup of Comet in SageMaker AI. Large enterprise generally have the following personas: 
 
 Administrators â€“ Responsible for setting up the common infrastructure services and environment for use case teams 
 Users â€“ ML practitioners from use case teams who use the environments set up by platform team to solve their business problems 
 
In the following sections, we go through each personaâ€™s journey. 
Comet works well with both SageMaker AI and Amazon SageMaker. SageMaker AI provides the Amazon SageMaker Studio integrated development environment (IDE), and SageMaker provides the Amazon SageMaker Unified Studio IDE. For this post, we use SageMaker Studio. 
Administrator journey 
In this scenario, the administrator receives a request from a team working on a fraud detection use case to provision an ML environment with a fully managed training and experimentation setup. The administratorâ€™s journey includes the following steps: 
 
 Follow the prerequisites to set up Partner AI Apps. This sets up permissions for administrators, allowing Comet to assume a SageMaker AI execution role on behalf of the users and additional privileges for managing the Comet subscription through AWS Marketplace. 
 On the SageMaker AI console, under Applications and IDEs in the navigation pane, choose Partner AI Apps, then choose View details for Comet. 
 
 
The details are shown, including the contract pricing model for Comet and infrastructure tier estimated costs. 
 
Comet provides different subscription options ranging from a 1-month to 36-month contract. With this contract, users can access Comet in SageMaker. Based on the number of users, the admin can review and analyze the appropriate instance size for the Comet dashboard server. Comet supports 5â€“500 users running more than 100 experiment jobs.. 
 
 Choose Go to Marketplace to subscribe to be redirected to the Comet listing on AWS Marketplace. 
 Choose View purchase options. 
 
 
 
 In the subscription form, provide the required details. 
 
 
When the subscription is complete, the admin can start configuring Comet. 
 
 
 While deploying Comet, add the project lead of the fraud detection use case team as an admin to manage the admin operations for the Comet dashboard. 
 
It takes a few minutes for the Comet server to be deployed. For more details on this step, refer to Partner AI App provisioning. 
 
 Set up a SageMaker AI domain following the steps in Use custom setup for Amazon SageMaker AI. As a best practice, provide a pre-signed domain URL for the use case team member to directly access the Comet UI without logging in to the SageMaker console. 
 Add the team members to this domain and enable access to Comet while configuring the domain. 
 
Now the SageMaker AI domain is ready for users to log in to and start working on the fraud detection use case. 
User journey 
Now letâ€™s explore the journey of an ML practitioner from the fraud detection use case. The user completes the following steps: 
 
 Log in to the SageMaker AI domain through the pre-signed URL. 
 
You will be redirected to the SageMaker Studio IDE. Your user name and AWS Identity and Access Management (IAM) execution role are preconfigured by the admin. 
 
 Create a JupyterLab Space following the JupyterLab user guide. 
 You can start working on the fraud detection use case by spinning up a Jupyter notebook. 
 
The admin has also set up required access to the data through an Amazon Simple Storage Service (Amazon S3) bucket. 
 
 To access Comet APIs, install the comet_ml library and configure the required environment variables as described in Set up the Amazon SageMaker Partner AI Apps SDKs. 
 To access the Comet UI, choose Partner AI Apps in the SageMaker Studio navigation pane and choose Open for Comet. 
 
 
Now, letâ€™s walk through the use case implementation. 
Solution overview 
This use case highlights common enterprise challenges: working with imbalanced datasets (in this example, only 0.17% of transactions are fraudulent), requiring multiple experiment iterations, and maintaining full reproducibility for regulatory compliance. To follow along, refer to the Comet documentation and Quickstart guide for additional setup and API details. 
For this use case, we use the Credit Card Fraud Detection dataset. The dataset contains credit card transactions with binary labels representing fraudulent (1) or legitimate (0) transactions. In the following sections, we walk through some of the important sections of the implementation. The entire code of the implementation is available in the GitHub repository. 
Prerequisites 
As a prerequisite, configure the necessary imports and environment variables for the Comet and SageMaker integration: 
 
 # Comet ML for experiment tracking
import comet_ml
from comet_ml import Experiment, API, Artifact
from comet_ml.integration.sagemaker import log_sagemaker_training_job_v1
AWS_PARTNER_APP_AUTH=true
AWS_PARTNER_APP_ARN=&lt;Your_AWS_PARTNER_APP_ARN&gt;
COMET_API_KEY=&lt;Your_Comet_API_Key&gt; 	
# From Details Page, click Open Comet. In the top #right corner, click on user -&gt; API # Key
# Comet ML configuration
COMET_WORKSPACE = '&lt;your-comet-workspace-name&gt;'
COMET_PROJECT_NAME = '&lt;your-comet-project-name&gt;' 
 
Prepare the dataset 
One of Cometâ€™s key enterprise features is automatic dataset versioning and lineage tracking. This capability provides full auditability of what data was used to train each model, which is critical for regulatory compliance and reproducibility. Start by loading the dataset: 
 
 # Create a Comet Artifact to track our raw dataset
dataset_artifact = Artifact(
    name="fraud-dataset",
    artifact_type="dataset",
    aliases=["raw"]
)
# Add the raw dataset file to the artifact
dataset_artifact.add_remote(s3_data_path, metadata={
    "dataset_stage": "raw", 
    "dataset_split": "not_split", 
    "preprocessing": "none"
}) 
 
Start a Comet experiment 
With the dataset artifact created, you can now start tracking the ML workflow. Creating a Comet experiment automatically begins capturing code, installed libraries, system metadata, and other contextual information in the background. You can log the dataset artifact created earlier in the experiment. See the following code: 
 
 # Create a new Comet experiment
experiment_1 = comet_ml.Experiment(
    project_name=COMET_PROJECT_NAME,
    workspace=COMET_WORKSPACE,
)
# Log the dataset artifact to this experiment for lineage tracking
experiment_1.log_artifact(dataset_artifact) 
 
Preprocess the data 
The next steps are standard preprocessing steps, including removing duplicates, dropping unneeded columns, splitting into train/validation/test sets, and standardizing features using scikit-learnâ€™s StandardScaler. We wrap the processing code in preprocess.py and run it as a SageMaker Processing job. See the following code: 
 
 # Run SageMaker processing job
processor = SKLearnProcessor(
    framework_version='1.0-1',
    role=sagemaker.get_execution_role(),
    instance_count=1,
    instance_type='ml.t3.medium'
)
processor.run(
    code='preprocess.py',
    inputs=[ProcessingInput(source=s3_data_path, destination='/opt/ml/processing/input')],
    outputs=[ProcessingOutput(source='/opt/ml/processing/output', destination=f's3://{bucket_name}/{processed_data_prefix}')]
) 
 
After you submit the processing job, SageMaker AI launches the compute instances, processes and analyzes the input data, and releases the resources upon completion. The output of the processing job is stored in the S3 bucket specified. 
Next, create a new version of the dataset artifact to track the processed data. Comet automatically versions artifacts with the same name, maintaining complete lineage from raw to processed data. 
 
 # Create an updated version of the 'fraud-dataset' Artifact for the preprocessed data
preprocessed_dataset_artifact = Artifact(
    name="fraud-dataset",
    artifact_type="dataset", 
    aliases=["preprocessed"],
    metadata={
        "description": "Credit card fraud detection dataset",
        "fraud_percentage": f"{fraud_percentage:.3f}%",
        "dataset_stage": "preprocessed",
        "preprocessing": "StandardScaler + train/val/test split",
    }
)
# Add our train, validation, and test dataset files as remote assets 
preprocessed_dataset_artifact.add_remote(
    uri=f's3://{bucket_name}/{processed_data_prefix}',
    logical_path='split_data'
)
# Log the updated dataset to the experiment to track the updates
experiment_1.log_artifact(preprocessed_dataset_artifact) 
 
The Comet and SageMaker AI experiment workflow 
Data scientists prefer rapid experimentation; therefore, we organized the workflow into reusable utility functions that can be called multiple times with different hyperparameters while maintaining consistent logging and evaluation across all runs. In this section, we showcase the utility functions along with a brief snippet of the code inside the function: 
 
 train() â€“ Spins up a SageMaker model training job using the SageMaker built-in XGBoost algorithm: 
 
 
     # Create SageMaker estimator
    estimator = Estimator(
        image_uri=xgboost_image,
        role=execution_role,
        instance_count=1,
        instance_type='ml.m5.large',
        output_path=model_output_path,
        sagemaker_session=sagemaker_session_obj,
        hyperparameters=hyperparameters_dict,
        max_run=1800  # Maximum training time in seconds
    )
    # Start training
    estimator.fit({
        'train': train_channel,
        'validation': val_channel
    }) 
 
 
 log_training_job() â€“ Captures the training metadata and metrics and links the model asset to the experiment for complete traceability: 
 
 
 # Log SageMaker training job to Comet 
    log_sagemaker_training_job_v1(
        estimator=training_estimator,
        experiment=api_experiment
    ) 
 
 
 log_model_to_comet() â€“ Links model artifacts to Comet, captures the training metadata, and links the model asset to the experiment for complete traceability: 
 
 
 experiment.log_remote_model(
        model_name=model_name,
        uri=model_artifact_path,
        metadata=metadata
    ) 
 
 
 deploy_and_evaluate_model() â€“ Performs model deployment and evaluation, and metric logging: 
 
 
 # Deploy to endpoint
predictor = estimator.deploy(
initial_instance_count=1,
       instance_type="ml.m5.xlarge")
# Log metrics and visualizations to Comet 
experiment.log_metrics(metrics) experiment.log_confusion_matrix(matrix=cm,labels=['Normal', 'Fraud']) 
# Log ROC curve 
fpr, tpr, _ = roc_curve(y_test, y_pred_prob_as_np_array) experiment.log_curve("roc_curve", x=fpr, y=tpr) 
 
The complete prediction and evaluation code is available in the GitHub repository. 
Run the experiments 
Now you can run multiple experiments by calling the utility functions with different configurations and compare experiments to find the most optimal settings for the fraud detection use case. 
For the first experiment, we establish a baseline using standard XGBoost hyperparameters: 
 
 # Define hyperparameters for first experiment
hyperparameters_v1 = {
    'objective': 'binary:logistic',	# Binary classification
    'num_round': 100,                   # Number of boosting rounds
    'eval_metric': 'auc',               # Evaluation metric
    'learning_rate': 0.15,              # Learning rate
    'booster': 'gbtree'                 # Booster algorithm
}
# Train the model
estimator_1 = train(
    model_output_path=f"s3://{bucket_name}/{model_output_prefix}/1",
    execution_role=role,
    sagemaker_session_obj=sagemaker_session,
    hyperparameters_dict=hyperparameters_v1,
    train_channel_loc=train_channel_location,
    val_channel_loc=validation_channel_location
)
# log the training job and model artifact
log_training_job(experiment_key = experiment_1.get_key(), training_estimator=estimator_1)
log_model_to_comet(experiment = experiment_1,
                   model_name="fraud-detection-xgb-v1", 
                   model_artifact_path=estimator_1.model_data, 
                   metadata=metadata)
# Deploy and evaluate
deploy_and_evaluate_model(experiment=experiment_1,
                          estimator=estimator_1,
                          X_test_scaled=X_test_scaled,
                          y_test=y_test
                          ) 
 
While running a Comet experiment from a Jupyter notebook, we need to end the experiment to make sure everything is captured and persisted in the Comet server. See the following code: experiment_1.end() 
When the baseline experiment is complete, you can run additional experiments with different hyperparameters. Check out the notebook to see the details of both experiments. 
When the second experiment is complete, navigate to the Comet UI to compare these two experiment runs. 
View Comet experiments in the UI 
To access the UI, you can locate the URL in the SageMaker Studio IDE or by executing the code provided in the notebook: experiment_2.url 
The following screenshot shows the Comet experiments UI. The experiment details are for illustration purposes only and do not represent a real-world fraud detection experiment. 
 
This concludes the fraud detection experiment. 
Clean up 
For the experimentation part, SageMaker processing and training infrastructure is ephemeral in nature and shuts down automatically when the job is complete. However, you must still manually clean up a few resources to avoid unnecessary costs: 
 
 Shut down the SageMaker JupyterLab Space after use. For instructions, refer to Idle shutdown. 
 The Comet subscription renews based on the contract chosen. Cancel the contract when there is no further requirement to renew the Comet subscription. 
 
Advantages of SageMaker and Comet integration 
Having demonstrated the technical workflow, letâ€™s examine the broader advantages this integration provides. 
Streamlined model development 
The Comet and SageMaker combination reduces the manual overhead of running ML experiments. While SageMaker handles infrastructure provisioning and scaling, Cometâ€™s automatic logging captures hyperparameters, metrics, code, installed libraries, and system performance from your training jobs without additional configuration. This helps teams focus on model development rather than experiment bookkeeping.Cometâ€™s visualization capabilities extend beyond basic metric plots. Built-in charts enable rapid experiment comparison, and custom Python panels support domain-specific analysis tools for debugging model behavior, optimizing hyperparameters, or creating specialized visualizations that standard tools canâ€™t provide. 
Enterprise collaboration and governance 
For enterprise teams, the combination creates a mature platform for scaling ML projects across regulated environments. SageMaker provides consistent, secure ML environments, and Comet enables seamless collaboration with complete artifact and model lineage tracking. This helps avoid costly mistakes that occur when teams canâ€™t recreate previous results. 
Complete ML lifecycle integration 
Unlike point solutions that only address training or monitoring, Comet paired with SageMaker supports your complete ML lifecycle. Models can be registered in Cometâ€™s model registry with full version tracking and governance. SageMaker handles model deployment, and Comet maintains the lineage and approval workflows for model promotion. Cometâ€™s production monitoring capabilities track model performance and data drift after deployment, creating a closed loop where production insights inform your next round of SageMaker experiments. 
Conclusion 
In this post, we showed how to use SageMaker and Comet together to spin up fully managed ML environments with reproducibility and experiment tracking capabilities. 
To enhance your SageMaker workflows with comprehensive experiment management, deploy Comet directly in your SageMaker environment through the AWS Marketplace, and share your feedback in the comments. 
For more information about the services and features discussed in this post, refer to the following resources: 
 
 Set up Partner AI Apps 
 Comet Quickstart 
 GitHub notebook 
 Comet Documentation 
 Opik open source platform for LLM observability 
 
 
 
About the authors 
Vikesh Pandey is a Principal GenAI/ML Specialist Solutions Architect at AWS, helping large financial institutions adopt and scale generative AI and ML workloads. He is the author of book â€œGenerative AI for financial services.â€ He carries more than 15 years of experience building enterprise-grade applications on generative AI/ML and related technologies. In his spare time, he plays an unnamed sport with his son that lies somewhere between football and rugby. 
Naufal Mir is a Senior GenAI/ML Specialist Solutions Architect at AWS. He focuses on helping customers build, train, deploy and migrate machine learning workloads to SageMaker. He previously worked at financial services institutes developing and operating systems at scale. Outside of work, he enjoys ultra endurance running and cycling. 
Sarah Ostermeier is a Technical Product Marketing Manager at Comet. She specializes in bringing Cometâ€™s GenAI and ML developer products to the engineers who need them through technical content, educational resources, and product messaging. She has previously worked as an ML engineer, data scientist, and customer success manager, helping customers implement and scale AI solutions. Outside of work she enjoys traveling off the beaten path, writing about AI, and reading science fiction.
â€¢ Move your AI agents from proof of concept to production with Amazon Bedrock AgentCore
  Building an AI agent that can handle a real-life use case in production is a complex undertaking. Although creating a proof of concept&nbsp;demonstrates the potential, moving to production requires addressing scalability, security, observability, and operational concerns that donâ€™t surface in development environments. 
This post explores how Amazon Bedrock AgentCore helps you transition your agentic applications from experimental proof of concept to production-ready systems. We follow the journey of a customer support agent that evolves from a simple local prototype to a comprehensive, enterprise-grade solution capable of handling multiple concurrent users while maintaining security and performance standards. 
Amazon Bedrock AgentCore is a comprehensive suite of services designed to help you build, deploy, and scale agentic AI applications. If youâ€™re new to AgentCore, we recommend exploring our existing deep-dive posts on individual services: AgentCore Runtime for secure agent deployment and scaling, AgentCore Gateway for enterprise tool development, AgentCore Identity for securing agentic AI at scale, AgentCore Memory for building context-aware agents, AgentCore Code Interpreter for code execution, AgentCore Browser Tool for web interaction, and AgentCore Observability for transparency on your agent behavior. This post demonstrates how these services work together in a real-world scenario. 
The customer support agent journey 
Customer support represents one of the most common and compelling use cases for agentic AI. Modern businesses handle thousands of customer inquiries daily, ranging from simple policy questions to complex technical troubleshooting. Traditional approaches often fall short: rule-based chatbots frustrate customers with rigid responses, and human-only support teams struggle with scalability and consistency. An intelligent customer support agent needs to seamlessly handle diverse scenarios: managing customer orders and accounts, looking up return policies, searching product catalogs, troubleshooting technical issues through web research, and remembering customer preferences across multiple interactions. Most importantly, it must do all this while maintaining the security and reliability standards expected in enterprise environments. Consider the typical evolution path many organizations follow when building such agents: 
 
 The proof of concept stage â€“ Teams start with a simple local prototype that demonstrates core capabilities, such as a basic agent that can answer policy questions and search for products. This works well for demos but lacks the robustness needed for real customer interactions. 
 The reality check â€“ As soon as you try to scale beyond a few test users, challenges emerge. The agent forgets previous conversations, tools become unreliable under load, thereâ€™s no way to monitor performance, and security becomes a paramount concern. 
 The production challenge â€“ Moving to production requires addressing session management, secure tool sharing, observability, authentication, and building interfaces that customers actually want to use. Many promising proofs of concept stall at this stage due to the complexity of these requirements. 
 
In this post, we address each challenge systematically. We start with a prototype agent equipped with three essential tools: return policy lookup, product information search, and web search for troubleshooting. From there, we add the capabilities needed for production deployment: persistent memory for conversation continuity and a hyper-personalized experience, centralized tool management for reliability and security, full observability for monitoring and debugging, and finally a customer-facing web interface. This progression mirrors the real-world path from proof of concept to production, demonstrating how Amazon Bedrock AgentCore services work together to solve the operational challenges that emerge as your agentic applications mature. For simplification and demonstration purposes, we consider a single-agent architecture. In real-life use cases, customer support agents are often created as multi-agent architectures and those scenarios are also supported by Amazon Bedrock AgentCore services. 
Solution overview 
Every production system starts with a proof of concept, and our customer support agent is no exception. In this first phase, we build a functional prototype that demonstrates the core capabilities needed for customer support. In this case, we use Strands Agents, an open source agent framework, to build the proof of concept and Anthropicâ€™s Claude 3.7 Sonnet on Amazon Bedrock as the large language model (LLM) powering our agent. For your application, you can use another agent framework and model of your choice. 
Agents rely on tools to take actions and interact with live systems. Several tools are used in customer support agents, but to keep our example simple, we focus on three core capabilities to handle the most common customer inquiries: 
 
 Return policy lookup â€“ Customers frequently ask about return windows, conditions, and processes. Our tool provides structured policy information based on product categories, covering everything from return timeframes to refund processing and shipping policies. 
 Product information retrieval â€“ Technical specifications, warranty details, and compatibility information are essential for both pre-purchase questions and troubleshooting. This tool serves as a bridge to your product catalog, delivering formatted technical details that customers can understand. 
 Web search for troubleshooting â€“ Complex technical issues often require the latest solutions or community-generated fixes not found in internal documentation. Web search capability allows the agent to access the web for current troubleshooting guides and technical solutions in real time. 
 
The tools implementation and the end-to-end code for this use case are available in our GitHub repository. In this post, we focus on the main code that connects with Amazon Bedrock AgentCore, but you can follow the end-to-end journey in the repository. 
Create the agent 
With the tools available, letâ€™s create the agent. The architecture for our proof of concept will look like the following diagram. 
 
You can find the end-to-end code for this post on the GitHub repository. For simplicity, we show only the essential parts for our end-to-end code here: 
 
 from strands import Agent
from strands.models import BedrockModel

@tool
def get_return_policy(product_category: str) -&gt; str:
&nbsp;&nbsp; &nbsp;"""Get return policy information for a specific product category."""
&nbsp;&nbsp; &nbsp;# Returns structured policy info: windows, conditions, processes, refunds
&nbsp; &nbsp; # check github for full code
&nbsp;&nbsp; &nbsp;return&nbsp;{"return_window":&nbsp;"10 days",&nbsp;"conditions":&nbsp;""}
&nbsp;&nbsp;&nbsp;&nbsp;
@tool &nbsp;
def get_product_info(product_type: str) -&gt; str:
&nbsp;&nbsp; &nbsp;"""Get detailed technical specifications and information for electronics products."""
&nbsp;&nbsp; &nbsp;# Returns warranty, specs, features, compatibility details
&nbsp;&nbsp; &nbsp;# check github for full code
&nbsp; &nbsp; return&nbsp;{"product":&nbsp;"ThinkPad X1 Carbon",&nbsp;"info":&nbsp;"ThinkPad X1 Carbon info"}
&nbsp;&nbsp;&nbsp;&nbsp;
@tool
def web_search(keywords: str, region: str = "us-en", max_results: int = 5) -&gt; str:
&nbsp;&nbsp; &nbsp;"""Search the web for updated troubleshooting information."""
&nbsp;&nbsp; &nbsp;# Provides access to current technical solutions and guides
&nbsp;&nbsp;&nbsp;&nbsp;# check github for full code
&nbsp; &nbsp; return&nbsp;"results from websearch"
&nbsp;&nbsp;&nbsp;&nbsp;
# Initialize the Bedrock model
model = BedrockModel(
&nbsp;&nbsp; &nbsp;model_id="us.anthropic.claude-3-7-sonnet-20250219-v1:0",
&nbsp;&nbsp; &nbsp;temperature=0.3
)

# Create the customer support agent
agent = Agent(
&nbsp;&nbsp; &nbsp;model=model,
&nbsp;&nbsp; &nbsp;tools=[
&nbsp; &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;get_product_info, 
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;get_return_policy, 
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;web_search
&nbsp;&nbsp;&nbsp;&nbsp;],
&nbsp;&nbsp; &nbsp;system_prompt="""You are a helpful customer support assistant for an electronics company.
&nbsp;&nbsp; &nbsp;Use the appropriate tools to provide accurate information and always offer additional help."""
) 
 
Test the proof of concept 
When we test our prototype with realistic customer queries, the agent demonstrates the correct tool selection and interaction with real-world systems: 
 
 # Return policy inquiry
response = agent("What's the return policy for my ThinkPad X1 Carbon?")
# Agent correctly uses get_return_policy with "laptops" category

# Technical troubleshooting &nbsp;
response = agent("My iPhone 14 heats up, how do I fix it?")
# Agent uses web_search to find current troubleshooting solutions 
 
The agent works well for these individual queries, correctly mapping laptop inquiries to return policy lookups and complex technical issues to web search, providing comprehensive and actionable responses. 
The proof of concept reality check 
Our proof of concept successfully demonstrates that an agent can handle diverse customer support scenarios using the right combination of tools and reasoning. The agent runs perfectly on your local machine and handles queries correctly. However, this is where the proof of concept gap becomes obvious. The tools are defined as local functions in your agent code, the agent responds quickly, and everything seems production-ready. But several critical limitations become apparent the moment you think beyond single-user testing: 
 
 Memory loss between sessions â€“ If you restart your notebook or application, the agent completely forgets previous conversations. A customer who was discussing a laptop return yesterday would need to start from scratch today, re-explaining their entire situation. This isnâ€™t just inconvenientâ€”itâ€™s a poor customer experience that breaks the conversational flow that makes AI agents valuable. 
 Single customer limitation â€“ Your current agent can only handle one conversation at a time. If two customers try to use your support system simultaneously, their conversations would interfere with each other, or worse, one customer might see anotherâ€™s conversation history. Thereâ€™s no mechanism to maintain separate conversation context for different users. 
 Tools embedded in code â€“ Your tools are defined directly in the agent code. This means: 
   
   You canâ€™t reuse these tools across different agents (sales agent, technical support agent, and so on). 
   Updating a tool requires changing the agent code and redeploying everything. 
   Different teams canâ€™t maintain different tools independently. 
    
 No production infrastructure â€“ The agent runs locally with no consideration for scalability, security, monitoring, and reliability. 
 
These fundamental architectural barriers can prevent real customer deployment. Agent building teams can take months to address these issues, which delays the time to value from their work and adds significant costs to the application. This is where Amazon Bedrock AgentCore services become essential. Rather than spending months building these production capabilities from scratch, Amazon Bedrock AgentCore provides managed services that address each gap systematically. 
Letâ€™s begin our journey to production by solving the memory problem first, transforming our agent from one that forgets every conversation into one that remembers customers across conversations and can hyper-personalize conversations using Amazon Bedrock AgentCore Memory. 
Add persistent memory for hyper-personalized agents 
The first major limitation we identified in our proof of concept was memory lossâ€”our agent forgot everything between sessions, forcing customers to repeat their context every time. This â€œgoldfish agentâ€ behavior breaks the conversational experience that makes AI agents valuable in the first place. 
Amazon Bedrock AgentCore Memory solves this by providing managed, persistent memory that operates on two complementary levels: 
 
 Short-term memory â€“ Immediate conversation context and session-based information for continuity within interactions 
 Long-term memory â€“ Persistent information extracted across multiple conversations, including customer preferences, facts, and behavioral patterns 
 
After adding Amazon Bedrock AgentCore Memory to our customer support agent, our new architecture will look like the following diagram. 
 
Install dependencies 
Before we start, letâ€™s install our dependencies: boto3, the AgentCore SDK, and the AgentCore Starter Toolkit SDK. Those will help us quickly add Amazon Bedrock AgentCore capabilities to our agent proof of concept. See the following code: 
 
 pip install boto3 bedrock-agentcore bedrock-agentcore-starter-toolkit 
 
Create the memory resources 
Amazon Bedrock AgentCore Memory uses configurable strategies to determine what information to extract and store. For our customer support use case, we use two complementary strategies: 
 
 USER_PREFERENCE â€“ Automatically extracts and stores customer preferences like â€œprefers ThinkPad laptops,â€ â€œuses Linux,â€ or â€œplays competitive FPS games.â€ This enables personalized recommendations across conversations. 
 SEMANTIC â€“ Captures factual information using vector embeddings, such as â€œcustomer has MacBook Pro order #MB-78432â€ or â€œreported overheating issues during video editing.â€ This provides relevant context for troubleshooting. 
 
See the following code: 
 
 from bedrock_agentcore.memory import MemoryClient
from bedrock_agentcore.memory.constants import StrategyType

memory_client = MemoryClient(region_name=region)

strategies = [
&nbsp;&nbsp; &nbsp;{
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;StrategyType.USER_PREFERENCE.value: {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"name": "CustomerPreferences",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"description": "Captures customer preferences and behavior",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"namespaces": ["support/customer/{actorId}/preferences"],
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp;},
&nbsp;&nbsp; &nbsp;{
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;StrategyType.SEMANTIC.value: {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"name": "CustomerSupportSemantic", 
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"description": "Stores facts from conversations",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"namespaces": ["support/customer/{actorId}/semantic"],
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp;},
]

# Create memory resource with both strategies
response = memory_client.create_memory_and_wait(
&nbsp;&nbsp; &nbsp;name="CustomerSupportMemory",
&nbsp;&nbsp; &nbsp;description="Customer support agent memory",
&nbsp;&nbsp; &nbsp;strategies=strategies,
&nbsp;&nbsp; &nbsp;event_expiry_days=90,
) 
 
Integrate with Strands Agents hooks 
The key to making memory work seamlessly is automationâ€”customers shouldnâ€™t need to think about it, and agents shouldnâ€™t require manual memory management. Strands Agents provides a powerful hook system that lets you intercept agent lifecycle events and handle memory operations automatically. The hook system enables both built-in components and user code to react to or modify agent behavior through strongly-typed event callbacks. For our use case, we create CustomerSupportMemoryHooks to retrieve the customer context and save the support interactions: 
 
 MessageAddedEvent hook â€“ Triggered when customers send messages, this hook automatically retrieves relevant memory context and injects it into the query. The agent receives both the customerâ€™s question and relevant historical context without manual intervention. 
 AfterInvocationEvent hook â€“ Triggered after agent responses, this hook automatically saves the interaction to memory. The conversation becomes part of the customerâ€™s persistent history immediately. 
 
See the following code: 
 
 class CustomerSupportMemoryHooks(HookProvider):
&nbsp;&nbsp; &nbsp;def retrieve_customer_context(self, event: MessageAddedEvent):
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"""Inject customer context before processing queries"""
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;user_query = event.agent.messages[-1]["content"][0]["text"]
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;# Retrieve relevant memories from both strategies
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;all_context = []
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;for context_type, namespace in self.namespaces.items():
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;memories = self.client.retrieve_memories(
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;memory_id=self.memory_id,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;namespace=namespace.format(actorId=self.actor_id),
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;query=user_query,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;top_k=3,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;)
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# Format and add to context
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;for memory in memories:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;if memory.get("content", {}).get("text"):
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;all_context.append(f"[{context_type.upper()}] {memory['content']['text']}")
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;# Inject context into the user query
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;if all_context:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;context_text = "\n".join(all_context)
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;original_text = event.agent.messages[-1]["content"][0]["text"]
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;event.agent.messages[-1]["content"][0]["text"] = f"Customer Context:\n{context_text}\n\n{original_text}"

&nbsp;&nbsp; &nbsp;def save_support_interaction(self, event: AfterInvocationEvent):
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"""Save interactions after agent responses"""
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Get last customer query and agent response check github for implementation
&nbsp; &nbsp; &nbsp; &nbsp; customer_query&nbsp;=&nbsp;"This is a sample query"
&nbsp; &nbsp; &nbsp; &nbsp; agent_response&nbsp;=&nbsp;"LLM gave a sample response"
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;# Extract customer query and agent response
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;# Save to memory for future retrieval
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;self.client.create_event(
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;memory_id=self.memory_id,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;actor_id=self.actor_id,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;session_id=self.session_id,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;messages=[(customer_query, "USER"), (agent_response, "ASSISTANT")]
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;) 
 
In this code, we can see that our hooks are the ones interacting with Amazon Bedrock AgentCore Memory to save and retrieve memory events. 
Integrate memory with the agent 
Adding memory to our existing agent requires minimal code changes; you can simply instantiate the memory hooks and pass them to the agent constructor. The agent code then only needs to connect with the memory hooks to use the full power of Amazon Bedrock AgentCore Memory. We will create a new hook for each session, which will help us handle different customer interactions. See the following code: 
 
 # Create memory hooks for this customer session
memory_hooks = CustomerSupportMemoryHooks(
&nbsp;&nbsp; &nbsp;memory_id=memory_id, 
&nbsp;&nbsp; &nbsp;client=memory_client, 
&nbsp;&nbsp; &nbsp;actor_id=customer_id, 
&nbsp;&nbsp; &nbsp;session_id=session_id
)

# Create agent with memory capabilities
agent = Agent(
&nbsp;&nbsp; &nbsp;model=model,

&nbsp;&nbsp; &nbsp;tools=[get_product_info, get_return_policy, web_search],
&nbsp;&nbsp; &nbsp;system_prompt=SYSTEM_PROMPT
) 
 
Test the memory in action 
Letâ€™s see how memory transforms the customer experience. When we invoke the agent, it uses the memory from previous interactions to show customer interests in gaming headphones, ThinkPad laptops, and MacBook thermal issues: 
 
 # Test personalized recommendations
response = agent("Which headphones would you recommend?")
# Agent remembers: "prefers low latency for competitive FPS games"
# Response includes gaming-focused recommendations

# Test preference recall
response = agent("What is my preferred laptop brand?") &nbsp;
# Agent remembers: "prefers ThinkPad models" and "needs Linux compatibility"
# Response acknowledges ThinkPad preference and suggests compatible models 
 
The transformation is immediately apparent. Instead of generic responses, the agent now provides personalized recommendations based on the customerâ€™s stated preferences and past interactions. The customer doesnâ€™t need to re-explain their gaming needs or Linux requirementsâ€”the agent already knows. 
Benefits of Amazon Bedrock AgentCore Memory 
With Amazon Bedrock AgentCore Memory integrated, our agent now delivers the following benefits: 
 
 Conversation continuity â€“ Customers can pick up where they left off, even across different sessions or support channels 
 Personalized service â€“ Recommendations and responses are tailored to individual preferences and past issues 
 Contextual troubleshooting â€“ Access to previous problems and solutions enables more effective support 
 Seamless experience â€“ Memory operations happen automatically without customer or agent intervention 
 
However, we still have limitations to address. Our tools remain embedded in the agent code, preventing reuse across different support agents or teams. Security and access controls are minimal, and we still canâ€™t handle multiple customers simultaneously in a production environment. 
In the next section, we address these challenges by centralizing our tools using Amazon Bedrock AgentCore Gateway and implementing proper identity management with Amazon Bedrock AgentCore Identity, creating a scalable and secure foundation for our customer support system. 
Centralize tools with Amazon Bedrock AgentCore Gateway and Amazon Bedrock AgentCore Identity 
With memory solved, our next challenge is tool architecture. Currently, our tools are embedded directly in the agent codeâ€”a pattern that works for prototypes but creates significant problems at scale. When you need multiple agents (customer support, sales, technical support), each one duplicates the same tools, leading to extensive code, inconsistent behavior, and maintenance nightmares. 
Amazon Bedrock AgentCore Gateway simplifies this process by centralizing tools into reusable, secure endpoints that agents can access. Combined with Amazon Bedrock AgentCore Identity for authentication, it creates an enterprise-grade tool sharing infrastructure. 
We will now update our agent to use Amazon Bedrock AgentCore Gateway and Amazon Bedrock AgentCore Identity. The architecture will look like the following diagram. 
 
In this case, we convert our web search tool to be used in the gateway and keep the return policy and get product information tools local to this agent. That is important because web search is a common capability that can be reused across different use cases in an organization, and return policy and production information are capabilities commonly associated with customer support services. With Amazon Bedrock AgentCore services, you can decide which capabilities to use and how to combine them. In this case, we also use two new tools that could have been developed by other teams: check warranty and get customer profile. Because those teams have already exposed those tools using AWS Lambda functions, we can use them as targets to our Amazon Bedrock AgentCore Gateway. Amazon Bedrock AgentCore Gateway can also support REST APIs as target. That means that if we have an OpenAPI specification or a Smithy model, we can also quickly expose our tools using Amazon Bedrock AgentCore Gateway. 
Convert existing services to MCP 
Amazon Bedrock AgentCore Gateway uses the Model Context Protocol (MCP) to standardize how agents access tools. Converting existing Lambda functions into MCP endpoints requires minimal changesâ€”mainly adding tool schemas and handling the MCP context. To use this functionality, we convert our local tools to Lambda functions and create the tools schema definitions to make these functions discoverable by agents: 
 
 # Original Lambda function (simplified)
def web_search(keywords: str, region: str = "us-en", max_results: int = 5) -&gt; str:
&nbsp; &nbsp; # web_search functionality
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
def lambda_handler(event, context):
&nbsp;&nbsp; &nbsp;if get_tool_name(event) == "web_search":
&nbsp; &nbsp; &nbsp; &nbsp; query&nbsp;= get_named_parameter(event=event, name="query")
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; search_result&nbsp;=&nbsp;web_search(keywords)
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;return {"statusCode": 200, "body": search_result} 
 
The following code is the tool schema definition: 
 
 {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"name": "web_search",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"description": "Search the web for updated information using DuckDuckGo",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"inputSchema": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"type": "object",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"properties": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"keywords": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"type": "string",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"description": "The search query keywords"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;},
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"region": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"type": "string",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"description": "The search region (e.g., us-en, uk-en, ru-ru)"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;},
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"max_results": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"type": "integer",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"description": "The maximum number of results to return"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;},
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"required": [
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"keywords"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;]
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp;} 
 
For demonstration purposes, we build a new Lambda function from scratch. In reality, organizations already have different functionalities available as REST services or Lambda functions, and this approach lets you expose existing enterprise services as agent tools without rebuilding them. 
Configure security with Amazon Bedrock AgentCore Gateway and integrate with Amazon Bedrock AgentCore Identity 
Amazon Bedrock AgentCore Gateway requires authentication for both inbound and outbound connections. Amazon Bedrock AgentCore Identity handles this through standard OAuth flows. After you set up an OAuth authorization configuration, you can create a new gateway and pass this configuration to it. See the following code: 
 
 # Create gateway with JWT-based authentication
auth_config = {
&nbsp;&nbsp; &nbsp;"customJWTAuthorizer": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"allowedClients": [cognito_client_id],
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"discoveryUrl": cognito_discovery_url
&nbsp;&nbsp; &nbsp;}
}

gateway_response = gateway_client.create_gateway(
&nbsp;&nbsp; &nbsp;name="customersupport-gw",
&nbsp;&nbsp; &nbsp;roleArn=gateway_iam_role,
&nbsp;&nbsp; &nbsp;protocolType="MCP",
&nbsp;&nbsp; &nbsp;authorizerType="CUSTOM_JWT",
&nbsp;&nbsp; &nbsp;authorizerConfiguration=auth_config,
&nbsp;&nbsp; &nbsp;description="Customer Support AgentCore Gateway"
) 
 
For inbound authentication, agents must present valid JSON Web Token (JWT) tokens (from identity providers like Amazon Cognito, Okta, and EntraID) as a compact, self-contained standard for securely transmitting information between parties to access Amazon Bedrock AgentCore Gateway tools. 
For outbound authentication, Amazon Bedrock AgentCore Gateway can authenticate to downstream services using AWS Identity and Access Management (IAM) roles, API keys, or OAuth tokens. 
For demonstration purposes, we have created an Amazon Cognito user pool with a dummy user name and password. For your use case, you should set a proper identity provider and manage the users accordingly. This configure makes sure only authorized agents can access specific tools and a full audit trail is provided. 
Add Lambda targets 
After you set up Amazon Bedrock AgentCore Gateway, adding Lambda functions as tool targets is straightforward: 
 
 lambda_target_config = {
&nbsp;&nbsp; &nbsp;"mcp": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"lambda": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"lambdaArn": lambda_function_arn,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"toolSchema": {"inlinePayload": api_spec},
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp;}
}

gateway_client.create_gateway_target(
&nbsp;&nbsp; &nbsp;gatewayIdentifier=gateway_id,
&nbsp;&nbsp; &nbsp;name="LambdaTools",
&nbsp;&nbsp; &nbsp;targetConfiguration=lambda_target_config,
&nbsp;&nbsp; &nbsp;credentialProviderConfigurations=[{
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"credentialProviderType": "GATEWAY_IAM_ROLE"
&nbsp;&nbsp; &nbsp;}]
) 
 
The gateway now exposes your Lambda functions as MCP tools that authorized agents can discover and use. 
Integrate MCP tools with Strands Agents 
Converting our agent to use centralized tools requires updating the tool configuration. We keep some tools local, such as product info and return policies specific to customer support that will likely not be reused in other use cases, and use centralized tools for shared capabilities. Because Strands Agents has a native integration for MCP tools, we can simply use the MCPClient from Strands with a streamablehttp_client. See the following code: 
 
 # Get OAuth token for gateway access
gateway_access_token = get_token(
&nbsp;&nbsp; &nbsp;client_id=cognito_client_id,
&nbsp;&nbsp; &nbsp;client_secret=cognito_client_secret,
&nbsp;&nbsp; &nbsp;scope=auth_scope,
&nbsp;&nbsp; &nbsp;url=token_url
)

# Create authenticated MCP client
mcp_client = MCPClient(
&nbsp;&nbsp; &nbsp;lambda: streamablehttp_client(
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;gateway_url,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;headers={"Authorization": f"Bearer {gateway_access_token['access_token']}"}
&nbsp;&nbsp; &nbsp;)
)

# Combine local and MCP tools
tools = [
&nbsp;&nbsp; &nbsp;get_product_info, &nbsp; &nbsp; # Local tool (customer support specific)
&nbsp;&nbsp; &nbsp;get_return_policy, &nbsp; &nbsp;# Local tool (customer support specific)
] + mcp_client.list_tools_sync() &nbsp;# Centralized tools from gateway

agent = Agent(
&nbsp;&nbsp; &nbsp;model=model,
&nbsp;&nbsp; &nbsp;tools=tools,
&nbsp;&nbsp; &nbsp;hooks=[memory_hooks],
&nbsp;&nbsp; &nbsp;system_prompt=SYSTEM_PROMPT
) 
 
Test the enhanced agent 
With the centralized tools integrated, our agent now has access to enterprise capabilities like warranty checking: 
 
 # Test web search using centralized tool &nbsp;
response = agent("How can I fix Lenovo ThinkPad with a blue screen?")
# Agent uses web_search from AgentCore Gateway 
 
The agent seamlessly combines local tools with centralized ones, providing comprehensive support capabilities while maintaining security and access control. 
However, we still have a significant limitation: our entire agent runs locally on our development machine. For production deployment, we need scalable infrastructure, comprehensive observability, and the ability to handle multiple concurrent users. 
In the next section, we address this by deploying our agent to Amazon Bedrock AgentCore Runtime, transforming our local prototype into a production-ready system with Amazon Bedrock AgentCore Observability and automatic scaling capabilities. 
Deploy to production with Amazon Bedrock AgentCore Runtime 
With the tools centralized and secured, our final major hurdle is production deployment. Our agent currently runs locally on your laptop, which is ideal for experimentation but unsuitable for real customers. Production requires scalable infrastructure, comprehensive monitoring, automatic error recovery, and the ability to handle multiple concurrent users reliably. 
Amazon Bedrock AgentCore Runtime transforms your local agent into a production-ready service with minimal code changes. Combined with Amazon Bedrock AgentCore Observability, it provides enterprise-grade reliability, automatic scaling, and comprehensive monitoring capabilities that operations teams need to maintain agentic applications in production. 
Our architecture will look like the following diagram. 
 
Minimal code changes for production 
Converting your local agent requires adding just four lines of code: 
 
 # Your existing agent code remains unchanged
model = BedrockModel(model_id="us.anthropic.claude-3-7-sonnet-20250219-v1:0")
memory_hooks = CustomerSupportMemoryHooks(memory_id, memory_client, actor_id, session_id)
agent = Agent(
&nbsp;&nbsp; &nbsp;model=model,
&nbsp;&nbsp; &nbsp;tools=[get_return_policy, get_product_info],
&nbsp;&nbsp; &nbsp;system_prompt=SYSTEM_PROMPT,
&nbsp;&nbsp; &nbsp;hooks=[memory_hooks]
)

def invoke(payload):
&nbsp;&nbsp; &nbsp;user_input = payload.get("prompt", "")
&nbsp;&nbsp; &nbsp;response = agent(user_input)
&nbsp;&nbsp; &nbsp;return response.message["content"][0]["text"]

if __name__ == "__main__":
 
 
BedrockAgentCoreApp automatically creates an HTTP server with the required /invocations and /ping endpoints, handles proper content types and response formats, manages error handling according to AWS standards, and provides the infrastructure bridge between your agent code and Amazon Bedrock AgentCore Runtime. 
Secure production deployment 
Production deployment requires proper authentication and access control. Amazon Bedrock AgentCore Runtime integrates with Amazon Bedrock AgentCore Identity to provide enterprise-grade security. Using the Bedrock AgentCore Starter Toolkit, we can deploy our application using three simple steps: configure, launch, and invoke. 
During the configuration, a Docker file is created to guide the deployment of our agent. It contains information about the agent and its dependencies, the Amazon Bedrock AgentCore Identity configuration, and the Amazon Bedrock AgentCore Observability configuration to be used. During the launch step, AWS CodeBuild is used to run this Dockerfile and an Amazon Elastic Container Registry (Amazon ECR) repository is created to store the agent dependencies. The Amazon Bedrock AgentCore Runtime agent is then created, using the image of the ECR repository, and an endpoint is generated and used to invoke the agent in applications. If your agent is configured with OAuth authentication through Amazon Bedrock AgentCore Identity, like ours will be, you also need to pass the authentication token during the agent invocation step. The following diagram illustrates this process. 
 
The code to configure and launch our agent on Amazon Bedrock AgentCore Runtime will look as follows: 
 
 from bedrock_agentcore_starter_toolkit import Runtime

# Configure secure deployment with Cognito authentication
agentcore_runtime = Runtime()

response = agentcore_runtime.configure(
&nbsp;&nbsp; &nbsp;entrypoint="lab_helpers/lab4_runtime.py",
&nbsp;&nbsp; &nbsp;execution_role=execution_role_arn,
&nbsp;&nbsp; &nbsp;auto_create_ecr=True,
&nbsp;&nbsp; &nbsp;requirements_file="requirements.txt",
&nbsp;&nbsp; &nbsp;region=region,
&nbsp;&nbsp; &nbsp;agent_name="customer_support_agent",
&nbsp;&nbsp; &nbsp;authorizer_configuration={
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"customJWTAuthorizer": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"allowedClients": [cognito_client_id],
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"discoveryUrl": cognito_discovery_url,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp;}
)

# Deploy to production
launch_result = agentcore_runtime.launch() 
 
This configuration creates a secure endpoint that only accepts requests with valid JWT tokens from your identity provider (such as Amazon Cognito, Okta, or Entra). For our agent, we use a dummy setup with Amazon Cognito, but your application can use an identity provider of your choosing. The deployment process automatically builds your agent into a container, creates the necessary AWS infrastructure, and establishes monitoring and logging pipelines. 
Session management and isolation 
One of the most critical production features for agents is proper session management. Amazon Bedrock AgentCore Runtime automatically handles session isolation, making sure different customersâ€™ conversations donâ€™t interfere with each other: 
 
 # Customer 1 conversation
response1 = agentcore_runtime.invoke(
&nbsp;&nbsp; &nbsp;{"prompt": "My iPhone Bluetooth isn't working. What should I do?"},
&nbsp;&nbsp; &nbsp;bearer_token=auth_token,
&nbsp;&nbsp; &nbsp;session_id="session-customer-1"
)

# Customer 1 follow-up (maintains context)
response2 = agentcore_runtime.invoke(
&nbsp;&nbsp; &nbsp;{"prompt": "I've turned Bluetooth on and off but it still doesn't work"},
&nbsp;&nbsp; &nbsp;bearer_token=auth_token,
&nbsp;&nbsp; &nbsp;session_id="session-customer-1" &nbsp;# Same session, context preserved
)

# Customer 2 conversation (completely separate)
response3 = agentcore_runtime.invoke(
&nbsp;&nbsp; &nbsp;{"prompt": "Still not working. What is going on?"},
&nbsp;&nbsp; &nbsp;bearer_token=auth_token,
&nbsp;&nbsp; &nbsp;session_id="session-customer-2" &nbsp;# Different session, no context
) 
 
Customer 1â€™s follow-up maintains full context about their iPhone Bluetooth issue, whereas Customer 2â€™s message (in a different session) has no context and the agent appropriately asks for more information. This automatic session isolation is crucial for production customer support scenarios. 
Comprehensive observability with Amazon Bedrock AgentCore Observability 
Production agents need comprehensive monitoring to diagnose issues, optimize performance, and maintain reliability. Amazon Bedrock AgentCore Observability automatically instruments your agent code and sends telemetry data to Amazon CloudWatch, where you can analyze patterns and troubleshoot issues in real time. The observability data includes session-level tracking, so you can trace individual customer session interactions and understand exactly what happened during a support interaction. You can use Amazon Bedrock AgentCore Observability with an agent of your choice, hosted in Amazon Bedrock AgentCore Runtime or not. Because Amazon Bedrock AgentCore Runtime automatically integrates with Amazon Bedrock AgentCore Observability, we donâ€™t need extra work to observe our agent. 
With Amazon Bedrock AgentCore Runtime deployment, your agent is ready to be used in production. However, we still have one limitation: our agent is accessible only through SDK or API calls, requiring customers to write code or use technical tools to interact with it. For true customer-facing deployment, we need a user-friendly web interface that customers can access through their browsers. 
In the following section, we demonstrate the complete journey by building a sample web application using Streamlit, providing an intuitive chat interface that can interact with our production-ready Amazon Bedrock AgentCore Runtime endpoint. The exposed endpoint maintains the security, scalability, and observability capabilities weâ€™ve built throughout our journey from proof of concept to production. In a real-world scenario, you would integrate this endpoint with your existing customer-facing applications and UI frameworks. 
Create a customer-facing UI 
With our agent deployed to production, the final step is creating a customer-facing UI that customers can use to interface with the agent. Although SDK access works for developers, customers need an intuitive web interface for seamless support interactions. 
To demonstrate a complete solution, we build a sample Streamlit-based web-application that connects to our production-ready Amazon Bedrock AgentCore Runtime endpoint. The frontend includes secure Amazon Cognito authentication, real-time streaming responses, persistent session management, and a clean chat interface. Although we use Streamlit for rapid-prototyping, enterprises would typically integrate the endpoint with their existing interface or preferred UI frameworks. 
The end-to-end application (shown in the following diagram) maintains full conversation context across the sessions while providing the security, scalability, and observability capabilities that we built throughout this post. The result is a complete customer support agentic system that handles everything from initial authentication to complex multi-turn troubleshooting conversations, demonstrating how Amazon Bedrock AgentCore services transform prototypes into production-ready customer applications. 
 
Conclusion 
Our journey from prototype to production demonstrates how Amazon Bedrock AgentCore services address the traditional barriers to deploying enterprise-ready agentic applications. What started as a simple local customer support chatbot transformed into a comprehensive, production-grade system capable of serving multiple concurrent users with persistent memory, secure tool sharing, comprehensive observability, and an intuitive web interfaceâ€”without months of custom infrastructure development. 
The transformation required minimal code changes at each step, showcasing how Amazon Bedrock AgentCore services work together to solve the operational challenges that typically stall promising proofs of concept. Memory capabilities avoid the â€œgoldfish agentâ€ problem, centralized tool management through Amazon Bedrock AgentCore Gateway creates a reusable infrastructure that securely serves multiple use cases, Amazon Bedrock AgentCore Runtime provides enterprise-grade deployment with automatic scaling, and Amazon Bedrock AgentCore Observability delivers the monitoring capabilities operations teams need to maintain production systems. 
The following video provides an overview of AgentCore capabilities. 

 
  
 
 
Ready to build your own production-ready agent? Start with our complete end-to-end tutorial, where you can follow along with the exact code and configurations weâ€™ve explored in this post. For additional use cases and implementation patterns, explore the broader GitHub repository, and dive deeper into service capabilities and best practices in the Amazon Bedrock AgentCore documentation. 
 
About the authors 
Maira Ladeira Tanke is a Tech Lead for Agentic AI at AWS, where she enables customers on their journey to develop autonomous AI systems. With over 10 years of experience in AI/ML, Maira partners with enterprise customers to accelerate the adoption of agentic applications using Amazon Bedrock AgentCore and Strands Agents, helping organizations harness the power of foundation models to drive innovation and business transformation. In her free time, Maira enjoys traveling, playing with her cat, and spending time with her family someplace warm.
â€¢ Scale visual production using Stability AI Image Services in Amazon Bedrock
  This post was written with Alex Gnibus of Stability AI. 
Stability AI Image Services are now available in Amazon Bedrock, offering ready-to-use media editing capabilities delivered through the Amazon Bedrock API. These image editing tools expand on the capabilities of Stability AIâ€™s Stable Diffusion 3.5 models (SD3.5) and Stable Image Core and Ultra models, which are already available in Amazon Bedrock and have set new standards in image generation. 
The professional creative production process consists of multiple editing steps to get the exact output needed. With Stability AI Image Services in Amazon Bedrock, you can modify, enhance, and transform existing images without jumping between multiple systems or sending files to external services. Everything runs through the same Amazon Bedrock experience youâ€™re already using. The business impact can be immediate for teams that produce visual content at scale. 
In this post, we explore examples of how these tools enable precise creative control to accelerate professional-grade visual content. 
Editing tools now available in Amazon Bedrock 
Stability AI Image Services span&nbsp;9 tools across two categories: Edit and Control. Each tool handles specific editing tasks that typically require specialized software or manual intervention. 
Edit: Advanced capabilities for granular editing steps 
The tools in the Edit category make complex editing tasks more accessible and efficient. 
The suite begins with fundamental yet powerful retouching tools. The Erase Object tool, for example, removes unwanted elements from images while intelligently maintaining background consistency. The following animation showcases the Erase Object tool removing a mannequin from a product shot while preserving the background. The tool can transform a source image based on a mask image or derive the mask from the source imageâ€™s alpha channel. 
 
The Remove Background tool automatically isolates subjects with precision. This enables the creation of clean, professional product listings with consistent backgrounds or a variety of lifestyle settings, which is a game changer for ecommerce. 
The following example illustrates the removal of an image background, while preserving details of a furniture product in the foreground. 
 
The Search and Recolor and Search and Replace tools target specific elements within images for modification. Search and Recolor changes object colors; for example, showing different colorways of a dress without new photoshoots. In the following illustration, Search and Recolor changes the color swatch on furniture. 
 
Search and Replace can swap objects entirely, which is useful for updating seasonal elements in marketing materials or replacing products. The following is an application of Search and Replace for virtual try-on experiences. 
 
The Inpaint tool intelligently modifies images by filling in or replacing specified areas with new content based on the content of a mask image. 
Control: Structural and stylistic precision 
This category of tools provides precise manipulation of image structure and style through three specialized tools. 
The Sketch tool transforms sketch-style renderings into photorealistic concepts. Architecture firms might use this to convert conceptual drawings into realistic visualizations, and apparel brands to turn design sketches into product mockups. The tool helps accelerate the creative production process from initial concepts to final visual execution. 
In this example, the Sketch tool transforms a building architecture drawing to help real estate developers visualize the concept against a cityscape. 
 
In another example, the Sketch tool transforms a mannequin drawing into a photorealistic model shot. 
 
The Structure tool maintains the structural elements of input images while allowing content modification. This tool helps preserve layouts, compositions, and spatial relationships while changing subjects or styles. Creative teams can use the Structure tool to recreate scenes with different subjects or render new characters while maintaining consistent framing. 
The following example demonstrates the Structure tool transforming a workshop scene into a new scene while preserving the composition and spatial relationships. 
 
The Style Guide and Style Transfer tools help marketing teams produce new images that align with brand style and guidelines. The Style Guide tool takes artistic styles and colors from a reference style image and generates new images based on text prompts. 
In the following example, the Style Guide tool takes clues from a brandâ€™s color palette and textures and generates new images matching brand identity. 
 
The Style Transfer tool uses visual characteristics from reference images to transform existing images, while preserving the original composition. For example, a home decor retailer can transform product imagery from modern minimalist to traditional styles without new photography. Marketing teams could create seasonal variations by applying different visual styles to existing product catalogs. 
Solution overview 
To demonstrate Stability AI Image Services in Amazon Bedrock, letâ€™s walk through an example using a Jupyter notebook found in the GitHub repo. 
Prerequisites 
To follow along, you must have the following prerequisites: 
 
 An AWS account. 
 AWS credentials configured for creating and accessing Amazon Bedrock and Amazon SageMaker AI resources. 
 An AWS Identity and Access Management (IAM) execution role for SageMaker AI, which has the AmazonSageMakerFullAccess and AmazonBedrockLimitedAccess AWS managed policies attached. For more details, see How to use SageMaker AI execution roles. 
 A SageMaker notebook instance. 
 Stability AI Image Services model access, which you can request through the Amazon Bedrock console. Refer to Access Amazon Bedrock foundation models for more details. 
 
Create a SageMaker AI notebook instance 
Complete the following steps to create a SageMaker AI notebook instance, which can be used to run the sample notebook: 
 
 On the SageMaker AI console, in the navigation pane, under Applications and IDEs, choose Notebooks. 
 Choose Create notebook instance. 
 For Notebook instance name, enter a name for your notebook instance (for example, ai-images-notebook-instance). 
 For Notebook Instance type, choose ml.t2.medium. 
 For Platform identifier, choose Amazon Linux 2. 
 For IAM role, choose either an existing IAM role, which has the AmazonSageMakerFullAccess and AmazonBedrockLimitedAccess policies attached, or choose Create a new role. 
 Note the name of the IAM role that you chose. 
 Leave other settings as default and choose Create notebook instance. 
 
After a few minutes, SageMaker AI creates a notebook instance, and its status changes from Pending to InService. 
Confirm the IAM role for the notebook instance has the necessary permissions 
Complete the following steps to verify that the SageMaker AI execution role that you assigned to the notebook instance has the correct permissions: 
 
 On the IAM console, in the navigation pane, under Access management, choose Roles. 
 In the Roles search bar, enter the name of the SageMaker AI execution role that you used when creating the notebook instance. 
 Choose the IAM role. 
 Under Permissions policies, verify that the AWS managed policies AmazonSageMakerFullAccess and AmazonBedrockLimitedAccess are present. 
 (Optional) If either policy is missing, choose Add permissions, then choose Attach policies to attach the missing policy. 
   
   In the Other permissions policies search bar, enter the policy name. 
   Select the policy, then chose Add permissions. 
    
 
Run the notebook 
Complete the following steps to run the notebook: 
 
 On the SageMaker AI console, in the navigation pane, under Applications and IDEs, choose Notebooks. 
 Choose the newly created ai-images-notebook-instance notebook instance. 
 Wait for the notebook to be in InService status. 
 Choose the Open JupyterLab link to launch JupyterLab in a new browser tab. 
 On the Git menu, choose Clone a Repository. 
 Enter the URI https://github.com/aws-samples/stabilityai-sample-notebooks.git and select Include submodules and Download the repository. 
 Choose Clone. 
 On the File menu, choose Open from path. 
 Enter the following: stabilityai-sample-notebooks/stability-ai-image-services/stability-ai-image-services-sample-notebook.ipynb 
 Choose Open. 
 When prompted, choose the kernel conda_python3, then choose Select. 
 Run through each notebook cell to experience Stability AI Image Services in Amazon Bedrock. 
 
Clean up 
To avoid ongoing charges, stop the ai-images-notebook-instance SageMaker AI notebook instance that you created in this walkthrough: 
 
 On the SageMaker AI console, in the navigation pane, under Applications and IDEs, choose Notebooks. 
 Choose the ai-images-notebook-instance SageMaker AI notebook instance that you created. 
 Choose Actions, then choose Stop. 
 
After a few minutes, the notebook instance transitions from Stopping to Stopped status. 
 
 Choose Actions, then Delete. 
 
After a few seconds, SageMaker AI deletes the notebook instance. 
For more details, refer to Clean up Amazon SageMaker notebook instance resources. 
Conclusion 
The availability of Stability AI Image Services in Amazon Bedrock is an exciting step forward for visual content creation and manipulation, with particularly time-saving implications for professional creative teams at enterprises. 
For example, in media and entertainment, creators can rapidly enhance scenes and create special effects, and marketing teams can generate multiple campaign variations effortlessly. Retail and ecommerce businesses can streamline product photography and digital catalog creation, and gaming developers can prototype environments more efficiently. Architecture firms can visualize design concepts instantly, and educational institutions can create more engaging visual content. 
With these tools, businesses of different sizes can produce professional-grade, highly engaging visual content with efficiency and creativity. These tools can streamline operations, reduce costs, and open new creative possibilities, helping brands tell their stories more effectively and engage customers in more compelling ways. 
To get started, check out Stability AI models in Amazon Bedrock and the AWS Samples GitHub repo. 
 
About the authors 
Alex Gnibus is a Product Marketing Manager at Stability AI, connecting the dots between cutting-edge research breakthroughs and practical use cases. With experience spanning from creative agencies to deep enterprise tech, Alex brings both technical expertise and an understanding of the challenges that professional creative teams can solve with generative AI. 
Isha Dua is a Senior Solutions Architect based in the San Francisco Bay Area. She helps AWS Enterprise customers grow by understanding their goals and challenges and guiding them on how they can architect their applications in a cloud-based manner while making sure they are resilient and scalable. Sheâ€™s passionate about machine learning technologies and environmental sustainability. 
Fabio Branco is a Senior Customer Solutions Manager at Amazon Web Services (AWS) and strategic advisor helping customers achieve business transformation, drive innovation through generative AI and data solutions, and successfully navigate their cloud journeys. Prior to AWS, he held Product Management, Engineering, Consulting, and Technology Delivery roles across multiple Fortune 500 companies in industries, including retail and consumer goods, oil and gas, financial services, insurance, and aerospace and defense. 
Suleman Patel is a Senior Solutions Architect at Amazon Web Services (AWS), with a special focus on machine learning and modernization. With expertise in both business and technology, Suleman helps customers design and build solutions that tackle real-world business problems. When heâ€™s not immersed in his work, Suleman loves exploring the outdoors, taking road trips, and cooking up delicious dishes in the kitchen.
â€¢ Prompting for precision with Stability AI Image Services in Amazon Bedrock
  Amazon Bedrock now offers Stability AI Image Services: 9 tools that improve how businesses create and modify images. The technology extends Stable Diffusion and Stable Image models to give you precise control over image creation and editing. Clear prompts are criticalâ€”they provide art direction to the AI system. Strong prompts control specific elements like tone, texture, lighting, and composition to create the desired visual outcomes. This capability serves professional needs across product photography, concept, and marketing campaigns. 
In this post, we expand on the post Understanding prompt engineering: Unlock the creative potential of Stability AI models on AWS. We show how to effectively use advanced prompting techniques to maximize image generation quality and precision for enterprise application using Stability AI Image Services in Amazon Bedrock. 
Solution overview 
Stability AI Image Services are available as APIs in Amazon Bedrock, featuring capabilities such as, in-painting, style transfer, recoloring, background removal, object removal, style guide, and much more. 
In the following sections, we first discuss prompt structure for maximum control of image generation, then we provide advanced techniques of prompting for stylistic guidance. Code samples can be found in the following GitHub repository. 
Prerequisites 
To get started with Stability AI Image Services in Amazon Bedrock, follow the instructions in Getting started with the API to complete the following prerequisites: 
 
 Set up your AWS account. 
 Acquire credentials to grant programmatic access. 
 Attach the Amazon Bedrock permission to an AWS Identity and Access Management (IAM) user or role. 
 Request access to the Amazon Bedrock models. 
 
Structure prompts that maximize control 
To maximize the granular capabilities of Stability AI Image Services in Amazon Bedrock, you must construct prompts that enable fine-grained control. 
This section outlines best practices for building effective prompts that produce the desired output. We demonstrate how prompt structure affects results and why more structured prompts typically yield more consistent and controllable outcomes. 
Choose the right prompt type for your use case 
Selecting the right prompt format helps the model better understand your intent. Three primary prompt formats deliver different levels of control and readability: 
 
 Natural language maximizes readability and is best for general usage 
 Tag-based formats enable precise structural control and are ideal for technical application 
 Hybrid formats combine natural language and the structural elements of tags to provide even more control 
 
The following table provides examples of these three common ways to phrase your prompts. Each prompt format has its strengths depending on your goal or the interface youâ€™re using. 
 
  
   
   Prompt type  
   Prompt example  
   Generated image using Stable Image Ultra in Amazon Bedrock 
   Description and use case 
   
   
   Basic Prompt (Natural Language) 
   â€œA clean product photo of a perfume bottle on a marble countertopâ€ 
    
   This is readable and intuitive. Great for exploration, conversational tools, and some model types. Stable Diffusion 3.5 responds best to this style. 
   
   
   Tag-Based Prompt 
   â€œperfume bottle, marble surface, soft light, high quality, product photoâ€ 
    
   Used in many generation UIs or with models trained on datasets like LAION or Danbooru. Compact and good for stacking details. 
   
   
   Hybrid Prompt 
   â€œperfume bottle on marble counter, soft studio lighting, sharp focus, f/2.8lensâ€ 
    
   Best of both worlds. Add emphasis with weighting syntax to influence the modelâ€™s priorities. 
   
  
 
Build modular prompts 
Modular prompting enhances AI image generation effectiveness. This approach divides prompts into distinct components, each specifying what to draw and how it should appear. Modular structures provide several benefits: they help prevent conflicting or confusing instructions, allow for precise output control, and simplify prompt debugging. By isolating individual elements, you can quickly identify and adjust effective or ineffective parts of your prompts. This method ultimately leads to more refined and targeted AI-generated images. 
The following table provides examples of modular prompt modules. Experiment with different prompt sequences for your desired outcome; for example, placing the style before the subject will give it a more visual weight. 
 
  
   
   Module 
   Example 
   Description 
   
   
   Prefix 
   â€œfashion editorial portrait ofâ€ 
   Sets the tone and intent for a high-fashion styled portrait 
   
   
   Subject 
   â€œa woman with medium-brown skin and short coiled hairâ€ 
   Gives the modelâ€™s look and surface detail to help guide facial features 
   
   
   Modifiers 
   â€œwearing an asymmetrical black mesh top, metallic jewelryâ€ 
   Adds stylized clothing and accessories for visual interest 
   
   
   Action 
   â€œseated with her shoulders angled, eyes locked on camera, one arm liftedâ€ 
   Describes body language and pose to give dynamic composition 
   
   
   Environment 
   â€œbathed in intersecting beams of hard directional light through window slatsâ€ 
   Adds context for dramatic light play and atmosphere 
   
   
   Style 
   â€œhigh-contrast chiaroscuro lighting, sculptural and abstractâ€ 
   Informs the aesthetic and mood (shadow-driven, moody, architectural) 
   
   
   Camera/Lighting 
   â€œshot on 85mm, studio setup, layered shadows and light falling across face and bodyâ€ 
   Adds technical precision and helps control realism and fidelity 
   
  
 
The following example illustrates how to use a modular prompt to generate the desired output. 
 
  
   
   Modular Prompt 
   Generated Image Using Stable Image Ultra in Amazon Bedrock 
   
   
   â€œfashion editorial portrait of a woman with medium-brown skin and short coiled hair, wearing an asymmetrical black mesh top and metallic jewelry, seated with shoulders angled and one arm lifted, eyes locked on camera, bathed in intersecting beams of hard directional light through window slats, layered shadows and highlights sculpting her face and body, high-contrast chiaroscuro lighting, abstract and bold, shot on 85mm in studioâ€ 
    
   
  
 
Use negative prompts for polished output 
Negative prompts improve AI output quality by removing specific visual elements. Explicitly defining what not to include in the prompt guides the modelâ€™s output, typically leading to professional outputs. Negative prompts act like a retoucherâ€™s checklist used to address aspects of an image to enhance quality and appeal. For example, â€œNo weird hands. No blurry corners. No cartoon filters. Definitely no watermarks.â€ Negative prompts result in clean, confident, compositions, free of distracting element and distortions. 
The following table provides examples of additional tokens that can be used in negative prompts. 
 
  
   
   Artifact Type 
   Tokens to Use 
   
   
   Low quality or noise 
   blurry, lowres, jpeg artifacts, noisy 
   
   
   Anatomy or model issues 
   deformed, extra limbs, bad hands, missing fingers 
   
   
   Style clashes 
   cartoon, illustration, anime, painting 
   
   
   Technical errors 
   watermark, text, signature, overexposed 
   
   
   General cleanup 
   ugly, poorly drawn, distortion, worst quality 
   
  
 
The following example illustrates how a well-structured negative prompt can enhance photorealism. 
 
  
   
   Without Negative Prompt 
    Prompt â€œ(medium full shot) of (charming office cubicle) made of glass material, multiple colors, modern style, space-saving, upholstered seat, patina, gold trim, located in a modern garden, with sleek furniture, stylish decor, bright lighting, comfortable seating, Masterpiece, best quality, raw photo, realistic, very aesthetic, dark â€œ 
    
   
   
   With Negative Prompt 
    Prompt â€œ(medium full shot) of (charming office cubicle) made of glass material, multiple colors, modern style, space-saving, upholstered seat, patina, gold trim, located in a modern garden, with sleek furniture, stylish decor, bright lighting, comfortable seating, Masterpiece, best quality, raw photo, realistic, very aesthetic, darkâ€ Negative Prompt â€œcartoon, 3d render, cgi, oversaturated, smooth plastic textures, unreal lighting, artificial, matte surface, painterly, dreamy, glossy finish, digital art, low detail backgroundâ€ 
    
   
  
 
Emphasize or suppress elements with prompt weighting 
Prompt weighting controls the influence of individual elements in AI image generation. These numerical weights prioritize specific prompt components over others. For example, to emphasize the character over the background, you can apply a 1.8 weight to â€œcharacterâ€ (character: 1.8) and 1.1 to â€œbackgroundâ€ (background: 1.1), which makes sure the model prioritizes character detail while maintaining environmental context. This targeted emphasis produces more precise outputs by minimizing competition between prompt elements and clarifying the modelâ€™s priorities. 
The syntax for prompt weights is (&lt;term&gt;:&lt;weight&gt;). You can also use a shorthand such as ((&lt;term&gt;)), where the number of parentheses represent the weight. Values between 0.0â€“1.0 deemphasize the term, and values between 1.1â€“2.0 emphasize the term.For example: 
 
 (term:1.2): Emphasize 
 (term:0.8): Deemphasize 
 ((term)): Shorthand for (term:1.2) 
 (((((((((term)))))))): Shorthand for (term:1.8) 
 
The following example shows how prompt weights contribute to the generated output. 
 
  
   
    Prompt with weights â€œeditorial product photo of (a translucent gel moisturizer jar:1.4) placed on a (frosted glass pedestal:1.2), surrounded by (dewy pink flower petals:1.1), with soft (diffused lighting:1.3), subtle water droplets, shallow depth of fieldâ€ 
    
   
   
    Prompt without weights â€œeditorial product photo of a translucent gel moisturizer jar placed on a frosted glass pedestal, surrounded by dewy pink flower petals, with soft, subtle water droplets, shallow depth of fieldâ€ 
    
   
  
 
You can also use weights in negative prompts to reduce how strongly the model avoids something. For example, â€œ(text:0.5), (blurry:0.2), (lowres:0.1).â€ This tells the model to be especially sure to avoid generating blurry text or low-resolution content. 
Giving specific stylistic guidance 
Effective prompt writing when using Stability AI Image Services such as Style Transfer and Style Guide requires a good understanding of style matching and reference-driven prompting. These techniques help provide clear stylistic direction for both text-to-image and image-to-image creation. 
Image-to-image style transfer extracts stylistic elements from an input image (control image) and uses it to guide the creation of an output image based on the prompt. Approach writing the prompt as if youâ€™re directing a professional photographer or stylist. Focus on materials, lighting quality, and artistic intentionâ€”not just objects. For example, a well-structured prompt might read: â€œClose-up editorial photo of a translucent green lip gloss tube on crushed iridescent plastic, diffused colored lighting, shallow DOF, high fashion product styling.â€ 
Style tag layering: Known aesthetic labels that align with brand identity 
The art of crafting effective prompts often relies on incorporating established style tags that resonate with familiar visual languages and datasets. By strategically blending terms from recognized aesthetic categories (ranging from editorial photography and analog film to anime, cyberpunk cityscapes, and brutalist structures), creators can guide the AI toward specific visual outcomes that align with their brand identity. These style descriptors serve as powerful anchors in the prompt engineering process. The versatility of these tags extends further through their ability to be combined and weighted, allowing for nuanced control over the final aesthetic. For instance, a skincare brand might blend the clean lines of product photography with dreamy, surreal elements, whereas a tech company could merge brutalist structure with cyberpunk elements for a distinctive visual identity. This approach to style mixing helps creators improve their outputs while maintaining clear ties to recognizable visual genres that resonate with their target audience. The key is understanding how these style tags interact and using their combinations to create unique, yet culturally relevant, visual expressions that serve specific creative or commercial objectives. The following table provides examples of prompts for a desired aesthetic. 
 
  
   
   Desired aesthetic 
   Prompt phrases 
   Example use case 
   
   
   Retro / Y2K 
   2000s nostalgia, flash photography, candy tones, harsh lighting 
   Metallic textures, thin fonts, early digital feel. 
   
   
   Clean modern 
   neutral tones, soft gradients, minimalist styling, editorial layout 
   Great for wellness or skincare products. 
   
   
   Bold streetwear 
   urban background, oversized fit, strong pose, midday shadow 
   Fashion photography and lifestyle ads. Prioritize outfit structure and location cues. 
   
   
   Hyperreal surrealism 
   dreamcore lighting, glossy textures, cinematic DOF, surreal shadows 
   Plays well in music, fashion, or alt-culture campaigns. 
   
  
 
Invoke a named style as a reference 
Some prompt structures benefit from invoking a named visual signature from a specific artist, especially when combined with your own stylistic phrasing or workflows, as shown in the following example. 
 
  
   
    Prompt â€œeditorial studio portrait of a woman with glowing skin in minimalist glam makeup, high-contrast lighting, clean background, (depiction of Van Gogh style:1.3)â€ 
    
   
  
 
The following is a more conceptual example. 
 
  
   
    Prompt â€œproduct shot of a silver hair oil bottle with soft reflections on curved chrome, (depiction of Wes Anderson style:1.2), under cold studio lightingâ€ 
    
   
  
 
These phrases function like calling on a genre; they imply choices around materials, lighting, layout, and color tonality. 
Use reference images to guide style 
Another useful technique is using a reference image to guide the pose, color, or composition of the output. For use cases like matching a pose from a lookbook image, transferring a color palette from a campaign still, or copying shadowplay from a photo shoot, you can extract and apply structure or style from reference images. 
Stability AI Image Services support a variety of image-to-image workflows where you can use a reference image (control image) to guide the output, such as Structure, Sketch, and Style. Tools like ControlNet (a neural network architecture developed by Stability AI that enhances control), IP-Adapter (an image prompt adapter), or clip-based captioning also enable further control when paired with Stability AI models. 
We will discuss ControlNet, IP-Adapter, and clip-based captioning in a subsequent post. 
The following is an example of an image-to-image workflow: 
 
 Find a high-quality editorial reference. 
 Use it with a depth, canny, or seg ControlNet to lock a pose. 
 Style with a prompt. 
 
 
  
   
    Prompt â€œfashion editorial of a model in layered knitwear, dramatic colored lighting, strong shadows, high ISO textureâ€ 
    
   
  
 
Create the right mood with lighting control 
In a prompt, lighting sets tone, adds dimensionality, and mimics the language of photography. It shouldnâ€™t just be â€œbright vs. dark.â€ Lighting is often the style itself, especially for audiences like Gen Z, for instance TikTok, early-aughts flash, harsh backlight, and color gels. The following table provides some useful lighting style prompt terms. 
 
  
   
   Lighting style 
   Prompt terms 
   Example use case 
   
   
   High-contrast studio 
   hard directional light, deep shadows, controlled highlights 
   Beauty, tech, fashion with punchy visuals 
   
   
   Soft editorial 
   diffused light, soft shadows, ambient glow, overcast 
   Skincare, fashion, wellness 
   
   
   Colored gel lighting 
   blue and pink gel lighting, dramatic color shadows, rim lighting 
   Nightlife, music-adjacent fashion, youth-forward styling 
   
   
   Natural bounce 
   golden hour, soft natural light, sun flare, warm tones 
   Outdoors, lifestyle, brand-friendly minimalism 
   
  
 
Build intent with posing and framing terms 
Good posing helps products feel aspirational and digital models more dynamic. With AI, you must be intentional. Framing and pose cues help avoid stiffness, anatomical errors, and randomness. The following table provides some useful posing and framing prompt terms. 
 
  
   
   Prompt cue 
   Description 
   Tip 
   
   
   looking off camera 
   Creates candid or editorial energy 
   Useful for lookbooks or ad pages 
   
   
   hands in motion 
   Adds realism and fluidity 
   Avoids awkward, static body posture 
   
   
   seated with body turned 
   Adds depth and twist to the torso 
   Reduces symmetry, feels natural 
   
   
   shot from low angle 
   Power or status cue 
   Works well for stylized streetwear or product hero shots 
   
  
 
Example: Putting it all together 
The following example puts together what weâ€™ve discussed in this post. 
 
  
   
    Prompt â€œstudio portrait of a model with platinum hair in metallic cargo pants and a cropped mesh hoodie, seated with legs wide on (acrylic stairs:1.6), magenta and teal gel lighting from left and behind, dramatic contrast, shot on 50mm, streetwear editorial for Gen Z campaignâ€ Negative prompt â€œblurry, extra limbs, watermark, cartoon, distorted face missing fingers, bad anatomyâ€ 
    
   
  
 
Letâ€™s break down the preceding prompt. We direct the look of the subject (platinum hair, metallic clothes), specify their pose (seated wide-legged, confident, unposed), define the environment (acrylic stairs and studio setup, controlled, modern), state the lighting (mixed gel sources, bold stylization), designate the lens (50mm, portrait realism), and lastly detail the purpose (for Gen Z campaign, sets visual and cultural tone). Together, the prompt produces the desired result. 
Best practices and troubleshooting 
Prompting is rarely a one-and-done task, especially for creative use cases. Most great images come from refining an idea over multiple attempts. Consider the following methodology to iterate over your prompts: 
 
 Keep a prompt log 
 Change one variable at a time 
 Save seeds and base images 
 Use comparison grids 
 
Sometimes things go wrongâ€”maybe the model ignores your prompt, or the image looks messy. These issues are common and often quick to fix, and you can get sharper, cleaner, and more intentional outputs with every adjustment. The following table provides useful tips for troubleshooting your prompts. 
 
  
   
   Problem 
   Cause of issue 
   How to fix it 
   
   
   Style feels random 
   Model is confused or terms are vague 
   Clarify style, add weight, remove conflicts 
   
   
   Face gets warped 
   Over-styled or lacks facial cues 
   Add portrait of, headshot, or adjust pose or lighting 
   
   
   Image is too dark 
   Lighting not defined 
   Add softbox from left, natural light, or time of day 
   
   
   Repetitive poses 
   Same seed or static structure 
   Switch seed or change camera angle or subject action 
   
   
   Lacks realism or feels â€œAI-ishâ€ 
   Wrong tone or artifacts 
   Add negatives like cartoon, digital texture, distorted 
   
  
 
Conclusion 
Mastering advanced prompting techniques can turn basic image generation into professional creative outputs. Stability AI Image Services in Amazon Bedrock provide precise control over visual creation and editing, helping businesses convert concepts into production-ready assets. The combination of technical expertise and creative intent can help creators achieve the precision and consistency required in professional settings. This control proves valuable across multiple applications, such as marketing campaigns, brand consistency, and product visualizations. This post demonstrated how to optimize Stability AI Image Services in Amazon Bedrock to produce high-quality imagery that aligns with your creative goals. 
To implement these techniques, access Stability AI Image Services through Amazon Bedrock or explore Stability AIâ€™s foundation models available in Amazon SageMaker JumpStart. You can also find practical code examples in our GitHub repository. 
 
About the authors 
Maxfield Hulker is the VP of Community and Business Development at Stability AI. He is a longtime leader in the generative AI space. He has helped build creator-focused platforms like Civitai and Dream Studio. Maxfield regularly publishes guides and tutorials to make advanced AI techniques more accessible. 
Suleman Patel is a Senior Solutions Architect at Amazon Web Services (AWS), with a special focus on machine learning and modernization. Leveraging his expertise in both business and technology, Suleman helps customers design and build solutions that tackle real-world business problems. When heâ€™s not immersed in his work, Suleman loves exploring the outdoors, taking road trips, and cooking up delicious dishes in the kitchen. 
Isha Dua is a Senior Solutions Architect based in the San Francisco Bay Area working with generative AI model providers and helping customer optimize their generative AI workloads on AWS. She helps enterprise customers grow by understanding their goals and challenges, and guides them on how they can architect their applications in a cloud-based manner while supporting resilience and scalability. Sheâ€™s passionate about machine learning technologies and environmental sustainability. 
Fabio Branco is a Senior Customer Solutions Manager at Amazon Web Services (AWS) and a strategic advisor, helping customers achieve business transformation, drive innovation through generative AI and data solutions, and successfully navigate their cloud journeys. Prior to AWS, he held Product Management, Engineering, Consulting, and Technology Delivery roles across multiple Fortune 500 companies in industries, including retail and consumer goods, oil and gas, financial services, insurance, and aerospace and defense.

â¸»