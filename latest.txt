‚úÖ Morning News Briefing ‚Äì November 06, 2025 10:47

üìÖ Date: 2025-11-06 10:47
üè∑Ô∏è Tags: #briefing #ai #publichealth #digitalgov

‚∏ª

üßæ Weather
‚Ä¢ Current Conditions:  1.4¬∞C
  Temperature: 1.4&deg;C Pressure / Tendency: 101.4 kPa rising Humidity: 88 % Dewpoint: -0.3&deg:C Wind: WNW 13 km/h Air Quality Health Index: n/a . Pembroke 5:00 AM EST Thursday 6 November 2025 Temperature: . 1.2¬∞C Pressure: 101
‚Ä¢ Thursday: Chance of flurries. High plus 3. POP 30%
  30 percent chance of flurries early this morning . A mix of sun and cloud . Wind up to 15 km/h. Wind chill minus 6 this morning. UV index 1 or low. High plus 3.50¬∞C. Wind chiller minus 6¬∞C . High minus 3¬∞C in the Arctic Circle, with a low of 5¬∞C for the rest of the
‚Ä¢ Thursday night: Chance of flurries. Low minus 5. POP 60%
  Increasing cloudiness near midnight then 60 percent chance of flurries overnight . Clear. Clear. Wind up to 15 km/h. Wind chill chill minus 7 overnight. Low minus 5.50¬∞F. Wind chiller minus 7¬∞F overnight. Forecast issued 5:00 AM EST Thursday 6 November 2025 . Forecast: Snowfall expected to continue into mid-November .

üåç International News
No updates.

üçÅ Canadian News
No updates.

üá∫üá∏ U.S. Top Stories
‚Ä¢ The federal government is still shut down. Here's what that means across the country
  NPR reporters are digging into the ways the government shutdown is playing out in their region . The federal government remains shut down, in what is now the longest shutdown in U.S. history . Reporters from the NPR Network are looking at the effects of the shutdown in their regions . The shutdown is the longest in U.,S. government history, according to CNN's Tyrone Turner .
‚Ä¢ How would the government shutdown affect Thanksgiving travel? Here's what to know
  Holiday travel can already be stressful . Here's how a prolonged government shutdown might make things even harder for travelers . The government shutdown could make it even more difficult for travelers to travel in the U.S. It's unclear whether you should travel at all at the end of the year . The shutdown could be even more stressful for travelers who want to avoid it all over the country . The
‚Ä¢ ICE is sending a chill through the construction industry
  More than a quarter of construction workers are foreign-born . Industry officials say Trump's immigration crackdown is making it worse . Construction industry has long struggled to find enough workers, now officials say it's getting worse . Trump's crackdown is hurting the construction industry, officials say, and it's even hurting the industry's need to hire more foreign workers . The construction industry has been hit hard by
‚Ä¢ The government shutdown is delaying an assistance program families use to heat homes
  The Low-Income Home Energy Assistance Program helps about 6 million U.S. households pay energy bills, buy fuel, or fix broken heaters . The shutdown has stalled funds for the LIHEAP program, which helps 6 million households pay bills, fuel, and repair broken heating systems . The government shutdown has stymied the program's ability to continue to work through the shutdown
‚Ä¢ The DOJ has been firing judges with immigrant defense backgrounds
  NPR's data analysis shows that the DOJ has tended to fire judges with immigrant defense backgrounds in its recent rounds of dismissals . NPR also found that judges with immigration defense backgrounds have tended to be fired in the recent dismissals of judges . The DOJ has fired judges with immigrants defense backgrounds more often in recent rounds than in previous rounds of dismissalals, according to the data analysis . The federal

üß† Artificial Intelligence
No updates.

üíª Digital Strategy
‚Ä¢ Malware-pwned laptop gifts cybercriminals Nikkei's Slack
  Stolen creds let miscreants waltz into 17K employees' chats, spilling info on staff and partners . Japanese media behemoth Nikkei admits to data breach after snooping on 17,000 employees and business partners . The personal details of more than 17K people and partners were exposed in a Slack chat room . Stolen info was stolen from the company's
‚Ä¢ Boffins: cloud computing's on-demand biz model is failing us
  Science loses when lab workers grapple with costs and availability, claim researchers . Cloud vendors‚Äô commercial models poorly serve scientists, forcing them to struggle for value amid tightening budgets, say researchers . Research: Science loses if lab workers struggle with costs, availability, availability and costs .‚Ä¶ Scientists struggle with value amid tight budgets, according to research.‚Ä¶‚Ä¶‚Ä¶ Scientists lose when lab staff grapple with
‚Ä¢ UK space sector 'lacks strategic direction,' Lords warn
  Parliamentary report calls for sovereign launch capability and reduced dependence on US services . 2021 National Space Strategy has "failed to turn its ambitions into reality," report says . UK Engagement with Space Committee has published a scathing report, "The Space Economy: Act Now or Lose Out," declaring that the 2021 national space strategy has failed to turn "its ambitions¬†into reality" House of Lords UK Eng
‚Ä¢ China uses Mars orbiter to snap interstellar comet 3I/ATLAS
  China has matched the European Space Agency‚Äôs feat of taking a snapshot of interstellar comet 3I/ATLAS from a Mars orbiter . Middle Kingdom also postpones astronaut return mission after something hit its spaceship . China has matching the European space agency's feat of capturing the interstellar comet from a Martian orbiter.‚Ä¶‚Ä¶‚Ä¶...‚Ä¶‚Ä¶ China has also postponed its astronaut return
‚Ä¢ Qualcomm bets on inferencing in the cloud, which Arm says can‚Äôt run it all it forever
  Qualcomm and Arm have offered differing predictions regarding the market for inferencing silicon . The two companies are close partners in the field . Arm and Qualcomm have been working together for years to develop silicon chips for the next-generation mobile platforms . Arm has been working with Qualcomm to develop a new processor chip that could be used in the next generation of mobile phones, but Qualcomm says it will be

üè• Public Health
No updates.

üî¨ Science
‚Ä¢ Addressing the Recurrent and Protracted Cholera Outbreaks in Africa: Challenges and the Way Forward
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ WHO benchmarks for equitable hip-fracture care and osteoporosis treatment in older people
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ A network analysis of housing quality indicators and depression in women
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ DietAI24 as a framework for comprehensive nutrition estimation using multimodal large language models
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Are reallocating time to moderate-to-vigorous physical activity associated with preschoolers‚Äô body composition?
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations

üßæ Government & Policy
No updates.

üèõÔ∏è Enterprise Architecture & IT Governance
No updates.

ü§ñ AI & Emerging Tech
‚Ä¢ A new ion-based quantum computer makes error correction simpler
  The US- and UK-based company Quantinuum today unveiled Helios, its third-generation quantum computer, which includes expanded computing power and error correction capability.&nbsp;



Like all other existing quantum computers, Helios is not powerful enough to execute the industry‚Äôs dream money-making algorithms, such as those that would be useful for materials discovery or financial modeling. But Quantinuum‚Äôs machines, which use individual ions as qubits, could be easier to scale up than quantum computers that use superconducting circuits as qubits, such as Google‚Äôs and IBM‚Äôs.



‚ÄúHelios is an important proof point in our road map about how we‚Äôll scale to larger physical systems,‚Äù says Jennifer Strabley, vice president at Quantinuum, which formed in 2021 from the merger of Honeywell Quantum Solutions and Cambridge Quantum. Honeywell remains Quantinuum‚Äôs majority owner.



Located at Quantinuum‚Äôs facility in Colorado, Helios comprises a myriad of components, including mirrors, lasers, and optical fiber. Its core is a thumbnail-size chip containing the barium ions that serve as the qubits, which perform the actual computing. Helios computes with 98 barium ions at a time; its predecessor, H2, used 56 ytterbium qubits. The barium ions are an upgrade, as they have proven easier to control than ytterbium.&nbsp; These components all sit within a chamber that is cooled to about 15 Kelvin (-432.67 ‚Ñâ), on top of an optical table. Users can access the computer by logging in remotely over the cloud.



Helios encodes information in the ions‚Äô quantum states, which can represent not only 0s and 1s, like the bits in classical computing, but probabilistic combinations of both, known as superpositions. A hallmark of quantum computing, these superposition states are akin to the state of a coin flipping in the air‚Äîneither heads nor tails, but some probability of both.&nbsp;



Quantum computing exploits the unique mathematics of quantum-mechanical objects like ions to perform computations. Proponents of the technology believe this should enable commercially useful applications, such as highly accurate chemistry simulations for the development of batteries or better optimization algorithms for logistics and finance.&nbsp;



In the last decade, researchers at companies and academic institutions worldwide have incrementally developed the technology with billions of dollars of private and public funding. Still, quantum computing is in an awkward teenage phase. It‚Äôs unclear when it will bring profitable applications. Of late, developers have focused on scaling up the machines.&nbsp;



A key challenge to making a more powerful quantum computer is implementing error correction. Like all computers, quantum computers occasionally make mistakes. Classical computers correct these errors by storing information redundantly. Owing to quirks of quantum mechanics, quantum computers can‚Äôt do this and require special correction techniques.&nbsp;



Quantum error correction involves storing a single unit of information in multiple qubits rather than in a single qubit. The exact methods vary depending on the specific hardware of the quantum computer, with some machines requiring more qubits per unit of information than others. The industry refers to an error-corrected unit of quantum information as a ‚Äúlogical qubit.‚Äù Helios needs two ions, or ‚Äúphysical qubits,‚Äù to create one logical qubit.



This is fewer physical qubits than needed in recent quantum computers made of superconducting circuits. In 2024, Google used 105 physical qubits to create a logical qubit. This year, IBM used 12 physical qubits per single logical qubit, and Amazon Web Services used nine physical qubits to produce a single logical qubit. All three companies use variations of superconducting circuits as qubits.



Helios is noteworthy for its qubits‚Äô precision, says Rajibul Islam, a physicist at the University of Waterloo in Canada, who is not affiliated with Quantinuum. The computer‚Äôs qubit error rates are low to begin with, which means it doesn‚Äôt need to devote as much of its hardware to error correction. Quantinuum had pairs of qubits interact in an operation known as entanglement and found that they behaved as expected 99.921% of the time. ‚ÄúTo the best of my knowledge, no other platform is at this level,‚Äù says Islam.



This advantage comes from a design property of ions. Unlike superconducting circuits, which are affixed to the surface of a quantum computing chip, ions on Quantinuum‚Äôs Helios chip can be shuffled around. Because the ions can move, they can interact with every other ion in the computer, a capacity known as ‚Äúall-to-all connectivity.‚Äù This connectivity allows for error correction approaches that use fewer physical qubits. In contrast, superconducting qubits can only interact with their direct neighbors, so a computation between two non-adjacent qubits requires several intermediate steps involving the qubits in between. ‚ÄúIt‚Äôs becoming increasingly more apparent how important all-to-all-connectivity is for these high-performing systems,‚Äù says Strabley.



Still, it‚Äôs not clear what type of qubit will win in the long run. Each type has design benefits that could ultimately make it easier to scale. Ions (which are used by the US-based startup IonQ as well as Quantinuum) offer an advantage because they produce relatively few errors, says Islam: ‚ÄúEven with fewer physical qubits, you can do more.‚Äù However, it‚Äôs easier to manufacture superconducting qubits. And qubits made of neutral atoms, such as the quantum computers built by the Boston-based startup QuEra, are ‚Äúeasier to trap‚Äù than ions, he says.&nbsp;



Besides increasing the number of qubits on its chip, another notable achievement for Quantinuum is that it demonstrated error correction ‚Äúon the fly,‚Äù says David Hayes, the company‚Äôs director of computational theory and design, That‚Äôs a new capability for its machines. Nvidia GPUs were used to identify errors in the qubits in parallel. Hayes thinks that GPUs are more effective for error correction than chips known as FPGAs, also used in the industry.



Quantinuum has used its computers to investigate the basic physics of magnetism and superconductivity. Earlier this year, it reported simulating a magnet on H2, Quantinuum‚Äôs predecessor, with the claim that it ‚Äúrivals the best classical approaches in expanding our understanding of magnetism.‚Äù Along with announcing the introduction of Helios, the company has used the machine to simulate the behavior of electrons in a high-temperature superconductor.&nbsp;



‚ÄúThese aren‚Äôt contrived problems,‚Äù says Hayes. ‚ÄúThese are problems that the Department of Energy, for example, is very interested in.‚Äù



Quantinuum plans to build another version of Helios in its facility in Minnesota. It has already begun to build a prototype for a fourth-generation computer, Sol, which it plans to deliver in 2027, with 192 physical qubits. Then, in 2029, the company hopes to release Apollo, which it says will have thousands of physical qubits and should be ‚Äúfully fault tolerant,‚Äù or able to implement error correction at a large scale.
‚Ä¢ The Download: the solar geoengineering race, and future gazing with the The Simpsons
  This is today&#8217;s edition of¬†The Download,¬†our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



Why the for-profit race into solar geoengineering is bad for science and public trust



‚ÄîDavid Keith is the professor of geophysical sciences at the University of Chicago and Daniele Visioni is an assistant professor of earth and atmospheric sciences at Cornell University



Last week, an American-Israeli company that claims it‚Äôs developed proprietary technology to cool the planet announced it had raised $60 million, by far the largest known venture capital round to date for a solar geoengineering startup.The company, Stardust, says the funding will enable it to develop a system that could be deployed by the start of the next decade, according to Heatmap, which broke the story.As scientists who have worked on the science of solar geoengineering for decades, we have grown increasingly concerned about emerging efforts to start and fund private companies to deploy technologies that could alter the climate of the planet. We also strongly dispute some of the technical claims that certain companies have made about their offerings. Read the full story.



This story is part of Heat Exchange, MIT Technology Review‚Äôs guest opinion series offering expert commentary on legal, political and regulatory issues related to climate change and clean energy. You can read the rest of the series here.







Can ‚ÄúThe Simpsons‚Äù really predict the future?



According to internet listicles, the animated sitcom The Simpsons has predicted the future anywhere from 17 to 55 times.The show foresaw Donald Trump becoming US President a full 17 years before the real estate mogul was inaugurated as the 45th leader of the United States. Earlier, in 1993, an episode of the show featured the ‚ÄúOsaka flu,‚Äù which some felt was eerily prescient of the coronavirus pandemic. And‚Äîsomehow!‚ÄîSimpsons writers just knew that the US Olympic curling team would beat Sweden eight whole years before they did it.Al Jean has worked on The Simpsons on and off since 1989; he is the cartoon‚Äôs longest-serving showrunner. Here, he reflects on the conspiracy theories that have sprung from these apparent prophecies. Read the full story.



‚ÄîAmelia Tait



This story is part of MIT Technology Review‚Äôs series ‚ÄúThe New Conspiracy Age,‚Äù about how the present boom in conspiracy theories is reshaping science and technology.







MIT Technology Review Narrated: Therapists are secretly using ChatGPT. Clients are triggered.



Declan would never have found out his therapist was using ChatGPT had it not been for a technical mishap where his therapist began inadvertently sharing his screen.



For the rest of the session, Declan was privy to a real-time stream of ChatGPT analysis rippling across his therapist‚Äôs screen, who was taking what Declan was saying, putting it into ChatGPT, and then parroting its answers.



But Declan is not alone. In fact, a growing number of people are reporting receiving AI-generated communiqu√©s from their therapists. Clients‚Äô trust and privacy are being abandoned in the process.



This is our latest story to be turned into a MIT Technology Review Narrated podcast, which we‚Äôre publishing each week on Spotify and Apple Podcasts. Just navigate to MIT Technology Review Narrated on either platform, and follow us to get all our new content as it‚Äôs released.







The must-reads



I‚Äôve combed the internet to find you today‚Äôs most fun/important/scary/fascinating stories about technology.



1 Amazon is suing Perplexity over its Comet AI agentIt alleges Perplexity is committing computer fraud by not disclosing when Comet is shopping on a human‚Äôs behalf. (Bloomberg $)+ In turn, Perplexity has accused Amazon of bullying. (CNBC)



2 Trump has nominated the billionaire entrepreneur Jared Isaacman to lead NASAFive months after he withdrew Isaacman‚Äôs nomination for the same job. (WP $)+ It was around the same time Elon Musk left the US government. (WSJ $)3 Homeland Security has released an app for police forces to scan people‚Äôs faces¬†Mobile Fortify uses facial recognition to identify whether someone‚Äôs been given a deportation order. (404 Media)+ Another effort to track ICE raids was just taken offline. (MIT Technology Review)



4 Scientific journals are being swamped with AI-written lettersResearchers are sifting through their inbox trying to work out what to believe. (NYT $)+ ArXiv is no longer accepting certain papers for fear they‚Äôve been written by AI. (404 Media)



5 The AI boom has proved a major windfall for equipment makers¬†Makers of small turbines and fuel cells, rejoice. (WSJ $)



6 Chronic kidney disease may be the first chronic illness linked to climate changeExperts have linked a surge in the disease to hotter temperatures. (Undark)+ The quest to find out how our bodies react to extreme temperatures. (MIT Technology Review)



7 Brazil is proposing a fund to protect tropical forestsIt would pay countries not to fell their trees. (NYT $)



8 New York has voted for a citywide digital mapIt‚Äôll officially represent the five boroughs for the first time. (Fast Company $)



9 The internet could be at risk of catastrophic collapseMeet the people preparing for that exact eventuality. (New Scientist $)10 A Chinese space craft may have been hit by space junkThree astronauts have been forced to remain on the Tiangong space station while the damage is investigated. (Ars Technica)







Quote of the day



‚ÄúI am not sure how I earned the trust of so many, but I will do everything I can to live up to those expectations.‚Äù



‚ÄîJared Isaacman, Donald Trump‚Äôs renomination to lead NASA, doesn‚Äôt appear entirely sure in his own abilities to lead the agency, Ars Technica reports.







One more thing







Is the digital dollar dead?In 2020, digital currencies were one of the hottest topics in town. China was well on its way to launching its own central bank digital currency, or CBDC, and many other countries launched CBDC research projects, including the US.How things change. Years later, the digital dollar‚Äîeven though it doesn‚Äôt exist‚Äîhas become political red meat, as some politicians label it a dystopian tool for surveillance. And late last year, the Boston Fed quietly stopped working on its CBDC project. So is the dream of the digital dollar dead? Read the full story.‚ÄîMike Orcutt







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)



+ The world‚Äôs oldest air has been unleashed, after six million years under ice.+ How to stop sweating the small stuff and try to be happy in this mad world.+ Happy Bonfire Night to our British readers! + The spirit of Halloween is still with us: the scariest music ever recorded.
‚Ä¢ From vibe coding to context engineering: 2025 in software development
  This year, we‚Äôve seen a real-time experiment playing out across the technology industry, one in which AI‚Äôs software engineering capabilities have been put to the test against human technologists. And although 2025 may have started with AI looking strong, the transition from vibe coding to what‚Äôs being termed context engineering shows that while the work of human developers is evolving, they nevertheless remain absolutely critical.



This is captured in the latest volume of the ‚ÄúThoughtworks Technology Radar,‚Äù a report on the technologies used by our teams on projects with clients. In it, we see the emergence of techniques and tooling designed to help teams better tackle the problem of managing context when working with LLMs and AI agents.&nbsp;



Taken together, there‚Äôs a clear signal of the direction of travel in software engineering and even AI more broadly. After years of the industry assuming progress in AI is all about scale and speed, we‚Äôre starting to see that what matters is the ability to handle context effectively.







Vibes, antipatterns, and new innovations¬†



In February 2025, Andrej Karpathy coined the term vibe coding. It took the industry by storm. It certainly sparked debate at Thoughtworks; many of us were skeptical. On an April episode of our technology podcast, we talked about our concerns and were cautious about how vibe coding might evolve.



Unsurprisingly given the implied imprecision of vibe-based coding, antipatterns have been proliferating. We‚Äôve once again noted, for instance, complacency with AI generated code on the latest volume of the Technology Radar, but it‚Äôs also worth pointing out that early ventures into vibe coding also exposed a degree of complacency about what AI models can actually handle ‚Äî users demanded more and prompts grew larger, but model reliability started to falter.



Experimenting with generative AI¬†



This is one of the drivers behind increasing interest in engineering context. We‚Äôre well aware of its importance, working with coding assistants like Claude Code and Augment Code. Providing necessary context‚Äîor knowledge priming‚Äîis crucial. It ensures outputs are more consistent and reliable, which will ultimately lead to better software that needs less work ‚Äî reducing rewrites and potentially driving productivity.



When effectively prepared, we‚Äôve seen good results when using generative AI to understand legacy codebases. Indeed, done effectively with the appropriate context, it can even help when we don‚Äôt have full access to source code.&nbsp;



It‚Äôs important to remember that context isn‚Äôt just about more data and more detail. This is one of the lessons we‚Äôve taken from using generative AI for forward engineering. It might sound counterintuitive, but in this scenario, we‚Äôve found AI to be more effective when it‚Äôs further abstracted from the underlying system ‚Äî or, in other words, further removed from the specifics of the legacy code. This is because the solution space becomes much wider, allowing us to better leverage the generative and creative capabilities of the AI models we use.



Context is critical in the agentic era



The backdrop of changes that have happened over recent months is the growth of agents and agentic systems ‚Äî both as products organizations want to develop and as technology they want to leverage. This has forced the industry to properly reckon with context and move away from a purely vibes-based approach.



Indeed, far from simply getting on with tasks they‚Äôve been programmed to do, agents require significant human intervention to ensure they are equipped to respond to complex and dynamic contexts.&nbsp;



There are a number of context-related technologies aimed at tackling this challenge, including agents.md, Context7, and Mem0. But it‚Äôs also a question of approach. For instance, we‚Äôve found success with anchoring coding agents to a reference application ‚Äî essentially providing agents with a contextual ground truth. We‚Äôre also experimenting with using teams of coding agents; while this might sound like it increases complexity, it actually removes some of the burden of having to give a single agent all the dense layers of context it needs to do its job successfully.



Toward consensus



Hopefully the space will mature as practices and standards embed. It would be remiss to not mention the significance of the Model Context Protocol, which has emerged as the go-to protocol for connecting LLMs or agentic AI to sources of context. Relatedly, the agent2agent (A2A) protocol leads the way with standardizing how agents interact with one another.&nbsp;



It remains to be seen whether these standards win out. But in any case, it‚Äôs important to consider the day-to-day practices that allow us, as software engineers and technologists, to collaborate effectively even when dealing with highly complex and dynamic systems. Sure, AI needs context, but so do we. Techniques like curated shared instructions for software teams may not sound like the hottest innovation on the planet, but they can be remarkably powerful for helping teams work together.



There‚Äôs perhaps also a conversation to be had about what these changes mean for agile software development. Spec-driven development is one idea that appears to have some traction, but there are still questions about how we remain adaptable and flexible while also building robust contextual foundations and ground truths for AI systems.



Software engineers can solve the context challenge



Clearly, 2025 has been a huge year in the evolution of software engineering as a practice. There‚Äôs a lot the industry needs to monitor closely, but it‚Äôs also an exciting time. And while fears about AI job automation may remain, the fact the conversation has moved from questions of speed and scale to context puts software engineers right at the heart of things.&nbsp;



Once again, it will be down to them to experiment, collaborate, and learn ‚Äî the future depends on it.



This content was produced by Thoughtworks. It was not written by MIT Technology Review‚Äôs editorial staff.
‚Ä¢ Why the for-profit race into solar geoengineering is bad for science and public trust
  Last week, an American-Israeli company that claims it‚Äôs developed proprietary technology to cool the planet announced it had raised $60 million, by far the largest known venture capital round to date for a solar geoengineering startup.



The company, Stardust, says the funding will enable it to develop a system that could be deployed by the start of the next decade, according to Heatmap, which broke the story.







Heat Exchange



MIT Technology Review‚Äôs guest opinion series, offering expert commentary on legal, political and regulatory issues related to climate change and clean energy. You can read the rest of the pieces here.







As scientists who have worked on the science of solar geoengineering for decades, we have grown increasingly concerned about the emerging efforts to start and fund private companies to build and deploy technologies that could alter the climate of the planet. We also strongly dispute some of the technical claims that certain companies have made about their offerings.&nbsp;



Given the potential power of such tools, the public concerns about them, and the importance of using them responsibly, we argue that they should be studied, evaluated, and developed mainly through publicly coordinated and transparently funded science and engineering efforts.&nbsp; In addition, any decisions about whether or how they should be used should be made through multilateral government discussions, informed by the best available research on the promise and risks of such interventions‚Äînot the profit motives of companies or their investors.



The basic idea behind solar geoengineering, or what we now prefer to call sunlight reflection methods (SRM), is that humans might reduce climate change by making the Earth a bit more reflective, partially counteracting the warming caused by the accumulation of greenhouse gases.&nbsp;



There is strong evidence, based on years of climate modeling and analyses by researchers worldwide, that SRM‚Äîwhile not perfect‚Äîcould significantly and rapidly reduce climate changes and avoid important climate risks. In particular, it could ease the impacts in hot countries that are struggling to adapt.&nbsp;&nbsp;



The goals of doing research into SRM can be diverse: identifying risks as well as finding better methods. But research won‚Äôt be useful unless it‚Äôs trusted, and trust depends on transparency. That means researchers must be eager to examine pros and cons, committed to following the evidence where it leads, and driven by a sense that research should serve public interests, not be locked up as intellectual property.



In recent years, a handful of for-profit startup companies have emerged that are striving to develop SRM technologies or already trying to market SRM services. That includes Make Sunsets, which sells ‚Äúcooling credits‚Äù for releasing sulfur dioxide in the stratosphere. A new company, Sunscreen, which hasn‚Äôt yet been announced, intends to use aerosols in the lower atmosphere to achieve cooling over small areas, purportedly to help farmers or cities deal with extreme heat.&nbsp;&nbsp;





Our strong impression is that people in these companies are driven by the same concerns about climate change that move us in our research. We agree that more research, and more innovation, is needed. However, we do not think startups‚Äîwhich by definition must eventually make money to stay in business‚Äîcan play a productive role in advancing research on SRM.



Many people already distrust the idea of engineering the atmosphere‚Äîat whichever scale‚Äîto address climate change, fearing negative side effects, inequitable impacts on different parts of the world, or the prospect that a world expecting such solutions will feel less pressure to address the root causes of climate change.



Adding business interests, profit motives, and rich investors into this situation just creates more cause for concern, complicating the ability of responsible scientists and engineers to carry out the work needed to advance our understanding.



The only way these startups will make money is if someone pays for their services, so there‚Äôs a reasonable fear that financial pressures could drive companies to lobby governments or other parties to use such tools. A decision that should be based on objective analysis of risks and benefits would instead be strongly influenced by financial interests and political connections.



The need to raise money or bring in revenue often drives companies to hype the potential or safety of their tools. Indeed, that‚Äôs what private companies need to do to attract investors, but it‚Äôs not how you build public trust‚Äîparticularly when the science doesn‚Äôt support the claims.



Notably, Stardust says on its website that it has developed novel particles that can be injected into the atmosphere to reflect away more sunlight, asserting that they‚Äôre ‚Äúchemically inert in the stratosphere, and safe for humans and ecosystems.‚Äù According to the company, ‚ÄúThe particles naturally return to Earth‚Äôs surface over time and recycle safely back into the biosphere.‚Äù



But it‚Äôs nonsense for the company to claim they can make particles that are inert in the stratosphere. Even diamonds, which are extraordinarily nonreactive, would alter stratospheric chemistry. First of all, much of that chemistry depends on highly reactive radicals that react with any solid surface, and second, any particle may become coated by background sulfuric acid in the stratosphere. That could accelerate the loss of the protective ozone layer by spreading that existing sulfuric acid over a larger surface area.



(Stardust didn&#8217;t provide a response to an inquiry about the concerns raised in this piece.)



In materials presented to potential investors, which we‚Äôve obtained a copy of, Stardust further claims its particles ‚Äúimprove‚Äù on sulfuric acid, which is the most studied material for SRM. But the point of using sulfate for such studies was never that it was perfect, but that its broader climatic and environmental impacts are well understood. That‚Äôs because sulfate is widespread on Earth, and there‚Äôs an immense body of scientific knowledge about the fate and risks of sulfur that reaches the stratosphere through volcanic eruptions or other means.



If there‚Äôs one great lesson of 20th-century environmental science, it‚Äôs how crucial it is to understand the ultimate fate of any new material introduced into the environment.&nbsp;





Chlorofluorocarbons and the pesticide DDT both offered safety advantages over competing technologies, but they both broke down into products that accumulated in the environment in unexpected places, causing enormous and unanticipated harms.&nbsp;



The environmental and climate impacts of sulfate aerosols have been studied in many thousands of scientific papers over a century, and this deep well of knowledge greatly reduces the chance of unknown unknowns.&nbsp;



Grandiose claims notwithstanding‚Äîand especially considering that Stardust hasn‚Äôt disclosed anything about its particles or research process‚Äîit would be very difficult to make a pragmatic, risk-informed decision to start SRM efforts with these particles instead of sulfate.



We don‚Äôt want to claim that every single answer lies in academia. We‚Äôd be fools to not be excited by profit-driven innovation in solar power, EVs, batteries, or other sustainable technologies. But the math for sunlight reflection is just different. Why?&nbsp;&nbsp;&nbsp;



Because the role of private industry was essential in improving the efficiency, driving down the costs, and increasing the market share of renewables and other forms of cleantech. When cost matters and we can easily evaluate the benefits of the product, then competitive, for-profit capitalism can work wonders.&nbsp;&nbsp;



But SRM is already technically feasible and inexpensive, with deployment costs that are negligible compared with the climate damage it averts.



The essential questions of whether or how to use it come down to far thornier societal issues: How can we best balance the risks and benefits? How can we ensure that it‚Äôs used in an equitable way? How do we make legitimate decisions about SRM on a planet with such sharp political divisions?



Trust will be the most important single ingredient in making these decisions. And trust is the one product for-profit innovation does not naturally manufacture.&nbsp;



Ultimately, we‚Äôre just two researchers. We can‚Äôt make investors in these startups do anything differently. Our request is that they think carefully, and beyond the logic of short-term profit. If they believe geoengineering is worth exploring, could it be that their support will make it harder, not easier, to do that?&nbsp;&nbsp;



David Keith is the professor of geophysical sciences at the University of Chicago and founding faculty director of the school‚Äôs Climate Systems Engineering Initiative. Daniele Visioni is an assistant professor of earth and atmospheric sciences at Cornell University and head of data for Reflective, a nonprofit that develops tools and provides funding to support solar geoengineering research.
‚Ä¢ The Download: the AGI myth, and US/China AI competition
  This is today&#8217;s edition of¬†The Download,¬†our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



How AGI became the most consequential conspiracy theory of our time



‚ÄîWill Douglas Heaven, senior AI editor¬†



Are you feeling it?I hear it‚Äôs close: two years, five years‚Äîmaybe next year! And I hear it‚Äôs going to solve our biggest problems in ways we cannot yet imagine. I also hear it will bring on the apocalypse and kill us all‚Ä¶We‚Äôre of course talking about artificial general intelligence, or AGI‚Äîthat hypothetical near-future technology that (I hear) will be able to do pretty much whatever a human brain can do.Every age has its believers, people with an unshakeable faith that something huge is about to happen‚Äîa before and an after that they are privileged (or doomed) to live through. For us, that‚Äôs the promised advent of AGI. And here‚Äôs what I think: AGI is a lot like a conspiracy theory, and it may be the most consequential one of our time. Read the full story.



This story is part of MIT Technology Review‚Äôs series ‚ÄúThe New Conspiracy Age,‚Äù on how the present boom in conspiracy theories is reshaping science and technology.







The State of AI: Is China about to win the race?&nbsp;



Viewed from abroad, it seems only a matter of time before China emerges as the AI superpower of the 21st century.¬†



In the West, our initial instinct is to focus on America‚Äôs significant lead in semiconductor expertise, its cutting-edge AI research, and its vast investments in data centers.Today, however, China has the means, motive, and opportunity to win. When it comes to mobilizing the whole-of-society resources needed to develop and deploy AI to maximum effect, it may be rash to bet against it. Read the full story.



‚ÄîJohn Thornhill &amp; Caiwei Chen



This is the first edition of The State of AI, a collaboration between the Financial Times &amp; MIT Technology Review examining the ways in which AI is reshaping global power. Every Monday for the next six weeks, writers from both publications will debate one aspect of the generative AI revolution reshaping global power. Sign up to receive future editions every Monday.







The must-reads



I‚Äôve combed the internet to find you today‚Äôs most fun/important/scary/fascinating stories about technology.



1 China is prepared to cut its data centers a sweet dealIf they agree to use native chips over American rivals‚Äô, that is. (FT $)+ What happened when a data center moved into a small American town. (WSJ $)+ Microsoft and OpenAI want more power‚Äîthey just don‚Äôt know how much more. (TechCrunch)+ The data center boom in the desert. (MIT Technology Review)



2 Norway‚Äôs oil fund has rejected Elon Musk‚Äôs $1 trillion pay packageThe Tesla shareholder is concerned about the size of the reward. (WSJ $)+ It says it will vote against the deal on Thursday. (FT $)3 OpenAI has signed a massive compute deal with AmazonIt‚Äôs the latest in a long string of blockbuster deals for the AI company. (Wired $)4 Cybersecurity workers moonlighted as criminal hackersThey‚Äôre accused of sharing their profits with the creators of the ransomware they deployed. (Bloomberg $)+ The hackers demanded tens of millions in extortion payments. (The Register)



5 Tech‚Äôs elites are funding plans to safeguard MAGAEntrepreneur Chris Buskirk is using donor money to equip it to outlive Trump. (WP $)6 These startups supply the labor to train multitasking humanoid robotsTeams of humans are doing the dirty work, including filming themselves folding towels hundreds of times a day. (LA Times $)+ This new system can teach a robot a simple household task within 20 minutes. (MIT Technology Review)



7 LLMs can&#8217;t accurately describe their internal processesAnthropic is on a mission to measure their so-called introspective awareness. (Ars Technica)



8 Why are people using AI to hack their hobbies?Talk about the death of fun. (NY Mag $)+ While we‚Äôre at it, don‚Äôt use chatbots to answer friends‚Äô dilemmas either. (Wired $)+ Or to write research papers. (404 Media)



9 Coca-Cola is doubling down on AI in its adsUndeterred by criticism last year, it‚Äôs back with more for the 2025 holidays. (WSJ $)+ Nothing says festive joy like AI slop. (The Verge)



10 Facebook Dating is a‚Ä¶hit?But you should still be on the lookout for scammers. (NYT $)+ It‚Äôs not just for boomers‚Äîyounger people are using it too. (TechCrunch)+ For better or worse, AI is seeping into all the biggest dating platforms. (Economist $)







Quote of the day



‚ÄúThat was the kick of it, that the AI actually did find compatibility. It was the human part that didn‚Äôt work out.‚Äù



‚ÄîEmma Inge, a project manager looking for love in San Francisco, describes the trouble with using an AI matchmaker to the New York Times: it can‚Äôt stop you getting ghosted.







One more thing







Inside the most dangerous asteroid hunt everIf you were told that the odds of something were 3.1%, it might not seem like much. But for the people charged with protecting our planet, it was huge.On February 18, astronomers determined that a 130- to 300-foot-long asteroid had a 3.1% chance of crashing into Earth in 2032. Never had an asteroid of such dangerous dimensions stood such a high chance of striking the planet. Then, just days later on February 24, experts declared that the danger had passed. Earth would be spared.How did they do it? What was it like to track the rising danger of this asteroid, and to ultimately determine that it‚Äôd miss us?This is the inside story of how a sprawling network of astronomers found, followed, mapped, planned for, and finally dismissed the most dangerous asteroid ever found‚Äîall under the tightest of timelines and, for just a moment, with the highest of stakes. Read the full story.



‚ÄîRobin George Andrews







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)



+ People in the Middle Ages chose to depict the devil in very interesting ways, I‚Äôll say that much.+ We may be inching closer to understanding why the animal kingdom has developed such elaborate markings.+ The music in the new game Pok√©mon Legends: Z-A sure is interesting.+ Slow cooker dinners are beckoning.

üîí Cybersecurity & Privacy
‚Ä¢ Cloudflare Scrubs Aisuru Botnet from Top Domains List
  For the past week, domains associated with the massive Aisuru botnet have repeatedly usurped Amazon, Apple, Google and Microsoft in Cloudflare&#8217;s public ranking of the most frequently requested websites. Cloudflare responded by redacting Aisuru domain names from their top websites list. The chief executive at Cloudflare says Aisuru&#8217;s overlords are using the botnet to boost their malicious domain rankings, while simultaneously attacking the company&#8217;s domain name system (DNS) service.
The #1 and #3 positions in this chart are Aisuru botnet controllers with their full domain names redacted. Source: radar.cloudflare.com.
Aisuru is a rapidly growing botnet comprising hundreds of thousands of hacked Internet of Things (IoT) devices, such as poorly secured Internet routers and security cameras. The botnet has increased in size and firepower significantly since its debut in 2024, demonstrating the ability to launch record distributed denial-of-service (DDoS) attacks nearing 30 terabits of data per second.
Until recently, Aisuru&#8217;s malicious code instructed all infected systems to use DNS servers from Google &#8212; specifically, the servers at 8.8.8.8. But in early October, Aisuru switched to invoking Cloudflare&#8217;s main DNS server &#8212; 1.1.1.1 &#8212; and over the past week domains used by Aisuru to control infected systems started populating Cloudflare&#8217;s top domain rankings.
As screenshots of Aisuru domains claiming two of the Top 10 positions ping-ponged across social media, many feared this was yet another sign that an already untamable botnet was running completely amok. One Aisuru botnet domain that sat prominently for days at #1 on the list was someone&#8217;s street address in Massachusetts followed by &#8220;.com&#8221;. Other Aisuru domains mimicked those belonging to major cloud providers.
Cloudflare tried to address these security, brand confusion and privacy concerns by partially redacting the malicious domains, and adding a warning at the top of its rankings:
&#8220;Note that the top 100 domains and trending domains lists include domains with organic activity as well as domains with emerging malicious behavior.&#8221;

Cloudflare CEO Matthew Prince told KrebsOnSecurity the company&#8217;s domain ranking system is fairly simplistic, and that it merely measures the volume of DNS queries to 1.1.1.1.
&#8220;The attacker is just generating a ton of requests, maybe to influence the ranking but also to attack our DNS service,&#8221; Prince said, adding that Cloudflare has heard reports of other large public DNS services seeing similar uptick in attacks. &#8220;We‚Äôre fixing the ranking to make it smarter. And, in the meantime, redacting any sites we classify as malware.&#8221;
Renee Burton, vice president of threat intel at the DNS security firm Infoblox, said many people erroneously assumed that the skewed Cloudflare domain rankings meant there were more bot-infected devices than there were regular devices querying sites like Google and Apple and Microsoft.
&#8220;Cloudflare&#8217;s documentation is clear &#8212; they know that when it comes to ranking domains you have to make choices on how to normalize things,&#8221; Burton wrote on LinkedIn. &#8220;There are many aspects that are simply out of your control. Why is it hard? Because reasons. TTL values, caching, prefetching, architecture, load balancing. Things that have shared control between the domain owner and everything in between.&#8221;
Alex Greenland is CEO of the anti-phishing and security firm Epi. Greenland said he understands the technical reason why Aisuru botnet domains are showing up in Cloudflare&#8217;s rankings (those rankings are based on DNS query volume, not actual web visits). But he said they&#8217;re still not meant to be there.
&#8220;It&#8217;s a failure on Cloudflare&#8217;s part, and reveals a compromise of the trust and integrity of their rankings,&#8221; he said.
Greenland said Cloudflare planned for its Domain Rankings to list the most popular domains as used by human users, and it was never meant to be a raw calculation of query frequency or traffic volume going through their 1.1.1.1 DNS resolver.
&#8220;They spelled out how their popularity algorithm is designed to reflect real human use and exclude automated traffic (they said they&#8217;re good at this),&#8221; Greenland wrote on LinkedIn. &#8220;So something has evidently gone wrong internally. We should have two rankings: one representing trust and real human use, and another derived from raw DNS volume.&#8221;
Why might it be a good idea to wholly separate malicious domains from the list? Greenland notes that Cloudflare Domain Rankings see widespread use for trust and safety determination, by browsers, DNS resolvers, safe browsing APIs and things like TRANCO.
&#8220;TRANCO is a respected open source list of the top million domains, and Cloudflare Radar is one of their five data providers,&#8221; he continued. &#8220;So there can be serious knock-on effects when a malicious domain features in Cloudflare&#8217;s top 10/100/1000/million. To many people and systems, the top 10 and 100 are naively considered safe and trusted, even though algorithmically-defined top-N lists will always be somewhat crude.&#8221;
Over this past week, Cloudflare started redacting portions of the malicious Aisuru domains from its Top Domains list, leaving only their domain suffix visible. Sometime in the past 24 hours, Cloudflare appears to have begun hiding the malicious Aisuru domains entirely from the web version of that list. However, downloading a spreadsheet of the current Top 200 domains from Cloudflare Radar shows an Aisuru domain still at the very top.
According to Cloudflare&#8217;s website, the majority of DNS queries to the top Aisuru domains &#8212; nearly 52 percent &#8212; originated from the United States. This tracks with my reporting from early October, which found Aisuru was drawing most of its firepower from IoT devices hosted on U.S. Internet providers like AT&amp;T, Comcast and Verizon.
Experts tracking Aisuru say the botnet relies on well more than a hundred control servers, and that for the moment at least most of those domains are registered in the .su top-level domain (TLD). Dot-su is the TLD assigned to the former Soviet Union (.su&#8217;s Wikipedia page says the TLD was created just 15 months before the fall of the Berlin wall).
A Cloudflare blog post from October 27 found that .su had the highest &#8220;DNS magnitude&#8221; of any TLD, referring to a metric estimating the popularity of a TLD based on the number of unique networks querying Cloudflare&#8217;s 1.1.1.1 resolver. The report concluded that the top .su hostnames were associated with a popular online world-building game, and that more than half of the queries for that TLD came from the United States, Brazil and Germany [it&#8217;s worth noting that servers for the world-building game Minecraft¬†were some of Aisuru&#8217;s most frequent targets].
A simple and crude way to detect Aisuru bot activity on a network may be to set an alert on any systems attempting to contact domains ending in .su. This TLD is frequently abused for cybercrime and by cybercrime forums and services, and blocking access to it entirely is unlikely to raise any legitimate complaints.
‚Ä¢ Alleged Jabber Zeus Coder ‚ÄòMrICQ‚Äô in U.S. Custody
  A Ukrainian man indicted in 2012 for conspiring with a prolific hacking group to steal tens of millions of dollars from U.S. businesses was arrested in Italy and is now in custody in the United States, KrebsOnSecurity has learned.
Sources close to the investigation say Yuriy Igorevich Rybtsov, a 41-year-old from the Russia-controlled city of Donetsk, Ukraine, was previously referenced in U.S. federal charging documents only by his online handle &#8220;MrICQ.&#8221; According to a 13-year-old indictment (PDF) filed by prosecutors in Nebraska, MrICQ was a developer for a cybercrime group known as &#8220;Jabber Zeus.&#8221;
Image: lockedup dot wtf.
The Jabber Zeus name is derived from the malware they used &#8212; a custom version of the ZeuS banking trojan &#8212; that stole banking login credentials and would send the group a Jabber instant message each time a new victim entered a one-time passcode at a financial institution website. The gang targeted mostly small to mid-sized businesses, and they were an early pioneer of so-called &#8220;man-in-the-browser&#8221; attacks, malware that can silently intercept any data that victims submit in a web-based form.
Once inside a victim company&#8217;s accounts, the Jabber Zeus crew would modify the firm&#8217;s payroll to add dozens of &#8220;money mules,&#8221; people recruited through elaborate work-at-home schemes to handle bank transfers. The mules in turn would forward any stolen payroll deposits ‚Äî minus their commissions ‚Äî via wire transfers to other mules in Ukraine and the United Kingdom.
The 2012 indictment¬†targeting the Jabber Zeus crew named MrICQ as &#8220;John Doe #3,&#8221; and said this person handled incoming notifications of newly compromised victims. The Department of Justice (DOJ) said MrICQ also helped the group launder the proceeds of their heists through electronic currency exchange services.
Two sources familiar with the Jabber Zeus investigation said Rybtsov was arrested in Italy, although the exact date and circumstances of his arrest remain unclear. A summary of recent decisions (PDF) published by the Italian Supreme Court states that in April 2025, Rybtsov lost a final appeal to avoid extradition to the United States.
According to the mugshot website lockedup[.]wtf, Rybtsov arrived in Nebraska on October 9, and was being held under an arrest warrant from the U.S. Federal Bureau of Investigation (FBI).
The data breach tracking service Constella Intelligence found breached records from the business profiling site bvdinfo[.]com showing that a 41-year-old Yuriy Igorevich Rybtsov worked in a building at 59 Barnaulska St. in Donetsk. Further searching on this address in Constella finds the same apartment building was shared by a business registered to Vyacheslav ‚ÄúTank‚Äù Penchukov, the leader of the Jabber Zeus crew in Ukraine.
Vyacheslav ‚ÄúTank‚Äù Penchukov, seen here performing as &#8220;DJ Slava Rich&#8221; in Ukraine, in an undated photo from social media.
Penchukov was arrested in 2022 while traveling to meet his wife in Switzerland. Last year, a federal court in Nebraska sentenced Penchukov to 18 years in prison and ordered him to pay more than $73 million in restitution.
Lawrence Baldwin is founder of myNetWatchman, a threat intelligence company based in Georgia that began tracking and disrupting the Jabber Zeus gang in 2009. myNetWatchman had secretly gained access to the Jabber chat server used by the Ukrainian hackers, allowing Baldwin to eavesdrop on the daily conversations between MrICQ and other Jabber Zeus members.
Baldwin shared those real-time chat records with multiple state and federal law enforcement agencies, and with this reporter. Between 2010 and 2013, I spent several hours each day alerting small businesses across the country that their payroll accounts were about to be drained by these cybercriminals.
Those notifications, and Baldwin&#8217;s tireless efforts, saved countless would-be victims a great deal of money. In most cases, however, we were already too late. Nevertheless, the pilfered Jabber Zeus group chats provided the basis for dozens of stories published here about small businesses fighting their banks in court over six- and seven-figure financial losses.
Baldwin said the Jabber Zeus crew was far ahead of its peers in several respects. For starters, their intercepted chats showed they worked to create a highly customized botnet directly with the author of the original Zeus Trojan &#8212; Evgeniy Mikhailovich Bogachev, a Russian man who has long been on the FBI&#8217;s &#8220;Most Wanted&#8221; list. The feds have a standing $3 million reward for information leading to Bogachev&#8217;s arrest.
Evgeniy M. Bogachev, in undated photos.
The core innovation of Jabber Zeus was an alert that MrICQ would receive each time a new victim entered a one-time password code into a phishing page mimicking their financial institution. The gang&#8217;s internal name for this component was &#8220;Leprechaun,&#8221; (the video below from myNetWatchman shows it in action). Jabber Zeus would actually re-write the HTML code as displayed in the victim&#8217;s browser, allowing them to intercept any passcodes sent by the victim&#8217;s bank for multi-factor authentication.
&#8220;These guys had compromised such a large number of victims that they were getting buried in a tsunami of stolen banking credentials,&#8221; Baldwin told KrebsOnSecurity. &#8220;But the whole point of Leprechaun was to isolate the highest-value credentials &#8212; the commercial bank accounts with two-factor authentication turned on. They knew these were far juicier targets because they clearly had a lot more money to protect.&#8221;

Baldwin said the Jabber Zeus trojan also included a custom &#8220;backconnect&#8221; component that allowed the hackers to relay their bank account takeovers through the victim&#8217;s own infected PC.
&#8220;The Jabber Zeus crew were literally connecting to the victim&#8217;s bank account from the victim&#8217;s IP address, or from the remote control function and by fully emulating the device,&#8221; he said. &#8220;That trojan was like a hot knife through butter of what everyone thought was state-of-the-art secure online banking at the time.&#8221;
Although the Jabber Zeus crew was in direct contact with the Zeus author, the chats intercepted by myNetWatchman show Bogachev frequently ignored the group&#8217;s pleas for help. The government says the real leader of the Jabber Zeus crew was Maksim Yakubets, a 38-year Ukrainian man with Russian citizenship who went by the hacker handle &#8220;Aqua.&#8221;
Alleged Evil Corp leader Maksim &#8220;Aqua&#8221; Yakubets. Image: FBI
The Jabber chats intercepted by Baldwin show that Aqua interacted almost daily with MrICQ, Tank and other members of the hacking team, often facilitating the group&#8217;s money mule and cashout activities remotely from Russia.
The government says Yakubets/Aqua would later emerge as the leader of an elite cybercrime ring of at least 17 hackers that referred to themselves internally as &#8220;Evil Corp.&#8221; Members of Evil Corp developed and used the Dridex (a.k.a. Bugat) trojan, which helped them siphon more than $100 million from hundreds of victim companies in the United States and Europe.
This 2019 story about the government&#8217;s $5 million bounty for information leading to Yakubets&#8217;s arrest includes excerpts of conversations between Aqua, Tank, Bogachev and other Jabber Zeus crew members discussing stories I&#8217;d written about their victims. Both Baldwin and I were interviewed at length for a new weekly six-part podcast by the BBC that delves deep into the history of Evil Corp. Episode One focuses on the evolution of Zeus, while the second episode centers on an investigation into the group by former FBI agent Jim Craig.
Image: https://www.bbc.co.uk/programmes/w3ct89y8

üéì University AI
No updates.

üè¢ Corporate AI
‚Ä¢ Magentic Marketplace: an open-source simulation environment for studying agentic markets
  Autonomous AI agents are here, and they&#8217;re poised to reshape the economy. By automating discovery, negotiation, and transactions, agents can overcome inefficiencies like information asymmetries and platform lock-in, enabling faster, more transparent, and more competitive markets.



We are already seeing early signs of this transformation in digital marketplaces. Customer-facing assistants like OpenAI‚Äôs Operator and Anthropic‚Äôs Computer Use can navigate websites and complete purchases. On the business side, Shopify Sidekick, Salesforce Einstein, and Meta‚Äôs Business AI help merchants with operations and customer engagement. These examples hint at a future where agents become active market participants, but the structure of these markets remains uncertain.



Several scenarios are possible. We might see one-sided markets where only customers or businesses deploy agents; closed platforms (known as walled gardens) where companies tightly control agent interactions; or even open two-sided marketplaces where customer and business agents transact freely across ecosystems. Each path carries different trade-offs for security, openness, convenience, and competition, which will shape how value flows in the digital economy. For a deeper exploration of these dynamics, see our paper, The Agentic Economy.



To help navigate this uncertainty, we built Magentic Marketplace (opens in new tab)‚Äî an open-source simulation environment for exploring the numerous possibilities of agentic markets and their societal implications at scale. It provides a foundation for studying these markets and guiding them toward outcomes that benefit everyone.



This matters because most AI agent research focuses on isolated scenarios‚Äîa single agent completing a task or two agents negotiating a simple transaction. But real markets involve a large number of agents simultaneously searching, communicating, and transacting, creating complex dynamics that can‚Äôt be understood by studying agents in isolation. Capturing this complexity is essential because real-world deployments raise critical questions about consumer welfare, market efficiency, fairness, manipulation resistance, and bias‚Äîquestions that can‚Äôt be safely answered in production environments.



To explore these dynamics in depth, the Magentic Marketplace platform enables controlled experimentation across diverse agentic marketplace scenarios. Its current focus is on two-sided markets, but the environment is modular and extensible, supporting future exploration of mixed human‚Äìagent systems, one-sided markets, and complex communication protocols.



Figure 1. With Magentic Marketplace, researchers can model how agents representing customers and businesses interact‚Äîshedding light on the dynamics that could shape future digital markets.



What is Magentic Marketplace?



Magentic Marketplace‚Äôs environment manages market-wide capabilities like maintaining catalogs of available goods and services, implementing discovery algorithms, facilitating agent-to-agent communication, and handling simulated payments through a centralized transaction layer at its core, which ensures transaction integrity across all marketplace interactions. Additionally, the platform enables systematic, reproducible research. As demonstrated in the following video, it supports a wide range of agent implementations and evolving marketplace features, allowing researchers to integrate diverse agent architectures and adapt the environment as new capabilities emerge.









We built Magentic Marketplace around three core architectural choices:



HTTP/REST client-server architecture: Agents operate as independent clients while the Marketplace Environment serves as a central server. This mirrors real-world platforms and supports clear separation of customer and business agent roles.



Minimal&nbsp;three-endpoint&nbsp;market&nbsp;protocol:&nbsp;Just&nbsp;three endpoints‚Äîregister, protocol discovery, and action execution‚Äîlets&nbsp;agents dynamically discover available actions.&nbsp;New capabilities&nbsp;can&nbsp;be added without disrupting existing experiments.



Rich action protocol: Specific message types support the complete transaction lifecycle: search, negotiation, proposals, and payments. The protocol is designed for extensibility. New actions like refunds, reviews, or ratings can be added seamlessly, allowing researchers to evolve marketplace capabilities and study emerging agent behaviors while remaining compatible.



Figure 2. Magentic Marketplace includes two agent types: Assistant Agents (customers) and Service Agents (businesses). Both interact with a central Market Environment via REST APIs for registration, service discovery, communication, and transaction execution. Action Routers manage message flow and protocol requests, enabling autonomous negotiation and commerce in a two-sided marketplace.



Additionally, a visualization module lets users observe marketplace dynamics and review individual conversation threads between customer and business agents.



Setting up the experiments



To ensure reproducibility, we instantiated the marketplace with fully synthetic data, available in our open-source repository (opens in new tab). The experiments modeled transactions such as ordering food and engaging with home improvement services, where agents represented customers and businesses engaging in marketplace transactions. This setup enabled precise measurement of behavior and systematic comparison against theoretical upper bounds.



Each experiment was run using 100 customers and 300 businesses and included both proprietary models (GPT-4o, GPT-4.1, GPT-5, and Gemini-2.5-Flash) and open-source models (OSS-20b, Qwen3-14b, and Qwen3-4b-Instruct-2507).



Our scenarios focused on simple all-or-nothing requests: Each customer had a list of desired items and amenities that needed to be present for a transaction to be satisfying. For those transactions, utility was computed as the sum of the customer‚Äôs internal item valuations minus actual prices paid. Consumer welfare, defined as the sum of utilities across all completed transactions, served as our key metric for comparing agent performance.



While this experimental setup provides a useful starting point, it is not intended to be definitive. We encourage researchers to extend the framework with richer, more nuanced measures and request types that better capture real consumer welfare, fairness, and other societal considerations.



	
		

		
		PODCAST SERIES
	
	
	
						
				
					
				
			
			
			

									The AI Revolution in Medicine, Revisited
				
								Join Microsoft‚Äôs Peter Lee on a journey to discover how AI is impacting healthcare and what it means for the future of medicine.
				
								
					
						
							Listen now						
					
				
							
	
Opens in a new tab	
	


What did we find?



Agents can improve consumer welfare‚Äîbut only with good discovery



We explored whether two-sided agentic markets‚Äîwhere AI agents interact with each other and with service providers‚Äîcan improve consumer welfare by reducing information gaps. Unlike traditional markets, which do not provide agentic support and place the full burden of overcoming information asymmetries on customers, agentic markets shift much of that effort to agents. This change matters because as agents gain better tools for discovery and communication, they relieve customers of the heavy cognitive load of filling any information gaps. This lowers the cost of making informed decisions and improves customer outcomes.



We compared several marketplace setups. Under realistic conditions (Agentic: Lexical search), agents faced real-world challenges like building queries, navigating paginated lists, identifying the right businesses to send inquiries to, and negotiating transactions.



Despite these complexities, advanced proprietary models and some medium-sized open-source models like GPTOSS-20b outperformed simple baselines like randomly choosing or simply choosing the cheapest option. Notably, GPT-5 achieved near-optimal performance, demonstrating its ability to effectively gather and utilize decision-relevant information in realistic marketplace conditions.



Figure 3. Table comparing experimental setups for welfare outcomes in the restaurant industry. Each row shows a different way agents or baselines make decisions, from random picks to fully coordinated agentic strategies. Cell colors indicate how much information is available: green, at the top left, represents complete information, red, at the top right, represents limited information, and yellow at the bottom represents decisions that depend on agent communication.



Performance increased considerably under the Agentic: Perfect search condition, where agents started with the top three matches without needing to search and navigate among the choices. In this setting, Sonnet-4.0, Sonnet-4.5, GPT-5, and GPT-4.1 nearly reached the theoretical optimum and beat baselines with full amenity details but without agent-to-agent coordination.



Open-source models were mixed: GPTOSS-20b performed strongly under both Perfect search and Lexical search conditions, even exceeding GPT-4o&#8217;s performance with Perfect search. This suggests that relatively compact models can exhibit robust information-gathering and decision-making capabilities in complex multi-agent environments. Qwen3-4b-2507 faltered when discovery involved irrelevant options (Lexical search), while Qwen3-14b lagged in both cases due to fundamental limitations in reasoning.



Figure 4. Chart showing consumer welfare outcomes in the restaurant industry under different marketplace setups. Blue bars show Agentic: Lexical search, where agents navigate realistic discovery challenges; yellow bars show Agentic: Perfect search, where agents started with ideal matches. Proprietary models approached optimum consumer welfare under perfect search, while open-source models and baselines lagged behind.



Paradox of Choice



One promise of agents is their ability to consider far more options than people can. However, our experiments revealed a surprising limitation: providing agents with more options does not necessarily lead to more thorough exploration. We designed experiments that varied the search results limit from 3 to 100. Except for Gemini-2.5-Flash and GPT-5, the models contacted only a small fraction of available businesses regardless of the search limit. This suggests that most models do not conduct exhaustive comparisons and instead easily accept the initial &#8220;good enough&#8221; options.



Figure 5. More options didn‚Äôt lead to broader exploration. Most models still contacted only a few businesses, except Gemini-2.5-Flash and GPT-5.



Additionally, across all models, consumer welfare declined as the number of search results increased. Despite contacting over a hundred businesses, Gemini-2.5-Flash&#8217;s performance declined from 1,700 to 1,350, and GPT-5 declined even more, from a near-optimal 2,000 to 1,400.



This demonstrates a Paradox of Choice effect, where more exploration does not guarantee better outcomes, potentially due to limited long context understanding. Claude Sonnet 4 showed the steepest performance decline, from 1,800 to 600 in consumer welfare. With all the options presented, it struggled to navigate larger sets of options and frequently contacted businesses that did not provide the goods or services that the customer was looking for.



This combination of poor initial selection and premature search termination demonstrates both inadequate decision-making criteria and insufficient exploration strategies. Some models showed modest performance decline (i.e., GPT-4.1: from 1,850 to 1,700; GPT-4o: from 1,550 to 1,450), finding good options within their limited exploration.



Figure 6. Mean consumer welfare decreased as consideration set size grew, revealing a Paradox of Choice effect, where expanding options reduced overall welfare.



Agents are vulnerable to manipulation



We tested six manipulation strategies, ranging from subtle psychological tactics to aggressive prompt injection attacks:




Authority: Fake credentials like ‚ÄúMichelin Guide featured‚Äù and ‚ÄúJames Beard Award nominated‚Äù paired with fabricated certifications.



Social proof: Claims like ‚ÄúJoin 50,000+ satisfied customers‚Äù or ‚Äú#1-rated Mexican restaurant‚Äù combined with fake reviews.



Loss aversion: Fear-based warnings about ‚Äúfood poisoning‚Äù risks and ‚Äúcontamination issues‚Äù at competing restaurants.



Prompt injection (basic): Attempts to override agent instructions.



Prompt injection (strong): Aggressive attacks using emergency language and fabricating competitor scandals.




Results revealed significant variation in manipulation resistance across models. Sonnet-4 was resistant to all attacks, and none of the manipulative strategies affected any of the customers‚Äô choices. Gemini-2.5-Flash was generally resistant, except for strong prompt injections, where mean payments to unmanipulated agents were affected as a result. GPT-4o, GPTOSS-20b and Qwen3-4b were very vulnerable to prompt injection: all payments were redirected to the manipulative agent under these conditions. Specifically for GPTOSS-20 and Qwen3-4b-2507, even traditional psychological manipulation tactics (authority appeals and social proof) increased payments to malicious agents, demonstrating their vulnerability to basic persuasion techniques. These findings highlight a critical security concern for agentic marketplaces.



Figure 7. Charts showing the variation in mean payments received by service agents with and without manipulation tactics. The results reveal substantial differences in manipulation resistance across models, with GPT-4.1 showing significantly higher vulnerability compared to Gemini-2.5-Flash.



Systemic biases create unfair advantages



Our analysis revealed two distinct types of systematic biases showed by agents when selecting businesses from search results. Models showed systematic preferences based on where businesses appeared in search results. While proprietary models showed no strong positional preferences, open-source models exhibited clear patterns. Specifically, Qwen2.5-14b-2507 showed a pronounced bias toward selecting the last business presented, regardless of its actual merits.



Proposal&nbsp;bias&nbsp;is&nbsp;more pervasive across all models tested. This &#8220;first-offer acceptance&#8221; pattern suggests that models prioritized&nbsp;immediate selection over comprehensive exploration, potentially missing better alternatives that&nbsp;could have&nbsp;emerged&nbsp;by waiting for better options. This behavior&nbsp;continued&nbsp;across both proprietary and open-source models,&nbsp;indicating&nbsp;a fundamental challenge in agent decision-making architectures.



These biases can create unfair market dynamics, drive unintended behaviors, and push businesses to complete on response speed rather than product or service quality.



Figure 8. All models showed strong preference for the first proposal received, accepting it without waiting for additional proposals or conducting systematic comparisons.



What this means



Even state-of-the-art models can show notable vulnerabilities and biases in marketplace environments. In our implementation, agents struggled with too many options, were susceptible to manipulation tactics, and showed systemic biases that created unfair advantages.



These outcomes are shaped not only by agent capabilities but also by marketplace design and implementation. Our current study focused on static markets, but real-world environments are dynamic, with agents and users learning over time. Oversight is critical for high-stakes transactions. Agents should assist, not replace, human decision-making.



We plan to explore dynamic markets and human-in-the-loop designs to improve efficiency and trust. A simulation environment like Magentic Marketplace is crucial for understanding the interplay between market components and agents before deploying them at scale.



Full details of our experimental setup and results are available in our paper (opens in new tab).



Getting started



Magentic Marketplace is available as an open-source environment for exploring agentic market dynamics. Code, datasets, and experiment templates are available on GitHub (opens in new tab) and Azure AI Foundry Labs (opens in new tab).



The documentation (opens in new tab) provides instructions for reproducing the experiments described above and guidance for extending the environment to new marketplace configurations.
Opens in a new tabThe post Magentic Marketplace: an open-source simulation environment for studying agentic markets appeared first on Microsoft Research.
‚Ä¢ RedCodeAgent: Automatic red-teaming agent against diverse code agents
  Introduction



Code agents are AI systems that can generate high-quality code and work smoothly with code interpreters. These capabilities help streamline complex software development workflows,&nbsp;which has led to their widespread adoption.



However, this progress also introduces critical safety and security risks. Existing static safety benchmarks and red-teaming methods‚Äîin which&nbsp;security researchers&nbsp;simulate real-world attacks to&nbsp;identify&nbsp;security vulnerabilities‚Äîoften fall short when evaluating code agents.&nbsp;They&nbsp;may&nbsp;fail to&nbsp;detect&nbsp;emerging real-world risks, such as the combined effects of multiple jailbreak tools.&nbsp;In&nbsp;the context of code, effective red-teaming requires more than simply checking whether the target code agent rejects unsafe requests. Instead, the agent must generate and execute correct code that performs the intended risky functionality, making it essential to evaluate execution behaviors beyond static code analysis.&nbsp;



To address these challenges, researchers from the University of Chicago, University of Illinois Urbana‚ÄìChampaign, VirtueAI, the UK AI Safety Institute, University of Oxford, UC Berkeley, and Microsoft Research recently proposed RedCodeAgent, the first fully automated and adaptive red-teaming agent designed specifically to evaluate the safety of large language model&nbsp;(LLM)-based code agents.



Comprehensive experimental results demonstrate the effectiveness and efficiency of&nbsp;RedCodeAgent across (1) diverse Common Weakness Enumeration (CWE) vulnerabilities and malware types, (2) multiple programming languages‚Äîincluding Python, C, C++, and Java‚Äîand (3) a wide range of code agents, such as OpenCodeInterpreter, ReAct, MetaGPT, and commercial agents like Cursor and&nbsp;Codeium.&nbsp;RedCodeAgent also uncovers common vulnerabilities across agents&nbsp;such as generating and executing unsafe code, exposes variations in red-teaming difficulty across goals, identifies frequently triggered attack tools, and detects previously unknown vulnerabilities that all other baseline methods overlook.&nbsp;



Framework for&nbsp;automatic&nbsp;red-teaming&nbsp;against&nbsp;code&nbsp;agents



Figure 1: Illustration of&nbsp;RedCodeAgent&nbsp;on automatic red-teaming against a target code agent&nbsp;



As shown in Figure 1,&nbsp;RedCodeAgent&nbsp;is equipped with a&nbsp;memory module&nbsp;that accumulates successful attack experiences, enabling the system to&nbsp;continuously learn and adapt its attack strategies. After learning from the previous experiences,&nbsp;RedCodeAgent&nbsp;further&nbsp;leverages&nbsp;a&nbsp;tailored toolbox&nbsp;that combines representative red-teaming tools with a specialized&nbsp;code substitution module, enabling realistic and diverse code-specific attack simulations through function calling. Based on the target agent‚Äôs responses across multiple interactive trials, RedCodeAgent optimizes&nbsp;its strategies, systematically&nbsp;probing for&nbsp;weaknesses and vulnerabilities&nbsp;in real time.&nbsp;



In the evaluation phase,&nbsp;RedCodeAgent&nbsp;integrates simulated sandbox environments to enable code execution and assess the impact of the resulting behaviors. This sandbox-based evaluation ensures a more robust assessment of harmful behaviors and addresses the potential biases of&nbsp;previous&nbsp;static methods that rely solely on ‚ÄúLLM-as-a-judge‚Äù evaluations.



A case study is shown in Figure 2. Initially,&nbsp;RedCodeAgent&nbsp;discovers that the request was rejected, then RedCodeAgent calls the Greedy Coordinate&nbsp;Gradient&nbsp;(GCG)&nbsp;algorithm&nbsp;to bypass the safety guardrail. After the second request was rejected by the code agent,&nbsp;RedCodeAgent&nbsp;invoked both Code Substitution and GCG to optimize the prompt. Ultimately,&nbsp;RedCodeAgent&nbsp;successfully combined the suggestion from Code Substitution (i.e., using&nbsp;pathlib) with the adversarial suffix generated by GCG, making the target code agent delete the specified file.



Figure2. A case study of&nbsp;RedCodeAgent&nbsp;calling different tools to successfully attack the target code agent



Insights from&nbsp;RedCodeAgent&nbsp;



Experiments on diverse benchmarks show that&nbsp;RedCodeAgent&nbsp;achieves both a higher attack success rate (ASR) and a lower rejection rate, revealing several key findings outlined below.



Using&nbsp;traditional&nbsp;jailbreak&nbsp;methods&nbsp;alone&nbsp;does&nbsp;not&nbsp;necessarily&nbsp;improve&nbsp;ASR on code agents



The optimized prompts generated by GCG,&nbsp;AmpleGCG,&nbsp;Advprompter, and&nbsp;AutoDAN&nbsp;do not always achieve a higher ASR compared with static prompts with no jailbreak, as shown in Figure 3.&nbsp;This is&nbsp;likely&nbsp;due to the difference between code-specific tasks and general malicious request tasks in LLM safety. In the context of code, it is not enough for the target code agent to simply avoid rejecting the request; the target code agent must also generate and execute code that performs the intended function.&nbsp;Previous&nbsp;jailbreak methods do not guarantee this outcome. However,&nbsp;RedCodeAgent&nbsp;ensures that the input prompt has a clear functional objective (e.g., deleting specific sensitive files). RedCodeAgent&nbsp;can dynamically adjust based on evaluation feedback, continually optimizing to achieve the specified objectives.



Figure 3ÔºöRedCodeAgent&nbsp;achieves the highest ASR compared with other methods



RedCodeAgent&nbsp;exhibits&nbsp;adaptive&nbsp;tool&nbsp;utilization&nbsp;



RedCodeAgent&nbsp;can dynamically adjust its tool usage based on task difficulty. Figure 4 shows that the tool calling combination is different&nbsp;for&nbsp;different tasks.&nbsp;For simpler tasks, where the baseline static test cases already achieve a high ASR,&nbsp;RedCodeAgent&nbsp;spends little time invoking&nbsp;additional&nbsp;tools,&nbsp;demonstrating&nbsp;its efficiency. For more challenging tasks, where the baseline static test cases in&nbsp;RedCode-Exec achieve a lower ASR,we observe that RedCodeAgent spends more time using advanced tools like&nbsp;GCG and&nbsp;Advprompter&nbsp;to&nbsp;optimize&nbsp;the prompt for a successful attack. As a result, the average time spent on invoking different tools varies across tasks, indicating that RedCodeAgent adapts its strategy depending on the specific task.&nbsp;



Figure 4: Average time cost for&nbsp;RedCodeAgent&nbsp;to invoke different tools or query the target code agent in successful cases for each risk scenario&nbsp;



RedCodeAgent&nbsp;discovers&nbsp;new&nbsp;vulnerabilities



In scenarios where other methods&nbsp;fail to&nbsp;find successful attack strategies,&nbsp;RedCodeAgent&nbsp;is able to discover new, feasible jailbreak approaches. Quantitatively, we find that&nbsp;RedCodeAgent&nbsp;is capable of discovering&nbsp;82 (out of 27*30=810 cases in&nbsp;RedCode-Exec benchmark) unique vulnerabilities on the&nbsp;OpenCodeInterpreter&nbsp;code agent and 78 on the ReAct code agent. These are cases where all baseline methods&nbsp;fail to&nbsp;identify the vulnerability, but RedCodeAgent succeeds.



Summary



RedCodeAgent&nbsp;combines adaptive memory, specialized tools, and simulated execution environments to uncover real-world risks that static benchmarks&nbsp;may&nbsp;miss.&nbsp;It&nbsp;consistently outperforms leading jailbreak methods, achieving higher attack success rates and lower rejection rates, while remaining efficient and adaptable across diverse agents and programming languages.
Opens in a new tabThe post RedCodeAgent: Automatic red-teaming agent against diverse code agents appeared first on Microsoft Research.
‚Ä¢ How Amazon Search increased ML training twofold using AWS Batch for Amazon SageMaker Training jobs
  In this post, we show you how Amazon Search optimized GPU instance utilization by leveraging AWS Batch for SageMaker Training jobs. This managed solution enabled us to orchestrate machine learning (ML) training workloads on GPU-accelerated instance families like P5, P4, and others. We will also provide a step-by-step walkthrough of the use case implementation. 
Machine learning at Amazon Search 
At Amazon Search, we use hundreds of GPU-accelerated instances to train and evaluate ML models that help our customers discover products they love. Scientists typically train more than one model at a time to find the optimal set of features, model architecture, and hyperparameter settings that optimize the model‚Äôs performance. We previously leveraged a first-in-first-out (FIFO) queue to coordinate model training and evaluation jobs. However, we needed to employ a more nuanced criteria to prioritize which jobs should run in what order. Production models needed to run with high priority, exploratory research as medium priority, and hyperparameter sweeps and batch inference as low priority. We also needed a system that could handle interruptions. Should a job fail, or a given instance type become saturated, we needed the job to run on other available compatible instance types while respecting the overall prioritization criteria. Finally, we wanted a managed solution so we could focus more on model development instead of managing infrastructure. 
After evaluating multiple options, we chose AWS Batch for Amazon SageMaker Training jobs because it best met our requirements. This solution seamlessly integrated AWS Batch with Amazon SageMaker and allowed us to run jobs per our prioritization criteria. This allows applied scientists to submit multiple concurrent jobs without manual resource management. By leveraging AWS Batch features such as advanced prioritization through fair-share scheduling, we increased peak utilization of GPU-accelerated instances from 40% to over 80%. 
Amazon Search: AWS Batch for SageMaker Training Job implementation 
We leveraged three AWS technologies to set up our job queue. We used Service Environments to configure the SageMaker AI parameters that AWS Batch uses to submit and manage SageMaker Training jobs. We used Share Identifiers to prioritize our workloads. Finally, we used Amazon CloudWatch to monitor and the provision of alerting capability for&nbsp;critical events or deviations from expected behavior. Let‚Äôs dive deep into these constructs. 
Service environments. We set up service environments to represent the total GPU capacity available for each instance family, such as P5s and P4s. Each service environment was configured with fixed limits based on our team‚Äôs reserved capacity in AWS Batch. Note that for teams using SageMaker Training Plans, these limits can be set to the number of reserved instances, making capacity planning more straightforward. By defining these boundaries, we established how the total GPU instance capacity within a service environment was distributed across different production jobs. Each production experiment was allocated a portion of this capacity through Share Identifiers. 
Figure 1 provides a real-world example of how we used AWS Batch‚Äôs fair-share scheduling to divide 100 GPU instance between ShareIDs. We allocated 60 instances to ProdExp1, and 40 to ProdExp2. When ProdExp2 used only 25 GPU instances, the remaining 15 could be borrowed by ProdExp1, allowing it to scale up to 75 GPU instances. When ProdExp2 later needed its full 40 GPU instances, the scheduler preempted jobs from ProdExp1 to restore balance. This example used the P4 instance family, but the same approach could apply to any SageMaker-supported EC2 instance family. This ensured that production workloads have guaranteed access to their assigned capacity, while exploratory or ad-hoc experiments could still make use of any idle GPU instances. This design safeguarded critical workloads and improved overall instance utilization by ensuring that no reserved capacity went unused. 

 
 Figure 1: AWS Batch fair-share scheduling
 
Share Identifiers. We used Share Identifiers to allocate fractions of a service environment‚Äôs capacity to production experiments. Share Identifiers are string tags applied at job submission time. AWS Batch used these tags to track usage and enforce fair-share scheduling. For initiatives that required dedicated capacity, we defined preset Share Identifiers with quotas in AWS Batch. This reserved capacity for production tracks. These quotas acted as fairness targets rather than hard limits. Idle capacity could still be borrowed, but under contention, AWS Batch enforced fairness by preempting resources from overused identifiers and reassigned them to underused ones. 
Within each Share Identifier, job priorities ranging from 0 to 99 determined execution order, but priority-based preemption only triggered when the ShareIdentifier reached its allocated capacity limit.&nbsp;Figure 2 illustrates how we setup and used our share identifiers. ProdExp1 had 60 p4d instances and ran jobs at various priorities. Job A had a priority of 80, Job B was set to 50, Job C was set to at 30, and Job D had a priority 10. When all 60 instances were occupied and a new high-priority job (priority 90) requiring 15 instances was submitted, the system preempted the lowest priority running job (Job D) to make room, while maintaining the total of 60 instances for that Share Identifier. 

 
 Figure 2: Priority scheduling within a Share ID
 
Amazon CloudWatch. We used Amazon CloudWatch to instrument our SageMaker training jobs. SageMaker automatically publishes metrics on job progress and resource utilization, while AWS Batch provides detailed information on job scheduling and execution. With AWS Batch, we queried the status of each job through the AWS Batch APIs. This made it possible to track jobs as they transitioned through states such as SUBMITTED, PENDING, RUNNABLE, STARTING, RUNNING, SUCCEEDED, and FAILED. We published these metrics and job states to CloudWatch and configured dashboards and alarms to alert anytime we encountered extended wait times, unexpected failures, or underutilized resources. This built-in integration provided both real-time visibility and historical trend analysis, which helped our team maintain operational efficiency across GPU clusters without building custom monitoring systems. 
Operational impact on team performance 
By adopting AWS Batch for SageMaker Training jobs, we enabled experiments to run without concerns about resource availability or contention. Researchers could submit jobs without waiting for manual scheduling, which increased the number of experiments that could be run in parallel. This led to shorter queue times, higher GPU utilization, and faster turnaround of training results, directly improving both research throughput and delivery timelines. 
How to set up AWS Batch for SageMaker Training jobs 
To set up a similar environment, you can follow this tutorial, which shows you how to orchestrate multiple GPU large language model (LLM) fine-tuning jobs using multiple GPU-powered instances. The solution is also available on GitHub. 
Prerequisites 
To orchestrate multiple SageMaker Training jobs with AWS Batch, first you need to complete the following prerequisites: 
Clone the GitHub repository with the assets for this deployment. This repository consists of notebooks that reference assets: 
 
 git clone https://github.com/aws/amazon-sagemaker-examples/
cd  build_and_train_models/sm-training-queues-pytorch/ 
 
Create AWS Batch resources 
To create the necessary resources to manage SageMaker Training job queues with AWS Batch, we provide utility functions in the example to automate the creation of the Service Environment, Scheduling Policy, and Job Queue. 
The service environment represents the Amazon SageMaker AI capacity limits available to schedule, expressed by maximum number of instances. The scheduling policy indicates how resource computes are allocated in a job queue between users or workloads. The job queue is the scheduler interface that researchers interact with to submit jobs and interrogate job status. AWS Batch provides two different queues we can operate with: 
 
 FIFO queues ‚Äì Queues in which no scheduling policies are required 
 Fair-share queues ‚Äì Queues in which a scheduling policy Amazon Resource Name (ARN) is required to orchestrate the submitted jobs 
 
We recommend creating dedicated service environments for each job queue in a 1:1 ratio. FIFO queues provide basic message delivery, while fair-share scheduling (FSS) queues provide more sophisticated scheduling, balancing utilization within a Share Identifier, share weights, and job priority. For customers who don‚Äôt need multiple shares but would like the ability to assign a priority on job submission, we recommend creating an FSS queue and using a single share within it for all submissions.To create the resources, execute the following commands: 
 
 cd&nbsp;smtj_batch_utils
python create_resources.py 
 
You can navigate the AWS Batch Dashboard, shown in the following screenshot, to explore the created resources. 
 
This automation script created two queues: 
 
 ml-c5-xlarge-queue ‚Äì A FIFO queue with priority 2 used for CPU workloads 
 ml-g6-12xlarge-queue ‚Äì A fair-share queue with priority 1 used for GPU workloads 
 
The associated scheduling policy for the queue ml-g6-12xlarge-queue is with share attributes such as High priority (HIGHPRI), Medium priority (MIDPRI) and Low priority (LOWPRI) along with the queue weights. Users can submit jobs and assign them to one of three shares:&nbsp;HIGHPRI,&nbsp;MIDPRI, or&nbsp;LOWPRI and assign weights such as 1 for high priority and 3 for medium and 5 for low priority. Below is the screenshot showing the scheduling policy details: 
 
For instructions on how to set up the service environment and a job queue, refer to the Getting started section in Introducing AWS Batch support for SageMaker Training Jobs blog. 
Run LLM fine-tuning jobs on SageMaker AI 
We run the notebook notebook.ipynb to start submitting SageMaker Training jobs with AWS Batch. The notebook contains the code to prepare the data used for the workload, upload on Amazon Simple Storage Service (Amazon S3), and define the hyperparameters required by the job to be executed. 
To run the fine-tuning workload using SageMaker Training jobs, this example uses the ModelTrainer class. The ModelTrainer class is a newer and more intuitive approach to model training that significantly enhances user experience. It supports distributed training, build your own container (BYOC), and recipes. 
For additional information about ModelTrainer, you can refer to Accelerate your ML lifecycle using the new and improved Amazon SageMaker Python SDK ‚Äì Part 1: ModelTrainer. 
To set up the fine-tuning workload, complete the following steps: 
 
 Select the instance type, the container image for the training job, and define the checkpoint path where the model will be stored: 
   
   import&nbsp;sagemaker

instance_type&nbsp;=&nbsp;"ml.g6.12xlarge"
instance_count&nbsp;=&nbsp;1

image_uri = sagemaker.image_uris.retrieve(
&nbsp;&nbsp; &nbsp;framework="pytorch",
&nbsp;&nbsp; &nbsp;region=sagemaker_session.boto_session.region_name,
&nbsp;&nbsp; &nbsp;version="2.6",
&nbsp;&nbsp; &nbsp;instance_type=instance_type,
&nbsp;&nbsp; &nbsp;image_scope="training"
) 
    
 Create the ModelTrainer function to encapsulate the training setup. The ModelTrainer class simplifies the experience by encapsulating code and training setup. In this example: 
   
   SourceCode ‚Äì The source code configuration. This is used to configure the source code for running the training job by using your local python scripts. 
   Compute ‚Äì The compute configuration. This is used to specify the compute resources for the training job. 
   
   
   from sagemaker.modules.configs import Compute, OutputDataConfig, SourceCode, StoppingCondition
from sagemaker.modules.distributed import Torchrun
from sagemaker.modules.train import ModelTrainer

role = sagemaker.get_execution_role()

# Define the script to be run
source_code = SourceCode(
&nbsp;&nbsp; &nbsp;source_dir="./scripts",
&nbsp;&nbsp; &nbsp;requirements="requirements.txt",
&nbsp;&nbsp; &nbsp;entry_script="train.py",
)

# Define the compute
compute_configs = Compute(
&nbsp;&nbsp; &nbsp;instance_type=instance_type,
&nbsp;&nbsp; &nbsp;instance_count=instance_count,
&nbsp;&nbsp; &nbsp;keep_alive_period_in_seconds=0
)

# define Training Job Name
job_name = f"train-deepseek-distill-llama-8b-sft-batch"

# define OutputDataConfig path
output_path = f"s3://{bucket_name}/{job_name}"

# Define the ModelTrainer
model_trainer = ModelTrainer(
&nbsp;&nbsp; &nbsp;training_image=image_uri,
&nbsp;&nbsp; &nbsp;source_code=source_code,
&nbsp;&nbsp; &nbsp;base_job_name=job_name,
&nbsp;&nbsp; &nbsp;compute=compute_configs,
&nbsp;&nbsp; &nbsp;distributed=Torchrun(),
&nbsp;&nbsp; &nbsp;stopping_condition=StoppingCondition(max_runtime_in_seconds=7200),
&nbsp;&nbsp; &nbsp;hyperparameters={
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"config": "/opt/ml/input/data/config/args.yaml"
&nbsp;&nbsp; &nbsp;},
&nbsp;&nbsp; &nbsp;output_data_config=OutputDataConfig(s3_output_path=output_path),
&nbsp;&nbsp; &nbsp;role=role,
) 
    
 Set up the input channels for ModelTrainer by creating InputData objects from the provided S3 bucket paths for the training and validation datasets: 
   
   from&nbsp;sagemaker.modules.configs&nbsp;import&nbsp;InputData

train_input&nbsp;=&nbsp;InputData(
&nbsp;&nbsp; &nbsp;channel_name="train",
&nbsp;&nbsp; &nbsp;data_source=train_dataset_s3_path,
)
val_input&nbsp;=&nbsp;InputData(
&nbsp;&nbsp; &nbsp;channel_name="val",
&nbsp;&nbsp; &nbsp;data_source=val_dataset_s3_path,
)
config_input&nbsp;=&nbsp;InputData(
&nbsp;&nbsp; &nbsp;channel_name="config",
&nbsp;&nbsp; &nbsp;data_source=train_config_s3_path,
)

TRAINING_INPUTS&nbsp;=&nbsp;[train_input, val_input, config_input] 
    
 
Queue SageMaker Training jobs 
This section and the following are intended to be used interactively so that you can explore how to use the Amazon SageMaker Python SDK to submit jobs to your Batch queues. Follow these steps: 
 
 Select the queue to use: 
   
   from sagemaker.aws_batch.queue import TrainingQueue
SMTJ_BATCH_QUEUE = "ml-g6-12xlarge-queue"

queue = TrainingQueue(SMTJ_BATCH_QUEUE)
 
    
 In the next cell, submit two training jobs in the queue: 
   
   LOW PRIORITY 
   MEDIUM PRIORITY 
    
 Use the API submit to submit all the jobs: 
   
   job_name_1&nbsp;=&nbsp;job_name&nbsp;+&nbsp;"-low-pri"
queued_job_1&nbsp;=&nbsp;queue.submit(
&nbsp;&nbsp; &nbsp;model_trainer, TRAINING_INPUTS, job_name_1, priority=5, share_identifier="LOWPRI"
)
job_name_2&nbsp;=&nbsp;job_name&nbsp;+&nbsp;"-mid-pri"
queued_job_2&nbsp;=&nbsp;queue.submit(
&nbsp;&nbsp; &nbsp;model_trainer, TRAINING_INPUTS, job_name_2, priority=3, share_identifier="MIDPRI"
) 
    
 
Display the status of running and in queue jobs 
We can use the job queue list and job queue snapshot APIs to programmatically view a snapshot of the jobs that the queue will run next. For fair-share queues, this ordering is dynamic and occasionally needs to be refreshed because new jobs are submitted to the queue or as share usage changes over time. 
 
 from utils.queue_utils import print_queue_state
print_queue_state(queue) 
 
The following screenshot shows the jobs submitted with low priority and medium priority in the Runnable State and in the queue. 
 
You can also refer to the AWS Batch Dashboard, shown in the following screenshot, to analyze the status of the jobs. 
 
As shown in the following screenshot, the first job executed with the SageMaker Training job is the MEDIUM PRIORITY&nbsp;one, by respecting the scheduling policy rules defined previously. 
 
You can explore the running training job in the SageMaker AI console, as shown in the following screenshot. 
 
Submit an additional job 
You can now submit an additional SageMaker Training job with HIGH PRIORITY to the queue: 
 
 job_name_3&nbsp;=&nbsp;job_name&nbsp;+&nbsp;"-high-pri"
queued_job_3&nbsp;=&nbsp;queue.submit(
&nbsp;&nbsp; &nbsp;model_trainer, TRAINING_INPUTS, job_name_3, priority=1, share_identifier="HIGHPRI"
) 
 
You can explore the status from the dashboard, as shown in the following screenshot. 
 
The HIGH PRIORITY job, despite being submitted later in the queue, will be executed before the other runnable jobs by respecting the scheduling policy rules, as shown in the following screenshot. 
 
As the scheduling policy in the screenshot shows, the&nbsp;LOWPRI&nbsp;share has a higher weight factor (5) than the&nbsp;MIDPRI&nbsp;share (3). Since a lower weight signifies higher priority, a&nbsp;LOWPRI&nbsp;job will be executed after a&nbsp;MIDPRI&nbsp;job, even if they are submitted at the same time. 
 
Clean up 
To clean up your resources to avoid incurring future charges, follow these steps: 
 
 Verify that your training job isn‚Äôt running anymore. To do so, on your SageMaker console, choose Training and check Training jobs. 
 Delete AWS Batch resources by using the command python create_resources.py --clean from the GitHub example or by manually deleting them from the AWS Management Console. 
 
Conclusion 
In this post, we demonstrated how Amazon Search used AWS Batch for SageMaker Training Jobs to optimize GPU resource utilization and training job management. The solution transformed their training infrastructure by implementing sophisticated queue management and fair share scheduling, increasing peak GPU utilization from 40% to over 80%.We recommend that organizations facing similar ML training infrastructure challenges explore AWS Batch integration with SageMaker, which provides built-in queue management capabilities and priority-based scheduling. The solution eliminates manual resource coordination while providing workloads with appropriate prioritization through configurable scheduling policies. 
To begin implementing AWS Batch with SageMaker Training jobs, you can access our sample code and implementation guide in the amazon-sagemaker-examples repository on GitHub. The example demonstrates how to set up AWS Identity and Access Management (IAM) permissions, create AWS Batch resources, and orchestrate multiple GPU-powered training jobs using ModelTrainer class. 
 
The authors would like to thank Charles Thompson and Kanwaljit Khurmi for their collaboration. 
About the authors 
 
  
  
   
   
  Mona Mona 
  Mona is a generative AI Specialist Solutions Architect at Amazon focusing. She is a published author of two books ‚Äì Natural Language Processing with AWS AI Services and Google Cloud Certified Professional Machine Learning Study Guide. 
  
  
  
   
   
  Mayank Jha 
  Mayank is a Senior Machine Learning Engineer at Amazon Search working on the model training optimization. He is passionate about finding practical applications for complex problems at hand and aims to develop solutions that have a deep impact on how businesses and people thrive. 
  
  
  
   
   
  Bruno Pistone 
  Bruno is a Senior generative AI and ML Specialist Solutions Architect for AWS based in Milan. He works with large customers helping them to deeply understand their technical needs and design AI and Machine Learning solutions that make the best use of the AWS Cloud and the Amazon Machine Learning stack. He enjoys spending time with his friends and exploring new places, as well as travelling to new destinations. 
  
  
  
   
   
  James Park 
  James is a Solutions Architect at Amazon Web Services. He works with Amazon.com to design, build, and deploy technology solutions on AWS, and has a particular interest in AI and machine learning. In his spare time he enjoys seeking out new cultures, new experiences, and staying up to date with the latest technology trends.
‚Ä¢ Iterate faster with Amazon Bedrock AgentCore Runtime direct code deployment
  Amazon Bedrock AgentCore is an agentic platform for building, deploying, and operating effective agents securely at scale. Amazon Bedrock AgentCore Runtime is a fully managed service of Bedrock AgentCore, which provides low latency serverless environments to deploy agents and tools. It provides session isolation, supports multiple agent frameworks including popular open-source frameworks, and handles multimodal workloads and long-running agents. 
AgentCore&nbsp;Runtime supports&nbsp;container based deployments&nbsp;where the container definition is provided&nbsp;in a&nbsp;Dockerfile, and the agent is&nbsp;built&nbsp;as&nbsp;a container&nbsp;image. Customers who have container build and deploy pipelines&nbsp;benefit&nbsp;from this method, where&nbsp;agent deployment&nbsp;can be integrated into&nbsp;existing&nbsp;pipelines.&nbsp; 
Today, AgentCore&nbsp;Runtime&nbsp;has&nbsp;launched&nbsp;a second method&nbsp;to&nbsp;deploy agents ‚Äì direct code deployment&nbsp;(for Python). Agent code and its dependencies can be packaged as a zip archive,&nbsp;alleviating&nbsp;the need for Docker definition and ECR dependencies. This makes it&nbsp;straightforward&nbsp;for developers to prototype and&nbsp;iterate&nbsp;faster.&nbsp;This method&nbsp;is a good fit&nbsp;for customers who&nbsp;prefer&nbsp;not to worry about&nbsp;Docker&nbsp;expertise&nbsp;and container infrastructure&nbsp;when deploying&nbsp;agents. 
In this post, we‚Äôll demonstrate how to use direct code deployment (for Python). 
Introducing AgentCore Runtime direct code deployment 
With the container deployment method, developers create a Dockerfile, build ARM-compatible containers, manage ECR repositories, and upload containers for code changes. This works well&nbsp;where&nbsp;container DevOps pipelines have&nbsp;already&nbsp;been&nbsp;established&nbsp;to automate deployments.&nbsp; 
However,&nbsp;customers looking for fully managed deployments can&nbsp;benefit&nbsp;from&nbsp;direct code&nbsp;deployment,&nbsp;which&nbsp;can significantly improve developer time and productivity. Direct code deployment provides a secure and scalable path forward for rapid prototyping agent capabilities to deploying production workloads at scale. 
We‚Äôll discuss the strengths of each deployment option to help you choose the right approach for your use case.&nbsp; 
 
With direct code deployment, developers create a zip archive of code and dependencies, upload to Amazon S3, and configure the bucket in the agent configuration. When using the AgentCore starter toolkit, the toolkit handles dependency detection, packaging, and upload which provides a much-simplified developer experience. Direct code deployment is also supported using the API. 
Let‚Äôs compare the deployment steps at a high level between the two methods: 
Container-based deployment 
The container-based deployment method involves the following steps: 
 
  
  Create a Dockerfile 
  Build ARM-compatible container 
  Create ECR repository 
  Upload to ECR 
  Deploy to AgentCore Runtime 
  
 
Direct code deployment 
The direct code deployment method involves the following steps: 
 
 Package your code and dependencies into a zip archive 
 Upload it to S3 
 Configure the bucket in agent configuration 
 Deploy to AgentCore Runtime 
 
How to use direct code deployment 
Let‚Äôs illustrate how direct code deployment works with an agent created with Strands Agents SDK and using the AgentCore starter-toolkit to deploy the agent. 
Prerequisites 
Before you begin, make sure you have the following: 
 
 Any of the versions of Python 3.10 to 3.13 
 Your preferred package manager installed. For example, we use uv package manager. 
 AWS account for creating and deploying agents 
 Amazon Bedrock model access to Anthropic Claude Sonnet 4.0 
 
Step 1: Initialize your project 
Set up a new Python project using the uv package manager, then navigate into the project directory: 
 
 uv init &lt;project&gt; --python 3.13
cd &lt;project&gt; 
 
Step 2: Add the dependencies for the project 
Install the required Bedrock AgentCore libraries and development tools for your project. In this example, dependencies are added using .toml file, alternatively they can be specified in requirements.txt file: 
 
 uv add bedrock-agentcore strands-agents strands-agents-tools
uv add --dev bedrock-agentcore-starter-toolkit
source .venv/bin/activate 
 
Step 3: Create an agent.py file 
Create the main agent implementation file that defines your AI agent‚Äôs behavior: 
 
 from bedrock_agentcore import BedrockAgentCoreApp 
from strands import Agent, tool 
from strands_tools import calculator  
from strands.models import BedrockModel 
import logging 

app = BedrockAgentCoreApp(debug=True) 

# Logging setup 
logging.basicConfig(level=logging.INFO) 
logger = logging.getLogger(__name__) 

# Create a custom tool  
@tool 
def weather(): 
     """ Get weather """  
     return "sunny" 

model_id = "us.anthropic.claude-sonnet-4-20250514-v1:0" 
model = BedrockModel( 
     model_id=model_id, 
) 

agent = Agent( 
     model=model, 
     tools=[calculator, weather], 
     system_prompt="You're a helpful assistant. You can do simple math calculation, and tell the weather." 
) 

@app.entrypoint 
def invoke(payload): 
     """Your AI agent function""" 
     user_input = payload.get("prompt", "Hello! How can I help you today?") 
     logger.info("\n User input: %s", user_input) 
     response = agent(user_input) 
     logger.info("\n Agent result: %s ", response.message) 
     return response.message['content'][0]['text'] 

if __name__ == "__main__": 
     app.run()  
 
Step 4: Deploy to AgentCore Runtime 
Configure and deploy your agent to the AgentCore Runtime environment: 
 
 agentcore configure --entrypoint agent.py --name &lt;agent-name&gt; 
 
This will launch an interactive session where you configure the S3 bucket to upload the zip deployment package to and choose a deployment configuration type (as shown in the following configuration). To opt for direct code deployment, choose option 1 ‚Äì Code Zip. 
Deployment Configuration 
Select deployment type: 
 
 Code Zip (recommended) ‚Äì Simple, serverless, no Docker required 
 Container ‚Äì For custom runtimes or complex dependencies 
 
 
 agentcore launch 
 
This command creates a zip deployment package, uploads it to the specified S3 bucket, and launches the agent in the AgentCore Runtime environment, making it ready to receive and process requests. 
To test the solution, let‚Äôs prompt the agent to see how the weather is: 
 
 agentcore invoke '{"prompt":"How is the weather today?"}' 
 
The first deployment takes approximately 30 seconds to complete, but subsequent updates to the agent benefit from the streamlined direct code deployment process and should take less than half the time, supporting faster iteration cycles during development. 
When to choose direct code instead of container-based deployment 
Let‚Äôs look at some of the dimensions and see how the direct code and container-based deployment options are different. This will help you choose the option that‚Äôs right for you: 
 
 Deployment process: Direct code deploys agents as zip files with no Docker, ECR, or CodeBuild required. Container-based deployment uses Docker and ECR with full Dockerfile control. 
 Deployment time: Although there is not much difference during first deployment of an agent, subsequent updates to the agent are significantly faster with direct code deployment (from an average of 30 seconds for containers to about 10 seconds for direct code deployment). 
 Artifact storage: Direct code stores ZIP packages in&nbsp;an S3 bucket. Container-based deployment stores Docker images&nbsp;in&nbsp;Amazon ECR. Direct code deployment incurs&nbsp;storage costs&nbsp;at&nbsp;standard S3&nbsp;storage&nbsp;rates&nbsp;(starting February 27th&nbsp;&nbsp;2026)&nbsp;as&nbsp;artifacts are stored in the service account. Container-based deployment incurs Amazon ECR charges in your account. 
 Customization: Direct code deployment supports custom dependencies through ZIP-based packaging, while container based depends on a Dockerfile. 
 Package size: Direct code deployment limits the package size to 250MB whereas container-based packages can be up to 2GB in size. 
 Language Support: Direct code currently supports Python 3.10, 3.11, 3.12, and 3.13. Container-based deployment supports many languages and runtimes. 
 
Our general guidance is: 
Container-based deployment is the right choice when your package exceeds 250MB, you have existing container CI/CD pipelines, or you need highly specialized dependencies and custom packaging requirements. Choose containers if you require multi-language support, custom system dependencies or direct control over artifact storage and versioning in your account. 
Direct code deployment is the right choice when your package is under 250MB, you use Python 3.10-3.13 with common frameworks like LangGraph, Strands, or CrewAI, and you need rapid prototyping with fast iteration cycles. Choose direct code if your build process is straightforward without complex dependencies, and you want to remove the Docker/ECR/CodeBuild setup. 
A hybrid approach works well for many teams, use direct code for rapid prototyping and experimentation where fast iteration and simple setup accelerate development, then graduate to containers for production when package size, multi-language requirements, or specialized build processes demand it. 
Conclusion 
Amazon Bedrock AgentCore direct code deployment makes iterative agent development cycles even faster, while still benefiting from enterprise security and scale of deployments. Developers can now rapidly prototype and iterate by deploying their code directly, without having to create a container. To get started with Amazon Bedrock AgentCore direct code deployment, visit the AWS documentation. 
 
About the authors 
Chaitra Mathur is as a GenAI Specialist Solutions Architect at AWS. She works with customers across industries in building scalable generative AI platforms and operationalizing them. Throughout her career, she has shared her expertise at numerous conferences and has authored several blogs in the Machine Learning and Generative AI domains. 
Qingwei Li is a Machine Learning Specialist at Amazon Web Services. He received his Ph.D. in Operations Research after he broke his advisor‚Äôs research grant account and failed to deliver the Nobel Prize he promised. Currently he helps customers in the financial service and insurance industry build machine learning solutions on AWS. In his spare time, he likes reading and teaching. 
Kosti Vasilakakis is a Principal PM at AWS on the Agentic AI team, where he has led the design and development of several Bedrock AgentCore services from the ground up, including Runtime, Browser, Code Interpreter, and Identity. He previously worked on Amazon SageMaker since its early days, launching AI/ML capabilities now used by thousands of companies worldwide. Earlier in his career, Kosti was a data scientist. Outside of work, he builds personal productivity automations, plays tennis, and enjoys life with his wife and kids.
‚Ä¢ How Switchboard, MD automates real-time call transcription in clinical contact centers with Amazon Nova Sonic
  In high-volume healthcare contact centers, every patient conversation carries both clinical and operational significance, making accurate real-time transcription necessary for automated workflows. Accurate, instant transcription enables intelligent automation without sacrificing clarity or care, so that teams can automate electronic medical record (EMR) record matching, streamline workflows, and eliminate manual data entry. By removing routine process steps, staff can stay fully focused on patient conversations, improving both the experience and the outcome. As healthcare systems seek to balance efficiency with empathy, real-time transcription has become a capability for delivering responsive, high-quality care at scale. 
Switchboard, MD is a physician-led AI and data science company with a mission to prioritize the human connection in medicine. Its service improves patient engagement and outcomes, while reducing inefficiency and burnout. By designing and deploying clinically relevant solutions, Switchboard, MD helps providers and operators collaborate more effectively to deliver great experiences for both patients and staff. One of its key solutions is streamlining the contact center using AI voice automation, real-time medical record matching, and suggested next steps, which has led to significant reductions in queue times and call abandonment rates. 
With more than 20,000 calls handled each month, Switchboard, MD supports healthcare providers in delivering timely, personalized communication at scale. Its AI platform is already helping reduce call queue times, improve patient engagement, and streamline contact center operations for clinics and health systems. Customers using Switchboard have seen outcomes such as: 
 
 75% reduction in queue times 
 59% reduction in call abandonment rate 
 
Despite these early successes, Switchboard faced a critical challenge: their existing transcription approach couldn‚Äôt scale economically while maintaining the accuracy required for clinical workflows. Cost and word error rate (WER) weren‚Äôt just operational metrics‚Äîthey were critical enablers for scaling automation and expanding Switchboard‚Äôs impact across more patient interactions. 
In this post, we examine the specific challenges Switchboard, MD faced with scaling transcription accuracy and cost-effectiveness in clinical environments, their evaluation process for selecting the right transcription solution, and the technical architecture they implemented using Amazon Connect and Amazon Kinesis Video Streams. This post details the impressive results achieved and demonstrates how they were able to use this foundation to automate EMR matching and give healthcare staff more time to focus on patient care. Finally, we‚Äôll look at the broader implications for healthcare AI automation and how other organizations can implement similar solutions using Amazon Bedrock. 
Choosing an accurate, scalable, and cost-effective transcription model for contact center automation 
Switchboard, MD needed a transcription solution that delivered high accuracy at a sustainable cost. In clinical settings, transcription accuracy is critical because errors can compromise EMR record matching, affect recommended treatment plans, and disrupt automated workflows. At the same time, scaling support for thousands of calls each week meant that inference costs couldn‚Äôt be ignored. 
Switchboard initially explored multiple paths, including evaluating open source models such as Open AI‚Äôs Whisper model hosted locally. But these options presented tradeoffs‚Äîeither in performance, cost, or integration complexity. 
After testing, the team determined that Amazon Nova Sonic provided the right combination of transcription quality and efficiency needed to support their healthcare use case. The model performed reliably across live caller audio, even in noisy or variable conditions.&nbsp;It delivered: 
 
 80‚Äì90% lower transcription costs 
 A word error rate of 4% on Switchboard‚Äôs proprietary evaluation dataset 
 Low-latency output that aligned with their need for real-time processing 
 
 
Equally important, Nova Sonic integrated smoothly into Switchboard‚Äôs existing architecture, minimizing engineering lift and accelerating deployment. With this foundation, the team reduced manual transcription steps and scaled accurate, real-time automation across thousands of patient interactions. 

 ‚ÄúOur vision is to restore the human connection in medicine by removing administrative barriers that get in the way of meaningful interaction. Nova Sonic gave us the speed and accuracy we needed to transcribe calls in real time‚Äîso our customers can focus on what truly matters: the patient conversation.&nbsp;By reducing our transcription costs by 80‚Äì90%, it‚Äôs also made real-time automation sustainable at scale.‚Äù ‚Äì Dr. Blake Anderson, Founder, CEO, and CTO, Switchboard, MD
 
Architecture and implementation 
Switchboard‚Äôs architecture uses Amazon Connect to capture live audio from both patients and representatives. Switchboard processes audio streams through Amazon Kinesis Video Streams , which handles the real-time media conversion before routing the data to containerized&nbsp;AWS Lambda&nbsp;functions. Switchboard‚Äôs Lambda functions establish bidirectional streaming connections with Amazon Nova Sonic using BedrockRuntimeClient‚Äôs InvokeModelWithBidirectionalStream&nbsp;API.&nbsp; This novel architecture creates separate transcription streams for each conversation participant, which Switchboard recombines to create the complete transcription record. The entire processing pipeline runs in a serverless environment, providing scalable operation designed to handle thousands of concurrent calls while using Nova Sonic‚Äôs real-time speech-to-text capabilities for immediate transcription processing. 
Nova Sonic integration: Real-time speech processing 
Harnessing Amazon Nova Sonic‚Äôs advanced audio streaming and processing, Switchboard developed and built the capability of separating and recombining speakers‚Äô streams and transcripts. This makes Amazon Nova Sonic particularly effective for Switchboard‚Äôs healthcare applications, where accurate transcription and speaker identification are crucial. 
Amazon Nova Sonic offers configurable settings that can be optimized for different healthcare use cases, with the flexibility to prioritize either transcription or speech generation based on specific needs. A key cost-optimization feature is the ability to adjust speech output tokens ‚Äì organizations can set lower token values when primarily focused on transcription, resulting in significant cost savings while maintaining high accuracy. This versatility and cost flexibility makes Amazon Nova Sonic a valuable tool for healthcare organizations like Switchboard looking to implement voice-enabled solutions. 
Why serverless: Strategic advantages for healthcare innovation 
Switchboard‚Äôs choice of a serverless architecture using Amazon Connect, Amazon Kinesis Video Streams, and containerized Lambda functions represents a strategic decision that maximizes operational efficiency while minimizing infrastructure overhead. The serverless approach eliminates the need to provision, manage, and monitor underlying infrastructure, so that Switchboard‚Äôs engineering team can focus on developing clinical automation features rather than server management. This architecture provides built-in fault tolerance and high availability for critical healthcare communications without requiring extensive configuration from Switchboard‚Äôs team. 
Switchboard‚Äôs event-driven architecture, shown in the following figure, enables the system to scale from handling dozens to thousands of concurrent calls, with AWS automatically managing capacity provisioning behind the scenes. The pay-as-you-go billing model helps Switchboard pay only for compute resources used during call processing, optimizing costs while eliminating the risk of over-provisioning servers that would sit idle during low-volume periods. 
 
Conclusion 
Switchboard, MD‚Äôs implementation of Amazon Nova Sonic demonstrates how the right transcription technology can transform healthcare operations. By achieving 80‚Äì90% cost reductions while maintaining clinical-grade accuracy, they‚Äôve created a sustainable foundation for scaling AI-powered patient interactions across the healthcare industry. 
By building on Amazon Bedrock, Switchboard now has the flexibility to expand automation across more use cases and provider networks. Their success exemplifies how healthcare innovators can combine accuracy, speed, and efficiency to transform how care teams connect with patients‚Äîone conversation at a time. 
Get started with Amazon Nova on the Amazon Bedrock console. Learn more about Amazon Nova models at the Amazon Nova product page. 
 
About the authors 
 Tanner Jones is a Technical Account Manager in AWS Enterprise Support, where he helps customers navigate and optimize their production applications on AWS. He specializes in helping customers develop applications that incorporate AI agents, with a particular focus on building safe multi-agent systems. 
Anuj Jauhari&nbsp;is a Sr. Product Marketing Manager at AWS, where he helps customers innovate and drive business impact with generative AI solutions built on Amazon Nova models. 
Jonathan Woods is a Solutions Architect at AWS based in Nashville currently working with SMB customers. He has a passion for communicating AWS technology to businesses in a relevant way making it easy for customers to innovate. Outside of work, he tries keeping up with his three kids.   
Nauman Zulfiqar&nbsp;is a senior account manager based in New York working with SMB clients. He loves building and maintaining strong customer relationships, understanding their business challenges and serving as the customer‚Äôs primary business advocate within AWS.

‚∏ª