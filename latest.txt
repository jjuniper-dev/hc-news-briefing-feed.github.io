‚úÖ Morning News Briefing ‚Äì September 04, 2025 10:42

üìÖ Date: 2025-09-04 10:42
üè∑Ô∏è Tags: #briefing #ai #publichealth #digitalgov

‚∏ª

üßæ Weather
‚Ä¢ Current Conditions:  16.0¬∞C
  Temperature: 16.0&deg;C Pressure / Tendency: 100.3 kPa falling Humidity: 84 % Dewpoint: 13.3&deg:C Wind: ESE 3 km/h . Air Quality Health Index: n/a . Pembroke 6:00 AM EDT Thursday 4 September 2025 . Weather forecast: 20,000 years from Pem
‚Ä¢ Thursday: Showers. Temperature falling to 14 this afternoon.
  Risk of a thunderstorm late this morning and this afternoon . Amount of rain expected to fall 15 to 25 mm . Wind southeast 20 km/h becoming northwest 30 gusting to 50 this morning . Temperature falling to 14 this afternoon, UV index 2 or low . Risk of thunderstorms late in the morning and later in the afternoon, with rain expected in the evening . Rainfall expected to
‚Ä¢ Thursday night: Clearing. Low 8.
  Partly cloudy. Clearing late this evening . Clearing later this evening. Low 8.50/50/80/80 . Partly sunny sunny skies this morning . Clear skies this evening, then sunny skies Sunday night. Clear skies Monday night. Sunny skies Tuesday night. Wednesday night. Thursday morning. Friday night. Saturday night. Sunday morning. Sunday afternoon. Sunday evening.

üåç International News
No updates.

üçÅ Canadian News
No updates.

üá∫üá∏ U.S. Top Stories
‚Ä¢ Are creatine supplements all that they're pumped up to be?
  Creatine supplements have long become increasingly popular as social media influencers tout its benefits . What does the evidence say? Take a look at the evidence to see if the supplement has any effect on your body . Take a quiz to find out how much of the evidence supports the benefits of taking a supplement with Creatine . Do you know more about the evidence? Share it on iReport
‚Ä¢ Sen. Warren says banking panel should focus on Trump's attacks on Fed independence
  Sen. Elizabeth Warren is the top Democrat on the Senate Banking Committee on Stephen Miran's nomination to the Fed . Warren is concerned about the central bank's independence . Miran is the nominee for the next chairman of the Federal Reserve . Warren says Miran would be the first person to be nominated to head of the Fed in the country's history . Warren: Miran should be confirmed
‚Ä¢ RFK Jr. to face questions about chaos at the CDC
  Democratic lawmakers and more than a thousand current and former HHS staff say Kennedy's actions are endangering America's health . Kennedy says he came to clean house and he's delivering . Kennedy: "I'm delivering. I'm delivering what I need to do, and I don't want to get back into the role of the White House. I want to keep my family safe" Kennedy:
‚Ä¢ Sell it, donate it ‚Äî recycle it? A beloved old minivan faces a fork in the road
  After 20 years of service, an NPR reporter's beloved minivan is on the fritz . What is its best and highest calling now? Pass it on to another family, or recycle it into parts? Do you know what is your best calling? Share it with us at iReport.com/thenextstop . Back to the page you came from.com: Share your own
‚Ä¢ How the Education Department is using civil rights laws to bring schools to heel
  The Trump administration is using decades-old laws, meant to prevent discrimination, to threaten school districts and states with cuts to vital federal funding . School districts are being targeted by the Trump administration for cutting vital federal funds . The White House is using the laws to threaten state and local governments with funding cuts to protect students' rights and rights . The administration is also threatening to cut funding for schools

üß† Artificial Intelligence
No updates.

üíª Digital Strategy
‚Ä¢ UK DARPA clone spared savings squeeze while Treasury raided government
  ARIA spent ¬£16.5M, has ¬£600M in the tank, and no one asked for it back . ARIA was not asked to make savings leading up to the Spending Review, unlike other government departments . The agency was inspired by DARPA in the U.S. and not asked for the money back . It was not the first time ARIA had been asked
‚Ä¢ UK government trial of M365 Copilot finds no clear productivity boost
  A UK government department's three-month trial of Microsoft's M365 Copilot has revealed no discernible gain in productivity . Microsoft's Copilot sped up some tasks yet made others slower due to lower quality output . AI tech shows promise writing emails or summarizing meetings. Don't bother with anything more complex, don't bother it more complex. Read more from the UK Government blog here
‚Ä¢ Sainsbury's eyes up shoplifters with live facial recognition
  Sainsbury's has launched eight-week trial of live facial recognition (LFR) tech in two of its stores to curb shoplifting . Privacy campaigners cry foul as grocer joins Asda, Iceland, and others in retail surveillance boom . LFR tech is being trialled in two stores in an eight week trial to crack down on shoplifting in Britain's second-largest supermarket chain
‚Ä¢ Microsoft open-sources the 6502 BASIC coded by Bill Gates himself
  Microsoft has open-sourced the version of BASIC it created in 1976 for the MOS 6502 processor used in many early microcomputers . BASIC was created for many early computers and was used in the 1970s and '80s . Microsoft's BASIC is now open-source for the first time ever software has been created by Microsoft in 1976 . The original BASIC
‚Ä¢ France fines Google, SHEIN for undercooked cookie policies that led to crummy privacy
  France‚Äôs data protection authority levied massive fines against Google and SHEIN for dropping cookies on customers without securing their permission . Shein and Google also whacked Google for showing ads in email service . France also fined Google for using cookies without their permission, and also for using them in email ads . The fines were imposed by France's data protection agency, which also fineed Google for

üè• Public Health
No updates.

üî¨ Science
‚Ä¢ Components of particulate matter as potential risk factors for acute myocardial infarction
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Inside a mosquito factory
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Ultra-processed foods ‚Äî it‚Äôs time for an improved definition
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ This is the world‚Äôs largest ‚Äòmosquito factory‚Äô: its goal is to stop dengue
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Are ultra-processed foods really so unhealthy? What the science says
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations

üßæ Government & Policy
No updates.

üèõÔ∏è Enterprise Architecture & IT Governance
No updates.

ü§ñ AI & Emerging Tech
‚Ä¢ Synthesia‚Äôs AI clones are more expressive than ever. Soon they‚Äôll be able to talk back.
  Earlier this summer, I walked through the glassy lobby of a fancy office in London, into an elevator, and then along a corridor into a clean, carpeted room. Natural light flooded in through its windows, and a large pair of umbrella-like lighting rigs made the room even brighter. I tried not to squint as I took my place in front of a tripod equipped with a large camera and a laptop displaying an autocue. I took a deep breath and started to read out the script.



I‚Äôm not a newsreader or an actor auditioning for a movie‚ÄîI was visiting the AI company Synthesia to give it what it needed to create a hyperrealistic AI-generated avatar of me. The company‚Äôs avatars are a decent barometer of just how dizzying progress has been in AI over the past few years, so I was curious just how accurately its latest AI model, introduced last month, could replicate me.&nbsp;



When Synthesia launched in 2017, its primary purpose was to match AI versions of real human faces‚Äîfor example, the former footballer David Beckham‚Äîwith dubbed voices speaking in different languages. A few years later, in 2020, it started giving the companies that signed up for its services the opportunity to make professional-level presentation videos starring either AI versions of staff members or consenting actors. But the technology wasn‚Äôt perfect. The avatars‚Äô body movements could be jerky and unnatural, their accents sometimes slipped, and the emotions indicated by their voices didn‚Äôt always match their facial expressions.



Now Synthesia‚Äôs avatars have been updated with more natural mannerisms and movements, as well as expressive voices that better preserve the speaker‚Äôs accent‚Äîmaking them appear more humanlike than ever before. For Synthesia‚Äôs corporate clients, these avatars will make for slicker presenters of financial results, internal communications, or staff training videos.



I found the video demonstrating my avatar as unnerving as it is technically impressive. It‚Äôs slick enough to pass as a high-definition recording of a chirpy corporate speech, and if you didn‚Äôt know me, you‚Äôd probably think that‚Äôs exactly what it was. This demonstration shows how much harder it‚Äôs becoming to distinguish the artificial from the real. And before long, these avatars will even be able to talk back to us. But how much better can they get? And what might interacting with AI clones do to us?&nbsp;&nbsp;



The creation process



When my former colleague Melissa visited Synthesia‚Äôs London studio to create an avatar of herself last year, she had to go through a long process of calibrating the system, reading out a script in different emotional states, and mouthing the sounds needed to help her avatar form vowels and consonants. As I stand in the brightly lit room 15 months later, I‚Äôm relieved to hear that the creation process has been significantly streamlined. Josh Baker-Mendoza, Synthesia‚Äôs technical supervisor, encourages me to gesture and move my hands as I would during natural conversation, while simultaneously warning me not to move too much. I duly repeat an overly glowing script that‚Äôs designed to encourage me to speak emotively and enthusiastically. The result is a bit as if if Steve Jobs had been resurrected as a blond British woman with a low, monotonous voice.&nbsp;



It also has the unfortunate effect of making me sound like an employee of Synthesia.‚ÄúI am so thrilled to be with you today to show off what we‚Äôve been working on. We are on the edge of innovation, and the possibilities are endless,‚Äù I parrot eagerly, trying to sound lively rather than manic. ‚ÄúSo get ready to be part of something that will make you go, ‚ÄòWow!‚Äô This opportunity isn‚Äôt just big‚Äîit‚Äôs monumental.‚Äù



Just an hour later, the team has all the footage it needs. A couple of weeks later I receive two avatars of myself: one powered by the previous Express-1 model and the other made with the latest Express-2 technology. The latter, Synthesia claims, makes its synthetic humans more lifelike and true to the people they‚Äôre modeled on, complete with more expressive hand gestures, facial movements, and speech. You can see the results for yourself below.&nbsp;



COURTESY SYNTHESIA




Last year, Melissa found that her Express-1-powered avatar failed to match her transatlantic accent. Its range of emotions was also limited‚Äîwhen she asked her avatar to read a script angrily, it sounded more whiny than furious. In the months since, Synthesia has improved Express-1, but the version of my avatar made with the same technology blinks furiously and still struggles to synchronize body movements with speech.



By way of contrast, I‚Äôm struck by just how much my new Express-2 avatar looks like me: Its facial features mirror my own perfectly. Its voice is spookily accurate too, and although it gesticulates more than I do, its hand movements generally marry up with what I‚Äôm saying.&nbsp;



But the tiny telltale signs of AI generation are still there if you know where to look. The palms of my hands are bright pink and as smooth as putty. Strands of hair hang stiffly around my shoulders instead of moving with me. Its eyes stare glassily ahead, rarely blinking. And although the voice is unmistakably mine, there‚Äôs something slightly off about my digital clone‚Äôs intonations and speech patterns. ‚ÄúThis is great!‚Äù my avatar randomly declares, before slipping back into a saner register.





Anna Eiserbeck, a postdoctoral psychology researcher at the Humboldt University of Berlin who has studied how humans react to perceived deepfake faces, says she isn‚Äôt sure she‚Äôd have been able to identify my avatar as a deepfake at first glance.



But she would eventually have noticed something amiss. It‚Äôs not just the small details that give it away‚Äîmy oddly static earring, the way my body sometimes moves in small, abrupt jerks. It‚Äôs something that runs much deeper, she explains.



‚ÄúSomething seemed a bit empty. I know there‚Äôs no actual emotion behind it‚Äî it‚Äôs not a conscious being. It does not feel anything,‚Äù she says. Watching the video gave her ‚Äúthis kind of uncanny feeling.‚Äù&nbsp;



My digital clone, and Eiserbeck‚Äôs reaction to it, make me wonder how realistic these avatars really need to be.&nbsp;



I realize that part of the reason I feel disconcerted by my avatar is that it behaves in a way I rarely have to. Its oddly upbeat register is completely at odds with how I normally speak; I‚Äôm a die-hard cynical Brit who finds it difficult to inject enthusiasm into my voice even when I‚Äôm genuinely thrilled or excited. It‚Äôs just the way I am. Plus, watching the videos on a loop makes me question if I really do wave my hands about that way, or move my mouth in such a weird manner. If you thought being confronted with your own face on a Zoom call was humbling, wait until you‚Äôre staring at a whole avatar of yourself.&nbsp;



When Facebook was first taking off in the UK almost 20 years ago, my friends and I thought illicitly logging into each other‚Äôs accounts and posting the most outrageous or rage-inducing status updates imaginable was the height of comedy. I wonder if the equivalent will soon be getting someone else‚Äôs avatar to say something truly embarrassing: expressing support for a disgraced politician or (in my case) admitting to liking Ed Sheeran‚Äôs music.&nbsp;



Express-2 remodels every person it‚Äôs presented with into a polished professional speaker with the body language of a hyperactive hype man. And while this makes perfect sense for a company focused on making glossy business videos, watching my avatar doesn‚Äôt feel like watching me at all. It feels like something else entirely.



How it works



The real technical challenge these days has less to do with creating avatars that match our appearance than with getting them to replicate our behavior, says Bj√∂rn Schuller, a professor of artificial intelligence at Imperial College London. ‚ÄúThere‚Äôs a lot to consider to get right; you have to have the right micro gesture, the right intonation, the sound of voice and the right word,‚Äù he says. ‚ÄúI don‚Äôt want an AI [avatar] to frown at the wrong moment‚Äîthat could send an entirely different message.‚Äù



To achieve an improved level of realism, Synthesia developed a number of new audio and video AI models. The team created a voice cloning model to preserve the human speaker‚Äôs accent, intonation, and expressiveness‚Äîunlike other voice models, which can flatten speakers‚Äô distinctive accents into generically American-sounding voices.





When a user uploads a script to Express-1, its system analyzes the words to infer the correct tone to use. That information is then fed into a diffusion model, which renders the avatar‚Äôs facial expressions and movements to match the speech.¬†



Alongside the voice model, Express-2 uses three other models to create and animate the avatars. The first generates an avatar‚Äôs gestures to accompany the speech fed into it by the Express-Voice model. A second evaluates how closely the input audio aligns with the multiple versions of the corresponding generated motion before selecting the best one. Then a final model renders the avatar with that chosen motion.¬†



This third rendering model is significantly more powerful than its Express-1 predecessor. Whereas the previous model had a few hundred million parameters, Express-2‚Äôs rendering model‚Äôs parameters number in the billions. This means it takes less time to create the avatar, says Youssef Alami Mejjati, Synthesia‚Äôs head of research and development:



‚ÄúWith Express-1, it needed to first see someone expressing emotions to be able to render them. Now, because we‚Äôve trained it on much more diverse data and much larger data sets, with much more compute, it just learns these associations automatically without needing to see them.‚Äù&nbsp;







Narrowing the uncanny valley



Although humanlike AI-generated avatars have been around for years, the recent boom in generative AI is making it increasingly easier and more affordable to create lifelike synthetic humans‚Äîand they‚Äôre already being put to work. Synthesia isn‚Äôt alone: AI avatar companies like Yuzu Labs, Creatify, Arcdads, and Vidyard give businesses the tools to quickly generate and edit videos starring either AI actors or artificial versions of members of staff, promising cost-effective ways to make compelling ads that audiences connect with. Similarly, AI-generated clones of livestreamers have exploded in popularity across China in recent years, partly because they can sell products 24/7 without getting tired or needing to be paid.¬†



For now at least, Synthesia is ‚Äúlaser focused‚Äù on the corporate sphere. But it‚Äôs not ruling out expanding into new sectors such as entertainment or education, says Peter Hill, the company‚Äôs chief technical officer. In an apparent step toward this, Synthesia recently partnered with Google to integrate Google‚Äôs powerful new generative video model Veo 3 into its platform, allowing users to directly generate and embed clips into Synthesia‚Äôs videos. It suggests that in the future, these hyperrealistic artificial humans could take up starring roles in detailed universes with ever-changeable backdrops.&nbsp;





At present this could, for example, involve using Veo 3 to generate a video of meat-processing machinery, with a Synthesia avatar next to the machines talking about how to use them safely. But future versions of Synthesia‚Äôs technology could result in educational videos customizable to an individual‚Äôs level of knowledge, says Alex Voica, head of corporate affairs and policy at Synthesia. For example, a video about the evolution of life on Earth could be tweaked for someone with a biology degree or someone with high-school-level knowledge. ‚ÄúIt‚Äôs going to be such a much more engaging and personalized way of delivering content that I‚Äôm really excited about,‚Äù he says.&nbsp;



The next frontier, according to Synthesia, will be avatars that can talk back, ‚Äúunderstanding‚Äù conversations with users and responding in real time Think ChatGPT, but with a lifelike digital human attached.&nbsp;



Synthesia has already added an interactive element by letting users click through on-screen questions during quizzes presented by its avatars. But it‚Äôs also exploring making them truly interactive: Future users could ask their avatar to pause and expand on a point, or ask it a question. ‚ÄúWe really want to make the best learning experience, and that means through video that‚Äôs entertaining but also personalized and interactive,‚Äù says Alami Mejjati. ‚ÄúThis, for me, is the missing part in online learning experiences today. And I know we‚Äôre very close to solving that.‚Äù



We already know that humans can‚Äîand do‚Äîform deep emotional bonds with AI systems, even with basic text-based chatbots. Combining agentic technology‚Äîwhich is already capable of navigating the web, coding, and playing video games unsupervised‚Äîwith a realistic human face could usher in a whole new kind of AI addiction, says Pat Pataranutaporn, an assistant professor at the MIT Media Lab.&nbsp;&nbsp;



‚ÄúIf you make the system too realistic, people might start forming certain kinds of relationships with these characters,‚Äù he says. ‚ÄúWe‚Äôve seen many cases where AI companions have influenced dangerous behavior even when they are basically texting. If an avatar had a talking head, it would be even more addictive.‚Äù



Schuller agrees that avatars in the near future will be perfectly optimized to adjust their projected levels of emotion and charisma so that their human audiences will stay engaged for as long as possible. ‚ÄúIt will be very hard [for humans] to compete with charismatic AI of the future; it‚Äôs always present, always has an ear for you, and is always understanding,‚Äù he says. ‚ÄúAl will change that human-to-human connection.‚Äù



As I pause and replay my Express-2 avatar, I imagine holding conversations with it‚Äîthis uncanny, permanently upbeat, perpetually available product of pixels and algorithms that looks like me and sounds like me, but fundamentally isn‚Äôt me. Virtual Rhiannon has never laughed until she‚Äôs cried, or fallen in love, or run a marathon, or watched the sun set in another country.&nbsp;



But, I concede, she could deliver a damned good presentation about why Ed Sheeran is the greatest musician ever to come out of the UK. And only my closest friends and family would know that it‚Äôs not the real me.
‚Ä¢ How Trump is helping China extend its massive lead in clean energy
  On a spring day in 1954, Bell Labs researchers showed off the first practical solar panels at a press conference in Murray Hill, New Jersey, using sunlight to spin a toy Ferris wheel before a stunned crowd.



The solar future looked bright. But in the race to commercialize the technology it invented, the US would lose resoundingly. Last year, China exported $40 billion worth of solar panels and modules, while America shipped just $69 million, according to the New York Times. It was a stunning forfeit of a huge technological lead.&nbsp;



And now the US seems determined to repeat the mistake. In its quest to prop up aging fossil-fuel industries, the Trump administration has slashed federal support for the emerging cleantech sector, handing his nation‚Äôs chief economic rival the most generous of gifts: an unobstructed path to locking in its control of emerging energy technologies, and a leg up in inventing the industries of the future.



China‚Äôs dominance of solar was no accident. In the late 2000s, the government simply determined that the sector was a national priority. Then it leveraged deep subsidies, targeted policies, and price wars to scale up production, drive product improvements, and slash costs. It‚Äôs made similar moves in batteries, electric vehicles, and wind turbines.&nbsp;



Meanwhile, President Donald Trump has set to work unraveling hard-won clean-energy achievements in the US, snuffing out the gathering momentum to rebuild the nation‚Äôs energy sector in cleaner, more sustainable ways.



The tax and spending bill that Trump signed into law in early July wound down the subsidies for solar and wind power contained in the Inflation Reduction Act of 2022. The legislation also cut off federal support for cleantech projects that rely too heavily on Chinese materials‚Äîa hamfisted bid to punish Chinese industries that will instead make many US projects financially unworkable.





Meanwhile, the administration has slashed federal funding for science and attacked the financial foundations of premier research universities, pulling up the roots of future energy innovations and industries.



A driving motivation for many of these policies is the quest to protect the legacy energy industry based on coal, oil, and natural gas, all of which the US is geologically blessed with. But this strategy amounts to the innovator‚Äôs dilemma playing out at a national scale‚Äîa country clinging to its declining industries rather than investing in the ones that will define the future.



It does not particularly matter whether Trump believes in or cares about climate change. The economic and international security imperatives to invest in modern, sustainable industries are every bit as indisputable as the chemistry of greenhouse gases.



Without sustained industrial policies that reward innovation, American entrepreneurs and investors won‚Äôt risk money and time creating new businesses, developing new products, or building first-of-a-kind projects here. Indeed, venture capitalists have told me that numerous US climate-tech companies are already looking overseas, seeking markets where they can count on government support. Some fear that many other companies will fail in the coming months as subsidies disappear, developments stall, and funding flags.&nbsp;



All of which will help China extend an already massive lead.



The nation has installed nearly three times as many wind turbines as the US, and it generates more than twice as much solar power. It boasts five of the 10 largest EV companies in the world, and the three largest wind turbine manufacturers. China absolutely dominates the battery market, producing the vast majority of the anodes, cathodes, and battery cells that increasingly power the world‚Äôs vehicles, grids, and gadgets.



China harnessed the clean-energy transition to clean up its skies, upgrade its domestic industries, create jobs for its citizens, strengthen trade ties, and build new markets in emerging economies. In turn, it‚Äôs using those business links to accrue soft power and extend its influence‚Äîall while the US turns it back on global institutions.



These widening relationships increasingly insulate China from external pressures, including those threatened by Trump‚Äôs go-to tactic: igniting or inflaming trade wars.&nbsp;



But stiff tariffs and tough talk aren‚Äôt what built the world‚Äôs largest economy and established the US as the global force in technology for more than a century. What did was deep, sustained federal investment into education, science, and research and development‚Äîthe very budget items that Trump and his party have been so eager to eliminate.&nbsp;



Another thing



Earlier this summer, the EPA announced plans to revoke the Obama-era ‚Äúendangerment finding,‚Äù the legal foundation for regulating the nation‚Äôs greenhouse-gas pollution.&nbsp;



The agency‚Äôs argument leans heavily on a report that rehashes decades-old climate-denial talking points to assert that rising emissions haven‚Äôt produced the harms that scientists expected. It‚Äôs a wild, Orwellian plea for you to reject the evidence of your eyes and ears in a summer that saw record heat waves in the Midwest and East and is now blanketing the West in wildfire smoke.



Over the weekend, more than 85 scientists sent a point-by-point, 459-page rebuttal to the federal government, highlighting myriad ways in which the report ‚Äúis biased, full of errors, and not fit to inform policy making,‚Äù as Bob Kopp, a climate scientist at Rutgers, put it on Bluesky.



‚ÄúThe authors reached these flawed conclusions through selective filtering of evidence (‚Äòcherry picking‚Äô), overemphasis of uncertainties, misquoting peer-reviewed research, and a general dismissal of the vast majority of decades of peer-reviewed research,‚Äù the dozens of reviewers found.The Trump administration handpicked researchers who would write the report it wanted to support its quarrel with thermometers and justify its preordained decision to rescind the endangerment finding. But it‚Äôs legally bound to hear from others as well, notes Karen McKinnon, a climate researcher at the University of California, Los Angeles.



‚ÄúLuckily, there is time to take action,‚Äù McKinnon said in a statement. ‚ÄúComment on the report, and contact your representatives to let them know we need to take action to bring back the tolerable summers of years past.‚Äù



You can read the full report here, or NPR‚Äôs take here. And be sure to read Casey Crownhart‚Äôs earlier piece in The Spark on the endangerment finding.



This article is from The Spark, MIT Technology Review‚Äôs weekly climate newsletter. To receive it in your inbox every Wednesday, sign up here.
‚Ä¢ The Download: sustainable architecture, and DeepSeek‚Äôs success
  This is today&#8217;s edition of¬†The Download,¬†our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



Material Cultures looks to the past to build the future



Despite decades of green certifications, better material sourcing, and the use of more sustainable materials, the built environment is still responsible for a third of global emissions worldwide. According to a 2024 UN report, the building sector has fallen ‚Äúsignificantly behind on progress‚Äù toward becoming more sustainable. Changing the way we erect and operate buildings remains key to tackling climate change.London-based design and research nonprofit Material Cultures is exploring how tradition can be harnessed in new ways to repair the contemporary building system. As many other practitioners look to artificial intelligence and other high-tech approaches, Material Cultures is focusing on sustainability, and finding creative ways to turn local materials into new buildings. Read the full story.



‚ÄîPatrick Sisson



This story is from our new print edition, which is all about the future of security. Subscribe here to catch future copies when they land.







MIT Technology Review Narrated: How a top Chinese AI model overcame US sanctions



Earlier this year, the AI community was abuzz over DeepSeek R1, a new open-source reasoning model. The model was developed by the Chinese AI startup DeepSeek, which claims that R1 matches or even surpasses OpenAI‚Äôs ChatGPT o1 on multiple key benchmarks but operates at a fraction of the cost.



DeepSeek‚Äôs success is even more remarkable given the constraints facing Chinese AI companies in the form of increasing US export controls on cutting-edge chips. Read the full story.This is our latest story to be turned into a MIT Technology Review Narrated podcast, which we‚Äôre publishing each week on Spotify and Apple Podcasts. Just navigate to MIT Technology Review Narrated on either platform, and follow us to get all our new content as it‚Äôs released.







The must-reads



I‚Äôve combed the internet to find you today‚Äôs most fun/important/scary/fascinating stories about technology.



1 Google won‚Äôt be forced to sell Chrome after allA federal judge has instead ruled it has to share search data with its rivals. (Politico)+ He also barred Google from making deals to make Chrome the default search engine on people‚Äôs phones. (The Register)+ The company‚Äôs critics feel the ruling doesn‚Äôt go far enough. (The Verge)



2 OpenAI is adding emotional guardrails to ChatGPTThe new rules are designed to better protect teens and vulnerable people. (Axios)+ Families of dead teenagers say AI companies aren‚Äôt doing enough. (FT $)+ An AI chatbot told a user how to kill himself‚Äîbut the company doesn‚Äôt want to ‚Äúcensor‚Äù it. (MIT Technology Review)



3 China‚Äôs military has showed off its robotic wolvesAlongside underwater torpedoes and hypersonic cruise missiles. (BBC)+ Xi Jinping has pushed to modernize the world‚Äôs largest standing army. (CNN)+ Phase two of military AI has arrived. (MIT Technology Review)4 ICE has resumed working with a previously banned spyware vendorParagon Solutions‚Äô software was found on the devices of journalists earlier this year. (WP $)+ The tool can manipulate a phone‚Äôs recorder to become a covert listening device. (The Guardian)



5 An identical twin has been convicted of a crime based on DNA analysis¬†It‚Äôs the first time the technology has been successfully used in the US, and solves a 38-year old cold case. (The Guardian)6 People who understand AI the least are the most likely to use it¬†Those with a better grasp of how AI works know more about its limitations. (WSJ $)+ What is AI? (MIT Technology Review)



7 BMW is preparing to unveil a super-smart EVIts new iX3 sport utility vehicle will have 20 times more computing power. (FT $)



8 Sick and lonely people are turning to AI ‚Äúdoctors‚ÄùPhysicians are too busy to spend much time with patients. Chatbots are filling the void. (Rest of World)+ AI companies have stopped warning you that their chatbots aren‚Äôt doctors. (MIT Technology Review)



9 Around 90% of life on Earth is still unknownBut shedding light on these mysterious organisms is essential to our future survival. (Vox)



10 Wax worms could help tackle our plastic pollution problem The plastic-hungry pests can eat a polythene bag in a matter of hours. (Wired $)+ Think that your plastic is being recycled? Think again. (MIT Technology Review)







Quote of the day



‚ÄúIt‚Äôs a nothingburger.‚Äù



‚ÄîGabriel Weinberg, chief executive of search engine DuckDuckGo, reacts to the judge‚Äôs decision in the Google Chrome monopoly case, the New York Times reports.







¬†One more thing







Why we can no longer afford to ignore the case for climate adaptationBack in the 1990s, anyone suggesting that we‚Äôd need to adapt to climate change while also cutting emissions was met with suspicion. Most climate change researchers felt adaptation studies would distract from the vital work of keeping pollution out of the atmosphere to begin with.Despite this hostile environment, a handful of experts were already sowing the seeds for a new field of research called ‚Äúclimate change adaptation‚Äù: study and policy on how the world could prepare for and adapt to the new disasters and dangers brought forth on a warming planet. Today, their research is more important than ever. Read the full story.¬†



‚ÄîMadeline Ostrander







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.&nbsp;



+ How to have a happier life, even when you‚Äôre living through bleak times (maybe skip the raisins on ice cream, though.)+ If you‚Äôre loving Alien: Earth right now, why not dive back into the tremendously terrifying Alien: Isolation game?+ The first freaky images of the second part of zombie flick 28 Years Later have landed.+ Anthony Gormley, you will always be cool.
‚Ä¢ The connected customer
  As brands compete for increasingly price conscious consumers, customer experience (CX) has become a decisive differentiator . Yet many struggle to deliver, constrained by outdated systems, fragmented data, and organizational silos that limit both agility and consistency . This report explores how leading organizations are navigating that shift, and what it takes to move from AI potential to CX impact . The most effective organizations treat AI as a collaborative tool that enhances rather than replaces human connection and expertise .
‚Ä¢ Building the AI-enabled enterprise of the future
  The use of AI technologies is already driving changes across industries . 98% say they feel an increased sense of urgency in the last year . 85% believe they have less than 18 months to deploy an AI strategy or they will see negative business effects . Just 13% of companies globally say they are ready to leverage AI to its full potential . IT infrastructure is an increasing challenge as workloads grow ever larger . Good quality, well-managed enterprise-wide data is essential .

üîí Cybersecurity & Privacy
‚Ä¢ The Ongoing Fallout from a Breach at AI Chatbot Maker Salesloft
  The recent mass-theft of authentication tokens from Salesloft, whose AI chatbot is used by a broad swath of corporate America to convert customer interaction into Salesforce leads, has left many companies racing to invalidate the stolen credentials before hackers can exploit them. Now Google warns the breach goes far beyond access to Salesforce data, noting the hackers responsible also stole valid authentication tokens for hundreds of online services that customers can integrate with Salesloft, including Slack, Google Workspace, Amazon S3, Microsoft Azure, and OpenAI.
Salesloft says its products are trusted by 5,000+ customers. Some of the bigger names are visible on the company&#8217;s homepage.
Salesloft disclosed on August 20 that, &#8220;Today, we detected a security issue in the Drift application,&#8221; referring to the technology that powers an AI chatbot used by so many corporate websites. The alert urged customers to re-authenticate the connection between the Drift and Salesforce apps to invalidate their existing authentication tokens, but it said nothing then to indicate those tokens had already been stolen.
On August 26, the Google Threat Intelligence Group (GTIG) warned that unidentified hackers tracked as UNC6395 used the access tokens stolen from Salesloft to siphon large amounts of data from numerous corporate Salesforce instances. Google said the data theft began as early as Aug. 8, 2025 and lasted through at least Aug. 18, 2025, and that the incident did not involve any vulnerability in the Salesforce platform.
Google said the attackers have been sifting through the massive data haul for credential materials such as AWS keys, VPN credentials, and credentials to the cloud storage provider Snowflake.
&#8220;If successful, the right credentials could allow them to further compromise victim and client environments, as well as pivot to the victim&#8217;s clients or partner environments,&#8221; the GTIG report stated.
The GTIG updated its advisory on August 28 to acknowledge the attackers used the stolen tokens to access email from &#8220;a very small number of Google Workspace accounts&#8221; that were specially configured to integrate with Salesloft. More importantly, it warned organizations to immediately invalidate all tokens stored in or connected to their Salesloft integrations &#8212; regardless of the third-party service in question.
&#8220;Given GTIG&#8217;s observations of data exfiltration associated with the campaign, organizations using Salesloft Drift to integrate with third-party platforms (including but not limited to Salesforce) should consider their data compromised and are urged to take immediate remediation steps,&#8221; Google advised.
On August 28, Salesforce blocked Drift from integrating with its platform, and with its productivity platforms Slack and Pardot.
The Salesloft incident comes on the heels of a broad social engineering campaign that used voice phishing to trick targets into connecting a malicious app to their organization&#8217;s Salesforce portal. That campaign led to data breaches and extortion attacks affecting a number of companies including Adidas, Allianz Life and Qantas.
On August 5, Google disclosed that one of its corporate Salesforce instances was compromised by the attackers, which the GTIG has dubbed UNC6040 (&#8220;UNC&#8221; stands for &#8220;uncategorized threat group&#8221;). Google said the extortionists consistently claimed to be the threat group ShinyHunters,¬†and that the group appeared to be preparing to escalate its extortion attacks by launching a data leak site.
ShinyHunters is an amorphous threat group known for using social engineering to break into cloud platforms and third-party IT providers, and for posting dozens of stolen databases to cybercrime communities like the now-defunct Breachforums.
The ShinyHunters brand dates back to 2020, and the group has been credited with or taken responsibility for dozens of data leaks that exposed hundreds of millions of breached records. The group&#8217;s member roster is thought to be somewhat fluid, drawing mainly from active denizens of the Com, a mostly English-language cybercrime community scattered across an ocean of Telegram and Discord servers.
Recorded Future&#8217;s Alan Liska told Bleeping Computer that the overlap in the &#8220;tools, techniques and procedures&#8221; used by ShinyHunters and the Scattered Spider extortion group likely indicate some crossover between the two groups.
To muddy the waters even further, on August 28 a Telegram channel that now has nearly 40,000 subscribers was launched under the intentionally confusing banner &#8220;Scattered LAPSUS$ Hunters 4.0,&#8221; wherein participants have repeatedly claimed responsibility for the Salesloft hack without actually sharing any details to prove their claims.
The Telegram group has been trying to attract media attention by threatening security researchers at Google and other firms. It also is using the channel&#8217;s sudden popularity to promote a new cybercrime forum called &#8220;Breachstars,&#8221; which they claim will soon host data stolen from victim companies who refuse to negotiate a ransom payment.
The &#8220;Scattered Lapsus$ Hunters 4.0&#8221; channel on Telegram now has roughly 40,000 subscribers.
But Austin Larsen, a principal threat analyst at Google&#8217;s threat intelligence group, said there is no compelling evidence to attribute the Salesloft activity to ShinyHunters or to other known groups at this time.
&#8220;Their understanding of the incident seems to come from public reporting alone,&#8221; Larsen told KrebsOnSecurity, referring to the most active participants in the Scattered LAPSUS$ Hunters 4.0 Telegram channel.
Joshua Wright, a senior technical director at Counter Hack,¬†is credited with coining the term &#8220;authorization sprawl&#8221; to describe one key reason that social engineering attacks from groups like Scattered Spider and ShinyHunters so often succeed: They abuse legitimate user access tokens to move seamlessly between on-premises and cloud systems.
Wright said this type of attack chain often goes undetected because the attacker sticks to the resources and access already allocated to the user.
&#8220;Instead of the conventional chain of initial access, privilege escalation and endpoint bypass, these threat actors are using centralized identity platforms that offer single sign-on (SSO) and integrated authentication and authorization schemes,&#8221; Wright wrote in a June 2025 column. &#8220;Rather than creating custom malware, attackers use the resources already available to them as authorized users.&#8221;
It remains unclear exactly how the attackers gained access to all Salesloft Drift authentication tokens. Salesloft announced on August 27 that it hired Mandiant, Google Cloud&#8217;s incident response division, to investigate the root cause(s).
&#8220;We are working with Salesloft Drift to investigate the root cause of what occurred and then it‚Äôll be up to them to publish that,&#8221; Mandiant Consulting CTO Charles Carmakal told Cyberscoop. &#8220;There will be a lot more tomorrow, and the next day, and the next day.&#8221;

üéì University AI
No updates.

üè¢ Corporate AI
‚Ä¢ Authenticate Amazon Q Business data accessors using a trusted token issuer
  Since its general availability in 2024, Amazon Q Business (Amazon Q) has enabled independent software vendors (ISVs) to enhance their Software as a Service (SaaS) solutions through secure access to customers‚Äô enterprise data by becoming Amazon Q Business data accessor. To find out more on data accessor, see this page. The data accessor now supports trusted identity propagation. With trusted token issuer (TTI) authorization support, ISVs as data accessor can integrate with Amazon Q index while maintaining enterprise-grade security standards for their software-as-a-service (SaaS) solutions. 
Prior to TTI support, data accessors needed to implement authorization code flow with AWS IAM Identity Center integration when accessing the Amazon Q index. With TTI support for data accessors, ISVs can now use their own OpenID Provider to authenticate enterprise users, alleviating the need for double authentication while maintaining security standards. 
In this blog post, we show you how to implement TTI authorization for data accessors, compare authentication options, and provide step-by-step guidance for both ISVs and enterprises. 
Prerequisites 
Before you begin, make sure you have the following requirements: 
 
 An AWS account with administrator access 
 Access to Amazon Q Business 
 For ISVs: 
   
   An OpenID Connect (OIDC) compatible authorization server 
    
 For enterprises: 
   
   Amazon Q Business administrator access 
   Permission to create trusted token issuers 
    
 
Solution Overview 
This solution demonstrates how to implement TTI authentication for Amazon Q Business data accessors. The following diagram illustrates the overall flow between different resources, from ISV becoming a data accessor, customer enabling ISV data accessor, to ISV accessing customer‚Äôs Amazon Q index: 
 
Understanding Trusted Token Issuer Authentication 
Trusted Token Issuer represents an advanced identity integration capability for Amazon Q. At its core, TTI is a token exchange API that propagates identity information into IAM role sessions, enabling AWS services to make authorization decisions based on the actual end user‚Äôs identity and group memberships. This mechanism allows AWS services to apply authorization and security controls based on the authenticated user context. The TTI support simplifies the identity integration process while maintaining robust security standards, making it possible for organizations to ensure that access to Amazon Q respects user-level permissions and group memberships. This enables fine-grained access control and maintains proper security governance within Amazon Q implementations. 
Trusted Token Issuer authentication simplifies the identity integration process for Amazon Q by enabling the propagation of user identity information into AWS IAM role sessions. Each token exchange allows AWS services to make authorization decisions based on the authenticated user‚Äôs identity and group memberships. The TTI support streamlines the integration process while maintaining robust security standards, enabling organizations to implement appropriate access controls within their Amazon Q implementations. 
Understanding Data Accessors 
A data accessor is an ISV that has registered with AWS and is authorized to use their customers‚Äô Amazon Q index for the ISV‚Äôs Large Language Model (LLM) solution. The process begins with ISV registration, where they provide configuration information including display name, business logo, and OpenID Connect (OIDC) configuration details for TTI support. 
During ISV registration, providers must specify their tenantId configuration ‚Äì a unique identifier for their application tenant. This identifier might be known by different names in various applications (such as Workspace ID in Slack or Domain ID in Asana) and is required for proper customer isolation in multi-tenant environments. 
Amazon Q customers then add the ISV as a data accessor to their environment, granting access to their Amazon Q index based on specific permissions and data source selections. Once authorized, the ISV can query the customers‚Äô index through API requests using their TTI authentication flow, creating a secure and controlled pathway for accessing customer data. 
Implementing TTI Authentication for Amazon Q index Access 
This section explains how to implement TTI authentication for accessing the Amazon Q index. The implementation involves initial setup by the customer and subsequent authentication flow implemented by data accessors for user access. 
TTI provides capabilities that enable identity-enhanced IAM role sessions through Trusted Identity Propagation (TIP), allowing AWS services to make authorization decisions based on authenticated user identities and group memberships. Here‚Äôs how it works: 
To enable data accessor access to a customer‚Äôs Amazon Q index through TTI, customers must perform an initial one-time setup by adding a data accessor on Amazon Q Business application. During setup, a TTI with the data accessor‚Äôs identity provider information is created in the customer‚Äôs AWS IAM Identity Center, allowing the data accessor‚Äôs identity provider to authenticate access to the customer‚Äôs Amazon Q index. 
 
The process to set up an ISV data accessor with TTI authentication consists of the following steps: 
 
 The customer‚Äôs IT administrator accesses their Amazon Q Business application and creates a trusted token issuer with the ISV‚Äôs OAuth information. This returns a TrustedTokenIssuer (TTI) Amazon Resource Name (ARN).  
 The IT administrator creates an ISV data accessor with the TTI ARN received in Step 1.  
 Amazon Q Business confirms the provided TTI ARN with AWS IAM Identity Center and creates a data accessor application. 
 Upon successful creation of the ISV data accessor, the IT administrator receives data accessor details to share with the ISV.  
 The IT administrator provides these details to the ISV application. 
 
Once the data accessor setup is complete in the customer‚Äôs Amazon Q environment, users can access the Amazon Q index through the ISV application by authenticating only against the data accessor‚Äôs identity provider. 
 
The authentication flow proceeds as follows: 
 
 A user authenticates against the data accessor‚Äôs identity provider through the ISV application. The ISV application receives an ID token for that user, generated from the ISV‚Äôs identity provider with the same client ID registered on their data accessor. 
 The ISV application needs to use the AWS Identity and Access Management (IAM) role that they created during the data accessor onboarding process by calling AssumeRole API, then make CreateTokenWithIAM API request to the customer‚Äôs AWS IAM Identity Center with the ID token. AWS IAM Identity Center validates the ID token with the ISV‚Äôs identity provider and returns an IAM Identity Center token. 
 The ISV application requests an AssumeRole API with: IAM Identity Center token, extracted identity context, and tenantId. The tenantId is a security control jointly established between the ISV and their customer, with the customer maintaining control over how it‚Äôs used in their trust relationships. This combination facilitates secure access to the correct customer environment. 
 The ISV application calls the SearchRelevantContent API with the session credentials and receives relevant content from the customer‚Äôs Amazon Q index. 
 
Choosing between TTI and Authorization Code 
When implementing Amazon Q integration, ISVs need to consider two approaches, each with its own benefits and considerations: 
 
  
   
    
   Trusted Token Issuer 
   Authorization Code 
   
   
   Advantages 
   Single authentication on the ISV system 
   Enhanced security through mandatory user initiation for each session 
   
   
   Enables backend-only access to SearchRelevantContent API without user interaction 
    
   
   
   Considerations 
   Some enterprises may prefer authentication flows that require explicit user consent for each session, providing additional control over API access timing and duration 
   Requires double authentication on the ISV system 
   
   
   Requires ISVs to host and maintain OpenID Provider 
    
   
  
 
TTI excels in providing a seamless user experience through single authentication on the ISV system and enables backend-only implementations for SearchRelevantContent API access without requiring direct user interaction. However, this approach requires ISVs to maintain their own OIDC authorization server, which may present implementation challenges for some organizations. Additionally, some enterprises might have concerns about ISVs having persistent ability to make API requests on behalf of their users without explicit per-session authorization. 
Next Steps 
For ISVs: Becoming a Data Accessor with TTI Authentication 
Getting started on Amazon Q data accessor registration process with TTI authentication is straightforward. If you already have an OIDC compatible authorization server for your application‚Äôs authentication, you‚Äôre most of the way there. 
To begin the registration process, you‚Äôll need to provide the following information: 
 
 Display name and business logo that will be displayed on AWS Management Console 
 OIDC configuration details (OIDC ClientId and discovery endpoint URL) 
 TenantID configuration details that specify how your application identifies different customer environments 
 
For details, see Information to be provided to the Amazon Q Business team. 
For ISVs using Amazon Cognito as their OIDC authorization server, here‚Äôs how to retrieve the required OIDC configuration details: 
 
 To get the OIDC ClientId:- Navigate to the Amazon Cognito console- Select your User Pool- Go to ‚ÄúApplications‚Äù &gt; ‚ÄúApp clients‚Äù- The ClientId is listed under ‚ÄúClient ID‚Äù for your app client 
 To get the discovery endpoint URL:- The URL follows this format:https://cognito-idp.{region}.amazonaws.com/{userPoolId}/.well-known/openid-configuration‚Äì Replace {region} with your AWS region (e.g., us-east-1)- Replace {userPoolId} with your Cognito User Pool IDFor example, if your User Pool is in us-east-1 with ID ‚Äòus-east-1_abcd1234‚Äô, your discovery endpoint URL would be: https://cognito-idp.us-east-1.amazonaws.com/us-east-1_abcd1234/.well-known/openid-configuration 
 
 
Note: While this example uses Amazon Cognito, the process will vary depending on your OIDC provider. Common providers like Auth0, Okta, or custom implementations will have their own methods for accessing these configuration details. 
Once registered, you can enhance your generative AI application with the powerful capabilities of Amazon Q, allowing your customers to access their enterprise knowledge base through your familiar interface. AWS provides comprehensive documentation and support to help you implement the authentication flow and API integration efficiently. 
For Enterprises: Enabling TTI-authenticated Data Accessor 
To enable a TTI-authenticated data accessor, your IT administrator needs to complete the following steps in the Amazon Q console: 
 
 Create a trusted token issuer using the ISV‚Äôs OAuth information 
 Set up the data accessor with the generated TTI ARN 
 Configure appropriate data source access permissions 
 
This streamlined setup allows your users to access Amazon Q index through the ISV‚Äôs application using their existing ISV application credentials, alleviating the need for multiple logins while maintaining security controls over your enterprise data. 
Both ISVs and enterprises benefit from AWS‚Äôs comprehensive documentation and support throughout the implementation process, facilitating a smooth and secure integration experience. 
Clean up resources 
To avoid unused resources, follow these steps if you no longer need the data accessor: 
 
 Delete the data accessor: 
   
   On the Amazon Q Business console, choose Data accessors in the navigation pane 
   Select your data accessor and choose Delete. 
    
 Delete the TTI: 
   
   On the IAM Identity Center console, choose Trusted Token Issuers in the navigation pane. 
   Select the associated issuer and choose Delete. 
    
 
Conclusion 
The introduction of Trusted Token Issuer (TTI) authentication for Amazon Q data accessors marks a significant advancement in how ISVs integrate with Amazon Q Business. By enabling data accessors to use their existing OIDC infrastructure, we‚Äôve alleviated the need for double authentication while maintaining enterprise-grade security standards through TTI‚Äôs robust tenant isolation mechanisms and secure multi-tenant access controls, making sure each customer‚Äôs data remains protected within their dedicated environment. This streamlined approach not only enhances the end-user experience but also simplifies the integration process for ISVs building generative AI solutions. 
In this post, we showed how to implement TTI authentication for Amazon Q data accessors. We covered the setup process for both ISVs and enterprises and demonstrated how TTI authentication simplifies the user experience while maintaining security standards. 
To learn more about Amazon Q Business and data accessor integration, refer to Share your enterprise data with data accessors using Amazon Q index and Information to be provided to the Amazon Q Business team. You can also contact your AWS account team for personalized guidance. Visit the Amazon Q Business console to begin using these enhanced authentication capabilities today. 
 
About the Authors 
Takeshi Kobayashi is a Senior AI/ML Solutions Architect within the Amazon Q Business team, responsible for developing advanced AI/ML solutions for enterprise customers. With over 14 years of experience at Amazon in AWS, AI/ML, and technology, Takeshi is dedicated to leveraging generative AI and AWS services to build innovative solutions that address customer needs. Based in Seattle, WA, Takeshi is passionate about pushing the boundaries of artificial intelligence and machine learning technologies. 
Siddhant Gupta is a Software Development Manager on the Amazon Q team based in Seattle, WA. He is driving innovation and development in cutting-edge AI-powered solutions. 
Akhilesh Amara is a Software Development Engineer on the Amazon Q team based in Seattle, WA. He is contributing to the development and enhancement of intelligent and innovative AI tools.
‚Ä¢ Unlocking the future of professional services: How Proofpoint uses Amazon Q Business
  This post was written with Stephen Coverdale and Alessandra Filice of Proofpoint. 
At the forefront of cybersecurity innovation, Proofpoint has redefined its professional services by integrating Amazon Q Business, a fully managed, generative AI powered assistant that you can configure to answer questions, provide summaries, generate content, and complete tasks based on your enterprise data. This synergy has transformed how Proofpoint delivers value to its customers, optimizing service efficiency and driving useful insights. In this post, we explore how Amazon Q Business transformed Proofpoint‚Äôs professional services, detailing its deployment, functionality, and future roadmap. 
We started this journey in January 2024 and launched production use within the services team in October 2024. Since that time, the active users have achieved a 40% productivity increase in administrative tasks, with Amazon Q Apps now saving us over 18,300 hours annually. The impact has been significant given that consultants typically spend about 12 hours per week on non-call administrative tasks. 
The time savings are evident in several key areas: 
 
 Over 10,000 hours annually through apps that support customer data analysis and deliver insights and recommendations 
 3,000 hours per year saved in executive reporting generation, which will likely double when we deploy automated presentation creation with AI-powered hyper-personalization 
 1,000 hours annually on meeting summarizations 
 300 hours per year preparing renewal justifications‚Äîbut the real benefit here is how quickly we can now turn around customized content at a scale we couldn‚Äôt achieve before 
 
Beyond these time savings, we‚Äôve seen benefits in upskilling our teams with better access to knowledge, delivering additional value to clients, improving our renewal processes, and gaining deeper customer understanding through Amazon Q Business. This productivity increase means our consultants can focus more time on strategic initiatives and direct customer engagement, ultimately delivering higher value to our customers. 
A paradigm shift in cybersecurity service delivery 
Proofpoint‚Äôs commitment to evolving our customer interactions into delightful experiences led us to adopt Amazon Q Business across our services and consulting teams. This integration has enabled: 
 
 Enhanced productivity ‚Äì Consultants save significant time on repetitive tasks, reallocating focus to high-value client interactions 
 Deeper insights ‚Äì AI-driven analytics provide a granular understanding of customer environments 
 Scalable solutions ‚Äì Tailored applications (Amazon Q Apps) empower consultants to address customer needs effectively 
 
Transformative use cases through Amazon Q Apps 
Amazon Q Business has been instrumental in our deployment, and we‚Äôve developed over 30 custom Amazon Q Apps, each addressing specific challenges within our service portfolio. 
Some of the use cases are: 
1. Follow-up email automation 
 
 Challenge ‚Äì Consultants spent hours drafting follow-up emails post-meetings 
 Solution ‚Äì Amazon Q Apps generates curated follow-up emails outlining discussion points and action items 
 Impact ‚Äì Consistent customer tracking, reduced response time, and multilingual capabilities for global reach 
 
2. Health check analysis 
 
 Challenge ‚Äì Analyzing complex customer health assessments and understanding customer changes over time 
 Solution ‚Äì Amazon Q Apps compares files, providing an overview of key changes between two health checks, and a generated summary to help support customer business reviews (CBRs) and progress updates 
 Impact ‚Äì Improved communication and enhanced customer satisfaction 
 
3. Renewal justifications 
 
 Challenge ‚Äì Time-intensive preparation for renewal discussions 
 Solution ‚Äì Tailored renewal justification points crafted to demonstrate the value we‚Äôre delivering 
 Impact ‚Äì Scalable, targeted value articulation, fostering customer retention 
 
4. Drafting custom responses 
 
 Challenge ‚Äì Providing in-depth and specific responses for customer inquiries 
 Solution ‚Äì Amazon Q Apps creates a personalized email draft using our best practices and documentation 
 Impact ‚Äì Faster, more accurate communication 
 
The following diagram shows the Proofpoint use cases for Amazon Q Business. 
 
The following diagram shows the Proofpoint implementation. Proofpoint Chat UI is the front end that connects to Amazon Q Business, which connects to data sources in Amazon Simple Storage Service (Amazon S3), Amazon Redshift, Microsoft SharePoint, and Totango. 
 
Data strategy: Laying the foundation to a successful deployment 
Proofpoint‚Äôs successful integration of Amazon Q Business followed a comprehensive data strategy and a phased deployment approach. The journey involved crucial data preparation and documentation overhaul with key aspects noted below. 
Quality documentation: 
 
 Conducted thorough review of existing documentation 
 Organized and added metadata to our documentation for improved accessibility 
 Established vetting process for new documents 
 
Knowledge capture: 
 
 Developed processes to document tribal knowledge 
 Created strategies for ongoing knowledge enrichment 
 Established metadata tagging standards for improved searchability 
 
We‚Äôve primarily used Microsoft SharePoint document libraries to manage and support this process, and we‚Äôre now replicating this model as we onboard additional teams. Conducting sufficient testing that Amazon Q Business remains accurate is a key to maintaining the high efficacy we‚Äôve seen from the results. 
Going forward, we‚Äôre also expanding our data strategy to capture more information and insights into our customer journey. We want to make more data sources available to Amazon Q Business to expand this project scope so it covers more work tasks and more teams. 
Journey of our successful Amazon Q Business rollout 
Through our AWS Enterprise Support relationship, Proofpoint received full support on this project from the AWS account team, who helped us evaluate in detail the viability of the project and use expert technical resources. They engaged fully to help our teams with the use of service features and functionality and gain early usage of new feature previews. These helped us optimize and align our development timelines with the service roadmaps. 
We established a rigorous vetting process for new documents to maintain data quality and developed strategies to document institutional knowledge. This made sure our AI assistant was trained in the most accurate and up-to-date information. This process enlightened us to the full benefits of Amazon Q Business. 
The pilot and discovery phases were critical in shaping our AI strategy. We quickly identified the limitations of solely having the chat functionality and recognized the game-changing potential of Amazon Q Apps. To make sure we were addressing real needs, we conducted in-depth interviews with consultants to determine pain points so we could then invest in developing the Amazon Q Apps that would provide the most benefits and time savings. App development and refinement became a central focus of our efforts. We spent a significant amount of time prompt engineering our apps to provide consistent high-quality results that would provide practical value to our users and encourage them to adopt the apps as part of their processes. We also continued updating the weighting of our documents, using the metadata to enhance the output. This additional work upfront led to a successful deployment. 
Lessons learned 
Throughout our journey of integrating Amazon Q Business, we‚Äôve gleaned valuable lessons that have shaped our approach to AI implementation within our services and consulting areas. One of the most compelling insights is the importance of a robust data strategy. We‚Äôve learned that AI is only as smart as we make it, and the quality of data fed into the system directly impacts its performance. This realization led us to invest significant time in identifying avenues to make our AI smarter, with a focus on developing a clear data strategy across our services and consulting teams to make sure we realize the full benefits of AI. We also discovered that having AI thought leaders embedded within our services function is key to the success of AI implementation, to bring that necessary understanding of both the technology and our business processes. 
Another lesson was that time investment is required to get the most out of Amazon Q Business. The customization and ongoing management are key to delivering optimal results. We found that creating custom apps is the most effective route to adoption. Amazon Q Business features no-code simplicity for creating the apps by business-oriented teams instead of programmers. The prompt engineering required to provide high-quality and consistent results is a time-intensive process. This underscores the need for dedicated resources with expertise in AI, our business, and our processes. 
Experimentation on agentic features 
Amazon Q Business has taken a significant leap forward in enhancing workplace productivity with the introduction of an intelligent orchestration feature for Amazon Q Business. This feature transforms how users interact with their enterprise data and applications by automatically directing queries to appropriate data sources and plugins. Instead of manually switching between different work applications, users now seamlessly interact with popular business tools such as Jira, Salesforce, ServiceNow, Smartsheet, and PagerDuty through a single conversational interface. The feature uses Retrieval Augmented Generation (RAG) data for enterprise-specific knowledge and works with both built-in and custom plugins, making it a powerful addition to the workplace technology landscape. We‚Äôre experimenting on agentic integration with Totango and a few other custom plugins with Orchestrator and are seeing good results. 
Looking ahead 
Looking ahead, Proofpoint has outlined an ambitious roadmap for expanding our Amazon Q Business deployment across our customer-facing teams. The key priorities of this roadmap include: 
 
 Expansion of data sources ‚Äì Proofpoint will be working to incorporate more data sources, helping to unify our information across our teams and allowing for a more comprehensive view of our customers. This will include using the many Amazon Q Business data source connectors, such as Salesforce, Confluence, Amazon S3, and Smartsheet, and will expand the impact of our Amazon Q Apps. 
 Using Amazon Q Business actions ‚Äì Building on our successful Amazon Q deployment, Proofpoint is set to enhance its tool integration strategy to further streamline operations and reduce administrative burden. We plan to take advantage of Amazon Q Business actions using the plugin capabilities so we can post data into our different customer success tools. With this integration approach, we can take note of more detailed customer insights. For example, we can capture project progress from a meeting transcript and store it in our customer success tool to identify sentiment concerns. We‚Äôll be able to gather richer data about our customer engagements, which translates to providing even greater and more personalized service to our customers. 
 Automated workflows ‚Äì Future enhancements will include expanded automation and integrations to further streamline our service delivery. By combining our enhanced data sources with automated actions, we can make sure our teams receive the right information and insights at the right time while reducing manual intervention. 
 Data strategy enhancement ‚Äì We‚Äôll continue to refine our data strategy across Proofpoint Premium Services, establishing best practices for documentation and implementing systems to record undocumented knowledge. This will include developing better ways to understand and document our customer journey through the integration of various tools and data sources. 
 
Security and compliance 
As a cybersecurity leader, Proofpoint makes sure that AI processes comply with strict security and privacy standards: 
 
 Secure integration ‚Äì Amazon Q Apps seamlessly connects to our various data sources, safeguarding sensitive data 
 Continuous monitoring ‚Äì Embedded feedback mechanisms and daily synchronization uphold quality control 
 
Conclusion: Redefining cybersecurity services 
Amazon Q Business exemplifies Proofpoint‚Äôs innovative approach to cybersecurity. With Amazon Q Business AI capabilities, we‚Äôre elevating our customer experience and scaling our service delivery. 
As we refine and expand this program, our focus remains unwavering: delivering unmatched value and protection to our clients. Through Amazon Q Business, Proofpoint continues to set the benchmark in cybersecurity services, making sure organizations can navigate an increasingly complex threat landscape with confidence. 
Learn more 
 
 More Amazon Q Business Blogs 
 Amazon Q main product page 
 Amazon Q details for IT pros and developers 
 Get started with Amazon Q 
 
 
 
About the Authors 
Stephen Coverdale is a Senior Manager, Professional Services at Proofpoint. In addition to managing a Professional Services team, he leads an AI integration team developing and driving a strategy to leverage the transformative capabilities of AI within Proofpoint‚Äôs services teams to enhance Proofpoint‚Äôs client engagements. 
Alessandra Filice is a Senior AI Integration Specialist at Proofpoint, where she plays a lead role in implementing AI solutions across Proofpoint‚Äôs services teams. In this role, she specializes in developing and deploying AI capabilities to enhance service delivery and operational efficiency. Working closely with stakeholders across Proofpoint, she identifies opportunities for AI implementation, designs innovative solutions, and facilitates successful integration of AI technologies. 
Ram Krishnan is a Senior Technical Account Manager at AWS. He serves as a key technical resource for independent software vendor (ISV) customers, providing help and guidance across their AWS needs including AI/ML focus ‚Äî from adoption and migration to design, deployment, and optimizations across AWS services. 
Abhishek Maligehalli Shivalingaiah is a Senior Generative AI Solutions Architect at AWS, specializing in Amazon Q Business. With a deep passion for using agentic AI frameworks to solve complex business challenges, he brings nearly a decade of expertise in developing data and AI solutions that deliver tangible value for enterprises. Beyond his professional endeavors, Abhishek is an artist who finds joy in creating portraits of family and friends, expressing his creativity through various artistic mediums.
‚Ä¢ Enhancing LLM accuracy with Coveo Passage Retrieval on Amazon Bedrock
  This post is co-written with Keith Beaudoin and Nicolas Bordeleau from Coveo. 
As generative AI transforms business operations, enterprises face a critical challenge: how can they help large language models (LLMs) provide accurate and trustworthy responses? Without reliable data foundations, these AI models can generate misleading or inaccurate responses, potentially reducing user trust and organizational credibility. 
As an AWS Partner, Coveo addresses this challenge with its Passage Retrieval API. This solution enhances the reliability of LLM-powered applications by providing them with relevant, context-aware enterprise knowledge to inform generated responses. In Retrieval Augmented Generation (RAG) systems, the retrieval process is the most complex component. It requires extracting the most relevant, precise information from enterprise data sources. By integrating the Coveo AI-Relevance Platform with Amazon Bedrock Agents, organizations gain access to an industry-leading enterprise search service featuring a secured, unified hybrid index that respects enterprise permission models and offers robust connectivity. The Coveo AI-Relevance Platform uses machine learning (ML) and in-depth usage analytics to continuously optimize relevance. This enables Amazon Bedrock Agents to deliver grounded, contextually relevant responses tailored to complex enterprise content. 
The Coveo AI-Relevance Platform is an industry-leading service that connects and unifies the content of cloud and on-premises repositories in a single index, making it fast and simple to find relevant content quickly. Its ML algorithms analyze user behavior, in-app context, as well as profile and permissions data to retrieve personalized search results and recommendations. It then aggregates and reports insights back to content and experience managers. By integrating seamlessly with enterprise systems (such as websites, knowledge bases, and CRM) and enforcing security permissions, Coveo helps users get the most pertinent information while maintaining data protection. 
In this post, we show how to deploy Coveo‚Äôs Passage Retrieval API as an Amazon Bedrock Agents action group to enhance response accuracy, so Coveo users can use their current index to rapidly deploy new generative experiences across their organization. 
Coveo‚Äôs Passage Retrieval API 
The Coveo Passage Retrieval API enhances LLM applications by passing ranked text passages (or chunks) retrieved from the unified index, along with appropriate metadata such as source URLs for citations, so that generated answers are grounded in an organization‚Äôs proprietary knowledge. Built on Coveo‚Äôs unified hybrid index, the Passage Retrieval API applies a two-stage retrieval process to extract the most relevant passages from structured and unstructured content sources, providing accuracy, security, and real-time relevance. The following diagram illustrates these stages. 
 
The process consists of the following key components: 
 
 Relevant passage extraction using a two-stage retrieval process ‚Äì In the first retrieval stage, Coveo‚Äôs hybrid search system is used to identify the most relevant documents. Then, it extracts the most relevant text passages from these documents, along with ranking scores, citation links, and other key metadata. This two-stage approach allows Coveo to identify accessible and relevant documents from the sources and then more precisely identify the most relevant passages. 
 Enhanced search results with hybrid ranking ‚Äì Combining semantic (vector) search, and lexical (keyword) matching helps Coveo retrieve the right information in the right context. 
 ML relevancy ‚Äì AI continuously learns from user interactions, tailoring retrieval to each user‚Äôs journey, behavior, and profile for context-aware responses. 
 Content connected across sources with a unified index ‚Äì A centralized hybrid index connects structured and unstructured content across sources. This unified hybrid index provides better multi-source relevancy than a federated search approach by applying the ranking function across sources. Coveo also provides a robust library of pre-built connectors to maintain seamless integration with third-party services (such as Salesforce, SharePoint, and Google Drive), facilitating data freshness with automatic updates for real-time retrieval. 
 Analytics and insights for performance tracking ‚Äì With events tracking through the Data Platform and Knowledge Hub, you can see exactly how your generated answers perform, where information is missing or underused, and which content needs tuning. With those insights, you can boost answer quality and drive measurable business impact. 
 Enterprise-grade security ‚Äì Coveo provides the native permission model of each connected content source by importing item‚Äëlevel permissions at crawl time through an early‚Äëbinding approach. Resolving access rights before indexing helps prevent data leakage and boosts search performance by filtering out items a user can‚Äôt see before a query is run. 
 Redefining enterprise-ready RAG ‚Äì Coveo reimagines what RAG systems can achieve by going beyond basic vector search, offering a dynamic and intelligent retrieval pipeline designed for enterprise needs. At its core is a unified hybrid index that seamlessly connects structured, unstructured, and permission-sensitive data. This foundation, combined with real-time ML-driven tuning, helps validate that every response delivered to an LLM is relevant and securely grounded in the right context. 
 
Through native access control enforcement, behavior-based relevance adjustment, and deep analytics into content usage and performance, Coveo empowers organizations to continuously refine their generative AI experiences. Backed by consistent recognition from leading analyst firms such as Gartner, Forrester, and IDC, Coveo delivers a reliable, secure, and scalable foundation for enterprise-grade generative AI. 
Solution overview 
This solution demonstrates how you can integrate Coveo‚Äôs Passage Retrieval API with Amazon Bedrock Agents to build LLM-powered assistants that deliver accurate, context-aware, and grounded responses. It applies broadly across use cases where agents need to access proprietary knowledge, whether to support customers, assist employees, or empower sales and service teams. This approach helps AI responses stay grounded in your most relevant and trusted information across structured and unstructured content. Amazon Bedrock Agents acts as the intelligent backbone of this solution, interpreting natural language queries and orchestrating the retrieval process to deliver grounded, contextually relevant insights. For this use case, the agent is equipped to answer a user‚Äôs questions on Coveo services capabilities, API documentation, and integration guides. The agent is designed to fetch precise passages from enterprise content in response to user questions, enabling applications such as virtual assistants, support copilots, or internal knowledge bots. By using structured definitions and instructions, the agent understands when and how to trigger Coveo‚Äôs Passage Retrieval API, making sure that LLM-generated responses are backed by accurate and trusted content. 
The action group defines the structured API operations that the Amazon Bedrock agent can invoke. Using OpenAPI specifications, it defines the interface between the agent and AWS Lambda functions. In this use case, fetching relevant passages based on the user‚Äôs search intent is the only available operation. 
The following diagram illustrates the solution architecture. 
 
For this demo, the agent is set with the following instructions during creation: 
 
 You will act as an expert on Coveo documentation, platform, APIs, analytics, and integration guides.
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;Use the CoveoPRAPI Action Group to retrieve relevant information on Coveo documentation, platform, APIs, analytics, and integration guides.
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;Summarize the information retrieved from the Action Group response clearly and accurately.
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;OuputFormatting guidelines:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;- Provide clear, direct answers without introductory phrases such as ‚ÄúAs an expert,‚Äù ‚ÄúSure,‚Äù or ‚ÄúHere is...‚Äù
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;- When appropriate, organize content using:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;- Numbered or bulleted lists
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;- Structured sections (e.g., features, steps, key points)
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;Keep answers concise, informative, and free of conversational filler. 
 
The Lambda function defined in the action group is essential for enabling the Amazon Bedrock agent to call the Coveo Passage Retrieval API. The Lambda function performs the following tasks: 
 
 Receives incoming requests from the Amazon Bedrock agent 
 Queries the Coveo Passage Retrieval API using the user‚Äôs input 
 Returns the relevant search results back to the Amazon Bedrock agent 
 
Lastly, the Coveo AI-Relevance Platform provides indexed and structured search results through the Passage Retrieval API. 
Prerequisites 
Before you begin, you must have the following prerequisites: 
 
 An AWS account with AWS Identity and Access Management (IAM) permissions to deploy an AWS CloudFormation stack 
 Access to models in Amazon Bedrock 
 A Coveo index created and ready to use 
 The following Coveo information: 
   
   Coveo organization ID 
   Coveo search hub 
   Coveo API key 
    
 
Deploy the solution with AWS CloudFormation 
To deploy your agent, complete the following steps: 
 
 Launch the CloudFormation stack:  
 Enter a stack name and values for AgentModelID, AgentName, CoveoApiKey, CoveoOrgID, and CoveoSearchHub. 
 In the Capabilities section, select the acknowledge check box. 
 Choose Create stack. 
 Wait for the stack creation to complete. 
 Verify all resources are created on the stack details page. 
 
Test the solution 
To test the solution, complete the following steps: 
 
 On the Amazon Bedrock console, choose Agents in the navigation pane. 
 Choose the agent created by the CloudFormation stack. 
 
 
 
 Under Test, enter your question in the message box. 
 
For this demo, Coveo technical documentation (from the website) was ingested in an existing Coveo Search index. Let‚Äôs start with the query ‚ÄúWhat is the difference between Coveo Atomic and Headless?‚Äù 
Here‚Äôs how the agent answered the question when Claude 3 Haiku v1 is used as the LLM. 
 
 
 Choose Show trace in the right pane and expand Trace step 1 to see the agent‚Äôs rationale. 
 
The following screenshot demonstrates how Amazon Bedrock Agents processed and answered the question. First, it formed a rationale: 
 
 To understand the difference between Coveo Atomic and Headless, I will need to retrieve relevant information from the Coveo technical documentation using the CoveoPRAPI Action Group. 
 
Then it invokes the CoveoPRAP action group, which is specifically designed to retrieve relevant passages, through a Lambda function to make an API call to /rest/search/v3/passages/retrieve. 
This example illustrates the agent‚Äôs systematic approach to planning and executing necessary actions through the CoveoPRAP1 action group, and retrieving relevant document chunks before formulating its final response. 
 
The Lambda function code includes a debugging feature that logs each retrieved passage from the Passage Retrieval API. This logging mechanism iterates through the returned chunks, numbering them sequentially for quick reference. These logs are available in Amazon CloudWatch, so you can see exactly which passages were retrieved for each user query, and how they contributed to the final response. To visualize the logs, open the CloudWatch console, and on the Log groups page, locate the Lambda function name. 
The following screenshot shows the agent detailed logs in CloudWatch. In the logs, the Coveo Passage Retrieval API returns the five most relevant chunks to the LLM. 
 
Clean up 
To avoid ongoing charges, delete the resources deployed as part of the CloudFormation stack: 
 
 On the AWS CloudFormation console, choose Stacks in the navigation pane. 
 Choose the stack you created, then choose Delete. 
 Choose Delete stack when prompted. 
 
For more information, refer to Deleting a stack on the AWS CloudFormation console. 
Conclusion 
By integrating Amazon Bedrock capabilities with Coveo‚Äôs AI-driven retrieval, organizations can develop AI applications that provide validated responses based on their enterprise content. This approach helps reduce inaccuracies while delivering real-time, secure responses. 
We encourage you to explore pre-built examples in the GitHub repository to help you get started with Amazon Bedrock. 
To learn more about the Coveo AI-Relevance Platform and how to implement the Passage Retrieval API in your Coveo organization, refer to Passage Retrieval (CPR) implementation overview. 
 
About the authors 
Yanick Houngbedji is a Solutions Architect for Independent Software Vendors (ISV) at Amazon Web Services (AWS), based in Montr√©al, Canada. He specializes in helping customers architect and implement highly scalable, performant, and secure cloud solutions on AWS. Before joining AWS, he spent over 8 years providing technical leadership in data engineering, big data analytics, business intelligence, and data science solutions. 
Keith Beaudoin is a Senior Solution Architect at Coveo Labs. He is specialized in designing and implementing intelligent search and AI-powered relevance solutions, with expertise in Agentic solutions, cloud technologies, search architecture, and third-party integrations. Keith helps organizations harness the full potential of Coveo‚Äôs platform, optimizing digital transformation strategies for seamless and impactful search implementations that drive business value 
Nicolas Bordeleau is Head of Product Relations at Coveo. With over 19 years of experience in the search industry, including 11 years in product management, Nicolas has a keen understanding of enterprises and developers‚Äô needs related to search and information retrieval. He has applied this knowledge to develop award-winning products that fulfill those needs in innovative ways.
‚Ä¢ Train and deploy models on Amazon SageMaker HyperPod using the new HyperPod CLI and SDK
  Training and deploying large AI models requires advanced distributed computing capabilities, but managing these distributed systems shouldn‚Äôt be complex for data scientists and machine learning (ML) practitioners. The newly released command line interface (CLI) and software development kit (SDK) for Amazon SageMaker HyperPod simplify how you can use the service‚Äôs distributed training and inference capabilities. 
The SageMaker HyperPod CLI provides data scientists with an intuitive command-line experience, abstracting away the underlying complexity of distributed systems. Built on top of the SageMaker HyperPod SDK, the CLI offers straightforward commands for common workflows like launching training or fine-tuning jobs, deploying inference endpoints, and monitoring cluster performance. This makes it ideal for quick experimentation and iteration. 
For more advanced use cases requiring fine-grained control, the SageMaker HyperPod SDK enables programmatic access to customize your ML workflows. Developers can use the SDK‚Äôs Python interface to precisely configure training and deployment parameters while maintaining the simplicity of working with familiar Python objects. 
In this post, we demonstrate how to use both the CLI and SDK to train and deploy large language models (LLMs) on SageMaker HyperPod. We walk through practical examples of distributed training using Fully Sharded Data Parallel (FSDP) and model deployment for inference, showcasing how these tools streamline the development of production-ready generative AI applications. 
Prerequisites 
To follow the examples in this post, you must have the following prerequisites: 
 
 An AWS account with access to SageMaker HyperPod, Amazon Simple Storage Service (Amazon S3) and Amazon FSx for Lustre. 
 A local environment (either your local machine or a cloud-based compute environment) from which to run the SageMaker HyperPod CLI commands, configured as follows: 
   
   Operating system based on Linux or MacOS. 
   Python 3.8, 3.9, 3.10 or 3.11 installed. 
   The AWS Command Line Interface (AWS CLI) configured with the appropriate credentials to use the aforementioned services. 
    
 A SageMaker HyperPod cluster orchestrated through Amazon Elastic Kubernetes Service (Amazon EKS) running with an instance group configured with 8 ml.g5.8xlarge instances. For more information on how to create and configured a new SageMaker HyperPod cluster, refer to Creating a SageMaker HyperPod cluster with Amazon EKS orchestration. 
 An FSx for Lustre persistent volume claim (PVC) to store checkpoints. This can be created either at cluster creation time or separately. 
 
Because the use cases that we demonstrate are about training and deploying LLMs with the SageMaker HyperPod CLI and SDK, you must also install the following Kubernetes operators in the cluster: 
 
 HyperPod training operator ‚Äì For installation instructions, see Installing the training operator. 
 HyperPod inference operator ‚Äì For installation instructions, see Setting up your HyperPod clusters for model deployment and the corresponding notebook. 
 
Install the SageMaker HyperPod CLI 
First, you must install the latest version of the SageMaker HyperPod CLI and SDK (the examples in this post are based on version 3.1.0). From the local environment, run the following command (you can also install in a Python virtual environment): 
 
 # Install the HyperPod CLI and SDK
pip install sagemaker-hyperpod 
 
This command sets up the tools needed to interact with SageMaker HyperPod clusters. For an existing installation, make sure you have the latest version of the package installed (sagemaker-hyperpod&gt;=3.1.0) to be able to use the relevant set of features. To verify if the CLI is installed correctly, you can run the hyp command and check the outputs: 
 
 # Check if the HyperPod CLI is correctly installed
hyp 
 
The output will be similar to the following, and includes instructions on how to use the CLI: 
 
 Usage: hyp [OPTIONS] COMMAND [ARGS]...

Options:
&nbsp;&nbsp;--help &nbsp;Show this message and exit.

Commands:
&nbsp;&nbsp;create &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Create endpoints or pytorch jobs.
&nbsp;&nbsp;delete &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Delete endpoints or pytorch jobs.
&nbsp;&nbsp;describe &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Describe endpoints or pytorch jobs.
&nbsp;&nbsp;get-cluster-context &nbsp;Get context related to the current set cluster.
&nbsp;&nbsp;get-logs &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Get pod logs for endpoints or pytorch jobs.
&nbsp;&nbsp;get-monitoring &nbsp; &nbsp; &nbsp; Get monitoring configurations for Hyperpod cluster.
&nbsp;&nbsp;get-operator-logs &nbsp; &nbsp;Get operator logs for endpoints.
&nbsp;&nbsp;invoke &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Invoke model endpoints.
&nbsp;&nbsp;list &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; List endpoints or pytorch jobs.
&nbsp;&nbsp;list-cluster &nbsp; &nbsp; &nbsp; &nbsp; List SageMaker Hyperpod Clusters with metadata.
&nbsp;&nbsp;list-pods &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;List pods for endpoints or pytorch jobs.
&nbsp;&nbsp;set-cluster-context &nbsp;Connect to a HyperPod EKS cluster. 
 
For more information on CLI usage and the available commands and respective parameters, refer to the CLI reference documentation. 
Set the cluster context 
The SageMaker HyperPod CLI and SDK use the Kubernetes API to interact with the cluster. Therefore, make sure the underlying Kubernetes Python client is configured to execute API calls against your cluster by setting the cluster context. 
Use the CLI to list the clusters available in your AWS account: 
 
 # List all HyperPod clusters in your AWS account
hyp list-cluster
[
&nbsp;&nbsp; &nbsp;{
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"Cluster": "ml-cluster",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"Instances": [
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;{
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"InstanceType": "ml.g5.8xlarge",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"TotalNodes": 8,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"AcceleratorDevicesAvailable": 8,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"NodeHealthStatus=Schedulable": 8,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"DeepHealthCheckStatus=Passed": "N/A"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;},
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;{
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"InstanceType": "ml.m5.12xlarge",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"TotalNodes": 1,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"AcceleratorDevicesAvailable": "N/A",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"NodeHealthStatus=Schedulable": 1,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"DeepHealthCheckStatus=Passed": "N/A"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;]
&nbsp;&nbsp; &nbsp;}
] 
 
Set the cluster context specifying the cluster name as input (in our case, we use ml-cluster as &lt;cluster_name&gt;): 
 
 # Set the cluster context for subsequent commands
hyp set-cluster-context --cluster-name&nbsp;&lt;cluster_name&gt; 
 
Train models with the SageMaker HyperPod CLI and SDK 
The SageMaker HyperPod CLI provides a straightforward way to submit PyTorch model training and fine-tuning jobs to a SageMaker HyperPod cluster. In the following example, we schedule a Meta Llama 3.1 8B model training job with FSDP. 
The CLI executes training using the HyperPodPyTorchJob Kubernetes custom resource, which is implemented by the HyperPod training operator, that needs to be installed in the cluster as discussed in the prerequisites section. 
First, clone the awsome-distributed-training repository and create the Docker image that you will use for the training job: 
 
 cd ~
git clone https://github.com/aws-samples/awsome-distributed-training/
cd awsome-distributed-training/3.test_cases/pytorch/FSDP 
 
Then, log in to the Amazon Elastic Container Registry (Amazon ECR) to pull the base image and build the new container: 
 
 export&nbsp;AWS_REGION=$(aws ec2 describe-availability-zones --output text --query 'AvailabilityZones[0].[RegionName]')
export&nbsp;ACCOUNT=$(aws sts get-caller-identity --query Account --output text)
export&nbsp;REGISTRY=${ACCOUNT}.dkr.ecr.${AWS_REGION}.amazonaws.com/
docker build -f Dockerfile -t ${REGISTRY}fsdp:pytorch2.7.1 . 
 
The Dockerfile in the awsome-distributed-training repository referenced in the preceding code already contains the HyperPod elastic agent, which orchestrates lifecycles of training workers on each container and communicates with the HyperPod training operator. If you‚Äôre using a different Dockerfile, install the HyperPod elastic agent following the instructions in HyperPod elastic agent. 
Next, create a new registry for your training image if needed and push the built image to it: 
 
 # Create registry if needed
REGISTRY_COUNT=$(aws ecr describe-repositories | grep "fsdp" | wc -l)
if [ "$REGISTRY_COUNT" -eq 0 ]; then
&nbsp;&nbsp; &nbsp;aws ecr create-repository --repository-name fsdp
fi

# Login to registry
echo "Logging in to $REGISTRY ..."
aws ecr get-login-password | docker login --username AWS --password-stdin $REGISTRY

# Push image to registry
docker image push ${REGISTRY}fsdp:pytorch2.7.1 
 
After you have successfully created the Docker image, you can submit the training job using the SageMaker HyperPod CLI. 
Internally, the SageMaker HyperPod CLI will use the Kubernetes Python client to build a HyperPodPyTorchJob custom resource and then create it on the Kubernetes the cluster. 
You can modify the CLI command for other Meta Llama configurations by exchanging the --args to the desired arguments and values; examples can be found in the Kubernetes manifests in the awsome-distributed-training repository. 
In the given configuration, the training job will write checkpoints to /fsx/checkpoints on the FSx for Lustre PVC. 
 
 hyp create hyp-pytorch-job \
&nbsp;&nbsp; &nbsp;--job-name fsdp-llama3-1-8b \
&nbsp;&nbsp; &nbsp;--image ${REGISTRY}fsdp:pytorch2.7.1 \
&nbsp;&nbsp; &nbsp;--command '[
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;hyperpodrun,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;--tee=3,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;--log_dir=/tmp/hyperpod,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;--nproc_per_node=1,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;--nnodes=8,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;/fsdp/train.py
&nbsp;&nbsp; &nbsp;]' \
&nbsp;&nbsp; &nbsp;--args '[
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;--max_context_width=8192,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;--num_key_value_heads=8,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;--intermediate_size=14336,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;--hidden_width=4096,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;--num_layers=32,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;--num_heads=32,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;--model_type=llama_v3,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;--tokenizer=hf-internal-testing/llama-tokenizer,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;--checkpoint_freq=50,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;--validation_freq=25,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;--max_steps=50,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;--checkpoint_dir=/fsx/checkpoints,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;--dataset=allenai/c4,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;--dataset_config_name=en,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;--resume_from_checkpoint=/fsx/checkpoints,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;--train_batch_size=1,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;--val_batch_size=1,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;--sharding_strategy=full,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;--offload_activations=1
&nbsp;&nbsp; &nbsp;]' \
&nbsp;&nbsp; &nbsp;--environment '{"PYTORCH_CUDA_ALLOC_CONF": "max_split_size_mb:32"}' \
&nbsp;&nbsp; &nbsp;--pull-policy "IfNotPresent" \
&nbsp;&nbsp; &nbsp;--instance-type ml.g5.8xlarge \
&nbsp;&nbsp; &nbsp;--node-count 8 \
&nbsp;&nbsp; &nbsp;--tasks-per-node 1 \
&nbsp;&nbsp; &nbsp;--deep-health-check-passed-nodes-only false \
&nbsp;&nbsp; &nbsp;--max-retry 3 \
&nbsp;&nbsp; &nbsp;--volume name=shmem,type=hostPath,mount_path=/dev/shm,path=/dev/shm,read_only=false&nbsp;\
&nbsp; &nbsp;&nbsp;--volume name=fsx,type=pvc,mount_path=/fsx,claim_name=fsx-claim,read_only=false 
 
The hyp create hyp-pytorch-job command supports additional arguments, which can be discovered by running the following: 
 
 hyp create hyp-pytorch-job --help 
 
The preceding example code contains the following relevant arguments: 
 
 --command and --args offer flexibility in setting the command to be executed in the container. The command executed is hyperpodrun, implemented by the HyperPod elastic agent that is installed in the training container. The HyperPod elastic agent extends PyTorch‚Äôs ElasticAgent and manages the communication of the various workers with the HyperPod training operator. For more information, refer to HyperPod elastic agent. 
 --environment defines environment variables and customizes the training execution. 
 --max-retry indicates the maximum number of restarts at the process level that will be attempted by the HyperPod training operator. For more information, refer to Using the training operator to run jobs. 
 --volume is used to map persistent or ephemeral volumes to the container. 
 
If successful, the command will output the following: 
 
 Using version: 1.0
2025-08-12&nbsp;10:03:03,270 - sagemaker.hyperpod.training.hyperpod_pytorch_job - INFO - Successfully submitted HyperPodPytorchJob 'fsdp-llama3-1-8b'! 
 
You can observe the status of the training job through the CLI. Running hyp list hyp-pytorch-job will show the status first as Created and then as Running after the containers have been started: 
 
 NAME &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;NAMESPACE &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; STATUS &nbsp; &nbsp; &nbsp; &nbsp; AGE &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
--------------------------------------------------------------------------------
fsdp-llama3-1-8b &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;default &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Running &nbsp; &nbsp; &nbsp; &nbsp;6m &nbsp; &nbsp; &nbsp; &nbsp; 
 
To list the pods that are created by this training job, run the following command: 
 
 hyp list-pods hyp-pytorch-job --job-name fsdp-llama3-1-8b
Pods for job: fsdp-llama3-1-8b

POD NAME &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;NAMESPACE &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 
----------------------------------------------------------------------
fsdp-llama3-1-8b-pod-0 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;default &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 
fsdp-llama3-1-8b-pod-1 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;default &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;
fsdp-llama3-1-8b-pod-2&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;default &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;
fsdp-llama3-1-8b-pod-3&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;default &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;
fsdp-llama3-1-8b-pod-4&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;default &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;
fsdp-llama3-1-8b-pod-5&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;default &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;
fsdp-llama3-1-8b-pod-6&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;default &nbsp; &nbsp; &nbsp; &nbsp;
fsdp-llama3-1-8b-pod-7&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;default &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 
 
You can observe the logs of one of the training pods that get spawned by running the following command: 
 
 hyp get-logs hyp-pytorch-job --pod-name fsdp-llama3-1-8b-pod-0&nbsp;\
--job-name fsdp-llama3-1-8b
...
2025-08-12T14:59:25.069208138Z [HyperPodElasticAgent] 2025-08-12 14:59:25,069 [INFO] [rank0-restart0] /usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/agent/server/api.py:685: [default] Starting worker group 
2025-08-12T14:59:25.069301320Z [HyperPodElasticAgent] 2025-08-12 14:59:25,069 [INFO] [rank0-restart0] /usr/local/lib/python3.10/dist-packages/hyperpod_elastic_agent/hyperpod_elastic_agent.py:221: Starting workers with worker spec worker_group.spec=WorkerSpec(role='default', local_world_size=1, rdzv_handler=&lt;hyperpod_elastic_agent.rendezvous.hyperpod_rendezvous_backend.HyperPodRendezvousBackend object at 0x7f0970a4dc30&gt;, fn=None, entrypoint='/usr/bin/python3', args=('-u', '/fsdp/train.py', '--max_context_width=8192', '--num_key_value_heads=8', '--intermediate_size=14336', '--hidden_width=4096', '--num_layers=32', '--num_heads=32', '--model_type=llama_v3', '--tokenizer=hf-internal-testing/llama-tokenizer', '--checkpoint_freq=50', '--validation_freq=50', '--max_steps=100', '--checkpoint_dir=/fsx/checkpoints', '--dataset=allenai/c4', '--dataset_config_name=en', '--resume_from_checkpoint=/fsx/checkpoints', '--train_batch_size=1', '--val_batch_size=1', '--sharding_strategy=full', '--offload_activations=1'), max_restarts=3, monitor_interval=0.1, master_port=None, master_addr=None, local_addr=None)... 
2025-08-12T14:59:30.264195963Z [default0]:2025-08-12 14:59:29,968 [INFO] **main**: Creating Model 
2025-08-12T15:00:51.203541576Z [default0]:2025-08-12 15:00:50,781 [INFO] **main**: Created model with total parameters: 7392727040 (7.39 B) 
2025-08-12T15:01:18.139531830Z [default0]:2025-08-12 15:01:18 I [checkpoint.py:79] Loading checkpoint from /fsx/checkpoints/llama_v3-24steps ... 
2025-08-12T15:01:18.833252603Z [default0]:2025-08-12 15:01:18,081 [INFO] **main**: Wrapped model with FSDP 
2025-08-12T15:01:18.833290793Z [default0]:2025-08-12 15:01:18,093 [INFO] **main**: Created optimizer 
 
We elaborate on more advanced debugging and observability features at the end of this section. 
Alternatively, if you prefer a programmatic experience and more advanced customization options, you can submit the training job using the SageMaker HyperPod Python SDK. For more information, refer to the SDK reference documentation. The following code will yield the equivalent training job submission to the preceding CLI example: 
 
 import&nbsp;os
from&nbsp;sagemaker.hyperpod.training&nbsp;import&nbsp;HyperPodPytorchJob
from&nbsp;sagemaker.hyperpod.training&nbsp;import&nbsp;ReplicaSpec, Template, VolumeMounts, Spec, Containers, Resources, RunPolicy, Volumes, HostPath, PersistentVolumeClaim
from&nbsp;sagemaker.hyperpod.common.config&nbsp;import&nbsp;Metadata

REGISTRY&nbsp;=&nbsp;os.environ['REGISTRY']

# Define job specifications
nproc_per_node&nbsp;=&nbsp;"1"&nbsp;&nbsp;# Number of processes per node
replica_specs&nbsp;=&nbsp;[
&nbsp;&nbsp; &nbsp;ReplicaSpec(
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;name&nbsp;=&nbsp;"pod", &nbsp;# Replica name
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;replicas&nbsp;=&nbsp;8,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;template&nbsp;=&nbsp;Template(
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;spec&nbsp;=&nbsp;Spec(
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;containers&nbsp;=
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;[
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Containers(
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# Container name
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;name="fsdp-training-container", &nbsp;
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# Training image
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;image=f"{REGISTRY}fsdp:pytorch2.7.1", &nbsp;
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# Volume mounts
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;volume_mounts=[
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;VolumeMounts(
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;name="fsx",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;mount_path="/fsx"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;),
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;VolumeMounts(
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;name="shmem", 
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;mount_path="/dev/shm"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;)
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;],
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;env=[
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;{"name": "PYTORCH_CUDA_ALLOC_CONF", "value": "max_split_size_mb:32"},
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;],
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# Image pull policy
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;image_pull_policy="IfNotPresent",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;resources=Resources(
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;requests={"nvidia.com/gpu": "1"}, &nbsp;
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;limits={"nvidia.com/gpu": "1"}, &nbsp; 
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;),
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# Command to run
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;command=[
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"hyperpodrun",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"--tee=3",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"--log_dir=/tmp/hyperpod",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"--nproc_per_node=1",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"--nnodes=8",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"/fsdp/train.py"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;], &nbsp;
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# Script arguments
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;args&nbsp;=&nbsp;[
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;'--max_context_width=8192',
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;'--num_key_value_heads=8',
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;'--intermediate_size=14336',
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;'--hidden_width=4096',
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;'--num_layers=32',
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;'--num_heads=32',
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;'--model_type=llama_v3',
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;'--tokenizer=hf-internal-testing/llama-tokenizer',
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;'--checkpoint_freq=2',
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;'--validation_freq=25',
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;'--max_steps=50',
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;'--checkpoint_dir=/fsx/checkpoints',
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;'--dataset=allenai/c4',
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;'--dataset_config_name=en',
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;'--resume_from_checkpoint=/fsx/checkpoints',
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;'--train_batch_size=1',
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;'--val_batch_size=1',
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;'--sharding_strategy=full',
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;'--offload_activations=1'
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;]
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;)
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;],
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;volumes&nbsp;=&nbsp;[
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Volumes(
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;name="fsx",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;persistent_volume_claim=PersistentVolumeClaim(
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;claim_name="fsx-claim",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;read_only=False
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;),
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;),
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Volumes(
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;name="shmem",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;host_path=HostPath(path="/dev/shm"),
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;)
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;],
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;node_selector={
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"node.kubernetes.io/instance-type": "ml.g5.8xlarge",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;},
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;)
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;),
&nbsp;&nbsp; &nbsp;)
]
run_policy&nbsp;=&nbsp;RunPolicy(clean_pod_policy="None", job_max_retry_count=3) &nbsp;
# Create and start the PyTorch job
pytorch_job&nbsp;=&nbsp;HyperPodPytorchJob(
&nbsp;&nbsp; &nbsp;# Job name
&nbsp;&nbsp; &nbsp;metadata&nbsp;=&nbsp;Metadata(
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;name="fsdp-llama3-1-8b", &nbsp; &nbsp; 
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;namespace="default",
&nbsp;&nbsp; &nbsp;),
&nbsp;&nbsp; &nbsp;# Processes per node
&nbsp;&nbsp; &nbsp;nproc_per_node&nbsp;=&nbsp;nproc_per_node, &nbsp; 
&nbsp;&nbsp; &nbsp;# Replica specifications
&nbsp;&nbsp; &nbsp;replica_specs&nbsp;=&nbsp;replica_specs, &nbsp; &nbsp; &nbsp; &nbsp;
)
# Launch the job
pytorch_job.create() &nbsp; 
 
Debugging training jobs 
In addition to monitoring the training pod logs as described earlier, there are several other useful ways of debugging training jobs: 
 
 You can submit training jobs with an additional --debug True flag, which will print the Kubernetes YAML to the console when the job starts so it can be inspected by users. 
 You can view a list of current training jobs by running hyp list hyp-pytorch-job. 
 You can view the status and corresponding events of the job by running hyp describe hyp-pytorch-job ‚Äîjob-name fsdp-llama3-1-8b. 
 If the HyperPod observability stack is deployed to the cluster, run hyp get-monitoring --grafana and hyp get-monitoring --prometheus to get the Grafana dashboard and Prometheus workspace URLs, respectively, to view cluster and job metrics. 
 To monitor GPU utilization or view directory contents, it can be useful to execute commands or open an interactive shell into the pods. You can run commands in a pod by running, for example, kubectl exec -it&lt;pod-name&gt;-- nvtop to run nvtop for visibility into GPU utilization. You can open an interactive shell by running kubectl exec -it&lt;pod-name&gt;-- /bin/bash. 
 The logs of the HyperPod training operator controller pod can have valuable information about scheduling. To view them, run kubectl get pods -n aws-hyperpod | grep hp-training-controller-manager to find the controller pod name and run kubectl logs -n aws-hyperpod&lt;controller-pod-name&gt; to view the corresponding logs. 
 
Deploy models with the SageMaker HyperPod CLI and SDK 
The SageMaker HyperPod CLI provides commands to quickly deploy models to your SageMaker HyperPod cluster for inference. You can deploy both foundation models (FMs) available on Amazon SageMaker JumpStart as well as custom models with artifacts that are stored on Amazon S3 or FSx for Lustre file systems. 
This functionality will automatically deploy the chosen model to the SageMaker HyperPod cluster through Kubernetes custom resources, which are implemented by the HyperPod inference operator, that needs to be installed in the cluster as discussed in the prerequisites section. It is optionally possible to automatically create a SageMaker inference endpoint as well as an Application Load Balancer (ALB), which can be used directly using HTTPS calls with a generated TLS certificate to invoke the model. 
Deploy SageMaker JumpStart models 
You can deploy an FM that is available on SageMaker JumpStart with the following command: 
 
 hyp create hyp-jumpstart-endpoint \
&nbsp;&nbsp;--model-id deepseek-llm-r1-distill-qwen-1-5b \
&nbsp;&nbsp;--instance-type ml.g5.8xlarge \
&nbsp;&nbsp;--endpoint-name \
&nbsp;&nbsp;--tls-certificate-output-s3-uri s3://&lt;certificate-bucket&gt;/ \
&nbsp;&nbsp;--namespace&nbsp;default 
 
The preceding code includes the following parameters: 
 
 --model-id is the model ID in the SageMaker JumpStart model hub. In this example, we deploy a DeepSeek R1-distilled version of Qwen 1.5B, which is available on SageMaker JumpStart. 
 --instance-type is the target instance type in your SageMaker HyperPod cluster where you want to deploy the model. This instance type must be supported by the chosen model. 
 --endpoint-name is the name that the SageMaker inference endpoint will have. This name must be unique. SageMaker inference endpoint creation is optional. 
 --tls-certificate-output-s3-uri is the S3 bucket location where the TLS certificate for the ALB will be stored. This can be used to directly invoke the model through HTTPS. You can use S3 buckets that are accessible by the HyperPod inference operator IAM role. 
 --namespace is the Kubernetes namespace the model will be deployed to. The default value is set to default. 
 
The CLI supports more advanced deployment configurations, including auto scaling, through additional parameters, which can be viewed by running the following command: 
 
 hyp create hyp-jumpstart-endpoint --help 
 
If successful, the command will output the following: 
 
 Creating JumpStart model and sagemaker endpoint. Endpoint name: deepseek-distill-qwen-endpoint-cli.
&nbsp;The process may take a few minutes... 
 
After a few minutes, both the ALB and the SageMaker inference endpoint will be available, which can be observed through the CLI. Running hyp list hyp-jumpstart-endpoint will show the status first as DeploymentInProgress and then as DeploymentComplete when the endpoint is ready to be used: 
 
 | name &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | namespace &nbsp; | labels &nbsp; | status &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; |
|------------------------------------|-------------|----------|--------------------|
| deepseek-distill-qwen-endpoint-cli | default &nbsp; &nbsp; | &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| DeploymentComplete | 
 
To get additional visibility into the deployment pod, run the following commands to find the pod name and view the corresponding logs: 
 
 hyp list-pods hyp-jumpstart-endpoint&nbsp;--namespace &lt;namespace&gt;
hyp get-logs&nbsp;hyp-jumpstart-endpoint --namespace &lt;namespace&gt;&nbsp;--pod-name &lt;model-pod-name&gt; 
 
The output will look similar to the following: 
 
 2025-08-12T15:53:14.042031963Z WARN &nbsp;PyProcess W-195-model-stderr: Capturing CUDA graph shapes: 100%|??????????| 35/35 [00:18&lt;00:00, &nbsp;1.63it/s]
2025-08-12T15:53:14.042257357Z WARN &nbsp;PyProcess W-195-model-stderr: Capturing CUDA graph shapes: 100%|??????????| 35/35 [00:18&lt;00:00, &nbsp;1.94it/s]
2025-08-12T15:53:14.042297298Z INFO &nbsp;PyProcess W-195-model-stdout: INFO 08-12 15:53:14 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 26.18 seconds
2025-08-12T15:53:15.215357997Z INFO &nbsp;PyProcess Model [model] initialized.
2025-08-12T15:53:15.219205375Z INFO &nbsp;WorkerThread Starting worker thread WT-0001 for model model (M-0001, READY) on device gpu(0)
2025-08-12T15:53:15.221591827Z INFO &nbsp;ModelServer Initialize BOTH server with: EpollServerSocketChannel.
2025-08-12T15:53:15.231404670Z INFO &nbsp;ModelServer BOTH API bind to: http://0.0.0.0:8080 
 
You can invoke the SageMaker inference endpoint you created through the CLI by running the following command: 
 
 hyp invoke hyp-jumpstart-endpoint \
&nbsp;&nbsp; &nbsp;--endpoint-name deepseek-distill-qwen-endpoint-cli \ &nbsp; &nbsp; &nbsp; 
&nbsp;&nbsp; &nbsp;--body '{"inputs":"What is the capital of USA?"}' 
 
You will get an output similar to the following: 
 
 {"generated_text": " What is the capital of France? What is the capital of Japan? What is the capital of China? What is the capital of Germany? What is"} 
 
Alternatively, if you prefer a programmatic experience and advanced customization options, you can use the SageMaker HyperPod Python SDK. The following code will yield the equivalent deployment to the preceding CLI example: 
 
 from&nbsp;sagemaker.hyperpod.inference.config.hp_jumpstart_endpoint_config&nbsp;import&nbsp;Model, Server, SageMakerEndpoint, TlsConfig
from&nbsp;sagemaker.hyperpod.inference.hp_jumpstart_endpoint&nbsp;import&nbsp;HPJumpStartEndpoint

model=Model(
&nbsp;&nbsp; &nbsp;model_id='deepseek-llm-r1-distill-qwen-1-5b',
)

server=Server(
&nbsp;&nbsp; &nbsp;instance_type='ml.g5.8xlarge',
)

endpoint_name=SageMakerEndpoint(name='deepseek-distill-qwen-endpoint-cli')

tls_config=TlsConfig(tls_certificate_output_s3_uri='s3://&lt;certificate-bucket&gt;')

js_endpoint=HPJumpStartEndpoint(
&nbsp;&nbsp; &nbsp;model=model,
&nbsp;&nbsp; &nbsp;server=server,
&nbsp;&nbsp; &nbsp;sage_maker_endpoint=endpoint_name,
&nbsp;&nbsp; &nbsp;tls_config=tls_config,
&nbsp;&nbsp; &nbsp;namespace="default"
)

js_endpoint.create()  
 
Deploy custom models 
You can also use the CLI to deploy custom models with model artifacts stored on either Amazon S3 or FSx for Lustre. This is useful for models that have been fine-tuned on custom data. You must provide the storage location of the model artifacts as well as a container image for inference that is compatible with the model artifacts and SageMaker inference endpoints. In the following example, we deploy a TinyLlama 1.1B model from Amazon S3 using the DJL Large Model Inference container image. 
In preparation, download the model artifacts locally and push them to an S3 bucket: 
 
 # Install huggingface-hub if not present on your machine
pip install huggingface-hub

# Download model
hf&nbsp;download TinyLlama/TinyLlama-1.1B-Chat-v1.0 --local-dir ./tinyllama-1.1b-chat

# Upload to S3
aws s3 cp ./tinyllama s3://&lt;model-bucket&gt;/models/tinyllama-1.1b-chat/ --recursive 
 
Now you can deploy the model with the following command: 
 
 hyp create hyp-custom-endpoint \
&nbsp; &nbsp; --endpoint-name my-custom-tinyllama-endpoint \
&nbsp; &nbsp; --model-name tinyllama \
&nbsp; &nbsp; --model-source-type s3 \
&nbsp; &nbsp; --model-location models/tinyllama-1.1b-chat/&nbsp;\
&nbsp; &nbsp; --s3-bucket-name &lt;model-bucket&gt; \
&nbsp; &nbsp; --s3-region &lt;model-bucket-region&gt;&nbsp;\
&nbsp; &nbsp; --instance-type ml.g5.8xlarge \
&nbsp; &nbsp; --image-uri 763104351884.dkr.ecr.us-west-2.amazonaws.com/djl-inference:0.33.0-lmi15.0.0-cu128 \
&nbsp; &nbsp; --container-port 8080 \
&nbsp; &nbsp; --model-volume-mount-name modelmount \
&nbsp;&nbsp;&nbsp;&nbsp;--tls-certificate-output-s3-uri s3://&lt;certificate-bucket&gt;/ \
&nbsp;&nbsp;&nbsp;&nbsp;--namespace default 
 
The preceding code contains the following key parameters: 
 
 --model-name is the name of the model that will be created in SageMaker 
 --model-source-type specifies either fsx or s3 for the location of the model artifacts 
 --model-location specifies the prefix or folder where the model artifacts are located 
 --s3-bucket-name and ‚Äîs3-region specify the S3 bucket name and AWS Region, respectively 
 --instance-type, --endpoint-name, --namespace, and --tls-certificate behave the same as for the deployment of SageMaker JumpStart models 
 
Similar to SageMaker JumpStart model deployment, the CLI supports more advanced deployment configurations, including auto scaling, through additional parameters, which you can view by running the following command: 
 
 hyp create hyp-custom-endpoint --help 
 
If successful, the command will output the following: 
 
 Creating sagemaker model and endpoint. Endpoint name: my-custom-tinyllama-endpoint.
&nbsp;The process may take a few minutes... 
 
After a few minutes, both the ALB and the SageMaker inference endpoint will be available, which you can observe through the CLI. Running hyp list hyp-custom-endpoint will show the status first as DeploymentInProgress and as DeploymentComplete when the endpoint is ready to be used: 
 
 | name &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | namespace &nbsp; | labels &nbsp; | status &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; |
|------------------------------|-------------|----------|----------------------|
| my-custom-tinyllama-endpoint | default &nbsp; &nbsp; | &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| DeploymentComplete&nbsp; &nbsp;| 
 
To get additional visibility into the deployment pod, run the following commands to find the pod name and view the corresponding logs: 
 
 hyp list-pods hyp-custom-endpoint&nbsp;--namespace &lt;namespace&gt;
hyp get-logs&nbsp;hyp-custom-endpoint --namespace &lt;namespace&gt;&nbsp;--pod-name &lt;model-pod-name&gt; 
 
The output will look similar to the following: 
 
 ‚îÇ INFO &nbsp;PyProcess W-196-model-stdout: INFO 08-12 16:00:36 [monitor.py:33] torch.compile takes 29.18 s in total &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;‚îÇ
‚îÇ INFO &nbsp;PyProcess W-196-model-stdout: INFO 08-12 16:00:37 [kv_cache_utils.py:634] GPU KV cache size: 809,792 tokens &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ‚îÇ
‚îÇ INFO &nbsp;PyProcess W-196-model-stdout: INFO 08-12 16:00:37 [kv_cache_utils.py:637] Maximum concurrency for 2,048 tokens per request: 395.41x &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ‚îÇ
‚îÇ INFO &nbsp;PyProcess W-196-model-stdout: INFO 08-12 16:00:59 [gpu_model_runner.py:1626] Graph capturing finished in 22 secs, took 0.37 GiB &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ‚îÇ
‚îÇ INFO &nbsp;PyProcess W-196-model-stdout: INFO 08-12 16:00:59 [core.py:163] init engine (profile, create kv cache, warmup model) took 59.39 seconds &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ‚îÇ
‚îÇ INFO &nbsp;PyProcess W-196-model-stdout: INFO 08-12 16:00:59 [core_client.py:435] Core engine process 0 ready. &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ‚îÇ
‚îÇ INFO &nbsp;PyProcess Model [model] initialized. &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;‚îÇ
‚îÇ INFO &nbsp;WorkerThread Starting worker thread WT-0001 for model model (M-0001, READY) on device gpu(0) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;‚îÇ
‚îÇ INFO &nbsp;ModelServer Initialize BOTH server with: EpollServerSocketChannel. &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;‚îÇ
‚îÇ INFO &nbsp;ModelServer BOTH API bind to: http://0.0.0.0:8080&nbsp; 
 
You can invoke the SageMaker inference endpoint you created through the CLI by running the following command: 
 
 hyp invoke hyp-custom-endpoint \
&nbsp;&nbsp; &nbsp;--endpoint-name my-custom-tinyllama-endpoint \ &nbsp; &nbsp; &nbsp; 
&nbsp;&nbsp; &nbsp;--body '{"inputs":"What is the capital of USA?"}' 
 
You will get an output similar to the following: 
 
 {"generated_text": " What is the capital of France? What is the capital of Japan? What is the capital of China? What is the capital of Germany? What is"} 
 
Alternatively, you can deploy using the SageMaker HyperPod Python SDK. The following code will yield the equivalent deployment to the preceding CLI example: 
 
 from sagemaker.hyperpod.inference.config.hp_endpoint_config import S3Storage, ModelSourceConfig, TlsConfig, EnvironmentVariables, ModelInvocationPort, ModelVolumeMount, Resources, Worker
from sagemaker.hyperpod.inference.hp_endpoint import HPEndpoint

model_source_config = ModelSourceConfig(
&nbsp;&nbsp; &nbsp;model_source_type='s3',
&nbsp;&nbsp; &nbsp;model_location="models/tinyllama-1.1b-chat/",
&nbsp;&nbsp; &nbsp;s3_storage=S3Storage(
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;bucket_name='&lt;model-bucket&gt;',
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;region='&lt;model-bucket-region&gt;',
&nbsp;&nbsp; &nbsp;),
)

worker = Worker(
&nbsp;&nbsp; &nbsp;image='763104351884.dkr.ecr.us-west-2.amazonaws.com/djl-inference:0.33.0-lmi15.0.0-cu128',
&nbsp;&nbsp; &nbsp;model_volume_mount=ModelVolumeMount(
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;name='modelmount',
&nbsp;&nbsp; &nbsp;),
&nbsp;&nbsp; &nbsp;model_invocation_port=ModelInvocationPort(container_port=8080),
&nbsp;&nbsp; &nbsp;resources=Resources(
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;requests={"cpu": "30000m", "nvidia.com/gpu": 1, "memory": "100Gi"},
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;limits={"nvidia.com/gpu": 1}
&nbsp;&nbsp; &nbsp;),
)

tls_config = TlsConfig(tls_certificate_output_s3_uri='s3://&lt;certificate-bucket&gt;/')

custom_endpoint = HPEndpoint(
&nbsp;&nbsp; &nbsp;endpoint_name='my-custom-tinyllama-endpoint',
&nbsp;&nbsp; &nbsp;instance_type='ml.g5.8xlarge',
&nbsp;&nbsp; &nbsp;model_name='tinyllama', &nbsp;
&nbsp;&nbsp; &nbsp;tls_config=tls_config,
&nbsp;&nbsp; &nbsp;model_source_config=model_source_config,
&nbsp;&nbsp; &nbsp;worker=worker,
)

custom_endpoint.create() 
 
Debugging inference deployments 
In addition to the monitoring of the inference pod logs, there are several other useful ways of debugging inference deployments: 
 
 You can access the HyperPod inference operator controller logs through the SageMaker HyperPod CLI. Run hyp get-operator-logs&lt;hyp-custom-endpoint/hyp-jumpstart-endpoint&gt;‚Äîsince-hours 0.5 to access the operator logs for custom and SageMaker JumpStart deployments, respectively. 
 You can view a list of inference deployments by running hyp list&lt;hyp-custom-endpoint/hyp-jumpstart-endpoint&gt;. 
 You can view the status and corresponding events of deployments by running hyp describe&lt;hyp-custom-endpoint/hyp-jumpstart-endpoint&gt;--name&lt;deployment-name&gt; to view the status and events for custom and SageMaker JumpStart deployments, respectively. 
 If the HyperPod observability stack is deployed to the cluster, run hyp get-monitoring --grafana and hyp get-monitoring --prometheus to get the Grafana dashboard and Prometheus workspace URLs, respectively, to view inference metrics as well. 
 To monitor GPU utilization or view directory contents, it can be useful to execute commands or open an interactive shell into the pods. You can run commands in a pod by running, for example, kubectl exec -it&lt;pod-name&gt;-- nvtop to run nvtop for visibility into GPU utilization. You can open an interactive shell by running kubectl exec -it&lt;pod-name&gt;-- /bin/bash. 
 
For more information on the inference deployment features in SageMaker HyperPod, see Amazon SageMaker HyperPod launches model deployments to accelerate the generative AI model development lifecycle and Deploying models on Amazon SageMaker HyperPod. 
Clean up 
To delete the training job from the corresponding example, use the following CLI command: 
 
 hyp delete hyp-pytorch-job --job-name fsdp-llama3-1-8b 
 
To delete the model deployments from the inference example, use the following CLI commands for SageMaker JumpStart and custom model deployments, respectively: 
 
 hyp delete hyp-jumpstart-endpoint --name deepseek-distill-qwen-endpoint-cli
hyp delete&nbsp;hyp-custom-endpoint --name&nbsp;my-custom-tinyllama-endpoint 
 
To avoid incurring ongoing costs for the instances running in your cluster, you can scale down the instances or delete instances. 
Conclusion 
The new SageMaker HyperPod CLI and SDK can significantly streamline the process of training and deploying large-scale AI models. Through the examples in this post, we‚Äôve demonstrated how these tools provide the following benefits: 
 
 Simplified workflows ‚Äì The CLI offers straightforward commands for common tasks like distributed training and model deployment, making powerful capabilities of SageMaker HyperPod accessible to data scientists without requiring deep infrastructure knowledge. 
 Flexible development options ‚Äì Although the CLI handles common scenarios, the SDK enables fine-grained control and customization for more complex requirements, so developers can programmatically configure every aspect of their distributed ML workloads. 
 Comprehensive observability ‚Äì Both interfaces provide robust monitoring and debugging capabilities through system logs and integration with the SageMaker HyperPod observability stack, helping quickly identify and resolve issues during development. 
 Production-ready deployment ‚Äì The tools support end-to-end workflows from experimentation to production, including features like automatic TLS certificate generation for secure model endpoints and integration with SageMaker inference endpoints. 
 
Getting started with these tools is as simple as installing the sagemaker-hyperpod package. The SageMaker HyperPod CLI and SDK provide the right level of abstraction for both data scientists looking to quickly experiment with distributed training and ML engineers building production systems. 
For more information about SageMaker HyperPod and these development tools, refer to the SageMaker HyperPod CLI and SDK documentation or explore the example notebooks. 
 
About the authors 
Giuseppe Angelo Porcelli&nbsp;is a Principal Machine Learning Specialist Solutions Architect for Amazon Web Services. With several years of software engineering and an ML background, he works with customers of any size to understand their business and technical needs and design AI and ML solutions that make the best use of the AWS Cloud and the Amazon Machine Learning stack. He has worked on projects in different domains, including MLOps, computer vision, and NLP, involving a broad set of AWS services. In his free time, Giuseppe enjoys playing football. 
Shweta Singh is a Senior Product Manager in the Amazon SageMaker Machine Learning platform team at AWS, leading the SageMaker Python SDK. She has worked in several product roles in Amazon for over 5 years. She has a Bachelor of Science degree in Computer Engineering and a Masters of Science in Financial Engineering, both from New York University. 
Nicolas Jourdan is a Specialist Solutions Architect at AWS, where he helps customers unlock the full potential of AI and ML in the cloud. He holds a PhD in Engineering from TU Darmstadt in Germany, where his research focused on the reliability, concept drift detection, and MLOps of industrial ML applications. Nicolas has extensive hands-on experience across industries, including autonomous driving, drones, and manufacturing, having worked in roles ranging from research scientist to engineering manager. He has contributed to award-winning research, holds patents in object detection and anomaly detection, and is passionate about applying cutting-edge AI to solve complex real-world problems.
‚Ä¢ Build a serverless Amazon Bedrock batch job orchestration workflow using AWS Step Functions
  As organizations increasingly adopt foundation models (FMs) for their artificial intelligence and machine learning (AI/ML) workloads, managing large-scale inference operations efficiently becomes crucial. Amazon Bedrock supports two general types of large-scale inference patterns: real-time inference and batch inference for use cases that involve processing massive datasets where immediate results aren‚Äôt necessary. 
Amazon Bedrock batch inference is a cost-effective solution that offers a 50% discount compared to on-demand processing, making it ideal for high-volume, time-insensitive workloads. However, implementing batch inference at scale comes with its own set of challenges, including managing input formatting and job quotas, orchestrating concurrent executions, and handling postprocessing tasks. Developers need a robust framework to streamline these operations. 
In this post, we introduce a flexible and scalable solution that simplifies the batch inference workflow. This solution provides a highly scalable approach to managing your FM batch inference needs, such as generating embeddings for millions of documents or running custom evaluation or completion tasks with large datasets. 
Solution overview 
The following diagram details a broad overview of the automated workflow, which includes three main phases: preprocessing of input datasets (for example, prompt formatting), execution of batch inference jobs in parallel, and postprocessing to parse the model outputs. 
 
This solution provides a flexible and scalable framework to simplify batch orchestration. Given a simple configuration input, the Step Functions state machine deployed in this AWS Cloud Development Kit (AWS CDK) stack handles preprocessing the dataset, launching parallel batch jobs, and postprocessing the output. 
In our specific use case, we use 2.2 million rows of data from the open source dataset SimpleCoT. The SimpleCoT dataset on Hugging Face is a collection of diverse task-oriented examples designed to demonstrate and train chain-of-thought (CoT) reasoning in language models. This dataset encompasses a wide range of problem types, including reading comprehension, mathematical reasoning, logical deduction, and natural language processing (NLP) tasks. The dataset is structured with each entry containing a task description, question, the correct answer, and a detailed explanation of the reasoning process. 
The following diagram illustrates the solution architecture. 
 
The Amazon Bedrock batch orchestration pattern uses scalable and serverless components to cover the key architectural considerations specific to batch processing workflows: 
 
 File format and storage ‚Äì Job inputs must be structured as JSONL files stored in an Amazon Simple Storage Service (Amazon S3) bucket, with each line representing a single input record that matches the API request structure for that FM or provider. For example, Anthropic‚Äôs Claude models have a different JSON structure compared to Amazon Titan Text Embeddings V2. There are also quotas to consider: at the time of writing, a minimum of 1,000 and maximum of 50,000 records per batch. You can request a quota increase using Service Quotas based on your use case requirements. 
 Step Functions state machine ‚Äì Orchestration of the asynchronous, long-running jobs requires a robust control flow system. Our architecture uses Step Functions to coordinate the overall process, with Amazon DynamoDB maintaining the inventory of individual jobs and their states. Again, there are important quota considerations: for example, the maximum sum of in-progress and submitted batch inference jobs using a base model for Amazon Titan Text Embeddings V2 is currently 20 per AWS Region. Using Map workflow states, Step Functions can help maximize throughput by controlling job submission and monitoring completion status. 
 Postprocessing ‚Äì Finally, you will likely want to perform some light postprocessing on the batch outputs (also JSONL files in Amazon S3) to parse the responses and join the output back to the original input. For example, when generating text embeddings, you must have a mechanism to map output vectors back to their source text. These configurable AWS Lambda functions are triggered as part of the Step Functions workflow after batch results arrive in Amazon S3. 
 
In the following sections, we walk through the steps to deploy the AWS CDK stack to your AWS environment. 
Prerequisites 
Complete the following prerequisite steps: 
 
 Install node and npm. 
 Install the AWS CDK: 
 
 
 npm install -g aws-cdk 
 
 
 Clone the GitHub repository into your local development environment: 
 
 
 git clone https://github.com/aws-samples/amazon-bedrock-samples
cd poc-to-prod/bedrock-batch-orchestrator 
 
Deploy the solution 
Install the required packages with the following code:npm i 
Check the prompt_templates.py file and add a new prompt template to prompt_id_to_template for your desired use case. 
prompt_id_to_template is a dict where the key is the prompt_id (allowing you to associate a given job with a particular prompt). Formatting keys in the prompt string template must also exist in your input file. For example, consider the following prompt template: 
 
 You are an AI assistant tasked with providing accurate and justified answers to users' questions.
    
You will be given a task, and you should respond with a chain-of-thought surrounded by &lt;thinking&gt; tags, then a final answer in &lt;answer&gt; tags.

For example, given the following task:

&lt;task&gt;
You are given an original reference as well as a system generated reference. Your task is to judge the naturaleness of the system generated reference. If the utterance could have been produced by a native speaker output 1, else output 0. System Reference: may i ask near where? Original Reference: where do you need a hotel near?.
&lt;/task&gt;

&lt;thinking&gt;
The utterance "may i ask near where?" is not natural. 
This utterance does not make sense grammatically.
Thus we output 0.
&lt;/thinking&gt;

&lt;answer&gt;0&lt;/answer&gt;

Your turn. Please respond to the following task:

&lt;task&gt;
{source}
&lt;/task&gt; 
 
You must make sure your input dataset has a column for each formatting key (for example, source in the preceding example code). 
Prompt templates are not used for embedding model-based jobs.Deploy the AWS CDK stack with the following code:npm run cdk deploy 
Take note of the AWS CloudFormation outputs denoting the names of the bucket and Step Functions workflow: 
 
 ‚úÖ BedrockBatchOrchestratorStack

‚ú® Deployment time: 23.16s

Outputs:
BedrockBatchOrchestratorStack.bucketName = batch-inference-bucket-&lt;YOUR_ACCOUNT_ID&gt;
BedrockBatchOrchestratorStack.stepFunctionName = bedrockBatchOrchestratorSfnE5E2B976-4yznxekguxxm
Stack ARN:
arn:aws:cloudformation:us-east-1:&lt;YOUR_ACCOUNT_ID&gt;:stack/BedrockBatchOrchestratorStack/0787ba80-b0cb-11ef-a481-0affd4b49c99

‚ú® Total time: 26.74s 
 
Job input structure 
As your input dataset, you can either use a Hugging Face dataset ID or point directly to a dataset in Amazon S3 (CSV or Parquet formats are supported at the time of writing). The source of the input dataset and the type of model (text generation or embedding) dictate the structure of the Step Functions input. 
Hugging Face dataset 
For a Hugging Face dataset, reference a dataset ID (for example, w601sxs/simpleCoT) and split (for example, train), and your dataset will be pulled directly from Hugging Face Hub. 
 
The question_answering prompt template in prompt_templates.py has a formatting key called source to match the name of the appropriate column in the referenced dataset (see the preceding example). We use this prompt to generate the rationale and answer for each of the 2.2 million rows in the dataset. See the following code: 
 
 {
  "job_name_prefix": "full-cot-job",
  "model_id": "us.anthropic.claude-3-5-haiku-20241022-v1:0",
  "prompt_id": "question_answering",
  "dataset_id": "w601sxs/simpleCoT",
  "split": "train",
  "max_records_per_job": 50000
} 
 
We also have optional keys for max_num_jobs (to limit the total number of jobs, which is useful for testing on a smaller scale) and max_records_per_batch. 
Amazon S3 dataset 
Upload an input CSV or parquet file to the S3 bucket and copy the S3 URI. For example:aws s3 cp topics.csv s3://batch-inference-bucket-&lt;YOUR_ACCOUNT_ID&gt;/inputs/jokes/topics.csv 
Open your Step Functions state machine on the Step Functions console and submit an input with the following structure. You must supply an s3_uri for S3 datasets. 
For example, for Anthropic models with an Amazon S3 input, use the following code: 
 
 {
"s3_uri": "s3://batch-inference-bucket-&lt;YOUR_ACCOUNT_ID&gt;/inputs/jokes/topics.csv",
"job_name_prefix": "test-joke-job1",
"model_id": "anthropic.claude-3-haiku-20240307-v1:0",
"prompt_id": "joke_about_topic"
} 
 
The prompt_id of joke_about_topic maps to a prompt template in prompt_templates.py, which has a formatting key for topic, which must be one of the columns in the input CSV file. 
Generate batch embeddings 
To generate embeddings with a model like Amazon Titan Text Embeddings V2, you don‚Äôt need to provide a prompt_id, but you do need to make sure your input CSV file has a column called input_text with the text you want to embed. For example: 
 
 {
"s3_uri": "s3://batch-inference-bucket-&lt;YOUR_ACCOUNT_ID&gt;/inputs/embeddings/embedding_input.csv",
"job_name_prefix": "test-embeddings-job1",
"model_id": "amazon.titan-embed-text-v2:0",
"prompt_id": null
} 
 
Step Functions workflow 
The following diagram shows an example of a successful Step Functions workflow execution. 
 
When a Step Functions state machine is initiated, it completes the following steps: 
 
 Preprocess input datasets to prepare batch job inputs for your particular model ID and prompt template. The BaseProcessor abstract class can quickly be extended for other model providers, such as Meta Llama 3 or Amazon Nova. 
 Orchestrate batch jobs in an event-driven fashion. We maintain an internal inventory of jobs in a DynamoDB table and keep it updated when Amazon Bedrock emits events related to job status changes. These updates are then transmitted back to the step function using the Wait for Task Token Callback integration pattern. Using a SFN Map, we make sure that the maximum capacity of concurrent jobs is maintained until the records have been processed. 
 Run concurrent postprocessing of batch outputs to perform some light parsing and merge model responses back to the original input data using the recordId field as a join key. The output data depends on the kind of model you use. For text-based models, the output string will be in a new column called response. 
 
Monitor your state machine as it runs the jobs. The maximum number of concurrent jobs is controlled by an AWS CDK context variable in cdk.json (key: maxConcurrentJobs). The paths to your resulting Parquet files will be aggregated in the outputs from the execution. 
The output Parquet files will contain the same columns as your input file alongside the generated responses. 
For text generation models, the output string will be in a new column called response, as shown in the following screenshot of a sample output. 
 
For embedding models, the output (list of floats) will be in a new column called embedding, as shown in the following screenshot. 
 
There are no guaranteed SLAs for the Batch Inference API. Runtimes will vary based on the demand of the desired model at the time of your request. For example, to process the 2.2 million records in the SimpleCoT dataset, execution was spread across 45 individual processing jobs, with a maximum of 20 concurrent jobs at a given time. In our experiment with Anthropic‚Äôs Claude Haiku 3.5 in the us-east-1 Region, each individual job execution took an average of 9 hours, for a total end-to-end processing time of about 27 hours. 
Clean up 
To avoid incurring additional costs, you can clean up the stack‚Äôs resources by running cdk destroy. 
Conclusion 
In this post, we outlined a serverless architecture for performing large-scale batch processing using Amazon Bedrock batch inference. We explored using the solution for various use cases, including large-scale data labeling and embedding generation. You can also generate a large amount synthetic data from a teacher model used to train a student model as part of model distillation process. 
The solution is publicly available in the GitHub repo. We can‚Äôt wait to see how you put this architecture to work for your use cases. 
 
About the authors 
Swagat Kulkarni is a Senior Solutions Architect at AWS and an active Generative AI practitioner. He is passionate about helping customers solve real-world challenges using cloud-native services and machine learning. With a strong background in driving digital transformation across diverse industries, Swagat has delivered impactful solutions that enable innovation and scale. Outside of work, he enjoys traveling, reading, and cooking. 
Evan Diewald is a Data &amp; Machine Learning Engineer with AWS Professional Services, where he helps AWS customers develop and deploy ML solutions in a variety of industry verticals. Prior to joining AWS, he received an M.S. from Carnegie Mellon University, where he conducted research at the intersection of advanced manufacturing and AI. Outside of work, he enjoys mountain biking and rock climbing. 
Shreyas Subramanian&nbsp;is a Principal Data Scientist and helps customers by using Generative AI and deep learning to solve their business challenges using AWS services like Amazon Bedrock and AgentCore. Dr. Subramanian contributes to cutting-edge research in deep learning, Agentic AI, foundation models and optimization techniques with several books, papers and patents to his name. In his current role at Amazon, Dr. Subramanian works with various science leaders and research teams within and outside Amazon, helping to guide customers to best leverage state-of-the-art algorithms and techniques to solve business critical problems. Outside AWS, Dr. Subramanian is a expert reviewer for AI papers and funding via organizations like Neurips, ICML, ICLR, NASA and NSF.

‚∏ª