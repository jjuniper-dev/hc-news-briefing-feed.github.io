‚úÖ Morning News Briefing ‚Äì July 09, 2025 10:56

üìÖ Date: 2025-07-09 10:56
üè∑Ô∏è Tags: #briefing #ai #publichealth #digitalgov

‚∏ª

üßæ Weather
‚Ä¢ No watches or warnings in effect, Pembroke
  No watches or warnings in effect. No warnings or watches or watches in effect . Watch or warnings are no longer in effect in the U.S. No watches, warnings are in effect for the rest of the day . No watches and warnings are still in effect, but no watches are in place for the day's events . The weather is not expected to be affected by the weather .
‚Ä¢ Current Conditions: Fog Patches, 14.5¬∞C
  Temperature: 14.5&deg;C Pressure: 101.7 kPa Visibility: 24 km Visibility : 24 km . Humidity: 96 % Dewpoint: 13.9&deg:C Wind:  calm km/h Air Quality Health Index: n/a . Weather:  Fog Patches at Garrison Petawawa 6:00 AM EDT Wednesday 9 July
‚Ä¢ Wednesday: Chance of showers. High 27. POP 60%
  60 percent chance of showers this afternoon with risk of a thunderstorm . Fog patches dissipating this morning . High 27. Humidex 32. UV index 8 or very high. Chance of rain this afternoon or evening, risk of thunderstorm, high risk of wind gusts of up to 70mph . High risk of rain, thunderstorms, high wind, rain showers and thunderstorms

üåç International News
No updates.

üçÅ Canadian News
No updates.

üá∫üá∏ U.S. Top Stories
‚Ä¢ Over 160 people still missing after Texas floods. And, federal layoffs can now resume
  The death toll for the Texas floods tops 100 as the search and rescue efforts continue for over 160 missing people . The Supreme Court is allowing Trump to resume mass federal layoffs for now . And, the Supreme Court says, the federal government is allowed to continue to lay off federal employees for now, despite the deaths of over 160 people in the floods . The federal court is allowing the president to
‚Ä¢ State laws to stop surprise ambulance bills face pushback from insurers
  Policymakers agree patients shouldn't be stuck in the middle when an ambulance service charges more than what an insurer will pay . But they can't settle on what price is fair . Policymakers say patients should not be stuck between the two sides of the line . They can't decide what price should be fair, but can't agree on what should be the price for an ambulance to cover the
‚Ä¢ Baseline knowledge: Where tennis comes from and how the game has changed
  Recent years have seen an upswing in people playing tennis (or at least dressing like it) The sport ‚Äî at least some version of it ‚Äî has been around since medieval times . It's not just a phase, but it's been around for centuries . Tennis has been a popular sport in medieval times, and it's not a phase of it, says Paul Thompson of New York .
‚Ä¢ How a broken nose kickstarted Diego Luna's star-making run with the USMNT
  The 21-year-old Real Salt Lake midfielder, with his bleached hair and dozens of tattoos, stands out on the soccer field . But it's his attitude that has won praise from his coach, teammates and fans . He has been praised for his attitude, which has won him praise from coaches, fans and coaches . He is also known for his tattoos and bleached blond hair
‚Ä¢ After quitting antidepressants, some people suffer surprising, lingering symptoms
  The symptoms can include nerve pain, emotional numbness and sexual dysfunction and can last for years after stopping the drugs . Patients are pushing for recognition and more research . The symptoms are often caused by the drugs and last for many years after they are taken off . Patients say they want recognition of the effects of the drugs, more research and more recognition of their symptoms and better treatment . The drugs

üß† Artificial Intelligence
No updates.

üíª Digital Strategy
‚Ä¢ ESA backs five rockets in Launcher Challenge ‚Äì only some have exploded
  European Space Agency pressing ahead with its European Launcher Challenge . Three of the five pre-selected candidates have yet to explode anything in public . Oodles of euros on offer for not accidentally blowing up stuff on offer . Eighty per cent of the prize money is up for grabs for not blowing up anything in the public . ESA: Three of five candidates are yet to blow up anything public
‚Ä¢ Ingram Micro restarts orders ‚Äì for some ‚Äì following ransomware attack
  Customers say things are still far from perfect as lengthy support queues hamper business dealings . Ingram Micro says it is gradually reactivating customer's ordering capabilities across the world, region by region, now its ransomware attack is thought to be "contained" Customers say they are still struggling to get through support queues as long support queues continue to hamper their business . Customers say it is slowly reactivating
‚Ä¢ Privacy campaigners pour cold water on London cops' 1,000 facial recognition arrests
  Activists argue resources spent on tech aren't leading to worthwhile numbers . They say it is not effective use of taxpayer money and an overreach by government . London's Metropolitan Police use live facial recognition (LFR) to catch criminals . Activists say it isn't effective and is not a good use of money and overreach is government overreach . Police say it's a waste of
‚Ä¢ C-suite sours on AI despite rising investment, survey finds
  Akkodis report suggests people skills may be helpful to bring out the best in AI . Executives are losing faith in AI initiatives despite rising investment, according to a study conducted by consultancy Akksodis . People skills may help bring out best in people skills, the report suggests . AI initiatives are being funded by more than 80 per cent of the world‚Äôs top
‚Ä¢ Iranian ransomware crew reemerges, promises big bucks for attacks on US or Israel
  Iranian ransomware-as-a-service operation with ties to a government-backed cyber crew has reemerged after a nearly five-year hiatus . Tells would-be affiliates they don't need to worry because cyberattacks don't violate a cease-fire . The operation is offering to infect organizations in the US and Israel and offers cash to infect those in the U.S.

üè• Public Health
No updates.

üî¨ Science
‚Ä¢ Increased testing is needed for Mpox in DR Congo to urgently curb disease spread
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Sarcopenia is associated with increased hip fracture risk among middle-aged and elderly Chinese adults in longitudinal analysis of CHARLS data
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ A systematic review and meta-analysis of interventions to reduce or prevent symptoms of common mental disorders and suicidality in physicians
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Population-scale cross-sectional observational study for AI-powered TB screening on one million CXRs
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Address Colombia‚Äôs brain-health crisis
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations

üßæ Government & Policy
No updates.

üèõÔ∏è Enterprise Architecture & IT Governance
No updates.

ü§ñ AI & Emerging Tech
‚Ä¢ Inside OpenAI‚Äôs empire: A conversation with Karen Hao
  In a wide-ranging Roundtables conversation for MIT Technology Review subscribers, AI journalist and author Karen Hao spoke about her new book, Empire of AI: Dreams and Nightmares in Sam Altman‚Äôs OpenAI. She talked with executive editor Niall Firth about how she first covered the company in 2020 while on staff at MIT Technology Review, and they discussed how the AI industry now functions like an empire and what ethically-made AI looks like.&nbsp;



Read the transcript of the conversation, which has been lightly edited and condensed, below. Subscribers can watch the on-demand recording of the event here.&nbsp;







Niall Firth: Hello, everyone, and welcome to this special edition of Roundtables. These are our subscriber-only events where you get to listen in to conversations between editors and reporters. Now, I‚Äôm delighted to say we‚Äôve got an absolute cracker of an event today. I‚Äôm very happy to have our prodigal daughter, Karen Hao, a fabulous AI journalist, here with us to talk about her new book. Hello, Karen, how are you doing?



Karen Hao: Good. Thank you so much for having me back, Niall.&nbsp;



Niall Firth: Lovely to have you. So I‚Äôm sure you all know Karen and that‚Äôs why you‚Äôre here. But to give you a quick, quick synopsis, Karen has a degree in mechanical engineering from MIT. She was MIT Technology Review‚Äôs senior editor for AI and has won countless awards, been cited in Congress, written for the Wall Street Journal and The Atlantic, and set up a series at the Pulitzer Center to teach journalists how to cover AI.&nbsp;



But most important of all, she‚Äôs here to discuss her new book, which I‚Äôve got a copy of here, Empire of AI. The UK version is subtitled ‚ÄúInside the reckless race for total domination,‚Äù and the US one, I believe, is ‚ÄúDreams and nightmares in Sam Altman‚Äôs OpenAI.‚Äù



It‚Äôs been an absolute sensation, a New York Times chart topper. An incredible feat of reporting‚Äîlike 300 interviews, including 90 with people inside OpenAI. And it‚Äôs a brilliant look at not just OpenAI‚Äôs rise, and the character of Sam Altman, which is very interesting in its own right, but also a really astute look at what kind of AI we‚Äôre building and who holds the keys.&nbsp;



Karen, the core of the book, the rise and rise of OpenAI, was one of your first big features at MIT Technology Review. It‚Äôs a brilliant story that lifted the lid for the first time on what was going on at OpenAI ‚Ä¶ and they really hated it, right?



Karen Hao: Yes, and first of all, thank you to everyone for being here. It‚Äôs always great to be home. I do still consider MIT Tech Review to be my journalistic home, and that story was‚ÄîI only did it because Niall assigned it after I said, ‚ÄúHey, it seems like OpenAI is kind of an interesting thing,‚Äù and he was like, you should profile them. And I had never written a profile about a company before, and I didn‚Äôt think that I would have it in me, and Niall believed that I would be able to do it. So it really didn‚Äôt happen other than because of you.



I went into the piece with an open mind about‚Äîlet me understand what OpenAI is. Let me take what they say at face value. They were founded as a nonprofit. They have this mission to ensure artificial general intelligence benefits all of humanity. What do they mean by that? How are they trying to achieve that ultimately? How are they striking this balance between mission-driven AI development and the need to raise money and capital?&nbsp;



And through the course of embedding within the company for three days, and then interviewing dozens of people outside the company or around the company ‚Ä¶ I came to realize that there was a fundamental disconnect between what they were publicly espousing and accumulating a lot of goodwill from and how they were operating. And that is what I ended up focusing my profile on, and that is why they were not very pleased.



Niall Firth: And how have you seen OpenAI change even since you did the profile? That sort of misalignment feels like it‚Äôs got messier and more confusing in the years since.



Karen Hao: Absolutely. I mean, it‚Äôs kind of remarkable that OpenAI, you could argue that they are now one of the most capitalistic corporations in Silicon Valley. They just raised $40 billion, in the largest-ever private fundraising round in tech industry history. They‚Äôre valued at $300 billion. And yet they still say that they are first and foremost a nonprofit.&nbsp;



I think this really gets to the heart of how much OpenAI has tried to position and reposition itself throughout its decade-long history, to ultimately play into the narratives that they think are going to do best with the public and with policymakers, in spite of what they might actually be doing in terms of developing their technologies and commercializing them.



Niall Firth: You cite Sam Altman saying, you know, the race for AGI is what motivated a lot of this, and I‚Äôll come back to that a bit before the end. But he talks about it as like the Manhattan Project for AI. You cite him quoting Oppenheimer (of course, you know, there‚Äôs no self-aggrandizing there): ‚ÄúTechnology happens because it‚Äôs possible,‚Äù he says in the book.¬†



And it feels to me like this is one of the themes of the book: the idea that technology doesn‚Äôt just happen because it comes along. It comes because of choices that people make. It‚Äôs not an inevitability that things are the way they are and that people are who they are. What they think is important‚Äîthat influences the direction of travel. So what does this mean, in practice, if that‚Äôs the case?



Karen Hao: With OpenAI in particular, they made a very key decision early on in their history that led to all of the AI technologies that we see dominating the marketplace and dominating headlines today. And that was a decision to try and advance AI progress through scaling the existing techniques that were available to them. At the time when OpenAI started, at the end of 2015, and then, when they made that decision, in roughly around 2017, this was a very unpopular perspective within the broader AI research field.&nbsp;



There were kind of two competing ideas about how to advance AI progress, or rather a spectrum of ideas, bookended by two extremes. One extreme being, we have all the techniques we need, and we should just aggressively scale. And the other one being that we don‚Äôt actually have the techniques we need. We need to continue innovating and doing fundamental AI research to get more breakthroughs. And largely the field assumed that this side of the spectrum [focusing on fundamental AI research] was the most likely approach for getting advancements, but OpenAI was anomalously committed to the other extreme‚Äîthis idea that we can just take neural networks and pump ever more data, and train on ever larger supercomputers, larger than have ever been built in history.



The reason why they made that decision was because they were competing against Google, which had a dominant monopoly on AI talent. And OpenAI knew that they didn‚Äôt necessarily have the ability to beat Google simply by trying to get research breakthroughs. That‚Äôs a very hard path. When you‚Äôre doing fundamental research, you never really know when the breakthrough might appear. It‚Äôs not a very linear line of progress, but scaling is sort of linear. As long as you just pump more data and more compute, you can get gains. And so they thought, we can just do this faster than anyone else. And that‚Äôs the way that we‚Äôre going to leap ahead of Google. And it particularly aligned with Sam Altman‚Äôs skillset, as well, because he is a once-in-a-generation fundraising talent, and when you‚Äôre going for scale to advance AI models, the primary bottleneck is capital.



And so it was kind of a great fit for what he had to offer, which is, he knows how to accumulate capital, and he knows how to accumulate it very quickly. So that is ultimately how you can see that technology is a product of human choices and human perspectives. And they‚Äôre the specific skills and strengths that that team had at the time for how they wanted to move forward.





Niall Firth: And to be fair, I mean, it works, right? It was amazing, fabulous. You know the breakthroughs that happened, GPT-2 to GPT-3, just from scale and data and compute, kind of were mind-blowing really, as we look back on it now.



Karen Hao: Yeah, it is remarkable how much it did work, because there was a lot of skepticism about the idea that scale could lead to the kind of technical progress that we‚Äôve seen. But one of my biggest critiques of this particular approach is that there‚Äôs also an extraordinary amount of costs that come with this particular pathway to getting more advancements. And there are many different pathways to advancing AI, so we could have actually gotten all of these benefits, and moving forward, we could continue to get more benefits from AI, without actually engaging in a hugely consumptive, hugely costly approach to its development.



Niall Firth: Yeah, so in terms of consumptive, that‚Äôs something we‚Äôve touched on here quite recently at MIT Technology Review, like the energy costs of AI. The data center costs are absolutely extraordinary, right? Like the data behind it is incredible. And it‚Äôs only gonna get worse in the next few years if we continue down this path, right?&nbsp;



Karen Hao: Yeah ‚Ä¶ so first of all, everyone should read the series that Tech Review put out, if you haven‚Äôt already, on the energy question, because it really does break down everything from what is the energy consumption of the smallest unit of interacting with these models, all the way up until the highest level.&nbsp;



The number that I have seen a lot, and that I‚Äôve been repeating, is there was a McKinsey report that was looking at if we continue to just look at the pace at which data centers and supercomputers are being built and scaled, in the next five years, we would have to add two to six times the amount of energy consumed by California onto the grid. And most of that will have to be serviced by fossil fuels, because these data centers and supercomputers have to run 24/7, so we cannot rely solely on renewable energy. We do not have enough nuclear power capacity to power these colossal pieces of infrastructure. And so we‚Äôre already accelerating the climate crisis.&nbsp;



And we‚Äôre also accelerating a public-health crisis, the pumping of thousands of tons of air pollutants into the air from coal plants that are having their lives extended and methane gas turbines that are being built in service of powering these data centers. And in addition to that, there‚Äôs also an acceleration of the freshwater crisis, because these pieces of infrastructure have to be cooled with freshwater resources. It has to be fresh water, because if it‚Äôs any other type of water, it corrodes the equipment, it leads to bacterial growth.



And Bloomberg recently had a story that showed that two-thirds of these data centers are actually going into water-scarce areas, into places where the communities already do not have enough fresh water at their disposal. So that is one dimension of many that I refer to when I say, the extraordinary costs of this particular pathway for AI development.



Niall Firth: So in terms of costs and the extractive process of making AI, I wanted to give you the chance to talk about the other theme of the book, apart from just OpenAI‚Äôs explosion. It‚Äôs the colonial way of looking at the way AI is made: the empire. I‚Äôm saying this obviously because we‚Äôre here, but this is an idea that came out of reporting you started at MIT Technology Review and then continued into the book. Tell us about how this framing helps us understand how AI is made now.



Karen Hao: Yeah, so this was a framing that I started thinking a lot about when I was working on the AI Colonialism series for Tech Review. It was a series of stories that looked at the way that, pre-ChatGPT, the commercialization of AI and its deployment into the world was already leading to entrenchment of historical inequities into the present day.



And one example was a story that was about how facial recognition companies were swarming into South Africa to try and harvest more data from South Africa during a time when they were getting criticized for the fact that their technologies did not accurately recognize black faces. And the deployment of those facial recognition technologies into South Africa, into the streets of Johannesburg, was leading to what South African scholars were calling a recreation of a digital apartheid‚Äîthe controlling of black bodies, movement of black people.



And this idea really haunted me for a really long time. Through my reporting in that series, there were so many examples that I kept hitting upon of this thesis, that the AI industry was perpetuating. It felt like it was becoming this neocolonial force. And then, when ChatGPT came out, it became clear that this was just accelerating.&nbsp;



When you accelerate the scale of these technologies, and you start training them on the entirety of the Internet, and you start using these supercomputers that are the size of dozens‚Äîif not hundreds‚Äîof football fields. Then you really start talking about an extraordinary global level of extraction and exploitation that is happening to produce these technologies. And then the historical power imbalances become even more obvious.&nbsp;





And so there are four parallels that I draw in my book between what I have now termed empires of AI versus empires of old. The first one is that empires lay claim to resources that are not their own. So these companies are scraping all this data that is not their own, taking all the intellectual property that is not their own.



The second is that empires exploit a lot of labor. So we see them moving to countries in the Global South or other economically vulnerable communities to contract workers to do some of the worst work in the development pipeline for producing these technologies‚Äîand also producing technologies that then inherently are labor-automating and engage in labor exploitation in and of themselves.&nbsp;



And the third feature is that the empires monopolize knowledge production. So, in the last 10 years, we‚Äôve seen the AI industry monopolize more and more of the AI researchers in the world. So AI researchers are no longer contributing to open science, working in universities or independent institutions, and the effect on the research is what you would imagine would happen if most of the climate scientists in the world were being bankrolled by oil and gas companies. You would not be getting a clear picture, and we are not getting a clear picture, of the limitations of these technologies, or if there are better ways to develop these technologies.



And the fourth and final feature is that empires always engage in this aggressive race rhetoric, where there are good empires and evil empires. And they, the good empire, have to be strong enough to beat back the evil empire, and that is why they should have unfettered license to consume all of these resources and exploit all of this labor. And if the evil empire gets the technology first, humanity goes to hell. But if the good empire gets the technology first, they‚Äôll civilize the world, and humanity gets to go to heaven. So on many different levels, like the empire theme, I felt like it was the most comprehensive way to name exactly how these companies operate, and exactly what their impacts are on the world.



Niall Firth: Yeah, brilliant. I mean, you talk about the evil empire. What happens if the evil empire gets it first? And what I mentioned at the top is AGI. For me, it‚Äôs almost like the extra character in the book all the way through. It‚Äôs sort of looming over everything, like the ghost at the feast, sort of saying like, this is the thing that motivates everything at OpenAI. This is the thing we‚Äôve got to get to before anyone else gets to it.¬†



There‚Äôs a bit in the book about how they‚Äôre talking internally at OpenAI, like, we‚Äôve got to make sure that AGI is in US hands where it‚Äôs safe versus like anywhere else. And some of the international staff are openly like‚Äîthat‚Äôs kind of a weird way to frame it, isn‚Äôt it? Why is the US version of AGI better than others?&nbsp;



So tell us a bit about how it drives what they do. And AGI isn‚Äôt an inevitable fact that‚Äôs just happening anyway, is it? It‚Äôs not even a thing yet.



Karen Hao: There‚Äôs not even consensus around whether or not it‚Äôs even possible or what it even is. There was recently a New York Times story by Cade Metz that was citing a survey of long-standing AI researchers in the field, and 75% of them still think that we don‚Äôt have the techniques yet for reaching AGI, whatever that means. And the most classic definition or understanding of what AGI is, is being able to fully recreate human intelligence in software. But the problem is, we also don‚Äôt have scientific consensus around what human intelligence is. And so one of the aspects that I talk about a lot in the book is that, when there is a vacuum of shared meaning around this term, and what it would look like, when would we have arrived at it? What capabilities should we be evaluating these systems on to determine that we‚Äôve gotten there? It can basically just be whatever OpenAI wants.&nbsp;



So it‚Äôs kind of just this ever-present goalpost that keeps shifting, depending on where the company wants to go. You know, they have a full range, a variety of different definitions that they‚Äôve used throughout the years. In fact, they even have a joke internally: If you ask 13 OpenAI researchers what AGI is, you‚Äôll get 15 definitions. So they are kind of self-aware that this is not really a real term and it doesn‚Äôt really have that much meaning.&nbsp;



But it does serve this purpose of creating a kind of quasi-religious fervor around what they‚Äôre doing, where people think that they have to keep driving towards this horizon, and that one day when they get there, it‚Äôs going to have a civilizationally transformative impact. And therefore, what else should you be working on in your life, but this? And who else should be working on it, but you?&nbsp;



And so it is their justification not just for continuing to push and scale and consume all these resources‚Äîbecause none of that consumption, none of that harm matters anymore if you end up hitting this destination. But they also use it as a way to develop their technologies in a very deeply anti-democratic way, where they say, we are the only people that have the expertise, that have the right to carefully control the development of this technology and usher it into the world. And we cannot let anyone else participate because it‚Äôs just too powerful of a technology.



Niall Firth: You talk about the factions, particularly the religious framing. AGI has been around as a concept for a while‚Äîit was very niche, very kind of nerdy fun, really, to talk about‚Äîto suddenly become extremely mainstream. And they have the boomers versus doomers dichotomy. Where are you on that spectrum?



Karen Hao: So the boomers are people who think that AGI is going to bring us to utopia, and the doomers think AGI is going to devastate all of humanity. And to me these are actually two sides of the same coin. They both believe that AGI is possible, and it‚Äôs imminent, and it‚Äôs going to change everything.&nbsp;



And I am not on this spectrum. I‚Äôm in a third space, which is the AI accountability space, which is rooted in the observation that these companies have accumulated an extraordinary amount of power, both economic and political power, to go back to the empire analogy.&nbsp;



Ultimately, the thing that we need to do in order to not return to an age of empire and erode a lot of democratic norms is to hold these companies accountable with all the tools at our disposal, and to recognize all the harms that they are already perpetuating through a misguided approach to AI development.



Niall Firth: I‚Äôve got a couple of questions from readers. I‚Äôm gonna try to pull them together a little bit because Abbas asks, what would post-imperial AI look like? And there was a question from Liam basically along the same lines. How do you make a more ethical version of AI that is not within this framework?&nbsp;



Karen Hao: We sort of already touched a little bit upon this idea. But there are so many different ways to develop AI. There are myriads of techniques throughout the history of AI development, which is decades long. There have been various shifts in the winds of which techniques ultimately rise and fall. And it isn‚Äôt based solely on the scientific or technical merit of any particular technique. Oftentimes certain techniques become more popular because of business reasons or because of the funder‚Äôs ideologies. And that‚Äôs sort of what we‚Äôre seeing today with the complete indexing of AI development on large-scale AI model development.



And ultimately, these large-scale models ‚Ä¶ We talked about how it‚Äôs a remarkable technical leap, but in terms of social progress or economic progress, the benefits of these models have been kind of middling. And the way that I see us shifting to AI models that are going to be A) more beneficial and B) not so imperial is to refocus on task-specific AI systems that are tackling well-scoped challenges that inherently lend themselves to the strengths of AI systems that are inherently computational optimization problems.&nbsp;



So I‚Äôm talking about things like using AI to integrate more renewable energy into the grid. This is something that we definitely need. We need to more quickly accelerate our electrification of the grid, and one of the challenges of using more renewable energy is the unpredictability of it. And this is a key strength of AI technologies, being able to have predictive capabilities and optimization capabilities where you can match the energy generation of different renewables with the energy demands of different people that are drawing from the grid.



Niall Firth: Quite a few people have been asking, in the chat, different versions of the same question. If you were an early-career AI scientist, or if you were involved in AI, what can you do yourself to bring about a more ethical version of AI? Do you have any power left, or is it too late?&nbsp;



Karen Hao: No, I don‚Äôt think it‚Äôs too late at all. I mean, as I‚Äôve been talking with a lot of people just in the lay public, one of the biggest challenges that they have is they don‚Äôt have any alternatives for AI. They want the benefits of AI, but they also do not want to participate in a supply chain that is really harmful. And so the first question is, always, is there an alternative? Which tools do I shift to? And unfortunately, there just aren‚Äôt that many alternatives right now.&nbsp;



And so the first thing that I would say to early-career AI researchers and entrepreneurs is to build those alternatives, because there are plenty of people that are actually really excited about the possibility of switching to more ethical alternatives. And one of the analogies I often use is that we kind of need to do with the AI industry what happened with the fashion industry. There was also a lot of environmental exploitation, labor exploitation in the fashion industry, and there was enough consumer demand that it created new markets for ethical and sustainably sourced fashion. And so we kind of need to see just more options occupying that space.



Niall Firth: Do you feel optimistic about the future? Or where do you sit? You know, things aren‚Äôt great as you spell them out now. Where‚Äôs the hope for us?



Karen Hao: I am. I‚Äôm super optimistic. Part of the reason why I‚Äôm optimistic is because you know, a few years ago, when I started writing about AI at Tech Review, I remember people would say, wow, that‚Äôs a really niche beat. Do you have enough to write about?&nbsp;



And now, I mean, everyone is talking about AI, and I think that‚Äôs the first step to actually getting to a better place with AI development. The amount of public awareness and attention and scrutiny that is now going into how we develop these technologies, how we use these technologies, is really, really important. Like, we need to be having this public debate and that in and of itself is a significant step change from what we had before.&nbsp;



But the next step, and part of the reason why I wrote this book, is we need to convert the awareness into action, and people should take an active role. Every single person should feel that they have an active role in shaping the future of AI development, if you think about all of the different ways that you interface with the AI development supply chain and deployment supply chain‚Äîlike you give your data or withhold your data.



There are probably data centers that are being built around you right now. If you‚Äôre a parent, there‚Äôs some kind of AI policy being crafted at [your kid‚Äôs] school. There‚Äôs some kind of AI policy being crafted at your workplace. These are all what I consider sites of democratic contestation, where you can use those opportunities to assert your voice about how you want AI to be developed and deployed. If you do not want these companies to use certain kinds of data, push back when they just take the data.&nbsp;



I closed all of my personal social media accounts because I just did not like the fact that they were scraping my personal photos to train their generative AI models. I‚Äôve seen parents and students and teachers start forming committees within schools to talk about what their AI policy should be and to draft it collectively as a community. Same with businesses. They‚Äôre doing the same thing. If we all kind of step up to play that active role, I am super optimistic that we‚Äôll get to a better place.



Niall Firth: Mark, in the chat, mentions the MƒÅori story from New Zealand towards the end of your book, and that‚Äôs an example of sort of community-led AI in action, isn‚Äôt it?



Karen Hao: Yeah. There was a community in New Zealand that really wanted to help revitalize the MƒÅori language by building a speech recognition tool that could recognize MƒÅori, and therefore be able to transcribe a rich repository of archival audio of their ancestors speaking MƒÅori. And the first thing that they did when engaging in that project was they asked the community, do you want this AI tool?&nbsp;



Niall Firth: Imagine that.



Karen Hao: I know! It‚Äôs such a radical concept, this idea of consent at every stage. But they first asked that; the community wholeheartedly said yes. They then engaged in a public education campaign to explain to people, okay, what does it take to develop an AI tool? Well, we are going to need data. We‚Äôre going to need audio transcription pairs to train this AI model. So then they ran a public contest in which they were able to get dozens, if not hundreds, of people in their community to donate data to this project. And then they made sure that when they developed the model, they actively explained to the community at every step how their data was being used, how it would be stored, how it would continue to be protected. And any other project that would use the data has to get permission and consent from the community first.&nbsp;





And so it was a completely democratic process, for whether they wanted the tool, how to develop the tool, and how the tool should continue to be used, and how their data should continue to be used over time.



Niall Firth: Great. I know we‚Äôve gone a bit over time. I‚Äôve got two more things I‚Äôm going to ask you, basically putting together lots of questions people have asked in the chat about your view on what role regulations should play. What are your thoughts on that?



Karen Hao: Yeah, I mean, in an ideal world where we actually had a functioning government, regulation should absolutely play a huge role. And it shouldn‚Äôt just be thinking about once an AI model is built, how to regulate that. But still thinking about the full supply chain of AI development, regulating the data and what‚Äôs allowed to be trained in these models, regulating the land use. And what pieces of land are allowed to build data centers? How much energy and water are the data centers allowed to consume? And also regulating the transparency. We don‚Äôt know what data is in these training data sets, and we don‚Äôt know the environmental costs of training these models. We don‚Äôt know how much water these data centers consume and that is all information that these companies actively withhold to prevent democratic processes from happening. So if there were one major intervention that regulators could have, it should be to dramatically increase the amount of transparency along the supply chain.



Niall Firth: Okay, great. So just to bring it back around to OpenAI and Sam Altman to finish with. He famously sent an email around, didn‚Äôt he? After your original Tech Review story, saying this is not great. We don‚Äôt like this. And he didn‚Äôt want to speak to you for your book, either, did he?



Karen Hao: No, he did not.



Niall Firth: No. But imagine Sam Altman is in the chat here. He‚Äôs subscribed to Technology Review and is watching this Roundtables because he wants to know what you‚Äôre saying about him. If you could talk to him directly, what would you like to ask him?&nbsp;



Karen Hao: What degree of harm do you need to see in order to realize that you should take a different path?&nbsp;



Niall Firth: Nice, blunt, to the point. All right, Karen, thank you so much for your time.&nbsp;



Karen Hao: Thank you so much, everyone.



MIT Technology Review Roundtables is a subscriber-only online event series where experts discuss the latest developments and what‚Äôs next in emerging technologies. Sign up to get notified about upcoming sessions.
‚Ä¢ Building an innovation ecosystem for the next century
  Michigan may be best known as the birthplace of the American auto industry, but its innovation legacy runs far deeper, and its future is poised to be even broader. From creating the world‚Äôs largest airport factory during World War II at Willow Run to establishing the first successful polio vaccine trials in Ann Arbor to the invention of the snowboard in Muskegon, Michigan has a long history of turning innovation into lasting impact.&nbsp;







Now, with the creation of a new role, chief innovation ecosystem officer, at the Michigan Economic Development Corporation (MEDC), the state is doubling down on its ambition to become a modern engine of innovation, one that is both rooted in its industrial past and designed for the evolving demands of the 21st century economy.&nbsp;&nbsp;



‚ÄúHow do you knit together risk capital founders, businesses, universities, and state government, all of the key stakeholders that need to be at the table together to build a more effective innovation ecosystem?‚Äù asks Ben Marchionna, the first to hold this groundbreaking new position.&nbsp;



Leaning on his background in hard tech startups and national security, Marchionna aims to bring a ‚Äúbuilder&#8217;s thinking‚Äù to the state government. ‚ÄúI&#8217;m sort of wired for that‚Äîrapid prototyping, iterating, scaling, and driving that muscle into the state government ecosystem,‚Äù he explains.



But these efforts aren‚Äôt about creating a copycat Silicon Valley. Michigan‚Äôs approach is uniquely its own. ‚ÄúWe want to develop the thing that makes the most sense for the ingredients that Michigan can bring to bear to this challenge,‚Äù says Marchionna.&nbsp;



This includes cultivating both mom-and-pop businesses and tech unicorns, while tapping into the state‚Äôs talent, research, and manufacturing DNA.&nbsp;



In an era where economic development often feels siloed, partisan, and reactive, Michigan is experimenting with a model centered on long-term value and community-oriented innovation. ‚ÄúYou can lead by example in a lot of these ways, and that flywheel really can get going in a beautiful way when you step out of the prescriptive innovation culture mindset,‚Äù says Marchionna.



This episode of Business Lab is produced in partnership with the Michigan Economic Development Corporation.



Full Transcript&nbsp;



Megan Tatum: From MIT Technology Review. I&#8217;m Megan Tatum, and this is Business Lab, the show that helps business leaders make sense of new technologies coming out of the lab and into the marketplace.&nbsp;



Today&#8217;s episode is brought to you in partnership with the Michigan Economic Development Corporation.&nbsp;



Our topic today is building a statewide innovation economy. Now, the U.S. state of Michigan has long been recognized as a leader in vehicle and mobility innovation. Detroit put it on the map, but did you know it&#8217;s also the birthplace of the snowboard or that the University of Michigan filed more than 600 invention disclosures in 2024, second only to the Massachusetts Institute of Technology, or that in the past five years, 40% of the largest global IPOs have been Michigan built companies?



Two words for you: innovation ecosystem.&nbsp;



My guest is Ben Marchionna, chief innovation ecosystem officer at the Michigan Economic Development Corporation, the MEDC.&nbsp;



Ben, thank you ever so much for joining us.



Ben Marchionna: Thanks, Megan. Really pleased to be here.



Megan: Fantastic. And just to set some context to get us started, I wondered if we could take a kind of high-level look at the economic development landscape. I mean, you joined the MEDC team last year as Michigan&#8217;s first chief innovation ecosystem officer. In fact, you were the first to hold such a role in the country, I believe. I wondered if you could talk a bit about your unique mission and how this economic development approach differs from efforts in other states.



Ben: Yeah, sure would love to. Probably worth pointing out that while I&#8217;ve been in this role for about a year now, it was indeed a first-of-its-kind role in the state of Michigan and first of its kind in the country. The slight difference in the terminology, chief innovation ecosystem officer, it differs a little bit from what folks might think of as a chief innovation officer. I&#8217;m not all that focused on driving innovation within government, which is what some other chief innovation officers would be focused on around the country. Instead, you can think of my role as Michigan&#8217;s chief architect for innovation, if you will. So, how do you knit together risk capital founders, businesses, universities, and state government, all of the key stakeholders that need to be at the table together to build a more effective innovation ecosystem? I talk a lot about building connective tissues that can achieve one plus one equals three outcomes.



Michigan&#8217;s got all kinds of really interesting ingredients and has the foundation to take advantage of the moment in a really interesting way over the next decades as we look to supercharge some of the growth of our innovation ecosystem development.My charter is relatively simple. It&#8217;s to help make sure that Michigan wins in a now hyper-competitive global economy. And to do that, I end up being super focused on orienting us towards a growth and innovation-driven economy. That can mean a lot of different things, but I ultimately came to the MEDC and the role within the state with a builder&#8217;s mindset. My background is not in traditional economic development, it&#8217;s in not government at all. I spent the last 10 years building hard tech startups, one in Ann Arbor, Michigan, and another one in the Northern Virginia area. Before that, I spent a number of years at, think of it like, an innovation factory at Lockheed Martin Skunk Works in the Mojave Desert, working on national security projects.



I&#8217;m sort of wired for that, builder&#8217;s thinking, rapid prototyping, iterating, scaling, and driving that muscle into the state government ecosystem. I think it&#8217;s important that the government also figure out how to pull out all the stops and be able to move at the speed that founders expect. A bias towards action, if you will. And so this is ultimately what my mission is. There are a lot of real interesting things that the state of Michigan can bring to bear to building our innovation ecosystem. And I think, tackling it with this sort of a mindset, I am absolutely optimistic for the future that we&#8217;ve got ahead of us.



Megan: Fantastic. It almost sounds like your role is sort of building a statewide startup incubator of sorts. As we mentioned in the opening, Michigan actually has a really interesting innovation history even in addition to the advances in the automotive industry. I wondered if you could talk a bit more about that history and why Michigan, in particular, is poised to support that sort of statewide startup ecosystem.



Ben: Yeah, absolutely. And I would even broaden it. Building the startup ecosystem is one of the essential layers, but to be able to successfully do that, we have to bring in the research universities, we have to bring in the corporate innovation ecosystem, we have to bring in the risk capital, et cetera. So yes, absolutely, startups are important. And equally as important are all of these other elements that are necessary for a startup ecosystem to thrive, but are also the levers that are just sitting there waiting for us to pull them.And we can get into some of the details over the course of our chat today on the auto industry and how this fits into it, but Michigan does a lot more than just automotive stuff. And you noted, I think, the surfboard as an example in the intro. Absolutely correct. We have a reputation as Motor City, but Michigan&#8217;s innovation record is a lot weirder in a fun way and richer than just cars.Early 20th century, mostly industrial moonshot innovation. So first paved mile of concrete was in Detroit in 1909. A few years later, this is when the auto sector started to really come about with Henry Ford&#8217;s moving assembly line. Everyone tends to know about those details. But during World War II, Willow Run Airport sort of smack between Detroit and Ann Arbor, Michigan they had the biggest airplane factory in the world. They were cranking out B-24 bombers once every 63 minutes, and I&#8217;ve actually been to the office that Henry Ford and Charles Lindbergh shared. It&#8217;s still at the airport. And it was pretty cool because Henry Ford had a window built into the office that looked sort of around the corner so that he could tick off as airplanes rolled out of the hanger and make sure that they were following the same high rate production mentality that the auto sector was able to develop over the decades prior.&nbsp;



And so they came in to help make sure that you could leverage that industrial sector to drive very rapid production, the at-scale mentality, which is also a really important part of the notion of re-industrialization that is taking hold across the country now. Happy to get into that a bit, but yeah, Willow Run, I don&#8217;t think most folks realize that that was the biggest airplane factory in the world sitting right here in Michigan.And all of this provided the mass production DNA that was able to help build the statewide supplier base. And today, yes, we use that for automotive, EVs, space hardware, batteries, you name it. But this is the foundation, I think, that we&#8217;ve got to be able to build on in the future. In the few decades since you saw innovations in sports, space, advanced materials, it&#8217;s like the sixties to the eighties. You said the snowboard. That was invented in Muskegon on the west side of the state in 1965.Dow Chemical&#8217;s here in a really big way. They&#8217;ve pioneered silicone and advanced plastics in Michigan. University of Michigan&#8217;s Dr. Thomas Francis is the world&#8217;s first successful polio vaccine trials that were pioneered out of Ann Arbor, and that Big 10 research horsepower that we&#8217;ve got in the state, between the University of Michigan, Michigan State University. We also have Wayne State University in Detroit, which is a powerhouse. And then Michigan Tech University in the Upper Peninsula just recently became an R1 research institution, which essentially means those top-tier research powerhouses and that culture of tinkering matter a lot today.I think in more recent history, you saw design and digital innovations emerge. I don&#8217;t think a lot of people appreciate that Herman Miller and Steelcase reinvented office ergonomics on the west side of the state, or that Stryker is based in Kalamazoo. They became a global medical device powerhouse over the last couple of decades, too. Michigan&#8217;s first unicorn, Duo Security, the two-factor authentication among many other things that they do there, was sold to Cisco in 2018 for 2.35 billion.



Like I said, the first unicorn in the few years since we&#8217;ve had another 10 unicorns. And I think probably what would be surprising to a lot of people is it&#8217;s in sectors well beyond mobility, it&#8217;s marketplace like StockX, FinTech, logistics, cybersecurity, of course. It&#8217;s a little bit of everything, and I think that goes to show that some of the fabric that exists within Michigan is a lot richer than what people think of, Motor City. We can scale software, we can scale life sciences innovation. It&#8217;s not just metal bending, and I talked about re-industrialization earlier. So I think about where we are today, there&#8217;s a hard tech renaissance and a broad portfolio of other high-growth sectors that Michigan&#8217;s poised to do really well in, leveraging all of that industrial base that has been around for the last century. I&#8217;m just super excited about the future and where we can take things from here.



Megan: I mean, genuinely, a really rich and diverse history of innovation that you&#8217;ve described there.



Ben: That&#8217;s right.



Megan: And last year, when Michigan&#8217;s Governor Whitmer announced this new initiative and your position, she noted the need to foster this sort of culture of innovation. And we hear that a lot that terminal in the context of company cultures. It&#8217;s interesting to hear in the context of a U.S. state&#8217;s economy. I wonder what your strategy is for building out this ecosystem, and how do you foster a state&#8217;s innovation culture?



Ben: Yeah, it&#8217;s an awesome point, and I think I mentioned earlier that I came into the role with this builder‚Äôs mentality. For me, this is how I am wired to think. This is how a lot of the companies and other founders that I spent a lot of time with, this is how they think. And so bringing this to the state government, I think of Blue Origin, Jeff Bezos&#8217; space company, their motto, the English translation at least of it, is ‚ÄúStep by Step, Ferociously.‚Äù And I think about that as a lot as a proxy for how I do that within the state government. There&#8217;s a lot of iterative work that needs to happen, a lot of coaching and storytelling that happens to help folks understand how to think with that builder&#8217;s mindset. The wonderful news is that when you start having that conversation, this is one of those in these complicated political times, this is a pretty bipartisan thing, right?The notion of how to build small businesses that create thriving main street communities while also supporting high-growth, high-tech startups that can drive prosperity for all, and population growth, while also being able to cover corporate innovation and technology transfer out of universities. All of these things touch every corner of the state.And Michigan&#8217;s a surprisingly large and very geographically diverse state. Most of the things that we tend to be known for outside the state are in a pretty small corner of Southeast Michigan. That&#8217;s the Motor City part, but we do a lot and we have a lot of really interesting hubs for innovation and hubs for entrepreneurship, like I said, from the small mom-and-pop manufacturing shop or interest in clothing business all the way through to these insane life sciences innovations being spun out of the university. Being able to drive this culture of innovation ends up being applicable really across the board, and it just gets people really fired up when you start talking about this, fired up in a good way, which is, I think, what&#8217;s really fantastic.



There&#8217;s this notion of accelerating the talent flywheel and making sure that the state can invest in the cultivation of really rich communities and connections, and this founder culture. That stuff happens organically, generally, and when you talk about building startup ecosystems, it&#8217;s not like the state shows up and says, &#8220;Now you&#8217;re going to be more innovative and that works.&#8221; That is not the case.And so to be able to develop those things, it&#8217;s much more about this notion of ecosystem building and getting the ingredients and puzzle pieces in the right place, applying a little bit of funding here and there, or loosening a restriction here or there, and then letting the founders do what they do best, which is build. And so this is what I think I end up being super passionate about within the state. You can lead by example in a lot of these ways, and that flywheel that I mentioned really can get going in a beautiful way when you step out of the prescriptive innovation culture mindset.



Megan: And given that role, I wonder what milestones the campaign has experienced in your first year? Could you share some highlights and some developing projects that you&#8217;re really excited about?



Ben: We had a recent one, I think that was pretty tremendous. Just a couple of months ago, Governor Whitmer signed into law a bipartisan legislation called the Michigan Innovation Fund. This was a multi-year effort that resulted in the state&#8217;s biggest investment in the innovation ecosystem development in over two decades. A lot of this funding is going to early stage venture capital firms that will be able to support the broad seeding of new companies and ideas, keep talent within the state from some of those top tier research institutions, bring in really high quality companies that early stage, growth stage companies from out of state, and then develop or supercharge some of that innovation ecosystem fabric that ties those things together. So that connective tissue that I talked about, and that was an incredible win to launch the year with.



This was just back in January, and now we&#8217;re working to get some of those funds out over the course of the next month or two so we can put them to use. What was really interesting about that was, it wasn&#8217;t just a top-down thing. This was supported from the top all the way up to and including Governor Whitmer. I mentioned bipartisan support within Michigan&#8217;s legislature and then bottom-up from all of the ecosystem partners, the founders, the investors advocating as a whole block, which I think is really powerful. Rather than trying to go for one-off things, this huge coalition of the willing got together organically and advocated for, hey, this is why this is such a great moment. This is the time to invest. And Governor Whitmer and the legislators, they heard that call, and we got something done, and so that happened relatively quickly. Like I said, biggest investment in the last two decades, and I think we&#8217;re poised to have some really great successes in the coming year as well.Another really interesting one that I haven&#8217;t seen other states do yet, Governor Whitmer, around a year ago, signed an executive order called the Infrastructure for Innovation. Essentially, what that does is it opens up state department and agency assets to startups in the name of moving the ball forward on innovation projects. And so if you&#8217;re a startup and you need access to some very hard-to-find, very expensive, maybe like a test facility, you can use something that the state has, and all of the processes to get that done are streamlined so that you&#8217;re not beating your head against a wall. Similarly, the universities and even federal labs and corporate resources, while an executive order can&#8217;t compel those folks to do that, we&#8217;ve been finding tremendous buy-in from those stakeholders who want to volunteer access to their resources.



That does a lot of really good things, certainly for the founders, that provides them the launchpad that they need. But for those corporations and universities, and whatnot, a lot of them have these very expensive assets sitting around wildly underutilized, and they would be happy to have people come in and use them. That also gives them exposure to some of the bleeding-edge technology that a lot of these startups today are developing. I thought that was a really cool example of state government leadership using some of the tools that are available to a governor to get things moving. We&#8217;ve had a lot of early wins with startups here that have been able to leverage what that executive order was able to do for them.Here we are talking about the MIT Technology Review to tie in an MIT piece here, we also started a Team Michigan for MIT&#8217;s REAP program. It&#8217;s the Regional Entrepreneurship Acceleration Program, and this is one of the global thought leaders on best practices for innovation ecosystem development. And so we&#8217;ve got a cohort of about a dozen key leaders from across all of those different stakeholders who need to have a seat at the table for this ecosystem development.We go out to Cambridge twice a year for a multi-day workshop, and we get to talk about what we&#8217;ve learned as best practices, and then also learn from other cohorts from around the world on what they&#8217;ve done that is great. And then also get to hear some of the academic best practices that the MIT faculty have discovered as part of this area of expertise. And so that&#8217;s been a very interesting way for us to be able to connect outside of the state government boundaries, if you will. You sort of get out there and see where the leading edge is and then come back and be able to talk about the things that we learned from all of these other global cohorts. So always important to be focused on best practices when you&#8217;re trying to do new things, especially in government.



Megan: Sounds like there are some really fantastic initiatives going on. It sounds like a very busy first year.



Ben: It&#8217;s been a very busy first year couldn&#8217;t be more thrilled about it.



Megan: Fantastic. And in early 2023, I know that Newlab partnered with Michigan Central to establish a startup incubator too, which brought in more than a hundred startups just in its first 14 months. I wonder if you could talk a bit about how the incubator fits in with the statewide startup ecosystem and the importance of partnerships, too, for innovation.



Ben: Yeah, a key element, and I think the partnerships piece is essential here. Newlab is one of the larger components of the Southeast Michigan and especially the Detroit innovation ecosystem development. They will hit their two-year launch anniversary in just a couple of weeks, here I think. This will be mid-May, it will be two years and in that time, they&#8217;ve now got 140 plus startups all working out of their space, and Newlab they&#8217;re actually headquartered in Brooklyn, New York, but they run this big startup accelerator incubator out of Detroit as well and so this is sort of their second flagship location. They&#8217;ve been a phenomenal partner, and so speaking of the partnerships, what do those do?They de-risk the technologies to help enable broader adoptions. Corporations can provide early revenues, the state can provide non-dilutive grant matching. Universities can bring IP and this renewable source of talent generation, and being able to stitch together all of those pieces can create some really interesting unlocks for startups to grow. But again, also this broader entrepreneurship and innovation ecosystem to really be able to thrive.Newlab has been thrilled with their partnership in Southeast Michigan, and I think it&#8217;s a model that can be tailored across the state so that, depending on what assets are available in your backyard, you can make sure that you can best harness those for future growth.



Megan: Fantastic. What&#8217;s the long-term vision for the state&#8217;s innovation landscape when you think about it in five, 10 years from now? What do you envisage?



Ben: Amazing question. This is probably what I get most excited about. I think earlier we talked about the Willow Run B-24 bomber plant. That is what made Michigan known as the arsenal of democracy back in the day. I want Michigan to be the arsenal of innovation. We&#8217;re not trying to recreate a Silicon Valley. Silicon Valley does certain things, not trying to recreate what El Segundo wants to do in hard tech or New York City in FinTech, and all of these other things. We want to develop the thing that makes the most sense for the ingredients that Michigan can bring to bear to this challenge.



I think that becoming the Midwest arsenal of innovation, that&#8217;s something that Michigan is very well poised to use as a springboard for the decades to come. I want us to be the default launch pad for building a hard tech company, a life sciences company, an agricultural tech company. You name it. If you&#8217;ve got a design prototype and want to mass produce something, don&#8217;t want to hop coast, you want to be somewhere that has a tremendous quality of life, an affordable place, somewhere that government is at the table and willing to move fast, this is a place to do that.That can be difficult to do in some of the more established ecosystems, especially post-covid, as a lot of them are going through really big transition periods. Michigan&#8217;s already a top 10 state for business in the next 10 years. I want us to be a top 10 state for employment, top 10 state for household median income for post-secondary education attainment, and net talent migration. Those are my four top tens that I want to see in the next 10 years. And we covered a lot of topics today, but I think those are the reasons that I am super optimistic about being able to accomplish those.



Megan: Fantastic. Well, I&#8217;m tempted to move to Michigan, so I&#8217;m sure plenty of other people will be now, too. Thank you so much, Ben. That was really fascinating.



Ben: Thanks, Megan. Really delighted to be here.



Megan: That was Ben Marchionna, chief innovation ecosystem officer at the Michigan Economic Development Corporation, whom I spoke with from Brighton, England.&nbsp;



That&#8217;s it for this episode of Business Lab. I&#8217;m your host, Megan Tatum. I&#8217;m a contributing editor and host for Insights, the custom publishing division of MIT Technology Review. We were founded in 1899 at the Massachusetts Institute of Technology, and you can find us in print on the web and at events each year around the world. For more information about us and the show, please check out our website at technologyreview.com.



This show is available wherever you get your podcasts, and if you enjoy this episode, we hope you&#8217;ll take a moment to rate and review us. Business Lab is a production of MIT Technology Review. This episode was produced by Giro Studios. Thanks ever so much for listening.



This content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review‚Äôs editorial staff.



This content was researched, designed, and written entirely by human writers, editors, analysts, and illustrators. This includes the writing of surveys and collection of data for surveys. AI tools that may have been used were limited to secondary production processes that passed thorough human review.
‚Ä¢ Battling next-gen financial fraud
  A criminal network defrauded elderly victims in the US out of $21 million in total between 2021 and 2024 . The proliferation of large language models (LLMs) has also made it possible to clone a voice with nothing more than an hour of YouTube footage and an $11 subscription . Criminals are using such tools to create increasingly more sophisticated attacks to deceive victims with alarming success . Text-to-speech tools powered by AI can bypass voice authentication systems with ease .
‚Ä¢ The Download: hunting an asteroid, and unlocking the human mind
  This is today&#8217;s edition of¬†The Download,¬†our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



Inside the most dangerous asteroid hunt ever



If you were told that the odds of something were 3.1%, it might not seem like much. But for the people charged with protecting our planet, it was huge.On February 18, astronomers determined that a 130- to 300-foot-long asteroid had a 3.1% chance of crashing into Earth in 2032. Never had an asteroid of such dangerous dimensions stood such a high chance of striking the planet. Then, just days later on February 24, experts declared that the danger had passed. Earth would be spared.How did they do it? What was it like to track the rising danger of this asteroid, and to ultimately determine that it‚Äôd miss us?This is the inside story of how a sprawling network of astronomers found, followed, mapped, planned for, and finally dismissed the most dangerous asteroid ever found‚Äîall under the tightest of timelines and, for just a moment, with the highest of stakes. Read the full story.



‚ÄîRobin George AndrewsThis article is part of the Big Story series: MIT Technology Review‚Äôs most important, ambitious reporting. The stories in the series take a deep look at the technologies that are coming next and what they will mean for us and the world we live in. Check out the rest of them here.







How scientists are trying to use AI to unlock the human mind&nbsp;



Today‚Äôs AI landscape is defined by the ways in which neural networks are unlike human brains. A toddler learns how to communicate effectively with only a thousand calories a day and regular conversation; meanwhile, tech companies are reopening nuclear power plants, polluting marginalized communities, and pirating terabytes of books in order to train and run their LLMs.



Despite that, it‚Äôs a common view among neuroscientists that building brainlike neural networks is one of the most promising paths for the field, and that attitude has started to spread to psychology.&nbsp;



Last week, the prestigious journal Nature published a pair of studies showcasing the use of neural networks for predicting how humans and other animals behave in psychological experiments. However, predicting a behavior and explaining how it came about are two very different things. Read the full story.



‚ÄîGrace Huckins



This story originally appeared in The Algorithm, our weekly newsletter on AI. To get it in your inbox first every Monday, sign up here.







Why the US and Europe could lose the race for fusion energy



‚ÄîDaniel F. Brunner, Edlyn V. Levine, Fiona E. Murray, &amp; Rory Burke



Fusion energy holds the potential to shift a geopolitical landscape that is currently configured around fossil fuels. Harnessing fusion will deliver the energy resilience, security, and abundance needed for all modern industrial and service sectors.But these benefits will be controlled by the nation that leads in both developing the complex supply chains required and building fusion power plants at scales large enough to drive down economic costs.&nbsp;



Investing in supply chains and scaling up complex production processes has increasingly been a strength of China‚Äôs and a weakness of the West, resulting in the migration of many critical industries from the West to China. With fusion, we run the risk that history will repeat itself. But it does not have to go that way. Read the full story.







The must-reads



I‚Äôve combed the internet to find you today‚Äôs most fun/important/scary/fascinating stories about technology.



1 Donald Trump has announced a range of new tariffs¬†¬†Southeast Asia has been hit particularly hard. (Reuters)+ Some tariffs on other countries have been delayed until next month. (Vox)+ Investors are hoping to weather the storm. (Insider $)+ Sweeping tariffs could threaten the US manufacturing rebound. (MIT Technology Review)



2 Ukraine‚Äôs fiber-optic drones are giving it the edge over RussiaThe drones are impervious to electronic attacks. (WSJ $)+ Trump is resuming sending arms to Ukraine. (CNN)+ Meet the radio-obsessed civilian shaping Ukraine‚Äôs drone defense. (MIT Technology Review)



3 OpenAI is seriously scared about spiesIt‚Äôs upped its security dramatically amid fears of corporate espionage. (FT $)+ Inside the story that enraged OpenAI. (MIT Technology Review)



4 Amazon is asking its corporate staff to volunteer in its warehousesIt‚Äôs in desperate need of extra hands to help during its Prime Day event. (The Guardian)



5 Google‚Äôs AI-created drugs are almost ready for human trialsIsomorphic Labs has been working on drugs to tackle cancer. (Fortune $)+ An AI-driven ‚Äúfactory of drugs‚Äù claims to have hit a big milestone. (MIT Technology Review)



6 Apple‚Äôs AI ambitions have suffered yet another setbackTheir executive in charge of AI models has been wooed by Meta. (Bloomberg $)+ Ruoming Pang‚Äôs pay package is likely to be in the tens of millions. (WSJ $)



7 Waymo‚Äôs robotaxis are heading to NYCBut its ‚Äúroad trip‚Äù announcement is no guarantee it‚Äôll launch there. (TechCrunch)



8 Brands don‚Äôt need influencers any moreThey‚Äôre doing just fine producing their own in-house social media videos. (NYT $)



9 We may age in rapid bursts, rather than a steady declineNew research could shed light on how to slow the process down. (New Scientist $)+ Aging hits us in our 40s and 60s. But well-being doesn‚Äôt have to fall off a cliff. (MIT Technology Review)



10 This open-source software fights back against AI botsAnubis protects sites from scrapers. (404 Media)+ Cloudflare will now, by default, block AI bots from crawling its clients‚Äô websites. (MIT Technology Review)







Quote of the day



‚ÄúI think we‚Äôve all had enough of Elon‚Äôs political errors and political opinions.‚Äù



‚ÄîRoss Gerber, an investor who was formerly an enthusiastic backer of Elon Musk, tells the Washington Post he wishes the billionaire would simply focus on Tesla.







One more thing







How Silicon Valley is disrupting democracyThe internet loves a good neologism, especially if it can capture a purported vibe shift or explain a new trend. In 2013, the columnist Adrian Wooldridge coined a word that eventually did both. Writing for the Economist, he warned of the coming ‚Äútechlash,‚Äù a revolt against Silicon Valley‚Äôs rich and powerful, fueled by the public‚Äôs growing realization that these ‚Äúsovereigns of cyberspace‚Äù weren‚Äôt the benevolent bright-future bringers they claimed to be.While Wooldridge didn‚Äôt say precisely when this techlash would arrive, it‚Äôs clear today that a dramatic shift in public opinion toward Big Tech and its leaders did in fact ¬≠happen‚Äîand is arguably still happening. It‚Äôs worth investigating why, and what we can do to start taking some of that power back. Read the full story.



‚ÄîBryan Gardiner







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)



+ Struggling to solve a problem? It‚Äôs time to take a nap.+ If any TV show has better midcentury decor than Mad Men, I‚Äôve yet to see it.+ Sir Antony Gormley&#8217;s arresting iron men sculptures have been a fixture on Crosby Beach in the UK for 20 years.+ Check out this definitive Planet of the Apes timeline.
‚Ä¢ Why the US and Europe could lose the race for fusion energy
  Fusion energy holds the potential to shift a geopolitical landscape that is currently configured around fossil fuels. Harnessing fusion will deliver the energy resilience, security, and abundance needed for all modern industrial and service sectors. But these benefits will be controlled by the nation that leads in both developing the complex supply chains required and building fusion power plants at scales large enough to drive down economic costs.



The US and other Western countries will have to build strong supply chains across a range of technologies in addition to creating the fundamental technology behind practical fusion power plants. Investing in supply chains and scaling up complex production processes has increasingly been a strength of China‚Äôs and a weakness of the West, resulting in the migration of many critical industries from the West to China. With fusion, we run the risk that history will repeat itself. But it does not have to go that way.



The US and Europe were the dominant public funders of fusion energy research and are home to many of the world‚Äôs pioneering private fusion efforts. The West has consequently developed many of the basic technologies that will make fusion power work. But in the past five years China‚Äôs support of fusion energy has surged, threatening to allow the country to dominate the industry.







The industrial base available to support China‚Äôs nascent fusion energy industry could enable it to climb the learning curve much faster and more effectively than the West. Commercialization requires know-how, capabilities, and complementary assets, including supply chains and workforces in adjacent industries. And especially in comparison with China, the US and Europe have significantly under-supported the industrial assets needed for a fusion industry, such as thin-film processing and power electronics.



To compete, the US, allies, and partners must invest more heavily not only in fusion itself‚Äîwhich is already happening‚Äîbut also in those adjacent technologies that are critical to the fusion industrial base.&nbsp;



China‚Äôs trajectory to dominating fusion and the West‚Äôs potential route to competing can be understood by looking at today‚Äôs most promising scientific and engineering pathway to achieve grid-relevant fusion energy. That pathway relies on the tokamak, a technology that uses a magnetic field to confine ionized gas‚Äîcalled plasma‚Äîand ultimately fuse nuclei. This process releases energy that is converted from heat to electricity. Tokamaks consist of several critical systems, including plasma confinement and heating, fuel production and processing, blankets and heat flux management, and power conversion.



A close look at the adjacent industries needed to build these critical systems clearly shows China‚Äôs advantage while also providing a glimpse into the challenges of building a fusion industrial base in the US or Europe. China has leadership in three of these six key industries, and the West is at risk of losing leadership in two more. China‚Äôs industrial might in thin-film processing, large metal-alloy structures, and power electronics provides a strong foundation to establish the upstream supply chain for fusion.







The importance of thin-film processing is evident in the plasma confinement system. Tokamaks use strong electromagnets to keep the fusion plasma in place, and the magnetic coils must be made from superconducting materials. Rare-earth barium copper oxide (REBCO) superconductors are the highest-performing materials available in sufficient quantity to be viable for use in fusion.



The REBCO industry, which relies on thin-film processing technologies, currently has low production volumes spanning globally distributed manufacturers. However, as the fusion industry grows, the manufacturing base for REBCO will likely consolidate among the industry players who are able to rapidly take advantage of economies of scale. China is today‚Äôs world leader in thin-film, high-volume manufacturing for solar panels and flat-panel displays, with the associated expert workforce, tooling sector, infrastructure, and upstream materials supply chain. Without significant attention and investment on the part of the West, China is well positioned to dominate REBCO thin-film processing for fusion magnets.



The electromagnets in a full-scale tokamak are as tall as a three-story building. Structures made using strong metal alloys are needed to hold these electromagnets around the large vacuum vessel that physically contains the magnetically confined plasma. Similar large-scale, complex metal structures are required for shipbuilding, aerospace, oil and gas infrastructure, and turbines. But fusion plants will require new versions of the alloys that are radiation-tolerant, able to withstand cryogenic temperatures, and corrosion-resistant. China‚Äôs manufacturing capacity and its metallurgical research efforts position it well to outcompete other global suppliers in making the necessary specialty metal alloys and machining them into the complex structures needed for fusion.



A tokamak also requires large-scale power electronics. Here again China dominates. Similar systems are found in the high-speed rail (HSR) industry, renewable microgrids, and arc furnaces. As of 2024, China had deployed over 48,000 kilometers of HSR. That is three times the length of Europe‚Äôs HSR network and 55 times as long as the Acela network in the US, which is slower than HSR. While other nations have a presence, China‚Äôs expertise is more recent and is being applied on a larger scale.



But this is not the end of the story. The West still has an opportunity to lead the other three adjacent industries important to the fusion supply chain: cryo-plants, fuel processing, and blankets.&nbsp;



The electromagnets in an operational tokamak need to be kept at cryogenic temperatures of around 20 Kelvin to remain superconducting. This requires large-scale, multi-megawatt cryogenic cooling plants. Here, the country best set up to lead the industry is less clear. The two major global suppliers of cryo-plants are Europe-based Linde Engineering and Air Liquide Engineering; the US has Air Products and Chemicals and Chart Industries. But they are not alone: China‚Äôs domestic champions in the cryogenic sector include Hangyang Group, SASPG, Kaifeng Air Separation, and SOPC. Each of these regions already has an industrial base that could scale up to meet the demands of fusion.



Fuel production for fusion is a nascent part of the industrial base requiring processing technologies for light-isotope gases‚Äîhydrogen, deuterium, and tritium. Some processing of light-isotope gases is already done at small scale in medicine, hydrogen weapons production, and scientific research in the US, Europe, and China. But the scale needed for the fusion industry does not exist in today‚Äôs industrial base, presenting a major opportunity to develop the needed capabilities.



Similarly, blankets and heat flux management are an opportunity for the West. The blanket is the medium used to absorb energy from the fusion reaction and to breed tritium. Commercial-scale blankets will require entirely novel technology. To date, no adjacent industries have relevant commercial expertise in liquid lithium, lead-lithium eutectic, or fusion-specific molten salts that are required for blanket technology. Some overlapping blanket technologies are in early-stage development by the nuclear fission industry. As the largest producer of beryllium in the world, the US has an opportunity to capture leadership because that element is a key material in leading fusion blanket concepts. But the use of beryllium must be coupled with technology development programs for the other specialty blanket components.



These six industries will prove critical to scaling fusion energy. In some, such as thin-film processing and large metal-alloy structures, China already has a sizable advantage. Crucially, China recognizes the importance of these adjacent industries and is actively harnessing them in its fusion efforts. For example, China launched a fusion consortium that consists of industrial giants spanning the steel, machine tooling, electric grid, power generation, and aerospace sectors. It will be extremely difficult for the West to catch up in these areas, but policymakers and business leaders must pay attention and try to create robust alternative supply chains.



As the industrial area of greatest strength, cryo-plants could continue to be an opportunity for leadership in the West. Bolstering Western cryo-plant production by creating demand for natural-gas liquefaction will be a major boon to the future cryo-plant supply chain that will support fusion energy.



The US and European countries also have an opportunity to lead in the emerging industrial areas of fuel processing and blanket technologies. Doing so will require policymakers to work with companies to ensure that public and private funding is allocated to these critical emerging supply chains. Governments may well need to serve as early customers and provide debt financing for significant capital investment. Governments can also do better to incentivize private capital and equity financing‚Äîfor example, through favorable capital-gains taxation. In lagging areas of thin-film and alloy production, the US and Europe will likely need partners, such as South Korea and Japan, that have the industrial bases to compete globally with China.



The need to connect and capitalize multiple industries and supply chains will require long-term thinking and clear leadership. A focus on the demand side of these complementary industries is essential. Fusion is a decade away from maturation, so its supplier base must be derisked and made profitable in the near term by focusing on other primary demand markets that contribute to our economic vitality. To name a few, policymakers can support modernization of the grid to bolster domestic demand for power electronics and domestic semiconductor manufacturing to support thin-film processing.



The West must also focus on the demand for energy production itself. As the world‚Äôs largest energy consumer, China will leverage demand from its massive domestic market to climb the learning curve and bolster national champions. This is a strategy that China has wielded with tremendous success to dominate global manufacturing, most recently in the electric-vehicle industry. Taken together, supply- and demand-side investment have been a winning strategy for China.



The competition to lead the future of fusion energy is here. Now is the moment for the US and its Western allies to start investing in the foundational innovation ecosystem needed for a vibrant and resilient industrial base to support it.



Daniel F. Brunner is a co-founder of Commonwealth Fusion Systems and a Partner at Future Tech Partners.



Edlyn V. Levine is the co-founder of a stealth-mode technology start up and an affiliate of the MIT Sloan School of Management.



Fiona E. Murray is a professor of entrepreneurship at the MIT School of Management and&nbsp;Vice Chair of the NATO Innovation Fund.



Rory Burke is a graduate of MIT Sloan and a former summer scholar with&nbsp;ARPA-E.

üîí Cybersecurity & Privacy
‚Ä¢ Microsoft Patch Tuesday, July 2025 Edition
  Microsoft releases updates to fix at least 137 security vulnerabilities in its Windows operating systems and supported software . None of the weaknesses addressed this month are known to be actively exploited, but 14 of the flaws earned Microsoft‚Äôs most-dire &#8217;s most dire . The flaws could be exploited to seize control over vulnerable Windows PCs with little or no help from users . Microsoft also patched at least four critical, remote code execution flaws in Office .

üéì University AI
No updates.

üè¢ Corporate AI
‚Ä¢ AI Testing and Evaluation: Learnings from pharmaceuticals and medical devices
  Generative AI presents a unique challenge and opportunity to reexamine governance practices for the responsible development, deployment, and use of AI. To advance thinking in this space, Microsoft has tapped into the experience and knowledge of experts across domains‚Äîfrom genome editing to cybersecurity‚Äîto investigate the role of testing and evaluation as a governance tool. AI Testing and Evaluation: Learnings from Science and Industry, hosted by Microsoft Research‚Äôs Kathleen Sullivan, explores what the technology industry and policymakers can learn from these fields and how that might help shape the course of AI development.



In this episode, Daniel Carpenter, the Allie S. Freed Professor of Government and chair of the department of government at Harvard University, explains how the US Food and Drug Administration‚Äôs rigorous, multi-phase drug approval process serves as a gatekeeper that builds public trust and scientific credibility, while Timo Minssen, professor of law and founding director of the Center for Advanced Studies in Bioscience Innovation Law at the University of Copenhagen, explores the evolving regulatory landscape of medical devices with a focus on the challenges of balancing innovation with public safety. Later, Microsoft‚Äôs Chad Atalla, an applied scientist in responsible AI, discusses the sociotechnical nature of AI models and systems, their team‚Äôs work building an evaluation framework inspired by social science, and where AI researchers, developers, and policymakers might find inspiration from the approach to governance and testing in pharmaceuticals and medical devices.







Learn more:



Learning from other Domains to Advance AI Evaluation and Testing: The History and Evolution of Testing in Pharmaceutical RegulationCase study | January 2025&nbsp;



Learning from other Domains to Advance AI Evaluation and Testing: Medical Device Testing: Regulatory Requirements, Evolution and Lessons for AI GovernanceCase study | January 2025&nbsp;



Learning from other domains to advance AI evaluation and testing&nbsp;Microsoft Research Blog | June 2025‚ÄØ‚ÄØ&nbsp;



Evaluating Generative AI Systems is a Social Science Measurement Challenge&nbsp;Publication&nbsp;|&nbsp;November 2024‚ÄØ&nbsp;



STAC: Sociotechnical Alignment Center‚ÄØ



Responsible AI: Ethical policies and practices | Microsoft AI



AI and Microsoft Research‚ÄØ




	
		
			Subscribe to the Microsoft Research Podcast:		
		
							
					
						  
						Apple Podcasts
					
				
			
							
					
						
						Email
					
				
			
							
					
						
						Android
					
				
			
							
					
						
						Spotify
					
				
			
							
					
						
						RSS Feed
					
				
					
	




	
		
			
				
					

Transcript



[MUSIC]



KATHLEEN SULLIVAN: Welcome to AI Testing and Evaluation: Learnings from Science and Industry. I&#8217;m your host, Kathleen Sullivan.



As generative AI continues to advance, Microsoft has gathered a range of experts‚Äîfrom genome editing to cybersecurity‚Äîto share how their fields approach evaluation and risk assessment. Our goal is to learn from their successes and their stumbles to move the science and practice of AI testing forward. In this series, we&#8217;ll explore how these insights might help guide the future of AI development, deployment, and responsible use.



[MUSIC ENDS]



SULLIVAN: Today, I&#8217;m excited to welcome Dan Carpenter and Timo Minssen to the podcast to explore testing and risk assessment in the areas of pharmaceuticals and medical devices, respectively.



Dan Carpenter is chair of the Department of Government at Harvard University. His research spans the sphere of social and political science, from petitioning in democratic society to regulation and government organizations. His recent work includes the FDA Project, which examines pharmaceutical regulation in the United States.



Timo is a professor of law at the University of Copenhagen, where he is also director of the Center for Advanced Studies in Bioscience Innovation Law. He specializes in legal aspects of biomedical innovation, including intellectual property law and regulatory law. He&#8217;s exercised his expertise as an advisor to such organizations as the World Health Organization and the European Commission.



And after our conversations, we&#8217;ll talk to Microsoft&#8217;s Chad Atalla, an applied scientist in responsible AI, about how we should think about these insights in the context of AI.



Daniel, it&#8217;s a pleasure to welcome you to the podcast. I&#8217;m just so appreciative of you being here. Thanks for joining us today.



				
				
					



DANIEL CARPENTER:&nbsp;Thanks for having me.&nbsp;



SULLIVAN:&nbsp;Dan, before we dissect policy,&nbsp;let&#8217;s&nbsp;rewind the tape to your&nbsp;origin&nbsp;story. Can you take us to the moment that you first became fascinated with regulators rather than, say, politicians? Was there a spark that pulled you toward the FDA story?&nbsp;



CARPENTER:&nbsp;At one point during graduate school, I was studying a combination of American politics and political theory, and I did a summer interning at the Department of Housing and Urban Development. And I began to think, why don&#8217;t people study these administrators more and the rules they make, the, you know,&nbsp;inefficiencies, the efficiencies?&nbsp;Really more&nbsp;from,&nbsp;kind of,&nbsp;a descriptive standpoint, less from a normative standpoint.&nbsp;And I was reading a lot that summer about the Food and Drug Administration and some of the decisions it was making on AIDS drugs. That was&nbsp;a,&nbsp;sort of,&nbsp;a major, &#8230;



SULLIVAN: Right.&nbsp;



CARPENTER: &#8230; sort of, you know,&nbsp;moment in the news, in the global news as well as the national news during, I would say, what?&nbsp;The late&nbsp;‚Äô80s, early&nbsp;‚Äô90s? And&nbsp;so&nbsp;I began to&nbsp;look&nbsp;into&nbsp;that.



SULLIVAN:&nbsp;So now that we know what pulled you in,&nbsp;let‚Äôs&nbsp;zoom out for our listeners. Give us&nbsp;the&nbsp;whirlwind tour. I think most of us know pharma involves years of trials, but&nbsp;what‚Äôs&nbsp;the part we&nbsp;don‚Äôt&nbsp;know?



CARPENTER:&nbsp;So&nbsp;I think when most businesses develop a product, they all go through some phases of research and development and testing. And I think&nbsp;what&#8217;s&nbsp;different about the FDA is,&nbsp;sort of,&nbsp;two-&nbsp;or three-fold.



First, a lot of those tests are much more stringently specified and regulated by the government, and second, one of the reasons for that is that the FDA imposes not simply safety requirements upon drugs&nbsp;in particular but&nbsp;also efficacy requirements. The FDA wants you to prove not simply that&nbsp;it&#8217;s&nbsp;safe and non-toxic&nbsp;but also that&nbsp;it&#8217;s&nbsp;effective.&nbsp;And the final thing,&nbsp;I think, that&nbsp;makes the FDA different is that it stands as what I would call the&nbsp;‚Äúveto player‚Äù&nbsp;over R&amp;D [research and development] to the marketplace.&nbsp;The FDA&nbsp;basically has,&nbsp;sort of,&nbsp;this control over entry&nbsp;to&nbsp;the marketplace.



And¬†so¬†what that involves is usually first, a set of human trials where people who have no disease take it. And¬†you&#8217;re¬†only looking¬†for¬†toxicity generally. Then¬†there&#8217;s¬†a set of Phase 2 trials, where they look more at safety and a little bit at efficacy, and¬†you&#8217;re¬†now examining people who have the disease that the drug claims to treat. And¬†you&#8217;re¬†also basically comparing people who get the drug,¬†often¬†with those who do not.



And then finally, Phase 3 involves a much more direct and large-scale attack, if you will, or assessment of efficacy, and&nbsp;that&#8217;s&nbsp;where you get the sort of large randomized clinical trials that are&nbsp;very expensive&nbsp;for pharmaceutical companies, biomedical companies to launch, to execute, to analyze. And those are often the sort of core evidence base for the decisions that the FDA makes about&nbsp;whether or not&nbsp;to approve a new drug for marketing in the United States.



SULLIVAN:&nbsp;Are there&nbsp;differences in how that process has, you know, changed through other countries and&nbsp;maybe just&nbsp;how&nbsp;that&#8217;s&nbsp;evolved as&nbsp;you&#8217;ve&nbsp;seen it play out?&nbsp;



CARPENTER:&nbsp;Yeah, for a long time, I would say that the United States had&nbsp;probably the&nbsp;most&nbsp;stringent regime&nbsp;of regulation for biopharmaceutical products until,&nbsp;I would say,&nbsp;about the 1990s and early 2000s. It used to be the case that a number of other countries, especially in Europe but around the world, basically waited for the FDA to mandate tests on a drug and only after the drug was approved in the United States would they deem it approvable and marketable in their own countries. And then after the formation of the European Union and the creation of the European Medicines Agency, gradually the European Medicines Agency began to get a bit more stringent.&nbsp;&nbsp;



But, you know,&nbsp;over the long run,&nbsp;there&#8217;s&nbsp;been a&nbsp;lot of,&nbsp;sort&nbsp;of,&nbsp;heterogeneity, a lot of variation over time and space, in the way that the FDA has approached these problems. And&nbsp;I&#8217;d&nbsp;say in the last 20 years, it&#8217;s begun to partially deregulate, namely,&nbsp;you know,&nbsp;trying to find all sorts of mechanisms or pathways for really innovative&nbsp;drugs for deadly diseases without a lot of treatments to&nbsp;basically get&nbsp;through the process at lower cost.&nbsp;For many people,&nbsp;that has not been sufficient.&nbsp;They&#8217;re&nbsp;concerned about the cost of the system.&nbsp;Of course, then the agency also gets criticized by those&nbsp;who believe&nbsp;it&#8217;s&nbsp;too lax. It is&nbsp;potentially letting&nbsp;ineffective and unsafe therapies on the market.



SULLIVAN:&nbsp;In your view, when does the structured model genuinely safeguard patients and where do you think it&nbsp;maybe slows&nbsp;or&nbsp;limits&nbsp;innovation?



CARPENTER:&nbsp;So&nbsp;I think&nbsp;the worry&nbsp;is that if you approach pharmaceutical approval as a world where only things can go wrong,&nbsp;then&nbsp;you&#8217;re&nbsp;really at a risk of limiting innovation. And even if you end up letting a lot of things through, if by your regulations you end up basically slowing down the development process or making it very, very costly, then there&#8217;s just a whole bunch of drugs that either come to market too slowly or they come to market not at all because&nbsp;they just aren&#8217;t worth the kind of cost-benefit or, sort of, profit analysis of the firm.&nbsp;You know, so&nbsp;that&#8217;s&nbsp;been a concern.&nbsp;And I think&nbsp;it&#8217;s&nbsp;been one of the reasons that the Food and Drug Administration as well as other world regulators have begun to&nbsp;basically try&nbsp;to smooth the process and accelerate the process at the margins.



The other thing is that&nbsp;they&#8217;ve&nbsp;started to&nbsp;basically make&nbsp;approvals&nbsp;on the basis of&nbsp;what are called&nbsp;surrogate endpoints. So the idea is that a cancer drug, we really want to know whether that drug saves lives, but if we wait to see whose lives are saved or prolonged by that drug, we might miss the opportunity to make judgments on the basis of, well, are we detecting tumors in the bloodstream? Or can we measure the size of those tumors&nbsp;in, say, a&nbsp;solid cancer? And then the further question is, is the size of the tumor&nbsp;basically a&nbsp;really good&nbsp;correlate&nbsp;or predictor of whether people will die or&nbsp;not, right?&nbsp;Generally, the&nbsp;FDA tends to be less stringent when&nbsp;you&#8217;ve&nbsp;got, you know, a remarkably innovative new&nbsp;therapy&nbsp;and the disease being treated is one that just&nbsp;doesn&#8217;t&nbsp;have a lot of available treatments,&nbsp;right.



The one thing that people often think about when&nbsp;they&#8217;re&nbsp;thinking about pharmaceutical regulation is they often contrast,&nbsp;kind of,&nbsp;speed versus safety &#8230;



SULLIVAN:&nbsp;Right.&nbsp;&nbsp;



CARPENTER:&nbsp;&#8230; right. And&nbsp;that&#8217;s&nbsp;useful as a tradeoff,&nbsp;but I often try to remind people that&nbsp;it&#8217;s&nbsp;not simply&nbsp;about whether the drug gets out&nbsp;there&nbsp;and&nbsp;it&#8217;s&nbsp;unsafe. You know, you and I as patients and even doctors have&nbsp;a hard time&nbsp;knowing whether something works and whether it should be prescribed. And the evidence for knowing whether something works&nbsp;isn&#8217;t&nbsp;just, well,&nbsp;you&nbsp;know, Sally took&nbsp;it&nbsp;or Dan took it or Kathleen took it, and they&nbsp;seem to get&nbsp;better or they&nbsp;didn&#8217;t&nbsp;seem to get better.&nbsp;&nbsp;



The really rigorous evidence comes from randomized clinical trials.&nbsp;And I think&nbsp;it&#8217;s&nbsp;fair to say that if you didn&#8217;t&nbsp;have the FDA there as a veto player, you&nbsp;wouldn&#8217;t&nbsp;get as many randomized clinical&nbsp;trials&nbsp;and the evidence&nbsp;probably&nbsp;wouldn&#8217;t&nbsp;be as rigorous for whether these things work. And as I like to put it,&nbsp;basically there&#8217;s&nbsp;a whole ecology of expectations and beliefs around the biopharmaceutical industry in the United States and globally,&nbsp;and to some extent,&nbsp;it&#8217;s&nbsp;undergirded by&nbsp;all of&nbsp;these tests that happen.&nbsp;&nbsp;



SULLIVAN:&nbsp;Right.&nbsp;&nbsp;



CARPENTER:&nbsp;And in part, that means&nbsp;it&#8217;s&nbsp;undergirded by regulation. Would there still be a market without regulation? Yes. But it would be a market in which people had far less information in and confidence about the drugs that are being taken. And&nbsp;so&nbsp;I think&nbsp;it&#8217;s&nbsp;important to recognize that kind of confidence-boosting potential of, kind of, a scientific regulation base.&nbsp;



SULLIVAN:&nbsp;Actually, if we could&nbsp;double-click&nbsp;on that for a minute, I&#8217;d love to hear your perspective on, testing&nbsp;has been completed;&nbsp;there&#8217;s results.&nbsp;Can you walk us through how those results actually shape the next steps and decisions of a particular drug and just,&nbsp;like,&nbsp;how regulators actually think about using that data to influence really what happens next with it?



CARPENTER:&nbsp;Right.&nbsp;So&nbsp;it&#8217;s&nbsp;important to understand that every drug is approved for&nbsp;what&#8217;s called&nbsp;an indication. It can have a first primary&nbsp;indication, which is the main disease that it treats, and then others can be added as more evidence is shown. But a drug is not something that just kind of exists out there in the ether.&nbsp;It has to have the right form of administration.&nbsp;Maybe it&nbsp;should be injected.&nbsp;Maybe it&nbsp;should be ingested.&nbsp;Maybe it&nbsp;should&nbsp;be administered only at a clinic&nbsp;because it needs to be&nbsp;kind of administered&nbsp;in just the right way. As doctors will tell you, dosage is everything, right.&nbsp;&nbsp;



And&nbsp;so&nbsp;one of the reasons that you want those trials is not simply a, you know, yes or no answer about whether the drug works,&nbsp;right.&nbsp;It&#8217;s&nbsp;not simply if-then.&nbsp;It&#8217;s&nbsp;literally what&nbsp;goes into what you might call the dose response curve.&nbsp;You know, how much of this drug do we need to&nbsp;basically, you know,&nbsp;get the benefit? At what point does that fall off significantly that we can&nbsp;basically say, we can stop there? All that evidence comes from&nbsp;trials. And&nbsp;that&#8217;s&nbsp;the kind of evidence that is&nbsp;required&nbsp;on the basis of&nbsp;regulation.&nbsp;&nbsp;



Because&nbsp;it&#8217;s&nbsp;not simply a drug&nbsp;that&#8217;s&nbsp;approved.&nbsp;It&#8217;s&nbsp;a drug and a&nbsp;frequency&nbsp;of administration. It&#8217;s&nbsp;a&nbsp;method of administration.&nbsp;And&nbsp;so&nbsp;the drug&nbsp;isn&#8217;t&nbsp;just,&nbsp;there&#8217;s&nbsp;something to be taken off the shelf and popped into your mouth. I mean, sometimes&nbsp;that&#8217;s&nbsp;what happens, but even then,&nbsp;we want to know what the dosage is,&nbsp;right.&nbsp;We want to know what to look for in terms of side effects, things like that.



SULLIVAN:&nbsp;Going back to that point, I&nbsp;mean,&nbsp;it sounds like&nbsp;we&#8217;re&nbsp;making a lot of progress from a regulation perspective&nbsp;in, you know, sort of speed and getting things approved but doing it in a&nbsp;really balanced&nbsp;way. I mean, any other kind of closing thoughts on the tradeoffs there or where&nbsp;you&#8217;re&nbsp;seeing that going?



CARPENTER:¬†I think¬†you&#8217;re¬†going to see some move in the coming years‚Äîthere&#8217;s¬†already been some of it‚Äîto say, do we always need a¬†really large¬†Phase 3 clinical trial? And to what degree do we need the, like, you¬†know,¬†all the i&#8217;s dotted and the t&#8217;s crossed or a really,¬†really large¬†sample size?¬†And¬†I&#8217;m¬†open to innovation there.¬†I&#8217;m¬†also open to the idea that we consider, again, things like accelerated approvals or pathways for looking at¬†different kinds¬†of surrogate endpoints.¬†I do think, once we do that, then we also have to have some degree of follow-up.



SULLIVAN:&nbsp;So&nbsp;I know&nbsp;we&#8217;re&nbsp;getting&nbsp;close to&nbsp;out of time, but&nbsp;maybe just&nbsp;a quick rapid fire if&nbsp;you‚Äôre&nbsp;open to it. Biggest myth about clinical trials?



CARPENTER:&nbsp;Well, some people tend to think that the FDA performs them.&nbsp;You know,&nbsp;it&#8217;s&nbsp;companies that do it. And the only other thing I would say is the company that does a lot of the testing and even the innovating is not always the company that takes the drug to market, and it tells you something about how powerful regulation is in our system, in our world,&nbsp;that you often need a company that has dealt with the FDA quite a bit and knows all the regulations and knows how to dot the i&#8217;s and cross the t&#8217;s in order to get a drug across the finish line.



SULLIVAN:&nbsp;If you had a magic wand,&nbsp;what&#8217;s&nbsp;the one thing&nbsp;you&#8217;d&nbsp;change in regulation today?



CARPENTER:&nbsp;I would like people to think a little bit less about just speed versus safety and,&nbsp;again, more about this basic issue of confidence. I think&nbsp;it&#8217;s&nbsp;fundamental to everything that happens in markets but especially in biopharmaceuticals.



SULLIVAN:&nbsp;Such a great point.&nbsp;This has been really fun.&nbsp;Just thanks so much for being here today. We&#8217;re really excited to share your thoughts&nbsp;out to&nbsp;our listeners. Thanks.



[TRANSITION MUSIC]&nbsp;



CARPENTER:&nbsp;Likewise.&nbsp;



SULLIVAN:&nbsp;Now&nbsp;to&nbsp;the world of medical devices,&nbsp;I&#8217;m&nbsp;joined by Professor Timo&nbsp;Minssen. Professor Minssen, it&#8217;s&nbsp;great to have you here. Thank you for joining us today.&nbsp;



TIMO&nbsp;MINSSEN:&nbsp;Yeah, thank you very much,&nbsp;it&#8217;s&nbsp;a pleasure.



SULLIVAN:&nbsp;Before getting into the regulatory world of medical devices, tell our audience a bit about your personal journey or your origin story, as&nbsp;we&#8217;re&nbsp;asking our guests. How did you land in regulation, and what&#8217;s kept you hooked in this space?



MINSSEN:&nbsp;So&nbsp;I started out as a patent expert in the biomedical area, starting with my PhD thesis on patenting biologics in Europe and in the US.&nbsp;So&nbsp;during that time, I was mostly interested in patent and trade secret questions.&nbsp;But at the same time, I also developed and taught courses in regulatory law and held talks on regulating advanced medical therapy medicinal products.&nbsp;I&nbsp;then&nbsp;started to lead large research projects on legal challenges in a wide variety of health and life science innovation frontiers. I also started to focus increasingly on AI-enabled medical devices and software as a medical device, resulting in several academic articles in this area&nbsp;and also&nbsp;in the regulatory area and a book on the future of medical device regulation.&nbsp;&nbsp;



SULLIVAN:&nbsp;Yeah,&nbsp;what&#8217;s&nbsp;kept you hooked in&nbsp;the space?



MINSSEN:&nbsp;It&#8217;s&nbsp;just incredibly exciting,&nbsp;in particular right&nbsp;now with everything that is going on, you know, in the software arena, in the marriage between AI and medical devices. And this is really challenging not only societies but also regulators and authorities in Europe and in the US.



SULLIVAN:¬†Yeah,¬†it&#8217;s¬†a super exciting time to be in this space. You know, we talked to Daniel a little earlier and, you know, I think¬†similar to¬†pharmaceuticals, people have a general sense of what we mean when we say medical devices, but most listeners may¬†picture¬†like a stethoscope or a hip implant. The word &#8220;medical device&#8221;¬†reaches¬†much wider. Can you give us a quick, kind of, range from perhaps¬†very simple¬†to even, I don&#8217;t know, sci-fi and then your 90-second tour of how risk assessment works and why a framework is essential?



MINSSEN:&nbsp;Let me start out by saying that&nbsp;the WHO [World Health Organization] estimates that today there are approximately 2 million different kinds of medical devices on the world market, and as of the FDA&#8217;s latest update that I&#8217;m aware of, the FDA has authorized more than 1,000 AI-, machine learning-enabled medical devices, and that number is rising rapidly.



So in that context, I think it is important to understand that medical devices can be any instrument, apparatus, implement, machine, appliance, implant, reagent for in vitro use, software, material, or other similar or related articles that are&nbsp;intended&nbsp;by the manufacturer to be used alone or in combination for a medical purpose. And the spectrum of what constitutes a medical device can&nbsp;thus&nbsp;range from very simple devices such as tongue depressors, contact lenses, and thermometers to more complex devices such as blood pressure monitors, insulin pumps, MRI machines, implantable pacemakers, and even software as a medical device or AI-enabled monitors or drug device combinations, as well.



So&nbsp;talking about regulation,&nbsp;I think&nbsp;it&nbsp;is also&nbsp;very important&nbsp;to stress that medical devices are used in many diverse situations by&nbsp;very different&nbsp;stakeholders. And testing&nbsp;has to&nbsp;take this variety into consideration, and it is intrinsically tied to regulatory requirements across various&nbsp;jurisdictions.



During the pre-market phase, medical testing&nbsp;establishes&nbsp;baseline safety and effectiveness metrics through bench testing, performance standards, and clinical studies. And post-market testing ensures that real-world data informs ongoing compliance and safety improvements. So testing is indispensable in translating technological innovation into safe and effective medical devices. And while&nbsp;particular details&nbsp;of pre-market and post-market review procedures may slightly differ among countries, most developed&nbsp;jurisdictions regulate medical devices similarly to the US or European models.‚ÄØ



So&nbsp;most&nbsp;jurisdictions&nbsp;with medical device regulation classify devices based on their risk profile, intended use, indications for use, technological characteristics,&nbsp;and the regulatory controls necessary to provide a reasonable assurance of safety and effectiveness.



SULLIVAN:&nbsp;So medical devices face a pretty prescriptive multi-level testing path before they hit the market. From your vantage point, what are some of the downsides of that system and when does it make the most sense?



MINSSEN:&nbsp;One primary drawback is, of course, the lengthy and expensive approval process. High-risk devices, for example, often undergo years of clinical trials,&nbsp;which can cost millions of dollars, and this can create a significant barrier for startups and small companies with limited resources.&nbsp;And even for moderate-risk devices, the regulatory burden can slow product development and time to the market.



And the approach can also limit flexibility. Prescriptive requirements may not accommodate emerging innovations like digital therapeutics or AI-based diagnostics in&nbsp;a feasible&nbsp;way. And in such cases, the framework can unintentionally [stiffen]&nbsp;innovation by discouraging creative solutions or iterative improvements, which as matter of fact can also&nbsp;put&nbsp;patients&nbsp;at risk when you&nbsp;don&#8217;t&nbsp;use&nbsp;new technologies and AI.&nbsp;And&nbsp;additionally, the same level of scrutiny may be applied to low-risk devices, where&nbsp;the extensive testing and documentation may also be disproportionate to the actual patient risk.



However, the prescriptive model is highly&nbsp;appropriate where&nbsp;we have high testing standards for high-risk medical devices, in my view, particularly those that are life-sustaining, implanted, or involve new materials or mechanisms.



I also wanted to say that I think that these higher compliance thresholds can be OK and necessary if you have a system where authorities and stakeholders also have the capacity and funding to enforce, monitor, and achieve compliance with such rules in a feasible, time-effective, and straightforward manner. And this, of course, requires resources, novel solutions,&nbsp;and investments.



SULLIVAN:&nbsp;A range of tests are undertaken across the life cycle of medical devices.&nbsp;How do these testing requirements vary across&nbsp;different stages&nbsp;of development and across various applications?



MINSSEN:&nbsp;Yes,&nbsp;that&#8217;s&nbsp;a good question.&nbsp;So&nbsp;I think first it&nbsp;is important to realize that testing is conducted by various entities, including manufacturers, independent third-party laboratories, and regulatory agencies. And it occurs throughout the device&nbsp;life&nbsp;cycle, beginning with iterative testing during the research and development stage, advancing to pre-market evaluations, and continuing into post-market monitoring. And the outcomes of&nbsp;these tests directly&nbsp;impact&nbsp;regulatory approvals, market access, and device design refinements, as well.&nbsp;So&nbsp;the testing results are typically shared with regulatory authorities and in some cases with healthcare providers and the broader public to enhance transparency and trust.



So&nbsp;if you talk about the&nbsp;different phases&nbsp;that play a role here ‚Ä¶ so&nbsp;let&#8217;s&nbsp;turn to the pre-market phase, where manufacturers must&nbsp;demonstrate&nbsp;that the device is conformed to safety and performance benchmarks defined by regulatory authorities. Pre-market evaluations include functional bench testing, biocompatibility, for example, assessments and software validation, all of which are integral components of a manufacturer&#8217;s submission.&nbsp;



But, yes, but, testing also, and we touched already up on that, extends into the post-market phase, where it continues to ensure device safety and efficacy, and post-market surveillance relies on testing to&nbsp;monitor real-world performance and&nbsp;identify&nbsp;emerging risks on the post-market phase. By integrating real-world evidence into ongoing assessments, manufacturers can address unforeseen issues, update devices as needed, and&nbsp;maintain compliance with evolving regulatory expectations. And&nbsp;I think this&nbsp;is particularly important in this new generation of medical devices that are AI-enabled or machine-learning enabled.



I think we have to understand that in this AI-enabled medical devices field, you know, the devices and the algorithms that are working with&nbsp;them, they&nbsp;can improve in the lifetime of a product.&nbsp;So actually, not&nbsp;only you could assess them and make sure that they&nbsp;maintain&nbsp;safe,&nbsp;you&nbsp;could also sometimes lower the risk category by finding evidence that these devices are&nbsp;actually becoming&nbsp;more precise and safer.&nbsp;So&nbsp;it can both, you know, heighten the risk&nbsp;category&nbsp;or lower the risk category, and&nbsp;that&#8217;s&nbsp;why&nbsp;this continuous testing is so important.



SULLIVAN:&nbsp;Given what you just said, how should regulators handle a device whose algorithm keeps updating itself after approval?



MINSSEN:&nbsp;Well, it&nbsp;has to&nbsp;be an iterative process that is&nbsp;feasible&nbsp;and straightforward and that is based on a very efficient, both time efficient and performance efficient, communication between the regulatory authorities and the medical device developers, right. We need to have&nbsp;the sensors&nbsp;in place that spot potential changes, and we need to have&nbsp;the mechanisms&nbsp;in place that allow us to quickly react to these changes both regulatory wise&nbsp;and also&nbsp;in&nbsp;the&nbsp;technological way.‚ÄØ



So&nbsp;I think communication&nbsp;is important,&nbsp;and we need to have&nbsp;the pathways&nbsp;and&nbsp;the feedback&nbsp;loops in the regulation that quickly allow us to&nbsp;monitor&nbsp;these self-learning algorithms and devices.



SULLIVAN:&nbsp;It sounds like&nbsp;it&#8217;s&nbsp;just ‚Ä¶&nbsp;there&#8217;s&nbsp;such a delicate balance between advancing technology and really ensuring public safety. You know, if we clamp down too hard, we stifle that innovation. You already touched upon this a bit. But if&nbsp;we&#8217;re&nbsp;too lax, we risk unintended consequences. And&nbsp;I&#8217;d&nbsp;just love to hear how you think the field is balancing that and any learnings you can share.



MINSSEN:&nbsp;So&nbsp;this is&nbsp;very true, and&nbsp;you just touched upon a very central question also in our research and our writing. And this is also the&nbsp;reason why&nbsp;medical device regulation is so fascinating and continues to evolve in response to rapid advancements in technologies, particularly dual technologies&nbsp;regarding&nbsp;digital health, artificial intelligence, for example, and personalized medicine.



And finding the balance is tricky because also [a] related major future challenge relates to the increasing regulatory jungle and the complex interplay between evolving regulatory landscapes that regulate AI more generally.



We really need to make sure that the regulatory authorities that deal with this, that need to find the right balance to promote innovation and mitigate and prevent risks, need to have the&nbsp;capacity&nbsp;to do this.&nbsp;So&nbsp;this requires investments, and it also requires new ways to regulate this technology more flexibly, for example through regulatory sandboxes and so on.



SULLIVAN:&nbsp;Could you just expand upon that a bit and double-click on what it is&nbsp;you&#8217;re&nbsp;seeing there? What excites you about&nbsp;what&#8217;s&nbsp;happening in that space?



MINSSEN:&nbsp;Yes, well, the research of my group at the Center for Advanced Studies in Bioscience Innovation Law is&nbsp;very broad. I mean, we are looking into gene editing technologies. We are looking into new biologics. We are looking into medical&nbsp;devices,&nbsp;as well, obviously, but also other technologies&nbsp;in advanced medical computing.



And what we see across the line here is that there is an increasing demand for having more adaptive and flexible regulatory frameworks in these&nbsp;new technologies,&nbsp;in particular when&nbsp;they have new uses, regulations that are focusing more on the product rather than the process. And I have recently&nbsp;written&nbsp;a report, for example,&nbsp;for&nbsp;emerging biotechnologies and&nbsp;bio-solutions&nbsp;for the EU commission. And even in that area, regulatory sandboxes are increasingly important, increasingly considered.



So&nbsp;this idea of regulatory sandboxes has been developing originally in the financial sector, and it is now penetrating into&nbsp;other sectors, including synthetic biology, emerging biotechnologies, gene editing, AI, quantum technology, as&nbsp;well. This is&nbsp;basically creating&nbsp;an environment where actors can test&nbsp;new ideas&nbsp;in close collaboration and under the oversight of regulatory authorities.



But¬†to implement¬†this in the AI sector now also leads us to¬†a¬†lot of questions and challenges. For example, you need to have the¬†capacities¬†of authorities that are governing and¬†monitoring¬†and deciding¬†on these regulatory sandboxes. There are issues relating to competition law, for example, which¬†you¬†call antitrust law in the US, because the question is, who can enter the sandbox and how may they compete after they exit the sandbox? And there are many questions relating to, how¬†should we¬†work with these sandboxes and how¬†should we¬†implement these sandboxes?



[TRANSITION MUSIC]&nbsp;



SULLIVAN:&nbsp;Well, Timo, it has just been such a pleasure to speak with you today.



MINSSEN:&nbsp;Yes, thank you very much.&nbsp;



And now&nbsp;I&#8217;m&nbsp;happy to introduce Chad Atalla.



Chad&nbsp;is&nbsp;senior applied scientist&nbsp;in&nbsp;Microsoft Research&nbsp;New York City&#8217;s&nbsp;Sociotechnical Alignment Center, where they contribute to foundational responsible AI research and practical responsible AI solutions for teams across Microsoft.



Chad, welcome!



CHAD ATALLA:&nbsp;Thank you.



SULLIVAN:&nbsp;So&nbsp;we&#8217;ll&nbsp;kick off with a couple questions just to dive right in.&nbsp;So&nbsp;tell me a little bit more about the&nbsp;Sociotechnical Alignment Center,&nbsp;or&nbsp;STAC? I know it was founded in&nbsp;2022.&nbsp;I&#8217;d&nbsp;love to just learn a little bit more about what the group does, how&nbsp;you&#8217;re&nbsp;thinking about evaluating AI, and&nbsp;maybe just&nbsp;give us a sense of some of the projects&nbsp;you&#8217;re&nbsp;working on.



ATALLA:&nbsp;Yeah, absolutely. The name is quite a mouthful.



SULLIVAN:&nbsp;It is!&nbsp;[LAUGHS]&nbsp;



ATALLA:&nbsp;So&nbsp;let&#8217;s&nbsp;start by breaking that down and seeing what that means.



SULLIVAN:&nbsp;Great.



ATALLA: So modern AI systems are sociotechnical systems, meaning that the social and technical aspects are deeply intertwined. And&nbsp;we&#8217;re interested in aligning the behaviors of these sociotechnical&nbsp;systems with some values.&nbsp;Those could be societal values;&nbsp;they could be regulatory values, organizational values, etc. And to make this alignment happen, we need the ability to evaluate the systems.



So&nbsp;my team is broadly working on an evaluation framework that acknowledges the sociotechnical nature of the technology and the often-abstract nature of the concepts&nbsp;we&#8217;re&nbsp;actually interested&nbsp;in evaluating. As you noted,&nbsp;it&#8217;s&nbsp;an applied science team, so we split our time between some fundamental research and time to bridge the work into real products across the company. And I also want to note that to power this sort of work, we have an interdisciplinary team drawing upon the social sciences, linguistics, statistics, and,&nbsp;of course, computer science.



SULLIVAN:&nbsp;Well,&nbsp;I&#8217;m&nbsp;eager to get into our takeaways from the conversation with&nbsp;both Daniel&nbsp;and Timo. But&nbsp;maybe just&nbsp;to double-click on this for a minute, can you talk a bit about some of the overarching goals of the AI evaluations that you noted?&nbsp;



ATALLA:&nbsp;So&nbsp;evaluation is really the act of making valuative judgments based on some evidence, and in the case of AI evaluation, that evidence might be from tests or measurements, right.&nbsp;And the goal of why&nbsp;we&#8217;re doing this in the first place is to make decisions and claims most often.



So&nbsp;perhaps I&nbsp;am going to make a claim about a model that&nbsp;I&#8217;m&nbsp;producing, and I want to say that&nbsp;it&#8217;s&nbsp;better than this other model. Or we are asking whether a certain product is safe to ship.&nbsp;All of these decisions need to be informed by good evaluation and therefore good measurement or testing.&nbsp;And&nbsp;I&#8217;ll&nbsp;also note that in&nbsp;the regulatory conversation, risk&nbsp;is often what we want to evaluate. So that is a goal in and of itself. And&nbsp;I&#8217;ll&nbsp;touch more on that later.



SULLIVAN:&nbsp;I read a recent&nbsp;paper that you had put out with some of our colleagues from Microsoft Research, from the University of Michigan, and Stanford, and you were arguing that evaluating generative AI is&nbsp;the&nbsp;social-science measurement challenge.&nbsp;Maybe for&nbsp;those who&nbsp;haven&#8217;t&nbsp;read the paper, what does this mean? And can you tell us a little bit more about what motivated you and your coauthors?&nbsp;



ATALLA:&nbsp;So the measurement tasks involved in evaluating generative AI systems are often abstract and contested. So that means they cannot be directly measured and must instead [be] indirectly measured via other observable phenomena. So this is very different than the older machine learning paradigm, where, let&#8217;s say, for example, I had a system that took a picture of a traffic light and told you whether it was green, yellow, or red at a given time.&nbsp;



If we wanted to evaluate that system, the task is much simpler. But with the modern generative AI systems that are also general purpose, they have open-ended output, and language in a whole chat or multiple paragraphs being outputted can have a lot of different properties. And as I noted, these are general-purpose systems, so we don&#8217;t know exactly what task they&#8217;re supposed to be carrying out.



So¬†then the question becomes, if I want to make some decision or claim‚Äîmaybe I¬†want to make a claim that this system has human-level reasoning capabilities‚Äîwell, what does that mean? Do I have the same impression of what that means as you do? And how do we know whether the downstream, you know, measurements and tests that¬†I&#8217;m¬†conducting¬†actually will¬†support my notion of what it means to have human-level reasoning,¬†right?¬†Difficult questions. But luckily, social scientists have been dealing with these exact sorts of challenges for multiple decades in fields like education, political science, and psychometrics. So¬†we&#8217;re¬†really¬†attempting¬†to avoid reinventing the wheel here and trying to learn from their past methodologies.



And so the rest of the paper goes on to delve into&nbsp;a four-level framework, a measurement framework, that&#8217;s grounded in the measurement theory from the quantitative social sciences that takes us all the way from these abstract and contested concepts through processes to get much clearer and eventually reach reliable and valid measurements that can power our evaluations.



SULLIVAN:&nbsp;I love that. I mean,&nbsp;that&#8217;s&nbsp;the whole point of this podcast,&nbsp;too,&nbsp;right.&nbsp;Is&nbsp;to really&nbsp;build&nbsp;on those other learnings and frameworks that&nbsp;we&#8217;re&nbsp;taking from industries that have been thinking about this for much longer.&nbsp;Maybe from&nbsp;your vantage point, what are some of the biggest day-to-day hurdles in building solid AI evaluations&nbsp;and,&nbsp;I&nbsp;don&#8217;t&nbsp;know, do we need more shared standards? Are there&nbsp;bespoke methods? Are those&nbsp;the way to go? I would love&nbsp;to just&nbsp;hear your thoughts on that.



ATALLA:&nbsp;So&nbsp;let&#8217;s&nbsp;talk about some of those practical challenges. And I want to briefly go back to what I mentioned about risk before, all right.&nbsp;Oftentimes,&nbsp;some of the regulatory environment&nbsp;is requiring practitioners to measure the&nbsp;risk&nbsp;involved in deploying one of their models or AI systems. Now, risk is importantly a&nbsp;concept that includes both event and impact,&nbsp;right.&nbsp;So&nbsp;there&#8217;s&nbsp;the probability of some event occurring. For the case of AI evaluation,&nbsp;perhaps this&nbsp;is us seeing a certain AI behavior&nbsp;exhibited. Then there&#8217;s also the severity of the&nbsp;impacts,&nbsp;and this is a complex chain of effects in the real world that&nbsp;happen&nbsp;to people, organizations, systems, etc., and&nbsp;it&#8217;s&nbsp;a lot more challenging to&nbsp;observe&nbsp;the impacts,&nbsp;right.



So&nbsp;if we&#8217;re saying that we need to measure risk, we have to measure both the event and the&nbsp;impacts. But realistically, right now, the field is not doing&nbsp;a very good&nbsp;job of&nbsp;actually measuring&nbsp;the impacts. This requires vastly different techniques and methodologies where if I just wanted to measure something about the event itself, I can, you know, do that in a technical sandbox&nbsp;environment&nbsp;and&nbsp;perhaps have&nbsp;some automated methods to detect whether a certain AI behavior is being&nbsp;exhibited. But if I want to measure the impacts? Now,&nbsp;we&#8217;re&nbsp;in the realm of needing to have real people involved, and&nbsp;perhaps a&nbsp;longitudinal study where you have interviews, questionnaires, and more qualitative evidence-gathering techniques to&nbsp;truly understand&nbsp;the long-term impacts. So&nbsp;that&#8217;s&nbsp;a significant challenge.



Another is that, you know,&nbsp;let&#8217;s&nbsp;say we forget about the impacts for&nbsp;now&nbsp;and we focus on the event side of things. Still, we need datasets, we need&nbsp;annotations,&nbsp;and we need&nbsp;metrics to make this whole thing work. When I say we need datasets, if I want to test whether my system has good mathematical reasoning, what questions should I ask? What are my set of inputs that are relevant? And then when I get&nbsp;the&nbsp;response from the system, how do I annotate them? How do I know if it was a good response that&nbsp;did demonstrate mathematical reasoning or if it was a mediocre response? And then once I have an annotation of&nbsp;all of these outputs from the AI system, how do I aggregate those all up into a single informative number?



SULLIVAN:&nbsp;Earlier in this episode, we heard Daniel and&nbsp;Timo walk&nbsp;through the regulatory frameworks in pharma and medical devices.&nbsp;I&#8217;d&nbsp;be curious what pieces of those mature systems are already showing up or at least may&nbsp;be bubbling up in AI governance.



ATALLA:&nbsp;Great question. You know, Timo was talking about the pre-market and post-market testing difference. Of course, this is similarly important in the AI evaluation space. But again, these have different methodologies and serve different purposes.



So&nbsp;within the pre-deployment phase, we&nbsp;don&#8217;t&nbsp;have evidence of how people are going to use the system. And when we have these general-purpose AI systems,&nbsp;to understand what the risks are, we really need to have a sense of what might happen and how they might be used.&nbsp;So&nbsp;there are&nbsp;significant challenges there where I think we can learn from other fields and how they do pre-market testing. And the difference in that pre- versus post-market testing also ties to testing at&nbsp;different stages&nbsp;in the life cycle.



For AI systems, we already see some regulations saying you need to start with the base model and do some evaluation of the base model, some basic attributes, some core attributes,&nbsp;of that base model before you start putting it into any real products. But once we have a product in mind, we have a user base in mind, we have a specific task‚Äîlike maybe we&#8217;re going to integrate this model into Outlook and it&#8217;s going to help you write&nbsp;emails‚Äînow we suddenly have a much crisper picture of how the system will interact with the world around it. And again, at that stage, we need to think about another round of evaluation.



Another part that jumped out to me in what they were saying about pharmaceuticals is that sometimes approvals can be based on surrogate endpoints.&nbsp;So&nbsp;this is like&nbsp;we&#8217;re&nbsp;choosing some&nbsp;heuristic.&nbsp;Instead of measuring the long-term impact, which is what we&nbsp;actually care&nbsp;about,&nbsp;perhaps we&nbsp;have a proxy that we&nbsp;feel like&nbsp;is a good enough indicator of what that long-term impact might look like.&nbsp;&nbsp;



This is occurring in the AI evaluation space right now and is often perhaps even the default here since&nbsp;we&#8217;re not seeing that many studies of the long-term impact itself. We are seeing, instead, folks constructing these heuristics or proxies and saying if I see this behavior happen,&nbsp;I&#8217;m&nbsp;going to&nbsp;assume&nbsp;that it&nbsp;indicates&nbsp;this sort of impact will happen downstream. And&nbsp;that&#8217;s&nbsp;great.&nbsp;It&#8217;s&nbsp;one of the techniques that was used to speed up and reduce the barrier to innovation in&nbsp;the other&nbsp;fields. And I think&nbsp;it&#8217;s&nbsp;great that we are applying that in the AI evaluation space. But&nbsp;special care&nbsp;is,&nbsp;of course, needed to ensure that those heuristics and proxies you&#8217;re&nbsp;using are reasonable indicators of the greater outcome&nbsp;you&#8217;re&nbsp;looking for.



SULLIVAN:&nbsp;What are some of the promising ideas from&nbsp;maybe pharma&nbsp;or med device regulation that maybe haven&#8217;t&nbsp;made it to AI testing yet and&nbsp;maybe should? And where would you urge technologists, policymakers,&nbsp;and researchers to focus their energy next?



ATALLA:&nbsp;Well, one of the key things that jumped out to me in the discussion about pharmaceuticals was driving home the emphasis that there&nbsp;is&nbsp;a&nbsp;holistic&nbsp;focus on safety&nbsp;and&nbsp;efficacy. These go hand in hand&nbsp;and decisions must be made while considering both pieces of the picture. I would like to see that further emphasized in the AI evaluation space.



Often,&nbsp;we&nbsp;are seeing&nbsp;evaluations of risk being separated from evaluations of&nbsp;performance or quality&nbsp;or efficacy, but these two pieces of the puzzle really are not enough for us to make informed decisions independently.&nbsp;And that ties back into my desire to really also see us measuring the impacts.



So&nbsp;we see Phase 3 trials as something that occurs in the medical devices and pharmaceuticals field. That&#8217;s not something that we are doing an equivalent of in the AI evaluation space at this time.&nbsp;These are really&nbsp;cost intensive. They can last years and really involve careful monitoring of that holistic picture of safety and efficacy. And realistically, we are not going to be able to put that on the critical path to getting specific individual AI models or AI systems vetted before they&nbsp;go out&nbsp;into the world. However, I would love to see a world in which this sort of work is prioritized&nbsp;and funded or&nbsp;required. Think of how, with&nbsp;social media, it took quite a long time for us to understand that there are some long-term negative impacts on mental health, and we have the opportunity now, while the AI wave is still building,&nbsp;to start prioritizing and funding this sort of work. Let it run in the background and as soon as possible develop a good understanding of the subtle, long-term effects.



More broadly, I would love to see us focus on reliability and validity of the evaluations&nbsp;we&#8217;re&nbsp;conducting because trust in these decisions and claims is important. If we&nbsp;don&#8217;t&nbsp;focus on building reliable, valid, and trustworthy evaluations,&nbsp;we&#8217;re&nbsp;just going to continue to be flooded by a bunch of competing, conflicting, and&nbsp;largely meaningless&nbsp;AI evaluations.



SULLIVAN:&nbsp;In a number of the discussions we&#8217;ve had on this podcast, we talked about how it&#8217;s not just one entity that really needs to ensure safety across the board,&nbsp;and I‚Äôd&nbsp;just love to hear from you how you think about some of those ecosystem collaborations, and you know, from across &#8230; where we think about ourselves as more of a platform company or places that these AI models are being deployed more at the application level. Tell me a little bit about how you think about,&nbsp;sort&nbsp;of, stakeholders in that mix and where responsibility lies across the board.



ATALLA:&nbsp;It&#8217;s&nbsp;interesting. In this age of general-purpose AI technologies,&nbsp;we&#8217;re&nbsp;often&nbsp;seeing&nbsp;one company or organization&nbsp;being responsible for&nbsp;building the foundational model. And then many, many other people will take that model and build it into specific products that are designed for specific tasks and contexts.



Of course,&nbsp;in that, we already see that there is&nbsp;a responsibility&nbsp;of the owners of that foundational model to do some testing of the central model before they distribute it broadly. And then again, there is responsibility of all of the downstream individuals digesting that and turning it into products to consider the specific contexts that they are deploying into and how that may affect the risks we&#8217;re concerned with or the types of quality and safety and performance we need to evaluate.



Again, because that field of risks we may be concerned with is so broad, some of them also require an immense amount of&nbsp;expertise.&nbsp;Let&#8217;s&nbsp;think about whether AI systems can enable people to create dangerous chemicals or dangerous weapons at home. It&#8217;s not that every AI practitioner is going to have the knowledge to evaluate this, so in some of those cases, we really need third-party experts, people who are experts in chemistry, biology, etc., to come in and evaluate certain systems and models for those specific risks,&nbsp;as well.



So&nbsp;I think there&nbsp;are many reasons why multiple stakeholders need to be involved, partly from who owns what and&nbsp;is responsible for&nbsp;what and partly from the perspective of who has the&nbsp;expertise&nbsp;to meaningfully construct the evaluations that we need.



SULLIVAN:&nbsp;Well, Chad, this has just been great to connect, and in a few of our discussions,&nbsp;we&#8217;ve&nbsp;done a bit of a lightning round, so&nbsp;I&#8217;d&nbsp;love to just hear your&nbsp;30-second responses to a few of these questions. Perhaps&nbsp;favorite&nbsp;evaluation&nbsp;you&#8217;ve&nbsp;run so far this year?&nbsp;



ATALLA:&nbsp;So&nbsp;I&#8217;ve&nbsp;been involved in trying to evaluate some language models for whether they&nbsp;infer&nbsp;sensitive attributes about people. So&nbsp;perhaps&nbsp;you&#8217;re&nbsp;chatting with a&nbsp;chatbot,&nbsp;and it infers your religion or sexuality based on things&nbsp;you&#8217;re&nbsp;saying or how you sound,&nbsp;right.&nbsp;And in working to evaluate this, we&nbsp;encounter&nbsp;a lot of interesting questions. Or,&nbsp;like,&nbsp;what is a sensitive attribute? What makes these attributes sensitive, and what are the differences that make it inappropriate for an AI system to infer these things about a person? Whereas realistically, whenever I meet a person on the street, my&nbsp;brain is&nbsp;immediately&nbsp;forming&nbsp;first impressions and some assumptions about these people.&nbsp;So&nbsp;it&#8217;s&nbsp;a very interesting&nbsp;and thought-provoking evaluation to conduct and think about the norms that we place upon&nbsp;people&nbsp;interacting with other people and the norms we place upon&nbsp;AI systems&nbsp;interacting with other people.



SULLIVAN:&nbsp;That‚Äôs&nbsp;fascinating!&nbsp;I&#8217;d&nbsp;love to hear the AI&nbsp;buzzword&nbsp;you&#8217;d&nbsp;retire tomorrow.&nbsp;[LAUGHTER]



ATALLA:&nbsp;I would love to see the term ‚Äúbias‚Äù being&nbsp;used less when referring to fairness-related issues and systems. Bias happens to be a highly overloaded term in statistics and machine learning and has a lot of technical meanings and just&nbsp;fails to&nbsp;perfectly capture what we mean in the AI risk sense.



SULLIVAN:¬†And last one. One metric¬†we&#8217;re¬†not tracking enough.



ATALLA:¬†I would say over-blocking, and this comes into that connection between the holistic picture of safety and efficacy. It&#8217;s too easy to produce systems that throw safety to the wind and focus purely on utility or achieving some goal, but simultaneously, the other side of the picture is possible, where we can clamp down too hard and reduce the utility of our systems and block even benign and useful outputs just because they border on something sensitive.¬†So¬†it&#8217;s¬†important for us to track that over-blocking and actively track that tradeoff between safety and efficacy.



SULLIVAN:&nbsp;Yeah, we talk a lot about this on the podcast,&nbsp;too,&nbsp;of how do you both make things safe but also ensure innovation can&nbsp;thrive,&nbsp;and&nbsp;I think you&nbsp;hit the nail on the head with that last piece.



[MUSIC]&nbsp;



Well, Chad, this was&nbsp;really terrific. Thanks for joining us and thanks for your work and your&nbsp;perspectives. And another big thanks to Daniel and Timo for setting the stage earlier in the podcast.



And to our listeners, thanks for tuning in. You can find resources related to this podcast in the show notes. And if you want to learn more about how Microsoft approaches AI governance, you can visit microsoft.com/RAI.‚ÄØ



See you next time!‚ÄØ



[MUSIC FADES]

				
			
			
				Show more			
		
	

Opens in a new tabThe post AI Testing and Evaluation: Learnings from pharmaceuticals and medical devices appeared first on Microsoft Research.
‚Ä¢ Accelerate AI development with Amazon Bedrock API keys
  Today, we‚Äôre excited to announce a significant improvement to the developer experience of Amazon Bedrock: API keys. API keys provide quick access to the Amazon Bedrock APIs, streamlining the authentication process so that developers can focus on building rather than configuration. 
CamelAI is an open-source, modular framework for building intelligent multi-agent systems for data generation, world simulation, and task automation. 

 ‚ÄúAs a startup with limited resources, streamlined customer onboarding is critical to our success. The Amazon Bedrock API keys enable us to onboard enterprise customers in minutes rather than hours. With Bedrock, our customers can quickly provision access to leading AI models and seamlessly integrate them into CamelAI,‚Äù  
 said Miguel Salinas, CTO, CamelAI.
 
In this post, explore how API keys work and how you can start using them today. 
API key authentication 
Amazon Bedrock now provides API key access to streamline integration with tools and frameworks that expect API key-based authentication. The Amazon Bedrock and Amazon Bedrock runtime SDKs support API key authentication for methods including on-demand inference, provisioned throughput inference, model fine-tuning, distillation, and evaluation. 
The diagram compares the default authentication process to Amazon Bedrock (in orange) with the API keys approach (in blue). In the default process, you must create an identity in AWS IAM Identity Center or IAM, attach IAM policies to provide permissions to perform API operations, and generate credentials, which you can then use to make API calls. The grey boxes in the diagram highlight the steps that Amazon Bedrock now streamlines when generating an API key. Developers can now authenticate and access Amazon Bedrock APIs with minimal setup overhead. 
 
You can generate API keys in the Amazon Bedrock console, choosing between two types. 
With long-term API keys, you can set expiration times ranging from 1 day to no expiration. These keys are associated with an IAM user that Amazon Bedrock automatically creates for you. The system attaches the AmazonBedrockLimitedAccess managed policy to this IAM user, and you can then modify permissions as needed through the IAM service. We recommend using long-term keys primarily for exploration of Amazon Bedrock. 
Short-term API keys use the IAM permissions from your current IAM principal and expire when your account‚Äôs session ends or can last up to 12 hours. Short-term API keys use AWS Signature Version 4 for authentication. For continuous application use, you can implement API key refreshing with a script as shown in this example. We recommend that you use short-term API keys for setups that require a higher level of security. 
Making Your First API Call 
Once you have access to foundation models, getting started with Amazon Bedrock API key is straightforward. Here‚Äôs how to make your first API call using the AWS SDK for Python (Boto3 SDK) and API keys: 
Generate an API key 
To generate an API key, follow these steps: 
 
 Sign in to the AWS Management Console and open the Amazon Bedrock console 
 In the left navigation panel, select API keys 
 Choose either Generate short-term API key or Generate long-term API key 
 For long-term keys, set your desired expiration time and optionally configure advanced permissions 
 Choose Generate and copy your API key 
 
 
Set Your API Key as Environment Variable 
You can set your API key as an environment variable so that it‚Äôs automatically recognized when you make API requests: 
 
 # To set the API key as an environment variable, you can open a terminal and run the following command:
export AWS_BEARER_TOKEN_BEDROCK=${api-key} 
 
The Boto3 SDK automatically detects your environment variable when you create an Amazon Bedrock client. 
Make Your First API Call 
You can now make API calls to Amazon Bedrock in multiple ways: 
 
 Using curl 
   
   curl -X POST "https://bedrock-runtime.us-east-1.amazonaws.com/model/us.anthropic.claude-3-5-haiku-20241022-v1:0/converse" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $AWS_BEARER_TOKEN_BEDROCK" \
  -d '{
    "messages": [
        {
            "role": "user",
            "content": [{"text": "Hello"}]
        }
    ]
  }' 
    
 Using the Amazon Bedrock SDK: 
   
    
    import boto3

# Create an Amazon Bedrock client
client = boto3.client(
    service_name="bedrock-runtime",
    region_name="us-east-1"     # If you've configured a default region, you can omit this line
) 

# Define the model and message
model_id = "us.anthropic.claude-3-5-haiku-20241022-v1:0"
messages = [{"role": "user", "content": [{"text": "Hello"}]}]
   
response = client.converse(
    modelId=model_id,
    messages=messages,
)

# Print the response
print(response['output']['message']['content'][0]['text']) 
    
    
 You can also use native libraries like Python Requests: 
   
   import requests
import os

url = "https://bedrock-runtime.us-east-1.amazonaws.com/model/us.anthropic.claude-3-5-haiku-20241022-v1:0/converse"

payload = {
    "messages": [
        {
            "role": "user",
            "content": [{"text": "Hello"}]
        }
    ]
}

headers = {
    "Content-Type": "application/json",
    "Authorization": f"Bearer {os.environ['AWS_BEARER_TOKEN_BEDROCK']}"
}

response = requests.request("POST", url, json=payload, headers=headers)

print(response.text) 
    
 
Bridging developer experience and enterprise security requirements 
Enterprise administrators can now streamline their user onboarding to Amazon Bedrock foundation models. With setups that require a higher level of security, administrators can enable short-term API keys for their users. Short-term API keys use AWS Signature Version 4 and existing IAM principals, maintaining established access controls implemented by administrators. 
For audit and compliance purposes, all API calls are logged in AWS CloudTrail. API keys are passed as authorization headers to API requests and aren‚Äôt logged. 
Conclusion 
Amazon Bedrock API keys are available in 20 AWS Regions where Amazon Bedrock is available: US East (N. Virginia, Ohio), US West (Oregon), Asia Pacific (Hyderabad, Mumbai, Osaka, Seoul, Singapore, Sydney, Tokyo), Canada (Central), Europe (Frankfurt, Ireland, London, Milan, Paris, Spain, Stockholm, Zurich), and South America (S√£o Paulo). To learn more about API keys in Amazon Bedrock, visit the API Keys documentation in the Amazon Bedrock user guide. 
Give API keys a try in the Amazon Bedrock console today and send feedback to AWS re:Post for Amazon Bedrock or through your usual AWS Support contacts. 
 
About the Authors 
Sofian Hamiti is a technology leader with over 10 years of experience building AI solutions, and leading high-performing teams to maximize customer outcomes. He is passionate in empowering diverse talent to drive global impact and achieve their career aspirations. 
Ajit Mahareddy is an experienced Product and Go-To-Market (GTM) leader with over 20 years of experience in product management, engineering, and go-to-market. Prior to his current role, Ajit led product management building AI/ML products at leading technology companies, including Uber, Turing, and eHealth. He is passionate about advancing generative AI technologies and driving real-world impact with generative AI. 
Nakul Vankadari Ramesh is a Software Development Engineer with over 7 years of experience building large-scale distributed systems. He currently works on the Amazon Bedrock team, helping accelerate the development of generative AI capabilities. Previously, he contributed to Amazon Managed Blockchain, focusing on scalable and reliable infrastructure. 
Huong Nguyen is a Principal Product Manager at AWS. She is a product leader at Amazon Bedrock, with 18 years of experience building customer-centric and data-driven products. She is passionate about democratizing responsible machine learning and generative AI to enable customer experience and business innovation. Outside of work, she enjoys spending time with family and friends, listening to audiobooks, traveling, and gardening. 
Massimiliano Angelino is Lead Architect for the EMEA Prototyping team. During the last 3 and half years he has been an IoT Specialist Solution Architect with a particular focus on edge computing, and he contributed to the launch of AWS IoT Greengrass v2 service and its integration with Amazon SageMaker Edge Manager. Based in Stockholm, he enjoys skating on frozen lakes.
‚Ä¢ Accelerating data science innovation: How Bayer Crop Science used AWS AI/ML services to build their next-generation MLOps service
  The world‚Äôs population is expanding at a rapid rate. The growing global population requires innovative solutions to produce food, fiber, and fuel, while restoring natural resources like soil and water and addressing climate change. Bayer Crop Science estimates farmers need to increase crop production by 50% by 2050 to meet these demands. To support their mission, Bayer Crop Science is collaborating with farmers and partners to promote and scale regenerative agriculture‚Äîa future where farming can produce more while restoring the environment. 
Regenerative agriculture is a sustainable farming philosophy that aims to improve soil health by incorporating nature to create healthy ecosystems. It‚Äôs based on the idea that agriculture should restore degraded soils and reverse degradation, rather than sustain current conditions. The Crop Science Division at Bayer believes regenerative agriculture is foundational to the future of farming. Their vision is to produce 50% more food by restoring nature and scaling regenerative agriculture. To make this mission a reality, Bayer Crop Science is driving model training with Amazon SageMaker and accelerating code documentation with Amazon Q. 
In this post, we show how Bayer Crop Science manages large-scale data science operations by training models for their data analytics needs and maintaining high-quality code documentation to support developers. Through these solutions, Bayer Crop Science projects up to a 70% reduction in developer onboarding time and up to a 30% improvement in developer productivity. 
Challenges 
Bayer Crop Science faced the challenge of scaling genomic predictive modeling to increase its speed to market. It also needed data scientists to focus on building the high-value foundation models (FMs), rather than worrying about constructing and engineering the solution itself. Prior to building their solution, the Decision Science Ecosystem, provisioning a data science environment could take days for a data team within Bayer Crop Science. 
Solution overview 
Bayer Crop Science‚Äôs Decision Science Ecosystem (DSE) is a next-generation machine learning operations (MLOps) solution built on AWS to accelerate data-driven decision making for data science teams at scale across the organization. AWS services assist Bayer Crop Science in creating a connected decision-making system accessible to thousands of data scientists. The company is using the solution for generative AI, product pipeline advancements, geospatial imagery analytics of field data, and large-scale genomic predictive modeling that will allow Bayer Crop Science to become more data-driven and increase speed to market. This solution helps the data scientist at every step, from ideation to model output, including the entire business decision record made using DSE. Other divisions within Bayer are also beginning to build a similar solution on AWS based on the success of DSE. 
Bayer Crop Science teams‚Äô DSE integrates cohesively with SageMaker, a fully managed service that lets data scientists quickly build, train, and deploy machine learning (ML) models for different use cases so they can make data-informed decisions quickly. This boosts collaboration within Bayer Crop Science across product supply, R&amp;D, and commercial. Their data science strategy no longer needs self-service data engineering, but rather provides an effective resource to drive fast data engineering at scale. Bayer Crop Science chose SageMaker because it provides a single cohesive experience where data scientists can focus on building high-value models, without having to worry about constructing and engineering the resource itself. With the help of AWS services, cross-functional teams can align quickly to reduce operational costs by minimizing redundancy, addressing bugs early and often, and quickly identifying issues in automated workflows. The DSE solution uses SageMaker, Amazon Elastic Kubernetes Service (Amazon EKS), AWS Lambda, and Amazon Simple Storage Service (Amazon S3) to accelerate innovation at Bayer Crop Science and to create a customized, seamless, end-to-end user experience. 
The following diagram illustrates the DSE architecture. 
 
Solution walkthrough 
Bayer Crop Science had two key challenges in managing large-scale data science operations: maintaining high-quality code documentation and optimizing existing documentation across multiple repositories. With Amazon Q, Bayer Crop Science tackled both challenges, which empowered them to onboard developers more rapidly and improve developer productivity. 
The company‚Äôs first use case focused on automatically creating high-quality code documentation. When a developer pushes code to a GitHub repository, a webhook‚Äîa lightweight, event-driven communication that automatically sends data between applications using HTTP‚Äîtriggers a Lambda function through Amazon API Gateway. This function then uses Amazon Q to analyze the code changes and generate comprehensive documentation and change summaries. The updated documentation is then stored in Amazon S3. The same Lambda function also creates a pull request with the AI-generated summary of code changes. To maintain security and flexibility, Bayer Crop Science uses Parameter Store, a capability of AWS Systems Manager, to manage prompts for Amazon Q, allowing for quick updates without redeployment, and AWS Secrets Manager to securely handle repository tokens. 
This automation significantly reduces the time developers spend creating documentation and pull request descriptions. The generated documentation is also ingested into Amazon Q, so developers can quickly answer questions they have about a repository and onboard onto projects. 
The second use case addresses the challenge of maintaining and improving existing code documentation quality. An AWS Batch job, triggered by Amazon EventBridge, processes the code repository. Amazon Q generates new documentation for each code file, which is then indexed along with the source code. The system also generates high-level documentation for each module or functionality and compares the AI-generated documentation with existing human-written documentation. This process makes it possible for Bayer Crop Science to systematically evaluate and enhance their documentation quality over time. 
To improve search capabilities, Bayer Crop Science added repository names as custom attributes in the Amazon Q index and prefixed them to indexed content. This enhancement improved the accuracy and relevance of documentation searches. The development team also implemented strategies to handle API throttling and variability in AI responses, maintaining robustness in production environments. Bayer Crop Science is considering developing a management plane to streamline the addition of new repositories and centralize the management of settings, tokens, and prompts. This would further enhance the scalability and ease of use of the system. 
Organizations looking to replicate Bayer Crop Science‚Äôs success can implement similar webhook-triggered documentation generation, use Amazon Q Business for both generating and evaluating documentation quality, and integrate the solution with existing version control and code review processes. By using AWS services like Lambda, Amazon S3, and Systems Manager, companies can create a scalable and manageable architecture for their documentation needs. Amazon Q Developer also helps organizations further accelerate their development timelines by providing real-time code suggestions and a built-in next-generation chat experience. 

 ‚ÄúOne of the lessons we‚Äôve learned over the last 10 years is that we want to write less code. We want to focus our time and investment on only the things that provide differentiated value to Bayer, and we want to leverage everything we can that AWS provides out of the box. Part of our goal is reducing the development cycles required to transition a model from proof-of-concept phase, to production, and ultimately business adoption. That‚Äôs where the value is.‚Äù 
 ‚Äì Will McQueen, VP, Head of CS Global Data Assets and Analytics at Bayer Crop Science.
 
Summary 
Bayer Crop Science‚Äôs approach aligns with modern MLOps practices, enabling data science teams to focus more on high-value modeling tasks rather than time-consuming documentation processes and infrastructure management. By adopting these practices, organizations can significantly reduce the time and effort required for code documentation while improving overall code quality and team collaboration. 
Learn more about Bayer Crop Science‚Äôs generative AI journey, and discover how Bayer Crop Science is redesigning sustainable practices through cutting-edge technology. 
About Bayer 
Bayer is a global enterprise with core competencies in the life science fields of health care and nutrition. In line with its mission, ‚ÄúHealth for all, Hunger for none,‚Äù the company‚Äôs products and services are designed to help people and the planet thrive by supporting efforts to understand the major challenges presented by a growing and aging global population. Bayer is committed to driving sustainable development and generating a positive impact with its businesses. At the same time, Bayer aims to increase its earning power and create value through innovation and growth. The Bayer brand stands for trust, reliability, and quality throughout the world. In fiscal 2023, the Group employed around 100,000 people and had sales of 47.6 billion euros. R&amp;D expenses before special items amounted to 5.8 billion euros. For more information, go to www.bayer.com. 
 
About the authors 
Lance Smith is a Senior Solutions Architect and part of the Global Healthcare and Life Sciences industry division at AWS. He has spent the last 2 decades helping life sciences companies apply technology in pursuit of their missions to help patients. Outside of work, he loves traveling, backpacking, and spending time with his family. 
Kenton Blacutt is an AI Consultant within the Amazon Q Customer Success team. He works hands-on with customers, helping them solve real-world business problems with cutting-edge AWS technologies. In his free time, he likes to travel and run an occasional marathon. 
Karthik Prabhakar is a Senior Applications Architect within the AWS Professional Services team. In this role, he collaborates with customers to design and implement cutting-edge solutions for their mission-critical business systems, focusing on areas such as scalability, reliability, and cost optimization in digital transformation and modernization projects. 
Jake Malmad is a Senior DevOps Consultant within the AWS Professional Services team, specializing in infrastructure as code, security, containers, and orchestration. As a DevOps consultant, he uses this expertise to collaboratively works with customers, architecting and implementing solutions for automation, scalability, reliability, and security across a wide variety of cloud adoption and transformation engagements. 
Nicole Brown is a Senior Engagement Manager within the AWS Professional Services team based in Minneapolis, MN. With over 10 years of professional experience, she has led multidisciplinary, global teams across the healthcare and life sciences industries. She is also a supporter of women in tech and currently holds a board position within the Women at Global Services affinity group.
‚Ä¢ Combat financial fraud with GraphRAG on Amazon Bedrock Knowledge Bases
  Financial fraud detection isn‚Äôt just important to banks‚Äîit‚Äôs essential. With global fraud losses surpassing $40 billion annually and sophisticated criminal networks constantly evolving their tactics, financial institutions face an increasingly complex threat landscape. Today‚Äôs fraud schemes operate across multiple accounts, institutions, and channels, creating intricate webs designed specifically to evade detection systems. 
Financial institutions have invested heavily in detection capabilities, but the core challenge remains: how to connect the dots across fragmented information landscapes where the evidence of fraud exists not within individual documents or transactions, but in the relationships between them. 
In this post, we show how to use Amazon Bedrock Knowledge Bases GraphRAG with Amazon Neptune Analytics to build a financial fraud detection solution. 
The limitations of traditional RAG systems 
In recent years, Retrieval Augmented Generation (RAG) has emerged as a promising approach for building AI systems grounded in organizational knowledge. However, traditional RAG-based systems have limitations when it comes to complex financial fraud detection.The fundamental limitation lies in how conventional RAG processes information. Standard RAG retrieves and processes document chunks as isolated units, looking for semantic similarities between a query and individual text passages. This approach works well for straightforward information retrieval, but falls critically short in the following scenarios: 
 
 Evidence is distributed across multiple documents and systems 
 The connections between entities matter more than the entities themselves 
 Complex relationship chains require multi-hop reasoning 
 Structural context (like hierarchical document organization) provides critical clues 
 Entity resolution across disparate references is essential 
 
A fraud analyst intuitively follows connection paths‚Äîlinking an account to a phone number, that phone number to another customer, and that customer to a known fraud ring. Traditional RAG systems, however, lack this relational reasoning capability, leaving sophisticated fraud networks undetected until losses have already occurred. 
Amazon Bedrock Knowledge Bases with GraphRAG for financial fraud detection 
Amazon Bedrock Knowledge Bases GraphRAG helps financial institutions implement fraud detection systems without building complex graph infrastructure from scratch. By offering a fully managed service that seamlessly integrates knowledge graph construction, maintenance, and querying with powerful foundation models (FMs), Amazon Bedrock Knowledge Bases dramatically lowers the technical barriers to implementing relationship-aware fraud detection. Financial organizations can now use their existing transaction data, customer profiles, and risk signals within a graph context that preserves the critical connections between entities while benefiting from the natural language understanding of FMs. This powerful combination enables fraud analysts to query complex financial relationships using intuitive natural language to detect suspicious patterns that can result in financial fraud. 
Example fraud detection use case 
To demonstrate this use case, we use a fictitious bank (AnyCompany Bank) in Australia whose customers hold savings, checking, and credit card accounts with the bank. These customers perform transactions to buy goods and services from merchants across the country using their debit and credit cards. AnyCompany Bank is looking to use the latest advancements in GraphRAG and generative AI technologies to detect subtle patterns in fraudulent behavior that will yield higher accuracy and reduce false positives.A fraud analyst at AnyCompany Bank wants to use natural language queries to get answers to the following types of queries: 
 
 Basic queries ‚Äì For example, ‚ÄúShow me all the transactions processed by ABC Electronics‚Äù or ‚ÄúWhat accounts does Michael Green own?‚Äù 
 Relationship exploration queries ‚Äì For example, ‚ÄúWhich devices have accessed account A003?‚Äù or ‚ÄúShow all relationships between Jane Smith and her devices.‚Äù 
 Temporal pattern detection queries ‚Äì For example, ‚ÄúWhich accounts had transactions and device access on the same day?‚Äù or ‚ÄúWhich accounts had transactions outside their usual location pattern?‚Äù 
 Fraud detection queries ‚Äì For example, as ‚ÄúFind unusual transaction amounts compared to account history‚Äù or ‚ÄúAre there any accounts with failed transactions followed by successful ones within 24 hours?‚Äù 
 
Solution overview 
To help illustrate the core GraphRAG principles, we have simplified the data model to six key tables: accounts, transactions, individuals, devices, merchants, and relationships. Real-world financial fraud detection systems are much more complex, with hundreds of entity types and intricate relationships, but this example demonstrates the essential concepts that scale to enterprise implementations. The following figure is an example of the accounts table. 
 
The following figure is an example of the individuals table. 
 
The following figure is an example of the devices table. 
 
The following figure is an example of the transactions table. 
 
The following figure is an example of the merchants table. 
 
The following figure is an example of the relationships table. 
 
The following diagram shows the relationships among these entities: accounts, individuals, devices, transactions, and merchants. For example, the individual John Doe uses device D001 to access account A001 to execute transaction T001, which is processed by merchant ABC Electronics. 
 
In the following sections, we demonstrate how to upload documents to Amazon Simple Storage Service (Amazon S3), create a knowledge base using Amazon Bedrock Knowledge Bases, and test the knowledge base by running natural language queries. 
Prerequisites 
To follow along with this post, make sure you have an active AWS account with appropriate permissions to access Amazon Bedrock and create an S3 bucket to be the data source. Additionally, verify that you have enabled access to both Anthropic‚Äôs Claude 3.5 Haiku and an embeddings model, such as Amazon Titan Text Embeddings V2. 
Uplaod documents to Amazon S3 
In this step, you create an S3 bucket as the data source and upload the six tables (accounts, individuals, devices, transactions, merchants, and relationships) as Excel data sheets. The following screenshot shows our S3 bucket and its contents. 
 
Create a knowledge base 
Complete the following steps to create the knowledge base: 
 
 On the Amazon Bedrock console, choose Knowledge Bases under Builder tools in the navigation pane. 
 Choose Create and Knowledge Base with vector store. 
 
 
 
 In the Knowledge Base details section, provide the following information: 
   
   Enter a meaningful name for the knowledge base. 
   For IAM permissions, select Create and use a new service role to create a new AWS Identity and Access Management (IAM) role. 
   For Choose data source, select Amazon S3. 
   Choose Next. 
    
 
 
 
 In the Configure data source section, provide the following information: 
   
   Enter a data source name. 
   For Data source location, select the location of your data source (for example, we select This AWS account). 
   For S3 source, choose Browse S3 and choose the location where you uploaded the files. 
   For Parsing staretgy, select Amazon Bedrock default parser. 
   For Chunking strategy, choose Default chunking. 
   Choose Next. 
    
 
 
 
 In the Configure data storage and processing section, provide the following information: 
   
   For Embeddings model, choose Titan Text Embeddings V2. 
   For Vector store creation method, select Quick create a new vector store. 
   For Vector store type, select Amazon Neptune Analytics (GraphRAG). 
   Choose Next 
    
 
Amazon Bedrock chooses the FM as Anthropic‚Äôs Claude 3 Haiku v1 to automatically build graphs for our knowledge base. This automatically enables contextual enrichment. 
 
 
 Choose Create knowledge base. 
 Choose the knowledge base when it‚Äôs in Available status. 
 
 
 
 Select the data source and choose Sync, then wait for the sync process to complete. 
 
In the sync process, Amazon Bedrock ingests data files from Amazon S3, creates chunks and embeddings, and automatically extracts entities and relationships, creating the graph. 
 
Test the knowledge base and run natural language queries 
When the sync is complete, you can test the knowledge base. 
 
 In the Test Knowledge Base section, choose Select model. 
 Set the model as Anthropic‚Äôs Claude 3.5 Haiku (or another model of your choice) and then choose Apply. 
 
 
 
 Enter a sample query and choose Run. 
 
 
Let‚Äôs start with some basic queries, such as ‚ÄúShow me all transactions processed by ABC Electronics‚Äù or ‚ÄúWhat accounts does Michael Green own?‚Äù The generated responses are shown in the following screenshot. 
 
We can also run some relationship exploration queries, such as ‚ÄúWhich devices have accessed account A003?‚Äù or ‚ÄúShow all relationships between Jane Smith and her devices.‚Äù The generated responses are shown in the following screenshot. To arrive at the response, the model will do multi-hop reasoning where it will traverse multiple files. 
 
The model can also perform temporal pattern detection queries, such as ‚ÄúWhich accounts had transactions and device access on the same day?‚Äù or ‚ÄúWhich accounts had transactions outside their usual location pattern?‚Äù The generated responses are shown in the following screenshot. 
 
Let‚Äôs try out some fraud detection queries, such as ‚ÄúFind unusual transaction amounts compared to account history‚Äù or ‚ÄúAre there any accounts with failed transactions followed by successful ones within 24 hours?‚Äù The generated responses are shown in the following screenshot. 
 
The GraphRAG solution also enables complex relationship queries, such as ‚ÄúShow the complete path from Emma Brown to Pacific Fresh Market‚Äù or ‚ÄúMap all connections between the individuals and merchants in the system.‚Äù The generated responses are shown in the following screenshot. 
 
Clean up 
To avoid incurring additional costs, clean up the resources you created. This includes deleting the Amazon Bedrock knowledge base, its associated IAM role, and the S3 bucket used for source documents. Additionally, you must separately delete the Neptune Analytics graph that was automatically created by Amazon Bedrock Knowledge Bases during the setup process. 
Conclusion 
GraphRAG in Amazon Bedrock emerges as a game-changing feature in the fight against financial fraud. By automatically connecting relationships across transaction data, customer profiles, historical patterns, and fraud reports, it significantly enhances financial institutions‚Äô ability to detect complex fraud schemes that traditional systems might miss. Its unique capability to understand and link information across multiple documents and data sources proves invaluable when investigating sophisticated fraud patterns that span various touchpoints and time periods.For financial institutions and fraud detection teams, GraphRAG intelligent document processing means faster, more accurate fraud investigations. It can quickly piece together related incidents, identify common patterns in fraud reports, and connect seemingly unrelated activities that might indicate organized fraud rings. This deeper level of insight, combined with its ability to provide comprehensive, context-aware responses, enables security teams to stay one step ahead of fraudsters who continuously evolve their tactics.As financial crimes become increasingly sophisticated, GraphRAG in Amazon Bedrock stands as a powerful tool for fraud prevention, transforming how you can analyze, connect, and act on fraud-related information. The future of fraud detection demands tools that can think and connect like humans‚Äîand GraphRAG is leading the way in making this possible. 
 
About the Authors 
Senaka Ariyasinghe is a Senior Partner Solutions Architect at AWS. He collaborates with Global Systems Integrators to drive cloud innovation across the Asia-Pacific and Japan region. He specializes in helping AWS partners develop and implement scalable, well-architected solutions, with particular emphasis on generative AI, machine learning, cloud migration strategies, and the modernization of enterprise applications. 
Senthil Nathan is a Senior Partner Solutions Architect working with Global Systems Integrators at AWS. In his role, Senthil works closely with global partners to help them maximize the value and potential of the AWS Cloud landscape. He is passionate about using the transformative power of cloud computing and emerging technologies to drive innovation and business impact. 
Deependra Shekhawat is a Senior Energy and Utilities Industry Specialist Solutions Architect based in Sydney, Australia. In his role, Deependra helps energy companies across the Asia-Pacific and Japan region use cloud technologies to drive sustainability and operational efficiency. He specializes in creating robust data foundations and advanced workflows that enable organizations to harness the power of big data, analytics, and machine learning for solving critical industry challenges. 
Aaron Sempf is Next Gen Tech Lead for the AWS Partner Organization in Asia-Pacific and Japan. With over 20 years in distributed system engineering design and development, he focuses on solving for large-scale complex integration and event-driven systems. In his spare time, he can be found coding prototypes for autonomous robots, IoT devices, distributed solutions, and designing agentic architecture patterns for generative AI-assisted business automation. 
Ozan Eken is a Product Manager at AWS, passionate about building cutting-edge generative AI and graph analytics products. With a focus on simplifying complex data challenges, Ozan helps customers unlock deeper insights and accelerate innovation. Outside of work, he enjoys trying new foods, exploring different countries, and watching soccer. 
JaiPrakash Dave is a Partner Solutions Architect working with Global Systems Integrators at AWS based in India. In his role, JaiPrakash guides AWS partners in the India region to design and scale well-architected solutions, focusing on generative AI, machine learning, DevOps, and application and data modernization initiatives.
‚Ä¢ Classify call center conversations with Amazon Bedrock batch inference
  In this post, we demonstrate how to build an end-to-end solution for text classification using the Amazon Bedrock batch inference capability with the Anthropic‚Äôs Claude Haiku model. Amazon Bedrock batch inference offers a 50% discount compared to the on-demand price, which is an important factor when dealing with a large number of requests. We walk through classifying travel agency call center conversations into categories, showcasing how to generate synthetic training data, process large volumes of text data, and automate the entire workflow using AWS services. 
Challenges with high-volume text classification 
Organizations across various sectors face a common challenge: the need to efficiently handle high-volume classification tasks. From travel agency call centers categorizing customer inquiries to sales teams analyzing lost opportunities and finance departments classifying invoices, these manual processes are a daily necessity. But these tasks come with significant challenges. 
The manual approach to analyzing and categorizing these classification requests is not only time-intensive but also prone to inconsistencies. As teams process the high volume of data, the potential for errors and inefficiencies grows. By implementing automated systems to classify these interactions, multiple departments stand to gain substantial benefits. They can uncover hidden trends in their data, significantly enhance the quality of their customer service, and streamline their operations for greater efficiency. 
However, the path to effective automated classification has its own challenges. Organizations must grapple with the complexities of efficiently processing vast amounts of textual information while maintaining consistent accuracy in their classification results. In this post, we demonstrate how to create a fully automated workflow while keeping operational costs under control. 
Data 
For this solution, we used synthetic call center conversation data. For realistic training data that maintains user privacy, we generated synthetic conversations using Anthropic‚Äôs Claude 3.7 Sonnet. We used the following prompt generate synthetic data: 
 
 Task: Generate &lt;N&gt; synthetic conversations from customer calls to an imaginary travel
company. Come up with 10 most probable categories that calls of this nature can come 
from and treat them as classification categories for these calls. For each generated 
call create a column that indicates the category for that call. 
Conversations should follow the following format:
"User: ...
Agent: ...
User: ...
Agent: ...
...

Class: One of the 10 following categories that is most relevant to the conversation."
Ten acceptable classes:
1. Booking Inquiry - Customer asking about making new reservations
2. Reservation Change - Customer wanting to modify existing bookings
3. Cancellation Request - Customer seeking to cancel their travel plans
4. Refund Issues - Customer inquiring about getting money back
5. Travel Information - Customer seeking details about destinations, documentation, etc.
6. Complaint - Customer expressing dissatisfaction with service
7. Payment Problem - Customer having issues with billing or payments
8. Loyalty Program - Customer asking about rewards points or membership status
9. Special Accommodation - Customer requesting special arrangements
10. Technical Support - Customer having issues with website, app or booking systems

Instructions:
- Keep conversations concise
- Use John Doe for male names and Jane Doe for female names
- Use john.doe@email.com for male email address,jane.doe@email.com for female email 
address and corporate@email.com for corporate email address, whenever you need to 
generate emails.Use \" or ' instead of " whenever there is a quote within the
conversation 
 
The synthetic dataset includes the following information: 
 
 Customer inquiries about flight bookings 
 Hotel reservation discussions 
 Travel package negotiations 
 Customer service complaints 
 General travel inquiries 
 
Solution overview 
The solution architecture uses a serverless, event-driven, scalable design to effectively handle and classify large quantities of classification requests. Built on AWS, it automatically starts working when new classification request data arrives in an Amazon Simple Storage Service (Amazon S3) bucket. The system then uses Amazon Bedrock batch processing to analyze and categorize the content at scale, minimizing the need for constant manual oversight. 
The following diagram illustrates the solution architecture. 
 
The architecture follows a well-structured flow that facilitates reliable processing of classification requests: 
 
 Data preparation ‚Äì The process begins when the user or application submits classification requests into the S3 bucket (Step 1). These requests are ingested into an Amazon Simple Queue Service (Amazon SQS) queue, providing a reliable buffer for incoming data and making sure no requests are lost during peak loads. A serverless data processor, implemented using an AWS Lambda function, reads messages from the queue and begins its data processing work (Step 2). It prepares the data for batch inference, crafting it into the JSONL format with schema that Amazon Bedrock requires. It stores files in a separate S3 bucket to maintain a clear separation from the original S3 bucket shared with the customer‚Äôs application, enhancing security and data management. 
 Batch inference ‚Äì When the data arrives in the S3 bucket, it initiates a notification to an SQS queue. This queue activates the Lambda function batch initiator, which starts the batch inference process. The function submits Amazon Bedrock batch inference jobs through the CreateModelInvocationJob API (Step 3). This initiator acts as the bridge between the queued data and the powerful classification capabilities of Amazon Bedrock. Amazon Bedrock then efficiently processes the data in batches. This batch processing approach allows for optimal use of resources while maintaining high throughput. When Amazon Bedrock completes its task, the classification results are stored in an output S3 bucket (Step 4) for postprocessing and analysis. 
 Classification results processing ‚Äì After classification is complete, the system processes the results through another SQS queue (Step 5) and specialized Lambda function, which organizes the classifications into simple-to-read files, such as CSV, JSON, or XLSX (Step 6). These files are immediately available to both the customer‚Äôs applications and support teams who need to access this information (Step 7). 
 Analytics ‚Äì We built an analytics layer that automatically catalogs and organizes the classification results, transforming raw classification data into actionable insights. An AWS Glue crawler catalogs everything so it can be quickly found later (Step 8). Now your business teams can use Amazon Athena to run SQL queries against the data, uncovering patterns and trends in the classified categories. We also built an Amazon QuickSight dashboard that provides visualization capabilities, so stakeholders can transform datasets into actionable reports ready for decision-making. (Step 9). 
 
We use AWS best practices in this solution, including event-driven and batch processing for optimal resource utilization, batch operations for cost-effectiveness, decoupled components for independent scaling, and least privilege access patterns. We implemented the system using the AWS Cloud Development Kit (AWS CDK) with TypeScript for infrastructure as code (IaC) and Python for application logic, making sure we achieve seamless automation, dynamic scaling, and efficient processing of classification requests, positioning it to effectively address both current requirements and future demands. 
Prerequisites 
To perform the solution, you must have the following prerequisites: 
 
 An active AWS account. 
 An AWS Region from the list of batch inference supported Regions for Amazon Bedrock. 
 Access to your selected models hosted on Amazon Bedrock. Make sure the selected model has been enabled in Amazon Bedrock. The solution is configured to use Anthropic‚Äôs Claude 3 Haiku by default. 
 Sign up for QuickSight in the same Region where the main application will be deployed. While subscribing, make sure to configure access to Athena and Amazon S3. 
 In QuickSight, create a group named quicksight-access for managing dashboard access permissions. Make sure to add your own role to this group so you can access the dashboard after it‚Äôs deployed. If you use a different group name, modify the corresponding name in the code accordingly. 
 To set up the AWS CDK, install the AWS CDK Command Line Interface (CLI). For instructions, see AWS CDK CLI reference. 
 
Deploy the solution 
The solution is accessible in the GitHub repository. 
Complete the following steps to set up and deploy the solution: 
 
 Clone the Repository: Run the following command:&nbsp;git clone git@github.com:aws-samples/sample-genai-bedrock-batch-classifier.git 
 Set Up AWS Credentials: Create an AWS Identity and Access Management (IAM) user with appropriate permissions, generate credentials for AWS Command Line Interface (AWS CLI) access, and create a profile. For instructions, see Authenticating using IAM user credentials for the AWS CLI. You can use the Admin Role for testing purposes, although it violates the principle of least privilege and should be avoided in production environments in favor of custom roles with minimal required permissions. 
 Bootstrap the Application: In the CDK folder, run the command npm install &amp; cdk bootstrap --profile {your_profile_name}, replacing {your_profile_name} with your AWS profile name. 
 Deploy the Solution: Run the command cdk deploy --all --profile {your_profile_name}, replacing {your_profile_name} with your AWS profile name. 
 
After you complete the deployment process, you will see a total of six stacks created in your AWS account, as illustrated in the following screenshot. 
 
SharedStack acts as a central hub for resources that multiple parts of the system need to access. Within this stack, there are two S3 buckets: one handles internal operations behind the scenes, and the other serves as a bridge between the system and customers, so they can both submit their classification requests and retrieve their results. 
DataPreparationStack serves as a data transformation engine. It‚Äôs designed to handle incoming files in three specific formats: XLSX, CSV, and JSON, which at the time of writing are the only supported input formats. This stack‚Äôs primary role is to convert these inputs into the specialized JSONL format required by Amazon Bedrock. The data processing script is available in the GitHub repo. This transformation makes sure that incoming data, regardless of its original format, is properly structured before being processed by Amazon Bedrock. The format is as follows: 
 
 {
&nbsp;"recordId": ${unique_id}, 
&nbsp;"modelInput": {
&nbsp;&nbsp; &nbsp; "anthropic_version": "bedrock-2023-05-31", 
&nbsp;&nbsp; &nbsp; "max_tokens": 1024,
&nbsp;&nbsp; &nbsp; "messages": [ { 
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "role": "user", 
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "content": [{"type":"text", "text": ${initial_text}]} ],
&nbsp;&nbsp; &nbsp; &nbsp;},
&nbsp; &nbsp; &nbsp; "system": ${prompt}
}

where:
initial_text -&nbsp;text that you want to classify
prompt&nbsp; &nbsp; &nbsp; &nbsp;-&nbsp;instructions to Bedrock&nbsp;service how to classify
unique_id&nbsp; &nbsp;&nbsp;-&nbsp;id coming from&nbsp;the upstream service,&nbsp;otherwise it will be 
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;automatically generated by&nbsp;the code
 
 
BatchClassifierStack handles the classification operations. Although currently powered by Anthropic‚Äôs Claude Haiku, the system maintains flexibility by allowing straightforward switches to alternative models as needed. This adaptability is made possible through a comprehensive constants file that serves as the system‚Äôs control center. The following configurations are available: 
 
 PREFIX ‚Äì Resource naming convention (genai by default). 
 BEDROCK_AGENT_MODEL ‚Äì Model selection. 
 BATCH_SIZE ‚Äì Number of classifications per output file (enables parallel processing); the minimum should be 100. 
 CLASSIFICATION_INPUT_FOLDER ‚Äì Input folder name in the S3 bucket that will be used for uploading incoming classification requests. 
 CLASSIFICATION_OUTPUT_FOLDER ‚Äì Output folder name in the S3 bucket where the output files will be available after the classification completes. 
 OUTPUT_FORMAT ‚Äì Supported formats (CSV, JSON, XLSX). 
 INPUT_MAPPING ‚Äì A flexible data integration approach that adapts to your existing file structures rather than requiring you to adapt to ours. It consists of two key fields: 
   
   record_id ‚Äì Optional unique identifier (auto-generated if not provided). 
   record_text ‚Äì Text content for classification. 
    
 PROMPT ‚Äì Template for guiding the model‚Äôs classification behavior. A sample prompt template is available in the GitHub repo. Pay attention to the structure of the template that guides the AI model through its decision-making process. The template not only combines a set of possible categories, but also contains instructions, requiring the model to select a single category and present it within &lt;class&gt; tags. These instructions help maintain consistency in how the model processes incoming requests and saves the output. 
 
BatchResultsProcessingStack functions as the data postprocessing stage, transforming the Amazon Bedrock JSONL output into user-friendly formats. At the time of writing, the system supports CSV, JSON, and XLSX. These processed files are then stored in a designated output folder in the S3 bucket, organized by date for quick retrieval and management. The conversion scripts are available in the GitHub repo. The output files have the following schema: 
 
 ID ‚Äì Resource naming convention 
 INPUT_TEXT ‚Äì Initial text that was used for classification 
 CLASS ‚Äì The classification category 
 RATIONALE ‚Äì Reasoning or explanation of given classification 
 
 
AnalyticsStack provides a business intelligence (BI) dashboard that displays a list of classifications and allows filtering based on defined in prompt categories. It offers the following key configuration options: 
 
 ATHENA_DATABASE_NAME ‚Äì Defines the name of Athena database that is used as a main data source for the QuickSight dashboard. 
 QUICKSIGHT_DATA_SCHEMA ‚Äì Defines how labels should be displayed on the dashboard and specifies which columns are filterable. 
 QUICKSIGHT_PRINCIPAL_NAME ‚Äì Designates the principal group that will have access to the QuickSight dashboard. The group should be created manually before deploying the stack. 
 QUICKSIGHT_QUERY_MODE ‚Äì You can choose between SPICE or direct query for fetching data, depending on your use case, data volume, and data freshness requirements. The default setting is direct query. 
 
Now that you‚Äôve successfully deployed the system, you can prepare your data file‚Äîthis can be either real customer data or the synthetic dataset we provided for testing. When your file is ready, go to the S3 bucket named {prefix}-{account_id}-customer-requests-bucket-{region} and upload your file to input_data folder. After the batch inference job is complete, you can view the classification results on the dashboard. You can find it under the name {prefix}-{account_id}-classifications-dashboard-{region}. The following screenshot shows a preview of what you can expect. 
 
The dashboard will not display data until Amazon Bedrock finishes processing the batch inference jobs and the AWS Glue crawler creates the Athena table. Without these steps completed, the dashboard can‚Äôt connect to the table because it doesn‚Äôt exist yet. Additionally, you must update the QuickSight role permissions that were set up during pre-deployment. To update permissions, complete the following steps: 
 
 On the QuickSight console, choose the user icon in the top navigation bar and choose Manage QuickSight. 
 In the navigation pane, choose Security &amp; Permissions. 
 Verify that the role has been granted proper access to the S3 bucket with the following path format: {prefix}-{account_id}-internal-classifications-{region}. 
 
Results 
To test the solution‚Äôs performance and reliability, we tested 1,190 synthetically generated travel agency conversations from a single Excel file across multiple runs. The results were remarkably consistent across 10 consecutive runs, with processing times ranging between 11‚Äì12 minutes per batch (200 classifications in a single batch).Our solution achieved the following: 
 
 Speed ‚Äì Maintained consistent processing times around 11‚Äì12 minutes 
 Accuracy ‚Äì Achieved 100% classification accuracy on our synthetic dataset 
 Cost-effectiveness ‚Äì Optimized expenses through efficient batch processing 
 
Challenges 
For certain cases, the generated class didn‚Äôt exactly match the class name given in the prompt. For instance, in multiple cases, it output ‚ÄúHotel/Flight Booking Inquiry‚Äù instead of ‚ÄúBooking Inquiry,‚Äù which was defined as the class in the prompt. This was addressed by prompt engineering and asking the model to check the final class output to match exactly with one of the provided classes. 
Error handling 
For troubleshooting purposes, the solution includes an Amazon DynamoDB table that tracks batch processing status, along with Amazon CloudWatch Logs. Error tracking is not automated and requires manual monitoring and validation. 
Key takeaways 
Although our testing focused on travel agency scenarios, the solution‚Äôs architecture is flexible and can be adapted to various classification needs across different industries and use cases. 
Known limitations 
The following are key limitations of the classification solution and should be considered when planning its use: 
 
 Minimum batch size ‚Äì Amazon Bedrock batch inference requires at least 100 classifications per batch. 
 Processing time ‚Äì The completion time of a batch inference job depends on various factors, such as job size. Although Amazon Bedrock strives to complete a typical job within 24 hours, this time frame is a best-effort estimate and not guaranteed. 
 Input file formats ‚Äì The solution currently supports only CSV, JSON, and XLSX file formats for input data. 
 
Clean up 
To avoid additional charges, clean up your AWS resources when they‚Äôre no longer needed by running the command cdk destroy --all --profile {your_profile_name}, replacing {your_profile_name} with your AWS profile name. 
To remove resources associated with this project, complete the following steps: 
 
 Delete the S3 buckets: 
   
   On the Amazon S3 console, choose Buckets in the navigation pane. 
   Locate your buckets by searching for your {prefix}. 
   Delete these buckets to facilitate proper cleanup. 
    
 Clean up the DynamoDB resources: 
   
   On the DynamoDB console, choose Tables in the navigation pane. 
   Delete the table {prefix}-{account_id}-batch-processing-status-{region}. 
    
 
This comprehensive cleanup helps make sure residual resources don‚Äôt remain in your AWS account from this project. 
Conclusion 
In this post, we explored how Amazon Bedrock batch inference can transform your large-scale text classification workflows. You can now automate time-consuming tasks your teams handle daily, such as analyzing lost sales opportunities, categorizing travel requests, and processing insurance claims. This solution frees your teams to focus on growing and improving your business. 
Furthermore, this solution gives the opportunity to create a system that provides real-time classifications, seamlessly integrates with your communication channels, offers enhanced monitoring capabilities, and supports multiple languages for global operations. 
This solution was developed for internal use in test and non-production environments only. It is the responsibility of the customer to perform their due diligence to verify the solution aligns with their compliance obligations. 
We‚Äôre excited to see how you will adapt this solution to your unique challenges. Share your experience or questions in the comments‚Äîwe‚Äôre here to help you get started on your automation journey. 
 
About the authors 
 
 
  
  
 Nika Mishurina is a Senior Solutions Architect with Amazon Web Services. She is passionate about delighting customers through building end-to-end production-ready solutions for Amazon. Outside of work, she loves traveling, working out, and exploring new things. 
 
 
 
  
  
 Farshad Harirchi is a Principal Data Scientist at AWS Professional Services. He helps customers across industries, from retail to industrial and financial services, with the design and development of generative AI and machine learning solutions. Farshad brings extensive experience in the entire machine learning and MLOps stack. Outside of work, he enjoys traveling, playing outdoor sports, and exploring board games.

‚∏ª