‚úÖ Morning News Briefing ‚Äì September 22, 2025 10:45

üìÖ Date: 2025-09-22 10:45
üè∑Ô∏è Tags: #briefing #ai #publichealth #digitalgov

‚∏ª

üßæ Weather
‚Ä¢ No watches or warnings in effect, Pembroke
  No watches or warnings in effect. No warnings or watches or watches in effect . Watch or warnings are no longer in effect in the U.S. No watches, warnings are in effect for the rest of the day . No watches and warnings are still in effect, but no watches are in place for the day's events . The weather is not expected to be affected by the weather .
‚Ä¢ Current Conditions: Mostly Cloudy, 16.1¬∞C
  Observed at Garrison Petawawa 6:00 AM EDT Monday 22 September 2025 . Temperature: 16.1&deg;C Pressure: 101.4 kPa  Visibility: 24 km Visibility : 24 km Humidity: 93 % Dewpoint: 15.0&deg:C Wind: SE 11 km/h Air Quality Health Index: n/a . Air Quality
‚Ä¢ Monday: Chance of showers. High 20. POP 40%
  Showers ending this morning then cloudy with 40 percent chance of showers . Risk of a thunderstorm. High 20. Humidex 25. UV index 4 or moderate. Showery winds will make it feel like it's a good day for the first time in 25 years . Forecast issued 5:00 AM EDT Monday 22 September 2025. Forecast: "Showery"

üåç International News
No updates.

üçÅ Canadian News
No updates.

üá∫üá∏ U.S. Top Stories
‚Ä¢ North Korea's Kim Jong Un ready to talk if U.S. drops denuclearization demand
  Kim told North Korea's legislature he's ready to resume talks, adding he had "good memories" of President Trump, despite ongoing tension over Washington's denuclearization stance . Kim said he had 'good memories' of the U.S. President Trump . Kim told the North Korean legislature he was ready for talks with President Trump despite ongoing tensions over U.N. stance on den
‚Ä¢ Rheumatoid arthritis kept her captive. This nerve stimulator set her free.
  A new surgically implanted device the size of a lima bean can help control rheumatoid arthritis that isn't responding to drugs . The device is a device that can be implanted surgically to help control the arthritis . It is the first time a new device has been implanted to control the condition that is not responding to the drugs that are needed to treat it . The new device
‚Ä¢ What to know as U.N. General Assembly opens, marking 80 years, tackling deep crises
  This year the United Nations is marking its 80th anniversary, but diplomats don't have much to celebrate . The U.N. is marking the anniversary of the U.S. embassy in New York City, New York, on Thursday . Diplomats are celebrating the anniversary but diplomats aren't celebrating much of what they have to celebrate this year's anniversary . The United Nations will be celebrating
‚Ä¢ This small Virginia island could be underwater before the next century
  Tangier Island in Virginia is one of the last inhabited islands in Chesapeake Bay under threat from rising sea levels and climate change . Tangier island is under threat to rise sea levels, climate change and is under attack from rising water levels and rising tide . The islands are one of last remaining islands in the Chesapeake¬†Bay under threat of rising seas and rising tides . The island is
‚Ä¢ Colleges pull back as Trump cuts programs that help migrant students
  Since 1972, the CAMP program has helped tens of thousands of migrant students succeed in college . The Trump administration has cut off funding for it, forcing some colleges to reduce or eliminate services . Some colleges have already reduced or eliminated services, forcing them to reduce services . The program was started in 1972 in response to the rise of immigrants to college in the United States in the 1970s .

üß† Artificial Intelligence
No updates.

üíª Digital Strategy
‚Ä¢ Brit scientists over the Moon after growing tea in lunar soil
  British boffins say they've discovered a way of taking one of the country's favorite pastimes ‚Äì having a nice cup of tea ‚Äì into outer space .‚Ä¶‚Ä¶‚Ä¶ It's one small sip for man... It's a way to take one of Britain's favourite pastimes, having a tea, into space, into a cup cup cup, scientists say . It's just one
‚Ä¢ Linux has the lineage to out-evolve the deadliest of cyber threats, given the right push
  The IT industry is not only full of sharks, it has shark nature itself itself . Darwin would understand microkernels that understand Darwin . Without innovation the sector would curdle and die, the IT sector would have to keep moving forward . It must keep moving ahead to survive, says Simon Tisdall, who says the sector needs to be innovated to survive in order to survive
‚Ä¢ 'Technical debt' in police database built to respond to child murders causing a 'failing service'
  The risk rating of the UK's crime intelligence database is being elevated to 'Red' by the governments projects' watchdog . The DB struggles to migrate from a legacy Oracle platform to the cloud and cloud . The UK crime database has been struggling to move off obsolete tech and cloud tech and is being put on a 'red' risk rating . It is the first project to be rated 'Red
‚Ä¢ FOMO? Brit banking biz rolls out AI tools, talks up security
  Lloyds Data and AI lead doesn't want devs downloading models from the likes of Hugging Face ‚Äì too risky . Data of its 28 million customers is kept away from untested AI models developers might be tempted to deploy . Lloysds Banking Group is leaning into 21st century tech - yet trying to do so in a way that the data of its customers is not kept from unt
‚Ä¢ Bored developers accidentally turned their watercooler into a bootleg brewery
  Who, Me? is our reader-contributed column in which you share stories of making a mess at work, and cleaning up afterwards to the best of your ability . IT pros make the sort of mistakes that The Register celebrates each week in the column . It‚Äôs our readers' favourite work-related mistakes, and we'll feature them next week in a new Who Me?

üè• Public Health
No updates.

üî¨ Science
‚Ä¢ Hotly anticipated US vaccine meeting ends with confusion ‚Äî and a few decisions
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Sleep and circadian rhythms in cardiovascular resilience: mechanisms, implications, and a Roadmap for research and interventions
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Comparison of Three Anonymization Tools for a Health Fitness Study
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations

üßæ Government & Policy
No updates.

üèõÔ∏è Enterprise Architecture & IT Governance
No updates.

ü§ñ AI & Emerging Tech
‚Ä¢ This medical startup uses LLMs to run appointments and make diagnoses
  Imagine this: You‚Äôve been feeling unwell, so you call up your doctor‚Äôs office to make an appointment. To your surprise, they schedule you in for the next day. At the appointment, you aren‚Äôt rushed through describing your health concerns; instead, you have a full half hour to share your symptoms and worries and the exhaustive details of your health history with someone who listens attentively and asks thoughtful follow-up questions. You leave with a diagnosis, a treatment plan, and the sense that, for once, you‚Äôve been able to discuss your health with the care that it merits.





The catch? You might not have spoken to a doctor, or other licensed medical practitioner, at all.



This is the new reality for patients at a small number of clinics in Southern California that are run by the medical startup Akido Labs. These patients‚Äîsome of whom are on Medicaid‚Äîcan access specialist appointments on short notice, a privilege typically only afforded to the wealthy few who patronize concierge clinics.



The key difference is that Akido patients spend relatively little time, or even no time at all, with their doctors. Instead, they see a medical assistant, who can lend a sympathetic ear but has limited clinical training. The job of formulating diagnoses and concocting a treatment plan is done by a proprietary, LLM-based system called ScopeAI that transcribes and analyzes the dialogue between patient and assistant. A doctor then approves, or corrects, the AI system‚Äôs recommendations.



‚ÄúOur focus is really on what we can do to pull the doctor out of the visit,‚Äù says Jared Goodner, Akido‚Äôs CTO.&nbsp;



According to Prashant Samant, Akido‚Äôs CEO, this approach allows doctors to see four to five times as many patients as they could previously. There‚Äôs good reason to want doctors to be much more productive. Americans are getting older and sicker, and many struggle to access adequate health care. The pending 15% reduction in federal funding for Medicaid will only make the situation worse.



But experts aren‚Äôt convinced that displacing so much of the cognitive work of medicine onto AI is the right way to remedy the doctor shortage. There‚Äôs a big gap in expertise between doctors and AI-enhanced medical assistants, says Emma Pierson, a computer scientist at UC Berkeley.&nbsp; Jumping such a gap may introduce risks. ‚ÄúI am broadly excited about the potential of AI to expand access to medical expertise,‚Äù she says. ‚ÄúIt‚Äôs just not obvious to me that this particular way is the way to do it.‚Äù



AI is already everywhere in medicine. Computer vision tools identify cancers during preventive scans, automated research systems allow doctors to quickly sort through the medical literature, and LLM-powered medical scribes can take appointment notes on a clinician‚Äôs behalf. But these systems are designed to support doctors as they go about their typical medical routines.



What distinguishes ScopeAI, Goodner says, is its ability to independently complete the cognitive tasks that constitute a medical visit, from eliciting a patient‚Äôs medical history to coming up with a list of potential diagnoses to identifying the most likely diagnosis and proposing appropriate next steps.



Under the hood, ScopeAI is a set of large language models, each of which can perform a specific step in the visit‚Äîfrom generating appropriate follow-up questions based on what a patient has said to to populating a list of likely conditions. For the most part, these LLMs are fine-tuned versions of Meta‚Äôs open-access Llama models, though Goodner says that the system also makes use of Anthropic‚Äôs Claude models.&nbsp;



During the appointment, assistants read off questions from the ScopeAI interface, and ScopeAI produces new questions as it analyzes what the patient says. For the doctors who will review its outputs later, ScopeAI produces a concise note that includes a summary of the patient‚Äôs visit, the most likely diagnosis, two or three alternative diagnoses, and recommended next steps, such as referrals or prescriptions. It also lists a justification for each diagnosis and recommendation.



ScopeAI is currently being used in cardiology, endocrinology, and primary care clinics and by Akido‚Äôs street medicine team, which serves the Los Angeles homeless population. That team‚Äîwhich is led by Steven Hochman, a doctor who specializes in addiction medicine‚Äîmeets patients out in the community to help them access medical care, including treatment for substance use disorders.&nbsp;



Previously, in order to prescribe a drug to treat an opioid addiction, Hochman would have to meet the patient in person; now, caseworkers armed with ScopeAI can interview patients on their own, and Hochman can approve or reject the system‚Äôs recommendations later. ‚ÄúIt allows me to be in 10 places at once,‚Äù he says.



Since they started using ScopeAI, the team has been able to get patients access to medications to help treat their substance use within 24 hours‚Äîsomething that Hochman calls ‚Äúunheard of.‚Äù



This arrangement is only possible because homeless patients typically get their health insurance from Medicaid, the public insurance system for low-income Americans. While Medicaid allows doctors to approve ScopeAI prescriptions and treatment plans asynchronously, both for street medicine and clinic visits, many other insurance providers require that doctors speak directly with patients before approving those recommendations. Pierson says that discrepancy raises concerns. ‚ÄúYou worry about that exacerbating health disparities,‚Äù she says.



Samant is aware of the appearance of inequity, and he says the discrepancy isn‚Äôt intentional‚Äîit‚Äôs just a feature of how the insurance plans currently work. He also notes that being seen quickly by an AI-enhanced medical assistant may be better than dealing with long wait times and limited provider availability, which is the status quo for Medicaid patients. And all Akido patients can opt for traditional doctor‚Äôs appointments, if they are willing to wait for them, he says.



Part of the challenge of deploying a tool like ScopeAI is navigating a regulatory and insurance landscape that wasn‚Äôt designed for AI systems that can independently direct medical appointments. Glenn Cohen, a professor at Harvard Law School, says that any AI system that effectively acts as a ‚Äúdoctor in a box‚Äù would likely need to be approved by the FDA and could run afoul of medical licensure laws, which dictate that only doctors and other licensed professionals can practice medicine.



The California Medical Practice Act says that AI can&#8217;t replace a doctor‚Äôs responsibility to diagnose and treat a patient, but doctors are allowed to use AI in their work, and they don‚Äôt need to see patients in-person or in real-time before diagnosing them. Neither the FDA nor the Medical Board of California were able to say whether or not ScopeAI was on solid legal footing based only on a written description of the system.



But Samant is confident that Akido is in compliance, as ScopeAI was intentionally designed to fall short of being a ‚Äúdoctor in a box.‚Äù Because the system requires a human doctor to review and approve of all of its diagnostic and treatment recommendations, he says, it doesn‚Äôt require FDA approval.&nbsp;



At the clinic, this delicate balance between AI and doctor decision making happens entirely behind the scenes. Patients don‚Äôt ever see the ScopeAI interface directly‚Äîinstead, they speak with a medical assistant who asks questions in the way that a doctor might in a typical appointment. That arrangement might make patients feel more comfortable. But Zeke Emanuel, a professor of medical ethics and health policy at the University of Pennsylvania who served in the Obama and Biden administrations, worries that this comfort could be obscuring from patients the extent to which an algorithm is influencing their care.



Pierson agrees. ‚ÄúThat certainly isn‚Äôt really what was traditionally meant by the human touch in medicine,‚Äù she says.



DeAndre Siringoringo, a medical assistant who works at Akido‚Äôs cardiology office in Rancho Cucamonga, says that while he tells the patients he works with that an AI system will be listening to the appointment in order to gather information for their doctor, he doesn‚Äôt inform them about the specifics of how ScopeAI works, including the fact that it makes diagnostic recommendations to doctors.&nbsp;



Because all ScopeAI recommendations are reviewed by a doctor, that might not seem like such a big deal‚Äîit‚Äôs the doctor who makes the final diagnosis, not the AI. But it‚Äôs been widely documented that doctors using AI systems tend to go along with the system‚Äôs recommendations more often than they should, a phenomenon known as automation bias.&nbsp;



At this point, it‚Äôs impossible to know whether automation bias is affecting doctors‚Äô decisions at Akido clinics, though Pierson says it‚Äôs a risk‚Äîespecially when doctors aren‚Äôt physically present for appointments. ‚ÄúI worry that it might predispose you to sort of nodding along in a way that you might not if you were actually in the room watching this happen,‚Äù she says.



An Akido spokesperson says that automation bias is a valid concern for any AI tool that assists a doctor‚Äôs decision-making and that the company has made efforts to mitigate that bias. ‚ÄúWe designed ScopeAI specifically to reduce bias by proactively countering blind spots that can influence medical decisions, which historically lean heavily on physician intuition and personal experience,‚Äù she says. ‚ÄúWe also train physicians explicitly on how to use ScopeAI thoughtfully, so they retain accountability and avoid over-reliance.‚Äù



Akido evaluates ScopeAI‚Äôs performance by testing it on historical data and monitoring how often doctors correct its recommendations; those corrections are also used to further train the underlying models. Before deploying ScopeAI in a given specialty, Akido ensures that when tested on historical data sets, the system includes the correct diagnosis in its top three recommendations at least 92% of the time.



But Akido hasn‚Äôt undertaken more rigorous testing, such as studies that compare ScopeAI appointments with traditional in-person or telehealth appointments, in order to determine whether the system improves‚Äîor at least maintains‚Äîpatient outcomes. Such a study could help indicate whether automation bias is a meaningful concern.



‚ÄúMaking medical care cheaper and more accessible is a laudable goal,‚Äù Pierson says. ‚ÄúBut I just think it‚Äôs important to conduct strong evaluations comparing to that baseline.‚Äù
‚Ä¢ The Download: the CDC‚Äôs vaccine chaos
  This is today&#8217;s edition of¬†The Download,¬†our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



A pivotal meeting on vaccine guidance is underway‚Äîand former CDC leaders are alarmed



This week has been an eventful one for America‚Äôs public health agency. Two former leaders of the US Centers for Disease Control and Prevention explained why they suddenly departed in a Senate hearing. They also described how CDC employees are being instructed to turn their backs on scientific evidence.They painted a picture of a health agency in turmoil‚Äîand at risk of harming the people it is meant to serve. And, just hours afterwards, a panel of CDC advisers voted to stop recommending the MMRV vaccine for children under four. Read the full story.



‚ÄîJessica Hamzelou



This article first appeared in The Checkup, MIT Technology Review‚Äôs weekly biotech newsletter. To receive it in your inbox every Thursday, and read articles like this first, sign up here.



If you‚Äôre interested in reading more about US vaccine policy, check out:



+ Read our profile of Jim O‚ÄôNeill, the deputy health secretary and current acting CDC director.+ Why US federal health agencies are abandoning mRNA vaccines. Read the full story.+ Why childhood vaccines are a public health success story. No vaccine is perfect, but these medicines are still saving millions of lives. Read the full story.&nbsp;



+ The FDA plans to limit access to covid vaccines. Here‚Äôs why that‚Äôs not all bad.







Meet Sneha Goenka: our 2025 Innovator of the Year



Every year, MIT Technology Review selects one individual whose work we admire to recognize as Innovator of the Year. For 2025, we chose Sneha Goenka, who designed the computations behind the world‚Äôs fastest whole-genome sequencing method.&nbsp;



Thanks to her work, physicians can now sequence a patient‚Äôs genome and diagnose a genetic condition in less than eight hours‚Äîan achievement that could transform medical care.



Register here to join an exclusive subscriber-only Roundtable conversation with Goenka, Leilani Battle, assistant professor at the University of Washington, and our editor in chief Mat Honan at 1pm ET next Tuesday September 23.







The must-reads



I‚Äôve combed the internet to find you today‚Äôs most fun/important/scary/fascinating stories about technology.



1 The CDC voted against giving some children a combined vaccine¬†If accepted, the agency will stop recommending the MMRV vaccine for children under 4. (CNN)+ Its vote on hepatitis B vaccines for newborns is expected today too. (The Atlantic $)+ RFK JR‚Äôs allies are closing ranks around him. (Politico)2 Russia is using Charlie Kirk‚Äôs murder to sow division in the USIt&#8217;s using the momentum to push pro-Kremlin narratives and divide Americans. (WP $)+ The complicated phenomenon of political violence. (Vox)+ We don‚Äôt know what being ‚Äòterminally online‚Äô means any more. (Wired $)3 Nvidia will invest $5 billion in IntelThe partnership allows Intel to develop custom CPUs to work with Nvidia‚Äôs chips. (WSJ $)+ It‚Äôs a much-needed financial shot in the arm for Intel. (WP $)+ It‚Äôs also great news for Intel‚Äôs Asian suppliers. (Bloomberg $)



4 Medical AI tools downplay symptoms in women and ethnic minoritiesExperts fear that LLM-powered tools could lead to worse health outcomes. (FT $)+ Artificial intelligence is infiltrating health care. We shouldn‚Äôt let it make all the decisions. (MIT Technology Review)



5 AI browsers have hit the mainstreamWhere‚Äôs the off switch? (Wired $)+ AI means the end of internet search as we‚Äôve known it. (MIT Technology Review)



6 China has entered the global brain interface raceIts ambitious government-backed startups are primed to challenge Neuralink. (Bloomberg $)+ This patient‚Äôs Neuralink brain implant gets a boost from generative AI. (MIT Technology Review)



7 What makes humans unique in the age of AI?Defining the distinctions between us and machines isn‚Äôt as easy as it used to be. (New Yorker $)+ How AI can help supercharge creativity. (MIT Technology Review)



8 This ship helps to reconnect Africa‚Äôs internetAI needs high speed internet, which needs undersea cables. (Rest of World)+ What Africa needs to do to become a major AI player. (MIT Technology Review)



9 Hundreds of people queued in Beijing to buy Apple‚Äôs new iPhoneDesire for Apple products in the country appears to be alive and well. (Reuters)



10 San Francisco‚Äôs idea of a great night out? A robot cage fightIt‚Äôs certainly one way to have a good time. (NYT $)







Quote of the day



&#8220;Get off the iPad!&#8221;



‚ÄîAn irate air traffic controller tells the pilots of a Spirit Airlines flight to pay attention to avoid potentially colliding with Donald Trump‚Äôs Air Force One aircraft, Ars Technica reports.







One more thing







We used to get excited about technology. What happened?As a philosopher who studies AI and data, Shannon Vallor‚Äôs Twitter feed is always filled with the latest tech news. Increasingly, she‚Äôs realized that the constant stream of information is no longer inspiring joy, but a sense of resignation.Joy is missing from our lives, and from our technology. Its absence is feeding a growing unease being voiced by many who work in tech or study it. Fixing it depends on understanding how and why the priorities in our tech ecosystem have changed. Read the full story.



We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)



+ Would you go about your daily business with a soft toy on your shoulder? This intrepid reporter gave it a go.+ How dying dinosaurs shaped the landscapes around us.+ I can‚Äôt believe I missed Pythagorean Theorem day earlier this week.+ Inside the rise in popularity of the no-water yard.
‚Ä¢ A pivotal meeting on vaccine guidance is underway‚Äîand former CDC leaders are alarmed
  Update Friday 6am ET: The advisory CDC panel recommended that children under the age of 4 do not receive the combined MMRV vaccine (for measles, mumps, rubella, and varicella) but instead receive two separate shots.



This week has been an eventful one for America‚Äôs public health agency. Two former leaders of the US Centers for Disease Control and Prevention explained the reasons for their sudden departures from the agency in a Senate hearing. And they described how CDC employees are being instructed to turn their backs on scientific evidence.



The CDC‚Äôs former director Susan Monarez and former chief medical officer Debra Houry took questions from a Senate committee on Wednesday. They painted a picture of a health agency in turmoil‚Äîand at risk of harming the people it is meant to serve.





On Thursday, an advisory CDC panel that develops vaccine guidance met for a two-day discussion on multiple childhood vaccines. During the meeting, which was underway as The Checkup went to press, members of the panel were set to discuss those vaccines and propose recommendations on their use.



Monarez worries that access to childhood vaccines is under threat‚Äîand that the public health consequences could be dire. ‚ÄúIf vaccine protections are weakened, preventable diseases will return,‚Äù she said.



As the current secretary of health and human services, Robert F. Kennedy Jr. oversees federal health and science agencies that include the CDC, which monitors and responds to threats to public health. Part of that role involves developing vaccine recommendations.



As we‚Äôve noted before, RFK Jr. has long been a prominent critic of vaccines. He has incorrectly linked commonly used ingredients to autism and made other incorrect statements about risks associated with various vaccines.



Still, he oversaw the recruitment of Monarez‚Äîwho does not share those beliefs‚Äîto lead the agency. When she was sworn in on July 31, Monarez, who is a microbiologist and immunologist, had already been serving as acting director of the agency. She had held prominent positions at other federal agencies and departments too, including the Advanced Research Projects Agency for Health (ARPA-H) and the Biomedical Advanced Research and Development Authority (BARDA). Kennedy&nbsp;described her as ‚Äúa public health expert with unimpeachable scientific credentials.‚Äù



His opinion seems to have changed somewhat since then. Just 29 days after Monarez took on her position, she was turfed out of the agency. And in yesterday‚Äôs hearing, she explained why.



On August 25, Kennedy asked Monarez to do two things, she said. First, he wanted her to commit to firing scientists at the agency. And second, he wanted her to ‚Äúpre-commit‚Äù to approve vaccine recommendations made by the agency‚Äôs Advisory Committee on Immunization Practices (ACIP), regardless of whether there was any scientific evidence to support those recommendations, she said. ‚ÄúHe just wanted blanket approval,‚Äù she said during her testimony.&nbsp;



She refused both requests.



Monarez testified that she didn‚Äôt want to get rid of hardworking scientists who played an important role in keeping Americans safe. And she said she could not commit to approving vaccine recommendations without reviewing the scientific evidence behind them and maintain her integrity. She was sacked.





Those vaccine recommendations are currently under discussion, and scientists like Monarez are worried about how they might change. Kennedy fired all 17 members of the previous committee in June. (Monarez said she was not consulted on the firings and found out about them through media reports.)



‚ÄúA clean sweep is needed to reestablish public confidence in vaccine science,‚Äù Kennedy wrote in&nbsp;a piece for the Wall Street Journal at the time. He went on to replace those individuals with eight new members, some of whom have been prominent vaccine critics and&nbsp;have spread misinformation about vaccines. One later withdrew.



That new panel met two weeks later. The&nbsp;meeting included a presentation about thimerosal‚Äîa chemical that Kennedy has incorrectly linked to autism, and which is no longer included in vaccines in the US‚Äîand a proposal to recommend that the MMRV vaccine (for measles, mumps, rubella, and varicella) not be offered to children under the age of four.



Earlier this week,&nbsp;five new committee members were named. They include&nbsp;individuals who have advocated against vaccine mandates and who have argued that mRNA-based covid vaccines should be removed from the market.



All 12 members are convening for a meeting that runs today and tomorrow. At that meeting, members will propose recommendations for the MMRV vaccine and vaccines for covid-19 and hepatitis B, according to an agenda published on the CDC website.



Those are the recommendations for which Monarez says she was asked to provide ‚Äúblanket approval.‚Äù ‚ÄúMy worst fear is that I would then be in a position of approving something that reduces access [to] lifesaving vaccines to children and others who need them,‚Äù she said.



That job now goes to Jim O‚ÄôNeill, the deputy health secretary and acting CDC director (also a longevity enthusiast), who now holds the authority to approve those recommendations.



We don‚Äôt yet know what those recommendations will be. But if they are approved, they could reshape access to vaccines for children and vulnerable people in the US. As&nbsp;six former chairs of the committee wrote for STAT: ‚ÄúACIP is directly linked to the Vaccines for Children program, which provides vaccines without cost to approximately 50% of children in the US, and the Affordable Care Act that requires insurance coverage for ACIP-recommended vaccines to approximately 150 million people in the US.‚Äù



Drops in vaccine uptake have already contributed to this year‚Äôs measles outbreak in the US, which is the biggest in decades. Two children have died. We are already seeing the impact of undermined trust in childhood vaccines. As Monarez put it: ‚ÄúThe stakes are not theoretical.‚Äù



This article first appeared in The Checkup,&nbsp;MIT Technology Review‚Äôs&nbsp;weekly biotech newsletter. To receive it in your inbox every Thursday, and read articles like this first,&nbsp;sign up here.
‚Ä¢ The Download: AI-designed viruses, and bad news for the hydrogen industry
  This is today&#8217;s edition of¬†The Download,¬†our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



AI-designed viruses are here and already killing bacteria



Artificial intelligence can draw cat pictures and write emails. Now the same technology can compose a working genome.A research team in California says it used AI to propose new genetic codes for viruses‚Äîand managed to get several of them to replicate and kill bacteria.The work, described in a preprint paper, has the potential to create new treatments and accelerate research into artificially engineered cells. But experts believe it is also an ‚Äúimpressive first step‚Äù toward AI-designed life forms. Read the full story.



‚ÄîAntonio Regalado







Clean hydrogen is facing a big reality check



Hydrogen is sometimes held up as a master key for the energy transition. It can be made using several low-emissions methods and could play a role in cleaning up industries ranging from agriculture to aviation to shipping.



This moment is a complicated one for the green fuel, though, as a new report from the International Energy Agency lays out. A number of major projects face cancellations and delays. The US in particular is seeing a slowdown after changes to key tax credits and cuts in support for renewable energy.Still, there are bright spots for the industry, including in China, and new markets could soon become crucial for growth. Here are three things to know about the state of hydrogen in 2025.



‚ÄîCasey Crownhart



This article is from The Spark, MIT Technology Review‚Äôs weekly climate newsletter. To receive it in your inbox every Wednesday, sign up here.







The must-reads



I‚Äôve combed the internet to find you today‚Äôs most fun/important/scary/fascinating stories about technology.



1 Meta‚Äôs new smart glasses have a tiny screenWelcome back, Google Glass. (NYT $)+ Mark Zuckerberg says the devices are our best bet at unlocking ‚Äúsuperintelligence.‚Äù (FT $)+ He‚Äôs also refusing to let his metaverse dream die. (WP $)+ What‚Äôs next for smart glasses. (MIT Technology Review)



2 DeepSeek writes flawed code for groups China disfavorsResearchers found that it produced code with major security weaknesses when told it was for the banned spiritual movement Falun Gong. (WP $)



3 The CDC is a messIts advice can no longer be trusted. Here‚Äôs where to turn instead. (The Atlantic $)+ Its ousted director claims RFK Jr pressured her to approve vaccine changes. (Wired $)+ Why childhood vaccines are a public health success story. (MIT Technology Review)



4 Google‚Äôs gen-AI image model Nano Banana is a global smash hitParticularly in India. (TechCrunch)+ Nvidia‚Äôs Jensen Huang really loves it, too. (Wired $)



5 OpenAI has found a way to reduce its models‚Äô schemingBut they weren‚Äôt able to eradicate it completely. (ZDNET)+ AI systems are getting better at tricking us. (MIT Technology Review)



6 Inside Texas‚Äô efforts to keep vector-borne diseases at bayThe Arbovirus-Entomology Laboratory analyzes mosquitos, but resources are drying up. (Vox)+ Brazil is fighting dengue with bacteria-infected mosquitos. (MIT Technology Review)



7 Financial AI advisors are comingBut companies are still cautious about rolling them out at scale. (WSJ $)+ Warning: ChatGPT‚Äôs advice may not necessarily be financially sound. (NYT $)+ Your most important customer may be AI. (MIT Technology Review)



8 China‚Äôs flying car market is raring to take offHovering taxis above the city of Guangzhou could soon become commonplace. (FT $)+ Eek‚Äîa pair of flying cars collided during an airshow earlier this week. (CNN)+ These aircraft could change how we fly. (MIT Technology Review)



9 Samsung‚Äôs US fridges will soon display adsWow, that‚Äôs not depressing at all. (The Verge)10 Online dating is getting even worse And AI is to blame. (NY Mag $)







Quote of the day



‚ÄúHow do educators have any real choice here about intentional use of AI when it is just being injected into educational environments without warning, without testing and without consultation?‚Äù



‚ÄîEamon Costello, an associate professor at Dublin City University, tells the Washington Post why he‚Äôs against Google adding a ‚Äòhomework help‚Äô button to its Chrome browser.







One more thing







Your boss is watchingWorking today‚Äîwhether in an office, a warehouse, or your car‚Äîcan mean constant electronic surveillance with little transparency, and potentially with livelihood-¬≠ending consequences if your productivity flags.But what matters even more than the effects of this ubiquitous monitoring on privacy may be how all that data is shifting the relationships between workers and managers, companies and their workforce. It‚Äôs a huge power shift that may require new policies and protections. Read the full story.



‚ÄîRebecca Ackermann







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)+ Find yourself feeling sleepy every afternoon? Here‚Äôs how to fight the post-lunch slump.+ Life lessons from a London graffiti artist.+ If you‚Äôre in need of a laugh, a good comedy is a great place to start.+ Yellowstone‚Äôs famous hot springs are under attack‚Äîfrom tourists‚Äô hats.
‚Ä¢ Clean hydrogen is facing a big reality check
  Hydrogen is sometimes held up as a master key for the energy transition . It can be made using several low-emissions methods and could play a role in cleaning up industries ranging from agriculture and chemicals to aviation and long-distance shipping . A number of major projects face cancellations and delays, especially in the US and Europe . Still, there are bright spots for the industry, including in China, and new markets could soon become crucial for growth .

üîí Cybersecurity & Privacy
No updates.

üéì University AI
No updates.

üè¢ Corporate AI
‚Ä¢ Move your AI agents from proof of concept to production with Amazon Bedrock AgentCore
  Building an AI agent that can handle a real-life use case in production is a complex undertaking. Although creating a proof of concept&nbsp;demonstrates the potential, moving to production requires addressing scalability, security, observability, and operational concerns that don‚Äôt surface in development environments. 
This post explores how Amazon Bedrock AgentCore helps you transition your agentic applications from experimental proof of concept to production-ready systems. We follow the journey of a customer support agent that evolves from a simple local prototype to a comprehensive, enterprise-grade solution capable of handling multiple concurrent users while maintaining security and performance standards. 
Amazon Bedrock AgentCore is a comprehensive suite of services designed to help you build, deploy, and scale agentic AI applications. If you‚Äôre new to AgentCore, we recommend exploring our existing deep-dive posts on individual services: AgentCore Runtime for secure agent deployment and scaling, AgentCore Gateway for enterprise tool development, AgentCore Identity for securing agentic AI at scale, AgentCore Memory for building context-aware agents, AgentCore Code Interpreter for code execution, AgentCore Browser Tool for web interaction, and AgentCore Observability for transparency on your agent behavior. This post demonstrates how these services work together in a real-world scenario. 
The customer support agent journey 
Customer support represents one of the most common and compelling use cases for agentic AI. Modern businesses handle thousands of customer inquiries daily, ranging from simple policy questions to complex technical troubleshooting. Traditional approaches often fall short: rule-based chatbots frustrate customers with rigid responses, and human-only support teams struggle with scalability and consistency. An intelligent customer support agent needs to seamlessly handle diverse scenarios: managing customer orders and accounts, looking up return policies, searching product catalogs, troubleshooting technical issues through web research, and remembering customer preferences across multiple interactions. Most importantly, it must do all this while maintaining the security and reliability standards expected in enterprise environments. Consider the typical evolution path many organizations follow when building such agents: 
 
 The proof of concept stage ‚Äì Teams start with a simple local prototype that demonstrates core capabilities, such as a basic agent that can answer policy questions and search for products. This works well for demos but lacks the robustness needed for real customer interactions. 
 The reality check ‚Äì As soon as you try to scale beyond a few test users, challenges emerge. The agent forgets previous conversations, tools become unreliable under load, there‚Äôs no way to monitor performance, and security becomes a paramount concern. 
 The production challenge ‚Äì Moving to production requires addressing session management, secure tool sharing, observability, authentication, and building interfaces that customers actually want to use. Many promising proofs of concept stall at this stage due to the complexity of these requirements. 
 
In this post, we address each challenge systematically. We start with a prototype agent equipped with three essential tools: return policy lookup, product information search, and web search for troubleshooting. From there, we add the capabilities needed for production deployment: persistent memory for conversation continuity and a hyper-personalized experience, centralized tool management for reliability and security, full observability for monitoring and debugging, and finally a customer-facing web interface. This progression mirrors the real-world path from proof of concept to production, demonstrating how Amazon Bedrock AgentCore services work together to solve the operational challenges that emerge as your agentic applications mature. For simplification and demonstration purposes, we consider a single-agent architecture. In real-life use cases, customer support agents are often created as multi-agent architectures and those scenarios are also supported by Amazon Bedrock AgentCore services. 
Solution overview 
Every production system starts with a proof of concept, and our customer support agent is no exception. In this first phase, we build a functional prototype that demonstrates the core capabilities needed for customer support. In this case, we use Strands Agents, an open source agent framework, to build the proof of concept and Anthropic‚Äôs Claude 3.7 Sonnet on Amazon Bedrock as the large language model (LLM) powering our agent. For your application, you can use another agent framework and model of your choice. 
Agents rely on tools to take actions and interact with live systems. Several tools are used in customer support agents, but to keep our example simple, we focus on three core capabilities to handle the most common customer inquiries: 
 
 Return policy lookup ‚Äì Customers frequently ask about return windows, conditions, and processes. Our tool provides structured policy information based on product categories, covering everything from return timeframes to refund processing and shipping policies. 
 Product information retrieval ‚Äì Technical specifications, warranty details, and compatibility information are essential for both pre-purchase questions and troubleshooting. This tool serves as a bridge to your product catalog, delivering formatted technical details that customers can understand. 
 Web search for troubleshooting ‚Äì Complex technical issues often require the latest solutions or community-generated fixes not found in internal documentation. Web search capability allows the agent to access the web for current troubleshooting guides and technical solutions in real time. 
 
The tools implementation and the end-to-end code for this use case are available in our GitHub repository. In this post, we focus on the main code that connects with Amazon Bedrock AgentCore, but you can follow the end-to-end journey in the repository. 
Create the agent 
With the tools available, let‚Äôs create the agent. The architecture for our proof of concept will look like the following diagram. 
 
You can find the end-to-end code for this post on the GitHub repository. For simplicity, we show only the essential parts for our end-to-end code here: 
 
 from strands import Agent
from strands.models import BedrockModel

@tool
def get_return_policy(product_category: str) -&gt; str:
&nbsp;&nbsp; &nbsp;"""Get return policy information for a specific product category."""
&nbsp;&nbsp; &nbsp;# Returns structured policy info: windows, conditions, processes, refunds
&nbsp; &nbsp; # check github for full code
&nbsp;&nbsp; &nbsp;return&nbsp;{"return_window":&nbsp;"10 days",&nbsp;"conditions":&nbsp;""}
&nbsp;&nbsp;&nbsp;&nbsp;
@tool &nbsp;
def get_product_info(product_type: str) -&gt; str:
&nbsp;&nbsp; &nbsp;"""Get detailed technical specifications and information for electronics products."""
&nbsp;&nbsp; &nbsp;# Returns warranty, specs, features, compatibility details
&nbsp;&nbsp; &nbsp;# check github for full code
&nbsp; &nbsp; return&nbsp;{"product":&nbsp;"ThinkPad X1 Carbon",&nbsp;"info":&nbsp;"ThinkPad X1 Carbon info"}
&nbsp;&nbsp;&nbsp;&nbsp;
@tool
def web_search(keywords: str, region: str = "us-en", max_results: int = 5) -&gt; str:
&nbsp;&nbsp; &nbsp;"""Search the web for updated troubleshooting information."""
&nbsp;&nbsp; &nbsp;# Provides access to current technical solutions and guides
&nbsp;&nbsp;&nbsp;&nbsp;# check github for full code
&nbsp; &nbsp; return&nbsp;"results from websearch"
&nbsp;&nbsp;&nbsp;&nbsp;
# Initialize the Bedrock model
model = BedrockModel(
&nbsp;&nbsp; &nbsp;model_id="us.anthropic.claude-3-7-sonnet-20250219-v1:0",
&nbsp;&nbsp; &nbsp;temperature=0.3
)

# Create the customer support agent
agent = Agent(
&nbsp;&nbsp; &nbsp;model=model,
&nbsp;&nbsp; &nbsp;tools=[
&nbsp; &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;get_product_info, 
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;get_return_policy, 
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;web_search
&nbsp;&nbsp;&nbsp;&nbsp;],
&nbsp;&nbsp; &nbsp;system_prompt="""You are a helpful customer support assistant for an electronics company.
&nbsp;&nbsp; &nbsp;Use the appropriate tools to provide accurate information and always offer additional help."""
) 
 
Test the proof of concept 
When we test our prototype with realistic customer queries, the agent demonstrates the correct tool selection and interaction with real-world systems: 
 
 # Return policy inquiry
response = agent("What's the return policy for my ThinkPad X1 Carbon?")
# Agent correctly uses get_return_policy with "laptops" category

# Technical troubleshooting &nbsp;
response = agent("My iPhone 14 heats up, how do I fix it?")
# Agent uses web_search to find current troubleshooting solutions 
 
The agent works well for these individual queries, correctly mapping laptop inquiries to return policy lookups and complex technical issues to web search, providing comprehensive and actionable responses. 
The proof of concept reality check 
Our proof of concept successfully demonstrates that an agent can handle diverse customer support scenarios using the right combination of tools and reasoning. The agent runs perfectly on your local machine and handles queries correctly. However, this is where the proof of concept gap becomes obvious. The tools are defined as local functions in your agent code, the agent responds quickly, and everything seems production-ready. But several critical limitations become apparent the moment you think beyond single-user testing: 
 
 Memory loss between sessions ‚Äì If you restart your notebook or application, the agent completely forgets previous conversations. A customer who was discussing a laptop return yesterday would need to start from scratch today, re-explaining their entire situation. This isn‚Äôt just inconvenient‚Äîit‚Äôs a poor customer experience that breaks the conversational flow that makes AI agents valuable. 
 Single customer limitation ‚Äì Your current agent can only handle one conversation at a time. If two customers try to use your support system simultaneously, their conversations would interfere with each other, or worse, one customer might see another‚Äôs conversation history. There‚Äôs no mechanism to maintain separate conversation context for different users. 
 Tools embedded in code ‚Äì Your tools are defined directly in the agent code. This means: 
   
   You can‚Äôt reuse these tools across different agents (sales agent, technical support agent, and so on). 
   Updating a tool requires changing the agent code and redeploying everything. 
   Different teams can‚Äôt maintain different tools independently. 
    
 No production infrastructure ‚Äì The agent runs locally with no consideration for scalability, security, monitoring, and reliability. 
 
These fundamental architectural barriers can prevent real customer deployment. Agent building teams can take months to address these issues, which delays the time to value from their work and adds significant costs to the application. This is where Amazon Bedrock AgentCore services become essential. Rather than spending months building these production capabilities from scratch, Amazon Bedrock AgentCore provides managed services that address each gap systematically. 
Let‚Äôs begin our journey to production by solving the memory problem first, transforming our agent from one that forgets every conversation into one that remembers customers across conversations and can hyper-personalize conversations using Amazon Bedrock AgentCore Memory. 
Add persistent memory for hyper-personalized agents 
The first major limitation we identified in our proof of concept was memory loss‚Äîour agent forgot everything between sessions, forcing customers to repeat their context every time. This ‚Äúgoldfish agent‚Äù behavior breaks the conversational experience that makes AI agents valuable in the first place. 
Amazon Bedrock AgentCore Memory solves this by providing managed, persistent memory that operates on two complementary levels: 
 
 Short-term memory ‚Äì Immediate conversation context and session-based information for continuity within interactions 
 Long-term memory ‚Äì Persistent information extracted across multiple conversations, including customer preferences, facts, and behavioral patterns 
 
After adding Amazon Bedrock AgentCore Memory to our customer support agent, our new architecture will look like the following diagram. 
 
Install dependencies 
Before we start, let‚Äôs install our dependencies: boto3, the AgentCore SDK, and the AgentCore Starter Toolkit SDK. Those will help us quickly add Amazon Bedrock AgentCore capabilities to our agent proof of concept. See the following code: 
 
 pip install boto3 bedrock-agentcore bedrock-agentcore-starter-toolkit 
 
Create the memory resources 
Amazon Bedrock AgentCore Memory uses configurable strategies to determine what information to extract and store. For our customer support use case, we use two complementary strategies: 
 
 USER_PREFERENCE ‚Äì Automatically extracts and stores customer preferences like ‚Äúprefers ThinkPad laptops,‚Äù ‚Äúuses Linux,‚Äù or ‚Äúplays competitive FPS games.‚Äù This enables personalized recommendations across conversations. 
 SEMANTIC ‚Äì Captures factual information using vector embeddings, such as ‚Äúcustomer has MacBook Pro order #MB-78432‚Äù or ‚Äúreported overheating issues during video editing.‚Äù This provides relevant context for troubleshooting. 
 
See the following code: 
 
 from bedrock_agentcore.memory import MemoryClient
from bedrock_agentcore.memory.constants import StrategyType

memory_client = MemoryClient(region_name=region)

strategies = [
&nbsp;&nbsp; &nbsp;{
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;StrategyType.USER_PREFERENCE.value: {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"name": "CustomerPreferences",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"description": "Captures customer preferences and behavior",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"namespaces": ["support/customer/{actorId}/preferences"],
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp;},
&nbsp;&nbsp; &nbsp;{
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;StrategyType.SEMANTIC.value: {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"name": "CustomerSupportSemantic", 
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"description": "Stores facts from conversations",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"namespaces": ["support/customer/{actorId}/semantic"],
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp;},
]

# Create memory resource with both strategies
response = memory_client.create_memory_and_wait(
&nbsp;&nbsp; &nbsp;name="CustomerSupportMemory",
&nbsp;&nbsp; &nbsp;description="Customer support agent memory",
&nbsp;&nbsp; &nbsp;strategies=strategies,
&nbsp;&nbsp; &nbsp;event_expiry_days=90,
) 
 
Integrate with Strands Agents hooks 
The key to making memory work seamlessly is automation‚Äîcustomers shouldn‚Äôt need to think about it, and agents shouldn‚Äôt require manual memory management. Strands Agents provides a powerful hook system that lets you intercept agent lifecycle events and handle memory operations automatically. The hook system enables both built-in components and user code to react to or modify agent behavior through strongly-typed event callbacks. For our use case, we create CustomerSupportMemoryHooks to retrieve the customer context and save the support interactions: 
 
 MessageAddedEvent hook ‚Äì Triggered when customers send messages, this hook automatically retrieves relevant memory context and injects it into the query. The agent receives both the customer‚Äôs question and relevant historical context without manual intervention. 
 AfterInvocationEvent hook ‚Äì Triggered after agent responses, this hook automatically saves the interaction to memory. The conversation becomes part of the customer‚Äôs persistent history immediately. 
 
See the following code: 
 
 class CustomerSupportMemoryHooks(HookProvider):
&nbsp;&nbsp; &nbsp;def retrieve_customer_context(self, event: MessageAddedEvent):
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"""Inject customer context before processing queries"""
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;user_query = event.agent.messages[-1]["content"][0]["text"]
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;# Retrieve relevant memories from both strategies
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;all_context = []
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;for context_type, namespace in self.namespaces.items():
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;memories = self.client.retrieve_memories(
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;memory_id=self.memory_id,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;namespace=namespace.format(actorId=self.actor_id),
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;query=user_query,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;top_k=3,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;)
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# Format and add to context
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;for memory in memories:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;if memory.get("content", {}).get("text"):
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;all_context.append(f"[{context_type.upper()}] {memory['content']['text']}")
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;# Inject context into the user query
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;if all_context:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;context_text = "\n".join(all_context)
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;original_text = event.agent.messages[-1]["content"][0]["text"]
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;event.agent.messages[-1]["content"][0]["text"] = f"Customer Context:\n{context_text}\n\n{original_text}"

&nbsp;&nbsp; &nbsp;def save_support_interaction(self, event: AfterInvocationEvent):
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"""Save interactions after agent responses"""
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Get last customer query and agent response check github for implementation
&nbsp; &nbsp; &nbsp; &nbsp; customer_query&nbsp;=&nbsp;"This is a sample query"
&nbsp; &nbsp; &nbsp; &nbsp; agent_response&nbsp;=&nbsp;"LLM gave a sample response"
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;# Extract customer query and agent response
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;# Save to memory for future retrieval
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;self.client.create_event(
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;memory_id=self.memory_id,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;actor_id=self.actor_id,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;session_id=self.session_id,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;messages=[(customer_query, "USER"), (agent_response, "ASSISTANT")]
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;) 
 
In this code, we can see that our hooks are the ones interacting with Amazon Bedrock AgentCore Memory to save and retrieve memory events. 
Integrate memory with the agent 
Adding memory to our existing agent requires minimal code changes; you can simply instantiate the memory hooks and pass them to the agent constructor. The agent code then only needs to connect with the memory hooks to use the full power of Amazon Bedrock AgentCore Memory. We will create a new hook for each session, which will help us handle different customer interactions. See the following code: 
 
 # Create memory hooks for this customer session
memory_hooks = CustomerSupportMemoryHooks(
&nbsp;&nbsp; &nbsp;memory_id=memory_id, 
&nbsp;&nbsp; &nbsp;client=memory_client, 
&nbsp;&nbsp; &nbsp;actor_id=customer_id, 
&nbsp;&nbsp; &nbsp;session_id=session_id
)

# Create agent with memory capabilities
agent = Agent(
&nbsp;&nbsp; &nbsp;model=model,

&nbsp;&nbsp; &nbsp;tools=[get_product_info, get_return_policy, web_search],
&nbsp;&nbsp; &nbsp;system_prompt=SYSTEM_PROMPT
) 
 
Test the memory in action 
Let‚Äôs see how memory transforms the customer experience. When we invoke the agent, it uses the memory from previous interactions to show customer interests in gaming headphones, ThinkPad laptops, and MacBook thermal issues: 
 
 # Test personalized recommendations
response = agent("Which headphones would you recommend?")
# Agent remembers: "prefers low latency for competitive FPS games"
# Response includes gaming-focused recommendations

# Test preference recall
response = agent("What is my preferred laptop brand?") &nbsp;
# Agent remembers: "prefers ThinkPad models" and "needs Linux compatibility"
# Response acknowledges ThinkPad preference and suggests compatible models 
 
The transformation is immediately apparent. Instead of generic responses, the agent now provides personalized recommendations based on the customer‚Äôs stated preferences and past interactions. The customer doesn‚Äôt need to re-explain their gaming needs or Linux requirements‚Äîthe agent already knows. 
Benefits of Amazon Bedrock AgentCore Memory 
With Amazon Bedrock AgentCore Memory integrated, our agent now delivers the following benefits: 
 
 Conversation continuity ‚Äì Customers can pick up where they left off, even across different sessions or support channels 
 Personalized service ‚Äì Recommendations and responses are tailored to individual preferences and past issues 
 Contextual troubleshooting ‚Äì Access to previous problems and solutions enables more effective support 
 Seamless experience ‚Äì Memory operations happen automatically without customer or agent intervention 
 
However, we still have limitations to address. Our tools remain embedded in the agent code, preventing reuse across different support agents or teams. Security and access controls are minimal, and we still can‚Äôt handle multiple customers simultaneously in a production environment. 
In the next section, we address these challenges by centralizing our tools using Amazon Bedrock AgentCore Gateway and implementing proper identity management with Amazon Bedrock AgentCore Identity, creating a scalable and secure foundation for our customer support system. 
Centralize tools with Amazon Bedrock AgentCore Gateway and Amazon Bedrock AgentCore Identity 
With memory solved, our next challenge is tool architecture. Currently, our tools are embedded directly in the agent code‚Äîa pattern that works for prototypes but creates significant problems at scale. When you need multiple agents (customer support, sales, technical support), each one duplicates the same tools, leading to extensive code, inconsistent behavior, and maintenance nightmares. 
Amazon Bedrock AgentCore Gateway simplifies this process by centralizing tools into reusable, secure endpoints that agents can access. Combined with Amazon Bedrock AgentCore Identity for authentication, it creates an enterprise-grade tool sharing infrastructure. 
We will now update our agent to use Amazon Bedrock AgentCore Gateway and Amazon Bedrock AgentCore Identity. The architecture will look like the following diagram. 
 
In this case, we convert our web search tool to be used in the gateway and keep the return policy and get product information tools local to this agent. That is important because web search is a common capability that can be reused across different use cases in an organization, and return policy and production information are capabilities commonly associated with customer support services. With Amazon Bedrock AgentCore services, you can decide which capabilities to use and how to combine them. In this case, we also use two new tools that could have been developed by other teams: check warranty and get customer profile. Because those teams have already exposed those tools using AWS Lambda functions, we can use them as targets to our Amazon Bedrock AgentCore Gateway. Amazon Bedrock AgentCore Gateway can also support REST APIs as target. That means that if we have an OpenAPI specification or a Smithy model, we can also quickly expose our tools using Amazon Bedrock AgentCore Gateway. 
Convert existing services to MCP 
Amazon Bedrock AgentCore Gateway uses the Model Context Protocol (MCP) to standardize how agents access tools. Converting existing Lambda functions into MCP endpoints requires minimal changes‚Äîmainly adding tool schemas and handling the MCP context. To use this functionality, we convert our local tools to Lambda functions and create the tools schema definitions to make these functions discoverable by agents: 
 
 # Original Lambda function (simplified)
def web_search(keywords: str, region: str = "us-en", max_results: int = 5) -&gt; str:
&nbsp; &nbsp; # web_search functionality
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
def lambda_handler(event, context):
&nbsp;&nbsp; &nbsp;if get_tool_name(event) == "web_search":
&nbsp; &nbsp; &nbsp; &nbsp; query&nbsp;= get_named_parameter(event=event, name="query")
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; search_result&nbsp;=&nbsp;web_search(keywords)
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;return {"statusCode": 200, "body": search_result} 
 
The following code is the tool schema definition: 
 
 {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"name": "web_search",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"description": "Search the web for updated information using DuckDuckGo",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"inputSchema": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"type": "object",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"properties": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"keywords": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"type": "string",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"description": "The search query keywords"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;},
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"region": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"type": "string",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"description": "The search region (e.g., us-en, uk-en, ru-ru)"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;},
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"max_results": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"type": "integer",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"description": "The maximum number of results to return"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;},
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"required": [
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"keywords"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;]
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp;} 
 
For demonstration purposes, we build a new Lambda function from scratch. In reality, organizations already have different functionalities available as REST services or Lambda functions, and this approach lets you expose existing enterprise services as agent tools without rebuilding them. 
Configure security with Amazon Bedrock AgentCore Gateway and integrate with Amazon Bedrock AgentCore Identity 
Amazon Bedrock AgentCore Gateway requires authentication for both inbound and outbound connections. Amazon Bedrock AgentCore Identity handles this through standard OAuth flows. After you set up an OAuth authorization configuration, you can create a new gateway and pass this configuration to it. See the following code: 
 
 # Create gateway with JWT-based authentication
auth_config = {
&nbsp;&nbsp; &nbsp;"customJWTAuthorizer": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"allowedClients": [cognito_client_id],
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"discoveryUrl": cognito_discovery_url
&nbsp;&nbsp; &nbsp;}
}

gateway_response = gateway_client.create_gateway(
&nbsp;&nbsp; &nbsp;name="customersupport-gw",
&nbsp;&nbsp; &nbsp;roleArn=gateway_iam_role,
&nbsp;&nbsp; &nbsp;protocolType="MCP",
&nbsp;&nbsp; &nbsp;authorizerType="CUSTOM_JWT",
&nbsp;&nbsp; &nbsp;authorizerConfiguration=auth_config,
&nbsp;&nbsp; &nbsp;description="Customer Support AgentCore Gateway"
) 
 
For inbound authentication, agents must present valid JSON Web Token (JWT) tokens (from identity providers like Amazon Cognito, Okta, and EntraID) as a compact, self-contained standard for securely transmitting information between parties to access Amazon Bedrock AgentCore Gateway tools. 
For outbound authentication, Amazon Bedrock AgentCore Gateway can authenticate to downstream services using AWS Identity and Access Management (IAM) roles, API keys, or OAuth tokens. 
For demonstration purposes, we have created an Amazon Cognito user pool with a dummy user name and password. For your use case, you should set a proper identity provider and manage the users accordingly. This configure makes sure only authorized agents can access specific tools and a full audit trail is provided. 
Add Lambda targets 
After you set up Amazon Bedrock AgentCore Gateway, adding Lambda functions as tool targets is straightforward: 
 
 lambda_target_config = {
&nbsp;&nbsp; &nbsp;"mcp": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"lambda": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"lambdaArn": lambda_function_arn,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"toolSchema": {"inlinePayload": api_spec},
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp;}
}

gateway_client.create_gateway_target(
&nbsp;&nbsp; &nbsp;gatewayIdentifier=gateway_id,
&nbsp;&nbsp; &nbsp;name="LambdaTools",
&nbsp;&nbsp; &nbsp;targetConfiguration=lambda_target_config,
&nbsp;&nbsp; &nbsp;credentialProviderConfigurations=[{
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"credentialProviderType": "GATEWAY_IAM_ROLE"
&nbsp;&nbsp; &nbsp;}]
) 
 
The gateway now exposes your Lambda functions as MCP tools that authorized agents can discover and use. 
Integrate MCP tools with Strands Agents 
Converting our agent to use centralized tools requires updating the tool configuration. We keep some tools local, such as product info and return policies specific to customer support that will likely not be reused in other use cases, and use centralized tools for shared capabilities. Because Strands Agents has a native integration for MCP tools, we can simply use the MCPClient from Strands with a streamablehttp_client. See the following code: 
 
 # Get OAuth token for gateway access
gateway_access_token = get_token(
&nbsp;&nbsp; &nbsp;client_id=cognito_client_id,
&nbsp;&nbsp; &nbsp;client_secret=cognito_client_secret,
&nbsp;&nbsp; &nbsp;scope=auth_scope,
&nbsp;&nbsp; &nbsp;url=token_url
)

# Create authenticated MCP client
mcp_client = MCPClient(
&nbsp;&nbsp; &nbsp;lambda: streamablehttp_client(
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;gateway_url,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;headers={"Authorization": f"Bearer {gateway_access_token['access_token']}"}
&nbsp;&nbsp; &nbsp;)
)

# Combine local and MCP tools
tools = [
&nbsp;&nbsp; &nbsp;get_product_info, &nbsp; &nbsp; # Local tool (customer support specific)
&nbsp;&nbsp; &nbsp;get_return_policy, &nbsp; &nbsp;# Local tool (customer support specific)
] + mcp_client.list_tools_sync() &nbsp;# Centralized tools from gateway

agent = Agent(
&nbsp;&nbsp; &nbsp;model=model,
&nbsp;&nbsp; &nbsp;tools=tools,
&nbsp;&nbsp; &nbsp;hooks=[memory_hooks],
&nbsp;&nbsp; &nbsp;system_prompt=SYSTEM_PROMPT
) 
 
Test the enhanced agent 
With the centralized tools integrated, our agent now has access to enterprise capabilities like warranty checking: 
 
 # Test web search using centralized tool &nbsp;
response = agent("How can I fix Lenovo ThinkPad with a blue screen?")
# Agent uses web_search from AgentCore Gateway 
 
The agent seamlessly combines local tools with centralized ones, providing comprehensive support capabilities while maintaining security and access control. 
However, we still have a significant limitation: our entire agent runs locally on our development machine. For production deployment, we need scalable infrastructure, comprehensive observability, and the ability to handle multiple concurrent users. 
In the next section, we address this by deploying our agent to Amazon Bedrock AgentCore Runtime, transforming our local prototype into a production-ready system with Amazon Bedrock AgentCore Observability and automatic scaling capabilities. 
Deploy to production with Amazon Bedrock AgentCore Runtime 
With the tools centralized and secured, our final major hurdle is production deployment. Our agent currently runs locally on your laptop, which is ideal for experimentation but unsuitable for real customers. Production requires scalable infrastructure, comprehensive monitoring, automatic error recovery, and the ability to handle multiple concurrent users reliably. 
Amazon Bedrock AgentCore Runtime transforms your local agent into a production-ready service with minimal code changes. Combined with Amazon Bedrock AgentCore Observability, it provides enterprise-grade reliability, automatic scaling, and comprehensive monitoring capabilities that operations teams need to maintain agentic applications in production. 
Our architecture will look like the following diagram. 
 
Minimal code changes for production 
Converting your local agent requires adding just four lines of code: 
 
 # Your existing agent code remains unchanged
model = BedrockModel(model_id="us.anthropic.claude-3-7-sonnet-20250219-v1:0")
memory_hooks = CustomerSupportMemoryHooks(memory_id, memory_client, actor_id, session_id)
agent = Agent(
&nbsp;&nbsp; &nbsp;model=model,
&nbsp;&nbsp; &nbsp;tools=[get_return_policy, get_product_info],
&nbsp;&nbsp; &nbsp;system_prompt=SYSTEM_PROMPT,
&nbsp;&nbsp; &nbsp;hooks=[memory_hooks]
)

def invoke(payload):
&nbsp;&nbsp; &nbsp;user_input = payload.get("prompt", "")
&nbsp;&nbsp; &nbsp;response = agent(user_input)
&nbsp;&nbsp; &nbsp;return response.message["content"][0]["text"]

if __name__ == "__main__":
 
 
BedrockAgentCoreApp automatically creates an HTTP server with the required /invocations and /ping endpoints, handles proper content types and response formats, manages error handling according to AWS standards, and provides the infrastructure bridge between your agent code and Amazon Bedrock AgentCore Runtime. 
Secure production deployment 
Production deployment requires proper authentication and access control. Amazon Bedrock AgentCore Runtime integrates with Amazon Bedrock AgentCore Identity to provide enterprise-grade security. Using the Bedrock AgentCore Starter Toolkit, we can deploy our application using three simple steps: configure, launch, and invoke. 
During the configuration, a Docker file is created to guide the deployment of our agent. It contains information about the agent and its dependencies, the Amazon Bedrock AgentCore Identity configuration, and the Amazon Bedrock AgentCore Observability configuration to be used. During the launch step, AWS CodeBuild is used to run this Dockerfile and an Amazon Elastic Container Registry (Amazon ECR) repository is created to store the agent dependencies. The Amazon Bedrock AgentCore Runtime agent is then created, using the image of the ECR repository, and an endpoint is generated and used to invoke the agent in applications. If your agent is configured with OAuth authentication through Amazon Bedrock AgentCore Identity, like ours will be, you also need to pass the authentication token during the agent invocation step. The following diagram illustrates this process. 
 
The code to configure and launch our agent on Amazon Bedrock AgentCore Runtime will look as follows: 
 
 from bedrock_agentcore_starter_toolkit import Runtime

# Configure secure deployment with Cognito authentication
agentcore_runtime = Runtime()

response = agentcore_runtime.configure(
&nbsp;&nbsp; &nbsp;entrypoint="lab_helpers/lab4_runtime.py",
&nbsp;&nbsp; &nbsp;execution_role=execution_role_arn,
&nbsp;&nbsp; &nbsp;auto_create_ecr=True,
&nbsp;&nbsp; &nbsp;requirements_file="requirements.txt",
&nbsp;&nbsp; &nbsp;region=region,
&nbsp;&nbsp; &nbsp;agent_name="customer_support_agent",
&nbsp;&nbsp; &nbsp;authorizer_configuration={
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"customJWTAuthorizer": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"allowedClients": [cognito_client_id],
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"discoveryUrl": cognito_discovery_url,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp;}
)

# Deploy to production
launch_result = agentcore_runtime.launch() 
 
This configuration creates a secure endpoint that only accepts requests with valid JWT tokens from your identity provider (such as Amazon Cognito, Okta, or Entra). For our agent, we use a dummy setup with Amazon Cognito, but your application can use an identity provider of your choosing. The deployment process automatically builds your agent into a container, creates the necessary AWS infrastructure, and establishes monitoring and logging pipelines. 
Session management and isolation 
One of the most critical production features for agents is proper session management. Amazon Bedrock AgentCore Runtime automatically handles session isolation, making sure different customers‚Äô conversations don‚Äôt interfere with each other: 
 
 # Customer 1 conversation
response1 = agentcore_runtime.invoke(
&nbsp;&nbsp; &nbsp;{"prompt": "My iPhone Bluetooth isn't working. What should I do?"},
&nbsp;&nbsp; &nbsp;bearer_token=auth_token,
&nbsp;&nbsp; &nbsp;session_id="session-customer-1"
)

# Customer 1 follow-up (maintains context)
response2 = agentcore_runtime.invoke(
&nbsp;&nbsp; &nbsp;{"prompt": "I've turned Bluetooth on and off but it still doesn't work"},
&nbsp;&nbsp; &nbsp;bearer_token=auth_token,
&nbsp;&nbsp; &nbsp;session_id="session-customer-1" &nbsp;# Same session, context preserved
)

# Customer 2 conversation (completely separate)
response3 = agentcore_runtime.invoke(
&nbsp;&nbsp; &nbsp;{"prompt": "Still not working. What is going on?"},
&nbsp;&nbsp; &nbsp;bearer_token=auth_token,
&nbsp;&nbsp; &nbsp;session_id="session-customer-2" &nbsp;# Different session, no context
) 
 
Customer 1‚Äôs follow-up maintains full context about their iPhone Bluetooth issue, whereas Customer 2‚Äôs message (in a different session) has no context and the agent appropriately asks for more information. This automatic session isolation is crucial for production customer support scenarios. 
Comprehensive observability with Amazon Bedrock AgentCore Observability 
Production agents need comprehensive monitoring to diagnose issues, optimize performance, and maintain reliability. Amazon Bedrock AgentCore Observability automatically instruments your agent code and sends telemetry data to Amazon CloudWatch, where you can analyze patterns and troubleshoot issues in real time. The observability data includes session-level tracking, so you can trace individual customer session interactions and understand exactly what happened during a support interaction. You can use Amazon Bedrock AgentCore Observability with an agent of your choice, hosted in Amazon Bedrock AgentCore Runtime or not. Because Amazon Bedrock AgentCore Runtime automatically integrates with Amazon Bedrock AgentCore Observability, we don‚Äôt need extra work to observe our agent. 
With Amazon Bedrock AgentCore Runtime deployment, your agent is ready to be used in production. However, we still have one limitation: our agent is accessible only through SDK or API calls, requiring customers to write code or use technical tools to interact with it. For true customer-facing deployment, we need a user-friendly web interface that customers can access through their browsers. 
In the following section, we demonstrate the complete journey by building a sample web application using Streamlit, providing an intuitive chat interface that can interact with our production-ready Amazon Bedrock AgentCore Runtime endpoint. The exposed endpoint maintains the security, scalability, and observability capabilities we‚Äôve built throughout our journey from proof of concept to production. In a real-world scenario, you would integrate this endpoint with your existing customer-facing applications and UI frameworks. 
Create a customer-facing UI 
With our agent deployed to production, the final step is creating a customer-facing UI that customers can use to interface with the agent. Although SDK access works for developers, customers need an intuitive web interface for seamless support interactions. 
To demonstrate a complete solution, we build a sample Streamlit-based web-application that connects to our production-ready Amazon Bedrock AgentCore Runtime endpoint. The frontend includes secure Amazon Cognito authentication, real-time streaming responses, persistent session management, and a clean chat interface. Although we use Streamlit for rapid-prototyping, enterprises would typically integrate the endpoint with their existing interface or preferred UI frameworks. 
The end-to-end application (shown in the following diagram) maintains full conversation context across the sessions while providing the security, scalability, and observability capabilities that we built throughout this post. The result is a complete customer support agentic system that handles everything from initial authentication to complex multi-turn troubleshooting conversations, demonstrating how Amazon Bedrock AgentCore services transform prototypes into production-ready customer applications. 
 
Conclusion 
Our journey from prototype to production demonstrates how Amazon Bedrock AgentCore services address the traditional barriers to deploying enterprise-ready agentic applications. What started as a simple local customer support chatbot transformed into a comprehensive, production-grade system capable of serving multiple concurrent users with persistent memory, secure tool sharing, comprehensive observability, and an intuitive web interface‚Äîwithout months of custom infrastructure development. 
The transformation required minimal code changes at each step, showcasing how Amazon Bedrock AgentCore services work together to solve the operational challenges that typically stall promising proofs of concept. Memory capabilities avoid the ‚Äúgoldfish agent‚Äù problem, centralized tool management through Amazon Bedrock AgentCore Gateway creates a reusable infrastructure that securely serves multiple use cases, Amazon Bedrock AgentCore Runtime provides enterprise-grade deployment with automatic scaling, and Amazon Bedrock AgentCore Observability delivers the monitoring capabilities operations teams need to maintain production systems. 
The following video provides an overview of AgentCore capabilities. 

 
  
 
 
Ready to build your own production-ready agent? Start with our complete end-to-end tutorial, where you can follow along with the exact code and configurations we‚Äôve explored in this post. For additional use cases and implementation patterns, explore the broader GitHub repository, and dive deeper into service capabilities and best practices in the Amazon Bedrock AgentCore documentation. 
 
About the authors 
Maira Ladeira Tanke is a Tech Lead for Agentic AI at AWS, where she enables customers on their journey to develop autonomous AI systems. With over 10 years of experience in AI/ML, Maira partners with enterprise customers to accelerate the adoption of agentic applications using Amazon Bedrock AgentCore and Strands Agents, helping organizations harness the power of foundation models to drive innovation and business transformation. In her free time, Maira enjoys traveling, playing with her cat, and spending time with her family someplace warm.
‚Ä¢ Scale visual production using Stability AI Image Services in Amazon Bedrock
  This post was written with Alex Gnibus of Stability AI. 
Stability AI Image Services are now available in Amazon Bedrock, offering ready-to-use media editing capabilities delivered through the Amazon Bedrock API. These image editing tools expand on the capabilities of Stability AI‚Äôs Stable Diffusion 3.5 models (SD3.5) and Stable Image Core and Ultra models, which are already available in Amazon Bedrock and have set new standards in image generation. 
The professional creative production process consists of multiple editing steps to get the exact output needed. With Stability AI Image Services in Amazon Bedrock, you can modify, enhance, and transform existing images without jumping between multiple systems or sending files to external services. Everything runs through the same Amazon Bedrock experience you‚Äôre already using. The business impact can be immediate for teams that produce visual content at scale. 
In this post, we explore examples of how these tools enable precise creative control to accelerate professional-grade visual content. 
Editing tools now available in Amazon Bedrock 
Stability AI Image Services span&nbsp;9 tools across two categories: Edit and Control. Each tool handles specific editing tasks that typically require specialized software or manual intervention. 
Edit: Advanced capabilities for granular editing steps 
The tools in the Edit category make complex editing tasks more accessible and efficient. 
The suite begins with fundamental yet powerful retouching tools. The Erase Object tool, for example, removes unwanted elements from images while intelligently maintaining background consistency. The following animation showcases the Erase Object tool removing a mannequin from a product shot while preserving the background. The tool can transform a source image based on a mask image or derive the mask from the source image‚Äôs alpha channel. 
 
The Remove Background tool automatically isolates subjects with precision. This enables the creation of clean, professional product listings with consistent backgrounds or a variety of lifestyle settings, which is a game changer for ecommerce. 
The following example illustrates the removal of an image background, while preserving details of a furniture product in the foreground. 
 
The Search and Recolor and Search and Replace tools target specific elements within images for modification. Search and Recolor changes object colors; for example, showing different colorways of a dress without new photoshoots. In the following illustration, Search and Recolor changes the color swatch on furniture. 
 
Search and Replace can swap objects entirely, which is useful for updating seasonal elements in marketing materials or replacing products. The following is an application of Search and Replace for virtual try-on experiences. 
 
The Inpaint tool intelligently modifies images by filling in or replacing specified areas with new content based on the content of a mask image. 
Control: Structural and stylistic precision 
This category of tools provides precise manipulation of image structure and style through three specialized tools. 
The Sketch tool transforms sketch-style renderings into photorealistic concepts. Architecture firms might use this to convert conceptual drawings into realistic visualizations, and apparel brands to turn design sketches into product mockups. The tool helps accelerate the creative production process from initial concepts to final visual execution. 
In this example, the Sketch tool transforms a building architecture drawing to help real estate developers visualize the concept against a cityscape. 
 
In another example, the Sketch tool transforms a mannequin drawing into a photorealistic model shot. 
 
The Structure tool maintains the structural elements of input images while allowing content modification. This tool helps preserve layouts, compositions, and spatial relationships while changing subjects or styles. Creative teams can use the Structure tool to recreate scenes with different subjects or render new characters while maintaining consistent framing. 
The following example demonstrates the Structure tool transforming a workshop scene into a new scene while preserving the composition and spatial relationships. 
 
The Style Guide and Style Transfer tools help marketing teams produce new images that align with brand style and guidelines. The Style Guide tool takes artistic styles and colors from a reference style image and generates new images based on text prompts. 
In the following example, the Style Guide tool takes clues from a brand‚Äôs color palette and textures and generates new images matching brand identity. 
 
The Style Transfer tool uses visual characteristics from reference images to transform existing images, while preserving the original composition. For example, a home decor retailer can transform product imagery from modern minimalist to traditional styles without new photography. Marketing teams could create seasonal variations by applying different visual styles to existing product catalogs. 
Solution overview 
To demonstrate Stability AI Image Services in Amazon Bedrock, let‚Äôs walk through an example using a Jupyter notebook found in the GitHub repo. 
Prerequisites 
To follow along, you must have the following prerequisites: 
 
 An AWS account. 
 AWS credentials configured for creating and accessing Amazon Bedrock and Amazon SageMaker AI resources. 
 An AWS Identity and Access Management (IAM) execution role for SageMaker AI, which has the AmazonSageMakerFullAccess and AmazonBedrockLimitedAccess AWS managed policies attached. For more details, see How to use SageMaker AI execution roles. 
 A SageMaker notebook instance. 
 Stability AI Image Services model access, which you can request through the Amazon Bedrock console. Refer to Access Amazon Bedrock foundation models for more details. 
 
Create a SageMaker AI notebook instance 
Complete the following steps to create a SageMaker AI notebook instance, which can be used to run the sample notebook: 
 
 On the SageMaker AI console, in the navigation pane, under Applications and IDEs, choose Notebooks. 
 Choose Create notebook instance. 
 For Notebook instance name, enter a name for your notebook instance (for example, ai-images-notebook-instance). 
 For Notebook Instance type, choose ml.t2.medium. 
 For Platform identifier, choose Amazon Linux 2. 
 For IAM role, choose either an existing IAM role, which has the AmazonSageMakerFullAccess and AmazonBedrockLimitedAccess policies attached, or choose Create a new role. 
 Note the name of the IAM role that you chose. 
 Leave other settings as default and choose Create notebook instance. 
 
After a few minutes, SageMaker AI creates a notebook instance, and its status changes from Pending to InService. 
Confirm the IAM role for the notebook instance has the necessary permissions 
Complete the following steps to verify that the SageMaker AI execution role that you assigned to the notebook instance has the correct permissions: 
 
 On the IAM console, in the navigation pane, under Access management, choose Roles. 
 In the Roles search bar, enter the name of the SageMaker AI execution role that you used when creating the notebook instance. 
 Choose the IAM role. 
 Under Permissions policies, verify that the AWS managed policies AmazonSageMakerFullAccess and AmazonBedrockLimitedAccess are present. 
 (Optional) If either policy is missing, choose Add permissions, then choose Attach policies to attach the missing policy. 
   
   In the Other permissions policies search bar, enter the policy name. 
   Select the policy, then chose Add permissions. 
    
 
Run the notebook 
Complete the following steps to run the notebook: 
 
 On the SageMaker AI console, in the navigation pane, under Applications and IDEs, choose Notebooks. 
 Choose the newly created ai-images-notebook-instance notebook instance. 
 Wait for the notebook to be in InService status. 
 Choose the Open JupyterLab link to launch JupyterLab in a new browser tab. 
 On the Git menu, choose Clone a Repository. 
 Enter the URI https://github.com/aws-samples/stabilityai-sample-notebooks.git and select Include submodules and Download the repository. 
 Choose Clone. 
 On the File menu, choose Open from path. 
 Enter the following: stabilityai-sample-notebooks/stability-ai-image-services/stability-ai-image-services-sample-notebook.ipynb 
 Choose Open. 
 When prompted, choose the kernel conda_python3, then choose Select. 
 Run through each notebook cell to experience Stability AI Image Services in Amazon Bedrock. 
 
Clean up 
To avoid ongoing charges, stop the ai-images-notebook-instance SageMaker AI notebook instance that you created in this walkthrough: 
 
 On the SageMaker AI console, in the navigation pane, under Applications and IDEs, choose Notebooks. 
 Choose the ai-images-notebook-instance SageMaker AI notebook instance that you created. 
 Choose Actions, then choose Stop. 
 
After a few minutes, the notebook instance transitions from Stopping to Stopped status. 
 
 Choose Actions, then Delete. 
 
After a few seconds, SageMaker AI deletes the notebook instance. 
For more details, refer to Clean up Amazon SageMaker notebook instance resources. 
Conclusion 
The availability of Stability AI Image Services in Amazon Bedrock is an exciting step forward for visual content creation and manipulation, with particularly time-saving implications for professional creative teams at enterprises. 
For example, in media and entertainment, creators can rapidly enhance scenes and create special effects, and marketing teams can generate multiple campaign variations effortlessly. Retail and ecommerce businesses can streamline product photography and digital catalog creation, and gaming developers can prototype environments more efficiently. Architecture firms can visualize design concepts instantly, and educational institutions can create more engaging visual content. 
With these tools, businesses of different sizes can produce professional-grade, highly engaging visual content with efficiency and creativity. These tools can streamline operations, reduce costs, and open new creative possibilities, helping brands tell their stories more effectively and engage customers in more compelling ways. 
To get started, check out Stability AI models in Amazon Bedrock and the AWS Samples GitHub repo. 
 
About the authors 
Alex Gnibus is a Product Marketing Manager at Stability AI, connecting the dots between cutting-edge research breakthroughs and practical use cases. With experience spanning from creative agencies to deep enterprise tech, Alex brings both technical expertise and an understanding of the challenges that professional creative teams can solve with generative AI. 
Isha Dua is a Senior Solutions Architect based in the San Francisco Bay Area. She helps AWS Enterprise customers grow by understanding their goals and challenges and guiding them on how they can architect their applications in a cloud-based manner while making sure they are resilient and scalable. She‚Äôs passionate about machine learning technologies and environmental sustainability. 
Fabio Branco is a Senior Customer Solutions Manager at Amazon Web Services (AWS) and strategic advisor helping customers achieve business transformation, drive innovation through generative AI and data solutions, and successfully navigate their cloud journeys. Prior to AWS, he held Product Management, Engineering, Consulting, and Technology Delivery roles across multiple Fortune 500 companies in industries, including retail and consumer goods, oil and gas, financial services, insurance, and aerospace and defense. 
Suleman Patel is a Senior Solutions Architect at Amazon Web Services (AWS), with a special focus on machine learning and modernization. With expertise in both business and technology, Suleman helps customers design and build solutions that tackle real-world business problems. When he‚Äôs not immersed in his work, Suleman loves exploring the outdoors, taking road trips, and cooking up delicious dishes in the kitchen.
‚Ä¢ Prompting for precision with Stability AI Image Services in Amazon Bedrock
  Amazon Bedrock now offers Stability AI Image Services: 9 tools that improve how businesses create and modify images. The technology extends Stable Diffusion and Stable Image models to give you precise control over image creation and editing. Clear prompts are critical‚Äîthey provide art direction to the AI system. Strong prompts control specific elements like tone, texture, lighting, and composition to create the desired visual outcomes. This capability serves professional needs across product photography, concept, and marketing campaigns. 
In this post, we expand on the post Understanding prompt engineering: Unlock the creative potential of Stability AI models on AWS. We show how to effectively use advanced prompting techniques to maximize image generation quality and precision for enterprise application using Stability AI Image Services in Amazon Bedrock. 
Solution overview 
Stability AI Image Services are available as APIs in Amazon Bedrock, featuring capabilities such as, in-painting, style transfer, recoloring, background removal, object removal, style guide, and much more. 
In the following sections, we first discuss prompt structure for maximum control of image generation, then we provide advanced techniques of prompting for stylistic guidance. Code samples can be found in the following GitHub repository. 
Prerequisites 
To get started with Stability AI Image Services in Amazon Bedrock, follow the instructions in Getting started with the API to complete the following prerequisites: 
 
 Set up your AWS account. 
 Acquire credentials to grant programmatic access. 
 Attach the Amazon Bedrock permission to an AWS Identity and Access Management (IAM) user or role. 
 Request access to the Amazon Bedrock models. 
 
Structure prompts that maximize control 
To maximize the granular capabilities of Stability AI Image Services in Amazon Bedrock, you must construct prompts that enable fine-grained control. 
This section outlines best practices for building effective prompts that produce the desired output. We demonstrate how prompt structure affects results and why more structured prompts typically yield more consistent and controllable outcomes. 
Choose the right prompt type for your use case 
Selecting the right prompt format helps the model better understand your intent. Three primary prompt formats deliver different levels of control and readability: 
 
 Natural language maximizes readability and is best for general usage 
 Tag-based formats enable precise structural control and are ideal for technical application 
 Hybrid formats combine natural language and the structural elements of tags to provide even more control 
 
The following table provides examples of these three common ways to phrase your prompts. Each prompt format has its strengths depending on your goal or the interface you‚Äôre using. 
 
  
   
   Prompt type  
   Prompt example  
   Generated image using Stable Image Ultra in Amazon Bedrock 
   Description and use case 
   
   
   Basic Prompt (Natural Language) 
   ‚ÄúA clean product photo of a perfume bottle on a marble countertop‚Äù 
    
   This is readable and intuitive. Great for exploration, conversational tools, and some model types. Stable Diffusion 3.5 responds best to this style. 
   
   
   Tag-Based Prompt 
   ‚Äúperfume bottle, marble surface, soft light, high quality, product photo‚Äù 
    
   Used in many generation UIs or with models trained on datasets like LAION or Danbooru. Compact and good for stacking details. 
   
   
   Hybrid Prompt 
   ‚Äúperfume bottle on marble counter, soft studio lighting, sharp focus, f/2.8lens‚Äù 
    
   Best of both worlds. Add emphasis with weighting syntax to influence the model‚Äôs priorities. 
   
  
 
Build modular prompts 
Modular prompting enhances AI image generation effectiveness. This approach divides prompts into distinct components, each specifying what to draw and how it should appear. Modular structures provide several benefits: they help prevent conflicting or confusing instructions, allow for precise output control, and simplify prompt debugging. By isolating individual elements, you can quickly identify and adjust effective or ineffective parts of your prompts. This method ultimately leads to more refined and targeted AI-generated images. 
The following table provides examples of modular prompt modules. Experiment with different prompt sequences for your desired outcome; for example, placing the style before the subject will give it a more visual weight. 
 
  
   
   Module 
   Example 
   Description 
   
   
   Prefix 
   ‚Äúfashion editorial portrait of‚Äù 
   Sets the tone and intent for a high-fashion styled portrait 
   
   
   Subject 
   ‚Äúa woman with medium-brown skin and short coiled hair‚Äù 
   Gives the model‚Äôs look and surface detail to help guide facial features 
   
   
   Modifiers 
   ‚Äúwearing an asymmetrical black mesh top, metallic jewelry‚Äù 
   Adds stylized clothing and accessories for visual interest 
   
   
   Action 
   ‚Äúseated with her shoulders angled, eyes locked on camera, one arm lifted‚Äù 
   Describes body language and pose to give dynamic composition 
   
   
   Environment 
   ‚Äúbathed in intersecting beams of hard directional light through window slats‚Äù 
   Adds context for dramatic light play and atmosphere 
   
   
   Style 
   ‚Äúhigh-contrast chiaroscuro lighting, sculptural and abstract‚Äù 
   Informs the aesthetic and mood (shadow-driven, moody, architectural) 
   
   
   Camera/Lighting 
   ‚Äúshot on 85mm, studio setup, layered shadows and light falling across face and body‚Äù 
   Adds technical precision and helps control realism and fidelity 
   
  
 
The following example illustrates how to use a modular prompt to generate the desired output. 
 
  
   
   Modular Prompt 
   Generated Image Using Stable Image Ultra in Amazon Bedrock 
   
   
   ‚Äúfashion editorial portrait of a woman with medium-brown skin and short coiled hair, wearing an asymmetrical black mesh top and metallic jewelry, seated with shoulders angled and one arm lifted, eyes locked on camera, bathed in intersecting beams of hard directional light through window slats, layered shadows and highlights sculpting her face and body, high-contrast chiaroscuro lighting, abstract and bold, shot on 85mm in studio‚Äù 
    
   
  
 
Use negative prompts for polished output 
Negative prompts improve AI output quality by removing specific visual elements. Explicitly defining what not to include in the prompt guides the model‚Äôs output, typically leading to professional outputs. Negative prompts act like a retoucher‚Äôs checklist used to address aspects of an image to enhance quality and appeal. For example, ‚ÄúNo weird hands. No blurry corners. No cartoon filters. Definitely no watermarks.‚Äù Negative prompts result in clean, confident, compositions, free of distracting element and distortions. 
The following table provides examples of additional tokens that can be used in negative prompts. 
 
  
   
   Artifact Type 
   Tokens to Use 
   
   
   Low quality or noise 
   blurry, lowres, jpeg artifacts, noisy 
   
   
   Anatomy or model issues 
   deformed, extra limbs, bad hands, missing fingers 
   
   
   Style clashes 
   cartoon, illustration, anime, painting 
   
   
   Technical errors 
   watermark, text, signature, overexposed 
   
   
   General cleanup 
   ugly, poorly drawn, distortion, worst quality 
   
  
 
The following example illustrates how a well-structured negative prompt can enhance photorealism. 
 
  
   
   Without Negative Prompt 
    Prompt ‚Äú(medium full shot) of (charming office cubicle) made of glass material, multiple colors, modern style, space-saving, upholstered seat, patina, gold trim, located in a modern garden, with sleek furniture, stylish decor, bright lighting, comfortable seating, Masterpiece, best quality, raw photo, realistic, very aesthetic, dark ‚Äú 
    
   
   
   With Negative Prompt 
    Prompt ‚Äú(medium full shot) of (charming office cubicle) made of glass material, multiple colors, modern style, space-saving, upholstered seat, patina, gold trim, located in a modern garden, with sleek furniture, stylish decor, bright lighting, comfortable seating, Masterpiece, best quality, raw photo, realistic, very aesthetic, dark‚Äù Negative Prompt ‚Äúcartoon, 3d render, cgi, oversaturated, smooth plastic textures, unreal lighting, artificial, matte surface, painterly, dreamy, glossy finish, digital art, low detail background‚Äù 
    
   
  
 
Emphasize or suppress elements with prompt weighting 
Prompt weighting controls the influence of individual elements in AI image generation. These numerical weights prioritize specific prompt components over others. For example, to emphasize the character over the background, you can apply a 1.8 weight to ‚Äúcharacter‚Äù (character: 1.8) and 1.1 to ‚Äúbackground‚Äù (background: 1.1), which makes sure the model prioritizes character detail while maintaining environmental context. This targeted emphasis produces more precise outputs by minimizing competition between prompt elements and clarifying the model‚Äôs priorities. 
The syntax for prompt weights is (&lt;term&gt;:&lt;weight&gt;). You can also use a shorthand such as ((&lt;term&gt;)), where the number of parentheses represent the weight. Values between 0.0‚Äì1.0 deemphasize the term, and values between 1.1‚Äì2.0 emphasize the term.For example: 
 
 (term:1.2): Emphasize 
 (term:0.8): Deemphasize 
 ((term)): Shorthand for (term:1.2) 
 (((((((((term)))))))): Shorthand for (term:1.8) 
 
The following example shows how prompt weights contribute to the generated output. 
 
  
   
    Prompt with weights ‚Äúeditorial product photo of (a translucent gel moisturizer jar:1.4) placed on a (frosted glass pedestal:1.2), surrounded by (dewy pink flower petals:1.1), with soft (diffused lighting:1.3), subtle water droplets, shallow depth of field‚Äù 
    
   
   
    Prompt without weights ‚Äúeditorial product photo of a translucent gel moisturizer jar placed on a frosted glass pedestal, surrounded by dewy pink flower petals, with soft, subtle water droplets, shallow depth of field‚Äù 
    
   
  
 
You can also use weights in negative prompts to reduce how strongly the model avoids something. For example, ‚Äú(text:0.5), (blurry:0.2), (lowres:0.1).‚Äù This tells the model to be especially sure to avoid generating blurry text or low-resolution content. 
Giving specific stylistic guidance 
Effective prompt writing when using Stability AI Image Services such as Style Transfer and Style Guide requires a good understanding of style matching and reference-driven prompting. These techniques help provide clear stylistic direction for both text-to-image and image-to-image creation. 
Image-to-image style transfer extracts stylistic elements from an input image (control image) and uses it to guide the creation of an output image based on the prompt. Approach writing the prompt as if you‚Äôre directing a professional photographer or stylist. Focus on materials, lighting quality, and artistic intention‚Äînot just objects. For example, a well-structured prompt might read: ‚ÄúClose-up editorial photo of a translucent green lip gloss tube on crushed iridescent plastic, diffused colored lighting, shallow DOF, high fashion product styling.‚Äù 
Style tag layering: Known aesthetic labels that align with brand identity 
The art of crafting effective prompts often relies on incorporating established style tags that resonate with familiar visual languages and datasets. By strategically blending terms from recognized aesthetic categories (ranging from editorial photography and analog film to anime, cyberpunk cityscapes, and brutalist structures), creators can guide the AI toward specific visual outcomes that align with their brand identity. These style descriptors serve as powerful anchors in the prompt engineering process. The versatility of these tags extends further through their ability to be combined and weighted, allowing for nuanced control over the final aesthetic. For instance, a skincare brand might blend the clean lines of product photography with dreamy, surreal elements, whereas a tech company could merge brutalist structure with cyberpunk elements for a distinctive visual identity. This approach to style mixing helps creators improve their outputs while maintaining clear ties to recognizable visual genres that resonate with their target audience. The key is understanding how these style tags interact and using their combinations to create unique, yet culturally relevant, visual expressions that serve specific creative or commercial objectives. The following table provides examples of prompts for a desired aesthetic. 
 
  
   
   Desired aesthetic 
   Prompt phrases 
   Example use case 
   
   
   Retro / Y2K 
   2000s nostalgia, flash photography, candy tones, harsh lighting 
   Metallic textures, thin fonts, early digital feel. 
   
   
   Clean modern 
   neutral tones, soft gradients, minimalist styling, editorial layout 
   Great for wellness or skincare products. 
   
   
   Bold streetwear 
   urban background, oversized fit, strong pose, midday shadow 
   Fashion photography and lifestyle ads. Prioritize outfit structure and location cues. 
   
   
   Hyperreal surrealism 
   dreamcore lighting, glossy textures, cinematic DOF, surreal shadows 
   Plays well in music, fashion, or alt-culture campaigns. 
   
  
 
Invoke a named style as a reference 
Some prompt structures benefit from invoking a named visual signature from a specific artist, especially when combined with your own stylistic phrasing or workflows, as shown in the following example. 
 
  
   
    Prompt ‚Äúeditorial studio portrait of a woman with glowing skin in minimalist glam makeup, high-contrast lighting, clean background, (depiction of Van Gogh style:1.3)‚Äù 
    
   
  
 
The following is a more conceptual example. 
 
  
   
    Prompt ‚Äúproduct shot of a silver hair oil bottle with soft reflections on curved chrome, (depiction of Wes Anderson style:1.2), under cold studio lighting‚Äù 
    
   
  
 
These phrases function like calling on a genre; they imply choices around materials, lighting, layout, and color tonality. 
Use reference images to guide style 
Another useful technique is using a reference image to guide the pose, color, or composition of the output. For use cases like matching a pose from a lookbook image, transferring a color palette from a campaign still, or copying shadowplay from a photo shoot, you can extract and apply structure or style from reference images. 
Stability AI Image Services support a variety of image-to-image workflows where you can use a reference image (control image) to guide the output, such as Structure, Sketch, and Style. Tools like ControlNet (a neural network architecture developed by Stability AI that enhances control), IP-Adapter (an image prompt adapter), or clip-based captioning also enable further control when paired with Stability AI models. 
We will discuss ControlNet, IP-Adapter, and clip-based captioning in a subsequent post. 
The following is an example of an image-to-image workflow: 
 
 Find a high-quality editorial reference. 
 Use it with a depth, canny, or seg ControlNet to lock a pose. 
 Style with a prompt. 
 
 
  
   
    Prompt ‚Äúfashion editorial of a model in layered knitwear, dramatic colored lighting, strong shadows, high ISO texture‚Äù 
    
   
  
 
Create the right mood with lighting control 
In a prompt, lighting sets tone, adds dimensionality, and mimics the language of photography. It shouldn‚Äôt just be ‚Äúbright vs. dark.‚Äù Lighting is often the style itself, especially for audiences like Gen Z, for instance TikTok, early-aughts flash, harsh backlight, and color gels. The following table provides some useful lighting style prompt terms. 
 
  
   
   Lighting style 
   Prompt terms 
   Example use case 
   
   
   High-contrast studio 
   hard directional light, deep shadows, controlled highlights 
   Beauty, tech, fashion with punchy visuals 
   
   
   Soft editorial 
   diffused light, soft shadows, ambient glow, overcast 
   Skincare, fashion, wellness 
   
   
   Colored gel lighting 
   blue and pink gel lighting, dramatic color shadows, rim lighting 
   Nightlife, music-adjacent fashion, youth-forward styling 
   
   
   Natural bounce 
   golden hour, soft natural light, sun flare, warm tones 
   Outdoors, lifestyle, brand-friendly minimalism 
   
  
 
Build intent with posing and framing terms 
Good posing helps products feel aspirational and digital models more dynamic. With AI, you must be intentional. Framing and pose cues help avoid stiffness, anatomical errors, and randomness. The following table provides some useful posing and framing prompt terms. 
 
  
   
   Prompt cue 
   Description 
   Tip 
   
   
   looking off camera 
   Creates candid or editorial energy 
   Useful for lookbooks or ad pages 
   
   
   hands in motion 
   Adds realism and fluidity 
   Avoids awkward, static body posture 
   
   
   seated with body turned 
   Adds depth and twist to the torso 
   Reduces symmetry, feels natural 
   
   
   shot from low angle 
   Power or status cue 
   Works well for stylized streetwear or product hero shots 
   
  
 
Example: Putting it all together 
The following example puts together what we‚Äôve discussed in this post. 
 
  
   
    Prompt ‚Äústudio portrait of a model with platinum hair in metallic cargo pants and a cropped mesh hoodie, seated with legs wide on (acrylic stairs:1.6), magenta and teal gel lighting from left and behind, dramatic contrast, shot on 50mm, streetwear editorial for Gen Z campaign‚Äù Negative prompt ‚Äúblurry, extra limbs, watermark, cartoon, distorted face missing fingers, bad anatomy‚Äù 
    
   
  
 
Let‚Äôs break down the preceding prompt. We direct the look of the subject (platinum hair, metallic clothes), specify their pose (seated wide-legged, confident, unposed), define the environment (acrylic stairs and studio setup, controlled, modern), state the lighting (mixed gel sources, bold stylization), designate the lens (50mm, portrait realism), and lastly detail the purpose (for Gen Z campaign, sets visual and cultural tone). Together, the prompt produces the desired result. 
Best practices and troubleshooting 
Prompting is rarely a one-and-done task, especially for creative use cases. Most great images come from refining an idea over multiple attempts. Consider the following methodology to iterate over your prompts: 
 
 Keep a prompt log 
 Change one variable at a time 
 Save seeds and base images 
 Use comparison grids 
 
Sometimes things go wrong‚Äîmaybe the model ignores your prompt, or the image looks messy. These issues are common and often quick to fix, and you can get sharper, cleaner, and more intentional outputs with every adjustment. The following table provides useful tips for troubleshooting your prompts. 
 
  
   
   Problem 
   Cause of issue 
   How to fix it 
   
   
   Style feels random 
   Model is confused or terms are vague 
   Clarify style, add weight, remove conflicts 
   
   
   Face gets warped 
   Over-styled or lacks facial cues 
   Add portrait of, headshot, or adjust pose or lighting 
   
   
   Image is too dark 
   Lighting not defined 
   Add softbox from left, natural light, or time of day 
   
   
   Repetitive poses 
   Same seed or static structure 
   Switch seed or change camera angle or subject action 
   
   
   Lacks realism or feels ‚ÄúAI-ish‚Äù 
   Wrong tone or artifacts 
   Add negatives like cartoon, digital texture, distorted 
   
  
 
Conclusion 
Mastering advanced prompting techniques can turn basic image generation into professional creative outputs. Stability AI Image Services in Amazon Bedrock provide precise control over visual creation and editing, helping businesses convert concepts into production-ready assets. The combination of technical expertise and creative intent can help creators achieve the precision and consistency required in professional settings. This control proves valuable across multiple applications, such as marketing campaigns, brand consistency, and product visualizations. This post demonstrated how to optimize Stability AI Image Services in Amazon Bedrock to produce high-quality imagery that aligns with your creative goals. 
To implement these techniques, access Stability AI Image Services through Amazon Bedrock or explore Stability AI‚Äôs foundation models available in Amazon SageMaker JumpStart. You can also find practical code examples in our GitHub repository. 
 
About the authors 
Maxfield Hulker is the VP of Community and Business Development at Stability AI. He is a longtime leader in the generative AI space. He has helped build creator-focused platforms like Civitai and Dream Studio. Maxfield regularly publishes guides and tutorials to make advanced AI techniques more accessible. 
Suleman Patel is a Senior Solutions Architect at Amazon Web Services (AWS), with a special focus on machine learning and modernization. Leveraging his expertise in both business and technology, Suleman helps customers design and build solutions that tackle real-world business problems. When he‚Äôs not immersed in his work, Suleman loves exploring the outdoors, taking road trips, and cooking up delicious dishes in the kitchen. 
Isha Dua is a Senior Solutions Architect based in the San Francisco Bay Area working with generative AI model providers and helping customer optimize their generative AI workloads on AWS. She helps enterprise customers grow by understanding their goals and challenges, and guides them on how they can architect their applications in a cloud-based manner while supporting resilience and scalability. She‚Äôs passionate about machine learning technologies and environmental sustainability. 
Fabio Branco is a Senior Customer Solutions Manager at Amazon Web Services (AWS) and a strategic advisor, helping customers achieve business transformation, drive innovation through generative AI and data solutions, and successfully navigate their cloud journeys. Prior to AWS, he held Product Management, Engineering, Consulting, and Technology Delivery roles across multiple Fortune 500 companies in industries, including retail and consumer goods, oil and gas, financial services, insurance, and aerospace and defense.
‚Ä¢ Monitor Amazon Bedrock batch inference using Amazon CloudWatch metrics
  As organizations scale their use of generative AI, many workloads require cost-efficient, bulk processing rather than real-time responses. Amazon Bedrock batch inference addresses this need by enabling large datasets to be processed in bulk with predictable performance‚Äîat 50% lower cost than on-demand inference. This makes it ideal for tasks such as historical data analysis, large-scale text summarization, and background processing workloads. 
In this post, we explore how to monitor and manage Amazon Bedrock batch inference jobs using Amazon CloudWatch metrics, alarms, and dashboards to optimize performance, cost, and operational efficiency. 
New features in Amazon Bedrock batch inference 
Batch inference in Amazon Bedrock is constantly evolving, and recent updates bring significant enhancements to performance, flexibility, and cost transparency: 
 
 Expanded model support ‚Äì Batch inference now supports additional model families, including Anthropic‚Äôs Claude Sonnet 4 and OpenAI OSS models. For the most up-to-date list, refer to Supported Regions and models for batch inference. 
 Performance enhancements ‚Äì Batch inference optimizations on newer Anthropic Claude and OpenAI GPT OSS models now deliver higher batch throughput as compared to previous models, helping you process large workloads more quickly. 
 Job monitoring capabilities ‚Äì You can now track how your submitted batch jobs are progressing directly in CloudWatch, without the heavy lifting of building custom monitoring solutions. This capability provides AWS account-level visibility into job progress, making it straightforward to manage large-scale workloads. 
 
Use cases for batch inference 
AWS recommends using batch inference in the following use cases: 
 
 Jobs are not time-sensitive and can tolerate minutes to hours of delay 
 Processing is periodic, such as daily or weekly summarization of large datasets (news, reports, transcripts) 
 Bulk or historical data needs to be analyzed, such as archives of call center transcripts, emails, or chat logs 
 Knowledge bases need enrichment, including generating embeddings, summaries, tags, or translations at scale 
 Content requires large-scale transformation, such as classification, sentiment analysis, or converting unstructured text into structured outputs 
 Experimentation or evaluation is needed, for example testing prompt variations or generating synthetic datasets 
 Compliance and risk checks must be run on historical content for sensitive data detection or governance 
 
Launch an Amazon Bedrock batch inference job 
You can start a batch inference job in Amazon Bedrock using the AWS Management Console, AWS SDKs, or AWS Command Line Interface (AWS CLI). For detailed instructions, see Create a batch inference job. 
To use the console, complete the following steps: 
 
 On the Amazon Bedrock console, choose Batch inference under Infer in the navigation pane. 
 Choose Create batch inference job. 
 For Job name, enter a name for your job. 
 For Model, choose the model to use. 
 For Input data, enter the location of the Amazon Simple Storage Service (Amazon S3) input bucket (JSONL format). 
 For Output data, enter the S3 location of the output bucket. 
 For Service access, select your method to authorize Amazon Bedrock. 
 Choose Create batch inference job. 
 
 
Monitor batch inference with CloudWatch metrics 
Amazon Bedrock now automatically publishes metrics for batch inference jobs under the AWS/Bedrock/Batch namespace. You can track batch workload progress at the AWS account level with the following CloudWatch metrics. For current Amazon Bedrock models, these metrics include records pending processing, input and output tokens processed per minute, and for Anthropic Claude models, they also include tokens pending processing. 
The following metrics can be monitored by modelId: 
 
 NumberOfTokensPendingProcessing ‚Äì Shows how many tokens are still waiting to be processed, helping you gauge backlog size 
 NumberOfRecordsPendingProcessing ‚Äì Tracks how many inference requests remain in the queue, giving visibility into job progress 
 NumberOfInputTokensProcessedPerMinute ‚Äì Measures how quickly input tokens are being consumed, indicating overall processing throughput 
 NumberOfOutputTokensProcessedPerMinute ‚Äì Measures generation speed 
 
To view these metrics using the CloudWatch console, complete the following steps: 
 
 On the CloudWatch console, choose Metrics in the navigation pane. 
 Filter metrics by AWS/Bedrock/Batch. 
 Select your modelId to view detailed metrics for your batch job. 
 
 
To learn more about how to use CloudWatch to monitor metrics, refer to Query your CloudWatch metrics with CloudWatch Metrics Insights. 
Best practices for monitoring and managing batch inference 
Consider the following best practices for monitoring and managing your batch inference jobs: 
 
 Cost monitoring and optimization ‚Äì By monitoring token throughput metrics (NumberOfInputTokensProcessedPerMinute and NumberOfOutputTokensProcessedPerMinute) alongside your batch job schedules, you can estimate inference costs using information on the Amazon Bedrock pricing page. This helps you understand how fast tokens are being processed, what that means for cost, and how to adjust job size or scheduling to stay within budget while still meeting throughput needs. 
 SLA and performance tracking ‚Äì The NumberOfTokensPendingProcessing metric is useful for understanding your batch backlog size and tracking overall job progress, but it should not be relied on to predict job completion times because they might vary depending on overall inference traffic to Amazon Bedrock. To understand batch processing speed, we recommend monitoring throughput metrics (NumberOfInputTokensProcessedPerMinute and NumberOfOutputTokensProcessedPerMinute) instead. If these throughput rates fall significantly below your expected baseline, you can configure automated alerts to trigger remediation steps‚Äîfor example, shifting some jobs to on-demand processing to meet your expected timelines. 
 Job completion tracking ‚Äì When the metric NumberOfRecordsPendingProcessing reaches zero, it indicates that all running batch inference jobs are complete. You can use this signal to trigger stakeholder notifications or start downstream workflows. 
 
Example of CloudWatch metrics 
In this section, we demonstrate how you can use CloudWatch metrics to set up proactive alerts and automation. 
For example, you can create a CloudWatch alarm that sends an Amazon Simple Notification Service (Amazon SNS) notification when the average NumberOfInputTokensProcessedPerMinute exceeds 1 million within a 6-hour period. This alert could prompt an Ops team review or trigger downstream data pipelines. 
 
The following screenshot shows that the alert has In alarm status because the batch inference job met the threshold. The alarm will trigger the target action, in our case an SNS notification email to the Ops team. 
 
The following screenshot shows an example of the email the Ops team received, notifying them that the number of processed tokens exceeded their threshold. 
 
You can also build a CloudWatch dashboard displaying the relevant metrics. This is ideal for centralized operational monitoring and troubleshooting. 
 
Conclusion 
Amazon Bedrock batch inference now offers expanded model support, improved performance, deeper visibility into the progress of your batch workloads, and enhanced cost monitoring. 
Get started today by launching an Amazon Bedrock batch inference job, setting up CloudWatch alarms, and building a monitoring dashboard, so you can maximize efficiency and value from your generative AI workloads. 
 
About the authors 
Vamsi Thilak Gudi&nbsp;is a Solutions Architect at Amazon Web Services (AWS) in Austin, Texas, helping Public Sector customers build effective cloud solutions. He brings diverse technical experience to show customers what‚Äôs possible with AWS technologies. He actively contributes to the AWS Technical Field Community for Generative AI. 
Yanyan Zhang&nbsp;is a Senior Generative AI Data Scientist at Amazon Web Services, where she has been working on cutting-edge AI/ML technologies as a Generative AI Specialist, helping customers use generative AI to achieve their desired outcomes. Yanyan graduated from Texas A&amp;M University with a PhD in Electrical Engineering. Outside of work, she loves traveling, working out, and exploring new things. 
Avish Khosla&nbsp;is a software developer on Bedrock‚Äôs Batch Inference team, where the team build reliable, scalable systems to run large-scale inference workloads on generative AI models. he care about clean architecture and great docs. When he is not shipping code, he is on a badminton court or glued to a good cricket match. 
Chintan Vyas serves as a Principal Product Manager‚ÄìTechnical at Amazon Web Services (AWS), where he focuses on Amazon Bedrock services. With over a decade of experience in Software Engineering and Product Management, he specializes in building and scaling large-scale, secure, and high-performance Generative AI services. In his current role, he leads the enhancement of programmatic interfaces for Amazon Bedrock. Throughout his tenure at AWS, he has successfully driven Product Management initiatives across multiple strategic services, including Service Quotas, Resource Management, Tagging, Amazon Personalize, Amazon Bedrock, and more. Outside of work, Chintan is passionate about mentoring emerging Product Managers and enjoys exploring the scenic mountain ranges of the Pacific Northwest. 
Mayank Parashar&nbsp;is a Software Development Manager for Amazon Bedrock services.
‚Ä¢ Use AWS Deep Learning Containers with Amazon SageMaker AI managed MLflow
  Organizations building custom machine learning (ML) models often have specialized requirements that standard platforms can‚Äôt accommodate. For example, healthcare companies need specific environments to protect patient data while meeting HIPAA compliance, financial institutions require specific hardware configurations to optimize proprietary trading algorithms, and research teams need flexibility to experiment with cutting-edge techniques using custom frameworks. These specialized needs drive organizations to build custom training environments that give them control over hardware selection, software versions, and security configurations. 
These custom environments provide the necessary flexibility, but they create significant challenges for ML lifecycle management. Organizations typically try to solve these problems by building additional custom tools, and some teams piece together various open source solutions. These approaches further increase operational costs and require engineering resources that could be better used elsewhere. 
AWS Deep Learning Containers (DLCs) and managed MLflow on Amazon SageMaker AI offer a powerful solution that addresses both needs. DLCs provide preconfigured Docker containers with frameworks like TensorFlow and PyTorch, including NVIDIA CUDA drivers for GPU support. DLCs are optimized for performance on AWS, regularly maintained to include the latest framework versions and patches, and designed to integrate seamlessly with AWS services for training and inference. AWS Deep Learning AMIs (DLAMIs) are preconfigured Amazon Machine Images (AMIs) for Amazon Elastic Compute Cloud (Amazon EC2) instances. DLAMIs come with popular deep learning frameworks like PyTorch and TensorFlow, and are available for CPU-based instances and high-powered GPU-accelerated instances. They include NVIDIA CUDA, cuDNN, and other necessary tools, with AWS managing the updates of DLAMIs. Together, DLAMIs and DLCs provide ML practitioners with the infrastructure and tools to accelerate deep learning in the cloud at scale. 
SageMaker managed MLflow delivers comprehensive lifecycle management with one-line automatic logging, enhanced comparison capabilities, and complete lineage tracking. As a fully managed service on SageMaker AI, it alleviates the operational burden of maintaining tracking infrastructure. 
In this post, we show how to integrate AWS DLCs with MLflow to create a solution that balances infrastructure control with robust ML governance. We walk through a functional setup that your team can use to meet your specialized requirements while significantly reducing the time and resources needed for ML lifecycle management. 
Solution overview 
In this section, we describe the architecture and AWS services used to integrate AWS DLCs with SageMaker managed MLflow to implement the solution.The solution uses several AWS services together to create a scalable environment for ML development: 
 
 AWS DLCs provide preconfigured Docker images with optimized ML frameworks 
 SageMaker managed MLflow introduces enhanced model registry capabilities with fine-grained access controls and adds generative AI support through specialized tracking for LLM experiments and prompt management 
 Amazon Elastic Container Registry (Amazon ECR) stores and manages container images 
 Amazon Simple Storage Service (Amazon S3) stores input and output artifacts 
 Amazon EC2 runs the AWS DLCs 
 
For this use case, you will develop a TensorFlow neural network model for abalone age prediction with integrated SageMaker managed MLflow tracking code. Next, you will pull an optimized TensorFlow training container from the AWS public ECR repository and configure an EC2 instance with access to the MLflow tracking server. You will then execute the training process within the DLC while storing model artifacts in Amazon S3 and logging experiment results to MLflow. Finally, you will view and compare experiment results in the MLflow UI to evaluate model performance. 
The following diagram that shows the interaction between various AWS services, AWS DLCs, and SageMaker managed MLflow for the solution. 
 
The workflow consists of the following steps: 
 
 Develop a TensorFlow neural network model for abalone age prediction. Integrate SageMaker managed MLflow tracking within the model code to log parameters, metrics, and artifacts. 
 Pull an optimized TensorFlow training container from the AWS public ECR repository. Configure Amazon EC2 and DLAMI with access to the MLflow tracking server using an AWS Identity and Access Management (IAM) role for EC2. 
 Execute the training process within the DLC running on Amazon EC2, store model artifacts in Amazon S3, and log all experiment results and register model in MLflow. 
 Compare experiment results through the MLflow UI. 
 
Prerequisites 
To follow along with this walkthrough, make sure you have the following prerequisites: 
 
 An AWS account with billing enabled. 
 An EC2 instance (t3.large or larger) running Ubuntu 20.4 or later with at least 20 GB of available disk space for Docker images and containers. 
 Docker (latest) installed on the EC2 instance. 
 The AWS Command Line Interface (AWS CLI) version 2.0 or later. 
 An IAM role with permissions for the following: 
   
   Amazon EC2 to talk to SageMaker managed MLflow. 
   Amazon ECR to pull the TensorFlow container. 
   SageMaker managed MLflow to track experiments and register models. 
    
 An Amazon SageMaker Studio domain. To create a domain, refer to Guide to getting set up with Amazon SageMaker AI. Add the sagemaker-mlflow:AccessUI permission to the SageMaker execution role created. This permission makes it possible to navigate to MLflow from the SageMaker Studio console. 
 A SageMaker managed MLflow tracking server set up in SageMaker AI. 
 Internet access from the EC2 instance to download the abalone dataset. 
 The GitHub repository cloned to your EC2 instance. 
 
Deploy the solution 
Detailed step-by-step instructions are available in the accompanying GitHub repository‚Äôs README file. The walkthrough covers the entire workflow‚Äîfrom provisioning infrastructure and setting up permissions to executing your first training job with comprehensive experiment tracking. 
Analyze experiment results 
After you‚Äôve implemented the solution following the steps in the README file, you can access and analyze your experiment results. The following screenshots demonstrate how SageMaker managed MLflow provides comprehensive experiment tracking, model governance, and auditability for your deep learning workloads. When training is complete, all experiment metrics, parameters, and artifacts are automatically captured in MLflow, providing a central location to track and compare your model development journey. The following screenshot shows the experiment abalone-tensorflow-experiment with a run named unique-cod-104. This dashboard gives you a complete overview of all experiment runs, so you can compare different approaches and model iterations at a glance. 
 
The following screenshot shows the detailed information for run unique-cod-104, including the registered model abalone-tensorflow-custom-callback-model (version v2). This view provides critical information about model provenance, showing exactly which experiment run produced which model version, which is a key component of model governance. 
 
The following visualization tracks the training loss across epochs, captured using a custom callback. Such metrics help you understand model convergence patterns and evaluate training performance, giving insights into potential optimization opportunities. 
 
The registered models view illustrated in the following screenshot shows how abalone-tensorflow-custom-callback-model is tracked in the model registry. This integration enables versioning, model lifecycle management, and deployment tracking. 
 
The following screenshot illustrates one of the solution‚Äôs most powerful governance features. When logging a model with mlflow.tensorflow.log_model() using the registered_model_name parameter, the model is automatically registered in the Amazon SageMaker Model Registry. This creates full traceability from experiment to deployed model, establishing an audit trail that connects your training runs directly to production models. 
 
This seamless integration between your custom training environments and SageMaker governance tools helps you maintain visibility and compliance throughout your ML lifecycle. 
The model artifacts are automatically uploaded to Amazon S3 after training completion, as illustrated in the following screenshot. This organized storage structure makes sure all model components including weights, configurations, and associated metadata are securely preserved and accessible through a standardized path. 
 
Cost implications 
The following resources incur costs. Refer to the respective pricing page to estimate costs. 
 
 Amazon EC2 On-Demand ‚Äì Refer to the instance size and AWS Region where it has been provisioned. Storage using Amazon Elastic Block Store (Amazon EBS) is additional. 
 SageMaker managed MLflow ‚Äì You can find the details under On-demand pricing for SageMaker MLflow for tracking and storage. 
 Amazon S3 ‚Äì Refer to pricing for storage and requests. 
 SageMaker Studio ‚Äì There is no additional charges for using the SageMaker Studio UI. However, any EFS or EBS volumes attached, jobs or resources launched from SageMaker Studio applications, or JupyterLab applications will incur costs. 
 
Clean up 
Complete the following steps to clean up your resources: 
 
 Delete the MLflow tracking server, because it continues to incur costs as long as it‚Äôs running: 
 
 
 aws sagemaker delete-mlflow-tracking-server \ 
   --tracking-server-name &lt;your-tracking-server-name&gt; 
 
 
 Stop the EC2 instance to avoid incurring additional costs: 
 
 
 aws ec2 stop-instances --instance-ids &lt;your-instance-id&gt; 
 
 
 Remove training data, model artifacts, and MLflow experiment data from S3 buckets: 
 
 
 aws s3 rm s3://&lt;your-bucket&gt;/&lt;your-MLflow-folder&gt; ‚Äìrecursive 
 
 
 Review and clean up any temporary IAM roles created for the EC2 instnce. 
 Delete your SageMaker Studio domain. 
 
Conclusion 
AWS DLCs and SageMaker managed MLflow provide ML teams a solution that balances the trade-off between governance and flexibility. This integration helps data scientists seamlessly track experiments and deploy models for inference, and helps administrators establish secure, scalable SageMaker managed MLflow environments. Organizations can now standardize their ML workflows using either AWS DLCs or DLAMIs while accommodating specialized requirements, ultimately accelerating the journey from model experimentation to business impact with greater control and efficiency. 
In this post, we explored how to integrate custom training environments with SageMaker managed MLflow to gain comprehensive experiment tracking and model governance. This approach maintains the flexibility of your preferred development environment while benefiting from centralized tracking, model registration, and lineage tracking. The integration provides a perfect balance between customization and standardization, so teams can innovate while maintaining governance best practices. 
Now that you understand how to track training in DLCs with SageMaker managed MLflow, you can implement this solution in your own environment. All code examples and implementation details from this post are available in our GitHub repository. 
For more information, refer to the following resources: 
 
 AWS Deep Learning AMIs Developer Guide 
 AWS Deep Learning Containers Developer Guide 
 Amazon SageMaker AI Developer Guide 
 Accelerate generative AI development using managed MLflow on Amazon SageMaker AI 
 
 
 
About the authors 
Gunjan Jain, an AWS Solutions Architect based in Southern California, specializes in guiding large financial services companies through their cloud transformation journeys. He expertly facilitates cloud adoption, optimization, and implementation of Well-Architected best practices. Gunjan‚Äôs professional focus extends to machine learning and cloud resilience, areas where he demonstrates particular enthusiasm. Outside of his professional commitments, he finds balance by spending time in nature. 
Rahul Easwar is a Senior Product Manager at AWS, leading managed MLflow and Partner AI Apps within the SageMaker AIOps team. With over 15 years of experience spanning startups to enterprise technology, he leverages his entrepreneurial background and MBA from Chicago Booth to build scalable ML platforms that simplify AI adoption for organizations worldwide. Connect with Rahul on&nbsp;LinkedIn to learn more about his work in ML platforms and enterprise AI solutions.

‚∏ª