âœ… Morning News Briefing â€“ July 13, 2025 10:53

ğŸ“… Date: 2025-07-13 10:53
ğŸ·ï¸ Tags: #briefing #ai #publichealth #digitalgov

â¸»

ğŸ§¾ Weather
â€¢ Current Conditions:  21.6Â°C
  Temperature: 21.6&deg;C Pressure / Tendency: 101.2 kPa rising Humidity: 95 % Humidex: 30 HumideX: 30 Dewpoint: 20.8&deg:C Wind: SSE 4 km/h Air Quality Health Index: n/a . Pembroke 6:00 AM EDT Sunday 13 July 2025 Temperature:
â€¢ Sunday: Chance of showers. High 29. POP 70%
  Mainly cloudy. 30 percent chance of showers changing to 70 percent . Risk of a thunderstorm. High 29. Humidex 36. UV index 8 or very high. Chance of rain in the morning. Risk of thunderstorm in the afternoon. Showers in the evening. Showery cloudy. Shakery. Rainy. Cloudy. Wet. Dry. Dry
â€¢ Sunday night: Chance of showers. Low 16. POP 40%
  Partly cloudy with 40 percent chance of showers early this evening . Risk of a thunderstorm early this . evening . Otherwise clear . Low 16.50/40/50/80/100/100 . Forecast issued 5:00 AM EDT Sunday 13 July 2025 . Weather forecast: Showery, thundery, rainy, windy, sunny, breezy, cloudy .

ğŸŒ International News
No updates.

ğŸ Canadian News
No updates.

ğŸ‡ºğŸ‡¸ U.S. Top Stories
â€¢ Folklife stars: Maya artist, Bolivian rappers, Dolly Parton's guitar restorer
  This year's Smithsonian Folklife Festival was held in Washington, DC . The theme: How youth keep cultural traditions alive . The event is based on the theme of preserving cultural traditions in the U.S. The festival was held at the National Museum of American Folklife in Washington DC, D.C. and Smithsonian Museum of Folklife at 8 p.m. ET on Monday .
â€¢ 59 Palestinians in Gaza are killed by Israeli airstrikes or shot dead while seeking aid
  At least 31 Palestinians were fatally shot on their way to an aid distribution site in the Gaza Strip on Saturday . Israeli airstrikes killed at least 28 Palestinians, including 28 Israeli airstrikes . At least 28 Israelis were also killed in Israeli airstrikes on Saturday, killing at least 30 Palestinians, according to reports . Israel has been accused of killing Palestinians in Gaza since the start of the Israeli-Gaza conflict .
â€¢ 'Panicking': Why recent college grads are struggling to find jobs
  Recent college graduates are facing one of the most challenging job markets in years . The overall unemployment rate remains low . Recent college grads face a challenging job market even as the overall job market is low . The job market remains low despite the low unemployment rate in the U.S., according to a recent study by the Federal Bureau of Employment and Human Rights Commission . The survey is based on
â€¢ The biggest piece of Mars on Earth is going up for auction in New York
  Sotheby's in New York will be auctioning the largest piece of Mars ever found on Earth . The auction will take place on November 11 . The largest part of Mars was found on the surface of the planet Mars and is expected to be auctioned at Sothebys' New York auction house on November 14 . The Mars-damaged piece is believed to be one of the
â€¢ Camp Mystic asked to remove buildings from government flood maps despite risk
  The data highlights critical risks in other areas along the Guadalupe River in Kerr County, and nationwide as many Americans have a flood risk they are not aware of . The data also highlights the dangers of flooding in other parts of the country . Many Americans have flood risks they are unaware of, including many who do not know they have a risk they do not need to be aware of,

ğŸ§  Artificial Intelligence
No updates.

ğŸ’» Digital Strategy
â€¢ The price of software freedom is eternal politics
  The new fork of the X.org X11 server is conservativeâ€¦ and we don't mean just technologically conservative . Many don't realize or forget, but the FOSS world has ideological wings, too . FOSS is not just technology, it's ideology, but it's also ideological, too. The X11 fork is the latest in a long line of conservative FOSS forks .
â€¢ If MCP is the USB-C of AI agents, A2A is their Ethernet
  We have protocols and standards for just about everything . It's generally helpful when we can all agree on how technologies should talk to one another . So, it was only a matter of time before the first protocols governing agentic AI started cropping up .â€¦â€¦â€¦ What good is an agent if it's unable to speak? We have no idea what agents should be able to talk to
â€¢ Looks like 1,300 Indeed and Glassdoor staffers will need their former employer's websites
  Recruit Holdings, the Japanese job site conglomerate that owns recruitment job site Indeed and employer reviewer Glassdoor, has eliminated about 1,300 positions . No reason given for the 6% cull, but CEO has previously talked up AI taking jobs . The CEO has talked up using AI to take jobs, but there is no reason for the cull . The company also owns Indeed and Glassdoor .
â€¢ AI coding tools make developers slower but they think they're faster, study finds
  Artificial intelligence coding tools are supposed to make software development faster . But a randomized, controlled trial found the opposite . Predicted a 24% boost, but clocked a 19% drag on the software development process . The tools were supposed to boost software development by 24%, but were only 19% faster than predicted by the trial . The trial was a controlled, randomized study of software development
â€¢ Hegseth signs flying memo to expand military use of cheap drones in oddball video
  US Secretary of Defense Pete Hegseth reached up to grab a memorandum hung from a third drone hovering above his head . An announcement so weird it could only come from the Trump administration video . The announcement was made by a pair of buzzing drones that threatened to drown out his voice . The video was part of a video from the US Defense Secretary's Office of the U.S.

ğŸ¥ Public Health
No updates.

ğŸ”¬ Science
â€¢ Understanding who volunteers globally through an examination of demographic variation in volunteering across 22 countries
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
â€¢ Factors associated with the acceptance of telemedicine services in Dusit model prototype area
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
â€¢ Diagnosing pathologic myopia by identifying morphologic patterns using ultra widefield images with deep learning
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
â€¢ Optimizing AI solutions for population health in primary care
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
â€¢ Healthcare professionalsâ€™ perspective on collaboration with traditional medical practitioners in HIV/AIDS and tuberculosis care in rural Ethiopia
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations

ğŸ§¾ Government & Policy
No updates.

ğŸ›ï¸ Enterprise Architecture & IT Governance
No updates.

ğŸ¤– AI & Emerging Tech
â€¢ The Download: cybersecurityâ€™s shaky alert system, and mobile IVF
  This is today&#8217;s edition of&nbsp;The Download,&nbsp;our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



Cybersecurityâ€™s global alarm system is breaking down



Every day, billions of people trust digital systems to run everything from communication to commerce to critical infrastructure. But the global early warning system that alerts security teams to dangerous software flaws is showing critical gaps in coverageâ€”and most users have no idea their digital lives are likely becoming more vulnerable.



Over the past eighteen months, two pillars of global cybersecurity have been shaken by funding issues: the US-backed National Vulnerability Database (NVD)â€”relied on globally for its free analysis of security threatsâ€”and the Common Vulnerabilities and Exposures (CVE) program, the numbering system for tracking software flaws.&nbsp;



Although the situation for both has stabilized, organizations and governments are confronting a critical weakness in our digital infrastructure: Essential global cybersecurity services depend on a complex web of US agency interests and government funding that can be cut or redirected at any time. Read the full story.&nbsp;



â€”Matthew King







The first babies have been born following â€œsimplifiedâ€ IVF in a mobile lab



This week Iâ€™m sending congratulations to two sets of new parents in South Africa. Babies Milayah and Rossouw arrived a few weeks ago. All babies are special, but these two set a new precedent. Theyâ€™re the first to be born following &#8220;simplified&#8221; IVF performed in a mobile lab.



This new mobile lab is essentially a trailer crammed with everything an embryologist needs to perform IVF on a shoestring. It was designed to deliver reproductive treatments to people who live in rural parts of low-income countries, where IVF can be prohibitively expensive or even nonexistent. And best of all: it seems to work! Read our story about why itâ€™s such an exciting development.&nbsp;



â€”Jessica HamzelouÂ 



This article first appeared in The Checkup, MIT Technology Reviewâ€™s weekly biotech newsletter. To receive it in your inbox every Thursday, sign up here.







The must-reads



Iâ€™ve combed the internet to find you todayâ€™s most fun/important/scary/fascinating stories about technology.



1 Trump is seeking huge cuts to basic scientific researchIf he gets his way, federal science funding will be slashed by a third for the next fiscal year. (NYT&nbsp;$)+&nbsp;The foundations of Americaâ€™s prosperity are being dismantled.&nbsp;(MIT Technology Review)+&nbsp;Senators are getting ready to push back against proposed NASA cuts. (Bloomberg&nbsp;$)



2 Conspiracy theorists are starting to turn on TrumpHe whipped them all up over the supposed existence of Epsteinâ€™s client list, and now theyâ€™re mad nothingâ€™s being released. (The Atlantic&nbsp;$)3 AI actually slows experienced software developers downThey end up wasting lots of time checking and correcting AI modelsâ€™ output. (Reuters&nbsp;$)4 The Pentagon is becoming the largest shareholder in a rare earth minerals companyIt shows just how much competition is hotting up to secure a steady supply of these materials. (Quartz&nbsp;$)+&nbsp;The race to produce rare earth elements. (MIT Technology Review)&nbsp;5 Solar power is starting to truly transform the worldâ€™s energy system&nbsp;Globally, roughly a third more power was generated from the sun this spring than last. (New Yorker&nbsp;$)6 Copsâ€™ favorite AI tool auto-deletes evidence of AI being used&nbsp;A pretty breathtaking attempt to avoid any sort of audit, transparency or accountability. (Ars Technica)+&nbsp;How a new type of AI is helping police skirt facial recognition bans.&nbsp;(MIT Technology Review)7 Why Chinese EV brands are being forced to go globalCompetition at home is becoming so intense that many have no choice but to seek profits elsewhere. (Rest of World)+&nbsp;Chinaâ€™s EV giants are betting big on humanoid robots.&nbsp;(MIT Technology Review)8 Which Big Tech execs are closest to the White House?&nbsp;Check out this scorecard showing how theyâ€™re all doing trying to stay in Trumpâ€™s good graces. (WSJ&nbsp;$)9 Elon Musk says Grok is coming to Tesla vehiclesYes, thatâ€™s the same Grok that keeps being racist. Shareholders must be delighted. (Insider&nbsp;$)+&nbsp;X is basically becoming a strip mine for AI training data.&nbsp;(Axios)10 Trump Mobile is charging peopleâ€™s credit cards without explanationBut Iâ€™m sure itâ€™s all perfectly explicable and above board, right? Right?! (404 Media)











Quote of the day



â€œIt has been nonstop pandemonium.â€



â€”Augustus Doricko, who founded a cloud seeding startup two years ago, tells theÂ Washington PostÂ heâ€™s received a deluge of fury online from conspiracy theorists who blame him for the catastrophic Texas floods.







One more thing



STEPHANIE ARNETT/MIT TECHNOLOGY REVIEW | LUMMI




Whatâ€™s next for AI in 2025



For the last couple of years weâ€™ve had a go at predicting whatâ€™s coming next in AI. A foolâ€™s game given how fast this industry moves. But we gave it a go anyway back in January. As we sail pass this yearâ€™s halfway mark, itâ€™s a good time to ask: how well did we do?&nbsp;Check out our predictions, and see for yourself!



â€”James O&#8217;Donnell, Will Douglas Heaven &amp; Melissa HeikkilÃ¤



This piece is part of MIT Technology Reviewâ€™s Whatâ€™s Next series, looking across industries, trends, and technologies to give you a first look at the future. You can read the rest of them&nbsp;here.







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)



+ Letâ€™s have more pop culture referencesÂ in journal article titles, please.+ Hereâ€™s someÂ inspirationÂ for things to cook this month (or, if itâ€™s hot, just assemble).+ Thereâ€™s something so relaxing about gazing atÂ these (award-winning!) landscape photos.Â + If you like birds, youâ€™ll enjoyÂ this artistâ€™s work
â€¢ Cybersecurityâ€™s global alarm system is breaking down
  Every day, billions of people trust digital systems to run everything from communication to commerce to critical infrastructure. But the global early warning system that alerts security teams to dangerous software flaws is showing critical gaps in coverageâ€”and most users have no idea their digital lives are likely becoming more vulnerable.



Over the past 18 months, two pillars of global cybersecurity have flirted with apparent collapse. In February 2024, the US-backed National Vulnerability Database (NVD)â€”relied on globally for its free analysis of security threatsâ€”abruptly stopped publishing new entries, citing a cryptic â€œchange in interagency support.â€ Then, in April of this year, the Common Vulnerabilities and Exposures (CVE) program, the fundamental numbering system for tracking software flaws, seemed at similar risk: A leaked letter warned of an imminent contract expiration.



Cybersecurity practitioners have since flooded Discord channels and LinkedIn feeds with emergency posts and memes of â€œNVDâ€ and â€œCVEâ€ engraved on tombstones. Unpatched vulnerabilities are the second most common way cyberattackers break in, and they have led to fatal hospital outages and critical infrastructure failures. In a social media post, Jen Easterly, a US cybersecurity expert, said: â€œLosing [CVE] would be like tearing out the card catalog from every library at onceâ€”leaving defenders to sort through chaos while attackers take full advantage.â€ If CVEs identify each vulnerability like a book in a card catalogue, NVD entries provide the detailed review with context around severity, scope, and exploitability.Â 



In the end, the Cybersecurity and Infrastructure Security Agency (CISA) extended funding for CVE another year, attributing the incident to a â€œcontract administration issue.â€ But the NVDâ€™s story has proved more complicated. Its parent organization, the National Institute of Standards and Technology (NIST), reportedly saw its budget cut roughly 12% in 2024, right around the time that CISA pulled its $3.7 million in annual funding for the NVD. Shortly after, as the backlog grew, CISA launched its own â€œVulnrichmentâ€ program to help address the analysis gap, while promoting a more distributed approach that allows multiple authorized partners to publish enriched data.&nbsp;



â€œCISA continuously assesses how to most effectively allocate limited resources to help organizations reduce the risk of newly disclosed vulnerabilities,â€ says Sandy Radesky, the agencyâ€™s associate director for vulnerability management. Rather than just filling the gap, she emphasizes, Vulnrichment was established to provide unique additional information, like recommended actions for specific stakeholders, and to â€œreduce dependency of the federal governmentâ€™s role to be the sole provider of vulnerability enrichment.â€



Meanwhile, NIST has scrambled to hire contractors to help clear the backlog. Despite a return to pre-crisis processing levels, a boom in vulnerabilities newly disclosed to the NVD has outpaced these efforts. Currently, over 25,000 vulnerabilities await processingâ€”nearly 10 times the previous high in 2017, according to data from the software company Anchore. Before that, the NVD largely kept pace with CVE publications, maintaining a minimal backlog.



â€œThings have been disruptive, and weâ€™ve been going through times of change across the board,â€ Matthew Scholl, then chief of the computer security division in NISTâ€™s Information Technology Laboratory, said at an industry event in April. â€œLeadership has assured me and everyone that NVD is and will continue to be a mission priority for NIST, both in resourcing and capabilities.â€ Scholl left NIST in May after 20 years at the agency, and NIST declined to comment on the backlog.&nbsp;



The situation has now prompted multiple government actions, with the Department of Commerce launching an audit of the NVD in May and House Democrats calling for a broader probe of both programs in June. But the damage to trust is already transforming geopolitics and supply chains as security teams prepare for a new era of cyber risk. â€œItâ€™s left a bad taste, and people are realizing they canâ€™t rely on this,â€ says Rose Gupta, who builds and runs enterprise vulnerability management programs. â€œEven if they get everything together tomorrow with a bigger budget, I donâ€™t know that this wonâ€™t happen again. So I have to make sure I have other controls in place.â€



As these public resources falter, organizations and governments are confronting a critical weakness in our digital infrastructure: Essential global cybersecurity services depend on a complex web of US agency interests and government funding that can be cut or redirected at any time.



Security haves and have-nots



What began as a trickle of software vulnerabilities in the early Internet era has become an unstoppable avalanche, and the free databases that have tracked them for decades have struggled to keep up. In early July, the CVE database crossed over 300,000 catalogued vulnerabilities. Numbers jump unpredictably each year, sometimes by 10% or much more. Even before its latest crisis, the NVD was notorious for delayed publication of new vulnerability analyses, often trailing private security software and vendor advisories by weeks or months.



Gupta has watched organizations increasingly adopt commercial vulnerability management (VM) software that includes its own threat intelligence services. â€œWeâ€™ve definitely become over-reliant on our VM tools,â€ she says, describing security teamsâ€™ growing dependence on vendors like Qualys, Rapid7, and Tenable to supplement or replace unreliable public databases. These platforms combine their own research with various data sources to create proprietary risk scores that help teams prioritize fixes. But not all organizations can afford to fill the NVDâ€™s gap with premium security tools. â€œSmaller companies and startups, already at a disadvantage, are going to be more at risk,â€ she explains.Â 



Komal Rawat, a security engineer in New Delhi whose mid-stage cloud startup has a limited budget, describes the impact in stark terms: â€œIf NVD goes, there will be a crisis in the market. Other databases are not that popular, and to the extent they are adopted, they are not free. If you donâ€™t have recent data, youâ€™re exposed to attackers who do.â€





The growing backlog means new devices could be more likely to have vulnerability blind spotsâ€”whether thatâ€™s a Ring doorbell at home or an office buildingâ€™s â€œsmartâ€ access control system. The biggest risk may be â€œone-offâ€ security flaws that fly under the radar. â€œThere are thousands of vulnerabilities that will not affect the majority of enterprises,â€ says Gupta. â€œThose are the ones that weâ€™re not getting analysis on, which would leave us at risk.â€



NIST acknowledges it has limited visibility into which organizations are most affected by the backlog. â€œWe donâ€™t track which industries use which products and therefore cannot measure impact to specific industries,â€ a spokesperson says. Instead, the team prioritizes vulnerabilities on the basis of CISAâ€™s known exploits list and those included in vendor advisories like Microsoft Patch Tuesday.



The biggest vulnerability



Brian Martin has watched this system evolveâ€”and deteriorateâ€”from the inside. A former CVE board member and an original project leader behind the Open Source Vulnerability Database, he has built a combative reputation over the decades as a leading historian and practitioner. Martin says his current project, VulnDB (part of Flashpoint Security), outperforms the official databases he once helped oversee. â€œOur team processes more vulnerabilities, at a much faster turnaround, and we do it for a fraction of the cost,&#8221; he says, referring to the tens of millions in government contracts that support the current system.&nbsp;



When we spoke in May, Martin said his database contains more than 112,000 vulnerabilities with no CVE identifiersâ€”security flaws that exist in the wild but remain invisible to organizations that rely solely on public channels. â€œIf you gave me the money to triple my team, that non-CVE number would be in the 500,000 range,â€ he said.





In the US, official vulnerability management duties are split between a web of contractors, agencies, and nonprofit centers like the Mitre Corporation. Critics like Martin say that creates potential for redundancy, confusion, and inefficiency, with layers of middle management and relatively few actual vulnerability experts. Others defend the value of this fragmentation. â€œThese programs build on or complement each other to create a more comprehensive, supportive, and diverse community,â€ CISA said in a statement. â€œThat increases the resilience and usefulness of the entire ecosystem.â€



As American leadership wavers, other nations are stepping up. China now operates multiple vulnerability databases, some surprisingly robust but tainted by the possibility that they are subject to state control. In May, the European Union accelerated the launch of its own database, as well as a decentralized â€œGlobal CVEâ€ architecture. Following social media and cloud services, vulnerability intelligence has become another front in the contest for technological independence.&nbsp;



That leaves security professionals to navigate multiple potentially conflicting sources of data. â€œItâ€™s going to be a mess, but I would rather have too much information than none at all,â€ says Gupta, describing how her team monitors multiple databases despite the added complexity.Â 



Resetting software liability



As defenders adapt to the fragmenting landscape, the tech industry faces another reckoning: Why donâ€™t software vendors carry more responsibility for protecting their customers from security issues? Major vendors routinely discloseâ€”but donâ€™t necessarily patchâ€”thousands of new vulnerabilities each year. A single exposure could crash critical systems or increase the risks of fraud and data misuse.&nbsp;



For decades, the industry has hidden behind legal shields. â€œShrink-wrap licensesâ€ once forced consumers to broadly waive their right to hold software vendors liable for defects. Todayâ€™s end-user license agreements (EULAs), often delivered in pop-up browser windows, have evolved into incomprehensibly long documents. Last November, a lab project called â€œEULAS of Despairâ€ used the length of War and Peace (587,287 words) to measure these sprawling contracts. The worst offender? Twitter, at 15.83 novelsâ€™ worth of fine print.



â€œThis is a legal fiction that weâ€™ve created around this whole ecosystem, and itâ€™s just not sustainable,â€ says Andrea Matwyshyn, a US special advisor and technology law professor at Penn State University, where she directs the Policy Innovation Lab of Tomorrow. â€œSome people point to the fact that software can contain a mix of products and services, creating more complex facts. But just like in engineering or financial litigation, even the most messy scenarios can be resolved with the assistance of experts.â€





This liability shield is finally beginning to crack. In July 2024, a faulty security update in CrowdStrikeâ€™s popular endpoint detection software crashed millions of Windows computers worldwide and caused outages at everything from airlines to hospitals to 911 systems. The incident led to billions in estimated damages, and the city of Portland, Oregon, even declared a â€œstate of emergency.â€ Now, affected companies like Delta Airlines have hired high-priced attorneys to pursue major damagesâ€”a signal opening of the floodgates to litigation.



Despite the soaring number of vulnerabilities, many fall into long-established categories, such as SQL injections that interfere with database queries and buffer memory overflows that enable code to be executed remotely. Matwyshyn advocates for a mandatory â€œsoftware bill of materials,â€ or S-BOMâ€”an ingredients list that would let organizations understand what components and potential vulnerabilities exist throughout their software supply chains. One recent report found 30% of data breaches stemmed from the vulnerabilities of third-party software vendors or cloud service providers.



She adds: â€œWhen you canâ€™t tell the difference between the companies that are cutting corners and a company that has really invested in doing right by their customers, that results in a market where everyone loses.â€



CISA leadership shares this sentiment, with a spokesperson emphasizing its â€œsecure-by-design principles,â€ such as â€œmaking essential security features available without additional cost, eliminating classes of vulnerabilities, and building products in a way that reduces the cybersecurity burden on customers.â€



Avoiding a digital â€˜dark ageâ€™



It will likely come as no surprise that practitioners are looking to AI to help fill the gap, while at the same time preparing for a coming swarm of cyberattacks by AI agents. Security researchers have used an OpenAI model to discover new â€œzero-dayâ€ vulnerabilities. And both the NVD and CVE teams are developing â€œAI-powered toolsâ€ to help streamline data collection, identification, and processing. NIST says that â€œup to 65% of our analysis time has been spent generating CPEsâ€â€”product information codes that pinpoint affected software. If AI can solve even part of this tedious process, it could dramatically speed up the analysis pipeline.



But Martin cautions against optimism around AI, noting that the technology remains unproven and often riddled with inaccuraciesâ€”which, in security, can be fatal. â€œRather than AI or ML [machine learning], there are ways to strategically automate bits of the processing of that vulnerability data while ensuring 99.5% accuracy,â€ he says.&nbsp;





AI also fails to address more fundamental challenges in governance. The CVE Foundation, launched in April 2025 by breakaway board members, proposes a globally funded nonprofit model similar to that of the internetâ€™s addressing system, which transitioned from US government control to international governance. Other security leaders are pushing to revitalize open-source alternatives like Googleâ€™s OSV Project or the NVD++ (maintained by VulnCheck), which are accessible to the public but currently have limited resources.



As these various reform efforts gain momentum, the world is waking up to the fact that vulnerability intelligenceâ€”like disease surveillance or aviation safetyâ€”requires sustained cooperation and public investment. Without it, a patchwork of paid databases will be all that remains, threatening to leave all but the richest organizations and nations permanently exposed.



Matthew King is a technology and environmental journalist based in New York. He previously worked for cybersecurity firm Tenable.
â€¢ The first babies have been born following â€œsimplifiedâ€ IVF in a mobile lab
  This week Iâ€™m sending congratulations to two sets of parents in South Africa. Babies Milayah and Rossouw arrived a few weeks ago. All babies are special, but these two set a new precedent. Theyâ€™re the first to be born following &#8220;simplified&#8221; IVF performed in a mobile lab.



This new mobile lab is essentially a trailer crammed with everything an embryologist needs to perform IVF on a shoestring. It was designed to deliver reproductive treatments to people who live in rural parts of low-income countries, where IVF can be prohibitively expensive or even nonexistent. And it seems to work!





While IVF is increasingly commonplace in wealthy countriesâ€”around 12% of all births in Spain result from such proceduresâ€”it remains expensive and isnâ€™t always covered by insurance or national health providers. And itâ€™s even less accessible in low-income countriesâ€”especially for people who live in rural areas.



People often assume that countries with high birth rates donâ€™t need access to fertility treatments, says Gerhard Boshoff, an embryologist at the University of Pretoria in South Africa. Sub-Saharan African countries like Niger, Angola, and Benin all have birth rates above 40 per 1,000 people, which is&nbsp;over four times the rates in Italy and Japan, for example.



But that doesnâ€™t mean people in Sub-Saharan Africa donâ€™t need IVF. Globally, around&nbsp;one in six adults experience infertility at some point in their lives, according to the World Health Organization. Research by the organization suggests that infertility rates are similar in high-income and low-income countries. As&nbsp;the WHOâ€™s director general Tedros Adhanom Ghebreyesus puts it: â€œInfertility does not discriminate.â€



For many people in rural areas of low-income countries, IVF clinics simply donâ€™t exist. South Africa is considered a â€œreproductive hubâ€ of the African continent, but even in that country there&nbsp;are fewer than 30 clinics for a population of over 60 million. A&nbsp;recent study found there were no such clinics in Angola or Malawi.&nbsp;&nbsp;



Willem Ombelet, a retired gynecologist, first noticed these disparities back in the 1980s, while he was working at an IVF lab in Pretoria. â€œI witnessed that infertility was [more prevalent] in the black population than the white populationâ€”but they couldnâ€™t access IVF because of apartheid,â€ he says. The experience spurred him to find ways to make IVF accessible for everyone. In the 1990s, he launched&nbsp;The Walking Eggâ€”a science and art project with that goal.



In 2008, Ombelet met Jonathan Van Blerkom, a reproductive biologist and embryologist who had already been experimenting with a simplified version of IVF. Typically, embryos are cultured in an incubator that provides a sterile mix of gases. Van Blerkomâ€™s approach was to preload tubes with the required gases and seal them with a rubber stopper. â€œWe donâ€™t need a fancy lab,â€ says Ombelet.



Milayah was born on June 18.COURTESY OF THE WALKING EGG




Eggs and sperm can be injected into the tubes through the stoppers, and the resulting embryos can be grown inside. All you really need is a good microscope and a way to keep the tube warm, says Ombelet. Once the embryos are around five days old, they can be transferred to a personâ€™s uterus or frozen. â€œThe cost is one tenth or one twentieth of a normal lab,â€ says Ombelet.



Ombelet, Van Blerkom, and their colleagues found that this approach appeared to work as well as regular IVF. The team ran their first pilot trial at a clinic in Belgium in 2012. The first babies conceived with the simplified IVF process&nbsp;were born later that year.





More recently, Boshoff wondered if the team could take the show on the road. Making IVF simpler and cheaper is one thing, but getting it to people who donâ€™t have access to IVF care is another. What if the team could pack the simplified IVF lab into a trailer and drive it around rural South Africa?



â€œWe just needed to figure out how to have everything in a very confined space,â€ says Boshoff. As part of the Walking Egg project, he and his colleagues found a way to organize the lab equipment and squeeze in air filters. He then designed a â€œfold-out systemâ€ that allowed the team to create a second room when the trailer was parked. This provides some privacy for people who are having embryos transferred, he says.



People who want to use the mobile IVF lab will first have to undergo treatment at a local medical facility, where they will take drugs that stimulate their ovaries to release eggs, and then have those eggs collected. The rest of the process can be done in the mobile lab, says Boshoff, who presented his work at&nbsp;the European Society of Human Reproduction and Embryologyâ€™s annual meeting in Paris earlier this month.



The first trial started last year. The team partnered with one of the few existing fertility clinics in rural South Africa, which put them in touch with 10 willing volunteers. Five of the 10 women got pregnant following their simplified IVF in the mobile lab. One miscarried, but four pregnancies continued. On June 18, baby Milayah arrived. Two days later, another mother welcomed baby Rossouw. The other babies could come any day now.



â€œWeâ€™ve proven that a very cheap and easy [IVF] method can be used even in a mobile unit and have comparable results to regular IVF,â€ says Ombelet, who says his team is planning similar trials in Egypt and Indonesia. â€œThe next step is to roll it out all over the world.â€



This article first appeared in The Checkup,&nbsp;MIT Technology Reviewâ€™s&nbsp;weekly biotech newsletter. To receive it in your inbox every Thursday, and read articles like this first,&nbsp;sign up here.
â€¢ The Download: flaws in anti-AI protections for art, and an AI regulation vibe shift
  This is today&#8217;s edition of&nbsp;The Download,&nbsp;our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



This tool strips away anti-AI protections from digital art



The news: A new technique called LightShed will make it harder for artists to use existing protective tools to stop their work from being ingested for AI training. Itâ€™s the next step in a cat-and-mouse gameâ€”across technology, law, and cultureâ€”that has been going on between artists and AI proponents for years.&nbsp;



How it works: Protective tools like Glaze and Nightshade change enough pixels to affect an image, so if itâ€™s scraped up by AI models, they see it as something itâ€™s not. LightShed essentially works by spotting just the â€œpoisonâ€ on poisoned images. To be clear, the researchers behind it arenâ€™t trying to steal artistsâ€™ work. They just donâ€™t want people to get a false sense of security. &nbsp;Read the full story.



â€”Peter Hall







Why the AI moratoriumâ€™s defeat may signal a new political era



The â€œBig, Beautiful Billâ€ that President Donald Trump signed into law on July 4 was chock full of controversial policies. But one highly contested provision was missing. Just days earlier, during a late-night voting session, the Senate had killed the billâ€™s 10-year moratorium on state-level AI regulation.&nbsp;



The bipartisan vote was seen as a victory by many, and may signal a bigger political shift, with a broader and more diverse coalition in favor of AI regulation starting to form. After years of relative inaction, politicians are getting concerned about the risks of unregulated artificial intelligence. Read the full story.&nbsp;



â€”Grace Huckins







Chinaâ€™s energy dominance in three charts



China is the dominant force in next-generation energy technologies today. Itâ€™s pouring hundreds of billions of dollars into putting renewable sources like wind and solar, manufacturing millions of electric vehicles, and building out capacity for energy storage, nuclear power, and more. This investment has been transformational for the countryâ€™s economy and has contributed to establishing China as a major player in global politics.Â 



So while we all try to get our heads around whatâ€™s next for climate tech in the US and beyond, letâ€™s look at just how dominant China is when it comes to clean energy, as documented in three charts. Read the full story.



â€”Casey Crownhart



This article is from The Spark, MIT Technology Reviewâ€™s weekly climate newsletter. To receive it in your inbox every Wednesday, sign up here.







The must-reads



Iâ€™ve combed the internet to find you todayâ€™s most fun/important/scary/fascinating stories about technology.



1 Linda Yaccarino is stepping down as CEO of XShe managed to last almost exactly two years reporting to owner Elon Musk.&nbsp; (Axios)+&nbsp;She was planning to leave before Grokâ€™s anti-Semitic rants, apparently.&nbsp;(NYT&nbsp;$)+&nbsp;Turkey has banned Grok after it insulted President ErdoÄŸan. (Politico)2 OpenAI is planning to release its own web browserIf it works out, itâ€™ll give it the same advantage as Google: direct ownership over usersâ€™ data. (Reuters&nbsp;$)+&nbsp;AI means the end of internet search as weâ€™ve known it.&nbsp;(MIT Technology Review)3 McDonaldâ€™s hiring chatbot exposed millions of applicantsâ€™ data to hackersAdding the insult of carelessness to an already pretty dystopian process! (Wired&nbsp;$)



4 AI-generated images of child sexual abuse are proliferating onlineThis is going to make an already very hard job for law enforcement even harder. (NYT&nbsp;$)5 Autonomous fighter jets are on the horizonEuropean defense start-up Helsing just completed two successful test flights. (FT&nbsp;$)+&nbsp;Generative AI is learning to spy for the US military. (MIT Technology Review)6 What happened to all the human bird flu cases?Since February, the CDC has not recorded a single new case in the US. (Undark)7 An interstellar object is cruising through the solar systemAnd itâ€™s giving astronomers a chance to test out early theories of interstellar-object-ology (yes, thatâ€™s what itâ€™s called!) (The Economist&nbsp;$)+&nbsp;Inside the most dangerous asteroid hunt ever.&nbsp;(MIT Technology Review)8 Apple is planning its first upgrade to its Vision Pro headsetBut no matter what upgrades itâ€™s got, itâ€™s going to be a real struggle to revive its flagging fortunes. (Bloomberg&nbsp;$)9 Where have all the mundane social media posts gone?Normies used to be what made social media good. We miss them and their photos of their breakfasts. (New Yorker&nbsp;$)+&nbsp;Itâ€™s heartening to see that â€˜missed connectionâ€™ posts are making a comeback, though. (The Guardian)10 A global shortage is turning MatchaTok sourBut itâ€™s pretty easy to explain why itâ€™s in short supply: the whole worldâ€™s started going mad for it. (WSJ&nbsp;$)







Quote of the day



Â â€œYouâ€™ll be hard pressed to find someone that really believes in our AI mission. To most, itâ€™s not even clear what our mission is.â€



â€”Tijmen Blankevoort, an AI researcher at Meta, explains why he thinks expensive hires alone might not cure the companyâ€™s woes, The Information reports.







One more thing



MIKE MCQUADE




The race to save our online lives from a digital dark ageThere is a photo of my daughter that I love. She is sitting, smiling, in our old back garden, chubby hands grabbing at the cool grass. It was taken on a digital camera in 2013, when she was almost one, but now lives on Google Photos.But what if, one day, Google ceased to function? What if I lost my treasured photos forever? For many archivists, alarm bells are ringing. Across the world, they are scraping up defunct websites or at-risk data collections to save as much of our digital lives as possible. Others are working on ways to store that data in formats that will last hundreds, perhaps even thousands, of years.The endeavor raises complex questions. What is important to us? How and why do we decide what to keepâ€”and what do we let go? And how will future generations make sense of what weâ€™re able to save? Read the full story.



â€”Niall Firth







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)



+ Why Hollywood is so hell-bent on making sequels.+ I love this sweet little town building program.+ What makes Severanceâ€™s opening credits so darn good?+ This ranking of HBOâ€™s finest shows is fun.
â€¢ Chinaâ€™s energy dominance in three charts
  China is the dominant force in next-generation energy technologies today . Itâ€™s pouring billions of dollars into renewable sources like wind and solar on its grid, manufacturing millions of electric vehicles, and building out capacity for energy storage, nuclear power, and more . In the US, a massive new tax and spending bill just cut hundreds of billions in credits, grants, and loans for clean energy technologies . The question is, will that change anytime soon?

ğŸ”’ Cybersecurity & Privacy
â€¢ UK Arrests Four in â€˜Scattered Spiderâ€™ Ransom Group
  Authorities in the United Kingdom this week arrested four people aged 17 to 20 in connection with recent data theft and extortion attacks against the retailers Marks &amp; Spencer and Harrods, and the British food retailer Co-op Group. The breaches have been linked to a prolific but loosely-affiliated cybercrime group dubbed &#8220;Scattered Spider,&#8221; whose other recent victims include multiple airlines.
The U.K.&#8217;s National Crime Agency (NCA) declined verify the names of those arrested, saying only that they included two males aged 19, another aged 17, and 20-year-old female.
Scattered Spider is the name given to an English-speaking cybercrime group known for using social engineering tactics to break into companies and steal data for ransom, often impersonating employees or contractors to deceive IT help desks into granting access. The FBI warned last month that Scattered Spider had recently shifted to targeting companies in the retail and airline sectors.
KrebsOnSecurity has learned the identities of two of the suspects. Multiple sources close to the investigation said those arrested include Owen David Flowers, a U.K. man alleged to have been involved in the cyber intrusion and ransomware attack that shut down several MGM Casino properties in September 2023. Those same sources said the woman arrested is or recently was in a relationship with Flowers.
Sources told KrebsOnSecurity that Flowers, who allegedly went by the hacker handles &#8220;bo764,&#8221; &#8220;Holy,&#8221; and &#8220;Nazi,&#8221; was the group member who anonymously gave interviews to the media in the days after the MGM hack. His real name was omitted from a September 2024 story about the group because he was not yet charged in that incident.
The bigger fish arrested this week is 19-year-old Thalha Jubair,Â a U.K. man whose alleged exploits under various monikers have been well-documented in stories on this site. Jubair is believed to have used the nickname &#8220;Earth2Star,&#8221; which corresponds to a founding member of the cybercrime-focused Telegram channel &#8220;Star Fraud Chat.&#8221;
In 2023, KrebsOnSecurity published an investigation into the work of three different SIM-swapping groups that phished credentials from T-Mobile employees and used that access to offer a service whereby any T-Mobile phone number could be swapped to a new device. Star Chat was by far the most active and consequential of the three SIM-swapping groups, who collectively broke into T-Mobile&#8217;s network more than 100 times in the second half of 2022.
Jubair allegedly used the handles &#8220;Earth2Star&#8221; and &#8220;Star Ace,&#8221; and was a core member of a prolific SIM-swapping group operating in 2022. Star Ace posted this image to the Star Fraud chat channel on Telegram, and it lists various prices for SIM-swaps.
Sources tell KrebsOnSecurity that Jubair also was a core member of the LAPSUS$ cybercrime group that broke into dozens of technology companies in 2022, stealing source code and other internal data from tech giants including Microsoft, Nvidia, Okta, Rockstar Games, Samsung, T-Mobile, and Uber.
In April 2022, KrebsOnSecurity published internal chat records from LAPSUS$, and those chats indicated Jubair was using the nicknames Amtrak and Asyntax. At one point in the chats, Amtrak told the LAPSUS$ group leader not to share T-Mobile&#8217;s logo in images sent to the group because he&#8217;d been previously busted for SIM-swapping and his parents would suspect he was back at it again.
As shown in those chats, the leader of LAPSUS$ eventually decided to betray Amtrak by posting his real name, phone number, and other hacker handles into a public chat room on Telegram.
In March 2022, the leader of the LAPSUS$ data extortion group exposed Thalha Jubair&#8217;s name and hacker handles in a public chat room on Telegram.
That story about the leaked LAPSUS$ chats connected Amtrak/Asyntax/Jubair to the identity &#8220;Everlynn,&#8221; the founder of a cybercriminal service that sold fraudulent &#8220;emergency data requests&#8221; targeting the major social media and email providers. In such schemes, the hackers compromise email accounts tied to police departments and government agencies, and then send unauthorized demands for subscriber data while claiming the information being requested canâ€™t wait for a court order because it relates to an urgent matter of life and death.
The roster of the now-defunct &#8220;Infinity Recursion&#8221; hacking team, from which some member of LAPSUS$ hail.
Sources say Jubair also used the nickname &#8220;Operator,&#8221; and that until recently he was the administrator of the Doxbin, a long-running and highly toxic online community that is used to â€œdoxâ€ or post deeply personal information on people. In May 2024, several popular cybercrime channels on Telegram ridiculed Operator after it was revealed that he&#8217;d staged his own kidnapping in a botched plan to throw off law enforcement investigators.
In November 2024, U.S. authorities charged five men aged 20 to 25 in connection with the Scattered Spider group, which has long relied on recruiting minors to carry out its most risky activities. Indeed, many of the group&#8217;s core members were recruited from online gaming platforms like Roblox and Minecraft in their early teens, and have been perfecting their social engineering tactics for years.
&#8220;There is a clear pattern that some of the most depraved threat actors first joined cybercrime gangs at an exceptionally young age,&#8221; said Allison Nixon, chief research officer at the New York based security firm Unit 221B. &#8220;Cybercriminals arrested at 15 or younger need serious intervention and monitoring to prevent a years long massive escalation.&#8221;
â€¢ Microsoft Patch Tuesday, July 2025 Edition
  Microsoft releases updates to fix at least 137 security vulnerabilities in its Windows operating systems and supported software . None of the weaknesses addressed this month are known to be actively exploited, but 14 of the flaws earned Microsoftâ€™s most-dire &#8217;s most dire . The flaws could be exploited to seize control over vulnerable Windows PCs with little or no help from users . Microsoft also patched at least four critical, remote code execution flaws in Office .

ğŸ“ University AI
No updates.

ğŸ¢ Corporate AI
â€¢ How AI will accelerate biomedical research and discovery
  In November 2022, OpenAIâ€™s ChatGPT kick-started a new era in AI. This was followed less than a half year later by the release of GPT-4. In the months leading up to GPT-4&#8217;s public release, Peter Lee, president of Microsoft Research, cowrote a book full of optimism for the potential of advanced AI models to transform the world of healthcare. What has happened since? In this special podcast series, The AI Revolution in Medicine, Revisited, Lee revisits the book, exploring how patients, providers, and other medical professionals are experiencing and using generative AI today while examining what he and his coauthors got rightâ€”and what they didnâ€™t foresee.



In this episode, Daphne Koller (opens in new tab), Noubar Afeyan (opens in new tab), and Dr. Eric Topol (opens in new tab), leaders in AI-driven medicine, join Lee to explore the rapidly evolving role of AI across the biomedical and healthcare landscape. Koller, founder and CEO of Insitro, shares how machine learning is transforming drug discovery, especially target identification for complex diseases like ALS, by uncovering biological patterns across massive datasets. Afeyan, founder and CEO of Flagship Pioneering and co-founder and chairman of Moderna, discusses how AI is being applied across biotech research and development, from protein design to autonomous science platforms. Topol, executive vice president of Scripps Research and founder and director of the Scripps Research Translational Institute, highlights how AI can today help mitigate and prevent the core diseases that erode our health and the possibility of realizing a virtual cell. Through his conversations with the three, Lee investigates how AI is reshaping the discovery, deployment, and delivery of medicine.Â 








Learn more:




How Machine Learning Is Revolutionising Drug Discovery (opens in new tab) (Koller)&nbsp;WIRED Health talk | March 2025



Insitro and Lilly Enter Strategic Agreements to Advance Novel Treatments for Metabolic Diseases (opens in new tab)&nbsp;(Koller)&nbsp;Insitro release | October 2024&nbsp;



2025 Annual Letter: Polyintelligence (opens in new tab) (Afeyan)&nbsp;Flagship Pioneering | 2025



The ultimate in mind extenders (opens in new tab) (Afeyan)&nbsp;McGill Daily article (college newspaper) | October 1982&nbsp;



Super Agers: An Evidence-Based Approach to Longevity (opens in new tab) (Topol)&nbsp;Book | May 2025&nbsp;



Deep Medicine: How Artificial Intelligence Can Make Healthcare Human Again (opens in new tab) (Topol)&nbsp;Book | March 2019&nbsp;



The AI Revolution in Medicine: GPT-4 and BeyondBook | Peter Lee, Carey Goldberg, Isaac Kohane | April 2023










	
		
			Subscribe to the Microsoft Research Podcast:		
		
							
					
						  
						Apple Podcasts
					
				
			
							
					
						
						Email
					
				
			
							
					
						
						Android
					
				
			
							
					
						
						Spotify
					
				
			
							
					
						
						RSS Feed
					
				
					
	




	
		
			
				
					

Transcript&nbsp;



[MUSIC]



[BOOK PASSAGE]&nbsp;



PETER LEE: â€œCan GPT-4 indeed accelerate the progression of medicine â€¦ ? It seems like a tall order, but if I had been told six months ago that it could rapidly summarize any published paper, that alone would have satisfied me as a strong contribution to research productivity. â€¦ But now that I&#8217;ve seen what GPT-4 can do with the healthcare process, I expect a lot more in the realm of research.â€&nbsp;



[END OF BOOK PASSAGE]



[THEME MUSIC]



This is The AI Revolution in Medicine, Revisited. Iâ€™m your host, Peter Lee.



Shortly after OpenAI&#8217;s GPT-4 was publicly released, Carey Goldberg, Dr. Zak Kohane, and I published The AI Revolution in Medicine to help educate the world of healthcare and medical research about the transformative impact this new generative AI technology could have. But because we wrote the book when GPT-4 was still a secret, we had to speculate. Now, two years later, what did we get right, and what did we get wrong?



In this series, weâ€™ll talk to clinicians, patients, hospital administrators, and others to understand the reality of AI in the field and where we go from here.



[THEME MUSIC FADES]



The book passage I read at the top was from â€œChapter 8: Smarter Science,â€ which was written by Zak.



In writing the book, we were optimistic about AIâ€™s potential to accelerate biomedical research and help get new and much-needed treatments and drugs to patients sooner. One area we explored was generative AI as a designer of clinical trials. We looked at generative AIâ€™s adeptness at summarizing helping speed up pre-trial triage and research. We even went so far as to predict the arrival of a large language model that can serve as a central intellectual tool.&nbsp;



For a look at how AI is impacting biomedical research today, Iâ€™m excited to welcome Daphne Koller, Noubar Afeyan, and Eric Topol.&nbsp;



				
				
					



Daphne Koller is the CEO and founder of Insitro, a machine learning-driven drug discovery and development company that recently made news for its identification of a novel drug target for ALS and its collaboration with Eli Lilly to license Lilly&#8217;s biochemical delivery systems. Prior to founding Insitro, Daphne was the co-founder, co-CEO, and president of the online education platform Coursera.



Noubar Afeyan is the founder and CEO of Flagship Pioneering, which creates biotechnology companies focused on transforming human health and environmental sustainability. He is also co-founder and chairman of the messenger RNA company Moderna. An entrepreneur and biochemical engineer, Noubar has numerous patents to his name and has co-founded many startups in science and technology.



Dr. Eric Topol is the executive vice president of the biomedical research non-profit Scripps Research, where he founded and now directs the Scripps Research Translational Institute. One of the most cited researchers in medicine, Eric has focused on promoting human health and individualized medicine through the use of genomic and digital data and AI.&nbsp;



These three are likely to have an outsized influence on how drugs and new medical technologies soon will be developed.



[TRANSITION MUSIC]&nbsp;



Hereâ€™s my interview with Daphne Koller:



LEE: Daphne, I&#8217;m just thrilled to have you join us.&nbsp;



DAPHNE KOLLER: Thank you for having me, Peter. It&#8217;s a pleasure to be here.&nbsp;



LEE: Well, you know, you&#8217;re quite well-known across several fields. But maybe for some audience members of this podcast, they might not have encountered you before. So where I&#8217;d like to start is a question I&#8217;ve been asking all of our guests.



How would you describe what you do? And the way I kind of put it is, you know, how do you explain to someone like your parents what you do for a living?&nbsp;



KOLLER: So that answer obviously has shifted over the years.



What I would say now is that we are working to leverage the incredible convergence of very powerful technologies, of which AI is one but not the only one, to change the way in which we discover and develop new treatments for diseases for which patients are currently suffering and even dying.&nbsp;



LEE: You know, I think I&#8217;ve known you for a long time.&nbsp;



KOLLER: Longer than I think either of us care to admit.&nbsp;



LEE: [LAUGHS] In fact, I think I remember you even when you were still a graduate student. But of course, I knew you best when you took up your professorship at Stanford. And I always, in my mind, think of you as a computer scientist and a machine learning person. And in fact, you really made a big name for yourself in computer science research in machine learning.



But now you&#8217;re, you know, leading one of the most important biotech companies on the planet. How did that happen?



KOLLER: So people often think that this is a recent transition. That is, after I left Coursera, I looked around and said, â€œHmm. What should I do next? Oh, biotech seems like a good thing,â€ but that&#8217;s actually not the way it transpired.



This goes all the way back to my early days at Stanford, where, in fact, I was, you know, as a young faculty member in machine learning, because I was the first machine learning hire into Stanford&#8217;s computer science department, I was looking for really exciting places in which this technology could be deployed, and applications back then, because of scarcity of data, were just not that inspiring.



And so I looked around, and this was around the late â€™90s, and realized that there was interesting data emerging in biology and medicine. My first application actually was in, interestingly, in epidemiologyâ€”patient tracking and tuberculosis. You know, you can think of it as a tiny microcosm of the very sophisticated models that COVID then enabled in a much later stage.



LEE: Right.&nbsp;



KOLLER: And so initially, this was based almost entirely on just technical interest. It&#8217;s kind of like, oh, this is more interesting as a question to tackle than spam filtering. But then I became interested in biology in its own right, biology and medicine, and ended up having a bifurcated existence as a Stanford professor where half my lab continued to do core computer science research published in, you know, NeurIPS and ICML. And the other half actually did biomedical research that was published in, you know, Nature Cell [and] Science. So that was back in, you know, the early, early 2000s, and for most of my Stanford career, I continued to have both interests.



And then the Coursera experience kind of took me out of Stanford and put me in an industry setting for the first time in my life actually. But then when my time at Coursera came to an end, you know, I&#8217;d been there for five years. And if you look at the timeline, I left Stanford in early 2012, right as the machine learning revolution was starting. So I missed the beginning.



And it was only in like 2016 or so that, as I picked my head up over the trenches, like, â€œOh my goodness, this technology is going to change the world.â€ And I wanted to deploy that big thing towards places where it would have beneficial impact on the world, like to make the world a better place.



LEE:â€¯Yeah.â€¯



KOLLER: And so I decided that one of the areas where I could make a unique, differentiated impact was in really bringing AI and machine learning to the life sciences, having spent, you know, the majority of my career at the boundary of those two disciplines. And notice I say â€œboundaryâ€ with deliberation because there wasn&#8217;t very much of an intersection.



LEE: Right.&nbsp;



KOLLER: I felt like I could do something that was unique.&nbsp;



LEE: So just to stick on you for a little bit longer, you know, we have been sort of getting into your origin story about what we call AI todayâ€”but machine learning, so deep learning.&nbsp;



And, you know, there has always been a kind of an emotional response for people like you and me and now the general public about their first encounters with what we now call generative AI. Iâ€™d love to hear what your first encounter was with generative AI and how you reacted to this.&nbsp;



KOLLER: I think my first encounter was actually an indirect one. Because, you know, the earlier generations of generative AI didnâ€™t directly touch our work at Insitro (opens in new tab).&nbsp;



And yet at the same time, I had always had an interest in computer vision. That was a large part of my non-bio work when I was at Stanford.&nbsp;



And so some of my earlier even presentations, when I was trying to convey to people back in 2016 how this technology was going to transform the world, I was talking about the incredible progress in image recognition that had happened up until that point.&nbsp;



So my first interaction was actually in the generative AI for images, where you are able to go the other way â€¦&nbsp;



LEE: Yes.&nbsp;



KOLLER: â€¦ where you can take a verbal description of an image and createâ€”and this was back in the days when the images weren&#8217;t particularly photorealistic, but still a natural language description to an image was magic given that only two or three years before that, we were barely able to look at an image and write a short phrase saying, â€œThis is a dog on the beach.â€ And so that arc, that hockey curve, was just mind blowing to me.&nbsp;



LEE: Did you have moments of skepticism?&nbsp;



KOLLER: Yeah, I mean the early, you know, early versions of ChatGPT, where it was more like parlor tricks and poking it a little bit revealed all of the easy ways that one could break it and make it do really stupid things. I was like, yeah, OK, this is kind of cute, but is it going to actually make a difference? Is it going to solve a problem that matters?&nbsp;



And I mean, obviously, I think now everyone agrees that the answer is yes, although there are still people who are like, yeah, but maybe it&#8217;s around the edges. I&#8217;m not among them, by the way, but &#8230; yeah, so initially there were like, â€œYeah, this is cute and very impressive, but is it going to make a difference to a problem that matters?â€&nbsp;



LEE: Yeah. So now, maybe this is a good time to get into what you&#8217;ve been doing with ALS [amyotrophic lateral sclerosis]. You know, there&#8217;s a knee-jerk reaction from the technology side to focus on designing small molecules, on predicting, you know, their properties, you know, maybe binding affinity or aspects of ADME [absorption, distribution, metabolism, and excretion], you know, like absorption or dispersion or whatever.&nbsp;



And all of that is very useful, but if I understand the work on ALS, you went to a much harder place, which is to actually identify and select targets.&nbsp;



KOLLER: Thatâ€™s right.&nbsp;



LEE: So first off, just for the benefit of the standard listeners of this podcast, explain what that problem is in general.&nbsp;



KOLLER: No, for sure. And I think maybe I&#8217;ll start by just very quickly talking about the drug discovery and development arc, &#8230;



LEE: Yeah.



KOLLER: &#8230; which, by and large, consists of three main phases. That&#8217;s the standard taxonomy.&nbsp;The first is what&#8217;s called sometimes target discovery or identifying a therapeutic hypothesis, which looks like: if I modulate this target in this disease, something beneficial will happen.&nbsp;



Then, you have to take that target and turn it into a molecule that you can actually put into a person. It could be a small molecule. It could be a large molecule like an antibody, whatever. And then you have that construct, that molecule. And the last piece is you put it into a person in the context of a clinical trial, and you measure what has happened. And there&#8217;s been AI deployed towards each of those three stages in different ways.&nbsp;



The last one is mostly like an efficiency gain. You know, the trial is kind of already defined, and you want to deploy technology to make it more efficient and effective, which is great because those are expensive operations.&nbsp;



LEE: Yep.&nbsp;



KOLLER: The middle one is where I would say the vast majority of efforts so far has been deployed in AI because it is a nice, well-defined problem. It doesn&#8217;t mean it&#8217;s easy, but it&#8217;s one where you can define the problem. It is, I need to inhibit this protein by this amount, and the molecule needs to be soluble and whatever and go past the blood-brain barrier. And you know probably within a year and a half or so, or two, if you succeeded or not.&nbsp;



The first stage is the one where I would say the least amount of energy has gone because when you&#8217;re uncovering a novel target in the context of an indication, you don&#8217;t know that you&#8217;ve been successful until you go all the way to the end, which is the clinical trial, which is what makes this a long and risky journey. And not a lot of people have the appetite or the capital to actually do that.&nbsp;



However, in my opinion, and that of, I think, quite a number of others, it is where the biggest impact can be made. And the reason is that while pharma has its deficiencies, making good molecules is actually something they&#8217;re pretty good at.&nbsp;



It might take them longer than it should, maybe it&#8217;s not as efficient as it could be, but at the end of the day, if you tell them to drug A target, pharma is actually pretty good at generating those molecules. However, when you put those molecules into the clinic, 90% of them fail. And the reason they fail is not by and large because the molecule wasn&#8217;t good. In the majority of cases, it&#8217;s because the target you went after didn&#8217;t do anything useful in the context of the patient population in which you put it.&nbsp;



And so in order to fix the inefficiency of this industry, which is incredible inefficiency, you need to address the problem at the root, and the root is picking the right targets to go after. And so that is what we elected to do.&nbsp;



It doesn&#8217;t mean we don&#8217;t make molecules. I mean, of course, you can&#8217;t just end up with a target because a target is not actionable. You need to turn it into a molecule. And we absolutely do that. And by the way, the partnership with Lilly (opens in new tab) is actually one where they help us make a molecule.&nbsp;



LEE: Yes.&nbsp;



KOLLER: I mean, it&#8217;s our target. It&#8217;s our program. But Lilly is deploying its very state-of-the-art molecule-making capabilities to help us turn that target into a drug.&nbsp;



LEE: So let&#8217;s get now into the machine learning of this. Again, this just strikes me as such a difficult problem to solve.&nbsp;



KOLLER: Yeah.&nbsp;



LEE: So how does machine learning &#8230; how does AI help you?&nbsp;



KOLLER: So I think when you look at how people currently select targets, it&#8217;s a combination of oftentimes at this point, with an increasing respect for the power of human genetics, some search for a genetic association, oftentimes with a human-defined, highly subjective, highly noisy clinical outcome, like some ICD [International Classification of Diseases] code.&nbsp;



And those are often underpowered and very difficult to deconvolute the underlying biology. You combine that with some mechanistic interrogation in a highly reductionist model system looking at a small number of readouts, biochemical readouts, that a biologist thinks are relevant to the disease. Like does this make this, whatever, cholesterol go up or amyloid beta go down? Or whatever. And then you take that as the second stage, and you pick, based on typically human intuition about, Oh, this one looks good to me, and then you take that forward.&nbsp;



What we&#8217;re doing is an attempt to be as unbiased and holistic as possible. So, first of all, rather than rely on human-defined clinical endpoints, like this person has been diagnosed with diabetes or fatty liver, we try and measure as much as we can a holistic physiological state and then use machine learning to find structure, patterns in that human physiological readouts, imaging readouts, and omics readouts from blood, from tissue, different kinds of imaging, and say, these are different vectors that this disease takes, this group of individuals, and here&#8217;s a different group of individuals that maybe from a diagnostical perspective are all called the same thing, but they are actually exhibiting a very different biology underlying it.&nbsp;



And so that is something that doesn&#8217;t emerge when a human being takes a reductionist view to looking at this high-content data, and oftentimes, they don&#8217;t even look at it and produce an ICD code.&nbsp;



LEE: Right. Yep.&nbsp;



KOLLER: The same approach, actually even the same code base, is taken in the cellular data. So we don&#8217;t just say, â€œWell, the thing that matters is, you know, the total amount of lipid in the cell or whatever.â€ Rather, we say, â€œLet&#8217;s look at multiple readouts, multiple ways of looking at the cells, combine them using the power of machine learning.â€ And again, looking at imaging readouts where a human&#8217;s eyes just glaze over looking at even a few dozen cells, far less a few hundreds of millions of cells, and understand what are the different biological processes that are going on. What are the vectors that the disease might take you in this direction, in this group of cells, or in that direction?&nbsp;



And then importantly, we take all of that information from the human side, from the cellular side, across these different readouts, and we combine them using an integrative approach that looks at the combined weight of evidence and says, these are the targets that I have the greatest amount of conviction about by looking across all of that information. Whereas we know, and we know this, I&#8217;m sure you&#8217;ve seen this analysis done for clinicians, a human being typically is able to keep three or four things in their head at the same time.&nbsp;



LEE: Right.&nbsp;



KOLLER: A really good human being who&#8217;s really expert at what they do can maybe get to six to eight.&nbsp;



LEE: Yeah.&nbsp;



KOLLER: The machine learning has no problem doing a few hundred.&nbsp;



LEE: Right.&nbsp;



KOLLER: And so you put that together, and that allows you, to your earlier question, really select the targets around which you have the highest conviction. And then those are the ones that we then prioritize for interrogation in more expensive systems like mice and monkeys and then at the end of the day pick the small handful that one can afford to actually take into clinical trials.&nbsp;



LEE: So now, Insitro recently received $25 million in milestone payments from Bristol Myers Squibb (opens in new tab) after discovering and selecting a novel drug target for ALS. Can you tell us a little bit more about that?â€¯



KOLLER: We are incredibly excited about the first novel target, and there is a couple of others just behind it in line that seem, you know, quite efficacious, as well, that truly seem to reverse, albeit in a cellular system, what we now understand to be ALS pathology across multiple different dimensions. There&#8217;s been obviously many attempts made to try and address ALS, which by the way, horrible, horrible disease, worse than most cancers. It kills you almost inevitably in three to five years in a particularly horrific way.&nbsp;



And what we have in our hands is a target that seems to revert a lot of the pathologies that are associated with the disease, which we now understand has to do with the mis-splicing of multiple proteins within the cell and creating defective versions of those proteins that are just not operational. And we are seeing reversion of many of those.&nbsp;



So can I tell you for sure it&#8217;ll work in a human? No, there&#8217;s many steps between now and then. But we couldn&#8217;t be more excited about the opportunity to provide what we hope will be a disease-modifying intervention for these patients who really desperately need something.&nbsp;



LEE: Well, it&#8217;s certainly been making waves in the biotech and biomedical world.&nbsp;



KOLLER: Thank you.&nbsp;



LEE: So we&#8217;ll be really watching very closely.&nbsp;



So, you know, I think just reflecting on, you know, what we missed and what we got right in our book, I think in our book, we did have the insight that there would be an ability to connect, say, genotypic and phenotypic data and, you know, just broadly the kinds of clinical measurements that get made on real patients and that these things could be brought together. And I think the work that you&#8217;re doing really illustrates that in a very, very sophisticated, very ambitious way.&nbsp;



But the fact that this could be connected all the way down to the biology, to the biochemistry, I think we didn&#8217;t have any clue what would happen, at least not this quickly.&nbsp;



KOLLER: Well, I think the &#8230;&nbsp;



LEE: And I realize, you&#8217;ve been at this for quite a few years, but still, it&#8217;s quite amazing.&nbsp;



KOLLER: The thread that connects them is human genetics. And I think that has, to us, been, sort of, the, kind of, the connective tissue that allows you to translate across different systems and say, â€œWhat does this gene do? What does this gene do in this organ and in that organ? What does it do in this type of cell and in that type of cell?â€&nbsp;



And then use that as sort of the thread, if you will, that follows the impact of modulating this gene all the way from the simple systems where you can do the experiment to the complex systems where you can&#8217;t do the experiment until the very end, but you have the human genetics as a way of looking at the statistics and understanding what the impact might be.&nbsp;



LEE: So I&#8217;d like to now switch gears and take â€¦ I want to take two steps in the remainder of this conversation towards the future. So one step into that future, of course, we&#8217;re living through now, which is just all of the crazy pace of work and advancement in generative AI generally, you know, just the scale of transformers, of post-training, and now inference scale and reasoning models and so on. And where do you see all of that going with respect to the goals that you have and that Insitro has?&nbsp;



KOLLER: So I think first and foremost is the parallel, if you will, to the predictions that you focused on in your book, which is this will transform a lot of the core data processing tasks, the information tasks. And sure, the doctors and nurses is one thing. But if you just think of clinical trial operations or the submission of regulatory documents, these are all kind of simple data â€¦ they&#8217;re not simple, obviously, but they&#8217;re data processing tasks. They involve natural language. That&#8217;s not going to be our focus, but I hope that others will use that to make clinical trials faster, more efficient, less expensive.&nbsp;



There&#8217;s already a lot of progress that&#8217;s happening on the molecular design side of things and taking hypotheses and turning them quickly and effectively into molecules. As I said, this is part of our work that we absolutely do and we don&#8217;t talk about it very much, simply because it&#8217;s a very crowded landscape and a lot of companies are engaged on that. But I think it&#8217;s really important to be able to take biological insights and turn them into new molecules.&nbsp;



And then, of course, the transformer models and their likes play a very significant role in that sort of turning insights into molecules because you can have foundation models for proteins. There are increasing efforts to create foundation models for other categories of molecules. And so that will undoubtedly accelerate the process by which you can quickly generate different molecular hypotheses and test them and learn from what you did so that you can do fewer iterations â€¦&nbsp;



LEE: Right.&nbsp;



KOLLER: â€¦ before you converge on a successful molecule.&nbsp;



I do think that arguably the biggest impact as yet to be had is in that understanding of core human biology and what are the right ways to intervene in it. And that plays a role in a couple different ways. First of all, it certainly plays a role in which â€¦ if we are able to understand the human physiological state and, you know, the state of different systems all the way down to the cell level, that will inform our ability to pick hypotheses that are more likely to actually impact the right biologies underneath.&nbsp;



LEE: Yep. Yeah.&nbsp;



KOLLER: And the more data we&#8217;re able to collect about humans and about cells, the more successful our models will be at representing that human physiological state or the cell biological state and making predictions reliably on the impact of these interventions.&nbsp;



The other side of it, though, and this comes back, I think, to themes that were very much in your book, is this will impact not only the early stages of which hypotheses we interrogate, which molecules we move forward, but also hopefully at the end of the day, which molecule we prescribe to which patient.&nbsp;



LEE: Right.&nbsp;



KOLLER: And I think there&#8217;s been obviously so much narrative over the years about precision medicine, personalized medicine, and very little of that has come to fruition, with the exception of, you know, certain islands in oncology, primarily on genetically driven cancers.&nbsp;



But I think the opportunity is still there. We just haven&#8217;t been able to bring it to life because of the lack of the right kind of data. And I think with the increasing amount of human, kind of, foundational data that we&#8217;re able to acquire, things that are not sort of distilled through the eye of a clinician, for example, â€¦&nbsp;



LEE: Yes.&nbsp;



KOLLER: â€¦ but really measurements of human pathology, we can start to get to some of that precision, carving out of the human population and then get to a world where we can prescribe the right medicine to the right patient and not only in cancer but also in other diseases that are also not a single disease.&nbsp;



LEE: All right, so now to wrap up this time together, I always try to ask one more provocative last question. One of the dreams that comes naturally to someone like me or any of my colleagues, probably even to you, is this idea of, you know, wouldn&#8217;t it be possible someday to have a foundation model for biology or for human biology or foundation model for the human cell or something along these lines?&nbsp;



And in fact, there are, of course, you and I are both aware of people who are taking that idea seriously and chasing after it. I have people in our labs that think hard about this kind of thing. Is it a reasonable thought at all?&nbsp;



KOLLER: I have learned over the years to avoid saying the word never because technology proceeds in ways that you often don&#8217;t expect. And so will we at some point be able to measure the cell in enough different ways across enough different channels at the same time that you can piece together what a cell does? I think that is eminently feasible, not today, but over time.&nbsp;



I don&#8217;t think it&#8217;s feasible using today&#8217;s technology, although the efforts to get there may expose where the biggest opportunities lie to, you know, build that next layer. So I think it&#8217;s good that people are working on really hard problems. I would also point out that even if one were to solve that really challenging problem of creating a model of a cell, there is thousands of different types of cells within the human body.&nbsp;



They&#8217;re very different. They also talk to each other â€¦&nbsp;



LEE: Yep.&nbsp;



KOLLER: â€¦ both within the cell type and across different cell types. So the combinatorial complexity of that system is, I think, unfathomable to many people. I mean, I would say to all of us.&nbsp;



LEE: Yeah.&nbsp;



KOLLER: And so even from that very lofty goal, there is multiple big steps that would need to be taken to a mechanistic model of the full organism. So will we ever get there? Again, you know, I don&#8217;t see a reason why this is impossible to do. So I think over time, technology will get better and will allow us to build more and more elaborate models of more and more complex systems.&nbsp;



Patients can&#8217;t wait &#8230;



LEE: Right. Yeah.&nbsp;



KOLLER: â€¦ for that to happen in order for us to get them better medicines. So I think there is a great basic science initiative on that side of things. And, in parallel, we need to make do with the data that we have or can collect or can print. We print a lot of data in our internal wet labs and get to drugs that are effective even though they don&#8217;t benefit from having a full-blown mechanistic model.&nbsp;



LEE: Last question: where do you think we&#8217;ll be in five years?&nbsp;



KOLLER: Phew. If I had answered that question five years ago, I would have been very badly embarrassed at the inaccuracy of my answer. [LAUGHTER] So I will not answer it today either.&nbsp;



I will say that the thing about exponential curves is that they are very, very tricky, and they move in unexpected ways. I would hope that in five years, we will have made a sufficient investment in the generation of scientific data that we will be able to move beyond data that was generated entirely by humans and therefore insights that are derivative of what people already know to things that are truly novel discoveries.&nbsp;



And I think in order to do that in, you know, math, maybe because math is entirely conceptual, maybe you can do that today. Math is effectively a construct of the human mind. I don&#8217;t think biology is a construct of the human mind, and therefore one needs to collect enough data to really build those models that will give rise to those novel insights.&nbsp;



And that&#8217;s where I hope we will have made considerable progress in five years.&nbsp;



LEE: Well, I&#8217;m with you. I hope so, too. Well, you know, thank you, Daphne, so much for this conversation. I learn a lot talking to you, and it was great to, you know, connect again on this. And congratulations on all of this success. It&#8217;s really groundbreaking.&nbsp;



KOLLER: Thank you very much, Peter. It was a pleasure chatting with you, as well.&nbsp;



[TRANSITION MUSIC]&nbsp;



LEE: I still think of Daphne first and foremost as an AI researcher. And for sure, her research work in machine learning continues to be incredibly influential to this day. But it&#8217;s her work on AI-enhanced drug development that now is on the verge of making a really big difference on some of the most difficult diseases afflicting people today.&nbsp;



In our book, Carey, Zak, and I predicted that AI might be a meaningful accelerant in biomedical research, but I don&#8217;t know that we foresaw the incredible potential specifically in drug development.&nbsp;



Today, we&#8217;re seeing a flurry of activity at companies, universities, and startups on generative AI systems that aid and maybe even completely automate the design of new molecules as drug candidates. But now, in our conversation with Daphne, seeing AI go even further than that to do what one might reasonably have assumed to be impossible, to identify and select novel drug targets, especially for a neurodegenerative disease like ALS, it&#8217;s just, well, mind blowing.â€¯



Let&#8217;s continue our deep dive on AI and biomedical research with this conversation with Noubar Afeyan:&nbsp;



LEE: Noubar, thanks so much for joining. I&#8217;m really looking forward to this conversation.&nbsp;



NOUBAR AFEYAN: Peter, thanks. Thrilled to be here.&nbsp;



LEE: While I think most of the listeners to this podcast have heard of Flagship Pioneering (opens in new tab), it&#8217;s still worth hearing from you, you know, what is Flagship? And maybe a little bit about your background. And finally, you found a way to balance science and business creation. And so, you know, your approach and philosophy to all of that.&nbsp;



AFEYAN: Well, great. So maybe I&#8217;ll just start out by way of quick background. You know, my &#8230; and since we&#8217;re going talk about AI, I&#8217;ll also highlight my first contact with the topic of AI. So as an undergraduate in 1980 up at McGill University, I was an engineering student, but I was really captivated by, at that time, the talk on the campus around the expert system, heuristic-based, rule-based kind of programs.&nbsp;



LEE: Right.&nbsp;



AFEYAN: And so actually I had the dubious distinction of writing my one and only college newspaper article. [LAUGHTER] That was a short career. And it was all about how artificial intelligence would be impacting medicine, would be impacting, you know, speech capture, translation, and some of the ideas that were there that it&#8217;s interesting to see now 45 years later re-emerge with some of the new learning-based models.&nbsp;



My journey after college ended up taking me into biotechnology. In the early â€™80s, I came to MIT to do a PhD. At the time, the field was brand new. I ended up being the first PhD graduate from MIT in this combination biology and engineering degree. And since then, I&#8217;ve basically beenâ€”so since 1987â€”a founder, a technologist in the space of biotechnology for human health and as well for planetary health.&nbsp;



And then in 1999/2000 formed what is now Flagship Pioneering, which essentially was an attempt to bring together the three elements of what we know are important in startups. That is scientific capital, human capital, and financial capital. Right now, startups get that from different places. The science in our fields mostly come from academia, research hospitals. The human capital comes from other startups â€¦&nbsp;



LEE: Yeah.&nbsp;



AFEYAN: â€¦ or large companies or some academics leave. And then the financial capital is usually venture capital, but there&#8217;s also now more and more other deeper pockets of money.&nbsp;



What we thought was, what if all that existed in one entity and instead of having to convince each other how much they should believe the other if we just said, â€œLet&#8217;s use that power to go work on much further out thingsâ€? But in a way where nobody would believe it in the beginning, but we could give ourselves a little bit of time to do impactful big things.&nbsp;



Twenty-five years later, that&#8217;s the road we&#8217;ve stayed on.&nbsp;



LEE: OK. So let&#8217;s get into AI. Now, you know, what I&#8217;ve been asking guests is kind of an origin story. And there&#8217;s the origin story of contact with AI, you know, before the emergence of generative AI and afterwards. I don&#8217;t think there&#8217;s much of a point to asking you the pre-ChatGPT. But â€¦ so let&#8217;s focus on your first encounter with ChatGPT or generative AI. When did that happen, and what went through your head?&nbsp;



AFEYAN: Yeah. So, if you permit me, Peter, just for very briefly, let me actually say I had the interesting opportunity over the last 25 years to actually stay pretty close to the machine learning world â€¦&nbsp;



LEE: Yeah. Yeah.&nbsp;



AFEYAN: â€¦ because one, as you well know, among the most prolific users of machine learning has been the bioinformatics computational biology world because it&#8217;s been so data rich that anything that can be done, people have thrown at these problems because unlike most other things, we&#8217;re not working on man-made data. We&#8217;re looking at data that comes from nature, the complexity of which far exceeds our ability to comprehend.&nbsp;



So you could imagine that any approach to statistically reduce complexity, get signal out of scant dataâ€”that&#8217;s a problem that&#8217;s been around.&nbsp;



The other place where I&#8217;ve been exposed to this, which I&#8217;m going to come back to because that&#8217;s where it first felt totally different to me, is that some 25 years ago, actually the very first company we started was a company that attempted to use evolutionary algorithms to essentially iteratively evolve consumer-packaged goods online. Literally, we tried to, you know, consider features of products as genes and create little genomes of them. And by recombination and mutation, we could create variety. And then we could get people through panels onlineâ€”this was 2002/2003 timeframeâ€”we could essentially get people through iterative cycles of voting to create a survival of the fittest. And that&#8217;s a company that was called Affinnova.&nbsp;



The reason I say that is that I knew that thereâ€™s a much better way to do this if only: one, you can generate variety â€¦&nbsp;



LEE: Yeah.&nbsp;



AFEYAN: â€¦ without having to prespecify genes. We couldnâ€™t do that before. And, two, which weâ€™ve come back to nowadays, you can actually mimic how humans think about voting on things and just get rid of that element of it.&nbsp;



So then to your question of when does this kind of begin to feel different? So you could imagine that in biotechnology, you know, as an engineer by background, I always wanted to do CAD, and I picked the one field in which CAD doesn&#8217;t exist, which is biology. Computer-aided design is kind of a notional thing in that space. But boy, have we tried. For a long time, &#8230;



LEE: Yep.&nbsp;



AFEYAN: &#8230; people would try to do, you know, hidden Markov models of genomes to try to figure out what should be the next, you know, base that you may want to or where genes might be, etc. But the notion of generating in biology has been something we&#8217;ve tried for a while. And in the late teens, so kind of 2018, â€™17, â€™18, because we saw deep learning come along, and you could basically generate novelty with some of the deep learning models â€¦ and so we started asking, â€œCould you generate a protein basically by training a correspondence table, if you will, between protein structures and their underlying DNA sequence?â€ Not their protein sequence, but their DNA sequence.&nbsp;



LEE: Yeah.&nbsp;



AFEYAN: So that&#8217;s a big leap. So â€™17/â€™18, we started this thing. It was called 56. It was FL56, Flagship Labs 56, our 56th project.&nbsp;



By the way, we started this parallel one called â€œ57â€ that did it in a very different way. So one of them did pure black box model-building. The other one said, you know what, we don&#8217;t want to do the kind of &#8230; at that time, AlphaFold was in its very early embodiments. And we said, â€œIs there a way we could actually take little, you know, multi amino acid kind of almost grammars, if you will, a little piece, and then see if we could compose a protein that way?â€ So we were experimenting.&nbsp;



And what we found was that actually, if you show enough instances and you could train a transformer modelâ€”back in the day, that&#8217;s what we were usingâ€”you could actually, say, predict another sequence that should have the same activity as the first one.&nbsp;



LEE: Yeah.&nbsp;



AFEYAN: So we trained on green fluorescent proteins. Now, we&#8217;re talking about seven years ago. We trained on enzymes, and then we got to antibodies.&nbsp;



With antibodies, we started seeing that, boy, this could be a pretty big deal because it has big market impact. And we started bringing in some of the diffusion models that were beginning to come along at that time. And so we started getting much more excited. This was all done in a company that subsequently got renamed from FL56 to Generate:Biomedicines (opens in new tab), â€¦&nbsp;



LEE: Yep, yep.&nbsp;



AFEYAN: â€¦ which is one of the leaders in protein design using the generative techniques. It was interesting because Generate:Biomedicines is a company that was called that before generative AI was a thing, [LAUGHTER] which was kind of very ironic.&nbsp;



And, of course, that team, which operates today very, very kind of at the cutting edge, has published their models. They came up with this first Chroma (opens in new tab) model, which is a diffusion-based model, and then started incorporating a lot of the LLM capabilities and fusing them.&nbsp;



Now we&#8217;re doing atomistic models and many other things. The point being, that gave us a glimpse of how quickly the capability was gaining, â€¦&nbsp;



LEE: Yeah. Yeah.&nbsp;



AFEYAN: â€¦ just like evolution shows you. Sometimes evolution is super silent, and then all of a sudden, all hell breaks loose. And that&#8217;s what we saw.&nbsp;



LEE: Right. One of the things that I reflect on just in my own journey through this is there are other emotions that come up. One that was prominent for me early on was skepticism. Were there points when even in your own work, transformer-based work on this early on, that you had doubts or skepticism that these transformer architectures would be or diffusion-based approaches would be worth anything?&nbsp;



AFEYAN: You know, it&#8217;s interesting, I think that, I&#8217;m going to say this to you in a kind of a friendly way, but you&#8217;ll understand what I mean. In the world I live in, it&#8217;s kind of like the slums of innovation, [LAUGHTER] kind of like just doing things that are not supposed to work. The notion of skepticism is a luxury, right. I assume everything we do won&#8217;t work. And then once in a while I&#8217;m wrong.&nbsp;



And so I don&#8217;t actually try to evaluate whether before I bring something in, like just think about it. We, some hundred or so times a year, ask â€œwhat ifâ€ questions that lead us to totally weird places of thought. We then try to iterate, iterate, iterate to come up with something that&#8217;s testable. Then we go into a lab, and we test it.&nbsp;



So in that world, right, sitting there going, like, â€œHow do I know this transformer is going to work?â€ The answer is, â€œFor what?â€ Like, it&#8217;s going to work. To make something up &#8230; well, guess what? We knew early on with LLMs that hallucination was a feature, not a bug for what we wanted to do.&nbsp;



So it&#8217;s just such a different use that, of course, I have trained scientific skepticism, but it&#8217;s a little bit like looking at a competitive situation in an ecology and saying, â€œI bet that thing&#8217;s going to die.â€ Well, you&#8217;d be rightâ€”most of the time, you&#8217;d be right. [LAUGHTER]&nbsp;



So I just don&#8217;t â€¦ like, it â€¦ and that&#8217;s whyâ€”I guess, call me an early adopterâ€”for us, things that could move the needle even a little, but then upon repetition a lot, let alone this, â€¦&nbsp;



LEE: Yeah.&nbsp;



AFEYAN: â€¦ you have to embrace. You can&#8217;t wait there and say, I&#8217;ll embrace it once it&#8217;s ready. And so that&#8217;s what we did.&nbsp;



LEE: Hmm. All right. So let&#8217;s get into some specifics and what you are seeing either in your portfolio companies or in the research projects or out in the industry. What is going on today with respect to AI really being used for something meaningful in the design and development of drugs?&nbsp;



AFEYAN: In companies that are doing as diverse things asâ€”let me give you a few examplesâ€”a project that&#8217;s now become a named company called ProFound Therapeutics (opens in new tab) that literally discovered three, four years ago, and would not have been able to without some of the big data-model-building capabilities, that our cells make literally thousands, if not tens of thousands, of more proteins than we were aware of, full stop.&nbsp;



We had done the human genome sequence, there was 20,000 genes, we thought that there was â€¦&nbsp;



LEE: Wow.&nbsp;



AFEYAN: â€¦ maybe 70-80,000, 100,000 proteins, and that&#8217;s that. And it turns out that our cells have a penchant to express themselves in the form of proteins, and they have many other ways than we knew to do that.&nbsp;



Now, so what does that mean? That means that we have generated a massive amount of data, the interpretation of which, the use of which to guide what you do and what these things might be involved with is purely being done using the most cutting-edge data-trained models that allow you to navigate such complexity.&nbsp;



LEE: Wow. Hmm.&nbsp;



AFEYAN: That&#8217;s just one example. Another example: a company called Quotient Therapeutics (opens in new tab), again three, four years old. I can talk about the ones that are three, four years old because we&#8217;ve kind of gotten to a place where we&#8217;ve decided that it&#8217;s not going to fail yet, [LAUGHTER] so we can talk about it.&nbsp;



You know, we discoveredâ€”our team discoveredâ€”that in our cells, right, so we know that when we get cancer, our cells have genetic mutations in them or DNA mutations that are correlated and often causal to the hyperproliferative stages of cancer. But what we assume is that all the other cells in our body, pretty much, have one copy of their genes from our mom, one copy from our dad, and that&#8217;s that.&nbsp;



And when very precise deep sequencing came along, we always asked the question, â€œHow much variation is there cell to cell?â€&nbsp;



LEE: Right.&nbsp;



AFEYAN: And the answer was it&#8217;s kind of noise, random variation. Well, our team said, â€œWell, what if it&#8217;s not really that random?â€ because upon cell division cycles, there&#8217;s selection happening on these cells. And so not just in cancer but in liver cells, in muscle cells, in skin cells â€¦&nbsp;



LEE: Oh, interesting.&nbsp;



AFEYAN: â€¦ can you imagine that there&#8217;s an evolutionary experiment that is favoring either compensatory mutations that are helping you avoid disease or disease-caused mutations that are gaining advantage as a way to understand the mechanism? Sure enoughâ€”I wouldn&#8217;t be telling you otherwiseâ€”with massive amount of single cell sequencing from individual patient samples, we&#8217;ve now discovered that the human genome is mutated on average in our bodies 10,000 times, like over every base, like, it&#8217;s huge numbers.&nbsp;



And we&#8217;re finding very interesting big signals come out of this massive amount of data. By the way, data of the sort that the human mind, if it tries to assign causal explanations to what&#8217;s happening â€¦&nbsp;



LEE: Right.&nbsp;



AFEYAN: â€¦ is completely inadequate.&nbsp;



LEE: When you think about a language model, we&#8217;re learning from human language, and the totality of human languageâ€”at least relative to what we&#8217;re able to compute today in terms of constructing a modelâ€”the totality of human language is actually pretty limited. And in fact, you know, as is always written about in click-baity titles, you know, the big model builders are actually starting to run short.&nbsp;



AFEYAN: Running out, running out, yes. [LAUGHTER]&nbsp;



LEE: But one of the things that perplexes me and maybe even worries meâ€”like these two examplesâ€”are generally in the realm of cellular biology and the complexity. Let&#8217;s just take the example of your company, ProFound. You know, the complexity of what&#8217;s going on and the potential genetic diversity is such that, can we ever have enough data? You know, because there just aren&#8217;t that many human beings. There just aren&#8217;t that many samples.&nbsp;



AFEYAN: Well, it depends on what you want to train, right. So if you want to train a de novo evolutionary model that could take you from bacteria to human mammalian cells and the like, there may not beâ€”and I&#8217;m not an expert in thatâ€”but that&#8217;s a question that we often kind of think about.&nbsp;



But if you&#8217;re trying to train a &#8230; like you know what the proteins we know about, how they interact with pathways and disease mechanisms and the like. Now all of a sudden you find out that there&#8217;s a whole continent of them missing in your explanations. But there are things you can reason, in quotations, through analogy, functional analogy, sequence analogy, homology. So there&#8217;s a lot of things that we could do to essentially make use of this, even though you may not have the totality of data needed to, kind of, predict, based on a de novo sequence, exactly what it&#8217;s going to do.&nbsp;



So I agree with the comparison. But &#8230; but you&#8217;re right. The complexity is â€¦ just keep in mind, on average, a protein may be interacting with 50 to 100 other proteins.&nbsp;



LEE: Right.&nbsp;



AFEYAN: So if you find thousands of proteins, you&#8217;ve found a massive interaction space through which information is being processed in a living cell.&nbsp;



LEE: But do you find in your AI companies that access to data ends up being a key challenge? Or, you know, how central is that?&nbsp;



AFEYAN: Access to data is a key challenge for the companies we have that are trying to build just models. But that&#8217;s the minority of things we do. The majority of things we do is to actually co-develop the data and the models. And as you know well, because you guys, you know, have given us some ideas around this space, that, you know, you could generate data and then think about what you&#8217;re to do with it, which is the way biotech is operated with bioinformatics.&nbsp;



LEE: Right, right.&nbsp;



AFEYAN: Or you could generate bespoke data that is used to train the model that&#8217;s quite separate from what you would have done in the natural course of biology. So we&#8217;re doing much more of the latter of late, and I think that&#8217;ll continue. So, but these things are proliferating.&nbsp;



I mean, it&#8217;s hard to find a place where we&#8217;re not using this. And the â€œthisâ€ is any and all data-driven model building, generative, LLM-based, but also every other technique to make progress.&nbsp;



LEE: Sure. So now moving away from the straight biochemistry applications, what about AI in the process of building a business, of making investment decisions, of actually running an operation? What are you seeing there?&nbsp;



AFEYAN: So, well, you know, Moderna, which is a company that I&#8217;m quite proud of being a founder and chairman of, has adopted a significant, significant amount of AI embedded into their operations in all aspects: from the manufacturing, quality control, the clinical monitoring, the designâ€”every aspect. And in fact, they&#8217;ve had a partnership that they&#8217;ve had for a little while here with OpenAI, and they&#8217;ve tried many different ways to stay at the cutting edge of that.&nbsp;



So we see that play out at some scale. Thatâ€™s a 5,000-, 6,000-person organization, and what they&#8217;re doing is a good example of what early adopters would do, at least in our kind of biotechnology company.&nbsp;



But then, you know, in our space, I would say the efficiency impact is kind of no different, than, you know, anywhere else in academia you might adopt it or in other kinds of companies. But where I find it an interesting kind of maybe segue is the degree to which it may fundamentally change the way we think about how to do science, which is a whole other use, right?&nbsp;



LEE: Right.&nbsp;



AFEYAN: So it&#8217;s not an efficiency gain per se, although it&#8217;s maybe an effectiveness gain when it comes to science, but can you just fundamentally train models to generate hypotheses?&nbsp;



LEE: Yep.&nbsp;



AFEYAN: And we have done that, and we&#8217;ve been doing this for the last three years. And now it&#8217;s getting better and better, the better these reasoning engines are getting and kind of being able to extrapolate and train for novelty. Can you convert that to the world&#8217;s best experimental protocol to very precisely falsify your hypothesis, on and on?&nbsp;



That closing of that loop, kind of what we call autonomous science, which we&#8217;ve been trying to do for the last two, three years and are making some progress in, that to me is another kind of bespoke use of these things, not to generate molecules in its chemistry, but to change the behavior of how science is done.&nbsp;



LEE: Yeah. So I always end with a couple of provocative questions, but I needâ€”before we do that, while we&#8217;re on this subjectâ€”to get your take on Lila Sciences (opens in new tab).&nbsp;



And there is a vision there that I think is very interesting. It&#8217;d be great to hear it described by you.&nbsp;



AFEYAN: Sure. So Lila, after operating for two to three years in kind of a preparatory kind of stealth mode, we&#8217;ve now had a little bit more visibility around, and essentially what we&#8217;re trying to do there is to create what we call automated science factories, and such a factory would essentially be able to take problems, either computationally specified or human-specified, and essentially do the experimental work in order to either make an optimization happen or enable something that just didnâ€™t exist. And itâ€™s really, at this point, weâ€™ve shown proof of concept in narrow areas.&nbsp;



LEE: Yep.&nbsp;



AFEYAN: But itâ€™s hard to say that if you can do this, you canâ€™t do some other things, so weâ€™re just expanding it that way. We donâ€™t think we need a complete proof or complete demonstration of it for every aspect.&nbsp;



LEE: Right.&nbsp;



AFEYAN: So we&#8217;re just kind of being opportunistic. The idea for Lila is to partner with a number of companies. The good news is, within Flagship, there&#8217;s 48 of them. And so there&#8217;s a whole lot of them they can partner with to get their learning cycles. But eventually they want to be a real alternative to every time somebody has an idea, having to kind of go into a lab and manually do this.&nbsp;



I do want to say one thing we touched on, Peter, though, just on that front, which is &#8230;&nbsp;



LEE: Yep.&nbsp;



AFEYAN: &#8230; if you say, like, â€œWhat problem is this going to solve?â€ It&#8217;s several but an important one is just the flat-out human capacity to reason on this much data and this much complexity that is real. Because nature doesn&#8217;t try to abstract itself in a human understandable form.&nbsp;



LEE: Right. Yeah.&nbsp;



AFEYAN: In biology, since it&#8217;s kind of like progress happens through evolutionary kind of selections, the evidence of which [has] long been lost, and so therefore, you just see what you have, and then it has a behavior. I really do think that there&#8217;s something to be said, and I want toâ€”just for your audienceâ€”lay out a provocative, at least, thought on all this, which Lila is a beginning embodiment of, which is that I really think that what&#8217;s going to happen over the next five, 10 years, even while we&#8217;re all fascinated with the impending arrival of AGI [artificial general intelligence] is really what I call poly-intelligence, which is the combination of human intelligence, machine intelligence, AI, and nature&#8217;s intelligence.&nbsp;



We&#8217;re all fascinated at the human-machine interface. We know the human-nature interface, but imagine the machine-nature interfaceâ€”that is, actually letting loose a digital kind of information processing life form through the algorithms that are being developed and the commensurately complex, maybe much more complex. We&#8217;ll see. And so now the question becomes, what does the human do?&nbsp;



And we&#8217;re living in a world which is human dominated, which means the humans say, â€œIf I don&#8217;t understand it, it&#8217;s not real, basically. And if I don&#8217;t understand it, I can&#8217;t regulate it.â€ And we&#8217;re going to have to make peace with the fact that we&#8217;re not going to be able to predictably affect things without necessarily understanding them the way we could if we just forced ourselves to only work on problems we can understand. And that world we&#8217;re not ready for at all.&nbsp;



LEE: Yeah. All right. So this one I predict is going to be a little harder for you because I think while you think about the future, you live very much in the present. But I&#8217;d like you to make some predictions about what the biotech and biopharmaceutical industries are going to be able to do two years from now, five years from now, 10 years from now.&nbsp;



AFEYAN: Yeah, well, it&#8217;s hard for me because you know my nature, which is that I think this is all emergent.&nbsp;



LEE: Right.&nbsp;



AFEYAN: And so I would be the conceit of predicting. So I would say with likelihood positive predictive value of less than 10%, I&#8217;m happy to answer your question. So I&#8217;m not trying to score high [LAUGHTER] because I really think that my job is to envision it, not to predict it. And that&#8217;s a little bit different, right?&nbsp;



LEE: Yeah, I actually was trying to pick what would be the hardest possible question I could ask you, [LAUGHTER] and this is what I came up with.&nbsp;



AFEYAN: Yeah, no, no, I&#8217;m kidding here. So now look, I think that we will cross this threshold of understandability. And of course you&#8217;re seeing that in a lot of LLM things today. And of course, people are trying to train for things that are explainers and all that whole, there&#8217;s a whole world of that. But I think at some point we&#8217;re going to have to kind of let go and get comfortable working on things that, you know â€¦&nbsp;



I sometimes tell people, you know, and I&#8217;m not the first, but scientists and engineers are different, it&#8217;s said, in that engineers work on things that they don&#8217;t wait until they get a full understanding of before they work with them. Well, now scientists are going to have to get used to that, too, right?&nbsp;



LEE: Yeah. Yeah.&nbsp;



AFEYAN: Because insisting that it&#8217;s only valid if it&#8217;s understandable. So, I would say, look, I hope that the time â€¦ for example, I think major improvements will be made in patient selection. If we can test drugs on patients that are more synchronized as to the stage of their disease â€¦&nbsp;



LEE: Yep.&nbsp;



AFEYAN: &#8230; I think the answer will be much better. We&#8217;re working on that. It&#8217;s a company called Etiome (opens in new tab), very, very early stage. It&#8217;s really beautiful data, very early data that shows that when we talk about MASH [metabolic dysfunction-associated steatohepatitis], liver disease, when we talk about Parkinson&#8217;s, there&#8217;s such a heterogeneity, not only of the subset type of the disease, but the stage of the disease, that this notion that you have stage one cancer, stage two cancer, again, nobody told nature there&#8217;s stages of that kind. It&#8217;s a continuum.&nbsp;



But if you can synchronize based on training, kind of, the ability to detect who are the patients that are in enough of a close proximity that should be treated so that the trialâ€”much smaller a trial sizeâ€”could give you a drug, then afterwards, you can prescribe it using these approaches.&nbsp;



Kind of we&#8217;re going to find that what we thought is one disease is more like 15 diseases. That&#8217;s bad news because we&#8217;re not going to be able to claim that we can treat everything which we can. It&#8217;s good news in that there&#8217;s going to be people who are going to start making much more specific solutions to things.&nbsp;



LEE: Right.&nbsp;



AFEYAN: So I can imagine that. I can imagine a generation of, kind of, students who are going to be able to play in this space without having 25 years of graduate education on the subject. So what is deemed knowledge sufficient to do creative things will change. I can go on and on, but I think all this is very close by and it&#8217;s very exciting.&nbsp;



LEE: Noubar, I just always have so much fun, and I learn really a lot. It&#8217;s high-density learning when I talk to you. And so I hope our listeners feel the same way. It&#8217;s something I really appreciate.&nbsp;



AFEYAN: Well, Peter, thanks for this. And I think your listeners know that if I was asking you questions, you would be answering them with equal if not more fascinating stuff. So, thanks for giving me the chance to do that today.&nbsp;



[TRANSITION MUSIC]&nbsp;



LEE: Iâ€™m always fascinated by Noubarâ€™s perspectives on fundamental research and how it connects to human health and the building of successful companies. I see him as a classic â€œsystems thinker,â€ and by that, I mean he builds impressive things like Flagship Pioneering itself, which he created as a kind of biomedical innovation system.&nbsp;



In our conversation, I was really struck by the fact that heâ€™s been thinking about the potential impact of transformersâ€”transformers being the fundamental building block of large language modelsâ€”as far back as 2017, when the first paper on the attention mechanism in transformers was published by Google.&nbsp;



But, you know, it isnâ€™t only about using AI to do things like understand and design molecules and antibodies faster. It&#8217;s interesting that he is also pushing really hard towards a future where AI might â€œclose the loopâ€ from hypothesis generation, to experiment design, to analysis, and so on.&nbsp;



Now, hereâ€™s my conversation with Dr. Eric Topol:&nbsp;



LEE: Eric, it&#8217;s really great to have you here.&nbsp;



ERIC TOPOL: Oh, Peter, I&#8217;m thrilled to be here with you here at Microsoft.&nbsp;



LEE: You&#8217;re a super famous person. Extremely well known to researchers even in computer science, as we have here at Microsoft Research.&nbsp;



But the question I&#8217;d like to ask is, how would you explain to your parents what you do every day?&nbsp;



TOPOL: [LAUGHS] That&#8217;s a good question. If I was just telling them I&#8217;m trying to come up with better ways to keep people healthy, that probably would be the easiest way to do it because if I ever got in deeper, I would lose them real quickly. They&#8217;re not around, but just thinking about what they could understand.&nbsp;



LEE: Right.&nbsp;



TOPOL: I think as long as they knew it was work centered on innovative paths to promoting and preserving human health, that would get to them, I think.&nbsp;



LEE: OK, so now, kind of the second topic, and then we let the conversation flow, is about origin stories with respect to AI. And with most of our guests, you know, I factor that into two pieces: the encounters with AI before ChatGPT and what we call generative AI and then the first contacts after.&nbsp;



And, of course, you have extensive contact with both now. But let&#8217;s start with how you got interested in machine learning and AI prior to ChatGPT. How did that happen?&nbsp;



TOPOL: Yeah, it was out of necessity. So back, you know, when I started at Scripps at the end of â€™06, we started accumulating, you know, massive datasets. First, it was whole genomes. We did one of the early big cohorts of 1,400 people of healthy aging. We called the Wellderly whole genome sequence (opens in new tab).&nbsp;



And then we started big in the sensor world, and then we started saying, what are we going to do with all this data, with electronic health records and all those sensors? And now we got whole genomes.&nbsp;



And basically, what we were doing, we were in hoarding mode. We didn&#8217;t have a way to meaningfully analyze it.&nbsp;



LEE: Right.&nbsp;



TOPOL: You would read about how, you know, data is the new oil and, you know, gold and whatnot. But we just didn&#8217;t have a way to extract the juice. And even when we wanted to analyze genomes, it was incredibly laborious.&nbsp;



LEE: Yeah.&nbsp;



TOPOL: And we weren&#8217;t extracting a lot of the important information. So that&#8217;s why &#8230; not having any training in computer science, when I was doing the &#8230; about three years of work to do the book Deep Medicine, I started really, first auto-didactic about, you know, machine learning. And then I started contacting a lot of the real top people in the field and hanging out with them, and learning from them, getting their views as to, you know, where we are today, what models are coming in the future.&nbsp;



And then I said, â€œYou know what? We are going to be able to fix this mess.â€ [LAUGHS] We&#8217;re going to get out of the hoarding phase, and we&#8217;re going to get into, you know, really making a difference.&nbsp;



So that&#8217;s when I embraced the future of AI. And I knew, you know, backâ€”that was six years ago when it was published and probably eight or nine years ago when I was doing the research, and I knew that we weren&#8217;t there yet.&nbsp;



You know, at the time, we were seeing the image interpretation. That was kind of the early promise. But really, the models that were transformative, the transformer models, they were incubating back in 2017. So people knew something was brewing.&nbsp;



LEE: Right. Yes.&nbsp;



TOPOL: And everyone said we&#8217;re going to get there.&nbsp;



LEE: So then, ChatGPT comes out November of 2022; thereâ€™s GPT-4 in 2023, and now a lot has happened. Do you remember what your first encounter with that technology was?&nbsp;



TOPOL: Oh, sure. First, ChatGPT. You know, in the last days of November â€™22, I was just blown away. I mean, I&#8217;m having a conversation. I&#8217;m having fun. And this is humanoid responding to me. I said, â€œWhat?â€ You know? So that was to me, a moment I&#8217;ll never forget. And so I knew that the world was, you know, at a very kind of momentous changing point.&nbsp;



Of course, knowing, too, that this is going to be built on, and built on quickly. Of course, I didn&#8217;t know how soon GPT-4 and all the others were going to come forward, but that was a wake-up call that the capabilities of AI had just made a humongous jump, which seemingly was all of a sudden, although I did know this had been percolating â€¦&nbsp;



LEE: Right.&nbsp;



TOPOL: â€¦ you know, for what, at least five years, that, you know, it really was getting into its position to do this.&nbsp;



LEE: I know one of the things that was challenging psychologically and emotionally for me is, it made me rethink a lot of things that were going on in Microsoft Research in areas like causal reasoning, natural language processing, speech processing, and so on.&nbsp;



I&#8217;m imagining you must have had some emotional struggles too because you have this amazing book, Deep Medicine. Did you have to â€¦ did it go through your mind to rethink what you wrote in Deep Medicine in light of this or, or, you know, how did that feel?&nbsp;



TOPOL: It&#8217;s funny you ask that because in this one chapter I have on the virtual health coach, I wrote a whole bunch of scenarios &#8230;&nbsp;



LEE: Yeah.&nbsp;



TOPOL: â€¦ that were very kind of futuristic. You know, about how the AI interacts with the person&#8217;s health and schedules their appointment for this and their scan and tells them what lab tests they should tell their doctor to have, and, you know, all these things. And I sent a whole bunch of these, thinking that they were a little too far-fetched.&nbsp;



LEE: Yes.&nbsp;



TOPOL: And I sent them to my editor when I wrote the book, and he says, â€œOh, these are great. You should put them all in.â€ [LAUGHTER] What I didn&#8217;t realize is they weren&#8217;t that, you know, they were all going to happen.&nbsp;



&nbsp;LEE: Yeah. They weren&#8217;t that far-fetched at all.&nbsp;



TOPOL: Not at all. If there&#8217;s one thing I&#8217;ve learned from all this, is our imagination isn&#8217;t big enough.&nbsp;



&nbsp;LEE: Yeah.&nbsp;



TOPOL: We think too small.&nbsp;



LEE: Now in our book that Carey, Zak, and I wrote, you know, we made, you know, we sort of guessed that GPT-4 might help biomedical researchers, but I don&#8217;t think that any of us had the thought in mind that the architecture around generative AI would be so directly applicable to, you know, say, protein structures or, you know, to clinical health records and so on.&nbsp;



And so a lot of that seems much more obvious today. But two years ago, it wasn&#8217;t. But we did guess that biomedical researchers would find this interesting and be helped along.&nbsp;



So as you reflect over the past two years, you know, do you have things that you think are very important, kind of, meaningful applications of generative AI in the kinds of research that Scripps does?&nbsp;



TOPOL: Yeah. I mean, I think for one, you pointed out how the term generative AI is a misnomer.&nbsp;



LEE: Yeah.&nbsp;



TOPOL: And so it really was prescient about how, you know, it had a pluripotent capability in every respect, you know, of editing and creating. So that was something that I think was telling us, an indicator that this is, you know, a lot bigger than how it&#8217;s being labeled. And our expectations can actually be more than what we had seen previously with the earlier version.&nbsp;



So I think what&#8217;s happened is that now, we keep jumping. It&#8217;s so quick that we can&#8217;t â€¦ you know, first we think, oh, well, weâ€™ve gone into the agentic era, and then we could pass that with reasoning. [LAUGHTER] And, you know, we just can&#8217;t â€¦&nbsp;



LEE: Right.&nbsp;



TOPOL: It&#8217;s just wild.&nbsp;



LEE: Yeah.&nbsp;



TOPOL: So I think so many of us now will put in prompts that will necessitate or ideally result in a not-immediate gratification, but rather one that requires, you know, quite a bit of combing through the corpus of knowledge &#8230;&nbsp;



LEE: Yeah.&nbsp;



TOPOL: â€¦ and getting, with all the citations, a report or a response. And I think now this has been a reset because to do that on our own, it takes, you know, many, many hours. And it&#8217;s usually incomplete.&nbsp;



But one of the things that was so different in the beginning was you would get the references from up to a year and a half previously.&nbsp;



LEE: Yep.&nbsp;



TOPOL: And that&#8217;s not good enough. [LAUGHS]&nbsp;



LEE: Right.&nbsp;



TOPOL: And now you get references, like, from the day before.&nbsp;



LEE: Yes. Yeah.&nbsp;



TOPOL: And so, you say, â€œWhy would you do a regular search for anything when you could do something like this?â€&nbsp;



LEE: Yeah.&nbsp;



TOPOL: And then, you know, the reasoning power. And a lot of people who are not using this enough still are talking about, â€œWell, there&#8217;s no reasoning.â€&nbsp;



LEE: Yeah.



TOPOL: Which you dealt with really well in the book. But what, of course, you couldn&#8217;t have predicted is the new dimensions.&nbsp;



LEE: Right.&nbsp;



TOPOL: I think you nailed it with GPT-4. But it&#8217;s all these just, kind of, stepwise progressions that have been occurring because of the velocity that&#8217;s unprecedented. I just can&#8217;t believe it.&nbsp;



LEE: We were aware of the idea of multi-modality, but we didn&#8217;t appreciate, you know, what that would mean. Like AlphaFold (opens in new tab) [protein structure database], you know, the ability for AI to understandâ€”or crystal structuresâ€”to really start understanding something more fundamental about biochemistry or medicinal chemistry.&nbsp;



I have to admit, when we wrote the book, we really had no idea.&nbsp;



TOPOL: Well, I feel the same way. I still today can&#8217;t get over it because the reason AlphaFold and Demis [Hassabis] and John Jumper [AlphaFoldâ€™s co-creators] were so successful is there was this protein databank.&nbsp;



LEE: Yes.&nbsp;



TOPOL: And it had been kept for decades. And so, they had the substrate to work with.&nbsp;



LEE: Right.&nbsp;



TOPOL: So, you say, â€œOK, we can do proteins.â€ But then how do you do everything else?&nbsp;



LEE: Right.&nbsp;



TOPOL: And so this whole, what I call, â€œlarge language of life modelâ€ work, which has gone into high gear like I&#8217;ve never seen.&nbsp;



LEE: Yeah.&nbsp;



TOPOL: You know, now to this holy grail of a virtual cell, and &#8230;&nbsp;



LEE: Yeah.&nbsp;



TOPOL: You know, it&#8217;s basically &#8230; it&#8217;s &#8230; it was inspired by proteins. But now it&#8217;s hitting on, you know, ligands and small molecules, cells. I mean, nothing is being held back here.&nbsp;



LEE: Yeah.&nbsp;



TOPOL: So how could anybody have predicted that?&nbsp;



LEE: Right.&nbsp;



TOPOL: I sure wouldn&#8217;t have thought it would be possible at this point.&nbsp;



LEE: Yeah. So just to challenge you, where do you think that is going to be two years from now? Five years from now? Ten years from now? Like, so you talk about a virtual cell. Is that achievable within 10 years, or is that still too far out?&nbsp;



TOPOL: No, I think within 10 years for sure. You know the group that got assembled that Steve Quake (opens in new tab) pulled together?&nbsp;



LEE: Right.&nbsp;



TOPOL: I think has 42 authors in a paper (opens in new tab) in Cell. The fact that he could get these 42 experts in life science and some in computer science to come together and all agree â€¦&nbsp;



LEE: Yeah.&nbsp;



TOPOL: â€¦ that not only is this a worthy goal, but it&#8217;s actually going to be realized, that was impressive.&nbsp;



I challenged him about that. How did you get these people all to agree? So many of them were naysayers. And by the time the workshop finished, they were fully convinced. I think that what we&#8217;re seeing is so much progress happening so quickly. And then all the different models, you know, across DNA, RNA, and everything are just zooming forward.&nbsp;



LEE: Yeah.&nbsp;



TOPOL: And it&#8217;s just a matter of pulling this together. Now when we have that, and I think it could easily be well before a decade and possibly, you know, between the five- and 10-year markâ€”that&#8217;s just a guessâ€”but then we&#8217;re moving into another era of life science because right now, you know, this whole buzz about drug discovery.&nbsp;



LEE: Yep.&nbsp;



TOPOL: It&#8217;s not&#8230; with the ability to do all these perturbations at a cellular level.&nbsp;



LEE: Right.&nbsp;



TOPOL: Or the cell of interest.&nbsp;



LEE: Yeah.&nbsp;



TOPOL: Or the cell-to-cell interactions or the intra-cell interaction. So once you nail that, yeah, it takes it to a kind of another predictive level that we haven&#8217;t really fathomed. So, yes, there&#8217;s going to be drug discovery that&#8217;s accelerated. But this would make that and also the underpinnings of diseases.&nbsp;



LEE: Yeah.&nbsp;



TOPOL: So the idea that there&#8217;s so many diseases we don&#8217;t understand now. And if you had virtual cell, â€¦&nbsp;



LEE: Yeah.&nbsp;



TOPOL: â€¦ you would probably get to that answer â€¦&nbsp;



LEE: Yeah.&nbsp;



TOPOL: â€¦ much more quickly. So whether it&#8217;s underpinnings of diseases or what it&#8217;s going to take to really come up with far better treatmentsâ€”preventionsâ€”I think that&#8217;s where virtual cell will get us.&nbsp;



LEE: There&#8217;s a technical question &#8230; I wonder if you have an opinion. You may or may not. There is sort of what I would refer to as ab initio approaches to this. You know, you start from the fundamental physics and chemistry, and we know the laws, we have the math and, you know, we can try to derive from there â€¦ in fact, we can even run simulations of that math to generate training data to build generative models and work up to a cell, or forget all of that and just take as many observations and measurements of, say, living cells as possible, and just have faith that hidden amongst all of the observational data, there is structure and language that can be derived.&nbsp;



So that&#8217;s sort of bottom-up versus top-down approaches. Do you have an opinion about which way?&nbsp;



TOPOL: Oh, I think you go after both. And clearly whenever you&#8217;re positing that you&#8217;ve got a virtual cell model that&#8217;s working, you&#8217;ve got to do the traditional methods as well to validate it, and â€¦ so all that. You know, I think if you&#8217;re going to go out after this seriously, you have to pull out all the stops. Both approaches, I think, are going to be essential.&nbsp;



LEE: You know, if what you&#8217;re saying is true, and it is amazing to hear the confidence, the one thing I tried to explain to someone nontechnical is that for a lot of problems in medicine, we just don&#8217;t have enough data in a really profound way. And the most profound way to say that is, since Adam and Eve, there have only been an estimated 106 billion people who have ever lived.&nbsp;



So even if we had the DNA of every human being, every individual of Homo sapiens, there are certain problems for which we would not have enough data.&nbsp;



TOPOL: Sure.&nbsp;



LEE: And so I think another thing that seems profound to me, if we can actually have a virtual cell, is we can actually make trillions of virtual â€¦&nbsp;



TOPOL: Yeah&nbsp;



LEE: â€¦ human beings. The true genetic diversity could be realized for our species.&nbsp;



TOPOL: I think you nailed it. The ability to have that type of data, no less synthetic data, I mean, itâ€™s just extraordinary.&nbsp;



LEE: Yeah.&nbsp;



TOPOL: We will get there someday. I&#8217;m confident of that. We may be wrong in projections. And I do think [science writer] Philip Ball won&#8217;t be right that it will never happen, though. [LAUGHTER] No, I think that if there&#8217;s a holy grail of biology, this is it.&nbsp;



LEE: Yeah.&nbsp;



TOPOL: And I think you&#8217;re absolutely right about where that will get us.&nbsp;



LEE: Yeah.&nbsp;



TOPOL: Transcending the beginning of the species.&nbsp;



LEE: Yeah.&nbsp;



TOPOL: Of our species.&nbsp;



LEE: Yeah. All right. So now, we&#8217;re starting to run short on time here. And so I wanted to ask you about, I&#8217;m in my 60s, so I actually think about this a lot more. [LAUGHTER] And I know you&#8217;ve been thinking a lot about longevity. And, of course, your new book, Super Agers.&nbsp;



And one of the reasons I&#8217;m so eager to read is it&#8217;s a topic very top of mind for me and actually for a lot of people. Where is this going? Because this is another area where you hear so much hype. At the same time, you see Nobel laureate scientists &#8230;&nbsp;



TOPOL: Yeah.&nbsp;



LEE: &#8230; working on this.&nbsp;



TOPOL: Yeah.&nbsp;



LEE: So, so what&#8217;s, what&#8217;s real there?&nbsp;



TOPOL: Yeah. Well, it&#8217;s really â€¦ the real deal is the science of aging is zooming forward.&nbsp;



And that&#8217;s exciting. But I see it bifurcating. On the one hand, all these new ideas, strategies to reverse aging are very ambitious. Like cell reprogramming and senolytics and, you know, the rejuvenation of our thymus gland, and it&#8217;s a long list.&nbsp;



LEE: Yeah.&nbsp;



TOPOL: And theyâ€™re really cool science, and it used to be the mouse lived longer. Now it&#8217;s the old mouse looks really young.&nbsp;



LEE: Yeah. Yeah.&nbsp;



TOPOL: All the different features. A blind mouse with cataracts is all of a sudden there&#8217;s no cataracts. I mean, so these things are exciting, but none of them are proven in people, and they all have significant risk, no less, you know, the expense that might be attached.&nbsp;



LEE: Right.&nbsp;



TOPOL: And some people are jumping the gun. They&#8217;re taking rapamycin, which can really knock out their immune system. So they all carry a lot of risk. And people are just getting a little carried away. We&#8217;re not there yet.&nbsp;



But the other side, which is what I emphasize in the book, which is exciting, is that we have all these new metrics that came out of the science of aging.&nbsp;



LEE: Yes.&nbsp;



TOPOL: So we have clocks of the body. Our biological clock versus our chronological clock, and we have organ clocks. So I can say, you know, Peter, we&#8217;ve assessed all your organs and your immune system. And guess what? Every one of them is either at or less than your actual age.&nbsp;



LEE: Right.&nbsp;



TOPOL: And that&#8217;s very reassuring. And by the way, your methylation clock is also â€¦ I don&#8217;t need to worry about you so much. And then I have these other tests that I can do now, like, for example, the brain. We have an amazing protein p-Tau217 that we can say over 20 years in advance of you developing Alzheimer&#8217;s, â€¦&nbsp;



LEE: Yeah.&nbsp;



TOPOL: â€¦ we can look at that, and it&#8217;s modifiable by lifestyle, bringing it down. It should be you can change the natural history. So what we&#8217;ve seen is an explosion of knowledge of metrics, proteins, no less, you know, our understanding at the gene level, the gut microbiome, the immune system. So that&#8217;s what&#8217;s so exciting. How our immune system ages. Immunosenescence. How we have more inflammationâ€”inflammagingâ€”with aging. So basically, we have three diseases that kill us, that take away our health: heart, cancer, and neurodegenerative.&nbsp;



LEE: Yep.&nbsp;



TOPOL: And they all take more than 20 years. They all have a defective immune system inflammation problem, and they&#8217;re all going to be preventable.&nbsp;



LEE: Yeah.&nbsp;



TOPOL: That&#8217;s what&#8217;s so exciting.&nbsp;So we don&#8217;t have to have reverse aging. We can actually work on â€¦&nbsp;



LEE: Just prevent aging in the first place.&nbsp;



TOPOL: â€¦ the age-related diseases. So basically, what it means is: I got to find out if you have a risk, if you&#8217;re in this high-risk group for this particular condition, because if you areâ€”and we have many levels, layers, orthogonal ways to checkâ€”we don&#8217;t just bank it all on one polygenic test. We&#8217;re going to have several ways, say this is the one we are going &#8230;&nbsp;



And then we go into high surveillance, where, let&#8217;s say if it&#8217;s your brain, we do more p-Tau, if we need to do brain imagingâ€”whatever it takes. And also, we do preventive treatments on top of the lifestyle [changes], that one of the problems we have today is a lot of people know generally, what are good lifestyle factors. Although, I go through a lot more than people generally acknowledge.&nbsp;



But they don&#8217;t incorporate them because they don&#8217;t know that they&#8217;re at risk and they could change their &#8230; extend their health span and prevent that disease. So what I at least put out there, a blueprint, is how we can use AI, because it&#8217;s multimodal AI, with all these layers of data, and then temporally, it&#8217;s like today you could say if you have two protein tests, not only are you going to have Alzheimer&#8217;s, but within a two-year time frame when &#8230;&nbsp;



LEE: Yep.&nbsp;



TOPOL: &#8230; and if you don&#8217;t change things, if we don&#8217;t gear up â€¦ you know, we can &#8230; we can completely prevent this, so â€¦ or at least defer it for a decade or more. So that&#8217;s why I&#8217;m excited, is that we made these strides in the science of aging. But we haven&#8217;t acknowledged the part that doesn&#8217;t require reversing aging. There&#8217;s this much less flashy, attainable, less risky approach &#8230;&nbsp;



LEE: Yeah.&nbsp;



TOPOL: &#8230; than the one that â€¦ when you reverse aging, you&#8217;re playing with the hallmarks of cancer. They are like, if you look at the hallmarks of cancer â€¦&nbsp;



LEE: That has been one of the primary challenges.&nbsp;



TOPOL: They&#8217;re lined up.&nbsp;



LEE: Yeah.&nbsp;



TOPOL: Theyâ€™re all the same, you know, whether it&#8217;s telomeres, or whether it&#8217;s &#8230; you know &#8230; so this is the problem. I actually say in the book, I do think one of theseâ€”we have so many shots on goalâ€”one of these reverse aging things will likely happen someday. But we&#8217;re nowhere close.&nbsp;



On the other hand, let&#8217;s gear up. Let&#8217;s do what we can do. Because we have these new metrics that&#8217;s &#8230; people don&#8217;t â€¦ like, when I read the organ clock paper (opens in new tab) from Tony Wyss-Coray from Stanford. It was published end of â€™23; it was the cover of Nature. It blew me away.&nbsp;



LEE: Yeah.&nbsp;



TOPOL: And I wrote a Substack (opens in new tab) [article] on it. And Tony said, â€œWell, that&#8217;s so nice of you.â€ I said, â€œSo nice? This is revolutionary, you know.â€ [LAUGHTER] So â€¦&nbsp;



LEE: By the way, what&#8217;s so interesting is, how these things, this kind of understanding and AI, are coming together.



TOPOL: Yes.&nbsp;



LEE: It&#8217;s almost eerie the timing of these things.&nbsp;



TOPOL: Absolutely. Because you couldn&#8217;t take all these layers of data, just like we were talking about data hoarding.



LEE: Yep.



TOPOL: Now we have data hoarding on individual with no way to be able to make these assessments of what level of risk, when, what are we going to do in this individual to prevent that? We can do that now.&nbsp;



We can do it today. And we could keep building on that. So I&#8217;m really excited about it. I think that, you know, when I wrote the last book on deep medicine, it was our overarching goal should be to bring back the patient-doctor relationship. I&#8217;m an old dog, and I know what it used to be when I got out of medical school.&nbsp;



It&#8217;s totally &#8230; you couldn&#8217;t imagine how much erosion from the â€™70s, â€™80s to now. But now I have a new overarching goal. I&#8217;m thinking that that still is really importantâ€”humanity in medicineâ€”but let&#8217;s prevent these three &#8230; big three diseases because it&#8217;s an opportunity that we&#8217;re not â€¦ you know, in medicine, all my life we&#8217;ve been hearing and talking about we need to prevent diseases.&nbsp;



Curing is much harder than prevention. And the economics. Oh my gosh. But we haven&#8217;t done it.&nbsp;



LEE: Yeah.&nbsp;



TOPOL: Now we can do it. Primary prevention. Weâ€™d do really well. Somebodyâ€™s had heart attack.&nbsp;



LEE: Yeah.&nbsp;



TOPOL: Oh, we&#8217;re going to get all over it. Why did they have a heart attack in the first place?&nbsp;



LEE: Well, the thing that makes so much sense in what you&#8217;re saying is that we understand we have an understanding both economically and medically that prevention is a good thing. And extending the concept of prevention to these age-related conditions, I think, makes all the sense in the world.&nbsp;



You know, Eric, maybe on that optimistic note, itâ€™s time to wrap up this conversation. Really appreciate you coming. Let me just brag in closing that I&#8217;m now the proud owner of an autographed copy of your latest book, and, really, thank you for that.&nbsp;



TOPOL: Oh, thank you. I could spend the rest of the day talking to you. I&#8217;ve really enjoyed it. Thanks.&nbsp;



[TRANSITION MUSIC]&nbsp;



LEE: For me, the biggest takeaway from our conversation was Ericâ€™s supremely optimistic predictions about what AI will allow us to do in much less than 10 years.&nbsp;



You know, for me personally, I started off several years ago with the typical techie naivete that if we could solve protein folding using machine learning, we would solve human biology. But as Iâ€™ve gotten smarter, Iâ€™ve realized that things are way, way more complicated than that, and so hearing Ericâ€™s techno-optimism on this is really both heartening and so interesting.&nbsp;



Another thing that really caught my attention are Ericâ€™s views on AI in medical diagnosis. That really stood out to me because within our labs here at Microsoft Research, we have been doing a lot of work on this, for example in creating foundation models for whole-slide digital pathology.&nbsp;



The bottom line, though, is that biomedical research and development is really changing and changing quickly. It&#8217;s something that we thought about and wrote briefly about in our book, but just hearing it from these three people gives me reason to believe that this is going to create tremendous benefits in the diagnosis and treatment of disease.&nbsp;



And in fact, I wonder now how regulators, such as the Food and Drug Administration here in the United States, will be able to keep up with what might become a really big increase in the number of animal and human studies that need to be approved. On this point, it&#8217;s clear that the FDA and other regulators will need to use AI to help process the likely rise in the pace of discovery and experimentation. And so stay tuned for more information about that.&nbsp;



[THEME MUSIC]â€¯



I&#8217;d like to thank Daphne, Noubar, and Eric again for their time and insights. And to our listeners, thank you for joining us. There are several episodes left in the series, including discussions on medical studentsâ€™ experiences with AI and AIâ€™s influence on the operation of health systems and public health departments. We hope you&#8217;ll continue to tune in.&nbsp;



Until next time.&nbsp;



[MUSIC FADES]â€¯

				
			
			
				Show more			
		
	





AI Revolution in Medicine podcast series

Opens in a new tabThe post How AI will accelerate biomedical research and discovery appeared first on Microsoft Research.
â€¢ Advanced fine-tuning methods on Amazon SageMaker AI
  This post provides the theoretical foundation and practical insights needed to navigate the complexities of LLM development on Amazon SageMaker AI, helping organizations make optimal choices for their specific use cases, resource constraints, and business objectives. 
We also address the three fundamental aspects of LLM development: the core lifecycle stages, the spectrum of fine-tuning methodologies, and the critical alignment techniques that provide responsible AI deployment. We explore how Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA and QLoRA have democratized model adaptation, so organizations of all sizes can customize large models to their specific needs. Additionally, we examine alignment approaches such as Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO), which help make sure these powerful systems behave in accordance with human values and organizational requirements. Finally, we focus on knowledge distillation, which enables efficient model training through a teacher/student approach, where a smaller model learns from a larger one, while mixed precision training and gradient accumulation techniques optimize memory usage and batch processing, making it possible to train large AI models with limited computational resources. 
Throughout the post, we focus on practical implementation while addressing the critical considerations of cost, performance, and operational efficiency. We begin with pre-training, the foundational phase where models gain their broad language understanding. Then we examine continued pre-training, a method to adapt models to specific domains or tasks. Finally, we discuss fine-tuning, the process that hones these models for particular applications. Each stage plays a vital role in shaping large language models (LLMs) into the sophisticated tools we use today, and understanding these processes is key to grasping the full potential and limitations of modern AI language models. 
If youâ€™re just getting started with large language models or looking to get more out of your current LLM projects, weâ€™ll walk you through everything you need to know about fine-tuning methods on Amazon SageMaker AI. 
Pre-training 
Pre-training represents the foundation of LLM development. During this phase, models learn general language understanding and generation capabilities through exposure to massive amounts of text data. This process typically involves training from scratch on diverse datasets, often consisting of hundreds of billions of tokens drawn from books, articles, code repositories, webpages, and other public sources. 
Pre-training teaches the model broad linguistic and semantic patterns, such as grammar, context, world knowledge, reasoning, and token prediction, using self-supervised learning techniques like masked language modeling (for example, BERT) or causal language modeling (for example, GPT). At this stage, the model is not tailored to any specific downstream task but rather builds a general-purpose language representation that can be adapted later using fine-tuning or PEFT methods. 
Pre-training is highly resource-intensive, requiring substantial compute (often across thousands of GPUs or AWS Trainium chips), large-scale distributed training frameworks, and careful data curation to balance performance with bias, safety, and accuracy concerns. 
Continued pre-training (also known as domain-adaptive pre-training or intermediate pre-training) is the process of taking a pre-trained language model and further training it on domain-specific or task-relevant corpora before fine-tuning. Unlike full pre-training from scratch, this approach builds on the existing capabilities of a general-purpose model, allowing it to internalize new patterns, vocabulary, or context relevant to a specific domain. 
This step is particularly useful when the models must handle specialized terminology or unique syntax, particularly in fields like law, medicine, or finance. This approach is also essential when organizations need to align AI outputs with their internal documentation standards and proprietary knowledge bases. Additionally, it serves as an effective solution for addressing gaps in language or cultural representation by allowing focused training on underrepresented dialects, languages, or regional content. 
To learn more, refer to the following resources: 
 
 Pre-training genomic language models using AWS HealthOmics and Amazon SageMaker 
 Customize models in Amazon Bedrock with your own data using fine-tuning and continued pre-training 
 
Alignment methods for LLMs 
The alignment of LLMs represents a crucial step in making sure these powerful systems behave in accordance with human values and preferences. AWS provides comprehensive support for implementing various alignment techniques, each offering distinct approaches to achieving this goal. The following are the key approaches. 
Reinforcement Learning from Human Feedback 
Reinforcement Learning from Human Feedback (RLHF) is one of the most established approaches to model alignment. This method transforms human preferences into a learned reward signal that guides model behavior. The RLHF process consists of three distinct phases. First, we collect comparison data, where human annotators choose between different model outputs for the same prompt. This data forms the foundation for training a reward model, which learns to predict human preferences. Finally, we fine-tune the language model using Proximal Policy Optimization (PPO), optimizing it to maximize the predicted reward. 
Constitutional AI represents an innovative approach to alignment that reduces dependence on human feedback by enabling models to critique and improve their own outputs. This method involves training models to internalize specific principles or rules, then using these principles to guide generation and self-improvement. The reinforcement learning phase is similar to RLHF, except that pairs of responses are generated and evaluated by an AI model, as opposed to a human. 
To learn more, refer to the following resources: 
 
 Fine-tune large language models with reinforcement learning from human or AI feedback 
 Machine-learning improving your LLMs with RLHF on Amazon Sagemaker 
 High-quality human feedback for your generative AI applications from Amazon SageMaker Ground Truth Plus 
 
Direct Preference Optimization 
Direct Preference Optimization (DPO) is an alternative to RLHF, offering a more straightforward path to model alignment. DPO alleviates the need for explicit reward modeling and complex RL training loops, instead directly optimizing the modelâ€™s policy to align with human preferences through a modified supervised learning approach. 
The key innovation of DPO lies in its formulation of preference learning as a classification problem. Given pairs of responses where one is preferred over the other, DPO trains the model to assign higher probability to preferred responses. This approach maintains theoretical connections to RLHF while significantly simplifying the implementation process. When implementing alignment methods, the effectiveness of DPO heavily depends on the quality, volume, and diversity of the preference dataset. Organizations must establish robust processes for collecting and validating human feedback while mitigating potential biases in label preferences. 
For more information about DPO, see Align Meta Llama 3 to human preferences with DPO Amazon SageMaker Studio and Amazon SageMaker Ground Truth. 
Fine-tuning methods on AWS 
Fine-tuning transforms a pre-trained model into one that excels at specific tasks or domains. This phase involves training the model on carefully curated datasets that represent the target use case. Fine-tuning can range from updating all model parameters to more efficient approaches that modify only a small subset of parameters. Amazon SageMaker HyperPod offers fine-tuning capabilities for supported foundation models (FMs), and Amazon SageMaker Model Training offers flexibility for custom fine-tuning implementations along with training the models at scale without the need to manage infrastructure. 
At its core, fine-tuning is a transfer learning process where a modelâ€™s existing knowledge is refined and redirected toward specific tasks or domains. This process involves carefully balancing the preservation of the modelâ€™s general capabilities while incorporating new, specialized knowledge. 
Supervised Fine-Tuning 
Supervised Fine-Tuning (SFT) involves updating model parameters using a curated dataset of input-output pairs that reflect the desired behavior. SFT enables precise behavioral control and is particularly effective when the model needs to follow specific instructions, maintain tone, or deliver consistent output formats, making it ideal for applications requiring high reliability and compliance. In regulated industries like healthcare or finance, SFT is often used after continued pre-training, which exposes the model to large volumes of domain-specific text to build contextual understanding. Although continued pre-training helps the model internalize specialized language (such as clinical or legal terms), SFT teaches it how to perform specific tasks such as generating discharge summaries, filling documentation templates, or complying with institutional guidelines. Both steps are typically essential: continued pre-training makes sure the model understands the domain, and SFT makes sure it behaves as required.However, because it updates the full model, SFT requires more compute resources and careful dataset construction. The dataset preparation process requires careful curation and validation to make sure the model learns the intended patterns and avoids undesirable biases. 
For more details about SFT, refer to the following resources: 
 
 Supervised fine-tuning on SageMaker training jobs 
 SageMaker HyperPod recipes 
 
Parameter-Efficient Fine-Tuning 
Parameter-Efficient Fine-Tuning (PEFT) represents a significant advancement in model adaptation, helping organizations customize large models while dramatically reducing computational requirements and costs. The following table summarizes the different types of PEFT. 
 
  
   
   PEFT Type 
   AWS Service 
   How It Works 
   Benefits 
   
   
   LoRA 
   LoRA (Low-Rank Adaptation) 
   SageMaker Training (custom implementation) 
   Instead of updating all model parameters, LoRA injects trainable rank decomposition matrices into transformer layers, reducing trainable parameters 
   Memory efficient, cost-efficient, opens up possibility of adapting larger models 
   
   
   QLoRA (Quantized LoRA) 
   SageMaker Training (custom implementation) 
   Combines model quantization with LoRA, loading the base model in 4-bit precision while adapting it with trainable LoRA parameters 
   Further reduces memory requirements compared to standard LoRA 
   
   
   Prompt Tuning 
   Additive 
   SageMaker Training (custom implementation) 
   Prepends a small set of learnable prompt tokens to the input embeddings; only these tokens are trained 
   Lightweight and fast tuning, good for task-specific adaptation with minimal resources 
   
   
   P-Tuning 
   Additive 
   SageMaker Training (custom implementation) 
   Uses a deep prompt (tunable embedding vector passed through an MLP) instead of discrete tokens, enhancing expressiveness of prompts 
   More expressive than prompt tuning, effective in low-resource settings 
   
   
   Prefix Tuning 
   Additive 
   SageMaker Training (custom implementation) 
   Prepends trainable continuous vectors (prefixes) to the attention keys and values in every transformer layer, leaving the base model frozen 
   Effective for long-context tasks, avoids full model fine-tuning, and reduces compute needs 
   
  
 
The selection of a PEFT method significantly impacts the success of model adaptation. Each technique presents distinct advantages that make it particularly suitable for specific scenarios. In the following sections, we provide a comprehensive analysis of when to employ different PEFT approaches. 
Low-Rank Adaptation 
Low-Rank Adaptation (LoRA) excels in scenarios requiring substantial task-specific adaptation while maintaining reasonable computational efficiency. Itâ€™s particularly effective in the following use cases: 
 
 Domain adaptation for enterprise applications â€“ When adapting models to specialized industry vocabularies and conventions, such as legal, medical, or financial domains, LoRA provides sufficient capacity for learning domain-specific patterns while keeping training costs manageable. For instance, a healthcare provider might use LoRA to adapt a base model to medical terminology and clinical documentation standards. 
 Multi-language adaptation â€“ Organizations extending their models to new languages find LoRA particularly effective. It allows the model to learn language-specific nuances while preserving the base modelâ€™s general knowledge. For example, a global ecommerce platform might employ LoRA to adapt their customer service model to different regional languages and cultural contexts. 
 
To learn more, refer to the following resources: 
 
 Accelerating Mixtral MOE fine-tuning on Amazon SageMaker with QLoRA 
 Fast and cost-effective LLaMA 2 fine-tuning with AWS Trainium 
 PEFT fine tuning of Llama 3 on SageMaker HyperPod with AWS Trainium 
 Efficient and cost-effective multi-tenant LoRA serving with Amazon SageMaker 
 
Prompt tuning 
Prompt tuning is ideal in scenarios requiring lightweight, switchable task adaptations. With prompt tuning, you can store multiple prompt vectors for different tasks without modifying the model itself. A primary use case could be when different customers require slightly different versions of the same basic functionality: prompt tuning allows efficient switching between customer-specific behaviors without loading multiple model versions. Itâ€™s useful in the following scenarios: 
 
 Personalized customer interactions â€“ Companies offering software as a service (SaaS) platform with customer support or virtual assistants can use prompt tuning to personalize response behavior for different clients without retraining the model. Each clientâ€™s brand tone or service nuance can be encoded in prompt vectors. 
 Task switching in multi-tenant systems â€“ In systems where multiple natural language processing (NLP) tasks (for example, summarization, sentiment analysis, classification) need to be served from a single model, prompt tuning enables rapid task switching with minimal overhead. 
 
For more information, see Prompt tuning for causal language modeling. 
P-tuning 
P-tuning extends prompt tuning by representing prompts as continuous embeddings passed through a small trainable neural network (typically an MLP). Unlike prompt tuning, which directly learns token embeddings, P-tuning enables more expressive and non-linear prompt representations, making it suitable for complex tasks and smaller models. Itâ€™s useful in the following use cases: 
 
 Low-resource domain generalization â€“ A common use case includes low-resource settings where labeled data is limited, yet the task requires nuanced prompt conditioning to steer model behavior. For example, organizations operating in low-data regimes (such as niche scientific research or regional dialect processing) can use P-tuning to extract better task-specific performance without the need for large fine-tuning datasets. 
 
To learn more, see P-tuning. 
Prefix tuning 
Prefix tuning prepends trainable continuous vectors, also called prefixes, to the key-value pairs in each attention layer of a transformer, while keeping the base model frozen. This provides control over the modelâ€™s behavior without altering its internal weights. Prefix tuning excels in tasks that benefit from conditioning across long contexts, such as document-level summarization or dialogue modeling. It provides a powerful compromise between performance and efficiency, especially when serving multiple tasks or clients from a single frozen base model. Consider the following use case: 
 
 Dialogue systems â€“ Companies building dialogue systems with varied tones (for example, friendly vs. formal) can use prefix tuning to control the persona and coherence across multi-turn interactions without altering the base model. 
 
For more details, see Prefix tuning for conditional generation. 
LLM optimization 
LLM optimization represents a critical aspect of their development lifecycle, enabling more efficient training, reduced computational costs, and improved deployment flexibility. AWS provides a comprehensive suite of tools and techniques for implementing these optimizations effectively. 
Quantization 
Quantization is a process of mapping a large set of input values to a smaller set of output values. In digital signal processing and computing, it involves converting continuous values to discrete values and reducing the precision of numbers (for example, from 32-bit to 8-bit). In machine learning (ML), quantization is particularly important for deploying models on resource-constrained devices, because it can significantly reduce model size while maintaining acceptable performance. One of the most used techniques is Quantized Low-Rank Adaptation (QLoRA).QLoRA is an efficient fine-tuning technique for LLMs that combines quantization and LoRA approaches. It uses 4-bit quantization to reduce model memory usage while maintaining model weights in 4-bit precision during training and employs double quantization for further memory reduction. The technique integrates LoRA by adding trainable rank decomposition matrices and keeping adapter parameters in 16-bit precision, enabling PEFT. QLoRA offers significant benefits, including up to 75% reduced memory usage, the ability to fine-tune large models on consumer GPUs, performance comparable to full fine-tuning, and cost-effective training of LLMs. This has made it particularly popular in the open-source AI community because it makes working with LLMs more accessible to developers with limited computational resources. 
To learn more, refer to the following resources: 
 
 Interactively fine-tune Falcon-40B and other LLMs on Amazon SageMaker Studio notebooks using QLoRA 
 Fine-tune Llama 2 using QLoRA and deploy it on Amazon SageMaker with AWS Inferentia2 
 
Knowledge distillation 
Knowledge distillation is a groundbreaking model compression technique in the world of AI, where a smaller student model learns to emulate the sophisticated behavior of a larger teacher model. This innovative approach has revolutionized the way we deploy AI solutions in real-world applications, particularly where computational resources are limited. By learning not only from ground truth labels but also from the teacher modelâ€™s probability distributions, the student model can achieve remarkable performance while maintaining a significantly smaller footprint. This makes it invaluable for various practical applications, from powering AI features on mobile devices to enabling edge computing solutions and Internet of Things (IoT) implementations. The key feature of distillation lies in its ability to democratize AI deploymentâ€”making sophisticated AI capabilities accessible across different platforms without compromising too much on performance. With knowledge distillation, you can run real-time speech recognition on smartphones, implement computer vision systems in resource-constrained environments, optimize NLP tasks for faster inference, and more. 
For more information about knowledge distillation, refer to the following resources: 
 
 A guide to Amazon Bedrock Model Distillation (preview) 
 Use Llama 3.1 405B for synthetic data generation and distillation to fine-tune smaller models 
 
Mixed precision training 
Mixed precision training is a cutting-edge optimization technique in deep learning that balances computational efficiency with model accuracy. By intelligently combining different numerical precisionsâ€”primarily 32-bit (FP32) and 16-bit (FP16) floating-point formatsâ€”this approach revolutionizes how we train complex AI models. Its key feature is selective precision usage: maintaining critical operations in FP32 for stability while using FP16 for less sensitive calculations, resulting in a balance of performance and accuracy. This technique has become a game changer in the AI industry, enabling up to three times faster training speeds, a significantly reduced memory footprint, and lower power consumption. Itâ€™s particularly valuable for training resource-intensive models like LLMs and complex computer vision systems. For organizations using cloud computing and GPU-accelerated workloads, mixed precision training offers a practical solution to optimize hardware utilization while maintaining model quality. This approach has effectively democratized the training of large-scale AI models, making it more accessible and cost-effective for businesses and researchers alike. 
To learn more, refer to the following resources: 
 
 Mixed precision training with FP8 on P5 instances using Transformer Engine 
 Mixed precision training with half-precision data types using PyTorch FSDP 
 Efficiently train models with large sequence lengths using Amazon SageMaker model parallel 
 
Gradient accumulation 
Gradient accumulation is a powerful technique in deep learning that addresses the challenges of training large models with limited computational resources. Developers can simulate larger batch sizes by accumulating gradients over multiple smaller forward and backward passes before performing a weight update. Think of it as breaking down a large batch into smaller, more manageable mini batches while maintaining the effective training dynamics of the larger batch size. This method has become particularly valuable in scenarios where memory constraints would typically prevent training with optimal batch sizes, such as when working with LLMs or high-resolution image processing networks. By accumulating gradients across several iterations, developers can achieve the benefits of larger batch trainingâ€”including more stable updates and potentially faster convergenceâ€”without requiring the enormous memory footprint typically associated with such approaches. This technique has democratized the training of sophisticated AI models, making it possible for researchers and developers with limited GPU resources to work on cutting-edge deep learning projects that would otherwise be out of reach. For more information, see the following resources: 
 
 Efficiently fine-tune the ESM 2 protein language model with Amazon SageMaker 
 End-to-end LLM training on instance clusters with over 100 nodes using AWS Trainium 
 
Conclusion 
When fine-tuning ML models on AWS, you can choose the right tool for your specific needs. AWS provides a comprehensive suite of tools for data scientists, ML engineers, and business users to achieve their ML goals. AWS has built solutions to support various levels of ML sophistication, from simple SageMaker training jobs for FM fine-tuning to the power of SageMaker HyperPod for cutting-edge research. 
We invite you to explore these options, starting with what suits your current needs, and evolve your approach as those needs change. Your journey with AWS is just beginning, and weâ€™re here to support you every step of the way. 
 
About the authors 
Ilan Gleiser is a Principal GenAI Specialist at AWS on the WWSO Frameworks team, focusing on developing scalable generative AI architectures and optimizing foundation model training and inference. With a rich background in AI and machine learning, Ilan has published over 30 blog posts and delivered more than 100 machine learning and HPC prototypes globally over the last 5 years. Ilan holds a masterâ€™s degree in mathematical economics. 
Prashanth Ramaswamy is a Senior Deep Learning Architect at the AWS Generative AI Innovation Center, where he specializes in model customization and optimization. In his role, he works on fine-tuning, benchmarking, and optimizing models by using generative AI as well as traditional AI/ML solutions. He focuses on collaborating with Amazon customers to identify promising use cases and accelerate the impact of AI solutions to achieve key business outcomes. 
Deeksha Razdan is an Applied Scientist at the AWS Generative AI Innovation Center, where she specializes in model customization and optimization. Her work resolves around conducting research and developing generative AI solutions for various industries. She holds a masterâ€™s in computer science from UMass Amherst. Outside of work, Deeksha enjoys being in nature.
â€¢ Streamline machine learning workflows with SkyPilot on Amazon SageMaker HyperPod
  This post is co-written with Zhanghao Wu, co-creator of SkyPilot. 
The rapid advancement of generative AI and foundation models (FMs) has significantly increased computational resource requirements for machine learning (ML) workloads. Modern ML pipelines require efficient systems for distributing workloads across accelerated compute resources, while making sure developer productivity remains high. Organizations need infrastructure solutions that are not only powerful but also flexible, resilient, and straightforward to manage. 
SkyPilot is an open source framework that simplifies running ML workloads by providing a unified abstraction layer that helps ML engineers run their workloads on different compute resources without managing underlying infrastructure complexities. It offers a simple, high-level interface for provisioning resources, scheduling jobs, and managing distributed training across multiple nodes. 
Amazon SageMaker HyperPod is a purpose-built infrastructure to develop and deploy large-scale FMs. SageMaker HyperPod not only provides the flexibility to create and use your own software stack, but also provides optimal performance through same spine placement of instances, as well as built-in resiliency. Combining the resiliency of SageMaker HyperPod and the efficiency of SkyPilot provides a powerful framework to scale up your generative AI workloads. 
In this post, we share how SageMaker HyperPod, in collaboration with SkyPilot, is streamlining AI development workflows. This integration makes our advanced GPU infrastructure more accessible to ML engineers, enhancing productivity and resource utilization. 
Challenges of orchestrating machine learning workloads 
Kubernetes has become popular for ML workloads due to its scalability and rich open source tooling. SageMaker HyperPod orchestrated on Amazon Elastic Kubernetes Service (Amazon EKS) combines the power of Kubernetes with the resilient environment of SageMaker HyperPod designed for training large models. Amazon EKS support in SageMaker HyperPod strengthens resilience through deep health checks, automated node recovery, and job auto-resume capabilities, providing uninterrupted training for large-scale and long-running jobs. 
ML engineers transitioning from traditional VM or on-premises environments often face a steep learning curve. The complexity of Kubernetes manifests and cluster management can pose significant challenges, potentially slowing down development cycles and resource utilization. 
Furthermore, AI infrastructure teams faced the challenge of balancing the need for advanced management tools with the desire to provide a user-friendly experience for their ML engineers. They required a solution that could offer both high-level control and ease of use for day-to-day operations. 
SageMaker HyperPod with SkyPilot 
To address these challenges, we partnered with SkyPilot to showcase a solution that uses the strengths of both platforms. SageMaker HyperPod excels at managing the underlying compute resources and instances, providing the robust infrastructure necessary for demanding AI workloads. SkyPilot complements this by offering an intuitive layer for job management, interactive development, and team coordination. 
Through this partnership, we can offer our customers the best of both worlds: the powerful, scalable infrastructure of SageMaker HyperPod, combined with a user-friendly interface that significantly reduces the learning curve for ML engineers. For AI infrastructure teams, this integration provides advanced management capabilities while simplifying the experience for their ML engineers, creating a win-win situation for all stakeholders. 
SkyPilot helps AI teams run their workloads on different infrastructures with a unified high-level interface and powerful management of resources and jobs. An AI engineer can bring in their AI framework and specify the resource requirements for the job; SkyPilot will intelligently schedule the workloads on the best infrastructure: find the available GPUs, provision the GPU, run the job, and manage its lifecycle. 
 
Solution overview 
Implementing this solution is straightforward, whether youâ€™re working with existing SageMaker HyperPod clusters or setting up a new deployment. For existing clusters, you can connect using AWS Command Line Interface (AWS CLI) commands to update your kubeconfig and verify the setup. For new deployments, we guide you through setting up the API server, creating clusters, and configuring high-performance networking options like Elastic Fabric Adapter (EFA). 
The following diagram illustrates the solution architecture. 
 
In the following sections, we show how to run SkyPilot jobs for multi-node distributed training on SageMaker HyperPod. We go over the process of creating a SageMaker HyperPod cluster, installing SkyPilot, creating a SkyPilot cluster, and deploying a SkyPilot training job. 
Prerequisites 
You must have the following prerequisites: 
 
 An existing SageMaker HyperPod cluster with Amazon EKS (to create one, refer to Deploy Your HyperPod Cluster). You must provision a single ml.p5.48xlarge instance for the code samples in the following sections. 
 Access to the AWS CLI and kubectl command line tools. 
 A Python environment for installing SkyPilot. 
 
Create a SageMaker HyperPod cluster 
You can create an EKS cluster with a single AWS CloudFormation stack following the instructions in Using CloudFormation, configured with a virtual private cloud (VPC) and storage resources. 
To create and manage SageMaker HyperPod clusters, you can use either the AWS Management Console or AWS CLI. If you use the AWS CLI, specify the cluster configuration in a JSON file and choose the EKS cluster created from the CloudFormation stack as the orchestrator of the SageMaker HyperPod cluster. You then create the cluster worker nodes with NodeRecovery set to Automatic to enable automatic node recovery, and for OnStartDeepHealthChecks, add InstanceStress and InstanceConnectivity to enable deep health checks. See the following code: 
 
 cat &gt; cluster-config.json &lt;&lt; EOL
{
&nbsp;&nbsp; &nbsp;"ClusterName": "hp-cluster",
&nbsp;&nbsp; &nbsp;"Orchestrator": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"Eks": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"ClusterArn": "${EKS_CLUSTER_ARN}"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp;},
&nbsp;&nbsp; &nbsp;"InstanceGroups": [
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;{
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"InstanceGroupName": "worker-group-1",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"InstanceType": "ml.p5.48xlarge",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"InstanceCount": 2,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"LifeCycleConfig": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"SourceS3Uri": "s3://${BUCKET_NAME}",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"OnCreate": "on_create.sh"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;},
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"ExecutionRole": "${EXECUTION_ROLE}",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"ThreadsPerCore": 1,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"OnStartDeepHealthChecks": [
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"InstanceStress",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"InstanceConnectivity"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;],
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;},
&nbsp;&nbsp;....
&nbsp;&nbsp; &nbsp;],
&nbsp;&nbsp; &nbsp;"VpcConfig": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"SecurityGroupIds": [
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"$SECURITY_GROUP"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;],
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"Subnets": [
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"$SUBNET_ID"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;]
&nbsp;&nbsp; &nbsp;},
&nbsp;&nbsp; &nbsp;"ResilienceConfig": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"NodeRecovery": "Automatic"
&nbsp;&nbsp; &nbsp;}
}
EOL 
 
You can add InstanceStorageConfigs to provision and mount additional Amazon Elastic Block Store (Amazon EBS) volumes on SageMaker HyperPod nodes. 
To create the cluster using the SageMaker HyperPod APIs, run the following AWS CLI command: 
 
 aws sagemaker create-cluster \&nbsp;
--cli-input-json file://cluster-config.json 
 
You are now ready to set up SkyPilot on your SageMaker HyperPod cluster. 
Connect to your SageMaker HyperPod EKS cluster 
From your AWS CLI environment, run the aws eks update-kubeconfig command to update your local kube config file (located at ~/.kube/config) with the credentials and configuration needed to connect to your EKS cluster using the kubectl command (provide your specific EKS cluster name): 
aws eks update-kubeconfig --name $EKS_CLUSTER_NAME 
You can verify that you are connected to the EKS cluster by running the following command: 
kubectl config current-context 
Install SkyPilot with Kubernetes support 
Use the following code to install SkyPilot with Kubernetes support using pip: 
pip install skypilot[kubernetes] 
This installs the latest build of SkyPilot, which includes the necessary Kubernetes integrations. 
Verify SkyPilotâ€™s connection to the EKS cluster 
Check if SkyPilot can connect to your Kubernetes cluster: 
sky check k8s 
The output should look similar to the following code: 
 
 Checking credentials to enable clouds for SkyPilot.
Kubernetes: enabled [compute]

To enable a cloud, follow the hints above and rerun: sky check
If any problems remain, refer to detailed docs at: https://docs.skypilot.co/en/latest/getting-started/installation.html

ğŸ‰ Enabled clouds ğŸ‰
Kubernetes [compute]
Active context: arn:aws:eks:us-east-2:XXXXXXXXXXXXX:cluster/sagemaker-hyperpod-eks-cluster

Using SkyPilot API server: http://127.0.0.1:46580 
 
If this is your first time using SkyPilot with this Kubernetes cluster, you might see a prompt to create GPU labels for your nodes. Follow the instructions by running the following code: 
python -m sky.utils.kubernetes.gpu_labeler --context &lt;your-eks-context&gt; 
This script helps SkyPilot identify what GPU resources are available on each node in your cluster. The GPU labeling job might take a few minutes depending on the number of GPU resources in your cluster. 
Discover available GPUs in the cluster 
To see what GPU resources are available in your SageMaker HyperPod cluster, use the following code: 
sky show-gpus --cloud k8s 
This will list the available GPU types and their counts. We have two p5.48xlarge instances, each equipped with 8 NVIDIA H100 GPUs: 
 
  Kubernetes GPUs
GPU REQUESTABLE_QTY_PER_NODE TOTAL_GPUS TOTAL_FREE_GPUS
H100 1, 2, 4, 8 16 16

Kubernetes per node accelerator availability
NODE_NAME GPU_NAME TOTAL_GPUS FREE_GPUS
hyperpod-i-00baa178bc31afde3 H100 8 8
hyperpod-i-038beefa954efab84 H100 8 8 
 
Launch an interactive development environment 
With SkyPilot, you can launch a SkyPilot cluster for interactive development: 
sky launch -c dev --gpus H100 
This command creates an interactive development environment (IDE) with a single H100 GPU and will sync the local working directory to the cluster. SkyPilot handles the pod creation, resource allocation, and setup of the IDE. 
 
 Considered resources (1 node):
-------------------------------------------------------------------------------------------------------------------------------------------------------------------
&nbsp;CLOUD &nbsp; &nbsp; &nbsp; &nbsp;INSTANCE &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;vCPUs &nbsp; Mem(GB) &nbsp; ACCELERATORS &nbsp; REGION/ZONE &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; COST ($) &nbsp; CHOSEN &nbsp; 
-------------------------------------------------------------------------------------------------------------------------------------------------------------------
&nbsp;Kubernetes &nbsp; 2CPU--8GB--H100:1&nbsp;&nbsp; 2&nbsp;&nbsp; &nbsp; &nbsp; 8&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; H100:1&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; arn:aws:eks:us-east-2:XXXXXXXXXX:cluster/sagemaker-hyperpod-eks-cluster &nbsp; 0.00&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;âœ” &nbsp; &nbsp; 
------------------------------------------------------------------------------------------------------------------------------------------------------------------
Launching a new cluster 'dev'. Proceed? [Y/n]: Y
â€¢ Launching on Kubernetes.
Pod is up.
âœ” Cluster launched: dev. View logs: sky api logs -1&nbsp;sky-2025-05-05-15-28-47-523797/provision. log
â€¢ Syncing files.
Run commands not specified or empty.
Useful Commands
Cluster name: dey
To log into the head VM: &nbsp; ssh dev
To submit a job: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; sky exec dev yaml_file
To stop the cluster: &nbsp; &nbsp; &nbsp; sky stop dev
To teardown the cluster: &nbsp; sky down dev 
 
After itâ€™s launched, you can connect to your IDE: 
ssh dev 
This gives you an interactive shell in your IDE, where you can run your code, install packages, and perform ML experiments. 
Run training jobs 
With SkyPilot, you can run distributed training jobs on your SageMaker HyperPod cluster. The following is an example of launching a distributed training job using a YAML configuration file. 
First, create a file named train.yaml with your training job configuration: 
 
 resources:
&nbsp;&nbsp; &nbsp;accelerators: H100

num_nodes: 1

setup: |
&nbsp;&nbsp; &nbsp;git clone --depth 1 https://github.com/pytorch/examples || true
&nbsp;&nbsp; &nbsp;cd examples
&nbsp;&nbsp; &nbsp;git filter-branch --prune-empty --subdirectory-filter distributed/minGPT-ddp
&nbsp;&nbsp; &nbsp;# SkyPilot's default image on AWS/GCP has CUDA 11.6 (Azure 11.5).
&nbsp;&nbsp; &nbsp;uv venv --python 3.10
&nbsp;&nbsp; &nbsp;source .venv/bin/activate
&nbsp;&nbsp; &nbsp;uv pip install -r requirements.txt "numpy&lt;2" "torch"

run: |
&nbsp;&nbsp; &nbsp;cd examples
&nbsp;&nbsp; &nbsp;source .venv/bin/activate
&nbsp;&nbsp; &nbsp;cd mingpt
&nbsp;&nbsp; &nbsp;export LOGLEVEL=INFO

&nbsp;&nbsp; &nbsp;MASTER_ADDR=$(echo "$SKYPILOT_NODE_IPS" | head -n1)
&nbsp;&nbsp; &nbsp;echo "Starting distributed training, head node: $MASTER_ADDR"

&nbsp;&nbsp; &nbsp;torchrun \
&nbsp;&nbsp; &nbsp;--nnodes=$SKYPILOT_NUM_NODES \
&nbsp;&nbsp; &nbsp;--nproc_per_node=$SKYPILOT_NUM_GPUS_PER_NODE \
&nbsp;&nbsp; &nbsp;--master_addr=$MASTER_ADDR \
&nbsp;&nbsp; &nbsp;--master_port=8008 \
&nbsp;&nbsp; &nbsp;--node_rank=${SKYPILOT_NODE_RANK} \
&nbsp;&nbsp; &nbsp;main.py 
 
Then launch your training job: 
sky launch -c train train.yaml 
This creates a training job on a single p5.48xlarge nodes, equipped with 8 H100 NVIDIA GPUs. You can monitor the output with the following command: 
sky logs train 
Running multi-node training jobs with EFA 
Elastic Fabric Adapter (EFA) is a network interface for Amazon Elastic Compute Cloud (Amazon EC2) instances that enables you to run applications requiring high levels of inter-node communications at scale on AWS through its custom-built operating system bypass hardware interface. This enables applications to communicate directly with the network hardware while bypassing the operating system kernel, significantly reducing latency and CPU overhead. This direct hardware access is particularly beneficial for distributed ML workloads where frequent inter-node communication during gradient synchronization can become a bottleneck. By using EFA-enabled instances such as p5.48xlarge or p6-b200.48xlarge, data scientists can scale their training jobs across multiple nodes while maintaining the low-latency, high-bandwidth communication essential for efficient distributed training, ultimately reducing training time and improving resource utilization for large-scale AI workloads. 
The following code snippet shows how to incorporate this into your SkyPilot job: 
 
 name: nccl-test-efa

resources:
&nbsp;&nbsp;cloud: kubernetes
&nbsp;&nbsp;accelerators:&nbsp;H100:8
&nbsp;&nbsp;image_id: docker:public.ecr.aws/hpc-cloud/nccl-tests:latest

num_nodes: 2

envs:
&nbsp;&nbsp;USE_EFA: "true"

run: |
&nbsp;&nbsp;if [ "${SKYPILOT_NODE_RANK}" == "0" ]; then
&nbsp;&nbsp; &nbsp;echo "Head node"

&nbsp;&nbsp; &nbsp;# Total number of processes, NP should be the total number of GPUs in the cluster
&nbsp;&nbsp; &nbsp;NP=$(($SKYPILOT_NUM_GPUS_PER_NODE * $SKYPILOT_NUM_NODES))

&nbsp;&nbsp; &nbsp;# Append :${SKYPILOT_NUM_GPUS_PER_NODE} to each IP as slots
&nbsp;&nbsp; &nbsp;nodes=""
&nbsp;&nbsp; &nbsp;for ip in $SKYPILOT_NODE_IPS; do
&nbsp;&nbsp; &nbsp; &nbsp;nodes="${nodes}${ip}:${SKYPILOT_NUM_GPUS_PER_NODE},"
&nbsp;&nbsp; &nbsp;done
&nbsp;&nbsp; &nbsp;nodes=${nodes::-1}
&nbsp;&nbsp; &nbsp;echo "All nodes: ${nodes}"

&nbsp;&nbsp; &nbsp;# Set environment variables
&nbsp;&nbsp; &nbsp;export PATH=$PATH:/usr/local/cuda-12.2/bin:/opt/amazon/efa/bin:/usr/bin
&nbsp;&nbsp; &nbsp;export LD_LIBRARY_PATH=/usr/local/cuda-12.2/lib64:/opt/amazon/openmpi/lib:/opt/nccl/build/lib:/opt/amazon/efa/lib:/opt/aws-ofi-nccl/install/lib:/usr/local/nvidia/lib:$LD_LIBRARY_PATH
&nbsp;&nbsp; &nbsp;export NCCL_HOME=/opt/nccl
&nbsp;&nbsp; &nbsp;export CUDA_HOME=/usr/local/cuda-12.2
&nbsp;&nbsp; &nbsp;export NCCL_DEBUG=INFO
&nbsp;&nbsp; &nbsp;export NCCL_BUFFSIZE=8388608
&nbsp;&nbsp; &nbsp;export NCCL_P2P_NET_CHUNKSIZE=524288
&nbsp;&nbsp; &nbsp;export NCCL_TUNER_PLUGIN=/opt/aws-ofi-nccl/install/lib/libnccl-ofi-tuner.so

&nbsp;&nbsp; &nbsp;if [ "${USE_EFA}" == "true" ]; then
&nbsp;&nbsp; &nbsp; &nbsp;export FI_PROVIDER="efa"
&nbsp;&nbsp; &nbsp;else
&nbsp;&nbsp; &nbsp; &nbsp;export FI_PROVIDER=""
&nbsp;&nbsp; &nbsp;fi

&nbsp;&nbsp; &nbsp;/opt/amazon/openmpi/bin/mpirun \
&nbsp;&nbsp; &nbsp; &nbsp;--allow-run-as-root \
&nbsp;&nbsp; &nbsp; &nbsp;--tag-output \
&nbsp;&nbsp; &nbsp; &nbsp;-H $nodes \
&nbsp;&nbsp; &nbsp; &nbsp;-np $NP \
&nbsp;&nbsp; &nbsp; &nbsp;-N $SKYPILOT_NUM_GPUS_PER_NODE \
&nbsp;&nbsp; &nbsp; &nbsp;--bind-to none \
&nbsp;&nbsp; &nbsp; &nbsp;-x FI_PROVIDER \
&nbsp;&nbsp; &nbsp; &nbsp;-x PATH \
&nbsp;&nbsp; &nbsp; &nbsp;-x LD_LIBRARY_PATH \
&nbsp;&nbsp; &nbsp; &nbsp;-x NCCL_DEBUG=INFO \
&nbsp;&nbsp; &nbsp; &nbsp;-x NCCL_BUFFSIZE \
&nbsp;&nbsp; &nbsp; &nbsp;-x NCCL_P2P_NET_CHUNKSIZE \
&nbsp;&nbsp; &nbsp; &nbsp;-x NCCL_TUNER_PLUGIN \
&nbsp;&nbsp; &nbsp; &nbsp;--mca pml ^cm,ucx \
&nbsp;&nbsp; &nbsp; &nbsp;--mca btl tcp,self \
&nbsp;&nbsp; &nbsp; &nbsp;--mca btl_tcp_if_exclude lo,docker0,veth_def_agent \
&nbsp;&nbsp; &nbsp; &nbsp;/opt/nccl-tests/build/all_reduce_perf \
&nbsp;&nbsp; &nbsp; &nbsp;-b 8 \
&nbsp;&nbsp; &nbsp; &nbsp;-e 2G \
&nbsp;&nbsp; &nbsp; &nbsp;-f 2 \
&nbsp;&nbsp; &nbsp; &nbsp;-g 1 \
&nbsp;&nbsp; &nbsp; &nbsp;-c 5 \
&nbsp;&nbsp; &nbsp; &nbsp;-w 5 \
&nbsp;&nbsp; &nbsp; &nbsp;-n 100
&nbsp;&nbsp;else
&nbsp;&nbsp; &nbsp;echo "Worker nodes"
&nbsp;&nbsp;fi

config:
&nbsp;&nbsp;kubernetes:
&nbsp;&nbsp; &nbsp;pod_config:
&nbsp;&nbsp; &nbsp; &nbsp;spec:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;containers:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;- resources:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;limits:
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;vpc.amazonaws.com/efa:&nbsp;32
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;requests:
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;vpc.amazonaws.com/efa:&nbsp;32 
 
Clean up 
To delete your SkyPilot cluster, run the following command: 
sky down &lt;cluster_name&gt; 
To delete the SageMaker HyperPod cluster created in this post, you can user either the SageMaker AI console or the following AWS CLI command: 
aws sagemaker delete-cluster --cluster-name &lt;cluster_name&gt; 
Cluster deletion will take a few minutes. You can confirm successful deletion after you see no clusters on the SageMaker AI console. 
If you used the CloudFormation stack to create resources, you can delete it using the following command: 
aws cloudformation delete-stack --stack-name &lt;stack_name&gt; 
Conclusion 
By combining the robust infrastructure capabilities of SageMaker HyperPod with SkyPilotâ€™s user-friendly interface, weâ€™ve showcased a solution that helps teams focus on innovation rather than infrastructure complexity. This approach not only simplifies operations but also enhances productivity and resource utilization across organizations of all sizes. To get started, refer to SkyPilot in the Amazon EKS Support in Amazon SageMaker HyperPod workshop. 
 
About the authors 
Roy Allela is a Senior AI/ML Specialist Solutions Architect at AWS. He helps AWS customersâ€”from small startups to large enterprisesâ€”train and deploy foundation models efficiently on AWS. He is passionate about computational optimization problems and improving the performance of AI workloads. 
Zhanghao Wu is a co-creator of the SkyPilot open source project and holds a PhD in computer science from UC Berkeley. He works on SkyPilot core, client-server architecture, managed jobs, and improving the AI experience on diverse cloud infrastructure in general. 
Ankit Anand is a Senior Foundation Models Go-To-Market (GTM) Specialist at AWS. He partners with top generative AI model builders, strategic customers, and AWS service teams to enable the next generation of AI/ML workloads on AWS. Ankitâ€™s experience includes product management expertise within the financial services industry for high-frequency and low-latency trading and business development for Amazon Alexa.
â€¢ Intelligent document processing at scale with generative AI and Amazon Bedrock Data Automation
  Extracting information from unstructured documents at scale is a recurring business task. Common use cases include creating product feature tables from descriptions, extracting metadata from documents, and analyzing legal contracts, customer reviews, news articles, and more. A classic approach to extracting information from text is named entity recognition (NER). NER identifies entities from predefined categories, such as persons and organizations. Although various AI services and solutions support NER, this approach is limited to text documents and only supports a fixed set of entities. Furthermore, classic NER models canâ€™t handle other data types such as numeric scores (such as sentiment) or free-form text (such as summary). Generative AI unlocks these possibilities without costly data annotation or model training, enabling more comprehensive intelligent document processing (IDP). 
AWS recently announced the general availability of Amazon Bedrock Data Automation, a feature of Amazon Bedrock that automates the generation of valuable insights from unstructured multimodal content such as documents, images, video, and audio. This service offers pre-built capabilities for IDP and information extraction through a unified API, alleviating the need for complex prompt engineering or fine-tuning, and making it an excellent choice for document processing workflows at scale. To learn more about Amazon Bedrock Data Automation, refer to Simplify multimodal generative AI with Amazon Bedrock Data Automation. 
Amazon Bedrock Data Automation is the recommended approach for IDP use case due to its simplicity, industry-leading accuracy, and managed service capabilities. It handles the complexity of document parsing, context management, and model selection automatically, so developers can focus on their business logic rather than IDP implementation details. 
Although Amazon Bedrock Data Automation meets most IDP needs, some organizations require additional customization in their IDP pipelines. For example, companies might need to use self-hosted foundation models (FMs) for IDP due to regulatory requirements. Some customers have builder teams who might prefer to maintain full control over the IDP pipeline instead of using a managed service. Finally, organizations might operate in AWS Regions where Amazon Bedrock Data Automation is not available (available in us-west-2 and us-east-1 as of June 2025). In such cases, builders might use Amazon Bedrock FMs directly or perform optical character recognition (OCR) with Amazon Textract. 
This post presents an end-to-end IDP application powered by Amazon Bedrock Data Automation and other AWS services. It provides a reusable AWS infrastructure as code (IaC) that deploys an IDP pipeline and provides an intuitive UI for transforming documents into structured tables at scale. The application only requires the user to provide the input documents (such as contracts or emails) and a list of attributes to be extracted. It then performs IDP with generative AI. 
The application code and deployment instructions are available on GitHub under the MIT license. 
Solution overview 
The IDP solution presented in this post is deployed as IaC using the AWS Cloud Development Kit (AWS CDK). Amazon Bedrock Data Automation serves as the primary engine for information extraction. For cases requiring further customization, the solution also provides alternative processing paths using Amazon Bedrock FMs and Amazon Textract integration. 
We use AWS Step Functions to orchestrate the IDP workflow and parallelize processing for multiple documents. As part of the workflow, we use AWS Lambda functions to call Amazon Bedrock Data Automation or Amazon Textract and Amazon Bedrock (depending on the selected parsing mode). Processed documents and extracted attributes are stored in Amazon Simple Storage Service (Amazon S3). 
A Step Functions workflow with the business logic is invoked through an API call performed using an AWS SDK. We also build a containerized web application running on Amazon Elastic Container Service (Amazon ECS) that is available to end-users through Amazon CloudFront to simplify their interaction with the solution. We use Amazon Cognito for authentication and secure access to the APIs. 
The following diagram illustrates the architecture and workflow of the IDP solution. 
 
The IDP workflow includes the following steps: 
 
 A user logs in to the web application using credentials managed by Amazon Cognito, selects input documents, and defines the fields to be extracted from them in the UI. Optionally, the user can specify the parsing mode, LLM to use, and other settings. 
 The user starts the IDP pipeline. 
 The application creates a pre-signed S3 URL for the documents and uploads them to Amazon S3. 
 The application triggers Step Functions to start the state machine with the S3 URIs and IDP settings as inputs. The Map state starts to process the documents concurrently. 
 Depending on the document type and the parsing mode, it branches to different Lambda functions that perform IDP, save results to Amazon S3, and send them back to the UI: 
   
   Amazon Bedrock Data Automation â€“ Documents are directed to the â€œRun Data Automationâ€ Lambda function. The Lambda function creates a blueprint with the user-defined fields schema and launches an asynchronous Amazon Bedrock Data Automation job. Amazon Bedrock Data Automation handles the complexity of document processing and attribute extraction using optimized prompts and models. When the job results are ready, theyâ€™re saved to Amazon S3 and sent back to the UI. This approach provides the best balance of accuracy, ease of use, and scalability for most IDP use cases. 
   Amazon Textract â€“ If the user specifies Amazon Textract as a parsing mode, the IDP pipeline splits into two steps. First, the â€œPerform OCRâ€ Lambda function is invoked to run an asynchronous document analysis job. The OCR outputs are processed using the amazon-textract-textractor library and formatted as Markdown. Second, the text is passed to the â€œExtract attributesâ€ Lambda function (Step 6), which invokes an Amazon Bedrock FM given the text and the attributes schema. The outputs are saved to Amazon S3 and sent to the UI. 
   Handling office documents â€“ Documents with suffixes like .doc, .ppt, and .xls are processed by the â€œParse officeâ€ Lambda function, which uses LangChain document loaders to extract the text content. The outputs are passed to the â€œExtract attributesâ€ Lambda function (Step 6) to proceed with the IDP pipeline. 
    
 If the user chooses an Amazon Bedrock FM for IDP, the document is sent to the â€œExtract attributesâ€ Lambda function. It converts a document into a set of images, which are sent to a multimodal FM with the attributes schema as part of a custom prompt. It parses the LLM response to extract JSON outputs, saves them to Amazon S3, and sends it back to the UI. This flow supports .pdf, .png, and .jpg documents. 
 The web application checks the state machine execution results periodically and returns the extracted attributes to the user when they are available. 
 
Prerequisites 
You can deploy the IDP solution from your local computer or from an Amazon SageMaker notebook instance. The deployment steps are detailed in the solution README file. 
If you choose to deploy using a SageMaker notebook, which is recommended, you will need access to an AWS account with permissions to create and launch a SageMaker notebook instance. 
Deploy the solution 
To deploy the solution to your AWS account, complete the following steps: 
 
 Open the AWS Management Console and choose the Region in which you want to deploy the IDP solution. 
 Launch a SageMaker notebook instance. Provide the notebook instance name and notebook instance type, which you can set to ml.m5.large. Leave other options as default. 
 Navigate to the Notebook instance and open the IAM role attached tothe notebook. Open the role on the AWS Identity and Access Management (IAM) console. 
 Attach an inline policy to the role and insert the following policy JSON: 
 
 
 {
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "cloudformation:*",
        "s3:*",
        "iam:*",
        "sts:AssumeRole"
      ],
      "Resource": "*"
    },
    {
      "Effect": "Allow",
      "Action": [
        "ssm:GetParameter",
        "ssm:GetParameters"
      ],
      "Resource": "arn:aws:ssm:*:*:parameter/cdk-bootstrap/*"
    }
  ]
}
 
 
 
 When the notebook instance status is marked as InService, choose Open JupyterLab. 
 In the JupyterLab environment, choose File, New, and Terminal. 
 Clone the solution repository by running the following commands: 
 
 
 cd SageMaker
git clone https://github.com/aws-samples/intelligent-document-processing-with-amazon-bedrock.git 
 
 
 Navigate to the repository folder and run the script to install requirements: 
 
 
 cd intelligent-document-processing-with-amazon-bedrock
sh install_deps.sh 
 
 
 Run the script to create a virtual environment and install dependencies: 
 
 
 sh install_env.sh
source .venv/bin/activate 
 
 
 Within the repository folder, copy the config-example.yml to a config.yml to specify your stack name. Optionally, configure the services and indicate the modules you want to deploy (for example, to disable deploying a UI, change deploy_streamlit to False). Make sure you add your user email to the Amazon Cognito users list. 
 Configure Amazon Bedrock model access by opening the Amazon Bedrock console in the Region specified in the config.yml file. In the navigation pane, choose Model Access and make sure to enable access for the model IDs specified in config.yml. 
 Bootstrap and deploy the AWS CDK in your account: 
 
 
 cdk bootstrap
cdk deploy 
 
Note that this step may take some time, especially on the first deployment. Once deployment is complete, you should see the message as shown in the following screenshot. You can access the Streamlit frontend using the CloudFront distribution URL provided in the AWS CloudFormation outputs. The temporary login credentials will be sent to the email specified in config.yml during the deployment. 
 
Using the solution 
This section guides you through two examples to showcase the IDP capabilities. 
Example 1: Analyzing financial documents 
In this scenario, we extract key features from a multi-page financial statement using Amazon Bedrock Data Automation. We use a sample document in PDF format with a mixture of tables, images, and text, and extract several financial metrics. Complete the following steps: 
 
 Upload a document by attaching a file through the solution UI. 
 
 
 
 On the Describe Attributes tab, either manually list the names and descriptions of the attributes or upload these fields in JSON format. We want to find the following metrics: 
   
   Current cash in assets in 2018 
   Current cash in assets in 2019 
   Operating profit in 2018 
   Operating profit in 2019 
    
 
 
 
 Choose Extract attributes to start the IDP pipeline. 
 
The provided attributes are integrated into a custom blueprint with the inferred attributes list, which is then used to invoke a data automation job on the uploaded documents. 
After the IDP pipeline is complete, you will see a table of results in the UI. It includes an index for each document in the _doc column, a column for each of the attributes you defined, and a file_name column that contains the document name. 
 
From the following statement excerpts, we can see that Amazon Bedrock Data Automation was able to correctly extract the values for current assets and operating profit. 
 
 
The IDP solution is also able to do complex calculations beyond well-defined entities. Letâ€™s say we want to calculate the following accounting metrics: 
 
 Liquidity ratios (Current assets/Current liabilities) 
 Working capitals (Current assets â€“ Current liabilities) 
 Revenue increase ((Revenue year 2/Revenue year 1) â€“ 1) 
 
We define the attributes and their formulas as parts of the attributesâ€™ schema. This time, we choose an Amazon Bedrock LLM as a parsing mode to demonstrate how the application can use a multimodal FM for IDP. When using an Amazon Bedrock LLM, starting the IDP pipeline will now combine the attributes and their description into a custom prompt template, which is sent to the LLM with the documents converted to images. As a user, you can specify the LLM powering the extraction and its inference parameters, such as temperature. 
 
The output, including the full results, is shown in the following screenshot. 
 
 
Example 2: Processing customer emails 
In this scenario, we want to extract multiple features from a list of emails with customer complaints due to delays in product shipments using Amazon Bedrock Data Automation. For each email, we want to find the following: 
 
 Customer name 
 Shipment ID 
 Email language 
 Email sentiment 
 Shipment delay (in days) 
 Summary of issue 
 Suggested response 
 
Complete the following steps: 
 
 Upload input emails as .txt files. You can download sample emails from GitHub. 
 
 
 
 On the Describe Attributes tab, list names and descriptions of the attributes. 
 
 
You can add few-shot examples for some fields (such as delay) to explain to the LLM how these fields values should be extracted. You can do this by adding an example input and the expected output for the attribute to the description. 
 
 Choose Extract attributes to start the IDP pipeline. 
 
The provided attributes and their descriptions will be integrated into a custom blueprint with the inferred attributes list, which is then used to invoke a data automation job on the uploaded documents. When the IDP pipeline is complete, you will see the results. 
 
The application allows downloading the extraction results as a CSV or a JSON file. This makes it straightforward to use the results for downstream tasks, such as aggregating customer sentiment scores. 
Pricing 
In this section, we calculate cost estimates for performing IDP on AWS with our solution. 
Amazon Bedrock Data Automation provides a transparent pricing schema depending on the input document size (number of pages, images, or minutes). When using Amazon Bedrock FMs, pricing depends on the number of input and output tokens used as part of the information extraction call. Finally, when using Amazon Textract, OCR is performed and priced separately based on the number of pages in the documents. 
Using the preceding scenarios as examples, we can approximate the costs depending on the selected parsing mode. In the following table, we show costs using two datasets: 100 20-page financial documents, and 100 1-page customer emails. We ignore costs of Amazon ECS and Lambda. 
 
  
   
   AWS service 
    Use case 1 (100 20-page financial documents) 
    Use case 2 (100 1-page customer emails) 
   
   
   IDP option 1: Amazon Bedrock Data Automation 
   
   
   Amazon Bedrock Data Automation (custom output) 
   $20.00 
   $1.00 
   
   
   IDP option 2: Amazon Bedrock FM 
   
   
   Amazon Bedrock (FM invocation, Anthropicâ€™s Claude 4 Sonnet) 
   $1.79 
   $0.09 
   
   
   IDP option 3: Amazon Textract and Amazon Bedrock FM 
   
   
   Amazon Textract (document analysis job with layout) 
   $30.00 
   $1.50 
   
   
   Amazon Bedrock (FM invocation, Anthropicâ€™s Claude 3.7 Sonnet) 
   $1.25 
   $0.06 
   
   
   Orchestration and storage (shared costs) 
   
   
   Amazon S3 
   $0.02 
   $0.02 
   
   
   AWS CloudFront 
   $0.09 
   $0.09 
   
   
   Amazon ECS 
   â€“ 
   â€“ 
   
   
   AWS Lambda 
   â€“ 
   â€“ 
   
   
   Total cost: Amazon Bedrock Data Automation 
   $20.11 
   $1.11 
   
   
   Total cost: Amazon Bedrock FM 
   $1.90 
   $0.20 
   
   
   Total cost: Amazon Textract and Amazon Bedrock FM 
   $31.36 
   $1.67 
   
  
 
The cost analysis suggests that using Amazon Bedrock FMs with a custom prompt template is a cost-effective method for IDP. However, this approach requires a bigger operational overhead, because the pipeline needs to be optimized depending on the LLM, and requires manual security and privacy management. Amazon Bedrock Data Automation offers a managed service that uses a choice of high-performing FMs through a single API. 
Clean up 
To remove the deployed resources, complete the following steps: 
 
 On the AWS CloudFormation console, delete the created stack. Alternatively, run the following command: 
   
   cdk destroy --region &lt;YOUR_DEPLOY_REGION&gt; 
    
 On the Amazon Cognito console, delete the user pool. 
 
Conclusion 
Extracting information from unstructured documents at scale is a recurring business task. This post discussed an end-to-end IDP application that performs information extraction using multiple AWS services. The solution is powered by Amazon Bedrock Data Automation, which provides a fully managed service for generating insights from documents, images, audio, and video. Amazon Bedrock Data Automation handles the complexity of document processing and information extraction, optimizing for both performance and accuracy without requiring expertise in prompt engineering. For extended flexibility and customizability in specific scenarios, our solution also supports IDP using Amazon Bedrock custom LLM calls and Amazon Textract for OCR. 
The solution supports multiple document types, including text, images, PDF, and Microsoft Office documents. At the time of writing, accurate understanding of information in documents rich with images, tables, and other visual elements is only available for PDF and images. We recommend converting complex Office documents to PDFs or images for best performance. Another solution limitation is the document size. As of June 2025, Amazon Bedrock Data Automation supports documents up to 20 pages for custom attributes extraction. When using custom Amazon Bedrock LLMs for IDP, the 300,000-token context window of Amazon Nova LLMs allows processing documents with up to roughly 225,000 words. To extract information from larger documents, you would currently need to split the file into multiple documents. 
In the next versions of the IDP solution, we plan to keep adding support for state-of-the-art language models available through Amazon Bedrock and iterate on prompt engineering to further improve the extraction accuracy. We also plan to implement techniques for extending the size of supported documents and providing users with a precise indication of where exactly in the document the extracted information is coming from. 
To get started with IDP with the described solution, refer to the GitHub repository. To learn more about Amazon Bedrock, refer to the documentation. 
 
About the authors 
Nikita Kozodoi, PhD, is a Senior Applied Scientist at the AWS Generative AI Innovation Center, where he works on the frontier of AI research and business. With rich experience in Generative AI and diverse areas of ML, Nikita is enthusiastic about using AI to solve challenging real-world business problems across industries. 
Zainab Afolabi is a Senior Data Scientist at the Generative AI Innovation Centre in London, where she leverages her extensive expertise to develop transformative AI solutions across diverse industries. She has over eight years of specialised experience in artificial intelligence and machine learning, as well as a passion for translating complex technical concepts into practical business applications. 
Aiham Taleb, PhD, is a Senior Applied Scientist at the Generative AI Innovation Center, working directly with AWS enterprise customers to leverage Gen AI across several high-impact use cases. Aiham has a PhD in unsupervised representation learning, and has industry experience that spans across various machine learning applications, including computer vision, natural language processing, and medical imaging. 
Liza (Elizaveta) Zinovyeva is an Applied Scientist at AWS Generative AI Innovation Center and is based in Berlin. She helps customers across different industries to integrate Generative AI into their existing applications and workflows. She is passionate about AI/ML, finance and software security topics. In her spare time, she enjoys spending time with her family, sports, learning new technologies, and table quizzes. 
Nuno Castro is a Sr. Applied Science Manager at AWS Generative AI Innovation Center. He leads Generative AI customer engagements, helping hundreds of AWS customers find the most impactful use case from ideation, prototype through to production. He has 19 years experience in AI in industries such as finance, manufacturing, and travel, leading AI/ML teams for 12 years. 
Ozioma Uzoegwu is a Principal Solutions Architect at Amazon Web Services. In his role, he helps financial services customers across EMEA to transform and modernize on the AWS Cloud, providing architectural guidance and industry best practices. Ozioma has many years of experience with web development, architecture, cloud and IT management. Prior to joining AWS, Ozioma worked with an AWS Advanced Consulting Partner as the Lead Architect for the AWS Practice. He is passionate about using latest technologies to build a modern financial services IT estate across banking, payment, insurance and capital markets. 
Eren Tuncer is a Solutions Architect at Amazon Web Services focused on Serverless and building Generative AI applications. With more than fifteen years experience in software development and architecture, he helps customers across various industries achieve their business goals using cloud technologies with best practices. As a builder, heâ€™s passionate about creating solutions with state-of-the-art technologies, sharing knowledge, and helping organizations navigate cloud adoption. 
Francesco Cerizzi is a Solutions Architect at Amazon Web Services exploring tech frontiers while spreading generative AI knowledge and building applications. With a background as a full stack developer, he helps customers across different industries in their journey to the cloud, sharing insights on AIâ€™s transformative potential along the way. Heâ€™s passionate about Serverless, event-driven architectures, and microservices in general. When not diving into technology, heâ€™s a huge F1 fan and loves Tennis.
â€¢ Build a conversational data assistant, Part 2 â€“ Embedding generative business intelligence with Amazon Q in QuickSight
  In Part 1 of this series, we explored how Amazonâ€™s Worldwide Returns &amp; ReCommerce (WWRR) organization built the Returns &amp; ReCommerce Data Assist (RRDA)â€”a generative AI solution that transforms natural language questions into validated SQL queries using Amazon Bedrock Agents. Although this capability improves data access for technical users, the WWRR organizationâ€™s journey toward truly democratized data doesnâ€™t end there. 
For many stakeholders across WWRR, visualizing trends and patterns is far more valuable than working with raw data. These users need quick insights to drive decisions without having to interpret SQL results. Although we maintain pre-built Amazon QuickSight dashboards for commonly tracked metrics, business users frequently require support for long-tail analyticsâ€”the ability to conduct deep dives into specific problems, anomalies, or regional variations not covered by standard reports. 
To bridge this gap, we extended our RRDA solution beyond SQL generation to include visualization capabilities. In this post, we dive into how we integrated Amazon Q in QuickSight to transform natural language requests like â€œShow me how many items were returned in the US over the past 6 monthsâ€ into meaningful data visualizations. We demonstrate how combining Amazon Bedrock Agents with Amazon Q in QuickSight creates a comprehensive data assistant that delivers both SQL code and visual insights through a single, intuitive conversational interfaceâ€”democratizing data access across the enterprise. 
Intent and domain classification for visual analytics 
The overall architecture diagram of the RRDA system has two parts. In Part 1, we focused on the upper pathway that generates SQL; for Part 2, we explore the lower pathway (highlighted in red in the following diagram) that connects users directly to visual insights through Amazon Q in QuickSight. 

 
 RRDA Architecture Overview highlighting the visualization pathway (shown in red) that processes SHOW_METRIC intents through Q topic retrieval and selection to deliver embedded Amazon Q in QuickSight visualizations.
 
As mentioned in Part 1, RRDA routes user queries through intent and domain classification systems that determine how each request should be processed. When a query is classified as SHOW_METRIC (triggered by phrases like â€œshow meâ€ or â€œdisplay,â€ or questions about visualizing trends), the system routes to our Amazon Q in QuickSight integration pathway instead of generating SQL. Simultaneously, our domain classifier identifies business contexts like Returns Processing or Promotions to focus the search scope. This dual classification makes it possible to seamlessly switch the user experience between receiving code and receiving visual insights while maintaining business context. 
For example, when a user asks â€œShow me the trend for how many items were returned for the past quarter,â€ our system identifies both the visualization intent and the Returns Processing business domain, allowing subsequent LLMs to search within domain-specific Amazon QuickSight Q topics rather than across the entire catalog. After the user intent is classified, RRDA notifies the user that itâ€™s searching for relevant metrics in Amazon Q in QuickSight and initiates the domain-filtered Q topic selection process described in the next section. 
Q topic retrieval and selection 
After the user intent is classified as SHOW_METRIC and the relevant business domain is identified, RRDA must now determine which specific Q topic can best visualize the requested information. Q topics are specialized configurations that enable business users to ask natural language questions about specific datasets. Each Q topic contains metadata about available metrics, dimensions, time periods, and synonymsâ€”making topic selection a critical step in delivering accurate visual insights. The challenge lies in intelligently mapping user requests to the most appropriate Q topics from our catalog of over 50 specialized configurations. To solve this problem, RRDA employs a two-step approach: first retrieving semantically relevant Q topics using vector search with metadata filtering to narrow down candidates efficiently, then selecting the best matches using an Amazon Bedrock foundation model (FM) to evaluate each candidateâ€™s ability to address the specific metrics and dimensions in the userâ€™s query. 
We implemented this retrieval-then-selection approach by building a dedicated Q topic knowledge base using Amazon Bedrock Knowledge Bases, a fully managed service that you can use to store, search, and retrieve organization-specific information for use with large language models (LLMs). The metadata for each Q topicâ€”including name, description, available metrics, dimensions, and sample questionsâ€”is converted into a searchable document and embedded using the Amazon Titan Text Embeddings V2 model. When a user asks a question, RRDA extracts the core query intent and specified domain, then performs a vector similarity search against our Q topic knowledge base. The system applies domain filters to the vector search configuration when a specific business domain is identified, making sure that only relevant Q topics are considered. From this retrieved list, a lightweight Amazon Bedrock FM analyzes the candidates and identifies the most relevant Q topic that can address the userâ€™s specific question, considering factors like metric availability, dimension support, and query compatibility. 
The following diagram illustrates the Q topic retrieval and selection workflow. 

 
 Q topic retrieval and selection workflow illustrating how RRDA identifies relevant visualization sources through vector search, then optimizes user questions for Amazon Q in QuickSight using Amazon Bedrock.
 
Continuing our previous example, when a user asks â€œShow me the trend for how many items were returned for the past quarter,â€ our system first detects the intent as SHOW_METRIC, then determines whether a domain like Returns Processing or Promotions is specified. The query is then passed to our Q topic retrieval function, which uses the Retrieve API to search for Q topics with relevant metadata. The search returns a ranked list of 10 candidate Q topics, each containing information about its capabilities, supported metrics, and visualization options. 
This retrieval mechanism solves a critical problem in enterprise business intelligence (BI) environmentsâ€”discovering which report or dataset has information to answer the userâ€™s question. Rather than requiring users to know which of our more than 50 Q topics to query, our system automatically identifies the relevant Q topics based on semantic understanding of the request. This intelligent Q topic selection creates a frictionless experience where business users can focus on asking questions in natural language while RRDA handles the complexity of finding the right data sources behind the scenes. 
Question rephrasing for optimal Q topic results 
After RRDA identifies relevant Q topics, we face another critical challenge: bridging the gap between how users naturally ask questions and the optimal format that Amazon Q in QuickSight expects. Without this crucial step, even with the right Q topic selected, users might receive suboptimal visualizations. 
The question rephrasing challenge 
Business users express their data needs in countless ways: 
 
 Imprecise time frames: â€œHow many items were returned lately?â€ 
 Complex multi-part questions: â€œCompare how many items were returned between NA &amp; EU for Q1â€ 
 Follow-up questions: â€œWrite a SQL query for how many returned items are currently being processed in the returns centerâ€ and after RRDA responds with the SQL query, then â€œGreat! Now show me this metric for this yearâ€ 
 
Although Amazon Q in QuickSight is designed to handle natural language, it is a generative Q&amp;A system rather than a conversational interface, and it performs best with clear, well-structured questions. The right phrasing significantly improves response quality by making sure Amazon Q in QuickSight precisely understands what to visualize. 
Structured question generation with the Amazon Bedrock Converse API 
To solve this challenge, we implemented a question rephrasing system using the Amazon Bedrock Converse API with structured outputs. This approach makes sure user queries are transformed into optimal formats for Amazon Q in QuickSight while preserving critical filters and parameters. 
First, we define Pydantic data models to structure the FM output: 
 
 class QTopicQuestionPair(BaseModel):
&nbsp;&nbsp; &nbsp;"""A question related to a Q&nbsp;topic."""
&nbsp;&nbsp; &nbsp;topic_id: str = Field(..., description="The ID of the chosen Q&nbsp;topic")
&nbsp;&nbsp; &nbsp;question: str = Field(..., description="The relevant question to ask this Q&nbsp;topic")

class ListOfQTopicQuestionPairs(BaseModel):
&nbsp;&nbsp; &nbsp;"""Response containing a list of questions corresponding to Q&nbsp;topics."""
&nbsp;&nbsp; &nbsp;pairs: List[QTopicQuestionPair] = Field(
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;default_factory=list, description="List of {Q&nbsp;topic Id, Question} pair objects."
&nbsp;&nbsp; &nbsp;) 
 
The Converse API tool use capability makes it possible to structure responses using these defined data models so our code can parse them reliably. Our system passes the user query, conversation history, and retrieved Q topic metadata to the Amazon Bedrock FM, which formulates questions following validated patterns: 
 
 â€œShow me the [specific metric(s)] for/over [user filters]â€ 
 â€œHow has [specific metric] changed from [start time frame] to [end time frame]?â€ 
 â€œCompare [metric A] to [metric B] for [time frame]â€ 
 â€œWhat is [specific metric] for [user filters]?â€ 
 
These formats consistently produce optimal results in Amazon Q in QuickSight across our business domains while preserving critical business contextâ€”including domain-specific terminology, time filters, dimensional constraints, and comparative elements. For example, after reviewing SQL results for a resolution index metric, a user might ask the follow-up question â€œNow show me how this metric trended over the last 6 months,â€ which our system rephrases to â€œShow me how the resolution index trended over the last 6 monthsâ€ while selecting only Amazon Q topics related to the Returns Processing business domain. 
Embedding Amazon Q in QuickSight visualizations within the chat interface 
When users ask to visualize data in the RRDA chat interface, the system seamlessly embeds Amazon Q in QuickSight directly within the conversation. Using the QuickSight embedding functionality, RRDA generates a secure embedding URL that renders interactive visualizations directly in the chat window. Users can view data visualizations without leaving the conversation flow. The system automatically selects the appropriate Q topic and rephrases the userâ€™s question for optimal results, then passes this information to the embedded QuickSight component, which generates and displays the visualization. The following screenshot shows a view (with sample data) of what this embedded experience looks like. For detailed information about the embedding capabilities used, see Embedding the Amazon Q in QuickSight Generative Q&amp;A experience. 

 
 Sample Q Topic showing embedded visualization capabilities within RRDAâ€™s chat interface, demonstrating how users can seamlessly transition from natural language questions to interactive data visualizations.
 
Direct Q topic suggestions for visualization requests 
When users ask visualization-oriented questions with the SHOW_METRIC intent (for example, â€œShow me how many items were returned over the past 3 monthsâ€), RRDA presents relevant Q topics as interactive suggestion cards within the conversation flow. Each suggestion includes the topic name (like â€œReturns Processing â€“ Sample Topicâ€), along with an optimized question format that the user can choose to immediately generate the visualization. 
 
These suggestions appear with a clear heading: â€œHere are the most relevant questions and Amazon QuickSight Q topics:â€ so itâ€™s straightforward for users to identify which data source will best answer their question. The system formats these suggestions as clickable prompts that, when selected, automatically trigger the embedded Amazon QuickSight generative Q&amp;A modal illustrated in the previous section. 
Contextual follow-up visualizations in conversations 
RRDA proactively suggests relevant visualizations even when users are engaged in regular text conversations with the Amazon Bedrock agent described in Part 1. For example, when a user asks for information about a specific metric (â€œWhat is the resolution index metric? Explain it to me in 1-2 sentencesâ€), the system provides a text explanation (which is blurred out in the following screenshot) but also automatically identifies opportunities for visual analysis. These suggestions appear as â€œGet related insights from Amazon Q in QuickSight:â€ followed by relevant questions like â€œWhat is resolution index for the last 3 months?â€ This creates a seamless bridge between learning about metrics and visualizing themâ€”users can start with simple queries about metric definitions and quickly transition to data exploration without reformulating their requests. This contextual awareness makes sure users discover valuable visualizations they might not have otherwise known to request, enhancing the overall analytics experience. 
 
Automating Q topic metadata management 
Behind RRDAâ€™s ability to suggest relevant visualizations is a knowledge base of over 50 Q topics and their metadata. To keep this knowledge base up-to-date as our analytics landscape grows, weâ€™ve implemented an automated metadata management workflow using AWS Step Functions that refreshes daily, pictured in the following diagram. 

 
 Automated Q topic metadata management workflow using AWS Step Functions that refreshes daily to keep the knowledge base current with the latest QuickSight Q topic configurations and validated questions.
 
The workflow begins with our FetchQTopicMetadata AWS Lambda function, which connects to QuickSight and gathers essential information about each Q topicâ€”what metrics it tracks, what filters users can apply, and importantly, what questions it has successfully answered in the past. We specifically use the DescribeTopic and ListTopicReviewedAnswers APIs, and store the resulting metadata in Amazon DynamoDB, a NoSQL key-value database. This creates a powerful feedback loopâ€”when Q topic authors review and validate questions from users, these validated questions are automatically incorporated into our knowledge base, making sure the system continuously learns from user interactions and expert curation. 
For the next step in the workflow, the BatchSummarizeQTopics function generates a concise summary of this collected metadata using Amazon Bedrock FMs. This step is necessary because raw Q topic configurations often exceed token limits for context windows in FMs. The function extracts essential informationâ€”metrics, measures, dimensions, and example questionsâ€”into compact summaries that can be effectively processed during vector retrieval and question formulation. 
The final step in our workflow is the SyncQTopicsKnowledgeBase function, which transforms this enriched metadata into vector-searchable documents, one for each Q topic with explicit business domain tags that enable domain-filtered retrieval. The function triggers Amazon Bedrock knowledge base ingestion to rebuild vector embeddings (numerical representations that capture semantic meaning), completing the cycle from Q topic configuration to searchable knowledge assets. This pipeline makes sure RRDA consistently has access to current, comprehensive Q topic metadata without requiring manual updates, significantly reducing operational overhead while improving the quality of Q topic recommendations. 
Best practices 
Based on our experience extending RRDA with visualization capabilities through Amazon Q in QuickSight, the following are some best practices for implementing generative BI solutions in the enterprise: 
 
 Anticipate analytical needs with intelligent suggestions â€“ Use AI to predict what visualizations users potentially expect, automatically surfacing relevant charts during conversations about metrics or SQL. This alleviates discovery barriers and empowers users with insights they might not have thought initially, seamlessly bridging the gap between understanding data and visualizing trends. 
 Build an automatic translation layer when connecting disparate AI systems â€“ Use AI to automatically reformat natural language queries into the structured patterns that systems expect, rather than requiring users to learn precise system-specific phrasing. Preserve the userâ€™s original intent and context while making sure each system receives input in its preferred format. 
 Design feedback loops â€“ Create automated systems that collect validated content from user interactions and feed it back into your knowledge bases. By harvesting human-reviewed questions from tools like Amazon Q in QuickSight, your system continuously improves through better retrieval data without requiring model retraining. 
 Implement retrieval before generation â€“ Use vector search with domain filtering to narrow down relevant visualization sources before employing generative AI to select and formulate the optimal question, dramatically improving response quality and reducing latency. 
 Maintain domain context across modalities â€“ Make sure that business domain context carries through the entire user journey, whether users are receiving SQL code or visualizations, to maintain consistency in metric definitions and data interpretation. 
 
For comprehensive guidance on building production-ready generative AI solutions, refer to the AWS Well-Architected Framework Generative AI Lens for best practices across security, reliability, performance efficiency, cost optimization, operational excellence, and sustainability. 
Conclusion 
In this two-part series, we explored how Amazonâ€™s Worldwide Returns &amp; ReCommerce organization built RRDA, a conversational data assistant that transforms natural language into both SQL queries and data visualizations. By combining Amazon Bedrock Agents for orchestration and SQL generation with Amazon Q in QuickSight for visual analytics, our solution democratizes data access across the organization. The domain-aware architecture, intelligent question rephrasing, and automated metadata management alleviate barriers between business questions and data insights, significantly reducing the time required to make data-driven decisions. As we continue enhancing RRDA with additional capabilities based on user feedback and advancements in FMs, we remain focused on our mission of creating a seamless bridge between natural language questions and actionable business insights. 
 
About the authors 
Dheer Toprani is a System Development Engineer within the Amazon Worldwide Returns and ReCommerce Data Services team. He specializes in large language models, cloud infrastructure, and scalable data systems, focusing on building intelligent solutions that enhance automation and data accessibility across Amazonâ€™s operations. Previously, he was a Data &amp; Machine Learning Engineer at AWS, where he worked closely with customers to develop enterprise-scale data infrastructure, including data lakes, analytics dashboards, and ETL pipelines. 
Nicolas Alvarez is a Data Engineer within the Amazon Worldwide Returns and ReCommerce Data Services team, focusing on building and optimizing recommerce data systems. He plays a key role in developing advanced technical solutions, including Apache Airflow implementations and front-end architecture for the teamâ€™s web presence. His work is crucial in enabling data-driven decision making for Amazonâ€™s reverse logistics operations and improving the efficiency of end-of-lifecycle product management. 
Lakshdeep Vatsa is a Senior Data Engineer within the Amazon Worldwide Returns and ReCommerce Data Services team. He specializes in designing, building, and optimizing large-scale data and reporting solutions. At Amazon, he plays a key role in developing scalable data pipelines, improving data quality, and enabling actionable insights for Reverse Logistics and ReCommerce operations. He is deeply passionate about enhancing self-service experiences for users and consistently seeks opportunities to utilize generative BI capabilities to solve complex customer challenges. 
Karam Muppidi is a Senior Engineering Manager at Amazon Retail, leading data engineering, infrastructure, and analytics teams within the Worldwide Returns and ReCommerce organization. He specializes in using LLMs and multi-agent architectures to transform data analytics and drive organizational adoption of AI tools. He has extensive experience developing enterprise-scale data architectures, analytics services, and governance strategies using AWS and third-party tools. Prior to his current role, Karam developed petabyte-scale data and compliance solutions for Amazonâ€™s Fintech and Merchant Technologies divisions. 
Sreeja Das is a Principal Engineer in the Returns and ReCommerce organization at Amazon. In her 10+ years at the company, she has worked at the intersection of high-scale distributed systems in eCommerce and Payments, Enterprise services, and Generative AI innovations. In her current role, Sreeja is focusing on system and data architecture transformation to enable better traceability and self-service in Returns and ReCommerce processes. Previously, she led architecture and tech strategy of some of Amazonâ€™s core systems including order and refund processing systems and billing systems that serve tens of trillions of customer requests everyday.

â¸»