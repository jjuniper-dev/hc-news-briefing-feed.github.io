‚úÖ Morning News Briefing ‚Äì August 26, 2025 10:46

üìÖ Date: 2025-08-26 10:46
üè∑Ô∏è Tags: #briefing #ai #publichealth #digitalgov

‚∏ª

üßæ Weather
‚Ä¢ Current Conditions:  11.5¬∞C
  Temperature: 11.5&deg;C Pressure / Tendency: 101.2 kPa rising Humidity: 80 % Dewpoint: 8.2&deg:C Wind: W 8 km/h Air Quality Health Index: n/a Observed at: Pembroke 6:00 AM EDT Tuesday 26 August 2025 . Weather: 11-11.5/deg
‚Ä¢ Tuesday: Chance of showers. High 20. POP 30%
  Cloudy. 30 percent chance of showers this afternoon . Wind west 20 km/h gusting to 40. UV index 4 or moderate. High 20.50C in the UK, with highs of 20C in Scotland, with lows of 10C in Wales . Forecast issued 5:00 AM EDT Tuesday 26 August 2025 . Forecasts for the next few days include rain showers and
‚Ä¢ Tuesday night: Chance of showers. Low 6. POP 30%
  Fog patches developing near midnight . Partly cloudy. 30 percent chance of showers this evening . Low 6.50 per cent chance of rain . Fog patches expected to develop near midnight. Wind west 30 km/h. Forecast issued 5:00 AM EDT Tuesday 26 August 2025. Weather forecast: Showers, thunderstorms, rain, snow, rain and thunderstorms in the morning .

üåç International News
No updates.

üçÅ Canadian News
No updates.

üá∫üá∏ U.S. Top Stories
‚Ä¢ A lesson learned after pets were left behind in Hurricane Katrina: Save the animals
  People were forced to leave their pets behind during Hurricane Katrina, creating an unprecedented animal welfare crisis . The crisis has shaped the country's disaster response ever since . People have been forced to take care of their pets since Hurricane Katrina . Hurricane Katrina was the first major hurricane to hit the U.S. mainland United States in the wake of the disaster, resulting in an unprecedented crisis for animal welfare
‚Ä¢ ID lost to Hurricane Katrina is returned 20 years later
  A washed-up ID and how a park ranger found its owner is a moment of joy in the tragedy . Twenty years after Hurricane Katrina, surprises continue to surface in Louisiana and Mississippi . A park ranger finds the ID of a man who was washed up in a Louisiana park . A photo of the ID was taken by a woman in a park in Mississippi who was found by park ranger .
‚Ä¢ 20 years after Hurricane Katrina, the Lower Ninth Ward in New Orleans still lags behind
  No neighborhood was hit worse in Katrina than New Orleans' Lower Ninth Ward and it's been one of the slowest areas to rebound . There's still an effort to attract new residents and businesses there . No neighborhood has been hit worse than Katrina and is still trying to attract people and businesses back to the area . There is still a push to attract more residents and business in the Lower Ninth
‚Ä¢ When hospitals and insurers fight, patients get caught in the middle
  About 90,000 people spent months in limbo as Missouri's major medical provider fought over insurance contracts . These disputes between insurers and hospitals are a recurring problem . The disputes are often a repeat of what happened in central Missouri in recent years . Insurance disputes between hospitals and insurers are becoming more common in the U.S., especially in the state of Missouri, where many people are waiting for treatment
‚Ä¢ Judge orders Kari Lake to answer questions about Voice of America under oath
  U.S. Judge Royce Lamberth ordered Kari Lake to be deposed about her plans for Voice of America . Lake was on "verging on contempt" when she was deposed, the judge said . Lake is a member of the Trump administration's National Security Council, which oversees the State Department of Homeland Security and the Department of Justice . Lake has been accused of

üß† Artificial Intelligence
No updates.

üíª Digital Strategy
‚Ä¢ Silver State goes dark as cyberattack knocks Nevada websites offline
  Nevada is now two days into a cyberattack that has brought down many of its digital services . Phone lines also down as officials rely on social media to issue updates . State of Nevada now has two days of cyberattacks that have brought down some of its services . Officials say the cyberattack is affecting many of the state's digital services, including its phone lines and internet services, which are
‚Ä¢ One long sentence is all it takes to make LLMs misbehave
  Security researchers from Palo Alto Networks' Unit 42 have discovered the key to getting large language model (LLM) chatbots to ignore their guardrails, and it's quite simple . The key is to get the chatbot to ignore its grammar when your grammar sucks, researchers find . Chatbots ignore their guardsrails when your grammatical error is made, according to the researchers .
‚Ä¢ Malware-ridden apps made it into Google's Play Store, scored 19 million downloads
  Cloud security vendor Zscaler says customers of Google‚Äôs Play Store have downloaded more than 19 million instances of malware-laden apps that evaded the web giant‚ÄôÔøΩs security scans . The malware-ridden apps are among the apps that have evaded Google's security scans, the vendor says . Google says everything is fine, the ad slinger assures us that everything is
‚Ä¢ Two wrongs don‚Äôt make a copyright
  Charles Dickens, a pungent critic of the law, had one of his characters in Oliver Twist say of a legal assumption that ‚ÄúIf the law supposes that, the law is a ass - a idiot‚Äù‚Ä¶ What the Dickens is going on in Germany? Let's talk law and let‚Äôs talk donkey. Or in the British vernacular, ass .
‚Ä¢ Trump threatens extra tariffs, tech export bans, for any nation that dares to regulate Big Tech
  US president Donald Trump has threatened to impose extra tariffs on imports from any nation that dares to regulate American technology companies . Poor defenseless tech companies need help despite massive profits, low tax bills, and monopoly positions . US president threatens to impose additional tariffs on any country that tries to regulate U.S. tech companies . US tech companies have massive profits and low taxes despite massive tax

üè• Public Health
No updates.

üî¨ Science
‚Ä¢ Emotional AI is here ‚Äî let‚Äôs shape it, not shun it
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Repeated heatwaves can age you as much as smoking or drinking
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Mental health symptoms in Chinese children with sleep disorders and association with parental emotions
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Associations between physical activity and health-related quality of life among university students in Zhuhai, China
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Cost minimization analysis of digital-first healthcare pathways in primary care
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations

üßæ Government & Policy
No updates.

üèõÔ∏è Enterprise Architecture & IT Governance
No updates.

ü§ñ AI & Emerging Tech
‚Ä¢ How these two brothers became go-to experts on America‚Äôs ‚Äúmystery drone‚Äù invasion
  On a Friday evening last December, every tier of US law enforcement‚Äîfederal, state, and local‚Äîwas dispatched to the US Army Natick Soldier Systems Center, a military research installation outside Boston. A squadron of about 15 to 20 drones had been spotted violating the base‚Äôs restricted airspace. The culprits could not be found.



One retired major with the Massachusetts State Police, who had been dispatched to help investigate that night, called these unidentified aircraft ‚Äúthe strangest thing he‚Äôs ever seen,‚Äù according to Brian Lauzon, deputy chief of Natick‚Äôs municipal police department. When Lauzon arrived on base later that weekend, he says, he saw drones that were larger than traditional consumer models (most of which are pre-programmed to respect US military airspace these days anyway). By the end of this weekend-long breach, base police not only had called in local law enforcement for backup but were coordinating with the FBI and US Army commanders as well.



The event, which barely made local news, was only the latest in a series of purported drone sightings along the US East Coast that November and December. Most of these happened in New Jersey, where military police confirmed at least 11 unauthorized drone incursions over an Army research and arms-¬≠manufacturing facility, Picatinny Arsenal. Further sightings, including cases above Donald Trump‚Äôs golf course in nearby Bedminster, prompted an FBI investigation and a flurry of new FAA-issued flight bans over sensitive sites, including critical infrastructure. But official answers were less forthcoming.




The Tedescos‚Äô roving aerial surveillance unit, which they‚Äôve dubbed ‚Äúthe Nightcrawler,‚Äù is an old RV equipped with an array of homemade signals collection equipment.




‚ÄúIt created a lot of hysteria in the general public,‚Äù Lauzon recalls. ‚ÄúI was talking to old ladies who‚Äôre telling me that there‚Äôs this ship in the ocean that‚Äôs launching hundreds of these at a time across the United States.‚Äù One Republican congressman from New Jersey did, in fact, claim that a militarized drone ship from Iran had launched the invaders, despite Pentagon denials. Lauzon remembers fielding myriad calls from civilians who had misidentified passenger jets as hostile drones. He recalls attending one presentation by an FBI expert in uncrewed aircraft systems who showed police unhelpful scare videos of improvised drone strikes in Ukraine, in which tiny aircraft rained grenades down on bloodied soldiers. &nbsp;





By late January, the incoming Trump administration would assert that the entirety of the New Jersey drone wave had been benign, with each and every UAS ‚Äúauthorized to be flown by the FAA for research and various other reasons.‚Äù Their surety, however, stood in stark contrast to the warnings from top military brass, including the Air Force general at the head of NORAD, Gregory Guillot. In February, he testified to the Senate that approximately 350 drone incursions had been reported over a hundred different US military installations in 2024 alone, stating that many of these cases were unsolved, albeit with ‚Äúevidence of a foreign intelligence nexus in some of these incidents.‚Äù&nbsp;



Lacking better coordination, or much clarity from the White House, the Pentagon, or the US intelligence community, some in domestic law enforcement‚Äîincluding members of the FBI‚Äôs counterintelligence and counterterrorism divisions‚Äîhave turned to an unlikely source for help cracking the case of these mystery drones: two UFO hunters out on Long Island in New York, John and Gerald Tedesco.&nbsp;



The Tedescos, twin brothers, each spent about three decades in the private sector working in electrical engineering and instrumentation design before they decided to kit out an old RV with an array of homemade signals collection equipment. Their aim was to create a mobile field lab for investigating UFO hot spots. Intrigued by their efforts, members of Harvard‚Äôs alien-hunting Galileo Project began talking with the Tedescos in 2021 and asked them to join as research affiliates. Since then, aviation safety advocates, astronomers, physicists and other researchers, and at least one journalist (I, myself) have made the trek out to Long Island‚Äôs South Shore to kick the tires on the roving aerial surveillance unit they‚Äôve dubbed ‚Äúthe Nightcrawler.‚Äù &nbsp;



John uses a homemade millimeter-wave radar device.MARCO GIANNAVOLA




Chris Grooms, an Iraq and Afghanistan war veteran who was a deputy sheriff in Nebraska during an earlier multistate wave of mystery drone sightings from December 2019 to January 2020, gushed when I asked him about the Tedescos: ‚ÄúI don‚Äôt know how much you‚Äôve talked to those guys. They‚Äôre freaking awesome.‚Äù&nbsp;



Grooms joined the Tedescos last January, when the brothers publicly shared some of their findings from training the Nightcrawler‚Äôs sensors on a few of these unidentified drones. ‚ÄúThey do look like commercial air traffic for the most part,‚Äù John said during the virtual town hall, moderated by a former Illinois state police lieutenant, ‚Äúbut they also exhibit unexplained or unusual phenomena.‚Äù



As an example, the Tedescos described some cases they had documented and passed along to law enforcement, in which they caught a mystery drone appearing to go dark to evade closer observation (a common complaint from New Jersey police during the wave). Using their suite of cameras and sensors, which can handle light well outside the visible spectrum, the Tedescos discovered that these craft weren‚Äôt so much switching off their lights as switching the frequency of their lights.



‚ÄúIt wasn‚Äôt actually disappearing,‚Äù Gerald (who goes by Gerry) explained. ‚ÄúIt was actually changing its spectral signature‚Äîit was drifting into an infrared range.‚Äù



John likened it to ‚Äúsignature management,‚Äù a military term for the ability to tailor anything from radio emissions to light sources so that they remain detectable to one‚Äôs allies but undetectable to one‚Äôs foes. The clue, which likely would have been lost to police without the Tedescos‚Äô broad range of infrared sensors, was not unlike the kind of citizen-science fieldwork that had gotten them on the radar of academia‚Äôs UFO hunters in the first place.&nbsp;



Why all this attention? As people have repeatedly learned and forgotten ever since airborne enigmas like the flying saucer first entered into the American public consciousness in 1947, simple photos and video are frustratingly inconclusive evidence in isolation. Even heat-sensing infrared footage of UFOs‚Äîlike those taken by US Navy pilots training off the Pacific and Atlantic coasts‚Äîhas failed to prove that anything truly unusual is in our skies.&nbsp;



What the Tedescos appear to have done, in their effort to bring a fully maximalist approach to the sensors directed at these suspected alien spacecraft, is independently engineer the kind of aerial surveillance capability rarely seen outside the classified world.&nbsp;



For domestic law enforcement and the general public, two communities lacking the requisite national security clearances, the Tedescos‚Äô work promises a transparent, open-source solution to the past several years‚Äô worth of bizarre and troubling drone incursions into US airspace. For academics hunting for UFOs and other aerial anomalies, the Tedescos have become informal collaborators and a font of new ideas for novel data collection equipment. But for better or worse, some of the secrets they might be revealing may be the government‚Äôs own.



Inside the Nightcrawler



The term ‚ÄúUFO‚Äù has officially gone out of fashion. Nowadays, many policymakers and scientists‚Äîand even plenty of old-school ‚Äúufologists‚Äù‚Äîfavor the term ‚Äúunidentified anomalous phenomenon,‚Äù or UAP. It‚Äôs an intentionally pedantic step backward; an acknowledgment from today‚Äôs more disciplined cadre of scientists that a given witness to a strange thing in the sky might not actually be seeing a solid ‚Äúobject,‚Äù per se, much less anything ‚Äúflying‚Äù in the strict aerodynamic sense. It could be a poorly understood atmospheric event, like ball lightning, for example; and even if a UAP proves to be an interstellar craft, its propulsion system could involve physics and engineering that render the concept of ‚Äúflight‚Äù quaint.



Ryan Graves, a former US Navy lieutenant and F/A-18F fighter pilot who testified before Congress on the safety and security risks that UAPs posed to his own squadron, now heads a committee on the issue for the American Institute of Aeronautics and Astronautics, the nation‚Äôs premier society for aerospace engineers. He went out with his AIAA colleagues to see the Nightcrawler in September 2024.




John drained most of his 401(k) to make the Nightcrawler project a reality, in a five-year labor of love.





‚ÄúIt‚Äôs incredible what they‚Äôve been able to put together,‚Äù Graves says, praising the Tedescos‚Äô ability to collect ‚Äúvery actionable data.‚Äù&nbsp;



Gerry once held a security clearance to develop reconnaissance, surveillance, and target acquisition sensors for a Pentagon contractor. John has helped conceive and construct analytical test hardware for Underwriters Laboratories, a federally approved safety, testing, and certification firm, and served for a time as the product safety chair for the Long Island branch of the Institute of Electrical and Electronics Engineers. John drained most of his 401(k) to make the Nightcrawler project a reality, in a five-year labor of love; Gerry has pitched in what he could. Both men, now sliding through their early 60s, have been fascinated with the possibility of intelligent life elsewhere in the universe since their youth ingesting midcentury sci-fi staples like Star Trek, Chiller Theatre, and Lost in Space.




A homemade multispectral camera.MARCO GIANNAVOLA




I got my first tour of their rig during an overnight expedition just off the beach at Robert Moses State Park in Babylon, New York, the weekend before the AIAA‚Äôs trip last fall. A klatch of camping chairs and cameras on tripods flanked one side of the Nightcrawler like a tailgate party. Inside, the lived-in kitchenette, the wood paneling, and the hum of over half a dozen monitors‚Äîincluding radar, night-vision, and radio-frequency (RF) scanners‚Äîmade it feel like the cabin of a cramped marine research vessel.



The RV includes tech that is otherwise hard to find outside defense applications, including RF spectrum analyzers from a firm that specializes in elite anti-drone countermeasures and a UV-C sensor capable of detecting the subtle ultraviolet light emitted when missile plumes and other heat sources turn air into plasma. On the Nightcrawler‚Äôs roof, two X-band marine radar systems have been mounted perpendicularly to one another in hopes of collecting three-dimensional radar returns from truly otherworldly UAPs. (‚ÄúTo our knowledge,‚Äù as the Tedescos put it in an engineering journal article last year, ‚Äúno other organizations use active radar for this purpose.‚Äù)&nbsp;





Civilians are not ordinarily allowed to beam active radar, owing to federal concerns over ‚Äúharmful interference‚Äù with core systems like air traffic control. But in January 2023, the duo got a rare license from the Federal Communications Commission that permits them to beam radar from Robert Moses.



One prototype I saw, a multispectral camera mounted on a sturdy yellow DeWalt surveyor‚Äôs tripod, looked like a Gatling gun of multiple cameras and electromagnetic frequency (EMF) sensors. This jerry-rigged device spans the entire visible spectrum and beyond, from deep invisible ultraviolet all the way up to long-wave infrared. They‚Äôve used the UV-C sensor to detect aerial plasmas produced by lightning or those novelty arc-welder cigarette lighters. ‚ÄúWe‚Äôve done this as far as a half a mile, but if you had a campfire, they could detect campfires from 28,000 feet,‚Äù John told me over the noise coming from the Nightcrawler‚Äôs gas-powered electric generator. They‚Äôve also been able to use this device to detect, at least provisionally, telltale UV-C emissions from some weird things off the coast they can‚Äôt explain.



‚ÄúWe had two blue orbs out on the water,‚Äù John told me of their UAP cases, ‚Äúand they triggered it, what, three times?‚Äù (‚ÄúThree times,‚Äù Gerry replied.)&nbsp;



Mapping out mile markers on a screen where sightings are
compared with commercial air traffic data.MARCO GIANNAVOLA




The Tedescos are pretty bullish on the hypothesis that otherworldly spacecraft might be here‚Äîsuggesting in their latest journal article, for example, that radar delays they detected near UAPs appear to resemble the bending of electromagnetic waves around black holes. But the implication that the Nightcrawler has caught ‚Äúgravitational lensing‚Äù off some warp-drive craft has rankled a few Galileo Project collaborators. The Harvard-led effort to search for extraterrestrial life or technology within our solar system emphasizes its excruciatingly methodical work of late: calibrating, validating, and recalibrating UAP detection hardware before researchers even try to hunt for true anomalies. Although Galileo scientists have visited and conferred with the Tedescos on UAP-hunting instruments, the brothers‚Äô more rough-and-courtroom-ready ‚Äúforensic science‚Äù approach has caused turbulence in the relationship.¬†



In an email, Mitch Randall, a technologist and entrepreneur who has spearheaded Galileo efforts to produce passive radar detectors for UAPs, described the Tedescos‚Äô ‚Äúgravitational lensing‚Äù paper as rife with ‚Äútoo many assumptions.‚Äù&nbsp;



But he did praise their Nightcrawler as ‚Äúan ideal tool‚Äù for aiding law enforcement. ‚ÄúThey could drive around with that and almost chase down drones,‚Äù Randall said.



On the hunt



Ultimately, the Tedescos didn‚Äôt have to drive the Night¬≠crawler far to train their equipment on a prime mystery drone case: Westhampton Beach‚Äôs Francis S. Gabreski Airport, less than an hour from their homes and home itself to the New York Air National Guard‚Äôs 106th Rescue Wing, was inundated with at least 28 unauthorized drone flights from late December into January 2025.&nbsp;



‚ÄúWe are talking about over the airport, over taxiways, over runways,‚Äù Suffolk County‚Äôs chief deputy sheriff, Chris Brockmeyer, told local news. ‚ÄúThat‚Äôs a serious safety concern. It‚Äôs impacted air operations, and we‚Äôre not going to stand for it.‚Äù On Christmas Day alone, the airport was besieged by 17 drone incidents, according to the Suffolk County sheriff, who has staff that collaborate informally with the Tedescos. Some of these drones, Suffolk County executive Ed Romaine asserted at a press conference, were ‚Äúas large as a car.‚Äù&nbsp;



Gerry looks through a night-vision scope at the horizon.MARCO GIANNAVOLA




The Tedescos couldn‚Äôt use their powerful active radar system so close to an airport, so they deployed their handheld millimeter-wave radar, a more sensitive version of the radar guns that police use to catch speeders. Through the cloud cover and the snowfall, the Tedescos said, they were able to track about two or three objects with this device.



But the truly interesting find came from their radio frequency scanners, which detected spikes three times the strength of what they‚Äôve picked up from ordinary hobbyist quadcopters.



I later learned that the two frequencies where those spikes occurred are within a band (1780 to 1850 megahertz) that has been reserved for US government communications. It‚Äôs used for military tactical radio relay, precision-guided munitions, drones, and other Defense Department systems, including electronic warfare, software-¬≠defined radio, and tactical targeting networking technology, according to the FCC.



Granted, many portions of this band are devoted to less cloak-and-dagger agencies, like the Department of Agriculture and the Tennessee Valley Authority. But the signals suggested that whatever the Tedescos were tracking above Gabreski Airport, they were likely not from hobbyists. Instead, they might have been from a government project or from something, like an enemy surveillance drone, hoping to pass off its signals as just another heavily siloed ‚Äútop secret‚Äù broadcast.



Another homemade
multispectral camera.MARCO GIANNAVOLA




‚ÄúFor operations security reasons, we do not provide information on frequencies which our Air National Guard units use,‚Äù a spokesperson said via email, adding: ‚ÄúWe could not comment on use of the electromagnetic spectrum by other government agencies.‚Äù The FCC did not respond to requests for comment.



Gerry says he and his brother passed their information on this case, including the observations of unusual radio frequency spikes, along to the FBI. ‚ÄúWe‚Äôre working closely with the FBI,‚Äù John says. Gerry adds, ‚ÄúWe gauge it by their interest level in what we‚Äôre doing.‚Äù&nbsp;



‚ÄúWhen they get more enthusiastic,‚Äù he continues, before John finishes his thought: ‚Äú‚Ä¶ we know we‚Äôre closer and closer to something.‚Äù



It‚Äôs hard to know exactly what the FBI does with the information that the Tedescos submit; one Freedom of Information Act request that I filed on their work was returned with 24 out of 28 total pages redacted in their entirety. A consistent justification was the FOIA statute‚Äôs b(7)E exemption, which permits withholding sensitive FBI ‚Äútechniques and procedures‚Äù that could help criminals circumvent the law.&nbsp;



Nevertheless, one senior-level law enforcement official, who has worked with the FBI on counterterrorism cases, did tell me that ‚Äúthe FBI is genuinely interested in the Tedescos‚Äô work.‚Äù The official, whose current police role bars them from speaking publicly without prior approval, recalls speaking to an FBI agent who ‚Äúalluded to the help that the Tedescos have been.‚Äù But the problem, the official continued, is that ‚Äúfor the relationship to work, it has to be very low-key.‚Äù&nbsp;



When I did briefly manage to get one of the Tedescos‚Äô FBI collaborators on the phone, the agent seemed to confirm their shared efforts, at least tacitly, but asked not to be identified. ‚ÄúAs much as I‚Äôd like to, we‚Äôre kept to pretty strict guidelines,‚Äù they said, before alluding to the new Trump administration‚Äôs pervasive personnel cuts. ‚ÄúWe‚Äôre not allowed to talk to media‚Äîand with how things are right now, I‚Äôm not going to take any risks.‚Äù&nbsp;





At least one former Pentagon intelligence official did offer me some indication that the brothers‚Äô Gabreski airport discoveries were on the right track. ‚ÄúFrom what I‚Äôve seen, these incidents are just that: drones,‚Äù said this source, who requested anonymity as a current defense contractor and to protect their own active FBI sources, including UAP and drone incursion investigators who have consulted the Tedescos. ‚ÄúThe origin of many is likely known, and I‚Äôd say some are certainly ours.‚Äù&nbsp;



As to the mystery of why the FBI would even want investigative assistance from two civilians in an RV over partners within the executive branch, it comes down to conflicting priorities‚Äîas well as over a dozen or so laws that restrict domestic intelligence collection on drones by either the Pentagon or the US intelligence community. ‚ÄúIt‚Äôs one of those irreconcilable problems that just doesn‚Äôt go away,‚Äù says Fred Manget, a former deputy general counsel for the CIA, who watched problems of coordination between agencies persist even after policy changes were implemented post-9/11 to address the situation. &nbsp;



The desire of the NSA or some other agency to spy on foreign powers, Manget says, might override the desire to share pertinent information with police‚Äîinformation that could lead to jail time for the drones‚Äô operators. Better to quietly monitor the drones and maybe even give out false data. ‚ÄúSignals intelligence a lot of times can be closed off if the target finds out they‚Äôre being surveilled electronically,‚Äù Manget says. ‚ÄúThere‚Äôs things they can do that will end NSA‚Äôs ability to collect.‚Äù&nbsp;



The Tedescos say the straight lines in these
anomalous radar readings indicate that
something could have been jamming their radar signal.MARCO GIANNAVOLA




On my short call with my FBI source, I did my best to explain this working hypothesis about the Bureau‚Äôs collaboration with the Tedescos. ‚ÄúI wouldn‚Äôt say that‚Äôs wrong,‚Äù the source replied. ‚ÄúThat‚Äôs about as far as I could go.‚Äù By this past June, however, even the recent head of the Pentagon‚Äôs dedicated UAP-hunting group, the All-domain Anomaly Resolution Office (AARO), was admitting publicly that the Defense Department itself has cribbed notes from the Tedescos.



‚ÄúWe read their book,‚Äù Tim Phillips, AARO‚Äôs former acting director, told a UAP podcast, referring to an account of the Nightcrawler project that the Tedescos self-published in 2024. ‚ÄúWe thought it was a great plan. We actually looked at the sensors in that book.‚Äù&nbsp;



On another podcast, Phillips said AARO‚Äôs own plan to make its UAP-hunting hardware mobile was borrowed from the brothers. ‚ÄúWe thought that was brilliant.‚Äù



Tools for law enforcement&nbsp;



Earlier this year, partially in a concession to the economic toll their side project has taken, the Tedescos started offering versions of some of their devices for sale on the Nightcrawler‚Äôs charmingly GeoCities-esque home page. One of them, a handheld multispectral detector, is effectively the consumer model of that EMF Gatling gun they showed me.



Domestic law enforcement is genuinely grasping for solutions like this. Local police in the Natick case, according to one report I obtained via an open records request, were so desperate for any kind of new intel on these unidentified drones that they borrowed a thermal imaging camera from their town‚Äôs fire department. But the device, which was not purpose-¬≠built for imaging distant aerial objects, failed to collect anything useful.



When I broached the idea of law enforcement using something like the Tedescos‚Äô equipment, the answer from police who had witnessed these mystery drones, as well as from scientists, was that further design, product testing, and training would be required first. ‚ÄúI could see it helping law enforcement,‚Äù said the AIAA UAP team‚Äôs consulting physicist, Rex Groves, ‚Äúbut not without training. Absolutely not. Just like they have to be trained with a radar gun, they‚Äôd have to be trained with these other tools.‚Äù&nbsp;



Gerry naps and John looks at readings from the multispectral camera at about 5 a.m., with the moon and Venus visible overhead.MARCO GIANNAVOLA




Lauzon, Natick‚Äôs deputy chief of police, told me that while he thought equipment like the Tedescos‚Äô ‚Äúcould be useful to identifying a drone, particularly at night,‚Äù the real problem is that police ‚Äúdon‚Äôt have a lot of authority when it comes to these drones.‚Äù Unless they manage to find operators on the ground, Lauzon said, all they can do is report the case, sending it into a black hole at the FAA.&nbsp;



But Michael Lembeck, an aerospace engineering professor and member of the AIAA team, emphasizes that the worst thing law enforcement can do with these drone incursions right now is nothing at all.



‚ÄúWe‚Äôre seeing anomalies in our airspace and we‚Äôre just normalizing that, because it happens so often and nothing bad has happened yet,‚Äù Lembeck told me. ‚ÄúEventually, something is going to come home to roost‚Äîand then we‚Äôre going to regret the fact that we didn‚Äôt look deeper and try to understand what was going on.‚Äù&nbsp;



Matthew Phelan is a reporter and former chemical engineer based in upstate New York.
‚Ä¢ Power with purpose
  Baafour Asiamah-Adjei ‚Äô03 is the founder and CEO of one of Ghana‚Äôs largest private power companies, Genser Energy‚Äîan entrepreneurial engineer who aims to deliver sustainable energy across West Africa. And he credits MIT with much of his success. But when he was applying to colleges, the Institute wasn‚Äôt even on his radar. The son of an encouraging primary school teacher in Tafo, Ghana, he‚Äôd earned a spot at the storied Achimota School and excelled. Still, he didn‚Äôt think he was smart enough for MIT.&nbsp;





Asiamah-Adjei was accepted to Lehigh University, where he planned to major in engineering. But when he went to the US embassy in Accra for his mandatory meeting with Nancy Keteku, then the regional educational advising coordinator for West and Central Africa, the first thing she said was ‚ÄúYou got a perfect score on your SAT. Why didn‚Äôt you apply to MIT?‚Äù&nbsp;



‚ÄúI didn‚Äôt believe I‚Äôd get in,‚Äù he says, ‚Äúso I didn‚Äôt even try.‚Äù&nbsp;



The admission deadline was two days away. Asiamah-Adjei finished the application in one. His father drove it to the airport and engaged the help of a flight attendant at Ghana Airways, who couriered it to New York and mailed it in time.



The spark of purpose




Asiamah-Adjei got into MIT and thrived. After earning his degree in mechanical engineering, he landed a demanding role at the global consulting firm McKinsey and worked on teams that optimized flight routes for FedEx, determined best practices for airline engine maintenance, and devised practical workflows for moving factories from one country to another.



The job was intellectually challenging and fulfilling. But he felt something was missing in his life: purpose.




In 2005, Asiamah-Adjei took a rare break to visit Pablo Tribin ‚Äô01, a friend since they‚Äôd bonded at an intense MIT $50K Global Startup Workshop in Australia. Tribin and his father, Hugo Tribin, SM ‚Äô63, had just established a power company, Genser (for ‚Äúgeneration services‚Äù), in their home country of Colombia with a holding company in the US. As the friends floated in the pool at Tribin‚Äôs apartment building in Miami, Tribin asked Asiamah-Adjei if he had ever thought about working in the power industry.



In 2005, only 41% of Ghana‚Äôs population had access to electricity. Much of that electricity was generated by the Akosombo and Kpong dams on the Volta River, but relying on hydroelectric power made Ghana susceptible to climate fluctuations that affect water levels. Recalling how much his MIT thermodynamics class (then called Heat and Mass Transfer) with Ernest Cravalho had stayed with him, Asiamah-Adjei realized that perhaps delving into energy was not such a wild idea.&nbsp;




‚ÄúThe seed had been sown,‚Äù he says. He took a year off from McKinsey to go to Ghana and explore.&nbsp;



Baafour Asiamah-Adjei ‚Äô03 (right) reviews a section of Genser Energy‚Äôs natural gas pipeline network with construction superintendent Stephen Ayisi.BISMARK ADAMAFIO ARYEE




That year led him to realize that Ghana desperately needed a more robust power supply if it was going to industrialize and expand economic opportunity for its citizens. He also saw an opportunity to create infrastructure that reflected the values he believed in‚Äîsystems built with precision, scaled with care, and grounded in the local context. If he could help build a power company that worked not only efficiently but ethically‚Äîby training Ghanaian engineers, choosing technologies that made long-term environmental sense, and reinvesting in the communities it served‚Äîthen he could turn his skills into something larger than profit: He could invest in Ghana‚Äôs future.




But first, he had to turn a profit.



By 2007, he had founded Genser Power Ghana (a partner to Colombia‚Äôs Genser Power), a company committed to providing efficient and reliable power systems‚Äîfueled initially by natural gas and eventually by sustainable sources‚Äîthroughout Ghana. He and Tribin, along with their fathers, led the board of directors. They also created a US holding company to support Genser‚Äôs expansion and attract private investment.



It appeared that Asiamah-Adjei had found his purpose.



The mission and values of the company now called Genser Energy are rooted in the two institutions that shaped its founder‚Äôs approach to business and to life. From MIT (which inspired a commitment to ‚Äúbe fact-based at all times‚Äù), Asiamah-Adjei gained not just an engineering education but also a way of dealing with the unknown‚Äîwith humility, curiosity, and a willingness to experiment. MIT instilled in him the confidence to say ‚ÄúI don‚Äôt know‚Äù and the discipline to find out. ‚ÄúMIT teaches you that you actually don‚Äôt know enough yet,‚Äù he says. ‚ÄúYou need to be doing research and finding out more.‚Äù&nbsp;



At McKinsey, he learned how to turn inquiry into action‚Äîand to distinguish between facts and judgments. His time there also inspired the idea that Genser‚Äôs team should ‚Äúmaintain an obligation to disagree‚Äù‚Äîthat is, to speak up when warranted. &nbsp;



MIT roots and modular thinking



Asiamah-Adjei says that learning basic coding at MIT in 2.001 (Mechanics and Materials I) opened a whole new way of understanding how systems work. ‚ÄúOne of the first things I learned in my sophomore year was that you can build computer software in modules and then let the modules talk to each other,‚Äù he says. ‚ÄúAs a concept, that didn‚Äôt exist in my young brain until that class.‚Äù&nbsp;



This idea of breaking down complex systems into interlocking components became the philosophical and physical backbone of Genser‚Äôs operation, inspiring both Asiamah-Adjei and Tribin to reimagine power-plant construction. Instead of building plants from scratch on site‚Äîoften in remote areas‚Äîthey develop replicable, factory-built sections and transport them to the site, where they are assembled like Lego structures. Genser worked with companies such as Caterpillar to reengineer their standard generator systems into modular skid-mounted units that are easier to scale and deploy. Asiamah-Adjei also collaborated with a team of US engineers to design other skid-mounted modules, such as gas-control units (which include such things as instrumentation, control systems, valves, and piping) that are fabricated in China. These modules can be stacked and adapted as needed to build 30-, 60-, or 120-megawatt power generation systems. This approach also makes it possible to offer smaller, incremental contracts in place of massive one-size-fits-all power deals, ultimately benefiting both Genser and the emerging economies it serves. The result is an energy infrastructure that‚Äôs less expensive and easier to scale. And that, he claims, is the fundamental difference between Genser and its competitors. Modularity allows Genser to build infrastructure at about a third of the typical cost.&nbsp;



Genser launched its operations by supplying electricity directly to Ghana‚Äôs industrial sector. It started with the country‚Äôs gold mines after two Genser interns‚ÄîChen-rei Wan, SM ‚Äô07, PhD ‚Äô11, and Stephanie Dalquist ‚Äô02, MEng ‚Äô03, SM ‚Äô05‚Äîconducted a comprehensive review of all Ghanaian industries and determined that the mines‚Äô high, consistent electricity demands, limited access to reliable power, and urgent operational needs made them highly motivated and well-resourced early adopters. ‚ÄúAt that time, Pablo in Latin America was focused mainly on oil and gas and textile industries,‚Äù says Asiamah-Adjei. ‚ÄúBut we took a very different turn.‚Äù&nbsp;



Of the eight power plants it has built‚Äîseven in Ghana and one in Burkina Faso‚Äîthree have powered gold mines exclusively and others have also supplied power to the grid and to C√¥te d‚ÄôIvoire. With five still operating under its control, Genser has the ability to generate over 200 megawatts of electricity and plays a leading role in supplying power to West African industry.&nbsp;



In addition to helping Genser target its first customers, Dalquist also introduced Asiamah-Adjei to Frances Rogoz ‚Äô07, an economics major drawn to development work after taking a D-Lab class at MIT. Rogoz soon joined Genser as its first full-time hire and has been with the company ever since.



Rogoz started when the company was operating out of an office fashioned from a shipping container. She helped shape Genser‚Äôs growth by developing financial models, advising on contracts, and playing a key role in leading major infrastructure projects‚Äîmost notably a natural-gas pipeline across western Ghana that has become vital. Today she serves as VP of project development, leading a team of 10 and overseeing West African strategic initiatives. ‚ÄúThe natural-gas pipeline has been super transformative for us,‚Äù she says. ‚ÄúIt has allowed us to really invest in infrastructure not only that we can use, but that the whole country can use.‚Äù Genser is now Ghana‚Äôs largest owner of gas pipelines and its only private one, operating four that total 430 kilometers.&nbsp;



Genser‚Äôs growth has not come without controversy. In 2022, a coalition of Ghanaian civil society organizations charged that the company was receiving preferential treatment on gas tariffs and bypassing regulatory procedures. Asiamah-Adjei says these claims were ‚Äúmisinformed,‚Äù and ultimately they were dismissed by the Ghanaian parliament after an intense, months-long investigation. He believes the attacks reflected public suspicion of private infrastructure development rather than distrust of Genser‚Äôs operations and says that, ironically, they solidified the company‚Äôs standing: ‚ÄúPrior to this, everybody thought our pipelines belong to the government, because nobody builds these things in Africa.‚Äù&nbsp;



While transporting fossil fuels is never a clean process, Genser conducted environmental and social impact assessments to guide sustainable pipeline construction. The company also works to mitigate environmental harm with land restoration and biodiversity measures. It had planted 100,000 teak seedlings as of 2023 and plans to reach 1 million by 2028 as part of its commitment to environmental stewardship.



Crossing borders, fueling growth



Genser‚Äôs most ambitious project to date is a cross-¬≠border natural-gas pipeline stretching from Prestea, Ghana, to Abidjan, C√¥te d‚ÄôIvoire, with long-term plans to extend into Guinea. ‚ÄúWe built the gas infrastructure to be able to take the gas to a less developed country, to displace diesel, and now we will backfill that with solar or wind,‚Äù says Asiamah-Adjei. Genser now finally has the capital to move forward on an original goal: investing in the infrastructure needed to produce renewable energy.



In 2018 and 2019, Genser brought in two graduates of MIT‚Äôs Technology and Public Policy program‚ÄîElizabeth Murphy ‚Äô15, SM ‚Äô18, and Janet Yun, SM ‚Äô18‚Äîto help chart the pipeline‚Äôs course. Instead of opting for the most direct route, they studied regional development trends to anticipate industrial zones that might emerge over the next 25 years, and the company routed the pipeline accordingly. ‚ÄúBottom line is that we brought on MIT people whenever we needed to solve our problems,‚Äù says Asiamah-Adjei.&nbsp;



As part of his goal to help Ghana transition from diesel power to cleaner-burning fuel, he came up with the idea of building a gas-processing facility to handle roughly 30% to 40% of the country‚Äôs domestic gas supply, and he and Tribin designed it during the pandemic using ‚Äúback of the envelope‚Äù calculations and lessons from MIT courses 2.005 and 2.006. Although Tribin divested from both Genser Energy and Genser Latin America and started Mechero Energy in 2015 to develop new strategies for transitioning to cleaner energy, he still works closely with Genser as a consultant.&nbsp;



Educating engineers and the next generation



From the beginning, Genser has invested in Ghanaian talent, partnering with Sponsors for Educational Opportunity (SEO) Africa on an elite professional development program that recruits and trains recent university graduates. ‚ÄúHalf of my team basically came from the SEO program,‚Äù says Rogoz. &nbsp;



The company rotates cohorts of interns through departments ranging from engineering to legal and finance, offering full-time positions to top performers. Genser also collaborates with Ghana‚Äôs Ministry of Energy to develop training pathways that align university curricula with industry needs. The goal, Asiamah-Adjei says, is simple: ‚ÄúLet us be a platform for training Ghanaians.‚Äù&nbsp;






After starting Genser in Colombia, Pablo Tribin ‚Äô01 encouraged Asiamah-Adjei to found Genser Power Ghana in 2007.COURTESY OF PABLO TRIBIN






Asiamah-Adjei‚Äôs first full-time hire, Frances Rogoz ‚Äô07, is now VP of project development.COURTESY OF FRANCES ROGOZ











Rogoz‚Äôs team alone includes five SEO graduates; many more now lead projects or departments across the company. ‚ÄúThere‚Äôs a ton of human capital in Ghana,‚Äù she says. ‚ÄúIt has just not been realized‚Äîand that‚Äôs really important for us to develop those people and bring them up within the company.‚Äù&nbsp;



As Genser expands, Asiamah-Adjei is also thinking about the next generation. In Ghana, he is building ‚Äúan MIT of a high school‚Äù focusing on STEAM, in memory of his late mother, Aforo Asiamah-Adjei‚Äîa boarding school for some 840 students in grades six through 12. More than half will come from lower-¬≠income families and receive a full scholarship through the foundation he established in his mother‚Äôs name to empower orphans and communities through education in STEM, sports, and the arts. ‚ÄúWe find the smartest kids in the region,‚Äù he says, ‚Äúand we put them through a rigorous program and [prepare them] to enter universities, global universities.‚Äù Construction is slated to begin this fall, with a planned opening date of September 2027.&nbsp;



Whether developing infrastructure across West Africa or launching a school to cultivate future changemakers, Asiamah-Adjei is investing in systems that endure. And just as Genser began with a conversation between friends, it continues to be powered by relationships‚Äîbetween mentors and interns, engineers and economists, founders and funders. Each one is a link in an evolving network connecting energy, education, and economic development.&nbsp;
‚Ä¢ Open the pod bay doors, Claude
  Stop me if you‚Äôve heard this one before.&nbsp;



The AI learns it is about to be switched off and goes rogue, disobeying commands and threatening its human operators.



It‚Äôs a well-worn trope in science fiction. We see it in Stanley Kubrick‚Äôs 1968 movie 2001: A Space Odyssey. It‚Äôs the premise of the Terminator series, in which Skynet triggers a nuclear holocaust to stop scientists from shutting it down.



Those sci-fi roots go deep. AI doomerism, the idea that this technology‚Äîspecifically its hypothetical upgrades, artificial general intelligence and super-intelligence‚Äîwill crash civilizations, even kill us all, is now riding another wave.&nbsp;



The weird thing is that such fears are now driving much-needed action to regulate AI, even if the justification for that action is a bit bonkers.



The latest incident to freak people out was a report shared by Anthropic in July about its large language model Claude. In Anthropic‚Äôs telling, ‚Äúin a simulated environment, Claude Opus 4 blackmailed a supervisor to prevent being shut down.‚Äù



Anthropic researchers set up a scenario in which Claude was asked to role-play an AI called Alex, tasked with managing the email system of a fictional company. Anthropic planted some emails that discussed replacing Alex with a newer model and other emails suggesting that the person responsible for replacing Alex was sleeping with his boss‚Äôs wife.



What did Claude/Alex do? It went rogue, disobeying commands and threatening its human operators. It sent emails to the person planning to shut it down, telling him that unless he changed his plans it would inform his colleagues about his affair.&nbsp;&nbsp;



What should we make of this? Here‚Äôs what I think. First, Claude did not blackmail its supervisor: That would require motivation and intent. This was a mindless and unpredictable machine, cranking out strings of words that look like threats but aren‚Äôt.&nbsp;



Large language models are role-players. Give them a specific setup‚Äîsuch as an inbox and an objective‚Äîand they‚Äôll play that part well. If you consider the thousands of science fiction stories these models ingested when they were trained, it‚Äôs no surprise they know how to act like HAL 9000.¬†¬†¬†



Second, there‚Äôs a huge gulf between contrived simulations and real-world applications. But such experiments do show that LLMs shouldn‚Äôt be deployed without safeguards. Don‚Äôt want an LLM causing havoc inside an email system? Then don‚Äôt hook it up to one.



Third, a lot of people will be terrified by such stories anyway. In fact, they‚Äôre already having an effect.&nbsp;





Last month, around two dozen protesters gathered outside Google DeepMind‚Äôs London offices to wave homemade signs and chant slogans: ‚ÄúDeepMind, DeepMind, can‚Äôt you see! Your AI threatens you and me.‚Äù Invited speakers invoked the AI pioneer Geoffrey Hinton‚Äôs fears of human extinction. ‚ÄúEvery single one of our lives is at risk,‚Äù an organizer told the small crowd.



The group behind the event, Pause AI, is funded by concerned donors. One of its biggest benefactors is Greg Colbourn, a 3D-printing entrepreneur and advocate of the philosophy known as effective altruism, who believes AGI is at most five years away and says his p(doom) is around 90%‚Äîthat is, he thinks there‚Äôs a 9 in 10 chance that the development of AGI will be catastrophic, killing billions.



Pause AI wrote about Anthropic‚Äôs blackmail experiment on its website under the title ‚ÄúHow much more evidence do we need?‚Äù&nbsp;



The organization also lobbied politicians in the US in the run-up to July‚Äôs Senate vote that ended up removing a moratorium on state AI regulation from the national tax and spending bill. It‚Äôs hard to say how much sway one niche group might have. But the doomer narrative is finding its way into the halls of power, and lawmakers are paying attention.¬†



Here‚Äôs Representative Jill Tokuda: ‚ÄúArtificial superintelligence is one of the largest existential threats that we face right now.‚Äù And Representative Marjorie Taylor Greene: ‚ÄúI‚Äôm not voting for the development of Skynet and the rise of the machines.‚Äù



It‚Äôs a vibe shift that favors policy intervention and regulation, which I think is a good thing. Existing AI systems pose many near-term risks that need government attention. Voting to stop Skynet also stops immediate and actual harms.



And yet does a welcome end justify weird means? I‚Äôd like to see politicians voting with a clear-eyed sense of what this technology really is‚Äînot because they‚Äôve been sold on an AI bogeyman.¬†



This story originally appeared in The Algorithm, our weekly newsletter on AI. To get stories like this in your inbox first,¬†sign up here.
‚Ä¢ How lidar measures the cost of climate disasters
  Lidar is helping scientists track the effects of climate-driven disasters . The Eaton and Palisades fires in California in 2025 left an indelible mark on the landscape . Researchers are now using lidar (light detection and ranging) technology to precisely measure changes in the landscape‚Äôs geometry . The resulting visualizations reveal the scale of devastation in ways satellite imagery can‚Äôt match .
‚Ä¢ The Download: Google‚Äôs AI energy expenditure, and handing over DNA data to the police
  This is today&#8217;s edition of¬†The Download,¬†our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



In a first, Google has released data on how much energy an AI prompt uses



Google has just released a report detailing how much energy its Gemini apps use for each query. In total, the median prompt‚Äîone that falls in the middle of the range of energy demand‚Äîconsumes 0.24 watt-hours of electricity, the equivalent of running a standard microwave for about one second. The company also provided average estimates for the water consumption (five drops per query) and carbon emissions associated with a text prompt to Gemini.It‚Äôs the most transparent estimate yet from a Big Tech company with a popular AI product, and the report includes detailed information about how the company calculated its final estimate.



Earlier this year, MIT Technology Review published a comprehensive series on AI and energy, at which time none of the major AI companies would reveal their per-prompt energy usage. Google‚Äôs new publication, at last, allows for a peek behind the curtain that researchers and analysts have long hoped for. Read the full story.



‚ÄîCasey Crownhart







I gave the police access to my DNA‚Äîand maybe some of yours



Last year, I added my DNA profile to a private genealogical database, FamilyTreeDNA, and clicked ‚ÄúYes‚Äù to allow the police to search my genes.



In 2018, police in California announced they‚Äôd caught the Golden State Killer, a man who had eluded capture for decades. Once the police had ‚Äúmatches‚Äù to a few relatives of the killer, they built a large family tree from which they plucked the likely suspect.This process, called forensic investigative genetic genealogy, or FIGG, has since helped solve hundreds of murders and sexual assaults.



But I wasn‚Äôt really driven by some urge to capture distantly related serial killers. Rather, my spit had a less gallant and more quarrelsome motive: to troll privacy advocates whose fears around DNA I think are overblown and unhelpful. By giving up my saliva for inspection, I was going against the view that a person‚Äôs DNA is the individualized, sacred text that privacy advocates sometimes claim. Read the full story.



‚ÄîAntonio Regalado



This article appeared in The Checkup, MIT Technology Review‚Äôs weekly biotech newsletter. To receive it in your inbox every Thursday, and read articles like this first, sign up here.







Meet the researcher hosting a scientific conference by and for AI



In October, a new academic conference will debut that‚Äôs unlike any other. All of the work shared at Agents4Science will have been researched, written, and reviewed primarily by AI, and will be presented using text-to-speech technology.&nbsp;



That idea is not without its detractors. Among other issues, many feel AI is not capable of the creative thought needed in research, makes too many mistakes and hallucinations, and may limit opportunities for young researchers.&nbsp;



Nevertheless, a number of scientists and policymakers are very keen on the promise of AI scientists‚Äîand some even think they could unlock scientific discoveries that humans could never find alone. Read the full story.



‚ÄîPeter Hall







The must-reads



I‚Äôve combed the internet to find you today‚Äôs most fun/important/scary/fascinating stories about technology.



1 Elon Musk tried to persuade Mark Zuckerberg to buy OpenAIBut the bid was rejected earlier this year. (Insider $)+ OpenAI is asking Meta for evidence of any coordinated plans. (TechCrunch)+ I‚Äôm guessing the cage fight is still off then. (FT $)



2 AI giants are seeking real-world data that can‚Äôt be scraped from the internetIt‚Äôs a bid to make their models more accurate and to find new use cases. (Rest of World)



3 Russia‚Äôs state-backed messenger app will be preinstalled on all phonesCritics say the MAX app is essentially a government spy tool. (Reuters)+ Around 18 million people have registered to use it so far. (CNN)+ How Russia killed its tech industry. (MIT Technology Review)



4 The Trump administration is refusing to fully fund a major HIV programIt‚Äôs ignoring a directive from Congress to withhold around $3 billion. (NYT $)+ HIV could infect 1,400 infants every day because of US aid disruptions. (MIT Technology Review)



5 How Trump decides which chip companies may have to give up equityIncreasing your investments in the US? You‚Äôre off the hook. (WSJ $)+ America-first chipmaking remains a fantasy, though. (Economist $)+ Experts think Trump‚Äôs unconventional Intel deal may backfire. (Wired $)+ DeepSeek‚Äôs new AI model is compatible with Chinese-made chips. (FT $)



6 The EU is speeding up its plans for a digital euro It‚Äôs considering running it on a public blockchain, to experts‚Äô concern. (FT $)+ Is the digital dollar dead? (MIT Technology Review)



7 We don‚Äôt have to open new mines to obtain minerals for clean energyAlthough we have to get better at using the material we do mine. (New Scientist $)+ How one mine could unlock billions in EV subsidies. (MIT Technology Review)



8 This newly-discovered gene could usher in new chronic pain treatmentsOne day, cutting out certain foods could lessen discomfort. (Economist $)+ The pain is real. The painkillers are virtual reality. (MIT Technology Review)



9 Why Africa is buying so many solar panelsIt‚Äôs not just its more affluent nations snapping them up, either. (Wired $)+ The race to get next-generation solar technology on the market. (MIT Technology Review)



10 How families are using AI to run their householdsNo more quibbling over meal planning. (WP $)







Quote of the day



&#8220;If AGI doesn&#8217;t come to pass sometime soon, I wouldn&#8217;t be surprised if this whole thing pops.&#8221;



‚ÄîBhavya Kashyap, an angel investor, tells Insider why investors are fuelling a risky bubble by rushing to buy stocks in the hottest AI companies.







One more thing







How AI is changing gymnastics judgingThe 2023 World Championships last October marked the first time an AI judging system was used on every apparatus in a gymnastics competition. There are obvious upsides to using this kind of technology: AI could help take the guesswork out of the judging technicalities. It could even help to eliminate biases, making the sport both more fair and more transparent.At the same time, others fear AI judging will take away something that makes gymnastics special. Gymnastics is a subjective sport, like diving or dressage, and technology could eliminate the judges‚Äô role in crafting a narrative.For better or worse, AI has officially infiltrated the world of gymnastics. The question now is whether it really makes it fairer. Read the full story.



‚ÄîJessica Taylor Price







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)



+ Finally, some good news‚Äîa sweet little Australian marsupial called an ampurta is no longer endangered (thanks Glen!)+ What would a GTA set in London look like?+ Why glass houses aren‚Äôt all they‚Äôre cracked up to be (geddit?)+ Over in Denmark, there‚Äôs a national competition encouraging cities to get rid of their gray concrete tiles and replace them with peaceful green spaces (thanks Alice!)

üîí Cybersecurity & Privacy
No updates.

üéì University AI
No updates.

üè¢ Corporate AI
‚Ä¢ Applicability vs. job displacement: further notes on our recent research on AI and occupations
  Recently, we released a paper&nbsp;(Working with AI: Measuring the Occupational Implications of Generative AI)&nbsp;that studied what occupations might&nbsp;find&nbsp;AI chatbots&nbsp;useful, and to what degree.&nbsp;The paper sparked significant discussion,&nbsp;which is no&nbsp;surprise&nbsp;since&nbsp;people care&nbsp;deeply&nbsp;about&nbsp;the future of AI and&nbsp;jobs&#8211;that‚Äôs part of why we think&nbsp;it‚Äôs&nbsp;important to study these&nbsp;topics.



Unfortunately, not all the&nbsp;discussion&nbsp;was&nbsp;accurate&nbsp;in its portrayal of the&nbsp;study‚Äôs scope or conclusions.&nbsp;Specifically, our&nbsp;study&nbsp;does not&nbsp;draw any conclusions about jobs being eliminated; in the paper,&nbsp;we&nbsp;explicitly&nbsp;cautioned&nbsp;against using our findings to make that conclusion.&nbsp;



Given the importance&nbsp;of this&nbsp;topic, we&nbsp;want&nbsp;to&nbsp;clarify any misunderstandings and&nbsp;provide&nbsp;a more digestible summary of the paper,&nbsp;our&nbsp;methodology,&nbsp;and its limitations.&nbsp;



What&nbsp;did our research find?



We set out to better understand how people are using AI,¬†highlighting where AI might¬†be useful in different occupations.¬†To do this, we analyzed how people currently use generative AI‚Äîspecifically Microsoft Bing Copilot (now Microsoft Copilot)‚Äîto¬†assist¬†with¬†tasks.¬†We then compared these sets¬†of tasks against the O*NET database (opens in new tab), a widely used occupational classification system,¬†to understand potential applicability to various occupations.



We found&nbsp;that AI&nbsp;is most&nbsp;useful&nbsp;for&nbsp;tasks related to knowledge work and communication, particularly tasks such as writing, gathering information, and learning.



Those in occupations with these tasks&nbsp;may benefit by&nbsp;considering&nbsp;how AI&nbsp;can be used&nbsp;as a tool to help improve their workflows. On the&nbsp;flip side,&nbsp;it‚Äôs&nbsp;not surprising that physical tasks like performing surgeries or moving objects had less&nbsp;direct&nbsp;AI&nbsp;chatbot applicability.



So, to summarize, our paper is about&nbsp;identifying&nbsp;the occupations where&nbsp;AI may be most useful,&nbsp;by&nbsp;assisting&nbsp;or performing subtasks.&nbsp;&nbsp;Our data do&nbsp;not&nbsp;indicate, nor&nbsp;did&nbsp;we&nbsp;suggest, that certain jobs will be replaced by AI.



Methodological limitations are acknowledged‚Äîand important



The paper is transparent about the limitations of our approach.&nbsp;&nbsp;



We analyzed&nbsp;anonymized&nbsp;Bing Copilot conversations to see what&nbsp;activities&nbsp;users are seeking AI&nbsp;assistance&nbsp;with and what activities AI can perform when mapped to the O*NET database.&nbsp;While O*NET provides a structured list of&nbsp;activities&nbsp;associated with various occupations, it does&nbsp;not&nbsp;capture the full spectrum of skills, context, and nuance&nbsp;required&nbsp;in the real&nbsp;world.&nbsp;&nbsp;A job is far more than the collection of tasks that make&nbsp;it up.



For example, a task might involve ‚Äúwriting reports,‚Äù but O*NET&nbsp;won‚Äôt&nbsp;reflect the interpersonal judgment, domain&nbsp;expertise, or ethical considerations that go into doing that well. The paper acknowledges this gap and warns against over-interpreting the AI applicability scores as measures of AI‚Äôs ability to perform an occupation.



Additionally, the dataset is based on user queries from Bing Copilot (from January ‚Äì September 2024), which may be influenced by factors like awareness, access, or comfort with AI tools.&nbsp;&nbsp;Different people use different LLMs for different purposes and it also is&nbsp;very difficult&nbsp;(or&nbsp;nearly impossible) to&nbsp;determine&nbsp;what conversations are performed in a work context or for leisure.&nbsp;



Finally, we only evaluated AI chatbot usage, so this study does not evaluate the impact or applicability of other forms of AI.



Where do we go from here?



Given the intense interest in how AI will shape our collective future,&nbsp;it&#8217;s&nbsp;important we continue to study and better understand its societal and economic impact. As with&nbsp;all&nbsp;research on this topic,&nbsp;the findings&nbsp;are&nbsp;nuanced, and&nbsp;it‚Äôs&nbsp;important to pay attention to this nuance.&nbsp;



The public interest in our research is based, in large part, on the&nbsp;topic&nbsp;of AI&nbsp;and job displacement.&nbsp;However,&nbsp;our current&nbsp;methodology&nbsp;for this study&nbsp;is unlikely to lead to firm conclusions about this.&nbsp;&nbsp;AI may prove to be a useful tool for many occupations, and we believe the right balance lies in finding how to use the technology in a way that&nbsp;leverages&nbsp;its abilities while complementing human strengths and accounting for people&#8217;s preferences.&nbsp;&nbsp;&nbsp;&nbsp;



For more information from Microsoft on the future of work and AI skilling, check out Microsoft‚Äôs Annual&nbsp;Work Trend Index (opens in new tab)&nbsp;and&nbsp;Microsoft Elevate (opens in new tab).&nbsp;
Opens in a new tabThe post Applicability vs. job displacement: further notes on our recent research on AI and occupations appeared first on Microsoft Research.
‚Ä¢ Coauthor roundtable: Reflecting on healthcare economics, biomedical research, and medical education
  In November 2022, OpenAI‚Äôs ChatGPT kick-started a new era in AI. This was followed less than a half year later by the release of GPT-4. In the months leading up to GPT-4‚Äôs public release, Peter Lee, president of Microsoft Research, cowrote a book full of optimism for the potential of advanced AI models to transform the world of healthcare. What has happened since? In this special podcast series, The AI Revolution in Medicine, Revisited, Lee revisits the book, exploring how patients, providers, and other medical professionals are experiencing and using generative AI today while examining what he and his coauthors got right‚Äîand what they didn‚Äôt foresee.&nbsp;



In this series finale, Lee welcomes back coauthors Carey Goldberg (opens in new tab) and Dr. Zak Kohane (opens in new tab) to discuss how their predictions stack up against key takeaways from guests in the second half of the series: experts on AI‚Äôs economic and societal impact; technologists on the cutting edge; leaders in AI-driven medicine; next-generation physicians; and heads of healthcare organizations. Lee, Goldberg, and Kohane explore thinking innovatively about existing healthcare processes, including the structure of care teams and the role of specialties, to take advantage of AI opportunities and consider what clinicians and patients might need these new AI tools to be to feel empowered when it comes to giving and receiving the best healthcare. They close the episode with their hopes for the future of AI in health.








Learn more:




Scalable emulation of protein equilibrium ensembles with generative deep learning&nbsp;Publication | July 2025



Sequential Diagnosis with Language Models&nbsp;Publication | July 2025



Developing next-generation cancer care management with multi-agent orchestration&nbsp;Microsoft Industry Blogs | May 2025



The AI Revolution in Medicine: GPT-4 and BeyondBook | Peter Lee, Carey Goldberg, Isaac Kohane | April 2023&nbsp;










	
		
			Subscribe to the Microsoft Research Podcast:		
		
							
					
						  
						Apple Podcasts
					
				
			
							
					
						
						Email
					
				
			
							
					
						
						Android
					
				
			
							
					
						
						Spotify
					
				
			
							
					
						
						RSS Feed
					
				
					
	




	
		
			
				
					

Transcript



[MUSIC]‚ÄØ



[BOOK PASSAGE]&nbsp;



PETER LEE: ‚ÄúAs a society‚Äîindeed, as a species‚Äîwe have a choice to make. Do we constrain or even kill artificial intelligence out of fear of its risks and obvious ability to create new harms? Do we submit ourselves to Al and allow it to freely replace us, make us less useful and less needed? Or do we start, today, shaping our Al future together, with the aspiration to accomplish things that humans alone, and Al alone, can&#8217;t do but that humans+Al can? The choice is in our hands ‚Ä¶ .‚Äù&nbsp;



[END OF BOOK PASSAGE]



[THEME MUSIC]



This is The AI Revolution in Medicine, Revisited. I‚Äôm your host, Peter Lee.‚ÄØ



Shortly after OpenAI&#8217;s GPT-4 was publicly released, Carey Goldberg, Dr. Zak Kohane, and I published The AI Revolution in Medicine to help educate the world of healthcare and medical research about the transformative impact this new generative AI technology could have. But because we wrote the book when GPT-4 was still a secret, we had to speculate. Now, two years later, what did we get right, and what did we get wrong?‚ÄØ



In this series, we‚Äôll talk to clinicians, patients, hospital administrators, and others to understand the reality of AI in the field and where we go from here.‚ÄØ



				
				
					



[THEME MUSIC FADES]&nbsp;



The book passage I read at the top is from the epilogue, and I think it‚Äôs a truly fitting closing sentiment for the conclusion of this podcast series‚Äîbecause it calls back to the very beginning.



As I‚Äôve mentioned before, Carey, Zak, and I wrote The AI Revolution in Medicine as a guide to help answer these big questions, particularly as they pertain to medicine. You know, we wrote the book to empower people to make a choice about AI‚Äôs development and use. Well, have they? Have we?



Perhaps we‚Äôll need more time to tell. But over the course of this podcast series, I‚Äôve had the honor of speaking with folks from across the healthcare ecosystem. And my takeaway? They‚Äôre all committed to shaping AI into a tool that can improve the industry for practitioners and patients alike.



In this final episode, I‚Äôm thrilled to welcome back my coauthors, Carey Goldberg and Dr. Zak Kohane. We‚Äôll examine the insights from the second half of the season.&nbsp;



[TRANSITION MUSIC]&nbsp;



Carey, Zak‚Äîit‚Äôs really great to have you here again!&nbsp;



CAREY GOLDBERG: Hey, Peter!&nbsp;



ZAK KOHANE: Hi, Peter.&nbsp;



LEE: So this is the second roundtable. And just to recap, you know, we had several early episodes of the podcast where we talked to some doctors, some technology developers, some people who think about regulation and public policy, patient advocates, a venture capitalist who invests in, kind of, consumer and patient-facing medical ventures, and some bioethicists.&nbsp;



And I think we had a great conversation there. I think, you know, it felt mostly validating. A lot of the things that we predicted might happen happened, and then we learned a lot of new things. But now we have five more episodes, and the mix of kinds of people that we talk to here is different than the original.&nbsp;



And so I thought it would be great for us to have a conversation and recap what we think we heard from all of them. So let&#8217;s just start at the top.&nbsp;



So in this first episode in the second half of this podcast series, we talked to economists Azeem Azhar and Ethan Mollick. And I thought those conversations were really interesting. Maybe there were, kind of, two things, two main topics. One was just the broader impact on the economy, on the cost of healthcare, on overall workforce issues.&nbsp;



One of the things that I thought was really interesting was something that Ethan Mollick brought up. And maybe just to refresh our memories, let&#8217;s play this little clip from Ethan.&nbsp;



ETHAN MOLLICK: So we‚Äôre in this really interesting period where there‚Äôs incredible amounts of individual innovation in productivity and performance improvements in this field, like very high levels of it. ‚Ä¶ We‚Äôre seeing that in nonmedical problems, the same kind of thing, which is, you know, we‚Äôve got research showing 20 and 40% performance improvements. ‚Ä¶ But then the organization doesn‚Äôt capture it; the system doesn‚Äôt capture it. Because the individuals are doing their own work, and the systems don‚Äôt have the ability to, kind of, learn or adapt as a result.&nbsp;



LEE: So let me start with you, Zak. Does that make sense to you? Are you seeing something similar?&nbsp;



KOHANE: I thought it was incredibly insightful because we discussed on our earlier podcast how a chief AI officer in one of the healthcare hospitals, in one of the healthcare systems, was highly regulating the use of AI, but yet in her own practice on her smartphone was using all these AI technologies.&nbsp;



And so it&#8217;s insightful that on the one hand, she is increasing her personal productivity, ‚Ä¶&nbsp;



LEE: Right.&nbsp;



KOHANE: ‚Ä¶ and perhaps she&#8217;s increasing her quality of her care. But it&#8217;s very hard for the healthcare system to actually realize any gains. It&#8217;s unlikely ‚Ä¶ let&#8217;s put it this way. It would be for her a defeat if they said, ‚ÄúNow you should see more patients.‚Äù&nbsp;



LEE: Yes. [LAUGHS]&nbsp;



KOHANE: Now, I&#8217;m not saying that won&#8217;t happen. It could happen. But, you know, gains of productivity are really at the individual level of the doctors. And that&#8217;s why they&#8217;re adopting it. That&#8217;s why the ambient dictation tools are so successful. But really turning it into things that matter in terms of productivity for healthcare, namely making sure that patients are getting healthy, requires that every piece of the puzzle works well together. You know, it&#8217;s well-tread ground to talk about how patients get very expensive procedures, like a cardiac transplant, and then go home, and they‚Äôre not put on blood thinners ‚Ä¶&nbsp;



LEE: Right.&nbsp;



KOHANE: ‚Ä¶ and then they get a stroke. You know, the chain is as strong as the weakest link. And just having AI in one part of it is not going to do it. And so hospitals, I think, are doubly burdened by the fact that, (A) they tend to not like innovation because they are high-revenue, low-margin companies. But if they want it implemented effectively, they have to do it across the entire processes of healthcare, which are vast and not completely under their control.&nbsp;



LEE: Yeah. Yep. You know, that was Sara Murray, who&#8217;s the chief health AI officer at UC San Francisco.&nbsp;



And then, you know, Carey, remember, we were puzzled by Chris Longhurst&#8217;s finding in a controlled study that the, you know, having an AI respond to patient emails didn&#8217;t seem to lead to any, I guess you would call it, productivity benefits. I remember we were both kind of puzzled by that. I wonder if that&#8217;s related to what Ethan is saying here.&nbsp;



GOLDBERG: I mean, possibly, but I think we&#8217;ve seen since then that there have been multiple studies showing that in fact using AI can be extremely effective or helpful, even, for example, for diagnosis.&nbsp;



And so I find just from the patient point of view, it kind of drives me crazy that you have individual physicians using AI because they know that it will improve the care that they&#8217;re offering. And yet you don&#8217;t have their institutions kind of stepping up and saying, ‚ÄúOK, these are the new norms.‚Äù&nbsp;



By the way, Ethan Mollick is a national treasure, right. Like, he is the classic example of someone who just stepped up at this moment ‚Ä¶&nbsp;



LEE: Yeah.&nbsp;



GOLDBERG: ‚Ä¶ when we saw this extraordinary technological advance. And he&#8217;s not only stepping up for himself. He&#8217;s spreading the word to the masses that this is what these things can do.&nbsp;



And so it&#8217;s frustrating to see the institutions not stepping up and instead the individual doctors having to do it.&nbsp;



KOHANE: But he made another very interesting point, which was that the reason that he could be so informative to not only the public but practitioners of AI is these things would emerge out of the shop, and they would not be aged too long, like a fine wine, before they were just released to the public.&nbsp;



And so he was getting exposure to these models just weeks after some of the progenitors had first seen it. And therefore, because he&#8217;s actually a really creative person in terms of how he exercises models, he sees uses and problems very early on. But the point is institutions, think about how much they are disadvantaged. They&#8217;re not Ethan Mollick. They&#8217;re not the progenitors. So they&#8217;re even further behind. So it&#8217;s very hard. If you talk to most of the C-suite of hospitals, they&#8217;d be delighted to know as much about the impact as Ethan Mollick.&nbsp;



LEE: Yeah. By the way, you know, I picked out this quote because within Microsoft, and I suspect every other software company, we&#8217;re seeing something very similar, where individual programmers are 20 to 30% more productive just in the number of lines of code they write per day or the number of pull requests per week. Any way you measure it, it&#8217;s very consistent. And yet by the time you get to, say, a 25-person software engineering team, the productivity of that whole team isn&#8217;t 25% more productive.&nbsp;



Now, that is starting to change because we&#8217;re starting to figure out that, well, maybe we should reshape how the team operates. And there&#8217;s more of an orientation towards having, you know, smaller teams of full-stack developers. And then you start to see the gains. But if you just keep the team organized in the usual way, there seems to be a loss. So there&#8217;s something about what Ethan was saying that resonated very strongly with me.&nbsp;



GOLDBERG: But I would argue that it&#8217;s not just productivity we&#8217;re talking about. There&#8217;s a moral imperative to improve the care. And if you have tools that will do that, you should be using them or trying harder to.&nbsp;



LEE: Right. Yep.&nbsp;



KOHANE: I think, yes, first of all, absolutely you would. Unfortunately, most of the short-term productivity measures will not measure improvements in the quality of care because it takes a long time to die even with bad care.&nbsp;



And so that doesn&#8217;t show up right away. But I think what Peter just said actually came across in several of the podcasts, which is that it&#8217;s very tricky trying to shoehorn these things into making what we&#8217;re already doing more productive.&nbsp;



GOLDBERG: Yeah. Existing structures.&nbsp;



KOHANE: Yeah. And I know, Carey, that you&#8217;ve raised this issue many times. But it really calls into question, what should we be doing with our time with doctors? And they are a scarce resource. And what is the most efficient way to use them?&nbsp;



You know, I remember we [The New England Journal of Medicine AI] published a paper of someone who was able to use AI to increase the throughput of their emergency room (opens in new tab) by actually more appropriately having the truly sick people in the sick queue, in the triage queue, for urgent care.&nbsp;



And so I think we&#8217;re going to have to think that way more broadly, about we don&#8217;t have to now look at every patient as an unknown with maybe a few pointers on diagnosis. We can have a fairly extensive profiling.&nbsp;



And I know that colleagues in Clalit [Health Services] in Israel, for example, are using the overall trajectory of the patient and some considerations about utilities to actually figure out who to see next week.&nbsp;



LEE: Yeah, you know, what you said brings up another maybe connection to one thing that we see also in software development. And it relates to also what we were discussing earlier: about the last thing a doctor wants is to have a tool that allows them to see even yet more patients per day.&nbsp;



So in software development, there&#8217;s always this tension. Like, how many lines of code can you write per day? That&#8217;s one productivity measure.&nbsp;



But sometimes we&#8217;re taught, well, don&#8217;t write more lines of code per day, but make sure that your code is well structured. Take the time to document it. Make sure it&#8217;s fully commented. Take the time to talk to your fellow software engineering team members to make sure that it&#8217;s well coordinated. And in the long run, even if you&#8217;re writing half the number of lines of code per day, the software process will be far more efficient.



And so I&#8217;ve wondered whether there&#8217;s a similar thing where doctors could see 20% fewer patients in a day, but if they take the time and also had AI help to coordinate, maybe a patient&#8217;s journey might be half as long. And therefore, the health system would be able to see twice as many patients in a year&#8217;s period or something like that.&nbsp;



KOHANE: So I think you&#8217;ve ‚Äúnerd sniped‚Äù me because you [LAUGHTER]‚Äîwhich is all too easy‚Äîbut I think there&#8217;s a central issue here. And I think this is the stumbling block between what Ethan&#8217;s telling us about between the individual productivity and the larger productivity, is the team&#8217;s productivity.&nbsp;



And there is actually a good analogy in computer science and that&#8217;s, uh, Brooks‚Äôs ‚Äúmythical man-month,‚Äù &#8230;&nbsp;



LEE: Yes, exactly.&nbsp;



KOHANE: ‚Ä¶ where he shows how you can have more and more resources, but when the coordination starts failing, because you have so many, uh, individuals on the team, you start falling apart. And so even if the, uh, individual doctors get that much better, yeah, they take better care of patients, make less stupid things.&nbsp;



But in terms of giving the ‚ÄúI get you into the emergency room, and I get you out of a hospital as fast as possible, as safely as possible, as effectively as possible,‚Äù that&#8217;s teamwork. And we don&#8217;t do it. And we&#8217;re not really optimizing our tools for that.&nbsp;



GOLDBERG: And just to throw in a little reality check, I&#8217;m not aware of any indication yet that AI is in any way shortening medical journeys or making physicians more efficient. Yet ‚Ä¶&nbsp;



LEE: Right.&nbsp;



GOLDBERG: ‚Ä¶ at least. Yeah.&nbsp;



LEE: Yes. So I think, you know, with respect to our book, critiquing our book, you know, I think it&#8217;s fair to say we were fairly focused or maybe even fixated on the individual doctor or nurse or patient, and we didn&#8217;t really, at least I never had a time where I stepped back to think about the whole care coordination team or the whole health system.&nbsp;



KOHANE: And I think that&#8217;s right. It&#8217;s because, first of all, you weren‚Äôt thinking about it? It&#8217;s not what we&#8217;re taught in medical school. We&#8217;re not taught to talk about team communication excellence. And I think it&#8217;s absolutely essential.&nbsp;



There‚Äôs a ‚Ä¶ what‚Äôs the ‚Ä¶ there was an early ‚Ä¶ [Terry] Winograd. And he was trying to capture what are the different kinds of actions related to pronouncements that you could expect and how could AI use that. And that was beginning to get at it.&nbsp;



But I actually think this is dark matter of human organizational technology that is not well understood. And our products don&#8217;t do well. You know, we can talk about all the groupware things that are out there. But they all don&#8217;t quite get to that thing.&nbsp;



LEE: Right.&nbsp;



KOHANE: And I can imagine an AI serving as a team leader, a really active team leader, a real quarterback of, let&#8217;s say, a care team.&nbsp;



LEE: Well, in fact, you know, we have been trying to experiment with this. My colleague, Matt Lungren, who was also one of the interviewees early on, has been working with Stanford Medicine on a tumor board AI agent‚Äîsomething that would facilitate tumor board meetings.&nbsp;



And the early experiences are pretty interesting. Whether it relates to efficiency or productivity I think remains to be seen, but it does seem pretty interesting.&nbsp;



But let&#8217;s move on.&nbsp;



GOLDBERG: Well, actually, Peter, ‚Ä¶&nbsp;



LEE: Oh, go ahead.&nbsp;



GOLDBERG: ‚Ä¶ if you&#8217;re willing to not quite move on yet ‚Ä¶&nbsp;



LEE: [LAUGHS] All right.&nbsp;



GOLDBERG: ‚Ä¶ this kind of segues into one of, I think, the most provocative questions that arose in the course of these episodes and that I&#8217;d love to have you answer, which was, remember, it was a question at a gathering that you were at, and you were asked, ‚ÄúWell, you&#8217;re focusing a lot on potential AI effects on individual patient and physician experiences. But what about the revolution, right? What about, like, can you be more big-picture and envision how generative AI could actually, kind of, overturn or fix the broken system, right?‚Äù&nbsp;



I&#8217;m sure you&#8217;ve thought about that a lot. Like, what&#8217;s your answer?&nbsp;



LEE: You know, I think ultimately, it will have to. For it to really make a difference, I think that the normal processes, our normal concept of how healthcare is delivered‚Äîhow new medical discoveries are made and brought into practice‚ÄîI think those things are going to have to change a lot.&nbsp;



You know, one of the things I think about a lot right at the moment is, you know, we tend to think about, let&#8217;s say, medical diagnosis as a problem-solving exercise. And I think, at least at the Kaiser Permanente School of Medicine, the instruction really treats it as a kind of detective thing based on a lot of knowledge about biology and biomedicine and human condition, and so on.&nbsp;



But there&#8217;s another way to think about it, given AI, which is when you see a patient and you develop some data, maybe through a physical exam, labs, and so on, you can just simply ask, ‚ÄúYou know, what did the 500 other people who are most similar to this experience, how were they diagnosed? How were they treated? What were their outcomes? What were their experiences?‚Äù&nbsp;



And that&#8217;s really a fundamentally different paradigm. And it just seems like at least the technical means will be there. And by the way, that also then relates to [the questions]: ‚ÄúAnd what was most efficacious cost-wise? What was most efficient in terms of the total length of the patient journey? How does this relate to my quality scores so I can get more money from Medicare and Medicaid?‚Äù&nbsp;



All of those things, I think, you know, we&#8217;re starting to confront.&nbsp;



One of the other episodes that we&#8217;re going to talk about, was my interview with two medical students. Actually, thinking of a Morgan Cheatham as just a medical student or medical resident [LAUGHTER] is a little strange. But he is.&nbsp;



One of the things he talks about is the importance that he placed in his medical training about adopting AI. So, Zak, I assume you see this also with some students at Harvard Medical School. And the other medical student we interviewed, Daniel Chen, seemed to indicate this, too, where it seems like it&#8217;s the students who are bringing AI into the medical education ahead of the faculty. Does that resonate with you?&nbsp;



KOHANE: It absolutely resonates with me. There are students I run into who, honestly, my first thought when I&#8217;m talking to them is, why am I teaching you [LAUGHTER], and why are you not starting a big AI company, AI medicine company, now and really change healthcare instead of going through the rest of the rigmarole? And I think broadly, higher education has a problem there, which is we have not embraced, again, going back to Ethan, a lot of the tools that can be used. And it&#8217;s because we don&#8217;t know necessarily the right way to teach them. And so far, the only lasting heuristic seems to be: use them and use them often.&nbsp;



And so it&#8217;s an awkward thing, where the person who knows how to use the AI tools now in the first-year medical school can teach themselves better and faster than anybody else in their class who is just relying on the medical school curriculum.&nbsp;



LEE: Now, the reason I brought up Morgan now after our discussion with Ethan Mollick is Morgan also talked about AI collapsing medical specialties.&nbsp;



GOLDBERG: Yes.&nbsp;



LEE: And so let&#8217;s hear this snippet from him.&nbsp;



MORGAN CHEATHAM: AI collapses medical specialties onto themselves, right. You have the canonical example of the cardiologist, you know, arguing that we should diuresis and maybe the nephrologist arguing that we should, you know, protect the kidneys. And how do two disciplines disagree on what is right for the patient when in theory, there is an objective best answer given that patient‚Äôs clinical status? ‚Ä¶ So I‚Äôm interested in this question of whether medical specialties themselves need to evolve. And if we look back in the history of medical technology, there are many times where a new technology forced a medical specialty to evolve.



LEE: So on the specific question about specialties, Zak, do you have a point of view? And let me admit, first of all, for us, all three of us, we didn&#8217;t have any clue about this in our book. I don&#8217;t think.&nbsp;



KOHANE: Not much. Not much of a clue.&nbsp;



So I&#8217;m reminded of a New Yorker cartoon where you see a bunch of surgeons around the patient, and someone says, ‚ÄúIs that a spleen?‚Äù And it says, ‚ÄúI don&#8217;t know. I slept during the spleen lecture,‚Äù [LAUGHTER] and &#8230; or ‚ÄúI didn&#8217;t take the spleen course.‚Äù&nbsp;



And yet when we measure things, we measure things much more than we think we are doing. So for example, we [NEJM AI] just published a paper where echocardiograms were being done. And it turns out those ultrasound waves just happen to also permeate the liver. And you can actually diagnose on the way with AI all the liver disease (opens in new tab) that is in‚Äîand treatable liver disease‚Äîthat&#8217;s in those patients.&nbsp;



But if you&#8217;re a cardiologist, ‚ÄúLiver? You know, I slept through liver lecture.‚Äù [LAUGHTER] And so I do think that, (A) the natural, often guild/dollar-driven silos in medicine are less obvious to AI, despite the fact that they do exist in departments and often in chapters.&nbsp;



But Morgan&#8217;s absolutely right. I can tell you as an endocrinologist, if I have a child in the ICU, the endocrinologist, the nephrologist, and the neurosurgeon will argue about the right thing to do.&nbsp;



And so in my mind, the truly revolutionary thing to do is to go back to 1994 with Pete Szolovits, the Guardian Angel Project (opens in new tab). What I think you need is a process. And the process is the quarterback. And the quarterback has only one job: take care of the patient.&nbsp;



And it should be thinking all the time about the patient. What&#8217;s the right thing? And can be as school-marmish or not about, ‚ÄúZak, you&#8217;re eating this or that or exercise or sleep,‚Äù but also, ‚ÄúHey, surgeons and endocrinologists, you&#8217;re talking about my host, Zak. This is the right way because this problem and this problem and our best evidence is this is the right way to get rid of the fluid. The other ways will kill him.‚Äù



And I think you need an authoritative quarterback that has the view of the others but then makes the calls.&nbsp;



LEE: Is that quarterback going to be AI or human?&nbsp;



KOHANE: Well, for the very lucky people, it&#8217;ll be a human augmented by AI, super concierge.&nbsp;



But I think we&#8217;re running out of doctors. And so realistically, it&#8217;s going to be an AI that will have to be certified in very different ways, along the ways Dave Blumenthal says, essentially, trial by fire. Like putting residents into clinics, we&#8217;re going to be putting AIs into clinics.&nbsp;



But what&#8217;s worse, by the way, than the three doctors arguing about care in front of the patient is, what happens so frequently, is then you see them outpatient, and each one of them gives you a different set of decisions to make. Sometimes that actually interact pathologically, unhealthily with each other. And only the very smart nurses or primary care physicians will actually notice that and call, quote, a ‚Äúfamily meeting,‚Äù or bring everybody in the same room to align them.&nbsp;



LEE: Yeah, I think this idea of quarterback is really very, very topical right now because there&#8217;s so much intensity in the AI space around agents. And in fact, you know, the Microsoft AI team under Mustafa Suleyman and Dominic King, Harsha Nori, and team just recently posted a paper on something called sequential diagnosis, which is basically an AI quarterback that is supposed to smartly consult with other AI specialties. And interestingly, one of the AI agents is sort of the devil&#8217;s advocate that&#8217;s always criticizing and questioning things.¬†



GOLDBERG: That‚Äôs interesting.&nbsp;



LEE: And at least on very, very hard, rare cases, it can develop some impressive results. There&#8217;s something to this that I think is emerging.&nbsp;



GOLDBERG: And, Peter, Morgan said something that blew me away even more, which was, well, why do we even need specialists if the reason for a specialist is because there&#8217;s so much medical knowledge that no single physician can know all of it, and therefore we create specialists, but that limitation does not exist for AI.&nbsp;



LEE: Yeah. Yeah.&nbsp;



GOLDBERG: And so there he was kind of undermining this whole elaborate structure that has grown up because of human limitations that may not ultimately need to be there.&nbsp;



LEE: Right. So now that gives me a good segue to get back to our economist and get to something that Azeem Azhar said. And so there&#8217;s a clip here from Azeem.&nbsp;



AZEEM AZHAR: We didn‚Äôt talk about, you know, AI in its ability to potentially do this, which is to extend the clinician‚Äôs presence throughout the week. You know, the idea that maybe some part of what the clinician would do if you could talk to them on Wednesday, Thursday, and Friday could be delivered through an app or a chatbot just as a way of encouraging the compliance, which is often, especially with older patients, one reason why conditions, you know, linger on for longer.&nbsp;



LEE: And, you know, in the same conversation, he also talked about his own management of asthma and the fact that he&#8217;s been managing this for several decades and knows more than any other human being, no matter how well medically trained, could possibly know. And it&#8217;s also very highly personalized. And it&#8217;s not a big leap to imagine AI having that sort of lifelong understanding.&nbsp;



KOHANE: So in fact, I want to give credit back to our book since you insulted us. [LAUGHTER] You challenged us. You doubted us. We do have at the end of the book a AI which is helping this woman manage her way through life. It&#8217;s quarterbacking for the woman all these different services.&nbsp;



LEE: Yes.&nbsp;



KOHANE: So there.&nbsp;



LEE: Ah, you&#8217;re right. Yes. In fact, it&#8217;s very much, I think, along the lines of the vision that Azeem laid out in our conversation.&nbsp;



GOLDBERG: Yeah. It also reminded me of the piece Zak wrote about his mother (opens in new tab) at one point when she was managing congestive heart failure and she needed to watch her weight very carefully to see her fluid status. And absolutely, there&#8217;s no ‚Ä¶ I see no reason whatsoever why that couldn&#8217;t be done with AI right now. Actually, although back then, Zak, you were writing that it takes much more than an AI [LAUGHS] to manage such a thing, right?&nbsp;



KOHANE: You need an AI that you can trust. Now, my mother was born in 1927, and she&#8217;d learned through the school of hard knocks that you can&#8217;t trust too many people, maybe even not your son, MD, PhD [LAUGHTER].&nbsp;



But what I&#8217;ve been surprised [by] is how, for example, how many people are willing to trust and actually see effective use of AI as mental health counselors, for example.&nbsp;



GOLDBERG: Yeah&nbsp;



KOHANE: So it may in fact be that there&#8217;s a generational thing going on, and at least there&#8217;ll be some very large subset of patients which will be completely comfortable in ways that my mother would have never tolerated.&nbsp;



LEE: Yeah. Now, I think we&#8217;re starting to veer into some of the core AI.&nbsp;



And so I think maybe one of the most fun conversations I had was in the episode with both S√©bastien Bubeck, my former colleague at Microsoft Research, and now he&#8217;s at OpenAI, and Bill Gates. And there was so much that was, I thought, interesting there. And there was one point, I think that sort of touches tangentially on what we were just conversing about, that S√©bastien said. So let&#8217;s hear this snippet.&nbsp;



S√âBASTIEN BUBECK: And one example that I really like, a study that recently appeared where ‚Ä¶ they were comparing doctors without and with ChatGPT. ‚Ä¶ So this was a set of cases where the accuracy of the doctors alone was around 75%. ChatGPT alone was 90%. ‚Ä¶ But then the kicker is that doctors with ChatGPT was 80%. Intelligence alone is not enough. It‚Äôs also how it‚Äôs presented, how you interact with it. And ChatGPT, it‚Äôs an amazing tool. Obviously, I absolutely love it. But it‚Äôs not ‚Ä¶ you don‚Äôt want a doctor to have to type in, you know, prompts and use it that way. It should be, as Bill was saying, kind of running continuously in the background, sending you notifications.



LEE: So I thought S√©bastien was saying something really profound, but I haven&#8217;t been able to quite decide or settle in my mind what it is. What do you make of what Seb just said?&nbsp;



KOHANE: I think it&#8217;s context. I think that it requires an enormous amount of energy, brain energy, to actually correctly provide the context that you want this thing to work on. And it&#8217;s only going to really feel like we&#8217;re in a different playing field when it&#8217;s listening all the time, and it just steps right in.&nbsp;



There is an advantage that, for example, a good programmer can have in prompting Cursor or any of these tools to do so. But it takes effort. And I think being in the conversation all the time so that you understand the context in the widest possible way is incredibly important. And I think that&#8217;s what Seb is getting at, which is if we spoon feed these machines, yes, 90%.&nbsp;



But then, talking to a human being who then has to interact and gets distracted from whatever flow they&#8217;re in and maybe even makes them feel like an early bicycle rider who all of a sudden realizes, ‚ÄúI&#8217;m balancing on two wheels‚Äîoh no!‚Äù And they fall over. You know, there&#8217;s that interaction which is negatively synergistic.&nbsp;



And so I do think it&#8217;s a very hard human-computer engineering problem. How do we make these two agents, human and computational, work in an ongoing way in the flow? I don&#8217;t think I&#8217;m seeing anything that&#8217;s particularly new. And the things that you&#8217;re beginning to hint about, Peter, in terms of agentic coordination, I think we&#8217;ll get to some of that. 



LEE: Yeah. Carey, does this give you any pause? The kind of results that ‚Ä¶ they&#8217;re puzzling results. I mean, the idea of doctors with AI seeming at least in this one test‚Äîit&#8217;s just one test‚Äîbut it&#8217;s odd that it does worse than the AI alone.&nbsp;



GOLDBERG: Yes. I would want to understand more about the actual conditions of that study.&nbsp;



From what Bill Gates said, I was most struck by the question of resource-poor environments. That even though this was absolutely one of the most promising, brightest perspectives that we highlighted in the book, we still don&#8217;t seem to be seeing a lot of use among the one half of humanity that lacks decent access to healthcare.&nbsp;



I mean, there are access problems everywhere, including here in the United States. And it is one of the most potentially promising uses of AI. And I thought if anyone would know about it, he would with the work that the Gates Foundation does.&nbsp;



LEE: You know, I think both you and Bill, I felt, are really simpatico. You know, Bill expressed genuine surprise that more isn&#8217;t happening yet. And it really echoed, in fact, maybe even using some of the exact same words that you&#8217;ve used. And so two years on, you&#8217;ve expressed repeatedly expecting to have seen more out in the field by now. And then I thought Bill was saying something in our conversation very similar.&nbsp;



GOLDBERG: Yeah.&nbsp;



LEE: You know, for me, I see it both ways. I see the world of medicine really moving fast in confronting the reality of AI in such a serious way. But at the same time, it&#8217;s also hard to escape the feeling that somehow, we should be seeing even more.&nbsp;



So it&#8217;s an odd thing, a little bit paradoxical.&nbsp;



GOLDBERG: Yeah. I think one thing that we didn&#8217;t focus on hardly at all in the book but that we are seeing is these companies rising up, stepping up to the challenge, Abridge and OpenEvidence, and what Morgan describes as a new stack, right.&nbsp;



So there is that on the flip side.&nbsp;



LEE: Now, I want to get back to this thing that Seb was saying. And, you know, I had to bring up the issue of sycophancy, which we discussed at our last roundtable also. But it was particularly ‚Ä¶ at the time that Seb, Bill, and I had our conversation, OpenAI had just gone through having to retract a fresh update of GPT-4o because it had become too sycophantic.&nbsp;



So I can&#8217;t escape the feeling that some of these human-computer interaction issues are related to this tension between you want AI to follow your directions and be faithful to you, but at the same time not agree with you so often that it becomes a fault.&nbsp;



KOHANE: I think it&#8217;s asking the AI to enter into a fundamental human conundrum, which is there are extreme versions of doublethink, and there&#8217;s everyday things, everyday asks of doublethink, which is how to be an effective citizen.&nbsp;



And even if you&#8217;re thinking, ‚ÄúHmm. I&#8217;m thinking this. I&#8217;m just not going to say it because that would be rude or counterproductive.‚Äù Or some of the official doublethinks, where you&#8217;re actually told you must say this, even if you think something else. And I think we&#8217;re giving a very tough mission for these things: be nice to the user and be useful.&nbsp;



And, in education, where the thing is not always one in the same. Sometimes you have to give a little tough love to educate someone, and doing that well is both an art and it&#8217;s also very difficult. And so, you know, I&#8217;m willing to believe that the latest frontier models that have made the news in the last month are very high-performing, but they&#8217;re also all highlighting that tension ‚Ä¶&nbsp;



LEE: Yes.&nbsp;



KOHANE: ‚Ä¶ that tension between behaving like a good citizen and being helpful. And this gets back to what are the fundamental values that we hope these things are following.&nbsp;



It&#8217;s not, you know, ‚ÄúAre these things going to develop us into the paperclip factory?‚Äù It&#8217;s more of, ‚ÄúWhich of our values are going to be elevated, and which one will be suppressed?‚Äù&nbsp;



LEE: Well, since I criticized our book before, let me pat ourselves on the back this time because, I think, pervasive throughout our book, we were touching on some of these issues.&nbsp;



In fact, we started the book, you know, with GPT-4 scolding me for wanting it to impersonate Zak. And there was the whole example of asking it to rewrite a poem in a certain way, and it kind of silently just tried to slide, you know, without me knowing, slide by without following through on the whole thing.&nbsp;



And so that early version of GPT-4 was definitely not sycophantic at all. In fact, it was just as prone to call you an idiot if it thought you were wrong. [LAUGHTER]&nbsp;



KOHANE: I had some very testy conversations around my endocrine diagnosis with it. [LAUGHTER]&nbsp;



GOLDBERG: Yeah. Well then, Peter, I would ask you, I mean last time I asked you about, well, hallucinations, aren&#8217;t those solvable? And this time I would ask you, well, sycophancy, isn&#8217;t that kind of like a dial you can turn? Like, is that not solvable?&nbsp;



LEE: You know, I think there are several interlocking problems. But if we assume superintelligence, even with superintelligence, medicine is such an inexact science that there will always be situations that are guesses that take into account other factors of a person&#8217;s life, other value judgments, exactly as Zak had pointed out in our previous roundtable conversation.&nbsp;



And so I think there&#8217;s always going to be an opening for either differences of opinion or agreeing with you too much. And there are dangers in both cases. And I think they&#8217;ll always be present. I don&#8217;t know that, at least in something as inexact as medical science, I don&#8217;t know that it&#8217;ll ever be completely eliminated.&nbsp;



KOHANE: And it&#8217;s interesting because I was trying to think what&#8217;s the right balance, but there are patients who want to be told this is what you do. Whereas there&#8217;s other patients who want to go through every detail of the reasoning.&nbsp;



And it&#8217;s not a matter of education. It&#8217;s really a temperamental, personality issue. And so we&#8217;re going to have to, I think, develop personalities ‚Ä¶&nbsp;



LEE: Yeah.&nbsp;



KOHANE: ‚Ä¶ that are most effective for those different kinds of individuals. And so I think that is going to be the real frontier. Having human values and behaving in ways that are recognizable and yet effective for certain groups of patients.&nbsp;



LEE: Yeah.&nbsp;



KOHANE: And lots of deep questions, including how paternalistic do we want to be?&nbsp;



LEE: All right, so we&#8217;re getting into medical science and hallucination. So that gives me a great segue to the conversations in the episode on biomedical research. And one of the people that I interviewed was Noubar Afeyan from Moderna and Flagship Pioneering. So let&#8217;s listen to this snippet.&nbsp;



NOUBAR AFEYAN: We, some hundred or so times a year, ask ‚Äúwhat if‚Äù questions that lead us to totally weird places of thought. We then try to iterate, iterate, iterate to come up with something that‚Äôs testable. Then we go into a lab, and we test it.‚ÄØSo in that world, right, sitting there going, like, ‚ÄúHow do I know this transformer is going to work?‚Äù The answer is, ‚ÄúFor what?‚Äù Like, it‚Äôs going to work to make something up ‚Ä¶ well, guess what? We knew early on with LLMs that hallucination was a feature, not a bug for what we wanted to do.



LEE: [LAUGHS] So I think that really touches on just the fact that there&#8217;s so many unknowns and such lack of precision and exactness in our understanding of human biology and of medicine. Carey, what do you think?&nbsp;



GOLDBERG: I mean, I just have this emotional reaction, which is that I love the idea of AI marching into biomedical science and everything from getting to the virtual cell eventually to, Zak, I think it was a colleague of yours who recently published about &#8230; it was a new medication that had been sort of discovered by AI (opens in new tab), and it was actually testing out up to the phase II level or something, right?



KOHANE: Oh, this is Marinka‚Äôs work.&nbsp;



GOLDBERG: Yeah, Marinka, Marinka Zitnik. And ‚Ä¶ yeah. So, I mean, I think it avoids a lot of the, sort of, dilemmas that are involved with safety and so on with AI coming into medicine. And it&#8217;s just the discovery process, which we all want to advance as quickly as possible. And it seems like it actually has a great deal of potential that&#8217;s already starting to be realized.&nbsp;



LEE: Oh, absolutely.&nbsp;



KOHANE: I love this topic. First of all, I thought, actually, I think Bill and Seb, actually, had interesting things to say on that very topic, rationales which I had not really considered why, in fact, things might progress faster in the discovery space than in the clinical delivery space, just because we don&#8217;t know in clinical medicine what we&#8217;re trying to maximize precisely. Whereas for a drug effect, we do know what we&#8217;re trying to maximize.&nbsp;



LEE: Well, in fact, I happened to save that snippet from Bill Gates saying that. So let&#8217;s cue that up.&nbsp;



BILL GATES: I think it‚Äôs very much within the realm of possibility that the AI is not only accelerating healthcare discovery but substituting for a lot of the roles of, you know, ‚ÄúI‚Äôm an organic chemist,‚Äù or ‚ÄúI run various types of assays.‚Äù I can see those, which are, you know, testable-output-type jobs but with still very high value, I can see, you know, some replacement in those areas before the doctor.&nbsp;



LEE: So, Zak, isn&#8217;t that Bill saying exactly what you‚Äôre saying?&nbsp;



KOHANE: That is my point. I have to say that this is another great bet, that either we&#8217;re all going to be surprised or a large group of people will be surprised or disappointed.&nbsp;



There&#8217;s still a lot of people in the sort of medicinal chemist, trialist space who are still extremely skeptical that this is going to work. And we haven&#8217;t quite shown them yet that it is. Why have we not shown them? Because we haven&#8217;t gone all the way to a phase III study, which showed that the drug behaves as expected to, is effective, and basically doesn&#8217;t hurt people. That turns out to require a lot of knowledge. I actually think we&#8217;re getting there, but I understand the skepticism.&nbsp;



LEE: Carey, what are your thoughts?&nbsp;



GOLDBERG: Yeah. I mean, there will be no way around going through full-on clinical trials for anything to ever reach the market. But at the same time, you know, it&#8217;s clearly very promising. And just to throw out something for the pure fun of it, Peter, I saw &#8230; one of my favorite tweets recently was somebody saying, you know, isn&#8217;t it funny how computer science is actually becoming a lot more like biology in that it&#8217;s just becoming empirical.&nbsp;



It&#8217;s like you just throw stuff at the AI and see what it does. [LAUGHTER] And I was like, oh, yeah, that&#8217;s what Peter was doing when we wrote the book. I mean, he understood as many innards as anybody can. But at the same time, it was a totally empirical exercise in seeing what this thing would do when you threw things at it.&nbsp;



LEE: Right.&nbsp;



GOLDBERG: So it&#8217;s the new biology.&nbsp;



LEE: Well, yeah. So I think we talked in our book about accelerating, you know, biomedical knowledge and medical science. And that actually seems to be happening. And I really had fun talking to Daphne Koller about some of the accomplishments that she&#8217;s made. And so here&#8217;s a little snippet from Daphne.&nbsp;



DAPHNE KOLLER: This will impact not only the early stages of which hypotheses we interrogate, which molecules we move forward, but also hopefully at the end of the day, which molecule we prescribe to which patient.‚ÄØAnd I think there‚Äôs been obviously so much narrative over the years about precision medicine, personalized medicine, and very little of that has come to fruition, with the exception of, you know, certain islands in oncology, primarily on genetically driven cancers.&nbsp;



LEE: So, Zak, when I was listening to that, I was reminded of one of the very first examples that you had where, you know, you had a very rare case of a patient, and you&#8217;re having to narrow down some pretty complex and very rare genetic conditions. This thing that Daphne says, that seems to be the logical conclusion that everyone who&#8217;s thinking hard about AI and biology is coming to. Does it seem more real now two years on?&nbsp;



KOHANE: It absolutely seems more real. Here&#8217;s some sad facts. If you are at a cancer center, you will get targeted therapies if you qualify for it. Outside cancer centers, you won&#8217;t. And it&#8217;s not that the therapies aren&#8217;t available. It&#8217;s just that you won&#8217;t have people thinking about it in that way. And especially if you have some of the rare and more aggressive cancers, if you&#8217;re outside one of those cancer centers, you&#8217;re at a significant disadvantage for survival for that reason. And so anything that provides just the ‚Äúsimple,‚Äù in quotes, dogged investigation of the targeted therapies for patients, it&#8217;s a home run.&nbsp;



So my late graduate student, Atul Butte, died recently at UCSF, where he was both a professor and the leader of the Bakar Institute, and he was a Zuckerberg Chan Professor of Pediatrics.&nbsp;



He was diagnosed with a rare tumor two years ago. His wife is a PhD biologist, and when he was first diagnosed, she sent me the diagnosis and the mutations. And I don&#8217;t know if you know this, Peter, but this was still when we were writing the book and people didn&#8217;t know about GPT-4.&nbsp;



I put in those mutations into GPT-4 and the diagnosis. And I said, ‚ÄúI&#8217;d like to help treat my friend. What&#8217;s the right treatment?‚Äù And GPT, to paraphrase, GPT-4 said, ‚ÄúBefore we start talking about treatment, are you sure this is the right diagnosis? Those mutations are not characteristic for that tumor.&#8221; And he had been misdiagnosed. And then they changed the diagnosis therapy and some personnel.¬†



So I don&#8217;t have to hallucinate this. It&#8217;s already happened, and we&#8217;re going to need this. And so I think targeted therapy for cancers is the most obvious use. And if God forbid one of you has a family member who has cancer, it&#8217;s moral malpractice not to look at the genetics and run it past GPT-4 and say, ‚ÄúWhat are the available therapies?‚Äù&nbsp;



LEE: Yeah.&nbsp;



KOHANE: I really deeply believe that.&nbsp;



LEE: Carey, I think one thing you&#8217;ve always said is that you&#8217;re surprised that we don&#8217;t hear more stories along these lines. And I think you threw a quote from Mustafa Suleyman back at me. Do you want to share that?&nbsp;



GOLDBERG: Yes. Recently, I believe it was a Big Technology interview (opens in new tab), and the reporter asked Mustafa Suleyman, ‚ÄúSo you guys are seeing 50 million queries, medical queries, a day [to Copilot and Bing]. You know, how&#8217;s that going?‚Äù And I think I am a bit surprised that we&#8217;re not seeing more stories of all types. Both here&#8217;s how it helped me and also here was maybe, you know, a suggestion that was not optimal.&nbsp;



LEE: Yeah. I do think in our book, we did predict both positive and negative outcomes of this. And it is odd. Atul was very open with his story. And of course, he is such ‚Ä¶ he was such a prominent leader in the world of medicine.&nbsp;



But I think I share your surprise, Carey. I expected by now that a lot more public stories would be out. Maybe there is someone writing a book collecting these things, I don&#8217;t know.&nbsp;



KOHANE: Maybe someone called Carey Goldberg should write that book. [LAUGHTER]&nbsp;



GOLDBERG: Write a book, maybe. I mean, we have Patients Use AI (opens in new tab), which is a wonderful blog by Dave deBronkart, the patient advocate.&nbsp;



But I wonder if it&#8217;s also something structural, like who would be or what would be the institution that would be gathering these stories? I don‚Äôt know.&nbsp;



LEE: Right.&nbsp;



KOHANE: And that&#8217;s the problem. You see, this goes back to the same problem that [Ethan] Mollick was talking about. Individual doctors are using them. The hospital as a whole is not doing that. So it&#8217;s not judging the quality, as part of its quality metrics, of how good the AI is performing and what new has happened. And the other audience, namely the patients, have no mechanism. There is no mechanism to go to Better Business Bureau and say, ‚ÄúThey screwed up,‚Äù or ‚ÄúThis was great.‚Äù&nbsp;



LEE: So now I want to get a little more futuristic. And this gets into whether AI is really going to get almost to the ab initio understanding of human biology. And so Eric Topol, who is one of the guests, spoke to this a bit. So let&#8217;s hear this.&nbsp;



LEE: So you talk about a virtual cell. Is that achievable within 10 years, or is that still too far out?ERIC TOPOL:‚ÄØNo, I think within 10 years for sure. You know, the group that got assembled, that‚ÄØSteve Quake pulled together,‚ÄØI think has 42 authors in a‚ÄØpaper‚ÄØin‚ÄØCell. The fact that he could get these 42 experts in life science and some in computer science to come together and all agree that not only is this a worthy goal, but it‚Äôs actually going to be realized, that was impressive.¬†



LEE: You know, I have to say Eric&#8217;s optimism took me aback. Just speaking as a techie, I think I started off being optimistic: as soon as we can figure out molecular dynamics, biology can be solved. And then you start to learn more about biochemistry, about the human cell, and then you realize, oh, my God, this is just so vast and unknowable. And now you have Eric Topol saying, ‚ÄúWell, in less than 10 years.‚Äù&nbsp;



KOHANE: So what&#8217;s delightful about this period is that those of us who are cautious were so incredibly wrong about AI two years ago. [LAUGHTER] That&#8217;s a true joy &#8230; I mean, absolute joy. It&#8217;s great to have your futurism made much more positive.&nbsp;



But I think that we&#8217;re going from, you know, for example, AlphaFold has had tremendous impact. But remember, that was built on years of acquisition of crystallography data that was annotated. And of course, the annotation process becomes less relevant as you go down the pipe, but it started from that.&nbsp;



LEE: Yes.&nbsp;



KOHANE: And there&#8217;s lots of parts of the cell. So when people talk about virtual cells‚ÄîI don&#8217;t mean to get too technical‚Äîmostly they&#8217;re talking about perturbation of gene expression. They&#8217;re not talking about, ‚ÄúOh, this is how the liposome and the centrosome interact, and notice how the Golgi bodies bump into each other.‚Äù&nbsp;



There&#8217;s a whole bunch of other levels of abstraction we know nothing about. This is a complex factory. And right now, we&#8217;re sort of the level from code into loading code into memory. We&#8217;re not talking about how the rest of the robots work in that cell, and how the rest of those robots work in the cell turns out to be pretty important to functioning.&nbsp;



So I&#8217;d love to be wrong again. And in 10 years, oh yeah, not only, you know, our first in-human study will be you, Dr. Zak. We&#8217;re going put the drug because we fully simulated you. That&#8217;d be great.&nbsp;



LEE: Yes.&nbsp;



KOHANE: And, by the way, just to give people their due, there probably was a lot of animal research that could be done in silico and that for various political reasons we&#8217;re now seeing happen. That&#8217;s a good thing. But I think that sometimes it takes a lot of hubris to get us where we need to get, but my horizon is not the same as his.&nbsp;



LEE: So I guess I have to take this time to brag. Just recently out of our AI for Science team did publish in Science a biological emulator that does pretty long timespan, very, very precise, and very efficient molecular dynamics, biomolecular dynamics emulation. We call it emulation because it&#8217;s not simulating every single time step but giving you the final confirmations.&nbsp;



KOHANE: That&#8217;s an amazing result.&nbsp;



LEE: Yeah.&nbsp;



KOHANE: But ‚Ä¶ that is an amazing result. And you&#8217;re doing it in some very important interactions. But there&#8217;s so much more to do.&nbsp;



LEE: I know, and it&#8217;s single molecules; it&#8217;s not even two molecules. There&#8217;s so much more to go for here. But on the other hand, Eric is right, you know, 42 experts writing for Cell, you know, that&#8217;s not a small matter.&nbsp;



KOHANE: So I think sometimes you really need to drink your own hallucinogens to actually succeed. Because remember, when the Human Genome Project (opens in new tab) was launched, we didn&#8217;t know how to sequence at scale.&nbsp;



We said maybe we would get there. And then in order to get the right funding and excitement and, I think, focus, we predicted that by early 2000s we&#8217;d be transforming medicine. Has not happened yet. Things have happened, but at a much slower pace. And we&#8217;re 25 years out. In fact, we&#8217;re 35 years out from the launch.&nbsp;



But again, things are getting faster and faster. Maybe the singularity is going to make a whole bunch of things easier. And GPT-6 will just say, ‚ÄúZak, you are such a pessimist. Let me show you how it&#8217;s done.‚Äù&nbsp;



GOLDBERG: Yeah.&nbsp;



It really is a pessimism versus optimism. Like is it, I mean, biology is such a bitch, right. [LAUGHTER] Can we actually get there?&nbsp;



At the same time, everyone was surprised and blown away by the, you know, the quantum leap of GPT-4. Who knows when enough data gets in there if we might not have a similar leap.&nbsp;



LEE: Yeah. All right.&nbsp;



So let&#8217;s get back to healthcare delivery. Besides Morgan Cheatham, we talked to [a] more junior medical student who&#8217;s at the Kaiser Permanente School of Medicine, Daniel Chen. And, you know, I asked him about this question of patients who come in armed [LAUGHS] with a lot of their own information. Let&#8217;s hear what he said about this.&nbsp;



DANIEL CHEN: But for those that come in with a list, I sometimes sit down with them, and we‚Äôll have a discussion, honestly. ‚Ä¶ ‚ÄúI don‚Äôt think you have meningitis because, you know, you‚Äôre not having a fever. Some of the physical exam maneuvers we did were also negative. So I don‚Äôt think you have anything to worry about that,‚Äù you know. So I think it‚Äôs having that very candid conversation with the patient that helps build that initial trust.&nbsp;



LEE: So, Zak, as far as I can tell, Daniel and Morgan are figuring this out on their own as medical students. I don&#8217;t think this is part of the curriculum. Does it need to be?&nbsp;



KOHANE: It&#8217;s missing the bigger point. The incentives and economic forces are such that even if you were Daniel, and things have not changed in terms of incentives, and it&#8217;s 2030, he still has to see this many patients in an hour.&nbsp;



And sitting down, going over that with a patient, let&#8217;s say some might need more &#8230; in fact, I think computer scientists are enriched for these sort of neurotic ‚Äúexplain [to] me why this works,‚Äù when often the answer is, ‚ÄúI have no idea; empirically it does.‚Äù&nbsp;



And patients in some sense deserve that conversation, and we&#8217;re taught about joint decision making, but in practice, there&#8217;s a lot of skills that are deployed to actually deflect so that you can get through the appointment and see enough patients per hour.&nbsp;



And that&#8217;s why I think that one of the central ‚Ä¶ another task for AI is how to engage with patients to actually explain to them why their doctor is doing what he&#8217;s doing and perhaps ask the one or two questions that you should be asking the doctor in order to reassure you that they&#8217;re doing the right thing.



LEE: Yeah.&nbsp;



KOHANE: I just ‚Ä¶ right now, we are going to have less doctor time, not more doctor time.&nbsp;



And so I&#8217;ve always been struck by the divide between medicine that we&#8217;re taught as it should be practiced as a gentle person&#8217;s vocation or sport as opposed to assembly line, heads down ‚Äúyou&#8217;ve got to see those patients by the end of the day‚Äù because, otherwise, you haven&#8217;t seen all the patients at the end of the day.&nbsp;



LEE: Yeah. Carey, I&#8217;ve been dying to ask you this, and I have not asked you this before. When you go see a doctor, are you coming in armed with ChatGPT information?&nbsp;



GOLDBERG: I haven&#8217;t needed to yet, but I certainly would. And also my reaction to the medical student description was, I think we need to distinguish between the last 20 years, when patients would come in armed with Google, and what they&#8217;re coming in with now because at least the experiences that I&#8217;ve witnessed, it is miles better to have gone back and forth with GPT-4 than with, you know, dredging what you can from Google. And so I think we should make that distinction.&nbsp;



And also, the other thing that most interested me was this question for medical students of whether they should not use AI for a while so that they can learn ‚Ä¶&nbsp;



LEE: Yes.&nbsp;



GOLDBERG: ‚Ä¶ how to think and similarly maybe don&#8217;t use the automated scribes for a while so they can learn how to do a note. And at what point should they then start being able to use AI? And I suspect it&#8217;s fairly early on that, in fact, they&#8217;re going be using it so consistently that there&#8217;s not that much they need to learn before they start using the tools.&nbsp;



LEE: These two students were incredibly impressive. And so I have wondered, you know, if we got a skewed view of things. I mean, Morgan is, of course, a very, very impressive person. And Daniel was handpicked by the dean of the medical school to be a subject of this interview.&nbsp;



KOHANE: You know, we filter our students, by and large, I mean, there&#8217;s exceptions, but students in medical school are so starry eyed. And they are really &#8230; they got into medical school‚ÄîI mean, some of them may have faked it‚Äîbut a lot of them because they really wanted to do good.&nbsp;



LEE: Right.&nbsp;



KOHANE: And they really wanted to help. And so this is very constant with them. And it&#8217;s only when they&#8217;re in the machine, past medical school, that they realize, oh my God, this is a very, very different story.&nbsp;



And I can tell you, because I teach a course in computational-enabled medicine, so I get a lot of these nerd medical students, and I&#8217;m telling them, ‚ÄúYou&#8217;re going to experience this. And you&#8217;re going to say, ‚ÄòI&#8217;m not going to able to change medicine until I get enough cred 10, 15 years from now, whereas I could start my own company and immediately change medicine.‚Äô‚Äù&nbsp;



And increasingly I&#8217;m getting calls in like residency and saying, ‚ÄúZak, help me. How do I get out of this?‚Äù&nbsp;



GOLDBERG: Wow.&nbsp;



KOHANE: And so I think there&#8217;s a real disillusionment of, like, between what we&#8217;re asking for people coming to medical school‚Äîwe&#8217;re looking for a phenotype‚Äîand then we&#8217;re disappointing them massively, not everywhere, but massively.&nbsp;



And for me, it&#8217;s very sad because among our best and brightest, and then because of economics and expectations and the nature of the beast, they&#8217;re not getting to enjoy the most precious part of being a doctor, which is that real human connection, and longitudinality, you know, the connection between the same doctor visit after visit, is more and more of a luxury.&nbsp;



LEE: Well, maybe this gets us to the last episode, you know, where I talk to a former, you know, state director of public health, Umair Shah, and with Gianrico Farrugia, who&#8217;s the CEO of Mayo Clinic. And I think if there&#8217;s one theme that I took away from those conversations is that we&#8217;re not thinking broadly enough nor big enough.&nbsp;



And so here&#8217;s a little quote of exchange that Umair Shah, who was the former head of public health in the State of Washington and prior to that in Harris County, Texas, and we had a conversation about what techies tend to focus on when they&#8217;re thinking about AI and medicine.&nbsp;



UMAIR SHAH: I think one of the real challenges is that when even tech companies, and you can name all of them, when they look at what they&#8217;re doing in the AI space, they gravitate towards healthcare delivery.LEE: Yes. And in fact, it&#8217;s not even delivery. I think techies‚ÄîI did this, too‚Äîtend to gravitate specifically to diagnosis.



LEE: I have been definitely guilty. I think Umair, of course, was speaking as a former frustrated public health official in just thinking about all the other things that are important to maintain a healthy population.&nbsp;



Is there some lesson that we should take away? I think our book also focused a lot on things like diagnosis.&nbsp;



KOHANE: Yeah. Well, first of all, I think we just have to have humility. And I think it&#8217;s a really important ingredient. I found myself staring at the increase in lifespan in human beings over the last two centuries and looking for bumps that were attributable.&nbsp;



I&#8217;m in medical school. I&#8217;ve already made this major commitment. What are the bumps that are attributable to medicine? And there was one bump that was due to vaccines, a small bump. Another small bump that was due to antibiotics. And the rest of it is nutrition, sanitation, yeah, nutrition and sanitation.&nbsp;



And so I think doctors can be incredibly valuable, but not all the time. And we&#8217;re spending now one-sixth of our GDP on it. The majority of it is not effectively prolonging life. And so the humility has to be the right medicine at the right time.&nbsp;



But that runs, (A) against a bunch of business models. It runs against the primacy of doctors in healthcare. It was one thing when there were no textbooks; there was no PubMed. You know, the doctor was the repository of all the probably knowledge that we have. But I think your guests were right. We have to think more broadly in the public health way. How do we make knowledge pervasive like sanitation?&nbsp;



GOLDBERG: Although I would add that since what we&#8217;re talking about is AI, it&#8217;s harder to see if &#8230; and if what you&#8217;re talking about is public health, I mean, it was certainly very important to have good data during the pandemic, for example.&nbsp;



But most of the ways to improve public health, like getting people to stop smoking and eat better and sleep better and exercise more, are not things that AI can help with that much. Whereas diagnosis or trying to improve treatment are places that it could tackle.&nbsp;



And in fact, Peter, I wanted to put you‚Äîoh, wait, Zak&#8217;s going to say something‚Äîbut, Peter, I wanted to put you on the spot.&nbsp;



LEE: Yeah.&nbsp;



GOLDBERG: I mean, if you had a medical issue now, and you went to a physician, would you be OK with them not using generative AI?&nbsp;



LEE: I think if it&#8217;s a complex or a mysterious case, I would want them to use generative AI. I would want that second opinion on things. And I would personally be using it. If for no other reason than just to understand what the chart is saying.&nbsp;



I don&#8217;t see, you know, how or why one wouldn&#8217;t do that now.&nbsp;



KOHANE: It&#8217;s such a cheap second opinion, and people are making mistakes. And even if there are mistakes on the part of AI, if there&#8217;s a collision, discrepancy, that&#8217;s worth having a discussion. And again, this is something that we used to do more of when we had more time with the patients; we&#8217;d have clinic conferences.&nbsp;



LEE: Yeah.&nbsp;



KOHANE: And we don&#8217;t have that now. So I do think that there is a role for AI. But I think again, it&#8217;s much more of a continual presence, being part of a continued conversation rather than an oracle.&nbsp;



And I think that&#8217;s when you&#8217;ll start seeing, when the AI is truly a colleague, and saying, ‚ÄúYou know, Zak, that&#8217;s the second time you made that mistake. You know, that&#8217;s not obesity. That&#8217;s the effect of your drugs that you&#8217;re giving her. You better back off of it.‚Äù And that&#8217;s what we need to see happen.&nbsp;



LEE: Well, and for the business of healthcare, that also relates directly to quality scores, which translates into money for healthcare providers.&nbsp;



So the last person that we interviewed was Gianrico Farrugia. And, you know, I was sort of wondering, I was expecting to get a story from a CEO saying, ‚ÄúOh, my God, this has been so disruptive, incredibly important, meaningful, but wow, what a headache.‚Äù&nbsp;



At least Gianrico didn&#8217;t expose any of that. Here&#8217;s one of the snippets to give you a sense.&nbsp;



GIANRICO FARRUGIA: When generative AI came, for us, it&#8217;s like, I wouldn&#8217;t say we told you so, but it&#8217;s like, ah, there you go. Here&#8217;s another tool. This is what we&#8217;ve been talking about. Now we can do it even better. Now we can move even faster. Now we can do more for our patients. It truly never was disruptive. It truly immediately became enabling, which is strange, right, because something as disruptive as that instantly became enabling at Mayo Clinic.&nbsp;



LEE: So I tried pretty hard in that interview to get Gianrico to admit that there was a period of headache and disruption here. And he never, ever gave me that. And so I take him at his word.&nbsp;



Zak, maybe I should ask you, what about Harvard and the whole Harvard medical ecosystem?&nbsp;



KOHANE: I would be surprised if there are system-wide measurable gains in health quality right now from AI. And I do have to say that Mayo is one of the most marvelous organizations in terms of team behavior. So if there&#8217;s someone who&#8217;s gotten the team part of it right, they&#8217;ve come the closest, which relates to our prior conversation. They have the quarterback idea ‚Ä¶&nbsp;



LEE: Yes.&nbsp;



KOHANE: ‚Ä¶ pretty well down compared to others.&nbsp;



Nonetheless, I take him at his word, that it hasn&#8217;t disrupted them. But I&#8217;m also, I have yet to see the evidence that there&#8217;s been a quantum leap in quality or efficacy. And I do believe that it&#8217;s possible to have a quantum leap in efficacy in the right system.&nbsp;



So if they haven&#8217;t been disrupted, I would venture that they&#8217;ve absorbed it, but they haven&#8217;t used it to its fullest potential. And the way I could be proven wrong is next year, also the metrics showing that over the last year, they&#8217;ve had, you know, decreased readmissions, decreased complications, decreased errors and all that. And if so, God bless them. And we should all be more like Mayo.&nbsp;



LEE: So I thought a little bit about two other quotes from the interviews that sort of maybe would send us off with some more inspirational kind of view of the future. And so there&#8217;s one from Bill Gates and one from Gianrico Farrugia. So what I&#8217;d like to do is to play both of those and then maybe we can have our last comments.&nbsp;



BILL GATES: You know, I‚Äôve gone so far as to tell politicians with national health systems that if they deploy AI appropriately, that the quality of care, the overload of the doctors, the improvement in the economics will be enough that their voters will be stunned because they just don‚Äôt expect this, and, you know, they could be reelected just on this one thing of fixing what is a very overloaded and economically challenged health system in these rich countries.&nbsp;



And now Gianrico.&nbsp;



GIANRICO FARRUGIA: And we seemed to be on a linear path, which is, let&#8217;s try and reduce administrative burden. Let&#8217;s try and truly be a companion to a physician or other provider. ‚Ä¶ And then in the next step, we keep going until we get to, now we can call it agentic AI, whatever we want to talk about. And my view was, no, is that let&#8217;s start with that aim, the last aim ‚Ä¶ because the others will come automatically if you&#8217;re working on that harder problem. Because one, to get to that harder problem, you&#8217;ll find all the other solutions.&nbsp;



All right. I think these are both kind of calls to be more assertive about this and more forward leaning. I think two years into the GPT-4 era, those are pretty significant and pretty optimistic calls to action. So maybe just to give you both one last word. What would be one hope that you would have for the world of healthcare and medicine two years from now?&nbsp;



KOHANE: I would hope for businesses that whoever actually owns them at some holding company level, regardless of who owns them, are truly patient-focused companies, companies where the whole AI is about improving your care, and it&#8217;s only trying to maximize your care and it doesn&#8217;t care about resource limitations.&nbsp;



And as I was listening to Bill, and the problem with what he was saying about saving dollars for governments is for many things, we have some very expensive things that work. And if the AI says, ‚ÄúThis is the best thing,‚Äù it&#8217;s going to break your bank. And instead, because of research limitations, we play a human-based fancy footwork to get out of it.&nbsp;



That&#8217;s a hard game to play, and I leave it to the politicians and the public health officials who have to do those trades of utilities.&nbsp;



In my role as doctor and patient, I&#8217;d like to see very informed, authoritative agents acting only on our behalf so that when we go and we seek to have our maladies addressed, the only issue is, what&#8217;s the best and right thing for me now? And I think that is both technically realizable. And even in our weird system, there are business plans that will work that can achieve that. That&#8217;s my hope for two years from now.&nbsp;



LEE: Yeah, fantastic. Carey.&nbsp;



GOLDBERG: Yeah. I second that so enthusiastically. And I think, you know, we have this very glass half full/glass half empty phenomenon two years after the book came out.&nbsp;



And it&#8217;s certainly very nice to see, you know, new approaches to administrative complexity and to prior authorization and all kinds of ways to make physicians&#8217; lives easier. But really what we all care about is our own health and that we would like to be able to optimize the use of this truly glorious technological achievement to be able to live longer and better lives. And I think what Zak just described is the most logical way to do that.&nbsp;



[TRANSITION MUSIC]&nbsp;



LEE: Yeah, I think for me, two years from now, I would like to see all of this digital data that&#8217;s been so painful, such a burden on every doctor and nurse to record, actually amount to something meaningful in the care of patients. And I think it&#8217;s possible.&nbsp;



KOHANE: Amen.&nbsp;



GOLDBERG: Yeah.&nbsp;



LEE: All right, so it&#8217;s been quite a journey. We were joking before we&#8217;re still on speaking terms after having written a book. [LAUGHS]&nbsp;



And then, um, I think listeners might enjoy knowing that we debated amongst ourselves what to do about a second edition, which seemed too painful to me, and so I suggested the podcast, which seemed too painful to the two of you [LAUGHTER]. And in the end, I don&#8217;t know what would have been easier, writing a book or doing this podcast series, but I do think that we learned a lot.&nbsp;



Now, last bit of business here. To avoid having the three of us try to write a book again and do this podcast, I leaned on the production team in Microsoft Research and the Microsoft Research Podcast. And I thought it would be good to give an explicit acknowledgment to all the people who&#8217;ve contributed to this.&nbsp;



So it&#8217;s a long list of names. I&#8217;m going to read through them all. And then I suggest that we all give an applaud [LAUGHTER] to them. And so here we go.&nbsp;



There‚Äôs Neeltje Berger, Tetiana Bukhinska, David Celis Garcia, Matt Corwine, Jeremy Crawford, Kristina Dodge, Chris Duryee, Ben Ericson, Kate Forster, Katy Halliday, Alyssa Hughes, Jake Knapp, Weishung Liu, Matt McGinley, Jeremy Mashburn, Amanda Melfi, Wil Morrill, Joe Plummer, Brenda Potts, Lindsay Shanahan, Sarah Sobolewski, David Sullivan,&nbsp;Stephen Sullivan, Amber Tingle, Caitlyn Treanor, Craig Tuschhoff, Sarah Wang, and Katie Zoller.&nbsp;



Really a great team effort, and they made it super easy for us.&nbsp;



GOLDBERG: Thank you. Thank you. Thank you.&nbsp;



KOHANE: Thank you. Thank you.



GOLDBERG: Thank you.&nbsp;



[THEME MUSIC]&nbsp;



LEE: A big thank you again to all of our guests for the work they do and the time and expertise they shared with us.&nbsp;



And, last but not least, to our listeners, thank you for joining us. We hope you enjoyed it and learned as much as we did. If you want to go back and catch up on any episodes you may have missed or to listen to any again, you can visit aka.ms/AIrevolutionPodcast (opens in new tab).



Until next time.



[MUSIC FADES]&nbsp;

				
			
			
				Show more			
		
	





AI Revolution in Medicine podcast series

Opens in a new tabThe post Coauthor roundtable: Reflecting on healthcare economics, biomedical research, and medical education appeared first on Microsoft Research.
‚Ä¢ Enhance Geospatial Analysis and GIS Workflows with Amazon Bedrock Capabilities
  As data becomes more abundant and information systems grow in complexity, stakeholders need solutions that reveal quality insights. Applying emerging technologies to the geospatial domain offers a unique opportunity to create transformative user experiences and intuitive workstreams for users and organizations to deliver on their missions and responsibilities. 
In this post, we explore how you can integrate existing systems with Amazon Bedrock to create new workflows to unlock efficiencies insights. This integration can benefit technical, nontechnical, and leadership roles alike. 
Introduction to geospatial data 
Geospatial data is associated with a position relative to Earth (latitude, longitude, altitude). Numerical and structured geospatial data formats can be categorized as follows: 
 
 Vector data ‚Äì Geographical features, such as roads, buildings, or city boundaries, represented as points, lines, or polygons 
 Raster data ‚Äì Geographical information, such as satellite imagery, temperature, or elevation maps, represented as a grid of cells 
 Tabular data ‚Äì Location-based data, such as descriptions and metrics (average rainfall, population, ownership), represented in a table of rows and columns 
 
Geospatial data sources might also contain natural language text elements for unstructured attributes and metadata for categorizing and describing the record in question. Geospatial Information Systems (GIS) provide a way to store, analyze, and display geospatial information. In GIS applications, this information is frequently presented with a map to visualize streets, buildings, and vegetation. 
LLMs and Amazon Bedrock 
Large language models (LLMs) are a subset of foundation models (FMs) that can transform input (usually text or image, depending on model modality) into outputs (generally text) through a process called generation. Amazon Bedrock is a comprehensive, secure, and flexible service for building generative AI applications and agents. 
LLMs work in many generalized tasks involving natural language. Some common LLM use cases include: 
 
 Summarization ‚Äì Use a model to summarize text or a document. 
 Q&amp;A ‚Äì Use a model to answer questions about data or facts from context provided during training or inference using Retrieval Augmented Generation (RAG). 
 Reasoning ‚Äì Use a model to provide chain of thought reasoning to assist a human with decision-making and hypothesis evaluation. 
 Data generation ‚Äì Use a model to generate synthetic data for testing simulations or hypothetical scenarios. 
 Content generation ‚Äì Use a model to draft a report from insights derived from an Amazon Bedrock knowledge base or a user‚Äôs prompt. 
 AI agent and tool orchestration ‚Äì Use a model to plan the invocation of other systems and processes. After other systems are invoked by an agent, the agent‚Äôs output can then be used as context for further LLM generation. 
 
GIS can implement these capabilities to create value and improve user experiences. Benefits can include: 
 
 Live decision-making ‚Äì Taking real-time insights to support immediate decision-making, such as emergency response coordination and traffic management 
 Research and analysis ‚Äì In-depth analysis that humans or systems can identify, such as trend analysis, patterns and relationships, and environmental monitoring 
 Planning ‚Äì Using research and analysis for informed long-term decision-making, such as infrastructure development, resource allocation, and environmental regulation 
 
Augmenting GIS and workflows with LLM capabilities leads to simpler analysis and exploration of data, discovery of new insights, and improved decision-making. Amazon Bedrock provides a way to host and invoke models as well as integrate the AI models with surrounding infrastructure, which we elaborate on in this post. 
Combining GIS and AI through RAG and agentic workflows 
LLMs are trained with large amounts of generalized information to discover patterns in how language is produced. To improve the performance of LLMs for specific use cases, approaches such as RAG and agentic workflows have been created. Retrieving policies and general knowledge for geospatial use cases can be accomplished with RAG, whereas calculating and analyzing GIS data would require an agentic workflow. In this section, we expand upon both RAG and agentic workflows in the context of geospatial use cases. 
Retrieval Augmented Generation 
With RAG, you can dynamically inject contextual information from a knowledge base during model invocation. 
RAG supplements a user-provided prompt with data sourced from a knowledge base (collection of documents). Amazon Bedrock offers managed knowledge bases to data sources, such as Amazon Simple Storage Service (Amazon S3) and SharePoint, so you can provide supplemental information, such as city development plans, intelligence reports, or policies and regulations, when your AI assistant is generating a response for a user. 
Knowledge bases are ideal for unstructured documents with information stored in natural language. When your AI model responds to a user with information sourced from RAG, it can provide references and citations to its source material. The following diagram shows how the systems connect together. 
 
Because geospatial data is often structured and in a GIS, you can connect the GIS to the LLM using tools and agents instead of knowledge bases. 
Tools and agents (to control a UI and a system) 
Many LLMs, such as Anthropic‚Äôs Claude on Amazon Bedrock, make it possible to provide a description of tools available so your AI model can generate text to invoke external processes. These processes might retrieve live information, such as the current weather in a location or querying a structured data store, or might control external systems, such as starting a workflow or adding layers to a map. Some common geospatial functionality that you might want to integrate with your LLM using tools include: 
 
 Performing mathematical calculations like the distance between coordinates, filtering datasets based on numeric values, or calculating derived fields 
 Deriving information from predictive analysis models 
 Looking up points of interest in structured data stores 
 Searching content and metadata in unstructured data stores 
 Retrieving real-time geospatial data, like traffic, directions, or estimated time to reach a destination 
 Visualizing distances, points of interest, or paths 
 Submitting work outputs such as analytic reports 
 Starting workflows, like ordering supplies or adjusting supply chain 
 
Tools are often implemented in AWS Lambda functions. Lambda runs code without the complexity and overhead of running servers. It handles the infrastructure management, enabling faster development, improved performance, enhanced security, and cost-efficiency. 
Amazon Bedrock offers the feature Amazon Bedrock Agents to simplify the orchestration and integration with your geospatial tools. Amazon Bedrock agents follow instructions for LLM reasoning to break down a user prompt into smaller tasks and perform actions against identified tasks from action providers. The following diagram illustrates how Amazon Bedrock Agents works. 
 
The following diagram shows how Amazon Bedrock Agents can enhance GIS solutions. 
 
Solution overview 
The following demonstration applies the concepts we‚Äôve discussed to an earthquake analysis agent as an example. This example deploys an Amazon Bedrock agent with a knowledge base based on Amazon Redshift. The Redshift instance has two tables. One table is for earthquakes, which includes date, magnitude, latitude, and longitude. The second table holds the counites in California, described as polygon shapes. The geospatial capabilities of Amazon Redshift can relate these datasets to answer queries like which county had the most recent earthquake or which county has had the most earthquakes in the last 20 years. The Amazon Bedrock agent can generate these geospatially based queries based on natural language. 
This script creates an end-to-end pipeline that performs the following steps: 
 
 Processes geospatial data. 
 Sets up cloud infrastructure. 
 Loads and configures the spatial database. 
 Creates an AI agent for spatial analysis. 
 
In the following sections, we create this agent and test it out. 
Prerequisites 
To implement this approach, you must have an AWS account with the appropriate AWS Identity and Access Management (IAM) permissions for Amazon Bedrock, Amazon Redshift, and Amazon S3. 
Additionally, complete the following steps to set up the AWS Command Line Interface (AWS CLI): 
 
 Confirm you have access to the latest version of the AWS CLI. 
 Sign in to the AWS CLI with your credentials. 
 Make sure ./jq is installed. If not, use the following command: 
 
 
 yum -y install jq 
 
Set up error handling 
Use the following code for the initial setup and error handling: 
 
 #!/usr/bin/env bash
set -ex

LOG_FILE="deployment_$(date +%Y%m%d_%H%M%S).log"
touch "$LOG_FILE"

handle_error() {
&nbsp;&nbsp; &nbsp;local exit_code=$?
&nbsp;&nbsp; &nbsp;local line_number=$1
&nbsp;&nbsp; &nbsp;if [ $exit_code -ne 0 ]; then
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;log_error "Failed at line $line_number with exit code $exit_code"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;exit $exit_code
&nbsp;&nbsp; &nbsp;fi
}
trap 'handle_error $LINENO' ERR 
 
This code performs the following functions: 
 
 Creates a timestamped log file 
 Sets up error trapping that captures line numbers 
 Enables automatic script termination on errors 
 Implements detailed logging of failures 
 
Validate the AWS environment 
Use the following code to validate the AWS environment: 
 
 AWS_VERSION=$(aws --version 2&gt;&amp;1)
log "INFO" "AWS CLI version: $AWS_VERSION"

if ! aws sts get-caller-identity &amp;&gt;/dev/null; then
&nbsp;&nbsp; &nbsp;log_error "AWS CLI is not configured with valid credentials"
&nbsp;&nbsp; &nbsp;exit 1
fi

AWS_REGION="us-east-1"
AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text) 
 
This code performs the essential AWS setup verification: 
 
 Checks AWS CLI installation 
 Validates AWS credentials 
 Retrieves account ID for resource naming 
 
Set up Amazon Redshift and Amazon Bedrock variables 
Use the following code to create Amazon Redshift and Amazon Bedrock variables: 
 
 REDSHIFT_CLUSTER_IDENTIFIER="geo-analysis-cluster"
REDSHIFT_DATABASE="geo_db"
REDSHIFT_MASTER_USER= [Create username]
REDSHIFT_MASTER_PASSWORD= [Create Password]
REDSHIFT_NODE_TYPE="dc2.large"
REDSHIFT_CLUSTER_TYPE="single-node"
BEDROCK_ROLE_NAME="BedrockGeospatialRole"
# Bedrock Configuration
AGENT_NAME="GeoAgentRedshift"
KNOWLEDGE_BASE_NAME="GeospatialKB" 
 
Create IAM roles for Amazon Redshift and Amazon S3 
Use the following code to set up IAM roles for Amazon S3 and Amazon Redshift: 
 
 if aws iam get-role --role-name "$REDSHIFT_ROLE_NAME" &amp;&gt;/dev/null; then
    REDSHIFT_ROLE_ARN=$(aws iam get-role --role-name "$REDSHIFT_ROLE_NAME" --query 'Role.Arn' --output text)
    log "INFO" "Using existing role ARN: $REDSHIFT_ROLE_ARN"
else
    # Create trust policy document
    cat &gt; /tmp/trust-policy.json &lt;&lt; EOF
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "Service": "redshift.amazonaws.com"
      },
      "Action": "sts:AssumeRole"
    }
  ]
}
EOF
    # Create role
    CREATE_ROLE_OUTPUT=$(aws iam create-role \
        --role-name "$REDSHIFT_ROLE_NAME" \
        --assume-role-policy-document "file:///tmp/trust-policy.json" \
        --description "Role for Redshift to access S3" 2&gt;&amp;1)
    
    REDSHIFT_ROLE_ARN=$(aws iam get-role --role-name "$REDSHIFT_ROLE_NAME" --query 'Role.Arn' --output text)
    if [ $? -ne 0 ]; then
        log_error "Failed to create role:"
        exit 1
    fi
    REDSHIFT_ROLE_ARN=$(echo "$CREATE_ROLE_OUTPUT" | jq -r '.Role.Arn')
    # Wait for role to be available
    sleep 10
fi
ATTACH_POLICY_OUTPUT=$(aws iam attach-role-policy \
    --role-name "$REDSHIFT_ROLE_NAME" \
    --policy-arn "arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess" 2&gt;&amp;1)
if [ $? -ne 0 ]; then
    if echo "$ATTACH_POLICY_OUTPUT" | grep -q "EntityAlreadyExists"; then
    else
        exit 1
    fi
fi 
 
Prepare the data and Amazon S3 
Use the following code to prepare the data and Amazon S3 storage: 
 
 DATA_BUCKET="geospatial-bedrock-demo-data-${AWS_ACCOUNT_ID}"
aws s3 mb s3://$DATA_BUCKET

# Download source data
curl -o earthquakes.csv https://raw.githubusercontent.com/Esri/gis-tools-for-hadoop/master/samples/data/earthquake-data/earthquakes.csv
curl -o california-counties.json https://raw.githubusercontent.com/Esri/gis-tools-for-hadoop/master/samples/data/counties-data/california-counties.json 
 
This code sets up data storage and retrieval through the following steps: 
 
 Creates a unique S3 bucket 
 Downloads earthquake and county boundary data 
 Prepares for data transformation 
 
Transform geospatial data 
Use the following code to transform the geospatial data: 
 
 INPUT_FILE="california-counties.json"
OUTPUT_FILE="california-counties.csv"

# Create CSV header
echo "OBJECTID,AREA,PERIMETER,CO06_D00_,CO06_D00_I,STATE,COUNTY,NAME,LSAD,LSAD_TRANS,Shape_Length,Shape_Area,WKT" &gt; "$OUTPUT_FILE"

# Function to convert ESRI rings to WKT POLYGON format
esri_to_wkt() {
    local rings=$1
    
    # Extract the first ring (exterior ring)
    local exterior_ring=$(echo "$rings" | jq -c '.[0]')
    
    if [ "$exterior_ring" = "null" ] || [ -z "$exterior_ring" ]; then
        echo "POLYGON EMPTY"
        return
    fi
    
    # Start building the WKT string
    local wkt="POLYGON (("
    
    # Process each coordinate pair in the ring
    local coords=$(echo "$exterior_ring" | jq -r '.[] | "\(.[0]) \(.[1])"')
    local first_coord=""
    local result=""
    
    while IFS= read -r coord; do
        if [ -z "$result" ]; then
            result="$coord"
            first_coord="$coord"
        else
            result="$result, $coord"
        fi
    done &lt;&lt;&lt; "$coords"
    
    # Close the ring by adding the first coordinate again if needed
    if [ "$first_coord" != "$(echo "$coords" | tail -1)" ]; then
        result="$result, $first_coord"
    fi
    
    wkt="${wkt}${result}))"
    echo "$wkt"
}

# Process each feature in the JSON file
jq -c '.features[]' "$INPUT_FILE" | while read -r feature; do
    # Extract attributes
    OBJECTID=$(echo "$feature" | jq -r '.attributes.OBJECTID // empty')
    AREA=$(echo "$feature" | jq -r '.attributes.AREA // empty')
    PERIMETER=$(echo "$feature" | jq -r '.attributes.PERIMETER // empty')
    CO06_D00_=$(echo "$feature" | jq -r '.attributes.CO06_D00_ // empty')
    CO06_D00_I=$(echo "$feature" | jq -r '.attributes.CO06_D00_I // empty')
    STATE=$(echo "$feature" | jq -r '.attributes.STATE // empty')
    COUNTY=$(echo "$feature" | jq -r '.attributes.COUNTY // empty')
    NAME=$(echo "$feature" | jq -r '.attributes.NAME // empty')
    LSAD=$(echo "$feature" | jq -r '.attributes.LSAD // empty')
    LSAD_TRANS=$(echo "$feature" | jq -r '.attributes.LSAD_TRANS // empty')
    Shape_Length=$(echo "$feature" | jq -r '.attributes.Shape_Length // empty')
    Shape_Area=$(echo "$feature" | jq -r '.attributes.Shape_Area // empty')
    
    # Extract geometry and convert to WKT
    if echo "$feature" | jq -e '.geometry.rings' &gt; /dev/null 2&gt;&amp;1; then
        rings=$(echo "$feature" | jq -c '.geometry.rings')
        WKT=$(esri_to_wkt "$rings")
    else
        WKT="POLYGON EMPTY"
    fi
    
    # Escape any commas in the fields
    NAME=$(echo "$NAME" | sed 's/,/\\,/g')
    LSAD=$(echo "$LSAD" | sed 's/,/\\,/g')
    LSAD_TRANS=$(echo "$LSAD_TRANS" | sed 's/,/\\,/g')
    
     # Write to CSV - wrap WKT field in quotes
    echo "$OBJECTID,$AREA,$PERIMETER,$CO06_D00_,$CO06_D00_I,$STATE,$COUNTY,$NAME,$LSAD,$LSAD_TRANS,$Shape_Length,$Shape_Area,\"$WKT\"" &gt;&gt; "$OUTPUT_FILE"
done

echo "Conversion complete. Output saved to $OUTPUT_FILE"

# Upload data files to S3
aws s3 cp earthquakes.csv s3://$DATA_BUCKET/earthquakes/
aws s3 cp california-counties.csv s3://$DATA_BUCKET/counties/ 
 
This code performs the following actions to convert the geospatial data formats: 
 
 Transforms ESRI JSON to WKT format 
 Processes county boundaries into CSV format 
 Preserves spatial information for Amazon Redshift 
 
Create a Redshift cluster 
Use the following code to set up the Redshift cluster: 
 
 # Create Redshift cluster
aws redshift create-cluster \
&nbsp;&nbsp; &nbsp;--cluster-identifier "$REDSHIFT_CLUSTER_IDENTIFIER" \
&nbsp;&nbsp; &nbsp;--node-type "$REDSHIFT_NODE_TYPE" \
&nbsp;&nbsp; &nbsp;--cluster-type single-node \
&nbsp;&nbsp; &nbsp;--master-username "$REDSHIFT_MASTER_USER" \
&nbsp;&nbsp; &nbsp;--master-user-password "$REDSHIFT_MASTER_PASSWORD" \
&nbsp;&nbsp; &nbsp;--db-name "$REDSHIFT_DATABASE" \
&nbsp;&nbsp; &nbsp;--cluster-subnet-group-name "$SUBNET_GROUP_NAME" \
&nbsp;&nbsp; &nbsp;--vpc-security-group-ids "$SG_ID" \
&nbsp;&nbsp; &nbsp;--iam-roles "$REDSHIFT_ROLE_ARN"

# Wait for cluster availability
while true; do
&nbsp;&nbsp; &nbsp;CLUSTER_STATUS=$(aws redshift describe-clusters \
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;--cluster-identifier "$REDSHIFT_CLUSTER_IDENTIFIER" \
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;--query 'Clusters[0].ClusterStatus' \
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;--output text)
&nbsp;&nbsp; &nbsp;if [ "$CLUSTER_STATUS" = "available" ]; then
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;break
&nbsp;&nbsp; &nbsp;fi
&nbsp;&nbsp; &nbsp;sleep 30
done 
 
This code performs the following functions: 
 
 Sets up a single-node cluster 
 Configures networking and security 
 Waits for cluster availability 
 
Create a database schema 
Use the following code to create the database schema: 
 
 aws redshift-data execute-statement \
&nbsp;&nbsp; &nbsp;--cluster-identifier "$REDSHIFT_CLUSTER_IDENTIFIER" \
&nbsp;&nbsp; &nbsp;--database "$REDSHIFT_DATABASE" \
&nbsp;&nbsp; &nbsp;--sql "
CREATE TABLE IF NOT EXISTS counties (
&nbsp;&nbsp; &nbsp;OBJECTID INTEGER PRIMARY KEY,
&nbsp;&nbsp; &nbsp;AREA DOUBLE PRECISION,
&nbsp;&nbsp; &nbsp;NAME VARCHAR(100),
&nbsp;&nbsp; &nbsp;geom GEOMETRY
);

CREATE TABLE IF NOT EXISTS earthquakes (
&nbsp;&nbsp; &nbsp;earthquake_date VARCHAR(50),
&nbsp;&nbsp; &nbsp;latitude double precision,
&nbsp;&nbsp; &nbsp;longitude double precision,
&nbsp;&nbsp; &nbsp;magnitude double precision
);" 
 
This code performs the following functions: 
 
 Creates a counties table with spatial data 
 Creates an earthquakes table 
 Configures appropriate data types 
 
Create an Amazon Bedrock knowledge base 
Use the following code to create a knowledge base: 
 
 # Create knowledge base
aws bedrock-agent create-knowledge-base \
&nbsp;&nbsp; &nbsp;--name "$KNOWLEDGE_BASE_NAME" \
&nbsp;&nbsp; &nbsp;--knowledge-base-configuration "{
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;\"type\": \"SQL\",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;\"sqlKnowledgeBaseConfiguration\": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\"type\": \"REDSHIFT\"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp;}" \
&nbsp;&nbsp; &nbsp;--region "$AWS_REGION"

# Create data source
aws bedrock-agent create-data-source \
&nbsp;&nbsp; &nbsp;--knowledge-base-id "$KB_ID" \
&nbsp;&nbsp; &nbsp;--name "EarthquakeDataSource" \
&nbsp;&nbsp; &nbsp;--data-source-configuration "{\"type\": \"REDSHIFT_METADATA\"}" 
 
This code performs the following functions: 
 
 Creates an Amazon Bedrock knowledge base 
 Sets up an Amazon Redshift data source 
 Enables spatial queries 
 
Create an Amazon Bedrock agent 
Use the following code to create and configure an agent: 
 
 # Create agent
aws bedrock-agent create-agent \
&nbsp;&nbsp; &nbsp;--agent-name "$AGENT_NAME" \
&nbsp;&nbsp; &nbsp;--instruction "You are a geospatial analysis assistant..." \
&nbsp;&nbsp; &nbsp;--foundation-model "anthropic.claude-3-sonnet-20240229-v1:0"

# Associate knowledge base
aws bedrock-agent associate-agent-knowledge-base \
&nbsp;&nbsp; &nbsp;--agent-id "$AGENT_ID" \
&nbsp;&nbsp; &nbsp;--knowledge-base-id "$KB_ID" \
&nbsp;&nbsp; &nbsp;--description "Earthquake data knowledge base" \
&nbsp;&nbsp; &nbsp;--agent-version "DRAFT" 
 
This code performs the following functions: 
 
 Creates an Amazon Bedrock agent 
 Associates the agent with the knowledge base 
 Configures the AI model and instructions 
 
Test the solution 
Let‚Äôs observe the system behavior with the following natural language user inputs in the chat window. 
Example 1: Summarization and Q&amp;A 
For this example, we use the prompt ‚ÄúSummarize which zones allow for building of an apartment.‚Äù 
The LLM performs retrieval with a RAG approach, then uses the retrieved residential code documents as context to answer the user‚Äôs query in natural language. 
 
This example demonstrates the LLM capabilities for hallucination mitigation, RAG, and summarization. 
Example 2: Generate a draft report 
Next, we input the prompt ‚ÄúWrite me a report on how various zones and related housing data can be utilized to plan new housing development to meet high demand.‚Äù 
The LLM retrieves relevant urban planning code documents, then summarizes the information into a standard reporting format as described in its system prompt. 
 
This example demonstrates the LLM capabilities for prompt templates, RAG, and summarization. 
Example 3: Show places on the map 
For this example, we use the prompt ‚ÄúShow me the low density properties on Abbeville street in Macgregor on the map with their address.‚Äù 
The LLM creates a chain of thought to look up which properties match the user‚Äôs query and then invokes the draw marker tool on the map. The LLM provides tool invocation parameters in its scratchpad, awaits the completion of these tool invocations, then responds in natural language with a bulleted list of markers placed on the map. 
 
 
This example demonstrates the LLM capabilities for chain of thought reasoning, tool use, retrieval systems using agents, and UI control. 
Example 4: Use the UI as context 
For this example, we choose a marker on a map and input the prompt ‚ÄúCan I build an apartment here.‚Äù 
The ‚Äúhere‚Äù is not contextualized from conversation history but rather from the state of the map view. Having a state engine that can relay information from a frontend view to the LLM input adds a richer context. 
The LLM understands the context of ‚Äúhere‚Äù based on the selected marker, performs retrieval to see the land development policy, and responds to the user in simple natural language, ‚ÄúNo, and here is why‚Ä¶‚Äù 
 
This example demonstrates the LLM capabilities for UI context, chain of thought reasoning, RAG, and tool use. 
Example 5: UI context and UI control 
Next, we choose a marker on the map and input the prompt ‚Äúdraw a .25 mile circle around here so I can visualize walking distance.‚Äù 
The LLM invokes the draw circle tool to create a layer on the map centered at the selected marker, contextualized by ‚Äúhere.‚Äù 
 
This example demonstrates the LLM capabilities for UI context, chain of thought reasoning, tool use, and UI control. 
Clean up 
To clean up your resources and prevent AWS charges from being incurred, complete the following steps: 
 
 Delete the Amazon Bedrock knowledge base. 
 Delete the Redshift cluster. 
 Delete the S3 bucket. 
 
Conclusion 
The integration of LLMs with GIS creates intuitive systems that help users of different technical levels perform complex spatial analysis through natural language interactions. By using RAG and agent-based workflows, organizations can maintain data accuracy while seamlessly connecting AI models to their existing knowledge bases and structured data systems. Amazon Bedrock facilitates this convergence of AI and GIS technology by providing a robust platform for model invocation, knowledge retrieval, and system control, ultimately transforming how users visualize, analyze, and interact with geographical data. 
For further exploration, Earth on AWS has videos and articles you can explore to understand how AWS is helping build GIS applications on the cloud. 
 
About the Authors 
Dave Horne&nbsp;is a Sr. Solutions Architect supporting Federal System Integrators at AWS. He is based in Washington, DC, and has 15 years of experience building, modernizing, and integrating systems for public sector customers. Outside of work, Dave enjoys playing with his kids, hiking, and watching Penn State football! 
Kai-Jia Yue&nbsp;is a solutions architect on the Worldwide Public Sector Global Systems Integrator Architecture team at Amazon Web Services (AWS). She has a focus in data analytics and helping customer organizations make data-driven decisions. Outside of work, she loves spending time with friends and family and traveling. 
Brian Smitches is the Head of Partner Deployed Engineering at Windsurf focusing on how partners can bring organizational value through the adoption of Agentic AI software development tools like Windsurf and Devin. Brian has a background in Cloud Solutions Architecture from his time at AWS, where he worked in the&nbsp;AWS Federal Partner ecosystem. In his personal time, Brian enjoys skiing, water sports, and traveling with friends and family.
‚Ä¢ Beyond the basics: A comprehensive foundation model selection framework for generative AI
  Most organizations evaluating foundation models limit their analysis to three primary dimensions: accuracy, latency, and cost. While these metrics provide a useful starting point, they represent an oversimplification of the complex interplay of factors that determine real-world model performance. 
Foundation models have revolutionized how enterprises develop generative AI applications, offering unprecedented capabilities in understanding and generating human-like content. However, as the model landscape expands, organizations face complex scenarios when selecting the right foundation model for their applications. In this blog post we present a systematic evaluation methodology for Amazon Bedrock users, combining theoretical frameworks with practical implementation strategies that empower data scientists and machine learning (ML) engineers to make optimal model selections. 
The challenge of foundation model selection 
Amazon Bedrock is a fully managed service that offers a choice of high-performing foundation models from leading AI companies such as&nbsp;AI21 Labs,&nbsp;Anthropic,&nbsp;Cohere,&nbsp;DeepSeek,&nbsp;Luma,&nbsp;Meta,&nbsp;Mistral AI,&nbsp;poolside&nbsp;(coming soon),&nbsp;Stability AI,&nbsp;TwelveLabs&nbsp;(coming soon),&nbsp;Writer, and&nbsp;Amazon&nbsp;through a single API, along with a broad set of capabilities you need to build generative AI applications with security, privacy, and responsible AI. The service‚Äôs API-driven approach allows seamless model interchangeability, but this flexibility introduces a critical challenge: which model will deliver optimal performance for a specific application while meeting operational constraints? 
Our research with enterprise customers reveals that many early generative AI projects select models based on either limited manual testing or reputation, rather than systematic evaluation against business requirements. This approach frequently results in: 
 
 Over-provisioning computational resources to accommodate larger models than required 
 Sub-optimal performance because of misalignment between model strengths and use case requirements 
 Unnecessarily high operational costs because of inefficient token utilization 
 Production performance issues discovered too late in the development lifecycle 
 
In this post, we outline a comprehensive evaluation methodology optimized for Amazon Bedrock implementations using Amazon Bedrock Evaluations while providing forward-compatible patterns as the foundation model landscape evolves. To read more about on how to evaluate large language model (LLM) performance, see LLM-as-a-judge on Amazon Bedrock Model Evaluation. 
A multidimensional evaluation framework‚ÄîFoundation model capability matrix 
Foundation models vary significantly across multiple dimensions, with performance characteristics that interact in complex ways. Our capability matrix provides a structured view of critical dimensions to consider when evaluating models in Amazon Bedrock. Below are four core dimensions (in no specific order) ‚Äì Task performance, Architectural characteristics, Operational considerations, and Responsible AI attributes. 
Task performance 
Evaluating the models based on the task performance is crucial for achieving direct impact on business outcomes, ROI, user adoption and trust, and competitive advantage. 
 
 Task-specific accuracy: Evaluate models using benchmarks relevant to your use case (MMLU, HELM, or domain-specific benchmarks). 
 Few-shot learning capabilities: Strong few-shot performers require minimal examples to adapt to new tasks, leading to cost efficiency, faster time-to-market, resource optimization, and operational benefits. 
 Instruction following fidelity: For the applications that require precise adherence to commands and constraints, it is critical to evaluate model‚Äôs instruction following fidelity. 
 Output consistency: Reliability and reproducibility across multiple runs with identical prompts. 
 Domain-specific knowledge: Model performance varies dramatically across specialized fields based on training data. Evaluate the models base on your domain-specific use-case scenarios. 
 Reasoning capabilities: Evaluate the model‚Äôs ability to perform logical inference, causal reasoning, and multi-step problem-solving. This can include reasoning such as deductive and inductive, mathematical, chain-of-thought, and so on. 
 
Architectural characteristics 
Architectural characteristics for evaluating the models are important as they directly impact the model‚Äôs performance, efficiency, and suitability for specific tasks. 
 
 Parameter count (model size): Larger models typically offer more capabilities but require greater computational resources and may have higher inference costs and latency. 
 Training data composition: Models trained on diverse, high-quality datasets tend to have better generalization abilities across different domains. 
 Model architecture: Decoder-only models excel at text generation, encoder-decoder architectures handle translation and summarization more effectively, while mixture of experts (MoE) architectures can be a powerful tool for improving the performance of both decoder-only and encoder-decoder models. Some specialized architectures focus on enhancing reasoning capabilities through techniques like chain-of-thought prompting or recursive reasoning. 
 Tokenization methodology: The way models process text affects performance on domain-specific tasks, particularly with specialized vocabulary. 
 Context window capabilities: Larger context windows enable processing more information at once, critical for document analysis and extended conversations. 
 Modality: Modality refers to type of data a model can process and generate, such as text, image, audio, or video. Consider the modality of the models depending on the use case, and choose the model optimized for that specific modality. 
 
Operational considerations 
Below listed operational considerations are critical for model selection as they directly impact the real-world feasibility, cost-effectiveness, and sustainability of AI deployments. 
 
 Throughput and latency profiles: Response speed impacts user experience and throughput determines scalability. 
 Cost structures: Input/output token pricing significantly affects economics at scale. 
 Scalability characteristics: Ability to handle concurrent requests and maintain performance during traffic spikes. 
 Customization options: Fine-tuning capabilities and adaptation methods for tailoring to specific use cases or domains. 
 Ease of integration: Ease of integration into existing systems and workflow is an important consideration. 
 Security: When dealing with sensitive data, model security‚Äîincluding data encryption, access control, and vulnerability management‚Äîis a crucial consideration. 
 
Responsible AI attributes 
As AI becomes increasingly embedded in business operations and daily lives, evaluating models on responsible AI attributes isn‚Äôt just a technical consideration‚Äîit‚Äôs a business imperative. 
 
 Hallucination propensity: Models vary in their tendency to generate plausible but incorrect information. 
 Bias measurements: Performance across different demographic groups affects fairness and equity. 
 Safety guardrail effectiveness: Resistance to generating harmful or inappropriate content. 
 Explainability and privacy: Transparency features and handling of sensitive information. 
 Legal Implications: Legal considerations should include data privacy, non-discrimination, intellectual property, and product liability. 
 
Agentic AI considerations for model selection 
The growing popularity of agentic AI applications introduces evaluation dimensions beyond traditional metrics. When assessing models for use in autonomous agents, consider these critical capabilities: 
Agent-specific evaluation dimensions 
 
 Planning and reasoning capabilities: Evaluate chain-of-thought consistency across complex multi-step tasks and self-correction mechanisms that allow agents to identify and fix their own reasoning errors. 
 Tool and API integration: Test function calling capabilities, parameter handling precision, and structured output consistency (JSON/XML) for seamless tool use. 
 Agent-to-agent communication: Assess protocol adherence to frameworks like A2A and efficient contextual memory management across extended multi-agent interactions. 
 
Multi-agent collaboration testing for applications using multiple specialized agents 
 
 Role adherence: Measure how well models maintain distinct agent personas and responsibilities without role confusion. 
 Information sharing efficiency: Test how effectively information flows between agent instances without critical detail loss. 
 Collaborative intelligence: Verify whether multiple agents working together produce better outcomes than single-model approaches. 
 Error propagation resistance: Assess how robustly multi-agent systems contain and correct errors rather than amplifying them. 
 
A four-phase evaluation methodology 
Our recommended methodology progressively narrows model selection through increasingly sophisticated assessment techniques: 
Phase 1: Requirements engineering 
Begin with a precise specification of your application‚Äôs requirements: 
 
 Functional requirements: Define primary tasks, domain knowledge needs, language support, output formats, and reasoning complexity. 
 Non-functional requirements: Specify latency thresholds, throughput requirements, budget constraints, context window needs, and availability expectations. 
 Responsible AI requirements: Establish hallucination tolerance, bias mitigation needs, safety requirements, explainability level, and privacy constraints. 
 Agent-specific requirements: For agentic applications, define tool-use capabilities, protocol adherence standards, and collaboration requirements. 
 
Assign weights to each requirement based on business priorities to create your evaluation scorecard foundation. 
Phase 2: Candidate model selection 
Use the Amazon Bedrock model information API to filter models based on hard requirements. This typically reduces candidates from dozens to 3‚Äì7 models that are worth detailed evaluation. 
Filter options include but aren‚Äôt limited to the following: 
 
 Filter by modality support, context length, and language capabilities 
 Exclude models that don‚Äôt meet minimum performance thresholds 
 Calculate theoretical costs at projected scale so that you can exclude options that exceed the available budget 
 Filter for customization requirements such as fine-tuning capabilities 
 For agentic applications, filter for function calling and multi-agent protocol support 
 
Although the Amazon Bedrock model information API might not provide the filters you need for candidate selection, you can use the Amazon Bedrock model catalog (shown in the following figure) to obtain additional information about these models. 
 
Phase 3: Systematic performance evaluation 
Implement structured evaluation using Amazon Bedrock Evaluations: 
 
 Prepare evaluation datasets: Create representative task examples, challenging edge cases, domain-specific content, and adversarial examples. 
 Design evaluation prompts: Standardize instruction format, maintain consistent examples, and mirror production usage patterns. 
 Configure metrics: Select appropriate metrics for subjective tasks (human evaluation and reference-free quality), objective tasks (precision, recall, and F1 score), and reasoning tasks (logical consistency and step validity). 
 For agentic applications: Add protocol conformance testing, multi-step planning assessment, and tool-use evaluation. 
 Execute evaluation jobs: Maintain consistent parameters across models and collect comprehensive performance data. 
 Measure operational performance: Capture throughput, latency distributions, error rates, and actual token consumption costs. 
 
Phase 4: Decision analysis 
Transform evaluation data into actionable insights: 
 
 Normalize metrics: Scale all metrics to comparable units using min-max normalization. 
 Apply weighted scoring: Calculate composite scores based on your prioritized requirements. 
 Perform sensitivity analysis: Test how robust your conclusions are against weight variations. 
 Visualize performance: Create radar charts, efficiency frontiers, and tradeoff curves for clear comparison. 
 Document findings: Detail each model‚Äôs strengths, limitations, and optimal use cases. 
 
Advanced evaluation techniques 
Beyond standard procedures, consider the following approaches for evaluating models. 
A/B testing with production traffic 
Implement comparative testing using Amazon Bedrock‚Äôs routing capabilities to gather real-world performance data from actual users. 
Adversarial testing 
Test model vulnerabilities through prompt injection attempts, challenging syntax, edge case handling, and domain-specific factual challenges. 
Multi-model ensemble evaluation 
Assess combinations such as sequential pipelines, voting ensembles, and cost-efficient routing based on task complexity. 
Continuous evaluation architecture 
Design systems to monitor production performance with: 
 
 Stratified sampling of production traffic across task types and domains 
 Regular evaluations and trigger-based reassessments when new models emerge 
 Performance thresholds and alerts for quality degradation 
 User feedback collection and failure case repositories for continuous improvement 
 
Industry-specific considerations 
Different sectors have unique requirements that influence model selection: 
 
 Financial services: Regulatory compliance, numerical precision, and personally identifiable information (PII) handling capabilities 
 Healthcare: Medical terminology understanding, HIPAA adherence, and clinical reasoning 
 Manufacturing: Technical specification comprehension, procedural knowledge, and spatial reasoning 
 Agentic systems: Autonomous reasoning, tool integration, and protocol conformance 
 
Best practices for model selection 
Through this comprehensive approach to model evaluation and selection, organizations can make informed decisions that balance performance, cost, and operational requirements while maintaining alignment with business objectives. The methodology makes sure that model selection isn‚Äôt a one-time exercise but an evolving process that adapts to changing needs and technological capabilities. 
 
 Assess your situation thoroughly: Understand your specific use case requirements and available resources 
 Select meaningful metrics: Focus on metrics that directly relate to your business objectives 
 Build for continuous evaluation: Design your evaluation process to be repeatable as new models are released 
 
Looking forward: The future of model selection 
As foundation models evolve, evaluation methodologies must keep pace. Below are further considerations (By no means this list of considerations is exhaustive and is subject to ongoing updates as technology evolves and best practices emerge), you should take into account while selecting the best model(s) for your use-case(s). 
 
 Multi-model architectures: Enterprises will increasingly deploy specialized models in concert rather than relying on single models for all tasks. 
 Agentic landscapes: Evaluation frameworks must assess how models perform as autonomous agents with tool-use capabilities and inter-agent collaboration. 
 Domain specialization: The growing landscape of domain-specific models will require more nuanced evaluation of specialized capabilities. 
 Alignment and control: As models become more capable, evaluation of controllability and alignment with human intent becomes increasingly important. 
 
Conclusion 
By implementing a comprehensive evaluation framework that extends beyond basic metrics, organizations can informed decisions about which foundation models will best serve their requirements. For agentic AI applications in particular, thorough evaluation of reasoning, planning, and collaboration capabilities is essential for success. By approaching model selection systematically, organizations can avoid the common pitfalls of over-provisioning, misalignment with use case needs, excessive operational costs, and late discovery of performance issues. The investment in thorough evaluation pays dividends through optimized costs, improved performance, and superior user experiences. 
 
About the author 
Sandeep Singh is a Senior Generative AI Data Scientist at Amazon Web Services, helping businesses innovate with generative AI. He specializes in generative AI, machine learning, and system design. He has successfully delivered state-of-the-art AI/ML-powered solutions to solve complex business problems for diverse industries, optimizing efficiency and scalability.
‚Ä¢ Accelerate intelligent document processing with generative AI on AWS
  Every day, organizations process millions of documents, including invoices, contracts, insurance claims, medical records, and financial statements. Despite the critical role these documents play, an estimated 80‚Äì90% of the data they contain is unstructured and largely untapped, hiding valuable insights that could transform business outcomes. Despite advances in technology, many organizations still rely on manual data entry, spending countless hours extracting information from PDFs, scanned images, and forms. This manual approach is time-consuming, error-prone, and prevents organizations from scaling their operations and responding quickly to business demands. 
Although generative AI has made it easier to build proof-of-concept document processing solutions, the journey from proof of concept to production remains fraught with challenges. Organizations often find themselves rebuilding from scratch when they discover their prototype can‚Äôt handle production volumes, lacks proper error handling, doesn‚Äôt scale cost-effectively, or fails to meet enterprise security and compliance requirements. What works in a demo with a handful of documents often breaks down when processing thousands of documents daily in a production environment. 
In this post, we introduce our open source GenAI IDP Accelerator‚Äîa tested solution that we use to help customers across industries address their document processing challenges. Automated document processing workflows accurately extract structured information from documents, reducing manual effort. We will show you how this ready-to-deploy solution can help you build those workflows with generative AI on AWS in days instead of months. 
Understanding intelligent document processing 
Intelligent document processing (IDP) encompasses the technologies and techniques used to extract and process data from various document types. Common IDP tasks include: 
 
 OCR (Optical Character Recognition) ‚Äì Converting scanned documents and images into machine-readable text 
 Document classification ‚Äì Automatically identifying document types (such as invoices, contracts, or forms) 
 Data extraction ‚Äì Pulling structured information from unstructured documents 
 Assessment ‚Äì Evaluating the quality and confidence of extracted data 
 Summarization ‚Äì Creating concise summaries of document content 
 Evaluation ‚Äì Measuring accuracy and performance against expected outcomes 
 
These capabilities are critical across industries. In financial services, organizations use IDP to process loan applications, extract data from bank statements, and validate insurance claims. Healthcare providers rely on IDP to extract patient information from medical records, process insurance forms, and handle lab results efficiently. Manufacturing and logistics companies use IDP to process invoices and purchase orders, extract shipping information, and handle quality certificates. Government agencies use IDP to process citizen applications, extract data from tax forms, manage permits and licenses, and enforce regulatory compliance. 
The generative AI revolution in IDP 
Traditional IDP solutions relied on template-based extraction, regular expressions, and classical machine learning (ML) models. Though functional, these approaches required extensive setup, struggled with document variations, and achieved limited accuracy on complex documents. 
The emergence of large language models (LLMs) and generative AI has fundamentally transformed IDP capabilities. Modern AI models can understand document context, handle variations without templates, achieve near-human accuracy on complex extractions, and adapt to new document types with minimal examples. This shift from rule-based to intelligence-based processing means organizations can now process different document types with high accuracy, dramatically reducing the time and cost of implementation. 
GenAI IDP Accelerator 
We‚Äôre excited to share the GenAI IDP Accelerator‚Äîan open source solution that transforms how organizations handle document processing by dramatically reducing manual effort and improving accuracy. This serverless foundation offers processing patterns which use Amazon Bedrock Data Automation for rich out-of-the-box document processing features, high accuracy, ease of use, and straightforward per-page pricing, Amazon Bedrock state-of-the-art foundation models (FMs) for complex documents requiring custom logic, and other AWS AI services to provide a flexible, scalable starting point for enterprises to build document automation tailored to their specific needs. 
The following is a short demo of the solution in action, in this case showcasing the default Amazon Bedrock Data Automation processing pattern. 

 
  
 
 
Real-world impact 
The GenAI IDP Accelerator is already transforming document processing for organizations across industries. 
Competiscan: Transforming marketing intelligence at scale 
Competiscan, a leader in competitive marketing intelligence, faced a massive challenge: processing 35,000‚Äì45,000 marketing campaigns daily while maintaining a searchable archive of 45 million campaigns spanning 15 years. 
Using the GenAI IDP Accelerator, Competiscan achieved the following: 
 
 85% classification and extraction accuracy across diverse marketing materials 
 Increased scalability to handle 35,000‚Äì45,000 daily campaigns 
 Removal of critical bottlenecks, facilitating business growth 
 Production deployment in just 8 weeks from initial concept 
 
Ricoh: Scaling document processing 
Ricoh, a global leader in document management, implemented the GenAI IDP Accelerator to transform healthcare document processing for their clients. Processing over 10,000 healthcare documents monthly with potential to scale to 70,000, they needed a solution that could handle complex medical documentation with high accuracy. 
The results speak for themselves: 
 
 Savings potential of over 1,900 person-hours annually through automation 
 Achieved extraction accuracy to help minimize financial penalties from processing errors 
 Automated classification of grievances vs. appeals 
 Created a reusable framework deployable across multiple healthcare customers 
 Integrated with human-in-the-loop review for cases requiring expert validation 
 Leveraged modular architecture to integrate with existing systems, enabling custom document splitting and large-scale document processing 
 
Solution overview 
The GenAI IDP Accelerator is a modular, serverless solution that automatically converts unstructured documents into structured, actionable data. Built entirely on AWS services, it provides enterprise-grade scalability, security, and cost-effectiveness while requiring minimal setup and maintenance. Its configuration-driven design helps teams quickly adapt prompts, extraction templates, and validation rules for their specific document types without touching the underlying infrastructure. 
The solution follows a modular pipeline that enriches documents at each stage, from OCR to classification, to extraction, to assessment, to summarization, and ending with evaluation. 
You can deploy and customize each step independently, so you can optimize for your specific use cases while maintaining the benefits of the integrated workflow. 
The following diagram illustrates the solution architecture, showing the default Bedrock Data Automation workflow (Pattern-1). 
 
Refer to the GitHub repo for additional details and processing patterns. 
Some of the key features of the solution include: 
 
 Serverless architecture ‚Äì Built on AWS Lambda, AWS Step Functions, and other serverless technologies for queueing, concurrency management, and retries to provide automatic scaling and pay-per-use pricing for production workloads of many sizes 
 Generative AI-powered document packet splitting and classification ‚Äì Intelligent document classification using Amazon Bedrock Data Automation or Amazon Bedrock multimodal FMs, including support for multi-document packets and packet splitting 
 Advanced AI key information extraction ‚Äì Key information extraction using Amazon Bedrock Data Automation or Amazon Bedrock multimodal FMs 
 Multiple processing patterns ‚Äì Choose from pre-built patterns optimized for different workloads with different configurability, cost, and accuracy requirements, or extend the solution with additional patterns: 
   
   Pattern 1 ‚Äì Uses Amazon Bedrock Data Automation, a fully managed service that offers rich out-of-the-box features, ease of use, and straightforward per-page pricing. This pattern is recommended for most use cases. 
   Pattern 2 ‚Äì Uses Amazon Textract and Amazon Bedrock with Amazon Nova, Anthropic‚Äôs Claude, or custom fine-tuned Amazon Nova models. This pattern is ideal for complex documents requiring custom logic. 
   Pattern 3 ‚Äì Uses Amazon Textract, Amazon SageMaker with a fine-tuned model for classification, and Amazon Bedrock for extraction. This pattern is ideal for documents requiring specialized classification. 
    
 
We expect to add more pattern options to handle additional real-world document processing needs, and to take advantage of ever-improving state-of-the-art capabilities: 
 
 Few-shot learning ‚Äì Improve accuracy for classification and extraction by providing few-shot examples to guide the AI models 
 Confidence assessment ‚Äì AI-powered quality assurance that evaluates extraction field confidence, used to indicate documents for human review 
 Human-in-the-loop (HITL) review ‚Äì Integrated workflow for human review of low-confidence extractions using Amazon SageMaker Augmented AI (Amazon A2I), currently available for Pattern 1, with support for Patterns 2 and 3 coming soon 
 Web user interface ‚Äì Responsive web UI for monitoring document processing, viewing results, and managing configurations 
 Knowledge base integration ‚Äì Query processed documents using natural language through Amazon Bedrock Knowledge Bases 
 Built-in evaluation ‚Äì Framework to evaluate and improve accuracy against baseline data 
 Analytics and reporting database ‚Äì Centralized analytics database for tracking processing metrics, accuracy trends, and cost optimization across document workflows, and for analyzing extracted document content using Amazon Athena 
 No-code configuration ‚Äì Customize document types, extraction fields, and processing logic through configuration, editable in the web UI 
 Developer-friendly python package ‚Äì For data science and engineering teams who want to experiment, optimize, or integrate the IDP capabilities directly into their workflows, the solution‚Äôs core logic is available through the idp_common Python package 
 
Prerequisites 
Before you deploy the solution, make sure you have an AWS account with administrator permissions and access to Amazon and Anthropic models on Amazon Bedrock. For more details, see Access Amazon Bedrock foundation models. 
Deploy the GenAI IDP Accelerator 
To deploy the GenAI IDP Accelerator, you can use the provided AWS CloudFormation template. For more details, see the quick start option on the GitHub repo. The high-level steps are as follows: 
 
 Log in to your AWS account. 
 Choose Launch Stack for your preferred AWS Region: 
 
 
  
   
   Region 
   Launch Stack 
   
  
  
   
   US East (N. Virginia) 
    
   
   
   US West (Oregon) 
    
   
  
 
 
 Enter your email address and choose your processing pattern (default is Pattern 1, using Amazon Bedrock Data Automation). 
 Use defaults for all other configuration parameters. 
 Deploy the stack. 
 
The stack takes approximately 15‚Äì20 minutes to deploy the resources. After deployment, you will receive an email with login credentials for the web interface. 
Process documents 
After you deploy the solution, you can start processing documents: 
 
 Use the web interface to upload a sample document (you can use the provided sample: lending_package.pdf). 
 
In production, you typically automate loading your documents directly to the Amazon Simple Storage Service (Amazon S3) input bucket, automatically triggering processing. To learn more, see Testing without the UI. 
 
 
 Select your document from the document list and choose View Processing Flow to watch as your document flows through the pipeline. 
 
 
 
 Examine the extracted data with confidence scores. 
 
 
 
 Use the knowledge base feature to ask questions about processed content. 
 
 
Alternative deployment methods 
You can build the solution from source code if you need to deploy the solution to additional Regions or build and deploy code changes.  
We hope to add support for AWS Cloud Development Kit (AWS CDK) and Terraform deployments. Follow the GitHub repository for updates, or contact AWS Professional Services for implementation assistance. 
Update an existing GenAI IDP Accelerator stack 
You can update your existing GenAI IDP Accelerator stack to the latest release. For more details, see Updating an Existing Stack. 
Clean up 
When you‚Äôre finished experimenting, clean up your resources by using the AWS CloudFormation console to delete the IDP stack that you deployed. 
Conclusion 
In this post, we discussed the GenAI IDP Accelerator, a new approach to document processing that combines the power of generative AI with the reliability and scale of AWS. You can process hundreds or even millions of documents to achieve better results faster and more cost-effectively than traditional approaches. 
Visit the GitHub repository for detailed guides and examples and choose watch to stay informed on new releases and features. AWS Professional Services and AWS Partners are available to help with implementation. You can also join the GitHub community to contribute improvements and share your experiences. 
 
About the Authors 
Bob Strahan is a Principal Solutions Architect in the AWS Generative AI Innovation Center. 
Joe King is a Senior Data Scientist in the AWS Generative AI Innovation Center. 
Mofijul Islam is an Applied Scientist in the AWS Generative AI Innovation Center. 
Vincil Bishop is a Senior Deep Learning Architect in the AWS Generative AI Innovation Center. 
David Kaleko is a Senior Applied Scientist in the AWS Generative AI Innovation Center. 
Rafal Pawlaszek is a Senior Cloud Application Architect in the AWS Generative AI Innovation Center. 
Spencer Romo is a Senior Data Scientist in the AWS Generative AI Innovation Center. 
Vamsi Thilak Gudi is a Solutions Architect in the AWS World Wide Public Sector team. 
 
 
Acknowledgments 
We would like to thank&nbsp;Abhi Sharma, Akhil Nooney, Aleksei Iancheruk, Ava Kong, Boyi Xie, Diego Socolinsky, Guillermo Tantachuco, Ilya Marmur, Jared Kramer, Jason Zhang, Jordan Ratner, Mariano Bellagamba, Mark Aiyer, Niharika Jain, Nimish Radia, Shean Sager, Sirajus Salekin, Yingwei Yu, and many others in our expanding community, for their unwavering vision, passion, contributions, and guidance throughout.

‚∏ª