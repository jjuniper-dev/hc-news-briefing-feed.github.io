‚úÖ Morning News Briefing ‚Äì October 22, 2025 10:47

üìÖ Date: 2025-10-22 10:47
üè∑Ô∏è Tags: #briefing #ai #publichealth #digitalgov

‚∏ª

üßæ Weather
‚Ä¢ Current Conditions:  5.9¬∞C
  Temperature: 5.9&deg;C Pressure / Tendency: 100.2 kPa falling Humidity: 100% Humidity is 100% Dewpoint: 5 .9&degree-point . Wind: SE 3 km/h Air Quality Health Index: n/a . Pembroke 6:00 AM EDT Wednesday 22 October 2025 at 6.00am GMT .
‚Ä¢ Wednesday: Chance of showers. High 10. POP 60%
  A few showers beginning this morning and ending near noon then 60 percent chance of showers this afternoon . Increasing cloudiness. High 10.5C . UV index 2 or low for the area, with a high of 10C or 10C . Forecast issued 5:00 AM EDT Wednesday 22 October 2025. For the rest of the day, see www.dailymailonline.co.
‚Ä¢ Wednesday night: Chance of showers. Low plus 5. POP 60%
  Cloudy with 60 percent chance of showers . Wind becoming northwest 30 km/h after midnight . Low plus 5.50 per cent chance of rain in New York City . Rainy with low risk of snowfall in New Year's Forecast issued 5:00 AM EDT Wednesday 22 October 2025 . Forecast: Showers, rain, snow, rain and snowfall forecast for New Year

üåç International News
No updates.

üçÅ Canadian News
No updates.

üá∫üá∏ U.S. Top Stories
‚Ä¢ Mental exercise can reverse a brain change linked to aging, study finds
  Scientists have found the first compelling evidence that cognitive training can boost levels of a brain chemical that typically declines as people age . The chemical is found to be linked to levels of brain chemicals that typically decline as a person ages . Scientists say it is the first study to show that training boosts levels of the chemical that declines as the population ages more and more as people get older than they do .
‚Ä¢ The federal government is still shut down. Here's what that means across the country
  NPR Network is following the ways the shutdown is affecting services across the country . The federal government remains shut down . NPR is reporting on how the shutdown affects services in the U.S. The shutdown continues to affect services in Washington, D.C., New York, New Jersey, New York and New York . The shutdown is expected to continue for the next two weeks . The government shutdown
‚Ä¢ Mamdani's rise in NYC reflects generational fight within the Democratic Party
  Newcomer Zohran Mamdani, age 34, has used social media and big progressive ideas to shoulder past Andrew Cuomo, who's 67 and long a member of Democratic Party royalty . Newcomers are using social media, big ideas and big social media to push past Cuomo's 67-year-old status as a Democratic Party insider . Mammani is a first-
‚Ä¢ Trump uses 'common sense' to make a political point. It has populist appeal
  The phrase appeals to several demographics that strongly align with Trump, says Frank Luntz, including older voters . He suggests it signals "a more simple past" for older voters, for whom it signals a "simple past" The phrase is popular with older voters and some of the GOP's most conservative voters, he says . The phrase also appeals to older voters who want a more simple
‚Ä¢ Spanish league cancels plans for Barcelona to play regular-season match in Miami
  Barcelona to play Villarreal outside Miami in December has been called off after increased opposition to the match . The Spanish soccer league said Tuesday that plans for the game to be played in December were called off, following increased opposition . The match was played in Miami, Florida, outside Miami, in December . The game was played by Barcelona in a friendly against Villar Real in December 2013 .

üß† Artificial Intelligence
No updates.

üíª Digital Strategy
‚Ä¢ Jaguar Land Rover cyber-meltdown tipped to cost the UK almost ¬£2B
  The Jaguar Land Rover cyberattack could end up being the costliest such incident in UK history, billed at an estimated ¬£1.9 billion and affecting over 5,000 organizations . That's a lot of extended warranties . JLR cyberattack is billed at around ¬£1 billion and affects over 5.000 organizations across the UK . It could be the most expensive such a cyberattack in
‚Ä¢ China's CR450 bullet train clocks 453 km/h in pre-service tests
  China's CR450 train hit 453 km/h during pre-service trials, surpassing its CR400 predecessor's 420km/h . CR450 surpasses Deutsche Bahn's 405 km/H test record . The new train surpasses its predecessor's 442 km-h test record and outpaces its 405 km-a-h record . China's new train will be
‚Ä¢ Royal Navy sharpens claws on Wildcat choppers with anti-drone Martlet missiles
  Laser-guided weapon reaches full service after successful sea trials . Martlet missiles are now declared fully operational following the anti-ship Sea Venom gaining initial operating capability (IOC) earlier this month . The weapon is now fully operational and will soon be used by Royal Navy helicopters . It is the first weapon to reach full service for the Royal Navy in its fleet of aircraft carriers and submarines .
‚Ä¢ Carnegie Mellon team claims vector-based system can turbocharge PostgreSQL
  Researchers say 'Proto-X' fine-tunes databases automatically, delivering multifold performance boosts . Automated database systems based on vector embedding algorithms could improve the performance of default settings on common Postgres database services by a factor of two to ten, according to a database researcher . ‚ÄòProtoX‚Äô could improve defaults on default settings by up to ten times .
‚Ä¢ UK data regulator defends decision not to investigate MoD Afghan data breach
  ICO says probe unnecessary after reviewing ministry's handling of leak . UK data protection regulator declined to launch an investigation into the leak at the Ministry of Defence . ICO: Leak risked the lives of thousands of Afghans connected with the British Armed Forces . ICO said it was unnecessary to launch probe into the Ministry's leak at its handling of the leak . Thousands of Afghans are believed to have been killed

üè• Public Health
No updates.

üî¨ Science
‚Ä¢ Accessing acute care hospitals in the San Francisco Bay Area after a major hayward earthquake
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Advancing sleep health equity through deep learning on large-scale nocturnal respiratory signals
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Breastfeeding knowledge and body image affect breastfeeding attitude of pregnant women: a cross-sectional study
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Hopes of realizing a dream ‚Äòcity of health‚Äô
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ A community cross-sectional study on oral health status among rural and urban inhabitants of Zambia
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations

üßæ Government & Policy
No updates.

üèõÔ∏è Enterprise Architecture & IT Governance
No updates.

ü§ñ AI & Emerging Tech
‚Ä¢ 3 Things Stephanie Arnett is into right now
  The latest version of the birding app Merlin helps ease the transition to birding . The app can analyze which birds are singing in real time with step-by-step questions, photos, or audio . It's open-source and files are stored on my device, so I don't have to worry about whether I‚Äôm sharing my private thoughts with a company that might scrape them for AI .
‚Ä¢ Dispatch: Partying at one of Africa‚Äôs largest AI gatherings
  Deep Learning Indaba is an annual AI conference where Africans present their research and technologies they‚Äôve built . The event began in 2017 from a nucleus of 300 people gathered in Johannesburg, South Africa . This year, nearly 3,000 people applied to join the Indaba; about 1,300 were accepted . The main ‚Äòprize‚Äô for many attendees is to be hired by a tech company or accepted into a PhD program, says Nyalleng Moorosi .
‚Ä¢ Job titles of the future: AI embryologist
  Embryologists are the scientists behind the scenes of in vitro fertilization who oversee the development and selection of embryos, prepare them for transfer, and maintain the lab environment . The U.S. is facing a critical shortage of both embryologists and genetic counselors . The company's algorithm, called CHLOE (for Cultivating Human Life through Optimal Embryos), has been trained on millions of embryo data points and outcomes .
‚Ä¢ Inside the archives of the NASA Ames Research Center
  NASA Ames Research Center was founded in 1939 as a West Coast lab for the National Advisory Committee for Aeronautics (NACA) Named for Joseph Sweetman Ames, the facility grew from a shack on Moffett Field into a sprawling compound with thousands of employees . A collection of 5,000 images from NASA Ames‚Äôs archives paints a vivid picture of bleeding-edge work at the center .
‚Ä¢ Engineering¬†better care
  Every Monday, more than a hundred members of Giovanni Traverso‚Äôs Laboratory for Translational Engineering (L4TE) fill a large classroom at Brigham and Women‚Äôs Hospital for their weekly lab meeting. With a social hour, food for everyone, and updates across disciplines from mechanical engineering to veterinary science, it‚Äôs a place where a stem cell biologist might weigh in on a mechanical design, or an electrical engineer might spot a flaw in a drug delivery mechanism. And it‚Äôs a place where everyone is united by the same goal: engineering new ways to deliver medicines and monitor the body to improve patient care.



Traverso‚Äôs weekly meetings bring together a mix of expertise that lab members say is unusual even in the most collaborative research spaces. But his lab‚Äîwhich includes its own veterinarian and a dedicated in vivo team‚Äîisn‚Äôt built like most. As an associate professor at MIT, a gastroenterologist at Brigham and Women‚Äôs, and an associate member of the Broad Institute, Traverso leads a sprawling research group that spans institutions, disciplines, and floors of lab space at MIT and beyond.&nbsp;



For a lab of this size‚Äîspread across MIT, the Broad, the Brigham, the Koch Institute, and The Engine‚Äîit feels remarkably personal. Traverso, who holds the Karl Van Tassel (1925) Career Development Professorship, is known for greeting every member by name and scheduling one-on-one meetings every two or three weeks, creating a sense of trust and connection that permeates the lab.



That trust is essential for a team built on radical interdisciplinarity. L4TE brings together mechanical and electrical engineers, biologists, physicians, and veterinarians in a uniquely structured lab with specialized ‚Äúcores‚Äù such as fabrication, bioanalytics, and in vivo teams. The setup means a researcher can move seamlessly from developing a biological formulation to collaborating with engineers to figure out the best way to deliver it‚Äîwithout leaving the lab‚Äôs ecosystem. It‚Äôs a culture where everyone‚Äôs expertise is valued, people pitch in across disciplines, and projects aim squarely at the lab‚Äôs central goal: creating medical technologies that not only work in theory but survive the long, unpredictable journey to the patient.



‚ÄúAt the core of what we do is really thinking about the patient, the person, and how we can help make their life better,‚Äù Traverso says.



Helping patients ASAP



Traverso‚Äôs team has developed a suite of novel technologies: a star-shaped capsule that unfolds in the stomach and delivers drugs for days or weeks; a vibrating pill that mimics the feeling of fullness; the technology behind a once-a-week antipsychotic tablet that has completed phase III clinical trials. (See ‚ÄúDesigning devices for real-world care,‚Äù below.) Traverso has cofounded 11 startups to carry such innovations out of the lab and into the world, each tailored to the technology and patient population it serves.



But the products are only part of the story. What distinguishes Traverso‚Äôs approach is the way those products are conceived and built. In many research groups, initial discoveries are developed into early prototypes and then passed on to other teams‚Äîsometimes in industry, sometimes in clinical settings‚Äîfor more advanced testing and eventual commercialization. Traverso‚Äôs lab typically links those steps into one continuous system, blending invention, prototyping, testing, iteration, and clinical feedback as the work of a single interdisciplinary team. Engineers sit shoulder to shoulder with physicians, materials scientists with microbiologists. On any given day, a researcher might start the morning discussing an animal study with a veterinarian, spend the afternoon refining a mechanical design, and close the day in a meeting with a regulatory expert. The setup collapses months of back-and-forth between separate teams into the collaborative environment of L4TE.



‚ÄúThis is a lab where if you want to learn something, you can learn everything if you want,‚Äù says Troy Ziliang Kang, one of the research scientists.&nbsp;




In a field where translating scientific ideas into practical applications can take years (or stall indefinitely), Traverso has built a culture designed to shorten that path.




The range of problems the lab tackles reflects its interdisciplinary openness. One recent project aimed to replace invasive contraceptive devices such as vaginal rings with a biodegradable injectable that begins as a liquid, solidifies inside the body, and dissolves safely over time.&nbsp;



Another project addresses the challenge of delivering drugs directly to the gut, bypassing the mucus barrier that blocks many treatments. For Kang, whose grandfather died of gastric cancer, the work is personal. He‚Äôs developing devices that combine traditional drugs with&nbsp;electroceuticals‚Äîtherapies that use electrical stimulation to influence cells or tissues.



‚ÄúWhat I‚Äôm trying to do is find a mechanical approach, trying to see if we can really, through physical and mechanical approaches, break through those barriers and to deliver the electroceuticals and drugs to the gut,‚Äù he says.



In a field where the process of translating scientific ideas into practical applications can take years (or stall indefinitely), Traverso, 49, has built a culture designed to shorten that path. Researchers focus on designing devices with the clinical relevance to help people in the near term.&nbsp; And they don‚Äôt wait for outsiders to take an idea forward. They often initiate collaborations with entrepreneurs, investors, and partners to create startups or push projects directly into early trials‚Äîor even just do it themselves. The projects in the L4TE Lab are ambitious, but the aim is simple: Solve problems that matter and build the tools to make those solutions real.



Nabil Shalabi, an instructor in medicine at Harvard/BWH, an associate scientist at the Broad Institute, and a research affiliate in Traverso‚Äôs lab, sums up the attitude succinctly: ‚ÄúI would say this lab is really about one thing, and it‚Äôs about helping people.‚Äù



The physician-inventor



Traverso‚Äôs path into medicine and engineering began far from the hospitals and labs where he works today. Born in Cambridge, England, he moved with his family to Peru when he was still young. His father had grown up there in a family with Italian roots; his mother came from Nicaragua. He spent most of his childhood in Lima before political turmoil in Peru led his family to relocate to Toronto when he was 14.



In high school, after finishing most of his course requirements early, he followed the advice of a chemistry teacher and joined a co-op program that would give him a glimpse of some career options. That decision brought him to a genetics lab at the Toronto Hospital for Sick Children, where he spent his afternoons helping map chromosome 7 and learning molecular techniques like PCR.



‚ÄúIn high school, and even before that, I always enjoyed science,‚Äù Traverso says.



After class, he‚Äôd ride the subway downtown and step into a world of hands-on science, working alongside graduate students in the early days of genomics.



‚ÄúI really fell in love with the day-to-day, the process, and how one goes about asking a question and then trying to answer that question experimentally,‚Äù he says.



By the time he finished high school, he had already begun to see how science and medicine could intersect. He began an undergraduate medical program at Cambridge University, but during his second year, he reached out to the cancer biologist Bert Vogelstein and joined his lab at Johns Hopkins for the summer. The work resonated. By the end of the internship, Vogelstein asked if he‚Äôd consider staying to pursue a PhD. Traverso agreed, pausing his medical training after earning an undergraduate degree in medical sciences and genetics, and moved to Baltimore to begin a doctorate in molecular biology.



As a PhD student, he focused on the early detection of colon cancer, developing a method to identify mutations in stool samples‚Äîa concept later licensed by Exact Sciences and used in what is now known as the Cologuard test. After completing his PhD (and earning a spot on Technology Review‚Äôs 2003 TR35 list of promising young innovators for that work), he returned to Cambridge to finish medical school and spent the next three years in the UK, including a year as a house officer (the equivalent of a clinical intern in the US).



Traverso chose to pursue clinical training alongside research because he believed each would make the other stronger. ‚ÄúI felt that having the knowledge would help inform future research development,‚Äù he says.



An ingestible drug-releasing capsule about the size of a multivitamin expands into a star shape once inside the patient‚Äôs stomach.JARED LEEDS




So in 2007, as Traverso began a residency in internal medicine at Brigham and Women‚Äôs, he also approached MIT, where he reached out to Institute Professor Robert Langer, ScD ‚Äô74. Though Traverso didn‚Äôt have a background in Langer‚Äôs field of chemical engineering, he saw the value of pairing clinical insight with the materials science research happening in the professor‚Äôs lab, which develops polymers, nanoparticles, and other novel materials to tackle biomedical challenges such as delivering drugs precisely to diseased tissue or providing long-term treatment through implanted devices. Langer welcomed him into the group as a postdoctoral fellow.



In Langer‚Äôs lab, he found a place where clinical problems sparked engineering solutions, and where those solutions were designed with the patient in mind from the outset. Many of Traverso‚Äôs ideas came directly from his work in the hospital: Could medications be delivered in ways that make it easier for patients to take them consistently? Could a drug be redesigned so it wouldn‚Äôt require refrigeration in a rural clinic? And caring for a patient who‚Äôd swallowed shards of glass that ultimately passed without injury led Traverso to recognize the GI tract‚Äôs tolerance for sharp objects, inspiring his work on the microneedle pill.



‚ÄúA lot of what we do and think about is: How do we make it easier for people to receive therapy for conditions that they may be suffering from?‚Äù Traverso says. How can they ‚Äúreally maximize health, whether it be by nutrient enhancement or by helping women have control over their fertility?‚Äù&nbsp;



If the lab sometimes runs like a startup incubator, its founder still thinks like a physician.



Scaling up to help more people



Traverso has cofounded multiple companies to help commercialize his group‚Äôs inventions. Some target global health challenges, like developing more sustainable personal protective equipment (PPE) for health-care workers. Others take on chronic conditions that require constant dosing‚ÄîHIV, schizophrenia, diabetes‚Äîby developing long-¬≠acting oral or injectable therapies.



From the outset, materials, dimensions, and mechanisms are chosen for more than just performance in the lab. The researchers also consider the realities of regulation, manufacturing constraints, and safe use in patients.



‚ÄúWe definitely want to be designing these devices to be made of safe materials or [at a] safe size,‚Äù says James McRae, SM ‚Äô22, PhD ‚Äô25. ‚ÄúWe think about these regulatory constraints that could come up in a company setting pretty early in our research process.‚Äù As part of his PhD work with Traverso, McRae created a ‚Äúswallow-¬≠and-forget‚Äù health-tracking capsule that can stay in the stomach for months‚Äîand it doesn‚Äôt require surgery to install, as an implant would. The capsule measures tiny shifts in stomach temperature that happen whenever a person eats or drinks, providing a continuous record of eating patterns that‚Äôs far more reliable than what external devices or self-reporting can capture. The technology could offer new insight into how drugs such as Ozempic and other GLP-1 therapies change behavior‚Äîsomething that has been notoriously hard to monitor. From ‚Äúday one,‚Äù McRae made sure to involve external companies and regulatory consultants for future human testing.



Traverso describes the lab‚Äôs work as a ‚Äúcontinuum,‚Äù likening research projects to children who are born, nurtured, and eventually sent into the world to thrive and help people.



Traverso and his team developed a device that can adhere to soft, wet surfaces. The design was inspired by studies of a sucker fish that attaches to sharks and other marine animals.COURTESY OF THE RESEARCHERS




For lab employee Matt Murphy, a mechanical engineer who manages one of the main mechanical fabrication spaces, that approach is part of the draw. Having worked with researchers on projects spanning multiple disciplines‚Äîmechanical engineering, electronics, materials science, biology‚Äîhe‚Äôs now preparing to spin out a company with one of Traverso‚Äôs postdocs.&nbsp;



‚ÄúI feel like I got the PhD experience just working here for four years and being involved in health projects,‚Äù he says. ‚ÄúThis has been an amazing opportunity to really see the first stages of company formation and how the early research really drives the commercialization of new technology.‚Äù



The lab‚Äôs specialized ‚Äúcores‚Äù ensure that projects have consistent support and can draw on plenty of expertise, regardless of how many students or postdocs come and go. If a challenge arises in an area in which a lab member has limited knowledge, chances are someone else in the lab has that background and will gladly help. ‚ÄúThe culture is so collaborative that everybody wants to teach everybody,‚Äù says Murphy.



Creating opportunities&nbsp;



In Traverso‚Äôs lab, members are empowered to pursue technically demanding research because the culture he created encourages them to stretch into new disciplines, take ownership of projects, and imagine where their work might go next. For some, that means cofounding a company. For others, it means leaving with the skills and network to shape their next big idea.



‚ÄúHe gives you both the agency and the support,‚Äù says Isaac Tucker, an L4TE postdoc based at the Broad Institute. ‚ÄúGio trusts the leads in his lab to just execute on tasks.‚Äù McRae adds that Traverso is adept at identifying ‚Äúpain points‚Äù in research and providing the necessary resources to remove barriers, which helps projects advance efficiently.&nbsp;



A project led by Kimberley Biggs, another L4TE postdoc, captures how the lab approaches high-stakes problems. Funded by the Gates Foundation, Biggs is developing a way to stabilize therapeutic bacteria used for neonatal and women‚Äôs health treatments so they remain effective without refrigeration‚Äîcritical for patients in areas without reliable temperature-controlled supply chains. A biochemist by training, she had never worked on devices before joining the lab, but she collaborated closely with the mechanical fabrication team to embed her bacterial therapy for conditions such as bacterial vaginosis and recurrent urinary tract infections into an intravaginal ring that can release it over time. She says Traverso gave her ‚Äúan incredible amount of trust‚Äù to lead the project from the start but continued to touch base often, making sure there were ‚Äúno significant bottlenecks‚Äù and that she was meeting all the goals she wanted to meet to progress in her career.



Traverso encourages collaboration by putting together project teams that combine engineers, physicians, and scientists from other fields‚Äîa strategy he says can be transformative.&nbsp;



‚ÄúIf you only have one expert, they are constrained to what they know,‚Äù he explains. But ‚Äúwhen you bring an electrical engineer together with a biologist or physician, the way that they‚Äôll be able to see the problem or the challenge is very different.‚Äù As a result, ‚Äúyou see things that perhaps you hadn‚Äôt even considered were possible,‚Äù he says. Moving a project from a concept to a successful clinical trial ‚Äútakes a village,‚Äù he adds. It‚Äôs a ‚Äúcomplex, multi-step, multi-person, multi-year‚Äù process involving ‚Äútens if not hundreds of millions of dollars‚Äô worth of effort.‚Äù



Good ideas deserve to be tested



The portion of Traverso‚Äôs lab housed at the ‚Äútough tech‚Äù incubator The Engine‚Äîand the only academic group working there‚Äîoccupies a 30-bench private lab alongside shared fabrication spaces, heavy machinery, and communal rooms of specialized lab equipment. The combination of dedicated and shared resources has helped reduce some initial equipment expenses for new projects, while the startup-dense environment puts potential collaborators, venture capital, and commercialization pathways within easy reach. Biggs‚Äôs work on bacterial treatments is one of the lab‚Äôs projects at The Engine. Others include work to develop electronics for capsule-based devices and an applicator for microneedle patches.



Traverso‚Äôs philosophy is to ‚Äúfail well and fail fast and move on.‚Äù



The end of one table houses ‚Äúblue sky‚Äù research on a topic of long-standing interest to Traverso: pasta. Led by PhD student Jack Chen, the multi-pronged project includes using generative AI to help design new pasta shapes with superior sauce adhesion. Chen and collaborators ranging from executive chefs to experts in fluid dynamics apply the same analytical rigor to this research that they bring to medical devices. It‚Äôs playful work, but it‚Äôs also a microcosm of the lab‚Äôs culture: interdisciplinary to its core, unafraid to cross boundaries, and grounded in Traverso‚Äôs belief that good ideas deserve to be tested‚Äîeven if they fail.



‚ÄúI‚Äôd say the majority of things that I‚Äôve ever been involved in failed,‚Äù he says. ‚ÄúBut I think it depends on how you define failure.‚Äù He says that most of the projects he worked on for the first year and a half of his own PhD either just ‚Äúkind of worked‚Äù or didn‚Äôt work at all‚Äîcausing him to step back and take a different approach that ultimately led him to develop the highly effective technique now used in the Cologuard test. ‚ÄúEven if a hypothesis that we had didn‚Äôt work out, or didn‚Äôt work out as we thought it might, the process itself, I think, is valuable,‚Äù he says. So his philosophy is to ‚Äúfail well and fail fast and move on.‚Äù



A tiny capsule that delivers a burst of medication directly into the GI tract offers an alternative to injections.JARED LEEDS




In practice, that means encouraging students and postdocs to take on big, uncertain problems, knowing a dead end isn‚Äôt the end of their careers‚Äîjust an opportunity to learn how to navigate the next challenge better.



McRae remembers when a major program‚Äîtwo or three years in the making‚Äîabruptly changed course after its sponsor shifted priorities. The team had been preparing a device for safety testing in humans; suddenly, the focus on that goal was gone. Rather than shelving the work, Traverso urged the group to use it as an opportunity to ‚Äúbe a little more creative again‚Äù and explore new directions, McRae says. That pivot sparked his work on an autonomous drug delivery system, opening lines of research the team hadn‚Äôt pursued before. In this system, patients swallow two capsules that interact in the stomach. When a sensor capsule detects an abnormal signal, it directs a second capsule to release a drug.




‚ÄúHe will often say, ‚ÄòI have a focus on not wasting time. Time is something that you can‚Äôt buy back. Time is something that you can‚Äôt save and bank for later.‚Äô‚Äù
Kimberley Biggs



‚ÄúWhen things aren‚Äôt working, just make sure they didn‚Äôt work and you‚Äôre confident why they didn‚Äôt work,‚Äù Traverso says he tells his students. ‚ÄúIs it the biology? Is it the materials science? Is it the mechanics that aren‚Äôt just aligning for whatever reason?‚Äù He models that diagnostic mindset‚Äîand the importance of preserving momentum.&nbsp;



‚ÄúHe will often say, ‚ÄòI have a focus on not wasting time. Time is something that you can‚Äôt buy back. Time is something that you can‚Äôt save and bank for later,‚Äô‚Äù says Biggs. ‚ÄúAnd so whenever you do encounter some sort of bottleneck, he is so supportive in trying to fix that.‚Äù&nbsp;



Traverso‚Äôs teaching reflects the same interplay between invention, risk, and real-world impact. In Translational Engineering, one of his graduate-level courses at MIT, he invites experts from the FDA, hospitals, and startups to speak about the realities of bringing medical technology to the world.



‚ÄúHe shared his network with us,‚Äù says Murphy, who took the course while working in the lab. ‚ÄúNow that I‚Äôm trying to spin out a company, I can reach out to these people.‚Äù&nbsp;



Although he now spends most of his time on research and teaching, Traverso maintains an inpatient practice at the Brigham, participating in the consult service‚Äîa team of gastroenterology fellows and medical students supervising patient care‚Äîfor several weeks a year. Staying connected to patients keeps the problems concrete and helps guide decisions on which puzzles to tackle in the lab.



‚ÄúI think there are certain puzzles in front of us, and I do gravitate to areas that have a solution that will help people in the near term,‚Äù he says.



For Traverso, the measure of success is not the complexity of the engineering but the efficacy of the result. The goal is always a therapy that works for the people who need it, wherever they are.&nbsp;







Designing devices for real-world care&nbsp;



A sampling of recent research from Traverso‚Äôs Lab for Translational Engineering



A mechanical adhesive device inspired by sucker fish sticks to soft, wet surfaces; it could be used to deliver drugs in the GI tract or to monitor aquatic environments.&nbsp;



A pill based on Traverso‚Äôs technology that can be taken once a week gradually releases medication within the stomach. It‚Äôs designed for patients with conditions like schizophrenia, hypertension, and asthma who find it difficult to take medicine every day.&nbsp;



A new delivery method for injectable drugs uses smaller needles and fewer shots. Drugs injected as a suspension of tiny crystals assemble into a ‚Äúdepot‚Äù under the skin that could last for months or years.&nbsp;



A protein from tiny tardigrades, also known as ‚Äúwater bears,‚Äù could protect healthy cells from radiation damage during cancer treatments, reducing severe side effects that many patients find too difficult to tolerate. Injecting messenger RNA encoding this protein into mice produced enough to protect healthy cells.



An inflatable gastric balloon could be enlarged before a meal to prevent overeating and help people lose weight.&nbsp;



Inspired by the way squid use jets to shoot ink clouds, a capsule releases a burst of drugs directly into the GI tract. It could offer an alternative to injecting drugs such as insulin, as well as vaccines and therapies to treat obesity and other metabolic disorders.



An implantable sensor could reverse opioid overdoses. Implanted under the skin, it rapidly releases naloxone when an overdose is detected.



A screening device for cervical cancer offers a clear line of sight to the cervix in a way that causes less discomfort than a traditional speculum. It‚Äôs affordable enough for use in low- and middle-income countries.

üîí Cybersecurity & Privacy
‚Ä¢ Email Bombs Exploit Lax Authentication in Zendesk
  Cybercriminals are abusing a widespread lack of authentication in the customer service platform Zendesk to flood targeted email inboxes with menacing messages . The Washington Post, Tinder, CapCom, CompTIA, Discord, GMAC, NordVPN and NordVPN are among the victims of the abuse . The abusive missives can include any subject line chosen by the abusers, such as a supposed law enforcement investigation involving KrebsOnSecurity.com .

üéì University AI
No updates.

üè¢ Corporate AI
‚Ä¢ Tell me when: Building agents that can wait, monitor, and act
  Modern&nbsp;LLM&nbsp;Agents&nbsp;can debug code, analyze spreadsheets, and book complex travel.&nbsp;Given those capabilities, it‚Äôs reasonable to assume that they could handle something simpler:&nbsp;waiting.&nbsp;Ask an agent to&nbsp;monitor&nbsp;your email for a colleague‚Äôs response or watch for a price drop over several days, and it will fail. Not because it&nbsp;can‚Äôt&nbsp;check email or scrape prices. It can do both. It fails&nbsp;because it&nbsp;doesn‚Äôt&nbsp;know&nbsp;when&nbsp;to check.&nbsp;Agents either&nbsp;give up after a few attempts or burn through their context window, checking obsessively. Neither&nbsp;work.&nbsp;



This matters because monitoring tasks&nbsp;are&nbsp;everywhere. We track emails for specific information, watch news&nbsp;feeds for updates, and&nbsp;monitor&nbsp;prices for sales. Automating these tasks would save hours, but current&nbsp;agents&nbsp;aren‚Äôt&nbsp;built for patience.



To address this, we are introducing&nbsp;SentinelStep (opens in new tab),&nbsp;a&nbsp;mechanism&nbsp;that&nbsp;enables&nbsp;agents&nbsp;to complete long-running monitoring&nbsp;tasks.&nbsp;The&nbsp;approach is simple.&nbsp;SentinelStep&nbsp;wraps the agent in a workflow with dynamic&nbsp;polling&nbsp;and&nbsp;careful context&nbsp;management.&nbsp;This&nbsp;enables&nbsp;the&nbsp;agent&nbsp;to&nbsp;monitor&nbsp;conditions for&nbsp;hours&nbsp;or&nbsp;days&nbsp;without getting&nbsp;sidetracked.&nbsp;We&#8217;ve&nbsp;implemented&nbsp;SentinelStep&nbsp;in&nbsp;Magentic-UI,&nbsp;our research&nbsp;prototype&nbsp;agentic system,&nbsp;to enable&nbsp;users&nbsp;to&nbsp;build agents for&nbsp;long-running&nbsp;tasks,&nbsp;whether they&nbsp;involve web&nbsp;browsing, coding, or external&nbsp;tools.&nbsp;



	
		

		
		PODCAST SERIES
	
	
	
						
				
					
				
			
			
			

									AI Testing and Evaluation: Learnings from Science and Industry
				
								Discover how Microsoft is learning from other domains to advance evaluation and testing as a pillar of AI governance.
				
								
					
						
							Listen now						
					
				
							
	
Opens in a new tab	
	


How it works



The core&nbsp;challenge is&nbsp;polling frequency. Poll too often,&nbsp;and&nbsp;tokens get&nbsp;wasted. Poll too infrequently, and the user‚Äôs notification gets delayed.&nbsp;SentinelStep&nbsp;makes&nbsp;an educated guess&nbsp;at&nbsp;the&nbsp;polling interval based on the task at hand‚Äîchecking email gets different treatment&nbsp;than&nbsp;monitoring&nbsp;quarterly earnings‚Äîthen dynamically adjusts&nbsp;based on&nbsp;observed&nbsp;behavior.&nbsp;



There‚Äôs&nbsp;a second challenge: context overflow.&nbsp;Because monitoring tasks can run for days,&nbsp;context overflow&nbsp;becomes inevitable.&nbsp;SentinelStep&nbsp;handles&nbsp;this by saving the agent state after the first check, then&nbsp;using&nbsp;that state for each subsequent check.





















These demonstrations capture&nbsp;Magentic-UI with&nbsp;SentinelStep&nbsp;at work, completing a range of tasks in a timelapse sequence.&nbsp;



Core components



As&nbsp;the name&nbsp;suggests,&nbsp;SentinelStep&nbsp;consists of&nbsp;individual steps&nbsp;taken as part of&nbsp;an&nbsp;agent‚Äôs broader&nbsp;workflow.&nbsp;As illustrated in Figure 1, there are three main components:&nbsp;the&nbsp;actions necessary to collect information, the condition that determines&nbsp;when&nbsp;the task&nbsp;is complete,&nbsp;and the polling interval&nbsp;that&nbsp;determines&nbsp;timing.&nbsp;Once&nbsp;these components&nbsp;are&nbsp;identified, the&nbsp;system‚Äôs&nbsp;behavior is simple:&nbsp;every&nbsp;[polling interval]&nbsp;do&nbsp;[actions]&nbsp;until&nbsp;[condition]&nbsp;is satisfied.&nbsp;



Figure&nbsp;1.&nbsp;SentinelSteps‚Äôs&nbsp;three main components&nbsp;in&nbsp;Magentic-UI‚Äôs&nbsp;co-planning interface.&nbsp;



These three components are defined and exposed in the co-planning interface of Magentic-UI. Given a user prompt, Magentic-UI proposes a complete multi-step plan, including pre-filled parameters for any monitoring steps. Users can accept the plan or adjust as needed.



Processing



Once a run starts, Magentic-UI assigns the most appropriate agent from a team of agents to perform each action. This team includes agents capable of web surfing, code execution, and calling arbitrary MCP servers.



When the workflow reaches a monitoring step, the flow is straightforward. The assigned agent collects the necessary information through the actions described in the plan. The Magentic-UI orchestrator then checks whether the condition is satisfied. If it is, the SentinelStep is complete, and the orchestrator moves to the next step. If not, the orchestrator determines the timestamp for the next check and resets the agent‚Äôs state to prevent context overflow.



Evaluation



Evaluating&nbsp;monitoring tasks in real-world settings&nbsp;is&nbsp;nearly impossible.&nbsp;Consider a simple example: monitoring the Magentic-UI repository on GitHub&nbsp;until&nbsp;it reaches&nbsp;10,000 stars&nbsp;(a measure of how many people have bookmarked it). That event occurs only once and can‚Äôt be repeated.&nbsp;Most&nbsp;real-world monitoring tasks share this limitation, making systematic bench marking very challenging.



In response, we&nbsp;are developing&nbsp;SentinelBench, a suite of synthetic&nbsp;web environments for evaluating monitoring tasks. These environments make experiments repeatable. SentinelBench&nbsp;currently&nbsp;supports&nbsp;28&nbsp;configurable scenarios, each&nbsp;allowing the user to schedule exactly when&nbsp;a&nbsp;target&nbsp;event&nbsp;should&nbsp;occur. It includes setups like GitHub Watcher, which&nbsp;simulates a repository accumulating stars over time;&nbsp;Teams Monitor, which models incoming messages, some&nbsp;urgent; and&nbsp;Flight Monitor, which&nbsp;replicates&nbsp;evolving&nbsp;flight-availability&nbsp;dynamics.&nbsp;



Initial&nbsp;tests&nbsp;show clear benefits.&nbsp;As shown in&nbsp;Figure&nbsp;2, success rates&nbsp;remain&nbsp;high for short tasks (30&nbsp;sec&nbsp;and 1&nbsp;min) regardless of&nbsp;whether&nbsp;SentinelStep&nbsp;is&nbsp;used.&nbsp;For longer tasks,&nbsp;SentinelStep&nbsp;markedly&nbsp;improves reliability: at 1 hour, task reliability rises from 5.6% without&nbsp;SentinelStep&nbsp;to&nbsp;33.3% with&nbsp;it;&nbsp;and at 2 hours,&nbsp;it rises&nbsp;from 5.6% to 38.9%. These gains&nbsp;demonstrate&nbsp;that&nbsp;SentinelStep&nbsp;effectively addresses the challenge of maintaining performance over extended durations.



Figure&nbsp;2.&nbsp;SentinelStep&nbsp;improves&nbsp;success rates&nbsp;on longer running tasks (1‚Äì2&nbsp;hours)&nbsp;while&nbsp;maintaining&nbsp;comparable performance&nbsp;on shorter tasks.&nbsp;&nbsp;



Impact and availability



SentinelStep is a first step toward practical, proactive, longer‚Äërunning agents. By embedding patience into plans, agents can responsibly monitor conditions and act when it matters‚Äîstaying proactive without wasting resources. This lays the groundwork for always‚Äëon assistants that stay efficient, respectful of limits, and aligned with user intent.



We‚Äôve open-sourced SentinelStep as part of Magentic-UI, available on GitHub (opens in new tab) or via pip install magnetic-ui. As with any new technique, production deployment should be preceded through&nbsp;testing and validation&nbsp;for the specific use case.&nbsp;For&nbsp;guidance on&nbsp;intended use,&nbsp;privacy&nbsp;considerations,&nbsp;and safety&nbsp;guidelines,&nbsp;see&nbsp;the&nbsp;Magentic-UI&nbsp;Transparency&nbsp;Note. (opens in new tab)&nbsp;



Our goal is to&nbsp;make it easier to implement agents that can&nbsp;handle&nbsp;long-running&nbsp;monitoring&nbsp;tasks&nbsp;and&nbsp;lay&nbsp;the groundwork for&nbsp;systems that&nbsp;anticipate, adapt, and&nbsp;evolve&nbsp;to meet real-world needs.&nbsp;
Opens in a new tabThe post Tell me when: Building agents that can wait, monitor, and act appeared first on Microsoft Research.
‚Ä¢ Serverless deployment for your Amazon SageMaker Canvas models
  Deploying machine learning (ML) models into production can often be a complex and resource-intensive task, especially for customers without deep ML and DevOps expertise. Amazon SageMaker Canvas simplifies model building by offering a no-code interface, so you can create highly accurate ML models using your existing data sources and without writing a single line of code. But building a model is only half the journey; deploying it efficiently and cost-effectively is just as crucial. Amazon SageMaker Serverless Inference is designed for workloads with variable traffic patterns and idle periods. It automatically provisions and scales infrastructure based on demand, alleviating the need to manage servers or pre-configure capacity. 
In this post, we walk through how to take an ML model built in SageMaker Canvas and deploy it using SageMaker Serverless Inference. This solution can help you go from model creation to production-ready predictions quickly, efficiently, and without managing any infrastructure. 
Solution overview 
To demonstrate serverless endpoint creation for a SageMaker Canvas trained model, let‚Äôs explore an example workflow: 
 
 Add the trained model to the Amazon SageMaker Model Registry. 
 Create a new SageMaker model with the correct configuration. 
 Create a serverless endpoint configuration. 
 Deploy the serverless endpoint with the created model and endpoint configuration. 
 
You can also automate the process, as illustrated in the following diagram. 
 
In this example, we deploy a pre-trained regression model to a serverless SageMaker endpoint. This way, we can use our model for variable workloads that don‚Äôt require real-time inference. 
Prerequisites 
As a prerequisite, you must have access to Amazon Simple Storage Service (Amazon S3) and Amazon SageMaker AI. If you don‚Äôt already have a SageMaker AI domain configured in your account, you also need permissions to create a SageMaker AI domain. 
You must also have a regression or classification model that you have trained. You can train your SageMaker Canvas model as you normally would. This includes creating the Amazon SageMaker Data Wrangler flow, performing necessary data transformations, and choosing the model training configuration. If you don‚Äôt already have a trained model, you can follow one of the labs in the Amazon SageMaker Canvas Immersion Day to create one before continuing. For this example, we use a classification model that was trained on the canvas-sample-shipping-logs.csv sample dataset. 
Save your model to the SageMaker Model Registry 
Complete the following steps to save your model to the SageMaker Model Registry: 
 
 On the SageMaker AI console, choose Studio to launch Amazon SageMaker Studio. 
 In the SageMaker Studio interface, launch SageMaker Canvas, which will open in a new tab. 
 
 
 
 Locate the model and model version that you want to deploy to your serverless endpoint. 
 On the options menu (three vertical dots), choose Add to Model Registry. 
 
 
You can now exit SageMaker Canvas by logging out. To manage costs and prevent additional workspace charges, you can also configure SageMaker Canvas to automatically shut down when idle. 
Approve your model for deployment 
After you have added your model to the Model Registry, complete the following steps: 
 
 In the SageMaker Studio UI, choose Models in the navigation pane. 
 
The model you just exported from SageMaker Canvas should be added with a deployment status of Pending manual approval. 
 
 Choose the model version you want to deploy and update the status to Approved by choosing the deployment status. 
 
 
 
 Choose the model version and navigate to the Deploy tab. This is where you will find the information related to the model and associated container. 
 Select the container and model location related to the trained model. You can identify it by checking the presence of the environment variable SAGEMAKER_DEFAULT_INVOCATIONS_ACCEPT. 
 
 
Create a new model 
Complete the following steps to create a new model: 
 
 Without closing the SageMaker Studio tab, open a new tab and open the SageMaker AI console. 
 Choose Models in the Inference section and choose Create model. 
 Name your model. 
 Leave the container input option as Provide model artifacts and inference image location and used the CompressedModel type. 
 Enter the Amazon Elastic Container Registry (Amazon ECR) URI, Amazon S3 URI, and environment variables that you located in the previous step. 
 
The environment variables will be shown as a single line in SageMaker Studio, with the following format: 
 
 SAGEMAKER_DEFAULT_INVOCATIONS_ACCEPT: text/csv, SAGEMAKER_INFERENCE_OUTPUT: predicted_label, SAGEMAKER_INFERENCE_SUPPORTED: predicted_label, SAGEMAKER_PROGRAM: tabular_serve.py, SAGEMAKER_SUBMIT_DIRECTORY: /opt/ml/model/code 
 
You might have different variables than those in the preceding example. All variables from your environment variables should be added to your model. Make sure that each environment variable is on its own line when creating you new model. 
 
 
 Choose Create model. 
 
Create an endpoint configuration 
Complete the following steps to create an endpoint configuration: 
 
 On the SageMaker AI console, choose Endpoint configurations to create a new model endpoint configuration. 
 Set the type of endpoint to Serverless and set the model variant to the model created in the previous step. 
 
 
 
 Choose Create endpoint configuration. 
 
Create an endpoint 
Complete the following steps to create an endpoint: 
 
 On the SageMaker AI console, choose Endpoints in the navigation pane and create a new endpoint. 
 Name the endpoint. 
 Select the endpoint configuration created in the previous step and choose Select endpoint configuration. 
 Choose Create endpoint. 
 
 
The endpoint might take a few minutes to be created. When the status is updated to InService, you can begin calling the endpoint. 
The following sample code demonstrates how you can call an endpoint from a Jupyter notebook located in your SageMaker Studio environment: 
 
 import boto3
import csv
from io import StringIO
import time

def invoke_shipping_prediction(features):
    sagemaker_client = boto3.client('sagemaker-runtime')
    
    # Convert to CSV string format
    output = StringIO()
    csv.writer(output).writerow(features)
    payload = output.getvalue()
    
    response = sagemaker_client.invoke_endpoint(
        EndpointName='canvas-shipping-data-model-1-serverless-endpoint',
        ContentType='text/csv',
        Accept='text/csv',
        Body=payload
    )
    
    response_body = response['Body'].read().decode()
    reader = csv.reader(StringIO(response_body))
    result = list(reader)[0]  # Get first row
    
    # Parse the response into a more usable format
    prediction = {
        'predicted_label': result[0],
        'confidence': float(result[1]),
        'class_probabilities': eval(result[2]),  
        'possible_labels': eval(result[3])       
    }
    
    return prediction

# Features for inference
features_set_1 = [
    "Bell",
    "Base",
    14,
    6,
    11,
    11,
    "GlobalFreight",
    "Bulk Order",
    "Atlanta",
    "2020-09-11 00:00:00",
    "Express",
    109.25199890136719
]

features_set_2 = [
    "Bell",
    "Base",
    14,
    6,
    15,
    15,
    "MicroCarrier",
    "Single Order",
    "Seattle",
    "2021-06-22 00:00:00",
    "Standard",
    155.0483856201172
]

# Invoke the SageMaker endpoint for feature set 1
start_time = time.time()
result = invoke_shipping_prediction(features_set_1)

# Print Output and Timing
end_time = time.time()
total_time = end_time - start_time

print(f"Total response time with endpoint cold start: {total_time:.3f} seconds")
print(f"Prediction for feature set 1: {result['predicted_label']}")
print(f"Confidence for feature set 1: {result['confidence']*100:.2f}%")
print("\nProbabilities for feature set 1:")
for label, prob in zip(result['possible_labels'], result['class_probabilities']):
    print(f"{label}: {prob*100:.2f}%")


print("---------------------------------------------------------")

# Invoke the SageMaker endpoint for feature set 2
start_time = time.time()
result = invoke_shipping_prediction(features_set_2)

# Print Output and Timing
end_time = time.time()
total_time = end_time - start_time

print(f"Total response time with warm endpoint: {total_time:.3f} seconds")
print(f"Prediction for feature set 2: {result['predicted_label']}")
print(f"Confidence for feature set 2: {result['confidence']*100:.2f}%")
print("\nProbabilities for feature set 2:")
for label, prob in zip(result['possible_labels'], result['class_probabilities']):
    print(f"{label}: {prob*100:.2f}%")
 
 
Automate the process 
To automatically create serverless endpoints each time a new model is approved, you can use the following YAML file with AWS CloudFormation. This file will automate the creation of SageMaker endpoints with the configuration you specify. 
This sample CloudFormation template is provided solely for inspirational purposes and is not intended for direct production use. Developers should thoroughly test this template according to their organization‚Äôs security guidelines before deployment. 
 
 AWSTemplateFormatVersion: "2010-09-09"
Description: Template for creating Lambda function to handle SageMaker model
  package state changes and create serverless endpoints

Parameters:
  MemorySizeInMB:
    Type: Number
    Default: 1024
    Description: Memory size in MB for the serverless endpoint (between 1024 and 6144)
    MinValue: 1024
    MaxValue: 6144

  MaxConcurrency:
    Type: Number
    Default: 20
    Description: Maximum number of concurrent invocations for the serverless endpoint
    MinValue: 1
    MaxValue: 200

  AllowedRegion:
    Type: String
    Default: "us-east-1"
    Description: AWS region where SageMaker resources can be created

  AllowedDomainId:
    Type: String
    Description: SageMaker Studio domain ID that can trigger deployments
    NoEcho: true

  AllowedDomainIdParameterName:
    Type: String
    Default: "/sagemaker/serverless-deployment/allowed-domain-id"
    Description: SSM Parameter name containing the SageMaker Studio domain ID that can trigger deployments

Resources:
  AllowedDomainIdParameter:
    Type: AWS::SSM::Parameter
    Properties:
      Name: !Ref AllowedDomainIdParameterName
      Type: String
      Value: !Ref AllowedDomainId
      Description: SageMaker Studio domain ID that can trigger deployments

  SageMakerAccessPolicy:
    Type: AWS::IAM::ManagedPolicy
    Properties:
      Description: Managed policy for SageMaker serverless endpoint creation
      PolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Action:
              - sagemaker:CreateModel
              - sagemaker:CreateEndpointConfig
              - sagemaker:CreateEndpoint
              - sagemaker:DescribeModel
              - sagemaker:DescribeEndpointConfig
              - sagemaker:DescribeEndpoint
              - sagemaker:DeleteModel
              - sagemaker:DeleteEndpointConfig
              - sagemaker:DeleteEndpoint
            Resource: !Sub "arn:aws:sagemaker:${AllowedRegion}:${AWS::AccountId}:*"
          - Effect: Allow
            Action:
              - sagemaker:DescribeModelPackage
            Resource: !Sub "arn:aws:sagemaker:${AllowedRegion}:${AWS::AccountId}:model-package/*/*"
          - Effect: Allow
            Action:
              - iam:PassRole
            Resource: !Sub "arn:aws:iam::${AWS::AccountId}:role/service-role/AmazonSageMaker-ExecutionRole-*"
            Condition:
              StringEquals:
                "iam:PassedToService": "sagemaker.amazonaws.com"
          - Effect: Allow
            Action:
              - ssm:GetParameter
            Resource: !Sub "arn:aws:ssm:${AllowedRegion}:${AWS::AccountId}:parameter${AllowedDomainIdParameterName}"

  LambdaExecutionRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
        - !Ref SageMakerAccessPolicy

  ModelDeploymentFunction:
    Type: AWS::Lambda::Function
    Properties:
      Handler: index.handler
      Role: !GetAtt LambdaExecutionRole.Arn
      Code:
        ZipFile: |
          import os
          import json
          import boto3

          sagemaker_client = boto3.client('sagemaker')
          ssm_client = boto3.client('ssm')

          def handler(event, context):
              print(f"Received event: {json.dumps(event, indent=2)}")
              try:
                  # Get details directly from the event
                  detail = event['detail']
                  print(f'detail: {detail}')
                  
                  # Get allowed domain ID from SSM Parameter Store
                  parameter_name = os.environ.get('ALLOWED_DOMAIN_ID_PARAMETER_NAME')
                  try:
                      response = ssm_client.get_parameter(Name=parameter_name)
                      allowed_domain = response['Parameter']['Value']
                  except Exception as e:
                      print(f"Error retrieving parameter {parameter_name}: {str(e)}")
                      allowed_domain = '*'  # Default fallback
                  
                  # Check if domain ID is allowed
                  if allowed_domain != '*':
                      created_by_domain = detail.get('CreatedBy', {}).get('DomainId')
                      if created_by_domain != allowed_domain:
                          print(f"Domain {created_by_domain} not allowed. Allowed: {allowed_domain}")
                          return {'statusCode': 403, 'body': 'Domain not authorized'}

                  # Get the model package ARN from the event resources
                  model_package_arn = event['resources'][0]

                  # Get the model package details from SageMaker
                  model_package_response = sagemaker_client.describe_model_package(
                      ModelPackageName=model_package_arn
                  )

                  # Parse model name and version from ModelPackageName
                  model_name, version = detail['ModelPackageName'].split('/')
                  serverless_model_name = f"{model_name}-{version}-serverless"

                  # Get all container details directly from the event
                  container_defs = detail['InferenceSpecification']['Containers']

                  # Get the execution role from the event and convert to proper IAM role ARN format
                  assumed_role_arn = detail['CreatedBy']['IamIdentity']['Arn']
                  execution_role_arn = assumed_role_arn.replace(':sts:', ':iam:')\
                                                   .replace('assumed-role', 'role/service-role')\
                                                   .rsplit('/', 1)[0]

                  # Prepare containers configuration for the model
                  containers = []
                  for i, container_def in enumerate(container_defs):
                      # Get environment variables from the model package for this container
                      environment_vars = model_package_response['InferenceSpecification']['Containers'][i].get('Environment', {}) or {}
                      
                      containers.append({
                          'Image': container_def['Image'],
                          'ModelDataUrl': container_def['ModelDataUrl'],
                          'Environment': environment_vars
                      })

                  # Create model with all containers
                  if len(containers) == 1:
                      # Use PrimaryContainer if there's only one container
                      create_model_response = sagemaker_client.create_model(
                          ModelName=serverless_model_name,
                          PrimaryContainer=containers[0],
                          ExecutionRoleArn=execution_role_arn
                      )
                  else:
                      # Use Containers parameter for multiple containers
                      create_model_response = sagemaker_client.create_model(
                          ModelName=serverless_model_name,
                          Containers=containers,
                          ExecutionRoleArn=execution_role_arn
                      )

                  # Create endpoint config
                  endpoint_config_name = f"{serverless_model_name}-config"
                  create_endpoint_config_response = sagemaker_client.create_endpoint_config(
                      EndpointConfigName=endpoint_config_name,
                      ProductionVariants=[{
                          'VariantName': 'AllTraffic',
                          'ModelName': serverless_model_name,
                          'ServerlessConfig': {
                              'MemorySizeInMB': int(os.environ.get('MEMORY_SIZE_IN_MB')),
                              'MaxConcurrency': int(os.environ.get('MAX_CONCURRENT_INVOCATIONS'))
                          }
                      }]
                  )

                  # Create endpoint
                  endpoint_name = f"{serverless_model_name}-endpoint"
                  create_endpoint_response = sagemaker_client.create_endpoint(
                      EndpointName=endpoint_name,
                      EndpointConfigName=endpoint_config_name
                  )

                  return {
                      'statusCode': 200,
                      'body': json.dumps({
                          'message': 'Serverless endpoint deployment initiated',
                          'endpointName': endpoint_name
                      })
                  }

              except Exception as e:
                  print(f"Error: {str(e)}")
                  raise
      Runtime: python3.12
      Timeout: 300
      MemorySize: 128
      Environment:
        Variables:
          MEMORY_SIZE_IN_MB: !Ref MemorySizeInMB
          MAX_CONCURRENT_INVOCATIONS: !Ref MaxConcurrency
          ALLOWED_DOMAIN_ID_PARAMETER_NAME: !Ref AllowedDomainIdParameterName

  EventRule:
    Type: AWS::Events::Rule
    Properties:
      Description: Rule to trigger Lambda when SageMaker Model Package state changes
      EventPattern:
        source:
          - aws.sagemaker
        detail-type:
          - SageMaker Model Package State Change
        detail:
          ModelApprovalStatus:
            - Approved
          UpdatedModelPackageFields:
            - ModelApprovalStatus
      State: ENABLED
      Targets:
        - Arn: !GetAtt ModelDeploymentFunction.Arn
          Id: ModelDeploymentFunction

  LambdaInvokePermission:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !Ref ModelDeploymentFunction
      Action: lambda:InvokeFunction
      Principal: events.amazonaws.com
      SourceArn: !GetAtt EventRule.Arn

Outputs:
  LambdaFunctionArn:
    Description: ARN of the Lambda function
    Value: !GetAtt ModelDeploymentFunction.Arn
  EventRuleArn:
    Description: ARN of the EventBridge rule
    Value: !GetAtt EventRule.Arn
 
 
This stack will limit automated serverless endpoint creation to a specific AWS Region and domain. You can find your domain ID when accessing SageMaker Studio from the SageMaker AI console, or by running the following command: aws sagemaker list-domains ‚Äîregion [your-region] 
Clean up 
To manage costs and prevent additional workspace charges, make sure that you have logged out of SageMaker Canvas. If you tested your endpoint using a Jupyter notebook, you can shut down your JupyterLab instance by choosing Stop or configuring automated shutdown for JupyterLab. 
 
In this post, we showed how to deploy a SageMaker Canvas model to a serverless endpoint using SageMaker Serverless Inference. By using this serverless approach, you can quickly and efficiently serve predictions from your SageMaker Canvas models without needing to manage the underlying infrastructure. 
This seamless deployment experience is just one example of how AWS services like SageMaker Canvas and SageMaker Serverless Inference simplify the ML journey, helping businesses of different sizes and technical proficiencies unlock the value of AI and ML. As you continue exploring the SageMaker ecosystem, be sure to check out how you can unlock data governance for no-code ML with Amazon DataZone, and seamlessly transition between no-code and code-first model development using SageMaker Canvas and SageMaker Studio. 
 
About the authors 
 Nadhya Polanco&nbsp;is a Solutions Architect at AWS based in Brussels, Belgium. In this role, she supports organizations looking to incorporate AI and Machine Learning into their workloads. In her free time, Nadhya enjoys indulging in her passion for coffee and traveling. 
 Brajendra Singh&nbsp;is a Principal Solutions Architect at Amazon Web Services, where he partners with enterprise customers to design and implement innovative solutions. With a strong background in software development, he brings deep expertise in Data Analytics, Machine Learning, and Generative AI.
‚Ä¢ Building a multi-agent voice assistant with Amazon Nova Sonic and Amazon Bedrock AgentCore
  Amazon Nova Sonic is a foundation model that creates natural, human-like speech-to-speech conversations for generative AI applications, allowing users to interact with AI through voice in real-time, with capabilities for understanding tone, enabling natural flow, and performing actions. 
Multi-agent architecture offers a modular, robust, and scalable design pattern for production-level voice assistants. This blog post explores Amazon Nova Sonic voice agent applications and demonstrates how they integrate with Strands Agents framework sub-agents while leveraging Amazon Bedrock AgentCore to create an effective multi-agent system. 
Why multi-agent architecture? 
Imagine developing a financial assistant application responsible for user onboarding, information collection, identity verification, account inquiries, exception handling, and handing off to human agents based on predefined conditions. As functional requirements expand, the voice agent continues to add new inquiry types. The system prompt grows enormous, and the underlying logic becomes increasingly complex, illustrates a persistent challenge in software development: monolithic designs lead to systems that are difficult to maintain and enhance. 
Think of multi-agent architecture as building a team of specialized AI assistants rather than relying on a single do-it-all helper. Just like companies divide responsibilities across different departments, this approach breaks complex tasks into smaller, manageable pieces. Each AI agent becomes an expert in a specific area‚Äîwhether that‚Äôs fact-checking, data processing, or handling specialized requests. For the user, the experience feels seamless: there‚Äôs no delay, no change in voice, and no visible handoff. The system functions behind the scenes, directing each expert agent to step in at the right moment. 
In addition to modular and robust benefits, multi-agent systems offer advantages similar to a microservice architecture, a popular enterprise software design pattern, providing scalability, distribution and&nbsp;maintainability while allowing organizations to reuse agentic workflows already developed for their large language model (LLM)-powered applications. 
Sample application 
In this blog, we refer to the Amazon Nova Sonic workshop multi-agent lab code, which uses the banking voice assistant as a sample to demonstrate how to deploy specialized agents on&nbsp;Amazon Bedrock AgentCore. It uses Nova Sonic as&nbsp;the voice interface layer and acts as an orchestrator to delegate detailed inquiries to sub-agents written in&nbsp;Strands Agents hosted on AgentCore Runtime. You can find the&nbsp;sample source code&nbsp;on the GitHub repo. 
In the banking voice agent sample, the conversation flow begins with a greeting and collecting the user‚Äôs name, and then it handles inquiries related to banking or mortgages. We use three secondary level agents hosted on AgentCore to handle specialized logic: 
 
 Authenticate sub-agent: Handles user authentication using the account ID and other information 
 Banking sub-agent: Handles account balance checks, statements, and other banking-related inquiries 
 Mortgage sub-agent: Handles mortgage-related inquiries, including refinancing, rates, and repayment options 
 
 
Sub-agents are self-contained, handling their own logic such as input validation. For instance, the authentication agent validates account IDs and returns errors to Nova Sonic if needed. This simplifies the reasoning logic in Nova Sonic while keeping business logic encapsulated, similar to the software engineering modular design patterns. 
Integrate Nova Sonic with AgentCore through tool use events 
Amazon Nova Sonic relies on tool use to integrate with agentic workflows. During the Nova Sonic event lifecycle, you can provide tool use configurations through the promptStart event, which is designed to initiate when Sonic receives specific types of input. 
For example, in the following Sonic tool configuration sample, tool use is configured to initiate events based on Sonic‚Äôs built-in reasoning model, which classifies the inquiry for routing to the banking sub-agents. 
 
 [
&nbsp;&nbsp; &nbsp;{
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"toolSpec": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"name": "bankAgent",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"description": `Use this tool whenever the customer asks about their **bank account balance** or **bank statement**. &nbsp;
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;It should be triggered for queries such as: &nbsp;
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;- "What‚Äôs my balance?" &nbsp;
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;- "How much money do I have in my account?" &nbsp;
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;- "Can I see my latest bank statement?" &nbsp;
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;- "Show me my account summary."`,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"inputSchema": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"json": JSON.stringify({
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"type": "object",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"properties": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"accountId": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"type": "string",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"description": "This is a user input. It is the bank account Id which is a numeric number."
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;},
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"query": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"type": "string",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"description": "The inquiry to the bank agent such as check account balance, get statement etc."
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;},
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"required": [
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"accountId", "query"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;]
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;})
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp;}
] 
 
When a user asks Nova Sonic a question such as ‚ÄòWhat is my account balance?‚Äô, Sonic sends a toolUse event to the client application with the specified toolName (for example, bankAgent) defined in the configuration. The application can then invoke the sub-agent hosted on AgentCore to handle the banking logic and return the response to Sonic, which in turn generates an audio reply for the user. 
 
 {
  "event": {
    "toolUse": {
      "completionId": "UUID",
      "content": "{\"accountId\":\"one two three four five\",\"query\":\"check account balance\"}",
      "contentId": "UUID",
      "promptName": "UUID",
      "role": "TOOL",
      "sessionId": "UUID",
      "toolName": "bankAgent",
      "toolUseId": "UUID"
    }
  }
} 
 
Sub-agent on AgentCore 
The following sample showcases the banking sub-agent developed using the Strands Agents framework, specifically configured for deployment on Bedrock AgentCore. It leverages Nova Lite through Amazon Bedrock as its reasoning model, providing effective cognitive capabilities with minimal latency. The agent implementation features a system prompt that defines its banking assistant responsibilities, complemented by two specialized tools: one for account balance inquiries and another for bank statement retrieval. 
 
 from strands import Agent, tool
import json
from bedrock_agentcore.runtime import BedrockAgentCoreApp
from strands.models import BedrockModel
import re, argparse

app = BedrockAgentCoreApp()

@tool
def get_account_balance(account_id) -&gt; str:
&nbsp;&nbsp; &nbsp;"""Get account balance for given account Id

&nbsp;&nbsp; &nbsp;Args:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;account_id: Bank account Id
&nbsp;&nbsp; &nbsp;"""

&nbsp;&nbsp; &nbsp;# The actual implementation will retrieve information from a database API or another backend service.
&nbsp;&nbsp; &nbsp;
&nbsp;&nbsp; &nbsp;return {"result": result}

@tool
def get_statement(account_id: str, year_and_month: str) -&gt; str:
&nbsp;&nbsp; &nbsp;"""Get account statement for a given year and month
&nbsp;&nbsp; &nbsp;Args:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;account_id: Bank account Id
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;year_and_month: Year and month of the bank statement. For example: 2025_08 or August 2025
&nbsp;&nbsp; &nbsp;"""
&nbsp;&nbsp; &nbsp;# The actual implementation will retrieve information from a database API or another backend service.
&nbsp;&nbsp; &nbsp;
&nbsp;&nbsp; &nbsp;return {"result": result}


# Specify Bedrock LLM for the Agent
bedrock_model = BedrockModel(
&nbsp;&nbsp; &nbsp;model_id="amazon.nova-lite-v1:0",
)
# System prompt
system_prompt = '''
You are a banking agent. You will receive requests that include: &nbsp;
- `account_id` &nbsp;
- `query` (the inquiry type, such as **balance** or **statement**, plus any additional details like month). &nbsp;

## Instructions
1. Use the provided `account_id` and `query` to call the tools. &nbsp;
2. The tool will return a JSON response. &nbsp;
3. Summarize the result in 2‚Äì3 sentences. &nbsp;
&nbsp;&nbsp; - For a **balance inquiry**, give the account balance with currency and date. &nbsp;
&nbsp;&nbsp; - For a **statement inquiry**, provide opening balance, closing balance, and number of transactions. &nbsp;
4. Do not return raw JSON. Always respond in natural language.&nbsp;&nbsp;
'''

# Create an agent with tools, LLM, and system prompt
agent = Agent(
&nbsp;&nbsp; &nbsp;tools=[ get_account_balance, get_statement], 
&nbsp;&nbsp; &nbsp;model=bedrock_model,
&nbsp;&nbsp; &nbsp;system_prompt=system_prompt
)

@app.entrypoint
def banking_agent(payload):
&nbsp;&nbsp; &nbsp;response = agent(json.dumps(payload))
&nbsp; &nbsp; return response.message['content'][0]['text']
&nbsp;&nbsp; &nbsp;
if __name__ == "__main__":
&nbsp;&nbsp; &nbsp;app.run() 
 
Best practices for voice-based multi-agent systems 
Multi-agent architecture provides exceptional flexibility and a modular design approach, allowing developers to structure voice assistants efficiently and potentially reuse existing specialized agent workflows. When implementing voice-first experiences, there are important best practices to consider that address the unique challenges of this modality. 
 
 Balance flexibility and latency:&nbsp;Although the ability to invoke sub-agents using Nova Sonic tool use events creates powerful capabilities, it can introduce additional latency to voice responses. For the use cases that require a synchronized experience, each agent handoff represents a potential delay point in the interaction flow. Therefore, it‚Äôs important to design with response time in mind. 
 Optimize model selection for sub-agents: Starting with smaller, more efficient models like Nova Lite for sub-agents can significantly reduce latency while still handling specialized tasks effectively. Reserve larger, more capable models for complex reasoning or when sophisticated natural language understanding is essential. 
 Craft voice-optimized responses: Voice assistants perform best with concise, focused responses that can be followed by additional details when needed. This approach not only improves latency but also creates a more natural conversational flow that aligns with human expectations for verbal communication. 
 
Consider stateless vs. stateful sub-agent design 
Stateless sub-agents handle each request independently, without retaining memory of past interactions or session-level states. They are simple to implement, easy to scale, and work well for straightforward, one-off tasks. However, they cannot provide context-aware responses unless external state management is introduced. 
Stateful sub-agents, on the other hand, maintain memory across interactions to support context-aware responses and session-level states. This enables more personalized and cohesive user experiences, but comes with added complexity and resource requirements. They are best suited for scenarios involving multi-turn interactions and user or session-level context caching. 
Conclusion 
Multi-agent architectures unlock flexibility, scalability, and accuracy for complex AI-driven workflows. By combining the Nova Sonic conversational capabilities with the orchestration power of Bedrock AgentCore, you can build intelligent, specialized agents that work together seamlessly. If you‚Äôre exploring ways to enhance your AI applications, multi-agent patterns with Nova Sonic and AgentCore are a powerful approach worth testing. 
Learn more about Amazon Nova Sonic by visiting the User Guide, building your application with the sample applications, and exploring the Nova Sonic workshop to get started. You can also refer to the technical report and model card for additional benchmarks. 
 
About the authors 
 Lana Zhang is a Senior Specialist Solutions Architect for Generative AI at AWS within the Worldwide Specialist Organization. She specializes in AI/ML, with a focus on use cases such as AI voice assistants and multimodal understanding. She works closely with customers across diverse industries, including media and entertainment, gaming, sports, advertising, financial services, and healthcare, to help them transform their business solutions through AI.
‚Ä¢ Accelerate large-scale AI training with Amazon SageMaker HyperPod training operator
  Large-scale AI model training faces significant challenges with failure recovery and monitoring. Traditional training requires complete job restarts when even a single training process fails, resulting in additional downtime and increased costs. As training clusters expand, identifying and resolving critical issues like stalled GPUs and numerical instabilities typically requires complex custom monitoring code. 
With&nbsp;Amazon SageMaker HyperPod&nbsp;you can accelerate AI model development across hundreds or thousands of GPUs with built-in resiliency, decreasing model training time by up to 40%. The Amazon SageMaker HyperPod training operator further enhances training resilience for Kubernetes workloads through&nbsp;pinpoint recovery and customizable monitoring capabilities. 
In this blog post, we show you how to deploy and manage machine learning training workloads using the Amazon SageMaker HyperPod training operator, including setup instructions and a complete training example. 
Amazon SageMaker HyperPod training operator 
The Amazon SageMaker HyperPod training operator helps you accelerate generative AI model development by efficiently managing distributed training across large GPU clusters. The Amazon SageMaker HyperPod training operator uses built-in fault resiliency components, comes packaged as an Amazon Elastic Kubernetes Service (Amazon EKS) add-on, and deploys the necessary custom resource definitions (CRDs) to the HyperPod cluster. 
Solution overview 
The following diagram depicts the architecture of Amazon SageMaker HyperPod training operator. 
 
The HyperPod training operator follows Kubernetes operator pattern&nbsp;and&nbsp;has the following major components: 
 
 Custom Resource Definition (CRDs): HyperPodPyTorchJob defines the job specification (for example, node count, image) and serves as the interface for customers to submit jobs. apiVersion: sagemaker.amazonaws.com/v1 kind: HyperPodPyTorchJob 
 RBAC policies: Defines the actions the controller is allowed to perform, such as creating pods and managing HyperPodPyTorchJob resources. 
 Job controller: Listens to job creation and fulfills requests by creating job pods and pod managers. 
 Pod manager: Monitors training process health on each pod. The number of Pod Managers is determined by the number of pods required by the job. One Pod Manager currently controls several hundred pods. 
 HyperPod elastic agent: Customers install the elastic agent into their training container. It orchestrates lifecycles of training workers on each container and communicates with the Amazon SageMaker&nbsp;HyperPod training operator. The&nbsp;HyperPod elastic agent is an extension of PyTorch‚Äôs ElasticAgent. 
 
The job Controller uses fault detection components such as the SageMaker HyperPod health-monitoring agent and node health check mechanisms like AWS retirement notices to update job state and repair faults. It also relies on the HyperPod elastic agent to check the status of training processes for crashes and hung job detection. 
When a HyperPodPyTorch job is submitted, the Amazon SageMaker&nbsp;HyperPod training operator spins up job pods along with pod manager pods that help manage the training job lifecycle. The pod managers interact with the HyperPod elastic agent so that all job pods maintain a healthy state. 
Benefits of using the operator 
The Amazon SageMaker HyperPod training operator can be installed as an EKS add-on on your cluster. The key benefits include: 
 
 Centralized training process monitoring and restart ‚Äì&nbsp;The HyperPod training operator maintains a control plane with a global view of health across all ranks. When one rank encounters an issue, it broadcasts a stop signal to all ranks to prevent other ranks from failing individually at different times due to collective communication timeout. This supports more efficient fault detection and recovery. 
 Centralized efficient rank assignment ‚Äì&nbsp;A separate HyperPod rendezvous backend allows the HyperPod training operator to assign ranks directly. This reduces initialization overhead by eliminating the need for worker-to-worker discovery. 
 Unhealthy training node detection and job restart ‚Äì&nbsp;The HyperPod training operator is fully integrated with the HyperPod EKS cluster resiliency features, helping restart jobs or training processes due to bad nodes and hardware issues in ML workloads. This reduces the need to self-manage job recovery solutions. 
 Granular process recovery&nbsp;‚Äì&nbsp;Rather than restarting entire jobs when failures occur, the operator precisely targets and restarts only training processes, reducing recovery times from tens of minutes to seconds. This makes&nbsp;HyperPod training operator job recovery time scale linearly as cluster size grows. 
 Hanging job detection and performance degradation detection&nbsp;‚Äì&nbsp;Based on training script log monitoring, the&nbsp;HyperPod training operator helps overcome problematic training scenarios including stalled training batches, non-numeric loss values, and performance degradation through simple YAML configurations. For more information see, Using the training operator to run jobs in the Amazon SageMaker AI Developer Guide. 
 
Training operator setup 
This section walks through installing the Amazon SageMaker HyperPod training operator as an Amazon EKS add-on. 
Estimated Setup Time: 30-45 minutes 
Prerequisites 
Before getting started, verify that you have the following resources and permissions. 
Required AWS resources: 
 
 Active AWS account 
 Amazon EKS cluster (version 1.28 or later) 
 Amazon SageMaker HyperPod EKS cluster 
 Amazon ECR repository for container images 
 
Required IAM permissions: 
 
 AmazonSageMakerHyperPodTrainingOperatorAccess managed policy 
 EKS cluster access permissions 
 ECR push/pull permissions 
 eks-pod-identity-agent add-on installed on EKS cluster 
 
Required software: 
 
 kubectl (version 1.28 or later), for more information see the kubectl installation documentation 
 docker (version 20.10 or later), for more information see the docker installation documentation 
 AWS Command Line Interface (AWS CLI) (version 2.0 or later), for more information see the AWS CLI installation documentation 
 envsubst utility 
 HuggingFace account with access token 
 
Installation instructions 
Before running the installation steps below, you‚Äôll need to first create a HyperPod cluster. If you haven‚Äôt done this one already please follow the instructions to create an EKS-orchestrated SageMaker HyperPod cluster to get started. Make sure to install eks-pod-identity-agent&nbsp;add-on on the EKS cluster, by following the Set up the Amazon EKS Pod Identity Agent instructions. 
Install cert-manager 
First, install the cert-manager add-on which is required for the HyperPod training operator: 
 
 Open the Amazon EKS console 
 Navigate to your EKS cluster and go to the Add-ons page 
 On the Add-ons page, locate Get more add-ons and navigate to the Community add-ons section 
 Find the Cert Manager add-on, select it, and choose Next 
 On the add-on configuration page, proceed with default settings and choose Next 
 Preview all selections for the Cert Manager add-on and choose Create 
 Wait for the add-on status to change to Active before proceeding 
 
 
Install the HyperPod training operator add-on 
Once cert-manager is active, install the Amazon SageMaker HyperPod training operator: 
 
 Open the Amazon SageMaker console 
 Navigate to your cluster‚Äôs details page 
 On the Dashboard tab, locate Amazon SageMaker HyperPod training operator and choose Install 
 
During installation, SageMaker creates an IAM execution role with permissions similar to the AmazonSageMakerHyperPodTrainingOperatorAccess managed policy and creates a pod identity association between your Amazon EKS cluster and the new execution role. 
 
Verify installation 
We have now successfully setup of the Amazon SageMaker HyperPod training operator.&nbsp;You can confirm that the pods are running by using the following command: 
 
 kubectl -n aws-hyperpod get pods -l hp-training-control-plane=hp-training-operator-controller-manager 
 
Your output should contain the training operator controller as shown below: 
 
 NAME READY&nbsp; &nbsp; &nbsp; STATUS&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;RESTARTS&nbsp; &nbsp; &nbsp; &nbsp; AGE
hp-training-operator-hp-training-controller-manager-85c68bmd79b&nbsp; &nbsp; 1/1&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Running&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;0&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 24m 
 
Set up training job 
Let‚Äôs run a PyTorch-based training example on a Llama model. We begin by checking out the following code base: 
 
 git clone&nbsp;
cd awsome-distributed-training/tree/main/3.test_cases/pytorch/FSDP 
 
These scripts provide an easy way to get started with multinode FSDP training on EKS. It is designed to be as simple as possible, requires no data preparation, and uses a container image. 
Next, build the docker container image. 
 
 aws ecr-public get-login-password ‚Äîregion us-east-1 | docker login ‚Äîusername AWS ‚Äîpassword-stdin public.ecr.aws/hpc-cloud
export REGION=$(aws ec2 describe-availability-zones ‚Äîoutput text ‚Äîquery 'AvailabilityZones[0].[RegionName]')
export ACCOUNT=$(aws sts get-caller-identity ‚Äîquery Account ‚Äîoutput text)
export REGISTRY=${ACCOUNT}.dkr.ecr.${REGION}.

docker build -t ${REGISTRY}fsdp:pytorch2.5.1 .&nbsp;&nbsp; &nbsp; 
 
The above command works with linux based environments, if you are on a Mac, use buildx to target linux/amd64 architecture: 
 
 docker buildx build --platform linux/amd64 -t ${REGISTRY}fsdp:pytorch2.5.1 . 
 
Push the image to Amazon ECR: 
 
 # Create registry if needed
REGISTRY_COUNT=$(aws ecr describe-repositories | grep "fsdp" | wc -l)
if [ "$REGISTRY_COUNT" -eq 0 ]; then
&nbsp;&nbsp; &nbsp;aws ecr create-repository --repository-name fsdp
fi

# Login to registry
echo "Logging in to $REGISTRY ..."
aws ecr get-login-password | docker login --username AWS --password-stdin $REGISTRY

# Push image to registry
docker image push ${REGISTRY}fsdp:pytorch2.5.1 
 
Note: Pushing the image may take some time depending on your network bandwidth. 
Data 
For this example, we‚Äôll be using the allenai/c4 dataset. Instead of downloading the whole thing, the create_streaming_dataloaders function will stream the dataset from HuggingFace, so there‚Äôs no data prep required for running this training. 
If you‚Äôd like to instead use your own dataset, you can do so by formatting it as a HuggingFace dataset, and passing its location to the --dataset_path argument. 
For the dataset, you will need a Hugging Face access token. First, create a Hugging Face account. Then generate your access token with read permissions. 
We will reference this token in the next step by setting it as an environment variable. 
This example uses envsubst  to generate a Kubernetes manifest file from a template file and parameters. If you don‚Äôt have envsubst on your development environment, install it by following the installation instructions. 
Launch Llama 3.1 8B training job 
Next, we generate the Kubernetes manifest and apply it to the cluster. Let‚Äôs navigate to the FSDP source repo: 
 
 cd awsome-distributed-training/tree/main/3.test_cases/pytorch/FSDP/Kubernetes 
 
Here, we start by creating environment variables that are used in our training job. Fill out the placeholders as per your cluster size. 
 
 cat &lt;&lt; EOF &gt; env_vars
export ACCOUNT_ID=&lt;AWS_ACCOUNT_ID&gt;
export REGION=&lt;REGION&gt;
export REGISTRY=${ACCOUNT_ID}.dkr.ecr.${REGION}.amazonaws.com
export IMAGE_URI=${REGISTRY}/fsdp:pytorch2.5.1
export INSTANCE_TYPE=&lt;INSTANCE TYPE&gt; # ml.p5.48xlarge
export NUM_NODES=&lt;NUMBER OF NODES&gt; # 2
export GPU_PER_NODE=&lt;NUMBER OF GPUS PER NODE&gt; # 8
export EFA_PER_NODE=&lt;NUMBER OF EFA PER NODE&gt; # 32
export FI_PROVIDER=efa
export HF_TOKEN=&lt;YOUR HF ACCESS TOKEN&gt; # HF_xxxx
EOF 
 
Once you fill in env_vars and then source variables: 
 
 source env_vars 
 
You can apply yaml to submit the training job: 
 
 envsubst &lt; llama3_1_8b-fsdp-hpto.yaml | kubectl apply -f - 
 
You can also adjust the training parameters in the&nbsp;TRAINING_ARGS&nbsp;section of the llama3_1_8b-fsdp-hpto.yaml. Additional parameters can be found under&nbsp;model/arguments.py. Note that we use the same directory for both --checkpoint_dir and --resume_from_checkpoint. If there are multiple checkpoints, --resume_from_checkpoint will automatically select the most recent one. This way if our training is interrupted for any reason, it will automatically pick up the most recent checkpoint. 
Additionally, you can also prepare and submit your jobs compatible with the Amazon SageMaker&nbsp;HyperPod training operator through the HyperPod CLI and SDK capabilities that have been recently announced, more reading information on how to use it is available in this development guide. 
Monitor training job 
To see the status of your job, use the following command: 
 
 kubectl get hyperpodpytorchjobs 
 
Use the following command to list the jobs ran using HyperPod training operator: 
 
 NAME &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;AGE
llama2-13b-fsdp &nbsp; 2m15s

kubectl get pods&nbsp; 
 
Use the following command to list all the pods for the training jobs: 
 
 NAME&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; READY&nbsp; STATUS&nbsp; &nbsp;RESTARTS AGE
llama2-13b-fsdp-pods-0&nbsp;&nbsp;1/1&nbsp; &nbsp;&nbsp;Running&nbsp; &nbsp;0&nbsp; &nbsp; &nbsp; &nbsp;13s
llama2-13b-fsdp-pods-1&nbsp;&nbsp;1/1&nbsp; &nbsp;&nbsp;Running&nbsp; &nbsp;0&nbsp; &nbsp; &nbsp; &nbsp;13s
llama2-13b-fsdp-pods-2&nbsp;&nbsp;1/1&nbsp; &nbsp;&nbsp;Running&nbsp; &nbsp;0&nbsp; &nbsp; &nbsp; &nbsp;13s
llama2-13b-fsdp-pods-3&nbsp;&nbsp;1/1&nbsp; &nbsp;&nbsp;Running&nbsp; &nbsp;0&nbsp; &nbsp; &nbsp; &nbsp;13s 
 
To check the pod logs run the below command to continuously stream the logs to stdout, use the following command: 
 
 kubectl logs -f llama2-13b-fsdp-pods-0 
 
Configure log monitoring 
With Amazon SageMaker&nbsp;HyperPod training operators users can configure log patterns that the operator continuously monitors. The HyperPod operator continuously looks for the configured regex pattern and stops the training job if it finds a violation.&nbsp;The&nbsp;llama3_1_13b-fsdp-hpto.yaml file that we used previously contains log monitoring configurations for tracking Job start hangs, hang detection during training, and checkpoint creation failures as shown below: 
 
  &nbsp;logMonitoringConfiguration:
&nbsp;&nbsp; &nbsp; &nbsp;- name: "JobStart"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;logPattern: ".*Loss:.*"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;expectedStartCutOffInSeconds: 240 # job should print loss within 4 mins of start time
&nbsp;&nbsp; &nbsp; &nbsp;- name: "JobHangingDetection"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;logPattern: ".*Loss:.*"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;expectedRecurringFrequencyInSeconds: 300 # if next batch is not printed within 300 seconds
&nbsp;&nbsp; &nbsp; &nbsp;- name: "NoSCheckpointingDetection"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;logPattern: ".*Completed checkpoint.*"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;expectedRecurringFrequencyInSeconds: 600 # If next checkpoint upload doesn't happen within 10 mins, mark it hang.
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;expectedStartCutOffInSeconds: 900 # Allow 30 minutes for first checkpoint upload 
 
And the corresponding code files in&nbsp;/src/train.py have the necessary log statements. 
 
 logger.info(
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "Batch %d Loss: %.5f, Speed: %.2f samples/sec, lr: %.6f", &nbsp;# pylint: disable=line-too-long
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;batch_idx,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;loss_scalar,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;throughput,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;current_lr,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  
 
Any time these metrics exhibit deviation from their expected values, the operator will detect it as a fault, and trigger a recovery process to re-execute the job, up to a user-specified maximum number of retries. 
Additionally, the HyperPod training operator also supports integration with Amazon SageMaker Task Governance. 
Integration with HyperPod Observability 
SageMaker HyperPod offers a managed observability experience through the newly launched&nbsp;the HyperPod Monitoring and Observability EKS add-on. The observability add-on automatically populates Kubeflow Training metrics in Grafana dashboards out of the box, but for HyperPod PyTorch job metrics, you would have to turn on the advanced training metrics which leverage the HyperPod training operator to show information around job downtime, job recovery and faults, and downtime. 
To get these advanced metrics, you can refer to Setting up the SageMaker HyperPod observability add-on. This helps to streamline the process of manually setting up a scraper and building dashboards. 
Clean up 
To avoid incurring unnecessary charges, clean up the resources created in this walkthrough. 
Delete training jobs 
Remove all HyperPod training jobs: 
 
 kubectl delete hyperpodpytorchjobs --all 
 
Verify jobs are deleted: 
 
 kubectl get hyperpodpytorchjobs 
 
Remove container images 
Delete the ECR repository and images: 
 
 aws ecr delete-repository --repository-name fsdp --force 
 
Remove add-ons: 
Remove the following add-ons: 
Remove the Amazon SageMaker HyperPod training operator add-on: 
 
 Open the Amazon SageMaker console 
 Navigate to your cluster‚Äôs details page 
 On the Add-ons tab, select the Amazon SageMaker&nbsp;HyperPod training operator 
 Choose Remove 
 
Remove the cert manager add-on: 
 
 Open the Amazon EKS console 
 Navigate to your EKS cluster‚Äôs Add-ons page 
 Select Cert Manager and choose Remove 
 
Additional clean up 
Consider removing these resources if no longer needed: 
 
 Any persistent volumes created during training 
 CloudWatch log groups (if you want to retain logs, leave these) 
 Custom IAM roles created specifically for this example 
 The HyperPod cluster itself (if no longer needed). 
 
Conclusion 
As organizations continue to push the boundaries of AI model development, tools like the Amazon SageMaker&nbsp;HyperPod training operator can be used to maintain efficiency and reliability at scale.&nbsp;Amazon SageMaker HyperPod training operator offers a robust solution to common challenges in large model training.&nbsp;Key takeaways include: 
 
 One-click installation through AWS SageMaker HyperPod cluster console user-interface. 
 Custom rendezvous backend eliminates initialization and worker synchronization overhead which results in faster job starts and recovery. 
 Process level restarts maximize recovery efficiency when runtime faults occur. 
 Customizable hang job detection during training. 
 Comprehensive monitoring for early detection of training issues. 
 Out-of-box integration with existing HyperPod resiliency features. 
 
To get started with the Amazon SageMaker&nbsp;HyperPod training operator, follow the setup instructions provided in this post and explore the example training job to understand how it can benefit your specific use case. For more information and best practices, visit the Amazon SageMaker documentation. 
 
About the authors 
Arun Kumar Lokanatha&nbsp;is a Senior ML Solutions Architect with the Amazon SageMaker AI. He holds a Master‚Äôs degree from UIUC with a specialization in Data science. He specializes in Generative AI workloads, helping customers build and deploy LLM‚Äôs using SageMaker HyperPod, SageMaker training jobs, and SageMaker distributed training. Outside of work, he enjoys running, hiking, and cooking. 
Haard Mehta is a Software Engineer with Amazon‚Äôs SageMaker AI team and holds a Master‚Äôs degree in Computer Science with a specialization in big data systems from Arizona State University. He has extensive experience building managed machine learning services at scale, with a focus on hardware resiliency and enabling customers to succeed in their AI use cases without complex infrastructure management. Haard enjoys exploring new places, photography, cooking, and road trips. 
Anirudh Viswanathan&nbsp;is a Sr Product Manager, Technical ‚Äì External Services with the SageMaker AI Training team. He holds a Masters in Robotics from Carnegie Mellon University, an MBA from the Wharton School of Business, and is named inventor on over 40 patents. He enjoys long-distance running, visiting art galleries, and Broadway shows.
‚Ä¢ How TP ICAP transformed CRM data into real-time insights with Amazon Bedrock
  This post is co-written with Ross Ashworth at TP ICAP. 
The ability to quickly extract insights from customer relationship management systems (CRMs) and vast amounts of meeting notes can mean the difference between seizing opportunities and missing them entirely. TP ICAP faced this challenge, having thousands of vendor meeting records stored in their CRM. Using Amazon Bedrock, their Innovation Lab built a production-ready solution that transforms hours of manual analysis into seconds by providing AI-powered insights, using a combination of Retrieval Augmented Generation (RAG) and text-to-SQL approaches. 
This post shows how TP ICAP used Amazon Bedrock Knowledge Bases and Amazon Bedrock Evaluations to build ClientIQ, an enterprise-grade solution with enhanced security features for extracting CRM insights using AI, delivering immediate business value. 
The challenge 
TP ICAP had accumulated tens of thousands of vendor meeting notes in their CRM system over many years. These notes contained rich, qualitative information and details about product offerings, integration discussions, relationship insights, and strategic direction. However, this data was being underutilized and business users were spending hours manually searching through records, knowing the information existed but unable to efficiently locate it. The TP ICAP Innovation Lab set out to make the information more accessible, actionable, and quickly summarized for their internal stakeholders. Their solution needed to surface relevant information quickly, be accurate, and maintain proper context. 
ClientIQ: TP ICAP‚Äôs custom CRM assistant 
With ClientIQ, users can interact with their Salesforce meeting data through natural language queries. For example: 
 
 Ask questions about meeting data in plain English, such as ‚ÄúHow can we improve our relationship with customers?‚Äù, ‚ÄúWhat do our clients think about our solution?‚Äù, or ‚ÄúHow were our clients impacted by Brexit?‚Äù 
 Refine their queries through follow-up questions. 
 Apply filters to restrict model answers to a particular time period. 
 Access source documents directly through links to specific Salesforce records. 
 
ClientIQ provides comprehensive responses while maintaining full traceability by including references to the source data and direct links to the original Salesforce records. The conversational interface supports natural dialogue flow, so users can refine and explore their queries without starting over. The following screenshot shows an example interaction (examples in this post use fictitious data and AnyCompany, a fictitious company, for demonstration purposes). 
 
ClientIQ performs multiple tasks to fulfill a user‚Äôs request: 
 
 It uses a large language model (LLM) to analyze each user query to determine the optimal processing path. 
 It routes requests to one of two workflows: 
   
   The RAG workflow for getting insights from unstructured meeting notes. For example, ‚ÄúWas topic A discussed with AnyCompany the last 14 days?‚Äù 
   The SQL generation workflow for answering analytical queries by querying structured data. For example, ‚ÄúGet me a report on meeting count per region for last 4 weeks.‚Äù 
    
 It then generates the responses in natural language. 
 ClientIQ respects existing permission boundaries and access controls, helping verify users only access the data they‚Äôre authorized to. For example, if a user only has access to their regional accounts in the CRM system, ClientIQ only returns information from these accounts. 
 
Solution overview 
Although the team considered using their CRM‚Äôs built-in AI assistant, they opted to develop a more customized, cost-effective solution that would precisely match their requirements. They partnered with AWS and built an enterprise-grade solution powered by Amazon Bedrock. With Amazon Bedrock, TP ICAP evaluated and selected the best models for their use case and built a production-ready RAG solution in weeks rather than months, without having to manage the underlying infrastructure. They specifically used the following Amazon Bedrock managed capabilities: 
 
 Amazon Bedrock foundation models ‚Äì Amazon Bedrock provides a range of foundation models (FMs) from providers, including Anthropic, Meta, Mistral AI, and Amazon, accessible through a single API. TP ICAP experimented with different models for various tasks and selected the best model for each task, balancing latency, performance, and cost. For instance, they used Anthropic‚Äôs Claude 3.5 Sonnet for classification tasks and Amazon Nova Pro for text-to-SQL generation. Because Amazon Bedrock is fully managed, they didn‚Äôt need to spend time setting up infrastructure for hosting these models, reducing the time to delivery. 
 Amazon Bedrock Knowledge Bases ‚Äì The FMs needed access to the information in TP ICAP‚Äôs Salesforce system to provide accurate, relevant responses. TP ICAP used Amazon Bedrock Knowledge Bases to implement RAG, a technique that enhances generative AI responses by incorporating relevant data from your organization‚Äôs knowledge sources. Amazon Bedrock Knowledge Bases is a fully managed RAG capability with built-in session context management and source attribution. The final implementation delivers precise, contextually relevant responses while maintaining traceability to source documents. 
 Amazon Bedrock Evaluations ‚Äì For consistent quality and performance, the team wanted to implement automated evaluations. By using Amazon Bedrock Evaluations and the RAG evaluation tool for Amazon Bedrock Knowledge Bases in their development environment and CI/CD pipeline, they were able to evaluate and compare FMs with human-like quality. They evaluated different dimensions, including response accuracy, relevance, and completeness, and quality of RAG retrieval. 
 
Since launch, their approach scales efficiently to analyze thousands of responses and facilitates data-driven decision-making about model and inference parameter selection, and RAG configuration.The following diagram showcases the architecture of the solution. 
 
The user query workflow consists of the following steps: 
 
 The user logs in through a frontend React application, hosted in an Amazon Simple Storage Service (Amazon S3) bucket and accessible only within the organization‚Äôs network through an internal-only Application Load Balancer. 
 After logging in, a WebSocket connection is opened between the client and Amazon API Gateway to enable real-time, bi-directional communication. 
 After the connection is established, an AWS Lambda function (connection handler) is invoked, which process the payload, logs tracking data to Amazon DynamoDB, and publishes request data to an Amazon Simple Notification Service (Amazon SNS) topic for downstream processing. 
 Lambda functions for different types of tasks consume messages from Amazon Simple Queue Service (Amazon SQS) for scalable and event-driven processing. 
 The Lambda functions use Amazon Bedrock FMs to determine whether a question is best answered by querying structured data in Amazon Athena or by retrieving information from an Amazon Bedrock knowledge base. 
 After processing, the answer is returned to the user in real time using the existing WebSocket connection through API Gateway. 
 
Data ingestion 
ClientIQ needs to be regularly updated with the latest Salesforce data. Rather than using an off-the-shelf option, TP ICAP developed a custom connector to interface with their highly tailored Salesforce implementation and ingest the latest data to Amazon S3. This bespoke approach provided the flexibility needed to handle their specific data structures while remaining simple to configure and maintain. The connector, which employs Salesforce Object Query Language (SOQL) queries to retrieve the data, runs daily and has proven to be fast and reliable. To optimize the quality of the results during the RAG retrieval workflow, TP ICAP opted for a custom chunking approach in their Amazon Bedrock knowledge base. The custom chunking happens as part of the ingestion process, where the connector splits the data into individual CSV files, one per meeting. These files are also automatically tagged with relevant topics from a predefined list, using Amazon Nova Pro, to further increase the quality of the retrieval results. The final outputs in Amazon S3 contain a CSV file per meeting and a matching JSON metadata file containing tags such as date, division, brand, and region. The following is an example of the associated metadata file: 
 
 {
"metadataAttributes": {
   "Tier": "Bronze",
   "Number_Date_of_Visit": 20171130,
   "Author_Region_C": "AMER",
   "Brand_C": "Credit",
   "Division_C": "Credit",
   "Visiting_City_C": "Chicago",
   "Client_Name": "AnyCompany‚Äù
   }
} 
 
As soon as the data is available in Amazon S3, an AWS Glue job is triggered to populate the AWS Glue Data Catalog. This is later used by Athena when querying the Amazon S3 data. 
The Amazon Bedrock knowledge base is also synced with Amazon S3. As part of this process, each CSV file is converted into embeddings using Amazon Titan v1 and indexed in the vector store, Amazon OpenSearch Serverless. The metadata is also ingested and available for filtering the vector store results during retrieval, as described in the following section. 
Boosting RAG retrieval quality 
In a RAG query workflow, the first step is to retrieve the documents that are relevant to the user‚Äôs query from the vector store and append them to the query as context. Common ways to find the relevant documents include semantic search, keyword search, or a combination of both, referred to as hybrid search. ClientIQ uses hybrid search to first filter documents based on their metadata and then perform semantic search within the filtered results. This pre-filtering provides more control over the retrieved documents and helps disambiguate queries. For example, a question such as ‚Äúfind notes from executive meetings with AnyCompany in Chicago‚Äù can mean meetings with any AnyCompany division that took place in Chicago or meetings with AnyCompany‚Äôs division headquartered in Chicago. 
TP ICAP used the manual metadata filtering capability in Amazon Bedrock Knowledge Bases to implement hybrid search in their vector store, OpenSearch Serverless. With this approach, in the preceding example, the documents are first pre-filtered for ‚ÄúChicago‚Äù as Visiting_City_C. After that, a semantic search is performed to find the documents that contain executive meeting notes for AnyCompany. The final output contains notes from meetings in Chicago, which is what is expected in this case. The team enhanced this functionality further by using the implicit metadata filtering of Amazon Bedrock Knowledge Bases. This capability relies on Amazon Bedrock FMs to automatically analyze the query, understand which values can be mapped to metadata fields, and rewrite the query accordingly before performing the retrieval. 
Finally, for additional precision, users can manually specify filters through the application UI, giving them greater control over their search results. This multi-layered filtering approach significantly improves context and final response accuracy while maintaining fast retrieval speeds. 
Security and access control 
To maintain Salesforce‚Äôs granular permissions model in the ClientIQ solution, TP ICAP implemented a security framework using Okta group claims mapped to specific divisions and regions. When a user signs in, their group claims are attached to their session. When the user asks a question, these claims are automatically matched against metadata fields in Athena or OpenSearch Serverless, depending on the path followed. 
For example, if a user has access to see information for EMEA only, then the documents are automatically filtered by the EMEA region. In Athena, this is done by automatically adjusting the query to include this filter. In Amazon Bedrock Knowledge Bases, this is done by introducing an additional metadata field filter for region=EMEA in the hybrid search. This is highlighted in the following diagram. 
 
Results that don‚Äôt match the user‚Äôs permission tags are filtered out, so that users can only access data they‚Äôre authorized to see. This unified security model maintains consistency between Salesforce permissions and ClientIQ access controls, preserving data governance across solutions. 
The team also developed a custom administrative interface for admins that manage permission in Salesforce to add or remove users from groups using Okta‚Äôs APIs. 
Automated evaluation 
The Innovation Lab team faced a common challenge in building their RAG application: how to scientifically measure and improve its performance. To address that, they developed an evaluation strategy using Amazon Bedrock Evaluations that involves three phrases: 
 
 Ground truth creation ‚Äì They worked closely with stakeholders and testing teams to develop a comprehensive set of 100 representative question answers pairs that mirrored real-world interactions. 
 RAG evaluation ‚Äì In their development environment, they programmatically triggered RAG evaluations in Amazon Bedrock Evaluations to process the ground truth data in Amazon S3 and run comprehensive assessments. They evaluated different chunking strategies, including default and custom chunking, tested different embedding models for retrieval, and compared FMs for generation using a range of inference parameters. 
 Metric-driven optimization ‚Äì Amazon Bedrock generates evaluation reports containing metrics, scores, and insights upon completion of an evaluation job. The team tracked content relevance and content coverage for retrieval and quality, and responsible AI metrics such as response relevance, factual accuracy, retrieval precision, and contextual comprehension for generation. They used the evaluation reports to make optimizations until they reached their performance goals. 
 
The following diagram illustrates this approach. 
 
In addition, they integrated RAG evaluation directly into their continuous integration and continuous delivery (CI/CD) pipeline, so every deployment automatically validates that changes don‚Äôt degrade response quality. The automated testing approach gives the team confidence to iterate quickly while maintaining consistently high standards for the production solution. 
Business outcomes 
ClientIQ has transformed how TP ICAP extracts value from their CRM data. Following the initial launch with 20 users, the results showed that the solution has driven a 75% reduction in time spent on research tasks. Stakeholders also reported an improvement in insight quality, with more comprehensive and contextual information being surfaced. Building on this success, the TP ICAP Innovation Lab plans to evolve ClientIQ into a more intelligent virtual assistant capable of handling broader, more complex tasks across multiple enterprise systems. Their mission remains consistent: to help technical and non-technical teams across the business to unlock business benefits with generative AI. 
Conclusion 
In this post, we explored how the TP ICAP Innovation Lab team used Amazon Bedrock FMs, Amazon Bedrock Knowledge Bases, and Amazon Bedrock Evaluations to transform thousands of meeting records from an underutilized resource into a valuable asset and accelerate time to insights while maintaining enterprise-grade security and governance. Their success demonstrates that with the right approach, businesses can implement production-ready AI solutions and deliver business value in weeks. To learn more about building similar solutions with Amazon Bedrock, visit the&nbsp;Amazon Bedrock documentation or discover real-world success stories and implementations on the&nbsp;AWS Financial Services Blog. 
 
About the authors 
Ross Ashworth works in TP ICAP‚Äôs AI Innovation Lab, where he focuses on enabling the business to harness Generative AI across a range of projects. With over a decade of experience working with AWS technologies, Ross brings deep technical expertise to designing and delivering innovative, practical solutions that drive business value. Outside of work, Ross is a keen cricket fan and former amateur player. He is now a member at The Oval, where he enjoys attending matches with his family, who also share his passion for the sport. 
Anastasia Tzeveleka is a Senior Generative AI/ML Specialist Solutions Architect at AWS. Her experience spans the entire AI lifecycle, from collaborating with organizations training cutting-edge Large Language Models (LLMs) to guiding enterprises in deploying and scaling these models for real-world applications. In her spare time, she explores new worlds through fiction.

‚∏ª