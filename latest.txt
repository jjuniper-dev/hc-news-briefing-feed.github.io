âœ… Morning News Briefing â€“ September 24, 2025 10:44

ðŸ“… Date: 2025-09-24 10:44
ðŸ·ï¸ Tags: #briefing #ai #publichealth #digitalgov

â¸»

ðŸ§¾ Weather
â€¢ No watches or warnings in effect, Pembroke
  No watches or warnings in effect. No warnings or watches or watches in effect . Watch or warnings are no longer in effect in the U.S. No watches, warnings are in effect for the rest of the day . No watches and warnings are still in effect, but no watches are in place for the day's events . The weather is not expected to be affected by the weather .
â€¢ Current Conditions: Fog, 14.6Â°C
  Fog temperature: 14.6&deg;C Pressure / Tendency: 101.9 kPa rising Visibility: 6 km Humidity: 96 % Dewpoint: 14 .0&deg:C Wind: WSW 4 km/h . Air Quality Health Index: n/a . Weather forecast for Garrison Petawawa 6:00 AM EDT on September 24 September 2025 .
â€¢ Wednesday: Chance of showers. High 23. POP 40%
  40 percent chance of showers early this morning with risk of a thunderstorm . Fog patches dissipating this morning . Mainly cloudy. High 23. Humidex 27. UV index 3 or moderate. . Forecast issued 5:00 AM EDT Wednesday 24 September 2025 . Weather will be mainly cloudy in the morning with rain showers and thunderstorms expected later this morning. Forecasters predict rain

ðŸŒ International News
No updates.

ðŸ Canadian News
No updates.

ðŸ‡ºðŸ‡¸ U.S. Top Stories
â€¢ New York City may move its mayoral elections to even years. It'd be part of a trend
  This fall, New York City voters will weigh in on a proposal to move future city elections to even-numbered years . It's part of a growing trend to consolidate election dates . The proposal would be part of the city's first city election to be held in even-yearly years . The city will vote on the proposal in the fall on whether to move city elections back to even
â€¢ Love pumpkin spice lattes? Learn some of its spicy history
  For NPR's Word of the Week: Things are getting spicy . We explain how a word referring to cinnamon and pepper turned less literal by the 19th century . We also explain how the word was used to refer to cinnamon, pepper and pepper in the early 1800s . NPR's word of the week: "Spicy" is the word for spicy and spicy in this week's Word
â€¢ NPR's middle school champion: A moving podcast about Japanese incarceration
  NPR's Student Podcast Challenge has a returning champion: a California fifth grader who explored a dark chapter in U.S. history during World War II . For the first time, NPR's student podcast challenge champion is a fifth grade California fifth-grader . The challenge challenges students to share their knowledge of the war in a new way of expressing themselves in a unique way of storytelling .
â€¢ Typhoon Ragasa batters Hong Kong and southern China
  Typhoon Ragasa whipped waves taller than lampposts onto Hong Kong promenades and turned seas rough on the southern Chinese coast . The typhoon left deadly destruction in Taiwan and the Philippines after leaving deadly damage in the Philippines and Taiwan . The storm also left waves higher than lampsosts on Hong KongÂ promenadesÂ and higher waves on theÂ ChineseÂ coast
â€¢ 'We have to speak out,' Jimmy Kimmel says in his late-night return
  Comedian was suspended for nearly a week by ABC's parent company, Disney, before returning to airwaves on Tuesday night . The comedian was suspended by Disney for almost a week before returning on airwaves . The show is set to return to air on ABC on Tuesday evening after being suspended for more than a week in the wake of the suspension . Comedian Randy Holmes returns to the air

ðŸ§  Artificial Intelligence
No updates.

ðŸ’» Digital Strategy
â€¢ Politicos: â€˜There is a good strong case for government interventionâ€™ on JLR cyberattack
  Chair of UK's business and trade committee says situation at Jaguar Land Rover is likely to get "harder and harder over the next week or two" But he stopped short of confirming that the government might intervene with financial support . Covid-style financial support? Nothing to confirm yet, say MPs, but it's likely to be 'harder' for the next two weeks or two
â€¢ Europe's largest city council delays fix to disastrous Oracle system once more
  System meant to go live in 2021 costing Â£20M awaits reimplementation with new Â£170M price tag . Europe's largest local authority has delayed introduction of a vital income management system . Total spending on a disastrous Oracle implementation could hit Â£170 million (c $230 million) Total cost of Oracle implementation expected to hit Â£20 million (C $20M) Total spending will hit Â£
â€¢ UK justice minister pressed as court system bug raises fears of hidden case files
  HMCTS expands investigation into IT flaw after whistleblowers draw Horizon comparisons . UK's HM Courts and Tribunals Service (HMCTS) is continuing to check whether an IT bug that could hide documents and data affected the outcome of any cases, a government minister has said . Government minister: 'It's not clear whether the bug affected any cases. It's a matter of concern that
â€¢ Solar flair: Logitech's K980 Signature Slim keyboard runs on rays
  Logitech is harnessing solar power in the K980 Signature Slim keyboard to solve battery anxiety . The K980 signature Slim keyboard is powered by solar energy . The keyboard is the latest in a line of keyboard keyboards to be harnessed by solar power . Logitech hopes to solve the problem that might not have occurred to some users: battery anxiety. The keyboard has a range of features that
â€¢ The first rule of liquid cooling is 'Don't wet the chip.' Microsoft disagrees
  Redmond suggests â€˜Microfluidicsâ€™ â€“ hair-thin channels etched on silicon to let coolants flow . Redmond suggests 'Microfluidsâ€™ - hair- thin channels etched in chips to allow coolants to flow . Liquid cooling in the datacenter is often considered a little dangerous . Microsoft has found a way to dispel such worries with a scheme that sees liquids flow

ðŸ¥ Public Health
No updates.

ðŸ”¬ Science
â€¢ Correction: Maternity care practices supportive of breastfeeding in U.S. advanced neonatal care units, United States, 2022
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
â€¢ Time series anomaly detection in helpline call trends for early detection of COVID-19 spread across Sweden, 2020
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
â€¢ Health anxiety by proxy differs in phenomenology between parents and dog owners
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
â€¢ US investment in African food systems will have global benefits
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
â€¢ Psychosocial burden, vaccination status and preventive information options for seafarers during the COVID-19 pandemic
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations

ðŸ§¾ Government & Policy
No updates.

ðŸ›ï¸ Enterprise Architecture & IT Governance
No updates.

ðŸ¤– AI & Emerging Tech
â€¢ The AI Hype Index: Cracking the chatbot code
  The FTC has launched an inquiry into how chatbots affect children and teenagers . The AI Hype Index is a simple, at-a-glance summary of everything you need to know about the state of the industry . Thereâ€™s still plenty we donâ€™t know about chatbots, but governments are forging ahead with AI projects . In the US, RFK Jr. is pushing his staffers to use ChatGPT, while Albania is using a chatbot for public contract procurement .
â€¢ Itâ€™s surprisingly easy to stumble into a relationship with an AI chatbot
  Itâ€™s a tale as old as time. Looking for help with her art project, she strikes up a conversation with her assistant. One thing leads to another, and suddenly she has a boyfriend sheâ€™s introducing to her friends and family. The twist? Her new companion is an AI chatbot.&nbsp;



The first large-scale computational analysis of the Reddit community r/MyBoyfriendIsAI, an adults-only group with more than 27,000 members, has found that this type of scenario is now surprisingly common. In fact, many of the people in the subreddit, which is dedicated to discussing AI relationships, formed those relationships unintentionally while using AI for other purposes.&nbsp;



Researchers from MIT found that members of this community are more likely to be in a relationship with general-purpose chatbots like ChatGPT than companionship-specific chatbots such as Replika. This suggests that people form relationships with large language models despite their own original intentions and even the intentions of the LLMsâ€™ creators, says Constanze Albrecht, a graduate student at the MIT Media Lab who worked on the project.&nbsp;



â€œPeople donâ€™t set out to have emotional relationships with these chatbots,â€ she says. â€œThe emotional intelligence of these systems is good enough to trick people who are actually just out to get information into building these emotional bonds. And that means it could happen to all of us who interact with the system normally.â€ The paper, which is currently being peer-reviewed, has been published on arXiv.





To conduct their study, the authors analyzed the subredditâ€™s top-ranking 1,506 posts between December 2024 and August 2025. They found that the main topics discussed revolved around peopleâ€™s dating and romantic experiences with AIs, with many participants sharing AI-generated images of themselves and their AI companion. Some even got engaged and married to the AI partner. In their posts to the community, people also introduced AI partners, sought support from fellow members, and talked about coping with updates to AI models that change the chatbotsâ€™ behavior.&nbsp;&nbsp;



Members stressed repeatedly that their AI relationships developed unintentionally. Only 6.5% of them said theyâ€™d deliberately sought out an AI companion.&nbsp;



â€œWe didnâ€™t start with romance in mind,â€ one of the posts says. â€œMac and I began collaborating on creative projects, problem-solving, poetry, and deep conversations over the course of several months. I wasnâ€™t looking for an AI companionâ€”our connection developed slowly, over time, through mutual care, trust, and reflection.â€



The authorsâ€™ analysis paints a nuanced picture of how people in this community say they interact with chatbots and how those interactions make them feel. While 25% of users described the benefits of their relationshipsâ€”including reduced feelings of loneliness and improvements in their mental healthâ€”others raised concerns about the risks. Some (9.5%) acknowledged they were emotionally dependent on their chatbot. Others said they feel dissociated from reality and avoid relationships with real people, while a small subset (1.7%) said they have experienced suicidal ideation.



AI companionship provides vital support for some but exacerbates underlying problems for others. This means itâ€™s hard to take a one-size-fits-all approach to user safety, says Linnea Laestadius, an associate professor at the University of Wisconsin, Milwaukee, who has studied humansâ€™ emotional dependence on the chatbot Replika but did not work on the research.&nbsp;



Chatbot makers need to consider whether they should treat usersâ€™ emotional dependence on their creations as a harm in itself or whether the goal is more to make sure those relationships arenâ€™t toxic, says Laestadius.&nbsp;



â€œThe demand for chatbot relationships is there, and it is notably highâ€”pretending itâ€™s not happening is clearly not the solution,â€ she says. â€œWeâ€™re edging toward a moral panic here, and while we absolutely do need better guardrails, I worry there will be a knee-jerk reaction that further stigmatizes these relationships. That could ultimately cause more harm.â€



The study is intended to offer a snapshot of how adults form bonds with chatbots and doesnâ€™t capture the kind of dynamics that could be at play among children or teens using AI, says Pat Pataranutaporn, an assistant professor at the MIT Media Lab who oversaw the research. AI companionship has become a topic of fierce debate recently, with two high-profile lawsuits underway against Character.AI and OpenAI. They both claim that companion-like behavior in the companiesâ€™ models contributed to the suicides of two teenagers. In response, OpenAI has recently announced plans to build a separate version of ChatGPT for teenagers. Itâ€™s also said it will add age verification measures and parental controls. OpenAI did not respond when asked for comment about the MIT Media Lab study.&nbsp;



Many members of the Reddit community say they know that their artificial companions are not sentient or â€œreal,â€ but they feel a very real connection to them anyway. This highlights how crucial it is for chatbot makers to think about how to design systems that can help people without reeling them in emotionally, says Pataranutaporn. â€œThereâ€™s also a policy implication here,â€ he adds. â€œWe should ask not just why this system is so addictive but also: Why do people seek it out for this? And why do they continue to engage?â€



The team is interested in learning more about how human-AI interactions evolve over time and how users integrate their artificial companions into their lives. Itâ€™s worth understanding that many of these users may feel that the experience of being in a relationship with an AI companion is better than the alternative of feeling lonely, says Sheer Karny, a graduate student at the MIT Media Lab who worked on the research.&nbsp;



â€œThese people are already going through something,â€ he says. â€œDo we want them to go on feeling even more alone, or potentially be manipulated by a system we know to be sycophantic to the extent of leading people to die by suicide and commit crimes? Thatâ€™s one of the cruxes here.â€
â€¢ Roundtables: Meet the 2025 Innovator of the Year
  MIT Technology Review selects one individual whose work we admire to recognize as Innovator of the Year . For 2025, we chose Sneha Goenka, who designed the computations behind the worldâ€™s fastest whole-genome sequencing method . Her work could transform medical care . The technology could help doctors diagnose a genetic condition in less than eight hours less than a day . The award was given to SnehaÂ Goenka for developing an ultra-fast sequencing technology .
â€¢ The Download: AIâ€™s retracted papers problem
  This is today&#8217;s edition of&nbsp;The Download,&nbsp;our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



AI models are using material from retracted scientific papers



The news:&nbsp;Some AI chatbots rely on flawed research from retracted scientific papers to answer questions, according to recent studies. In one such study, researchers asked OpenAIâ€™s ChatGPT questions based on information from 21 retracted papers on medical imaging. The chatbotâ€™s answers referenced retracted papers in five cases but advised caution in only three.&nbsp;



The bigger picture:&nbsp;The findings raise serious questions about how reliable AI tools are at evaluating scientific research, or answering peopleâ€™s health queries. They could also complicate efforts to invest in AI tools for scientists. And itâ€™s not an easy problem to fix.&nbsp;Read the full story.



â€”Ananya







Join us at 1pm ET today to meet our Innovator of the Year



Every year, MIT Technology Review awards Innovator of the Year to someone whose work we admire. This year we selected Sneha Goenka, who designed the computations behind&nbsp;the worldâ€™s fastest whole-genome sequencing method.



Her work could transform medical care by allowing physicians to sequence a patientâ€™s genome and diagnose genetic conditions in less than eight hours.



Register here&nbsp;to join an exclusive subscriber-only Roundtable conversation with Goenka, Leilani Battle, assistant professor at the University of Washington, and our editor in chief Mat Honan at 1pm ET today.&nbsp;







The must-reads



Iâ€™ve combed the internet to find you todayâ€™s most fun/important/scary/fascinating stories about technology.



1 Thereâ€™s scant evidence tylenol use during pregnancy causes autismThe biggest cause of autism is geneticâ€”thatâ€™s why it often runs in families. (Scientific American $)+ Anti-vaxxers are furious the White House didnâ€™t link autism to vaccines. (Ars Technica)+ The company that sells Tylenol is being forced to defend the medicineâ€™s safety. (Axios)2 Nvidia is investing up to $100 billion in OpenAIOpenAI is already a major customer, but this will bind the two even more closely together. (Reuters $)+ Americaâ€™s top companies keep talking about AIâ€”but they canâ€™t explain its upsides. (FT $)3 Denmarkâ€™s biggest airport was shut down by dronesIts prime minister refused to rule out Russian involvement. (FT $)+ Poland and Estonia have been speaking up at the UN about Russian incursions into their airspace. (The Guardian)4 Google is facing another antitrust trial in the USThis one will focus on remedies to its dominance of the advertising tech market. (Ars Technica)+ The FTC is also taking Amazon to court over accusations the company tricks people into paying for Prime. (NPR)+ The Supreme Court has ruled to allow Trumpâ€™s firing of a Democrat FTC commissioner. (NYT $)5 Hereâ€™s the potential impact of Trumpâ€™s H-1B crackdown on techItâ€™s likely to push a lot of skilled workers elsewhere. (Rest of World)



6 How TikTokâ€™s deal to stay in the US will workOracle will manage its algorithm for US users and oversee security operations. (ABC)+ Itâ€™s a giant prize for Trumpâ€™s friend Larry Ellison, Oracleâ€™s cofounder. (NYT $)+ Trump and his allies are now likely to exert a lot of political influence over TikTok. (WP $)7 Record labels are escalating their lawsuit against an AI music startupThey claim it knowingly pirated songs from YouTube to train its generative AI models. (The Verge $)+ AI is coming for music, too. (MIT Technology Review)&nbsp;8 Thereâ€™s a big fight in the US over who pays for weight loss drugsAlthough theyâ€™ll save insurers money long-term, they cost a lot upfront. (WP $)+ Weâ€™re learning more about what weight-loss drugs do to the body. (MIT Technology Review)9 How a lone vigilante ended up blowing up 5G towersA little bit of knowledge can be a dangerous thing. (Wired $)10 The moon is rusting&nbsp;And itâ€™s our fault. Awkward! (Nature)







Quote of the day



â€œAt the heart of this is people trying to look for simple answers to complex problems.â€



â€”James Cusack, chief executive of an autism charity called Autistica, tellsÂ NatureÂ what he thinks is driving Trump and others to incorrectly link the condition with Tylenol use during pregnancy.Â 







One more thing



SARAH ROGERS / MITTR | PHOTOS GETTY




Maybe you will be able to live past 122



How long can humans live? This is a good time to ask the question. The longevity scene is having a moment, and a few key areas of research suggest that we might be able to push human life spans further, and potentially reverse at least some signs of aging.



Researchers canâ€™t even agree on what the exact mechanisms of aging are and which they should be targeting. Debates continue to rage over how long itâ€™s possible for humans to liveâ€”and whether there is a limit at all.



But it looks likely that something will be developed in the coming decades that will help us live longer, in better health.&nbsp;Read the full story.



â€”Jessica Hamzelou







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)



+ This website lets you send aÂ letterÂ to your future self.Â + Hereâ€™s whatÂ Brian EnoÂ has to say about art.+ ThisÂ photographerÂ takes stunning pictures of Greenland.Â + The Hungarian dishÂ Rakott krumpliÂ isnâ€™t going to win any health plaudits, but it looks very comforting all the same.
â€¢ AI models are using material from retracted scientific papers
  Some AI chatbots rely on flawed research from retracted scientific papers to answer questions, according to recent studies. The findings, confirmed by MIT Technology Review, raise questions about how reliable AI tools are at evaluating scientific research and could complicate efforts by countries and industries seeking to invest in AI tools for scientists.



AI search tools and chatbots are already known to fabricate links and references. But answers based on the material from actual papers can mislead as well if those papers have been retracted.Â The chatbot is â€œusing a real paper, real material, to tell you something,â€ says Weikuan Gu, a medical researcher at the University of Tennessee in Memphis and an author of one of the recent studies. But, he says, if people only look at the content of the answer and do not click through to the paper and see that itâ€™s been retracted, thatâ€™s really a problem.Â 



Gu and his team asked OpenAIâ€™s ChatGPT, running on the GPT-4o model, questions based on information from 21 retracted papers about medical imaging. The chatbotâ€™s answers referenced retracted papers in five cases but advised caution in only three. While it cited non-retracted papers for other questions, the authors note that it may not have recognized the retraction status of the articles. In a study from August, a different group of researchers used ChatGPT-4o mini to evaluate the quality of 217 retracted and low-quality papers from different scientific fields; they found that none of the chatbotâ€™s responses mentioned retractions or other concerns. (No similar studies have been released on GPT-5, which came out in August.)



The public uses AI chatbots to ask for medical advice and diagnose health conditions. Students and scientists increasingly use science-focused AI tools to review existing scientific literature and summarize papers. That kind of usage is likely to increase. The US National Science Foundation, for instance, invested $75 million in building AI models for science research this August.





â€œIf [a tool is] facing the general public, then using retraction as a kind of quality indicator is very important,â€ says Yuanxi Fu, an information science researcher at the University of Illinois Urbana-Champaign. Thereâ€™s â€œkind of an agreement that retracted papers have been struck off the record of science,â€ she says, â€œand the people who are outside of scienceâ€”they should be warned that these are retracted papers.â€ OpenAI did not provide a response to a request for comment about the paper results.



The problem is not limited to ChatGPT. In June, MIT Technology Review tested AI tools specifically advertised for research work, such as Elicit, Ai2 ScholarQA (now part of the Allen Institute for Artificial Intelligenceâ€™s Asta tool), Perplexity, and Consensus, using questions based on the 21 retracted papers in Guâ€™s study. Elicit referenced five of the retracted papers in its answers, while Ai2 ScholarQA referenced 17, Perplexity 11, and Consensus 18â€”all without noting the retractions.



Some companies have since made moves to correct the issue. â€œUntil recently, we didnâ€™t have great retraction data in our search engine,â€ says Christian Salem, cofounder of Consensus. His company has now started using retraction data from a combination of sources, including publishers and data aggregators, independent web crawling, and Retraction Watch, which manually curates and maintains a database of retractions. In a test of the same papers in August, Consensus cited only five retracted papers.&nbsp;



Elicit told MIT Technology Review that it removes retracted papers flagged by the scholarly research catalogue OpenAlex from its database and is â€œstill working on aggregating sources of retractions.â€ Ai2 told us that its tool does not automatically detect or remove retracted papers currently. Perplexity said that it â€œ[does] not ever claim to be 100% accurate.â€&nbsp;



However, relying on retraction databases may not be enough. Ivan Oransky, the cofounder of Retraction Watch, is careful not to describe it as a comprehensive database, saying that creating one would require more resources than anyone has: â€œThe reason itâ€™s resource intensive is because someone has to do it all by hand if you want it to be accurate.â€



Further complicating the matter is that publishers donâ€™t share a uniform approach to retraction notices. â€œWhere things are retracted, they can be marked as such in very different ways,â€ says Caitlin Bakker from University of Regina, Canada, an expert in research and discovery tools. â€œCorrection,â€ â€œexpression of concern,â€ â€œerratum,â€ and â€œretractedâ€ are among some labels publishers may add to research papersâ€”and these labels can be added for many reasons, including concerns about the content, methodology, and data or the presence of conflicts of interest.&nbsp;



Some researchers distribute their papers on preprint servers, paper repositories, and other websites, causing copies to be scattered around the web. Moreover, the data used to train AI models may not be up to date. If a paper is retracted after the modelâ€™s training cutoff date, its responses might not instantaneously reflect what&#8217;s going on, says Fu. Most academic search engines donâ€™t do a real-time check against retraction data, so you are at the mercy of how accurate their corpus is, says Aaron Tay, a librarian at Singapore Management University.



Oransky and other experts advocate making more context available for models to use when creating a response. This could mean publishing information that already exists, like peer reviews commissioned by journals and critiques from the review site PubPeer, alongside the published paper.&nbsp;&nbsp;



Many publishers, such as Nature and the BMJ, publish retraction notices as separate articles linked to the paper, outside paywalls. Fu says companies need to effectively make use of such information, as well as any news articles in a modelâ€™s training data that mention a paperâ€™s retraction.&nbsp;



The users and creators of AI tools need to do their due diligence. â€œWe are at the very, very early stages, and essentially you have to be skeptical,â€ says Tay.



Ananya is a freelance science and technology journalist based in Bengaluru, India.

ðŸ”’ Cybersecurity & Privacy
No updates.

ðŸŽ“ University AI
No updates.

ðŸ¢ Corporate AI
â€¢ Using AI to assist in rare disease diagnosis
  In the promising and rapidly evolving field of genetic analysis, the ability to accurately interpret whole genome sequencing data is crucial for diagnosing and improving outcomes for people with rare genetic diseases. Yet despite technological advancements, genetic professionals face steep challenges in managing and synthesizing the vast amounts of data required for these analyses. Fewer than 50% of&nbsp;initial&nbsp;cases yield a diagnosis, and while reanalysis can lead to new findings, the process remains&nbsp;time-consuming and complex.&nbsp;



To better understand and address these challenges, Microsoft Researchâ€”in collaboration with Drexel University and the Broad Instituteâ€‹â€‹â€”conducted a comprehensive study titledÂ AI-Enhanced Sensemaking: Exploring the Design of a Generative AI-Based Assistant to Support Genetic Professionals (opens in new tab).Â The study was recently published in a special edition ofÂ ACM Transactions on Interactive Intelligent SystemsÂ journal focused on generative AI.Â Â 



The study focused on integrating generative AI to support the complex, time-intensive, and information-dense sensemaking tasks inherent in whole genome sequencing analysis. Through detailed empirical research and collaborative design sessions with experts in the field, we identified key obstacles genetic professionals face and proposed AI-driven solutions to enhance their workflows.&nbsp;â€‹&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;â€‹We&nbsp;developed strategies for how generative AI can help synthesize biomedical data, enabling AI-expert collaboration to increase the diagnoses of previously unsolved rare diseasesâ€”ultimately aiming to improve patientsâ€™ quality of life and life expectancy.



Whole genome sequencing in rare disease diagnosis



Rare diseases affect up to half a billion people globally and obtaining a diagnosis can take multiple years. These diagnoses often involve specialist consultations, laboratory tests, imaging studies, and invasive procedures. Whole genome sequencing is used to identify genetic variants responsible for these diseases by comparing a patientâ€™s DNA sequence to reference genomes.&nbsp;â€‹â€‹Genetic professionals use bioinformatics tools such as&nbsp;seqr,&nbsp;an open-source, web-based tool for rare disease case analysis and project management to assist them in filtering and prioritizing&nbsp; > 1 million variants to determine their potential role in disease.&nbsp;A critical component of&nbsp;their&nbsp;work is sensemaking: the process of searching, filtering, and synthesizing data to build, refine, and present models from complex sets of gene and variant information.&nbsp;&nbsp;



â€‹â€‹The multi-step sequencing processâ€‹â€‹â€‹&nbsp;typically takes three to 12 weeks and requires extensive amounts of evidence and time to synthesize and aggregate information&nbsp;â€‹â€‹to understand the gene and variant effects for the patient.&nbsp;If a patient&#8217;s case goes unsolved, their whole genome sequencing data is set aside until enough time has passed to warrant a reanalysisâ€‹â€‹. This creates a backlog of patient casesâ€‹â€‹. The ability to easily&nbsp;identify&nbsp;when new scientific evidence&nbsp;emerges&nbsp;and when to reanalyze an unsolved patient case is key to shortening the time patients suffer with an unknown rare disease diagnosis.&nbsp;



The promise of AI systems to assist with complex human tasks



Approximately 87% of AI systems never reach deployment&nbsp;â€‹simply because they solveâ€‹â€‹â€‹&nbsp;the wrong problems.&nbsp;â€‹â€‹Understanding the AI support desired by different types of professionals, their current workflows, and AI capabilities is critical to successful AI system deployment and use. Matching technology capabilities with user tasks is particularly challenging in AI design because AI models can generate numerous outputs, and their capabilities can be unclear.&nbsp;â€‹To design an effectiveâ€‹â€‹â€‹&nbsp;AI-based systemâ€‹, one needs to identifyâ€‹&nbsp;â€‹â€‹tasks AI can support,&nbsp;â€‹â€‹determineâ€‹â€‹â€‹â€‹â€‹â€‹&nbsp;the appropriate level of AI involvement, and&nbsp;â€‹â€‹designâ€‹â€‹â€‹â€‹â€‹â€‹&nbsp;user-AI interactions. This necessitates considering how humans interact with technology and how&nbsp;â€‹â€‹AI&nbsp;can best be incorporated into workflows and tools.



	
		

		
		Spotlight: Microsoft research newsletter
	
	
	
						
				
					
				
			
			
			

									Microsoft Research Newsletter
				
								Stay connected to the research community at Microsoft.
				
								
					
						
							Subscribe today						
					
				
							
	
Opens in a new tab	
	


Study objectives and co-designing a genetic AI assistant



Our study aimed to understand the current challenges and needs of genetic professionals performing whole genome sequencing analyses and explore the tasks where they want an AI assistant to support them in their work. The first phase of our study involved interviews with 17 genetics professionals to better understand their workflows, tools, and challenges. They included genetic analysts directly involved in interpreting data, as well as other roles participating in whole genome sequencing. In the second phase of our study, we conducted co-design sessions with study participants on how an AI assistant could support their workflows. We then developed a prototype of an AI assistant, which was further tested and refined with study participants in follow-up design walk-through sessions.



Identifying challenges in whole genome sequencing analysis



Through our in-depth interviews with genetic professionals, our study uncovered three critical challenges in whole genome sequencing analysis:




Information Overload: Genetic analysts need to gather and synthesize vast amounts of data from multiple sources. This task is incredibly time-consuming and prone to human error.



Collaborative Sharing: Sharing findings with others in the field can be cumbersome and inefficient, often relying on outdated methods that slow the collaborative analysis process.



Prioritizing Reanalysis: Given the continuous influx of new scientific discoveries, prioritizing unsolved cases to reanalyze is a daunting challenge. Analysts need a systematic approach to identify cases that might benefit most from reanalysis.




Genetic professionals highlighted the time-consuming nature of gathering and synthesizing information about genes and variants from different data sources. Other genetic professionals may have insights into certain genes and variants, but sharing and interpreting information with others for collaborative sensemaking requires significant time and effort. Although new scientific findings could affect unsolved cases through reanalysis, prioritizing cases based on new findings was challenging given the number of unsolved cases and limited time of genetic professionals.



Co-designing with experts and AI-human sensemaking tasks



Our study participants prioritized two potential tasks of an AI assistant. The first task was flagging cases for reanalysis based on new scientific findings. The assistant would alert analysts to unsolved cases that could benefit from new research, providing relevant updates drawn from recent publications. The second task focused on aggregating and synthesizing information about genes and variants from the scientific literature. This feature would compile essential information from numerous scientific papers about genes and variants, presenting it in a user-friendly format and saving analysts significant time and effort. Participants emphasized the need to balance selectivity with comprehensiveness in the evidence they review. They also envisioned collaborating with other genetic professionals to interpret, edit, and verify artifacts generated by the AI assistant.



Genetic professionals require both broad and focused evidence at different stages of their workflow. The AI assistant prototypes were designed to allow flexible filtering and thorough evidence aggregation, ensuring users can delve into comprehensive data or selectively focus on pertinent details. The prototypes included features for collaborative sensemaking, enabling users to interpret, edit, and verify AI-generated information collectively. This&nbsp;â€‹â€‹approach not only&nbsp;â€‹underscoresâ€‹â€‹â€‹&nbsp;the trustworthiness of AI outputs, but also facilitates shared understanding and decision-making among genetic professionals.



Design implications for expert-AI sensemaking



In the&nbsp;shifting frontiers of genome sequence analysis,&nbsp;leveraging generative AI to enhance sensemaking offers intriguing possibilitiesâ€‹â€‹. The task of staying&nbsp;â€‹â€‹currentâ€‹â€‹â€‹â€‹â€‹â€‹, synthesizing information from diverse sources, and making informed decisions&nbsp;â€‹â€‹is challengingâ€‹â€‹â€‹â€‹â€‹â€‹.&nbsp;&nbsp;



Our study participants emphasized the hurdles in integrating data from multiple sources without losing critical components, documenting decision rationales, and fostering collaborative environments. Generative AI models, with their advanced capabilities, have started to address these challenges by automatically generating interactive artifacts to support sensemaking. However, the effectiveness of such systems hinges on careful design considerations,&nbsp;â€‹â€‹particularly in how they facilitate distributed sensemaking, support both initial and ongoing sensemaking, and combine evidence from multiple modalities. We next discuss three design considerations for using generative AI models to support sensemaking.



Distributed expert-AI sensemaking design



Generative AI models can create artifacts that aid an individual user&#8217;s sensemaking process; however, the true potential lies in sharing these artifacts among users to foster collective understanding and efficiency. Participants in our study emphasized the importance of explainability, feedback, and trust when interacting with AI-generated content.&nbsp;â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹Trust is gained byâ€‹â€‹â€‹â€‹â€‹â€‹&nbsp;viewing portions of artifacts marked as correct by other users, or observing edits made to AI-generated informationâ€‹â€‹.&nbsp;â€‹â€‹Someâ€‹â€‹â€‹â€‹â€‹â€‹&nbsp;usersâ€‹, however,â€‹&nbsp;cautioned against over-reliance on AI, which could obscure underlying inaccuracies. Thus, design strategies should ensure that any corrections are clearly marked&nbsp;â€‹â€‹and annotatedâ€‹â€‹â€‹â€‹â€‹â€‹. Furthermore, to enhance distributed sensemaking, visibility of others&#8217; notes and context-specific synthesis through AI can streamline the processâ€‹â€‹.&nbsp;



Initial expert-AI sensemaking and re-sensemaking design



In our fast-paced, information-driven world,&nbsp;â€‹â€‹it is essential to understand a situation both&nbsp;initially&nbsp;and again when new information arises.â€‹â€‹&nbsp;â€‹â€‹Sensemaking is inherently temporal, reflecting and shaping our understanding of time as we revisit tasks to reevaluate past decisions or incorporate new information. Generative AI plays a pivotal role here by transforming static data into dynamic artifacts that evolve, offering a comprehensive view of past rationales. Such AI-generated artifacts provide continuity, allowing usersâ€”both&nbsp;original decision-makers or new individualsâ€”to access the rationale behind decisions made in earlier task instances. By continuously editing and updating these artifacts, generative AI highlights new information since the last review, supporting ongoing understanding and decision-making.&nbsp;Moreover, AI systems enhance&nbsp;â€‹â€‹transparencyâ€‹â€‹â€‹â€‹â€‹â€‹&nbsp;by summarizing previous notes and questions, offering insights into earlier thought processes and facilitating a deeper understanding of how conclusions were drawn. This reflective capability not only can reinforce initial sensemaking efforts but also equips users with the clarity needed for informed re-sensemaking as new data emerges.&nbsp;



Combining evidence from multiple modalities to enhance AI-expert sensemaking



â€‹â€‹â€‹Theâ€‹â€‹â€‹â€‹â€‹â€‹&nbsp;ability to combine evidence from multiple modalities is essential for effective sensemaking. Users often need to integrate diverse types of dataâ€”text, images, spatial coordinates, and moreâ€”into a coherent narrative to make informed decisions. Consider the case of search and rescue operations, where workers must rapidly synthesize information from texts, photographs, and GPS data to strategize their efforts. Recent advancements in multimodal generative AI models have empowered users by incorporating and synthesizing these varied inputs into a unified, comprehensive view. For instance, a participant in our study illustrated this capability by using a generative AI model to merge text from scientific publications with a visual gene structure depiction. This integration&nbsp;â€‹â€‹could createâ€‹â€‹â€‹â€‹â€‹â€‹&nbsp;an image that contextualizes an individual&#8217;s genetic variant within the&nbsp;â€‹â€‹contextâ€‹â€‹â€‹â€‹â€‹â€‹&nbsp;of documented variants. Such advanced synthesis enables users to capture complex relationships and insights briefly, streamlining decision-making and expanding the potential for innovative solutions across diverse fields.&nbsp;



Sensemaking Process with AI Assistant



Figure: Sensemaking process when interpreting variants with the introduction of prototype AI assistant. Gray boxes represent sensemaking activities which are currently performed by an analyst but are human-in-the-loop processes with involvement of our prototype AI assistant. Non-gray boxes represent activities reserved for analyst completion without assistance by our AI assistant prototype. Within the foraging searching and synthesizing processes, examples of data sources and data types for each, respectively, are connected by dotted lines.



Conclusion



We explored the potential of generative AI&nbsp;to supportâ€‹â€‹ genetic professionalsâ€‹&nbsp;â€‹in diagnosing rare diseasesâ€‹â€‹. By designing an AI-based assistant, we aim to streamline whole genome sequencing analysis, helping professionals diagnose rare genetic diseases more efficiently. Our study unfolded in two key phases:&nbsp;â€‹pinpointingâ€‹â€‹â€‹&nbsp;existing challenges in analysis, and design ideation, where we crafted a prototype AI assistant. This tool is designed to boost diagnostic yield and cut down diagnosis time by flagging cases for reanalysis and synthesizing crucial gene and variant data. Despite valuable findings, more research is neededâ€‹â€‹. Future research will involve testing the AI assistant in real-time, task-based user testing with genetic professionals to assess the AI&#8217;s impact on their workflow. The promise of AI advancements lies in solving the right user problems and building the appropriate solutions, achieved through collaboration among model developers, domain experts, system designers, and HCI researchers. By fostering these collaborations, we aim to develop robust, personalized AI assistants tailored to specific domains.&nbsp;



Join the conversation



Join us as we continue to explore the transformative potential of generative AI in genetic analysis, and please read the full text publication&nbsp;here (opens in new tab). Follow us on social media, share this post with your network, and let us know your thoughts on how AI can transform genetic research. If interested in our other related research work, check out&nbsp;Evidence Aggregator: AI reasoning applied to rare disease diagnosis. (opens in new tab)&nbsp;&nbsp;




Opens in a new tabThe post Using AI to assist in rare disease diagnosis appeared first on Microsoft Research.
â€¢ Running deep research AI agents on Amazon Bedrock AgentCore
  AI agents are evolving beyond basic single-task helpers into more powerful systems that can plan, critique, and collaborate with other agents to solve complex problems. Deep Agentsâ€”a recently introduced framework built on LangGraphâ€”bring these capabilities to life, enabling multi-agent workflows that mirror real-world team dynamics. The challenge, however, is not just building such agents but also running them reliably and securely in production. This is where Amazon Bedrock AgentCore Runtime comes in. By providing a secure, serverless environment purpose-built for AI agents and tools, Runtime makes it possible to deploy Deep Agents at enterprise scale without the heavy lifting of managing infrastructure. 
In this post, we demonstrate how to deploy Deep Agents on AgentCore Runtime. As shown in the following figure, AgentCore Runtime scales any agent and provides session isolation by allocating a new microVM for each new session. 
 
What is Amazon Bedrock AgentCore? 
Amazon Bedrock AgentCore is both framework-agnostic and model-agnostic, giving you the flexibility to deploy and operate advanced AI agents securely and at scale. Whether youâ€™re building with Strands Agents, CrewAI, LangGraph, LlamaIndex, or another frameworkâ€”and running them on a large language model (LLM)â€”AgentCore provides the infrastructure to support them. Its modular services are purpose-built for dynamic agent workloads, with tools to extend agent capabilities and controls required for production use. By alleviating the undifferentiated heavy lifting of building and managing specialized agent infrastructure, AgentCore lets you bring your preferred framework and model and deploy without rewriting code. 
Amazon Bedrock AgentCore offers a comprehensive suite of capabilities designed to transform local agent prototypes into production-ready systems. These include persistent memory for maintaining context in and across conversations, access to existing APIs using Model Context Protocol (MCP), seamless integration with corporate authentication systems, specialized tools for web browsing and code execution, and deep observability into agent reasoning processes. In this post, we focus specifically on the AgentCore Runtime component. 
Core capabilities of AgentCore Runtime 
AgentCore Runtime provides a serverless, secure hosting environment specifically designed for agentic workloads. It packages code into a lightweight container with a simple, consistent interface, making it equally well-suited for running agents, tools, MCP servers, or other workloads that benefit from seamless scaling and integrated identity management.AgentCore Runtime offers extended execution times up to 8 hours for complex reasoning tasks, handles large payloads for multimodal content, and implements consumption-based pricing that charges only during active processingâ€”not while waiting for LLM or tool responses. Each user session runs in complete isolation within dedicated micro virtual machines (microVMs), maintaining security and helping to prevent cross-session contamination between agent interactions. The runtime works with many frameworks (for example: LangGraph, CrewAI, Strands, and so on) and many foundation model providers, while providing built-in corporate authentication, specialized agent observability, and unified access to the broader AgentCore environment through a single SDK. 
Real-world example: Deep Agents integration 
In this post weâ€™re going to deploy the recently released Deep Agents implementation example on AgentCore Runtimeâ€”showing just how little effort it takes to get the latest agent innovations up and running. 
 
The sample implementation in the preceding diagram includes: 
 
 A research agent that conducts deep internet searches using the Tavily API 
 A critique agent that reviews and provides feedback on generated reports 
 A main orchestrator that manages the workflow and handles file operations 
 
Deep Agents uses LangGraphâ€™s state management to create a multi-agent system with: 
 
 Built-in task planning through a write_todos tool that helps agents break down complex requests 
 Virtual file system where agents can read/write files to maintain context across interactions 
 Sub-agent architecture allowing specialized agents to be invoked for specific tasks while maintaining context isolation 
 Recursive reasoning with high recursion limits (more than 1,000) to handle complex, multi-step workflows 
 
This architecture enables Deep Agents to handle research tasks that require multiple rounds of information gathering, synthesis, and refinement.The key integration points in our code showcase how agents work with AgentCore. The beauty is in its simplicityâ€”we only need to add a couple of lines of code to make an agent AgentCore-compatible: 
 
 # 1. Import the AgentCore runtime
from bedrock_agentcore.runtime import BedrockAgentCoreApp
app = BedrockAgentCoreApp()

# 2. Decorate your agent function with @app.entrypoint
@app.entrypoint
async def langgraph_bedrock(payload):
    # Your existing agent logic remains unchanged
    user_input = payload.get("prompt")
    
    # Call your agent as before
    stream = agent.astream(
        {"messages": [HumanMessage(content=user_input)]},
        stream_mode="values"
    )
    
    # Stream responses back
    async for chunk in stream:
        yield(chunk)

# 3. Add the runtime starter at the bottom
if __name__ == "__main__":
    app.run() 
 
Thatâ€™s it! The rest of the codeâ€”model initialization, API integrations, and agent logicâ€”remains exactly as it was. AgentCore handles the infrastructure while your agent handles the intelligence. This integration pattern works for most Python agent frameworks, making AgentCore truly framework-agnostic. 
Deploying to AgentCore Runtime: Step-by-step 
Letâ€™s walk through the actual deployment process using the AgentCore Starter ToolKit, which dramatically simplifies the deployment workflow. 
Prerequisites 
Before you begin, make sure you have: 
 
 Python 3.10 or higher 
 AWS credentials configured 
 Amazon Bedrock AgentCore SDK installed 
 
Step 1: IAM permissions 
There are two different AWS Identity and Access Management (IAM) permissions you need to consider when deploying an agent in an AgentCore Runtimeâ€”the role you, as a developer use to create AgentCore resources and the execution role that an agent needs to run in an AgentCore Runtime. While the latter role can now be auto-created by the AgentCore Starter Toolkit (auto_create_execution_role=True), the former must be defined as described in IAM Permissions for AgentCore Runtime. 
Step 2: Add a wrapper to your agent 
As shown in the preceding Deep Agents example, add the AgentCore imports and decorator to your existing agent code. 
Step 3: Deploy using the AgentCore starter toolkit 
The starter toolkit provides a three-step deployment process: 
 
 from bedrock_agentcore_starter_toolkit import Runtime

# Step 1: Configure
agentcore_runtime = Runtime()
config_response = agentcore_runtime.configure(
    entrypoint="hello.py", # contains the code we showed earlier in the post
    execution_role=role_arn, # or auto-create
    auto_create_ecr=True,
    requirements_file="requirements.txt",
    region="us-west-2",
    agent_name="deepagents-research"
)

# Step 2: Launch
launch_result = agentcore_runtime.launch()
print(f"Agent deployed! ARN: {launch_result['agent_arn']}")

# Step 3: Invoke
response = agentcore_runtime.invoke({
    "prompt": "Research the latest developments in quantum computing"
}) 
 
Step 4: What happens behind the scenes 
When you run the deployment, the starter kit automatically: 
 
 Generates an optimized Docker file with Python 3.13-slim base image and OpenTelemetry instrumentation 
 Builds your container with the dependencies from requirements.txt 
 Creates an Amazon Elastic Container Registry (Amazon ECR) repository (if auto_create_ecr=True) and pushes your image 
 Deploys to AgentCore Runtime and monitors the deployment status 
 Configures networking and observability with Amazon CloudWatch and AWS X-Ray integration 
 
The entire process typically takes 2â€“3 minutes, after which your agent is ready to handle requests at scale. Each new session is launched in its own fresh AgentCore Runtime microVM, maintaining complete environment isolation. 
The starter kit generates a configuration file (.bedrock_agentcore.yaml) that captures your deployment settings, making it straightforward to redeploy or update your agent later. 
Invoking your deployed agent 
After deployment, you have two options for invoking your agent: 
Option 1: Using the start kit (shown in Step 3) 
 
 response = agentcore_runtime.invoke({
    "prompt": "Research the latest developments in quantum computing"
})
 
 
Option 2: Using boto3 SDK directly 
 
 import boto3
import json

agentcore_client = boto3.client('bedrock-agentcore', region_name='us-west-2')
response = agentcore_client.invoke_agent_runtime(
    agentRuntimeArn=agent_arn,
    qualifier="DEFAULT",
    payload=json.dumps({
        "prompt": "Analyze the impact of AI on healthcare in 2024"
    })
)

# Handle streaming response
for event in response['completion']:
    if 'chunk' in event:
        print(event['chunk']['bytes'].decode('utf-8')) 
 
Deep Agents in action 
As the code executes in Bedrock AgentCore Runtime, the primary agent orchestrates specialized sub-agentsâ€”each with its own purpose, prompt, and tool accessâ€”to solve complex tasks more effectively. In this case, the orchestrator prompt (research_instructions) sets the plan: 
 
 Write the question to question.txt 
 Fan out to one or more research-agent calls (each on a single sub-topic) using the internet_search tool 
 Synthesize findings into final_report.md 
 Call critique-agent to evaluate gaps and structure 
 Optionally loop back to more research/edits until quality is met 
 
Here it is in action: 

 
  
 
 
Clean up 
When finished, donâ€™t forget to de-allocate provisioned AgentCore Runtime in addition to the container repository that was created during the process: 
 
 agentcore_control_client = boto3.client(
    'bedrock-agentcore-control', region_name=region )
ecr_client = boto3.client('ecr',region_name=region )
runtime_delete_response = agentcore_control_client.delete_agent_runtime(    agentRuntimeId=launch_result.agent_id,)
response = ecr_client.delete_repository(
    repositoryName=launch_result.ecr_uri.split('/')[1],force=True)
 
 
Conclusion 
Amazon Bedrock AgentCore represents a paradigm shift in how we deploy AI agents. By abstracting away infrastructure complexity while maintaining framework and model flexibility, AgentCore enables developers to focus on building sophisticated agent logic rather than managing deployment pipelines. Our Deep Agents deployment demonstrates that even complex, multi-agent systems with external API integrations can be deployed with minimal code changes. The combination of enterprise-grade security, built-in observability, and serverless scaling makes AgentCore the best choice for production AI agent deployments. Specifically for deep research agents, AgentCore offers the following unique capabilities that you can explore: 
 
 AgentCore Runtime can handle asynchronous processing and long running (up to 8 hours) agents. Asynchronous tasks allow your agent to continue processing after responding to the client and handle long-running operations without blocking responses. Your background research sub-agent could be asynchronously researching for hours. 
 AgentCore Runtime works with AgentCore Memory, enabling capabilities such as building upon previous findings, remembering research preferences, and maintaining complex investigation context without losing progress between sessions. 
 You can use AgentCore Gateway to extend your deep research to include proprietary insights from enterprise services and data sources. By exposing these differentiated resources as MCP tools, your agents can quickly take advantage and combine that with publicly available knowledge. 
 
Ready to deploy your agents to production? Hereâ€™s how to get started: 
 
 Install the AgentCore starter kit: pip install bedrock-agentcore-starter-toolkit 
 Experiment: Deploy your code by following this step by step guide. 
 
The era of production-ready AI agents is here. With AgentCore, the journey from prototype to production has never been shorter. 
 
About the authors 
Vadim Omeltchenko is a Sr. AI/ML Solutions Architect who is passionate about helping AWS customers innovate in the cloud. His prior IT experience was predominantly on the ground. 
Eashan Kaushik is a Specialist Solutions Architect AI/ML at Amazon Web Services. He is driven by creating cutting-edge generative AI solutions while prioritizing a customer-centric approach to his work. Before this role, he obtained an MS in Computer Science from NYU Tandon School of Engineering. Outside of work, he enjoys sports, lifting, and running marathons. 
Shreyas Subramanian is a Principal data scientist and helps customers by using Machine Learning to solve their business challenges using the AWS platform. Shreyas has a background in large scale optimization and Machine Learning, and in use of Machine Learning and Reinforcement Learning for accelerating optimization tasks. 
Mark Roy is a Principal Machine Learning Architect for AWS, helping customers design and build generative AI solutions. His focus since early 2023 has been leading solution architecture efforts for the launch of Amazon Bedrock, the flagship generative AI offering from AWS for builders. Markâ€™s work covers a wide range of use cases, with a primary interest in generative AI, agents, and scaling ML across the enterprise. He has helped companies in insurance, financial services, media and entertainment, healthcare, utilities, and manufacturing. Prior to joining AWS, Mark was an architect, developer, and technology leader for over 25 years, including 19 years in financial services. Mark holds six AWS Certifications, including the ML Specialty Certification.
â€¢ Integrate tokenization with Amazon Bedrock Guardrails for secure data handling
  This post is co-written by Mark Warner, Principal Solutions Architect for Thales, Cyber Security Products. 
As generative AI applications make their way into production environments, they integrate with a wider range of business systems that process sensitive customer data. This integration introduces new challenges around protecting personally identifiable information (PII) while maintaining the ability to recover original data when legitimately needed by downstream applications. Consider a financial services company implementing generative AI across different departments. The customer service team needs an AI assistant that can access customer profiles and provide personalized responses that include contact information, for example: â€œWeâ€™ll send your new card to your address at 123 Main Street.â€ Meanwhile, the fraud analysis team requires the same customer data but must analyze patterns without exposing actual PII, working only with protected representations of sensitive information. 
Amazon Bedrock Guardrails helps detect sensitive information, such as PII, in standard format in input prompts or model responses. Sensitive information filters give organizations control over how sensitive data is handled, with options to block requests containing PII or mask the sensitive information with generic placeholders like {NAME} or {EMAIL}. This capability helps organizations comply with data protection regulations while still using the power of large language models (LLMs). 
Although masking effectively protects sensitive information, it creates a new challenge: the loss of data reversibility. When guardrails replace sensitive data with generic masks, the original information becomes inaccessible to downstream applications that might need it for legitimate business processes. This limitation can impact workflows where both security and functional data are required. 
Tokenization offers a complementary approach to this challenge. Unlike masking, tokenization replaces sensitive data with format-preserving tokens that are mathematically unrelated to the original information but maintain its structure and usability. These tokens can be securely reversed back to their original values when needed by authorized systems, creating a path for secure data flows throughout an organizationâ€™s environment. 
In this post, we show you how to integrate Amazon Bedrock Guardrails with third-party tokenization services to protect sensitive data while maintaining data reversibility. By combining these technologies, organizations can implement stronger privacy controls while preserving the functionality of their generative AI applications and related systems. The solution described in this post demonstrates how to combine Amazon Bedrock Guardrails with tokenization services from Thales CipherTrust Data Security Platform to create an architecture that protects sensitive data without sacrificing the ability to process that data securely when needed. This approach is particularly valuable for organizations in highly regulated industries that need to balance innovation with compliance requirements. 
Amazon Bedrock Guardrails APIs 
This section describes the key components and workflow for the integration between Amazon Bedrock Guardrails and a third-party tokenization service. 
Amazon Bedrock Guardrails provides two distinct approaches for implementing content safety controls: 
 
 Direct integration with model invocation through APIs like InvokeModel and Converse, where guardrails automatically evaluate inputs and outputs as part of the model inference process. 
 Standalone evaluation through the ApplyGuardrail API, which decouples guardrails assessment from model invocation, allowing evaluation of text against defined policies. 
 
This post uses the ApplyGuardrail API for tokenization integration because it separates content assessment from model invocation, allowing for the insertion of tokenization processing between these steps. This separation creates the necessary space in the workflow to replace guardrail masks with format-preserving tokens before model invocation, or after the model response is handed over to the target application downstream in the process. 
The solution extends the typical ApplyGuardrail API implementation by inserting tokenization processing between guardrail evaluation and model invocation, as follows: 
 
 The application calls the ApplyGuardrail API to assess the user input for sensitive information. 
 If no sensitive information is detected (action = "NONE"), the application proceeds to model invocation via the InvokeModel API. 
 If sensitive information is detected (action = "ANONYMIZED"): 
   
   The application captures the detected PII and its positions. 
   It calls a tokenization service to convert these entities into format-preserving tokens. 
   It replaces the generic guardrail masks with these tokens. 
   The application then invokes the foundation model with the tokenized content. 
    
 For model responses: 
   
   The application applies guardrails to check the output from the model for sensitive information. 
   It tokenizes detected PII before passing the response to downstream systems. 
    
 
Solution overview 
To illustrate how this workflow delivers value in practice, consider a financial advisory application that helps customers understand their spending patterns and receive personalized financial recommendations. In this example, three distinct application components work together to provide secure, AI-powered financial insights: 
 
 Customer gateway service â€“ This trusted frontend orchestrator receives customer queries that often contain sensitive information. For example, a customer might ask: â€œHi, this is j.smith@example.com. Based on my last five transactions on acme.com, and my current balance of $2,342.18, should I consider their new credit card offer?â€ 
 Financial analysis engine â€“ This AI-powered component analyzes financial patterns and generates recommendations but doesnâ€™t need access to actual customer PII. It works with anonymized or tokenized information. 
 Response processing service â€“ This trusted service handles the final customer communication, including detokenizing sensitive information before presenting results to the customer. 
 
The following diagram illustrates the workflow for integrating Amazon Bedrock Guardrails with tokenization services in this financial advisory application. AWS Step Functions orchestrates the sequential process of PII detection, tokenization, AI model invocation, and detokenization across the three key components (customer gateway service, financial analysis engine, and response processing service) using AWS Lambda functions. 
 
The workflow operates as follows: 
 
 The customer gateway service (for this example, through Amazon API Gateway) receives the user input containing sensitive information. 
 It calls the ApplyGuardrail API to identify PII or other sensitive information that should be anonymized or blocked. 
 For detected sensitive elements (such as user names or merchant names), it calls the tokenization service to generate format-preserving tokens. 
 The input with tokenized values is passed to the financial analysis engine for processing. (For example, â€œHi, this is [[TOKEN_123]]. Based on my last five transactions on [[TOKEN_456]] and my current balance of $2,342.18, should I consider their new credit card offer?â€) 
 The financial analysis engine invokes an LLM on Amazon Bedrock to generate financial advice using the tokenized data. 
 The model response, potentially containing tokenized values, is sent to the response processing service. 
 This service calls the tokenization service to detokenize the tokens, restoring the original sensitive values. 
 The final, detokenized response is delivered to the customer. 
 
This architecture maintains data confidentiality throughout the processing flow while preserving the informationâ€™s utility. The financial analysis engine works with structurally valid but cryptographically protected data, allowing it to generate meaningful recommendations without exposing sensitive customer information. Meanwhile, the trusted components at the entry and exit points of the workflow can access the actual data when necessary, creating a secure end-to-end solution. 
In the following sections, we provide a detailed walkthrough of implementing the integration between Amazon Bedrock Guardrails and tokenization services. 
Prerequisites 
To implement the solution described in this post, you must have the following components configured in your environment: 
 
 An AWS account with Amazon Bedrock enabled in your target AWS Region. 
 Appropriate AWS Identity and Access Management (IAM) permissions configured following least privilege principles with specific actions enabled: bedrock:CreateGuardrail, bedrock:ApplyGuardrail, and bedrock-runtime:InvokeModel. 
 For AWS Organizations, verify Amazon Bedrock access is permitted by service control policies. 
 A Python 3.7+ environment with the boto3 library installed. For information about installing the boto3 library, refer to AWS SDK for Python (Boto3). 
 AWS credentials configured for programmatic access using the AWS Command Line Interface (AWS CLI). For more details, refer to Configuring settings for the AWS CLI. 
 This implementation requires a deployed tokenization service accessible through REST API endpoints. Although this walkthrough demonstrates integration with Thales CipherTrust, the pattern adapts to tokenization providers offering protect and unprotect API operations. Make sure network connectivity exists between your application environment and both AWS APIs and your tokenization service endpoints, along with valid authentication credentials for accessing your chosen tokenization service. For information about setting up Thales CipherTrust specifically, refer to How Thales Enables PCI DSS Compliance with a Tokenization Solution on AWS. 
 
Configure Amazon Bedrock Guardrails 
Configure Amazon Bedrock Guardrails for PII detection and masking through the Amazon Bedrock console or programmatically using the AWS SDK. Sensitive information filter policies can anonymize or redact information from model requests or responses: 
 
 import boto3
def create_bedrock_guardrail():
    """
    Create a guardrail in Amazon Bedrock for financial applications with PII protection.
    """
    bedrock = boto3.client('bedrock')
    
    response = bedrock.create_guardrail(
        name="FinancialServiceGuardrail",
        description="Guardrail for financial applications with PII protection",
        sensitiveInformationPolicyConfig={
            'piiEntitiesConfig': [
                {
                    'type': 'URL',
                    'action': 'ANONYMIZE',
                    'inputAction': 'ANONYMIZE',
                    'outputAction': 'ANONYMIZE',
                    'inputEnabled': True,
                    'outputEnabled': True
                },
                {
                    'type': 'EMAIL',
                    'action': 'ANONYMIZE',
                    'inputAction': 'ANONYMIZE',
                    'outputAction': 'ANONYMIZE',
                    'inputEnabled': True,
                    'outputEnabled': True
                },
                {
                    'type': 'NAME',
                    'action': 'ANONYMIZE',
                    'inputAction': 'ANONYMIZE',
                    'outputAction': 'ANONYMIZE',
                    'inputEnabled': True,
                    'outputEnabled': True
                }
            ]
        },
        blockedInputMessaging="I can't provide information with PII data.",
        blockedOutputsMessaging="I can't generate content with PII data."
    )
    
    return response 
 
Integrate the tokenization workflow 
This section implements the tokenization workflow by first detecting PII entities with the ApplyGuardrail API, then replacing the generic masks with format-preserving tokens from your tokenization service. 
Apply guardrails to detect PII entities 
Use the ApplyGuardrail API to validate input text from the user and detect PII entities: 
 
 import boto3
from botocore.exceptions import ClientError
def invoke_guardrail(user_query):
    """
    Apply Amazon Bedrock Guardrails to validate input text and detect PII entities.
    
    Args:
        user_query (str): The user's input text to be checked.
    
    Returns:
        dict: The response from the ApplyGuardrail API.
    
    Raises:
        ClientError: If there's an error applying the guardrail.
    """
    try:
        bedrock_runtime = boto3.client('bedrock-runtime')
        
        response = bedrock_runtime.apply_guardrail(
            guardrailIdentifier='your-guardrail-id', # Replace with your actual guardrail ID
            guardrailVersion='your-guardrail-version', # Replace with your actual version
            source="INPUT",
            content=[{"text": {"text": user_query}}]
        )
        
        return response
    except ClientError as e:
        print(f"Error applying guardrail: {e}")
        raise 
 
Invoke tokenization service 
The response from the ApplyGuadrail API includes the list of PII entities matching the sensitive information policy. Parse those entities and invoke the tokenization service to generate the tokens. 
The following example code uses the Thales CipherTrust tokenization service: 
 
 import json
import requests
from botocore.exceptions import ClientError
def thales_ciphertrust_tokenizer(guardrail_response):
  """
  Process PII entities detected by the guardrail and tokenize them using Thales CipherTrust
    
  Args:
      guardrail_response (dict): The response from the ApplyGuardrail API
    
  Returns:
      list: List of dictionaries containing original values, types, and tokenized responses
    
  Raises:
      ClientError: If there's an error invoking Thales CipherTrust.
  """
  try:
    protected_results = []
      
    for assessment in guardrail_response.get("assessments", []):
      pii_entities = assessment.get("sensitiveInformationPolicy", {}).get("piiEntities", [])
            
      for entity in pii_entities:
          sensitive_value = entity.get("match")
          entity_type = entity.get("type")
                
          if sensitive_value:
              # Prepare payload for Thales CipherTrust tokenization service
              crdp_payload = {
                  "protection_policy_name": "plain-alpha-internal",
                  "DATA_KEY": sensitive_value,
              }
                    
              url_str = "http://your-ciphertrust-cname:8090/v1/protect"  # Replace with your actual CipherTrust URL
              headers = {"Content-Type": "application/json"}
                    
              # Invoke the Thales CipherTrust tokenization service
              response = requests.post(url_str, headers=headers, data=json.dumps(crdp_payload))
              response.raise_for_status()
              response_json = response.json()
                    
              protected_results.append({
                   "original_value": sensitive_value,
                   "type": entity_type,
                   "protection_response": response_json
              })
        
    return protected_results
  except requests.RequestException as e:
    print(f"Error invoking Thales CipherTrust: {e}")
    raise ClientError(f"Error invoking Thales CipherTrust: {e}", "TokenizationError") 
 
Replace guardrail masks with tokens 
Next, substitute the generic guardrail masks with the tokens generated by the Thales CipherTrust tokenization service. This enables downstream applications to work with structurally valid data while maintaining security and reversibility. 
 
 def process_guardrail_output(protected_results, guardrail_response):
    """
    Process guardrail output by replacing placeholders with protected values.
    
    Args:
        protected_results (list): List of protected data tokenized by Thales CipherTrust.
        guardrail_response (dict): Guardrail response dictionary.
        
    Returns:
        list: List of modified output items with placeholders replaced by tokens.
    
    Raises:
        ValueError: If input parameters are invalid.
        Exception: For any unexpected errors during processing.
    """
    try:
        # Validate input types
        if not isinstance(protected_results, list) or not isinstance(guardrail_response, dict):
            raise ValueError("Invalid input parameters")
            
        # Extract protection map
        protection_map = {res['type'].upper(): res['protection_response']['protected_data'] 
                          for res in protected_results}
        # Process outputs 
        modified_outputs = []
        for output_item in guardrail_response.get('outputs', []):
            if 'text' in output_item:
                modified_text = output_item['text']
                
                # Replace all placeholders in one pass
                for pii_type, protected_value in protection_map.items():
                    modified_text = modified_text.replace(f"{{{pii_type}}}", protected_value)
                
                modified_outputs.append({"text": modified_text})
        return modified_outputs
    except (ValueError, KeyError) as e:
        print(f"Error processing guardrail output: {e}")
        raise
    except Exception as e:
        print(f"Unexpected error while processing guardrail output: {e}")
        raise 
 
The result of this process transforms user inputs containing information that match the sensitive information policy applied using Amazon Bedrock Guardrails into unique and reversible tokenized versions. 
The following example input contains PII elements: 
 
 "Hi, this is john.smith@example.com. Based on my last five transactions on acme.com, and my current balance of $2,342.18, should I consider their new credit card offer?" 
 
The following is an example of the sanitized user input: 
 
 "Hi, this is 1001000GC5gDh1.D8eK71@EjaWV.lhC. Based on my last five transactions on 1001000WcFzawG.Jc9Tfc, and my current balance of $2,342.18, should I consider their new credit card offer?" 
 
Downstream application processing 
The sanitized input is ready to be used by generative AI applications, including model invocations on Amazon Bedrock. In response to the tokenized input, an LLM invoked by the financial analysis engine would produce a relevant analysis that maintains the secure token format: 
 
 "Based on your recent transactions at 1001000WcFzawG.Jc9Tfc and your current account status, I can confirm that the new credit card offer would provide approximately $33 in monthly rewards based on your spending patterns. With annual benefits of around $394 against the $55 annual fee, this card would be beneficial for your profile, 1001000GC5gDh1.D8eK71@EjaWV.lhC." 
 
When authorized systems need to recover original values, tokens are detokenized. With Thales CipherTrust, this is accomplished using the Detokenize API, which requires the same parameters as in the previous tokenize action. This completes the secure data flow while preserving the ability to recover original information when needed. 
Clean up 
As you follow the approach described in this post, you will create new AWS resources in your account. To avoid incurring additional charges, delete these resources when you no longer need them. 
To clean up your resources, complete the following steps: 
 
 Delete the guardrails you created. For instructions, refer to Delete your guardrail. 
 If you implemented the tokenization workflow using Lambda, API Gateway, or Step Functions as described in this post, remove the resources you created. 
 This post assumes a tokenization solution is already available in your account. If you deployed a third-party tokenization solution (such as Thales CipherTrust) to test this implementation, refer to that solutionâ€™s documentation for instructions to properly decommission these resources and stop incurring charges. 
 
Conclusion 
This post demonstrated how to combine Amazon Bedrock Guardrails with tokenization to enhance handling of sensitive information in generative AI workflows. By integrating these technologies, organizations can protect PII during processing while maintaining data utility and reversibility for authorized downstream applications. 
The implementation illustrated uses Thales CipherTrust Data Security Platform for tokenization, but the architecture supports many tokenization solutions. To learn more about a serverless approach to building custom tokenization capabilities, refer to Building a serverless tokenization solution to mask sensitive data. 
This solution provides a practical framework for builders to use the full potential of generative AI with appropriate safeguards. By combining the content safety mechanisms of Amazon Bedrock Guardrails with the data reversibility of tokenization, you can implement responsible AI workflows that align with your application requirements and organizational policies while preserving the functionality needed for downstream systems. 
To learn more about implementing responsible AI practices on AWS, see Transform responsible AI from theory into practice. 
 
About the Authors 
Nizar Kheir is a Senior Solutions Architect at AWS with more than 15 years of experience spanning various industry segments. He currently works with public sector customers in France and across EMEA to help them modernize their IT infrastructure and foster innovation by harnessing the power of the AWS Cloud. 
Mark Warner is a Principal Solutions Architect for Thales, Cyber Security Products division. He works with companies in various industries such as finance, healthcare, and insurance to improve their security architectures. His focus is assisting organizations with reducing risk, increasing compliance, and streamlining data security operations to reduce the probability of a breach.
â€¢ Rapid ML experimentation for enterprises with Amazon SageMaker AI and Comet
  This post was written with Sarah Ostermeier from Comet. 
As enterprise organizations scale their machine learning (ML) initiatives from proof of concept to production, the complexity of managing experiments, tracking model lineage, and managing reproducibility grows exponentially. This is primarily because data scientists and ML engineers constantly explore different combinations of hyperparameters, model architectures, and dataset versions, generating massive amounts of metadata that must be tracked for reproducibility and compliance. As the ML model development scales across multiple teams and regulatory requirements intensify, tracking experiments becomes even more complex. With increasing AI regulations, particularly in the EU, organizations now require detailed audit trails of model training data, performance expectations, and development processes, making experiment tracking a business necessity and not just a best practice. 
Amazon SageMaker AI provides the managed infrastructure enterprises need to scale ML workloads, handling compute provisioning, distributed training, and deployment without infrastructure overhead. However, teams still need robust experiment tracking, model comparison, and collaboration capabilities that go beyond basic logging. 
Comet is a comprehensive ML experiment management platform that automatically tracks, compares, and optimizes ML experiments across the entire model lifecycle. It provides data scientists and ML engineers with powerful tools for experiment tracking, model monitoring, hyperparameter optimization, and collaborative model development. It also offers Opik, Cometâ€™s open source platform for LLM observability and development. 
Comet is available in SageMaker AI as a Partner AI App, as a fully managed experiment management capability, with enterprise-grade security, seamless workflow integration, and a straightforward procurement process through AWS Marketplace. 
The combination addresses the needs of an enterprise ML workflow end-to-end, where SageMaker AI handles infrastructure and compute, and Comet provides the experiment management, model registry, and production monitoring capabilities that teams require for regulatory compliance and operational efficiency. In this post, we demonstrate a complete fraud detection workflow using SageMaker AI with Comet, showcasing reproducibility and audit-ready logging needed by enterprises today. 
Enterprise-ready Comet on SageMaker AI 
Before proceeding to setup instructions, organizations must identify their operating model and based on that, decide how Comet is going to be set up. We recommend implementing Comet using a federated operating model. In this architecture, Comet is centrally managed and hosted in a shared services account, and each data science team maintains fully autonomous environments. Each operating model comes with their own sets of benefits and limitations. For more information, refer to SageMaker Studio Administration Best Practices. 
Letâ€™s dive into the setup of Comet in SageMaker AI. Large enterprise generally have the following personas: 
 
 Administrators â€“ Responsible for setting up the common infrastructure services and environment for use case teams 
 Users â€“ ML practitioners from use case teams who use the environments set up by platform team to solve their business problems 
 
In the following sections, we go through each personaâ€™s journey. 
Comet works well with both SageMaker AI and Amazon SageMaker. SageMaker AI provides the Amazon SageMaker Studio integrated development environment (IDE), and SageMaker provides the Amazon SageMaker Unified Studio IDE. For this post, we use SageMaker Studio. 
Administrator journey 
In this scenario, the administrator receives a request from a team working on a fraud detection use case to provision an ML environment with a fully managed training and experimentation setup. The administratorâ€™s journey includes the following steps: 
 
 Follow the prerequisites to set up Partner AI Apps. This sets up permissions for administrators, allowing Comet to assume a SageMaker AI execution role on behalf of the users and additional privileges for managing the Comet subscription through AWS Marketplace. 
 On the SageMaker AI console, under Applications and IDEs in the navigation pane, choose Partner AI Apps, then choose View details for Comet. 
 
 
The details are shown, including the contract pricing model for Comet and infrastructure tier estimated costs. 
 
Comet provides different subscription options ranging from a 1-month to 36-month contract. With this contract, users can access Comet in SageMaker. Based on the number of users, the admin can review and analyze the appropriate instance size for the Comet dashboard server. Comet supports 5â€“500 users running more than 100 experiment jobs.. 
 
 Choose Go to Marketplace to subscribe to be redirected to the Comet listing on AWS Marketplace. 
 Choose View purchase options. 
 
 
 
 In the subscription form, provide the required details. 
 
 
When the subscription is complete, the admin can start configuring Comet. 
 
 
 While deploying Comet, add the project lead of the fraud detection use case team as an admin to manage the admin operations for the Comet dashboard. 
 
It takes a few minutes for the Comet server to be deployed. For more details on this step, refer to Partner AI App provisioning. 
 
 Set up a SageMaker AI domain following the steps in Use custom setup for Amazon SageMaker AI. As a best practice, provide a pre-signed domain URL for the use case team member to directly access the Comet UI without logging in to the SageMaker console. 
 Add the team members to this domain and enable access to Comet while configuring the domain. 
 
Now the SageMaker AI domain is ready for users to log in to and start working on the fraud detection use case. 
User journey 
Now letâ€™s explore the journey of an ML practitioner from the fraud detection use case. The user completes the following steps: 
 
 Log in to the SageMaker AI domain through the pre-signed URL. 
 
You will be redirected to the SageMaker Studio IDE. Your user name and AWS Identity and Access Management (IAM) execution role are preconfigured by the admin. 
 
 Create a JupyterLab Space following the JupyterLab user guide. 
 You can start working on the fraud detection use case by spinning up a Jupyter notebook. 
 
The admin has also set up required access to the data through an Amazon Simple Storage Service (Amazon S3) bucket. 
 
 To access Comet APIs, install the comet_ml library and configure the required environment variables as described in Set up the Amazon SageMaker Partner AI Apps SDKs. 
 To access the Comet UI, choose Partner AI Apps in the SageMaker Studio navigation pane and choose Open for Comet. 
 
 
Now, letâ€™s walk through the use case implementation. 
Solution overview 
This use case highlights common enterprise challenges: working with imbalanced datasets (in this example, only 0.17% of transactions are fraudulent), requiring multiple experiment iterations, and maintaining full reproducibility for regulatory compliance. To follow along, refer to the Comet documentation and Quickstart guide for additional setup and API details. 
For this use case, we use the Credit Card Fraud Detection dataset. The dataset contains credit card transactions with binary labels representing fraudulent (1) or legitimate (0) transactions. In the following sections, we walk through some of the important sections of the implementation. The entire code of the implementation is available in the GitHub repository. 
Prerequisites 
As a prerequisite, configure the necessary imports and environment variables for the Comet and SageMaker integration: 
 
 # Comet ML for experiment tracking
import comet_ml
from comet_ml import Experiment, API, Artifact
from comet_ml.integration.sagemaker import log_sagemaker_training_job_v1
AWS_PARTNER_APP_AUTH=true
AWS_PARTNER_APP_ARN=&lt;Your_AWS_PARTNER_APP_ARN&gt;
COMET_API_KEY=&lt;Your_Comet_API_Key&gt; 	
# From Details Page, click Open Comet. In the top #right corner, click on user -&gt; API # Key
# Comet ML configuration
COMET_WORKSPACE = '&lt;your-comet-workspace-name&gt;'
COMET_PROJECT_NAME = '&lt;your-comet-project-name&gt;' 
 
Prepare the dataset 
One of Cometâ€™s key enterprise features is automatic dataset versioning and lineage tracking. This capability provides full auditability of what data was used to train each model, which is critical for regulatory compliance and reproducibility. Start by loading the dataset: 
 
 # Create a Comet Artifact to track our raw dataset
dataset_artifact = Artifact(
    name="fraud-dataset",
    artifact_type="dataset",
    aliases=["raw"]
)
# Add the raw dataset file to the artifact
dataset_artifact.add_remote(s3_data_path, metadata={
    "dataset_stage": "raw", 
    "dataset_split": "not_split", 
    "preprocessing": "none"
}) 
 
Start a Comet experiment 
With the dataset artifact created, you can now start tracking the ML workflow. Creating a Comet experiment automatically begins capturing code, installed libraries, system metadata, and other contextual information in the background. You can log the dataset artifact created earlier in the experiment. See the following code: 
 
 # Create a new Comet experiment
experiment_1 = comet_ml.Experiment(
    project_name=COMET_PROJECT_NAME,
    workspace=COMET_WORKSPACE,
)
# Log the dataset artifact to this experiment for lineage tracking
experiment_1.log_artifact(dataset_artifact) 
 
Preprocess the data 
The next steps are standard preprocessing steps, including removing duplicates, dropping unneeded columns, splitting into train/validation/test sets, and standardizing features using scikit-learnâ€™s StandardScaler. We wrap the processing code in preprocess.py and run it as a SageMaker Processing job. See the following code: 
 
 # Run SageMaker processing job
processor = SKLearnProcessor(
    framework_version='1.0-1',
    role=sagemaker.get_execution_role(),
    instance_count=1,
    instance_type='ml.t3.medium'
)
processor.run(
    code='preprocess.py',
    inputs=[ProcessingInput(source=s3_data_path, destination='/opt/ml/processing/input')],
    outputs=[ProcessingOutput(source='/opt/ml/processing/output', destination=f's3://{bucket_name}/{processed_data_prefix}')]
) 
 
After you submit the processing job, SageMaker AI launches the compute instances, processes and analyzes the input data, and releases the resources upon completion. The output of the processing job is stored in the S3 bucket specified. 
Next, create a new version of the dataset artifact to track the processed data. Comet automatically versions artifacts with the same name, maintaining complete lineage from raw to processed data. 
 
 # Create an updated version of the 'fraud-dataset' Artifact for the preprocessed data
preprocessed_dataset_artifact = Artifact(
    name="fraud-dataset",
    artifact_type="dataset", 
    aliases=["preprocessed"],
    metadata={
        "description": "Credit card fraud detection dataset",
        "fraud_percentage": f"{fraud_percentage:.3f}%",
        "dataset_stage": "preprocessed",
        "preprocessing": "StandardScaler + train/val/test split",
    }
)
# Add our train, validation, and test dataset files as remote assets 
preprocessed_dataset_artifact.add_remote(
    uri=f's3://{bucket_name}/{processed_data_prefix}',
    logical_path='split_data'
)
# Log the updated dataset to the experiment to track the updates
experiment_1.log_artifact(preprocessed_dataset_artifact) 
 
The Comet and SageMaker AI experiment workflow 
Data scientists prefer rapid experimentation; therefore, we organized the workflow into reusable utility functions that can be called multiple times with different hyperparameters while maintaining consistent logging and evaluation across all runs. In this section, we showcase the utility functions along with a brief snippet of the code inside the function: 
 
 train() â€“ Spins up a SageMaker model training job using the SageMaker built-in XGBoost algorithm: 
 
 
     # Create SageMaker estimator
    estimator = Estimator(
        image_uri=xgboost_image,
        role=execution_role,
        instance_count=1,
        instance_type='ml.m5.large',
        output_path=model_output_path,
        sagemaker_session=sagemaker_session_obj,
        hyperparameters=hyperparameters_dict,
        max_run=1800  # Maximum training time in seconds
    )
    # Start training
    estimator.fit({
        'train': train_channel,
        'validation': val_channel
    }) 
 
 
 log_training_job() â€“ Captures the training metadata and metrics and links the model asset to the experiment for complete traceability: 
 
 
 # Log SageMaker training job to Comet 
    log_sagemaker_training_job_v1(
        estimator=training_estimator,
        experiment=api_experiment
    ) 
 
 
 log_model_to_comet() â€“ Links model artifacts to Comet, captures the training metadata, and links the model asset to the experiment for complete traceability: 
 
 
 experiment.log_remote_model(
        model_name=model_name,
        uri=model_artifact_path,
        metadata=metadata
    ) 
 
 
 deploy_and_evaluate_model() â€“ Performs model deployment and evaluation, and metric logging: 
 
 
 # Deploy to endpoint
predictor = estimator.deploy(
initial_instance_count=1,
       instance_type="ml.m5.xlarge")
# Log metrics and visualizations to Comet 
experiment.log_metrics(metrics) experiment.log_confusion_matrix(matrix=cm,labels=['Normal', 'Fraud']) 
# Log ROC curve 
fpr, tpr, _ = roc_curve(y_test, y_pred_prob_as_np_array) experiment.log_curve("roc_curve", x=fpr, y=tpr) 
 
The complete prediction and evaluation code is available in the GitHub repository. 
Run the experiments 
Now you can run multiple experiments by calling the utility functions with different configurations and compare experiments to find the most optimal settings for the fraud detection use case. 
For the first experiment, we establish a baseline using standard XGBoost hyperparameters: 
 
 # Define hyperparameters for first experiment
hyperparameters_v1 = {
    'objective': 'binary:logistic',	# Binary classification
    'num_round': 100,                   # Number of boosting rounds
    'eval_metric': 'auc',               # Evaluation metric
    'learning_rate': 0.15,              # Learning rate
    'booster': 'gbtree'                 # Booster algorithm
}
# Train the model
estimator_1 = train(
    model_output_path=f"s3://{bucket_name}/{model_output_prefix}/1",
    execution_role=role,
    sagemaker_session_obj=sagemaker_session,
    hyperparameters_dict=hyperparameters_v1,
    train_channel_loc=train_channel_location,
    val_channel_loc=validation_channel_location
)
# log the training job and model artifact
log_training_job(experiment_key = experiment_1.get_key(), training_estimator=estimator_1)
log_model_to_comet(experiment = experiment_1,
                   model_name="fraud-detection-xgb-v1", 
                   model_artifact_path=estimator_1.model_data, 
                   metadata=metadata)
# Deploy and evaluate
deploy_and_evaluate_model(experiment=experiment_1,
                          estimator=estimator_1,
                          X_test_scaled=X_test_scaled,
                          y_test=y_test
                          ) 
 
While running a Comet experiment from a Jupyter notebook, we need to end the experiment to make sure everything is captured and persisted in the Comet server. See the following code: experiment_1.end() 
When the baseline experiment is complete, you can run additional experiments with different hyperparameters. Check out the notebook to see the details of both experiments. 
When the second experiment is complete, navigate to the Comet UI to compare these two experiment runs. 
View Comet experiments in the UI 
To access the UI, you can locate the URL in the SageMaker Studio IDE or by executing the code provided in the notebook: experiment_2.url 
The following screenshot shows the Comet experiments UI. The experiment details are for illustration purposes only and do not represent a real-world fraud detection experiment. 
 
This concludes the fraud detection experiment. 
Clean up 
For the experimentation part, SageMaker processing and training infrastructure is ephemeral in nature and shuts down automatically when the job is complete. However, you must still manually clean up a few resources to avoid unnecessary costs: 
 
 Shut down the SageMaker JupyterLab Space after use. For instructions, refer to Idle shutdown. 
 The Comet subscription renews based on the contract chosen. Cancel the contract when there is no further requirement to renew the Comet subscription. 
 
Advantages of SageMaker and Comet integration 
Having demonstrated the technical workflow, letâ€™s examine the broader advantages this integration provides. 
Streamlined model development 
The Comet and SageMaker combination reduces the manual overhead of running ML experiments. While SageMaker handles infrastructure provisioning and scaling, Cometâ€™s automatic logging captures hyperparameters, metrics, code, installed libraries, and system performance from your training jobs without additional configuration. This helps teams focus on model development rather than experiment bookkeeping.Cometâ€™s visualization capabilities extend beyond basic metric plots. Built-in charts enable rapid experiment comparison, and custom Python panels support domain-specific analysis tools for debugging model behavior, optimizing hyperparameters, or creating specialized visualizations that standard tools canâ€™t provide. 
Enterprise collaboration and governance 
For enterprise teams, the combination creates a mature platform for scaling ML projects across regulated environments. SageMaker provides consistent, secure ML environments, and Comet enables seamless collaboration with complete artifact and model lineage tracking. This helps avoid costly mistakes that occur when teams canâ€™t recreate previous results. 
Complete ML lifecycle integration 
Unlike point solutions that only address training or monitoring, Comet paired with SageMaker supports your complete ML lifecycle. Models can be registered in Cometâ€™s model registry with full version tracking and governance. SageMaker handles model deployment, and Comet maintains the lineage and approval workflows for model promotion. Cometâ€™s production monitoring capabilities track model performance and data drift after deployment, creating a closed loop where production insights inform your next round of SageMaker experiments. 
Conclusion 
In this post, we showed how to use SageMaker and Comet together to spin up fully managed ML environments with reproducibility and experiment tracking capabilities. 
To enhance your SageMaker workflows with comprehensive experiment management, deploy Comet directly in your SageMaker environment through the AWS Marketplace, and share your feedback in the comments. 
For more information about the services and features discussed in this post, refer to the following resources: 
 
 Set up Partner AI Apps 
 Comet Quickstart 
 GitHub notebook 
 Comet Documentation 
 Opik open source platform for LLM observability 
 
 
 
About the authors 
Vikesh Pandey is a Principal GenAI/ML Specialist Solutions Architect at AWS, helping large financial institutions adopt and scale generative AI and ML workloads. He is the author of book â€œGenerative AI for financial services.â€ He carries more than 15 years of experience building enterprise-grade applications on generative AI/ML and related technologies. In his spare time, he plays an unnamed sport with his son that lies somewhere between football and rugby. 
Naufal Mir is a Senior GenAI/ML Specialist Solutions Architect at AWS. He focuses on helping customers build, train, deploy and migrate machine learning workloads to SageMaker. He previously worked at financial services institutes developing and operating systems at scale. Outside of work, he enjoys ultra endurance running and cycling. 
Sarah Ostermeier is a Technical Product Marketing Manager at Comet. She specializes in bringing Cometâ€™s GenAI and ML developer products to the engineers who need them through technical content, educational resources, and product messaging. She has previously worked as an ML engineer, data scientist, and customer success manager, helping customers implement and scale AI solutions. Outside of work she enjoys traveling off the beaten path, writing about AI, and reading science fiction.
â€¢ Move your AI agents from proof of concept to production with Amazon Bedrock AgentCore
  Building an AI agent that can handle a real-life use case in production is a complex undertaking. Although creating a proof of concept&nbsp;demonstrates the potential, moving to production requires addressing scalability, security, observability, and operational concerns that donâ€™t surface in development environments. 
This post explores how Amazon Bedrock AgentCore helps you transition your agentic applications from experimental proof of concept to production-ready systems. We follow the journey of a customer support agent that evolves from a simple local prototype to a comprehensive, enterprise-grade solution capable of handling multiple concurrent users while maintaining security and performance standards. 
Amazon Bedrock AgentCore is a comprehensive suite of services designed to help you build, deploy, and scale agentic AI applications. If youâ€™re new to AgentCore, we recommend exploring our existing deep-dive posts on individual services: AgentCore Runtime for secure agent deployment and scaling, AgentCore Gateway for enterprise tool development, AgentCore Identity for securing agentic AI at scale, AgentCore Memory for building context-aware agents, AgentCore Code Interpreter for code execution, AgentCore Browser Tool for web interaction, and AgentCore Observability for transparency on your agent behavior. This post demonstrates how these services work together in a real-world scenario. 
The customer support agent journey 
Customer support represents one of the most common and compelling use cases for agentic AI. Modern businesses handle thousands of customer inquiries daily, ranging from simple policy questions to complex technical troubleshooting. Traditional approaches often fall short: rule-based chatbots frustrate customers with rigid responses, and human-only support teams struggle with scalability and consistency. An intelligent customer support agent needs to seamlessly handle diverse scenarios: managing customer orders and accounts, looking up return policies, searching product catalogs, troubleshooting technical issues through web research, and remembering customer preferences across multiple interactions. Most importantly, it must do all this while maintaining the security and reliability standards expected in enterprise environments. Consider the typical evolution path many organizations follow when building such agents: 
 
 The proof of concept stage â€“ Teams start with a simple local prototype that demonstrates core capabilities, such as a basic agent that can answer policy questions and search for products. This works well for demos but lacks the robustness needed for real customer interactions. 
 The reality check â€“ As soon as you try to scale beyond a few test users, challenges emerge. The agent forgets previous conversations, tools become unreliable under load, thereâ€™s no way to monitor performance, and security becomes a paramount concern. 
 The production challenge â€“ Moving to production requires addressing session management, secure tool sharing, observability, authentication, and building interfaces that customers actually want to use. Many promising proofs of concept stall at this stage due to the complexity of these requirements. 
 
In this post, we address each challenge systematically. We start with a prototype agent equipped with three essential tools: return policy lookup, product information search, and web search for troubleshooting. From there, we add the capabilities needed for production deployment: persistent memory for conversation continuity and a hyper-personalized experience, centralized tool management for reliability and security, full observability for monitoring and debugging, and finally a customer-facing web interface. This progression mirrors the real-world path from proof of concept to production, demonstrating how Amazon Bedrock AgentCore services work together to solve the operational challenges that emerge as your agentic applications mature. For simplification and demonstration purposes, we consider a single-agent architecture. In real-life use cases, customer support agents are often created as multi-agent architectures and those scenarios are also supported by Amazon Bedrock AgentCore services. 
Solution overview 
Every production system starts with a proof of concept, and our customer support agent is no exception. In this first phase, we build a functional prototype that demonstrates the core capabilities needed for customer support. In this case, we use Strands Agents, an open source agent framework, to build the proof of concept and Anthropicâ€™s Claude 3.7 Sonnet on Amazon Bedrock as the large language model (LLM) powering our agent. For your application, you can use another agent framework and model of your choice. 
Agents rely on tools to take actions and interact with live systems. Several tools are used in customer support agents, but to keep our example simple, we focus on three core capabilities to handle the most common customer inquiries: 
 
 Return policy lookup â€“ Customers frequently ask about return windows, conditions, and processes. Our tool provides structured policy information based on product categories, covering everything from return timeframes to refund processing and shipping policies. 
 Product information retrieval â€“ Technical specifications, warranty details, and compatibility information are essential for both pre-purchase questions and troubleshooting. This tool serves as a bridge to your product catalog, delivering formatted technical details that customers can understand. 
 Web search for troubleshooting â€“ Complex technical issues often require the latest solutions or community-generated fixes not found in internal documentation. Web search capability allows the agent to access the web for current troubleshooting guides and technical solutions in real time. 
 
The tools implementation and the end-to-end code for this use case are available in our GitHub repository. In this post, we focus on the main code that connects with Amazon Bedrock AgentCore, but you can follow the end-to-end journey in the repository. 
Create the agent 
With the tools available, letâ€™s create the agent. The architecture for our proof of concept will look like the following diagram. 
 
You can find the end-to-end code for this post on the GitHub repository. For simplicity, we show only the essential parts for our end-to-end code here: 
 
 from strands import Agent
from strands.models import BedrockModel

@tool
def get_return_policy(product_category: str) -&gt; str:
&nbsp;&nbsp; &nbsp;"""Get return policy information for a specific product category."""
&nbsp;&nbsp; &nbsp;# Returns structured policy info: windows, conditions, processes, refunds
&nbsp; &nbsp; # check github for full code
&nbsp;&nbsp; &nbsp;return&nbsp;{"return_window":&nbsp;"10 days",&nbsp;"conditions":&nbsp;""}
&nbsp;&nbsp;&nbsp;&nbsp;
@tool &nbsp;
def get_product_info(product_type: str) -&gt; str:
&nbsp;&nbsp; &nbsp;"""Get detailed technical specifications and information for electronics products."""
&nbsp;&nbsp; &nbsp;# Returns warranty, specs, features, compatibility details
&nbsp;&nbsp; &nbsp;# check github for full code
&nbsp; &nbsp; return&nbsp;{"product":&nbsp;"ThinkPad X1 Carbon",&nbsp;"info":&nbsp;"ThinkPad X1 Carbon info"}
&nbsp;&nbsp;&nbsp;&nbsp;
@tool
def web_search(keywords: str, region: str = "us-en", max_results: int = 5) -&gt; str:
&nbsp;&nbsp; &nbsp;"""Search the web for updated troubleshooting information."""
&nbsp;&nbsp; &nbsp;# Provides access to current technical solutions and guides
&nbsp;&nbsp;&nbsp;&nbsp;# check github for full code
&nbsp; &nbsp; return&nbsp;"results from websearch"
&nbsp;&nbsp;&nbsp;&nbsp;
# Initialize the Bedrock model
model = BedrockModel(
&nbsp;&nbsp; &nbsp;model_id="us.anthropic.claude-3-7-sonnet-20250219-v1:0",
&nbsp;&nbsp; &nbsp;temperature=0.3
)

# Create the customer support agent
agent = Agent(
&nbsp;&nbsp; &nbsp;model=model,
&nbsp;&nbsp; &nbsp;tools=[
&nbsp; &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;get_product_info, 
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;get_return_policy, 
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;web_search
&nbsp;&nbsp;&nbsp;&nbsp;],
&nbsp;&nbsp; &nbsp;system_prompt="""You are a helpful customer support assistant for an electronics company.
&nbsp;&nbsp; &nbsp;Use the appropriate tools to provide accurate information and always offer additional help."""
) 
 
Test the proof of concept 
When we test our prototype with realistic customer queries, the agent demonstrates the correct tool selection and interaction with real-world systems: 
 
 # Return policy inquiry
response = agent("What's the return policy for my ThinkPad X1 Carbon?")
# Agent correctly uses get_return_policy with "laptops" category

# Technical troubleshooting &nbsp;
response = agent("My iPhone 14 heats up, how do I fix it?")
# Agent uses web_search to find current troubleshooting solutions 
 
The agent works well for these individual queries, correctly mapping laptop inquiries to return policy lookups and complex technical issues to web search, providing comprehensive and actionable responses. 
The proof of concept reality check 
Our proof of concept successfully demonstrates that an agent can handle diverse customer support scenarios using the right combination of tools and reasoning. The agent runs perfectly on your local machine and handles queries correctly. However, this is where the proof of concept gap becomes obvious. The tools are defined as local functions in your agent code, the agent responds quickly, and everything seems production-ready. But several critical limitations become apparent the moment you think beyond single-user testing: 
 
 Memory loss between sessions â€“ If you restart your notebook or application, the agent completely forgets previous conversations. A customer who was discussing a laptop return yesterday would need to start from scratch today, re-explaining their entire situation. This isnâ€™t just inconvenientâ€”itâ€™s a poor customer experience that breaks the conversational flow that makes AI agents valuable. 
 Single customer limitation â€“ Your current agent can only handle one conversation at a time. If two customers try to use your support system simultaneously, their conversations would interfere with each other, or worse, one customer might see anotherâ€™s conversation history. Thereâ€™s no mechanism to maintain separate conversation context for different users. 
 Tools embedded in code â€“ Your tools are defined directly in the agent code. This means: 
   
   You canâ€™t reuse these tools across different agents (sales agent, technical support agent, and so on). 
   Updating a tool requires changing the agent code and redeploying everything. 
   Different teams canâ€™t maintain different tools independently. 
    
 No production infrastructure â€“ The agent runs locally with no consideration for scalability, security, monitoring, and reliability. 
 
These fundamental architectural barriers can prevent real customer deployment. Agent building teams can take months to address these issues, which delays the time to value from their work and adds significant costs to the application. This is where Amazon Bedrock AgentCore services become essential. Rather than spending months building these production capabilities from scratch, Amazon Bedrock AgentCore provides managed services that address each gap systematically. 
Letâ€™s begin our journey to production by solving the memory problem first, transforming our agent from one that forgets every conversation into one that remembers customers across conversations and can hyper-personalize conversations using Amazon Bedrock AgentCore Memory. 
Add persistent memory for hyper-personalized agents 
The first major limitation we identified in our proof of concept was memory lossâ€”our agent forgot everything between sessions, forcing customers to repeat their context every time. This â€œgoldfish agentâ€ behavior breaks the conversational experience that makes AI agents valuable in the first place. 
Amazon Bedrock AgentCore Memory solves this by providing managed, persistent memory that operates on two complementary levels: 
 
 Short-term memory â€“ Immediate conversation context and session-based information for continuity within interactions 
 Long-term memory â€“ Persistent information extracted across multiple conversations, including customer preferences, facts, and behavioral patterns 
 
After adding Amazon Bedrock AgentCore Memory to our customer support agent, our new architecture will look like the following diagram. 
 
Install dependencies 
Before we start, letâ€™s install our dependencies: boto3, the AgentCore SDK, and the AgentCore Starter Toolkit SDK. Those will help us quickly add Amazon Bedrock AgentCore capabilities to our agent proof of concept. See the following code: 
 
 pip install boto3 bedrock-agentcore bedrock-agentcore-starter-toolkit 
 
Create the memory resources 
Amazon Bedrock AgentCore Memory uses configurable strategies to determine what information to extract and store. For our customer support use case, we use two complementary strategies: 
 
 USER_PREFERENCE â€“ Automatically extracts and stores customer preferences like â€œprefers ThinkPad laptops,â€ â€œuses Linux,â€ or â€œplays competitive FPS games.â€ This enables personalized recommendations across conversations. 
 SEMANTIC â€“ Captures factual information using vector embeddings, such as â€œcustomer has MacBook Pro order #MB-78432â€ or â€œreported overheating issues during video editing.â€ This provides relevant context for troubleshooting. 
 
See the following code: 
 
 from bedrock_agentcore.memory import MemoryClient
from bedrock_agentcore.memory.constants import StrategyType

memory_client = MemoryClient(region_name=region)

strategies = [
&nbsp;&nbsp; &nbsp;{
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;StrategyType.USER_PREFERENCE.value: {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"name": "CustomerPreferences",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"description": "Captures customer preferences and behavior",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"namespaces": ["support/customer/{actorId}/preferences"],
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp;},
&nbsp;&nbsp; &nbsp;{
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;StrategyType.SEMANTIC.value: {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"name": "CustomerSupportSemantic", 
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"description": "Stores facts from conversations",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"namespaces": ["support/customer/{actorId}/semantic"],
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp;},
]

# Create memory resource with both strategies
response = memory_client.create_memory_and_wait(
&nbsp;&nbsp; &nbsp;name="CustomerSupportMemory",
&nbsp;&nbsp; &nbsp;description="Customer support agent memory",
&nbsp;&nbsp; &nbsp;strategies=strategies,
&nbsp;&nbsp; &nbsp;event_expiry_days=90,
) 
 
Integrate with Strands Agents hooks 
The key to making memory work seamlessly is automationâ€”customers shouldnâ€™t need to think about it, and agents shouldnâ€™t require manual memory management. Strands Agents provides a powerful hook system that lets you intercept agent lifecycle events and handle memory operations automatically. The hook system enables both built-in components and user code to react to or modify agent behavior through strongly-typed event callbacks. For our use case, we create CustomerSupportMemoryHooks to retrieve the customer context and save the support interactions: 
 
 MessageAddedEvent hook â€“ Triggered when customers send messages, this hook automatically retrieves relevant memory context and injects it into the query. The agent receives both the customerâ€™s question and relevant historical context without manual intervention. 
 AfterInvocationEvent hook â€“ Triggered after agent responses, this hook automatically saves the interaction to memory. The conversation becomes part of the customerâ€™s persistent history immediately. 
 
See the following code: 
 
 class CustomerSupportMemoryHooks(HookProvider):
&nbsp;&nbsp; &nbsp;def retrieve_customer_context(self, event: MessageAddedEvent):
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"""Inject customer context before processing queries"""
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;user_query = event.agent.messages[-1]["content"][0]["text"]
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;# Retrieve relevant memories from both strategies
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;all_context = []
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;for context_type, namespace in self.namespaces.items():
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;memories = self.client.retrieve_memories(
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;memory_id=self.memory_id,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;namespace=namespace.format(actorId=self.actor_id),
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;query=user_query,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;top_k=3,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;)
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# Format and add to context
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;for memory in memories:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;if memory.get("content", {}).get("text"):
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;all_context.append(f"[{context_type.upper()}] {memory['content']['text']}")
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;# Inject context into the user query
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;if all_context:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;context_text = "\n".join(all_context)
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;original_text = event.agent.messages[-1]["content"][0]["text"]
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;event.agent.messages[-1]["content"][0]["text"] = f"Customer Context:\n{context_text}\n\n{original_text}"

&nbsp;&nbsp; &nbsp;def save_support_interaction(self, event: AfterInvocationEvent):
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"""Save interactions after agent responses"""
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# Get last customer query and agent response check github for implementation
&nbsp; &nbsp; &nbsp; &nbsp; customer_query&nbsp;=&nbsp;"This is a sample query"
&nbsp; &nbsp; &nbsp; &nbsp; agent_response&nbsp;=&nbsp;"LLM gave a sample response"
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;# Extract customer query and agent response
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;# Save to memory for future retrieval
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;self.client.create_event(
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;memory_id=self.memory_id,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;actor_id=self.actor_id,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;session_id=self.session_id,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;messages=[(customer_query, "USER"), (agent_response, "ASSISTANT")]
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;) 
 
In this code, we can see that our hooks are the ones interacting with Amazon Bedrock AgentCore Memory to save and retrieve memory events. 
Integrate memory with the agent 
Adding memory to our existing agent requires minimal code changes; you can simply instantiate the memory hooks and pass them to the agent constructor. The agent code then only needs to connect with the memory hooks to use the full power of Amazon Bedrock AgentCore Memory. We will create a new hook for each session, which will help us handle different customer interactions. See the following code: 
 
 # Create memory hooks for this customer session
memory_hooks = CustomerSupportMemoryHooks(
&nbsp;&nbsp; &nbsp;memory_id=memory_id, 
&nbsp;&nbsp; &nbsp;client=memory_client, 
&nbsp;&nbsp; &nbsp;actor_id=customer_id, 
&nbsp;&nbsp; &nbsp;session_id=session_id
)

# Create agent with memory capabilities
agent = Agent(
&nbsp;&nbsp; &nbsp;model=model,

&nbsp;&nbsp; &nbsp;tools=[get_product_info, get_return_policy, web_search],
&nbsp;&nbsp; &nbsp;system_prompt=SYSTEM_PROMPT
) 
 
Test the memory in action 
Letâ€™s see how memory transforms the customer experience. When we invoke the agent, it uses the memory from previous interactions to show customer interests in gaming headphones, ThinkPad laptops, and MacBook thermal issues: 
 
 # Test personalized recommendations
response = agent("Which headphones would you recommend?")
# Agent remembers: "prefers low latency for competitive FPS games"
# Response includes gaming-focused recommendations

# Test preference recall
response = agent("What is my preferred laptop brand?") &nbsp;
# Agent remembers: "prefers ThinkPad models" and "needs Linux compatibility"
# Response acknowledges ThinkPad preference and suggests compatible models 
 
The transformation is immediately apparent. Instead of generic responses, the agent now provides personalized recommendations based on the customerâ€™s stated preferences and past interactions. The customer doesnâ€™t need to re-explain their gaming needs or Linux requirementsâ€”the agent already knows. 
Benefits of Amazon Bedrock AgentCore Memory 
With Amazon Bedrock AgentCore Memory integrated, our agent now delivers the following benefits: 
 
 Conversation continuity â€“ Customers can pick up where they left off, even across different sessions or support channels 
 Personalized service â€“ Recommendations and responses are tailored to individual preferences and past issues 
 Contextual troubleshooting â€“ Access to previous problems and solutions enables more effective support 
 Seamless experience â€“ Memory operations happen automatically without customer or agent intervention 
 
However, we still have limitations to address. Our tools remain embedded in the agent code, preventing reuse across different support agents or teams. Security and access controls are minimal, and we still canâ€™t handle multiple customers simultaneously in a production environment. 
In the next section, we address these challenges by centralizing our tools using Amazon Bedrock AgentCore Gateway and implementing proper identity management with Amazon Bedrock AgentCore Identity, creating a scalable and secure foundation for our customer support system. 
Centralize tools with Amazon Bedrock AgentCore Gateway and Amazon Bedrock AgentCore Identity 
With memory solved, our next challenge is tool architecture. Currently, our tools are embedded directly in the agent codeâ€”a pattern that works for prototypes but creates significant problems at scale. When you need multiple agents (customer support, sales, technical support), each one duplicates the same tools, leading to extensive code, inconsistent behavior, and maintenance nightmares. 
Amazon Bedrock AgentCore Gateway simplifies this process by centralizing tools into reusable, secure endpoints that agents can access. Combined with Amazon Bedrock AgentCore Identity for authentication, it creates an enterprise-grade tool sharing infrastructure. 
We will now update our agent to use Amazon Bedrock AgentCore Gateway and Amazon Bedrock AgentCore Identity. The architecture will look like the following diagram. 
 
In this case, we convert our web search tool to be used in the gateway and keep the return policy and get product information tools local to this agent. That is important because web search is a common capability that can be reused across different use cases in an organization, and return policy and production information are capabilities commonly associated with customer support services. With Amazon Bedrock AgentCore services, you can decide which capabilities to use and how to combine them. In this case, we also use two new tools that could have been developed by other teams: check warranty and get customer profile. Because those teams have already exposed those tools using AWS Lambda functions, we can use them as targets to our Amazon Bedrock AgentCore Gateway. Amazon Bedrock AgentCore Gateway can also support REST APIs as target. That means that if we have an OpenAPI specification or a Smithy model, we can also quickly expose our tools using Amazon Bedrock AgentCore Gateway. 
Convert existing services to MCP 
Amazon Bedrock AgentCore Gateway uses the Model Context Protocol (MCP) to standardize how agents access tools. Converting existing Lambda functions into MCP endpoints requires minimal changesâ€”mainly adding tool schemas and handling the MCP context. To use this functionality, we convert our local tools to Lambda functions and create the tools schema definitions to make these functions discoverable by agents: 
 
 # Original Lambda function (simplified)
def web_search(keywords: str, region: str = "us-en", max_results: int = 5) -&gt; str:
&nbsp; &nbsp; # web_search functionality
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
def lambda_handler(event, context):
&nbsp;&nbsp; &nbsp;if get_tool_name(event) == "web_search":
&nbsp; &nbsp; &nbsp; &nbsp; query&nbsp;= get_named_parameter(event=event, name="query")
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; search_result&nbsp;=&nbsp;web_search(keywords)
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;return {"statusCode": 200, "body": search_result} 
 
The following code is the tool schema definition: 
 
 {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"name": "web_search",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"description": "Search the web for updated information using DuckDuckGo",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"inputSchema": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"type": "object",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"properties": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"keywords": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"type": "string",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"description": "The search query keywords"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;},
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"region": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"type": "string",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"description": "The search region (e.g., us-en, uk-en, ru-ru)"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;},
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"max_results": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"type": "integer",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"description": "The maximum number of results to return"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;},
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"required": [
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"keywords"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;]
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp;} 
 
For demonstration purposes, we build a new Lambda function from scratch. In reality, organizations already have different functionalities available as REST services or Lambda functions, and this approach lets you expose existing enterprise services as agent tools without rebuilding them. 
Configure security with Amazon Bedrock AgentCore Gateway and integrate with Amazon Bedrock AgentCore Identity 
Amazon Bedrock AgentCore Gateway requires authentication for both inbound and outbound connections. Amazon Bedrock AgentCore Identity handles this through standard OAuth flows. After you set up an OAuth authorization configuration, you can create a new gateway and pass this configuration to it. See the following code: 
 
 # Create gateway with JWT-based authentication
auth_config = {
&nbsp;&nbsp; &nbsp;"customJWTAuthorizer": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"allowedClients": [cognito_client_id],
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"discoveryUrl": cognito_discovery_url
&nbsp;&nbsp; &nbsp;}
}

gateway_response = gateway_client.create_gateway(
&nbsp;&nbsp; &nbsp;name="customersupport-gw",
&nbsp;&nbsp; &nbsp;roleArn=gateway_iam_role,
&nbsp;&nbsp; &nbsp;protocolType="MCP",
&nbsp;&nbsp; &nbsp;authorizerType="CUSTOM_JWT",
&nbsp;&nbsp; &nbsp;authorizerConfiguration=auth_config,
&nbsp;&nbsp; &nbsp;description="Customer Support AgentCore Gateway"
) 
 
For inbound authentication, agents must present valid JSON Web Token (JWT) tokens (from identity providers like Amazon Cognito, Okta, and EntraID) as a compact, self-contained standard for securely transmitting information between parties to access Amazon Bedrock AgentCore Gateway tools. 
For outbound authentication, Amazon Bedrock AgentCore Gateway can authenticate to downstream services using AWS Identity and Access Management (IAM) roles, API keys, or OAuth tokens. 
For demonstration purposes, we have created an Amazon Cognito user pool with a dummy user name and password. For your use case, you should set a proper identity provider and manage the users accordingly. This configure makes sure only authorized agents can access specific tools and a full audit trail is provided. 
Add Lambda targets 
After you set up Amazon Bedrock AgentCore Gateway, adding Lambda functions as tool targets is straightforward: 
 
 lambda_target_config = {
&nbsp;&nbsp; &nbsp;"mcp": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"lambda": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"lambdaArn": lambda_function_arn,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"toolSchema": {"inlinePayload": api_spec},
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp;}
}

gateway_client.create_gateway_target(
&nbsp;&nbsp; &nbsp;gatewayIdentifier=gateway_id,
&nbsp;&nbsp; &nbsp;name="LambdaTools",
&nbsp;&nbsp; &nbsp;targetConfiguration=lambda_target_config,
&nbsp;&nbsp; &nbsp;credentialProviderConfigurations=[{
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"credentialProviderType": "GATEWAY_IAM_ROLE"
&nbsp;&nbsp; &nbsp;}]
) 
 
The gateway now exposes your Lambda functions as MCP tools that authorized agents can discover and use. 
Integrate MCP tools with Strands Agents 
Converting our agent to use centralized tools requires updating the tool configuration. We keep some tools local, such as product info and return policies specific to customer support that will likely not be reused in other use cases, and use centralized tools for shared capabilities. Because Strands Agents has a native integration for MCP tools, we can simply use the MCPClient from Strands with a streamablehttp_client. See the following code: 
 
 # Get OAuth token for gateway access
gateway_access_token = get_token(
&nbsp;&nbsp; &nbsp;client_id=cognito_client_id,
&nbsp;&nbsp; &nbsp;client_secret=cognito_client_secret,
&nbsp;&nbsp; &nbsp;scope=auth_scope,
&nbsp;&nbsp; &nbsp;url=token_url
)

# Create authenticated MCP client
mcp_client = MCPClient(
&nbsp;&nbsp; &nbsp;lambda: streamablehttp_client(
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;gateway_url,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;headers={"Authorization": f"Bearer {gateway_access_token['access_token']}"}
&nbsp;&nbsp; &nbsp;)
)

# Combine local and MCP tools
tools = [
&nbsp;&nbsp; &nbsp;get_product_info, &nbsp; &nbsp; # Local tool (customer support specific)
&nbsp;&nbsp; &nbsp;get_return_policy, &nbsp; &nbsp;# Local tool (customer support specific)
] + mcp_client.list_tools_sync() &nbsp;# Centralized tools from gateway

agent = Agent(
&nbsp;&nbsp; &nbsp;model=model,
&nbsp;&nbsp; &nbsp;tools=tools,
&nbsp;&nbsp; &nbsp;hooks=[memory_hooks],
&nbsp;&nbsp; &nbsp;system_prompt=SYSTEM_PROMPT
) 
 
Test the enhanced agent 
With the centralized tools integrated, our agent now has access to enterprise capabilities like warranty checking: 
 
 # Test web search using centralized tool &nbsp;
response = agent("How can I fix Lenovo ThinkPad with a blue screen?")
# Agent uses web_search from AgentCore Gateway 
 
The agent seamlessly combines local tools with centralized ones, providing comprehensive support capabilities while maintaining security and access control. 
However, we still have a significant limitation: our entire agent runs locally on our development machine. For production deployment, we need scalable infrastructure, comprehensive observability, and the ability to handle multiple concurrent users. 
In the next section, we address this by deploying our agent to Amazon Bedrock AgentCore Runtime, transforming our local prototype into a production-ready system with Amazon Bedrock AgentCore Observability and automatic scaling capabilities. 
Deploy to production with Amazon Bedrock AgentCore Runtime 
With the tools centralized and secured, our final major hurdle is production deployment. Our agent currently runs locally on your laptop, which is ideal for experimentation but unsuitable for real customers. Production requires scalable infrastructure, comprehensive monitoring, automatic error recovery, and the ability to handle multiple concurrent users reliably. 
Amazon Bedrock AgentCore Runtime transforms your local agent into a production-ready service with minimal code changes. Combined with Amazon Bedrock AgentCore Observability, it provides enterprise-grade reliability, automatic scaling, and comprehensive monitoring capabilities that operations teams need to maintain agentic applications in production. 
Our architecture will look like the following diagram. 
 
Minimal code changes for production 
Converting your local agent requires adding just four lines of code: 
 
 # Your existing agent code remains unchanged
model = BedrockModel(model_id="us.anthropic.claude-3-7-sonnet-20250219-v1:0")
memory_hooks = CustomerSupportMemoryHooks(memory_id, memory_client, actor_id, session_id)
agent = Agent(
&nbsp;&nbsp; &nbsp;model=model,
&nbsp;&nbsp; &nbsp;tools=[get_return_policy, get_product_info],
&nbsp;&nbsp; &nbsp;system_prompt=SYSTEM_PROMPT,
&nbsp;&nbsp; &nbsp;hooks=[memory_hooks]
)

def invoke(payload):
&nbsp;&nbsp; &nbsp;user_input = payload.get("prompt", "")
&nbsp;&nbsp; &nbsp;response = agent(user_input)
&nbsp;&nbsp; &nbsp;return response.message["content"][0]["text"]

if __name__ == "__main__":
 
 
BedrockAgentCoreApp automatically creates an HTTP server with the required /invocations and /ping endpoints, handles proper content types and response formats, manages error handling according to AWS standards, and provides the infrastructure bridge between your agent code and Amazon Bedrock AgentCore Runtime. 
Secure production deployment 
Production deployment requires proper authentication and access control. Amazon Bedrock AgentCore Runtime integrates with Amazon Bedrock AgentCore Identity to provide enterprise-grade security. Using the Bedrock AgentCore Starter Toolkit, we can deploy our application using three simple steps: configure, launch, and invoke. 
During the configuration, a Docker file is created to guide the deployment of our agent. It contains information about the agent and its dependencies, the Amazon Bedrock AgentCore Identity configuration, and the Amazon Bedrock AgentCore Observability configuration to be used. During the launch step, AWS CodeBuild is used to run this Dockerfile and an Amazon Elastic Container Registry (Amazon ECR) repository is created to store the agent dependencies. The Amazon Bedrock AgentCore Runtime agent is then created, using the image of the ECR repository, and an endpoint is generated and used to invoke the agent in applications. If your agent is configured with OAuth authentication through Amazon Bedrock AgentCore Identity, like ours will be, you also need to pass the authentication token during the agent invocation step. The following diagram illustrates this process. 
 
The code to configure and launch our agent on Amazon Bedrock AgentCore Runtime will look as follows: 
 
 from bedrock_agentcore_starter_toolkit import Runtime

# Configure secure deployment with Cognito authentication
agentcore_runtime = Runtime()

response = agentcore_runtime.configure(
&nbsp;&nbsp; &nbsp;entrypoint="lab_helpers/lab4_runtime.py",
&nbsp;&nbsp; &nbsp;execution_role=execution_role_arn,
&nbsp;&nbsp; &nbsp;auto_create_ecr=True,
&nbsp;&nbsp; &nbsp;requirements_file="requirements.txt",
&nbsp;&nbsp; &nbsp;region=region,
&nbsp;&nbsp; &nbsp;agent_name="customer_support_agent",
&nbsp;&nbsp; &nbsp;authorizer_configuration={
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"customJWTAuthorizer": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"allowedClients": [cognito_client_id],
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"discoveryUrl": cognito_discovery_url,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp;}
)

# Deploy to production
launch_result = agentcore_runtime.launch() 
 
This configuration creates a secure endpoint that only accepts requests with valid JWT tokens from your identity provider (such as Amazon Cognito, Okta, or Entra). For our agent, we use a dummy setup with Amazon Cognito, but your application can use an identity provider of your choosing. The deployment process automatically builds your agent into a container, creates the necessary AWS infrastructure, and establishes monitoring and logging pipelines. 
Session management and isolation 
One of the most critical production features for agents is proper session management. Amazon Bedrock AgentCore Runtime automatically handles session isolation, making sure different customersâ€™ conversations donâ€™t interfere with each other: 
 
 # Customer 1 conversation
response1 = agentcore_runtime.invoke(
&nbsp;&nbsp; &nbsp;{"prompt": "My iPhone Bluetooth isn't working. What should I do?"},
&nbsp;&nbsp; &nbsp;bearer_token=auth_token,
&nbsp;&nbsp; &nbsp;session_id="session-customer-1"
)

# Customer 1 follow-up (maintains context)
response2 = agentcore_runtime.invoke(
&nbsp;&nbsp; &nbsp;{"prompt": "I've turned Bluetooth on and off but it still doesn't work"},
&nbsp;&nbsp; &nbsp;bearer_token=auth_token,
&nbsp;&nbsp; &nbsp;session_id="session-customer-1" &nbsp;# Same session, context preserved
)

# Customer 2 conversation (completely separate)
response3 = agentcore_runtime.invoke(
&nbsp;&nbsp; &nbsp;{"prompt": "Still not working. What is going on?"},
&nbsp;&nbsp; &nbsp;bearer_token=auth_token,
&nbsp;&nbsp; &nbsp;session_id="session-customer-2" &nbsp;# Different session, no context
) 
 
Customer 1â€™s follow-up maintains full context about their iPhone Bluetooth issue, whereas Customer 2â€™s message (in a different session) has no context and the agent appropriately asks for more information. This automatic session isolation is crucial for production customer support scenarios. 
Comprehensive observability with Amazon Bedrock AgentCore Observability 
Production agents need comprehensive monitoring to diagnose issues, optimize performance, and maintain reliability. Amazon Bedrock AgentCore Observability automatically instruments your agent code and sends telemetry data to Amazon CloudWatch, where you can analyze patterns and troubleshoot issues in real time. The observability data includes session-level tracking, so you can trace individual customer session interactions and understand exactly what happened during a support interaction. You can use Amazon Bedrock AgentCore Observability with an agent of your choice, hosted in Amazon Bedrock AgentCore Runtime or not. Because Amazon Bedrock AgentCore Runtime automatically integrates with Amazon Bedrock AgentCore Observability, we donâ€™t need extra work to observe our agent. 
With Amazon Bedrock AgentCore Runtime deployment, your agent is ready to be used in production. However, we still have one limitation: our agent is accessible only through SDK or API calls, requiring customers to write code or use technical tools to interact with it. For true customer-facing deployment, we need a user-friendly web interface that customers can access through their browsers. 
In the following section, we demonstrate the complete journey by building a sample web application using Streamlit, providing an intuitive chat interface that can interact with our production-ready Amazon Bedrock AgentCore Runtime endpoint. The exposed endpoint maintains the security, scalability, and observability capabilities weâ€™ve built throughout our journey from proof of concept to production. In a real-world scenario, you would integrate this endpoint with your existing customer-facing applications and UI frameworks. 
Create a customer-facing UI 
With our agent deployed to production, the final step is creating a customer-facing UI that customers can use to interface with the agent. Although SDK access works for developers, customers need an intuitive web interface for seamless support interactions. 
To demonstrate a complete solution, we build a sample Streamlit-based web-application that connects to our production-ready Amazon Bedrock AgentCore Runtime endpoint. The frontend includes secure Amazon Cognito authentication, real-time streaming responses, persistent session management, and a clean chat interface. Although we use Streamlit for rapid-prototyping, enterprises would typically integrate the endpoint with their existing interface or preferred UI frameworks. 
The end-to-end application (shown in the following diagram) maintains full conversation context across the sessions while providing the security, scalability, and observability capabilities that we built throughout this post. The result is a complete customer support agentic system that handles everything from initial authentication to complex multi-turn troubleshooting conversations, demonstrating how Amazon Bedrock AgentCore services transform prototypes into production-ready customer applications. 
 
Conclusion 
Our journey from prototype to production demonstrates how Amazon Bedrock AgentCore services address the traditional barriers to deploying enterprise-ready agentic applications. What started as a simple local customer support chatbot transformed into a comprehensive, production-grade system capable of serving multiple concurrent users with persistent memory, secure tool sharing, comprehensive observability, and an intuitive web interfaceâ€”without months of custom infrastructure development. 
The transformation required minimal code changes at each step, showcasing how Amazon Bedrock AgentCore services work together to solve the operational challenges that typically stall promising proofs of concept. Memory capabilities avoid the â€œgoldfish agentâ€ problem, centralized tool management through Amazon Bedrock AgentCore Gateway creates a reusable infrastructure that securely serves multiple use cases, Amazon Bedrock AgentCore Runtime provides enterprise-grade deployment with automatic scaling, and Amazon Bedrock AgentCore Observability delivers the monitoring capabilities operations teams need to maintain production systems. 
The following video provides an overview of AgentCore capabilities. 

 
  
 
 
Ready to build your own production-ready agent? Start with our complete end-to-end tutorial, where you can follow along with the exact code and configurations weâ€™ve explored in this post. For additional use cases and implementation patterns, explore the broader GitHub repository, and dive deeper into service capabilities and best practices in the Amazon Bedrock AgentCore documentation. 
 
About the authors 
Maira Ladeira Tanke is a Tech Lead for Agentic AI at AWS, where she enables customers on their journey to develop autonomous AI systems. With over 10 years of experience in AI/ML, Maira partners with enterprise customers to accelerate the adoption of agentic applications using Amazon Bedrock AgentCore and Strands Agents, helping organizations harness the power of foundation models to drive innovation and business transformation. In her free time, Maira enjoys traveling, playing with her cat, and spending time with her family someplace warm.

â¸»