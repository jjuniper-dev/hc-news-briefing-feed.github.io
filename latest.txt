‚úÖ Morning News Briefing ‚Äì July 05, 2025 10:53

üìÖ Date: 2025-07-05 10:53
üè∑Ô∏è Tags: #briefing #ai #publichealth #digitalgov

‚∏ª

üßæ Weather
‚Ä¢ Current Conditions:  13.4¬∞C
  Temperature: 13.4&deg;C Pressure / Tendency: 101.8 kPa falling Humidity: 95 % Dewpoint: 12.6&deg:C Wind: SE calm km/h Air Quality Health Index: n/a Observed at: Pembroke 6:00 AM EDT Saturday 5 July 2025 . Temperature: . 13.2¬∞C Pressure
‚Ä¢ Saturday: Chance of showers. High 29. POP 30%
  Mainly cloudy. 30 percent chance of showers this afternoon with risk of thunderstorm . Wind becoming southwest 20 km/h near noon. High 29. Humidex 38. UV index 9 or very high. Chance of rain in the morning with a risk of a thunderstorm in the afternoon . Forecast issued 5:00 AM EDT Saturday 5 July 2025. For the rest of the
‚Ä¢ Saturday night: Partly cloudy. Low 21.
  A few clouds. Increasing cloudiness after midnight. Wind southwest 20 km/h becoming light this evening . Low 21.50C is expected to be sunny and breezy in the morning . Forecast issued 5:00 AM EDT Saturday 5 July 2025. Weather forecast: Sunday, July 5, July 6, July 7, July 8, July 4, July 10, July 11,

üåç International News
No updates.

üçÅ Canadian News
No updates.

üá∫üá∏ U.S. Top Stories
‚Ä¢ Ukraine says it struck a Russian airbase as Russia sent drones into Ukraine
  Russia continued to pound Ukraine with hundreds of drones overnight, dashing hopes for a breakthrough in efforts to end the war . Ukraine said it struck a Russian airbase on Saturday, while Russia continued its drone strikes overnight . Ukraine says it struck an airbase in the airbase Saturday, and Russia said it had struck another airbase earlier this week . Russia has been pounding Ukraine with drones overnight
‚Ä¢ Will Trump's megabill help Democrats win the House?
  Democrats feel that Trump's tax and spending bill gives them an opening ahead of the 2026 midterms . But if they want to win back the House, they're going to have to get their own house in order first . Democrats feel they can win the House of Representatives if they win in the midterms, but they need to make sure they have their own own house is in order
‚Ä¢ 'Buy now, pay later' purchases can now affect your credit score. Here's what that means
  FICO credit scoring company will be tracking that debt . Services that split up payments into installments are increasingly popular, especially among young and low-to-middle income shoppers . FICO will track that debt, which will be tracked by the company's credit scorecard company . The company will also track the amount of money spent on each payment . It will be available to help people split up
‚Ä¢ At least 24 dead in catastrophic Texas flooding. Rescue efforts underway
  At least 24 people are dead following flooding that slammed Texas Hill Country overnight on Friday, according to officials . At least 20 girls from a summer camp remain missing, including at least 20 of them from the summer camp . Flooding is believed to be the result of heavy rains that hit Texas Hill County on Friday night, killing 24 people and injuring at least 50 others . The flooding is the
‚Ä¢ Adult education programs in limbo as Trump administration withholds grant funds
  The Trump administration is withholding $715 million for adult funding nationwide . This has left programs that serve over a million students a year scrambling for answers . This is the first time the administration has taken action to withhold funding for adult education programs nationwide . The administration has been criticized for pulling the money away from programs that help students in the U.S. It has also left programs across the

üß† Artificial Intelligence
No updates.

üíª Digital Strategy
‚Ä¢ Financial 'stretch' for UK to join Europe's Starlink rival, says minister
  UK minister says joining Europe's answer to Starlink would be a 'stretch' given the nation's current financial challenges . IRIS¬≤ remote as Britain grapples with fiscal squeeze . UK minister tells Parliament joining satellite-based mobile internet service would be "stretch" given the country's current fiscal challenges . Britain has been in the throes of a financial crisis since 2010 . IR
‚Ä¢ Ousted US copyright chief argues Trump did not have power to remove her
  Shira Perlmutter lost her job after her office published report on generative AI and fair use limits . The former head of the US Copyright Office has pushed back against arguments from President Donald Trump's team that her dismissal was lawful . President Trump's administration has argued that she was fired because of her office's report on fair use and generative artificial intelligence . She has now pushed back
‚Ä¢ Microsoft finally bids farewell to PowerShell 2.0
  Venerable command line tool to depart Windows . Users still clinging on to PowerShell 2.0 just received notice to quit as the command-line tool is officially leaving Windows . Windows users still clinging to the tool will be able to use it for the rest of the next few years . The tool is set to be replaced by Microsoft's new version of Microsoft's version of the tool .
‚Ä¢ Amazon built a massive AI supercluster for Anthropic called Project Rainier ‚Äì here's what we know so far
  Amazon Web Services (AWS) is in the process of building out a massive supercomputing cluster containing "hundreds of thousands" of accelerators . It's almost like AWS is building its own Stargate deep dive, it says . The cluster will give Anthropic's model building buddies at Anthropic a leg up in the AI arms race .‚Ä¶‚Ä¶‚Ä¶ It is almost like
‚Ä¢ Mars was once a desert with intermittent oases, Curiosity data suggests
  New modeling of carbon cycle shows unsteady but habitable history before liquid water disappeared . New models from recent Martian probe data suggest the fourth planet from the Sun once hosted a fluctuating desert environment with intermittent oases of water .‚Ä¶‚Ä¶‚Ä¶ New models show that liquid water may have been present on Mars before it evaporated in some form of life on Earth's fourth planet, Mars .

üè• Public Health
No updates.

üî¨ Science
‚Ä¢ Audio long read: How to speak to a vaccine sceptic ‚Äî research reveals what works
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Assessment of the occurrence of adverse events through the global trigger tool in a university hospital in Italy
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Changes in the comprehensive unassisted pregnancy rate as a possible marker of declining human fecundity
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Comparative performance and health economic analysis of prenatal screening for down syndrome in Fujian province, China
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Association of change in physical activity with use of outpatient specialist care and hospitalisations among breast cancer survivors with type 2 diabetes in Sweden
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations

üßæ Government & Policy
No updates.

üèõÔ∏è Enterprise Architecture & IT Governance
No updates.

ü§ñ AI & Emerging Tech
‚Ä¢ The Download: India‚Äôs AI independence, and predicting future epidemics
  This is today&#8217;s edition of¬†The Download,¬†our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



Inside India‚Äôs scramble for AI independence



Despite its status as a global tech hub, India lags far behind the likes of the US and China when it comes to homegrown AI.That gap has opened largely because India has chronically underinvested in R&amp;D, institutions, and invention. Meanwhile, since no one native language is spoken by the majority of the population, training language models is far more complicated than it is elsewhere.



So when the open-source foundation model DeepSeek-R1 suddenly outperformed many global peers, it struck a nerve. This launch by a Chinese startup prompted Indian policymakers to confront just how far behind the country was in AI infrastructure‚Äîand how urgently it needed to respond. Read the full story.



‚ÄîShadma Shaikh







Job titles of the future: Pandemic oracle



Officially, Conor Browne is a biorisk consultant. Based in Belfast, Northern Ireland, he has advanced degrees in security studies and medical and business ethics, along with United Nations certifications in counterterrorism and conflict resolution.Early in the emergence of SARS-CoV-2, international energy conglomerates seeking expert guidance on navigating the potential turmoil in markets and transportation became his main clients.&nbsp;



Having studied the 2002 SARS outbreak, he predicted the exponential spread of the new airborne virus. In fact, he forecast the epidemic‚Äôs broadscale impact and its implications for business so accurately that he has come to be seen as a pandemic oracle. Read the full story.



‚ÄîBritta Shoot



This story is from the most recent print edition of MIT Technology Review, which explores power‚Äîwho has it, and who wants it. Subscribe here to receive future copies once they drop.







The must-reads



I‚Äôve combed the internet to find you today‚Äôs most fun/important/scary/fascinating stories about technology.



1 Donald Trump‚Äôs ‚Äòbig beautiful bill‚Äô has passed&nbsp;



Which is terrible news for the clean energy industry. (Vox)+ An energy-affordability crisis is looming in the US. (The Atlantic $)+ The President struck deals with House Republican holdouts to get it over the line. (WSJ $)+ The Trump administration has shut down more than 100 climate studies. (MIT Technology Review)



2 Daniel Gross is joining Meta‚Äôs superintelligence lab¬†He‚Äôs jumping ship from the startup he co-founded with Ilya Sutskever. (Bloomberg $)+ Sutskever is stepping into the CEO role in his absence. (TechCrunch)+ Here‚Äôs what we can infer from Meta‚Äôs recent hires. (Semafor)3 AI‚Äôs energy demands could destabilize the global supplyThat‚Äôs according to the head of the world‚Äôs largest transformer maker. (FT $)+ We did the math on AI‚Äôs energy footprint. Here‚Äôs the story you haven‚Äôt heard. (MIT Technology Review)4 Elon Musk is threatening to start his own political partyWould anyone vote for him, though? (WP $)+ You‚Äôd think his bruising experience in the White House would have put him off. (NY Mag $)



5 The US has lifted exports on chip design software to ChinaIt suggests that frosty relations between the nations may be thawing. (Reuters)



6 Trump officials are going after this ICE warning appBut lawyers say there‚Äôs nothing illegal about it. (Wired $)+ Downloads of ICEBlock are rising. (NBC News)



7 Wildfires are making it harder to monitor air pollutantsCurrent tracking technology isn‚Äôt built to accommodate shifting smoke. (Undark)+ How AI can help spot wildfires. (MIT Technology Review)



8 Apple‚Äôs iOS 26 software can detect nudity on FaceTime callsThe feature will pause the call and ask if you want to continue. (Gizmodo)



9 Threads has finally launched DMsBut users are arguing there should be a way to opt out of them entirely. (TechCrunch)



10 You can hire a robot to write a handwritten note Or, y‚Äôknow, pick up a pen and write it yourself. (Insider $)







Quote of the day



‚ÄúIt‚Äôs almost like we never even spoke.‚Äù



Richard Wilson, an online dater who is convinced his most recent love interest used a chatbot to converse with him online before they awkwardly met in person, tells the Washington Post about his disappointment.







One more thing







Deepfakes of your dead loved ones are a booming Chinese businessOnce a week, Sun Kai has a video call with his mother, and they discuss his day-to-day life. But Sun‚Äôs mother died five years ago, and the person he‚Äôs talking to isn‚Äôt actually a person, but a digital replica he made of her.There are plenty of people like Sun who want to use AI to preserve, animate, and interact with lost loved ones as they mourn and try to heal. The market is particularly strong in China, where at least half a dozen companies are now offering such technologies and thousands of people have already paid for them.But some question whether interacting with AI replicas of the dead is truly a healthy way to process grief, and it‚Äôs not entirely clear what the legal and ethical implications of this technology may be. Read the full story.



‚ÄîZeyi Yang







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)+ There‚Äôs nothing cooler than wooden interiors right now.+ Talented artist Ian Robinson creates beautiful paintings of people‚Äôs vinyl collections.+ You‚Äôll find me in every one of Europe‚Äôs top wine destinations this summer.+ Here‚Äôs everything you need to remember before Stranger Things returns this fall.
‚Ä¢ Inside India‚Äôs scramble for AI independence
  In Bengaluru, India, Adithya Kolavi felt a mix of excitement and validation as he watched DeepSeek unleash its disruptive language model on the world earlier this year. The Chinese technology rivaled the best of the West in terms of benchmarks, but it had been built with far less capital in far less time.&nbsp;



‚ÄúI thought: ‚ÄòThis is how we disrupt with less,‚Äô‚Äù says Kolavi, the 20-year-old founder of the Indian AI startup CognitiveLab. ‚ÄúIf DeepSeek could do it, why not us?‚Äù&nbsp;



But for Abhishek Upperwal, founder of Soket AI Labs and architect of one of India‚Äôs earliest efforts to develop a foundation model, the moment felt more bittersweet.&nbsp;



Upperwal‚Äôs model, called Pragna-1B, had struggled to stay afloat with tiny grants while he watched global peers raise millions. The multilingual model had a relatively modest 1.25 billion parameters and was designed to reduce the ‚Äúlanguage tax,‚Äù the extra costs that arise because India‚Äîunlike the US or even China‚Äîhas a multitude of languages to support. His team had trained it, but limited resources meant it couldn‚Äôt scale. As a result, he says, the project became a proof of concept rather than a product.&nbsp;



‚ÄúIf we had been funded two years ago, there‚Äôs a good chance we‚Äôd be the ones building what DeepSeek just released,‚Äù he says.



Kolavi‚Äôs enthusiasm and Upperwal‚Äôs dismay reflect the spectrum of emotions among India‚Äôs AI builders. Despite its status as a global tech hub, the country lags far behind the likes of the US and China when it comes to homegrown AI. That gap has opened largely because India has chronically underinvested in R&amp;D, institutions, and invention. Meanwhile, since no one native language is spoken by the majority of the population, training language models is far more complicated than it is elsewhere.&nbsp;





Historically known as the global back office for the software industry, India has a tech ecosystem that evolved with a services-first mindset. Giants like Infosys and TCS built their success on efficient software delivery, but invention was neither prioritized nor rewarded. Meanwhile, India‚Äôs R&amp;D spending hovered at just 0.65% of GDP ($25.4 billion) in 2024, far behind China‚Äôs 2.68% ($476.2 billion) and the US‚Äôs 3.5% ($962.3 billion). The muscle to invent and commercialize deep tech, from algorithms to chips, was just never built.



Isolated pockets of world-class research do exist within government agencies like the DRDO (Defense Research &amp; Development Organization) and ISRO (Indian Space Research Organization), but their breakthroughs rarely spill into civilian or commercial use. India lacks the bridges to connect risk-taking research to commercial pathways, the way DARPA does in the US. Meanwhile, much of India‚Äôs top talent migrates abroad, drawn to ecosystems that better understand and, crucially, fund deep tech.So when the open-source foundation model DeepSeek-R1 suddenly outperformed many global peers, it struck a nerve. This launch by a Chinese startup prompted Indian policymakers to confront just how far behind the country was in AI infrastructure, and how urgently it needed to respond.



India responds



In January 2025, 10 days after DeepSeek-R1‚Äôs launch, the Ministry of Electronics and Information Technology (MeitY) solicited proposals for India‚Äôs own foundation models, which are large AI models that can be adapted to a wide range of tasks. Its public tender invited private-sector cloud and data‚Äëcenter companies to reserve GPU compute capacity for government‚Äëled AI research.&nbsp;



Providers including Jio, Yotta, E2E Networks, Tata, AWS partners, and CDAC responded. Through this arrangement, MeitY suddenly had access to nearly 19,000 GPUs at subsidized rates, repurposed from private infrastructure and allocated specifically to foundational AI projects. This triggered a surge of proposals from companies wanting to build their own models.&nbsp;



Within two weeks, it had 67 proposals in hand. That number tripled by mid-March.&nbsp;



In April, the government announced plans to develop six large-scale models by the end of 2025, plus 18 additional AI applications targeting sectors like agriculture, education, and climate action. Most notably, it tapped Sarvam AI to build a 70-billion-parameter model optimized for Indian languages and needs.&nbsp;



For a nation long restricted by limited research infrastructure, things moved at record speed, marking a rare convergence of ambition, talent, and political will.



‚ÄúIndia could do a Mangalyaan in AI,‚Äù said Gautam Shroff of IIIT-Delhi, referencing the country‚Äôs cost-effective, and successful, Mars orbiter mission.&nbsp;



Jaspreet Bindra, cofounder of AI&amp;Beyond, an organization focused on teaching AI literacy, captured the urgency: ‚ÄúDeepSeek is probably the best thing that happened to India. It gave us a kick in the backside to stop talking and start doing something.‚Äù



The language problem



One of the most fundamental challenges in building foundational AI models for India is the country‚Äôs sheer linguistic diversity. With 22 official languages, hundreds of dialects, and millions of people who are multilingual, India poses a problem that few existing LLMs are equipped to handle.



Whereas a massive amount of high-quality web data is available in English, Indian languages collectively make up less than 1% of online content. The lack of digitized, labeled, and cleaned data in languages like Bhojpuri and Kannada makes it difficult to train LLMs that understand how Indians actually speak or search.



Global tokenizers, which break text into units a model can process, also perform poorly on many Indian scripts, misinterpreting characters or skipping some altogether. As a result, even when Indian languages are included in multilingual models, they‚Äôre often poorly understood and inaccurately generated.



And unlike OpenAI and DeepSeek, which achieved scale using structured English-language data, Indian teams often begin with fragmented and low-quality data sets encompassing dozens of Indian languages. This makes the early steps of training foundation models far more complex.





Nonetheless, a small but determined group of Indian builders is starting to shape the country‚Äôs AI future.



For example, Sarvam AI has created OpenHathi-Hi-v0.1, an open-source Hindi language model that shows the Indian AI field‚Äôs growing ability to address the country‚Äôs vast linguistic diversity. The model, built on Meta‚Äôs Llama 2 architecture, was trained on 40 billion tokens of Hindi and related Indian-language content, making it one of the largest open-source Hindi models available to date.



Pragna-1B, the multilingual model from Upperwal, is more evidence that India could solve for its own linguistic complexity. Trained on 300 billion tokens for just $250,000, it introduced a technique called ‚Äúbalanced tokenization‚Äù to address a unique challenge in Indian AI, enabling a 1.25-billion-parameter model to behave like a much larger one.The issue is that Indian languages use complex scripts and agglutinative grammar, where words are formed by stringing together many smaller units of meaning using prefixes and suffixes. Unlike English, which separates words with spaces and follows relatively simple structures, Indian languages like Hindi, Tamil, and Kannada often lack clear word boundaries and pack a lot of information into single words. Standard tokenizers struggle with such inputs. They end up breaking Indian words into too many tokens, which bloats the input and makes it harder for models to understand the meaning efficiently or respond accurately.



With the new technique, however, ‚Äúa billion-parameter model was equivalent to a 7 billion one like Llama 2,‚Äù Upperwal says. This performance was particularly marked in Hindi and Gujarati, where global models often underperform because of limited multilingual training data. It was a reminder that with smart engineering, small teams could still push boundaries.Upperwal eventually repurposed his core tech to build speech APIs for 22 Indian languages, a more immediate solution better suited to rural users who are often left out of English-first AI experiences.



‚ÄúIf the path to AGI is a hundred-step process, training a language model is just step one,‚Äù he says.&nbsp;



At the other end of the spectrum are startups with more audacious aims. Krutrim-2, for instance, is a 12-billion-parameter multilingual language model optimized for English and 22 Indian languages.&nbsp;



Krutrim-2 is attempting to solve India‚Äôs specific problems of linguistic diversity, low-quality data, and cost constraints. The team built a custom Indic tokenizer, optimized training infrastructure, and designed models for multimodal and voice-first use cases from the start, crucial in a country where text interfaces can be a problem.



Krutrim‚Äôs bet is that its approach will not only enable Indian AI sovereignty but also offer a model for AI that works across the Global South.



Besides public funding and compute infrastructure, India also needs the institutional support of talent, the research depth, and the long-horizon capital that produce globally competitive science.



While venture capital still hesitates to bet on research, new experiments are emerging. Paras Chopra, an entrepreneur who previously built and sold the software-as-a-service company Wingify, is now personally funding Lossfunk, a Bell Labs‚Äìstyle AI residency program designed to attract independent researchers with a taste for open-source science.&nbsp;



‚ÄúWe don‚Äôt have role models in academia or industry,‚Äù says Chopra. ‚ÄúSo we‚Äôre creating a space where top researchers can learn from each other and have startup-style equity upside.‚Äù



Government-backed bet on sovereign AI



The clearest marker of India‚Äôs AI ambitions came when the government selected Sarvam AI to develop a model focused on Indian languages and voice fluency.



The idea is that it would not only help Indian companies compete in the global AI arms race but benefit the wider population as well. ‚ÄúIf it becomes part of the India stack, you can educate hundreds of millions through conversational interfaces,‚Äù says Bindra.&nbsp;



Sarvam was given access to 4,096 Nvidia H100 GPUs for training a 70-billion-parameter Indian language model over six months. (The company previously released a 2-billion-parameter model trained in 10 Indian languages, called Sarvam-1.)



Sarvam‚Äôs project and others are part of a larger strategy called the IndiaAI Mission, a $1.25 billion national initiative launched in March 2024 to build out India‚Äôs core AI infrastructure and make advanced tools more widely accessible. Led by MeitY, the mission is focused on supporting AI startups, particularly those developing foundation models in Indian languages and applying AI to key sectors such as health care, education, and agriculture.



Under its compute program, the government is deploying more than 18,000 GPUs, including nearly 13,000 high-end H100 chips, to a select group of Indian startups that currently includes Sarvam, Upperwal‚Äôs Soket Labs, Gnani AI, and Gan AI.&nbsp;



The mission also includes plans to launch a national multilingual data set repository, establish AI labs in smaller cities, and fund deep-tech R&amp;D. The broader goal is to equip Indian developers with the infrastructure needed to build globally competitive AI and ensure that the results are grounded in the linguistic and cultural realities of India and the Global South.According to Abhishek Singh, CEO of IndiaAI and an officer with MeitY, India‚Äôs broader push into deep tech is expected to raise around $12 billion in research and development investment over the next five years.&nbsp;



This includes approximately $162 million through the IndiaAI Mission, with about $32 million earmarked for direct startup funding. The National Quantum Mission is contributing another $730 million to support India‚Äôs ambitions in quantum research. In addition to this, the national budget document for 2025-26 announced a $1.2 billion Deep Tech Fund of Funds aimed at catalyzing early-stage innovation in the private sector.



The rest, nearly $9.9 billion, is expected to come from private and international sources including corporate R&amp;D, venture capital firms, high-net-worth individuals, philanthropists, and global technology leaders such as Microsoft.&nbsp;



IndiaAI has now received more than 500 applications from startups proposing use cases in sectors like health, governance, and agriculture.&nbsp;



‚ÄúWe‚Äôve already announced support for Sarvam, and 10 to 12 more startups will be funded solely for foundational models,‚Äù says Singh. Selection criteria include access to training data, talent depth, sector fit, and scalability.



Open or closed?



The IndiaAI program, however, is not without controversy. Sarvam is being built as a closed model, not open-source, despite its public tech roots. That has sparked debate about the proper balance between private enterprise and the public good.&nbsp;



‚ÄúTrue sovereignty should be rooted in openness and transparency,‚Äù says Amlan Mohanty, an AI policy specialist. He points to DeepSeek-R1, which despite its 236-billion parameter size was made freely available for commercial use.&nbsp;





Its release allowed developers around the world to fine-tune it on low-cost GPUs, creating faster variants and extending its capabilities to non-English applications.



‚ÄúReleasing an open-weight model with efficient inference can democratize AI,‚Äù says Hancheng Cao, an assistant professor of information systems and operations management at Emory University. ‚ÄúIt makes it usable by developers who don‚Äôt have massive infrastructure.‚Äù



IndiaAI, however, has taken a neutral stance on whether publicly funded models should be open-source.&nbsp;



‚ÄúWe didn‚Äôt want to dictate business models,‚Äù says Singh. ‚ÄúIndia has always supported open standards and open source, but it‚Äôs up to the teams. The goal is strong Indian models, whatever the route.‚Äù



There are other challenges as well. In late May, Sarvam‚ÄØAI unveiled Sarvam‚ÄëM, a 24-billion-parameter multilingual LLM fine-tuned for 10 Indian languages and built on top of Mistral‚ÄØSmall, an efficient model developed by the French company Mistral AI. Sarvam‚Äôs cofounder Vivek‚ÄØRaghavan called the model ‚Äúan important stepping stone on our journey to build sovereign AI for India.‚Äù But its download numbers were underwhelming, with only 300 in the first two days. The venture capitalist Deedy Das called the launch ‚Äúembarrassing.‚ÄùAnd the issues go beyond the lukewarm early reception. Many developers in India still lack easy access to GPUs and the broader ecosystem for Indian-language AI applications is still nascent.&nbsp;



The compute question



Compute scarcity is emerging as one of the most significant bottlenecks in generative AI, not just in India but across the globe. For countries still heavily reliant on imported GPUs and lacking domestic fabrication capacity, the cost of building and running large models is often prohibitive.&nbsp;



India still imports most of its chips rather than producing them domestically, and training large models remains expensive. That‚Äôs why startups and researchers alike are focusing on software-level efficiencies that involve smaller models, better inference, and fine-tuning frameworks that optimize for performance on fewer GPUs.



‚ÄúThe absence of infrastructure doesn‚Äôt mean the absence of innovation,‚Äù says Cao. ‚ÄúSupporting optimization science is a smart way to work within constraints.‚Äù&nbsp;



Yet Singh of IndiaAI argues that the tide is turning on the infrastructure challenge thanks to the new government programs and private-public partnerships. ‚ÄúI believe that within the next three months, we will no longer face the kind of compute bottlenecks we saw last year,‚Äù he says.



India also has a cost advantage.According to Gupta, building a hyperscale data center in India costs about $5 million, roughly half what it would cost in markets like the US, Europe, or Singapore. That‚Äôs thanks to affordable land, lower construction and labor costs, and a large pool of skilled engineers.&nbsp;



For now, India‚Äôs AI ambitions seem less about leapfrogging OpenAI or DeepSeek and more about strategic self-determination. Whether its approach takes the form of smaller sovereign models, open ecosystems, or public-private hybrids, the country is betting that it can chart its own course.&nbsp;



While some experts argue that the government‚Äôs action, or reaction (to DeepSeek), is performative and aligned with its nationalistic agenda, many startup founders are energized. They see the growing collaboration between the state and the private sector as a real opportunity to overcome India&#8217;s long-standing structural challenges in tech innovation.



At a Meta summit held in Bengaluru last year, Nandan Nilekani, the chairman of Infosys, urged India to resist chasing a me-too AI dream.&nbsp;



‚ÄúLet the big boys in the Valley do it,‚Äù he said of building LLMs. ‚ÄúWe will use it to create synthetic data, build small language models quickly, and train them using appropriate data.‚Äù&nbsp;



His view that India should prioritize strength over spectacle had a divided reception. But it reflects a broader growing consensus on whether India should play a different game altogether.



‚ÄúTrying to dominate every layer of the stack isn‚Äôt realistic, even for China,‚Äù says Shobhankita Reddy, a researcher at the Takshashila Institution, an Indian public policy nonprofit. ‚ÄúDominate one layer, like applications, services, or talent, so you remain indispensable.‚Äù¬†



Correction: We amended Reddy&#8217;s name
‚Ä¢ The Download: AI agents hype, and Google‚Äôs electricity plans
  This is today&#8217;s edition of¬†The Download,¬†our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



Don‚Äôt let hype about AI agents get ahead of reality



‚ÄîYoav Shoham is a professor emeritus at Stanford University and cofounder of AI21 Labs.



At Google‚Äôs I/O 2025 event in May, the company showed off a digital assistant that didn‚Äôt just answer questions; it helped work on a bicycle repair by finding a matching user manual, locating a YouTube tutorial, and even calling a local store to ask about a part, all with minimal human nudging. Such capabilities could soon extend far outside the Google ecosystem.The vision is exciting: Intelligent software agents that act like digital coworkers, booking your flights, rescheduling meetings, filing expenses, and talking to each other behind the scenes to get things done.But if we‚Äôre not careful, we‚Äôre going to derail the whole idea before it has a chance to deliver real benefits. As with many tech trends, there‚Äôs a risk of hype racing ahead of reality. And when expectations get out of hand, a backlash isn‚Äôt far behind. Read the full story.







Google‚Äôs electricity demand is skyrocketing



We got two big pieces of energy news from Google this week. The company announced that it‚Äôs signed an agreement to purchase electricity from a fusion company‚Äôs forthcoming first power plant. Google also released its latest environmental report, which shows that its energy use from data centers has doubled since 2020.



Taken together, these two bits of news offer a fascinating look at just how desperately big tech companies are hunting for clean electricity to power their data centers as energy demand and emissions balloon in the age of AI. Of course, we don‚Äôt know exactly how much of this pollution is attributable to AI because Google doesn‚Äôt break that out. (Also a problem!) So, what‚Äôs next and what does this all mean?



‚ÄîCasey Crownhart



This article is from The Spark, MIT Technology Review‚Äôs weekly climate newsletter. To receive it in your inbox every Wednesday, sign up here.+ To read more about whether nuclear energy is really a viable way to power the AI boom, check out Casey‚Äôs recent article, which is part of Power Hungry: AI and our energy future‚Äîour new series shining a light on the energy demands and carbon costs of the artificial intelligence revolution. You can take a look at the rest of the package here.







The must-reads



I‚Äôve combed the internet to find you today‚Äôs most fun/important/scary/fascinating stories about technology.



1 Meta‚Äôs climate tool was ‚Äòtrained using faulty data‚ÄôScientists claim it raised false hopes about the feasibility of removing carbon dioxide from the atmosphere. (FT $)+ xAI‚Äôs gas turbines have been greenlit, despite community backlash. (Wired $)+ Why we need to shoot carbon dioxide thousands of feet underground. (MIT Technology Review)



2 We don‚Äôt know whether US insurers will cover vaccines for kidsMajor insurers haven‚Äôt confirmed whether they‚Äôll keep covering the costs of shots. (Wired $)+ What‚Äôs next for the Gates Foundation‚Äôs global health initiatives? (Undark)+ How measuring vaccine hesitancy could help health professionals tackle it. (MIT Technology Review)



3 The Trump administration wants to gut Biden‚Äôs climate lawThe Inflation Reduction Act‚Äôs green energy tax incentives are hanging in the balance. (WP $)+ It‚Äôs bad news for one of the US economy‚Äôs biggest growth sectors. (Vox)+ How are we going to feed the world without making climate change worse? (New Yorker $)+ The President threatened to unravel the landmark law long before he was elected. (MIT Technology Review)



4 There are certain tells a scientific study abstract has been written by AIUse of hundreds of words has shot up since ChatGPT was made public. (NYT $)+ Beware over-reliance on AI-text detection tools, though. (MIT Technology Review)



5 Elon Musk doesn‚Äôt care about cars any moreWhich is terrible news for Tesla and its investors. (WSJ $)+ Things aren‚Äôt looking too hot for Rivian, either. (Insider $)



6 America‚Äôs weather forecasting is getting worseJust a year ago, US storm forecasting was the best it had ever been. Now, its accuracy is rapidly declining. (The Atlantic $)



7 Brazil has sustainable data center ambitionsEnvironmentalists aren‚Äôt convinced, however. (Rest of World)



8 A mysterious object has been spotted passing through the solar systemAnd we‚Äôve got good reason to believe it originated outside our system. (Ars Technica)



9 A rising band on Spotify is probably AI-generatedBut no one seems able to say for sure. (Vice)



10 The homes float in flood waterIt‚Äôs one solution to building homes on known flood plains. (Fast Company $)+ How to stop a state from sinking. (MIT Technology Review)







Quote of the day



‚ÄúAI doesn‚Äôt know what an orgasm sounds like.‚Äù&nbsp;



‚ÄîAnnabelle Tudor, an audiobook narrator, tells the Guardian why she‚Äôs not convinced by the industry‚Äôs plans to have AI narrate audiobooks.







One more thing







Who gets to decide who receives experimental medical treatments?



There has been a trend toward lowering the bar for new medicines, and it is becoming easier for people to access treatments that might not help them‚Äîand could even harm them. Anecdotes appear to be overpowering evidence in decisions on drug approval. As a result, we‚Äôre ending up with some drugs that don‚Äôt work.



We urgently need to question how these decisions are made. Who should have access to experimental therapies? And who should get to decide? Such questions are especially pressing considering how quickly biotechnology is advancing. We‚Äôre not just improving on existing classes of treatments‚Äîwe‚Äôre creating entirely new ones. Read the full story.



‚ÄîJessica Hamzelou







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)



+ These aerial shots of Glastonbury festival are crazy.+ Our oceans really are amazing places‚Äîtake a moment to appreciate them.+ How to be truly cool, according to science.+ Happy 62nd birthday to Tracey Emin, still an enfant terrible after all these years.
‚Ä¢ Google‚Äôs electricity demand is skyrocketing
  We got two big pieces of energy news from Google this week. The company announced that it‚Äôs signed an agreement to purchase electricity from a fusion company‚Äôs forthcoming first power plant. Google also released its latest environmental report, which shows that its energy use from data centers has doubled since 2020.



Taken together, these two bits of news offer a fascinating look at just how desperately big tech companies are hunting for clean electricity to power their data centers as energy demand and emissions balloon in the age of AI. Of course, we don‚Äôt know exactly how much of this pollution is attributable to AI because Google doesn‚Äôt break that out. (Also a problem!) So, what‚Äôs next and what does this all mean?&nbsp;





Let‚Äôs start with fusion: Google‚Äôs deal with Commonwealth Fusion Systems is intended to provide the tech giant with 200 megawatts of power. This will come from Commonwealth‚Äôs first commercial plant, a facility planned for Virginia that the company refers to as the Arc power plant. The agreement represents half its capacity.



What‚Äôs important to note here is that this power plant doesn‚Äôt exist yet. In fact, Commonwealth still needs to get its Sparc demonstration reactor, located outside Boston, up and running. That site, which I visited in the fall, should be completed in 2026.



(An aside: This isn‚Äôt the first deal between Big Tech and a fusion company. Microsoft signed an agreement with Helion a couple of years ago to buy 50 megawatts of power from a planned power plant, scheduled to come online in 2028. Experts expressed skepticism in the wake of that deal, as my colleague James Temple reported.)



Nonetheless, Google‚Äôs announcement is a big moment for fusion, in part because of the size of the commitment and also because Commonwealth, a spinout company from MIT‚Äôs Plasma Science and Fusion Center, is seen by many in the industry as a likely candidate to be the first to get a commercial plant off the ground. (MIT Technology Review is owned by MIT but is editorially independent.)



Google leadership was very up-front about the length of the timeline. ‚ÄúWe would certainly put this in the long-term category,‚Äù said Michael Terrell, Google‚Äôs head of advanced energy, in a press call about the deal.



The news of Google‚Äôs foray into fusion comes just days after the tech giant‚Äôs release of its latest environmental report. While the company highlighted some wins, some of the numbers in this report are eye-catching, and not in a positive way.



Google‚Äôs emissions have increased by over 50% since 2019, rising 6% in the last year alone. That‚Äôs decidedly the wrong direction for a company that‚Äôs set a goal to reach net-zero greenhouse-gas emissions by the end of the decade.



It‚Äôs true that the company has committed billions to clean energy projects, including big investments in next-generation technologies like advanced nuclear and enhanced geothermal systems. Those deals have helped dampen emissions growth, but it‚Äôs an arguably impossible task to keep up with the energy demand the company is seeing.



Google‚Äôs electricity consumption from data centers was up 27% from the year before. It‚Äôs doubled since 2020, reaching over 30 terawatt-hours. That‚Äôs nearly the annual electricity consumption from the entire country of Ireland.



As an outsider, it‚Äôs tempting to point the finger at AI, since that technology has crashed into the mainstream and percolated into every corner of Google‚Äôs products and business. And yet the report downplays the role of AI. Here‚Äôs one bit that struck me:



‚ÄúHowever, it‚Äôs important to note that our growing electricity needs aren‚Äôt solely driven by AI. The accelerating growth of Google Cloud, continued investments in Search, the expanding reach of YouTube, and more, have also contributed to this overall growth.‚Äù



There is enough wiggle room in that statement to drive a large electric truck through. When I asked about the relative contributions here, company representative Mara Harris said via email that they don‚Äôt break out what portion comes from AI. When I followed up asking if the company didn‚Äôt have this information or just wouldn‚Äôt share it, she said she‚Äôd check but didn‚Äôt get back to me.



I‚Äôll make the point here that we‚Äôve made before, including in our recent package on AI and energy: Big companies should be disclosing more about the energy demands of AI. We shouldn‚Äôt be guessing at this technology‚Äôs effects.



Google has put a ton of effort and resources into setting and chasing ambitious climate goals. But as its energy needs and those of the rest of the industry continue to explode, it‚Äôs obvious that this problem is getting tougher, and it‚Äôs also clear that more transparency is a crucial part of the way forward.



This article is from The Spark, MIT Technology Review‚Äôs weekly climate newsletter. To receive it in your inbox every Wednesday, sign up here.
‚Ä¢ Don‚Äôt let hype about AI agents get ahead of reality
  Google‚Äôs recent unveiling of what it calls a ‚Äúnew class of agentic experiences‚Äù feels like a turning point. At its I/O 2025 event in May, for example, the company showed off a digital assistant that didn‚Äôt just answer questions; it helped work on a bicycle repair by finding a matching user manual, locating a YouTube tutorial, and even calling a local store to ask about a part, all with minimal human nudging. Such capabilities could soon extend far outside the Google ecosystem. The company has introduced an open standard called Agent-to-Agent, or A2A, which aims to let agents from different companies talk to each other and work together.



The vision is exciting: Intelligent software agents that act like digital coworkers, booking your flights, rescheduling meetings, filing expenses, and talking to each other behind the scenes to get things done. But if we‚Äôre not careful, we‚Äôre going to derail the whole idea before it has a chance to deliver real benefits. As with many tech trends, there‚Äôs a risk of hype racing ahead of reality. And when expectations get out of hand, a backlash isn‚Äôt far behind.





Let‚Äôs start with the term ‚Äúagent‚Äù itself. Right now, it‚Äôs being slapped on everything from simple scripts to sophisticated AI workflows. There‚Äôs no shared definition, which leaves plenty of room for companies to market basic automation as something much more advanced. That kind of ‚Äúagentwashing‚Äù doesn‚Äôt just confuse customers; it invites disappointment. We don‚Äôt necessarily need a rigid standard, but we do need clearer expectations about what these systems are supposed to do, how autonomously they operate, and how reliably they perform.



And reliability is the next big challenge. Most of today‚Äôs agents are powered by large language models (LLMs), which generate probabilistic responses. These systems are powerful, but they‚Äôre also unpredictable. They can make things up, go off track, or fail in subtle ways‚Äîespecially when they‚Äôre asked to complete multistep tasks, pulling in external tools and chaining LLM responses together. A recent example: Users of Cursor, a popular AI programming assistant, were told by an automated support agent that they couldn‚Äôt use the software on more than one device. There were widespread complaints and reports of users canceling their subscriptions. But it turned out the policy didn‚Äôt exist. The AI had invented it.



In enterprise settings, this kind of mistake could create immense damage. We need to stop treating LLMs as standalone products and start building complete systems around them‚Äîsystems that account for uncertainty, monitor outputs, manage costs, and layer in guardrails for safety and accuracy. These measures can help ensure that the output adheres to the requirements expressed by the user, obeys the company‚Äôs policies regarding access to information, respects privacy issues, and so on. Some companies, including AI21 (which I cofounded and which has received funding from Google), are already moving in that direction, wrapping language models in more deliberate, structured architectures. Our latest launch, Maestro, is designed for enterprise reliability, combining LLMs with company data, public information, and other tools to ensure dependable outputs.



Still, even the smartest agent won‚Äôt be useful in a vacuum. For the agent model to work, different agents need to cooperate (booking your travel, checking the weather, submitting your expense report) without constant human supervision. That‚Äôs where Google‚Äôs A2A protocol comes in. It‚Äôs meant to be a universal language that lets agents share what they can do and divide up tasks. In principle, it‚Äôs a great idea.In practice, A2A still falls short. It defines how agents talk to each other, but not what they actually mean. If one agent says it can provide ‚Äúwind conditions,‚Äù another has to guess whether that‚Äôs useful for evaluating weather on a flight route. Without a shared vocabulary or context, coordination becomes brittle. We‚Äôve seen this problem before in distributed computing. Solving it at scale is far from trivial.





There‚Äôs also the assumption that agents are naturally cooperative. That may hold inside Google or another single company‚Äôs ecosystem, but in the real world, agents will represent different vendors, customers, or even competitors. For example, if my travel planning agent is requesting price quotes from your airline booking agent, and your agent is incentivized to favor certain airlines, my agent might not be able to get me the best or least expensive itinerary. Without some way to align incentives through contracts, payments, or game-theoretic mechanisms, expecting seamless collaboration may be wishful thinking.



None of these issues are insurmountable. Shared semantics can be developed. Protocols can evolve. Agents can be taught to negotiate and collaborate in more sophisticated ways. But these problems won‚Äôt solve themselves, and if we ignore them, the term ‚Äúagent‚Äù will go the way of other overhyped tech buzzwords. Already, some CIOs are rolling their eyes when they hear it.



That‚Äôs a warning sign. We don‚Äôt want the excitement to paper over the pitfalls, only to let developers and users discover them the hard way and develop a negative perspective on the whole endeavor. That would be a shame. The potential here is real. But we need to match the ambition with thoughtful design, clear definitions, and realistic expectations. If we can do that, agents won‚Äôt just be another passing trend; they could become the backbone of how we get things done in the digital world.



Yoav Shoham is a professor emeritus at Stanford University and cofounder of AI21 Labs. His 1993 paper on agent-oriented programming received the AI Journal Classic Paper Award. He is coauthor of Multiagent Systems: Algorithmic, Game-Theoretic, and Logical Foundations, a standard textbook in the field.

üîí Cybersecurity & Privacy
‚Ä¢ Big Tech‚Äôs Mixed Response to U.S. Treasury Sanctions
  In May 2025, the U.S. government sanctioned a Chinese national for operating a cloud provider linked to the majority of virtual currency investment scam websites reported to the FBI. But a new report finds the accused continues to operate a slew of established accounts at American tech companies &#8212; including Facebook, Github, PayPal and Twitter/X.
On May 29, the U.S. Department of the Treasury¬†announced economic sanctions against Funnull Technology Inc., a Philippines-based company alleged to provide infrastructure for hundreds of thousands of websites involved in virtual currency investment scams known as &#8220;pig butchering.&#8221; In January 2025, KrebsOnSecurity detailed how Funnull was designed as a content delivery network that catered to foreign cybercriminals seeking to route their traffic through U.S.-based cloud providers.

The Treasury also sanctioned Funnull&#8217;s alleged operator, a 40-year-old Chinese national named Liu &#8220;Steve&#8221; Lizhi. The government says Funnull directly facilitated financial schemes resulting in more than $200 million in financial losses by Americans, and that the company&#8217;s operations were linked to the majority of pig butchering scams reported to the FBI.
It is generally illegal for U.S. companies or individuals to transact with people sanctioned by the Treasury. However, as Mr. Lizhi&#8217;s case makes clear, just because someone is sanctioned doesn&#8217;t necessarily mean big tech companies are going to suspend their online accounts.
The government says Lizhi was born November 13, 1984, and used the nicknames &#8220;XXL4&#8221; and &#8220;Nice Lizhi.&#8221; Nevertheless, Steve Liu&#8217;s 17-year-old account on LinkedIn (in the name &#8220;Liulizhi&#8221;) had hundreds of followers (Lizhi&#8217;s LinkedIn profile helpfully confirms his birthday) until quite recently: The account was deleted this morning, just hours after KrebsOnSecurity sought comment from LinkedIn.
Mr. Lizhi&#8217;s LinkedIn account was suspended sometime in the last 24 hours, after KrebsOnSecurity sought comment from LinkedIn.
In an emailed response, a LinkedIn spokesperson said the company&#8217;s &#8220;Prohibited countries policy&#8221; states that LinkedIn &#8220;does not sell, license, support or otherwise make available its Premium accounts or other paid products and services to individuals and companies sanctioned by the U.S. government.&#8221; LinkedIn declined to say whether the profile in question was a premium or free account.
Mr. Lizhi also maintains a working PayPal account under the name Liu Lizhi and username &#8220;@nicelizhi,&#8221; another nickname listed in the Treasury sanctions. PayPal did not respond to a request for comment. A 15-year-old Twitter/X account named &#8220;Lizhi&#8221; that links to Mr. Lizhi&#8217;s personal domain remains active, although it has few followers and hasn&#8217;t posted in years.
These accounts and many others were flagged by the security firm Silent Push, which has been tracking Funnull&#8217;s operations for the past year and calling out U.S. cloud providers like Amazon and Microsoft for failing to more quickly sever ties with the company.
Liu Lizhi&#8217;s PayPal account.
In a report released today, Silent Push found Lizhi still operates numerous Facebook accounts and groups, including a private Facebook account under the name Liu Lizhi. Another Facebook account clearly connected to Lizhi is a tourism page for Ganzhou, China called &#8220;EnjoyGanzhou&#8221; that was named in the Treasury Department sanctions.
&#8220;This guy is the technical administrator for the infrastructure that is hosting a majority of scams targeting people in the United States, and hundreds of millions have been lost based on the websites he&#8217;s been hosting,&#8221; said Zach Edwards, senior threat researcher at Silent Push. &#8220;It&#8217;s crazy that the vast majority of big tech companies haven&#8217;t done anything to cut ties with this guy.&#8221;
The FBI says it received nearly 150,000 complaints last year involving digital assets and $9.3 billion in losses &#8212; a 66 percent increase from the previous year. Investment scams were the top crypto-related crimes reported, with $5.8 billion in losses.
In a statement, a Meta spokesperson said the company continuously takes steps to meet its legal obligations, but that sanctions laws are complex and varied. They explained that sanctions are often targeted in nature and don&#8217;t always prohibit people from having a presence on its platform. Nevertheless, Meta confirmed it had removed the account, unpublished Pages, and removed Groups and events associated with the user for violating its policies.
Attempts to reach Mr. Lizhi via his primary email addresses at Hotmail and Gmail bounced as undeliverable. Likewise, his 14-year-old YouTube channel appears to have been taken down recently.
However, anyone interested in viewing or using Mr. Lizhi&#8217;s 146 computer code repositories will have no problem finding GitHub accounts for him, including one registered under the NiceLizhi and XXL4 nicknames mentioned in the Treasury sanctions.
One of multiple GitHub profiles used by Liu &#8220;Steve&#8221; Lizhi, who uses the nickname XXL4 (a moniker listed in the Treasury sanctions for Mr. Lizhi).
Mr. Lizhi also operates a GitHub page for an open source e-commerce platform called NexaMerchant, which advertises itself as a payment gateway working with numerous American financial institutions. Interestingly, this profile&#8217;s &#8220;followers&#8221; page shows several other accounts that appear to be Mr. Lizhi&#8217;s. All of the account&#8217;s followers are tagged as &#8220;suspended,&#8221; even though that suspended message does not display when one visits those individual profiles.
In response to questions, GitHub said it has a process in place to identify when users and customers are Specially Designated Nationals or other denied or blocked parties, but that it locks those accounts instead of removing them. According to its policy, GitHub takes care that users and customers aren&#8217;t impacted beyond what is required by law.
All of the follower accounts for the XXL4 GitHub account appear to be Mr. Lizhi&#8217;s, and have been suspended by GitHub, but their code is still accessible.
&#8220;This includes keeping public repositories, including those for open source projects, available and accessible to support personal communications involving developers in sanctioned regions,&#8221; the policy states. &#8220;This also means GitHub will advocate for developers in sanctioned regions to enjoy greater access to the platform and full access to the global open source community.&#8221;
Edwards said it&#8217;s great that GitHub has a process for handling sanctioned accounts, but that the process doesn&#8217;t seem to communicate risk in a transparent way, noting that the only indicator on the locked accounts is the message, &#8220;This repository has been archived by the owner. It is not read-only.&#8221;
&#8220;It&#8217;s an odd message that doesn&#8217;t communicate, &#8216;This is a sanctioned entity, don&#8217;t fork this code or use it in a production environment&#8217;,&#8221; Edwards said.
Mark Rasch is a former federal cybercrime prosecutor who now serves as counsel for the New York City based security consulting firm Unit 221B. Rasch said when Treasury&#8217;s Office of Foreign Assets Control (OFAC) sanctions a person or entity, it then becomes illegal for businesses or organizations to transact with the sanctioned party.
Rasch said financial institutions have very mature systems for severing accounts tied to people who become subject to OFAC sanctions, but that tech companies may be far less proactive &#8212; particularly with free accounts.
&#8220;Banks have established ways of checking [U.S. government sanctions lists] for sanctioned entities, but tech companies don&#8217;t necessarily do a good job with that, especially for services that you can just click and sign up for,&#8221; Rasch said. &#8220;It&#8217;s potentially a risk and liability for the tech companies involved, but only to the extent OFAC is willing to enforce it.&#8221;
Liu Lizhi operates numerous Facebook accounts and groups, including this one for an entity specified in the OFAC sanctions: The &#8220;Enjoy Ganzhou&#8221; tourism page for Ganzhou, China. Image: Silent Push.
In July 2024, Funnull purchased the domain polyfill[.]io, the longtime home of a legitimate open source project that allowed websites to ensure that devices using legacy browsers could still render content in newer formats. After the Polyfill domain changed hands, at least 384,000 websites were caught in a supply-chain attack that redirected visitors to malicious sites. According to the Treasury, Funnull used the code to redirect people to scam websites and online gambling sites, some of which were linked to Chinese criminal money laundering operations.
The U.S. government says Funnull provides domain names for websites on its purchased IP addresses, using domain generation algorithms (DGAs) ‚Äî programs that generate large numbers of similar but unique names for websites ‚Äî and that it sells web design templates to cybercriminals.
&#8220;These services not only make it easier for cybercriminals to impersonate trusted brands when creating scam websites, but also allow them to quickly change to different domain names and IP addresses when legitimate providers attempt to take the websites down,&#8221; reads a Treasury statement.
Meanwhile, Funnull appears to be morphing nearly all aspects of its business in the wake of the sanctions, Edwards said.
&#8220;Whereas before they might have used 60 DGA domains to hide and bounce their traffic, we&#8217;re seeing far more now,&#8221; he said. &#8220;They&#8217;re trying to make their infrastructure harder to track and more complicated, so for now they&#8217;re not going away but more just changing what they&#8217;re doing. And a lot more organizations should be holding their feet to the fire.&#8221;
Update, 2:48 PM ET: Added response from Meta, which confirmed it has closed the accounts and groups connected to Mr. Lizhi.
‚Ä¢ Senator Chides FBI for Weak Advice on Mobile Security
  Agents with the Federal Bureau of Investigation (FBI) briefed Capitol Hill staff recently on hardening the security of their mobile devices, after a contacts list stolen from the personal phone of the White House Chief of Staff Susie Wiles was reportedly used to fuel a series of text messages and phone calls impersonating her to U.S. lawmakers. But in a letter this week to the FBI, one of the Senate&#8217;s most tech-savvy lawmakers says the feds aren&#8217;t doing enough to recommend more appropriate security protections that are already built into most consumer mobile devices.
A screenshot of the first page from Sen. Wyden&#8217;s letter to FBI Director Kash Patel.
On May 29, The Wall Street Journal reported that federal authorities were investigating a clandestine effort to impersonate Ms. Wiles via text messages and in phone calls that may have used AI to spoof her voice. According to The Journal, Wiles told associates her cellphone contacts were hacked, giving the impersonator access to the private phone numbers of some of the country&#8217;s most influential people.
The execution of this phishing and impersonation campaign &#8212; whatever its goals may have been &#8212; suggested the attackers were financially motivated, and not particularly sophisticated.
&#8220;It became clear to some of the lawmakers that the requests were suspicious when the impersonator began asking questions about Trump that Wiles should have known the answers to‚Äîand in one case, when the impersonator asked for a cash transfer, some of the people said,&#8221; the Journal wrote. &#8220;In many cases, the impersonator‚Äôs grammar was broken and the messages were more formal than the way Wiles typically communicates, people who have received the messages said. The calls and text messages also didn‚Äôt come from Wiles‚Äôs phone number.&#8221;
Sophisticated or not, the impersonation campaign was soon punctuated by the murder of Minnesota House of Representatives Speaker Emerita Melissa Hortman and her husband, and the shooting of Minnesota State Senator John Hoffman and his wife. So when FBI agents offered in mid-June to brief U.S. Senate staff on mobile threats, more than 140 staffers took them up on that invitation (a remarkably high number considering that no food was offered at the event).
But according to Sen. Ron Wyden (D-Ore.), the advice the FBI provided to Senate staffers was largely limited to remedial tips, such as not clicking on suspicious links or attachments, not using public wifi networks, turning off bluetooth, keeping phone software up to date, and rebooting regularly.
&#8220;This is insufficient to protect Senate employees and other high-value targets against foreign spies using advanced cyber tools,&#8221; Wyden wrote in a letter sent today to FBI Director Kash Patel. &#8220;Well-funded foreign intelligence agencies do not have to rely on phishing messages and malicious attachments to infect unsuspecting victims with spyware. Cyber mercenary companies sell their government customers advanced &#8216;zero-click&#8217; capabilities to deliver spyware that do not require any action by the victim.&#8221;
Wyden stressed that to help counter sophisticated attacks, the FBI should be encouraging lawmakers and their staff to enable anti-spyware defenses that are built into Apple&#8217;s iOS and Google&#8217;s Android phone software.
These include Apple&#8217;s Lockdown Mode, which is designed for users who are worried they may be subject to targeted attacks. Lockdown Mode restricts non-essential iOS features to reduce the device&#8217;s overall attack surface. Google Android devices carry a similar feature called Advanced Protection Mode.
Wyden also urged the FBI to update its training to recommend a number of other steps that people can take to make their mobile devices less trackable, including the use of ad blockers to guard against malicious advertisements, disabling ad tracking IDs in mobile devices, and opting out of commercial data brokers (the suspect charged in the Minnesota shootings reportedly used multiple people-search services to find the home addresses of his targets).
The senator&#8217;s letter notes that while the FBI has recommended all of the above precautions in various advisories issued over the years, the advice the agency is giving now to the nation&#8217;s leaders needs to be more comprehensive, actionable and urgent.
&#8220;In spite of the seriousness of the threat, the FBI has yet to provide effective defensive guidance,&#8221; Wyden said.
Nicholas Weaver is a researcher with the International Computer Science Institute, a nonprofit in Berkeley, Calif. Weaver said Lockdown Mode or Advanced Protection will mitigate many vulnerabilities, and should be the default setting for all members of Congress and their staff.
&#8220;Lawmakers are at exceptional risk and need to be exceptionally protected,&#8221; Weaver said. &#8220;Their computers should be locked down and well administered, etc. And the same applies to staffers.&#8221;
Weaver noted that Apple&#8217;s Lockdown Mode has a track record of blocking zero-day attacks on iOS applications; in September 2023, Citizen Lab documented how Lockdown Mode foiled a zero-click flaw capable of installing spyware on iOS devices without any interaction from the victim.

Earlier this month, Citizen Lab researchers documented a zero-click attack used to infect the iOS devices of two journalists with Paragon&#8217;s Graphite spyware. The vulnerability could be exploited merely by sending the target a booby-trapped media file delivered via iMessage. Apple also recently updated its advisory for the zero-click flaw (CVE-2025-43200), noting that it was mitigated as of iOS 18.3.1, which was released in February 2025.
Apple has not commented on whether CVE-2025-43200 could be exploited on devices with Lockdown Mode turned on. But HelpNetSecurity observed that at the same time Apple addressed CVE-2025-43200 back in February, the company fixed another vulnerability flagged by Citizen Lab researcher Bill Marczak: CVE-2025-24200, which Apple said was used in an extremely sophisticated physical attack against specific targeted individuals that allowed attackers to disable USB Restricted Mode on a locked device.
In other words, the flaw could apparently be exploited only if the attacker had physical access to the targeted vulnerable device. And as the old infosec industry adage goes, if an adversary has physical access to your device, it&#8217;s most likely not your device anymore.
I can&#8217;t speak to Google&#8217;s Advanced Protection Mode personally, because I don&#8217;t use Google or Android devices. But I have had Apple&#8217;s Lockdown Mode enabled on all of my Apple devices since it was first made available in September 2022. I can only think of a single occasion when one of my apps failed to work properly with Lockdown Mode turned on, and in that case I was able to add a temporary exception for that app in Lockdown Mode&#8217;s settings.
My main gripe with Lockdown Mode was captured in a March 2025 column by TechCrunch&#8217;s Lorenzo Francheschi-Bicchierai, who wrote about its penchant for periodically sending mystifying notifications that someone has been blocked from contacting you, even though nothing then prevents you from contacting that person directly. This has happened to me at least twice, and in both cases the person in question was already an approved contact, and said they had not attempted to reach out.
Although it would be nice if Apple&#8217;s Lockdown Mode sent fewer, less alarming and more informative alerts, the occasional baffling warning message is hardly enough to make me turn it off.

üéì University AI
No updates.

üè¢ Corporate AI
‚Ä¢ AI Testing and Evaluation: Learnings from genome editing
  Generative AI presents a unique challenge and opportunity to reexamine governance practices for the responsible development, deployment, and use of AI. To advance thinking in this space, Microsoft has tapped into the experience and knowledge of experts across domains‚Äîfrom genome editing to cybersecurity‚Äîto investigate the role of testing and evaluation as a governance tool. AI Testing and Evaluation: Learnings from Science and Industry, hosted by Microsoft Research‚Äôs Kathleen Sullivan, explores what the technology industry and policymakers can learn from these fields and how that might help shape the course of AI development.



In this episode, Alta Charo (opens in new tab), emerita professor of law and bioethics at the University of Wisconsin‚ÄìMadison, joins Sullivan for a conversation on the evolving landscape of genome editing and its regulatory implications. Drawing on decades of experience in biotechnology policy, Charo emphasizes the importance of distinguishing between hazards and risks and describes the field&#8217;s approach to regulating applications of technology rather than the technology itself. The discussion also explores opportunities and challenges in biotech‚Äôs multi-agency oversight model and the role of international coordination. Later, Daniel Kluttz (opens in new tab), a partner general manager in Microsoft&#8217;s Office of Responsible AI, joins Sullivan to discuss how insights from genome editing could inform more nuanced and robust governance frameworks for emerging technologies like AI.








Learn more:



Learning from other Domains to Advance AI Evaluation and Testing: Governance of Genome Edition in Human Therapeutics and Agricultural ApplicationsCase study | January 2025&nbsp;



Learning from other domains to advance AI evaluation and testing&nbsp;Microsoft Research Blog | June 2025&nbsp;



Responsible AI: Ethical policies and practices | Microsoft AI&nbsp;



AI and Microsoft Research&nbsp;









	
		
			Subscribe to the Microsoft Research Podcast:		
		
							
					
						  
						Apple Podcasts
					
				
			
							
					
						
						Email
					
				
			
							
					
						
						Android
					
				
			
							
					
						
						Spotify
					
				
			
							
					
						
						RSS Feed
					
				
					
	




	
		
			
				
					

Transcript



[MUSIC]



KATHLEEN SULLIVAN: Welcome to AI Testing and Evaluation: Learnings from Science and Industry. I&#8217;m your host, Kathleen Sullivan.



As generative AI continues to advance, Microsoft has gathered a range of experts‚Äîfrom genome editing to cybersecurity‚Äîto share how their fields approach evaluation and risk assessment. Our goal is to learn from their successes and their stumbles to move the science and practice of AI testing forward. In this series, we&#8217;ll explore how these insights might help guide the future of AI development, deployment, and responsible use.



[MUSIC ENDS]



Today I&#8217;m excited to welcome R. Alta Charo, the Warren P. Knowles Professor Emerita of Law and Bioethics at the University of Wisconsin‚ÄìMadison, to explore testing and risk assessment in genome editing.



Professor Charo has been at the forefront of biotechnology policy and governance for decades, advising former President Obama&#8217;s transition team on issues of medical research and public health, as well as serving as a senior policy advisor at the Food and Drug Administration. She consults on gene therapy and genome editing for various companies and organizations and has held positions on a number of advisory committees, including for the National Academy of Sciences. Her committee work has spanned women&#8217;s health, stem cell research, genome editing, biosecurity, and more.



After our conversation with Professor Charo, we&#8217;ll hear from Daniel Kluttz, a partner general manager in Microsoft&#8217;s Office of Responsible AI, about what these insights from biotech regulation could mean for AI governance and risk assessment and his team&#8217;s work governing sensitive AI uses and emerging technologies.



Alta, thank you so much for being here today. I&#8217;m a follower of your work and have really been looking forward to our conversation.



				
				
					



ALTA CHARO: It&#8217;s my pleasure. Thanks for having me.



SULLIVAN: Alta, I&#8217;d love to begin by stepping back in time a bit before you became a leading figure in bioethics and legal policy. You&#8217;ve shared that your interest in science was really inspired by your brothers‚Äô interest in the topic and that your upbringing really helped shape your perseverance and resilience. Can you talk to us about what put you on the path to law and policy?



CHARO: Well, I think it&#8217;s true that many of us are strongly influenced by our families and certainly my family had, kind of, a science-y, techy orientation. My father was a refugee, you know, escaping the Nazis, and when he finally was able to start working in the United States, he took advantage of the G.I. Bill to learn how to repair televisions and radios, which were really just coming in in the 1950s. So he was, kind of, technically oriented.



My mother retrained from being a talented amateur artist to becoming a math teacher, and not surprisingly, both my brothers began to aim toward things like engineering and chemistry and physics. And our form of entertainment was to watch PBS or Star Trek. [LAUGHTER]



And so the interest comes from that background coupled with, in the 1960s, this enormous surge of interest in the so-called nature-versus-nurture debate about the degree to which we are destined by our biology or shaped by our environments. It was a heady debate, and one that perfectly combined the two interests in politics and science.



SULLIVAN: For listeners who are brand new to your field in genomic editing, can you give us what I&#8217;ll call a ‚Äú90-second survey‚Äù of the space in perhaps plain language and why it&#8217;s important to have a framework for ensuring its responsible use.



CHARO: Well, you know, genome editing is both very old and very new. At base, what we&#8217;re talking about is a way to either delete sections of the genome, our collection of genes, or to add things or to alter what&#8217;s there. The goal is simply to be able to take what might not be healthy and make it healthy, whether it&#8217;s a plant, an animal, or a human.



Many people have compared it to a word processor, where you can edit text by swapping things in and out. You could change the letter g to the letter h in every word, and in our genomes, you can do similar kinds of things.



But because of this, we have a responsibility to make sure that whatever we change doesn&#8217;t become dangerous and that it doesn&#8217;t become socially disruptive. Now the earliest forms of genome editing were very inefficient, and so we didn&#8217;t worry that much. But with the advances that were spearheaded by people like Jennifer Doudna and Emmanuelle Charpentier, who won the Nobel Prize for their work in this area, genome editing has become much easier to do.



It&#8217;s become more efficient. It doesn&#8217;t require as much sophisticated laboratory equipment. It&#8217;s moved from being something that only a few people can do to something that we&#8217;re going to be seeing in our junior high school biology labs. And that means you have to pay attention to who&#8217;s doing it, why are they doing it, what are they releasing, if anything, into the environment, what are they trying to sell, and is it honest and is it safe?



SULLIVAN: How would you describe the risks, and are there, you know, sort of, specifically inherent risks in the technology itself, or do those risks really emerge only when it&#8217;s applied in certain contexts, like CRISPR in agriculture or CRISPR for human therapies?



CHARO: Well, to answer that, I&#8217;m going to do something that may seem a little picky, even pedantic. [LAUGHTER] But I&#8217;m going to distinguish between hazards and risks. So there are certain intrinsic hazards. That is, there are things that can go wrong.



You want to change one particular gene or one particular portion of a gene, and you might accidentally change something else, a so-called off-target effect. Or you might change something in a gene expecting a certain effect but not necessarily anticipating that there&#8217;s going to be an interaction between what you changed and what was there, a gene-gene interaction, that might have an unanticipated kind of result, a side effect essentially.



So there are some intrinsic hazards, but risk is a hazard coupled with the probability that it&#8217;s going to actually create something harmful. And that really depends upon the application.



If you are doing something that is making a change in a human being that is going to be a lifelong change, that enhances the significance of that hazard. It amplifies what I call the risk because if something goes wrong, then its consequences are greater.



It may also be that in other settings, what you&#8217;re doing is going to have a much lower risk because you&#8217;re working with a more familiar substance, your predictive power is much greater, and it&#8217;s not going into a human or an animal or into the environment. So I think that you have to say that the risk and the benefits, by the way, all are going to depend upon the particular application.



SULLIVAN: Yeah, I think on this point of application, there&#8217;s many players involved in that, right. Like, we often hear about this puzzle of who&#8217;s actually responsible for ensuring safety and a reasonable balance between risks and benefits or hazards and benefits, to quote you. Is it the scientists, the biotech companies, government agencies? And then if you could touch upon, as well, maybe how does the nature of genome editing risks ‚Ä¶ how do those responsibilities get divvied up?



CHARO: Well, in the 1980s, we had a very significant policy discussion about whether we should regulate the technology‚Äîno matter how it&#8217;s used or for whatever purpose‚Äîor if we should simply fold the technology in with all the other technologies that we currently have and regulate its applications the way we regulate applications generally. And we went for the second, the so-called coordinated framework.



So what we have in the United States is a system in which if you use genome editing in purely laboratory-based work, then you will be regulated the way we regulate laboratories.



There&#8217;s also, at most universities because of the way the government works with this, something called Institutional Biosafety Committees, IBCs. You want to do research that involves recombinant DNA and modern biotechnology, including genome editing but not limited to it, you have to go first to your IBC, and they look and see what you&#8217;re doing to decide if there&#8217;s a danger there that you have not anticipated that requires special attention.



If what you&#8217;re doing is going to get released into the environment or it&#8217;s going to be used to change an animal that&#8217;s going to be in the environment, then there are agencies that oversee the safety of our environment, predominantly the Environmental Protection Agency and the U.S. Department of Agriculture.



If you&#8217;re working with humans and you&#8217;re doing medical therapies, like you&#8217;re doing the gene therapies that just have been developed for things like sickle cell anemia, then you have to go through a very elaborate regulatory process that&#8217;s overseen by the Food and Drug Administration and also seen locally at the research stages overseen by institutional review boards that make sure the people who are being recruited into research understand what they&#8217;re getting into, that they&#8217;re the right people to be recruited, etc.



So we do have this kind of Jenga game ‚Ä¶



SULLIVAN: [LAUGHS] Yeah, sounds like it.



CHARO: ‚Ä¶ of regulatory agencies. And on top of all that, most of this involves professionals who&#8217;ve had to be licensed in some way. There may be state laws specifically on licensing. If you are dealing with things that might cross national borders, there may be international treaties and agreements that cover this.



And, of course, the insurance industry plays a big part because they decide whether or not what you&#8217;re doing is safe enough to be insured. So all of these things come together in a way that is not at all easy to understand if you&#8217;re not, kind of, working in the field. But the bottom-line thing to remember, the way to really think about it is, we don&#8217;t regulate genome editing; we regulate the things that use genome editing.



SULLIVAN: Yeah, that makes a lot of sense. Actually, maybe just following up a little bit on this notion of a variety of different, particularly like government agencies being involved. You know, in this multi-stakeholder model, where do you see gaps today that need to be filled, some of the pros and cons to keep in mind, and, you know, just as we think about distributing these systems at a global level, like, what are some of the considerations you are keeping in mind on that front?



CHARO: Well, certainly there are times where the way the statutes were written that govern the regulation of drugs or the regulation of foods did not anticipate this tremendous capacity we now have in the area of biotechnology generally or genome editing in particular. And so you can find that there are times where it feels a little bit ambiguous, and the agencies have to figure out how to apply their existing rules.



So an example. If you&#8217;re going to make alterations in an animal, right, we have a system for regulating drugs, including veterinary drugs. But we didn&#8217;t have something that regulated genome editing of animals. But in a sense, genome editing of an animal is the same thing as using a veterinary drug. You&#8217;re trying to affect the animal&#8217;s physical constitution in some fashion.



And it took a long time within the FDA to, sort of, work out how the regulation of veterinary drugs would apply if you think about the genetic construct that&#8217;s being used to alter the animal as the same thing as injecting a chemically based drug. And on that basis, they now know here&#8217;s the regulatory path‚Äîhere are the tests you have to do; here are the permissions you have to do; here&#8217;s the surveillance you have to do after it goes on the market.



Even there, sometimes, it was confusing. What happens when it&#8217;s not the kind of animal you&#8217;re thinking about when you think about animal drugs? Like, we think about pigs and dogs, but what about mosquitoes?



Because there, you&#8217;re really thinking more about pests, and if you&#8217;re editing the mosquito so that it can&#8217;t, for example, transmit dengue fever, right, it feels more like a public health thing than it is a drug for the mosquito itself, and it, kind of, fell in between the agencies that possibly had jurisdiction. And it took a while for the USDA, the Department of Agriculture, and the Food and Drug Administration to work out an agreement about how they would share this responsibility. So you do get those kinds of areas in which you have at least ambiguity.



We also have situations where frankly the fact that some things can move across national borders means you have to have a system for harmonizing or coordinating national rules. If you want to, for example, genetically engineer mosquitoes that can&#8217;t transmit dengue, mosquitoes have a tendency to fly. [LAUGHTER] And so &#8230; they can&#8217;t fly very far. That&#8217;s good. That actually makes it easier to control.



But if you&#8217;re doing work that&#8217;s right near a border, then you have to be sure that the country next to you has the same rules for whether it&#8217;s permitted to do this and how to surveil what you&#8217;ve done in order to be sure that you got the results you wanted to get and no other results. And that also is an area where we have a lot of work to be done in terms of coordinating across government borders and harmonizing our rules.



SULLIVAN: Yeah, I mean, you&#8217;ve touched on this a little bit, but there is such this striking balance between advancing technology, ensuring public safety, and sometimes, I think it feels just like you&#8217;re walking a tightrope where, you know, if we clamp down too hard, we&#8217;ll stifle innovation, and if we&#8217;re too lax, we risk some of these unintended consequences. And on a global scale like you just mentioned, as well. How has the field of genome editing found its balance?



CHARO: It&#8217;s still being worked out, frankly, but it&#8217;s finding its balance application by application. So in the United States, we have two very different approaches on regulation of things that are going to go into the market.



Some things can&#8217;t be marketed until they&#8217;ve gotten an approval from the government. So you come up with a new drug, you can&#8217;t sell that until it&#8217;s gone through FDA approval.



On the other hand, for most foods that are made up of familiar kinds of things, you can go on the market, and it&#8217;s only after they&#8217;re on the market that the FDA can act to withdraw it if a problem arises. So basically, we have either pre-market controls: you can&#8217;t go on without permission. Or post-market controls: we can take you off the market if a problem occurs.



How do we decide which one is appropriate for a particular application? It&#8217;s based on our experience. New drugs typically are both less familiar than existing things on the market and also have a higher potential for injury if they, in fact, are not effective or they are, in fact, dangerous and toxic.



If you have foods, even bioengineered foods, that are basically the same as foods that are already here, it can go on the market with notice but without a prior approval. But if you create something truly novel, then it has to go through a whole long process.



And so that is the way that we make this balance. We look at the application area. And we&#8217;re just now seeing in the Department of Agriculture a new approach on some of the animal editing, again, to try and distinguish between things that are simply a more efficient way to make a familiar kind of animal variant and those things that are genuinely novel and to have a regulatory process that is more rigid the more unfamiliar it is and the more that we see a risk associated with it.



SULLIVAN: I know we&#8217;re at the end of our time here and maybe just a quick kind of lightning-round of a question. For students, young scientists, lawyers, or maybe even entrepreneurs listening who are inspired by your work, what&#8217;s the single piece of advice you give them if they&#8217;re interested in policy, regulation, the ethical side of things in genomics or other fields?



CHARO: I&#8217;d say be a bio-optimist and read a lot of science fiction. Because it expands your imagination about what the world could be like. Is it going to be a world in which we&#8217;re now going to be growing our buildings instead of building them out of concrete?



Is it going to be a world in which our plants will glow in the evening so we don&#8217;t need to be using batteries or electrical power from other sources but instead our environment is adapting to our needs?



You know, expand your imagination with a sense of optimism about what could be and see ethics and regulation not as an obstacle but as a partner to bringing these things to fruition in a way that&#8217;s responsible and helpful to everyone.



[TRANSITION MUSIC]



SULLIVAN: Wonderful. Well, Alta, this has been just an absolute pleasure. So thank you.



CHARO: It was my pleasure. Thank you for having me.



SULLIVAN: Now, I&#8217;m happy to bring in Daniel Kluttz. As a partner general manager in Microsoft&#8217;s Office of Responsible AI, Daniel leads the group‚Äôs Sensitive Uses and Emerging Technologies program.



Daniel, it&#8217;s great to have you here. Thanks for coming in.



DANIEL KLUTTZ: It&#8217;s great to be here, Kathleen.



SULLIVAN: Yeah. So maybe before we unpack Alta Charo‚Äôs insights, I&#8217;d love to just understand the elevator pitch here. What exactly is [the] Sensitive Uses and Emerging Tech program, and what was the impetus for establishing it?



KLUTTZ: Yeah. So the Sensitive Uses and Emerging Technologies program sits within our Office of Responsible AI at Microsoft. And inherent in the name, there are two real core functions. There&#8217;s the sensitive uses and emerging technologies. What does that mean?



Sensitive uses, think of that as Microsoft&#8217;s internal consulting and oversight function for our higher-risk, most impactful AI system deployments. And so my team is a team of multidisciplinary experts who engages in sort of a white-glove-treatment sort of way with product teams at Microsoft that are designing, building, and deploying these higher-risk AI systems, and where that sort of consulting journey culminates is in a set of bespoke requirements tailored to the use case of that given system that really implement and apply our more standardized, generalized requirements that apply across the board.



Then the emerging technologies function of my team faces a little bit further out, trying to look around corners to see what new and novel and emerging risks are coming out of new AI technologies with the idea that we work with our researchers, our engineering partners, and, of course, product leaders across the company to understand where Microsoft is going with those emerging technologies, and we&#8217;re developing sort of rapid, quick-fire early-steer guidance that implements our policies ahead of that formal internal policymaking process, which can take a bit of time. So it&#8217;s designed to, sort of, both afford that innovation speed that we like to optimize for at Microsoft but also integrate our responsible AI commitments and our AI principles into emerging product development.



SULLIVAN: That segues really nicely, actually, as we met with Professor Charo and she was, you know, talking about the field of genome editing and the governing at the application level. I&#8217;d love to just understand how similar or not is that to managing the risks of AI in our world?



KLUTTZ: Yeah. I mean, Professor Charo‚Äôs comments were music to my ears because, you know, where we make our bread and butter, so to speak, in our team is in applying to use cases. AI systems, especially in this era of generative AI, are almost inherently multi-use, dual use. And so what really matters is how you&#8217;re going to apply that more general-purpose technology. Who&#8217;s going to use it? In what domain is it going to be deployed? And then tailor that oversight to those use cases. Try to be risk proportionate.



Professor Charo talked a little bit about this, but if it&#8217;s something that&#8217;s been done before and it&#8217;s just a new spin on an old thing, maybe we&#8217;re not so concerned about how closely we need to oversee and gate that application of that technology, whereas if it&#8217;s something new and novel or some new risk that might be posed by that technology, we take a little bit closer look and we are overseeing that in a more sort of high-touch way.



SULLIVAN: Maybe following up on that, I mean, how do you define sensitive use or maybe like high-impact application, and once that&#8217;s labeled, what happens? Like, what kind of steps kick in from there?



KLUTTZ: Yeah. So we have this Sensitive Uses program that&#8217;s been at Microsoft since 2019. I came to Microsoft in 2019 when we were starting this program in the Office of Responsible AI, and it had actually been incubated in Microsoft Research with our Aether community of colleagues who are experts in sociotechnical approaches to responsible AI, as well. Once we put it in the Office of Responsible AI, I came over. I came from academia. I was a researcher myself ‚Ä¶



SULLIVAN: At Berkeley, right?



KLUTTZ: At Berkeley. That&#8217;s right. Yep. Sociologist by training and a lawyer in a past life. [LAUGHTER] But that has helped sort of bridge those fields for me.



But Sensitive Uses, we force all of our teams when they&#8217;re envisioning their system design to think about, could the reasonably foreseeable use or misuse of the system that they&#8217;re developing in practice result in three really major, sort of, risk types. One is, could that deployment result in a consequential impact on someone&#8217;s legal position or life opportunity? Another category we have is, could that foreseeable use or misuse result in significant psychological or physical injury or harm? And then the third really ties in with a longstanding commitment we&#8217;ve had to human rights at Microsoft. And so could that system in it&#8217;s reasonably foreseeable use or misuse result in human rights impacts and injurious consequences to folks along different dimensions of human rights? 



Once you decide, we have a process to reporting that project into my office, and we will triage that project, working with the product team, for example, and our Responsible AI Champs community, which are folks who are dispersed throughout the ecosystem at Microsoft and educated in our responsible AI program, and then determine, OK, is it in scope for our program? If it is, say, OK, we&#8217;re going to go along for that ride with you, and then we get into that whole sort of consulting arrangement that then culminates in this set of bespoke use-case-based requirements applying our AI principles.



SULLIVAN: That&#8217;s super fascinating. What are some of the approaches in the governance of genome editing are you maybe seeing happening in AI governance or maybe just, like, bubbling up in conversations around it?



KLUTTZ: Yeah, I mean, I think we&#8217;ve learned a lot from fields like genome editing that Professor Charo talked about and others. And again, it gets back to this, sort of, risk-proportionate-based approach. It&#8217;s a balancing test. It&#8217;s a tradeoff of trying to, sort of, foster innovation and really look for the beneficial uses of these technologies. I appreciated her speaking about that. What are the intended uses of the system, right? And then getting to, OK, how do we balance trying to, again, foster that innovation in a very fast-moving space, a pretty complex space, and a very unsettled space contrasting to other, sort of, professional fields or technological fields that have a long history and are relatively settled from an oversight and regulatory standpoint? This one is not, and for good reason. It is still developing.



And I think, you know, there are certain oversight and policy regimes that exist today that can be applied. Professor Charo talked about this, as well, where, you know, maybe you have certain policy and oversight regimes that, depending on how the application of that technology is applied, applies there versus some horizontal, overarching regulatory sort of framework. And I think that applies from an internal governance standpoint, as well.



SULLIVAN: Yeah. It&#8217;s a great point. So what isn&#8217;t being explored from genome editing that, you know, maybe we think could be useful to AI governance, or as we think about the evolving frameworks ‚Ä¶



KLUTTZ: Yeah.



SULLIVAN: ‚Ä¶ what maybe we should be taking into account from what Professor Charo shared with us?



KLUTTZ: So one of the things I&#8217;ve thought about and took from Professor Charo‚Äôs discussion was she had just this amazing way of framing up how genome editing regulation is done. And she said, you know, we don&#8217;t regulate genome editing; we regulate the things that use genome editing. And while it&#8217;s not a one-to-one analogy with the AI space because we do have this sort of very general model level distinction versus application layer and even platform layer distinctions, I think it&#8217;s fair to say, you know, we don&#8217;t regulate AI applications writ large. We regulate the things that use AI in a very similar way. And that&#8217;s how we think of our internal policy and oversight process at Microsoft, as well.



And maybe there are things that we regulated and oversaw internally at the first instance and the first time we saw it come through, and it graduates into more of a programmatic framework for how we manage that. So one good example of that is some of our higher-risk AI systems that we offer out of Azure at the platform level. When I say that, I mean APIs that you call that developers can then build their own applications on top of. We were really deep in evaluating and assessing mitigations on those platform systems in the first instance, but we also graduated them into what we call our Limited Access AI services program.



And some of the things that Professor Charo discussed really resonated with me. You know, she had this moment where she was mentioning how, you know, you want to know who&#8217;s using your tools and how they&#8217;re being used. And it&#8217;s the same concepts. We want to have trust in our customers, we want to understand their use cases, and we want to apply technical controls that, sort of, force those use cases or give us signal post-deployment that use cases are being done in a way that may give us some level of concern, to reach out and understand what those use cases are.



SULLIVAN: Yeah, you&#8217;re hitting on a great point. And I love this kind of layered approach that we&#8217;re taking and that Alta highlighted, as well. Maybe to double-click a little bit just on that post-market control and what we&#8217;re tracking, kind of, once things are out and being used by our customers. How do we take some of that deployment data and bring it back in to maybe even better inform upfront governance or just how we think about some of the frameworks that we&#8217;re operating in?



KLUTTZ: It&#8217;s a great question. The number one thing is for us at Microsoft, we want to know the voice of our customer. We want our customers to talk to us. We don&#8217;t want to just understand telemetry and data. But it&#8217;s really getting out there and understanding from our customers and not just our customers. I would say our stakeholders is maybe a better term because that includes civil society organizations. It includes governments. It includes all of these non, sort of, customer actors that we care about and that we&#8217;re trying to sort of optimize for, as well. It includes end users of our enterprise customers. If we can gather data about how our products are being used and trying to understand maybe areas that we didn&#8217;t foresee how customers or users might be using those things, and then we can tune those systems to better align with what both customers and users want but also our own AI principles and policies and programs.



SULLIVAN: Daniel, before coming to Microsoft, you led social science research and sociotechnical applications of AI-driven tech at Berkeley. What do you think some of the biggest challenges are in defining and maybe even just, kind of, measuring at, like, a societal level some of the impacts of AI more broadly?



KLUTTZ: Measuring social phenomenon is a difficult thing. And one of the things that, as social scientists, you&#8217;re very interested in is scientifically observing and measuring social phenomena. Well, that sounds great. It sounds also very high level and jargony. What do we mean by that? You know, it&#8217;s very easy to say that you&#8217;re collecting data and you&#8217;re measuring, I don&#8217;t know, trust in AI, right? That&#8217;s a very fuzzy concept.



SULLIVAN: Right. Definitely.



KLUTTZ: It is a concept that we want to get to, but we have to unpack that, and we have to develop what we call measurable constructs. What are the things that we might observe that could give us an indication toward what is a very fuzzy and general concept. And there&#8217;s challenges with that everywhere. And I&#8217;m extremely fortunate to work at Microsoft with some of the world&#8217;s leading sociotechnical researchers and some of these folks who are thinking about‚Äîyou know, very steeped in measurement theory, literally PhDs in these fields‚Äîhow to both measure and allow for a scalable way to do that at a place the size of Microsoft. And that is trying to develop frameworks that are scalable and repeatable and put into our platform that then serves our product teams. Are we providing, as a platform, a service to those product teams that they can plug in and do their automated evaluations at scale as much as possible and then go back in over the top and do some of your more qualitative targeted testing and evaluations.



SULLIVAN: Yeah, makes a lot of sense. Before we close out, if you&#8217;re game for it, maybe we do a quick lightning round. Just 30-second answers here. Favorite real-world sensitive use case you&#8217;ve ever reviewed.



KLUTTZ: Oh gosh. Wow, this is where I get to be the social scientist.



SULLIVAN: [LAUGHS] Yes.



KLUTTZ: It‚Äôs like, define favorite, Kathleen. [LAUGHS] Most memorable, most painful.



SULLIVAN: Let&#8217;s do most memorable.



KLUTTZ: We‚Äôll do most memorable.



SULLIVAN: Yeah.



KLUTTZ: You know, I would say the most memorable project I worked on was when we rolled out the new Bing Chat, which is no longer called Bing Chat, because that was the first really big cross-company effort to deploy GPT-4, which was, you know, the next step up in AI innovation from our partners at OpenAI. And I really value working hand in hand with engineering teams and with researchers and that was us at our best and really sort of turbocharged the model that we have.



SULLIVAN: Wonderful. What&#8217;s one of the most overused phrases that you have in your AI governance meetings?



KLUTTZ: Gosh. [LAUGHS] If I hear ‚ÄúWe need to get aligned; we need to align on this more‚Äù ‚Ä¶



SULLIVAN: [LAUGHS] Right.



KLUTTZ: But, you know, it&#8217;s said for a reason. And I think it sort of speaks to that clever nature. That&#8217;s one that comes to mind.



SULLIVAN: That&#8217;s great. And then maybe, maybe last one. What are you most excited about in the next, I don&#8217;t know, let&#8217;s say three months? This world is moving so fast!



KLUTTZ: You know, the pace of innovation, as you just said, is just staggering. It is unbelievable. And sometimes it can feel overwhelming in my space. But what I am most excited about is how we are building up this Emerging ‚Ä¶ I mentioned this Emerging Technologies program in my team as a, sort of, formal program is relatively new. And I really enjoy being able to take a step back and think a little bit more about the future and a little bit more holistically. And I love working with engineering teams and sort of strategic visionaries who are thinking about what we&#8217;re doing a year from now or five years from now, or even 10 years from now, and I get to be a part of those conversations. And that really gives me energy and helps me ‚Ä¶ helps keep me grounded and not just dealing with the day to day, and, you know, various fire drills that you may run. It&#8217;s thinking strategically and having that foresight about what&#8217;s to come. And it&#8217;s exciting.



SULLIVAN: Great. Well, Daniel, just thanks so much for being here. I had such a wonderful discussion with you, and I think the thoughtfulness in our discussion today I hope resonates with our listeners. And again, thanks to Alta for setting the stage and sharing her really amazing, insightful thoughts here, as well. So thank you.



[MUSIC]



KLUTTZ: Thank you, Kathleen. I appreciate it. It&#8217;s been fun.



SULLIVAN: And to our listeners, thanks for tuning in. You can find resources related to this podcast in the show notes. And if you want to learn more about how Microsoft approaches AI governance, you can visit microsoft.com/RAI.



See you next time!&nbsp;



[MUSIC FADES]

				
			
			
				Show more			
		
	

Opens in a new tabThe post AI Testing and Evaluation: Learnings from genome editing appeared first on Microsoft Research.
‚Ä¢ Transforming network operations with AI: How Swisscom built a network assistant using Amazon Bedrock
  In the telecommunications industry, managing complex network infrastructures requires processing vast amounts of data from multiple sources. Network engineers often spend considerable time manually gathering and analyzing this data, taking away valuable hours that could be spent on strategic initiatives. This challenge led Swisscom, Switzerland‚Äôs leading telecommunications provider, to explore how AI can transform their network operations. 
Swisscom‚Äôs Network Assistant, built on Amazon Bedrock, represents a significant step forward in automating network operations. This solution combines generative AI capabilities with a sophisticated data processing pipeline to help engineers quickly access and analyze network data. Swisscom used AWS services to create a scalable solution that reduces manual effort and provides accurate and timely network insights. 
In this post, we explore how Swisscom developed their Network Assistant. We discuss the initial challenges and how they implemented a solution that delivers measurable benefits. We examine the technical architecture, discuss key learnings, and look at future enhancements that can further transform network operations. We highlight best practices for handling sensitive data for Swisscom to comply with the strict regulations governing the telecommunications industry. This post provides telecommunications providers or other organizations managing complex infrastructure with valuable insights into how you can use AWS services to modernize operations through AI-powered automation. 
The opportunity: Improve network operations 
Network engineers at Swisscom faced the daily challenge to manage complex network operations and maintain optimal performance and compliance. These skilled professionals were tasked to monitor and analyze vast amounts of data from multiple and decoupled sources. The process was repetitive and demanded considerable time and attention to detail. In certain scenarios, fulfilling the assigned tasks consumed more than 10% of their availability. The manual nature of their work presented several critical pain points. The data consolidation process from multiple network entities into a coherent overview was particularly challenging, because engineers had to navigate through various tools and systems to retrieve telemetry information about data sources and network parameters from extensive documentation, verify KPIs through complex calculations, and identify potential issues of diverse nature. This fragmented approach consumed valuable time and introduced the risk of human error in data interpretation and analysis. The situation called for a solution to address three primary concerns: 
 
 Efficiency in data retrieval and analysis 
 Accuracy in calculations and reporting 
 Scalability to accommodate growing data sources and use cases 
 
The team required a streamlined approach to access and analyze network data, maintain compliance with defined metrics and thresholds, and deliver fast and accurate responses to events while maintaining the highest standards of data security and sovereignty. 
Solution overview 
Swisscom‚Äôs approach to develop the Network Assistant was methodical and iterative. The team chose Amazon Bedrock as the foundation for their generative AI application and implemented a Retrieval Augmented Generation (RAG) architecture using Amazon Bedrock Knowledge Bases to enable precise and contextual responses to engineer queries. The RAG approach is implemented in three distinct phases: 
 
 Retrieval ‚Äì User queries are matched with relevant knowledge base content through embedding models 
 Augmentation ‚Äì The context is enriched with retrieved information 
 Generation ‚Äì The large language model (LLM) produces informed responses 
 
The following diagram illustrates the solution architecture. 
 
The solution architecture evolved through several iterations. The initial implementation established basic RAG functionality by feeding the Amazon Bedrock knowledge base with tabular data and documentation. However, the Network Assistant struggled to manage large input files containing thousands of rows with numerical values across multiple parameter columns. This complexity highlighted the need for a more selective approach that could identify only the rows relevant for specific KPI calculations. At that point, the retrieval process wasn‚Äôt returning the precise number of vector embeddings required to calculate the formulas, prompting the team to refine the solution for greater accuracy. 
Next iterations enhanced the assistant with agent-based processing and action groups. The team implemented AWS Lambda functions using Pandas or Spark for data processing, facilitating accurate numerical calculations retrieval using natural language from the user input prompt. 
A significant advancement was introduced with the implementation of a multi-agent approach, using Amazon Bedrock Agents, where specialized agents handle different aspects of the system: 
 
 Supervisor agent ‚Äì Orchestrates interactions between documentation management and calculator agents to provide comprehensive and accurate responses. 
 Documentation management agent ‚Äì Helps the network engineers access information in large volumes of data efficiently and extract insights about data sources, network parameters, configuration, or tooling. 
 Calculator agent ‚Äì Supports the network engineers to understand complex network parameters and perform precise data calculations out of telemetry data. This produces numerical insights that help perform network management tasks; optimize performance; maintain network reliability, uptime, and compliance; and assist in troubleshooting. 
 
This following diagram illustrates the enhanced data extract, transform, and load (ETL) pipeline interaction with Amazon Bedrock. 
 
To achieve the desired accuracy in KPI calculations, the data pipeline was refined to achieve consistent and precise performance, which leads to meaningful insights. The team implemented an ETL pipeline with Amazon Simple Storage Service (Amazon S3) as the data lake to store input files following a daily batch ingestion approach, AWS Glue for automated data crawling and cataloging, and Amazon Athena for SQL querying. At this point, it became possible for the calculator agent to forego the Pandas or Spark data processing implementation. Instead, by using Amazon Bedrock Agents, the agent translates natural language user prompts into SQL queries. In a subsequent step, the agent runs the relevant SQL queries selected dynamically through analysis of various input parameters, providing the calculator agent an accurate result. This serverless architecture supports scalability, cost-effectiveness, and maintains high accuracy in KPI calculations. The system integrates with Swisscom‚Äôs on-premises data lake through daily batch data ingestion, with careful consideration of data security and sovereignty requirements. 
To enhance data security and appropriate ethics in the Network Assistant responses, a series of guardrails were defined in Amazon Bedrock. The application implements a comprehensive set of data security guardrails to protect against malicious inputs and safeguard sensitive information. These include content filters that block harmful categories such as hate, insults, violence, and prompt-based threats like SQL injection. Specific denied topics and sensitive identifiers (for example, IMSI, IMEI, MAC address, or GPS coordinates) are filtered through manual word filters and pattern-based detection, including regular expressions (regex). Sensitive data such as personally identifiable information (PII), AWS access keys, and serial numbers are blocked or masked. The system also uses contextual grounding and relevance checks to verify model responses are factually accurate and appropriate. In the event of restricted input or output, standardized messaging notifies the user that the request can‚Äôt be processed. These guardrails help prevent data leaks, reduce the risk of DDoS-driven cost spikes, and maintain the integrity of the application‚Äôs outputs. 
Results and benefits 
The implementation of the Network Assistant is set to deliver substantial and measurable benefits to Swisscom‚Äôs network operations. The most significant impact is time savings. Network engineers are estimated to experience 10% reduction in time spent on routine data retrieval and analysis tasks. This efficiency gain translates to nearly 200 hours per engineer saved annually, and represents a significant improvement in operational efficiency. The financial impact is equally impressive. The solution is projected to provide substantial cost savings per engineer annually, with minimal operational costs at less than 1% of the total value generated. The return on investment increases as additional teams and use cases are incorporated into the system, demonstrating strong scalability potential. 
Beyond the quantifiable benefits, the Network Assistant is expected to transform how engineers interact with network data. The enhanced data pipeline supports accuracy in KPI calculations, critical for network health tracking, and the multi-agent approach provides orchestrated and comprehensive responses to complex queries out of user natural language. 
As a result, engineers can have instant access to a wide range of network parameters, data source information, and troubleshooting guidance from an individual personalized endpoint with which they can quickly interact and obtain insights through natural language. This enables them to focus on strategic tasks rather than routine data gathering and analysis, leading to a significant work reduction that aligns with Swisscom SRE principles. 
Lessons learned 
Throughout the development and implementation of the Swisscom Network Assistant, several learnings emerged that shaped the solution. The team needed to address data sovereignty and security requirements for the solution, particularly when processing data on AWS. This led to careful consideration of data classification and compliance with applicable regulatory requirements in the telecommunications sector, to make sure that sensitive data is handled appropriately. In this regard, the application underwent a strict threat model evaluation, verifying the robustness of its interfaces against vulnerabilities and acting proactively towards securitization. The threat model was applied to assess doomsday scenarios, and data flow diagrams were created to depict major data flows inside and beyond the application boundaries. The AWS architecture was specified in detail, and trust boundaries were set to indicate which portions of the application trusted each other. Threats were identified following the STRIDE methodology (Spoofing, Tampering, Repudiation, Information disclosure, Denial of service, Elevation of privilege), and countermeasures, including Amazon Bedrock Guardrails, were defined to avoid or mitigate threats in advance. 
A critical technical insight was that complex calculations involving significant data volume management required a different approach than mere AI model interpretation. The team implemented an enhanced data processing pipeline that combines the contextual understanding of AI models with direct database queries for numerical calculations. This hybrid approach facilitates both accuracy in calculations and richness in contextual responses. 
The choice of a serverless architecture proved to be particularly beneficial: it minimized the need to manage compute resources and provides automatic scaling capabilities. The pay-per-use model of AWS services helped keep operational costs low and maintain high performance. Additionally, the team‚Äôs decision to implement a multi-agent approach provided the flexibility needed to handle diverse types of queries and use cases effectively. 
Next steps 
Swisscom has ambitious plans to enhance the Network Assistant‚Äôs capabilities further. A key upcoming feature is the implementation of a network health tracker agent to provide proactive monitoring of network KPIs. This agent will automatically generate reports to categorize issues based on criticality, enable faster response time, and improve the quality of issue resolution to potential network issues. The team is also exploring the integration of Amazon Simple Notification Service (Amazon SNS) to enable proactive alerting for critical network status changes. This can include direct integration with operational tools that alert on-call engineers, to further streamline the incident response process. The enhanced notification system will help engineers address potential issues before they critically impact network performance and obtain a detailed action plan including the affected network entities, the severity of the event, and what went wrong precisely. 
The roadmap also includes expanding the system‚Äôs data sources and use cases. Integration with additional internal network systems will provide more comprehensive network insights. The team is also working on developing more sophisticated troubleshooting features, using the growing knowledge base and agentic capabilities to provide increasingly detailed guidance to engineers. 
Additionally, Swisscom is adopting infrastructure as code (IaC) principles by implementing the solution using AWS CloudFormation. This approach introduces automated and consistent deployments while providing version control of infrastructure components, facilitating simpler scaling and management of the Network Assistant solution as it grows. 
Conclusion 
The Network Assistant represents a significant advancement in how Swisscom can manage its network operations. By using AWS services and implementing a sophisticated AI-powered solution, they have successfully addressed the challenges of manual data retrieval and analysis. As a result, they have boosted both accuracy and efficiency so network engineers can respond quickly and decisively to network events. The solution‚Äôs success is aided not only by the quantifiable benefits in time and cost savings but also by its potential for future expansion. The serverless architecture and multi-agent approach provide a solid foundation for adding new capabilities and scaling across different teams and use cases.As organizations worldwide grapple with similar challenges in network operations, Swisscom‚Äôs implementation serves as a valuable blueprint for using cloud services and AI to transform traditional operations. The combination of Amazon Bedrock with careful attention to data security and accuracy demonstrates how modern AI solutions can help solve real-world engineering challenges. 
As managing network operations complexity continues to grow, the lessons from Swisscom‚Äôs journey can be applied to many engineering disciplines. We encourage you to consider how Amazon Bedrock and similar AI solutions might help your organization overcome its own comprehension and process improvement barriers. To learn more about implementing generative AI in your workflows, explore Amazon Bedrock Resources or contact AWS. 
Additional resources 
For more information about Amazon Bedrock Agents and its use cases, refer to the following resources: 
 
 Generative AI for telecom 
 Best practices for building robust generative AI applications with Amazon Bedrock Agents ‚Äì Part 1 
 Best practices for building robust generative AI applications with Amazon Bedrock Agents ‚Äì Part 2 
 
 
 
About the authors 
Pablo Garc√≠a Benedicto is an experienced Data &amp; AI Cloud Engineer with strong expertise in cloud hyperscalers and data engineering. With a background in telecommunications, he currently works at Swisscom, where he leads and contributes to projects involving Generative AI applications and agents using Amazon Bedrock. Aiming for AI and data specialization, his latest projects focus on building intelligent assistants and autonomous agents that streamline business information retrieval, leveraging cloud-native architectures and scalable data pipelines to reduce toil and drive operational efficiency. 
Rajesh Sripathi is a Generative AI Specialist Solutions Architect at AWS, where he partners with global Telecommunication and Retail &amp; CPG customers to develop and scale generative AI applications. With over 18 years of experience in the IT industry, Rajesh helps organizations use cutting-edge cloud and AI technologies for business transformation. Outside of work, he enjoys exploring new destinations through his passion for travel and driving. 
Ruben Merz Ruben Merz is a Principal Solutions Architect at AWS. With a background in distributed systems and networking, his work with customers at AWS focuses on digital sovereignty, AI, and networking. 
Jordi Montoliu Nerin is a Data &amp; AI Leader currently serving as Senior AI/ML Specialist at AWS, where he helps worldwide telecommunications customers implement AI strategies after previously driving Data &amp; Analytics business across EMEA regions. He has over 10 years of experience, where he has led multiple Data &amp; AI implementations at scale, led executions of data strategy and data governance frameworks, and has driven strategic technical and business development programs across multiple industries and continents. Outside of work, he enjoys sports, cooking and traveling.
‚Ä¢ End-to-End model training and deployment with Amazon SageMaker Unified Studio
  Although rapid generative AI advancements are revolutionizing organizational natural language processing tasks, developers and data scientists face significant challenges customizing these large models. These hurdles include managing complex workflows, efficiently preparing large datasets for fine-tuning, implementing sophisticated fine-tuning techniques while optimizing computational resources, consistently tracking model performance, and achieving reliable, scalable deployment.The fragmented nature of these tasks often leads to reduced productivity, increased development time, and potential inconsistencies in the model development pipeline. Organizations need a unified, streamlined approach that simplifies the entire process from data preparation to model deployment. 
To address these challenges, AWS has expanded Amazon SageMaker with a comprehensive set of data, analytics, and generative AI capabilities. At the heart of this expansion is Amazon SageMaker Unified Studio, a centralized service that serves as a single integrated development environment (IDE). SageMaker Unified Studio streamlines access to familiar tools and functionality from purpose-built AWS analytics and artificial intelligence and machine learning (AI/ML) services, including Amazon EMR, AWS Glue, Amazon Athena, Amazon Redshift, Amazon Bedrock, and Amazon SageMaker AI. With SageMaker Unified Studio, you can discover data through Amazon SageMaker Catalog, access it from Amazon SageMaker Lakehouse, select foundation models (FMs) from Amazon SageMaker JumpStart or build them through JupyterLab, train and fine-tune them with SageMaker AI training infrastructure, and deploy and test models directly within the same environment. SageMaker AI is a fully managed service to build, train, and deploy ML models‚Äîincluding FMs‚Äîfor different use cases by bringing together a broad set of tools to enable high-performance, low-cost ML. It‚Äôs available as a standalone service on the AWS Management Console, or through APIs. Model development capabilities from SageMaker AI are available within SageMaker Unified Studio. 
In this post, we guide you through the stages of customizing large language models (LLMs) with SageMaker Unified Studio and SageMaker AI, covering the end-to-end process starting from data discovery to fine-tuning FMs with SageMaker AI distributed training, tracking metrics using MLflow, and then deploying models using SageMaker AI inference for real-time inference. We also discuss best practices to choose the right instance size and share some debugging best practices while working with JupyterLab notebooks in SageMaker Unified Studio. 
Solution overview 
The following diagram illustrates the solution architecture. There are three personas: admin, data engineer, and user, which can be a data scientist or an ML engineer. 

 
 AWS SageMaker Unified Studio ML workflow showing data processing, model training, and deployment stages
 
Setting up the solution consists of the following steps: 
 
 The admin sets up the SageMaker Unified Studio domain for the user and sets the access controls. The admin also publishes the data to SageMaker Catalog in SageMaker Lakehouse. 
 Data engineers can create and manage extract, transform, and load (ETL) pipelines directly within Unified Studio using Visual ETL. They can transform raw data sources into datasets ready for exploratory data analysis. The admin can then manage the publication of these assets to the SageMaker Catalog, making them discoverable and accessible to other team members or users such as data engineers in the organization. 
 Users or data engineers can log in to the Unified Studio web-based IDE using the login provided by the admin to create a project and create a managed MLflow server for tracking experiments. Users can discover available data assets in the SageMaker Catalog and request a subscription to an asset published by the data engineer. After the data engineer approves the subscription request, the user performs an exploratory data analysis of the content of the table with the query editor or with a JupyterLab notebook, then prepares the dataset by connecting with SageMaker Catalog through an AWS Glue or Athena connection. 
 You can explore models from SageMaker JumpStart, which hosts over 200 models for various tasks, and fine-tune directly with the UI, or develop a training script for fine-tuning the LLM in the JupyterLab IDE. SageMaker AI provides distributed training libraries and supports various distributed training options for deep learning tasks. For this post, we use the PyTorch framework and use Hugging Face open source FMs for fine-tuning. We will show you how you can use parameter efficient fine-tuning (PEFT) with Low-Rank Adaptation (LoRa), where you freeze the model weights, train the model with modifying weight metrics, and then merge these LoRa adapters back to the base model after distributed training. 
 You can track and monitor fine-tuning metrics directly in SageMaker Unified Studio using MLflow, by analyzing metrics such as loss to make sure the model is correctly fine-tuned. 
 You can deploy the model to a SageMaker AI endpoint after the fine-tuning job is complete and test it directly from SageMaker Unified Studio. 
 
Prerequisites 
Before starting this tutorial, make sure you have the following: 
 
 An AWS account with permissions to create SageMaker resources. For setup instructions, see Set up an AWS account and create an administrator user. 
 Familiarity with Python and PyTorch for distributed training and model customization. 
 
Set up SageMaker Unified Studio and configure user access 
SageMaker Unified Studio is built on top of Amazon DataZone capabilities such as domains to organize your assets and users, and projects to collaborate with others users, securely share artifacts, and seamlessly work across compute services. 
To set up Unified Studio, complete the following steps: 
 
 As an admin, create a SageMaker Unified Studio domain, and note the URL. 
 On the domain‚Äôs details page, on the User management tab, choose Configure SSO user access. For this post, we recommend setting up using single sign-on (SSO) access using the URL. 
 
For more information about setting up user access, see Managing users in Amazon SageMaker Unified Studio. 
Log in to SageMaker Unified Studio 
Now that you have created your new SageMaker Unified Studio domain, complete the following steps to access SageMaker Unified Studio: 
 
 On the SageMaker console, open the details page of your domain. 
 Choose the link for the SageMaker Unified Studio URL. 
 Log in with your SSO credentials. 
 
Now you‚Äôre signed in to SageMaker Unified Studio. 
Create a project 
The next step is to create a project. Complete the following steps: 
 
 In SageMaker Unified Studio, choose Select a project on the top menu, and choose Create project. 
 For Project name, enter a name (for example, demo). 
 For Project profile, choose your profile capabilities. A project profile is a collection of blueprints, which are configurations used to create projects. For this post, we choose All capabilities, then choose Continue. 
 

 
 Creating a project in Amazon SageMaker Unified Studio
 
Create a compute space 
SageMaker Unified Studio provides compute spaces for IDEs that you can use to code and develop your resources. By default, it creates a space for you to get started with you project. You can find the default space by choosing Compute in the navigation pane and choosing the Spaces tab. You can then choose Open to go to the JuypterLab environment and add members to this space. You can also create a new space by choosing Create space on the Spaces tab. 
 
To use SageMaker Studio notebooks cost-effectively, use smaller, general-purpose instances (like the T or M families) for interactive data exploration and prototyping. For heavy lifting like training or large-scale processing or deployment, use SageMaker AI training jobs and SageMaker AI prediction to offload the work to separate and more powerful instances such as the P5 family. We will show you in the notebook how you can run training jobs and deploy LLMs in the notebook with APIs. It is not recommended to run distributed workloads in notebook instances. The chances of kernel failures is high because JupyterLab notebooks should not be used for large distributed workloads (both for data and ML training). 
The following screenshot shows the configuration options for your space. You can change your instance size from default (ml.t3.medium) to (ml.m5.xlarge) for the JupyterLab IDE. You can also increase the Amazon Elastic Block Store (Amazon EBS) volume capacity from 16 GB to 50 GB for training LLMs. 

 
 Canfigure space in Amazon SageMaker Unified Studio
 
Set up MLflow to track ML experiments 
You can use MLflow in SageMaker Unified Studio to create, manage, analyze, and compare ML experiments. Complete the following steps to set up MLflow: 
 
 In SageMaker Unified Studio, choose Compute in the navigation pane. 
 On the MLflow Tracking Servers tab, choose Create MLflow Tracking Server. 
 Provide a name and create your tracking server. 
 Choose Copy ARN to copy the Amazon Resource Name (ARN) of the tracking server. 
 
 
You will need this MLflow ARN in your notebook to set up distributed training experiment tracking. 
Set up the data catalog 
For model fine-tuning, you need access to a dataset. After you set up the environment, the next step is to find the relevant data from the SageMaker Unified Studio data catalog and prepare the data for model tuning. For this post, we use the Stanford Question Answering Dataset (SQuAD) dataset. This dataset is a reading comprehension dataset, consisting of questions posed by crowd workers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable. 
Download the SQuaD dataset and upload it to SageMaker Lakehouse by following the steps in Uploading data. 

 
 Adding data to Catalog in Amazon SageMaker Unified Studio
 
To make this data discoverable by the users or ML engineers, the admin needs to publish this data to the Data Catalog. For this post, you can directly download the SQuaD dataset and upload it to the catalog. To learn how to publish the dataset to SageMaker Catalog, see Publish assets to the Amazon SageMaker Unified Studio catalog from the project inventory. 
Query data with the query editor and JupyterLab 
In many organizations, data preparation is a collaborative effort. A data engineer might prepare an initial raw dataset, which a data scientist then refines and augments with feature engineering before using it for model training. In the SageMaker Lakehouse data and model catalog, publishers set subscriptions for automatic or manual approval (wait for admin approval). Because you already set up the data in the previous section, you can skip this section showing how to subscribe to the dataset. 
To subscribe to another dataset like SQuAD, open the data and model catalog in Amazon SageMaker Lakehouse, choose SQuAD, and subscribe. 

 
 Subscribing to any asset or dataset published by Admin
 
Next, let‚Äôs use the data explorer to explore the dataset you subscribed to. Complete the following steps: 
 
 On the project page, choose Data. 
 Under Lakehouse, expand AwsDataCatalog. 
 Expand your database starting from glue_db_. 
 Choose the dataset you created (starting with squad) and choose Query with Athena. 
 

 
 Querying the data using Query Editor in Amazon SageMaker Unfied Studio
 
Process your data through a multi-compute JupyterLab IDE notebook 
SageMaker Unified Studio provides a unified JupyterLab experience across different languages, including SQL, PySpark, Python, and Scala Spark. It also supports unified access across different compute runtimes such as Amazon Redshift and Athena for SQL, Amazon EMR Serverless, Amazon EMR on EC2, and AWS Glue for Spark. 
Complete the following steps to get started with the unified JupyterLab experience: 
 
 Open your SageMaker Unified Studio project page. 
 On the top menu, choose Build, and under IDE &amp; APPLICATIONS, choose JupyterLab. 
 Wait for the space to be ready. 
 Choose the plus sign and for Notebook, choose Python 3. 
 Open a new terminal and enter git clonehttps://github.com/aws-samples/amazon-sagemaker-generativeai. 
 Go to the folder amazon-sagemaker-generativeai/3_distributed_training/distributed_training_sm_unified_studio/ and open the distributed training in unified studio.ipynb notebook to get started. 
 Enter the MLflow server ARN you created in the following code: 
 
 
 import&nbsp;os
os.environ["mlflow_uri"]&nbsp;=&nbsp;""
os.environ["mlflow_experiment_name"]&nbsp;=&nbsp;"deepseek-r1-distill-llama-8b-sft" 
 
Now you an visualize the data through the notebook. 
 
 On the project page, choose Data. 
 Under Lakehouse, expand AwsDataCatalog. 
 Expand your database starting from glue_db, copy the name of the database, and enter it in the following code: 
 
 
 db_name&nbsp;=&nbsp;"&lt;enter your db name&gt;"
table&nbsp;=&nbsp;"sqad" 
 
 
 You can now access the entire dataset directly by using the in-line SQL query capabilities of JupyterLab notebooks in SageMaker Unified Studio. You can follow the data preprocessing steps in the notebook. 
 
 
 %%sql project.athena
SELECT * FROM "&lt;DATABASE_NAME&gt;"."sqad"; 
 
The following screenshot shows the output. 
 
We are going to split the dataset into a test set and training set for model training. When the data processing in done and we have split the data into test and training sets, the next step is to perform fine-tuning of the model using SageMaker Distributed Training. 
Fine-tune the model with SageMaker Distributed training 
You‚Äôre now ready to fine-tune your model by using SageMaker AI capabilities for training. Amazon SageMaker Training is a fully managed ML service offered by SageMaker that helps you efficiently train a wide range of ML models at scale. The core of SageMaker AI jobs is the containerization of ML workloads and the capability of managing AWS compute resources. SageMaker Training takes care of the heavy lifting associated with setting up and managing infrastructure for ML training workloads 
We select one model directly from the Hugging Face Hub, DeepSeek-R1-Distill-Llama-8B, and develop our training script in the JupyterLab space. Because we want to distribute the training across all the available GPUs in our instance, by using PyTorch Fully Sharded Data Parallel (FSDP), we use the Hugging Face Accelerate library to run the same PyTorch code across distributed configurations. You can start the fine-tuning job directly in your JupyterLab notebook or use the SageMaker Python SDK to start the training job. We use the Trainer from transfomers to fine-tune our model. We prepared the script train.py, which loads the dataset from disk, prepares the model and tokenizer, and starts the training. 
For configuration, we use TrlParser, and provide hyperparameters in a YAML file. You can upload this file and provide it to SageMaker similar to your datasets. The following is the config file for fine-tuning the model on ml.g5.12xlarge. Save the config file as args.yaml and upload it to Amazon Simple Storage Service (Amazon S3). 
 
 cat&nbsp;&gt;&nbsp;./args.yaml&nbsp;&lt;&lt;EOF
model_id: "deepseek-ai/DeepSeek-R1-Distill-Llama-8B" &nbsp; &nbsp; &nbsp; # Hugging Face model id
mlflow_uri: "${mlflow_uri}"
mlflow_experiment_name: "${mlflow_experiment_name}"
# sagemaker specific parameters
output_dir: "/opt/ml/model" &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; # path to where SageMaker will upload the model 
train_dataset_path: "/opt/ml/input/data/train/" &nbsp; # path to where FSx saves train dataset
test_dataset_path: "/opt/ml/input/data/test/" &nbsp; &nbsp; # path to where FSx saves test dataset
# training parameters
lora_r: 8
lora_alpha: 16
lora_dropout: 0.1 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 
learning_rate: 2e-4 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# learning rate scheduler
num_train_epochs: 1 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# number of training epochs
per_device_train_batch_size: 2 &nbsp; &nbsp; &nbsp; &nbsp; # batch size per device during training
per_device_eval_batch_size: 1 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# batch size for evaluation
gradient_accumulation_steps: 2 &nbsp; &nbsp; &nbsp; &nbsp; # number of steps before performing a backward/update pass
gradient_checkpointing: true &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; # use gradient checkpointing
bf16: true &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; # use bfloat16 precision
tf32: false &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# use tf32 precision
fsdp: "full_shard auto_wrap offload"
fsdp_config: 
&nbsp;&nbsp; &nbsp;backward_prefetch: "backward_pre"
&nbsp;&nbsp; &nbsp;cpu_ram_efficient_loading: true
&nbsp;&nbsp; &nbsp;offload_params: true
&nbsp;&nbsp; &nbsp;forward_prefetch: false
&nbsp;&nbsp; &nbsp;use_orig_params: true
merge_weights: true &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# merge weights in the base model
EOF 
 
Use the following code to use the native PyTorch container image, pre-built for SageMaker: 
 
 image_uri = sagemaker.image_uris.retrieve(
&nbsp;&nbsp; &nbsp;framework="pytorch",
&nbsp;&nbsp; &nbsp;region=sagemaker_session.boto_session.region_name,
&nbsp;&nbsp; &nbsp;version="2.6.0",
&nbsp;&nbsp; &nbsp;instance_type=instance_type,
&nbsp;&nbsp; &nbsp;image_scope="training"
)

image_uri 
 
Define the trainer as follows: 
 
 Define the ModelTrainer
model_trainer = ModelTrainer(
&nbsp;&nbsp; &nbsp;training_image=image_uri,
&nbsp;&nbsp; &nbsp;source_code=source_code,
&nbsp;&nbsp; &nbsp;base_job_name=job_name,
&nbsp;&nbsp; &nbsp;compute=compute_configs,
&nbsp;&nbsp; &nbsp;distributed=Torchrun(),
&nbsp;&nbsp; &nbsp;stopping_condition=StoppingCondition(
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;max_runtime_in_seconds=7200
&nbsp;&nbsp; &nbsp;),
&nbsp;&nbsp; &nbsp;hyperparameters={
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"config": "/opt/ml/input/data/config/args.yaml" # path to TRL config which was uploaded to s3
&nbsp;&nbsp; &nbsp;},
&nbsp;&nbsp; &nbsp;output_data_config=OutputDataConfig(
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;s3_output_path=output_path
&nbsp;&nbsp; &nbsp;),
) 
 
Run the trainer with the following: 
 
 # starting the train job with our uploaded datasets as input
model_trainer.train(input_data_config=data,&nbsp;wait=True) 
 
You can follow the steps in the notebook. 
You can explore the job execution in SageMaker Unified Studio. The training job runs on the SageMaker training cluster by distributing the computation across the four available GPUs on the selected instance type ml.g5.12xlarge. We choose to merge the LoRA adapter with the base model. This decision was made during the training process by setting the merge_weights parameter to True in our train_fn() function. Merging the weights provides a single, cohesive model that incorporates both the base knowledge and the domain-specific adaptations we‚Äôve made through fine-tuning. 
Track training metrics and model registration using MLflow 
You created an MLflow server in an earlier step to track experiments and registered models, and provided the server ARN in the notebook. 
You can log MLflow models and automatically register them with Amazon SageMaker Model Registry using either the Python SDK or directly through the MLflow UI. Use mlflow.register_model() to automatically register a model with SageMaker Model Registry during model training. You can explore the MLflow tracking code in train.py and the notebook. The training code tracks MLflow experiments and registers the model to the MLflow model registry. To learn more, see Automatically register SageMaker AI models with SageMaker Model Registry. 
To see the logs, complete the following steps: 
 
 Choose Build, then choose Spaces. 
 Choose Compute in the navigation pane. 
 On the MLflow Tracking Servers tab, choose Open to open the tracking server. 
 
You can see both the experiments and registered models. 
 
Deploy and test the model using SageMaker AI Inference 
When deploying a fine-tuned model on AWS, SageMaker AI Inference offers multiple deployment strategies. In this post, we use SageMaker real-time inference. The real-time inference endpoint is designed for having full control over the inference resources. You can use a set of available instances and deployment options for hosting your model. By using the SageMaker built-in container DJL Serving, you can take advantage of the inference script and optimization options available directly in the container. In this post, we deploy the fine-tuned model to a SageMaker endpoint for running inference, which will be used for testing the model. 
In SageMaker Unified Studio, in JupyterLab, we create the Model object, which is a high-level SageMaker model class for working with multiple container options. The image_uri parameter specifies the container image URI for the model, and model_data points to the Amazon S3 location containing the model artifact (automatically uploaded by the SageMaker training job). We also specify a set of environment variables to configure the specific inference backend option (OPTION_ROLLING_BATCH), the degree of tensor parallelism based on the number of available GPUs (OPTION_TENSOR_PARALLEL_DEGREE), and the maximum allowable length of input sequences (in tokens) for models during inference (OPTION_MAX_MODEL_LEN). 
 
 model = Model(
&nbsp;&nbsp; &nbsp;image_uri=image_uri,
&nbsp;&nbsp; &nbsp;model_data=f"s3://{bucket_name}/{job_prefix}/{job_name}/output/model.tar.gz",
&nbsp;&nbsp; &nbsp;role=get_execution_role(),
&nbsp;&nbsp; &nbsp;env={
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;'HF_MODEL_ID': "/opt/ml/model",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;'OPTION_TRUST_REMOTE_CODE': 'true',
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;'OPTION_ROLLING_BATCH': "vllm",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;'OPTION_DTYPE': 'bf16',
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;'OPTION_TENSOR_PARALLEL_DEGREE': 'max',
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;'OPTION_MAX_ROLLING_BATCH_SIZE': '1',
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;'OPTION_MODEL_LOADING_TIMEOUT': '3600',
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;'OPTION_MAX_MODEL_LEN': '4096'
&nbsp;&nbsp; &nbsp;}
) 
 
After you create the model object, you can deploy it to an endpoint using the deploy method. The initial_instance_count and instance_type parameters specify the number and type of instances to use for the endpoint. We selected the ml.g5.4xlarge instance for the endpoint. The container_startup_health_check_timeout and model_data_download_timeout parameters set the timeout values for the container startup health check and model data download, respectively. 
 
 model_id&nbsp;=&nbsp;"deepseek-ai/DeepSeek-R1-Distill-Llama-8B"
endpoint_name&nbsp;=&nbsp;f"{model_id.split('/')[-1].replace('.',&nbsp;'-')}-sft-djl"
predictor = model.deploy(
&nbsp;&nbsp; &nbsp;initial_instance_count=instance_count,
&nbsp;&nbsp; &nbsp;instance_type=instance_type,
&nbsp;&nbsp; &nbsp;container_startup_health_check_timeout=1800,
&nbsp;&nbsp; &nbsp;model_data_download_timeout=3600
) 
 
It takes a few minutes to deploy the model before it becomes available for inference and evaluation. You can test the endpoint invocation in JupyterLab, by using the AWS SDK with the boto3 client for sagemaker-runtime, or by using the SageMaker Python SDK and the predictor previously created, by using the predict API. 
 
 base_prompt = f"""&lt;s&gt; [INST] {{question}} [/INST] """

prompt = base_prompt.format(
&nbsp; &nbsp; question="What statue is in front of the Notre Dame building?"
)

predictor.predict({
&nbsp;&nbsp; &nbsp;"inputs": prompt,
&nbsp;&nbsp; &nbsp;"parameters": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"max_new_tokens": 300,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"temperature": 0.2,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"top_p": 0.9,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"return_full_text": False,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"stop": ['&lt;/s&gt;']
&nbsp;&nbsp; &nbsp;}
}) 
 
You can also test the model invocation in SageMaker Unified Studio, on the Inference endpoint page and Text inference tab. 
Troubleshooting 
You might encounter some of the following errors while running your model training and deployment: 
 
 Training job fails to start ‚Äì If a training job fails to start, make sure your IAM role AmazonSageMakerDomainExecution has the necessary permissions, verify the instance type is available in your AWS Region, and check your S3 bucket permissions. This role is created when an admin creates the domain, and you can ask the admin to check your IAM access permissions associated with this role. 
 Out-of-memory errors during training ‚Äì If you encounter out-of-memory errors during training, try reducing the batch size, use gradient accumulation to simulate larger batches, or consider using a larger instance. 
 Slow model deployment ‚Äì For slow model deployment, make sure model artifacts aren‚Äôt excessively large, and use appropriate instance types for inference and capacity available for that instance in your Region. 
 
For more troubleshooting tips, refer to Troubleshooting guide. 
Clean up 
SageMaker Unified Studio by default shuts down idle resources such as JupyterLab spaces after 1 hour. However, you must delete the S3 bucket and the hosted model endpoint to stop incurring costs. You can delete the real-time endpoints you created using the SageMaker console. For instructions, see Delete Endpoints and Resources. 
Conclusion 
This post demonstrated how SageMaker Unified Studio serves as a powerful centralized service for data and AI workflows, showcasing its seamless integration capabilities throughout the fine-tuning process. With SageMaker Unified Studio, data engineers and ML practitioners can efficiently discover and access data through SageMaker Catalog, prepare datasets, fine-tune models, and deploy them‚Äîall within a single, unified environment. The service‚Äôs direct integration with SageMaker AI and various AWS analytics services streamlines the development process, alleviating the need to switch between multiple tools and environments. The solution highlights the service‚Äôs versatility in handling complex ML workflows, from data discovery and preparation to model deployment, while maintaining a cohesive and intuitive user experience. Through features like integrated MLflow tracking, built-in model monitoring, and flexible deployment options, SageMaker Unified Studio demonstrates its capability to support sophisticated AI/ML projects at scale. 
To learn more about SageMaker Unified Studio, see An integrated experience for all your data and AI with Amazon SageMaker Unified Studio. 
If this post helps you or inspires you to solve a problem, we would love to hear about it! The code for this solution is available on the GitHub repo for you to use and extend. Contributions are always welcome! 
 
About the authors 
Mona Mona currently works as a Sr World Wide Gen AI Specialist Solutions Architect at Amazon focusing on Gen AI Solutions. She was a Lead Generative AI specialist in Google Public Sector at Google before joining Amazon. She is a published author of two books ‚Äì Natural Language Processing with AWS AI Services and Google Cloud Certified Professional Machine Learning Study Guide. She has authored 19 blogs on AI/ML and cloud technology and a co-author on a research paper on CORD19 Neural Search which won an award for Best Research Paper at the prestigious AAAI (Association for the Advancement of Artificial Intelligence) conference. 
Bruno Pistone is a Senior Generative AI and ML Specialist Solutions Architect for AWS based in Milan. He works with large customers helping them to deeply understand their technical needs and design AI and Machine Learning solutions that make the best use of the AWS Cloud and the Amazon Machine Learning stack. His expertise include: Machine Learning end to end, Machine Learning Industrialization, and Generative AI. He enjoys spending time with his friends and exploring new places, as well as travelling to new destinations. 
Lauren Mullennex is a Senior GenAI/ML Specialist Solutions Architect at AWS. She has a decade of experience in DevOps, infrastructure, and ML. Her areas of focus include MLOps/LLMOps, generative AI, and computer vision.
‚Ä¢ Optimize RAG in production environments using Amazon SageMaker JumpStart and Amazon OpenSearch Service
  Generative AI has revolutionized customer interactions across industries by offering personalized, intuitive experiences powered by unprecedented access to information. This transformation is further enhanced by Retrieval Augmented Generation (RAG), a technique that allows large language models (LLMs) to reference external knowledge sources beyond their training data. RAG has gained popularity for its ability to improve generative AI applications by incorporating additional information, often preferred by customers over techniques like fine-tuning due to its cost-effectiveness and faster iteration cycles. 
The RAG approach excels in grounding language generation with external knowledge, producing more factual, coherent, and relevant responses. This capability proves invaluable in applications such as question answering, dialogue systems, and content generation, where accuracy and informative outputs are crucial. For businesses, RAG offers a powerful way to use internal knowledge by connecting company documentation to a generative AI model. When an employee asks a question, the RAG system retrieves relevant information from the company‚Äôs internal documents and uses this context to generate an accurate, company-specific response. This approach enhances the understanding and usage of internal company documents and reports. By extracting relevant context from corporate knowledge bases, RAG models facilitate tasks like summarization, information extraction, and complex question answering on domain-specific materials, enabling employees to quickly access vital insights from vast internal resources. This integration of AI with proprietary information can significantly improve efficiency, decision-making, and knowledge sharing across the organization. 
A typical RAG workflow consists of four key components: input prompt, document retrieval, contextual generation, and output. The process begins with a user query, which is used to search a comprehensive knowledge corpus. Relevant documents are then retrieved and combined with the original query to provide additional context for the LLM. This enriched input allows the model to generate more accurate and contextually appropriate responses. RAG‚Äôs popularity stems from its ability to use frequently updated external data, providing dynamic outputs without the need for costly and compute-intensive model retraining. 
To implement RAG effectively, many organizations turn to platforms like Amazon SageMaker JumpStart. This service offers numerous advantages for building and deploying generative AI applications, including access to a wide range of pre-trained models with ready-to-use artifacts, a user-friendly interface, and seamless scalability within the AWS ecosystem. By using pre-trained models and optimized hardware, SageMaker JumpStart enables rapid deployment of both LLMs and embedding models, minimizing the time spent on complex scalability configurations. 
In the previous post, we showed how to build a RAG application on SageMaker JumpStart using Facebook AI Similarity Search (Faiss). In this post, we show how to use Amazon OpenSearch Service as a vector store to build an efficient RAG application. 
Solution overview 
To implement our RAG workflow on SageMaker, we use a popular open source Python library known as LangChain. With LangChain, the RAG components are simplified into independent blocks that you can bring together using a chain object that will encapsulate the entire workflow. The solution consists of the following key components: 
 
 LLM (inference) ‚Äì We need an LLM that will do the actual inference and answer the end-user‚Äôs initial prompt. For our use case, we use Meta Llama3 for this component. LangChain comes with a default wrapper class for SageMaker endpoints with which we can simply pass in the endpoint name to define an LLM object in the library. 
 Embeddings model ‚Äì We need an embeddings model to convert our document corpus into textual embeddings. This is necessary for when we‚Äôre doing a similarity search on the input text to see what documents share similarities or contain the information to help augment our response. For this post, we use the BGE Hugging Face Embeddings model available in SageMaker JumpStart. 
 Vector store and retriever ‚Äì To house the different embeddings we have generated, we use a vector store. In this case, we use OpenSearch Service, which allows for similarity search using k-nearest neighbors (k-NN) as well as traditional lexical search. Within our chain object, we define the vector store as the retriever. You can tune this depending on how many documents you want to retrieve. 
 
The following diagram illustrates the solution architecture. 
 
In the following sections, we walk through setting up OpenSearch, followed by exploring the notebook that implements a RAG solution with LangChain, Amazon SageMaker AI, and OpenSearch Service. 
Benefits of using OpenSearch Service as a vector store for RAG 
In this post, we showcase how you can use a vector store such as OpenSearch Service as a knowledge base and embedding store. OpenSearch Service offers several advantages when used for RAG in conjunction with SageMaker AI: 
 
 Performance ‚Äì Efficiently handles large-scale data and search operations 
 Advanced search ‚Äì Offers full-text search, relevance scoring, and semantic capabilities 
 AWS integration ‚Äì Seamlessly integrates with SageMaker AI and other AWS services 
 Real-time updates ‚Äì Supports continuous knowledge base updates with minimal delay 
 Customization ‚Äì Allows fine-tuning of search relevance for optimal context retrieval 
 Reliability ‚Äì Provides high availability and fault tolerance through a distributed architecture 
 Analytics ‚Äì Provides analytical features for data understanding and performance improvement 
 Security ‚Äì Offers robust features such as encryption, access control, and audit logging 
 Cost-effectiveness ‚Äì Serves as an economical solution compared to proprietary vector databases 
 Flexibility ‚Äì Supports various data types and search algorithms, offering versatile storage and retrieval options for RAG applications 
 
You can use SageMaker AI with OpenSearch Service to create powerful and efficient RAG systems. SageMaker AI provides the machine learning (ML) infrastructure for training and deploying your language models, and OpenSearch Service serves as an efficient and scalable knowledge base for retrieval. 
OpenSearch Service optimization strategies for RAG 
Based on our learnings from the hundreds of RAG applications deployed using OpenSearch Service as a vector store, we‚Äôve developed several best practices: 
 
 If you are starting from a clean slate and want to move quickly with something simple, scalable, and high-performing, we recommend using an Amazon OpenSearch Serverless vector store collection. With OpenSearch Serverless, you benefit from automatic scaling of resources, decoupling of storage, indexing compute, and search compute, with no node or shard management, and you only pay for what you use. 
 If you have a large-scale production workload and want to take the time to tune for the best price-performance and the most flexibility, you can use an OpenSearch Service managed cluster. In a managed cluster, you pick the node type, node size, number of nodes, and number of shards and replicas, and you have more control over when to scale your resources. For more details on best practices for operating an OpenSearch Service managed cluster, see Operational best practices for Amazon OpenSearch Service. 
 OpenSearch supports both exact k-NN and approximate k-NN. Use exact k-NN if the number of documents or vectors in your corpus is less than 50,000 for the best recall. For use cases where the number of vectors is greater than 50,000, exact k-NN will still provide the best recall but might not provide sub-100 millisecond query performance. Use approximate k-NN in use cases above 50,000 vectors for the best performance. 
 OpenSearch uses algorithms from the NMSLIB, Faiss, and Lucene libraries to power approximate k-NN search. There are pros and cons to each k-NN engine, but we find that most customers choose Faiss due to its overall performance in both indexing and search as well as the variety of different quantization and algorithm options that are supported and the broad community support. 
 Within the Faiss engine, OpenSearch supports both Hierarchical Navigable Small World (HNSW) and Inverted File System (IVF) algorithms. Most customers find HNSW to have better recall than IVF and choose it for their RAG use cases. To learn more about the differences between these engine algorithms, see Vector search. 
 To reduce the memory footprint to lower the cost of the vector store while keeping the recall high, you can start with Faiss HNSW 16-bit scalar quantization. This can also reduce search latencies and improve indexing throughput when used with SIMD optimization. 
 If using an OpenSearch Service managed cluster, refer to Performance tuning for additional recommendations. 
 
Prerequisites 
Make sure you have access to one ml.g5.4xlarge and ml.g5.2xlarge instance each in your account. A secret should be created in the same region as the stack is deployed.Then complete the following prerequisite steps to create a secret using AWS Secrets Manager: 
 
 On the Secrets Manager console, choose Secrets in the navigation pane. 
 Choose Store a new secret. 
 
 
 
 For Secret type, select Other type of secret. 
 For Key/value pairs, on the Plaintext tab, enter a complete password. 
 Choose Next. 
 
 
 
 For Secret name, enter a name for your secret. 
 Choose Next. 
 
 
 
 Under Configure rotation, keep the settings as default and choose Next. 
 
 
 
 Choose Store to save your secret. 
 
 
 
 On the secret details page, note the secret Amazon Resource Name (ARN) to use in the next step. 
 
 
Create an OpenSearch Service cluster and SageMaker notebook 
We use AWS CloudFormation to deploy our OpenSearch Service cluster, SageMaker notebook, and other resources. Complete the following steps: 
 
 Launch the following CloudFormation template. 
 Provide the ARN of the secret you created as a prerequisite and keep the other parameters as default. 
 
 
 
 Choose Create to create your stack, and wait for the stack to complete (about 20 minutes). 
 When the status of the stack is CREATE_COMPLETE, note the value of OpenSearchDomainEndpoint on the stack Outputs tab. 
 Locate SageMakerNotebookURL in the outputs and choose the link to open the SageMaker notebook. 
 
Run the SageMaker notebook 
After you have launched the notebook in JupyterLab, complete the following steps: 
 
 Go to genai-recipes/RAG-recipes/llama3-RAG-Opensearch-langchain-SMJS.ipynb. 
 
You can also clone the notebook from the GitHub repo. 
 
 
 Update the value of&nbsp;OPENSEARCH_URL&nbsp;in the notebook with the value copied from&nbsp;OpenSearchDomainEndpoint in the previous step (look for os.environ['OPENSEARCH_URL'] = "").&nbsp; The port needs to be 443. 
 Run the cells in the notebook. 
 
The notebook provides a detailed explanation of all the steps. We explain some of the key cells in the notebook in this section. 
For the RAG workflow, we deploy the huggingface-sentencesimilarity-bge-large-en-v1-5 embedding model and meta-textgeneration-llama-3-8b-instruct LLM from Hugging Face. SageMaker JumpStart simplifies this process because the model artifacts, data, and container specifications are all prepackaged for optimal inference. These are then exposed using the SageMaker Python SDK high-level API calls, which let you specify the model ID for deployment to a SageMaker real-time endpoint: 
 
 
&nbsp;sagemaker.jumpstart.model&nbsp;&nbsp;JumpStartModel

model_id&nbsp;&nbsp;"meta-textgeneration-llama-3-8b-instruct"
accept_eula&nbsp;&nbsp;
model&nbsp;&nbsp;JumpStartModel(model_idmodel_id)
llm_predictor&nbsp;&nbsp;modeldeploy(accept_eulaaccept_eula)

model_id&nbsp;&nbsp;"huggingface-sentencesimilarity-bge-large-en-v1-5"
text_embedding_model&nbsp;&nbsp;JumpStartModel(model_idmodel_id)
embedding_predictor&nbsp;&nbsp;text_embedding_modeldeploy() 
 
Content handlers are crucial for formatting data for SageMaker endpoints. They transform inputs into the format expected by the model and handle model-specific parameters like temperature and token limits. These parameters can be tuned to control the creativity and consistency of the model‚Äôs responses. 
 
 class Llama38BContentHandler(LLMContentHandler):
&nbsp;&nbsp; &nbsp;content_type = "application/json"
&nbsp;&nbsp; &nbsp;accepts = "application/json"

&nbsp;&nbsp; &nbsp;def transform_input(self, prompt: str, model_kwargs: dict) -&gt; bytes:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;payload = {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"inputs": prompt,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"parameters": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"max_new_tokens": 1000,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"top_p": 0.9,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"temperature": 0.6,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"stop": ["&lt;|eot_id|&gt;"],
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;},
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;input_str = json.dumps(
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;payload,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;)
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;#print(input_str)
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;return input_str.encode("utf-8") 
 
We use PyPDFLoader from LangChain to load PDF files, attach metadata to each document fragment, and then use RecursiveCharacterTextSplitter to break the documents into smaller, manageable chunks. The text splitter is configured with a chunk size of 1,000 characters and an overlap of 100 characters, which helps maintain context between chunks. This preprocessing step is crucial for effective document retrieval and embedding generation, because it makes sure the text segments are appropriately sized for the embedding model and the language model used in the RAG system. 
 
 import&nbsp;numpy as&nbsp;np
from&nbsp;langchain_community.document_loaders import&nbsp;PyPDFLoader
from&nbsp;langchain.text_splitter import&nbsp;RecursiveCharacterTextSplitter
documents&nbsp;=&nbsp;[]
for&nbsp;idx, file&nbsp;in&nbsp;enumerate(filenames):
&nbsp;&nbsp; &nbsp;loader&nbsp;=&nbsp;PyPDFLoader(data_root&nbsp;+&nbsp;file)
&nbsp;&nbsp; &nbsp;document&nbsp;=&nbsp;loader.load()
&nbsp;&nbsp; &nbsp;for&nbsp;document_fragment&nbsp;in&nbsp;document:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;document_fragment.metadata =&nbsp;metadata[idx]
&nbsp;&nbsp; &nbsp;documents&nbsp;+=&nbsp;document
# - in our testing Character split works better with this PDF data set
text_splitter&nbsp;=&nbsp;RecursiveCharacterTextSplitter(
&nbsp;&nbsp; &nbsp;# Set a really small chunk size, just to show.
&nbsp;&nbsp; &nbsp;chunk_size=1000,
&nbsp;&nbsp; &nbsp;chunk_overlap=100,
)
docs&nbsp;=&nbsp;text_splitter.split_documents(documents)
print(docs[100]) 
 
The following block initializes a vector store using OpenSearch Service for the RAG system. It converts preprocessed document chunks into vector embeddings using a SageMaker model and stores them in OpenSearch Service. The process is configured with security measures like SSL and authentication to provide secure data handling. The bulk insertion is optimized for performance with a sizeable batch size. Finally, the vector store is wrapped with VectorStoreIndexWrapper, providing a simplified interface for operations like querying and retrieval. This setup creates a searchable database of document embeddings, enabling quick and relevant context retrieval for user queries in the RAG pipeline. 
 
 from&nbsp;langchain.indexes.vectorstore import&nbsp;VectorStoreIndexWrapper
# Initialize OpenSearchVectorSearch
vectorstore_opensearch&nbsp;=&nbsp;OpenSearchVectorSearch.from_documents(
&nbsp;&nbsp; &nbsp;docs,
&nbsp;&nbsp; &nbsp;sagemaker_embeddings,
&nbsp;&nbsp; &nbsp;http_auth=awsauth, &nbsp;# Auth will use the IAM role
&nbsp;&nbsp; &nbsp;use_ssl=True,
&nbsp;&nbsp; &nbsp;verify_certs=True,
&nbsp;&nbsp; &nbsp;connection_class=RequestsHttpConnection,
&nbsp;&nbsp; &nbsp;bulk_size=2000&nbsp;&nbsp;# Increase this to accommodate the number of documents you have
)
# Wrap the OpenSearch vector store with the VectorStoreIndexWrapper
wrapper_store_opensearch&nbsp;=&nbsp;VectorStoreIndexWrapper(vectorstore=vectorstore_opensearch) 
 
Next, we use the wrapper from the previous step along with the prompt template. We define the prompt template for interacting with the Meta Llama 3 8B Instruct model in the RAG system. The template uses specific tokens to structure the input in a way that the model expects. It sets up a conversation format with system instructions, user query, and a placeholder for the assistant‚Äôs response. The PromptTemplate class from LangChain is used to create a reusable prompt with a variable for the user‚Äôs query. This structured approach to prompt engineering helps maintain consistency in the model‚Äôs responses and guides it to act as a helpful assistant. 
 
 prompt_template&nbsp;=&nbsp;"""&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;
You are a helpful assistant.
&lt;|eot_id|&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;
{query}
&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;
"""
PROMPT&nbsp;=&nbsp;PromptTemplate(
&nbsp;&nbsp; &nbsp;template=prompt_template, input_variables=["query"]
)
query&nbsp;=&nbsp;"How did AWS perform in 2021?"

answer&nbsp;=&nbsp;wrapper_store_opensearch.query(question=PROMPT.format(query=query), llm=llm)
print(answer) 
 
Similarly, the notebook also shows how to use Retrieval QA, where you can customize how the documents fetched should be added to prompt using the chain_type parameter. 
Clean up 
Delete your SageMaker endpoints from the notebook to avoid incurring costs: 
 
 # Delete resources
llm_predictor.delete_model()
llm_predictor.delete_endpoint()
embedding_predictor.delete_model()
embedding_predictor.delete_endpoint() 
 
Next, delete your OpenSearch cluster to stop incurring additional charges:aws cloudformation delete-stack --stack-name rag-opensearch 
Conclusion 
RAG has revolutionized how businesses use AI by enabling general-purpose language models to work seamlessly with company-specific data. The key benefit is the ability to create AI systems that combine broad knowledge with up-to-date, proprietary information without expensive model retraining. This approach transforms customer engagement and internal operations by delivering personalized, accurate, and timely responses based on the latest company data. The RAG workflow‚Äîcomprising input prompt, document retrieval, contextual generation, and output‚Äîallows businesses to tap into their vast repositories of internal documents, policies, and data, making this information readily accessible and actionable. For businesses, this means enhanced decision-making, improved customer service, and increased operational efficiency. Employees can quickly access relevant information, while customers receive more accurate and personalized responses. Moreover, RAG‚Äôs cost-efficiency and ability to rapidly iterate make it an attractive solution for businesses looking to stay competitive in the AI era without constant, expensive updates to their AI systems. By making general-purpose LLMs work effectively on proprietary data, RAG empowers businesses to create dynamic, knowledge-rich AI applications that evolve with their data, potentially transforming how companies operate, innovate, and engage with both employees and customers. 
SageMaker JumpStart has streamlined the process of developing and deploying generative AI applications. It offers pre-trained models, user-friendly interfaces, and seamless scalability within the AWS ecosystem, making it straightforward for businesses to harness the power of RAG. 
Furthermore, using OpenSearch Service as a vector store facilitates swift retrieval from vast information repositories. This approach not only enhances the speed and relevance of responses, but also helps manage costs and operational complexity effectively. 
By combining these technologies, you can create robust, scalable, and efficient RAG systems that provide up-to-date, context-aware responses to customer queries, ultimately enhancing user experience and satisfaction. 
To get started with implementing this Retrieval Augmented Generation (RAG) solution using Amazon SageMaker JumpStart and Amazon OpenSearch Service, check out the example notebook on GitHub. You can also learn more about Amazon OpenSearch Service in the developer guide. 
 
About the authors 
Vivek Gangasani is a Lead Specialist Solutions Architect for Inference at AWS. He helps emerging generative AI companies build innovative solutions using AWS services and accelerated compute. Currently, he is focused on developing strategies for fine-tuning and optimizing the inference performance of large language models. In his free time, Vivek enjoys hiking, watching movies, and trying different cuisines. 
Harish Rao is a Senior Solutions Architect at AWS, specializing in large-scale distributed AI training and inference. He empowers customers to harness the power of AI to drive innovation and solve complex challenges. Outside of work, Harish embraces an active lifestyle, enjoying the tranquility of hiking, the intensity of racquetball, and the mental clarity of mindfulness practices. 
Raghu Ramesha is an ML Solutions Architect. He specializes in machine learning, AI, and computer vision domains, and holds a master‚Äôs degree in Computer Science from UT Dallas. In his free time, he enjoys traveling and photography. 
Sohaib Katariwala is a Sr. Specialist Solutions Architect at AWS focused on Amazon OpenSearch Service. His interests are in all things data and analytics. More specifically he loves to help customers use AI in their data strategy to solve modern day challenges. 
Karan Jain is a Senior Machine Learning Specialist at AWS, where he leads the worldwide Go-To-Market strategy for Amazon SageMaker Inference. He helps customers accelerate their generative AI and ML journey on AWS by providing guidance on deployment, cost-optimization, and GTM strategy. He has led product, marketing, and business development efforts across industries for over 10 years, and is passionate about mapping complex service features to customer solutions.
‚Ä¢ Advancing AI agent governance with Boomi and AWS: A unified approach to observability and compliance
  Just as APIs became the standard for integration, AI agents are transforming workflow automation through intelligent task coordination. AI agents are already enhancing decision-making and streamlining operations across enterprises. But as adoption accelerates, organizations face growing complexity in managing them at scale. Organizations struggle with observability and lifecycle management, finding it difficult to monitor performance and manage versions effectively. Governance and security concerns arise as these agents process sensitive data, which requires strict compliance and access controls. Perhaps most concerningly, without proper management, organizations face the risk of agent sprawl‚Äîthe unchecked proliferation of AI agents leading to inefficiency and security vulnerabilities. 
Boomi and AWS have collaborated to address the complexity surrounding AI agents with Agent Control Tower, an AI agent management solution developed by Boomi and tightly integrated with Amazon Bedrock. Agent Control Tower, part of the Boomi Agentstudio solution, provides the governance framework to manage this transformation, with capabilities that address both current and emerging compliance needs. 
As a leader in enterprise iPaaS per Gartner‚Äôs Magic Quadrant, based on Completeness of Vision and Ability to Execute, Boomi serves over 20,000 enterprise customers, with three-quarters of these customers operating on AWS. This includes a significant presence among Fortune 500 and Global 2000 organizations across critical sectors such as healthcare, finance, technology, and manufacturing. Boomi is innovating with generative AI, with more than 2,000 customers using its AI agents. The convergence of capabilities that Boomi provides‚Äîspanning AI, integration, automation, API management, and data management‚Äîwith AWS and its proven track record in reliability, security, and AI innovation creates a compelling foundation for standardized AI agent governance at scale. In this post, we share how Boomi partnered with AWS to help enterprises accelerate and scale AI adoption with confidence using Agent Control Tower. 
A unified AI management solution 
Built on AWS, Agent Control Tower uniquely delivers a single control plane for managing AI agents across multiple systems, including other cloud providers and on-premises environments. At its core, it offers comprehensive observability and monitoring, providing real-time performance tracking and deep visibility into agent decision-making and behavior. 
The following screenshot showcases how users can view summary data across agent providers and add or manage providers. 
 
The following screenshot shows an example of the Monitoring and Compliance dashboard. 
 
Agent Control Tower also provides a single pane of glass for visibility into the tools used by each agent, as illustrated in the following screenshot. 
 
Agent Control Tower provides key governance and security controls such as centralized policy enforcement and role-based access control, and enables meeting regulatory compliance with frameworks like GDPR and HIPAA. Furthermore, its lifecycle management capabilities enable automated agent discovery, version tracking, and operational control through features such as pause and resume functionality. Agent Control Tower is positioned as one of the first, if not the first, unified solutions that provides full lifecycle AI agent management with integrated governance and orchestration features. Although many vendors focus on releasing AI agents, there are few that focus on solutions for managing, deploying, and governing AI agents at scale. 
The following screenshot shows an example of how users can review agent details and disable or enable an agent. 
 
As shown in the following screenshot, users can drill down into details for each part of the agent. 
 
Amazon Bedrock: Enabling and enhancing AI governance 
Using Amazon Bedrock, organizations can implement security guardrails and content moderation while maintaining the flexibility to select and switch between AI models for optimized performance and accuracy. Organizations can create and enable access to curated knowledge bases and predefined action groups, enabling sophisticated multi-agent collaboration. Amazon Bedrock also provides comprehensive metrics and trace logs for agents to help facilitate complete transparency and accountability in agent operations. Through deep integration with Amazon Bedrock, Boomi‚Äôs Agent Control Tower enhances agent transparency and governance, offering a unified, actionable view of agent configurations and activities across environments. 
The following diagram illustrates the Agent Control Tower architecture on AWS. 
 
Business impact: Transforming enterprise AI operations 
Consider a global manufacturer using AI agents for supply chain optimization. With Agent Control Tower, they can monitor agent performance across regions in real time, enforce consistent security policies, and enable regulatory compliance. When issues arise, they can quickly identify and resolve them while maintaining the ability to scale AI operations confidently. With this level of control and visibility, organizations can deploy AI agents more effectively while maintaining robust security and compliance standards. 
Conclusion 
Boomi customers have already deployed more than 33,000 agents and are seeing up to 80% less time spent on documentation and 50% faster issue resolution. With Boomi and AWS, enterprises can accelerate and scale AI adoption with confidence, backed by a product that puts visibility, governance, and security first. Discover how Agent Control Tower can help your organization manage AI agent sprawl and take advantage of scalable, compliance-aligned innovation. Take a guided tour and learn more about Boomi Agent Control Tower and Amazon Bedrock integration. Or, you can get started today with AI FastTrack. 
 
About the authors 
 Deepak Chandrasekar is the VP of Software Engineering &amp; User Experience and leads multidisciplinary teams at Boomi. He oversees flagship initiatives like Boomi‚Äôs Agent Control Tower, Task Automation, and Market Reach, while driving a cohesive and intelligent experience layer across products. Previously, Deepak held a key leadership role at Unifi Software, which was acquired by Boomi. With a passion for building scalable, and intuitive AI-powered solutions, he brings a commitment to engineering excellence and responsible innovation. 
 Sandeep Singh is Director of Engineering at Boomi, where he leads global teams building solutions that enable enterprise integration and automation at scale. He drives initiatives like Boomi Agent Control Tower, Marketplace, and Labs, empowering partners and customers with intelligent, trusted solutions. With leadership experience at GE and Fujitsu, Sandeep brings expertise in API strategy, product engineering, and AI/ML solutions. A former solution architect, he is passionate about designing mission-critical systems and driving innovation through scalable, intelligent solutions. 
 Santosh Ameti is a seasoned Engineering leader in the Amazon Bedrock team and has built Agents, Evaluation, Guardrails, and Prompt Management solutions. His team continuously innovates in the agentic space, delivering one of the most secure and managed agentic solutions for enterprises. 
 Greg Sligh is a Senior Solutions Architect at AWS with more than 25 years of experience in software engineering, software architecture, consulting, and IT and Engineering leadership roles across multiple industries. For the majority of his career, he has focused on creating and delivering distributed, data-driven applications with particular focus on scale, performance, and resiliency. Now he helps ISVs meet their objectives across technologies, with particular focus on AI/ML. 
 Padma Iyer is a Senior Customer Solutions Manager at Amazon Web Services, where she specializes in supporting ISVs. With a passion for cloud transformation and financial technology, Padma works closely with ISVs to guide them through successful cloud transformations, using best practices to optimize their operations and drive business growth. Padma has over 20 years of industry experience spanning banking, tech, and consulting.

‚∏ª