‚úÖ Morning News Briefing ‚Äì October 28, 2025 10:47

üìÖ Date: 2025-10-28 10:47
üè∑Ô∏è Tags: #briefing #ai #publichealth #digitalgov

‚∏ª

üßæ Weather
‚Ä¢ Current Conditions:  -4.4¬∞C
  Temperature: -4.4&deg;C Pressure / Tendency: 103.3 kPa rising Humidity: 95 % Wind Chill: -5 . Dewpoint: 5.1.1&deg:C Wind: NW 2 km/h . Air Quality Health Index: n/a . Pembroke 6:00 AM EDT Tuesday 28 October 2025 temperature: -
‚Ä¢ Tuesday: Sunny. High 10.
  Fog patches dissipating this morning. Sunny. High 10.5C in the morning . Sunny. UV index 3 or moderate in the afternoon . Forecast issued 5:00 AM EDT Tuesday 28 October 2025. For the rest of the year, see www.jenn.com/jennifera.org for the latest update on the weather . For the latest in-depth
‚Ä¢ Tuesday night: Clear. Low minus 4.
  Fog patches developing after midnight . Clear. Clear. Low minus 4.50 degrees Fahrenheit . Low minus 5 degrees Fahrenheit for Tuesday morning . Clear skies expected to be sunny and breezy on Wednesday night . Forecast issued 5:00 AM EDT Tuesday 28 October 2025 for the year on October 28, 2025 for most of the year . For the year, the National Weather Service will issue the

üåç International News
No updates.

üçÅ Canadian News
No updates.

üá∫üá∏ U.S. Top Stories
‚Ä¢ America's immigration crackdown is disrupting the global remittance market
  America's immigration crackdown might have serious financial consequences for a range of countries . U.S. immigration crackdown has serious financial implications for a host of countries, including the U.K. It could also have serious implications for the rest of the world's economies . The U.N. government is cracking down on the issue of illegal immigration in the wake of the Obama administration's crackdown .
‚Ä¢ 'Washington Post' editorials omit a key disclosure: Bezos' financial ties
  Editorials at the 'Washington Post' failed to disclose that they focused on matters in which owner Jeff Bezos had a material interest . Three times in the past two weeks, editorials at Washington Post didn't disclose Bezos' interest in matters that he has a material stake in . The Washington Post has been accused of over-indeflecting its editorial decisions in favor of Jeff Bezos .
‚Ä¢ Fight over government layoffs continues as shutdown drags on
  A federal judge in San Francisco will consider whether to indefinitely halt the thousands of layoffs of federal employees announced by the Trump administration since Oct. 1 . Federal employees have been laid off by the administration since October 1 . A judge will decide whether to halt the layoffs indefinitely . The judge is expected to make a decision on whether to temporarily halt the layoff . The federal government has announced thousands of
‚Ä¢ Volunteers foster literacy by reading to children and giving them books
  Volunteers with the LiTEArary society read to children who live in "book deserts" and bring them their own books . Volunteers read books to children living in book deserts and bring books to them . Volunteers also read books for children who have no access to books in their homes . The LiTEARARary society is raising money to help children in "Book deserts" through donations .
‚Ä¢ As Trump talks of designating antifa a foreign terrorist group, experts see danger
  Domestic terrorism expert says designation would have a "cascading effect across civil society," including social media organizations, civic organizations and everything in between" The designation could have a huge impact on civil society, including social networks and civic organizations, experts say . The designation is expected to have "a cascading effect," one expert says of the designation of domestic terror groups in the U.S

üß† Artificial Intelligence
No updates.

üíª Digital Strategy
‚Ä¢ Google says reports of a Gmail breach have been greatly exaggerated
  Panic spread faster than a phishing email on Tuesday after claims of a massive Gmail breach hit the headlines . But Google says it's all nonsense . Ad and cloud biz rubbishes claims that 183 million accounts were broken into . Google says the claims are all nonsense and that Gmail accounts were hacked into the wrong way of answering questions about the breach . The breach is believed to be the
‚Ä¢ Chatbots parrot Putin's propaganda about the illegal invasion of Ukraine
  Fake views from Moscow's pet media outlets appear in about one in five responses . Popular chatbots powered by large language models cited links to Russian state-attributed sources in up to a quarter of answers about the war in Ukraine . AI risks undermining efforts to enforce sanctions on Moscow-backed media, raising fresh questions over whether AI could undermine efforts to impose sanctions on Russia-backed news .
‚Ä¢ UK government inflates G-Cloud framework to ¬£14B
  UK government launches competition for cloud services worth up to ¬£14 billion over four years . Nearly triple ¬£4.8 billion over 18 months announced in an earlier market engagement . Procurement delays and lock-in fears see framework balloon in size and scope . UK government has launched competition for services worth ¬£14bn over 4 years ‚Äì nearly triple the ¬£4bn announced in earlier engagement .
‚Ä¢ Marks &amp; Spencer swaps out TCS for fresh helpdesk deal
  Move follows months-long procurement process as retailer refreshes parts of its IT support setup . Marks &amp; Spencer replaced Tata Consultancy Services as its IT service desk provider following procurement process that began in January . Retailer says it has replaced its IT services desk provider after a procurement process started in January, which began months earlier this year, following months of procurement process which began in February
‚Ä¢ IBM Cloud stops signing and seeking new customers for its VMware service
  IBM has announced it will stop marketing its VMware on IBM Cloud service to new customers . Blames Broadcom‚Äôs licensing changes that haven‚Äôt caused other hyperscalers to pull the pin . IBM: Broadcom's licensing changes haven't caused other companies to pull away from the IBM Cloud Service to stop marketing the service to a new customer base . The company says it will

üè• Public Health
No updates.

üî¨ Science
‚Ä¢ Cost effectiveness analysis of low dose computed tomography lung cancer screening in Chinese population
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ This ‚Äòminor‚Äô bird flu strain has potential to spark human pandemic
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Evaluating voluntary care seeking effects on COVID-19 outcomes and health system costs
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Applying machine learning to predict quality ANC determinants in Bangladesh: a BDHS-2022 cross-sectional study
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Perceptions of air pollution from stubble burning and its health risks in Punjab, India
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations

üßæ Government & Policy
No updates.

üèõÔ∏è Enterprise Architecture & IT Governance
No updates.

ü§ñ AI & Emerging Tech
‚Ä¢ An AI adoption riddle
  A few weeks ago, a report found 95% of generative AI pilots were failing, causing a brief stock market panic . But the companies that took these developments as a sign to perhaps not go all in on AI were nowhere to be found . The speed of AI progress and adoption has made me think industries are more sensitive to news than they perhaps should be . The question remains unanswered: Are there companies out there rethinking how much their bets on AI will pay off?
‚Ä¢ The Download: what to make of OpenAI‚Äôs Atlas browser, and how to make climate progress
  This is today&#8217;s edition of¬†The Download,¬†our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



I tried OpenAI‚Äôs new Atlas browser but I still don‚Äôt know what it‚Äôs for



‚ÄîMat Honan



OpenAI rolled out a new web browser last week called Atlas. It comes with ChatGPT built in, along with an agent, so that you can browse, get answers, and have automated tasks performed on your behalf all at the same time.



I‚Äôve spent the past several days tinkering with Atlas. I‚Äôve used it to do all my normal web browsing, and also tried to take advantage of the ChatGPT functions‚Äîplus I threw some weird agentic tasks its way to see how it did with those.My impression is that Atlas is‚Ä¶&nbsp; fine? But my big takeaway is that it‚Äôs pretty pointless for anyone not employed by OpenAI. In fact, Atlas seems to be little more than cynicism masquerading as software. Read the full story.



This review first appeared in The Debrief, Mat Honan&#8217;s weekly subscriber-only newsletter.







Seeking climate solutions in turbulent times



Despite recent political shifts in the US, companies are continuing to pursue exciting new climate solutions. Tomorrow we‚Äôre holding an exclusive subscriber-only Roundtable event digging into the most promising technologies of the moment drawing from our recently released 10 Climate Tech Companies to Watch list.This conversation will give subscribers insight into where tangible climate progress is happening today, and how recent political changes are reshaping the path toward a more sustainable future. Join us at 1pm ET on Tuesday October 28‚Äîregister here!







The must-reads



I‚Äôve combed the internet to find you today‚Äôs most fun/important/scary/fascinating stories about technology.



1 Donald Trump says a TikTok deal could be reached this week¬†Perhaps on Thursday, when he‚Äôs due to meet Xi Jinping. (CNBC)+ US treasury secretary Scott Bessent appeared to jump the gun when he said the deal had already been done. (The Guardian)



2 Covid vaccines helped to prolong the life of cancer patientsThe findings raise hopes a universal vaccine could help patients with different cancers. (WP $)+ Why US federal health agencies are abandoning mRNA vaccines. (MIT Technology Review)



3 How developing nations benefit from ‚ÄúAI decolonization‚ÄùRules forcing Silicon Valley‚Äôs giants to process data locally has helped to spread the AI boom‚Äôs wealth. (WSJ $)+ Meanwhile, Saudi Arabia wants to be known as the ‚ÄúAI exporter.‚Äù (NYT $)+ Inside India‚Äôs scramble for AI independence. (MIT Technology Review)4 Those rising electricity costs aren‚Äôt just down to AICostly electrical equipment and disaster prep are bigger factors pushing up prices. (WP $)+ Amazon considered concealing its data centers&#8217; water usage. (The Guardian)+ AI is changing the grid. Could it help more than it harms? (MIT Technology Review)



5 California State wants to become America‚Äôs largest ‚ÄúAI-empowered‚Äù UniversityIt‚Äôs teaming up with Amazon, OpenAI and Nvidia to prepare its students for increasingly AI-driven careers. (NYT $)+ How do technologies change our abilities to learn skills? (The Atlantic $)+ Why the ultra-wealthy are sending their kids to High Point University. (WSJ $)+ The job market is tough right now, but we‚Äôve weathered this kind of storm before. (Insider $)



6 This new startup sells AI bot interactions to manipulate social mediaEven though it violates every major platforms‚Äô policies. (404 Media)



7 Even real estate isn‚Äôt safe from AI slop House hunters are being forced to wade through AI-enhanced listings. (Wired $)



8 Why we‚Äôre so obsessed with sleepmaxxing¬†Yes, sleep is good for you. But does the tech that tracks it really do the job it claims to? (The Atlantic $)+ I tried to hack my insomnia with technology. Here‚Äôs what worked. (MIT Technology Review)



9 It‚Äôs probably not worth buying an Ultra-HD TVSo feel free to ignore all that persuasive marketing jargon. (The Guardian)10 Sneaky employees are using AI to fake their expense receipts So expense firms are in turn deploying AI to try and detect the fakes. (FT $)







Quote of the day



‚ÄúI‚Äôm skeptical of all of the hype around AI right now. This is not my first bubble.‚Äù



‚ÄîJay Goldberg, a senior analyst at Seaport Global Securities, is no stranger to the hysteria that surrounds overhyped technologies, he tells Bloomberg.







One more thing







Inside Clear‚Äôs ambitions to manage your identity beyond the airportClear Secure is the most visible biometric identity company in the United States. Best known for its line-jumping service in airports, it‚Äôs also popping up at sports arenas and stadiums all over the country. You can also use its identity verification platform to rent tools at Home Depot, put your profile in front of recruiters on LinkedIn, and, as of this month, verify your identity as a rider on Uber.And soon enough, if Clear has its way, it may also be in your favorite retailer, bank, and even doctor‚Äôs office‚Äîor anywhere else that you currently have to pull out a wallet (or wait in line).While the company has been building toward this sweeping vision for years, it now seems its time has finally come. But as biometrics go mainstream, what‚Äîand who‚Äîbears the cost? Read the full story.¬†‚ÄîEileen Guo







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)



+ Ancient manuscripts are jam packed with weird and wonderful beasts.+ Horror writers tell us the spooky stories that send a shiver down their respective spines.+ Here‚Äôs why living on a red dwarf isn‚Äôt quite as crazy as it sounds.+ Kiki the sheep may not be able to walk, but she isn‚Äôt letting it get in the way of her getting around  (thanks Amy!)
‚Ä¢ I tried OpenAI‚Äôs new Atlas browser but I still don‚Äôt know what it‚Äôs for
  OpenAI rolled out a new web browser last week called Atlas . It comes with ChatGPT built in, along with an agent, so that you can browse, get direct answers, and have automated tasks performed on your behalf all at the same time . In some cases, the built-in chatbot was worse and dumber . The real user is the company collecting data about what Atlas is browsing websites, it's collecting data .
‚Ä¢ Stand Up for Research, Innovation, and Education
  The MIT community is standing up for MIT and its mission to serve the nation and the world . Right now, MIT alumni and friends are voicing their support for:¬†America‚Äôs scientific and technological leadership¬†- and affordable education . We need you to join us at this critical moment. The community needs to voice support for the MIT mission and the future of the university . The community
‚Ä¢ The Download: carbon removal‚Äôs future, and measuring pain using an app
  This is today&#8217;s edition of¬†The Download,¬†our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



What‚Äôs next for carbon removal?



After years of growth that spawned hundreds of startups, the nascent carbon removal sector appears to be facing a reckoning.Running Tide, a promising aquaculture company, shut down its operations last summer, and a handful of other companies have shuttered, downsized, or pivoted in recent months as well. Venture investments have flagged. And the collective industry hasn‚Äôt made a whole lot more progress toward Running Tide‚Äôs ambitious plans to sequester a billion tons of carbon dioxide by this year.



The hype phase is over and the sector is sliding into the turbulent business trough that follows, experts warn.¬†



And the open question is: If the carbon removal sector is heading into a painful if inevitable clearing-out cycle, where will it go from there? Read the full story.



‚ÄîJames Temple



This story is part of MIT Technology Review‚Äôs What‚Äôs Next series, which looks across industries, trends, and technologies to give you a first look at the future. You can read the rest of them here.







An AI app to measure pain is here



This week I‚Äôve also been wondering how science and technology can help answer that question‚Äîespecially when it comes to pain.&nbsp;



In the latest issue of MIT Technology Review‚Äôs print magazine, Deena Mousa describes how an AI-powered smartphone app is being used to assess how much pain a person is in.



The app, and other tools like it, could help doctors and caregivers. They could be especially useful in the care of people who aren‚Äôt able to tell others how they are feeling.



But they are far from perfect. And they open up all kinds of thorny questions about how we experience, communicate, and even treat pain. Read the full story.



‚ÄîJessica Hamzelou



This article first appeared in The Checkup, MIT Technology Review‚Äôs weekly biotech newsletter. To receive it in your inbox every Thursday, and read articles like this first, sign up here.







The must-reads



I‚Äôve combed the internet to find you today‚Äôs most fun/important/scary/fascinating stories about technology.



1 Meta‚Äôs lawyers advised workers to remove parts of its teen mental health researchIts counsel told researchers to block or update their work to reduce legal liability. (Bloomberg $)+ Meta recently laid off more than 100 staff tasked with monitoring risks to user privacy. (NYT $)&nbsp;



2 Donald Trump has pardoned the convicted Binance founderChangpeng Zhao pleaded guilty to violating US money laundering laws in 2023. (WSJ $)+ The move is likely to enable Binance to resume operating in the US. (CNN)+ Trump has vowed to be more crypto-friendly than the Biden administration. (Axios)



3 Anthropic and Google Cloud have signed a major chips dealThe agreement is worth tens of billions of dollars. (FT $)



4 Microsoft doesn‚Äôt want you to talk dirty to its AIIt‚Äôll leave that kind of thing to OpenAI, thank you very much. (CNBC)+ Copilot now has its own version of Clippy‚Äîjust don‚Äôt try to get erotic with it. (The Verge)+ It‚Äôs pretty easy to get DeepSeek to talk dirty, however. (MIT Technology Review)5 Big Tech is footing the bill for Trump‚Äôs White House ballroomStand up Amazon, Apple, Google, Meta, and Microsoft. (TechCrunch)+ Crypto twins Tyler and Cameron Winklevoss are also among the donors. (CNN)



6 US investigators have busted a series of high-tech gambling schemesInvolving specially-designed contact lenses and x-ray tables. (NYT $)+ The case follows insider bets on basketball and poker games rigged by the mafia. (BBC)+ Automatic card shufflers can be compromised, too. (Wired $)



7 Deepfake harassment tools are easily accessible on social mediaAnd simple web searches. (404 Media)+ Bans on deepfakes take us only so far‚Äîhere‚Äôs what we really need. (MIT Technology Review)



8 How algorithms can drive up prices onlineEven benign algorithms can sometimes yield bad outcomes for buyers. (Quanta Magazine)+ When AIs bargain, a less advanced agent could cost you. (MIT Technology Review)



9 How to give an LLM brain rotTrain it on short ‚Äúsuperficial‚Äù posts from X, for a start. (Ars Technica)+ AI trained on AI garbage spits out AI garbage. (MIT Technology Review)



10 Meet the tech workers using AI as little as possibleIn a bid to keep their skills sharp. (WP $)+ This professor thinks there are other ways to teach people how to learn. (The Atlantic $)







Quote of the day



‚ÄúHe was convicted. He‚Äôs not innocent.‚Äù



‚ÄîRepublican Senator Thom Tillis criticises Donald Trump‚Äôs decision to pardon convicted cryptocurrency mogul Changpeng Zhao, Politico reports.







One more thing







We‚Äôve never understood how hunger works. That might be about to change.



When you‚Äôre starving, hunger is like a demon. It awakens the most ancient and primitive parts of the brain, then commandeers other neural machinery to do its bidding until it gets what it wants.Although scientists have had some success in stimulating hunger in mice, we still don‚Äôt really understand how the impulse to eat works. Now, some experts are following known parts of the neural hunger circuits into uncharted parts of the brain to try and find out.Their work could shed new light on the factors that have caused the number of overweight adults worldwide to skyrocket in recent years. And it could also help solve the mysteries around how and why a new class of weight-loss drugs seems to work so well. Read the full story.



‚ÄîAdam Piore







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)+¬† Middle aged men are getting into cliff-jumping. Should you?+ Pumpkin spice chocolate chip cookies sounds like a great idea to me.+ Christmas Island‚Äôs crabs are on the move! + Watch out if you‚Äôre taking the NY subway today: you might bump into these terrifying witches.

üîí Cybersecurity & Privacy
No updates.

üéì University AI
No updates.

üè¢ Corporate AI
‚Ä¢ Responsible AI design in healthcare and life sciences
  Generative AI has emerged as a transformative technology in healthcare, driving digital transformation in essential areas such as patient engagement and care management. It has shown potential to revolutionize how clinicians provide improved care through automated systems with diagnostic support tools that provide timely, personalized suggestions, ultimately leading to better health outcomes. For example, a study reported in BMC Medical Education that medical students who received large language model (LLM)-generated feedback during simulated patient interactions significantly improved their clinical decision-making compared to those who did not. 
At the center of most generative AI systems are LLMs capable of generating remarkably natural conversations, enabling healthcare customers to build products across billing, diagnosis, treatment, and research that can perform tasks and operate independently with human oversight. However, the utility of generative AI requires an understanding of the potential risks and impacts on healthcare service delivery, which necessitates the need for careful planning, definition, and execution of a system-level approach to building safe and responsible generative AI-infused applications. 
In this post, we focus on the design phase of building healthcare generative AI applications, including defining system-level policies that determine the inputs and outputs. These policies can be thought of as guidelines that, when followed, help build a responsible AI system. 
Designing responsibly 
LLMs can transform healthcare by reducing the cost and time required for considerations such as quality and reliability. As shown in the following diagram, responsible AI considerations can be successfully integrated into an LLM-powered healthcare application by considering quality, reliability, trust, and fairness for everyone. The goal is to promote and encourage certain responsible AI functionalities of AI systems. Examples include the following: 
 
 Each component‚Äôs input and output is aligned with clinical priorities to maintain alignment and promote controllability 
 Safeguards, such as guardrails, are implemented to enhance the safety and reliability of your AI system 
 Comprehensive AI red-teaming and evaluations are applied to the entire end-to-end system to assess safety and privacy-impacting inputs and outputs 
 
Conceptual architecture 
The following diagram shows a conceptual architecture of a generative AI application with an LLM. The inputs (directly from an end-user) are mediated through input guardrails. After the input has been accepted, the LLM can process the user‚Äôs request using internal data sources. The output of the LLM is again mediated through guardrails and can be shared with end-users. 
 
Establish governance mechanisms 
When building generative AI applications in healthcare, it‚Äôs essential to consider the various risks at the individual model or system level, as well as at the application or implementation level. The risks associated with generative AI can differ from or even amplify existing AI risks. Two of the most important risks are confabulation and bias: 
 
 Confabulation ‚Äî The model generates confident but erroneous outputs, sometimes referred to as hallucinations. This could mislead patients or clinicians. 
 Bias ‚Äî This refers to the risk of exacerbating historical societal biases among different subgroups, which can result from non-representative training data. 
 
To mitigate these risks, consider establishing content policies that clearly define the types of content your applications should avoid generating. These policies should also guide how to fine-tune models and which appropriate guardrails to implement. It is crucial that the policies and guidelines are tailored and specific to the intended use case. For instance, a generative AI application designed for clinical documentation should have a policy that prohibits it from diagnosing diseases or offering personalized treatment plans. 
Additionally, defining clear and detailed policies that are specific to your use case is fundamental to building responsibly. This approach fosters trust and helps developers and healthcare organizations carefully consider the risks, benefits, limitations, and societal implications associated with each LLM in a particular application. 
The following are some example policies you might consider using for your healthcare-specific applications. The first table summarizes the roles and responsibilities for human-AI configurations. 
 
  
   
   Action ID 
   Suggested Action 
   Generative AI Risks 
   
   
   GV-3.2-001 
   Policies are in place to bolster oversight of generative AI systems with independent evaluations or assessments of generative AI models or systems where the type and robustness of evaluations are proportional to the identified risks. 
   CBRN Information or Capabilities; Harmful Bias and Homogenization 
   
   
   GV-3.2-002 
   Consider adjustment of organizational roles and components across lifecycle stages of large or complex generative AI systems, including: test and evaluation, validation, and red-teaming of generative AI systems; generative AI content moderation; generative AI system development and engineering; increased accessibility of generative AI tools, interfaces, and systems; and incident response and containment. 
   Human-AI Configuration; Information Security; Harmful Bias and Homogenization 
   
   
   GV-3.2-003 
   Define acceptable use policies for generative AI interfaces, modalities, and human-AI configurations (for example, for AI assistants and decision-making tasks), including criteria for the kinds of queries generative AI applications should refuse to respond to. 
   Human-AI Configuration 
   
   
   GV-3.2-004 
   Establish policies for user feedback mechanisms for generative AI systems that include thorough instructions and any mechanisms for recourse. 
   Human-AI Configuration 
   
   
   GV-3.2-005 
   Engage in threat modeling to anticipate potential risks from generative AI systems. 
   CBRN Information or Capabilities; Information Security 
   
  
 
The following table summarizes policies for risk management in AI system design. 
 
  
   
   Action ID 
   Suggested Action 
   Generative AI Risks 
   
   
   GV-4.1-001 
   Establish policies and procedures that address continual improvement processes for generative AI risk measurement. Address general risks associated with a lack of explainability and transparency in generative AI systems by using ample documentation and techniques such as application of gradient-based attributions, occlusion or term reduction, counterfactual prompts and prompt engineering, and analysis of embeddings. Assess and update risk measurement approaches at regular cadences. 
   Confabulation 
   
   
   GV-4.1-002 
   Establish policies, procedures, and processes detailing risk measurement in context of use with standardized measurement protocols and structured public feedback exercises such as AI red-teaming or independent external evaluations. 
   CBRN Information and Capability; Value Chain and Component Integration 
   
  
 
Transparency artifacts 
Promoting transparency and accountability throughout the AI lifecycle can foster trust, facilitate debugging and monitoring, and enable audits. This involves documenting data sources, design decisions, and limitations through tools like model cards and offering clear communication about experimental features. Incorporating user feedback mechanisms further supports continuous improvement and fosters greater confidence in AI-driven healthcare solutions. 
AI developers and DevOps engineers should be transparent about the evidence and reasons behind all outputs by providing clear documentation of the underlying data sources and design decisions so that end-users can make informed decisions about the use of the system. Transparency enables the tracking of potential problems and facilitates the evaluation of AI systems by both internal and external teams. Transparency artifacts guide AI researchers and developers on the responsible use of the model, promote trust, and help end-users make informed decisions about the use of the system. 
The following are some implementation suggestions: 
 
 When building AI features with experimental models or services, it‚Äôs essential to highlight the possibility of unexpected model behavior so healthcare professionals can accurately assess whether to use the AI system. 
 Consider publishing artifacts such as Amazon SageMaker model cards or AWS system cards. Also, at AWS we provide detailed information about our AI systems through AWS AI Service Cards, which list intended use cases and limitations, responsible AI design choices, and deployment and performance optimization best practices for some of our AI services. AWS also recommends establishing transparency policies and processes for documenting the origin and history of training data while balancing the proprietary nature of training approaches. Consider creating a hybrid document that combines elements of both model cards and service cards, because your application likely uses foundation models (FMs) but provides a specific service. 
 Offer a feedback user mechanism. Gathering regular and scheduled feedback from healthcare professionals can help developers make necessary refinements to improve system performance. Also consider establishing policies to help developers allow for user feedback mechanisms for AI systems. These should include thorough instructions and consider establishing policies for any mechanisms for recourse. 
 
Security by design 
When developing AI systems, consider security best practices at each layer of the application. Generative AI systems might be vulnerable to adversarial attacks suck as prompt injection, which exploits the vulnerability of LLMs by manipulating their inputs or prompt. These types of attacks can result in data leakage, unauthorized access, or other security breaches. To address these concerns, it can be helpful to perform a risk assessment and implement guardrails for both the input and output layers of the application. As a general rule, your operating model should be designed to perform the following actions: 
 
 Safeguard patient privacy and data security by implementing personally identifiable information (PII) detection, configuring guardrails that check for prompt attacks 
 Continually assess the benefits and risks of all generative AI features and tools and regularly monitor their performance through Amazon CloudWatch or other alerts 
 Thoroughly evaluate all AI-based tools for quality, safety, and equity before deploying 
 
Developer resources 
The following resources are useful when architecting and building generative AI applications: 
 
 Amazon Bedrock Guardrails helps you implement safeguards for your generative AI applications based on your use cases and responsible AI policies. You can create multiple guardrails tailored to different use cases and apply them across multiple FMs, providing a consistent user experience and standardizing safety and privacy controls across your generative AI applications. 
 The AWS responsible AI whitepaper serves as an invaluable resource for healthcare professionals and other developers that are developing AI applications in critical care environments where errors could have life-threatening consequences. 
 AWS AI Service Cards explains the use cases for which the service is intended, how machine learning (ML) is used by the service, and key considerations in the responsible design and use of the service. 
 
Conclusion 
Generative AI has the potential to improve nearly every aspect of healthcare by enhancing care quality, patient experience, clinical safety, and administrative safety through responsible implementation. When designing, developing, or operating an AI application, try to systematically consider potential limitations by establishing a governance and evaluation framework grounded by the need to maintain the safety, privacy, and trust that your users expect. 
For more information about responsible AI, refer to the following resources: 
 
 NIST Trustworthy and Responsible AI 
 OWASP Top 10 for Large Language Model applications 
 
 
 
About the authors 
Tonny Ouma is an Applied AI Specialist at AWS, specializing in generative AI and machine learning. As part of the Applied AI team, Tonny helps internal teams and AWS customers incorporate leading-edge AI systems into their products. In his spare time, Tonny enjoys riding sports bikes, golfing, and entertaining family and friends with his mixology skills. 
Simon Handley, PhD, is a Senior AI/ML Solutions Architect in the Global Healthcare and Life Sciences team at Amazon Web Services. He has more than 25 years‚Äô experience in biotechnology and machine learning and is passionate about helping customers solve their machine learning and life sciences challenges. In his spare time, he enjoys horseback riding and playing ice hockey.
‚Ä¢ Beyond pilots: A proven framework for scaling AI to production
  The era of perpetual AI pilots is over. This year, 65% of AWS Generative AI Innovation Center customer projects moved from concept to production‚Äîsome launching in just 45 days, as AWS VP Swami Sivasubramanian shared on LinkedIn. These results come from insights gained across more than one thousand customer implementations. 
The Generative AI Innovation Center pairs organizations across industries with AWS scientists, strategists, and engineers to implement practical AI solutions that drive measurable outcomes. These initiatives transform diverse sectors worldwide. For example, through a cross-functional AWS collaboration, we supported the National Football League (NFL) to create a generative AI-powered solution that obtains statistical game insights within 30 seconds. This helps their media and production teams locate video content six times faster. Similarly, we helped Druva‚Äôs DruAI system streamline customer support and data protection through natural language processing, reducing investigation time from hours to minutes. 
These achievements reflect a broader pattern of success, driven by a powerful methodology: The Five V‚Äôs Framework for AI Implementation. 
 
This framework takes projects from initial testing to full deployment by focusing on concrete business outcomes and operational excellence. It‚Äôs grounded in two of Amazon‚Äôs Leadership Principles, Customer Obsession and Deliver Results. By starting with what customers actually need and working backwards, we‚Äôve helped companies across industries modernize their operations and better serve their customers. 
The Five V‚Äôs Framework: A foundation for success 
Every successful AI deployment begins with groundwork. In our experience, projects thrive when organizations first identify specific challenges they need to solve, align key stakeholders around these goals, and establish clear accountability for results. The Five V‚Äôs Framework helps guide organizations through a structured process: 
 
 Value: Target high-impact opportunities aligned with your strategic priorities 
 Visualize: Define clear success metrics that link directly to business outcomes 
 Validate: Test solutions against real-world requirements and constraints 
 Verify: Create a scalable path to production that delivers sustainable results 
 Venture: Secure the resources and support needed for long-term success 
 
Value: The critical first step 
The Value phase emphasizes working backwards from your most pressing business challenges. By starting with existing pain points and collaborating across technical and business teams, organizations can develop solutions that deliver meaningful return on investment (ROI). This focused approach helps direct resources where they‚Äôll have the greatest impact. 
Visualize: Defining success through measurement 
The next step requires translating the potential benefits‚Äîcost reduction, revenue growth, risk mitigation, improved customer experience, and competitive advantage‚Äîinto clear, measurable performance indicators. A comprehensive measurement framework starts with baseline metrics using historical data where available. These metrics should address both technical aspects like accuracy and response time, as well as business outcomes such as productivity gains and customer satisfaction. 
The Visualize phase examines data availability and quality to support proper measurement while working with stakeholders to define success criteria that align with strategic objectives. This dual focus helps organizations track not just the performance of the AI solution, but its actual impact on business goals. 
Validate: Where ambition meets reality 
The Validate phase focuses on testing solutions against real-world conditions and constraints. Our approach integrates strategic vision with implementation expertise from day one. As Sri Elaprolu, Director of the Generative AI Innovation Center, explains: ‚ÄúEffective validation creates alignment between vision and execution. We unite diverse perspectives‚Äîfrom scientists to business leaders‚Äîso that solutions deliver both technical excellence and measurable business impact.‚Äù 
This process involves systematic integration testing, stress testing for expected loads, verifying compliance requirements, and gathering end-user feedback. Security specialists shape the core architecture. Industry subject matter experts define the operational processes and decision logic that guide prompt design and model refinement. Change management strategies are integrated early to ensure alignment and adoption. 
The Generative AI Innovation Center partnered with SparkXGlobal, an AI-driven marketing-technology company, to validate their new solution through comprehensive testing. Their platform, Xnurta, provides business analytics and reporting for Amazon merchants, demonstrating impressive results: report processing time dropped from 6-8 hours to just 8 minutes while maintaining 95% accuracy. This successful validation established a foundation for SparkXGlobal‚Äôs continued innovation and enhanced AI capabilities. 
Working with the Generative AI Innovation Center, the U.S. Environmental Protection Agency (EPA) created an intelligent document processing solution powered by Anthropic models on Amazon Bedrock. This solution helped EPA scientists accelerate chemical risk assessments and pesticide reviews through transparent, verifiable, and human-controlled AI practices. The impact has been substantial: document processing time decreased by 85%, evaluation costs dropped by 99%, and more than 10,000 regulatory applications have advanced faster to protect public health. 
Verify: The path to production 
Moving from pilot to production requires more than proof of concept‚Äîit demands scalable solutions that integrate with existing systems and deliver consistent value. While demos can seem compelling, verification reveals the true complexity of enterprise-wide deployment. This critical stage maps the journey from prototype to production, establishing a foundation for sustainable success. 
Building production-ready AI solutions brings together several key elements. Robust governance structures must facilitate responsible AI deployment and oversight, managing risk and compliance in an evolving regulatory landscape. Change management prepares teams and processes for new ways of working, driving organization-wide adoption. Operational readiness assessments evaluate existing workflows, integration points, and team capabilities to facilitate smooth implementation. 
Architectural decisions in the verification phase balance scale, reliability, and operability, with security and compliance woven into the solution‚Äôs fabric. This often involves practical trade-offs based on real-world constraints. A simpler solution aligned to existing team capabilities may prove more valuable than a complex one requiring specialized expertise. Similarly, meeting strict latency requirements might necessitate choosing a streamlined model over a more sophisticated one, as model selection requires a balance of performance, accuracy, and computational costs based on the use case. 
Generative AI Innovation Center Principal Data Scientist, Isaac Privitera, captures this philosophy: ‚ÄúWhen building a generative AI solution, we focus primarily on three things: measurable business impact, production readiness from day one, and sustained operational excellence. This trinity drives solutions that thrive in real-world conditions.‚Äù 
Effective verification demands both technical expertise and practical wisdom from real-world deployments. It requires proving not just that a solution works in principle, but that it can operate at scale within existing systems and team capabilities. By systematically addressing these factors, we help make sure deployments deliver sustainable, long-term value. 
Venture: Securing long-term success 
Long-term success in AI also requires mindful resource planning across people, processes, and funding. The Venture phase maps the full journey from implementation through sustained organizational adoption. 
Financial viability starts with understanding the total cost of ownership, from initial development through deployment, integration, training, and ongoing operations. Promising projects can stall mid-implementation due to insufficient resource planning. Success requires strategic budget allocation across all phases, with clear ROI milestones and the flexibility to scale. 
Successful ventures demand organizational commitment through executive sponsorship, stakeholder alignment, and dedicated teams for ongoing optimization and maintenance. Organizations must also account for both direct and indirect costs‚Äîfrom infrastructure and development, to team training, process adaptation, and change management. A blend of sound financial planning and flexible resource strategies allows teams to accelerate and adjust as opportunities and challenges arise. 
From there, the solution must integrate seamlessly into daily operations with clear ownership and widespread adoption. This transforms AI from a project into a core organizational capability. 
Adopting the Five V‚Äôs Framework in your enterprise 
The Five V‚Äôs Framework shifts AI focus from technical capabilities to business results, replacing ‚ÄòWhat can AI do?‚Äô with ‚ÄòWhat do we need AI to do?‚Äô. Successful implementation requires both an innovative culture and access to specialized expertise. 
 
AWS resources to support your journey 
AWS offers a variety of resources to help you scale your AI to production. 
Expert guidance 
The AWS Partnership Network (APN) offers multiple pathways to access specialized expertise, while AWS Professional Services brings proven methodologies from its own successful AI implementations. Certified partners, including Generative AI Partner Innovation Alliance members who receive direct enablement training from the Generative AI Innovation Center team, extend this expertise across industries. AWS Generative AI Competency Partners bring use case-specific success, while specialized partners focus on model customization and evaluation. 
Self-service learning 
For teams building internal capabilities, AWS provides technical blogs with implementation guides based on real-world experience, GitHub repositories with production-ready code, and AWS Workshop Studio for hands-on learning that bridges theory and practice. 
Balancing learning and innovation 
Even with the right framework and resources, not every AI project will reach production. These initiatives still provide valuable lessons that strengthen your overall program. Organizations can build lasting AI capabilities through three key principles: 
 
 Embracing a portfolio approach: Treat AI initiatives as an investment portfolio where diversification drives risk management and value creation. Balance quick wins (delivering value within months), strategic initiatives (driving longer-term transformation), and moonshot projects (potentially revolutionizing your business). 
 Creating a culture of safe experimentation: Organizations thrive with AI when teams can innovate boldly. In rapidly evolving fields, the cost of inaction often exceeds the risk of calculated experiments. 
 Learning from ‚Äúproductive failures‚Äù: Capture insights systematically across projects. Technical challenges reveal capability gaps, data issues expose information needs, and organizational readiness concerns illuminate broader transformation requirements ‚Äì all shaping future initiatives. 
 
The path forward 
The next 12-18 months present a pivotal opportunity for organizations to harness generative AI and agentic AI to solve previously intractable problems, establish competitive advantages, and explore entirely new frontiers of business possibility. Those who successfully move from pilot to production will help define what‚Äôs possible within their industries and beyond. 
Are you ready to move your AI initiatives into production? 
 
 Learn more about the AWS Generative AI Innovation Center and contact your AWS Account Manager to be connected to our expert guidance and support. 
 Join our AWS Builder community to connect with others on a similar AI journey. 
 
 
 
About the authors 
Sri Elaprolu serves as Director of the AWS Generative AI Innovation Center, where he leverages nearly three decades of technology leadership experience to drive artificial intelligence and machine learning innovation. In this role, he leads a global team of machine learning scientists and engineers who develop and deploy advanced generative and agentic AI solutions for enterprise and government organizations facing complex business challenges. Throughout his nearly 13-year tenure at AWS, Sri has held progressively senior positions, including leadership of ML science teams that partnered with high-profile organizations such as the NFL, Cerner, and NASA. These collaborations enabled AWS customers to harness AI and ML technologies for transformative business and operational outcomes. Prior to joining AWS, he spent 14 years at Northrop Grumman, where he successfully managed product development and software engineering teams. Sri holds a Master‚Äôs degree in Engineering Science and an MBA with a concentration in general management, providing him with both the technical depth and business acumen essential for his current leadership role. 
 Dr. Diego Socolinsky is currently the North America Head of the Generative AI Innovation Center at Amazon Web Services (AWS). With over 25 years of experience at the intersection of technology, machine learning, and computer vision, he has built a career driving innovation from cutting-edge research to production-ready solutions. Dr. Socolinsky holds a Ph.D. in Mathematics from The Johns Hopkins University and has been a pioneer in various fields including thermal imaging biometrics, augmented/mixed reality, and generative AI initiatives. His technical expertise spans from optimizing low-level embedded systems to architecting complex real-time deep learning solutions, with particular focus on generative AI platforms, large-scale unstructured data classification, and advanced computer vision applications. He is known for his ability to bridge the gap between technical innovation and strategic business objectives, consistently delivering transformative technology that solves complex real-world problems. 
Sabine Khan is a Strategic Initiatives Leader with the AWS Generative AI Innovation Center, where she implements delivery and strategy initiatives focused on scaling enterprise-grade Generative AI solutions. She specializes in production-ready AI systems and drives agentic AI projects from concept to deployment. With over twenty years of experience in software delivery and a strong focus on AI/ML during her tenure at AWS, she has established a track record of successful enterprise implementations. Prior to AWS, she led digital transformation initiatives and held product development and software engineering leadership roles in Houston‚Äôs energy sector. Sabine holds a Master‚Äôs degree in GeoScience and an MBA. 
Andrea Jimenez is a dual master‚Äôs candidate at the Massachusetts Institute of Technology, pursuing an M.S. in Computer Science from the School of Engineering and an MBA from the Sloan School of Management. As a GenAI Lead Graduate Fellow at the MIT GenAI Innovation Center, she researches agentic AI systems and the economic implications of generative AI technologies, while leveraging her background in artificial intelligence, product development, and startup innovation to lead teams at the intersection of technology and business strategy. Her work focuses on advancing human-AI collaboration and translating cutting-edge research into scalable, high-impact solutions. Prior to AWS and MIT, she led product and engineering teams in the tech industry and founded and sold a startup that helped early-stage companies build and launch SaaS products. 
Randi Larson connects AI innovation with executive strategy for the AWS Generative AI Innovation Center, shaping how organizations understand and translate technical breakthroughs into business value. She combines strategic storytelling with data-driven insight through global keynotes, Amazon‚Äôs first tech-for-good podcast, and conversations with industry and Amazon leaders on AI transformation. Before Amazon, Randi refined her analytical precision as a Bloomberg journalist and advisor to economic institutions, think tanks, and family offices on technology initiatives. Randi holds an MBA from Duke University‚Äôs Fuqua School of Business and a B.S. in Journalism and Spanish from Boston University.
‚Ä¢ Generate Gremlin queries using Amazon Bedrock models
  Graph databases have revolutionized how organizations manage complex, interconnected data. However, specialized query languages such as Gremlin often create a barrier for teams looking to extract insights efficiently. Unlike traditional relational databases with well-defined schemas, graph databases lack a centralized schema, requiring deep technical expertise for effective querying. 
To address this challenge, we explore an approach that converts natural language to Gremlin queries, using Amazon Bedrock models such as Amazon Nova Pro. This approach helps business analysts, data scientists, and other non-technical users access and interact with graph databases seamlessly. 
In this post, we outline our methodology for generating Gremlin queries from natural language, comparing different techniques and demonstrating how to evaluate the effectiveness of these generated queries using large language models (LLMs) as judges. 
Solution overview 
Transforming natural language queries into Gremlin queries requires a deep understanding of graph structures and the domain-specific knowledge encapsulated within the graph database. To achieve this, we divided our approach into three key steps: 
 
 Understanding and extracting graph knowledge 
 Structuring the graph similar to text-to-SQL processing 
 Generating and executing Gremlin queries 
 
The following diagram illustrates this workflow. 
 
Step 1: Extract graph knowledge 
A successful query generation framework must integrate both graph knowledge and domain knowledge to accurately translate natural language queries. Graph knowledge encompasses structural and semantic information extracted directly from the graph database. Specifically, it includes: 
 
 Vertex labels and properties ‚Äì A listing of vertex types, names, and their associated attributes 
 Edge labels and properties ‚Äì Information about edge types and their attributes 
 One-hop neighbors for each vertex ‚Äì Capturing local connectivity information, such as direct relationships between vertices 
 
With this graph-specific knowledge, the framework can effectively reason about the heterogeneous properties and complex connections inherent to graph databases. 
Domain knowledge captures additional context that augments the graph knowledge and is tailored specifically to the application domain. It is sourced in two ways: 
 
 Customer-provided domain knowledge ‚Äì For example, the customer kscope.ai helped specify those vertices that represent metadata and should never be queried. Such constraints are encoded to guide the query generation process. 
 LLM-generated descriptions ‚Äì To enhance the system‚Äôs understanding of vertex labels and their relevance to specific questions, we use an LLM to generate detailed semantic descriptions of vertex names, properties, and edges. These descriptions are stored within the domain knowledge repository and provide additional context to improve the relevance of the generated queries. 
 
Step 2: Structure the graph as a text-to-SQL schema 
To improve the model‚Äôs comprehension of graph structures, we adopt an approach similar to text-to-SQL processing, where we construct a schema representing vertex types, edges, and properties. This structured representation enhances the model‚Äôs ability to interpret and generate meaningful queries. 
The question processing component transforms natural language input into structured elements for query generation. It operates in three stages: 
 
 Entity recognition and classification ‚Äì Identifies key database elements in the input question (such as vertices, edges, and properties) and categorizes the question based on its intent 
 Context enhancement ‚Äì Enriches the question with relevant information from the knowledge component, so both graph-specific and domain-specific context is properly captured 
 Query planning ‚Äì Maps the enhanced question to specific database elements needed for query execution 
 
The context generation component makes sure the generated queries accurately reflect the underlying graph structure by assembling the following: 
 
 Element properties ‚Äì Retrieves attributes of vertices and edges along with their data types 
 Graph structure ‚Äì Facilitates alignment with the database‚Äôs topology 
 Domain rules ‚Äì Applies business constraints and logic 
 
Step 3: Generate and execute Gremlin queries 
The final step is query generation, where the LLM constructs a Gremlin query based on the extracted context. The process follows these steps: 
 
 The LLM generates an initial Gremlin query. 
 The query is executed within a Gremlin engine. 
 If the execution is successful, results are returned. 
 If execution fails, an error message parsing mechanism analyzes the returned errors and refines the query using LLM-based feedback. 
 
This iterative refinement makes sure the generated queries align with the database‚Äôs structure and constraints, improving overall accuracy and usability. 
Prompt template 
Our final prompt template is as follows: 
 
 ## Request
Please write a gremlin query to answer the given question:
{{question}}
You will be provided with couple relevant vertices, together with their 
schema and other information.
Please choose the most relevant vertex according to its schema and other 
information to make the gremlin query correct.


## Instructions
1. Here are related vertices and their details:
{{schema}}
2. Don't rename properties.
3. Don't change lines (using slash n) in the generated query.


## IMPORTANT
Return the results in the following XML format:

&lt;Results&gt;
    &lt;Query&gt;INSERT YOUR QUERY HERE&lt;/Query&gt;
    &lt;Explanation&gt;
        PROVIDE YOUR EXPLANATION ON HOW THIS QUERY WAS GENERATED 
        AND HOW THE PROVIDED SCHEMA WAS LEVERAGED
    &lt;/Explanation&gt;
&lt;/Results&gt; 
 
Comparing LLM-generated queries to ground truth 
We implemented an LLM-based evaluation system using Anthropic‚Äôs Claude 3.5 Sonnet on Amazon Bedrock as a judge to assess both query generation and execution results for Amazon Nova Pro and a benchmark model. The system operates in two key areas: 
 
 Query evaluation ‚Äì Assesses correctness, efficiency, and similarity to ground-truth queries; calculates exact matching component percentages; and provides an overall rating based on predefined rules developed with domain experts 
 Execution evaluation ‚Äì Initially used a single-stage approach to compare generated results with ground truth, then enhanced to a two-stage evaluation process: 
   
   Item-by-item verification against ground truth 
   Calculation of overall match percentage 
    
 
Testing across 120 questions demonstrated the framework‚Äôs ability to effectively distinguish correct from incorrect queries. The two-stage approach particularly improved the reliability of execution result evaluation by conducting thorough comparison before scoring. 
Experiments and results 
In this section, we discuss the experiments we conducted and their results. 
Query similarity 
In the query evaluation case, we propose two metrics: query exact match and query overall rating. An exact match score is calculated by identifying matching vs. non-matching components between generated and ground truth queries. The following table summarizes the scores for query exact match. 
 
  
   
    
   Easy 
   Medium 
   Hard 
   Overall 
   
   
   Amazon Nova Pro 
   82.70% 
   61% 
   46.60% 
   70.36% 
   
   
   Benchmark Model  
   92.60% 
   68.70% 
   56.20% 
   78.93% 
   
  
 
An overall rating is provided after considering factors including query correctness, efficiency, and completeness as instructed in the prompt. The overall rating is on scale 1‚Äì10. The following table summarizes the scores for query overall rating. 
 
  
   
    
   Easy 
   Medium 
   Hard 
   Overall 
   
   
   Amazon Nova Pro 
   8.7 
   7 
   5.3 
   7.6 
   
   
   Benchmark Model 
   9.7 
   8 
   6.1 
   8.5 
   
  
 
One limitation in the current query evaluation setup is that we rely solely on the LLM‚Äôs ability to compare ground truth against LLM-generated queries and arrive at the final scores. As a result, the LLM can fail to align with human preferences and under- or over-penalize the generated query. To address this, we recommend working with a subject matter expert to include domain-specific rules in the evaluation prompt. 
Execution accuracy 
To calculate accuracy, we compare the results of the LLM-generated Gremlin queries against the results of ground truth queries. If the results from both queries match exactly, we count the instance as correct; otherwise, it is considered incorrect. Accuracy is then computed as the ratio of correct query executions to the total number of queries tested. This metric provides a straightforward evaluation of how well the model-generated queries retrieve the expected information from the graph database, facilitating alignment with the intended query logic. 
The following table summarizes the scores for execution results count match. 
 
  
   
    
   Easy 
   Medium 
   Hard 
   Overall 
   
   
   Amazon Nova Pro 
   80% 
   50% 
   10% 
   60.42% 
   
   
   Benchmark Model 
   90% 
   70% 
   30% 
   74.83% 
   
  
 
Query execution latency 
In addition to accuracy, we evaluate the efficiency of generated queries by measuring their runtime and comparing it with the ground truth queries. For each query, we record the runtime in milliseconds and analyze the difference between the generated query and the corresponding ground truth query. A lower runtime indicates a more optimized query, whereas significant deviations might suggest inefficiencies in query structure or execution planning. By considering both accuracy and runtime, we gain a more comprehensive assessment of query quality, making sure the generated queries are correct and performant within the graph database. The following box plot showcases query execution latency with respect to time for the ground truth query and the query generated by Amazon Nova Pro. As illustrated, all three types of queries exhibit comparable runtimes, with similar median latencies and overlapping interquartile ranges. Although the ground truth queries display a slightly wider range and a higher outlier, the median values across all three groups remain close. This suggests that the model-generated queries are at the same level as human-written ones in terms of execution efficiency, supporting the claim that AI-generated queries are of similar quality and don‚Äôt incur additional latency overhead. 
 
Query generation latency and cost 
Finally, we compare the time taken to generate each query and calculate the cost based on token consumption. More specifically, we measure the query generation time and track the number of tokens used, because most LLM-based APIs charge based on token usage. By analyzing both the generation speed and token cost, we can determine whether the model is efficient and cost-effective. These results provide insights in selecting the optimal model that balances query accuracy, execution efficiency, and economic feasibility. 
As shown in the following plots, Amazon Nova Pro consistently outperforms the benchmark model in both generation latency and cost. In the left plot, which depicts query generation latency, Amazon Nova Pro demonstrates a significantly lower median generation time, with most values clustered between 1.8‚Äì4 seconds, compared to the benchmark model‚Äôs broader range from around 5‚Äì11 seconds. The right plot, illustrating query generation cost, shows that Amazon Nova Pro maintains a much smaller cost per query‚Äîcentered well below $0.005‚Äîwhereas the benchmark model incurs higher and more variable costs, reaching up to $0.025 in some cases. These results highlight Amazon Nova Pro‚Äôs advantage in terms of both speed and affordability, making it a strong candidate for deployment in time-sensitive or large-scale systems. 
 
Conclusion 
We experimented with all 120 ground truth queries provided to us by kscope.ai and achieved an overall accuracy of 74.17% in generating correct results. The proposed framework demonstrates its potential by effectively addressing the unique challenges of graph query generation, including handling heterogeneous vertex and edge properties, reasoning over complex graph structures, and incorporating domain knowledge. Key components of the framework, such as the integration of graph and domain knowledge, the use of Retrieval Augmented Generation (RAG) for query plan creation, and the iterative error-handling mechanism for query refinement, have been instrumental in achieving this performance. 
In addition to improving accuracy, we are actively working on several enhancements. These include refining the evaluation methodology to handle deeply nested query results more effectively and further optimizing the use of LLMs for query generation. Moreover, we are using the RAGAS-faithfulness metric to improve the automated evaluation of query results, resulting in greater reliability and consistency in assessing the framework‚Äôs outputs. 
 
About the authors 
Mengdie (Flora) Wang is a Data Scientist at AWS Generative AI Innovation Center, where she works with customers to architect and implement scalable Generative AI solutions that address their unique business challenges. She specializes in model customization techniques and agent-based AI systems, helping organizations harness the full potential of generative AI technology. Prior to AWS, Flora earned her Master‚Äôs degree in Computer Science from the University of Minnesota, where she developed her expertise in machine learning and artificial intelligence. 
Jason Zhang&nbsp;has expertise in machine learning, reinforcement learning, and generative AI. He earned his Ph.D. in Mechanical Engineering in 2014, where his research focused on applying reinforcement learning to real-time optimal control problems. He began his career at Tesla, applying machine learning to vehicle diagnostics, then advanced NLP research at Apple and Amazon Alexa. At AWS, he worked as a Senior Data Scientist on generative AI solutions for customers. 
Rachel Hanspal is a Deep Learning Architect at AWS Generative AI Innovation Center, specializing in end-to-end GenAI solutions with a focus on frontend architecture and LLM integration. She excels in translating complex business requirements into innovative applications, leveraging expertise in natural language processing, automated visualization, and secure cloud architectures. 
Zubair Nabi is the CTO and Co-Founder of Kscope, an Integrated Security Posture Management (ISPM) platform. His expertise lies at the intersection of Big Data, Machine Learning, and Distributed Systems, with over a decade of experience building software, data, and AI platforms. Zubair is also an adjunct faculty member at George Washington University and the author of Pro Spark Streaming: The Zen of Real-Time Analytics Using Apache Spark. He holds an MPhil from the University of Cambridge. 
Suparna Pal  ‚Äì CEO &amp; Co-Founder of kscope.ai ‚Äì 20+ years of journey of building innovative platforms &amp; solutions for Industrial, Health Care and IT operations at PTC, GE, and Cisco. 
Wan Chen is an Applied Science Manager at AWS Generative AI Innovation Center. As a ML/AI veteran in tech industry, she has wide range of expertise on traditional machine learning, recommender system, deep learning and Generative AI. She is a stronger believer of Superintelligence and is very passionate to push the boundary of AI research and application to enhance human life and drive business growth. She holds Ph.D in Applied Mathematics from University of British Columbia and had worked as postdoctoral fellow in Oxford University. 
Mu Li is a Principal Solutions Architect with AWS Energy. He‚Äôs also the Worldwide Tech Leader for the AWS Energy &amp; Utilities Technical Field Community (TFC), a community of 300+ industry and technical experts. Li is passionate about working with customers to achieve business outcomes using technology. Li has worked with customers to migrate all-in to AWS from on-prem and Azure, launch the Production Monitoring and Surveillance industry solution, deploy ION/OpenLink Endur on AWS, and implement AWS-based IoT and machine learning workloads. Outside of work, Li enjoys spending time with his family, investing, following Houston sports teams, and catching up on business and technology.
‚Ä¢ Incorporating responsible AI into generative AI project prioritization
  Over the past two years, companies have seen an increasing need to develop a project prioritization methodology for generative AI. There is no shortage of generative AI use cases to consider. Rather, companies want to evaluate the business value against the cost, level of effort, and other concerns, for a large number of potential generative AI projects. One new concern for generative AI compared to other domains is considering issues like hallucination, generative AI agents making incorrect decisions and then acting on those decisions through tool calls to downstream systems, and dealing with the rapidly changing regulatory landscape. In this post we describe how to incorporate responsible AI practices into a prioritization method to systematically address these types of concerns. 
Responsible AI overview 
The AWS Well-Architected Framework defines responsible AI as ‚Äúthe practice of designing, developing, and using AI technology with the goal of maximizing benefits and minimizing risks.‚Äù The AWS responsible AI framework begins by defining eight dimensions of responsible AI: fairness, explainability, privacy and security, safety, controllability, veracity and robustness, governance, and transparency. At key points in the development lifecycle, a generative AI team should consider the possible harms or risks for each dimension (inherent and residual risks), implements risk mitigations, and monitors risk on an ongoing basis. Responsible AI applies across the entire development lifecycle and should be considered during initial project prioritization. That‚Äôs especially true for generative AI projects, where there are novel types of risks to consider, and mitigations might not be as well understood or researched. Considering responsible AI up front gives a more accurate picture of project risk and mitigation level of effort and reduces the chance of costly rework if risks are uncovered later in the development lifecycle. In addition to potentially delayed projects due to rework, unmitigated concerns might also harm customer trust, result in representational harm, or fail to meet regulatory requirements. 
Generative AI prioritization 
While most companies have their own prioritization methods, here we‚Äôll demonstrate how to use the weighted shortest job first (WSJF) method from the Scaled Agile system. WSJF assigns a priority using this formula: 
Priority = (cost of delay) / (job size) 
The cost of delay is a measure of business value. It includes the direct value (for example, additional revenue or cost savings), the timeliness (such as, is shipping this project worth a lot more today than a year from now), and the adjacent opportunities (such as, would delivering this project open up other opportunities down the road). 
The job size is where you consider the level of effort to deliver the project. That normally includes direct development costs and paying for any infrastructure or software you need. The job size is where you can include the results of the initial responsible AI risk assessment and expected mitigations. For example, if the initial assessment uncovers three risks that require mitigation, you include the development cost for those mitigations in the job size. You can also qualitatively assess that a project with ten high-priority risks is more complex than a project with only two high-priority risks. 
Example scenario 
Now, let‚Äôs walk through a prioritization exercise that compares two generative AI projects. The first project uses a large language model (LLM) to generate product descriptions. A marketing team will use this application to automatically create production descriptions that go into the online product catalog website. The second project uses a text-to-image model to generate new visuals for advertising campaigns and the product catalog. The marketing team will use this application to more quickly create customized brand assets. 
First pass prioritization 
First, we‚Äôll go through the prioritization method without considering responsible AI, assigning a score of 1‚Äì5 for each part of the WSJF formula. The specific scores vary by organization. Some companies prefer to use t-shirt sizing (S, M, L, and XL), others prefer a score of 1‚Äì5, and others will use a more granular score. A score of 1‚Äì5 is a common and straightforward way to start. For example, the direct value scores can be calculated as: 
1 = no direct value 
2 = 20% improvement in KPI (time to create high-quality descriptions) 
3 = 40% improvement in KPI 
4 = 80% improvement in KPI 
5 = 100% or more improvement in KPI 
 
  
   
    
   Project 1: Automated product descriptions (scored from 1‚Äì5) 
   Project 2: Creating visual brand assets (scored from 1‚Äì5) 
   
   
   Direct value 
   3: Helps marketing team create higher quality descriptions more quickly 
   3: Helps marketing team create higher quality assets more quickly 
   
   
   Timeliness 
   2: Not particularly urgent 
   4: New ad campaign planned this quarter; without this project, cannot create enough brand assets without hiring a new agency to supplement the team 
   
   
   Adjacent opportunities 
   2: Might be able to reuse for similar scenarios) 
   3: Experience gained in image generation will build competence for future projects 
   
   
   Job size 
   2: Basic, well-known pattern 
   2: Basic, well-known pattern 
   
   
   Score 
   (3+2+2)/2 = 3.5 
   (3+4+3)/2 = 5 
   
  
 
At first glance, it looks like Project 2 is more compelling. Intuitively that makes sense‚Äîit takes people a lot longer to make high-quality visuals than to create textual product descriptions. 
Risk assessment 
Now let‚Äôs go through a risk assessment for each project. The following table lists a brief overview of the outcome of a risk assessment along each of the AWS responsible AI dimensions, along with a t-shirt size (S, M, L, and XL) severity level. The table also includes suggested mitigations. 
 
  
   
    
   Project 1: Automated product descriptions 
   Project 2: Creating visual brand assets 
   
   
   Fairness 
   L: Are descriptions appropriate in terms of gender and demographics? Mitigate using guardrails. 
   L: Images must not portray particular demographics in a biased way. Mitigate using human and automated checks. 
   
   
   Explainability 
   No risks identified. 
   No risks identified. 
   
   
   Privacy and security 
   L: Some product information is proprietary and cannot be listed on a public site. Mitigate using data governance. 
   L: Model must not be trained on any images that contain proprietary information. Mitigate using data governance. 
   
   
   Safety 
   M: Language must be age-appropriate and not cover offensive topics. Mitigate using guardrails. 
   L: Images must not contain adult content or images of drugs, alcohol, or weapons. Mitigate using guardrails. 
   
   
   Controllability 
   S: Need to track customer feedback on the descriptions. Mitigate using customer feedback collection. 
   L: Do images align to our brand guidelines? Mitigate using human and automated checks. 
   
   
   Veracity and robustness 
   M: Will the system hallucinate and imply product capabilities that aren‚Äôt real? Mitigate using guardrails. 
   L: Are images realistic enough to avoid uncanny valley effects? Mitigate using human and automated checks. 
   
   
   Governance 
   M: Prefer LLM providers that offer copyright indemnification. Mitigate using LLM provider selection. 
   L: Require copyright indemnification and image source attribution. Mitigate using model provider selection. 
   
   
   Transparency 
   S: Disclose that descriptions are AI generated. 
   S: Disclose that descriptions are AI generated. 
   
  
 
The risks and mitigations are use-case specific. The preceding table is for illustrative purposes only. 
Second pass prioritization 
How does the risk assessment affect the prioritization? 
 
  
   
    
   Project 1: Automated product descriptions (scored from 1‚Äì5) 
   Project 2: Creating visual brand assets (scored from 1‚Äì5) 
   
   
   Job size 
   3: Basic, well-known pattern; requires fairly standard guardrails, governance, and feedback collection. 
   5: Basic, well-known pattern. Requires advanced image guardrails with human oversight, and a more expensive commercial model. Research spike needed. 
   
   
   Score 
   (3+2+2)/3 = 2.3 
   (3+4+3)/5 = 2 
   
  
 
Now it looks like Project 1 is a better one to start with. Intuitively, after you consider responsible AI, that makes sense. Poorly crafted or offensive images are more noticeable and have a larger impact than a poorly phrased product description. And the guardrails you can use for maintaining image safety are less mature than the equivalent guardrails for text, particularly in ambiguous cases like adhering to brand guidelines. In fact, an image guardrail system might require training a monitoring model or using people to spot-check some percentage of the output. You might need to dedicate a small science team to study this problem first. 
Conclusion 
In this post, you saw how to include responsible AI considerations in a generative AI project prioritization method. You saw how conducting a responsible AI risk assessment in the initial prioritization phase can change the outcome by uncovering a substantial amount of mitigation work. Moving forward, you should develop your own responsible AI policy and start adopting responsible AI practices for generative AI projects. You can find additional details and resources at Transform responsible AI from theory into practice. 
 
About the author 
Randy DeFauw is a Sr. Principal Solutions Architect at AWS. He has over 20 years of experience in technology, starting with his university work on autonomous vehicles. He has worked with and for customers ranging from startups to Fortune 50 companies, launching Big Data and Machine Learning applications. He holds an MSEE and an MBA, serves as a board advisor to K-12 STEM education initiatives, and has spoken at leading conferences including Strata and GlueCon. He is the co-author of the books SageMaker Best Practices and Generative AI Cloud Solutions. Randy currently acts as a technical advisor to AWS‚Äô director of technology in North America.

‚∏ª