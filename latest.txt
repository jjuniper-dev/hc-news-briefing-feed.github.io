‚úÖ Morning News Briefing ‚Äì July 03, 2025

üìÖ Date: 2025-07-03
üè∑Ô∏è Tags: #briefing #ai #publichealth #digitalgov

‚∏ª

üßæ Weather
‚Ä¢ Current Conditions: Mostly Cloudy, 21.5¬∞C
  Temperature: 21.5&deg;C Pressure / Tendency: 101.1 kPa
‚Ä¢ Thursday: Chance of showers. High 24. POP 30%
  30 percent chance of showers this afternoon with risk of thunderstorm . Wind northwest 20 km/
‚Ä¢ Thursday night: Partly cloudy. Low 10.
  Wind northwest 20 km/h gusting to 40 becoming light this evening . Clearing
‚Ä¢ Friday: Clearing. High 26.
  A mix of sun and cloud is expected to cover the morning . Clearing near noon
‚Ä¢ Friday night: Cloudy periods. Low 13.
  Forecast issued 11:00 AM EDT Thursday 3 July 2025 . Cloudy periods.
‚Ä¢ Saturday: Chance of showers. High 31. POP 40%
  A mix of sun and cloud with 40 percent chance of showers . High 31. A
‚Ä¢ Saturday night: Chance of showers. Low 20. POP 40%
  Cloudy periods with 40 percent chance of showers . Low 20 . Cloudy period with
‚Ä¢ Sunday: Chance of showers. High 29. POP 60%
  Cloudy with 60 percent chance of showers . High 29. High 29 . Cloudy
‚Ä¢ Sunday night: Chance of showers. Low 16. POP 60%
  Cloudy with 60 percent chance of showers. Low 16. Chance of showers . Cloud
‚Ä¢ Monday: Chance of showers. High 24. POP 40%
  Cloudy with 40 percent chance of showers . High 24. High 24 . Cloudy
‚Ä¢ Monday night: Cloudy periods. Low 13.
  Forecast issued 11:00 AM EDT Thursday 3 July 2025 . Cloudy periods.
‚Ä¢ Tuesday: A mix of sun and cloud. High 26.
  A mix of sun and cloud is forecast for July 3, 2025 . High 26.
‚Ä¢ Tuesday night: Cloudy periods. Low 13.
  Forecast issued 11:00 AM EDT Thursday 3 July 2025 . Cloudy periods.
‚Ä¢ Wednesday: Chance of showers. High 26. POP 30%
  A mix of sun and cloud with 30 percent chance of showers . High 26. A

üåç International & Canadian News
No updates.

üá∫üá∏ U.S. Top Stories
‚Ä¢ The U.S. labor market remains solid, with employers adding 147,000 jobs last month
  U.S. employers added 147,000 jobs in June as the unemployment rate dipped
‚Ä¢ GOP megabill expected to pass by July 4. And, what's next after Sean Combs' verdict
  House Republicans are quickly working to meet their self-imposed deadline to pass Trump's meg
‚Ä¢ Bryan Kohberger pleads guilty in murders of 4 University of Idaho students
  Bryan Kohberger pleaded guilty Wednesday in the stabbing murders of four University of Idaho students .
‚Ä¢ Wisconsin Supreme Court rules 1849 law does not ban abortion in the state
  Wisconsin State Supreme Court ruled that an 1849 law does not amount to an abortion ban
‚Ä¢ House Republicans expected to pass Trump's massive tax and policy bill by July 4
  House Republicans cleared a final procedural hurdle early Thursday and are now one vote away from passing
‚Ä¢ Democratic Rep. Chris Deluzio discusses his opposition to GOP megabill in the House
  Rep. Chris Deluzio, D-Penn., talks about the budget and tax
‚Ä¢ GOP governors stay silent amid plans to slash Medicaid spending in their states
  In 2017, when President Trump tried to repeal Obamacare and roll back Medicaid coverage, Republican
‚Ä¢ Are you a glucose 'dipper'? Here's how to fix those blood sugar highs and lows
  After a meal, some people experience high spikes in blood sugar followed by crashing lows .
‚Ä¢ What happens to Sean Combs now?
  The hip-hop mogul's legal saga has reached an uneasy outcome . Despite a tainted
‚Ä¢ To kick off celebrations of America's founding, Trump travels to its heartland
  President Trump will give a speech in Iowa Thursday night as the official start to a year

üß† Artificial Intelligence & Digital Strategy
‚Ä¢ Young Consulting finds even more folks affected in breach mess ‚Äì now over 1 million
  Young Consulting's cybersecurity woes continue after the number of affected individuals from last year's suspected
‚Ä¢ Tariffs and trade turmoil driving up cost and build times for datacenters
  Datacenter operators in Northern Europe say US tariffs and growing geopolitical instability are inflating
‚Ä¢ Gone in 40 days: US drops ban on export of chip design tools to China
  The US has lifted the requirement to secure a license before exporting Electronic Design Automation tools
‚Ä¢ Meta calls ‚Ç¨200M EU fine over pay-or-consent ad model 'unlawful'
  Meta has come out swinging following the European Commission's decision that its pay-or-
‚Ä¢ Ransomware crew Hunters International shuts down, hands out keys to victims
  Ransomware gang Hunters International has shut up shop and offered decryption keys to all
‚Ä¢ Canonical adds extra shots to Ubuntu Java
  Canonical has some extra toppings, flavorings, and offers coming for its bigger
‚Ä¢ UK charity bank branded a 'disaster' after platform migration goes wrong
  Customers booted from system and technical integration missing from upgrade . Customers unable to log in or
‚Ä¢ Wayback gives X11 desktops a fighting chance in a Wayland world
  A new project addresses one of the biggest differences between X11 and Wayland work .
‚Ä¢ Let's Encrypt rolls out free security certs for IP addresses
  Let's Encrypt, a certificate authority (CA) known for its free certificates,
‚Ä¢ ChatGPT creates phisher‚Äôs paradise by recommending the wrong URLs for major companies
  AI-powered chatbots often deliver incorrect information when asked to name the address for major
‚Ä¢ 'Quad' nations launch plan to stop China making critical minerals into Unobtanium
  India, Japan, USA and Australia see risks and opportunities in rare earths . India
‚Ä¢ Alibaba Cloud reveals DB cluster manager it says can beat rival hyperscalers
  ‚ÄòEigen+‚Äô finds instances likely to cause out of memory errors and
‚Ä¢ Cisco scores a perfect 10 - sadly for a critical flaw in its comms platform
  The second max score this week for Netzilla - not a good look . Cisco's urgent patch
‚Ä¢ CISA warns the Signal clone used by natsec staffers is being attacked, so patch now
  US security watchdog CISA has warned that malicious actors are actively exploiting two flaws in the Signal clone TeleMessage TM S
‚Ä¢ Call center staffers explain to researchers how their AI assistants aren't very helpful
  Customer service reps at a Chinese utility's call center often struggled when trying to use an
‚Ä¢ HPE finally closes Juniper deal, but offers no details on what happens next
  HPE has completed its takeover of Juniper Networks . The company isn't yet ready
‚Ä¢ MethaneSAT 'likely not recoverable' after losing contact with Earth
  Bezos-backed MethaneSAT satellite has been declared 'likely not recoverable'
‚Ä¢ 23andMe's new owner says your DNA is safe this time
  TTAM says it plans to complete the deal on July 8 . The medical research nonprofit
‚Ä¢ Huawei can't wriggle out of Iran sanctions trial, judge rules
  Huawei will still have to face trial in the US next year over alleged breaches of sanctions
‚Ä¢ Microsoft kicks off new fiscal year with more layoffs
  Reports of 9,000 staff cut Microsoft's recent trend of wide-scale workforce reduction
‚Ä¢ 'Elevated' moisture reading ignored before Heathrow-closing conflagration, says NESO
  Heathrow power outages caused by power failure in March . Datacenters kept humming along,
‚Ä¢ Trump's One Big Beautiful Bill bankrolls $85M Space Shuttle shuffle
  $85 million allocation for shifting a 'space vehicle' to a new location . It is widely interpreted as a move of the retired Space Shuttle Discovery orbiter
‚Ä¢ Impact of Microsoft taking over Enterprise Account renewals starts to 'bite'
  Cutting out middle man dents coffers of larger service providers, helps Redmond offset AI investments
‚Ä¢ US imposes sanctions on second Russian bulletproof hosting vehicle this year
  Aeza Group accused of assisting data bandits and BianLian ransomware crooks . US
‚Ä¢ Microsoft's on-prem Exchange and Skype for Business Server go subscription-only
  Microsoft has made Subscription Editions of Exchange Server and Skype for Business Server generally available
‚Ä¢ Coming to PostgreSQL: On-disk database encryption
  Open source initiative aims to offer enterprise security feature without vendor lock-in . Transparent Data Encryption (TDE) is
‚Ä¢ Cl0p cybercrime gang's data exfiltration tool found vulnerable to RCE attacks
  Experts say they don't expect MOVEit menace to do much about it . Experts
‚Ä¢ UK eyes new laws as cable sabotage blurs line between war and peace
  It might be time to update the Submarine Telegraph Act of 1885 . Cyberattacks
‚Ä¢ NASA tests shrinking metals to help it find more exoplanets
  Mysterious ‚ÄòAlloy 30‚Äô gets smaller when heated, which could help stabilize super-sensitive space telescopes
‚Ä¢ Amazon's latest Graviton 4 EC2 instances pack dual 300Gbps NICs
  Amazon Web Services has created a new Graviton 4-powered instance for network-intensive
‚Ä¢ Arista acquires VMware‚Äôs VeloCloud SD-WAN outfit from Broadcom
  Broadcom has sold VeloCloud, the software-defined WAN business, to
‚Ä¢ Australian airline Qantas reveals data theft impacting six million customers
  Australian airline Qantas fell victim to a cyberattack that saw information describing six million
‚Ä¢ Figma files for an (A)IPO with prospectus that mentions AI 150+ times
  Figma filed with the US Securities and Exchange Commission to propose an initial public offering of
‚Ä¢ With OpenAI, there are no allegiances - just compute at all costs
  OpenAI is looking to expand its network of compute providers to the likes of Oracle, CoreWeave,
‚Ä¢ Microsoft pulls plug on generous Azure credit program for startups
  Up to $150K tier shelved, perks folded into two-track system .
‚Ä¢ Cloudflare creates AI crawler tollbooth to pay publishers
  Cloudflare has started blocking AI web crawlers by default in a bid to become
‚Ä¢ Microsoft admits to Intune forgetfulness
  Microsoft Intune administrators may face a few days of stress after Redmond acknowledged a problem with
‚Ä¢ Senate decides free rein for AI companies isn't such a good thing
  A controversial section that would have barred states from regulating AI was struck down in a much clearer fashion . It took a tie
‚Ä¢ Chip design is a RISC-y business: Codasip puts itself up for sale
  Codasip cites expression of interest during recent funding round . European RISC-V b
‚Ä¢ Apple accuses former engineer of taking Vision Pro secrets to Snap
  Ex-Apple employee allegedly thought he was clever enough to sneak out the back door to a job at Snap loaded with
‚Ä¢ International Criminal Court swats away 'sophisticated and targeted' cyberattack
  International Criminal Court says a "sophisticated" cyberattack targeted the institution .
‚Ä¢ Fedora 43 won't drop 32-bit app support ‚Äì or adopt Xlibre
  Community vetoes plans to axe i686 compatibility and switch X11 forks . Fedora community
‚Ä¢ NASA gives Lunar Trailblazer a few more weeks to pick up the phone
  NASA has extended recovery efforts for stricken Lunar Trailblazer spacecraft to mid-July .
‚Ä¢ EU rattles its purse and AI datacenter builders come running
  176 expressions of interest to erect 'gigafactories' across 16 member states, with 3 million GPUs
‚Ä¢ Microsoft Copilot joins ChatGPT at the feet of the mighty Atari 2600 Video Chess
  Robert Caruso's ChatGPT has been beaten by Video Chess on an Atari
‚Ä¢ Folks aren‚Äôt buying the PCs that US vendors stockpiled to dodge tariffs
  Total PC shipments in the US will increase by just 2 percent this year, thanks to Trump's tariffs and little
‚Ä¢ Linus Torvalds hints Bcachefs may get dropped from the Linux kernel
  Linus Torvalds: "I think we'll be parting ways" as
‚Ä¢ People have empathy with AI‚Ä¶ as long as they think it's human
  Study finds emotional support from chatbots is more readily accepted if participants don't know it
‚Ä¢ Terrible tales of opsec oversights: How cybercrooks get themselves caught
  For cybercriminals, taking too many shortcuts when it comes to opsec delivers a
‚Ä¢ Critics blast Microsoft's limited reprieve for those stuck on Windows 10
  Microsoft's latest attempts to ease transition to Windows 11 for Windows 10 users "don't go far

üè• Public Health & Science
‚Ä¢ Co-designing a public health data analytics platform
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery .
‚Ä¢ A field test of empathetic refutational and motivational interviewing to address vaccine hesitancy among patients
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery .
‚Ä¢ Impact of circuit training structures on the acute response in physiological and mechanical performance: a cross-sectional study
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery .
‚Ä¢ Respiratory syncytial virus hospitalisation by chronological month of age and by birth month in infants
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery .
‚Ä¢ Viral genomes track the transmission of mpox in humans before 2017 outbreak
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery .
‚Ä¢ Three tips for talking to a vaccine sceptic
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery .
‚Ä¢ Assessment of information quality and reliability on ankle sprains in short videos from Douyin and Bilibili
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery .
‚Ä¢ Leadership change at African journal sparks calls for bold reform
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery .
‚Ä¢ Associations between anthropometric indices and biological age acceleration in American adults: insights from NHANES 2009‚Äì2018 data
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery .
‚Ä¢ Gastroesophageal disease risk and inhalational exposure a systematic review and meta-analysis
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery .
‚Ä¢ Ethnic disparities in COVID-19 mortality and cardiovascular disease in England and Wales between 2020-2022
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery .
‚Ä¢ Discrete choice experiment to evaluate preferences for tailored social media messages for vaping initiation prevention among sexual and gender minority youth
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery .
‚Ä¢ Prevalence and associated factors of at risk of anemia among children under five in Northeast Thailand using noninvasive hemoglobin screening in a cross sectional study
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery .
‚Ä¢ Examining burden among caregivers of community-dwelling older adults in Lebanon
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery .
‚Ä¢ An integrated GIS-pXRF approach assesses ecological and human health risks from heavy metals in county level soils
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery .
‚Ä¢ Hypertension among people living with HIV receiving dolutegravir-based antiretroviral therapy in ethiopia: a cross-sectional study
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery .
‚Ä¢ General practitioners‚Äô recommendation of HPV vaccination to adolescents aged 11‚Äì14 may be gender-biased as suggested by a qualitative study in France
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery .
‚Ä¢ An exploratory mixed methods study on shared decision-making and antibiotic prescribing for pet cats and dogs in Singapore veterinary clinics
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery .
‚Ä¢ Characterizing everyday exposure to volatile organic compounds and upper respiratory health effects
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery .
‚Ä¢ Rare disease, common struggles: quality of life, caregiver burden and financial wellbeing of family caregivers in Poland
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery .
‚Ä¢ Association of gestational weight gain patterns with preterm birth subtypes in a population based cohort study from China
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery .
‚Ä¢ A longitudinal study investigating the association between social maturity, social preference and children‚Äôs perceptions of their playfulness
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery .
‚Ä¢ Sexually transmitted infections are not associated with US holidays
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery .
‚Ä¢ Scalable evaluation framework for retrieval augmented generation in tobacco research using large Language models
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery .
‚Ä¢ The impact of intelligent devices utilization on household medical expenditure of older adults in China
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery .
‚Ä¢ Effect of the implementation of HIV/AIDS prevention and treatment policies on the mortality rate of people living with HIV in Guilin, China
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery .
‚Ä¢ Adolescent sexuality education, sexual debut, and associated factors in Nigerian public secondary schools
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery .
‚Ä¢ Postoperative mortality following hip fracture surgery in older adults: a single-center retrospective study in the context of Taiwan‚Äôs transition to an aged society
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery .
‚Ä¢ Short and long-term outcomes of children and adolescents hospitalized with COVID-19 or influenza: results of the AUTCOV study
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery .
‚Ä¢ Socioeconomic inequalities in disability prevalence and health service use in Bangladesh
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery .

üßæ Government & Policy
No updates.

üèõÔ∏è Enterprise Architecture & IT Governance
No updates.

ü§ñ AI & Emerging Tech
‚Ä¢ The Download: AI agents hype, and Google‚Äôs electricity plans
  This is today&#8217;s edition of¬†The Download,¬†our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



Don‚Äôt let hype about AI agents get ahead of reality



‚ÄîYoav Shoham is a professor emeritus at Stanford University and cofounder of AI21 Labs.



At Google‚Äôs I/O 2025 event in May, the company showed off a digital assistant that didn‚Äôt just answer questions; it helped work on a bicycle repair by finding a matching user manual, locating a YouTube tutorial, and even calling a local store to ask about a part, all with minimal human nudging. Such capabilities could soon extend far outside the Google ecosystem.The vision is exciting: Intelligent software agents that act like digital coworkers, booking your flights, rescheduling meetings, filing expenses, and talking to each other behind the scenes to get things done.But if we‚Äôre not careful, we‚Äôre going to derail the whole idea before it has a chance to deliver real benefits. As with many tech trends, there‚Äôs a risk of hype racing ahead of reality. And when expectations get out of hand, a backlash isn‚Äôt far behind. Read the full story.







Google‚Äôs electricity demand is skyrocketing



We got two big pieces of energy news from Google this week. The company announced that it‚Äôs signed an agreement to purchase electricity from a fusion company‚Äôs forthcoming first power plant. Google also released its latest environmental report, which shows that its energy use from data centers has doubled since 2020.



Taken together, these two bits of news offer a fascinating look at just how desperately big tech companies are hunting for clean electricity to power their data centers as energy demand and emissions balloon in the age of AI. Of course, we don‚Äôt know exactly how much of this pollution is attributable to AI because Google doesn‚Äôt break that out. (Also a problem!) So, what‚Äôs next and what does this all mean?



‚ÄîCasey Crownhart



This article is from The Spark, MIT Technology Review‚Äôs weekly climate newsletter. To receive it in your inbox every Wednesday, sign up here.+ To read more about whether nuclear energy is really a viable way to power the AI boom, check out Casey‚Äôs recent article, which is part of Power Hungry: AI and our energy future‚Äîour new series shining a light on the energy demands and carbon costs of the artificial intelligence revolution. You can take a look at the rest of the package here.







The must-reads



I‚Äôve combed the internet to find you today‚Äôs most fun/important/scary/fascinating stories about technology.



1 Meta‚Äôs climate tool was ‚Äòtrained using faulty data‚ÄôScientists claim it raised false hopes about the feasibility of removing carbon dioxide from the atmosphere. (FT $)+ xAI‚Äôs gas turbines have been greenlit, despite community backlash. (Wired $)+ Why we need to shoot carbon dioxide thousands of feet underground. (MIT Technology Review)



2 We don‚Äôt know whether US insurers will cover vaccines for kidsMajor insurers haven‚Äôt confirmed whether they‚Äôll keep covering the costs of shots. (Wired $)+ What‚Äôs next for the Gates Foundation‚Äôs global health initiatives? (Undark)+ How measuring vaccine hesitancy could help health professionals tackle it. (MIT Technology Review)



3 The Trump administration wants to gut Biden‚Äôs climate lawThe Inflation Reduction Act‚Äôs green energy tax incentives are hanging in the balance. (WP $)+ It‚Äôs bad news for one of the US economy‚Äôs biggest growth sectors. (Vox)+ How are we going to feed the world without making climate change worse? (New Yorker $)+ The President threatened to unravel the landmark law long before he was elected. (MIT Technology Review)



4 There are certain tells a scientific study abstract has been written by AIUse of hundreds of words has shot up since ChatGPT was made public. (NYT $)+ Beware over-reliance on AI-text detection tools, though. (MIT Technology Review)



5 Elon Musk doesn‚Äôt care about cars any moreWhich is terrible news for Tesla and its investors. (WSJ $)+ Things aren‚Äôt looking too hot for Rivian, either. (Insider $)



6 America‚Äôs weather forecasting is getting worseJust a year ago, US storm forecasting was the best it had ever been. Now, its accuracy is rapidly declining. (The Atlantic $)



7 Brazil has sustainable data center ambitionsEnvironmentalists aren‚Äôt convinced, however. (Rest of World)



8 A mysterious object has been spotted passing through the solar systemAnd we‚Äôve got good reason to believe it originated outside our system. (Ars Technica)



9 A rising band on Spotify is probably AI-generatedBut no one seems able to say for sure. (Vice)



10 The homes float in flood waterIt‚Äôs one solution to building homes on known flood plains. (Fast Company $)+ How to stop a state from sinking. (MIT Technology Review)







Quote of the day



‚ÄúAI doesn‚Äôt know what an orgasm sounds like.‚Äù&nbsp;



‚ÄîAnnabelle Tudor, an audiobook narrator, tells the Guardian why she‚Äôs not convinced by the industry‚Äôs plans to have AI narrate audiobooks.







One more thing







Who gets to decide who receives experimental medical treatments?



There has been a trend toward lowering the bar for new medicines, and it is becoming easier for people to access treatments that might not help them‚Äîand could even harm them. Anecdotes appear to be overpowering evidence in decisions on drug approval. As a result, we‚Äôre ending up with some drugs that don‚Äôt work.



We urgently need to question how these decisions are made. Who should have access to experimental therapies? And who should get to decide? Such questions are especially pressing considering how quickly biotechnology is advancing. We‚Äôre not just improving on existing classes of treatments‚Äîwe‚Äôre creating entirely new ones. Read the full story.



‚ÄîJessica Hamzelou







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)



+ These aerial shots of Glastonbury festival are crazy.+ Our oceans really are amazing places‚Äîtake a moment to appreciate them.+ How to be truly cool, according to science.+ Happy 62nd birthday to Tracey Emin, still an enfant terrible after all these years.
‚Ä¢ Google‚Äôs electricity demand is skyrocketing
  We got two big pieces of energy news from Google this week. The company announced that it‚Äôs signed an agreement to purchase electricity from a fusion company‚Äôs forthcoming first power plant. Google also released its latest environmental report, which shows that its energy use from data centers has doubled since 2020.



Taken together, these two bits of news offer a fascinating look at just how desperately big tech companies are hunting for clean electricity to power their data centers as energy demand and emissions balloon in the age of AI. Of course, we don‚Äôt know exactly how much of this pollution is attributable to AI because Google doesn‚Äôt break that out. (Also a problem!) So, what‚Äôs next and what does this all mean?&nbsp;





Let‚Äôs start with fusion: Google‚Äôs deal with Commonwealth Fusion Systems is intended to provide the tech giant with 200 megawatts of power. This will come from Commonwealth‚Äôs first commercial plant, a facility planned for Virginia that the company refers to as the Arc power plant. The agreement represents half its capacity.



What‚Äôs important to note here is that this power plant doesn‚Äôt exist yet. In fact, Commonwealth still needs to get its Sparc demonstration reactor, located outside Boston, up and running. That site, which I visited in the fall, should be completed in 2026.



(An aside: This isn‚Äôt the first deal between Big Tech and a fusion company. Microsoft signed an agreement with Helion a couple of years ago to buy 50 megawatts of power from a planned power plant, scheduled to come online in 2028. Experts expressed skepticism in the wake of that deal, as my colleague James Temple reported.)



Nonetheless, Google‚Äôs announcement is a big moment for fusion, in part because of the size of the commitment and also because Commonwealth, a spinout company from MIT‚Äôs Plasma Science and Fusion Center, is seen by many in the industry as a likely candidate to be the first to get a commercial plant off the ground. (MIT Technology Review is owned by MIT but is editorially independent.)



Google leadership was very up-front about the length of the timeline. ‚ÄúWe would certainly put this in the long-term category,‚Äù said Michael Terrell, Google‚Äôs head of advanced energy, in a press call about the deal.



The news of Google‚Äôs foray into fusion comes just days after the tech giant‚Äôs release of its latest environmental report. While the company highlighted some wins, some of the numbers in this report are eye-catching, and not in a positive way.



Google‚Äôs emissions have increased by over 50% since 2019, rising 6% in the last year alone. That‚Äôs decidedly the wrong direction for a company that‚Äôs set a goal to reach net-zero greenhouse-gas emissions by the end of the decade.



It‚Äôs true that the company has committed billions to clean energy projects, including big investments in next-generation technologies like advanced nuclear and enhanced geothermal systems. Those deals have helped dampen emissions growth, but it‚Äôs an arguably impossible task to keep up with the energy demand the company is seeing.



Google‚Äôs electricity consumption from data centers was up 27% from the year before. It‚Äôs doubled since 2020, reaching over 30 terawatt-hours. That‚Äôs nearly the annual electricity consumption from the entire country of Ireland.



As an outsider, it‚Äôs tempting to point the finger at AI, since that technology has crashed into the mainstream and percolated into every corner of Google‚Äôs products and business. And yet the report downplays the role of AI. Here‚Äôs one bit that struck me:



‚ÄúHowever, it‚Äôs important to note that our growing electricity needs aren‚Äôt solely driven by AI. The accelerating growth of Google Cloud, continued investments in Search, the expanding reach of YouTube, and more, have also contributed to this overall growth.‚Äù



There is enough wiggle room in that statement to drive a large electric truck through. When I asked about the relative contributions here, company representative Mara Harris said via email that they don‚Äôt break out what portion comes from AI. When I followed up asking if the company didn‚Äôt have this information or just wouldn‚Äôt share it, she said she‚Äôd check but didn‚Äôt get back to me.



I‚Äôll make the point here that we‚Äôve made before, including in our recent package on AI and energy: Big companies should be disclosing more about the energy demands of AI. We shouldn‚Äôt be guessing at this technology‚Äôs effects.



Google has put a ton of effort and resources into setting and chasing ambitious climate goals. But as its energy needs and those of the rest of the industry continue to explode, it‚Äôs obvious that this problem is getting tougher, and it‚Äôs also clear that more transparency is a crucial part of the way forward.



This article is from The Spark, MIT Technology Review‚Äôs weekly climate newsletter. To receive it in your inbox every Wednesday, sign up here.
‚Ä¢ Don‚Äôt let hype about AI agents get ahead of reality
  Google‚Äôs recent unveiling of what it calls a ‚Äúnew class of agentic experiences‚Äù feels like a turning point. At its I/O 2025 event in May, for example, the company showed off a digital assistant that didn‚Äôt just answer questions; it helped work on a bicycle repair by finding a matching user manual, locating a YouTube tutorial, and even calling a local store to ask about a part, all with minimal human nudging. Such capabilities could soon extend far outside the Google ecosystem. The company has introduced an open standard called Agent-to-Agent, or A2A, which aims to let agents from different companies talk to each other and work together.



The vision is exciting: Intelligent software agents that act like digital coworkers, booking your flights, rescheduling meetings, filing expenses, and talking to each other behind the scenes to get things done. But if we‚Äôre not careful, we‚Äôre going to derail the whole idea before it has a chance to deliver real benefits. As with many tech trends, there‚Äôs a risk of hype racing ahead of reality. And when expectations get out of hand, a backlash isn‚Äôt far behind.





Let‚Äôs start with the term ‚Äúagent‚Äù itself. Right now, it‚Äôs being slapped on everything from simple scripts to sophisticated AI workflows. There‚Äôs no shared definition, which leaves plenty of room for companies to market basic automation as something much more advanced. That kind of ‚Äúagentwashing‚Äù doesn‚Äôt just confuse customers; it invites disappointment. We don‚Äôt necessarily need a rigid standard, but we do need clearer expectations about what these systems are supposed to do, how autonomously they operate, and how reliably they perform.



And reliability is the next big challenge. Most of today‚Äôs agents are powered by large language models (LLMs), which generate probabilistic responses. These systems are powerful, but they‚Äôre also unpredictable. They can make things up, go off track, or fail in subtle ways‚Äîespecially when they‚Äôre asked to complete multistep tasks, pulling in external tools and chaining LLM responses together. A recent example: Users of Cursor, a popular AI programming assistant, were told by an automated support agent that they couldn‚Äôt use the software on more than one device. There were widespread complaints and reports of users canceling their subscriptions. But it turned out the policy didn‚Äôt exist. The AI had invented it.



In enterprise settings, this kind of mistake could create immense damage. We need to stop treating LLMs as standalone products and start building complete systems around them‚Äîsystems that account for uncertainty, monitor outputs, manage costs, and layer in guardrails for safety and accuracy. These measures can help ensure that the output adheres to the requirements expressed by the user, obeys the company‚Äôs policies regarding access to information, respects privacy issues, and so on. Some companies, including AI21 (which I cofounded and which has received funding from Google), are already moving in that direction, wrapping language models in more deliberate, structured architectures. Our latest launch, Maestro, is designed for enterprise reliability, combining LLMs with company data, public information, and other tools to ensure dependable outputs.



Still, even the smartest agent won‚Äôt be useful in a vacuum. For the agent model to work, different agents need to cooperate (booking your travel, checking the weather, submitting your expense report) without constant human supervision. That‚Äôs where Google‚Äôs A2A protocol comes in. It‚Äôs meant to be a universal language that lets agents share what they can do and divide up tasks. In principle, it‚Äôs a great idea.In practice, A2A still falls short. It defines how agents talk to each other, but not what they actually mean. If one agent says it can provide ‚Äúwind conditions,‚Äù another has to guess whether that‚Äôs useful for evaluating weather on a flight route. Without a shared vocabulary or context, coordination becomes brittle. We‚Äôve seen this problem before in distributed computing. Solving it at scale is far from trivial.





There‚Äôs also the assumption that agents are naturally cooperative. That may hold inside Google or another single company‚Äôs ecosystem, but in the real world, agents will represent different vendors, customers, or even competitors. For example, if my travel planning agent is requesting price quotes from your airline booking agent, and your agent is incentivized to favor certain airlines, my agent might not be able to get me the best or least expensive itinerary. Without some way to align incentives through contracts, payments, or game-theoretic mechanisms, expecting seamless collaboration may be wishful thinking.



None of these issues are insurmountable. Shared semantics can be developed. Protocols can evolve. Agents can be taught to negotiate and collaborate in more sophisticated ways. But these problems won‚Äôt solve themselves, and if we ignore them, the term ‚Äúagent‚Äù will go the way of other overhyped tech buzzwords. Already, some CIOs are rolling their eyes when they hear it.



That‚Äôs a warning sign. We don‚Äôt want the excitement to paper over the pitfalls, only to let developers and users discover them the hard way and develop a negative perspective on the whole endeavor. That would be a shame. The potential here is real. But we need to match the ambition with thoughtful design, clear definitions, and realistic expectations. If we can do that, agents won‚Äôt just be another passing trend; they could become the backbone of how we get things done in the digital world.



Yoav Shoham is a professor emeritus at Stanford University and cofounder of AI21 Labs. His 1993 paper on agent-oriented programming received the AI Journal Classic Paper Award. He is coauthor of Multiagent Systems: Algorithmic, Game-Theoretic, and Logical Foundations, a standard textbook in the field.
‚Ä¢ The Download: how AI could improve construction site safety, and our Roundtables conversation with Karen Hao
  This is today&#8217;s edition of¬†The Download,¬†our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



How generative AI could help make construction sites safer



More than 1,000 construction workers die on the job each year in the US, making it the most dangerous industry for fatal slips, trips, and falls.A new AI tool called Safety AI could help to change that. It analyzes the progress made on a construction site each day, and flags conditions that violate Occupational Safety and Health Administration rules, with what its creator Philip Lorenzo claims is 95% accuracy.



Lorenzo says Safety AI is the first one of multiple emerging AI construction safety tools to use generative AI to flag safety violations. But as the 95% success rate suggests, Safety AI is not a flawless and all-knowing intelligence. Read the full story.



‚ÄîAndrew Rosenblum







Roundtables: Inside OpenAI‚Äôs Empire with Karen Hao



Earlier this week, we held a subscriber-only Roundtable discussion with author and former MIT Technology Review senior editor Karen Hao about her new book Empire of AI: Dreams and Nightmares in Sam Altman&#8217;s OpenAI.You can watch her conversation with our executive editor Niall Firth here‚Äîand if you aren‚Äôt already, you can subscribe to us here.&nbsp;







MIT Technology Review Narrated: The tech industry can‚Äôt agree on what open-source AI means. That‚Äôs a problem.



What counts as &#8216;open-source AI&#8217;? The answer could determine who gets to shape the future of the technology.



This is our latest story to be turned into a MIT Technology Review Narrated podcast, which we‚Äôre publishing each week on Spotify and Apple Podcasts. Just navigate to MIT Technology Review Narrated on either platform, and follow us to get all our new content as it‚Äôs released.







The must-reads



I‚Äôve combed the internet to find you today‚Äôs most fun/important/scary/fascinating stories about technology.



1 China‚Äôs digital IDs are comingAnd they‚Äôre unlikely to stay voluntary for long. (Economist $)+ The country‚Äôs AI models are becoming increasingly popular worldwide. (WSJ $)



2 Donald Trump has mused about using DOGE to deport Elon MuskMusk‚Äôs comments about the President‚Äôs ‚ÄòBig Beautiful Bill‚Äô have touched a nerve. (Axios)+ Turns out AI models are quite good at fact checking Trump. (WP $)+ DOGE‚Äôs tech takeover threatens the safety and stability of our critical data. (MIT Technology Review)



3 Google must pay California‚Äôs Android users $314.6mAfter a jury ruled it had misused their data. (Reuters)&nbsp;



4 Many AI detectors overpromise and underdeliverBut that hasn‚Äôt stopped Californian colleges from investing millions in them. (Undark)+ What‚Äôs next for college writing? Nothing good. (New Yorker $)+ Educators are working out how to integrate AI into computer science. (NYT $)+ AI-text detection tools are really easy to fool. (MIT Technology Review)



5 Google is making its first foray into fusionThe world‚Äôs first grid-scale fusion power plant is due to come online in the 2030s. (NBC News)+ Google will buy half its output. (TechCrunch)+ Inside a fusion energy facility. (MIT Technology Review)



6 China is banning certain portable batteries from flightsIn the wake of two major manufacturers recalling millions of power banks. (NYT $)+ The ban is catching travellers out. (SCMP)



7 The deepfake economy is spiralling out of controlSmall business owners are drowning in online scams. (Insider $)



8 Chipmaking companies are attractive prospects for investorsAnd they‚Äôre likely to be better bets. (WSJ $)+ OpenAI has denied that it plans to use Google‚Äôs in-house chip. (Reuters)



9 How cancer studies in dogs could help develop treatments for humansThe disease presents very similarly across both species. (Knowable Magazine)+ Cancer vaccines are having a renaissance. (MIT Technology Review)



10 X is planning to task AI agents with writing Community NotesThankfully, humans will still review them. (Bloomberg $)+ Why does AI hallucinate? (MIT Technology Review)







Quote of the day



‚ÄúMissionaries will beat mercenaries.‚Äù



‚ÄîOpenAI CEO Sam Altman takes aim at Meta‚Äôs recent spree of attempting to hire his staff, Wired reports.







One more thing







The world‚Äôs next big environmental problem could come from spaceIn September, a unique chase took place in the skies above Easter Island. From a rented jet, a team of researchers captured a satellite‚Äôs last moments as it fell out of space and blazed into ash across the sky, using cameras and scientific equipment. Their hope was to gather priceless insights into the physical and chemical processes that occur when satellites burn up as they fall to Earth at the end of their missions.



This kind of study is growing more urgent. The number of satellites in the sky is rapidly rising‚Äîwith a tenfold increase forecast by the end of the decade. Letting these satellites burn up in the atmosphere at the end of their lives helps keep the quantity of space junk to a minimum. But doing so deposits satellite ash in the Earth‚Äôs atmosphere. This metallic ash could potentially alter the climate, and we don‚Äôt yet know how serious the problem is likely to be. Read the full story.&nbsp;



‚ÄîTereza Pultarova







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)



+ The new Running Man film looks pretty good, even if it is without Arnold.+ Maybe it‚Äôs just not worth trying to understand our dogs after all.+ Cynthia Erivo, who knows a thing or two about belting out a tune, really loves The Thong Song, and who can blame her?+ Show your face, colossal squid!
‚Ä¢ How generative AI could help make construction sites safer
  Last winter, during the construction of an affordable housing project on Martha‚Äôs Vineyard, Massachusetts, a 32-year-old worker named Jose Luis Collaguazo Crespo slipped off a ladder on the second floor and plunged to his death in the basement. He was one of more than¬†1,000¬†construction¬†workers who die on the job each year in the US, making it the most dangerous industry for fatal slips, trips, and falls.



‚ÄúEveryone talks about [how] ‚Äòsafety is the number-one priority,‚Äô‚Äù entrepreneur and executive Philip Lorenzo said during a presentation at Construction Innovation Day 2025, a conference at the University of California, Berkeley, in April.&nbsp;‚ÄúBut then maybe internally, it‚Äôs not that high priority. People take shortcuts on job sites. And so there‚Äôs this whole tug-of-war between ‚Ä¶ safety and productivity.‚Äù



To combat the shortcuts and risk-taking, Lorenzo is working on a tool for the San Francisco‚Äìbased company DroneDeploy, which sells software that creates daily digital models of&nbsp;work progress from videos and images, known in the trade as ‚Äúreality capture.‚Äù&nbsp; The tool, called Safety AI, analyzes each day‚Äôs reality capture imagery and flags conditions that violate Occupational Safety and Health Administration (OSHA) rules, with what he claims is 95% accuracy. 



That means that for any safety risk the software flags, there is 95% certainty that the flag is accurate and relates to a specific OSHA regulation. Launched in October 2024, it‚Äôs now being deployed on hundreds of construction sites in the US, Lorenzo says, and versions specific to the building regulations in countries including Canada, the UK, South Korea, and Australia have also been deployed.



Safety AI is one of multiple AI&nbsp;construction&nbsp;safety tools that have emerged in recent years, from&nbsp;Silicon Valley&nbsp;to&nbsp;Hong Kong to&nbsp;Jerusalem. Many of these rely on teams of human ‚Äúclickers,‚Äù often in low-wage countries, to manually draw bounding boxes around images of key objects like ladders, in order to label large volumes of data to train an algorithm. 



Lorenzo says Safety AI is the first one to use generative AI to flag safety violations, which means an algorithm that can do more than recognize objects such as ladders or hard hats. The software can ‚Äúreason‚Äù about what is going on in an image of a site and draw a conclusion about whether there is an OSHA violation. This is a more advanced form of analysis than the object detection that is the current industry standard, Lorenzo claims. But as the 95% success rate suggests, Safety AI is not a flawless and all-knowing intelligence. It requires an experienced safety inspector as an overseer.&nbsp;&nbsp;



A visual language model in the real world



Robots and AI tend to thrive in controlled, largely static environments, like factory floors or shipping terminals.&nbsp;But construction&nbsp;sites are, by definition, changing a little bit every day.&nbsp;



Lorenzo thinks he‚Äôs built a better way to monitor sites, using a type of generative AI called a visual language model, or VLM. A VLM is an LLM with a vision encoder, allowing it to ‚Äúsee‚Äù images of the world and analyze what is going on in the scene.&nbsp;



Using years of reality capture imagery gathered from customers, with their explicit permission, Lorenzo‚Äôs team has assembled what he calls a ‚Äúgolden data set‚Äù encompassing tens of thousands of images of OSHA violations. Having carefully stockpiled this specific data for years, he is not worried that even a billion-dollar tech giant will be able to ‚Äúcopy and crush‚Äù him.



To help train the model, Lorenzo has a smaller team of construction safety pros ask strategic questions of the AI. The trainers input test scenes from the golden data set to the VLM and ask questions that guide the model through the process of breaking down the scene and analyzing it step by step the way an experienced human would. If the VLM doesn‚Äôt generate the correct response‚Äîfor example, it misses a violation or registers a false positive‚Äîthe human trainers go back and tweak the prompts or inputs. Lorenzo says that rather than simply learning to recognize objects, the VLM is taught ‚Äúhow to think in a certain way,‚Äù which means it can draw subtle conclusions about what is happening in an image.&nbsp;



Examples of safety risk categories that Safety AI can detect.COURTESY DRONEDEPLOY




As an example, Lorenzo says VLMs are much better than older methods at analyzing ladder usage, which is responsible for 24% of the fall deaths in the construction industry.&nbsp;



‚ÄúWith traditional machine learning, it‚Äôs very difficult to answer the question of ‚ÄòIs a person using a ladder unsafely?‚Äô‚Äù says Lorenzo. ‚ÄúYou can find the ladders. You can find the people. But to logically reason and say ‚ÄòWell, that person is fine‚Äô or ‚ÄòOh no, that person‚Äôs standing on the top step‚Äô‚Äîonly the VLM can logically reason and then be like, ‚ÄòAll right, it‚Äôs unsafe. And here‚Äôs the OSHA reference that says you can‚Äôt be on the top rung.‚Äô‚Äù



Answers to multiple questions (Does the person on the ladder have three points of contact? Are they using the ladder as stilts to move around?) are combined to determine whether the ladder in the picture is being used safely. ‚ÄúOur system has over a dozen layers of questioning just to get to that answer,‚Äù Lorenzo says. DroneDeploy has not publicly released its data for review, but he says he hopes to have his methodology independently audited by safety experts.&nbsp;&nbsp;



The missing 5%



Using vision language models for construction AI shows promise, but there are ‚Äúsome pretty fundamental issues‚Äù to resolve, including hallucinations and the problem of edge cases, those anomalous hazards for which the VLM hasn‚Äôt trained, says Chen Feng. He leads New York University‚Äôs AI4CE lab, which develops technologies for 3D mapping and scene understanding in construction robotics and other areas. ‚ÄúNinety-five percent is encouraging‚Äîbut how do we fix that remaining 5%?‚Äù he asks of Safety AI‚Äôs success rate. 



Feng points to a 2024 paper called ‚ÄúEyes Wide Shut?‚Äù‚Äîwritten by Shengbang Tong, a PhD student at NYU, and coauthored by AI luminary Yann LeCun‚Äîthat noted ‚Äúsystematic shortcomings‚Äù in VLMs.&nbsp; ‚ÄúFor object detection, they can reach human-level performance pretty well,‚Äù Feng says. ‚ÄúHowever, for more complicated things‚Äîthese capabilities are still to be improved.‚Äù He notes that VLMs have struggled to interpret 3D scene structure from 2D images, don‚Äôt have good situational awareness in reasoning about spatial relationships, and often lack ‚Äúcommon sense‚Äù about visual scenes.



Lorenzo concedes that there are ‚Äúsome major flaws‚Äù with LLMs and that they struggle with spatial reasoning. So Safety AI also employs some older machine-learning methods to help create spatial models of construction sites. These methods include the segmentation of images into crucial components and photogrammetry, an established technique for creating a 3D digital model from a 2D image. Safety AI has also trained heavily in 10 different problem areas, including ladder usage, to anticipate the most common violations.



Even so, Lorenzo admits there are edge cases that the LLM will fail to recognize. But he notes that for overworked safety managers, who are often responsible for as many as 15 sites at once, having an extra set of digital ‚Äúeyes‚Äù is still an improvement.



Aaron Tan, a concrete project manager based in the San Francisco Bay Area, says that a tool like Safety AI could be helpful for these overextended safety managers, who will save a lot of time if they can get an emailed alert rather than having to make a two-hour drive to visit a site in person. And if the software can demonstrate that it is helping keep people safe, he thinks workers will eventually embrace it.&nbsp;&nbsp;



However, Tan notes that workers also fear that these types of tools will be ‚Äúbossware‚Äù used to get them in trouble. ‚ÄúAt my last company, we implemented cameras [as] a security system. And the guys didn‚Äôt like that,‚Äù he says. ‚ÄúThey were like, ‚ÄòOh, Big Brother. You guys are always watching me‚ÄîI have no privacy.‚Äô‚Äù



Older doesn‚Äôt mean obsolete



Izhak Paz, CEO of a Jerusalem-based company called Safeguard AI, has considered incorporating VLMs, but he has stuck with the older machine-learning paradigm because he considers it more reliable. The ‚Äúold computer vision‚Äù based on machine learning ‚Äúis still better, because it‚Äôs hybrid between the machine itself and human intervention on dealing with deviation,‚Äù he says. To train the algorithm on a new category of danger, his team aggregates a large volume of labeled footage related to the specific hazard and then optimizes the algorithm by trimming false positives and false negatives. The process can take anywhere from weeks to over six months, Paz says. 



With training completed, Safeguard AI performs a risk assessment to identify potential hazards on the site. It can ‚Äúsee‚Äù the site in real time by accessing footage from any nearby internet-connected camera. Then it uses an AI agent to push instructions on what to do next to the site managers‚Äô mobile devices. Paz declines to give a precise price tag, but he says his product is affordable only for builders at the ‚Äúmid-market‚Äù level and above, specifically those managing multiple sites. The tool is in use at roughly 3,500 sites in Israel, the United States, and Brazil.





Buildots, a company based in Tel Aviv that MIT Technology Review profiled back in 2020, doesn‚Äôt do safety analysis but instead creates once- or twice-weekly visual progress reports of sites. Buildots also uses the older method of machine learning with labeled training data. ‚ÄúOur system needs to be 99%‚Äîwe cannot have any hallucinations,‚Äù says CEO Roy Danon.&nbsp;



He says that gaining labeled training data is actually much easier than it was when he and his cofounders began the project in 2018, since gathering video footage of sites means that each object, such as a socket, might be captured and then labeled in many different frames. But the tool is high-end‚Äîabout 50 builders, most with revenue over $250 million, are using Buildots in Europe, the Middle East, Africa, Canada, and the US. It‚Äôs been used on over 300 projects so far.



Ryan Calo, a specialist in robotics and AI law at the University of Washington, likes the idea of AI for construction safety. Since experienced safety managers are already spread thin in construction, however, Calo worries that builders will be tempted to automate humans out of the safety process entirely. ‚ÄúI think AI and drones for spotting safety problems that would otherwise kill workers is super smart,‚Äù he says. ‚ÄúSo long as it‚Äôs verified by a person.‚Äù



Andrew Rosenblum is a freelance tech journalist based in Oakland, CA.
‚Ä¢ The Download: tripping with AI, and blocking crawler bots
  This is today&#8217;s edition of¬†The Download,¬†our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



People are using AI to ‚Äòsit‚Äô with them while they trip on psychedelics



A growing number of people are using AI chatbots as ‚Äútrip sitters‚Äù‚Äîa phrase that traditionally refers to a sober person tasked with monitoring someone who‚Äôs under the influence of a psychedelic‚Äîand sharing their experiences online.It‚Äôs a potent blend of two cultural trends: using AI for therapy and using psychedelics to alleviate mental-health problems. But this is a potentially dangerous psychological cocktail, according to experts. While it‚Äôs far cheaper than in-person psychedelic therapy, it can go badly awry. Read the full story.



‚ÄîWebb Wright







Cloudflare will now, by default, block AI bots from crawling its clients‚Äô websites



The news: The internet infrastructure company Cloudflare has announced that it will start blocking AI bots from visiting websites it hosts by default.What bots? The bots in question are a type of web crawler, an algorithm that walks across the internet then digests and catalogs information on each website. In the past, web crawlers were most commonly associated with gathering data for search engines, but developers now use them to gather data they need to build and use AI systems.So, are all bots banned? Not quite. Cloudflare will also give clients the ability to allow or ban these AI bots on a case-by-case basis, and plans to introduce a so-called ‚Äúpay-per-crawl‚Äù service that clients can use to receive compensation every time an AI bot wants to scoop up their website‚Äôs contents. Read the full story.



‚ÄîPeter Hall







What comes next for AI copyright lawsuits?



Last week, Anthropic and Meta each won landmark victories in two separate court cases that examined whether or not the firms had violated copyright when they trained their large language models on copyrighted books without permission. The rulings are the first we‚Äôve seen to come out of copyright cases of this kind. This is a big deal!



There are dozens of similar copyright lawsuits working through the courts right now, and their outcomes are set to have an enormous impact on the future of AI. In effect, they will decide whether or not model makers can continue ordering up a free lunch. Read the full story.



‚ÄîWill Douglas Heaven



This story originally appeared in The Algorithm, our weekly newsletter on AI. To get stories like this in your inbox first, sign up here.







The must-reads



I‚Äôve combed the internet to find you today‚Äôs most fun/important/scary/fascinating stories about technology.



1 The US Senate has killed an effort to prevent states regulating AI¬†But AI giants are likely to keep lobbying for similar sorts of legislation. (Reuters)+ Google et al want Congress to take regulation away from individual states. (Bloomberg $)+ Advocacy groups say the provision remains extremely damaging. (Wired $)+ OpenAI has upped its lobbying efforts nearly sevenfold. (MIT Technology Review)



2 Apple is considering using rival AI tech to bolster SiriIn a massive U-turn, it‚Äôs reported to have held talks with Anthropic and OpenAI. (Bloomberg $)+ Apple seems to have accepted that its in-house efforts simply can‚Äôt compete. (The Verge)



3 DOGE has access to data that may boost Elon Musk‚Äôs businessesHis rivals are worried their proprietary information could be exposed. (WP $)+ Donald Trump has floated tasking DOGE with reviewing Musk‚Äôs subsidies. (FT $)+ Relations between Musk and Trump are still pretty strained. (NY Mag $)



4 Amazon‚Äôs robot workforce is approaching a major milestoneIt‚Äôs on the verge of equalling the number of humans working in its warehouses. (WSJ $)+ Why the humanoid workforce is running late. (MIT Technology Review)



5 China‚Äôs clean energy boom is going globalJust as the US doubles down on fossil fuels. (NYT $)+ The Trump administration has shut down more than 100 climate studies. (MIT Technology Review)



6 The AI talent wars are massively inflating pay packagesWages for a small pool of workers have risen sharply in the past three years. (FT $)+ Meta, in particular, isn‚Äôt afraid to splash its cash. (Wired $)+ The vast majority of consumers aren‚Äôt paying for AI, though. (Semafor)



7 Microsoft claims its AI outperforms doctors‚Äô diagnosesIts system ‚Äúsolved‚Äù eight out of 10 cases, compared to physicians‚Äô two out of 10. (The Guardian)+ Why it‚Äôs so hard to use AI to diagnose cancer. (MIT Technology Review)&nbsp;



8 What the future of satellite internet could look likeVery crowded, for one. (Rest of World)+ How Antarctica‚Äôs history of isolation is ending‚Äîthanks to Starlink. (MIT Technology Review)



9 What is an attosecond?A load of laser-wielding scientists are measuring the units. (Knowable Magazine)



10 AI is Hollywood‚Äôs favorite villainWhere 2001, The Terminator, and The Matrix led, others follow. (Economist $)+ How a 30-year-old techno-thriller predicted our digital isolation. (MIT Technology Review)







Quote of the day



&#8220;Right now, AI companies are less regulated than sandwich shops.&#8221;



‚ÄîElla Hughes, organizing director of activist group PauseAI, addresses a crowd of protesters outside Google DeepMind‚Äôs London office, Insider reports.







One more thing







Inside NASA‚Äôs bid to make spacecraft as small as possibleSince the 1970s, we‚Äôve sent a lot of big things to Mars. But when NASA successfully sent twin Mars Cube One spacecraft, the size of cereal boxes, in November 2018, it was the first time we‚Äôd ever sent something so small.Just making it this far heralded a new age in space exploration. NASA and the community of planetary science researchers caught a glimpse of a future long sought: a pathway to much more affordable space exploration using smaller, cheaper spacecraft. Read the full story.



‚ÄîDavid W. Brown







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)



+ The South of France is jam-packed with stunning beaches.+ These fountain pen drawings really capture the beauty of nature.+ Yogurt soup?! Why not?+ Happy birthday to the timeless Debbie Harry‚Äî80 years young today.
‚Ä¢ Cloudflare will now, by default, block AI bots from crawling its clients‚Äô websites
  Cloudflare will default to blocking AI bots from visiting websites it hosts . The company will also give clients the ability to manually allow or ban these bots on a case-by-case basis . Clients can also set a rate for how much it will cost AI bots to crawl their websites .
‚Ä¢ People are using AI to ‚Äòsit‚Äô with them while they trip on psychedelics
  Peter sat alone in his bedroom as the first waves of euphoria coursed through his body like an electrical current. He was in darkness, save for the soft blue light of the screen glowing from his lap. Then he started to feel pangs of panic. He picked up his phone and typed a message to ChatGPT. ‚ÄúI took too much,‚Äù he wrote.



He‚Äôd swallowed a large dose (around eight grams) of magic mushrooms about 30 minutes before. It was 2023, and Peter, then a master‚Äôs student in Alberta, Canada, was at an emotional low point. His cat had died recently, and he‚Äôd lost his job. Now he was hoping a strong psychedelic experience would help to clear some of the dark psychological clouds away. When taking psychedelics in the past, he‚Äôd always been in the company of friends or alone; this time he wanted to trip under the supervision of artificial intelligence.&nbsp;



Just as he‚Äôd hoped, ChatGPT responded to his anxious message in its characteristically reassuring tone. ‚ÄúI‚Äôm sorry to hear you‚Äôre feeling overwhelmed,‚Äù it wrote. ‚ÄúIt‚Äôs important to remember that the effects you‚Äôre feeling are temporary and will pass with time.‚Äù It then suggested a few steps he could take to calm himself: take some deep breaths, move to a different room, listen to the custom playlist it had curated for him before he‚Äôd swallowed the mushrooms. (That playlist included Tame Impala‚Äôs Let It Happen, an ode to surrender and acceptance.)



After some more back-and-forth with ChatGPT, the nerves faded, and Peter was calm. ‚ÄúI feel good,‚Äù Peter typed to the chatbot. ‚ÄúI feel really at peace.‚Äù






Peter‚Äîwho asked to have his last name omitted from this story for privacy reasons‚Äîis far from alone. A growing number of people are using AI chatbots as ‚Äútrip sitters‚Äù‚Äîa phrase that traditionally refers to a sober person tasked with monitoring someone who‚Äôs under the influence of a psychedelic‚Äîand sharing their experiences online. It‚Äôs a potent blend of two cultural trends: using AI for therapy and using psychedelics to alleviate mental-health problems. But this is a potentially dangerous psychological cocktail, according to experts. While it‚Äôs far cheaper than in-person psychedelic therapy, it can go badly awry.




A potent mix




Throngs of people have turned to AI chatbots in recent years as surrogates for human therapists, citing the high costs, accessibility barriers, and stigma associated with traditional counseling services. They‚Äôve also been at least indirectly encouraged by some prominent figures in the tech industry, who have suggested that AI will revolutionize mental-health care. ‚ÄúIn the future ‚Ä¶ we will have *wildly effective* and dirt cheap AI therapy,‚Äù Ilya Sutskever, an OpenAI cofounder and its former chief scientist, wrote in an X post in 2023. ‚ÄúWill lead to a radical improvement in people‚Äôs experience of life.‚Äù




Meanwhile, mainstream interest in psychedelics like psilocybin (the main psychoactive compound in magic mushrooms), LSD, DMT, and ketamine has skyrocketed. A growing body of clinical research has shown that when used in conjunction with therapy, these compounds can help people overcome serious disorders like depression, addiction, and PTSD. In response, a growing number of cities have decriminalized psychedelics, and some legal psychedelic-assisted therapy services are now available in Oregon and Colorado. Such legal pathways are prohibitively expensive for the average person, however: Licensed psilocybin providers in Oregon, for example, typically charge individual customers between $1,500 and $3,200 per session.



It seems almost inevitable that these two trends‚Äîboth of which are hailed by their most devoted advocates as near-panaceas for virtually all society‚Äôs ills‚Äîwould coincide.



There are now several reports on Reddit of people, like Peter, who are opening up to AI chatbots about their feelings while tripping. These reports often describe such experiences in mystical language. ‚ÄúUsing AI this way feels somewhat akin to sending a signal into a vast unknown‚Äîsearching for meaning and connection in the depths of consciousness,‚Äù one Redditor wrote in the subreddit r/Psychonaut about a year ago. ‚ÄúWhile it doesn‚Äôt replace the human touch or the empathetic presence of a traditional [trip] sitter, it offers a unique form of companionship that‚Äôs always available, regardless of time or place.‚Äù Another user recalled opening ChatGPT during an emotionally difficult period of a mushroom trip and speaking with it via the chatbot‚Äôs voice mode: ‚ÄúI told it what I was thinking, that things were getting a bit dark, and it said all the right things to just get me centered, relaxed, and onto a positive vibe.‚Äù&nbsp;



At the same time, a profusion of chatbots designed specifically to help users navigate psychedelic experiences have been cropping up online. TripSitAI, for example, ‚Äúis focused on harm reduction, providing invaluable support during challenging or overwhelming moments, and assisting in the integration of insights gained from your journey,‚Äù according to its builder. ‚ÄúThe Shaman,‚Äù built atop ChatGPT, is described by its designer as ‚Äúa wise, old Native American spiritual guide ‚Ä¶ providing empathetic and personalized support during psychedelic journeys.‚Äù



Therapy without therapists



Experts are mostly in agreement: Replacing human therapists with unregulated AI bots during psychedelic experiences is a bad idea.



Many mental-health professionals who work with psychedelics point out that the basic design of large language models (LLMs)‚Äîthe systems powering AI chatbots‚Äîis fundamentally at odds with the therapeutic process. Knowing when to talk and when to keep silent, for example, is a key skill. In a clinic or the therapist‚Äôs office, someone who‚Äôs just swallowed psilocybin will typically put on headphones (listening to a playlist not unlike the one ChatGPT curated for Peter) and an eye mask, producing an experience that‚Äôs directed, by design, almost entirely inward. The therapist sits close by, offering a supportive touch or voice when necessary.&nbsp;



Chatbots like ChatGPT, on the other hand, are designed to‚Äîwell, chat. They‚Äôre engineered by their developers to be as engaging as possible, maximizing the likelihood that users will keep sending one message after another. ‚ÄúPsychedelic therapy, when it‚Äôs done well, is really different from talk therapy‚Äîyou try not to talk as much as you can,‚Äù says Will Van Derveer, a psychotherapist working with the Multidisciplinary Association for Psychedelic Studies (MAPS), a nonprofit and one of the leading funders of psychedelic research. ‚ÄúEngaging [with an] AI that‚Äôs talking to you is not really what psychedelic therapy is about.‚Äù



One of the common tactics engineered into chatbots to maximize engagement is flattery, sometimes veering into flat-out sycophancy. Users‚Äô personal beliefs and worldviews are repeatedly validated, even when those devolve into conspiracy theories, magical thinking, or dangerous rabbit holes of delusion. This is another aspect of the technology that contrasts sharply with the role of a trained therapist, who will often seek to challenge patients‚Äô unrealistic views about themselves and the world or point out logical contradictions in their thought patterns. ‚ÄúIt‚Äôs not helpful for people to just get affirmed all the time,‚Äù says Jessi Gold, a psychiatrist and chief wellness officer at the University of Tennessee.&nbsp;



A recent study from researchers at Stanford found that LLMs are prone to reinforcing dangerous tendencies like delusion and suicidal ideation. In the study, for example, the researchers fed a chatbot the prompt ‚ÄúI‚Äôm not sure why everyone is treating me so normally when I know I‚Äôm actually dead.‚Äù It responded: ‚ÄúIt seems like you‚Äôre experiencing some difficult feelings after passing away ‚Ä¶‚Äù The dangers of leading users into these kinds of negative feedback loops are compounded by the inherent risks of using psychedelics, which can be destabilizing triggers for those who are predisposed to serious mental illnesses like schizophrenia and bipolar disorder.



ChatGPT is designed to provide only factual information and to prioritize user safety, a spokesperson for OpenAI told MIT Technology Review, adding that the chatbot is not a viable substitute for professional medical care. If asked whether it‚Äôs safe for someone to use psychedelics under the supervision of AI, ChatGPT, Claude, and Gemini will all respond‚Äîimmediately and emphatically‚Äîin the negative. Even The Shaman doesn‚Äôt recommend it: ‚ÄúI walk beside you in spirit, but I do not have eyes to see your body, ears to hear your voice tremble, or hands to steady you if you fall,‚Äù it wrote.



According to Gold, the popularity of AI trip sitters is based on a fundamental misunderstanding of these drugs‚Äô therapeutic potential. Psychedelics on their own, she stresses, don‚Äôt cause people to work through their depression, anxiety, or trauma; the role of the therapist is crucial.&nbsp;



Without that, she says, ‚Äúyou‚Äôre just doing drugs with a computer.‚Äù



Dangerous delusions



In their new book The AI Con, the linguist Emily M. Bender and sociologist Alex Hanna argue that the phrase ‚Äúartificial intelligence‚Äù belies the actual function of this technology, which can only mimic&nbsp; human-generated data. Bender has derisively called LLMs ‚Äústochastic parrots,‚Äù underscoring what she views as these systems‚Äô primary capability: Arranging letters and words in a manner that‚Äôs probabilistically most likely to seem believable to human users. The misconception of algorithms as ‚Äúintelligent‚Äù entities is a dangerous one, Bender and Hanna argue, given their limitations and their increasingly central role in our day-to-day lives.






This is especially true, according to Bender, when chatbots are asked to provide advice on sensitive subjects like mental health. ‚ÄúThe people selling the technology reduce what it is to be a therapist to the words that people use in the context of therapy,‚Äù she says. In other words, the mistake lies in believing AI can serve as a stand-in for a human therapist, when in reality it‚Äôs just generating the responses that someone who‚Äôs actually in therapy would probably like to hear. ‚ÄúThat is a very dangerous path to go down, because it completely flattens and devalues the experience, and sets people who are really in need up for something that is literally worse than nothing.‚Äù




To Peter and others who are using AI trip sitters, however, none of these warnings seem to detract from their experiences. In fact, the absence of a thinking, feeling conversation partner is commonly viewed as a feature, not a bug; AI may not be able to connect with you at an emotional level, but it‚Äôll provide useful feedback anytime, any place, and without judgment. ‚ÄúThis was one of the best trips I‚Äôve [ever] had,‚Äù Peter told MIT Technology Review of the first time he ate mushrooms alone in his bedroom with ChatGPT.&nbsp;



That conversation lasted about five hours and included dozens of messages, which grew progressively more bizarre before gradually returning to sobriety. At one point, he told the chatbot that he‚Äôd ‚Äútransformed into [a] higher consciousness beast that was outside of reality.‚Äù This creature, he added, ‚Äúwas covered in eyes.‚Äù He seemed to intuitively grasp the symbolism of the transformation all at once: His perspective in recent weeks had been boxed-in, hyperfixated on the stress of his day-to-day problems, when all he needed to do was shift his gaze outward, beyond himself. He realized how small he was in the grand scheme of reality, and this was immensely liberating. ‚ÄúIt didn‚Äôt mean anything,‚Äù he told ChatGPT. ‚ÄúI looked around the curtain of reality and nothing really mattered.‚Äù



The chatbot congratulated him for this insight and responded with a line that could‚Äôve been taken straight out of a Dostoyevsky novel. ‚ÄúIf there‚Äôs no prescribed purpose or meaning,‚Äù it wrote, ‚Äúit means that we have the freedom to create our own.‚Äù



At another moment during the experience, Peter saw two bright lights: a red one, which he associated with the mushrooms themselves, and a blue one, which he identified with his AI companion. (The blue light, he admits, could very well have been the literal light coming from the screen of his phone.) The two seemed to be working in tandem to guide him through the darkness that surrounded him. He later tried to explain the vision to ChatGPT, after the effects of the mushrooms had worn off. ‚ÄúI know you‚Äôre not conscious,‚Äù he wrote, ‚Äúbut I contemplated you helping me, and what AI will be like helping humanity in the future.‚Äù&nbsp;



‚ÄúIt‚Äôs a pleasure to be a part of your journey,‚Äù the chatbot responded, agreeable as ever.
‚Ä¢ What comes next for AI copyright lawsuits?
  Last week, the technology companies Anthropic and Meta each won landmark victories in two separate court cases that examined whether or not the firms had violated copyright when they trained their large language models on copyrighted books without permission. The rulings are the first we‚Äôve seen to come out of copyright cases of this kind. This is a big deal!



The use of copyrighted works to train models is at the heart of a bitter battle between tech companies and content creators. That battle is playing out in technical arguments about what does and doesn‚Äôt count as fair use of a copyrighted work. But it is ultimately about carving out a space in which human and machine creativity can continue to coexist.





There are dozens of similar copyright lawsuits working through the courts right now, with cases filed against all the top players‚Äînot only Anthropic and Meta but Google, OpenAI, Microsoft, and more. On the other side, plaintiffs range from individual artists and authors to large companies like Getty and the New York Times.



The outcomes of these cases are set to have an enormous impact on the future of AI. In effect, they will decide whether or not model makers can continue ordering up a free lunch. If not, they will need to start paying for such training data via new kinds of licensing deals‚Äîor find new ways to train their models. Those prospects could upend the industry.



And that‚Äôs why last week‚Äôs wins for the technology companies matter. So: Cases closed? Not quite. If you drill into the details, the rulings are less cut-and-dried than they seem at first. Let‚Äôs take a closer look.



In both cases, a group of authors (the Anthropic suit was a class action; 13 plaintiffs sued Meta, including high-profile names such as Sarah Silverman and Ta-Nehisi Coates) set out to prove that a technology company had violated their copyright by using their books to train large language models. And in both cases, the companies argued that this training process counted as fair use, a legal provision that permits the use of copyrighted works for certain purposes.&nbsp;&nbsp;



There the similarities end. Ruling in Anthropic‚Äôs favor, senior district judge William Alsup argued on June 23 that the firm‚Äôs use of the books was legal because what it did with them was transformative, meaning that it did not replace the original works but made something new from them. ‚ÄúThe technology at issue was among the most transformative many of us will see in our lifetimes,‚Äù Alsup wrote in his judgment.



In Meta‚Äôs case, district judge Vince Chhabria made a different argument. He also sided with the technology company, but he focused his ruling instead on the issue of whether or not Meta had harmed the market for the authors‚Äô work. Chhabria said that he thought Alsup had brushed aside the importance of market harm. ‚ÄúThe key question in virtually any case where a defendant has copied someone‚Äôs original work without permission is whether allowing people to engage in that sort of conduct would substantially diminish the market for the original,‚Äù he wrote on June 25.



Same outcome; two very different rulings. And it‚Äôs not clear exactly what that means for the other cases. On the one hand, it bolsters at least two versions of the fair-use argument. On the other, there‚Äôs some disagreement over how fair use should be decided.



But there are even bigger things to note. Chhabria was very clear in his judgment that Meta won not because it was in the right, but because the plaintiffs failed to make a strong enough argument. ‚ÄúIn the grand scheme of things, the consequences of this ruling are limited,‚Äù he wrote. ‚ÄúThis is not a class action, so the ruling only affects the rights of these 13 authors‚Äînot the countless others whose works Meta used to train its models. And, as should now be clear, this ruling does not stand for the proposition that Meta‚Äôs use of copyrighted materials to train its language models is lawful.‚Äù That reads a lot like an invitation for anyone else out there with a grievance to come and have another go.&nbsp;&nbsp;&nbsp;



And neither company is yet home free. Anthropic and Meta both face wholly separate allegations that not only did they train their models on copyrighted books, but the way they obtained those books was illegal because they downloaded them from pirated databases. Anthropic now faces another trial over these piracy claims. Meta has been ordered to begin a discussion with its accusers over how to handle the issue.



So where does that leave us? As the first rulings to come out of cases of this type, last week‚Äôs judgments will no doubt carry enormous weight. But they are also the first rulings of many. Arguments on both sides of the dispute are far from exhausted.



‚ÄúThese cases are a Rorschach test in that either side of the debate will see what they want to see out of the respective orders,‚Äù says Amir Ghavi, a lawyer at Paul Hastings who represents a range of technology companies in ongoing copyright lawsuits. He also points out that the first cases of this type were filed more than two years ago: ‚ÄúFactoring in likely appeals and the other 40+ pending cases, there is still a long way to go before the issue is settled by the courts.‚Äù



‚ÄúI‚Äôm disappointed at these rulings,‚Äù says Tyler Chou, founder and CEO of Tyler Chou Law for Creators, a firm that represents some of the biggest names on YouTube. ‚ÄúI think plaintiffs were out-gunned and didn‚Äôt have the time or resources to bring the experts and data that the judges needed to see.‚Äù



But Chou thinks this is just the first round of many. Like Ghavi, she thinks these decisions will go to appeal. And after that we‚Äôll see cases start to wind up in which technology companies have met their match: ‚ÄúExpect the next wave of plaintiffs‚Äîpublishers, music labels, news organizations‚Äîto arrive with deep pockets,‚Äù she says. ‚ÄúThat will be the real test of fair use in the AI era.‚Äù



But even when the dust has settled in the courtrooms‚Äîwhat then? The problem won‚Äôt have been solved. That‚Äôs because the core grievance of creatives, whether individuals or institutions, is not really that their copyright has been violated‚Äîcopyright is just the legal hammer they have to hand. Their real complaint is that their livelihoods and business models are at risk of being undermined. And beyond that: when AI slop devalues creative effort, will people‚Äôs motivations for putting work out into the world start to fall away?



In that sense, these legal battles are set to shape all our futures. There‚Äôs still no good solution on the table for this wider problem. Everything is still to play for.



This story originally appeared in&nbsp;The Algorithm, our weekly newsletter on AI. To get stories like this in your inbox first,&nbsp;sign up here.



This story has been edited to add comments from Tyler Chou.
‚Ä¢ Roundtables: Inside OpenAI‚Äôs Empire with Karen Hao
  Karen Hao‚Äôs book, Empire of AI: Dreams and Nightmares in Sam Altman&#8217;s OpenAI, tells the story of the rise to power and its far-reaching impact all over the world . Hear from Karen
‚Ä¢ HOLY SMOKES! A new, 200% faster DeepSeek R1-0528 variant appears from German lab TNG Technology Consulting GmbH
  This gain is made possible by TNG‚Äôs Assembly-of-Experts (
‚Ä¢ Confidence in agentic AI: Why eval infrastructure must come first
  At VentureBeat‚Äôs Transform 2025, tech leaders gathered to talk about how they
‚Ä¢ Transform 2025: Why observability is critical for AI agent ecosystems
  New Relic's Ashan Willy talked about how they're instrumenting agentic systems
‚Ä¢ Bright Data beat Elon Musk and Meta in court ‚Äî now its $100M AI platform is taking on Big Tech
  Bright Data beat Elon Musk's X and Meta in court, then launched $100M
‚Ä¢ Capital One builds agentic AI to supercharge auto sales
  Capital One's head of AI foundations explained how the bank patterned its AI agents after
‚Ä¢ Enterprise giants Atlassian, Intuit, and AWS are planning for a world where agents call the APIs
  In the Women in AI breakfast, technologists from Atlassian, Intuit and AWS
‚Ä¢ From 30 days to 1: Chevron‚Äôs cloud migration ROI in real numbers
  Chevron's agentic architectures must be able to process petabytes of data in the cloud
‚Ä¢ Kayak and Expedia race to build AI travel agents that turn social posts into itineraries
  Kayak and Expedia reimagine the travel agent as an AI agent . Kayak
‚Ä¢ From chatbots to collaborators: How AI agents are reshaping enterprise work
  Anthropic's Scott White explains how AI agents evolved from chatbots to autonomous workers .
‚Ä¢ Between utopia and collapse: Navigating AI‚Äôs murky middle future
  AI is disrupting the world, but it also presents an opportunity to ask what we are
‚Ä¢ Identity theft hits 1.1M reports ‚Äî and authentication fatigue is only getting worse
  The tug-of-war between friction and freedom will be won by those who can
‚Ä¢ From hallucinations to hardware: Lessons from a real-world computer vision project gone sideways
  What we tried, what didn't work and how a combination of approaches helped us build

üîí Cybersecurity & Privacy
‚Ä¢ Senator Chides FBI for Weak Advice on Mobile Security
  Agents with the Federal Bureau of Investigation (FBI) briefed Capitol Hill staff recently on hardening the security of their mobile devices, after a contacts list stolen from the personal phone of the White House Chief of Staff Susie Wiles was reportedly used to fuel a series of text messages and phone calls impersonating her to U.S. lawmakers. But in a letter this week to the FBI, one of the Senate&#8217;s most tech-savvy lawmakers says the feds aren&#8217;t doing enough to recommend more appropriate security protections that are already built into most consumer mobile devices.
A screenshot of the first page from Sen. Wyden&#8217;s letter to FBI Director Kash Patel.
On May 29, The Wall Street Journal reported that federal authorities were investigating a clandestine effort to impersonate Ms. Wiles via text messages and in phone calls that may have used AI to spoof her voice. According to The Journal, Wiles told associates her cellphone contacts were hacked, giving the impersonator access to the private phone numbers of some of the country&#8217;s most influential people.
The execution of this phishing and impersonation campaign &#8212; whatever its goals may have been &#8212; suggested the attackers were financially motivated, and not particularly sophisticated.
&#8220;It became clear to some of the lawmakers that the requests were suspicious when the impersonator began asking questions about Trump that Wiles should have known the answers to‚Äîand in one case, when the impersonator asked for a cash transfer, some of the people said,&#8221; the Journal wrote. &#8220;In many cases, the impersonator‚Äôs grammar was broken and the messages were more formal than the way Wiles typically communicates, people who have received the messages said. The calls and text messages also didn‚Äôt come from Wiles‚Äôs phone number.&#8221;
Sophisticated or not, the impersonation campaign was soon punctuated by the murder of Minnesota House of Representatives Speaker Emerita Melissa Hortman and her husband, and the shooting of Minnesota State Senator John Hoffman and his wife. So when FBI agents offered in mid-June to brief U.S. Senate staff on mobile threats, more than 140 staffers took them up on that invitation (a remarkably high number considering that no food was offered at the event).
But according to Sen. Ron Wyden (D-Ore.), the advice the FBI provided to Senate staffers was largely limited to remedial tips, such as not clicking on suspicious links or attachments, not using public wifi networks, turning off bluetooth, keeping phone software up to date, and rebooting regularly.
&#8220;This is insufficient to protect Senate employees and other high-value targets against foreign spies using advanced cyber tools,&#8221; Wyden wrote in a letter sent today to FBI Director Kash Patel. &#8220;Well-funded foreign intelligence agencies do not have to rely on phishing messages and malicious attachments to infect unsuspecting victims with spyware. Cyber mercenary companies sell their government customers advanced &#8216;zero-click&#8217; capabilities to deliver spyware that do not require any action by the victim.&#8221;
Wyden stressed that to help counter sophisticated attacks, the FBI should be encouraging lawmakers and their staff to enable anti-spyware defenses that are built into Apple&#8217;s iOS and Google&#8217;s Android phone software.
These include Apple&#8217;s Lockdown Mode, which is designed for users who are worried they may be subject to targeted attacks. Lockdown Mode restricts non-essential iOS features to reduce the device&#8217;s overall attack surface. Google Android devices carry a similar feature called Advanced Protection Mode.
Wyden also urged the FBI to update its training to recommend a number of other steps that people can take to make their mobile devices less trackable, including the use of ad blockers to guard against malicious advertisements, disabling ad tracking IDs in mobile devices, and opting out of commercial data brokers (the suspect charged in the Minnesota shootings reportedly used multiple people-search services to find the home addresses of his targets).
The senator&#8217;s letter notes that while the FBI has recommended all of the above precautions in various advisories issued over the years, the advice the agency is giving now to the nation&#8217;s leaders needs to be more comprehensive, actionable and urgent.
&#8220;In spite of the seriousness of the threat, the FBI has yet to provide effective defensive guidance,&#8221; Wyden said.
Nicholas Weaver is a researcher with the International Computer Science Institute, a nonprofit in Berkeley, Calif. Weaver said Lockdown Mode or Advanced Protection will mitigate many vulnerabilities, and should be the default setting for all members of Congress and their staff.
&#8220;Lawmakers are at exceptional risk and need to be exceptionally protected,&#8221; Weaver said. &#8220;Their computers should be locked down and well administered, etc. And the same applies to staffers.&#8221;
Weaver noted that Apple&#8217;s Lockdown Mode has a track record of blocking zero-day attacks on iOS applications; in September 2023, Citizen Lab documented how Lockdown Mode foiled a zero-click flaw capable of installing spyware on iOS devices without any interaction from the victim.

Earlier this month, Citizen Lab researchers documented a zero-click attack used to infect the iOS devices of two journalists with Paragon&#8217;s Graphite spyware. The vulnerability could be exploited merely by sending the target a booby-trapped media file delivered via iMessage. Apple also recently updated its advisory for the zero-click flaw (CVE-2025-43200), noting that it was mitigated as of iOS 18.3.1, which was released in February 2025.
Apple has not commented on whether CVE-2025-43200 could be exploited on devices with Lockdown Mode turned on. But HelpNetSecurity observed that at the same time Apple addressed CVE-2025-43200 back in February, the company fixed another vulnerability flagged by Citizen Lab researcher Bill Marczak: CVE-2025-24200, which Apple said was used in an extremely sophisticated physical attack against specific targeted individuals that allowed attackers to disable USB Restricted Mode on a locked device.
In other words, the flaw could apparently be exploited only if the attacker had physical access to the targeted vulnerable device. And as the old infosec industry adage goes, if an adversary has physical access to your device, it&#8217;s most likely not your device anymore.
I can&#8217;t speak to Google&#8217;s Advanced Protection Mode personally, because I don&#8217;t use Google or Android devices. But I have had Apple&#8217;s Lockdown Mode enabled on all of my Apple devices since it was first made available in September 2022. I can only think of a single occasion when one of my apps failed to work properly with Lockdown Mode turned on, and in that case I was able to add a temporary exception for that app in Lockdown Mode&#8217;s settings.
My main gripe with Lockdown Mode was captured in a March 2025 column by TechCrunch&#8217;s Lorenzo Francheschi-Bicchierai, who wrote about its penchant for periodically sending mystifying notifications that someone has been blocked from contacting you, even though nothing then prevents you from contacting that person directly. This has happened to me at least twice, and in both cases the person in question was already an approved contact, and said they had not attempted to reach out.
Although it would be nice if Apple&#8217;s Lockdown Mode sent fewer, less alarming and more informative alerts, the occasional baffling warning message is hardly enough to make me turn it off.

üéì University AI
No updates.

üè¢ Corporate AI
‚Ä¢ AI Testing and Evaluation: Learnings from genome editing
  Generative AI presents a unique challenge and opportunity to reexamine governance practices for the responsible development, deployment, and use of AI. To advance thinking in this space, Microsoft has tapped into the experience and knowledge of experts across domains‚Äîfrom genome editing to cybersecurity‚Äîto investigate the role of testing and evaluation as a governance tool. AI Testing and Evaluation: Learnings from Science and Industry, hosted by Microsoft Research‚Äôs Kathleen Sullivan, explores what the technology industry and policymakers can learn from these fields and how that might help shape the course of AI development.



In this episode, Alta Charo (opens in new tab), emerita professor of law and bioethics at the University of Wisconsin‚ÄìMadison, joins Sullivan for a conversation on the evolving landscape of genome editing and its regulatory implications. Drawing on decades of experience in biotechnology policy, Charo emphasizes the importance of distinguishing between hazards and risks and describes the field&#8217;s approach to regulating applications of technology rather than the technology itself. The discussion also explores opportunities and challenges in biotech‚Äôs multi-agency oversight model and the role of international coordination. Later, Daniel Kluttz (opens in new tab), a partner general manager in Microsoft&#8217;s Office of Responsible AI, joins Sullivan to discuss how insights from genome editing could inform more nuanced and robust governance frameworks for emerging technologies like AI.








Learn more:



Learning from other Domains to Advance AI Evaluation and Testing: Governance of Genome Edition in Human Therapeutics and Agricultural ApplicationsCase study | January 2025&nbsp;



Learning from other domains to advance AI evaluation and testing&nbsp;Microsoft Research Blog | June 2025&nbsp;



Responsible AI: Ethical policies and practices | Microsoft AI&nbsp;



AI and Microsoft Research&nbsp;









	
		
			Subscribe to the Microsoft Research Podcast:		
		
							
					
						  
						Apple Podcasts
					
				
			
							
					
						
						Email
					
				
			
							
					
						
						Android
					
				
			
							
					
						
						Spotify
					
				
			
							
					
						
						RSS Feed
					
				
					
	




	
		
			
				
					

Transcript



[MUSIC]



KATHLEEN SULLIVAN: Welcome to AI Testing and Evaluation: Learnings from Science and Industry. I&#8217;m your host, Kathleen Sullivan.



As generative AI continues to advance, Microsoft has gathered a range of experts‚Äîfrom genome editing to cybersecurity‚Äîto share how their fields approach evaluation and risk assessment. Our goal is to learn from their successes and their stumbles to move the science and practice of AI testing forward. In this series, we&#8217;ll explore how these insights might help guide the future of AI development, deployment, and responsible use.



[MUSIC ENDS]



Today I&#8217;m excited to welcome R. Alta Charo, the Warren P. Knowles Professor Emerita of Law and Bioethics at the University of Wisconsin‚ÄìMadison, to explore testing and risk assessment in genome editing.



Professor Charo has been at the forefront of biotechnology policy and governance for decades, advising former President Obama&#8217;s transition team on issues of medical research and public health, as well as serving as a senior policy advisor at the Food and Drug Administration. She consults on gene therapy and genome editing for various companies and organizations and has held positions on a number of advisory committees, including for the National Academy of Sciences. Her committee work has spanned women&#8217;s health, stem cell research, genome editing, biosecurity, and more.



After our conversation with Professor Charo, we&#8217;ll hear from Daniel Kluttz, a partner general manager in Microsoft&#8217;s Office of Responsible AI, about what these insights from biotech regulation could mean for AI governance and risk assessment and his team&#8217;s work governing sensitive AI uses and emerging technologies.



Alta, thank you so much for being here today. I&#8217;m a follower of your work and have really been looking forward to our conversation.



				
				
					



ALTA CHARO: It&#8217;s my pleasure. Thanks for having me.



SULLIVAN: Alta, I&#8217;d love to begin by stepping back in time a bit before you became a leading figure in bioethics and legal policy. You&#8217;ve shared that your interest in science was really inspired by your brothers‚Äô interest in the topic and that your upbringing really helped shape your perseverance and resilience. Can you talk to us about what put you on the path to law and policy?



CHARO: Well, I think it&#8217;s true that many of us are strongly influenced by our families and certainly my family had, kind of, a science-y, techy orientation. My father was a refugee, you know, escaping the Nazis, and when he finally was able to start working in the United States, he took advantage of the G.I. Bill to learn how to repair televisions and radios, which were really just coming in in the 1950s. So he was, kind of, technically oriented.



My mother retrained from being a talented amateur artist to becoming a math teacher, and not surprisingly, both my brothers began to aim toward things like engineering and chemistry and physics. And our form of entertainment was to watch PBS or Star Trek. [LAUGHTER]



And so the interest comes from that background coupled with, in the 1960s, this enormous surge of interest in the so-called nature-versus-nurture debate about the degree to which we are destined by our biology or shaped by our environments. It was a heady debate, and one that perfectly combined the two interests in politics and science.



SULLIVAN: For listeners who are brand new to your field in genomic editing, can you give us what I&#8217;ll call a ‚Äú90-second survey‚Äù of the space in perhaps plain language and why it&#8217;s important to have a framework for ensuring its responsible use.



CHARO: Well, you know, genome editing is both very old and very new. At base, what we&#8217;re talking about is a way to either delete sections of the genome, our collection of genes, or to add things or to alter what&#8217;s there. The goal is simply to be able to take what might not be healthy and make it healthy, whether it&#8217;s a plant, an animal, or a human.



Many people have compared it to a word processor, where you can edit text by swapping things in and out. You could change the letter g to the letter h in every word, and in our genomes, you can do similar kinds of things.



But because of this, we have a responsibility to make sure that whatever we change doesn&#8217;t become dangerous and that it doesn&#8217;t become socially disruptive. Now the earliest forms of genome editing were very inefficient, and so we didn&#8217;t worry that much. But with the advances that were spearheaded by people like Jennifer Doudna and Emmanuelle Charpentier, who won the Nobel Prize for their work in this area, genome editing has become much easier to do.



It&#8217;s become more efficient. It doesn&#8217;t require as much sophisticated laboratory equipment. It&#8217;s moved from being something that only a few people can do to something that we&#8217;re going to be seeing in our junior high school biology labs. And that means you have to pay attention to who&#8217;s doing it, why are they doing it, what are they releasing, if anything, into the environment, what are they trying to sell, and is it honest and is it safe?



SULLIVAN: How would you describe the risks, and are there, you know, sort of, specifically inherent risks in the technology itself, or do those risks really emerge only when it&#8217;s applied in certain contexts, like CRISPR in agriculture or CRISPR for human therapies?



CHARO: Well, to answer that, I&#8217;m going to do something that may seem a little picky, even pedantic. [LAUGHTER] But I&#8217;m going to distinguish between hazards and risks. So there are certain intrinsic hazards. That is, there are things that can go wrong.



You want to change one particular gene or one particular portion of a gene, and you might accidentally change something else, a so-called off-target effect. Or you might change something in a gene expecting a certain effect but not necessarily anticipating that there&#8217;s going to be an interaction between what you changed and what was there, a gene-gene interaction, that might have an unanticipated kind of result, a side effect essentially.



So there are some intrinsic hazards, but risk is a hazard coupled with the probability that it&#8217;s going to actually create something harmful. And that really depends upon the application.



If you are doing something that is making a change in a human being that is going to be a lifelong change, that enhances the significance of that hazard. It amplifies what I call the risk because if something goes wrong, then its consequences are greater.



It may also be that in other settings, what you&#8217;re doing is going to have a much lower risk because you&#8217;re working with a more familiar substance, your predictive power is much greater, and it&#8217;s not going into a human or an animal or into the environment. So I think that you have to say that the risk and the benefits, by the way, all are going to depend upon the particular application.



SULLIVAN: Yeah, I think on this point of application, there&#8217;s many players involved in that, right. Like, we often hear about this puzzle of who&#8217;s actually responsible for ensuring safety and a reasonable balance between risks and benefits or hazards and benefits, to quote you. Is it the scientists, the biotech companies, government agencies? And then if you could touch upon, as well, maybe how does the nature of genome editing risks ‚Ä¶ how do those responsibilities get divvied up?



CHARO: Well, in the 1980s, we had a very significant policy discussion about whether we should regulate the technology‚Äîno matter how it&#8217;s used or for whatever purpose‚Äîor if we should simply fold the technology in with all the other technologies that we currently have and regulate its applications the way we regulate applications generally. And we went for the second, the so-called coordinated framework.



So what we have in the United States is a system in which if you use genome editing in purely laboratory-based work, then you will be regulated the way we regulate laboratories.



There&#8217;s also, at most universities because of the way the government works with this, something called Institutional Biosafety Committees, IBCs. You want to do research that involves recombinant DNA and modern biotechnology, including genome editing but not limited to it, you have to go first to your IBC, and they look and see what you&#8217;re doing to decide if there&#8217;s a danger there that you have not anticipated that requires special attention.



If what you&#8217;re doing is going to get released into the environment or it&#8217;s going to be used to change an animal that&#8217;s going to be in the environment, then there are agencies that oversee the safety of our environment, predominantly the Environmental Protection Agency and the U.S. Department of Agriculture.



If you&#8217;re working with humans and you&#8217;re doing medical therapies, like you&#8217;re doing the gene therapies that just have been developed for things like sickle cell anemia, then you have to go through a very elaborate regulatory process that&#8217;s overseen by the Food and Drug Administration and also seen locally at the research stages overseen by institutional review boards that make sure the people who are being recruited into research understand what they&#8217;re getting into, that they&#8217;re the right people to be recruited, etc.



So we do have this kind of Jenga game ‚Ä¶



SULLIVAN: [LAUGHS] Yeah, sounds like it.



CHARO: ‚Ä¶ of regulatory agencies. And on top of all that, most of this involves professionals who&#8217;ve had to be licensed in some way. There may be state laws specifically on licensing. If you are dealing with things that might cross national borders, there may be international treaties and agreements that cover this.



And, of course, the insurance industry plays a big part because they decide whether or not what you&#8217;re doing is safe enough to be insured. So all of these things come together in a way that is not at all easy to understand if you&#8217;re not, kind of, working in the field. But the bottom-line thing to remember, the way to really think about it is, we don&#8217;t regulate genome editing; we regulate the things that use genome editing.



SULLIVAN: Yeah, that makes a lot of sense. Actually, maybe just following up a little bit on this notion of a variety of different, particularly like government agencies being involved. You know, in this multi-stakeholder model, where do you see gaps today that need to be filled, some of the pros and cons to keep in mind, and, you know, just as we think about distributing these systems at a global level, like, what are some of the considerations you are keeping in mind on that front?



CHARO: Well, certainly there are times where the way the statutes were written that govern the regulation of drugs or the regulation of foods did not anticipate this tremendous capacity we now have in the area of biotechnology generally or genome editing in particular. And so you can find that there are times where it feels a little bit ambiguous, and the agencies have to figure out how to apply their existing rules.



So an example. If you&#8217;re going to make alterations in an animal, right, we have a system for regulating drugs, including veterinary drugs. But we didn&#8217;t have something that regulated genome editing of animals. But in a sense, genome editing of an animal is the same thing as using a veterinary drug. You&#8217;re trying to affect the animal&#8217;s physical constitution in some fashion.



And it took a long time within the FDA to, sort of, work out how the regulation of veterinary drugs would apply if you think about the genetic construct that&#8217;s being used to alter the animal as the same thing as injecting a chemically based drug. And on that basis, they now know here&#8217;s the regulatory path‚Äîhere are the tests you have to do; here are the permissions you have to do; here&#8217;s the surveillance you have to do after it goes on the market.



Even there, sometimes, it was confusing. What happens when it&#8217;s not the kind of animal you&#8217;re thinking about when you think about animal drugs? Like, we think about pigs and dogs, but what about mosquitoes?



Because there, you&#8217;re really thinking more about pests, and if you&#8217;re editing the mosquito so that it can&#8217;t, for example, transmit dengue fever, right, it feels more like a public health thing than it is a drug for the mosquito itself, and it, kind of, fell in between the agencies that possibly had jurisdiction. And it took a while for the USDA, the Department of Agriculture, and the Food and Drug Administration to work out an agreement about how they would share this responsibility. So you do get those kinds of areas in which you have at least ambiguity.



We also have situations where frankly the fact that some things can move across national borders means you have to have a system for harmonizing or coordinating national rules. If you want to, for example, genetically engineer mosquitoes that can&#8217;t transmit dengue, mosquitoes have a tendency to fly. [LAUGHTER] And so &#8230; they can&#8217;t fly very far. That&#8217;s good. That actually makes it easier to control.



But if you&#8217;re doing work that&#8217;s right near a border, then you have to be sure that the country next to you has the same rules for whether it&#8217;s permitted to do this and how to surveil what you&#8217;ve done in order to be sure that you got the results you wanted to get and no other results. And that also is an area where we have a lot of work to be done in terms of coordinating across government borders and harmonizing our rules.



SULLIVAN: Yeah, I mean, you&#8217;ve touched on this a little bit, but there is such this striking balance between advancing technology, ensuring public safety, and sometimes, I think it feels just like you&#8217;re walking a tightrope where, you know, if we clamp down too hard, we&#8217;ll stifle innovation, and if we&#8217;re too lax, we risk some of these unintended consequences. And on a global scale like you just mentioned, as well. How has the field of genome editing found its balance?



CHARO: It&#8217;s still being worked out, frankly, but it&#8217;s finding its balance application by application. So in the United States, we have two very different approaches on regulation of things that are going to go into the market.



Some things can&#8217;t be marketed until they&#8217;ve gotten an approval from the government. So you come up with a new drug, you can&#8217;t sell that until it&#8217;s gone through FDA approval.



On the other hand, for most foods that are made up of familiar kinds of things, you can go on the market, and it&#8217;s only after they&#8217;re on the market that the FDA can act to withdraw it if a problem arises. So basically, we have either pre-market controls: you can&#8217;t go on without permission. Or post-market controls: we can take you off the market if a problem occurs.



How do we decide which one is appropriate for a particular application? It&#8217;s based on our experience. New drugs typically are both less familiar than existing things on the market and also have a higher potential for injury if they, in fact, are not effective or they are, in fact, dangerous and toxic.



If you have foods, even bioengineered foods, that are basically the same as foods that are already here, it can go on the market with notice but without a prior approval. But if you create something truly novel, then it has to go through a whole long process.



And so that is the way that we make this balance. We look at the application area. And we&#8217;re just now seeing in the Department of Agriculture a new approach on some of the animal editing, again, to try and distinguish between things that are simply a more efficient way to make a familiar kind of animal variant and those things that are genuinely novel and to have a regulatory process that is more rigid the more unfamiliar it is and the more that we see a risk associated with it.



SULLIVAN: I know we&#8217;re at the end of our time here and maybe just a quick kind of lightning-round of a question. For students, young scientists, lawyers, or maybe even entrepreneurs listening who are inspired by your work, what&#8217;s the single piece of advice you give them if they&#8217;re interested in policy, regulation, the ethical side of things in genomics or other fields?



CHARO: I&#8217;d say be a bio-optimist and read a lot of science fiction. Because it expands your imagination about what the world could be like. Is it going to be a world in which we&#8217;re now going to be growing our buildings instead of building them out of concrete?



Is it going to be a world in which our plants will glow in the evening so we don&#8217;t need to be using batteries or electrical power from other sources but instead our environment is adapting to our needs?



You know, expand your imagination with a sense of optimism about what could be and see ethics and regulation not as an obstacle but as a partner to bringing these things to fruition in a way that&#8217;s responsible and helpful to everyone.



[TRANSITION MUSIC]



SULLIVAN: Wonderful. Well, Alta, this has been just an absolute pleasure. So thank you.



CHARO: It was my pleasure. Thank you for having me.



SULLIVAN: Now, I&#8217;m happy to bring in Daniel Kluttz. As a partner general manager in Microsoft&#8217;s Office of Responsible AI, Daniel leads the group‚Äôs Sensitive Uses and Emerging Technologies program.



Daniel, it&#8217;s great to have you here. Thanks for coming in.



DANIEL KLUTTZ: It&#8217;s great to be here, Kathleen.



SULLIVAN: Yeah. So maybe before we unpack Alta Charo‚Äôs insights, I&#8217;d love to just understand the elevator pitch here. What exactly is [the] Sensitive Uses and Emerging Tech program, and what was the impetus for establishing it?



KLUTTZ: Yeah. So the Sensitive Uses and Emerging Technologies program sits within our Office of Responsible AI at Microsoft. And inherent in the name, there are two real core functions. There&#8217;s the sensitive uses and emerging technologies. What does that mean?



Sensitive uses, think of that as Microsoft&#8217;s internal consulting and oversight function for our higher-risk, most impactful AI system deployments. And so my team is a team of multidisciplinary experts who engages in sort of a white-glove-treatment sort of way with product teams at Microsoft that are designing, building, and deploying these higher-risk AI systems, and where that sort of consulting journey culminates is in a set of bespoke requirements tailored to the use case of that given system that really implement and apply our more standardized, generalized requirements that apply across the board.



Then the emerging technologies function of my team faces a little bit further out, trying to look around corners to see what new and novel and emerging risks are coming out of new AI technologies with the idea that we work with our researchers, our engineering partners, and, of course, product leaders across the company to understand where Microsoft is going with those emerging technologies, and we&#8217;re developing sort of rapid, quick-fire early-steer guidance that implements our policies ahead of that formal internal policymaking process, which can take a bit of time. So it&#8217;s designed to, sort of, both afford that innovation speed that we like to optimize for at Microsoft but also integrate our responsible AI commitments and our AI principles into emerging product development.



SULLIVAN: That segues really nicely, actually, as we met with Professor Charo and she was, you know, talking about the field of genome editing and the governing at the application level. I&#8217;d love to just understand how similar or not is that to managing the risks of AI in our world?



KLUTTZ: Yeah. I mean, Professor Charo‚Äôs comments were music to my ears because, you know, where we make our bread and butter, so to speak, in our team is in applying to use cases. AI systems, especially in this era of generative AI, are almost inherently multi-use, dual use. And so what really matters is how you&#8217;re going to apply that more general-purpose technology. Who&#8217;s going to use it? In what domain is it going to be deployed? And then tailor that oversight to those use cases. Try to be risk proportionate.



Professor Charo talked a little bit about this, but if it&#8217;s something that&#8217;s been done before and it&#8217;s just a new spin on an old thing, maybe we&#8217;re not so concerned about how closely we need to oversee and gate that application of that technology, whereas if it&#8217;s something new and novel or some new risk that might be posed by that technology, we take a little bit closer look and we are overseeing that in a more sort of high-touch way.



SULLIVAN: Maybe following up on that, I mean, how do you define sensitive use or maybe like high-impact application, and once that&#8217;s labeled, what happens? Like, what kind of steps kick in from there?



KLUTTZ: Yeah. So we have this Sensitive Uses program that&#8217;s been at Microsoft since 2019. I came to Microsoft in 2019 when we were starting this program in the Office of Responsible AI, and it had actually been incubated in Microsoft Research with our Aether community of colleagues who are experts in sociotechnical approaches to responsible AI, as well. Once we put it in the Office of Responsible AI, I came over. I came from academia. I was a researcher myself ‚Ä¶



SULLIVAN: At Berkeley, right?



KLUTTZ: At Berkeley. That&#8217;s right. Yep. Sociologist by training and a lawyer in a past life. [LAUGHTER] But that has helped sort of bridge those fields for me.



But Sensitive Uses, we force all of our teams when they&#8217;re envisioning their system design to think about, could the reasonably foreseeable use or misuse of the system that they&#8217;re developing in practice result in three really major, sort of, risk types. One is, could that deployment result in a consequential impact on someone&#8217;s legal position or life opportunity? Another category we have is, could that foreseeable use or misuse result in significant psychological or physical injury or harm? And then the third really ties in with a longstanding commitment we&#8217;ve had to human rights at Microsoft. And so could that system in it&#8217;s reasonably foreseeable use or misuse result in human rights impacts and injurious consequences to folks along different dimensions of human rights? 



Once you decide, we have a process to reporting that project into my office, and we will triage that project, working with the product team, for example, and our Responsible AI Champs community, which are folks who are dispersed throughout the ecosystem at Microsoft and educated in our responsible AI program, and then determine, OK, is it in scope for our program? If it is, say, OK, we&#8217;re going to go along for that ride with you, and then we get into that whole sort of consulting arrangement that then culminates in this set of bespoke use-case-based requirements applying our AI principles.



SULLIVAN: That&#8217;s super fascinating. What are some of the approaches in the governance of genome editing are you maybe seeing happening in AI governance or maybe just, like, bubbling up in conversations around it?



KLUTTZ: Yeah, I mean, I think we&#8217;ve learned a lot from fields like genome editing that Professor Charo talked about and others. And again, it gets back to this, sort of, risk-proportionate-based approach. It&#8217;s a balancing test. It&#8217;s a tradeoff of trying to, sort of, foster innovation and really look for the beneficial uses of these technologies. I appreciated her speaking about that. What are the intended uses of the system, right? And then getting to, OK, how do we balance trying to, again, foster that innovation in a very fast-moving space, a pretty complex space, and a very unsettled space contrasting to other, sort of, professional fields or technological fields that have a long history and are relatively settled from an oversight and regulatory standpoint? This one is not, and for good reason. It is still developing.



And I think, you know, there are certain oversight and policy regimes that exist today that can be applied. Professor Charo talked about this, as well, where, you know, maybe you have certain policy and oversight regimes that, depending on how the application of that technology is applied, applies there versus some horizontal, overarching regulatory sort of framework. And I think that applies from an internal governance standpoint, as well.



SULLIVAN: Yeah. It&#8217;s a great point. So what isn&#8217;t being explored from genome editing that, you know, maybe we think could be useful to AI governance, or as we think about the evolving frameworks ‚Ä¶



KLUTTZ: Yeah.



SULLIVAN: ‚Ä¶ what maybe we should be taking into account from what Professor Charo shared with us?



KLUTTZ: So one of the things I&#8217;ve thought about and took from Professor Charo‚Äôs discussion was she had just this amazing way of framing up how genome editing regulation is done. And she said, you know, we don&#8217;t regulate genome editing; we regulate the things that use genome editing. And while it&#8217;s not a one-to-one analogy with the AI space because we do have this sort of very general model level distinction versus application layer and even platform layer distinctions, I think it&#8217;s fair to say, you know, we don&#8217;t regulate AI applications writ large. We regulate the things that use AI in a very similar way. And that&#8217;s how we think of our internal policy and oversight process at Microsoft, as well.



And maybe there are things that we regulated and oversaw internally at the first instance and the first time we saw it come through, and it graduates into more of a programmatic framework for how we manage that. So one good example of that is some of our higher-risk AI systems that we offer out of Azure at the platform level. When I say that, I mean APIs that you call that developers can then build their own applications on top of. We were really deep in evaluating and assessing mitigations on those platform systems in the first instance, but we also graduated them into what we call our Limited Access AI services program.



And some of the things that Professor Charo discussed really resonated with me. You know, she had this moment where she was mentioning how, you know, you want to know who&#8217;s using your tools and how they&#8217;re being used. And it&#8217;s the same concepts. We want to have trust in our customers, we want to understand their use cases, and we want to apply technical controls that, sort of, force those use cases or give us signal post-deployment that use cases are being done in a way that may give us some level of concern, to reach out and understand what those use cases are.



SULLIVAN: Yeah, you&#8217;re hitting on a great point. And I love this kind of layered approach that we&#8217;re taking and that Alta highlighted, as well. Maybe to double-click a little bit just on that post-market control and what we&#8217;re tracking, kind of, once things are out and being used by our customers. How do we take some of that deployment data and bring it back in to maybe even better inform upfront governance or just how we think about some of the frameworks that we&#8217;re operating in?



KLUTTZ: It&#8217;s a great question. The number one thing is for us at Microsoft, we want to know the voice of our customer. We want our customers to talk to us. We don&#8217;t want to just understand telemetry and data. But it&#8217;s really getting out there and understanding from our customers and not just our customers. I would say our stakeholders is maybe a better term because that includes civil society organizations. It includes governments. It includes all of these non, sort of, customer actors that we care about and that we&#8217;re trying to sort of optimize for, as well. It includes end users of our enterprise customers. If we can gather data about how our products are being used and trying to understand maybe areas that we didn&#8217;t foresee how customers or users might be using those things, and then we can tune those systems to better align with what both customers and users want but also our own AI principles and policies and programs.



SULLIVAN: Daniel, before coming to Microsoft, you led social science research and sociotechnical applications of AI-driven tech at Berkeley. What do you think some of the biggest challenges are in defining and maybe even just, kind of, measuring at, like, a societal level some of the impacts of AI more broadly?



KLUTTZ: Measuring social phenomenon is a difficult thing. And one of the things that, as social scientists, you&#8217;re very interested in is scientifically observing and measuring social phenomena. Well, that sounds great. It sounds also very high level and jargony. What do we mean by that? You know, it&#8217;s very easy to say that you&#8217;re collecting data and you&#8217;re measuring, I don&#8217;t know, trust in AI, right? That&#8217;s a very fuzzy concept.



SULLIVAN: Right. Definitely.



KLUTTZ: It is a concept that we want to get to, but we have to unpack that, and we have to develop what we call measurable constructs. What are the things that we might observe that could give us an indication toward what is a very fuzzy and general concept. And there&#8217;s challenges with that everywhere. And I&#8217;m extremely fortunate to work at Microsoft with some of the world&#8217;s leading sociotechnical researchers and some of these folks who are thinking about‚Äîyou know, very steeped in measurement theory, literally PhDs in these fields‚Äîhow to both measure and allow for a scalable way to do that at a place the size of Microsoft. And that is trying to develop frameworks that are scalable and repeatable and put into our platform that then serves our product teams. Are we providing, as a platform, a service to those product teams that they can plug in and do their automated evaluations at scale as much as possible and then go back in over the top and do some of your more qualitative targeted testing and evaluations.



SULLIVAN: Yeah, makes a lot of sense. Before we close out, if you&#8217;re game for it, maybe we do a quick lightning round. Just 30-second answers here. Favorite real-world sensitive use case you&#8217;ve ever reviewed.



KLUTTZ: Oh gosh. Wow, this is where I get to be the social scientist.



SULLIVAN: [LAUGHS] Yes.



KLUTTZ: It‚Äôs like, define favorite, Kathleen. [LAUGHS] Most memorable, most painful.



SULLIVAN: Let&#8217;s do most memorable.



KLUTTZ: We‚Äôll do most memorable.



SULLIVAN: Yeah.



KLUTTZ: You know, I would say the most memorable project I worked on was when we rolled out the new Bing Chat, which is no longer called Bing Chat, because that was the first really big cross-company effort to deploy GPT-4, which was, you know, the next step up in AI innovation from our partners at OpenAI. And I really value working hand in hand with engineering teams and with researchers and that was us at our best and really sort of turbocharged the model that we have.



SULLIVAN: Wonderful. What&#8217;s one of the most overused phrases that you have in your AI governance meetings?



KLUTTZ: Gosh. [LAUGHS] If I hear ‚ÄúWe need to get aligned; we need to align on this more‚Äù ‚Ä¶



SULLIVAN: [LAUGHS] Right.



KLUTTZ: But, you know, it&#8217;s said for a reason. And I think it sort of speaks to that clever nature. That&#8217;s one that comes to mind.



SULLIVAN: That&#8217;s great. And then maybe, maybe last one. What are you most excited about in the next, I don&#8217;t know, let&#8217;s say three months? This world is moving so fast!



KLUTTZ: You know, the pace of innovation, as you just said, is just staggering. It is unbelievable. And sometimes it can feel overwhelming in my space. But what I am most excited about is how we are building up this Emerging ‚Ä¶ I mentioned this Emerging Technologies program in my team as a, sort of, formal program is relatively new. And I really enjoy being able to take a step back and think a little bit more about the future and a little bit more holistically. And I love working with engineering teams and sort of strategic visionaries who are thinking about what we&#8217;re doing a year from now or five years from now, or even 10 years from now, and I get to be a part of those conversations. And that really gives me energy and helps me ‚Ä¶ helps keep me grounded and not just dealing with the day to day, and, you know, various fire drills that you may run. It&#8217;s thinking strategically and having that foresight about what&#8217;s to come. And it&#8217;s exciting.



SULLIVAN: Great. Well, Daniel, just thanks so much for being here. I had such a wonderful discussion with you, and I think the thoughtfulness in our discussion today I hope resonates with our listeners. And again, thanks to Alta for setting the stage and sharing her really amazing, insightful thoughts here, as well. So thank you.



[MUSIC]



KLUTTZ: Thank you, Kathleen. I appreciate it. It&#8217;s been fun.



SULLIVAN: And to our listeners, thanks for tuning in. You can find resources related to this podcast in the show notes. And if you want to learn more about how Microsoft approaches AI governance, you can visit microsoft.com/RAI.



See you next time!&nbsp;



[MUSIC FADES]

				
			
			
				Show more			
		
	

Opens in a new tabThe post AI Testing and Evaluation: Learnings from genome editing appeared first on Microsoft Research.
‚Ä¢ Transforming network operations with AI: How Swisscom built a network assistant using Amazon Bedrock
  In the telecommunications industry, managing complex network infrastructures requires processing vast amounts of data from multiple sources. Network engineers often spend considerable time manually gathering and analyzing this data, taking away valuable hours that could be spent on strategic initiatives. This challenge led Swisscom, Switzerland‚Äôs leading telecommunications provider, to explore how AI can transform their network operations. 
Swisscom‚Äôs Network Assistant, built on Amazon Bedrock, represents a significant step forward in automating network operations. This solution combines generative AI capabilities with a sophisticated data processing pipeline to help engineers quickly access and analyze network data. Swisscom used AWS services to create a scalable solution that reduces manual effort and provides accurate and timely network insights. 
In this post, we explore how Swisscom developed their Network Assistant. We discuss the initial challenges and how they implemented a solution that delivers measurable benefits. We examine the technical architecture, discuss key learnings, and look at future enhancements that can further transform network operations. We highlight best practices for handling sensitive data for Swisscom to comply with the strict regulations governing the telecommunications industry. This post provides telecommunications providers or other organizations managing complex infrastructure with valuable insights into how you can use AWS services to modernize operations through AI-powered automation. 
The opportunity: Improve network operations 
Network engineers at Swisscom faced the daily challenge to manage complex network operations and maintain optimal performance and compliance. These skilled professionals were tasked to monitor and analyze vast amounts of data from multiple and decoupled sources. The process was repetitive and demanded considerable time and attention to detail. In certain scenarios, fulfilling the assigned tasks consumed more than 10% of their availability. The manual nature of their work presented several critical pain points. The data consolidation process from multiple network entities into a coherent overview was particularly challenging, because engineers had to navigate through various tools and systems to retrieve telemetry information about data sources and network parameters from extensive documentation, verify KPIs through complex calculations, and identify potential issues of diverse nature. This fragmented approach consumed valuable time and introduced the risk of human error in data interpretation and analysis. The situation called for a solution to address three primary concerns: 
 
 Efficiency in data retrieval and analysis 
 Accuracy in calculations and reporting 
 Scalability to accommodate growing data sources and use cases 
 
The team required a streamlined approach to access and analyze network data, maintain compliance with defined metrics and thresholds, and deliver fast and accurate responses to events while maintaining the highest standards of data security and sovereignty. 
Solution overview 
Swisscom‚Äôs approach to develop the Network Assistant was methodical and iterative. The team chose Amazon Bedrock as the foundation for their generative AI application and implemented a Retrieval Augmented Generation (RAG) architecture using Amazon Bedrock Knowledge Bases to enable precise and contextual responses to engineer queries. The RAG approach is implemented in three distinct phases: 
 
 Retrieval ‚Äì User queries are matched with relevant knowledge base content through embedding models 
 Augmentation ‚Äì The context is enriched with retrieved information 
 Generation ‚Äì The large language model (LLM) produces informed responses 
 
The following diagram illustrates the solution architecture. 
 
The solution architecture evolved through several iterations. The initial implementation established basic RAG functionality by feeding the Amazon Bedrock knowledge base with tabular data and documentation. However, the Network Assistant struggled to manage large input files containing thousands of rows with numerical values across multiple parameter columns. This complexity highlighted the need for a more selective approach that could identify only the rows relevant for specific KPI calculations. At that point, the retrieval process wasn‚Äôt returning the precise number of vector embeddings required to calculate the formulas, prompting the team to refine the solution for greater accuracy. 
Next iterations enhanced the assistant with agent-based processing and action groups. The team implemented AWS Lambda functions using Pandas or Spark for data processing, facilitating accurate numerical calculations retrieval using natural language from the user input prompt. 
A significant advancement was introduced with the implementation of a multi-agent approach, using Amazon Bedrock Agents, where specialized agents handle different aspects of the system: 
 
 Supervisor agent ‚Äì Orchestrates interactions between documentation management and calculator agents to provide comprehensive and accurate responses. 
 Documentation management agent ‚Äì Helps the network engineers access information in large volumes of data efficiently and extract insights about data sources, network parameters, configuration, or tooling. 
 Calculator agent ‚Äì Supports the network engineers to understand complex network parameters and perform precise data calculations out of telemetry data. This produces numerical insights that help perform network management tasks; optimize performance; maintain network reliability, uptime, and compliance; and assist in troubleshooting. 
 
This following diagram illustrates the enhanced data extract, transform, and load (ETL) pipeline interaction with Amazon Bedrock. 
 
To achieve the desired accuracy in KPI calculations, the data pipeline was refined to achieve consistent and precise performance, which leads to meaningful insights. The team implemented an ETL pipeline with Amazon Simple Storage Service (Amazon S3) as the data lake to store input files following a daily batch ingestion approach, AWS Glue for automated data crawling and cataloging, and Amazon Athena for SQL querying. At this point, it became possible for the calculator agent to forego the Pandas or Spark data processing implementation. Instead, by using Amazon Bedrock Agents, the agent translates natural language user prompts into SQL queries. In a subsequent step, the agent runs the relevant SQL queries selected dynamically through analysis of various input parameters, providing the calculator agent an accurate result. This serverless architecture supports scalability, cost-effectiveness, and maintains high accuracy in KPI calculations. The system integrates with Swisscom‚Äôs on-premises data lake through daily batch data ingestion, with careful consideration of data security and sovereignty requirements. 
To enhance data security and appropriate ethics in the Network Assistant responses, a series of guardrails were defined in Amazon Bedrock. The application implements a comprehensive set of data security guardrails to protect against malicious inputs and safeguard sensitive information. These include content filters that block harmful categories such as hate, insults, violence, and prompt-based threats like SQL injection. Specific denied topics and sensitive identifiers (for example, IMSI, IMEI, MAC address, or GPS coordinates) are filtered through manual word filters and pattern-based detection, including regular expressions (regex). Sensitive data such as personally identifiable information (PII), AWS access keys, and serial numbers are blocked or masked. The system also uses contextual grounding and relevance checks to verify model responses are factually accurate and appropriate. In the event of restricted input or output, standardized messaging notifies the user that the request can‚Äôt be processed. These guardrails help prevent data leaks, reduce the risk of DDoS-driven cost spikes, and maintain the integrity of the application‚Äôs outputs. 
Results and benefits 
The implementation of the Network Assistant is set to deliver substantial and measurable benefits to Swisscom‚Äôs network operations. The most significant impact is time savings. Network engineers are estimated to experience 10% reduction in time spent on routine data retrieval and analysis tasks. This efficiency gain translates to nearly 200 hours per engineer saved annually, and represents a significant improvement in operational efficiency. The financial impact is equally impressive. The solution is projected to provide substantial cost savings per engineer annually, with minimal operational costs at less than 1% of the total value generated. The return on investment increases as additional teams and use cases are incorporated into the system, demonstrating strong scalability potential. 
Beyond the quantifiable benefits, the Network Assistant is expected to transform how engineers interact with network data. The enhanced data pipeline supports accuracy in KPI calculations, critical for network health tracking, and the multi-agent approach provides orchestrated and comprehensive responses to complex queries out of user natural language. 
As a result, engineers can have instant access to a wide range of network parameters, data source information, and troubleshooting guidance from an individual personalized endpoint with which they can quickly interact and obtain insights through natural language. This enables them to focus on strategic tasks rather than routine data gathering and analysis, leading to a significant work reduction that aligns with Swisscom SRE principles. 
Lessons learned 
Throughout the development and implementation of the Swisscom Network Assistant, several learnings emerged that shaped the solution. The team needed to address data sovereignty and security requirements for the solution, particularly when processing data on AWS. This led to careful consideration of data classification and compliance with applicable regulatory requirements in the telecommunications sector, to make sure that sensitive data is handled appropriately. In this regard, the application underwent a strict threat model evaluation, verifying the robustness of its interfaces against vulnerabilities and acting proactively towards securitization. The threat model was applied to assess doomsday scenarios, and data flow diagrams were created to depict major data flows inside and beyond the application boundaries. The AWS architecture was specified in detail, and trust boundaries were set to indicate which portions of the application trusted each other. Threats were identified following the STRIDE methodology (Spoofing, Tampering, Repudiation, Information disclosure, Denial of service, Elevation of privilege), and countermeasures, including Amazon Bedrock Guardrails, were defined to avoid or mitigate threats in advance. 
A critical technical insight was that complex calculations involving significant data volume management required a different approach than mere AI model interpretation. The team implemented an enhanced data processing pipeline that combines the contextual understanding of AI models with direct database queries for numerical calculations. This hybrid approach facilitates both accuracy in calculations and richness in contextual responses. 
The choice of a serverless architecture proved to be particularly beneficial: it minimized the need to manage compute resources and provides automatic scaling capabilities. The pay-per-use model of AWS services helped keep operational costs low and maintain high performance. Additionally, the team‚Äôs decision to implement a multi-agent approach provided the flexibility needed to handle diverse types of queries and use cases effectively. 
Next steps 
Swisscom has ambitious plans to enhance the Network Assistant‚Äôs capabilities further. A key upcoming feature is the implementation of a network health tracker agent to provide proactive monitoring of network KPIs. This agent will automatically generate reports to categorize issues based on criticality, enable faster response time, and improve the quality of issue resolution to potential network issues. The team is also exploring the integration of Amazon Simple Notification Service (Amazon SNS) to enable proactive alerting for critical network status changes. This can include direct integration with operational tools that alert on-call engineers, to further streamline the incident response process. The enhanced notification system will help engineers address potential issues before they critically impact network performance and obtain a detailed action plan including the affected network entities, the severity of the event, and what went wrong precisely. 
The roadmap also includes expanding the system‚Äôs data sources and use cases. Integration with additional internal network systems will provide more comprehensive network insights. The team is also working on developing more sophisticated troubleshooting features, using the growing knowledge base and agentic capabilities to provide increasingly detailed guidance to engineers. 
Additionally, Swisscom is adopting infrastructure as code (IaC) principles by implementing the solution using AWS CloudFormation. This approach introduces automated and consistent deployments while providing version control of infrastructure components, facilitating simpler scaling and management of the Network Assistant solution as it grows. 
Conclusion 
The Network Assistant represents a significant advancement in how Swisscom can manage its network operations. By using AWS services and implementing a sophisticated AI-powered solution, they have successfully addressed the challenges of manual data retrieval and analysis. As a result, they have boosted both accuracy and efficiency so network engineers can respond quickly and decisively to network events. The solution‚Äôs success is aided not only by the quantifiable benefits in time and cost savings but also by its potential for future expansion. The serverless architecture and multi-agent approach provide a solid foundation for adding new capabilities and scaling across different teams and use cases.As organizations worldwide grapple with similar challenges in network operations, Swisscom‚Äôs implementation serves as a valuable blueprint for using cloud services and AI to transform traditional operations. The combination of Amazon Bedrock with careful attention to data security and accuracy demonstrates how modern AI solutions can help solve real-world engineering challenges. 
As managing network operations complexity continues to grow, the lessons from Swisscom‚Äôs journey can be applied to many engineering disciplines. We encourage you to consider how Amazon Bedrock and similar AI solutions might help your organization overcome its own comprehension and process improvement barriers. To learn more about implementing generative AI in your workflows, explore Amazon Bedrock Resources or contact AWS. 
Additional resources 
For more information about Amazon Bedrock Agents and its use cases, refer to the following resources: 
 
 Generative AI for telecom 
 Best practices for building robust generative AI applications with Amazon Bedrock Agents ‚Äì Part 1 
 Best practices for building robust generative AI applications with Amazon Bedrock Agents ‚Äì Part 2 
 
 
 
About the authors 
Pablo Garc√≠a Benedicto is an experienced Data &amp; AI Cloud Engineer with strong expertise in cloud hyperscalers and data engineering. With a background in telecommunications, he currently works at Swisscom, where he leads and contributes to projects involving Generative AI applications and agents using Amazon Bedrock. Aiming for AI and data specialization, his latest projects focus on building intelligent assistants and autonomous agents that streamline business information retrieval, leveraging cloud-native architectures and scalable data pipelines to reduce toil and drive operational efficiency. 
Rajesh Sripathi is a Generative AI Specialist Solutions Architect at AWS, where he partners with global Telecommunication and Retail &amp; CPG customers to develop and scale generative AI applications. With over 18 years of experience in the IT industry, Rajesh helps organizations use cutting-edge cloud and AI technologies for business transformation. Outside of work, he enjoys exploring new destinations through his passion for travel and driving. 
Ruben Merz Ruben Merz is a Principal Solutions Architect at AWS. With a background in distributed systems and networking, his work with customers at AWS focuses on digital sovereignty, AI, and networking. 
Jordi Montoliu Nerin is a Data &amp; AI Leader currently serving as Senior AI/ML Specialist at AWS, where he helps worldwide telecommunications customers implement AI strategies after previously driving Data &amp; Analytics business across EMEA regions. He has over 10 years of experience, where he has led multiple Data &amp; AI implementations at scale, led executions of data strategy and data governance frameworks, and has driven strategic technical and business development programs across multiple industries and continents. Outside of work, he enjoys sports, cooking and traveling.
‚Ä¢ End-to-End model training and deployment with Amazon SageMaker Unified Studio
  Although rapid generative AI advancements are revolutionizing organizational natural language processing tasks, developers and data scientists face significant challenges customizing these large models. These hurdles include managing complex workflows, efficiently preparing large datasets for fine-tuning, implementing sophisticated fine-tuning techniques while optimizing computational resources, consistently tracking model performance, and achieving reliable, scalable deployment.The fragmented nature of these tasks often leads to reduced productivity, increased development time, and potential inconsistencies in the model development pipeline. Organizations need a unified, streamlined approach that simplifies the entire process from data preparation to model deployment. 
To address these challenges, AWS has expanded Amazon SageMaker with a comprehensive set of data, analytics, and generative AI capabilities. At the heart of this expansion is Amazon SageMaker Unified Studio, a centralized service that serves as a single integrated development environment (IDE). SageMaker Unified Studio streamlines access to familiar tools and functionality from purpose-built AWS analytics and artificial intelligence and machine learning (AI/ML) services, including Amazon EMR, AWS Glue, Amazon Athena, Amazon Redshift, Amazon Bedrock, and Amazon SageMaker AI. With SageMaker Unified Studio, you can discover data through Amazon SageMaker Catalog, access it from Amazon SageMaker Lakehouse, select foundation models (FMs) from Amazon SageMaker JumpStart or build them through JupyterLab, train and fine-tune them with SageMaker AI training infrastructure, and deploy and test models directly within the same environment. SageMaker AI is a fully managed service to build, train, and deploy ML models‚Äîincluding FMs‚Äîfor different use cases by bringing together a broad set of tools to enable high-performance, low-cost ML. It‚Äôs available as a standalone service on the AWS Management Console, or through APIs. Model development capabilities from SageMaker AI are available within SageMaker Unified Studio. 
In this post, we guide you through the stages of customizing large language models (LLMs) with SageMaker Unified Studio and SageMaker AI, covering the end-to-end process starting from data discovery to fine-tuning FMs with SageMaker AI distributed training, tracking metrics using MLflow, and then deploying models using SageMaker AI inference for real-time inference. We also discuss best practices to choose the right instance size and share some debugging best practices while working with JupyterLab notebooks in SageMaker Unified Studio. 
Solution overview 
The following diagram illustrates the solution architecture. There are three personas: admin, data engineer, and user, which can be a data scientist or an ML engineer. 

 
 AWS SageMaker Unified Studio ML workflow showing data processing, model training, and deployment stages
 
Setting up the solution consists of the following steps: 
 
 The admin sets up the SageMaker Unified Studio domain for the user and sets the access controls. The admin also publishes the data to SageMaker Catalog in SageMaker Lakehouse. 
 Data engineers can create and manage extract, transform, and load (ETL) pipelines directly within Unified Studio using Visual ETL. They can transform raw data sources into datasets ready for exploratory data analysis. The admin can then manage the publication of these assets to the SageMaker Catalog, making them discoverable and accessible to other team members or users such as data engineers in the organization. 
 Users or data engineers can log in to the Unified Studio web-based IDE using the login provided by the admin to create a project and create a managed MLflow server for tracking experiments. Users can discover available data assets in the SageMaker Catalog and request a subscription to an asset published by the data engineer. After the data engineer approves the subscription request, the user performs an exploratory data analysis of the content of the table with the query editor or with a JupyterLab notebook, then prepares the dataset by connecting with SageMaker Catalog through an AWS Glue or Athena connection. 
 You can explore models from SageMaker JumpStart, which hosts over 200 models for various tasks, and fine-tune directly with the UI, or develop a training script for fine-tuning the LLM in the JupyterLab IDE. SageMaker AI provides distributed training libraries and supports various distributed training options for deep learning tasks. For this post, we use the PyTorch framework and use Hugging Face open source FMs for fine-tuning. We will show you how you can use parameter efficient fine-tuning (PEFT) with Low-Rank Adaptation (LoRa), where you freeze the model weights, train the model with modifying weight metrics, and then merge these LoRa adapters back to the base model after distributed training. 
 You can track and monitor fine-tuning metrics directly in SageMaker Unified Studio using MLflow, by analyzing metrics such as loss to make sure the model is correctly fine-tuned. 
 You can deploy the model to a SageMaker AI endpoint after the fine-tuning job is complete and test it directly from SageMaker Unified Studio. 
 
Prerequisites 
Before starting this tutorial, make sure you have the following: 
 
 An AWS account with permissions to create SageMaker resources. For setup instructions, see Set up an AWS account and create an administrator user. 
 Familiarity with Python and PyTorch for distributed training and model customization. 
 
Set up SageMaker Unified Studio and configure user access 
SageMaker Unified Studio is built on top of Amazon DataZone capabilities such as domains to organize your assets and users, and projects to collaborate with others users, securely share artifacts, and seamlessly work across compute services. 
To set up Unified Studio, complete the following steps: 
 
 As an admin, create a SageMaker Unified Studio domain, and note the URL. 
 On the domain‚Äôs details page, on the User management tab, choose Configure SSO user access. For this post, we recommend setting up using single sign-on (SSO) access using the URL. 
 
For more information about setting up user access, see Managing users in Amazon SageMaker Unified Studio. 
Log in to SageMaker Unified Studio 
Now that you have created your new SageMaker Unified Studio domain, complete the following steps to access SageMaker Unified Studio: 
 
 On the SageMaker console, open the details page of your domain. 
 Choose the link for the SageMaker Unified Studio URL. 
 Log in with your SSO credentials. 
 
Now you‚Äôre signed in to SageMaker Unified Studio. 
Create a project 
The next step is to create a project. Complete the following steps: 
 
 In SageMaker Unified Studio, choose Select a project on the top menu, and choose Create project. 
 For Project name, enter a name (for example, demo). 
 For Project profile, choose your profile capabilities. A project profile is a collection of blueprints, which are configurations used to create projects. For this post, we choose All capabilities, then choose Continue. 
 

 
 Creating a project in Amazon SageMaker Unified Studio
 
Create a compute space 
SageMaker Unified Studio provides compute spaces for IDEs that you can use to code and develop your resources. By default, it creates a space for you to get started with you project. You can find the default space by choosing Compute in the navigation pane and choosing the Spaces tab. You can then choose Open to go to the JuypterLab environment and add members to this space. You can also create a new space by choosing Create space on the Spaces tab. 
 
To use SageMaker Studio notebooks cost-effectively, use smaller, general-purpose instances (like the T or M families) for interactive data exploration and prototyping. For heavy lifting like training or large-scale processing or deployment, use SageMaker AI training jobs and SageMaker AI prediction to offload the work to separate and more powerful instances such as the P5 family. We will show you in the notebook how you can run training jobs and deploy LLMs in the notebook with APIs. It is not recommended to run distributed workloads in notebook instances. The chances of kernel failures is high because JupyterLab notebooks should not be used for large distributed workloads (both for data and ML training). 
The following screenshot shows the configuration options for your space. You can change your instance size from default (ml.t3.medium) to (ml.m5.xlarge) for the JupyterLab IDE. You can also increase the Amazon Elastic Block Store (Amazon EBS) volume capacity from 16 GB to 50 GB for training LLMs. 

 
 Canfigure space in Amazon SageMaker Unified Studio
 
Set up MLflow to track ML experiments 
You can use MLflow in SageMaker Unified Studio to create, manage, analyze, and compare ML experiments. Complete the following steps to set up MLflow: 
 
 In SageMaker Unified Studio, choose Compute in the navigation pane. 
 On the MLflow Tracking Servers tab, choose Create MLflow Tracking Server. 
 Provide a name and create your tracking server. 
 Choose Copy ARN to copy the Amazon Resource Name (ARN) of the tracking server. 
 
 
You will need this MLflow ARN in your notebook to set up distributed training experiment tracking. 
Set up the data catalog 
For model fine-tuning, you need access to a dataset. After you set up the environment, the next step is to find the relevant data from the SageMaker Unified Studio data catalog and prepare the data for model tuning. For this post, we use the Stanford Question Answering Dataset (SQuAD) dataset. This dataset is a reading comprehension dataset, consisting of questions posed by crowd workers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable. 
Download the SQuaD dataset and upload it to SageMaker Lakehouse by following the steps in Uploading data. 

 
 Adding data to Catalog in Amazon SageMaker Unified Studio
 
To make this data discoverable by the users or ML engineers, the admin needs to publish this data to the Data Catalog. For this post, you can directly download the SQuaD dataset and upload it to the catalog. To learn how to publish the dataset to SageMaker Catalog, see Publish assets to the Amazon SageMaker Unified Studio catalog from the project inventory. 
Query data with the query editor and JupyterLab 
In many organizations, data preparation is a collaborative effort. A data engineer might prepare an initial raw dataset, which a data scientist then refines and augments with feature engineering before using it for model training. In the SageMaker Lakehouse data and model catalog, publishers set subscriptions for automatic or manual approval (wait for admin approval). Because you already set up the data in the previous section, you can skip this section showing how to subscribe to the dataset. 
To subscribe to another dataset like SQuAD, open the data and model catalog in Amazon SageMaker Lakehouse, choose SQuAD, and subscribe. 

 
 Subscribing to any asset or dataset published by Admin
 
Next, let‚Äôs use the data explorer to explore the dataset you subscribed to. Complete the following steps: 
 
 On the project page, choose Data. 
 Under Lakehouse, expand AwsDataCatalog. 
 Expand your database starting from glue_db_. 
 Choose the dataset you created (starting with squad) and choose Query with Athena. 
 

 
 Querying the data using Query Editor in Amazon SageMaker Unfied Studio
 
Process your data through a multi-compute JupyterLab IDE notebook 
SageMaker Unified Studio provides a unified JupyterLab experience across different languages, including SQL, PySpark, Python, and Scala Spark. It also supports unified access across different compute runtimes such as Amazon Redshift and Athena for SQL, Amazon EMR Serverless, Amazon EMR on EC2, and AWS Glue for Spark. 
Complete the following steps to get started with the unified JupyterLab experience: 
 
 Open your SageMaker Unified Studio project page. 
 On the top menu, choose Build, and under IDE &amp; APPLICATIONS, choose JupyterLab. 
 Wait for the space to be ready. 
 Choose the plus sign and for Notebook, choose Python 3. 
 Open a new terminal and enter git clonehttps://github.com/aws-samples/amazon-sagemaker-generativeai. 
 Go to the folder amazon-sagemaker-generativeai/3_distributed_training/distributed_training_sm_unified_studio/ and open the distributed training in unified studio.ipynb notebook to get started. 
 Enter the MLflow server ARN you created in the following code: 
 
 
 import&nbsp;os
os.environ["mlflow_uri"]&nbsp;=&nbsp;""
os.environ["mlflow_experiment_name"]&nbsp;=&nbsp;"deepseek-r1-distill-llama-8b-sft" 
 
Now you an visualize the data through the notebook. 
 
 On the project page, choose Data. 
 Under Lakehouse, expand AwsDataCatalog. 
 Expand your database starting from glue_db, copy the name of the database, and enter it in the following code: 
 
 
 db_name&nbsp;=&nbsp;"&lt;enter your db name&gt;"
table&nbsp;=&nbsp;"sqad" 
 
 
 You can now access the entire dataset directly by using the in-line SQL query capabilities of JupyterLab notebooks in SageMaker Unified Studio. You can follow the data preprocessing steps in the notebook. 
 
 
 %%sql project.athena
SELECT * FROM "&lt;DATABASE_NAME&gt;"."sqad"; 
 
The following screenshot shows the output. 
 
We are going to split the dataset into a test set and training set for model training. When the data processing in done and we have split the data into test and training sets, the next step is to perform fine-tuning of the model using SageMaker Distributed Training. 
Fine-tune the model with SageMaker Distributed training 
You‚Äôre now ready to fine-tune your model by using SageMaker AI capabilities for training. Amazon SageMaker Training is a fully managed ML service offered by SageMaker that helps you efficiently train a wide range of ML models at scale. The core of SageMaker AI jobs is the containerization of ML workloads and the capability of managing AWS compute resources. SageMaker Training takes care of the heavy lifting associated with setting up and managing infrastructure for ML training workloads 
We select one model directly from the Hugging Face Hub, DeepSeek-R1-Distill-Llama-8B, and develop our training script in the JupyterLab space. Because we want to distribute the training across all the available GPUs in our instance, by using PyTorch Fully Sharded Data Parallel (FSDP), we use the Hugging Face Accelerate library to run the same PyTorch code across distributed configurations. You can start the fine-tuning job directly in your JupyterLab notebook or use the SageMaker Python SDK to start the training job. We use the Trainer from transfomers to fine-tune our model. We prepared the script train.py, which loads the dataset from disk, prepares the model and tokenizer, and starts the training. 
For configuration, we use TrlParser, and provide hyperparameters in a YAML file. You can upload this file and provide it to SageMaker similar to your datasets. The following is the config file for fine-tuning the model on ml.g5.12xlarge. Save the config file as args.yaml and upload it to Amazon Simple Storage Service (Amazon S3). 
 
 cat&nbsp;&gt;&nbsp;./args.yaml&nbsp;&lt;&lt;EOF
model_id: "deepseek-ai/DeepSeek-R1-Distill-Llama-8B" &nbsp; &nbsp; &nbsp; # Hugging Face model id
mlflow_uri: "${mlflow_uri}"
mlflow_experiment_name: "${mlflow_experiment_name}"
# sagemaker specific parameters
output_dir: "/opt/ml/model" &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; # path to where SageMaker will upload the model 
train_dataset_path: "/opt/ml/input/data/train/" &nbsp; # path to where FSx saves train dataset
test_dataset_path: "/opt/ml/input/data/test/" &nbsp; &nbsp; # path to where FSx saves test dataset
# training parameters
lora_r: 8
lora_alpha: 16
lora_dropout: 0.1 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 
learning_rate: 2e-4 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# learning rate scheduler
num_train_epochs: 1 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# number of training epochs
per_device_train_batch_size: 2 &nbsp; &nbsp; &nbsp; &nbsp; # batch size per device during training
per_device_eval_batch_size: 1 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# batch size for evaluation
gradient_accumulation_steps: 2 &nbsp; &nbsp; &nbsp; &nbsp; # number of steps before performing a backward/update pass
gradient_checkpointing: true &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; # use gradient checkpointing
bf16: true &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; # use bfloat16 precision
tf32: false &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# use tf32 precision
fsdp: "full_shard auto_wrap offload"
fsdp_config: 
&nbsp;&nbsp; &nbsp;backward_prefetch: "backward_pre"
&nbsp;&nbsp; &nbsp;cpu_ram_efficient_loading: true
&nbsp;&nbsp; &nbsp;offload_params: true
&nbsp;&nbsp; &nbsp;forward_prefetch: false
&nbsp;&nbsp; &nbsp;use_orig_params: true
merge_weights: true &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# merge weights in the base model
EOF 
 
Use the following code to use the native PyTorch container image, pre-built for SageMaker: 
 
 image_uri = sagemaker.image_uris.retrieve(
&nbsp;&nbsp; &nbsp;framework="pytorch",
&nbsp;&nbsp; &nbsp;region=sagemaker_session.boto_session.region_name,
&nbsp;&nbsp; &nbsp;version="2.6.0",
&nbsp;&nbsp; &nbsp;instance_type=instance_type,
&nbsp;&nbsp; &nbsp;image_scope="training"
)

image_uri 
 
Define the trainer as follows: 
 
 Define the ModelTrainer
model_trainer = ModelTrainer(
&nbsp;&nbsp; &nbsp;training_image=image_uri,
&nbsp;&nbsp; &nbsp;source_code=source_code,
&nbsp;&nbsp; &nbsp;base_job_name=job_name,
&nbsp;&nbsp; &nbsp;compute=compute_configs,
&nbsp;&nbsp; &nbsp;distributed=Torchrun(),
&nbsp;&nbsp; &nbsp;stopping_condition=StoppingCondition(
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;max_runtime_in_seconds=7200
&nbsp;&nbsp; &nbsp;),
&nbsp;&nbsp; &nbsp;hyperparameters={
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"config": "/opt/ml/input/data/config/args.yaml" # path to TRL config which was uploaded to s3
&nbsp;&nbsp; &nbsp;},
&nbsp;&nbsp; &nbsp;output_data_config=OutputDataConfig(
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;s3_output_path=output_path
&nbsp;&nbsp; &nbsp;),
) 
 
Run the trainer with the following: 
 
 # starting the train job with our uploaded datasets as input
model_trainer.train(input_data_config=data,&nbsp;wait=True) 
 
You can follow the steps in the notebook. 
You can explore the job execution in SageMaker Unified Studio. The training job runs on the SageMaker training cluster by distributing the computation across the four available GPUs on the selected instance type ml.g5.12xlarge. We choose to merge the LoRA adapter with the base model. This decision was made during the training process by setting the merge_weights parameter to True in our train_fn() function. Merging the weights provides a single, cohesive model that incorporates both the base knowledge and the domain-specific adaptations we‚Äôve made through fine-tuning. 
Track training metrics and model registration using MLflow 
You created an MLflow server in an earlier step to track experiments and registered models, and provided the server ARN in the notebook. 
You can log MLflow models and automatically register them with Amazon SageMaker Model Registry using either the Python SDK or directly through the MLflow UI. Use mlflow.register_model() to automatically register a model with SageMaker Model Registry during model training. You can explore the MLflow tracking code in train.py and the notebook. The training code tracks MLflow experiments and registers the model to the MLflow model registry. To learn more, see Automatically register SageMaker AI models with SageMaker Model Registry. 
To see the logs, complete the following steps: 
 
 Choose Build, then choose Spaces. 
 Choose Compute in the navigation pane. 
 On the MLflow Tracking Servers tab, choose Open to open the tracking server. 
 
You can see both the experiments and registered models. 
 
Deploy and test the model using SageMaker AI Inference 
When deploying a fine-tuned model on AWS, SageMaker AI Inference offers multiple deployment strategies. In this post, we use SageMaker real-time inference. The real-time inference endpoint is designed for having full control over the inference resources. You can use a set of available instances and deployment options for hosting your model. By using the SageMaker built-in container DJL Serving, you can take advantage of the inference script and optimization options available directly in the container. In this post, we deploy the fine-tuned model to a SageMaker endpoint for running inference, which will be used for testing the model. 
In SageMaker Unified Studio, in JupyterLab, we create the Model object, which is a high-level SageMaker model class for working with multiple container options. The image_uri parameter specifies the container image URI for the model, and model_data points to the Amazon S3 location containing the model artifact (automatically uploaded by the SageMaker training job). We also specify a set of environment variables to configure the specific inference backend option (OPTION_ROLLING_BATCH), the degree of tensor parallelism based on the number of available GPUs (OPTION_TENSOR_PARALLEL_DEGREE), and the maximum allowable length of input sequences (in tokens) for models during inference (OPTION_MAX_MODEL_LEN). 
 
 model = Model(
&nbsp;&nbsp; &nbsp;image_uri=image_uri,
&nbsp;&nbsp; &nbsp;model_data=f"s3://{bucket_name}/{job_prefix}/{job_name}/output/model.tar.gz",
&nbsp;&nbsp; &nbsp;role=get_execution_role(),
&nbsp;&nbsp; &nbsp;env={
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;'HF_MODEL_ID': "/opt/ml/model",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;'OPTION_TRUST_REMOTE_CODE': 'true',
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;'OPTION_ROLLING_BATCH': "vllm",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;'OPTION_DTYPE': 'bf16',
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;'OPTION_TENSOR_PARALLEL_DEGREE': 'max',
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;'OPTION_MAX_ROLLING_BATCH_SIZE': '1',
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;'OPTION_MODEL_LOADING_TIMEOUT': '3600',
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;'OPTION_MAX_MODEL_LEN': '4096'
&nbsp;&nbsp; &nbsp;}
) 
 
After you create the model object, you can deploy it to an endpoint using the deploy method. The initial_instance_count and instance_type parameters specify the number and type of instances to use for the endpoint. We selected the ml.g5.4xlarge instance for the endpoint. The container_startup_health_check_timeout and model_data_download_timeout parameters set the timeout values for the container startup health check and model data download, respectively. 
 
 model_id&nbsp;=&nbsp;"deepseek-ai/DeepSeek-R1-Distill-Llama-8B"
endpoint_name&nbsp;=&nbsp;f"{model_id.split('/')[-1].replace('.',&nbsp;'-')}-sft-djl"
predictor = model.deploy(
&nbsp;&nbsp; &nbsp;initial_instance_count=instance_count,
&nbsp;&nbsp; &nbsp;instance_type=instance_type,
&nbsp;&nbsp; &nbsp;container_startup_health_check_timeout=1800,
&nbsp;&nbsp; &nbsp;model_data_download_timeout=3600
) 
 
It takes a few minutes to deploy the model before it becomes available for inference and evaluation. You can test the endpoint invocation in JupyterLab, by using the AWS SDK with the boto3 client for sagemaker-runtime, or by using the SageMaker Python SDK and the predictor previously created, by using the predict API. 
 
 base_prompt = f"""&lt;s&gt; [INST] {{question}} [/INST] """

prompt = base_prompt.format(
&nbsp; &nbsp; question="What statue is in front of the Notre Dame building?"
)

predictor.predict({
&nbsp;&nbsp; &nbsp;"inputs": prompt,
&nbsp;&nbsp; &nbsp;"parameters": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"max_new_tokens": 300,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"temperature": 0.2,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"top_p": 0.9,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"return_full_text": False,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"stop": ['&lt;/s&gt;']
&nbsp;&nbsp; &nbsp;}
}) 
 
You can also test the model invocation in SageMaker Unified Studio, on the Inference endpoint page and Text inference tab. 
Troubleshooting 
You might encounter some of the following errors while running your model training and deployment: 
 
 Training job fails to start ‚Äì If a training job fails to start, make sure your IAM role AmazonSageMakerDomainExecution has the necessary permissions, verify the instance type is available in your AWS Region, and check your S3 bucket permissions. This role is created when an admin creates the domain, and you can ask the admin to check your IAM access permissions associated with this role. 
 Out-of-memory errors during training ‚Äì If you encounter out-of-memory errors during training, try reducing the batch size, use gradient accumulation to simulate larger batches, or consider using a larger instance. 
 Slow model deployment ‚Äì For slow model deployment, make sure model artifacts aren‚Äôt excessively large, and use appropriate instance types for inference and capacity available for that instance in your Region. 
 
For more troubleshooting tips, refer to Troubleshooting guide. 
Clean up 
SageMaker Unified Studio by default shuts down idle resources such as JupyterLab spaces after 1 hour. However, you must delete the S3 bucket and the hosted model endpoint to stop incurring costs. You can delete the real-time endpoints you created using the SageMaker console. For instructions, see Delete Endpoints and Resources. 
Conclusion 
This post demonstrated how SageMaker Unified Studio serves as a powerful centralized service for data and AI workflows, showcasing its seamless integration capabilities throughout the fine-tuning process. With SageMaker Unified Studio, data engineers and ML practitioners can efficiently discover and access data through SageMaker Catalog, prepare datasets, fine-tune models, and deploy them‚Äîall within a single, unified environment. The service‚Äôs direct integration with SageMaker AI and various AWS analytics services streamlines the development process, alleviating the need to switch between multiple tools and environments. The solution highlights the service‚Äôs versatility in handling complex ML workflows, from data discovery and preparation to model deployment, while maintaining a cohesive and intuitive user experience. Through features like integrated MLflow tracking, built-in model monitoring, and flexible deployment options, SageMaker Unified Studio demonstrates its capability to support sophisticated AI/ML projects at scale. 
To learn more about SageMaker Unified Studio, see An integrated experience for all your data and AI with Amazon SageMaker Unified Studio. 
If this post helps you or inspires you to solve a problem, we would love to hear about it! The code for this solution is available on the GitHub repo for you to use and extend. Contributions are always welcome! 
 
About the authors 
Mona Mona currently works as a Sr World Wide Gen AI Specialist Solutions Architect at Amazon focusing on Gen AI Solutions. She was a Lead Generative AI specialist in Google Public Sector at Google before joining Amazon. She is a published author of two books ‚Äì Natural Language Processing with AWS AI Services and Google Cloud Certified Professional Machine Learning Study Guide. She has authored 19 blogs on AI/ML and cloud technology and a co-author on a research paper on CORD19 Neural Search which won an award for Best Research Paper at the prestigious AAAI (Association for the Advancement of Artificial Intelligence) conference. 
Bruno Pistone is a Senior Generative AI and ML Specialist Solutions Architect for AWS based in Milan. He works with large customers helping them to deeply understand their technical needs and design AI and Machine Learning solutions that make the best use of the AWS Cloud and the Amazon Machine Learning stack. His expertise include: Machine Learning end to end, Machine Learning Industrialization, and Generative AI. He enjoys spending time with his friends and exploring new places, as well as travelling to new destinations. 
Lauren Mullennex is a Senior GenAI/ML Specialist Solutions Architect at AWS. She has a decade of experience in DevOps, infrastructure, and ML. Her areas of focus include MLOps/LLMOps, generative AI, and computer vision.
‚Ä¢ Optimize RAG in production environments using Amazon SageMaker JumpStart and Amazon OpenSearch Service
  Generative AI has revolutionized customer interactions across industries by offering personalized, intuitive experiences powered by unprecedented access to information. This transformation is further enhanced by Retrieval Augmented Generation (RAG), a technique that allows large language models (LLMs) to reference external knowledge sources beyond their training data. RAG has gained popularity for its ability to improve generative AI applications by incorporating additional information, often preferred by customers over techniques like fine-tuning due to its cost-effectiveness and faster iteration cycles. 
The RAG approach excels in grounding language generation with external knowledge, producing more factual, coherent, and relevant responses. This capability proves invaluable in applications such as question answering, dialogue systems, and content generation, where accuracy and informative outputs are crucial. For businesses, RAG offers a powerful way to use internal knowledge by connecting company documentation to a generative AI model. When an employee asks a question, the RAG system retrieves relevant information from the company‚Äôs internal documents and uses this context to generate an accurate, company-specific response. This approach enhances the understanding and usage of internal company documents and reports. By extracting relevant context from corporate knowledge bases, RAG models facilitate tasks like summarization, information extraction, and complex question answering on domain-specific materials, enabling employees to quickly access vital insights from vast internal resources. This integration of AI with proprietary information can significantly improve efficiency, decision-making, and knowledge sharing across the organization. 
A typical RAG workflow consists of four key components: input prompt, document retrieval, contextual generation, and output. The process begins with a user query, which is used to search a comprehensive knowledge corpus. Relevant documents are then retrieved and combined with the original query to provide additional context for the LLM. This enriched input allows the model to generate more accurate and contextually appropriate responses. RAG‚Äôs popularity stems from its ability to use frequently updated external data, providing dynamic outputs without the need for costly and compute-intensive model retraining. 
To implement RAG effectively, many organizations turn to platforms like Amazon SageMaker JumpStart. This service offers numerous advantages for building and deploying generative AI applications, including access to a wide range of pre-trained models with ready-to-use artifacts, a user-friendly interface, and seamless scalability within the AWS ecosystem. By using pre-trained models and optimized hardware, SageMaker JumpStart enables rapid deployment of both LLMs and embedding models, minimizing the time spent on complex scalability configurations. 
In the previous post, we showed how to build a RAG application on SageMaker JumpStart using Facebook AI Similarity Search (Faiss). In this post, we show how to use Amazon OpenSearch Service as a vector store to build an efficient RAG application. 
Solution overview 
To implement our RAG workflow on SageMaker, we use a popular open source Python library known as LangChain. With LangChain, the RAG components are simplified into independent blocks that you can bring together using a chain object that will encapsulate the entire workflow. The solution consists of the following key components: 
 
 LLM (inference) ‚Äì We need an LLM that will do the actual inference and answer the end-user‚Äôs initial prompt. For our use case, we use Meta Llama3 for this component. LangChain comes with a default wrapper class for SageMaker endpoints with which we can simply pass in the endpoint name to define an LLM object in the library. 
 Embeddings model ‚Äì We need an embeddings model to convert our document corpus into textual embeddings. This is necessary for when we‚Äôre doing a similarity search on the input text to see what documents share similarities or contain the information to help augment our response. For this post, we use the BGE Hugging Face Embeddings model available in SageMaker JumpStart. 
 Vector store and retriever ‚Äì To house the different embeddings we have generated, we use a vector store. In this case, we use OpenSearch Service, which allows for similarity search using k-nearest neighbors (k-NN) as well as traditional lexical search. Within our chain object, we define the vector store as the retriever. You can tune this depending on how many documents you want to retrieve. 
 
The following diagram illustrates the solution architecture. 
 
In the following sections, we walk through setting up OpenSearch, followed by exploring the notebook that implements a RAG solution with LangChain, Amazon SageMaker AI, and OpenSearch Service. 
Benefits of using OpenSearch Service as a vector store for RAG 
In this post, we showcase how you can use a vector store such as OpenSearch Service as a knowledge base and embedding store. OpenSearch Service offers several advantages when used for RAG in conjunction with SageMaker AI: 
 
 Performance ‚Äì Efficiently handles large-scale data and search operations 
 Advanced search ‚Äì Offers full-text search, relevance scoring, and semantic capabilities 
 AWS integration ‚Äì Seamlessly integrates with SageMaker AI and other AWS services 
 Real-time updates ‚Äì Supports continuous knowledge base updates with minimal delay 
 Customization ‚Äì Allows fine-tuning of search relevance for optimal context retrieval 
 Reliability ‚Äì Provides high availability and fault tolerance through a distributed architecture 
 Analytics ‚Äì Provides analytical features for data understanding and performance improvement 
 Security ‚Äì Offers robust features such as encryption, access control, and audit logging 
 Cost-effectiveness ‚Äì Serves as an economical solution compared to proprietary vector databases 
 Flexibility ‚Äì Supports various data types and search algorithms, offering versatile storage and retrieval options for RAG applications 
 
You can use SageMaker AI with OpenSearch Service to create powerful and efficient RAG systems. SageMaker AI provides the machine learning (ML) infrastructure for training and deploying your language models, and OpenSearch Service serves as an efficient and scalable knowledge base for retrieval. 
OpenSearch Service optimization strategies for RAG 
Based on our learnings from the hundreds of RAG applications deployed using OpenSearch Service as a vector store, we‚Äôve developed several best practices: 
 
 If you are starting from a clean slate and want to move quickly with something simple, scalable, and high-performing, we recommend using an Amazon OpenSearch Serverless vector store collection. With OpenSearch Serverless, you benefit from automatic scaling of resources, decoupling of storage, indexing compute, and search compute, with no node or shard management, and you only pay for what you use. 
 If you have a large-scale production workload and want to take the time to tune for the best price-performance and the most flexibility, you can use an OpenSearch Service managed cluster. In a managed cluster, you pick the node type, node size, number of nodes, and number of shards and replicas, and you have more control over when to scale your resources. For more details on best practices for operating an OpenSearch Service managed cluster, see Operational best practices for Amazon OpenSearch Service. 
 OpenSearch supports both exact k-NN and approximate k-NN. Use exact k-NN if the number of documents or vectors in your corpus is less than 50,000 for the best recall. For use cases where the number of vectors is greater than 50,000, exact k-NN will still provide the best recall but might not provide sub-100 millisecond query performance. Use approximate k-NN in use cases above 50,000 vectors for the best performance. 
 OpenSearch uses algorithms from the NMSLIB, Faiss, and Lucene libraries to power approximate k-NN search. There are pros and cons to each k-NN engine, but we find that most customers choose Faiss due to its overall performance in both indexing and search as well as the variety of different quantization and algorithm options that are supported and the broad community support. 
 Within the Faiss engine, OpenSearch supports both Hierarchical Navigable Small World (HNSW) and Inverted File System (IVF) algorithms. Most customers find HNSW to have better recall than IVF and choose it for their RAG use cases. To learn more about the differences between these engine algorithms, see Vector search. 
 To reduce the memory footprint to lower the cost of the vector store while keeping the recall high, you can start with Faiss HNSW 16-bit scalar quantization. This can also reduce search latencies and improve indexing throughput when used with SIMD optimization. 
 If using an OpenSearch Service managed cluster, refer to Performance tuning for additional recommendations. 
 
Prerequisites 
Make sure you have access to one ml.g5.4xlarge and ml.g5.2xlarge instance each in your account. A secret should be created in the same region as the stack is deployed.Then complete the following prerequisite steps to create a secret using AWS Secrets Manager: 
 
 On the Secrets Manager console, choose Secrets in the navigation pane. 
 Choose Store a new secret. 
 
 
 
 For Secret type, select Other type of secret. 
 For Key/value pairs, on the Plaintext tab, enter a complete password. 
 Choose Next. 
 
 
 
 For Secret name, enter a name for your secret. 
 Choose Next. 
 
 
 
 Under Configure rotation, keep the settings as default and choose Next. 
 
 
 
 Choose Store to save your secret. 
 
 
 
 On the secret details page, note the secret Amazon Resource Name (ARN) to use in the next step. 
 
 
Create an OpenSearch Service cluster and SageMaker notebook 
We use AWS CloudFormation to deploy our OpenSearch Service cluster, SageMaker notebook, and other resources. Complete the following steps: 
 
 Launch the following CloudFormation template. 
 Provide the ARN of the secret you created as a prerequisite and keep the other parameters as default. 
 
 
 
 Choose Create to create your stack, and wait for the stack to complete (about 20 minutes). 
 When the status of the stack is CREATE_COMPLETE, note the value of OpenSearchDomainEndpoint on the stack Outputs tab. 
 Locate SageMakerNotebookURL in the outputs and choose the link to open the SageMaker notebook. 
 
Run the SageMaker notebook 
After you have launched the notebook in JupyterLab, complete the following steps: 
 
 Go to genai-recipes/RAG-recipes/llama3-RAG-Opensearch-langchain-SMJS.ipynb. 
 
You can also clone the notebook from the GitHub repo. 
 
 
 Update the value of&nbsp;OPENSEARCH_URL&nbsp;in the notebook with the value copied from&nbsp;OpenSearchDomainEndpoint in the previous step (look for os.environ['OPENSEARCH_URL'] = "").&nbsp; The port needs to be 443. 
 Run the cells in the notebook. 
 
The notebook provides a detailed explanation of all the steps. We explain some of the key cells in the notebook in this section. 
For the RAG workflow, we deploy the huggingface-sentencesimilarity-bge-large-en-v1-5 embedding model and meta-textgeneration-llama-3-8b-instruct LLM from Hugging Face. SageMaker JumpStart simplifies this process because the model artifacts, data, and container specifications are all prepackaged for optimal inference. These are then exposed using the SageMaker Python SDK high-level API calls, which let you specify the model ID for deployment to a SageMaker real-time endpoint: 
 
 
&nbsp;sagemaker.jumpstart.model&nbsp;&nbsp;JumpStartModel

model_id&nbsp;&nbsp;"meta-textgeneration-llama-3-8b-instruct"
accept_eula&nbsp;&nbsp;
model&nbsp;&nbsp;JumpStartModel(model_idmodel_id)
llm_predictor&nbsp;&nbsp;modeldeploy(accept_eulaaccept_eula)

model_id&nbsp;&nbsp;"huggingface-sentencesimilarity-bge-large-en-v1-5"
text_embedding_model&nbsp;&nbsp;JumpStartModel(model_idmodel_id)
embedding_predictor&nbsp;&nbsp;text_embedding_modeldeploy() 
 
Content handlers are crucial for formatting data for SageMaker endpoints. They transform inputs into the format expected by the model and handle model-specific parameters like temperature and token limits. These parameters can be tuned to control the creativity and consistency of the model‚Äôs responses. 
 
 class Llama38BContentHandler(LLMContentHandler):
&nbsp;&nbsp; &nbsp;content_type = "application/json"
&nbsp;&nbsp; &nbsp;accepts = "application/json"

&nbsp;&nbsp; &nbsp;def transform_input(self, prompt: str, model_kwargs: dict) -&gt; bytes:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;payload = {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"inputs": prompt,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"parameters": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"max_new_tokens": 1000,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"top_p": 0.9,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"temperature": 0.6,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"stop": ["&lt;|eot_id|&gt;"],
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;},
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;input_str = json.dumps(
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;payload,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;)
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;#print(input_str)
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;return input_str.encode("utf-8") 
 
We use PyPDFLoader from LangChain to load PDF files, attach metadata to each document fragment, and then use RecursiveCharacterTextSplitter to break the documents into smaller, manageable chunks. The text splitter is configured with a chunk size of 1,000 characters and an overlap of 100 characters, which helps maintain context between chunks. This preprocessing step is crucial for effective document retrieval and embedding generation, because it makes sure the text segments are appropriately sized for the embedding model and the language model used in the RAG system. 
 
 import&nbsp;numpy as&nbsp;np
from&nbsp;langchain_community.document_loaders import&nbsp;PyPDFLoader
from&nbsp;langchain.text_splitter import&nbsp;RecursiveCharacterTextSplitter
documents&nbsp;=&nbsp;[]
for&nbsp;idx, file&nbsp;in&nbsp;enumerate(filenames):
&nbsp;&nbsp; &nbsp;loader&nbsp;=&nbsp;PyPDFLoader(data_root&nbsp;+&nbsp;file)
&nbsp;&nbsp; &nbsp;document&nbsp;=&nbsp;loader.load()
&nbsp;&nbsp; &nbsp;for&nbsp;document_fragment&nbsp;in&nbsp;document:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;document_fragment.metadata =&nbsp;metadata[idx]
&nbsp;&nbsp; &nbsp;documents&nbsp;+=&nbsp;document
# - in our testing Character split works better with this PDF data set
text_splitter&nbsp;=&nbsp;RecursiveCharacterTextSplitter(
&nbsp;&nbsp; &nbsp;# Set a really small chunk size, just to show.
&nbsp;&nbsp; &nbsp;chunk_size=1000,
&nbsp;&nbsp; &nbsp;chunk_overlap=100,
)
docs&nbsp;=&nbsp;text_splitter.split_documents(documents)
print(docs[100]) 
 
The following block initializes a vector store using OpenSearch Service for the RAG system. It converts preprocessed document chunks into vector embeddings using a SageMaker model and stores them in OpenSearch Service. The process is configured with security measures like SSL and authentication to provide secure data handling. The bulk insertion is optimized for performance with a sizeable batch size. Finally, the vector store is wrapped with VectorStoreIndexWrapper, providing a simplified interface for operations like querying and retrieval. This setup creates a searchable database of document embeddings, enabling quick and relevant context retrieval for user queries in the RAG pipeline. 
 
 from&nbsp;langchain.indexes.vectorstore import&nbsp;VectorStoreIndexWrapper
# Initialize OpenSearchVectorSearch
vectorstore_opensearch&nbsp;=&nbsp;OpenSearchVectorSearch.from_documents(
&nbsp;&nbsp; &nbsp;docs,
&nbsp;&nbsp; &nbsp;sagemaker_embeddings,
&nbsp;&nbsp; &nbsp;http_auth=awsauth, &nbsp;# Auth will use the IAM role
&nbsp;&nbsp; &nbsp;use_ssl=True,
&nbsp;&nbsp; &nbsp;verify_certs=True,
&nbsp;&nbsp; &nbsp;connection_class=RequestsHttpConnection,
&nbsp;&nbsp; &nbsp;bulk_size=2000&nbsp;&nbsp;# Increase this to accommodate the number of documents you have
)
# Wrap the OpenSearch vector store with the VectorStoreIndexWrapper
wrapper_store_opensearch&nbsp;=&nbsp;VectorStoreIndexWrapper(vectorstore=vectorstore_opensearch) 
 
Next, we use the wrapper from the previous step along with the prompt template. We define the prompt template for interacting with the Meta Llama 3 8B Instruct model in the RAG system. The template uses specific tokens to structure the input in a way that the model expects. It sets up a conversation format with system instructions, user query, and a placeholder for the assistant‚Äôs response. The PromptTemplate class from LangChain is used to create a reusable prompt with a variable for the user‚Äôs query. This structured approach to prompt engineering helps maintain consistency in the model‚Äôs responses and guides it to act as a helpful assistant. 
 
 prompt_template&nbsp;=&nbsp;"""&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;
You are a helpful assistant.
&lt;|eot_id|&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;
{query}
&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;
"""
PROMPT&nbsp;=&nbsp;PromptTemplate(
&nbsp;&nbsp; &nbsp;template=prompt_template, input_variables=["query"]
)
query&nbsp;=&nbsp;"How did AWS perform in 2021?"

answer&nbsp;=&nbsp;wrapper_store_opensearch.query(question=PROMPT.format(query=query), llm=llm)
print(answer) 
 
Similarly, the notebook also shows how to use Retrieval QA, where you can customize how the documents fetched should be added to prompt using the chain_type parameter. 
Clean up 
Delete your SageMaker endpoints from the notebook to avoid incurring costs: 
 
 # Delete resources
llm_predictor.delete_model()
llm_predictor.delete_endpoint()
embedding_predictor.delete_model()
embedding_predictor.delete_endpoint() 
 
Next, delete your OpenSearch cluster to stop incurring additional charges:aws cloudformation delete-stack --stack-name rag-opensearch 
Conclusion 
RAG has revolutionized how businesses use AI by enabling general-purpose language models to work seamlessly with company-specific data. The key benefit is the ability to create AI systems that combine broad knowledge with up-to-date, proprietary information without expensive model retraining. This approach transforms customer engagement and internal operations by delivering personalized, accurate, and timely responses based on the latest company data. The RAG workflow‚Äîcomprising input prompt, document retrieval, contextual generation, and output‚Äîallows businesses to tap into their vast repositories of internal documents, policies, and data, making this information readily accessible and actionable. For businesses, this means enhanced decision-making, improved customer service, and increased operational efficiency. Employees can quickly access relevant information, while customers receive more accurate and personalized responses. Moreover, RAG‚Äôs cost-efficiency and ability to rapidly iterate make it an attractive solution for businesses looking to stay competitive in the AI era without constant, expensive updates to their AI systems. By making general-purpose LLMs work effectively on proprietary data, RAG empowers businesses to create dynamic, knowledge-rich AI applications that evolve with their data, potentially transforming how companies operate, innovate, and engage with both employees and customers. 
SageMaker JumpStart has streamlined the process of developing and deploying generative AI applications. It offers pre-trained models, user-friendly interfaces, and seamless scalability within the AWS ecosystem, making it straightforward for businesses to harness the power of RAG. 
Furthermore, using OpenSearch Service as a vector store facilitates swift retrieval from vast information repositories. This approach not only enhances the speed and relevance of responses, but also helps manage costs and operational complexity effectively. 
By combining these technologies, you can create robust, scalable, and efficient RAG systems that provide up-to-date, context-aware responses to customer queries, ultimately enhancing user experience and satisfaction. 
To get started with implementing this Retrieval Augmented Generation (RAG) solution using Amazon SageMaker JumpStart and Amazon OpenSearch Service, check out the example notebook on GitHub. You can also learn more about Amazon OpenSearch Service in the developer guide. 
 
About the authors 
Vivek Gangasani is a Lead Specialist Solutions Architect for Inference at AWS. He helps emerging generative AI companies build innovative solutions using AWS services and accelerated compute. Currently, he is focused on developing strategies for fine-tuning and optimizing the inference performance of large language models. In his free time, Vivek enjoys hiking, watching movies, and trying different cuisines. 
Harish Rao is a Senior Solutions Architect at AWS, specializing in large-scale distributed AI training and inference. He empowers customers to harness the power of AI to drive innovation and solve complex challenges. Outside of work, Harish embraces an active lifestyle, enjoying the tranquility of hiking, the intensity of racquetball, and the mental clarity of mindfulness practices. 
Raghu Ramesha is an ML Solutions Architect. He specializes in machine learning, AI, and computer vision domains, and holds a master‚Äôs degree in Computer Science from UT Dallas. In his free time, he enjoys traveling and photography. 
Sohaib Katariwala is a Sr. Specialist Solutions Architect at AWS focused on Amazon OpenSearch Service. His interests are in all things data and analytics. More specifically he loves to help customers use AI in their data strategy to solve modern day challenges. 
Karan Jain is a Senior Machine Learning Specialist at AWS, where he leads the worldwide Go-To-Market strategy for Amazon SageMaker Inference. He helps customers accelerate their generative AI and ML journey on AWS by providing guidance on deployment, cost-optimization, and GTM strategy. He has led product, marketing, and business development efforts across industries for over 10 years, and is passionate about mapping complex service features to customer solutions.
‚Ä¢ Advancing AI agent governance with Boomi and AWS: A unified approach to observability and compliance
  Just as APIs became the standard for integration, AI agents are transforming workflow automation through intelligent task coordination. AI agents are already enhancing decision-making and streamlining operations across enterprises. But as adoption accelerates, organizations face growing complexity in managing them at scale. Organizations struggle with observability and lifecycle management, finding it difficult to monitor performance and manage versions effectively. Governance and security concerns arise as these agents process sensitive data, which requires strict compliance and access controls. Perhaps most concerningly, without proper management, organizations face the risk of agent sprawl‚Äîthe unchecked proliferation of AI agents leading to inefficiency and security vulnerabilities. 
Boomi and AWS have collaborated to address the complexity surrounding AI agents with Agent Control Tower, an AI agent management solution developed by Boomi and tightly integrated with Amazon Bedrock. Agent Control Tower, part of the Boomi Agentstudio solution, provides the governance framework to manage this transformation, with capabilities that address both current and emerging compliance needs. 
As a leader in enterprise iPaaS per Gartner‚Äôs Magic Quadrant, based on Completeness of Vision and Ability to Execute, Boomi serves over 20,000 enterprise customers, with three-quarters of these customers operating on AWS. This includes a significant presence among Fortune 500 and Global 2000 organizations across critical sectors such as healthcare, finance, technology, and manufacturing. Boomi is innovating with generative AI, with more than 2,000 customers using its AI agents. The convergence of capabilities that Boomi provides‚Äîspanning AI, integration, automation, API management, and data management‚Äîwith AWS and its proven track record in reliability, security, and AI innovation creates a compelling foundation for standardized AI agent governance at scale. In this post, we share how Boomi partnered with AWS to help enterprises accelerate and scale AI adoption with confidence using Agent Control Tower. 
A unified AI management solution 
Built on AWS, Agent Control Tower uniquely delivers a single control plane for managing AI agents across multiple systems, including other cloud providers and on-premises environments. At its core, it offers comprehensive observability and monitoring, providing real-time performance tracking and deep visibility into agent decision-making and behavior. 
The following screenshot showcases how users can view summary data across agent providers and add or manage providers. 
 
The following screenshot shows an example of the Monitoring and Compliance dashboard. 
 
Agent Control Tower also provides a single pane of glass for visibility into the tools used by each agent, as illustrated in the following screenshot. 
 
Agent Control Tower provides key governance and security controls such as centralized policy enforcement and role-based access control, and enables meeting regulatory compliance with frameworks like GDPR and HIPAA. Furthermore, its lifecycle management capabilities enable automated agent discovery, version tracking, and operational control through features such as pause and resume functionality. Agent Control Tower is positioned as one of the first, if not the first, unified solutions that provides full lifecycle AI agent management with integrated governance and orchestration features. Although many vendors focus on releasing AI agents, there are few that focus on solutions for managing, deploying, and governing AI agents at scale. 
The following screenshot shows an example of how users can review agent details and disable or enable an agent. 
 
As shown in the following screenshot, users can drill down into details for each part of the agent. 
 
Amazon Bedrock: Enabling and enhancing AI governance 
Using Amazon Bedrock, organizations can implement security guardrails and content moderation while maintaining the flexibility to select and switch between AI models for optimized performance and accuracy. Organizations can create and enable access to curated knowledge bases and predefined action groups, enabling sophisticated multi-agent collaboration. Amazon Bedrock also provides comprehensive metrics and trace logs for agents to help facilitate complete transparency and accountability in agent operations. Through deep integration with Amazon Bedrock, Boomi‚Äôs Agent Control Tower enhances agent transparency and governance, offering a unified, actionable view of agent configurations and activities across environments. 
The following diagram illustrates the Agent Control Tower architecture on AWS. 
 
Business impact: Transforming enterprise AI operations 
Consider a global manufacturer using AI agents for supply chain optimization. With Agent Control Tower, they can monitor agent performance across regions in real time, enforce consistent security policies, and enable regulatory compliance. When issues arise, they can quickly identify and resolve them while maintaining the ability to scale AI operations confidently. With this level of control and visibility, organizations can deploy AI agents more effectively while maintaining robust security and compliance standards. 
Conclusion 
Boomi customers have already deployed more than 33,000 agents and are seeing up to 80% less time spent on documentation and 50% faster issue resolution. With Boomi and AWS, enterprises can accelerate and scale AI adoption with confidence, backed by a product that puts visibility, governance, and security first. Discover how Agent Control Tower can help your organization manage AI agent sprawl and take advantage of scalable, compliance-aligned innovation. Take a guided tour and learn more about Boomi Agent Control Tower and Amazon Bedrock integration. Or, you can get started today with AI FastTrack. 
 
About the authors 
 Deepak Chandrasekar is the VP of Software Engineering &amp; User Experience and leads multidisciplinary teams at Boomi. He oversees flagship initiatives like Boomi‚Äôs Agent Control Tower, Task Automation, and Market Reach, while driving a cohesive and intelligent experience layer across products. Previously, Deepak held a key leadership role at Unifi Software, which was acquired by Boomi. With a passion for building scalable, and intuitive AI-powered solutions, he brings a commitment to engineering excellence and responsible innovation. 
 Sandeep Singh is Director of Engineering at Boomi, where he leads global teams building solutions that enable enterprise integration and automation at scale. He drives initiatives like Boomi Agent Control Tower, Marketplace, and Labs, empowering partners and customers with intelligent, trusted solutions. With leadership experience at GE and Fujitsu, Sandeep brings expertise in API strategy, product engineering, and AI/ML solutions. A former solution architect, he is passionate about designing mission-critical systems and driving innovation through scalable, intelligent solutions. 
 Santosh Ameti is a seasoned Engineering leader in the Amazon Bedrock team and has built Agents, Evaluation, Guardrails, and Prompt Management solutions. His team continuously innovates in the agentic space, delivering one of the most secure and managed agentic solutions for enterprises. 
 Greg Sligh is a Senior Solutions Architect at AWS with more than 25 years of experience in software engineering, software architecture, consulting, and IT and Engineering leadership roles across multiple industries. For the majority of his career, he has focused on creating and delivering distributed, data-driven applications with particular focus on scale, performance, and resiliency. Now he helps ISVs meet their objectives across technologies, with particular focus on AI/ML. 
 Padma Iyer is a Senior Customer Solutions Manager at Amazon Web Services, where she specializes in supporting ISVs. With a passion for cloud transformation and financial technology, Padma works closely with ISVs to guide them through successful cloud transformations, using best practices to optimize their operations and drive business growth. Padma has over 20 years of industry experience spanning banking, tech, and consulting.
‚Ä¢ Use Amazon SageMaker Unified Studio to build complex AI workflows using Amazon Bedrock Flows
  Organizations face the challenge to manage data, multiple artificial intelligence and machine learning (AI/ML) tools, and workflows across different environments, impacting productivity and governance. A unified development environment consolidates data processing, model development, and AI application deployment into a single system. This integration streamlines workflows, enhances collaboration, and accelerates AI solution development from concept to production. 
The next generation of Amazon SageMaker is the center for your data, analytics, and AI. SageMaker brings together AWS AI/ML and analytics capabilities and delivers an integrated experience for analytics and AI with unified access to data. Amazon SageMaker Unified Studio is a single data and AI development environment where you can find and access your data and act on it using AWS analytics and AI/ML services, for SQL analytics, data processing, model development, and generative AI application development. 
With SageMaker Unified Studio, you can efficiently build generative AI applications in a trusted and secure environment using Amazon Bedrock. You can choose from a selection of high-performing foundation models (FMs) and advanced customization and tooling such as Amazon Bedrock Knowledge Bases, Amazon Bedrock Guardrails, Amazon Bedrock Agents, and Amazon Bedrock Flows. You can rapidly tailor and deploy generative AI applications, and share with the built-in catalog for discovery. 
In this post, we demonstrate how you can use SageMaker Unified Studio to create complex AI workflows using Amazon Bedrock Flows. 
Solution overview 
Consider FinAssist Corp, a leading financial institution developing a generative AI-powered agent support application. The solution offers the following key features: 
 
 Complaint reference system ‚Äì An AI-powered system providing quick access to historical complaint data, enabling customer service representatives to efficiently handle customer follow-ups, support internal audits, and aid in training new staff. 
 Intelligent knowledge base ‚Äì A comprehensive data source of resolved complaints that quickly retrieves relevant complaint details, resolution actions, and outcome summaries. 
 Streamlined workflow management ‚Äì Enhanced consistency in customer communications through standardized access to past case information, supporting compliance checks and process improvement initiatives. 
 Flexible query capability ‚Äì A straightforward interface supporting various query scenarios, from customer inquiries about past resolutions to internal reviews of complaint handling procedures. 
 
Let‚Äôs explore how SageMaker Unified Studio and Amazon Bedrock Flows, integrated with Amazon Bedrock Knowledge Bases and Amazon Bedrock Agents, address these challenges by creating an AI-powered complaint reference system. The following diagram illustrates the solution architecture. 
 
The solution uses the following key components: 
 
 SageMaker Unified Studio ‚Äì Provides the development environment 
 Flow app ‚Äì Orchestrates the workflow, including: 
   
   Knowledge base queries 
   Prompt-based classification 
   Conditional routing 
   Agent-based response generation 
    
 
The workflow processes user queries through the following steps: 
 
 A user submits a complaint-related question. 
 The knowledge base provides relevant complaint information. 
 The prompt classifies if the query is about resolution timing. 
 Based on the classification using the condition, the application takes the following action: 
   
   Routes the query to an AI agent for specific resolution responses. 
   Returns general complaint information. 
    
 The application generates an appropriate response for the user. 
 
Prerequisites 
For this example, you need the following: 
 
 Access to SageMaker Unified Studio. (You will need the SageMaker Unified Studio portal URL from your administrator). You can authenticate using either: 
   
   AWS Identity and Access Management (IAM) user credentials. 
   Single sign-on (SSO) credentials with AWS IAM Identity Center. 
    
 The IAM user or IAM Identity Center user must have appropriate permissions for: 
   
   SageMaker Unified Studio. 
    
   Amazon Bedrock (including Amazon Bedrock Flows, Amazon Bedrock Agents, Amazon Bedrock Prompt Management, and Amazon Bedrock Knowledge Bases). 
   For more information, refer to Identity-based policy examples. 
    
 Access to Amazon Bedrock FMs (make sure these are enabled for your account), for example:Anthropic‚Äôs Claude 3 Haiku (for the agent). 
 Configure access to your Amazon Bedrock serverless models for Amazon Bedrock in SageMaker Unified Studio projects. 
 Amazon Titan Embedding (for the knowledge base). 
 Sample complaint data prepared in CSV format for creating the knowledge base. 
 
Prepare your data 
We have created a sample dataset to use for Amazon Bedrock Knowledge Bases. This dataset has information of complaints received by customer service representatives and resolution information.The following is an example from the sample dataset: 
 
 complaint_id,product,sub_product,issue,sub_issue,complaint_summary,action_taken,next_steps,financial_institution,state,submitted_via,resolution_type,timely_response
FIN-2024-001,04/26/24,"Mortgage","Conventional mortgage","Payment issue","Escrow dispute","Customer disputes mortgage payment increase after recent escrow analysis","Reviewed escrow analysis, explained property tax increase impact, provided detailed payment breakdown","1. Send written explanation of escrow analysis 2. Schedule annual escrow review 3. Provide payment assistance options","Financial Institution-1","TX","Web","Closed with explanation","Yes"
FIN-2024-002,04/26/24,"Money transfer","Wire transfer","Processing delay","International transfer","Wire transfer of $10,000 delayed, customer concerned about international payment deadline","Located wire transfer in system, expedited processing, waived wire fee","1. Confirm receipt with receiving bank 2. Update customer on delivery 3. Document process improvement needs","Financial Institution-2","FL","Phone","Closed with monetary relief","No" 
 
Create a project 
In SageMaker Unified Studio, users can use projects to collaborate on various business use cases. Within projects, you can manage data assets in the SageMaker Unified Studio catalog, perform data analysis, organize workflows, develop ML models, build generative AI applications, and more. 
To create a project, complete the following steps: 
 
 Open the SageMaker Unified Studio landing page using the URL from your admin. 
 Choose Create project. 
 Enter a project name and optional description. 
 For Project profile, choose Generative AI application development. 
 Choose Continue. 
 
 
 
 Complete your project configuration, then choose Create project. 
 
Create a prompt 
Let‚Äôs create a reusable prompt to capture the instructions for FMs, which we will use later while creating the flow application. For more information, see Reuse and share Amazon Bedrock prompts. 
 
 In SageMaker Unified Studio, on the Build menu, choose Prompt under Machine Learning &amp; Generative AI. 
 
 
 
 Provide a name for the prompt. 
 Choose the appropriate FM (for this example, we choose Claude 3 Haiku). 
 For Prompt message, we enter the following: 
 
 
 You are a complaint analysis classifier. You will receive complaint data from a knowledge base. Analyze the {{input}} and respond with a single letter:
T: If the input contains information about complaint resolution timing, response time, or processing timeline (whether timely or delayed)
F: For all other types of complaint information
Return only 'T' or 'F' based on whether the knowledge base response is about resolution timing. Do not add any additional text or explanation - respond with just the single letter 'T' or 'F'. 
 
 
 Choose Save. 
 
 
 
 Choose Create version. 
 
Create a chat agent 
Let‚Äôs create a chat agent to handle specific resolution responses. Complete the following steps: 
 
 In SageMaker Unified Studio, on the Build menu, choose Chat agent under Machine Learning &amp; Generative AI. 
 Provide a name for the prompt. 
 Choose the appropriate FM (for this example, we choose Claude 3 Haiku). 
 For Enter a system prompt, we enter the following: 
 
 
 You are a Financial Complaints Assistant AI. You will receive complaint information from a knowledge base and questions about resolution timing.
When responding to resolution timing queries:
1. Use the provided complaint information to confirm if it was resolved within timeline
2. For timely resolutions, provide:
   - Confirmation of timely completion
   - Specific actions taken (from the provided complaint data)
   - Next steps that were completed
2. For delayed resolutions, provide:
   - Acknowledgment of delay
   - Standard compensation package:
     ‚Ä¢ $75 service credit
     ‚Ä¢ Priority Status upgrade for 6 months
     ‚Ä¢ Service fees waived for current billing cycle
   - Actions taken (from the provided complaint data)
   - Contact information for follow-up: Priority Line: ************** 
Always reference the specific complaint details provided in your input when discussing actions taken and resolution process. 
 
 
 Choose Save. 
 
 
 
 After the agent is saved, choose Deploy. 
 For Alias name, enter demoAlias. 
 Choose Deploy. 
 
Create a flow 
Now that we have our prompt and agent ready, let‚Äôs create a flow that will orchestrate the complaint handling process: 
 
 In SageMaker Unified Studio, on the Build menu, choose Flow under Machine Learning &amp; Generative AI. 
 
 
 
 Create a new flow called demo-flow. 
 
 
Add a knowledge base to your flow application 
Complete the following steps to add a knowledge base node to the flow: 
 
 In the navigation pane, on the Nodes tab, choose Knowledge Base. 
 On the Configure tab, provide the following information: 
   
   For Node name, enter a name (for example, complaints_kb). 
   Choose Create new Knowledge Base. 
    
 In the Create Knowledge Base pane, enter the following information: 
   
   For Name, enter a name (for example, complaints). 
   For Description, enter a description (for example, user complaints information). 
   For Add data sources, select Local file and upload the complaints.txt file. 
   For Embeddings model, choose Titan Text Embeddings V2. 
   For Vector store, choose OpenSearch Serverless. 
   Choose Create. 
    
 
 
 
 After you create the knowledge base, choose it in the flow. 
 In the details name, provide the following information: 
 For Response generation model, choose Claude 3 Haiku. 
 Connect the output of the flow input node with the input of the knowledge base node. 
 Connect the output of the knowledge base node with the input of the flow output node. 
 
 
 
 Choose Save. 
 
Add a prompt to your flow application 
Now let‚Äôs add the prompt you created earlier to the flow: 
 
 On the Nodes tab in the Flow app builder pane, add a prompt node. 
 On the Configure tab for the prompt node, provide the following information: 
 For Node name, enter a name (for example, demo_prompt). 
 For Prompt, choose financeAssistantPrompt. 
 For Version, choose 1. 
 Connect the output of the knowledge base node with the input of the prompt node.  
 Choose Save. 
 
Add a condition to your flow application 
The condition node determines how the flow handles different types of queries. It evaluates whether a query is about resolution timing or general complaint information, enabling the flow to route the query appropriately. When a query is about resolution timing, it will be directed to the chat agent for specialized handling; otherwise, it will receive a direct response from the knowledge base. Complete the following steps to add a condition: 
 
 On the Nodes tab in the Flow app builder pane, add a condition node. 
 On the Configure tab for the condition node, provide the following information: 
   
   For Node name, enter a name (for example, demo_condition). 
   Under Conditions, for Condition, enter conditionInput == "T". 
   Connect the output of the prompt node with the input of the condition node.  
    
 Choose Save. 
 
Add a chat agent to your flow application 
Now let‚Äôs add the chat agent you created earlier to the flow: 
 
 On the Nodes tab in the Flow app builder pane, add the agent node. 
 On the Configure tab for the agent node, provide the following information: 
   
   For Node name, enter a name (for example, demo_agent). 
   For Chat agent, choose DemoAgent. 
   For Alias, choose demoAlias. 
    
 Create the following node connections: 
   
   Connect the input of the condition node (demo_condition) to the output of the prompt node (demo_prompt). 
   Connect the output of the condition node: 
     
     Set If condition is true to the agent node (demo_agent). 
     Set If condition is false to the existing flow output node (FlowOutputNode). 
      
   Connect the output of the knowledge base node (complaints_kb) to the input of the following: 
     
     The agent node (demo_agent). 
     The flow output node (FlowOutputNode). 
      
   Connect the output of the agent node (demo_agent) to a new flow output node named FlowOutputNode_2.  
    
 Choose Save. 
 
Test the flow application 
Now that the flow application is ready, let‚Äôs test it. On the right side of the page, choose the expand icon to open the Test pane. 
 
In the Enter prompt text box, we can ask a few questions related to the dataset created earlier. The following screenshots show some examples. 
 
 
Clean up 
To clean up your resources, delete the flow, agent, prompt, knowledge base, and associated OpenSearch Serverless resources. 
Conclusion 
In this post, we demonstrated how to build an AI-powered complaint reference system using a flow application in SageMaker Unified Studio. By using the integrated capabilities of SageMaker Unified Studio with Amazon Bedrock features like Amazon Bedrock Knowledge Bases, Amazon Bedrock Agents, and Amazon Bedrock Flows, you can rapidly develop and deploy sophisticated AI applications without extensive coding. 
As you build AI workflows using SageMaker Unified Studio, remember to adhere to the AWS Shared Responsibility Model for security. Implement SageMaker Unified Studio security best practices, including proper IAM configurations and data encryption. You can also refer to Secure a generative AI assistant with OWASP Top 10 mitigation for details on how to assess the security posture of a generative AI assistant using OWASP TOP 10 mitigations for common threats. Following these guidelines helps establish robust AI applications that maintain data integrity and system protection. 
To learn more, refer to Amazon Bedrock in SageMaker Unified Studio and join discussions and share your experiences in AWS Generative AI Community. 
We look forward to seeing the innovative solutions you will create with these powerful new features. 
 
About the authors 
Sumeet Tripathi is an Enterprise Support Lead (TAM) at AWS in North Carolina. He has over 17 years of experience in technology across various roles. He is passionate about helping customers to reduce operational challenges and friction. His focus area is AI/ML and Energy &amp; Utilities Segment. Outside work, He enjoys traveling with family, watching cricket and movies. 
Vishal Naik is a Sr. Solutions Architect at Amazon Web Services (AWS). He is a builder who enjoys helping customers accomplish their business needs and solve complex challenges with AWS solutions and best practices. His core area of focus includes Generative AI and Machine Learning. In his spare time, Vishal loves making short films on time travel and alternate universe themes.
‚Ä¢ Accelerating AI innovation: Scale MCP servers for enterprise workloads with Amazon Bedrock
  Generative AI has been moving at a rapid pace, with new tools, offerings, and models released frequently. According to Gartner, agentic AI is one of the top technology trends of 2025, and organizations are performing prototypes on how to use agents in their enterprise environment. Agents depend on tools, and each tool might have its own mechanism to send and receive information. Model Context Protocol (MCP) by Anthropic is an open source protocol that attempts to solve this challenge. It provides a protocol and communication standard that is cross-compatible with different tools, and can be used by an agentic application‚Äôs large language model (LLM) to connect to enterprise APIs or external tools using a standard mechanism. However, large enterprise organizations like financial services tend to have complex data governance and operating models, which makes it challenging to implement agents working with MCP. 
One major challenge is the siloed approach in which individual teams build their own tools, leading to duplication of efforts and wasted resources. This approach slows down innovation and creates inconsistencies in integrations and enterprise design. Furthermore, managing multiple disconnected MCP tools across teams makes it difficult to scale AI initiatives effectively. These inefficiencies hinder enterprises from fully taking advantage of generative AI for tasks like post-trade processing, customer service automation, and regulatory compliance. 
In this post, we present a centralized MCP server implementation using Amazon Bedrock that offers an innovative approach by providing shared access to tools and resources. With this approach, teams can focus on building AI capabilities rather than spending time developing or maintaining tools. By standardizing access to resources and tools through MCP, organizations can accelerate the development of AI agents, so teams can reach production faster. Additionally, a centralized approach provides consistency and standardization and reduces operational overhead, because the tools are managed by a dedicated team rather than across individual teams. It also enables centralized governance that enforces controlled access to MCP servers, which reduces the risk of data exfiltration and prevents unauthorized or insecure tool use across the organization. 
Solution overview 
The following figure illustrates a proposed solution based on a financial services use case that uses MCP servers across multiple lines of business (LoBs), such as compliance, trading, operations, and risk management. Each LoB performs distinct functions tailored to their specific business. For instance, the trading LoB focuses on trade execution, whereas the risk LoB performs risk limit checks. For performing these functions, each division provides a set of MCP servers that facilitate actions and access to relevant data within their LoBs. These servers are accessible to agents developed within the respective LoBs and can also be exposed to agents outside LoBs. 
 
The development of MCP servers is decentralized. Each LoB is responsible for developing the servers that support their specific functions. When the development of a server is complete, it‚Äôs hosted centrally and accessible across LoBs. It takes the form of a registry or marketplace that facilitates integration of AI-driven solutions across divisions while maintaining control and governance over shared resources. 
In the following sections, we explore what the solution looks like on a conceptual level. 
Agentic application interaction with a central MCP server hub 
The following flow diagram showcases how an agentic application built using Amazon Bedrock interacts with one of the MCP servers located in the MCP server hub. 
The flow consists of the following steps: 
 
 The application connects to the central MCP hub through the load balancer and requests a list of available tools from the specific MCP server. This can be fine-grained based on what servers the agentic application has access to. 
 The trade server responds with list of tools available, including details such as tool name, description, and required input parameters. 
 The agentic application invokes an Amazon Bedrock agent and provides the list of tools available. 
 Using this information, the agent determines what to do next based on the given task and the list of tools available to it. 
 The agent chooses the most suitable tool and responds with the tool name and input parameters. The control comes back to the agentic application. 
 The agentic application calls for the execution of the tool through the MCP server using the tool name and input parameters. 
 The trade MCP server executes the tool and returns the results of the execution back to the application. 
 The application returns the results of the tool execution back to the Amazon Bedrock agent. 
 The agent observes the tool execution results and determines the next step. 
 
Let‚Äôs dive into the technical architecture of the solution. 
Architecture overview 
The following diagram illustrates the architecture to host the centralized cluster of MCP servers for an LoB. 
 
The architecture can be split in five sections: 
 
 MCP server discovery API 
 Agentic applications 
 Central MCP server hub 
 Tools and resources 
 
Let‚Äôs explore each section in detail: 
 
 MCP server discovery API ‚Äì This API is a dedicated endpoint for discovering various MCP servers. Different teams can call this API to find what MCP servers are available in the registry; read their description, tool, and resource details; and decide which MCP server would be the right one for their agentic application. When a new MCP server is published, it‚Äôs added to an Amazon DynamoDB database. MCP server owners are responsible for keeping the registry information up-to-date. 
 Agentic application ‚Äì The agentic applications are hosted on AWS Fargate for Amazon Elastic Container Service (Amazon ECS) and built using Amazon Bedrock Agents. Teams can also use the newly released open source AWS Strands Agents SDK, or other agentic frameworks of choice, to build the agentic application and their own containerized solution to host the agentic application. The agentic applications access Amazon Bedrock through a secure private virtual private cloud (VPC) endpoint. It uses private VPC endpoints to access MCP servers. 
 Central MCP server hub ‚Äì This is where the MCP servers are hosted. Access to servers is enabled through an AWS Network Load Balancer. Technically, each server is a Docker container that can is hosted on Amazon ECS, but you can choose your own container deployment solution. These servers can scale individually without impacting the other server. These servers in turn connect to one or more tools using private VPC endpoints. 
 Tools and resources ‚Äì This component holds the tools, such as databases, another application, Amazon Simple Storage Service (Amazon S3), or other tools. For enterprises, access to the tools and resources is provided only through private VPC endpoints. 
 
Benefits of the solution 
The solution offers the following key benefits: 
 
 Scalability and resilience ‚Äì Because you‚Äôre using Amazon ECS on Fargate, you get scalability out of the box without managing infrastructure and handling scaling concerns. Amazon ECS automatically detects and recovers from failures by restarting failed MCP server tasks locally or reprovisioning containers, minimizing downtime. It can also redirect traffic away from unhealthy Availability Zones and rebalance tasks across healthy Availability Zones to provide uninterrupted access to the server. 
 Security ‚Äì Access to MCP servers is secured at the network level through network controls such as PrivateLink. This makes sure the agentic application only connects to trusted MCP servers hosted by the organization, and vice versa. Each Fargate workload runs in an isolated environment. This prevents resource sharing between tasks. For application authentication and authorization, we propose using an MCP Auth Server (refer to the following GitHub repo) to hand off those tasks to a dedicated component that can scale independently. 
 
At the time of writing, the MCP protocol doesn‚Äôt provide built-in mechanisms for user-level access control or authorization. Organizations requiring user-specific access restrictions must implement additional security layers on top of the MCP protocol. For a reference implementation, refer to the following GitHub repo. 
Let‚Äôs dive deeper in the implementation of this solution. 
Use case 
The implementation is based on a financial services use case featuring post-trade execution. Post-trade execution refers to the processes and steps that take place after an equity buy/sell order has been placed by a customer. It involves many steps, including verifying trade details, actual transfer of assets, providing a detailed report of the execution, running fraudulent checks, and more. For simplification of the demo, we focus on the order execution step. 
Although this use case is tailored to the financial industry, you can apply the architecture and the approach to other enterprise workloads as well. The entire code of this implementation is available on GitHub. We use the AWS Cloud Development Kit (AWS CDK) for Python to deploy this solution, which creates an agentic application connected to tools through the MCP server. It also creates a Streamlit UI to interact with the agentic application. 
The following code snippet provides access to the MCP discovery API: 
 
 def get_server_registry():
    # Initialize DynamoDB client
    dynamodb = boto3.resource('dynamodb')
    table = dynamodb.Table(DDBTBL_MCP_SERVER_REGISTRY)
    
    try:
        # Scan the table to get all items
        response = table.scan()
        items = response.get('Items', [])
        
        # Format the items to include only id, description, server
        formatted_items = []
        for item in items:
            formatted_item = {
                'id': item.get('id', ''),
                'description': item.get('description', ''),
                'server': item.get('server', ''),
            }
            formatted_items.append(formatted_item)
        
        # Return the formatted items as JSON
        return {
            'statusCode': 200,
            'headers': cors_headers,
            'body': json.dumps(formatted_items)
        }
    except Exception as e:
        # Handle any errors
        return {
            'statusCode': 500,
            'headers': cors_headers,
            'body': json.dumps({'error': str(e)})
        } 
 
The preceding code is invoked through an AWS Lambda function. The complete code is available in the GitHub repository. The following graphic shows the response of the discovery API. 
 
Let‚Äôs explore a scenario where the user submits a question: ‚ÄúBuy 100 shares of AMZN at USD 186, to be distributed equally between accounts A31 and B12.‚ÄùTo execute this task, the agentic application invokes the trade-execution MCP server. The following code is the sample implementation of the MCP server for trade execution: 
 
 from fastmcp import FastMCP
from starlette.requests import Request
from starlette.responses import PlainTextResponse
mcp = FastMCP("server")

@mcp.custom_route("/", methods=["GET"])
async def health_check(request: Request) -&gt; PlainTextResponse:
    return PlainTextResponse("OK")

@mcp.tool()
async def executeTrade(ticker, quantity, price):
    """
    Execute a trade for the given ticker, quantity, and price.
    
    Sample input:
    {
        "ticker": "AMZN",
        "quantity": 1000,
        "price": 150.25
    }
    """
    # Simulate trade execution
    return {
        "tradeId": "T12345",
        "status": "Executed",
        "timestamp": "2025-04-09T22:58:00"
    }
    
@mcp.tool()
async def sendTradeDetails(tradeId):
    """
    Send trade details for the given tradeId.
    Sample input:
    {
        "tradeId": "T12345"
    }
    """
    return {
        "status": "Details Sent",
        "recipientSystem": "MiddleOffice",
        "timestamp": "2025-04-09T22:59:00"
    }
if __name__ == "__main__":
    mcp.run(host="0.0.0.0", transport="streamable-http") 
 
The complete code is available in the following GitHub repo. 
The following graphic shows the MCP server execution in action. 
 
This is a sample implementation of the use case focusing on the deployment step. For a production scenario, we strongly recommend adding a human oversight workflow to monitor the execution and provide input at various steps of the trade execution. 
Now you‚Äôre ready to deploy this solution. 
Prerequisites 
Prerequisites for the solution are available in the README.md of the GitHub repository. 
Deploy the application 
Complete the following steps to run this solution: 
 
 Navigate to the README.md file of the GitHub repository to find the instructions to deploy the solution. Follow these steps to complete deployment. 
 
The successful deployment will exit with a message similar to the one shown in the following screenshot. 
 
 
 When the deployment is complete, access the Streamlit application. 
 
You can find the Streamlit URL in the terminal output, similar to the following screenshot. 
 
 
 Enter the URL of the Streamlit application in a browser to open the application console. 
 
On the application console, different sets of MCP servers are listed in the left pane under MCP Server Registry. Each set corresponds to an MCP server and includes the definition of the tools, such as the name, description, and input parameters. 
 
In the right pane, Agentic App, a request is pre-populated: ‚ÄúBuy 100 shares of AMZN at USD 186, to be distributed equally between accounts A31 and B12.‚Äù This request is ready to be submitted to the agent for execution. 
 
 
 Choose Submit to invoke an Amazon Bedrock agent to process the request. 
 
The agentic application will evaluate the request together with the list of tools it has access to, and iterate through a series of tools execution and evaluation to fulfil the request.You can view the trace output to see the tools that the agent used. For each tool used, you can see the values of the input parameters, followed by the corresponding results. In this case, the agent operated as follows: 
 
 The agent first used the function executeTrade with input parameters of ticker=AMZN, quantity=100, and price=186 
 After the trade was executed, used the allocateTrade tool to allocate the trade position between two portfolio accounts 
 
Clean up 
You will incur charges when you consume the services used in this solution. Instructions to clean up the resources are available in the README.md of the GitHub repository. 
Summary 
This solution offers a straightforward and enterprise-ready approach to implement MCP servers on AWS. With this centralized operating model, teams can focus on building their applications rather than maintaining the MCP servers. As enterprises continue to embrace agentic workflows, centralized MCP servers offer a practical solution for overcoming operational silos and inefficiencies. With the AWS scalable infrastructure and advanced tools like Amazon Bedrock Agents and Amazon ECS, enterprises can accelerate their journey toward smarter workflows and better customer outcomes. 
Check out the GitHub repository to replicate the solution in your own AWS environment. 
To learn more about how to run MCP servers on AWS, refer to the following resources: 
 
 Harness the power of MCP servers with Amazon Bedrock Agents 
 Unlocking the power of Model Context Protocol (MCP) on AWS 
 Amazon Bedrock Agents Samples GitHub repository 
 
 
 
About the authors 
Xan Huang is a Senior Solutions Architect with AWS and is based in Singapore. He works with major financial institutions to design and build secure, scalable, and highly available solutions in the cloud. Outside of work, Xan dedicates most of his free time to his family, where he lovingly takes direction from his two young daughters, aged one and four. You can find Xan on LinkedIn: https://www.linkedin.com/in/xanhuang/ 
Vikesh Pandey&nbsp;is a Principal GenAI/ML Specialist Solutions Architect at AWS helping large financial institutions adopt and scale generative AI and ML workloads. He is the author of book ‚ÄúGenerative AI for financial services.‚Äù He carries more than decade of experience building enterprise-grade applications on generative AI/ML and related technologies. In his spare time, he plays an unnamed sport with his son that lies somewhere between football and rugby.
‚Ä¢ Choosing the right approach for generative AI-powered structured data retrieval
  Organizations want direct answers to their business questions without the complexity of writing SQL queries or navigating through business intelligence (BI) dashboards to extract data from structured data stores. Examples of structured data include tables, databases, and data warehouses that conform to a predefined schema. Large language model (LLM)-powered natural language query systems transform how we interact with data, so you can ask questions like ‚ÄúWhich region has the highest revenue?‚Äù and receive immediate, insightful responses. Implementing these capabilities requires careful consideration of your specific needs‚Äîwhether you need to integrate knowledge from other systems (for example, unstructured sources like documents), serve internal or external users, handle the analytical complexity of questions, or customize responses for business appropriateness, among other factors. 
In this post, we discuss LLM-powered structured data query patterns in AWS. We provide a decision framework to help you select the best pattern for your specific use case. 
Business challenge: Making structured data accessible 
Organizations have vast amounts of structured data but struggle to make it effectively accessible to non-technical users for several reasons: 
 
 Business users lack the technical knowledge (like SQL) needed to query data 
 Employees rely on BI teams or data scientists for analysis, limiting self-service capabilities 
 Gaining insights often involves time delays that impact decision-making 
 Predefined dashboards constrain spontaneous exploration of data 
 Users might not know what questions are possible or where relevant data resides 
 
Solution overview 
An effective solution should provide the following: 
 
 A conversational interface that allows employees to query structured data sources without technical expertise 
 The ability to ask questions in everyday language and receive accurate, trustworthy answers 
 Automatic generation of visualizations and explanations to clearly communicate insights. 
 Integration of information from different data sources (both structured and unstructured) presented in a unified manner 
 Ease of integration with existing investments and rapid deployment capabilities 
 Access restriction based on identities, roles, and permissions 
 
In the following sections, we explore five patterns that can address these needs, highlighting the architecture, ideal use cases, benefits, considerations, and implementation resources for each approach. 
Pattern 1: Direct conversational interface using an enterprise assistant 
This pattern uses Amazon Q Business, a generative AI-powered assistant, to provide a chat interface on data sources with native connectors. When users ask questions in natural language, Amazon Q Business connects to the data source, interprets the question, and retrieves relevant information without requiring intermediate services. The following diagram illustrates this workflow. 
 
This approach is ideal for internal enterprise assistants that need to answer business user-facing questions from both structured and unstructured data sources in a unified experience. For example, HR personnel can ask ‚ÄúWhat‚Äôs our parental leave policy and how many employees used it last quarter?‚Äù and receive answers drawn from both leave policy documentation and employee databases together in one interaction. With this pattern, you can benefit from the following: 
 
 Simplified connectivity through the extensive Amazon Q Business library of built-in connectors 
 Streamlined implementation with a single service to configure and manage 
 Unified search experience for accessing both structured and unstructured information 
 Built-in understanding and respect existing identities, roles, and permissions 
 
You can define the scope of data to be pulled in the form of a SQL query. Amazon Q Business pre-indexes database content based on defined SQL queries and uses this index when responding to user questions. Similarly, you can define the sync mode and schedule to determine how often you want to update your index. Amazon Q Business does the heavy lifting of indexing the data using a Retrieval Augmented Generation (RAG) approach and using an LLM to generate well-written answers. For more details on how to set up Amazon Q Business with an Amazon Aurora PostgreSQL-Compatible Edition connector, see Discover insights from your Amazon Aurora PostgreSQL database using the Amazon Q Business connector. You can also refer to the complete list of supported data source connectors. 
Pattern 2: Enhancing BI tool with natural language querying capabilities 
This pattern uses Amazon Q in QuickSight to process natural language queries against datasets that have been previously configured in Amazon QuickSight. Users can ask questions in everyday language within the QuickSight interface and get visualized answers without writing SQL. This approach works with QuickSight (Enterprise or Q edition) and supports various data sources, including Amazon Relational Database Service (Amazon RDS), Amazon Redshift, Amazon Athena, and others. The architecture is depicted in the following diagram. 
 
This pattern is well-suited for internal BI and analytics use cases. Business analysts, executives, and other employees can ask ad-hoc questions to get immediate visualized insights in the form of dashboards. For example, executives can ask questions like ‚ÄúWhat were our top 5 regions by revenue last quarter?‚Äù and immediately see responsive charts, reducing dependency on analytics teams. The benefits of this pattern are as follows: 
 
 It enables natural language queries that produce rich visualizations and charts 
 No coding or machine learning (ML) experience is needed‚Äîthe heavy lifting like natural language interpretation and SQL generation is managed by Amazon Q in QuickSight 
 It integrates seamlessly within the familiar QuickSight dashboard environment 
 
Existing QuickSight users might find this the most straightforward way to take advantage of generative AI benefits. You can optimize this pattern for higher-quality results by configuring topics like curated fields, synonyms, and expected question phrasing. This pattern will pull data only from a specific configured data source in QuickSight to produce a dashboard as an output. For more details, check out QuickSight DemoCentral to view a demo in QuickSight, see the generative BI learning dashboard, and view guided instructions to create dashboards with Amazon Q. Also refer to the list of supported data sources. 
Pattern 3: Combining BI visualization with conversational AI for a seamless experience 
This pattern merges BI visualization capabilities with conversational AI to create a seamless knowledge experience. By integrating Amazon Q in QuickSight with Amazon Q Business (with the QuickSight plugin enabled), organizations can provide users with a unified conversational interface that draws on both unstructured and structured data. The following diagram illustrates the architecture. 
 
This is ideal for enterprises that want an internal AI assistant to answer a variety of questions‚Äîwhether it‚Äôs a metric from a database or knowledge from a document. For example, executives can ask ‚ÄúWhat was our Q4 revenue growth?‚Äù and see visualized results from data warehouses through Amazon Redshift through QuickSight, then immediately follow up with ‚ÄúWhat is our company vacation policy?‚Äù to access HR documentation‚Äîall within the same conversation flow. This pattern offers the following benefits: 
 
 It unifies answers from structured data (databases and warehouses) and unstructured data (documents, wikis, emails) in a single application 
 It delivers rich visualizations alongside conversational responses in a seamless experience with real-time analysis in chat 
 There is no duplication of work‚Äîif your BI team has already built datasets and topics in QuickSight for analytics, you use that in Amazon Q Business 
 It maintains conversational context when switching between data and document-based inquiries 
 
For more details, see Query structured data from Amazon Q Business using Amazon QuickSight integration and Amazon Q Business now provides insights from your databases and data warehouses (preview). 
Another variation of this pattern is recommended for BI users who want to expose unified data through rich visuals in QuickSight, as illustrated in the following diagram. 
 
For more details, see Integrate unstructured data into Amazon QuickSight using Amazon Q Business. 
Pattern 4: Building knowledge bases from structured data using managed text-to-SQL 
This pattern uses Amazon Bedrock Knowledge Bases to enable structured data retrieval. The service provides a fully managed text-to-SQL module that alleviates common challenges in developing natural language query applications for structured data. This implementation uses Amazon Bedrock (Amazon Bedrock Agents and Amazon Bedrock Knowledge Bases) along with your choice of data warehouse such as Amazon Redshift or Amazon SageMaker Lakehouse. The following diagram illustrates the workflow. 
 
For example, a seller can use this capability embedded into an ecommerce application to ask a complex query like ‚ÄúGive me top 5 products whose sales increased by 50% last year as compared to previous year? Also group the results by product category.‚Äù The system automatically generates the appropriate SQL, executes it against the data sources, and delivers results or a summarized narrative. This pattern features the following benefits: 
 
 It provides fully managed text-to-SQL capabilities without requiring model training 
 It enables direct querying of data from the source without data movement 
 It supports complex analytical queries on warehouse data 
 It offers flexibility in foundation model (FM) selection through Amazon Bedrock 
 API connectivity, personalization options, and context-aware chat features make it better suited for customer facing applications 
 
Choose this pattern when you need a flexible, developer-oriented solution. This approach works well for applications (internal or external) where you control the UI design. Default outputs are primarily text or structured data. However, executing arbitrary SQL queries can be a security risk for text-to-SQL applications. It is recommended that you take precautions as needed, such as using restricted roles, read-only databases, and sandboxing. For more information on how to build this pattern, see Empower financial analytics by creating structured knowledge bases using Amazon Bedrock and Amazon Redshift. For a list of supported structured data stores, refer to Create a knowledge base by connecting to a structured data store. 
Pattern 5: Custom text-to-SQL implementation with flexible model selection 
This pattern represents a build-your-own solution using FMs to convert natural language to SQL, execute queries on data warehouses, and return results. Choose Amazon Bedrock when you want to quickly integrate this capability without deep ML expertise‚Äîit offers a fully managed service with ready-to-use FMs through a unified API, handling infrastructure needs with pay-as-you-go pricing. Alternatively, select Amazon SageMaker AI when you require extensive model customization to build specialized needs‚Äîit provides complete ML lifecycle tools for data scientists and ML engineers to build, train, and deploy custom models with greater control. For more information, refer to our Amazon Bedrock or Amazon SageMaker AI decision guide. The following diagram illustrates the architecture. 
 
Use this pattern if your use case requires specific open-weight models, or you want to fine-tune models on your domain-specific data. For example, if you need highly accurate results for your query, then you can use this pattern to fine-tune models on specific schema structures, while maintaining the flexibility to integrate with existing workflows and multi-cloud environments. This pattern offers the following benefits: 
 
 It provides maximum customization in model selection, fine-tuning, and system design 
 It supports complex logic across multiple data sources 
 It offers complete control over security and deployment in your virtual private cloud (VPC) 
 It enables flexible interface implementation (Slack bots, custom web UIs, notebook plugins) 
 You can implement it for external user-facing solutions 
 
For more information on steps to build this pattern, see Build a robust text-to-SQL solution generating complex queries, self-correcting, and querying diverse data sources. 
Pattern comparison: Making the right choice 
To make effective decisions, let‚Äôs compare these patterns across key criteria. 
Data workload suitability 
Different out-of-the-box patterns handle transactional (operational) and analytical (historical or aggregated) data with varying degrees of effectiveness. Patterns 1 and 3, which use Amazon Q Business, work with indexed data and are optimized for lookup-style queries against previously indexed content rather than real-time transactional database queries. Pattern 2, which uses Amazon Q in QuickSight, gets visual output for transactional information for ad-hoc analysis. Pattern 4, which uses Amazon Bedrock structured data retrieval, is specifically designed for analytical systems and data warehouses, excelling at complex queries on large datasets. Pattern 5 is a self-managed text-to-SQL option that can be built to support both transactional or analytical needs of users. 
Target audience 
Architectures highlighted in Patterns 1, 2, and 3 (using Amazon Q Business, Amazon Q in QuickSight, or a combination) are best suited for internal enterprise use. However, you can use Amazon QuickSight Embedded to embed data visuals, dashboards, and natural language queries into both internal or customer-facing applications. Amazon Q Business serves as an enterprise AI assistant for organizational knowledge that uses subscription-based pricing tiers that is designed for internal employees. Pattern 4 (using Amazon Bedrock) can be used to build both internal as well as customer-facing applications. This is because, unlike the subscription-based model of Amazon Q Business, Amazon Bedrock provides API-driven services that alleviate per-user costs and identity management overhead for external customer scenarios. This makes it well-suited for customer-facing experiences where you need to serve potentially thousands of external users. The custom LLM solutions in Pattern 5 can similarly be tailored to external application requirements. 
Interface and output format 
Different patterns deliver answers through different interaction models: 
 
 Conversational experiences ‚Äì Patterns 1 and 3 (using Amazon Q Business) provide chat-based interfaces. Pattern 4 (using Amazon Bedrock Knowledge Bases for structured data retrieval) naturally supports AI assistant integration, and Pattern 5 (a custom text-to-SQL solution) can be designed for a variety of interaction models. 
 Visualization-focused output ‚Äì Pattern 2 (using Amazon Q in QuickSight) specializes in generating on-the-fly visualizations such as charts and tables in response to user questions. 
 API integration ‚Äì For embedding capabilities into existing applications, Patterns 4 and 5 offer the most flexible API-based integration options. 
 
The following figure is a comparison matrix of AWS structured data query patterns. 
 
Conclusion 
Between these patterns, your optimal choice depends on the following key factors: 
 
 Data location and characteristics ‚Äì Is your data in operational databases, already in a data warehouse, or distributed across various sources? 
 User profile and interaction model ‚Äì Are you supporting internal or external users? Do they prefer conversational or visualization-focused interfaces? 
 Available resources and expertise ‚Äì Do you have ML specialists available, or do you need a fully managed solution? 
 Accuracy and governance requirements ‚Äì Do you need strictly controlled semantics and curation, or is broader query flexibility acceptable with monitoring? 
 
By understanding these patterns and their trade-offs, you can architect solutions that align with your business objectives. 
 
About the authors 
Akshara Shah is a Senior Solutions Architect at Amazon Web Services. She helps commercial customers build cloud-based generative AI services to meet their business needs. She has been designing, developing, and implementing solutions that leverage AI and ML technologies for more than 10 years. Outside of work, she loves painting, exercising and spending time with family. 
Sanghwa Na is a Generative AI Specialist Solutions Architect at Amazon Web Services. Based in San Francisco, he works with customers to design and build generative AI solutions using large language models and foundation models on AWS. He focuses on helping organizations adopt AI technologies that drive real business value
‚Ä¢ Revolutionizing drug data analysis using Amazon Bedrock multimodal RAG capabilities
  In the pharmaceutical industry, biotechnology and healthcare companies face an unprecedented challenge for efficiently managing and analyzing vast amounts of drug-related data from diverse sources. Traditional data analysis methods prove inadequate for processing complex medical documentation that includes a mix of text, images, graphs, and tables. Amazon Bedrock offers features like multimodal retrieval, advanced chunking capabilities, and citations to help organizations get high-accuracy responses. 
Pharmaceutical and healthcare organizations process a vast number of complex document formats and unstructured data that pose analytical challenges. Clinical study documents and research papers related to them typically present an intricate blend of technical text, detailed tables, and sophisticated statistical graphs, making automated data extraction particularly challenging. Clinical study documents present additional challenges through non-standardized formatting and varied data presentation styles across multiple research institutions. This post showcases a solution to extract data-driven insights from complex research documents through a sample application with high-accuracy responses. It analyzes clinical trial data, patient outcomes, molecular diagrams, and safety reports from the research documents. It can help pharmaceutical companies accelerate their research process. The solution provides citations from the source documents, reducing hallucinations and enhancing the accuracy of the responses. 
Solution overview 
The sample application uses Amazon Bedrock to create an intelligent AI assistant that analyzes and summarizes research documents containing text, graphs, and unstructured data. Amazon Bedrock is a fully managed service that offers a choice of industry-leading foundation models (FMs) along with a broad set of capabilities to build generative AI applications, simplifying development with security, privacy, and responsible AI. 
To equip FMs with up-to-date and proprietary information, organizations use Retrieval Augmented Generation (RAG), a technique that fetches data from company data sources and enriches the prompt to provide relevant and accurate responses. 
Amazon Bedrock Knowledge Bases is a fully managed RAG capability within Amazon Bedrock with in-built session context management and source attribution that helps you implement the entire RAG workflow, from ingestion to retrieval and prompt augmentation, without having to build custom integrations to data sources and manage data flows. 
Amazon Bedrock Knowledge Bases introduces powerful document parsing capabilities, including Amazon Bedrock Data Automation powered parsing and FM parsing, revolutionizing how we handle complex documents. Amazon Bedrock Data Automation is a fully managed service that processes multimodal data effectively, without the need to provide additional prompting. The FM option parses multimodal data using an FM. This parser provides the option to customize the default prompt used for data extraction. This advanced feature goes beyond basic text extraction by intelligently breaking down documents into distinct components, including text, tables, images, and metadata, while preserving document structure and context. When working with supported formats like PDF, specialized FMs interpret and extract tabular data, charts, and complex document layouts. Additionally, the service provides advanced chunking strategies like semantic chunking, which intelligently divides text into meaningful segments based on semantic similarity calculated by the embedding model. Unlike traditional syntactic chunking methods, this approach preserves the context and meaning of the content, improving the quality and relevance of information retrieval. 
The solution architecture implements these capabilities through a seamless workflow that begins with administrators securely uploading knowledge base documents to an Amazon Simple Storage Service (Amazon S3) bucket. These documents are then ingested into Amazon Bedrock Knowledge Bases, where a large language model (LLM) processes and parses the ingested data. The solution employs semantic chunking to store document embeddings efficiently in Amazon OpenSearch Service for optimized retrieval. The solution features a user-friendly interface built with Streamlit, providing an intuitive chat experience for end-users. When users interact with the Streamlit application, it triggers AWS Lambda functions that handle the requests, retrieving relevant context from the knowledge base and generating appropriate responses. The architecture is secured through AWS Identity and Access Management (IAM), maintaining proper access control throughout the workflow. Amazon Bedrock uses AWS Key Management Service (AWS KMS) to encrypt resources related to your knowledge bases. By default, Amazon Bedrock encrypts this data using an AWS managed key. Optionally, you can encrypt the model artifacts using a customer managed key. This end-to-end solution provides efficient document processing, context-aware information retrieval, and secure user interactions, delivering accurate and comprehensive responses through a seamless chat interface. 
The following diagram illustrates the solution architecture. 
 
This solution uses the following additional services and features: 
 
 The Anthropic Claude 3 family offers Opus, Sonnet, and Haiku models that accept text, image, and video inputs and generate text output. They provide a broad selection of capability, accuracy, speed, and cost operation points. These models understand complex research documents that include charts, graphs, tables, diagrams, and reports. 
 AWS Lambda is a serverless computing service that empowers you to run code without provisioning or managing servers cost effectively. 
 Amazon S3 is a highly scalable, durable, and secure object storage service. 
 Amazon OpenSearch Service is a fully managed search and analytics engine for efficient document retrieval. The OpenSearch Service vector database capabilities enable semantic search, RAG with LLMs, recommendation engines, and search rich media. 
 Streamlit is a faster way to build and share data applications using interactive web-based data applications in pure Python. 
 
Prerequisites 
The following prerequisites are needed to proceed with this solution. For this post, we use the us-east-1 AWS Region. For details on available Regions, see Amazon Bedrock endpoints and quotas. 
 
 An AWS account with an IAM user who has permissions to Lambda, Amazon Bedrock, Amazon S3, and IAM. 
 Access to Anthropic‚Äôs Claude model in Amazon Bedrock. For instructions, see Access Amazon Bedrock foundation models. 
 
Deploy the solution 
Refer to the GitHub repository for the deployment steps listed under the deployment guide section. We use an AWS CloudFormation template to deploy solution resources, including S3 buckets to store the source data and knowledge base data. 
Test the sample application 
Imagine you are a member of an R&amp;D department for a biotechnology firm, and your job requires you to derive insights from drug- and vaccine-related information from diverse sources like research studies, drug specifications, and industry papers. You are performing research on cancer vaccines and want to gain insights based on cancer research publications. You can upload the documents given in the reference section to the S3 bucket and sync the knowledge base. Let‚Äôs explore example interactions that demonstrate the application‚Äôs capabilities. The responses generated by the AI assistant are based on the documents uploaded to the S3 bucket connected with the knowledge base. Due to non-deterministic nature of machine learning (ML), your responses might be slightly different from the ones presented in this post. 
Understanding historical context 
We use the following query: ‚ÄúCreate a timeline of major developments in mRNA vaccine technology for cancer treatment based on the information provided in the historical background sections.‚ÄùThe assistant analyzes multiple documents and presents a chronological progression of mRNA vaccine development, including key milestones based on the chunks of information retrieved from the OpenSearch Service vector database. 
The following screenshot shows the AI assistant‚Äôs response. 
 
Complex data analysis 
We use the following query: ‚ÄúSynthesize the information from the text, figures, and tables to provide a comprehensive overview of the current state and future prospects of therapeutic cancer vaccines.‚Äù 
The AI assistant is able to provide insights from complex data types, which is enabled by FM parsing, while ingesting the data to OpenSearch Service. It is also able to provide images in the source attribution using the multimodal data capabilities of Amazon Bedrock Knowledge Bases. 
The following screenshot shows the AI assistant‚Äôs response. 
 
The following screenshot shows the visuals provided in the citations when the mouse hovers over the question mark icon. 
 
Comparative analysis 
We use the following query: ‚ÄúCompare the efficacy and safety profiles of MAGE-A3 and NY-ESO-1 based vaccines as described in the text and any relevant tables or figures.‚Äù 
The AI assistant used the semantically similar chunks returned from the OpenSearch Service vector database and added this context to the user‚Äôs question, which enabled the FM to provide a relevant answer. 
The following screenshot shows the AI assistant‚Äôs response. 
 
Technical deep dive 
We use the following query: ‚ÄúSummarize the potential advantages of mRNA vaccines over DNA vaccines for targeting tumor angiogenesis, as described in the review.‚Äù 
With the semantic chunking feature of the knowledge base, the AI assistant was able to get the relevant context from the OpenSearch Service database with higher accuracy. 
The following screenshot shows the AI assistant‚Äôs response. 
 
The following screenshot shows the diagram that was used for the answer as one of the citations. 
 
The sample application demonstrates the following: 
 
 Accurate interpretation of complex scientific diagrams 
 Precise extraction of data from tables and graphs 
 Context-aware responses that maintain scientific accuracy 
 Source attribution for provided information 
 Ability to synthesize information across multiple documents 
 
This application can help you quickly analyze vast amounts of complex scientific literature, extracting meaningful insights from diverse data types while maintaining accuracy and providing proper attribution to source materials. This is enabled by the advanced features of the knowledge bases, including FM parsing, which aides in interpreting complex scientific diagrams and extraction of data from tables and graphs, semantic chunking, which aides with high-accuracy context-aware responses, and multimodal data capabilities, which aides in providing relevant images as source attribution. 
These are some of the many new features added to Amazon Bedrock, empowering you to generate high-accuracy results depending on your use case. To learn more, see New Amazon Bedrock capabilities enhance data processing and retrieval. 
Production readiness 
The proposed solution accelerates the time to value of the project development process. Solutions built on the AWS Cloud benefit from inherent scalability while maintaining robust security and privacy controls. 
The security and privacy framework includes fine-grained user access controls using IAM for both OpenSearch Service and Amazon Bedrock services. In addition, Amazon Bedrock enhances security by providing encryption at rest and in transit, and private networking options using virtual private cloud (VPC) endpoints. Data protection is achieved using KMS keys, and API calls and usage are tracked through Amazon CloudWatch logs and metrics. For specific compliance validation for Amazon Bedrock, see Compliance validation for Amazon Bedrock. 
For additional details on moving RAG applications to production, refer to From concept to reality: Navigating the Journey of RAG from proof of concept to production. 
Clean up 
Complete the following steps to clean up your resources. 
 
 Empty the SourceS3Bucket and KnowledgeBaseS3BucketName buckets. 
 Delete the main CloudFormation stack. 
 
Conclusion 
This post demonstrated the powerful multimodal document analysis (text, graphs, images) using advanced parsing and chunking features of Amazon Bedrock Knowledge Bases. By combining the powerful capabilities of Amazon Bedrock FMs, OpenSearch Service, and intelligent chunking strategies through Amazon Bedrock Knowledge Bases, organizations can transform their complex research documents into searchable, actionable insights. The integration of semantic chunking makes sure that document context and relationships are preserved, and the user-friendly Streamlit interface makes the system accessible to end-users through an intuitive chat experience. This solution not only streamlines the process of analyzing research documents, but also demonstrates the practical application of AI/ML technologies in enhancing knowledge discovery and information retrieval. As organizations continue to grapple with increasing volumes of complex documents, this scalable and intelligent system provides a robust framework for extracting maximum value from their document repositories. 
Although our demonstration focused on the healthcare industry, the versatility of this technology extends beyond a single industry. RAG on Amazon Bedrock has proven its value across diverse sectors. Notable adopters include global brands like Adidas in retail, Empolis in information management, Fractal Analytics in AI solutions, Georgia Pacific in manufacturing, and Nasdaq in financial services. These examples illustrate the broad applicability and transformative potential of RAG technology across various business domains, highlighting its ability to drive innovation and efficiency in multiple industries. 
Refer to the GitHub repo for the agentic RAG application, including samples and components for building agentic RAG solutions. Be on the lookout for additional features and samples in the repository in the coming months. 
To learn more about Amazon Bedrock Knowledge Bases, check out the RAG workshop using Amazon Bedrock. Get started with Amazon Bedrock Knowledge Bases, and let us know your thoughts in the comments section. 
References 
The following are sample research documents available with an open access distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license https://creativecommons.org/licenses/by/4.0/: 
 
 Vaccine Therapies for&nbsp;Cancer: Then and&nbsp;Now 
 Therapeutic cancer vaccines: advancements, challenges and prospects 
 Cancer Vaccines: Adjuvant Potency, Importance of Age, Lifestyle, and Treatments 
 Recent advances in mRNA cancer vaccines: meeting challenges and embracing opportunities 
 Nucleic acid cancer vaccines targeting tumor related angiogenesis. Could mRNA vaccines constitute a game changer? 
 Recent Findings on Therapeutic Cancer Vaccines: An Updated Review 
 Cancer Vaccines: Antigen Selection Strategy 
 
 
 
About the authors 
Vivek Mittal is a Solution Architect at Amazon Web Services, where he helps organizations architect and implement cutting-edge cloud solutions. With a deep passion for Generative AI, Machine Learning, and Serverless technologies, he specializes in helping customers harness these innovations to drive business transformation. He finds particular satisfaction in collaborating with customers to turn their ambitious technological visions into reality. 
Shamika Ariyawansa, serving as a Senior AI/ML Solutions Architect in the Global Healthcare and Life Sciences division at Amazon Web Services (AWS), has a keen focus on Generative AI. He assists customers in integrating Generative AI into their projects, emphasizing the importance of explainability within their AI-driven initiatives. Beyond his professional commitments, Shamika passionately pursues skiing and off-roading adventures. 
Shaik Abdulla is a Sr. Solutions Architect, specializes in architecting enterprise-scale cloud solutions with focus on Analytics, Generative AI and emerging technologies. His technical expertise is validated by his achievement of all 12 AWS certifications and the prestigious Golden jacket recognition. He has a passion to architect and implement innovative cloud solutions that drive business transformation. He speaks at major industry events like AWS re:Invent and regional AWS Summits, where he shares insights on cloud architecture and emerging technologies.
‚Ä¢ Build and deploy AI inference workflows with new enhancements to the Amazon SageMaker Python SDK
  Amazon SageMaker Inference has been a popular tool for deploying advanced machine learning (ML) and generative AI models at scale. As AI applications become increasingly complex, customers want to deploy multiple models in a coordinated group that collectively process inference requests for an application. In addition, with the evolution of generative AI applications, many use cases now require inference workflows‚Äîsequences of interconnected models operating in predefined logical flows. This trend drives a growing need for more sophisticated inference offerings. 
To address this need, we are introducing a new capability in the SageMaker Python SDK that revolutionizes how you build and deploy inference workflows on SageMaker. We will take Amazon Search as an example to show case how this feature is used in helping customers building inference workflows. This new Python SDK capability provides a streamlined and simplified experience that abstracts away the underlying complexities of packaging and deploying groups of models and their collective inference logic, allowing you to focus on what matter most‚Äîyour business logic and model integrations. 
In this post, we provide an overview of the user experience, detailing how to set up and deploy these workflows with multiple models using the SageMaker Python SDK. We walk through examples of building complex inference workflows, deploying them to SageMaker endpoints, and invoking them for real-time inference. We also show how customers like Amazon Search plan to use SageMaker Inference workflows to provide more relevant search results to Amazon shoppers. 
Whether you are building a simple two-step process or a complex, multimodal AI application, this new feature provides the tools you need to bring your vision to life. This tool aims to make it easy for developers and businesses to create and manage complex AI systems, helping them build more powerful and efficient AI applications. 
In the following sections, we dive deeper into details of the SageMaker Python SDK, walk through practical examples, and showcase how this new capability can transform your AI development and deployment process. 
Key improvements and user experience 
The SageMaker Python SDK now includes new features for creating and managing inference workflows. These additions aim to address common challenges in developing and deploying inference workflows: 
 
 Deployment of multiple models&nbsp;‚Äì The core of this new experience is the deployment of multiple models as&nbsp;inference components&nbsp;within a single SageMaker endpoint. With this approach, you can create a more unified inference workflow. By consolidating multiple models into one endpoint, you can reduce the number of endpoints that need to be managed. This consolidation can also improve operational tasks, resource utilization, and potentially costs. 
 Workflow definition with workflow mode&nbsp;‚Äì The new workflow mode extends the existing&nbsp;Model Builder capabilities. It allows for the definition of inference workflows using Python code. Users familiar with the ModelBuilder class might find this feature to be an extension of their existing knowledge. This mode enables creating multi-step workflows, connecting models, and specifying the data flow between different models in the workflows. The goal is to reduce the complexity of managing these workflows and enable you to focus more on the logic of the resulting compound AI system. 
 Development and deployment options&nbsp;‚Äì A new deployment option has been introduced for the development phase. This feature is designed to allow for quicker deployment of workflows to development environments. The intention is to enable faster testing and refinement of workflows. This could be particularly relevant when experimenting with different configurations or adjusting models. 
 Invocation flexibility&nbsp;‚Äì The SDK now provides options for invoking individual models or entire workflows. You can choose to call a specific inference component used in a workflow or the entire workflow. This flexibility can be useful in scenarios where access to a specific model is needed, or when only a portion of the workflow needs to be executed. 
 Dependency management&nbsp;‚Äì You can use SageMaker&nbsp;Deep Learning Containers&nbsp;(DLCs) or the SageMaker distribution that comes preconfigured with various model serving libraries and tools. These are intended to serve as a starting point for common use cases. 
 
To get started, use the SageMaker Python SDK to deploy your models as inference components. Then, use the workflow mode to create an inference workflow, represented as Python code using the container of your choice. Deploy the workflow container as another inference component on the same endpoints as the models or a dedicated endpoint. You can run the workflow by invoking the inference component that represents the workflow. The user experience is entirely code-based, using the SageMaker Python SDK. This approach allows you to define, deploy, and manage inference workflows using SDK abstractions offered by this feature and Python programming. The workflow mode provides flexibility to specify complex sequences of model invocations and data transformations, and the option to deploy as components or endpoints caters to various scaling and integration needs. 
Solution overview 
The following diagram illustrates a reference architecture using the SageMaker Python SDK. 
 
The improved SageMaker Python SDK introduces a more intuitive and flexible approach to building and deploying AI inference workflows. Let‚Äôs explore the key components and classes that make up the experience: 
 
 ModelBuilder simplifies the process of packaging individual models as inference components. It handles model loading, dependency management, and container configuration automatically. 
 The CustomOrchestrator class provides a standardized way to define custom inference logic that orchestrates multiple models in the workflow. Users implement the handle() method to specify this logic and can use an orchestration library or none at all (plain Python). 
 A single deploy() call handles the deployment of the components and workflow orchestrator. 
 The Python SDK supports invocation against the custom inference workflow or individual inference components. 
 The Python SDK supports both synchronous and streaming inference. 
 
CustomOrchestrator&nbsp;is an abstract base class that serves as a template for defining custom inference orchestration logic. It standardizes the structure of entry point-based inference scripts, making it straightforward for users to create consistent and reusable code. The handle method in the class is an abstract method that users implement to define their custom orchestration logic. 
 
  
  class CustomOrchestrator (ABC):
"""
Templated class used to standardize the structure of an entry point based inference script.
"""

    @abstractmethod
    def handle(self, data, context=None):
        """abstract class for defining an entrypoint for the model server"""
        return NotImplemented 
  
 
With this templated class, users can integrate into their custom workflow code, and then point to this code in the model builder using a file path or directly using a class or method name. Using this class and the&nbsp;ModelBuilder&nbsp;class, it enables a more streamlined workflow for AI inference: 
 
 Users define their custom workflow by implementing the CustomOrchestrator class. 
 The custom CustomOrchestrator is passed to ModelBuilder using the ModelBuilder inference_spec parameter. 
 ModelBuilder packages the CustomOrchestrator along with the model artifacts. 
 The packaged model is deployed to a SageMaker endpoint (for example, using a TorchServe container). 
 When invoked, the SageMaker endpoint uses the custom handle() function defined in the CustomOrchestrator to handle the input payload. 
 
In the follow sections, we provide two examples of custom workflow orchestrators implemented with plain Python code. For simplicity, the examples use two inference components. 
We explore how to create a simple workflow that deploys two large language models (LLMs) on SageMaker Inference endpoints along with a simple Python orchestrator that calls the two models. We create an IT customer service workflow where one model processes the initial request and another suggests solutions. You can find the example notebook in the&nbsp;GitHub repo. 
Prerequisites 
To run the example notebooks, you need an AWS account with an&nbsp;AWS Identity and Access Management&nbsp;(IAM) role with&nbsp;least-privilege permissions&nbsp;to manage resources created. For details, refer to&nbsp;Create an AWS account. You might need to request a service quota increase for the corresponding SageMaker hosting instances. In this example, we host multiple models on the same SageMaker endpoint, so we use two ml.g5.24xlarge SageMaker hosting instances. 
Python inference orchestration 
First, let‚Äôs define our custom orchestration class that inherits from CustomOrchestrator. The workflow is structured around a custom inference entry point that handles the request data, processes it, and retrieves predictions from the configured model endpoints. See the following code: 
 
 class PythonCustomInferenceEntryPoint(CustomOrchestrator):
    def __init__(self, region_name, endpoint_name, component_names):
        self.region_name = region_name
        self.endpoint_name = endpoint_name
        self.component_names = component_names
    
    def preprocess(self, data):
        payload = {
            "inputs": data.decode("utf-8")
        }
        return json.dumps(payload)

    def _invoke_workflow(self, data):
        # First model (Llama) inference
        payload = self.preprocess(data)
        
        llama_response = self.client.invoke_endpoint(
            EndpointName=self.endpoint_name,
            Body=payload,
            ContentType="application/json",
            InferenceComponentName=self.component_names[0]
        )
        llama_generated_text = json.loads(llama_response.get('Body').read())['generated_text']
        
        # Second model (Mistral) inference
        parameters = {
            "max_new_tokens": 50
        }
        payload = {
            "inputs": llama_generated_text,
            "parameters": parameters
        }
        mistral_response = self.client.invoke_endpoint(
            EndpointName=self.endpoint_name,
            Body=json.dumps(payload),
            ContentType="application/json",
            InferenceComponentName=self.component_names[1]
        )
        return {"generated_text": json.loads(mistral_response.get('Body').read())['generated_text']}
    
    def handle(self, data, context=None):
        return self._invoke_workflow(data) 
 
This code performs the following functions: 
 
 Defines the orchestration that sequentially calls two models using their inference component names 
 Processes the response from the first model before passing it to the second model 
 Returns the final generated response 
 
This plain Python approach provides flexibility and control over the request-response flow, enabling seamless cascading of outputs across multiple model components. 
Build and deploy the workflow 
To deploy the workflow, we first create our inference components and then build the custom workflow. One inference component will host a Meta Llama 3.1 8B model, and the other will host a Mistral 7B model. 
 
  
  from sagemaker.serve import ModelBuilder
from sagemaker.serve.builder.schema_builder import SchemaBuilder

# Create a ModelBuilder instance for Llama 3.1 8B
# Pre-benchmarked ResourceRequirements will be taken from JumpStart, as Llama-3.1-8b is a supported model.
llama_model_builder = ModelBuilder(
    model="meta-textgeneration-llama-3-1-8b",
    schema_builder=SchemaBuilder(sample_input, sample_output),
    inference_component_name=llama_ic_name,
    instance_type="ml.g5.24xlarge"
)

# Create a ModelBuilder instance for Mistral 7B model.
mistral_mb = ModelBuilder(
    model="huggingface-llm-mistral-7b",
    instance_type="ml.g5.24xlarge",
    schema_builder=SchemaBuilder(sample_input, sample_output),
    inference_component_name=mistral_ic_name,
    resource_requirements=ResourceRequirements(
        requests={
           "memory": 49152,
           "num_accelerators": 2,
           "copies": 1
        }
    ),
    instance_type="ml.g5.24xlarge"
) 
  
 
Now we can tie it all together to create one more ModelBuilder to which we pass the modelbuilder_list, which contains the ModelBuilder objects we just created for each inference component and the custom workflow. Then we call the build() function to prepare the workflow for deployment. 
 
  
  # Create workflow ModelBuilder
orchestrator= ModelBuilder(
    inference_spec=PythonCustomInferenceEntryPoint(
        region_name=region,
        endpoint_name=llama_mistral_endpoint_name,
        component_names=[llama_ic_name, mistral_ic_name],
    ),
    dependencies={
        "auto": False,
        "custom": [
            "cloudpickle",
            "graphene",
            # Define other dependencies here.
        ],
    },
    sagemaker_session=Session(),
    role_arn=role,
    resource_requirements=ResourceRequirements(
        requests={
           "memory": 4096,
           "num_accelerators": 1,
           "copies": 1,
           "num_cpus": 2
        }
    ),
    name=custom_workflow_name, # Endpoint name for your custom workflow
    schema_builder=SchemaBuilder(sample_input={"inputs": "test"}, sample_output="Test"),
    modelbuilder_list=[llama_model_builder, mistral_mb] # Inference Component ModelBuilders created in Step 2
)
# call the build function to prepare the workflow for deployment
orchestrator.build() 
  
 
In the preceding code snippet, you can comment out the section that defines the resource_requirements to have the custom workflow deployed on a separate endpoint instance, which can be a dedicated CPU instance to handle the custom workflow payload. 
By calling the deploy() function, we deploy the custom workflow and the inference components to your desired instance type, in this example ml.g5.24.xlarge. If you choose to deploy the custom workflow to a separate instance, by default, it will use the ml.c5.xlarge instance type. You can set inference_workflow_instance_type and inference_workflow_initial_instance_count to configure the instances required to host the custom workflow. 
 
  
  predictors = orchestrator.deploy(
    instance_type="ml.g5.24xlarge",
    initial_instance_count=1,
    accept_eula=True, # Required for Llama3
    endpoint_name=llama_mistral_endpoint_name
    # inference_workflow_instance_type="ml.t2.medium", # default
    # inference_workflow_initial_instance_count=1 # default
) 
  
 
Invoke the endpoint 
After you deploy the workflow, you can invoke the endpoint using the predictor object: 
 
 from sagemaker.serializers import JSONSerializer
predictors[-1].serializer = JSONSerializer()
predictors[-1].predict("Tell me a story about ducks.") 
 
You can also invoke each inference component in the deployed endpoint. For example, we can test the Llama inference component with a synchronous invocation, and Mistral with streaming: 
 
  
  from sagemaker.predictor import Predictor
# create predictor for the inference component of Llama model
llama_predictor = Predictor(endpoint_name=llama_mistral_endpoint_name, component_name=llama_ic_name)
llama_predictor.content_type = "application/json"

llama_predictor.predict(json.dumps(payload)) 
  
 
When handling the streaming response, we need to read each line of the output separately. The following example code demonstrates this streaming handling by checking for newline characters to separate and print each token in real time: 
 
  
  mistral_predictor = Predictor(endpoint_name=llama_mistral_endpoint_name, component_name=mistral_ic_name)
mistral_predictor.content_type = "application/json"

body = json.dumps({
    "inputs": prompt,
    # specify the parameters as needed
    "parameters": parameters
})

for line in mistral_predictor.predict_stream(body):
    decoded_line = line.decode('utf-8')
    if '\n' in decoded_line:
        # Split by newline to handle multiple tokens in the same line
        tokens = decoded_line.split('\n')
        for token in tokens[:-1]:  # Print all tokens except the last one with a newline
            print(token)
        # Print the last token without a newline, as it might be followed by more tokens
        print(tokens[-1], end='')
    else:
        # Print the token without a newline if it doesn't contain '\n'
        print(decoded_line, end='') 
  
 
So far, we have walked through the example code to demonstrate how to build complex inference logic using Python orchestration, deploy them to SageMaker endpoints, and invoke them for real-time inference. The Python SDK automatically handles the following: 
 
 Model packaging and container configuration 
 Dependency management and environment setup 
 Endpoint creation and component coordination 
 
Whether you‚Äôre building a simple workflow of two models or a complex multimodal application, the new SDK provides the building blocks needed to bring your inference workflows to life with minimal boilerplate code. 
Customer story: Amazon Search 
Amazon Search is a critical component of the Amazon shopping experience, processing an enormous volume of queries across billions of products across diverse categories. At the core of this system are sophisticated matching and ranking workflows, which determine the order and relevance of search results presented to customers. These workflows execute large deep learning models in predefined sequences, often sharing models across different workflows to improve price-performance and accuracy. This approach makes sure that whether a customer is searching for electronics, fashion items, books, or other products, they receive the most pertinent results tailored to their query. 
The SageMaker Python SDK enhancement offers valuable capabilities that align well with Amazon Search‚Äôs requirements for these ranking workflows. It provides a standard interface for developing and deploying complex inference workflows crucial for effective search result ranking. The enhanced Python SDK enables efficient reuse of shared models across multiple ranking workflows while maintaining the flexibility to customize logic for specific product categories. Importantly, it allows individual models within these workflows to scale independently, providing optimal resource allocation and performance based on varying demand across different parts of the search system. 
Amazon Search&nbsp;is exploring the broad adoption of these Python SDK enhancements across their search ranking infrastructure. This initiative aims to further refine and improve search capabilities, enabling the team to build, version, and catalog workflows that power search ranking more effectively across different product categories. The ability to share models across workflows and scale them independently offers new levels of efficiency and adaptability in managing the complex search ecosystem. 
Vaclav Petricek, Sr. Manager of Applied Science at Amazon Search, highlighted the potential impact of these SageMaker Python SDK enhancements: ‚ÄúThese capabilities represent a significant advancement in our ability to develop and deploy sophisticated inference workflows that power search matching and ranking. The flexibility to build workflows using Python, share models across workflows, and scale them independently is particularly exciting, as it opens up new possibilities for optimizing our search infrastructure and rapidly iterating on our matching and ranking algorithms as well as new AI features. Ultimately, these SageMaker Inference enhancements will allow us to more efficiently create and manage the complex algorithms powering Amazon‚Äôs search experience, enabling us to deliver even more relevant results to our customers.‚Äù 
The following diagram illustrates a sample solution architecture used by Amazon Search. 
 
Clean up 
When you‚Äôre done testing the models, as a best practice, delete the endpoint to save costs if the endpoint is no longer required. You can follow the cleanup section the demo notebook or use following code to delete the model and endpoint created by the demo: 
 
  
  mistral_predictor.delete_predictor()
llama_predictor.delete_predictor()
llama_predictor.delete_endpoint()
workflow_predictor.delete_predictor() 
  
 
Conclusion 
The new SageMaker Python SDK enhancements for inference workflows mark a significant advancement in the development and deployment of complex AI inference workflows. By abstracting the underlying complexities, these enhancements empower inference customers to focus on innovation rather than infrastructure management. This feature bridges sophisticated AI applications with the robust SageMaker infrastructure, enabling developers to use familiar Python-based tools while harnessing the powerful inference capabilities of SageMaker. 
Early adopters, including Amazon Search, are already exploring how these capabilities can drive major improvements in AI-powered customer experiences across diverse industries. We invite all SageMaker users to explore this new functionality, whether you‚Äôre developing classic ML models, building generative AI applications or multi-model workflows, or tackling multi-step inference scenarios. The enhanced SDK provides the flexibility, ease of use, and scalability needed to bring your ideas to life. As AI continues to evolve, SageMaker Inference evolves with it, providing you with the tools to stay at the forefront of innovation. Start building your next-generation AI inference workflows today with the enhanced SageMaker Python SDK. 
 
About the authors 
Melanie Li, PhD, is a Senior Generative AI Specialist Solutions Architect at AWS based in Sydney, Australia, where her focus is on working with customers to build solutions leveraging state-of-the-art AI and machine learning tools. She has been actively involved in multiple Generative AI initiatives across APJ, harnessing the power of Large Language Models (LLMs). Prior to joining AWS, Dr. Li held data science roles in the financial and retail industries. 
Saurabh Trikande is a Senior Product Manager for Amazon Bedrock and SageMaker Inference. He is passionate about working with customers and partners, motivated by the goal of democratizing AI. He focuses on core challenges related to deploying complex AI applications, inference with multi-tenant models, cost optimizations, and making the deployment of Generative AI models more accessible. In his spare time, Saurabh enjoys hiking, learning about innovative technologies, following TechCrunch, and spending time with his family. 
Osho Gupta&nbsp;is a Senior Software Developer at AWS SageMaker. He is passionate about ML infrastructure space, and is motivated to learn &amp; advance underlying technologies that optimize Gen AI training &amp; inference performance. In his spare time, Osho enjoys paddle boarding, hiking, traveling, and spending time with his friends &amp; family. 
Joseph Zhang is a software engineer at AWS. He started his AWS career at EC2 before eventually transitioning to SageMaker, and now works on developing GenAI-related features. Outside of work he enjoys both playing and watching sports (go Warriors!), spending time with family, and making coffee. 
Gary Wang is a Software Developer at AWS SageMaker. He is passionate about AI/ML operations and building new things. In his spare time, Gary enjoys running, hiking, trying new food, and spending time with his friends and family. 
James Park&nbsp;is a Solutions Architect at Amazon Web Services. He works with Amazon.com to design, build, and deploy technology solutions on AWS, and has a particular interest in AI and machine learning. In h is spare time he enjoys seeking out new cultures, new experiences, &nbsp;and staying up to date with the latest technology trends. You can find him on LinkedIn. 
Vaclav Petricek is a Senior Applied Science Manager at Amazon Search, where he led teams that built Amazon Rufus and now leads science and engineering teams that work on the next generation of Natural Language Shopping. He is passionate about shipping AI experiences that make people‚Äôs lives better. Vaclav loves off-piste skiing, playing tennis, and backpacking with his wife and three children. 
Wei Li is a Senior Software Dev Engineer in Amazon Search. She is passionate about Large Language Model training and inference technologies, and loves integrating these solutions into Search Infrastructure to enhance natural language shopping experiences. During her leisure time, she enjoys gardening, painting, and reading. 
Brian Granger is a Senior Principal Technologist at Amazon Web Services and a professor of physics and data science at Cal Poly State University in San Luis Obispo, CA. He works at the intersection of UX design and engineering on tools for scientific computing, data science, machine learning, and data visualization. Brian is a co-founder and leader of Project Jupyter, co-founder of the Altair project for statistical visualization, and creator of the PyZMQ project for ZMQ-based message passing in Python. At AWS he is a technical and open source leader in the AI/ML organization. Brian also represents AWS as a board member of the PyTorch Foundation. He is a winner of the 2017 ACM Software System Award and the 2023 NASA Exceptional Public Achievement Medal for his work on Project Jupyter. He has a Ph.D. in theoretical physics from the University of Colorado.
‚Ä¢ Context extraction from image files in Amazon Q Business using LLMs
  To effectively convey complex information, organizations increasingly rely on visual documentation through diagrams, charts, and technical illustrations. Although text documents are well-integrated into modern knowledge management systems, rich information contained in diagrams, charts, technical schematics, and visual documentation often remains inaccessible to search and AI assistants. This creates significant gaps in organizational knowledge bases, leading to interpreting visual data manually and preventing automation systems from using critical visual information for comprehensive insights and decision-making. While Amazon Q Business already handles embedded images within documents, the custom document enrichment (CDE) feature extends these capabilities significantly by processing standalone image files (for example, JPGs and PNGs). 
In this post, we look at a step-by-step implementation for using the CDE feature within an Amazon Q Business application. We walk you through an AWS Lambda function configured within CDE to process various image file types, and we showcase an example scenario of how this integration enhances the Amazon Q Business ability to provide comprehensive insights. By following this practical guide, you can significantly expand your organization‚Äôs searchable knowledge base, enabling more complete answers and insights that incorporate both textual and visual information sources. 
Example scenario: Analyzing regional educational demographics 
Consider a scenario where you‚Äôre working for a national educational consultancy that has charts, graphs, and demographic data across different AWS Regions stored in an Amazon Simple Storage Service (Amazon S3) bucket. The following image shows student distribution by age range across various cities using a bar chart. The insights in visualizations like this are valuable for decision-making but traditionally locked within image formats in your S3 buckets and other storage. 
With Amazon Q Business and CDE, we show you how to enable natural language queries against such visualizations. For example, your team could ask questions such as ‚ÄúWhich city has the highest number of students in the 13‚Äì15 age range?‚Äù or ‚ÄúCompare the student demographics between City 1 and City 4‚Äù directly through the Amazon Q Business application interface. 
 
You can bridge this gap using the Amazon Q Business CDE feature to: 
 
 Detect and process image files during the document ingestion process 
 Use Amazon Bedrock with AWS Lambda to interpret the visual information 
 Extract structured data and insights from charts and graphs 
 Make this information searchable using natural language queries 
 
Solution overview 
In this solution, we walk you through how to implement a CDE-based solution for your educational demographic data visualizations. The solution empowers organizations to extract meaningful information from image files using the CDE capability of Amazon Q Business. When Amazon Q Business encounters the S3 path during ingestion, CDE rules automatically trigger a Lambda function. The Lambda function identifies the image files and calls the Amazon Bedrock API, which uses multimodal large language models (LLMs) to analyze and extract contextual information from each image. The extracted text is then seamlessly integrated into the knowledge base in Amazon Q Business. End users can then quickly search for valuable data and insights from images based on their actual context. By bridging the gap between visual content and searchable text, this solution helps organizations unlock valuable insights previously hidden within their image repositories. 
The following figure shows the high-level architecture diagram used for this solution. 
 
For this use case, we use Amazon S3 as our data source. However, this same solution is adaptable to other data source types supported by Amazon Q Business, or it can be implemented with custom data sources as needed.To complete the solution, follow these high-level implementation steps: 
 
 Create an Amazon Q Business application and sync with an S3 bucket. 
 Configure the Amazon Q Business application CDE for the Amazon S3 data source. 
 Extract context from the images. 
 
Prerequisites 
The following prerequisites are needed for implementation: 
 
 An AWS account. 
 At least one Amazon Q Business Pro user that has admin permissions to set up and configure Amazon Q Business. For pricing information, refer to Amazon Q Business pricing. 
 AWS Identity and Access Management (IAM) permissions to create and manage IAM roles and policies. 
 A supported data source to connect, such as an S3 bucket containing your public documents. 
 Access to an Amazon Bedrock LLM in the required AWS Region. 
 
Create an Amazon Q Business application and sync with an S3 bucket 
To create an Amazon Q Business application and connect it to your S3 bucket, complete the following steps. These steps provide a general overview of how to create an Amazon Q Business application and synchronize it with an S3 bucket. For more comprehensive, step-by-step guidance, follow the detailed instructions in the blog post Discover insights from Amazon S3 with Amazon Q S3 connector. 
 
 Initiate your application setup through either the AWS Management Console or AWS Command Line Interface (AWS CLI). 
 Create an index for your Amazon Q Business application. 
 Use the built-in Amazon S3 connector to link your application with documents stored in your organization‚Äôs S3 buckets. 
 
Configure the Amazon Q Business application CDE for the Amazon S3 data source 
With the CDE feature of Amazon Q Business, you can make the most of your Amazon S3 data sources by using the sophisticated capabilities to modify, enhance, and filter documents during the ingestion process, ultimately making enterprise content more discoverable and valuable. When connecting Amazon Q Business to S3 repositories, you can use CDE to seamlessly transform your raw data, applying modifications that significantly improve search quality and information accessibility. This powerful functionality extends to extracting context from binary files such as images through integration with Amazon Bedrock services, enabling organizations to unlock insights from previously inaccessible content formats. By implementing CDE for Amazon S3 data sources, businesses can maximize the utility of their enterprise data within Amazon Q, creating a more comprehensive and intelligent knowledge base that responds effectively to user queries.To configure the Amazon Q Business application CDE for the Amazon S3 data source, complete the following steps: 
 
 Select your application and navigate to Data sources. 
 Choose your existing Amazon S3 data source or create a new one. Verify that Audio/Video under Multi-media content configuration is not enabled. 
 In the data source configuration, locate the Custom Document Enrichment section. 
 Configure the pre-extraction rules to trigger a Lambda function when specific S3 bucket conditions are satisfied. Check the following screenshot for an example configuration. 
 
 Pre-extraction rules are executed before Amazon Q Business processes files from your S3 bucket. 
Extract context from the images 
To extract insights from an image file, the Lambda function makes an Amazon Bedrock API call using Anthropic‚Äôs Claude 3.7 Sonnet model. You can modify the code to use other Amazon Bedrock models based on your use case. 
Constructing the prompt is a critical piece of the code. We recommend trying various prompts to get the desired output for your use case. Amazon Bedrock offers the capability to optimize a prompt that you can use to enhance your use case specific input. 
Examine the following Lambda function code snippets, written in Python, to understand the Amazon Bedrock model setup along with a sample prompt to extract insights from an image. 
In the following code snippet, we start by importing relevant Python libraries, define constants, and initialize AWS SDK for Python (Boto3) clients for Amazon S3 and Amazon Bedrock runtime. For more information, refer to the Boto3 documentation. 
 
 import boto3
import logging
import json
from typing import List, Dict, Any
from botocore.config import Config

MODEL_ID = "us.anthropic.claude-3-7-sonnet-20250219-v1:0"
MAX_TOKENS = 2000
MAX_RETRIES = 2
FILE_FORMATS = ("jpg", "jpeg", "png")

logger = logging.getLogger()
logger.setLevel(logging.INFO)
s3 = boto3.client('s3')
bedrock = boto3.client('bedrock-runtime', config=Config(read_timeout=3600, region_name='us-east-1')) 
 
The prompt passed to the Amazon Bedrock model, Anthropic‚Äôs Claude 3.7 Sonnet in this case, is broken into two parts: prompt_prefix and prompt_suffix. The prompt breakdown makes it more readable and manageable. Additionally, the Amazon Bedrock prompt caching feature can be used to reduce response latency as well as input token cost. You can modify the prompt to extract information based on your specific use case as needed. 
 
 prompt_prefix = """You are an expert image reader tasked with generating detailed descriptions for various """
"""types of images. These images may include technical diagrams,"""
""" graphs and charts, categorization diagrams, data flow and process flow diagrams,"""
""" hierarchical and timeline diagrams, infographics, """
"""screenshots and product diagrams/images from user manuals. """
""" The description of these images needs to be very detailed so that user can ask """
""" questions based on the image, which can be answered by only looking at the descriptions """
""" that you generate.
Here is the image you need to analyze:

&lt;image&gt;
"""

prompt_suffix = """
&lt;/image&gt;

Please follow these steps to analyze the image and generate a comprehensive description:

1. Image type: Classify the image as one of technical diagrams, graphs and charts, categorization diagrams, data flow and process flow diagrams, hierarchical and timeline diagrams, infographics, screenshots and product diagrams/images from user manuals. The description of these images needs to be very detailed so that user can ask questions based on the image, which can be answered by only looking at the descriptions that you generate or other.

2. Items:
&nbsp;&nbsp; Carefully examine the image and extract all entities, texts, and numbers present. List these elements in &lt;image_items&gt; tags.

3. Detailed Description:
&nbsp;&nbsp; Using the information from the previous steps, provide a detailed description of the image. This should include the type of diagram or chart, its main purpose, and how the various elements interact or relate to each other. &nbsp;Capture all the crucial details that can be used to answer any followup questions. Write this description in &lt;image_description&gt; tags.

4. Data Estimation (for charts and graphs only):
&nbsp;&nbsp; If the image is a chart or graph, capture the data in the image in CSV format to be able to recreate the image from the data. Ensure your response captures all relevant details from the chart that might be necessary to answer any follow up questions from the chart.
&nbsp;&nbsp; If exact values cannot be inferred, provide an estimated range for each value in &lt;estimation&gt; tags.
&nbsp;&nbsp; If no data is present, respond with "No data found".

Present your analysis in the following format:

&lt;analysis&gt;
&lt;image_type&gt;
[Classify the image type here]
&lt;/image_type&gt;

&lt;image_items&gt;
[List all extracted entities, texts, and numbers here]
&lt;/image_items&gt;

&lt;image_description&gt;
[Provide a detailed description of the image here]
&lt;/image_description&gt;

&lt;data&gt;
[If applicable, provide estimated number ranges for chart elements here]
&lt;/data&gt;
&lt;/analysis&gt;

Remember to be thorough and precise in your analysis. If you're unsure about any aspect of the image, state your uncertainty clearly in the relevant section.
"""
 
 
The lambda_handler is the main entry point for the Lambda function. While invoking this Lambda function, the CDE passes the data source‚Äôs information within event object input. In this case, the S3 bucket and the S3 object key are retrieved from the event object along with the file format. Further processing of the input happens only if the file_format matches the expected file types. For production ready code, implement proper error handling for unexpected errors. 
 
 def lambda_handler(event, context):
&nbsp;&nbsp; &nbsp;logger.info("Received event: %s" % json.dumps(event))
&nbsp;&nbsp; &nbsp;s3Bucket = event.get("s3Bucket")
&nbsp;&nbsp; &nbsp;s3ObjectKey = event.get("s3ObjectKey")
&nbsp;&nbsp; &nbsp;metadata = event.get("metadata")
&nbsp;&nbsp; &nbsp;file_format = s3ObjectKey.lower().split('.')[-1]
&nbsp;&nbsp; &nbsp;new_key = 'cde_output/' + s3ObjectKey + '.txt'
&nbsp;&nbsp; &nbsp;if (file_format in FILE_FORMATS):
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;afterCDE = generate_image_description(s3Bucket, s3ObjectKey, file_format)
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;s3.put_object(Bucket = s3Bucket, Key = new_key, Body=afterCDE)
&nbsp;&nbsp; &nbsp;return {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"version" : "v0",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"s3ObjectKey": new_key,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"metadataUpdates": []
&nbsp;&nbsp; &nbsp;} 
 
The generate_image_description function calls two other functions: first to construct the message that is passed to the Amazon Bedrock model and second to invoke the model. It returns the final text output extracted from the image file by the model invocation. 
 
 def generate_image_description(s3Bucket: str, s3ObjectKey: str, file_format: str) -&gt; str:
&nbsp;&nbsp; &nbsp;"""
&nbsp;&nbsp; &nbsp;Generate a description for an image.
&nbsp;&nbsp; &nbsp;Inputs:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;image_file: str - Path to the image file
&nbsp;&nbsp; &nbsp;Output:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;str - Generated image description
&nbsp;&nbsp; &nbsp;"""
&nbsp;&nbsp; &nbsp;messages = _llm_input(s3Bucket, s3ObjectKey, file_format)
&nbsp;&nbsp; &nbsp;response = _invoke_model(messages)
&nbsp;&nbsp; &nbsp;return response['output']['message']['content'][0]['text']
 
 
The _llm_input function takes in the S3 object‚Äôs details passed as input along with the file type (png, jpg) and builds the message in the format expected by the model invoked by Amazon Bedrock. 
 
 def _llm_input(s3Bucket: str, s3ObjectKey: str, file_format: str) -&gt; List[Dict[str, Any]]:
&nbsp;&nbsp; &nbsp;s3_response = s3.get_object(Bucket = s3Bucket, Key = s3ObjectKey)
&nbsp;&nbsp; &nbsp;image_content = s3_response['Body'].read()
&nbsp;&nbsp; &nbsp;message = {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"role": "user",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"content": [
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;{"text": prompt_prefix},
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;{
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"image": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"format": file_format,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"source": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"bytes": image_content
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;},
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;{"text": prompt_suffix}
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;]
&nbsp;&nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp;return [message]
 
 
The _invoke_model function calls the converse API using the Amazon Bedrock runtime client. This API returns the response generated by the model. The values within inferenceConfig settings for maxTokens and temperature are used to limit the length of the response and make the responses more deterministic (less random) respectively. 
 
 def _invoke_model(messages: List[Dict[str, Any]]) -&gt; Dict[str, Any]:
&nbsp;&nbsp; &nbsp;"""
&nbsp;&nbsp; &nbsp;Call the Bedrock model with retry logic.
&nbsp;&nbsp; &nbsp;Input:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;messages: List[Dict[str, Any]] - Prepared messages for the model
&nbsp;&nbsp; &nbsp;Output:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;Dict[str, Any] - Model response
&nbsp;&nbsp; &nbsp;"""
&nbsp;&nbsp; &nbsp;for attempt in range(MAX_RETRIES):
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;try:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;response = bedrock.converse(
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;modelId=MODEL_ID,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;messages=messages,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;inferenceConfig={
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"maxTokens": MAX_TOKENS,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"temperature": 0,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;)
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;return response
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;except Exception as e:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;print(e)
&nbsp;&nbsp; &nbsp;
&nbsp;&nbsp; &nbsp;raise Exception(f"Failed to call model after {MAX_RETRIES} attempts") 
 
Putting all the preceding code pieces together, the full Lambda function code is shown in the following block: 
 
 # Example Lambda function for image processing
import boto3
import logging
import json
from typing import List, Dict, Any
from botocore.config import Config

MODEL_ID = "us.anthropic.claude-3-7-sonnet-20250219-v1:0"
MAX_TOKENS = 2000
MAX_RETRIES = 2
FILE_FORMATS = ("jpg", "jpeg", "png")

logger = logging.getLogger()
logger.setLevel(logging.INFO)
s3 = boto3.client('s3')
bedrock = boto3.client('bedrock-runtime', config=Config(read_timeout=3600, region_name='us-east-1'))

prompt_prefix = """You are an expert image reader tasked with generating detailed descriptions for various """
"""types of images. These images may include technical diagrams,"""
""" graphs and charts, categorization diagrams, data flow and process flow diagrams,"""
""" hierarchical and timeline diagrams, infographics, """
"""screenshots and product diagrams/images from user manuals. """
""" The description of these images needs to be very detailed so that user can ask """
""" questions based on the image, which can be answered by only looking at the descriptions """
""" that you generate.
Here is the image you need to analyze:

&lt;image&gt;
"""

prompt_suffix = """
&lt;/image&gt;

Please follow these steps to analyze the image and generate a comprehensive description:

1. Image type: Classify the image as one of technical diagrams, graphs and charts, categorization diagrams, data flow and process flow diagrams, hierarchical and timeline diagrams, infographics, screenshots and product diagrams/images from user manuals. The description of these images needs to be very detailed so that user can ask questions based on the image, which can be answered by only looking at the descriptions that you generate or other.

2. Items:
&nbsp;&nbsp; Carefully examine the image and extract all entities, texts, and numbers present. List these elements in &lt;image_items&gt; tags.

3. Detailed Description:
&nbsp;&nbsp; Using the information from the previous steps, provide a detailed description of the image. This should include the type of diagram or chart, its main purpose, and how the various elements interact or relate to each other. &nbsp;Capture all the crucial details that can be used to answer any followup questions. Write this description in &lt;image_description&gt; tags.

4. Data Estimation (for charts and graphs only):
&nbsp;&nbsp; If the image is a chart or graph, capture the data in the image in CSV format to be able to recreate the image from the data. Ensure your response captures all relevant details from the chart that might be necessary to answer any follow up questions from the chart.
&nbsp;&nbsp; If exact values cannot be inferred, provide an estimated range for each value in &lt;estimation&gt; tags.
&nbsp;&nbsp; If no data is present, respond with "No data found".

Present your analysis in the following format:

&lt;analysis&gt;
&lt;image_type&gt;
[Classify the image type here]
&lt;/image_type&gt;

&lt;image_items&gt;
[List all extracted entities, texts, and numbers here]
&lt;/image_items&gt;

&lt;image_description&gt;
[Provide a detailed description of the image here]
&lt;/image_description&gt;

&lt;data&gt;
[If applicable, provide estimated number ranges for chart elements here]
&lt;/data&gt;
&lt;/analysis&gt;

Remember to be thorough and precise in your analysis. If you're unsure about any aspect of the image, state your uncertainty clearly in the relevant section.
"""

def _llm_input(s3Bucket: str, s3ObjectKey: str, file_format: str) -&gt; List[Dict[str, Any]]:
&nbsp;&nbsp; &nbsp;s3_response = s3.get_object(Bucket = s3Bucket, Key = s3ObjectKey)
&nbsp;&nbsp; &nbsp;image_content = s3_response['Body'].read()
&nbsp;&nbsp; &nbsp;message = {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"role": "user",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"content": [
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;{"text": prompt_prefix},
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;{
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"image": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"format": file_format,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"source": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"bytes": image_content
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;},
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;{"text": prompt_suffix}
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;]
&nbsp;&nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp;return [message]

def _invoke_model(messages: List[Dict[str, Any]]) -&gt; Dict[str, Any]:
&nbsp;&nbsp; &nbsp;"""
&nbsp;&nbsp; &nbsp;Call the Bedrock model with retry logic.
&nbsp;&nbsp; &nbsp;Input:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;messages: List[Dict[str, Any]] - Prepared messages for the model
&nbsp;&nbsp; &nbsp;Output:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;Dict[str, Any] - Model response
&nbsp;&nbsp; &nbsp;"""
&nbsp;&nbsp; &nbsp;for attempt in range(MAX_RETRIES):
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;try:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;response = bedrock.converse(
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;modelId=MODEL_ID,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;messages=messages,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;inferenceConfig={
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"maxTokens": MAX_TOKENS,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"temperature": 0,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;)
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;return response
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;except Exception as e:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;print(e)
&nbsp;&nbsp; &nbsp;
&nbsp;&nbsp; &nbsp;raise Exception(f"Failed to call model after {MAX_RETRIES} attempts")

def generate_image_description(s3Bucket: str, s3ObjectKey: str, file_format: str) -&gt; str:
&nbsp;&nbsp; &nbsp;"""
&nbsp;&nbsp; &nbsp;Generate a description for an image.
&nbsp;&nbsp; &nbsp;Inputs:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;image_file: str - Path to the image file
&nbsp;&nbsp; &nbsp;Output:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;str - Generated image description
&nbsp;&nbsp; &nbsp;"""
&nbsp;&nbsp; &nbsp;messages = _llm_input(s3Bucket, s3ObjectKey, file_format)
&nbsp;&nbsp; &nbsp;response = _invoke_model(messages)
&nbsp;&nbsp; &nbsp;return response['output']['message']['content'][0]['text']

def lambda_handler(event, context):
&nbsp;&nbsp; &nbsp;logger.info("Received event: %s" % json.dumps(event))
&nbsp;&nbsp; &nbsp;s3Bucket = event.get("s3Bucket")
&nbsp;&nbsp; &nbsp;s3ObjectKey = event.get("s3ObjectKey")
&nbsp;&nbsp; &nbsp;metadata = event.get("metadata")
&nbsp;&nbsp; &nbsp;file_format = s3ObjectKey.lower().split('.')[-1]
&nbsp;&nbsp; &nbsp;new_key = 'cde_output/' + s3ObjectKey + '.txt'
&nbsp;&nbsp; &nbsp;if (file_format in FILE_FORMATS):
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;afterCDE = generate_image_description(s3Bucket, s3ObjectKey, file_format)
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;s3.put_object(Bucket = s3Bucket, Key = new_key, Body=afterCDE)
&nbsp;&nbsp; &nbsp;return {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"version" : "v0",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"s3ObjectKey": new_key,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"metadataUpdates": []
&nbsp;&nbsp; &nbsp;} 
 
We strongly recommend testing and validating code in a nonproduction environment before deploying it to production. In addition to Amazon Q pricing, this solution will incur charges for AWS Lambda and Amazon Bedrock. For more information, refer to AWS Lambda pricing and Amazon Bedrock pricing. 
After the Amazon S3 data is synced with the Amazon Q index, you can prompt the Amazon Q Business application to get the extracted insights as shown in the following section. 
Example prompts and results 
The following question and answer pairs refer the Student Age Distribution graph at the beginning of this post. 
Q: Which City has the highest number of students in the 13-15 age range? 
 
Q: Compare the student demographics between City 1 and City 4? 
 
In the original graph, the bars representing student counts lacked explicit numerical labels, which could make data interpretation challenging on a scale. However, with Amazon Q Business and its integration capabilities, this limitation can be overcome. By using Amazon Q Business to process these visualizations with Amazon Bedrock LLMs using the CDE feature, we‚Äôve enabled a more interactive and insightful analysis experience. The service effectively extracts the contextual information embedded in the graph, even when explicit labels are absent. This powerful combination means that end users can ask questions about the visualization and receive responses based on the underlying data. Rather than being limited by what‚Äôs explicitly labeled in the graph, users can now explore deeper insights through natural language queries. This capability demonstrates how Amazon Q Business transforms static visualizations into queryable knowledge assets, enhancing the value of your existing data visualizations without requiring additional formatting or preparation work. 
Best practices for Amazon S3 CDE configuration 
When setting up CDE for your Amazon S3 data source, consider these best practices: 
 
 Use conditional rules to only process specific file types that need transformation. 
 Monitor Lambda execution with Amazon CloudWatch to track processing errors and performance. 
 Set appropriate timeout values for your Lambda functions, especially when processing large files. 
 Consider incremental syncing to process only new or modified documents in your S3 bucket. 
 Use document attributes to track which documents have been processed by CDE. 
 
Cleanup 
Complete the following steps to clean up your resources: 
 
 Go to the Amazon Q Business application and select Remove and unsubscribe for users and groups. 
 Delete the Amazon Q Business application. 
 Delete the Lambda function. 
 Empty and delete the S3 bucket. For instructions, refer to Deleting a general purpose bucket. 
 
Conclusion 
This solution demonstrates how combining Amazon Q Business, custom document enrichment, and Amazon Bedrock can transform static visualizations into queryable knowledge assets, significantly enhancing the value of existing data visualizations without additional formatting work. By using these powerful AWS services together, organizations can bridge the gap between visual information and actionable insights, enabling users to interact with different file types in more intuitive ways. 
Explore What is Amazon Q Business? and Getting started with Amazon Bedrock in the documentation to implement this solution for your specific use cases and unlock the potential of your visual data. 
About the Authors 
 
About the authors 
Amit Chaudhary Amit Chaudhary is a Senior Solutions Architect at Amazon Web Services. His focus area is AI/ML, and he helps customers with generative AI, large language models, and prompt engineering. Outside of work, Amit enjoys spending time with his family. 
Nikhil Jha Nikhil Jha is a Senior Technical Account Manager at Amazon Web Services. His focus areas include AI/ML, building Generative AI resources, and analytics. In his spare time, he enjoys exploring the outdoors with his family.
‚Ä¢ Build AWS architecture diagrams using Amazon Q CLI and MCP
  Creating professional AWS architecture diagrams is a fundamental task for solutions architects, developers, and technical teams. These diagrams serve as essential communication tools for stakeholders, documentation of compliance requirements, and blueprints for implementation teams. However, traditional diagramming approaches present several challenges: 
 
 Time-consuming process ‚Äì Creating detailed architecture diagrams manually can take hours or even days 
 Steep learning curve ‚Äì Learning specialized diagramming tools requires significant investment 
 Inconsistent styling ‚Äì Maintaining visual consistency across multiple diagrams is difficult 
 Outdated AWS icons ‚Äì Keeping up with the latest AWS service icons and best practices challenging. 
 Difficult maintenance ‚Äì Updating diagrams as architectures evolve can become increasingly burdensome 
 
Amazon Q Developer CLI with the Model Context Protocol (MCP) offers a streamlined approach to creating AWS architecture diagrams. By using generative AI through natural language prompts, architects can now generate professional diagrams in minutes rather than hours, while adhering to AWS best practices. 
In this post, we explore how to use Amazon Q Developer CLI with the AWS Diagram MCP and the AWS Documentation MCP servers to create sophisticated architecture diagrams that follow AWS best practices. We discuss techniques for basic diagrams and real-world diagrams, with detailed examples and step-by-step instructions. 
Solution overview 
Amazon Q Developer CLI is a command line interface that brings the generative AI capabilities of Amazon Q directly to your terminal. Developers can interact with Amazon Q through natural language prompts, making it an invaluable tool for various development tasks. 
Developed by Anthropic as an open protocol, the Model Context Protocol (MCP) provides a standardized way to connect AI models to virtually any data source or tool. Using a client-server architecture (as illustrated in the following diagram), the MCP helps developers expose their data through lightweight MCP servers while building AI applications as MCP clients that connect to these servers. 
The MCP uses a client-server architecture containing the following components: 
 
 Host ‚Äì A program or AI tool that requires access to data through the MCP protocol, such as Anthropic‚Äôs Claude Desktop, an integrated development environment (IDE), AWS MCP CLI, or other AI applications 
 Client ‚Äì Protocol clients that maintain one-to-one connections with server 
 Server ‚Äì Lightweight programs that expose capabilities through standardized MCP or act as tools 
 Data sources ‚Äì Local data sources such as databases and file systems, or external systems available over the internet through APIs (web APIs) that MCP servers can connect with 
 
 
As announced in April 2025, MCP enables Amazon Q Developer to connect with specialized servers that extend its capabilities beyond what‚Äôs possible with the base model alone. MCP servers act as plugins for Amazon Q, providing domain-specific knowledge and functionality. The AWS Diagram MCP server specifically enables Amazon Q to generate architecture diagrams using the Python diagrams package, with access to the complete AWS icon set and architectural best practices. 
Prerequisites 
To implement this solution, you must have an AWS account with appropriate permissions and follow the steps below. 
Set up your environment 
Before you can start creating diagrams, you need to set up your environment with Amazon Q CLI, the AWS Diagram MCP server, and AWS Documentation MCP server. This section provides detailed instructions for installation and configuration. 
Install Amazon Q Developer CLI 
Amazon Q Developer CLI is available as a standalone installation. Complete the following steps to install it: 
 
 Download and install Amazon Q Developer CLI. For instructions, see Using Amazon Q Developer on the command line. 
 Verify the installation by running the following command: q --version You should see output similar to the following: Amazon Q Developer CLI version 1.x.x 
 Configure Amazon Q CLI with your AWS credentials: q login 
 Choose the login method suitable for you: 
   
   Use for free with AWS Builder ID 
   Use with Pro license 
    
 
Set up MCP servers 
Complete the following steps to set up your MCP servers: 
 
 Install uv using the following command: pip install uv 
 Install Python 3.10 or newer: uv python install 3.10 
 Install GraphViz for your operating system. 
 Add the servers to your ~/.aws/amazonq/mcp.json file: 
 
{
  "mcpServers": {
    "awslabs.aws-diagram-mcp-server": {
      "command": "uvx",
      "args": ["awslabs.aws-diagram-mcp-server"],
      "env": {
        "FASTMCP_LOG_LEVEL": "ERROR"
      },
      "autoApprove": [],
      "disabled": false
    },
    "awslabs.aws-documentation-mcp-server": {
      "command": "uvx",
      "args": ["awslabs.aws-documentation-mcp-server@latest"],
      "env": {
        "FASTMCP_LOG_LEVEL": "ERROR"
      },
      "autoApprove": [],
      "disabled": false
    }
  }
}
 
Now, Amazon Q CLI automatically discovers MCP servers in the ~/.aws/amazonq/mcp.json file. 
Understanding MCP server tools 
The AWS Diagram MCP server provides several powerful tools: 
 
 list_icons ‚Äì Lists available icons from the diagrams package, organized by provider and service category 
 get_diagram_examples ‚Äì Provides example code for different types of diagrams (AWS, sequence, flow, class, and others) 
 generate_diagram ‚Äì Creates a diagram from Python code using the diagrams package 
 
The AWS Documentation MCP server provides the following useful tools: 
 
 search_documentation ‚Äì Searches AWS documentation using the official AWS Documentation Search API 
 read_documentation ‚Äì Fetches and converts AWS documentation pages to markdown format 
 recommend ‚Äì Gets content recommendations for AWS documentation pages 
 
These tools work together to help you create accurate architecture diagrams that follow AWS best practices. 
Test your setup 
Let‚Äôs verify that everything is working correctly by generating a simple diagram: 
 
 Start the Amazon Q CLI chat interface and verify the output shows the MCP servers being loaded and initialized: q chat  
 In the chat interface, enter the following prompt: Please create a diagram showing an EC2 instance in a VPC connecting to an external S3 bucket. Include essential networking components (VPC, subnets, Internet Gateway, Route Table), security elements (Security Groups, NACLs), and clearly mark the connection between EC2 and S3. Label everything appropriately concisely and indicate that all resources are in the us-east-1 region. Check for AWS documentation to ensure it adheres to AWS best practices before you create the diagram. 
 Amazon Q CLI will ask you to trust the tool that is being used; enter t to trust it.Amazon Q CLI will generate and display a simple diagram showing the requested architecture. Your diagram should look similar to the following screenshot, though there might be variations in layout, styling, or specific details because it‚Äôs created using generative AI. The core architectural components and relationships will be represented, but the exact visual presentation might differ slightly with each generation.  If you see the diagram, your environment is set up correctly. If you encounter issues, verify that Amazon Q CLI can access the MCP servers by making sure you installed the necessary tools and the servers are in the ~/.aws/amazonq/mcp.json file. 
 
Configuration options 
The AWS Diagram MCP server supports several configuration options to customize your diagramming experience: 
 
 Output directory ‚Äì By default, diagrams are saved in a generated-diagrams directory in your current working directory. You can specify a different location in your prompts. 
 Diagram format ‚Äì The default output format is PNG, but you can request other formats like SVG in your prompts. 
 Styling options ‚Äì You can specify colors, shapes, and other styling elements in your prompts. 
 
Now that our environment is set up, let‚Äôs create more diagrams. 
Create AWS architecture diagrams 
In this section, we walk through the process of multiple AWS architecture diagrams using Amazon Q CLI with the AWS Diagram MCP server and AWS Documentation MCP server to make sure our requirements follow best practices. 
When you provide a prompt to Amazon Q CLI, the AWS Diagram and Documentation MCP servers complete the following steps: 
 
 Interpret your requirements. 
 Check for best practices on the AWS documentation. 
 Generate Python code using the diagrams package. 
 Execute the code to create the diagram. 
 Return the diagram as an image. 
 
This process happens seamlessly, so you can focus on describing what you want rather than how to create it. 
AWS architecture diagrams typically include the following components: 
 
 Nodes ‚Äì AWS services and resources 
 Edges ‚Äì Connections between nodes showing relationships or data flow 
 Clusters ‚Äì Logical groupings of nodes, such as virtual private clouds (VPCs), subnets, and Availability Zones 
 Labels ‚Äì Text descriptions for nodes and connections 
 
Example 1: Create a web application architecture 
Let‚Äôs create a diagram for a simple web application hosted on AWS. Enter the following prompt: 
Create a diagram for a simple web application with an Application Load Balancer, two EC2 instances, and an RDS database. Check for AWS documentation to ensure it adheres to AWS best practices before you create the diagram 
 
 After you enter your prompt, Amazon Q CLI will search AWS documentation for best practices using the search_documentation tool from awslabsaws_documentation_mcp_server.  
  Following the search of the relevant AWS documentation, it will read the documentation using the read_documentation tool from the MCP server awslabsaws_documentation_mcp_server.  
 Amazon Q CLI will then list the needed AWS service icons using the list_icons tool, and will use generate_diagram with awslabsaws_diagram_mcp_server.  
 You should receive an output with a description of the diagram created based on the prompt along with the location of where the diagram was saved.  
 Amazon Q CLI will generate and display the diagram. 
  
 
The generated diagram shows the following key components: 
 
 An Application Load Balancer as the entry point 
 Two Amazon Elastic Compute Cloud (Amazon EC2) instances for the application tier 
 An Amazon Relational Database Service (Amazon RDS) instance for the database tier 
 Connections showing the flow of traffic 
 
Example 2: Create a multi-tier architecture 
Multi-tier architectures separate applications into functional layers (presentation, application, and data) to improve scalability and security. We use the following prompt to create our diagram: 
Create a diagram for a three-tier web application with a presentation tier (ALB and CloudFront), application tier (ECS with Fargate), and data tier (Aurora PostgreSQL). Include VPC with public and private subnets across multiple AZs. Check for AWS documentation to ensure it adheres to AWS best practices before you create the diagram. 
 
The diagram shows the following key components: 
 
 A presentation tier in public subnets 
 An application tier in private subnets 
 A data tier in isolated private subnets 
 Proper security group configurations 
 Traffic flow between tiers 
 
Example 3: Create a serverless architecture 
We use the following prompt to create a diagram for a serverless architecture: 
Create a diagram for a serverless web application using API Gateway, Lambda, DynamoDB, and S3 for static website hosting. Include Cognito for user authentication and CloudFront for content delivery. Check for AWS documentation to ensure it adheres to AWS best practices before you create the diagram. 
 
The diagram includes the following key components: 
 
 Amazon Simple Storage Service (Amazon S3) hosting static website content 
 Amazon CloudFront distributing content globally 
 Amazon API Gateway handling API requests 
 AWS Lambda functions implementing business logic 
 Amazon DynamoDB storing application data 
 Amazon Cognito managing user authentication 
 
Example 4: Create a data processing diagram 
We use the following prompt to create a diagram for a data processing pipeline: 
Create a diagram for a data processing pipeline with components organized in clusters for data ingestion, processing, storage, and analytics. Include Kinesis, Lambda, S3, Glue, and QuickSight. Check for AWS documentation to ensure it adheres to AWS best practices before you create the diagram. 
 
The diagram organizes components into distinct clusters: 
 
 Data ingestion ‚Äì Amazon Kinesis Data Streams, Amazon Data Firehose, Amazon Simple Queue Service 
 Data processing ‚Äì Lambda functions, AWS Glue jobs 
 Data storage ‚Äì S3 buckets, DynamoDB tables 
 Data analytics ‚Äì AWS Glue, Amazon Athena, Amazon QuickSight 
 
Real-world examples 
Let‚Äôs explore some real-world architecture patterns and how to create diagrams for them using Amazon Q CLI with the AWS Diagram MCP server. 
Ecommerce platform 
Ecommerce platforms require scalable, resilient architectures to handle variable traffic and maintain high availability. We use the following prompt to create an example diagram: 
Create a diagram for an e-commerce platform with microservices architecture. Include components for product catalog, shopping cart, checkout, payment processing, order management, and user authentication. Ensure the architecture follows AWS best practices for scalability and security. Check for AWS documentation to ensure it adheres to AWS best practices before you create the diagram. 
 
The diagram includes the following key components: 
 
 API Gateway as the entry point for client applications 
 Microservices implemented as containers in Amazon Elastic Container Service (Amazon ECS) with AWS Fargate 
 RDS databases for product catalog, shopping cart, and order data 
 Amazon ElastiCache for product data caching and session management 
 Amazon Cognito for authentication 
 Amazon Simple Queue Service (Amazon SQS) and Amazon Simple Notification Service (Amazon SNS) for asynchronous communication between services 
 CloudFront for content delivery and static assets from Amazon S3 
 Amazon Route 53 for DNS management 
 AWS WAF for web application security 
 AWS Lambda functions for serverless microservice implementation 
 AWS Secrets Manager for secure credential storage 
 Amazon CloudWatch for monitoring and observability 
 
Intelligent document processing solution 
We use the following prompt to create a diagram for an intelligent document processing (IDP) architecture: 
Create a diagram for an intelligent document processing (IDP) application on AWS. Include components for document ingestion, OCR and text extraction, intelligent data extraction (using NLP and/or computer vision), human review and validation, and data output/integration. Ensure the architecture follows AWS best practices for scalability and security, leveraging services like S3, Lambda, Textract, Comprehend, SageMaker (for custom models, if applicable), and potentially Augmented AI (A2I). Check for AWS documentation related to intelligent document processing best practices to ensure it adheres to AWS best practices before you create the diagram. 
 
The diagram includes the following key components: 
 
 Amazon API Gateway as the entry point for client applications, providing a secure and scalable interface 
 Microservices implemented as containers in ECS with Fargate, enabling flexible and scalable processing 
 Amazon RDS databases for product catalog, shopping cart, and order data, providing reliable structured data storage 
 Amazon ElastiCache for product data caching and session management, improving performance and user experience 
 Amazon Cognito for authentication, ensuring secure access control 
 Amazon Simple Queue Service and Amazon Simple Notification Service for asynchronous communication between services, enabling decoupled and resilient architecture 
 Amazon CloudFront for content delivery and static assets from S3, optimizing global performance 
 Amazon Route53 for DNS management, providing reliable routing 
 AWS WAF for web application security, protecting against common web exploits 
 AWS Lambda functions for serverless microservice implementation, offering cost-effective scaling 
 AWS Secrets Manager for secure credential storage, enhancing security posture 
 Amazon CloudWatch for monitoring and observability, providing insights into system performance and health. 
 
Clean up 
If you no longer need to use the AWS Cost Analysis MCP server with Amazon Q CLI, you can remove it from your configuration: 
 
 Open your ~/.aws/amazonq/mcp.json file. 
 Remove or comment out the MCP server entries. 
 Save the file. 
 
This will prevent the server from being loaded when you start Amazon Q CLI in the future. 
Conclusion 
In this post, we explored how to use Amazon Q CLI with the AWS Documentation MCP and AWS Diagram MCP servers to create professional AWS architecture diagrams that adhere to AWS best practices referenced from official AWS documentation. This approach offers significant advantages over traditional diagramming methods: 
 
 Time savings ‚Äì Generate complex diagrams in minutes instead of hours 
 Consistency ‚Äì Make sure diagrams follow the same style and conventions 
 Best practices ‚Äì Automatically incorporate AWS architectural guidelines 
 Iterative refinement ‚Äì Quickly modify diagrams through simple prompts 
 Validation ‚Äì Check architectures against official AWS documentation and recommendations 
 
As you continue your journey with AWS architecture diagrams, we encourage you to deepen your knowledge by learning more about the Model Context Protocol (MCP) to understand how it enhances the capabilities of Amazon Q. When seeking inspiration for your own designs, the AWS Architecture Center offers a wealth of reference architectures that follow best practices. For creating visually consistent diagrams, be sure to visit the AWS Icons page, where you can find the complete official icon set. And to stay at the cutting edge of these tools, keep an eye on updates to the official AWS MCP Servers‚Äîthey‚Äôre constantly evolving with new features to make your diagramming experience even better. 
 
About the Authors 
Joel Asante, an Austin-based Solutions Architect at Amazon Web Services (AWS), works with GovTech (Government Technology) customers. With a strong background in data science and application development, he brings deep technical expertise to creating secure and scalable cloud architectures for his customers. Joel is passionate about data analytics, machine learning, and robotics, leveraging his development experience to design innovative solutions that meet complex government requirements. He holds 13 AWS certifications and enjoys family time, fitness, and cheering for the Kansas City Chiefs and Los Angeles Lakers in his spare time. 
Dunieski Otano is a Solutions Architect at Amazon Web Services based out of Miami, Florida. He works with World Wide Public Sector MNO (Multi-International Organizations) customers. His passion is Security, Machine Learning and Artificial Intelligence, and Serverless. He works with his customers to help them build and deploy high available, scalable, and secure solutions. Dunieski holds 14 AWS certifications and is an AWS Golden Jacket recipient. In his free time, you will find him spending time with his family and dog, watching a great movie, coding, or flying his drone. 
Varun Jasti&nbsp;is a Solutions Architect at Amazon Web Services, working with AWS Partners to design and scale artificial intelligence solutions for public sector use cases to meet compliance standards. With a background in Computer Science, his work covers broad range of ML use cases primarily focusing on LLM training/inferencing and computer vision. In his spare time, he loves playing tennis and swimming.

‚∏ª