‚úÖ Morning News Briefing ‚Äì July 25, 2025 10:50

üìÖ Date: 2025-07-25 10:50
üè∑Ô∏è Tags: #briefing #ai #publichealth #digitalgov

‚∏ª

üßæ Weather
‚Ä¢ No watches or warnings in effect, Pembroke
  No watches or warnings in effect. No warnings or watches or watches in effect . Watch or warnings are no longer in effect in the U.S. No watches, warnings are in effect for the rest of the day . No watches and warnings are still in effect, but no watches are in place for the day's events . The weather is not expected to be affected by the weather .
‚Ä¢ Current Conditions: Cloudy, 20.8¬∞C
  Temperature: 20.8&deg;C Pressure / Tendency: 101.2 kPa rising Visibility: 24 km Humidity: 93 % Humidex: 28 Dewpoint: 19.7&deg:C Wind: WNW 11 km/h Air Quality Health Index: n/a . Observed at Garrison Petawawa 6:00 AM EDT Friday 25 July
‚Ä¢ Friday: Chance of showers. High 26. POP 40%
  Mainly cloudy. 40 percent chance of showers early this morning . High 26. Humidex 31. UV index 9 or very high. High 31.40 percent chance for showers early Sunday morning. High of 26.50 per cent chance of rain in the early hours of Sunday morning . Forecast issued 5:00 AM EDT Friday 25 July 2025. Forecast also issued Friday 25

üåç International News
No updates.

üçÅ Canadian News
No updates.

üá∫üá∏ U.S. Top Stories
‚Ä¢ People like extroverted robots ‚Äî but they relate to the neurotic ones
  Neurotic traits in a robot can make them seem more relatable . Neurotic personalities are a staple of science fiction . Researchers have found that neurotic traits can make robots seem relatable to a robot . The findings were made by researchers who study how people react to robot personalities have recently found that people react differently to robots that have neurotic personalities . A robot with neurotic tendencies can
‚Ä¢ Who or what is Trump's "beautiful baby?" Find out in the quiz
  RIP Ozzy Osbourne and Hulk Hogan . Plus, what are Tesla and Coca-Cola up to? And what's going on with Tesla and Coke? Plus, Ozzy and Hulk Hulk Hogan are dead, and what's up to be? Have you ever been to the U.S. market? Share your thoughts with iReport.com/thousands of miles of video .
‚Ä¢ Warning labels on menopause treatments 'scare women', doctors tell FDA
  The FDA may remove the warning labels on hormone replacement therapies used to treat the symptoms of menopause . Doctors say the warning is scaring people who could benefit from these treatments . The warning labels may be removed from the labels . The FDA is looking at the possibility of removing the labels from the products as part of a nationwide effort to remove them from the market as soon as possible .
‚Ä¢ Fearing coverage could change, some parents rush to vaccinate their kids
  Health Secretary Robert F. Kennedy Jr. is changing how federal agencies handle vaccine recommendations . Some parents are hurrying to get kids their shots, fearing future changes could limit access . Health secretary: Vaccine recommendations will be handled differently by federal agencies . Parents are worried that future changes will limit access to the drugs and other drugs available for kids . The drug is now available in the form of
‚Ä¢ Making a living as a poker player is hard. The 'Big Beautiful Bill' makes it harder
  A new law includes a provision that could mean bettors pay more during tax season . Major poker players are calling on Congress to royally flush the measure down the drain . Poker players call for Congress to ditch the measure . The new law could mean bets will be more likely to pay more in taxes during the tax season. Poker players are urging Congress to flush it down the the drain

üß† Artificial Intelligence
No updates.

üíª Digital Strategy
‚Ä¢ Advisor to Brit tech contractors Qdos confirms client data leak
  Insurance and employment status specialist Qdos confirms that an intruder has stolen some customers' personal data . Policy management not affected, but some personal data may have been snaffled . Qdos confirmed that some customers personal data was stolen by an intruder, according to a communication to tech contractors that was seen by The Register . The Register has been contacted by Qdos to discuss the snaffling
‚Ä¢ Microsoft walks us through Copilot Search with a domain it doesn't even own
  Mock-ups feature m365.com, which could be yours for a few bucks . Microsoft this week trumpeted the launch of Microsoft 365 Copilot Search with lofty promises and slick mock-ups . But the domain plastered all over them didn't belong to the corporation, but the domain is available for free . Microsoft 365 is now available to buy for as little as a few dollars
‚Ä¢ Problem PC had graybeards stumped until trainee rummaged through trash
  On Call: Evidence of copious sugar hits hinted at unauthorized usage . On Call is a reader-contributed column in which we share your tech support tales . By Friday, many readers will feel they need a sugar hit to get through the day, which is why The Register tries to offer a jolt of amusement in the form of a new installment of On Call .‚Ä¶‚Ä¶‚Ä¶
‚Ä¢ DNS security is important but DNSSEC may be a failed experiment
  Last week I turned on DNSSEC (Domain Name System Security Extensions) for the systemsapproach.org domain . No need to applaud; I was just trying to get an understanding of what the barriers to adoption might be while teaching myself about the technology . Nobody thinks of running a website without HTTPs. Safer DNS still seems optional, but it's not necessary to run a
‚Ä¢ You DO see Windows 11 as an AI PC opportunity, say Dell and Intel
  Dell and Intel want to convince corporate buyers that upgrading their PC fleet is a virtue and not a necessity . Time to 'reimagine' it as a gateway, a gateway to inner peace, er, sales . Microsoft ends support for Windows 10 three months to go until the end of support for the next generation of PCs . Dell wants to convince those who want to upgrade their PCs to

üè• Public Health
No updates.

üî¨ Science
‚Ä¢ Assessing the impact of voluntary food reformulation targets: Mid-point assessment of Australia‚Äôs voluntary sodium and saturated fat reduction policy
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Impact of the COVID-19 pandemic on incident diagnoses in German refugee centres 2018 to 2023
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Comparative efficacy of topical interventions for facial photoaging: a network meta-analysis
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Past, current status, and future trends of the rural healthcare network in the Republic of Kazakhstan
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Association of impaired pulmonary function and objectively measured physical activity in a population study
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations

üßæ Government & Policy
No updates.

üèõÔ∏è Enterprise Architecture & IT Governance
No updates.

ü§ñ AI & Emerging Tech
‚Ä¢ The deadly saga of the controversial gene therapy Elevidys
  It has been a grim few months for the Duchenne muscular dystrophy (DMD) community. There had been some excitement when, a couple of years ago, a gene therapy for the disorder was approved by the US Food and Drug Administration for the first time. That drug, Elevidys, has now been implicated in the deaths of two teenage boys.



The drug‚Äôs approval was always controversial‚Äîthere was a lack of evidence that it actually worked, for starters. But the agency that once rubber-stamped the drug has now turned on its manufacturer, Sarepta Therapeutics. In a remarkable chain of events, the FDA asked the company to stop shipping the drug on July 18. Sarepta refused to comply.





In the days since, the company has acquiesced. But its reputation has already been hit. And the events have dealt a devastating blow to people desperate for treatments that might help them, their children, or other family members with DMD.



DMD is a rare genetic disorder that causes muscles to degenerate over time. It‚Äôs caused by a mutation in a gene that codes for a protein called dystrophin. That protein is essential for muscles‚Äîwithout it, muscles weaken and waste away. The disease mostly affects boys, and symptoms usually start in early childhood.



At first, affected children usually start to find it hard to jump or climb stairs. But as the disease progresses, other movements become difficult too. Eventually, the condition might affect the heart and lungs. The life expectancy of a person with DMD has recently improved, but it is still only around 30 or 40 years. There is no cure. It‚Äôs a devastating diagnosis.



Elevidys was designed to replace missing dystrophin with a shortened, engineered version of the protein. In June 2023,¬†the FDA approved the therapy for eligible four- and five-year-olds. It came with a $3.2 million price tag.



The approval was celebrated by people affected by DMD, says Debra Miller, founder of CureDuchenne, an organization that funds research into the condition and offers support to those affected by it. &#8220;We&#8217;ve not had much in the way of meaningful therapies,&#8221; she says. &#8220;The excitement was great.&#8221;



But the approval was controversial. It came under an ‚Äúaccelerated approval‚Äù program that essentially lowers the bar of evidence for drugs designed to treat ‚Äúserious or life-threatening diseases where there is an unmet medical need.‚Äù



Elevidys was approved because it appeared to increase levels of the engineered protein in patients‚Äô muscles. But it had not been shown to improve patient outcomes: It had failed a randomized clinical trial.



The FDA approval was granted on the condition that Sarepta complete another clinical trial. The¬†topline results of that trial were described in October 2023 and¬†were published in detail a year later. Again, the drug failed to meet its ‚Äúprimary endpoint‚Äù‚Äîin other words, it didn‚Äôt work as well as hoped.



In June 2024, the FDA expanded the approval of Elevidys. It granted traditional approval for the drug to treat people with DMD who are over the age of four and can walk independently, and another accelerated approval for those who can‚Äôt.





Some experts were appalled at the FDA‚Äôs decision‚Äîeven some within the FDA disagreed with it. But things weren‚Äôt so simple for people living with DMD. I spoke to some parents of such children¬†a couple of years ago. They pointed out that drug approvals can help bring interest and investment to DMD research. And, above all, they were desperate for any drug that might help their children. They were desperate for hope.



Unfortunately, the treatment does not appear to be delivering on that hope. There have always been questions over whether it works. But now there are serious questions over how safe it is.¬†



In March 2025, a 16-year-old boy died after being treated with Elevidys. He had developed acute liver failure (ALF) after having the treatment, Sarepta¬†said in a statement. On June 15, the company¬†announced a second death‚Äîa 15-year-old who also developed ALF following Elevidys treatment. The company said it would pause shipments of the drug,¬†but only for patients who are not able to walk.



The following day, Sarepta held¬†an online presentation in which CEO Doug Ingram said that the company was exploring ways to make the treatment safer, perhaps by treating recipients with another drug that dampens their immune systems. But that same day, the company announced that¬†it was laying off 500 employees‚Äî36% of its workforce. Sarepta did not respond to a request for comment.



On June 24, the FDA¬†announced that it was investigating the risks of serious outcomes ‚Äúincluding hospitalization and death‚Äù associated with Elevidys, and ‚Äúevaluating the need for further regulatory action.‚Äù



There was more tragic news on July 18, when there were¬†reports that a third patient had died following a Sarepta treatment. This patient, a 51-year-old, hadn‚Äôt been taking Elevidys but was enrolled in a clinical trial for a different Sarepta gene therapy designed to treat limb-girdle muscular dystrophy. The same day,¬†the FDA asked Sarepta to voluntarily pause all shipments of Elevidys. Sarepta¬†refused to do so.



The refusal was surprising, says Michael Kelly, chief scientific officer at CureDuchenne: ‚ÄúIt was an unusual step to take.‚Äù



After¬†significant¬†media¬†coverage, including reporting that¬†the FDA was ‚Äúdeeply troubled‚Äù by the decision and would use its ‚Äúfull regulatory authority,‚Äù Sarepta backed down a few days later. On July 21, the company announced its decision to¬†‚Äúvoluntarily and temporarily‚Äù pause all shipments of Elevidys in the US.



Sarepta says it will now work with the FDA to address safety and labeling concerns. But in the meantime, the saga has left the DMD community grappling with ‚Äúa mix of disappointment and concern,‚Äù says Kelly. Many are worried about the risks of taking the treatment. Others are devastated that they are no longer able to access it.



Miller says she knows of families who have been working with their insurance providers to get authorization for the drug. &#8220;It&#8217;s like the rug has been pulled out from under them,&#8221; she says. Many families have no other treatment options. &#8220;And we know what happens when you do nothing with Duchenne,&#8221; she says. Others, particularly those with teenage children with DMD, are deciding against trying the drug, she adds. 



The decision over whether to take Elevidys was already a personal one based on several factors, he says. People with DMD and their families deserve clear and transparent information about the treatment in order to make that decision.



The FDA&#8217;s decision to approve Elevidys was made on limited data, says Kelly. But as things stand today, over 900 people have been treated with Elevidys. &#8220;That gives the FDA&#8230; an opportunity to look at real data and make informed decisions,&#8221; he says. 



‚ÄúFamilies facing Duchenne do not have time to waste,‚Äù Kelly says. ‚ÄúThey must navigate a landscape where hope is tempered by the realities of medical complexity.‚Äù



A version of this article first appeared in The Checkup,¬†MIT Technology Review‚Äôs¬†weekly biotech newsletter. To receive it in your inbox every Thursday, and read articles like this first,¬†sign up here.
‚Ä¢ How nonprofits and academia are stepping up to salvage US climate programs
  Nonprofits are striving to preserve a US effort to modernize greenhouse-gas measurements, amid growing fears that the Trump administration‚Äôs dismantling of federal programs will obscure the nation‚Äôs contributions to climate change.



The Data Foundation, a Washington, DC, nonprofit that advocates for open data, is fundraising for an initiative that will coordinate efforts among nonprofits, technical experts, and companies to improve the accuracy and accessibility of climate emissions information. It will build on an effort to improve the collection of emissions data that former president Joe Biden launched in 2023‚Äîand which President Trump nullified on his first day in office.&nbsp;



The initiative will help prioritize responses to changes in federal greenhouse-gas monitoring and measurement programs, but the Data Foundation stresses that it will primarily serve a ‚Äúlong-standing need for coordination‚Äù of such efforts outside of government agencies.



The new greenhouse-gas coalition is one of a growing number of nonprofit and academic groups that have spun up or shifted focus to keep essential climate monitoring and research efforts going amid the Trump administration‚Äôs assault on environmental funding, staffing, and regulations. Those include efforts to ensure that US scientists can continue to contribute to the UN‚Äôs major climate report and publish assessments of the rising domestic risks of climate change. Otherwise, the loss of these programs will make it increasingly difficult for communities to understand how more frequent or severe wildfires, droughts, heat waves, and floods will harm them‚Äîand how dire the dangers could become.&nbsp;



Few believe that nonprofits or private industry can come close to filling the funding holes that the Trump administration is digging. But observers say it‚Äôs essential to try to sustain efforts to understand the risks of climate change that the federal government has historically overseen, even if the attempts are merely stopgap measures.&nbsp;



If we give up these sources of emissions data, ‚Äúwe‚Äôre flying blind,‚Äù says Rachel Cleetus, senior policy director with the climate and energy program at the Union of Concerned Scientists. ‚ÄúWe‚Äôre deliberating taking away the very information that would help us understand the problem and how to address it best.‚Äù



Improving emissions estimates



The Environmental Protection Agency, the National Oceanic and Atmospheric Administration, the US Forest Service, and other agencies have long collected information about greenhouse gases in a variety of ways. These include self-reporting by industry; shipboard, balloon, and aircraft readings of gas concentrations in the atmosphere; satellite measurements of the carbon dioxide and methane released by wildfires; and on-the-ground measurements of trees. The EPA, in turn, collects and publishes the data from these disparate sources as the Inventory of US Greenhouse Gas Emissions and Sinks.





But that report comes out on a two-year lag, and studies show that some of the estimates it relies on could be way off‚Äîparticularly the self-reported ones.



A recent analysis using satellites to measure methane pollution from four large landfills found they produce, on average, six times more emissions than the facilities had reported to the EPA. Likewise, a 2018 study in Science found that the actual methane leaks from oil and gas infrastructure were about 60% higher than the self-reported estimates in the agency‚Äôs inventory.



The Biden administration‚Äôs initiative‚Äîthe National Strategy to Advance an Integrated US Greenhouse Gas Measurement, Monitoring, and Information System‚Äîaimed to adopt state-of-the-art tools and methods to improve the accuracy of these estimates, including satellites and other monitoring technologies that can replace or check self-reported information.



The administration specifically sought to achieve these improvements through partnerships between government, industry, and nonprofits. The initiative called for the data collected across groups to be published to an online portal in formats that would be accessible to policymakers and the public.



Moving toward a system that produces more current and reliable data is essential for understanding the rising risks of climate change and tracking whether industries are abiding by government regulations and voluntary climate commitments, says Ben Poulter, a former NASA scientist who coordinated the Biden administration effort as a deputy director in the Office of Science and Technology Policy.



‚ÄúOnce you have this operational system, you can provide near-real-time information that can help drive climate action,‚Äù Poulter says. He is now a senior scientist at Spark Climate Solutions, a nonprofit focused on accelerating emerging methods of combating climate change, and he is advising the Data Foundation‚Äôs Climate Data Collaborative, which is overseeing the new greenhouse-gas initiative.&nbsp;



Slashed staffing and funding&nbsp;&nbsp;



But the momentum behind the federal strategy deflated when Trump returned to office. On his first day, he signed an executive order that effectively halted it. The White House has since slashed staffing across the agencies at the heart of the effort, sought to shut down specific programs that generate emissions data, and raised uncertainties about the fate of numerous other program components.&nbsp;



In April, the administration missed a deadline to share the updated greenhouse-gas inventory with the United Nations, for the first time in three decades, as E&amp;E News reported. It eventually did release the report in May, but only after the Environmental Defense Fund filed a Freedom of Information Act request.



There are also indications that the collection of emissions data might be in jeopardy. In March, the EPA said it would ‚Äúreconsider‚Äù the Greenhouse Gas Reporting Program, which requires thousands of power plants, refineries, and other industrial facilities to report emissions each year.



In addition, the tax and spending bill that Trump signed into law earlier this month rescinds provisions in Biden‚Äôs Inflation Reduction Act that provided incentives or funding for corporate greenhouse-gas reporting and methane monitoring.&nbsp;



Meanwhile, the White House has also proposed slashing funding for the National Oceanic and Atmospheric Administration and shuttering a number of its labs. Those include the facility that supports the Mauna Loa Observatory in Hawaii, the world‚Äôs longest-running carbon dioxide measuring program, as well as the Global Monitoring Laboratory, which operates a global network of collection flasks that capture air samples used to measure concentrations of nitrous oxide, chlorofluorocarbons, and other greenhouse gases.



Under the latest appropriations negotiations, Congress seems set to spare NOAA and other agencies the full cuts pushed by the Trump administration, but that may or may not protect various climate programs within them. As observers have noted, the loss of experts throughout the federal government, coupled with the priorities set by Trump-appointed leaders of those agencies, could still prevent crucial emissions data from being collected, analyzed, and published.



‚ÄúThat‚Äôs a huge concern,‚Äù says David Hayes, a professor at the Stanford Doerr School of Sustainability, who previously worked on the effort to upgrade the nation‚Äôs emissions measurement and monitoring as special assistant to President Biden for climate policy. It‚Äôs not clear ‚Äúwhether they‚Äôre going to continue and whether the data availability will drop off.‚Äù



‚ÄòA natural disaster‚Äô



Amid all these cutbacks and uncertainties, those still hoping to make progress toward an improved system for measuring greenhouse gases have had to adjust their expectations: It‚Äôs now at least as important to simply preserve or replace existing federal programs as it is to move toward more modern tools and methods.



But Ryan Alexander, executive director of the Data Foundation‚Äôs Climate Data Collaborative, is optimistic that there will be opportunities to do both.&nbsp;



She says the new greenhouse-gas coalition will strive to identify the highest-priority needs and help other nonprofits or companies accelerate the development of new tools or methods. It will also aim to ensure that these organizations avoid replicating one another‚Äôs efforts and deliver data with high scientific standards, in open and interoperable formats.&nbsp;



The Data Foundation declines to say what other nonprofits will be members of the coalition or how much money it hopes to raise, but it plans to make a formal announcement in the coming weeks.&nbsp;



Nonprofits and companies are already playing a larger role in monitoring emissions, including organizations like Carbon Mapper, which operates satellites and aircraft that detect and measure methane emissions from particular facilities. The EDF also launched a satellite last year, known as MethaneSAT, that could spot large and small sources of emissions‚Äîthough it lost power earlier this month and probably cannot be recovered.&nbsp;



Alexander notes that shifting from self-reported figures to observational technology like satellites could not just replace but perhaps also improve on the EPA reporting program that the Trump administration has moved to shut down.



Given the ‚Äúdramatic changes‚Äù brought about by this administration, ‚Äúthe future will not be the past,‚Äù she says. ‚ÄúThis is like a natural disaster. We can‚Äôt think about rebuilding in the way that things have been in the past. We have to look ahead and say, ‚ÄòWhat is needed? What can people afford?‚Äô‚Äù



Organizations can also use this moment to test and develop emerging technologies that could improve greenhouse-gas measurements, including novel sensors or artificial intelligence tools, Hayes says.&nbsp;



‚ÄúWe are at a time when we have these new tools, new technologies for measurement, measuring, and monitoring,‚Äù he says. ‚ÄúTo some extent it‚Äôs a new era anyway, so it‚Äôs a great time to do some pilot testing here and to demonstrate how we can create new data sets in the climate area.‚Äù



Saving scientific contributions



It‚Äôs not just the collection of emissions data that nonprofits and academic groups are hoping to save. Notably, the American Geophysical Union and its partners have taken on two additional climate responsibilities that traditionally fell to the federal government.



The US State Department‚Äôs Office of Global Change historically coordinated the nation‚Äôs contributions to the UN Intergovernmental Panel on Climate Change‚Äôs major reports on climate risks, soliciting and nominating US scientists to help write, oversee, or edit sections of the assessments. The US Global Change Research Program, an interagency group that ran much of the process, also covered the cost of trips to a series of in-person meetings with international collaborators.&nbsp;



But the US government seems to have relinquished any involvement as the IPCC kicks off the process for the Seventh Assessment Report. In late February, the administration blocked federal scientists including NASA‚Äôs Katherine Calvin, who was previously selected as a cochair for one of the working groups, from attending an early planning meeting in China. (Calvin was the agency‚Äôs chief scientist at the time but was no longer serving in that role as of April, according to NASA‚Äôs website.)



The agency didn‚Äôt respond to inquiries from interested scientists after the UN panel issued a call for nominations in March, and it failed to present a list of nominations by the deadline in April, scientists involved in the process say. The Trump administration also canceled funding for the Global Change Research Program and, earlier this month, fired the last remaining staffers working at the Office of Global Change.



In response, 10 universities came together in March to form the US Academic Alliance for the IPCC, in partnership with the AGU, to request and evalute applications from US researchers. The universities‚Äîwhich include Yale, Princeton, and the University of California, San Diego‚Äîtogether nominated nearly 300 scientists, some of whom the IPCC has since officially selected. The AGU is now conducting a fundraising campaign to help pay for travel expenses.&nbsp;





Pamela McElwee, a professor at Rutgers who helped establish the academic coalition, says it‚Äôs crucial for US scientists to continue participating in the IPCC process.



‚ÄúIt is our flagship global assessment report on the state of climate, and it plays a really important role in influencing country policies,‚Äù she says. ‚ÄúTo not be part of it makes it much more difficult for US scientists to be at the cutting edge and advance the things we need to do.‚Äù&nbsp;



The AGU also stepped in two months later, after the White House dismissed hundreds of researchers working on the National Climate Assessment, an annual report analyzing the rising dangers of climate change across the country. The AGU and American Meteorological Society together announced plans to publish a ‚Äúspecial collection‚Äù to sustain the momentum of that effort.



‚ÄúIt‚Äôs incumbent on us to ensure our communities, our neighbors, our children are all protected and prepared for the mounting risks of climate change,‚Äù said Brandon Jones, president of the AGU, in an earlier statement.



The AGU declined to discuss the status of the project.



Stopgap solution



The sheer number of programs the White House is going after will require organizations to make hard choices about what they attempt to save and how they go about it. Moreover, relying entirely on nonprofits and companies to take over these federal tasks is not viable over the long term.&nbsp;



Given the costs of these federal programs, it could prove prohibitive to even keep a minimum viable version of some essential monitoring systems and research programs up and running. Dispersing across various organizations the responsibility of calculating the nation‚Äôs emissions sources and sinks also creates concerns about the scientific standards applied and the accessibility of that data, Cleetus says. Plus, moving away from the records that NOAA, NASA, and other agencies have collected for decades would break the continuity of that data, undermining the ability to detect or project trends.



More basically, publishing national emissions data should be a federal responsibility, particularly for the government of the world‚Äôs second-largest climate polluter, Cleetus adds. Failing to calculate and share its contributions to climate change sidesteps the nation‚Äôs global responsibilities and sends a terrible signal to other countries.&nbsp;



Poulter stresses that nonprofits and the private sector can do only so much, for so long, to keep these systems up and running.



‚ÄúWe don‚Äôt want to give the impression that this greenhouse-gas coalition, if it gets off the ground, is a long-term solution,‚Äù he says. ‚ÄúBut we can‚Äôt afford to have gaps in these data sets, so somebody needs to step in and help sustain those measurements.‚Äù
‚Ä¢ Trump‚Äôs AI Action Plan is a distraction
  On Wednesday, President Trump issued three executive orders, delivered a speech, and released an action plan, all on the topic of continuing American leadership in AI.¬†



The plan contains dozens of proposed actions, grouped into three ‚Äúpillars‚Äù: accelerating innovation, building infrastructure, and leading international diplomacy and security. Some of its recommendations are thoughtful even if incremental, some clearly serve ideological ends, and many enrich big tech companies, but the plan is just a set of recommended actions.&nbsp;



The three executive orders, on the other hand, actually operationalize one subset of actions from each pillar:&nbsp;




One aims to prevent ‚Äúwoke AI‚Äù by mandating that the federal government procure only large language models deemed ‚Äútruth-seeking‚Äù and ‚Äúideologically neutral‚Äù rather than ones allegedly favoring DEI. This action purportedly accelerates AI innovation.



A second aims to accelerate construction of AI data centers. A much more industry-friendly version of an order issued under President Biden, it makes available rather extreme policy levers, like effectively waiving a broad swath of environmental protections, providing government grants to the wealthiest companies in the world, and even offering federal land for private data centers.



A third promotes and finances the export of US AI technologies and infrastructure, aiming to secure American diplomatic leadership and reduce international dependence on AI systems from adversarial countries.




This flurry of actions made for glitzy press moments, including an hour-long speech from the president and onstage signings. But while the tech industry cheered these announcements (which will swell their coffers), they obscured the fact that the administration is currently decimating the very policies that enabled America to become the world leader in AI in the first place.



To maintain America‚Äôs leadership in AI, you have to understand what produced it. Here are four specific long-standing public policies that helped the US achieve this leadership‚Äîadvantages that the administration is undermining.&nbsp;



Investing federal funding in R&amp;D&nbsp;



Generative AI products released recently by American companies, like ChatGPT, were developed with industry-funded research and development. But the R&amp;D that enables today‚Äôs AI was actually funded in large part by federal government agencies‚Äîlike the Defense Department, the National Science Foundation, NASA, and the National Institutes of Health‚Äîstarting in the 1950s. This includes the first successful AI program in 1956, the first chatbot in 1961, and the first expert systems for doctors in the 1970s, along with breakthroughs in machine learning, neural networks, backpropagation, computer vision, and natural-language processing.





American tax dollars also funded advances in hardware, communications networks, and other technologies underlying AI systems. Public research funding undergirded the development of lithium-ion batteries, micro hard drives, LCD screens, GPS, radio-frequency signal compression, and more in today‚Äôs smartphones, along with the chips used in AI data centers, and even the internet itself.



Instead of building on this world-class research history, the Trump administration is slashing R&amp;D funding, firing federal scientists, and squeezing leading research universities. This week‚Äôs action plan recommends investing in R&amp;D, but the administration‚Äôs actual budget proposes cutting nondefense R&amp;D by 36%. It also proposed actions to better coordinate and guide federal R&amp;D, but coordination won‚Äôt yield more funding.



Some say that companies‚Äô R&amp;D investments will make up the difference. However, companies conduct research that benefits their bottom line, not necessarily the national interest. Public investment allows broad scientific inquiry, including basic research that lacks immediate commercial applications but sometimes ends up opening massive markets years or decades later. That‚Äôs what happened with today‚Äôs AI industry.



Supporting immigration and immigrants



Beyond public R&amp;D investment, America has long attracted the world‚Äôs best researchers and innovators.



Today‚Äôs generative AI is based on the transformer model (the T in ChatGPT), first described by a team at Google in 2017. Six of the eight researchers on that team were born outside the US, and the other two are children of immigrants.&nbsp;



This isn‚Äôt an exception. Immigrants have been central to American leadership in AI. Of the 42 American companies included in the 2025 Forbes ranking of the 50 top AI startups, 60% have at least one immigrant cofounder, according to an analysis by the Institute for Progress. Immigrants also cofounded or head the companies at the center of the AI ecosystem: OpenAI, Anthropic, Google, Microsoft, Nvidia, Intel, and AMD.



‚ÄúBrain drain‚Äù is a term that was first coined to describe scientists‚Äô leaving other countries for the US after World War II‚Äîto the Americans‚Äô benefit. Sadly, the trend has begun reversing this year. Recent studies suggest that the US is already losing its AI talent edge through the administration‚Äôs anti-immigration actions (including actions taken against AI researchers) and cuts to R&amp;D funding.



Banning noncompetes



Attracting talented minds is only half the equation; giving them freedom to innovate is just as crucial.



Silicon Valley got its name because of mid-20th&#8211;century companies that made semiconductors from silicon, starting with the founding of Shockley Semiconductor in 1955. Two years later, a group of employees, the ‚ÄúTraitorous Eight,‚Äù quit to launch a competitor, Fairchild Semiconductor. By the end of the 1960s, successive groups of former Fairchild employees had left to start Intel, AMD, and others collectively dubbed the ‚ÄúFairchildren.‚Äù&nbsp;



Software and internet companies eventually followed, again founded by people who had worked for their predecessors. In the 1990s, former Yahoo employees founded WhatsApp, Slack, and Cloudera; the ‚ÄúPayPal Mafia‚Äù created LinkedIn, YouTube, and fintech firms like Affirm. Former Google employees have launched more than 1,200 companies, including Instagram and Foursquare.





AI is no different. OpenAI has founders that worked at other tech companies and alumni who have gone on to launch over a dozen AI startups, including notable ones like Anthropic and Perplexity.



This labor fluidity and the innovation it has created were possible in large part, according to many historians, because California‚Äôs 1872 constitution has been interpreted to prohibit noncompete agreements in employment contracts‚Äîa statewide protection the state originally shared only with North Dakota and Oklahoma. These agreements bind one in five American workers.



Last year, the Federal Trade Commission under President Biden moved to ban noncompetes nationwide, but a Trump-appointed federal judge has halted the action. The current FTC has signaled limited support for the ban and may be comfortable dropping it. If noncompetes persist, American AI innovation, especially outside California, will be limited.



Pursuing antitrust actions



One of this week‚Äôs announcements requires the review of FTC investigations and settlements that ‚Äúburden AI innovation.‚Äù During the last administration the agency was reportedly investigating Microsoft‚Äôs AI actions, and several big tech companies have settlements that their lawyers surely see as burdensome, meaning this one action could thwart recent progress in antitrust policy. That‚Äôs an issue because, in addition to the labor fluidity achieved by banning noncompetes, antitrust policy has also acted as a key lubricant to the gears of Silicon Valley innovation.&nbsp;





Major antitrust cases in the second half of the 1900s, against AT&amp;T, IBM, and Microsoft, allowed innovation and a flourishing market for semiconductors, software, and internet companies, as the antitrust scholar Giovanna Massarotto has described.



William Shockley was able to start the first semiconductor company in Silicon Valley only because AT&amp;T had been forced to license its patent on the transistor as part of a consent decree resolving a DOJ antitrust lawsuit against the company in the 1950s.&nbsp;



The early software market then took off because in the late 1960s, IBM unbundled its software and hardware offerings as a response to antitrust pressure from the federal government. As Massarotto explains, the 1950s AT&amp;T consent decree also aided the flourishing of open-source software, which plays a major role in today‚Äôs technology ecosystem, including the operating systems for mobile phones and cloud computing servers.



Meanwhile, many attribute the success of early 2000s internet companies like Google to the competitive breathing room created by the federal government‚Äôs antitrust lawsuit against Microsoft in the 1990s.&nbsp;



Over and over, antitrust actions targeting the dominant actors of one era enabled the formation of the next. And today, big tech is stifling the AI market. While antitrust advocates were rightly optimistic about this administration‚Äôs posture given key appointments early on, this week‚Äôs announcements should dampen that excitement.&nbsp;



I don‚Äôt want to lose focus on where things are: We should want a future in which lives are improved by the positive uses of AI.&nbsp;



But if America wants to continue leading the world in this technology, we must invest in what made us leaders in the first place: bold public research, open doors for global talent, and fair competition.&nbsp;



Prioritizing short-term industry profits over these bedrock principles won‚Äôt just put our technological future at risk‚Äîit will jeopardize America‚Äôs role as the world‚Äôs innovation superpower.&nbsp;



Asad Ramzanali is the director of artificial intelligence and technology policy at the Vanderbilt Policy Accelerator. He previously served as the chief of staff and deputy director of strategy of the White House Office of Science and Technology Policy under President Biden.
‚Ä¢ America‚Äôs AI watchdog is losing its bite
  Most Americans encounter the Federal Trade Commission only if they‚Äôve been scammed: It handles identity theft, fraud, and stolen data. During the Biden administration, the agency went after AI companies for scamming customers with deceptive advertising or harming people by selling irresponsible technologies. With yesterday‚Äôs announcement of President Trump‚Äôs AI Action Plan, that era may now be over.&nbsp;



In the final months of the Biden administration under chair Lina Khan, the FTC levied a series of high-profile fines and actions against AI companies for overhyping their technology and bending the truth‚Äîor in some cases making claims that were entirely false.



It found that the security giant Evolv lied about the accuracy of its AI-powered security checkpoints, which are used in stadiums and schools but failed to catch a seven-inch knife that was ultimately used to stab a student. It went after the facial recognition company Intellivision, saying the company made unfounded claims that its tools operated without gender or racial bias. It fined startups promising bogus ‚ÄúAI lawyer‚Äù services and one that sold fake product reviews generated with AI.



These actions did not result in fines that crippled the companies, but they did stop them from making false statements and offered customers ways to recover their money or get out of contracts. In each case, the FTC found, everyday people had been harmed by AI companies that let their technologies run amok.



The plan released by the Trump administration yesterday suggests it believes these actions went too far. In a section about removing ‚Äúred tape and onerous regulation,‚Äù the White House says it will review all FTC actions taken under the Biden administration ‚Äúto ensure that they do not advance theories of liability that unduly burden AI innovation.‚Äù In the same section, the White House says it will withhold AI-related federal funding from states with ‚Äúburdensome‚Äù regulations.



This move by the Trump administration is the latest in its evolving attack on the agency, which provides a significant route of redress for people harmed by AI in the US. It‚Äôs likely to result in faster deployment of AI with fewer checks on accuracy, fairness, or consumer harm.



Under Khan, a Biden appointee, the FTC found fans in unexpected places. Progressives called for it to break up monopolistic behavior in Big Tech, but some in Trump‚Äôs orbit, including Vice President JD Vance, also supported Khan in her fights against tech elites, albeit for the different goal of ending their supposed censorship of conservative speech.¬†



But in January, with Khan out and Trump back in the White House, this dynamic all but collapsed. Trump released an executive order in February promising to ‚Äúrein in‚Äù independent agencies like the FTC that wage influence without consulting the president. The next month, he started taking that vow to‚Äîand past‚Äîits legal limits.



In March, he fired the only two Democratic commissioners at the FTC. On July 17 a federal court ruled that one of those firings, of commissioner Rebecca Slaughter, was illegal given the independence of the agency, which restored Slaughter to her position (the other fired commissioner, Alvaro Bedoya, opted to resign rather than battle the dismissal in court, so his case was dismissed). Slaughter now serves as the sole Democrat.



In naming the FTC in its action plan, the White House now goes a step further, painting the agency&#8217;s actions as a major obstacle to US victory in the ‚Äúarms race‚Äù to develop better AI more quickly than China. It promises not just to change the agency‚Äôs tack moving forward, but to review and perhaps even repeal AI-related sanctions it has imposed in the past four years.



How might this play out? Leah Frazier, who worked at the FTC for 17 years before leaving in May and served as an advisor to Khan, says it‚Äôs helpful to think about the agency‚Äôs actions against AI companies as falling into two areas, each with very different levels of support across political lines.&nbsp;



The first is about cases of deception, where AI companies mislead consumers. Consider the case of Evolv, or a recent case announced in April where the FTC alleges that a company called Workado, which offers a tool to detect whether something was written with AI, doesn‚Äôt have the evidence to back up its claims. Deception cases enjoyed fairly bipartisan support during her tenure, Frazier says.



‚ÄúThen there are cases about responsible use of AI, and those did not seem to enjoy too much popular support,‚Äù adds Frazier, who now directs the Digital Justice Initiative at the Lawyers‚Äô Committee for Civil Rights Under Law. These cases don‚Äôt allege deception; rather, they charge that companies have deployed AI in a way that harms people.



The most serious of these, which resulted in perhaps the most significant AI-related action ever taken by the FTC and was investigated by Frazier, was announced in 2023. The FTC banned Rite Aid from using AI facial recognition in its stores after it found the technology falsely flagged people, particularly women and people of color, as shoplifters. ‚ÄúActing on false positive alerts,‚Äù the FTC wrote, Rite Aid‚Äôs employees ‚Äúfollowed consumers around its stores, searched them, ordered them to leave, [and] called the police to confront or remove consumers.‚Äù



The FTC found that Rite Aid failed to protect people from these mistakes, did not monitor or test the technology, and did not properly train employees on how to use it. The company was banned from using facial recognition for five years.&nbsp;



This was a big deal. This action went beyond fact-checking the deceptive promises made by AI companies to make Rite Aid liable for how its AI technology harmed consumers. These types of responsible-AI cases are the ones Frazier imagines might disappear in the new FTC, particularly if they involve testing AI models for bias.



‚ÄúThere will be fewer, if any, enforcement actions about how companies are deploying AI,‚Äù she says. The White House‚Äôs broader philosophy toward AI, referred to in the plan, is a ‚Äútry first‚Äù approach that attempts to propel faster AI adoption everywhere from the Pentagon to doctor&#8217;s offices. The lack of FTC enforcement that is likely to ensue, Frazier says, ‚Äúis dangerous for the public.‚Äù
‚Ä¢ The Download: gas and oil‚Äôs role in climate tech, and using AI to decipher ancient Latin
  This is today&#8217;s edition of¬†The Download,¬†our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



What role should oil and gas companies play in climate tech?



‚ÄîCasey Crownhart



After writing about Quaise, a geothermal startup that‚Äôs trying to commercialize new drilling technology, I&#8217;ve been thinking about the role oil and gas companies are playing in the energy transition. It‚Äôs becoming increasingly common in climate tech to see a startup join up with a bigger fossil fuel company in its field, like Quaise has with Nabors Industries, one of the biggest drilling firms in the world.



This industry has resources and energy expertise‚Äîbut also a vested interest in fossil fuels. Can it really be part of addressing climate change? Read the full story.



This article is from The Spark, MIT Technology Review‚Äôs weekly climate newsletter. To receive it in your inbox every Wednesday, sign up here.







Google DeepMind‚Äôs new AI can help historians understand ancient Latin inscriptions



Google DeepMind has unveiled new artificial intelligence software that could help historians recover the meaning and context behind ancient Latin engravings. Aeneas can analyze words written in long-weathered stone to say when and where they were originally inscribed.&nbsp;



It follows Google‚Äôs previous archaeological tool Ithaca, which also used deep learning to reconstruct and contextualize ancient text, in its case Greek. But while Ithaca and Aeneas use some similar systems, Aeneas also promises to give researchers jumping-off points for further analysis. Read the full story.



‚ÄîPeter Hall







The must-reads



I‚Äôve combed the internet to find you today‚Äôs most fun/important/scary/fascinating stories about technology.



1 Donald Trump has unveiled his AI Action Plan



He signed multiple orders to boost US AI exports and loosen regulations. (Bloomberg $)+ The plans could reshape how US tech firms train their models. (TechCrunch)+ We‚Äôre living in the age of chatbot culture wars. (NYT $)2 China hopes to sell its excess computing powerAfter rapidly building too many data centers. (Reuters)+ China built hundreds of AI data centers to catch the AI boom. Now many stand unused. (MIT Technology Review)



3 How corn is worsening Indiana‚Äôs dangerous heatwaveResidents are increasingly at risk of severe heat illness, as the moisture from corn increases humidity levels. (Axios)+ What is corn sweat, exactly? (Vox)+ Here‚Äôs how much heat your body can take. (MIT Technology Review)4 What‚Äôs next for Tesla?Its sales are falling, and its push into robotaxis is coming at a steep cost. (TechCrunch)+ Elon Musk appeared oddly upbeat on an analyst call. (The Information $)+ Why scaling up robotaxi fleets is such a challenge. (FT $)5 The US is poised to reinstate a banned herbicideDicamba has caused substantial damage to neighboring crops in the past. (WP $)+ The weeds are winning. (MIT Technology Review)



6 Why Amazon is eyeing AI gadgetsA bracelet that records conversations is the latest addition to its roster. (WSJ $)+ Why AI hardware needs to be open. (MIT Technology Review)7 Americans love China‚Äôs short video dramasWatch out Hollywood‚Äîduanju clips are on the rise. (Wired $)¬†+ China‚Äôs next cultural export could be TikTok-style short soap operas. (MIT Technology Review)



8 How a YouTube channel captured the spirit of rogue music discoveryMusic Place has gained a cult following from sharing obscure gems. (Pitchfork)



9 Pinterest isn‚Äôt immune to AI slopGood luck remodelling your home based on its fantastical designs. (FT $)



10 AI videos are coming to YouTube ShortsIt‚Äôll do everything from creating backgrounds to conjuring up video elements from a text prompt. (Ars Technica)+ What‚Äôs next for generative video. (MIT Technology Review)







Quote of the day



‚ÄúYou could throw out the results of all these papers.‚Äù



‚ÄîJeffrey Morris, a biostatistics professor at the University of Pennsylvania, criticizes scientific papers co-authored by the US government‚Äôs vaccine safety investigator and vaccine skeptic David Geier to the Atlantic.







One more thing







What is AI?Artificial intelligence is the hottest technology of our time. But what is it? It sounds like a stupid question, but it‚Äôs one that‚Äôs never been more urgent.If you‚Äôre willing to buckle up and come for a ride, I can tell you why nobody really knows, why everybody seems to disagree, and why you‚Äôre right to care about it. Read the full story.



‚ÄîWill Douglas Heaven







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)+ What could be more fun than bumping into Billy Joel while cycling a pedicab around NYC.+ Margarita ice cream is certainly one way to cool down in the summer heat.+ The Denver Museum of Nature &amp; Science has made an extremely unusual discovery‚Äîa 67.5 million year old dinosaur fossil under its parking lot.+ In praise of Jane Austen, by way of Clueless.

üîí Cybersecurity & Privacy
‚Ä¢ Phishers Target Aviation Execs to Scam Customers
  KrebsOnSecurity recently heard from a reader whose boss&#8217;s email account got phished and was used to trick one of the company&#8217;s customers into sending a large payment to scammers. An investigation into the attacker&#8217;s infrastructure points to a long-running Nigerian cybercrime ring that is actively targeting established companies in the transportation and aviation industries.
Image: Shutterstock, Mr. Teerapon Tiuekhom.
A reader who works in the transportation industry sent a tip about a recent successful phishing campaign that tricked an executive at the company into entering their credentials at a fake Microsoft 365 login page. From there, the attackers quickly mined the executive&#8217;s inbox for past communications about invoices, copying and modifying some of those messages with new invoice demands that were sent to some of the company&#8217;s customers and partners.
Speaking on condition of anonymity, the reader said the resulting phishing emails to customers came from a newly registered domain name that was remarkably similar to their employer&#8217;s domain, and that at least one of their customers fell for the ruse and paid a phony invoice. They said the attackers had spun up a look-alike domain just a few hours after the executive&#8217;s inbox credentials were phished, and that the scam resulted in a customer suffering a six-figure financial loss.
The reader also shared that the email addresses in the registration records for the imposter domain &#8212; roomservice801@gmail.com &#8212; is tied to many such phishing domains. Indeed, a search on this email address at DomainTools.com finds it is associated with at least 240 domains registered in 2024 or 2025. Virtually all of them mimic legitimate domains for companies in the aerospace and transportation industries worldwide.
An Internet search for this email address reveals a humorous blog post from 2020 on the Russian forum hackware[.]ru, which found roomservice801@gmail.com was tied to a phishing attack that used the lure of phony invoices to trick the recipient into logging in at a fake Microsoft login page. We&#8217;ll come back to this research in a moment.
JUSTY JOHN
DomainTools shows that some of the early domains registered to roomservice801@gmail.com in 2016 include other useful information. For example, the WHOIS records for alhhomaidhicentre[.]biz reference the technical contact of &#8220;Justy John&#8221; and the email address justyjohn50@yahoo.com.
A search at DomainTools found justyjohn50@yahoo.com has been registering one-off phishing domains since at least 2012. At this point, I was convinced that some security company surely had already published an analysis of this particular threat group, but I didn&#8217;t yet have enough information to draw any solid conclusions.
DomainTools says the Justy John email address is tied to more than two dozen domains registered since 2012, but we can find hundreds more phishing domains and related email addresses simply by pivoting on details in the registration records for these Justy John domains. For example, the street address used by the Justy John domain axisupdate[.]net &#8212; 7902 Pelleaux Road in Knoxville, TN &#8212; also appears in the registration records for accountauthenticate[.]com, acctlogin[.]biz, and loginaccount[.]biz, all of which at one point included the email address rsmith60646@gmail.com.
That Rsmith Gmail address is connected to the 2012 phishing domain alibala[.]biz (one character off of the Chinese e-commerce giant alibaba.com, with a different top-level domain of .biz). A search in DomainTools on the phone number in those domain records &#8212; 1.7736491613 &#8212; reveals even more phishing domains as well as the Nigerian phone number &#8220;2348062918302&#8221; and the email address michsmith59@gmail.com.
DomainTools shows michsmith59@gmail.com appears in the registration records for the domain seltrock[.]com, which was used in the phishing attack documented in the 2020 Russian blog post mentioned earlier. At this point, we are just two steps away from identifying the threat actor group.
The same Nigerian phone number shows up in dozens of domain registrations that reference the email address sebastinekelly69@gmail.com, including 26i3[.]net, costamere[.]com, danagruop[.]us, and dividrilling[.]com. A Web search on any of those domains finds they were indexed in an &#8220;indicator of compromise&#8221; list on GitHub maintained by Palo Alto Networks&#8216; Unit 42 research team.
SILVERTERRIER
According to Unit 42, the domains are the handiwork of a vast cybercrime group based in Nigeria that it dubbed &#8220;SilverTerrier&#8221; back in 2014. In an October 2021 report, Palo Alto said SilverTerrier excels at so-called &#8220;business e-mail compromise&#8221; or BEC scams, which target legitimate business email accounts through social engineering or computer intrusion activities. BEC criminals use that access to initiate or redirect the transfer of business funds for personal gain.
Palo Alto says SilverTerrier encompasses hundreds of BEC fraudsters, some of whom have been arrested in various international law enforcement operations by Interpol. In 2022, Interpol and the Nigeria Police Force arrested 11 alleged SilverTerrier members, including a prominent SilverTerrier leader who&#8217;d been flaunting his wealth on social media for years. Unfortunately, the lure of easy money, endemic poverty and corruption, and low barriers to entry for cybercrime in Nigeria conspire to provide a constant stream of new recruits.
BEC scams were the 7th most reported crime tracked by the FBI&#8217;s Internet Crime Complaint Center (IC3) in 2024, generating more than 21,000 complaints. However, BEC scams were the second most costly form of cybercrime reported to the feds last year, with nearly $2.8 billion in claimed losses.¬†In its 2025 Fraud and Control Survey Report, the Association for Financial Professionals found 63 percent of organizations experienced a BEC last year.
Poking at some of the email addresses that spool out from this research reveals a number of Facebook accounts for people residing in Nigeria or in the United Arab Emirates, many of whom do not appear to have tried to mask their real-life identities. Palo Alto&#8217;s Unit 42 researchers reached a similar conclusion, noting that although a small subset of these crooks went to great lengths to conceal their identities, it was usually simple to learn their identities on social media accounts and the major messaging services.
Palo Alto said BEC actors have become far more organized over time, and that while it remains easy to find actors working as a group, the practice of using one phone number, email address or alias to register malicious infrastructure in support of multiple actors has made it far more time consuming (but not impossible) for cybersecurity and law enforcement organizations to sort out which actors committed specific crimes.
&#8220;We continue to find that SilverTerrier actors, regardless of geographical location, are often connected through only a few degrees of separation on social media platforms,&#8221; the researchers wrote.
FINANCIAL FRAUD KILL CHAIN
Palo Alto has published a useful list of recommendations that organizations can adopt to minimize the incidence and impact of BEC attacks. Many of those tips are prophylactic, such as conducting regular employee security training and reviewing network security policies.
But one recommendation &#8212; getting familiar with a process known as the &#8220;financial fraud kill chain&#8221; or FFKC &#8212; bears specific mention because it offers the single best hope for BEC victims who are seeking to claw back payments made to fraudsters, and yet far too many victims don&#8217;t know it exists until it is too late.
Image: ic3.gov.
As explained in this FBI primer, the International Financial Fraud Kill Chain is a partnership between federal law enforcement and financial entities whose purpose is to freeze fraudulent funds wired by victims. According to the FBI, viable victim complaints filed with ic3.gov promptly after a fraudulent transfer (generally less than 72 hours) will be automatically triaged by the Financial Crimes Enforcement Network (FinCEN).
The FBI noted in its IC3 annual report (PDF) that the FFKC had a 66 percent success rate in 2024. Viable ic3.gov complaints involve losses of at least $50,000, and include all records from the victim or victim bank, as well as a completed FFKC form (provided by FinCEN) containing victim information, recipient information, bank names, account numbers, location, SWIFT, and any additional information.
‚Ä¢ Microsoft Fix Targets Attacks on SharePoint Zero-Day
  Microsoft Corp. issued an emergency security update for a vulnerability in SharePoint Server that is actively being exploited to compromise vulnerable organizations . The patch comes amid reports that malicious hackers have used the SharePoint flaw to breach U.S. federal and state agencies, universities, and energy companies . The Washington Post reported on Sunday that the government and partners in Canada and Australia are investigating the hack of SharePoint servers, which provide a platform for sharing and managing documents .

üéì University AI
No updates.

üè¢ Corporate AI
‚Ä¢ Navigating medical education in the era of generative AI
  In November 2022, OpenAI‚Äôs ChatGPT kick-started a new era in AI. This was followed less than a half year later by the release of GPT-4. In the months leading up to GPT-4‚Äôs public release, Peter Lee, president of Microsoft Research, cowrote a book full of optimism for the potential of advanced AI models to transform the world of healthcare. What has happened since? In this special podcast series, The AI Revolution in Medicine, Revisited, Lee revisits the book, exploring how patients, providers, and other medical professionals are experiencing and using generative AI today while examining what he and his coauthors got right‚Äîand what they didn‚Äôt foresee.&nbsp;&nbsp;&nbsp;



In this episode, Dr. Morgan Cheatham (opens in new tab) and Daniel Chen (opens in new tab), two rising physicians and experts in both medicine and technology, join Lee to explore how generative AI is reshaping medical education. Cheatham, a partner and head of healthcare and life sciences at Breyer Capital and a resident physician at Boston Children‚Äôs Hospital, discusses how AI is changing how clinicians acquire and apply medical knowledge at the point of care, emphasizing the need for training and curriculum changes to help ensure AI is used responsibly and that clinicians are equipped to maximize its potential. Chen, a medical student at the Kaiser Permanente Bernard J. Tyson School of Medicine, shares how he and his peers use AI tools as study aids, clinical tutors, and second opinions and reflects on the risks of overreliance and the importance of preserving critical thinking.







Learn more:



Perspectives on the Current and Future State of Artificial Intelligence in Medical Genetics (opens in new tab)&nbsp;(Cheatham)Publication | May 2025



Performance of ChatGPT on USMLE: Potential for AI-assisted medical education using large language models (opens in new tab) (Cheatham)&nbsp;Publication | February 2023&nbsp;



The AI Revolution in Medicine: GPT-4 and Beyond&nbsp;Book | Peter Lee, Carey Goldberg, Isaac Kohane | April 2023&nbsp;




	
		
			Subscribe to the Microsoft Research Podcast:		
		
							
					
						  
						Apple Podcasts
					
				
			
							
					
						
						Email
					
				
			
							
					
						
						Android
					
				
			
							
					
						
						Spotify
					
				
			
							
					
						
						RSS Feed
					
				
					
	




	
		
			
				
					

Transcript



[MUSIC]‚ÄØ‚ÄØ‚ÄØ‚ÄØ&nbsp;



[BOOK PASSAGE]&nbsp;



PETER LEE: ‚ÄúMedicine often uses the training approach when trying to assess multipurpose talent. To ensure students can safely and effectively take care of patients, we have them jump through quite a few hoops, &#8230; [and] they need good evaluations once they reach the clinic, passing grades on more exams like the USMLE [United States Medical Licensing Examination]. &#8230; [But] GPT-4 gets more than 90 percent of questions on licensing exams correct. &#8230; Does that provide any level of comfort in using GPT-4 in medicine?‚Äù&nbsp;



[END OF BOOK PASSAGE]‚ÄØ‚ÄØ‚ÄØ‚ÄØ&nbsp;



[THEME MUSIC]‚ÄØ‚ÄØ‚ÄØ‚ÄØ&nbsp;



This is The AI Revolution in Medicine, Revisited. I‚Äôm your host, Peter Lee.‚ÄØ‚ÄØ‚ÄØ‚ÄØ&nbsp;



Shortly after OpenAI&#8217;s GPT-4 was publicly released, Carey Goldberg, Dr. Zak Kohane, and I published The AI Revolution in Medicine to help educate the world of healthcare and medical research about the transformative impact this new generative AI technology could have. But because we wrote the book when GPT-4 was still a secret, we had to speculate. Now, two years later, what did we get right, and what did we get wrong?‚ÄØ‚ÄØ‚ÄØ‚ÄØ‚ÄØ&nbsp;



In this series, we‚Äôll talk to clinicians, patients, hospital administrators, and others to understand the reality of AI in the field and where we go from here.



				
				
					



[THEME MUSIC FADES]‚ÄØ&nbsp;



The book passage I read at the top is from Chapter 4, ‚ÄúTrust but Verify.‚Äù In it, we explore how AI systems like GPT-4 should be evaluated for performance, safety, and reliability and compare this to how humans are both trained and assessed for readiness to deliver healthcare.&nbsp;



In previous conversations with guests, we‚Äôve spoken a lot about AI in the clinic as well as in labs and companies developing AI-driven tools. We‚Äôve also talked about AI in the hands of patients and consumers. But there has been some discussion also about AI‚Äôs role in medical training. And, as a founding board member of a new medical school at Kaiser Permanente, I definitely have my own thoughts about this. But today, I‚Äôm excited to welcome two guests who represent the next generation of medical professionals for their insights, Morgan Cheatham and Daniel Chen.&nbsp;



Morgan Cheatham is a graduate of Brown University&#8217;s Warren Alpert Medical School with clinical training in genetics at Harvard and is a clinical fellow at Boston Children&#8217;s Hospital. While Morgan is a bona fide doctor in training, he‚Äôs also amazingly an influential health technology strategist. He was recently named partner and head of healthcare and life sciences at Breyer Capital and has led investments in several healthcare AI companies that have eclipsed multibillion-dollar valuations.&nbsp;&nbsp;



Daniel Chen is finishing his second year as a medical student at the Kaiser Permanente Bernard J. Tyson School of Medicine. He holds a neuroscience degree from the University of Washington and was a research assistant in the Raskind Lab at the UW School of Medicine, working with imaging and genetic data analyses for biomedical research. Prior to med school, Daniel pursued experiences that cultivated his interest in the application of AI in medical practice and education.&nbsp;&nbsp;



Daniel and Morgan exemplify the real-world future of healthcare, a student entering his third year of medical school and a fresh medical-school graduate who is starting a residency while at the same time continuing his work on investing in healthcare startups.&nbsp;



[TRANSITION MUSIC]‚ÄØ



Here is my interview with Morgan Cheatham:&nbsp;



LEE: Morgan, thanks for joining. Really, really looking forward to this chat.&nbsp;



MORGAN CHEATHAM: Peter, it&#8217;s a privilege to be here with you. Thank you.&nbsp;



LEE: So are there any other human beings who are partners at big-league venture firms, residents at, you know, a Harvard-affiliated medical center, author, editor for a leading medical journal? I mean, who are your ‚Ä¶ who&#8217;s your cohort? Who are your peers?&nbsp;



CHEATHAM: I love this question. There are so many people who I consider peers that I look up to who have paved this path. And I think what is distinct about each of them is they have this physician-plus orientation. They are multilingual in terms of knowing the language of medicine but having learned other disciplines. And we share a common friend, Dr. Zak Kohane, who was among the first to really show how you can meld two worlds as a physician and make significant contributions to the intersections thereof.&nbsp;&nbsp;



I also deeply, in the world of business, respect physicians like Dr. Krishna Yeshwant at Google Ventures, who simultaneously pursued residency training and built what is now, you know, a large and enduring venture firm.&nbsp;&nbsp;



So there are plenty of people out there who&#8217;ve carved their own path and become these multilingual beings, and I aspire to be one.&nbsp;



LEE: So, you know, one thing I&#8217;ve been trying to explore with people are their origins with respect to the technology of AI. And there&#8217;s two eras for that. There&#8217;s AI before ChatGPT and before, you know, generative AI really became a big thing, and then afterwards.&nbsp;&nbsp;



So let&#8217;s start first before ChatGPT. You know, what was your contact? What was, you know, your knowledge of AI and machine learning?&nbsp;



CHEATHAM: Sure, so my experiences in computer science date back to high school. I went to Thomas Jefferson, which is a high school in Alexandria, Virginia, that prides itself on requiring students to take computer science in their first year of high school as kind of a required torturous experience. [LAUGHTER] And I remember that fondly. Our final project was Brick Breaker. It was actually, I joke, all hard coded. So there was nothing intelligent about the Brick Breaker that we built. But that was my first exposure.&nbsp;&nbsp;



I was a classics nerd, and I was really interested in biology and chemistry as a pre-medical student. So I really wouldn&#8217;t intersect with this field again until I was shadowing at Inova Hospital, which was a local hospital near me. And it was interesting because, at the time‚ÄîI was shadowing in the anesthesia department‚Äîthey were actually implementing their first instance of Epic.&nbsp;



LEE: Mmm. Wow.&nbsp;



CHEATHAM: And I remember that experience fondly because the entire hospital was going from analog‚Äîthey were going from paper-based charts‚Äîto this new digital system. And I didn&#8217;t quite know in that moment what it would mean for the field or for my career, but I knew it was a big deal because a lot of people had a lot of emotion around what was going on, and it was in that experience that I kind of decided to attach myself to the intersection of computation and medicine. So when I got to undergrad, I was a pre-medical student. I was very passionate about studying the sacred physician-patient relationship and everything that had to go on in that exam room to provide excellent care.&nbsp;&nbsp;



But there were a few formative experiences: one, working at a physician-founded startup that was using at the time we called it big data, if you remember, ‚Ä¶&nbsp;



LEE: Yup.&nbsp;



CHEATHAM: ‚Ä¶ to match the right patient to the right provider at the right time. And it was in that moment that I realized that as a physician, I could utilize technology to scale that sacred one-to-one patient-provider interaction in nonlinear ways. So that was, kind of, the first experience where I saw deployed systems that were using patient data and clinical information in an intelligent format.&nbsp;



LEE: Yeah. And so you&#8217;re a pre-medical student, but you have this computer science understanding. You have an intuition, I guess is the right way to say it, that the clinical data becoming digital is going to be important. So then what happens from there to your path to medical school?&nbsp;



CHEATHAM: Yeah, so I had a few formative research experiences in my undergraduate years. You know, nothing that ever amounted to a significant publication, but I was toying around with SVMs [support vector machines] for sepsis and working with the MIMIC [Medical Information Mart for Intensive Care] database early days and really just trying to understand what it meant that medical data was becoming digitized.&nbsp;&nbsp;



And at the same time, again, I was rather unsatisfied doing that purely in an academic context. And I so early craved seeing how this would roll out in the wild, roll out in a clinical setting that I would soon occupy. And that was really what drove me to work at this company called Kyruus [Health] and understand how these systems, you know, scaled. Obviously, that&#8217;s something with AI that we&#8217;re now grappling with in a real way because it looks much different.&nbsp;&nbsp;



LEE: Right. Yep.&nbsp;



CHEATHAM: So the other experience I had, which is less relevant to AI, but I did do a summer in banking. And I mention this because what I learned in the experience was ‚Ä¶ it was a master class in business. And I learned that there was another scaling factor that I should appreciate as we think about medicine, and that was capital and business formation. And that was also something that could scale nonlinearly.&nbsp;&nbsp;



So when you married that with technology, it was, kind of, a natural segue for me before going to med school to think about venture capital and partnering with founders who were going to be building these technologies for the long term. And so that&#8217;s how I landed on the venture side.&nbsp;



LEE: And then how long of a break before you started your medical studies?&nbsp;



CHEATHAM: It was about four years. Originally, it was going to be a two-year deferral, and the pandemic happened. Our space became quite active in terms of new companies and investment. So it was about four years before I went back.&nbsp;



LEE: I see. And so you&#8217;re in medical school. ChatGPT happened while you were in medical school, is that right?&nbsp;



CHEATHAM: That&#8217;s right. That&#8217;s right. Right before I was studying for Step 1. So the funny story, Peter, that I like to share with folks is ‚Ä¶&nbsp;



LEE: Yeah.&nbsp;



CHEATHAM: ‚Ä¶ I was just embarking on designing my Step 1 study plan with my mentor. And I went to NeurIPS [Conference] for the first time. And that was in 2022, when, of course, ChatGPT was released.&nbsp;&nbsp;



And for the remainder of that fall period, you know, I should have been studying for these shelf exams and, you know, getting ready ‚Ä¶&nbsp;&nbsp;



LEE: Yeah.&nbsp;



CHEATHAM: ‚Ä¶ for this large board exam. And I was fortunate to partner, actually, with one of our portfolio company CEOs who is a physician‚Äîhe is an MD/PhD‚Äîto work on the first paper that showed that ChatGPT could pass the US Medical Licensing Exam (opens in new tab).&nbsp;&nbsp;



LEE: Yes.&nbsp;



CHEATHAM: And that was a riveting experience for a number of reasons. I joke with folks that it was both the best paper I was ever, you know, a part of and proud to be a coauthor of, but also the worst for a lot of reasons that we could talk about.&nbsp;&nbsp;



It was the best in terms of canonical metrics like citations, but the worst in terms of, wow, did we spend six months as a field thinking this was the right benchmark ‚Ä¶ [LAUGHTER]&nbsp;



LEE: Right.&nbsp;



CHEATHAM: ‚Ä¶ for how to assess the performance of these models. And I&#8217;m so encouraged ‚Ä¶&nbsp;



LEE: You shouldn&#8217;t feel bad that way because, you know, at that time, I was secretly, you know, assessing what we now know of as GPT-4 in that period. And what was the first thing I tried to do? Step 1 medical exam.&nbsp;&nbsp;



By the way, just for our listeners who don&#8217;t understand about medical education‚Äîin the US, there&#8217;s a three-part exam that extends over a couple of years of medical school. Step 1, Step 2, Step 3. And Step 1 and Step 2 in particular are multiple-choice exams.&nbsp;



And they are very high stakes when you&#8217;re in medical school. And you really have to have a command of quite a lot of clinical knowledge to pass these. And it&#8217;s funny to hear you say what you were just sharing there because it was also the first impulse I had with GPT-4. And in retrospect, I feel silly about that.&nbsp;



CHEATHAM: I think many of us do, but I&#8217;ve been encouraged over the last two years, to your point, that we really have moved our discourse beyond these exams to thinking about more robust systems for the evaluation of performance, which becomes even more interesting as you and I have spoken about these multi-agent frameworks that we are now, you know, compelled to explore further.&nbsp;



LEE: Yeah. Well, and even though I know you&#8217;re a little sheepish about it now, I think in the show notes, we&#8217;ll link to that paper because it really was one of the seminal moments when we think about AI, AI in medicine.



And so you&#8217;re seeing this new technology, and it&#8217;s happening at a moment when you yourself have to confront taking the Step 1 exam. So how did that feel?&nbsp;



CHEATHAM: It was humbling. It was shocking. What I had worked two years for, grueling over textbooks and, you know, flashcards and all of the things we do as medical students, to see a system emerge out of thin air that was arguably going to perform far better than I ever would, no matter how much ‚Ä¶&nbsp;&nbsp;



LEE: Yeah.&nbsp;



CHEATHAM: ‚Ä¶ I was going to study for that exam, it set me back. It forced me to interrogate what my role in medicine would be. And it dramatically changed the specialties that I considered for myself long term.&nbsp;&nbsp;



And I hope we talk about, you know, how I stumbled upon genetics and why I&#8217;m so excited about that field and its evolution in this computational landscape. I had to do a lot of soul searching to relinquish what I thought it meant to be a physician and how I would adapt in this new environment.&nbsp;



LEE: You know, one of the things that we wrote in our book, I think it was in a chapter that I contributed, I was imagining that students studying for Step 1 would be able to do it more actively.&nbsp;&nbsp;



Or you could even do sort of a pseudo-Step 3 exam by having a conversation. You provide the presentation of a patient and then have an encounter, you know, where the ChatGPT is the patient, and then you pretend to be the doctor.&nbsp;And then in the example that we published, then you say, ‚ÄúEnd of encounter.‚Äù And then you ask ChatGPT for an assessment of things.&nbsp;&nbsp;



So, you know, maybe it all came too late for Step 1 for you because you were already very focused and, you know, had your own kind of study framework. But did you have an influence or use of this kind of technology for Step 2 and Step 3?&nbsp;



CHEATHAM: So even for Step 1, I would say, it [ChatGPT], you know, dropped in November. I took it [Step 1 exam] in the spring, so I was able to use it to study. But the lesson I learned in that moment, Peter, was really about the importance of trust with AI and clinicians or clinicians in training, because we all have the same resources that we use for these board exams, right. UWorld is this question bank. It&#8217;s been around forever. If you&#8217;re not using UWorld, like, good luck. And so why would you deviate off of a well-trodden path to study for this really important exam?&nbsp;&nbsp;



And so I kind of adjunctively used GPT alongside UWorld to come up with more personalized explanations for concepts that I wasn&#8217;t understanding, and I found that it was pretty good and it was certainly helpful for me.&nbsp;&nbsp;



Fortunately, I was, you know, able to pass, but I was very intentional about dogfooding AI when I was a medical student, and part of that was because I had been a venture capitalist, and I&#8217;d made investments in companies whose products I could actually use.&nbsp;&nbsp;



And so, you know, Abridge is a company in the scribing space that you and I have talked about.&nbsp;&nbsp;



LEE: Yeah.&nbsp;



CHEATHAM: I was so fortunate in the early days of their product to not just be a user but to get to bring their product across the hospital. I could bring the product to the emergency department one week, to neurology another week, to the PICU [pediatric intensive care unit], you know, the next week, and assess the relative performance of, you know, how it handled really complex genetics cases ‚Ä¶&nbsp;



LEE: Yeah.&nbsp;



CHEATHAM: ‚Ä¶ versus these very challenging social situations that you often find yourself navigating in primary care. So not only was I emotional about this technology, but I was a voracious adopter in the moment.&nbsp;



LEE: Yeah, right. And you had a financial interest then on top of that, right?&nbsp;



CHEATHAM: I was not paid by Abridge to use the product, but, you know, I joke that the team was probably sick of me. [LAUGHTER]&nbsp;



LEE: No, no, but you were working for a venture firm that was invested in these, right? So all of these things are wrapped up together. You know, you&#8217;re having to get licensed as a doctor while doing all of this.&nbsp;&nbsp;



So I want to get into that investment and new venture stuff there, but let&#8217;s stick just for a few more minutes on medical education. So I mentioned, you know, what we wrote in the book, and I remember writing the example, you know, of an encounter. Is that at all realistic? Is anything like that ‚Ä¶ that was pure speculation on our part. What&#8217;s really happening?&nbsp;&nbsp;



And then after we talk about what&#8217;s really happening, what do you think should happen in medical education given the reality of generative AI?&nbsp;



CHEATHAM: I&#8217;ve been pleasantly surprised talking with my colleagues about AI in clinical settings, how curious people are and how curious they&#8217;ve been over the last two years. I think, oftentimes, we say, oh, you know, this technology really is stratified by age and the younger clinicians are using it more and the older physicians are ignoring it. And, you know, maybe that&#8217;s true in some regards, but I&#8217;ve seen, you know, many, you know, senior attendings pulling up Perplexity, GPT, more recently OpenEvidence (opens in new tab), which has been a really essential tool for me personally at the point of care, to come up with the best decisions for our patients.&nbsp;&nbsp;



The general skepticism arises when people reflect on their own experience in training and they think, ‚ÄúWell, I had to learn how to do it this way.‚Äù&nbsp;&nbsp;



LEE: Yeah.&nbsp;



CHEATHAM: ‚ÄúAnd therefore, you using an AI scribe to document this encounter doesn&#8217;t feel right to me because I didn&#8217;t get to do that.‚Äù And I did face some of those critiques or criticisms, where you need to learn how to do it the old-school way first and then you can use an AI scribe.&nbsp;&nbsp;



And I haven&#8217;t yet seen‚Äîmaybe even taking a step back‚ÄîI haven&#8217;t seen a lot of integration of AI into the core medical curriculum, period.&nbsp;&nbsp;



LEE: Yeah.&nbsp;



CHEATHAM: And, as you know, if you want to add something to medical school curriculum, you can get in a long line of people who also want to do that.&nbsp;



LEE: Yes. Yeah.



CHEATHAM: But it is urgent that our medical schools do create formalized required trainings for this technology because people are already using it.&nbsp;&nbsp;



LEE: Yes.&nbsp;



CHEATHAM: I think what we will need to determine is how much of the old way do people need to learn in order to earn the right to use AI at the point of care and how much of that old understanding, that prior experience, is essential to be able to assess the performance of these tools and whether or not they are having the intended outcome.&nbsp;&nbsp;



I kind of joke it&#8217;s like learning cursive, right?&nbsp;



LEE: Yes.&nbsp;



CHEATHAM: I&#8217;m old enough to have had to learn cursive. I don&#8217;t think people really have to learn it these days. When do I use it? Well, when I&#8217;m signing something. I don&#8217;t even really sign checks anymore, but &#8230;&nbsp;



LEE: Well ‚Ä¶ the example I&#8217;ve used, which you&#8217;ve heard, is, I&#8217;m sure you were still taught the technique of manual palpation, even though ‚Ä¶&nbsp;



CHEATHAM: Of course.&nbsp;



LEE: ‚Ä¶ you have access to technologies like ultrasound. And in fact, you would use ultrasound in many cases.&nbsp;&nbsp;



And so I need to pin you down. What is your opinion on these things? Do you need to be trained in the old ways?&nbsp;



CHEATHAM: When it comes to understanding the architecture of the medical note, I believe it is important for clinicians in training to know how that information is generated, how it&#8217;s characterized, and how to go from a very broad-reaching conversation to a distilled clinical document that serves many functions.&nbsp;



Does that mean that you should be forced to practice without an AI scribe for the entirety of your medical education? No. And I think that as you are learning the architecture of that document, you should be handed an AI scribe and you should be empowered to have visits with patients both in an analog setting‚Äîwhere you are transcribing and generating that note‚Äîand soon thereafter, I&#8217;m talking in a matter of weeks, working with an AI scribe. That&#8217;s my personal belief.&nbsp;&nbsp;



LEE: Yeah, yeah. So you&#8217;re going to &#8230; well, first off, congratulations on landing a residency at Boston Children&#8217;s [Hospital].&nbsp;&nbsp;



CHEATHAM: Thank you, Peter.&nbsp;



LEE: I understand there were only two people selected for this and super competitive. You know, with that perspective, you know, how do you see your future in medicine, just given everything that&#8217;s happening with AI right now?&nbsp;&nbsp;



And are there things that you would urge, let&#8217;s say, the dean of the Brown Medical School to consider or to change? Or maybe not the dean of Brown but the head of the LCME [Liaison Committee on Medical Education], the accrediting body for US medical schools. What in your mind needs to change?&nbsp;



CHEATHAM: Sure. I&#8217;ll answer your first question first and then talk about the future.&nbsp;&nbsp;



LEE: Yeah.&nbsp;



CHEATHAM: For me personally, I fell into the field of genomics. And so my training program will cover both pediatrics as well as clinical genetics and genomics.&nbsp;&nbsp;



And I alluded to this earlier, but one of the reasons I&#8217;m so excited to join the field is because I really feel like the field of genetics not only is focused on a very underserved patient population, but not in how we typically think of underserved. I&#8217;m talking about underserved as in patients who don&#8217;t always have answers. Patients for whom the current guidelines don&#8217;t offer information or comfort or support.&nbsp;



Those are patients that are extremely underserved. And I think in this moment of AI, there&#8217;s a unique opportunity to utilize the computational systems that we now have access to, to provide these answers more precisely, more quickly.&nbsp;&nbsp;



And so I&#8217;m excited to marry those two fields. And genetics has long been a field that has adopted technology. We just think about the foundational technology of genomic sequencing and variant interpretation. And so it&#8217;s a kind of natural evolution of the field, I believe, to integrate AI and specifically generative AI.&nbsp;



If I were speaking directly to the LCME, I mean, I would just have to encourage the organization, as well as medical societies who partner with attending physicians across specialties, to lean in here.



When I think about prior revolutions in technology and medicine, physicians were not always at the helm. We have a unique opportunity now, and you talk about companies like Abridge in the AI space, companies like Viz.ai, Cleerly‚ÄîI mean, I could go on: Iterative Health ‚Ä¶ I could list 20 organizations that are bringing AI to the point of care that are founded by physicians.



This is our moment to have a seat at the table and to shape not only the discourse but the deployment. And the unique lens, of course, that a physician brings is that of prioritizing the patient, and with AI and this time around, we have to do that.



LEE: So LCME for our listeners is, I think it stands for the Liaison Committee on Medical Education (opens in new tab). It&#8217;s basically the accrediting body for US medical schools, and it&#8217;s very high stakes. It&#8217;s very, very rigorous, which I think is a good thing, but it&#8217;s also a bit of a straitjacket.&nbsp;&nbsp;



So if you are on the LCME, are there specific new curricular elements that you would demand that LCME, you know, add to its accreditation standards?&nbsp;



CHEATHAM: We need to unbundle the different components of the medical appointment and think about the different functions of a human clinician to answer that question.&nbsp;&nbsp;



There are a couple of areas that are top of mind for me, the first being medical search. There are large organizations and healthcare incumbents that have been around for many decades, companies like UpToDate or even, you know, the guidelines that are housed by our medical societies, that need to respond to the information demands of clinicians at the point of care in a new way with AI.&nbsp;&nbsp;



And so I would love to see our medical institutions teaching more students how to use AI for medical search problems at the point of care. How to not only, you know, from a prompt perspective, ask questions about patients in a high-efficacy way, but also to interpret the outputs of these systems to inform downstream clinical decision-making.&nbsp;&nbsp;



People are already adopting, as you know, GPT, OpenEvidence, Perplexity, all of these tools to make these decisions now.&nbsp;&nbsp;



And so by not‚Äîagain, it&#8217;s a moral imperative of the LCME‚Äîby not having curriculum and support for clinicians doing that, we run the risk of folks not utilizing these tools properly but also to their greatest potential.&nbsp;



LEE: Yeah, then, but zooming forward then, what about board certification?&nbsp;



CHEATHAM: Board certification today is already transitioning to an open-book format for many specialties, is my understanding. And in talking to some of my fellow geneticists, who, you know, that&#8217;s a pretty challenging board exam in clinical genetics or biochemical genetics. They are using OpenEvidence during those open-book exams.&nbsp;&nbsp;



So what I would like to see us do is move from a system of rote memorization and regurgitation of fact to an assessment framework that is adaptive, is responsive, and assesses for your ability to use the tools that we now have at our disposal to make sound clinical decisions.&nbsp;



LEE: Yeah. We&#8217;ve heard from Sara Murray, you know, that when she&#8217;s doing her rounds, she consults routinely with ChatGPT. And that was something we also predicted, especially Carey Goldberg in our book, you know, wrote this fictional account.&nbsp;&nbsp;



Is that the primary real-world use of AI? Not only by clinicians, but also by medical students &#8230; are medical students, you know, engaged with ChatGPT or, you know, similar?&nbsp;



CHEATHAM: Absolutely. I&#8217;ve listed some of the tools. I think there, in general, Peter, there is this new clinician stack that is emerging of these tools that people are trying, and I think the cycles of iteration are quick, right. Some folks are using Claude [Claude.ai] one week, and they&#8217;re trying Perplexity, or they&#8217;re trying OpenEvidence, they&#8217;re trying GPT for a different task.¬†¬†



There&#8217;s this kind of moment in medicine that every clinician experiences where you&#8217;re on rounds, and there&#8217;s that very senior attending. And you&#8217;ve just presented a patient to them, and you think you did an elegant job, and you&#8217;ve summarized all the information, and you really feel good about your differential, and they ask you, like, the one question you didn&#8217;t think to address. [LAUGHTER]&nbsp;



And I&#8217;ll tell you, some of the funniest moments I&#8217;ve had using AI in the hospital has been, and let me take a step back, that process of an attending physician interrogating a medical student is called ‚Äúpimping,‚Äù for lack of a better phrase.&nbsp;&nbsp;



And some of the funniest use cases I&#8217;ve had for AI in that setting is actually using OpenEvidence or GPT as defense against pimping. [LAUGHTER] So quickly while they&#8217;re asking me the question, I put it in, and I&#8217;m actually able to answer it right away. So it&#8217;s been effective for that. But I would say, you know, [in] the halls of most of the hospitals where I&#8217;ve trained, I&#8217;m seeing this technology in the wild.



LEE: So now you&#8217;re so tech-forward, but that off-label use of AI, we also, when we wrote our book, we weren&#8217;t sure that at least top health systems would tolerate this. Do you have an opinion about this? Should these things be better regulated or controlled by the CIOs of Boston Children&#8217;s?&nbsp;



CHEATHAM: I&#8217;m a big believer that transparency encourages good behaviors.&nbsp;



And so the first time I actually tried to use ChatGPT in a clinical setting, it was at a hospital in Rhode Island. I will not name which hospital. But the site was actually blocked. I wasn&#8217;t able to access it from a desktop. That was the hospital&#8217;s first response to this technology was, let&#8217;s make sure none of our clinicians can access it. It has so much potential for medicine. The irony of that today.&nbsp;&nbsp;



And it&#8217;s since, you know, become unblocked. But I was able to use it on my phone. So, to your point, if there&#8217;s a will, there&#8217;s a way. And we will utilize this technology if we are seeing perceived value.&nbsp;



LEE: So, yeah, no, absolutely. So now, you know, in some discussions, one superpower that seems to be common across people who are really leading the charge here is they seem to be very good readers and students.&nbsp;&nbsp;



And I understand you also as a voracious reader. In fact, you&#8217;re even on an editorial team for a major medical journal. To what extent does that help?&nbsp;&nbsp;



And then from your vantage point at New England Journal of Medicine AI‚Äîand I&#8217;ll have a chance to ask Zak Kohane as the editor in chief the same question‚Äîyou know, what&#8217;s your assessment as you reflect over the last two years for the submitted manuscripts? Are you overall surprised at what you&#8217;re seeing? Disappointed? Any kind of notable hits or misses, just in the steady stream of research papers that are getting submitted to that leading journal?&nbsp;



CHEATHAM: I would say overall, the field is becoming more expansive in the kinds of questions that people are asking.&nbsp;&nbsp;



Again, when we started, it was this very myopic approach of: ‚ÄúCan we pass these medical licensing exams? Can we benchmark this technology to how we benchmark our human clinicians?‚Äù I think that&#8217;s a trap. Some folks call this the Turing Trap, right, of let&#8217;s just compare everything to what a human is capable of.&nbsp;&nbsp;



Instead of interrogating what is the unique, as you all talk about in the book, what are the unique attributes of this new substrate for computation and what new behaviors emerge from it, whether that&#8217;s from a workflow perspective in the back office, or‚Äîas I&#8217;m personally more passionate and as we&#8217;re seeing more people focus on in the literature‚Äîwhat are the diagnostic capabilities, right.&nbsp;



I love Eric Topol&#8217;s framework for ‚Äúmachine eyes,‚Äù right, as this notion of like, yes, we as humans have eyes, and we have looked at medical images for many, many decades, but these machines can take a different approach to a retinal image, right.&nbsp;&nbsp;



It&#8217;s not just what you can diagnose in terms of an ophthalmological disease but maybe a neurologic disease or, you know, maybe liver disease, right.&nbsp;



So I think the literature is, in general, moving to this place of expansion, and I&#8217;m excited by that.&nbsp;



LEE: Yeah, I kind of have referred to that as triangulation. You know, one of the things I think a trap that specialists in medicine can fall into, like a cardiologist will see everything in terms of the cardiac system. And ‚Ä¶ whereas a nephrologist will see things in a certain lens.&nbsp;&nbsp;



And one of the things that you oftentimes see in the responses from a large language model is that more expansive view. At the same time, you know, I wonder ‚Ä¶ we have medical specialties for good reason. And, you know, at times I do wonder, you know, if there can be confusion that builds up.&nbsp;&nbsp;



CHEATHAM: This is an under-discussed area of AI‚ÄîAI collapses medical specialties onto themselves, right.&nbsp;



You have the canonical example of the cardiologist, you know, arguing that, you know, we should diuresis and maybe the nephrologist arguing that we should, you know, protect the kidneys. And how do two disciplines disagree on what is right for the patient when in theory, there is an objective best answer given that patient&#8217;s clinical status?&nbsp;



My understanding is that the emergence of medical specialties was a function of the cognitive overload of medicine in general and how difficult it was to keep all of the specifics of a given specialty in the mind. Of course, general practitioners are tasked with doing this at some level, but they&#8217;re also tasked with knowing when they&#8217;ve reached their limit and when they need to refer to a specialist.&nbsp;&nbsp;



So I&#8217;m interested in this question of whether medical specialties themselves need to evolve.&nbsp;&nbsp;



And if we look back in the history of medical technology, there are many times where a new technology forced a medical specialty to evolve, whether it was certain diagnostic tools that have been introduced or, as we&#8217;re seeing now with GLP-1s, the entire cardiometabolic field ‚Ä¶&nbsp;



LEE: Right.&nbsp;



CHEATHAM: ‚Ä¶ is having to really reimagine itself with these new tools. So I think AI will look very similar, and we should not hold on to this notion of classical medical specialties simply out of convention.&nbsp;&nbsp;



LEE: Right. All right. So now you&#8217;re starting your residency. You&#8217;re, you know, basically leading a charge in health and life sciences for a leading venture firm. I&#8217;d like you to predict what the world of healthcare is going to look like, you know, two years from now, five years from now, 10 years from now. And to frame that, to make it a little more specific, you know, what do you think will be possible that you, as a doctor and an investor, will be able to do two years from now, five years from now, 10 years from now that you can&#8217;t do today?&nbsp;&nbsp;



CHEATHAM: Two years from now, I&#8217;m optimistic we&#8217;ll have greater adoption of AI by clinicians, both for back-office use cases. So whether that&#8217;s the scribe and the generation of the note for billing purposes, but also now thinking about that for patient-facing applications.&nbsp;&nbsp;



We&#8217;re already doing this with drafting of notes. I think we&#8217;ll see greater proliferation of those more obvious use cases over the next two years. And hopefully we&#8217;re seeing that across hospital systems, not just large well-funded academics, but really reaching our community hospitals, our rural hospitals, our under-resourced settings.&nbsp;&nbsp;



I think hopefully we&#8217;ll see greater conversion. Right now, we have this challenge of ‚Äúpilotitis,‚Äù right. A lot of people are trying things, but the data shows that only one in three pilots are really converting to production use. So hopefully we&#8217;ll kind of move things forward that are working and pare back on those that are not.&nbsp;



We will not solve the problem of payment models in the next two years. That is a prediction I have.&nbsp;&nbsp;



Over the next five years, I suspect that, with the help of regulators, we will identify better payment mechanisms to support the adoption of AI because it cannot and will not sustain itself simply by asking health systems and hospitals to pay for it. That is not a scalable solution.&nbsp;&nbsp;



LEE: Yes. Right. Yep. In fact, I think there have to be new revenue-positive incentives if providers are asked to do more in the adoption of technology.&nbsp;



CHEATHAM: Absolutely. But as we appreciate, some of the most promising applications of AI have nothing to do with revenue. It might simply be providing a diagnosis to somebody, you know, for whom that might drive additional intervention, but may also not.&nbsp;&nbsp;



And we have to be OK with that because that&#8217;s the right thing to do. It&#8217;s our moral imperative as clinicians to implement this where it provides value to the patient.



Over the next 10 years, what I‚Äîagain, being a techno-optimist‚Äîam hopeful we start to see is a dissolving of the barrier that exists between care delivery and biomedical discovery.&nbsp;&nbsp;



This is the vision of the learning health system that was written over 10 years ago, and we have not realized it in practice. I&#8217;m a big proponent of ensuring that every single patient that enters our healthcare system not only receives the best care, but that we learn from the experiences of that individual to help the next.&nbsp;



And in our current system, that is not how it works. But, with AI, that now becomes possible.&nbsp;



LEE: Well, I think connecting healthcare experiences to medical discovery‚ÄîI think that that is really such a great vision for the future. And I do agree [that] AI really gives us real hope that we can make it true.&nbsp;&nbsp;



Morgan, I think we could talk for a few hours more. It&#8217;s just incredible what you&#8217;re up to nowadays. Thank you so much for this conversation. I&#8217;ve learned a lot talking to you.&nbsp;



CHEATHAM: Peter, thank you so much for your time. I will be clutching my signed copy of The AI Revolution in Medicine for many years to come.&nbsp;&nbsp;



[TRANSITION MUSIC]&nbsp;



LEE: Morgan obviously is not an ordinary med school graduate. In previous episodes, one of the things we&#8217;ve seen is that people on the leading edge of real-world AI in medicine oftentimes are both practicing doctors as well as technology developers. Morgan is another example of this type of polymath, being both a med student and a venture capitalist.&nbsp;



One thing that struck me about Morgan is he&#8217;s just incredibly hands-on. He goes out, finds leading-edge tools and technologies, and often these things, even though they&#8217;re experimental, he takes them into his education and into his clinical experiences. I think this highlights a potentially important point for medical schools, and that is, it might be incredibly important to provide the support‚Äîand, let&#8217;s be serious, the permission‚Äîto students to access and use new tools and technologies. Indeed, the insight for me when I interact with Morgan is that in these early days of AI in medicine, there is no substitute for hands-on experimentation, and that is likely best done while in medical school.



Here&#8217;s my interview with Daniel Chen:&nbsp;



LEE: Daniel, it&#8217;s great to have you here.&nbsp;



DANIEL CHEN: Yeah, it&#8217;s a pleasure being here.&nbsp;



LEE: Well, you know, I normally get started in these conversations by asking, you know, how do you explain to your mother what you do all day? And the reason that that&#8217;s a good question is a lot of the people we have on this podcast have fancy titles and unusual jobs, but I&#8217;m guessing that your mother would have already a preconceived notion of what a medical student does. So I&#8217;d like to twist the question around a little bit for you and ask, what does your mother not realize about how you spend your days at school?&nbsp;&nbsp;



Or does she get it all right? [LAUGHS]&nbsp;



CHEN: Oh, no, she is very observant. I&#8217;ll say that off the bat. But I think something that she might not realize, is the amount of efforts spent, kind of, outside the classroom or outside the hospital. You know, she&#8217;s always, like, saying you have such long days in the hospital. You&#8217;re there so early in the morning.&nbsp;&nbsp;



But what she doesn&#8217;t realize is that maybe when I come back from the hospital, it&#8217;s not just like, oh, I&#8217;m done for the day. Let&#8217;s wind down, go to bed. But it&#8217;s more like, OK, I have some more practice questions I need to get through; I didn&#8217;t get through my studying. Let me write on, like wrap up this research project I&#8217;m working on, get that back to the PI [principal investigator]. It&#8217;s never ending to a certain extent. Those are some things she doesn&#8217;t realize.&nbsp;



LEE: Yeah, I think, you know, all the time studying, I think, is something that people expect of second-year medical students. And even nowadays at the top medical schools like this one, being involved in research is also expected.&nbsp;&nbsp;



I think one thing that is a little unusual is that you are actually in clinic, as well, as a second-year student. How has that gone for you?&nbsp;



CHEN: Yeah, I mean, it&#8217;s definitely interesting. I would say I spend my time, especially this year, it&#8217;s kind of three things. There&#8217;s the preclinical stuff I&#8217;m doing. So that&#8217;s your classic, you know, you&#8217;re learning from the books, though I don&#8217;t feel like many of us do have textbooks anymore. [LAUGHTER]&nbsp;



There&#8217;s the clinical aspect, which you mentioned, which is we have an interesting model, longitudinal integrated clerkships. We can talk about that. And the last component is the research aspect, right. The extracurriculars.&nbsp;&nbsp;



But I think starting out as a second year and doing your rotations, probably early on in, kind of, the clinical medical education, has been really interesting, especially with our format, because typically med students have two years to read up on all the material and, like, get some foundational knowledge. With us, it&#8217;s a bit more, we have maybe one year under our belt before we&#8217;re thrown into like, OK, go talk to this patient; they have ankle pain, right. But we might have not even started talking about ankle pain in class, right. Well, where do I begin?&nbsp;&nbsp;



So I think starting out, it&#8217;s kind of, like, you know, the classic drinking from a fire hydrant. But you also, kind of, have that embarrassment of you&#8217;re talking to the patient like, I have no clue what&#8217;s happening [LAUGHTER] or you might have ‚Ä¶ my differentials all over the place, right.&nbsp;&nbsp;



But I think the beauty of the longitudinal aspect is that now that we&#8217;re, like, in our last trimester, everything&#8217;s kind of coming together. Like, OK, I can start to see, you know, here&#8217;s what you&#8217;re telling me. Here&#8217;s what the physical exam findings are. I&#8217;m starting to form a differential. Like, OK, I think these are the top three things.&nbsp;



But in addition to that, I think these are the next steps you should take so we can really focus and hone in on what exact diagnosis this might be.&nbsp;



LEE: All right. So, of course, what we&#8217;re trying to get into is about AI.&nbsp;&nbsp;



And, you know, the funny thing about AI and the book that Carey, Zak, and I wrote is we actually didn&#8217;t think too much about medical education, although we did have some parts of our book where we, well, first off, we made the guess that medical students would find AI to be useful. And we even had some examples, you know, where, you know, you would have a vignette of a mythical patient, and you would ask the AI to pretend to be that patient.&nbsp;&nbsp;



And then you would have an interaction and have to have an encounter. And so I want to delve into whether any of that is happening. How real it is. But before we do that, let&#8217;s get into first off, your own personal contact with AI. So let me start with a very simple question. Do you ever use generative AI systems like ChatGPT or similar?&nbsp;



CHEN: All the time, if not every day.&nbsp;



LEE: [LAUGHS] Every day, OK. And when did that start?&nbsp;



CHEN: I think when it first launched with GPT-3.5, I was, you know, curious. All my friends work in tech. You know, they&#8217;re either software engineers, PMs. They&#8217;re like, ‚ÄúHey, Daniel, take a look at this,‚Äù and at first, I thought it was just more of a glorified search engine. You know, I was actually looking back.&nbsp;&nbsp;



My first question to ChatGPT was, what was the weather going to be like the next week, you know? Something very, like, something easily you could have looked up on Google or your phone app, right.&nbsp;&nbsp;



I was like, oh, this is pretty cool. But then, kind of, fast-forwarding to, I think, the first instance I was using it in med school. I think the first, like, thing that really helped me was actually a coding problem. It was for a research project. I was trying to use SQL.&nbsp;&nbsp;



Obviously, I&#8217;ve never taken a SQL class in my life. So I asked Chat like, ‚ÄúHey, can you write me this code to maybe morph two columns together,‚Äù right? Something that might have taken me hours to maybe Google on YouTube or like try to read some documentation which just goes through my head.



But ChatGPT was able to, you know, not only produce the code, but, like, walk me through like, OK, you&#8217;re going to launch SQL. You&#8217;re going to click on this menu, [LAUGHTER] put the code in here, make sure your file names are correct. And it worked.&nbsp;&nbsp;



So it&#8217;s been a very powerful tool in that way in terms of, like, giving me expertise in something that maybe I traditionally had no training in.&nbsp;



LEE: And so while you&#8217;re doing this, I assume you had fellow students, friends, and others. And so what were you observing about their contact with AI? I assume you weren&#8217;t alone in this.&nbsp;



CHEN: Yeah, yeah, I think, &#8230; I&#8217;m not too sure in terms of what they were doing when it first came out, but I think if we were talking about present day, um, a lot of it&#8217;s kind of really spot on to what you guys talked about in the book.&nbsp;&nbsp;



Um, I think the idea around this personal tutor, personal mentor, is something that we&#8217;re seeing a lot. Even if we&#8217;re having in-class discussions, the lecturer might be saying something, right. And then I might be or I see a friend in ChatGPT or some other model looking up a question.&nbsp;&nbsp;



And you guys talked about, you know, how it can, like, explain a concept at different levels, right. But honestly, sometimes if there&#8217;s a complex topic, I ask ChatGPT, like, can you explain this to me as if I was a 6-year-old?&nbsp;&nbsp;



LEE: Yeah. [LAUGHS]&nbsp;&nbsp;



CHEN: Breaking down complex topics. Yeah. So I think it&#8217;s something that we see in the pre-clinical space, in lecture, but also even in the clinical space, there&#8217;s a lot of teaching, as well.&nbsp;



Sometimes if my preceptor is busy with patients, but I had maybe a question, I would maybe converse with ChatGPT, like, ‚ÄúHey, what are your thoughts about this?‚Äù Or, like, a common one is, like, medical doctors love to use abbreviations, ‚Ä¶&nbsp;



LEE: Yes.&nbsp;&nbsp;



CHEN: ‚Ä¶ and these abbreviations are sometimes only very niche and unique to their specialty, right. [LAUGHTER]&nbsp;



And I was reading this note from a urogynecologist. [In] the entire first sentence, I think there were, like, 10 abbreviations. Obviously, I compile lists and ask ChatGPT, like, ‚ÄúHey, in the context of urogynecology, can you define what these could possibly mean,‚Äù right? Instead of hopelessly searching in a Google or maybe, embarrassing, asking the preceptor. So in these instances, it&#8217;s played a huge role.&nbsp;



LEE: Yeah. And when you&#8217;re doing things like that, it can make mistakes. And so what are your views of the reliability of generative AI, at least in the form of ChatGPT?&nbsp;



CHEN: Yeah, I think into the context of medicine, right, we fear a lot about the hallucinations that these models might have. And it&#8217;s something I&#8217;m always checking for. When I talk with peers about this, we find it most helpful when the model gives us a source linking it back. I think the gold standard nowadays in medicine is using something called UpToDate (opens in new tab) that&#8217;s written by clinicians, for clinicians.&nbsp;



But sometimes searching on UpToDate can be a lot of time as well because it&#8217;s a lot of information to, like, sort through. But nowadays a lot of us are using something called OpenEvidence, which is also an LLM. But they always cite their citations with, like, published literature, right.&nbsp;&nbsp;



So I think being able to be conscious of the downfalls of these models and also being able to have the critical understanding of, like, analyzing the actual literature. I think double checking is just something that we&#8217;ve been also getting really good at.&nbsp;



LEE: How would you assess student attitudes‚Äîmed student attitudes‚Äîabout AI? Is it ‚Ä¶ the way you&#8217;re coming across is it&#8217;s just a natural part of life. But do people have firm opinions, you know, pro or con, when it comes to AI, and especially AI in medicine?&nbsp;



CHEN: I think it&#8217;s pretty split right now. I think there&#8217;s the half, kind of, like us, where we&#8217;re very optimistic‚Äîcautiously optimistic about, you know, the potential of this, right. It&#8217;s able to, you know, give us that extra information, of being that extra tutor, right. It&#8217;s also able to give us information very quickly, as well.&nbsp;&nbsp;&nbsp;



But I think the other flip side of what a lot of students hesitate to, which I agree, is this loss of the ability to critically think. Something that you can easily do is, you know, give these models, like, relevant information about the patient history and be like, ‚ÄúGive me a 10-list differential,‚Äù right.



LEE: Yes.&nbsp;&nbsp;



CHEN: And I think it&#8217;s very easy as a student to, you know, [say], ‚ÄúThis is difficult. Let me just use what the model says, and we&#8217;ll go with that,‚Äù right.&nbsp;



So I think being able to separate that, you know, medical school is a time where, you know, you&#8217;re learning to become a good doctor. And part of that requires the ability to be observant and critically think. Having these models simultaneously might hinder the ability to do that.&nbsp;&nbsp;



LEE: Yeah.&nbsp;



CHEN: So I think, you know, the next step is, like, these models can be great‚Äîa great tool, absolutely wonderful. But how do you make sure that it&#8217;s not hindering these abilities to critically think?&nbsp;



LEE: Right. And so when you&#8217;re doing your LIC [longitudinal integrated clerkship] work, these longitudinal experiences, and you&#8217;re in clinic, are you pulling the phone out of your pocket and consulting with AI?&nbsp;



CHEN: Definitely. And I think my own policy for this, to kind of counter this, is that the night before when I&#8217;m looking over the patient list, the clinic [schedule] of who&#8217;s coming, I&#8217;m always giving it my best effort first.&nbsp;&nbsp;



Like, OK, the chief complaint is maybe just a runny nose for a kid in a pediatric clinic. What could this possibly be? Right? At this point, we&#8217;ve seen a lot. Like, OK, it could be URI [upper respiratory infection], it could be viral, it could be bacterial, you know, and then I go through the‚Äîyou know, I try to do my due diligence of, like, going through the history and everything like that, right.&nbsp;



But sometimes if it&#8217;s a more complex case, something maybe a presentation I&#8217;ve never seen before, I&#8217;ll still kind of do my best coming up with maybe a differential that might not be amazing. But then I&#8217;ll ask, you know, ChatGPT like, OK, in addition to these ideas, what do you think?&nbsp;&nbsp;



LEE: Yeah.&nbsp;



CHEN: Am I missing something? You know, and usually, it gives a pretty good response.&nbsp;



LEE: You know, that particular idea is something that I think Carey, Zak, and I thought would be happening a lot more today than we&#8217;re observing. And it&#8217;s the idea of a second set of eyes on your work. And somehow, at least our observation is that that isn&#8217;t happening quite as much by today as we thought it might.&nbsp;&nbsp;



And it just seems like one of the really safest and most effective use cases. When you go and you&#8217;re looking at yourself and other fellow medical students, other second-year students, what do you see when it comes to the ‚Äúsecond set of eyes‚Äù idea?&nbsp;



CHEN: I think, like, a lot of students are definitely consulting ChatGPT in that regard because, you know, even in the very beginning, we&#8217;re taught to be, like, never miss these red flags, right. So these red flags are always on our differential, but sometimes, it can be difficult to figure out where to place them on that, right.&nbsp;&nbsp;



So I think in addition to, you know, coming up with these differentials, something I&#8217;ve been finding a lot of value [in] is just chatting with these tools to get their rationale behind their thinking, you know.&nbsp;&nbsp;



Something I find really helpful‚ÄîI think this is also a part of the, kind of, art of medicine‚Äîis figuring out what to order, right, what labs to order.&nbsp;&nbsp;



LEE: Right.



CHEN: Obviously, you have your order sets that automate some of the things, like in the ED [emergency department], or, like, there are some gold standard imaging things you should do for certain presentations.&nbsp;



But then you chat to, like, 10 different physicians on maybe the next steps after that, and they give you 10 different answers.&nbsp;&nbsp;



LEE: Yes.&nbsp;&nbsp;



CHEN: But there&#8217;s never &#8230; I never understand exactly why. It&#8217;s always like, I&#8217;ve just been doing this for all my training, or that&#8217;s how I was taught.&nbsp;&nbsp;



So asking ChatGPT, like, ‚ÄúWhy would you do this next?‚Äù Or, like, ‚ÄúIs this a good idea?‚Äù And seeing the pros and cons has also been really helpful in my learning.&nbsp;



LEE: Yeah, wow, that&#8217;s super interesting. So now, you know, I&#8217;d like to get into the education you&#8217;re receiving. And, you know, I think it&#8217;s fair to say Kaiser Permanente is very progressive in really trying to be very cutting-edge in how the whole curriculum is set up.&nbsp;&nbsp;



And for the listeners who don&#8217;t know this, I&#8217;m actually on the board of directors of the school and have been since the founding of the school. And I think one of the reasons why I was invited to be on the board is the school really wanted to think ahead and be cutting edge when it comes to technology.&nbsp;&nbsp;



So from where I&#8217;ve sat, I&#8217;ve never been completely satisfied with the amount of tech that has made it into the curriculum. But at the same time, I&#8217;ve also made myself feel better about that just understanding that it&#8217;s sort of unstoppable, that students are so tech-forward already.&nbsp;&nbsp;



But I wanted to delve into a little bit here into what your honest opinions are and your fellow students&#8217; opinions are about whether you feel like you&#8217;re getting adequate training and background formally as part of your medical education when it comes to things like artificial intelligence or other technologies.&nbsp;&nbsp;



What do you think? Are you ‚Ä¶ would you wish the curriculum would change?&nbsp;



CHEN: Yeah, I think that&#8217;s a great question.&nbsp;&nbsp;



I think from a tech perspective, the school is very good about implementing, you know, opportunities for us to learn. Like, for example, learning how to use Epic, right, or at Kaiser Permanente, what we call HealthConnect, right. These electronic health records. That, my understanding is, a lot of schools maybe don&#8217;t teach that.&nbsp;&nbsp;



That&#8217;s something where we get training sessions maybe once or twice a year, like, ‚ÄúHey, here&#8217;s how to make a shortcut in the environment,‚Äù right.&nbsp;&nbsp;



So I think from that perspective, the school is really proactive in providing those opportunities, and they make it very easy to find resources for that, too. I think it &#8230;&nbsp;



LEE: Yeah, I think you&#8217;re pretty much guaranteed to be an Epic black belt by the time you [LAUGHS] finish your degree.&nbsp;&nbsp;



CHEN: Yes, yes.&nbsp;&nbsp;



But then I think in terms of the aspects of artificial intelligence, I think the school&#8217;s taken a more cautiously optimistic viewpoint. They&#8217;re just kind of looking around right now.&nbsp;&nbsp;



Formally in the curriculum, there hasn&#8217;t been anything around this topic. I believe the fourth-year students last year got a student-led lecture around this topic.&nbsp;&nbsp;



But talking to other peers at other institutions, it looks like it&#8217;s something that&#8217;s very slowly being built into the curriculum, and it seems like a lot of it is actually student-led, you know.&nbsp;&nbsp;



You know, my friend at Feinberg [School of Medicine] was like we just got a session before clerkship about best practices on how to use these tools.&nbsp;&nbsp;



I have another friend at Pitt talking about how they‚Äôre leading efforts of maybe incorporating some sort of LLM into their in-house curriculum where students can, instead of clicking around the website trying to find the exact slide, they can just ask this tool, like, ‚ÄúOK. We had class this day. They talked about this &#8230; but can you provide more information?‚Äù and it can pull from that.&nbsp;&nbsp;



So I think a lot of this, a lot of it is student-driven. Which I think is really exciting because it begs the question, I think, you know current physicians may not be very well equipped with these tools as well, right?&nbsp;&nbsp;



So maybe they don&#8217;t have a good idea of what exactly is the next steps or what does the curriculum look like. So I think the future in terms of this AI curriculum is really student-led, as well.&nbsp;



LEE: Yeah, yeah, it&#8217;s really interesting.&nbsp;&nbsp;



I think one of the reasons I think also that that happens is [that] it&#8217;s not just necessarily the curriculum that lags but the accreditation standards. You know, accreditation is really important for medical schools because you want to make sure that anyone who holds an MD, you know, is a bona fide doctor, and so accreditation standards are pretty strictly monitored in most countries, including the United States.&nbsp;&nbsp;



And I think accreditation standards are also‚Äîmy observation‚Äîare slow to understand how to adopt or integrate AI. And it&#8217;s not meant as a criticism. It&#8217;s a big unknown. No one knows exactly what to do and how to do. And so it&#8217;s really interesting to see that, as far as I can tell, I&#8217;ve observed the same thing that you just have seen, that most of the innovation in this area about how AI should be integrated into medical education is coming from the students themselves.&nbsp;&nbsp;



It seems, I think, I&#8217;d like to think it&#8217;s a healthy development. [LAUGHS]



CHEN: Something tells me maybe the students are a bit better at using these tools, as well.&nbsp;&nbsp;



You know, I talk to my preceptors because KP [Kaiser Permanente] also has their own version ‚Ä¶&nbsp;



LEE: Preceptor, maybe we should explain what that is.&nbsp;



CHEN: Yeah, sorry. So a preceptor is an attending physician, fully licensed, finished residency, and they are essentially your kind of teacher in the clinical environment.&nbsp;&nbsp;



So KP has their own version of some ambient documentation device, as well. And something I always like to ask, you know, like, ‚ÄúHey, what are your thoughts on these tools,‚Äù right?&nbsp;&nbsp;



And it&#8217;s always so polarizing, as well, even among the same specialty. Like, if you ask psychiatrists, which I think is a great use case of these tools, right. My preceptor hates it. Another preceptor next door loves it. [LAUGHTER]&nbsp;



So I think a lot of it‚Äôs, like, it‚Äôs still, like, a lot of unknowns, like you were mentioning.&nbsp;



LEE: Right. Well, in fact, I&#8217;m glad you brought that up because one thing that we&#8217;ve been hearing from previous guests a lot when it comes to AI in clinic is about ambient listening by AI, for example, to help set up a clinical note or even write a clinical note.&nbsp;&nbsp;



And another big use case that we heard a lot about that seems to be pretty popular is the use of generative AI to respond to patient messages.&nbsp;&nbsp;



So let&#8217;s start with the clinical note thing. First off, do you have opinions about that technology?&nbsp;



CHEN: I think it&#8217;s definitely good.&nbsp;&nbsp;



I think especially where, you know, if you&#8217;re in the family medicine environment or pediatric environment where you&#8217;re spending so much time with patients, a note like that is great, right.&nbsp;



I think coming from a strictly medical student standpoint, I think it&#8217;s‚Äîhonestly, it&#8217;d be great to have‚Äîbut I think there&#8217;s a lot of learning when you write the note, you know. There&#8217;s a lot of, you know, all of my preceptors talk about, like, when I read your note, you should present it in a way where I can see your thoughts and then once I get to the assessment and plan, it&#8217;s kind of funneling down towards a single diagnosis or a handful of diagnoses. And that&#8217;s, I think, a skill that requires you to practice over time, right.&nbsp;&nbsp;



So a part of me thinks, like, if I had this tool where [it] can just automatically give me a note as a first year, then it takes away from that learning experience, you know.&nbsp;



Even during our first year throughout school, we frequently get feedback from professors and doctors about these notes. And it&#8217;s a lot of feedback. [LAUGHTER] It&#8217;s like, ‚ÄúI don&#8217;t think you should have written that,‚Äù ‚ÄúThat should be in this section ‚Äù ‚Ä¶ you know, like a medical note or a SOAP note [Subjective, Objective, Assessment, and Plan], where, you know, the subjective is, like, what the patient tells you. Objective is what the physical findings are, and then your assessment of what&#8217;s happening, and then your plan. Like, it&#8217;s very particular, and then I think medicine is so structured in a way, that&#8217;s kind of, like, how everyone does it, right. So kind of going back to the question, I think it&#8217;s a great tool, but I don&#8217;t think it&#8217;s appropriate for a medical student.&nbsp;



LEE: Yeah, it&#8217;s so interesting to hear you say that. I was ‚Ä¶ one of our previous guests is the head of R&amp;D at Epic, Seth Hain. He said, ‚ÄúYou know, Peter, doctors do a lot of their thinking when they write the note.‚Äù&nbsp;



And, of course, Epic is providing ambient, you know, clinical notetaking automation. But he was urging caution because, you know, you&#8217;re saying, well, this is where you&#8217;re learning a lot. But actually, it&#8217;s also a point where, as a doctor, you&#8217;re thinking about the patient. And we do probably have to be careful with how we automate parts of that.&nbsp;&nbsp;



All right. So you&#8217;re gearing up for Step 1 of the USMLE [United States Medical Licensing Examination]. That&#8217;ll be a big multiple-choice exam. Then Step 2 is similar: very, very focused on advanced clinical knowledge. And then Step 3, you know, is a little more interactive.&nbsp;&nbsp;



And so one question that people have had about AI is, you know, how do we regulate the use of AI in medicine? And one of the famous papers that came out of both academia and industry was the concept that you might be able to treat AI like a person and have it go through the same licensing. And this is something that Carey, Zak, and I contemplated in our book.&nbsp;&nbsp;



In the end, at the time we wrote the book, I personally rejected the idea, but I think it&#8217;s still alive. And so I‚Äôve wondered if you have any &#8230; you know, first off, are you opinionated at all about, what should the requirements be for the allowable use of AI in the kind of work that you&#8217;re going to be doing?&nbsp;



CHEN: Yeah, I think it&#8217;s a tough question because, like, where do you draw that line, right? If you apply the human standards of it&#8217;s passing exams, then yes, in theory, it could be maybe a medical doctor, as well, right? It&#8217;s more empathetic than medical doctors, right? So where do you draw that line?&nbsp;&nbsp;



I think, you know, part of me thinks it&#8217;s maybe it is that human aspect that patients like to connect with, right. And maybe this really is just, like, these tools are just aids in helping, you know, maybe load off some cognitive load, right.&nbsp;&nbsp;



But I think the other part of me, I&#8217;m thinking about this is the next generation who are growing up with this technology, right. They&#8217;re interacting with applications all day. Maybe they&#8217;re on their iPads. They&#8217;re talking to chatbots. They&#8217;re using ChatGPT. This is, kind of, the environment they grew up with. Does that mean they also have increased, like, trust in these tools that maybe our generation or the generations above us don&#8217;t have that value that human connection? Would they value human connection less?&nbsp;&nbsp;



You know, I think those are some troubling thoughts that, you know, yes, at end of the day, maybe I&#8217;m not as smart as these tools, but I can still provide that human comfort. But if, at the end of the day, the future generation doesn&#8217;t really care about that or they perfectly trust these tools because that&#8217;s all they&#8217;ve kind of known, then where do human doctors stand?&nbsp;&nbsp;



I think part of that is, there would be certain specialties where maybe the human connection is more important. The longitudinal aspect of building that trust, I think is important. Family medicine is a great example. I think hematology oncology with cancer treatment.&nbsp;&nbsp;



Obviously, I think anyone&#8217;s not going to be thrilled to hear cancer diagnosis, but something tells me that seeing that on a screen versus maybe a physician prompting you and telling you about that tells me that maybe in those aspects, you know, the human nature, the human touch plays an important role there, too.&nbsp;



LEE: Yeah, you know, I think it strikes me that it&#8217;s going to be your generation that really is going to set the pattern probably for the next 50 years about how this goes. And it&#8217;s just so interesting because I think a lot will depend on your reactions to things.&nbsp;&nbsp;



So, for example, you know, one thing that is already starting to happen are patients who are coming in armed, you know, with a differential [LAUGHS], you know, that they&#8217;ve developed themselves with the help of ChatGPT. So let me &#8230; you must have thought about these things. So, in fact, has it happened in your clinical work already?&nbsp;



CHEN: Yeah, I&#8217;ve seen people come into the ED during my ED shift, like emergency department, and they‚Äôll be like, ‚ÄúOh, I have neck pain and here are all the things that, you know, Chat told me, ChatGPT told me. What do you think &#8230; do I need? I want this lab ordered, that lab ordered.‚Äù&nbsp;&nbsp;



LEE: Right.&nbsp;



CHEN: And I think my initial reaction is, ‚ÄúGreat. Maybe we should do that.‚Äù But I think the other reaction is understanding that not everyone has the clinical background of understanding what&#8217;s most important, what do we need to absolutely rule out, right?&nbsp;&nbsp;&nbsp;



So, I think in some regards, I would think that maybe ChatGPT errs on the side of caution, &#8230;&nbsp;



LEE: Yes.&nbsp;&nbsp;



CHEN: &#8230; giving maybe patients more extreme examples of what this could be just to make sure that it&#8217;s, in a way, is not missing any red flags as well, right.&nbsp;&nbsp;



LEE: Right. Yeah.&nbsp;&nbsp;



CHEN: But I think a lot of this is ‚Ä¶ what we&#8217;ve been learning is it&#8217;s all about shared decision making with the patient, right. Being able to acknowledge like, ‚ÄúYeah, [in] that list, most of the stuff is very plausible, but maybe you didn&#8217;t think about this one symptom you have.‚Äù&nbsp;&nbsp;



So I think part of it, maybe it&#8217;s a sidebar here, is the idea of prompting, right. You know, they&#8217;ve always talked about all these, you know, prompt engineers, you know, how well can you, like, give it context to answer your question?&nbsp;



LEE: Yeah.&nbsp;



CHEN: So I think being able to give these models the correct information and the relevant information and keyword relevant, because relevant is, I guess, where your clinical expertise comes in. Like, what do you give the model, what do you not give? So I think that difference between a medical provider versus maybe your patients is ultimately the difference.&nbsp;



LEE: Let me press on that a little bit more because you brought up the issue of trust, and trust is so essential for patients to feel good about their medical care.&nbsp;&nbsp;



And I can imagine you&#8217;re a medical student seeing a patient for the first time. So you don&#8217;t have a trust relationship with that patient. And the patient comes in maybe trusting ChatGPT more than you.&nbsp;



CHEN: Very valid. No. I mean, I get that a lot, surprisingly, you know. [LAUGHTER] Sometimes [they‚Äôre] like, ‚ÄúOh, I don&#8217;t want to see the medical student,‚Äù because we always give the patient an option, right. Like, it&#8217;s their time, whether it&#8217;s a clinic visit.&nbsp;&nbsp;



But yeah, those patients, I think it&#8217;s perfectly reasonable. If I heard a second-year medical student was going to be part of my care team, taking that history, I&#8217;d be maybe a little bit concerned, too. Like, are they asking all the right questions? Are they relaying that information back to their attending physician correctly?&nbsp;&nbsp;



So I think a lot of it is, at least from a medical student perspective, is framing it so the patient understands that this is a learning opportunity for the students. And something I do a lot is tell them like, ‚ÄúHey, like, you know, at the end of the day, there is someone double-checking all my work.‚Äù&nbsp;&nbsp;



LEE: Yeah.&nbsp;



CHEN: But for those that come in with a list, I sometimes sit down with them, and we&#8217;ll have a discussion, honestly.&nbsp;&nbsp;



I‚Äôll be like, ‚ÄúI don&#8217;t think you have meningitis because you&#8217;re not having a fever. Some of the physical exam maneuvers we did were also negative. So I don&#8217;t think you have anything to worry about that,‚Äù you know.&nbsp;&nbsp;



So I think it&#8217;s having that very candid conversation with the patient that helps build that initial trust. Telling them like, ‚ÄúHey ‚Ä¶ ‚Äù&nbsp;



LEE: It&#8217;s impressive to hear how even keeled you are about this. You know, I think, of course, and you&#8217;re being very humble saying, well, you know, as a second-year medical student, of course, someone might not, you know, have complete trust. But I think that we will be entering into a world where no doctor is going to be, no matter how experienced or how skilled, is going to be immune from this issue.&nbsp;



So we&#8217;re starting to run toward the end of our time together. And I like to end with one or two more provocative questions.&nbsp;&nbsp;



And so let me start with this one. Undoubtedly, I mean, you&#8217;re close enough to tech and digital stuff, digital health, that you&#8217;re undoubtedly familiar with famous predictions, you know, by Turing and Nobel laureates that someday certain medical specialties, most notably radiology, would be completely supplanted by machines. And more recently, there have been predictions by others, like, you know, Elon Musk, that maybe even some types of surgery would be replaced by machines.&nbsp;&nbsp;



What do you think? Do you have an opinion?&nbsp;



CHEN: I think replace is a strong term, right. To say that doctors are completely obsolete, I think, is unlikely.&nbsp;&nbsp;



If anything, I think there might be a shift maybe in what it means to be a doctor, right. Undoubtedly, maybe the demands of radiologists are going to go down because maybe more of the simple things can truly be automated, right. And you just have a supervising radiologist whose output is maybe 10 times as maybe 10 single radiologists, right.&nbsp;&nbsp;



So I definitely see a future where the demand of certain specialties might go down.&nbsp;&nbsp;



And I think when I talk about a shift of what it means to be a physician, maybe it&#8217;s not so much diagnostic anymore, right, if these models get so good at, like, just taking in large amounts of information, but maybe it pivots to being really good at understanding the limitations of these models and knowing when to intervene is what it means to be the kind of the next generation of physicians.&nbsp;&nbsp;



I think in terms of surgery, yeah, I think it&#8217;s a concern, but maybe not in the next 50 years. Like those Da Vinci robots are great. I think out of Mayo Clinic, they were demoing some videos of these robots leveraging computer vision to, like, close portholes, like laparoscopic scars. And that&#8217;s something I do in the OR [operating room], right. And we&#8217;re at the same level at this point. [LAUGHTER] So at that point, maybe.&nbsp;&nbsp;



But I think robotics still has to address the understanding of like, what if something goes wrong, right? Who&#8217;s responsible? And I don&#8217;t see a future where a robot is able to react to these, you know, dangerous situations when maybe something goes wrong. You still have to have a surgeon on board to, kind of, take over. So in that regard, that&#8217;s kind of where I see maybe the future going.&nbsp;



LEE: So last question. You know, when you are thinking about the division of time, one of the themes that we&#8217;ve seen in the previous guests is more and more doctors are doing more technology work, like writing code and so on. And more and more technologists are thinking deeply and getting educated in clinical and preclinical work.&nbsp;&nbsp;



So for you, let&#8217;s look ahead 10 years. What do you see your division of labor to be? Or, you know, how would you ‚Ä¶ what would you tell your mom then about how you spend a typical day?&nbsp;



CHEN: Yeah, I mean, I think for me, technology is something I definitely want to be involved in in my line of work, whether it&#8217;s, you know, AI work, whether it&#8217;s improving quality of healthcare through technology.&nbsp;&nbsp;



My perfect division would be maybe still being able to see patients but also balancing some maybe more of these higher-level kind of larger projects. But I think having that division would be something nice.&nbsp;



LEE: Yeah, well, I think you would be great just from the little bit I know about you. And, Daniel, it&#8217;s been really great chatting with you. I wish you the best of luck, you know, with your upcoming exams and getting past this year two of your medical studies. And perhaps someday I&#8217;ll be your patient.&nbsp;



[TRANSITION MUSIC]‚ÄØ&nbsp;



CHEN: Thank you so much.&nbsp;



LEE: You know, one of the lucky things about my job is that I pretty regularly get to talk to students at all levels, spanning high school to graduate school. And when I get to talk especially to med students, I&#8217;m always impressed with their intelligence, just how serious they are, and their high energy levels. Daniel is absolutely a perfect example of all that.&nbsp;&nbsp;



Now, it comes across as trite to say that the older generation is less adept at technology adoption than younger people. But actually, there probably is a lot of truth to that. And in the conversation with Daniel, I think he was actually being pretty diplomatic but also clear that he and his fellow med students don&#8217;t necessarily expect the professors in their med school to understand AI as well as they do.&nbsp;



There&#8217;s no doubt in my mind that medical education will have to evolve a lot to help prepare doctors and nurses for an AI future. But where will this evolution come from?&nbsp;&nbsp;



As I reflect on my conversations with Morgan and Daniel, I start to think that it&#8217;s most likely to come from the students themselves. And when you meet people like Morgan and Daniel, it&#8217;s impossible to not be incredibly optimistic about the next generation of clinicians.&nbsp;



[THEME MUSIC]&nbsp;



Another big thank-you to Morgan and Daniel for taking time to share their experiences with us. And to our listeners, thank you for joining us. We have just a couple of episodes left, one on AI‚Äôs impact on the operation of public health departments and healthcare systems and another coauthor roundtable. We hope you&#8217;ll continue to tune in.&nbsp;&nbsp;



Until next time.&nbsp;



[MUSIC FADES]&nbsp;

				
			
			
				Show more			
		
	





AI Revolution in Medicine podcast series

Opens in a new tabThe post Navigating medical education in the era of generative AI appeared first on Microsoft Research.
‚Ä¢ Xinxing Xu bridges AI research and real-world impact at Microsoft Research Asia ‚Äì Singapore
  AI has made remarkable progress in recent years, but turning experimental models into tools that work in the real world is still a major challenge. Bridging this gap between innovation and application has shaped the career of Xinxing Xu, principal researcher at Microsoft Research Asia ‚Äì Singapore, and underpins the mission of the lab‚Äôs newly established presence in the region.



Xinxing Xu, Principal Researcher, Microsoft Research Asia ‚Äì Singapore



‚ÄúInnovative algorithms can only demonstrate their true value when tested with real-world data and in actual scenarios, where they can be continuously optimized through iteration,‚Äù he says.



Xu‚Äôs commitment to balancing algorithmic innovation with practical application has shaped his entire career. During his PhD studies at Nanyang Technological University, Singapore, Xu focused on emerging technologies like multiple kernel learning methods and multimodal machine learning. Today he‚Äôs applying these techniques to real-world use cases like image recognition and video classification.



After completing his doctorate, he joined the Institute of High Performance Computing at Singapore‚Äôs Agency for Science, Technology and Research (A*STAR), where he worked on interdisciplinary projects ranging from medical image recognition to AI systems for detecting defects on facade of buildings. These experiences broadened his perspective and deepened his passion for translating AI into real-world impact.



In 2024, Xu joined Microsoft Research Asia where he began a new chapter focused on bridging between academic research and real-world AI applications.



‚ÄúMicrosoft Research Asia is committed to integrating scientific exploration with real-world applications, which creates a unique research environment,‚Äù Xu says. ‚ÄúIt brings together top talent and resources, and Microsoft&#8217;s engineering and product ecosystem strongly supports turning research into impactful technology. The lab‚Äôs open and inclusive culture encourages innovation with broader societal impact. It reflects the approach to research I‚Äôve always hoped to contribute to.‚Äù



	
		

		
		Spotlight: AI-POWERED EXPERIENCE
	
	
	
						
				
					
				
			
			
			

									Microsoft research copilot experience
				
								Discover more about research at Microsoft through our AI-powered experience
				
								
					
						
							Start now						
					
				
							
	
Opens in a new tab	
	


Bringing cross-domain expertise to AI‚Äôs real-world frontiers



As a key hub in Microsoft Research‚Äôs network across Asia, the Singapore lab is guided by a three-part mission: to drive industry-transforming AI deployment, pursue fundamental breakthroughs in the field, and promote responsible, socially beneficial applications of the technology.



To reach these goals, Xu and his colleagues are working closely with local collaborators, combining cross-disciplinary expertise to tackle complex, real-world challenges.



To deliver on that mission, Xinxing Xu and his colleagues are working closely with local collaborators, drawing on cross-disciplinary expertise to solve real-world problems. One key focus is healthcare, where Xu leads a collaboration with Singapore‚Äôs SingHealth to explore how AI can support precision medicine. By combining SingHealth‚Äôs clinical data with advanced AI models, the team aims to deliver more personalized analyses and sharper diagnostic tools‚Äîlaying the groundwork for improved patient outcomes.&nbsp;



Beyond healthcare, the team is also targeting key sectors like finance and logistics. By developing domain-specific foundation models and AI agents, they aim to support smarter decision-making and accelerate digital transformation across industries. ‚ÄúSingapore has a strong foundation in these sectors,‚Äù Xu notes, ‚Äúmaking it an ideal environment for technology validation and iteration.‚Äù



The team is also partnering with leading academic institutions, including the National University of Singapore (NUS) and Nanyang Technological University, Singapore (NTU Singapore), to advance the field of spatial intelligence. Their goal is to develop embodied intelligence systems capable of carrying out complex tasks in smart environments.



As AI becomes more deeply embedded in everyday life, researchers at the Singapore lab are also increasingly focused on what they call ‚Äúsocietal AI‚Äù‚Äîbuilding AI systems that are culturally relevant and trustworthy within Southeast Asia‚Äôs unique cultural and social contexts. In collaboration with global colleagues, they‚Äôre helping to advance a more culturally grounded and responsible approach to AI research in the region.



Microsoft Research Asia ‚Äì Singapore: Expanding global reach, connecting regional innovation&nbsp;



Realizing AI‚Äôs full potential requires more than technical breakthroughs. It also depends on collaboration‚Äîacross industries, academia, and policy. Only through this intersection of forces can AI move beyond the lab to deliver meaningful societal value.&nbsp;



Singapore‚Äôs strengths in science, engineering, and digital governance make it an ideal setting for this kind of work. Its collaborative culture, robust infrastructure, international talent pool, and strong policy support for science and technology make it fertile ground for interdisciplinary research.&nbsp;



This is why Microsoft Research Asia continues to collaborate closely with Singapore‚Äôs top universities, research institutions, and industry partners. These partnerships support joint research, talent development, and technical exchange. Building on this foundation, Microsoft Research Asia ‚Äì Singapore will further deepen its collaboration with NUS, NTU Singapore, and Singapore Management University (SMU) to advance both fundamental and applied research, while equipping the next generation of researchers with real-world experience. In addition, Microsoft Research Asia is fostering academic exchange and strengthening the research ecosystem through summer schools and joint workshops with NUS, NTU Singapore, and SMU.&nbsp;



The launch of the Singapore lab further marks an important step in expanding the company‚Äôs global research footprint, serving as a bridge between regional innovation and Microsoft‚Äôs global ecosystem. Through its integrated lab network, Microsoft Research fosters the sharing of technologies, methods, and real-world insights, creating a virtuous cycle of innovation.



‚ÄúWe aim to build a research hub in Singapore that is globally connected and deeply rooted in the local ecosystem,‚Äù Xu says. ‚ÄúMany breakthroughs come from interdisciplinary and cross-regional collaboration. By breaking boundaries‚Äîacross disciplines, industries, and geographies‚Äîwe can drive research that has lasting impact.‚Äù



As AI becomes more deeply woven into industry and everyday life, Xu believes that meaningful research must be closely connected to regional development and social well-being. ‚ÄúMicrosoft Research Asia ‚Äì Singapore is a future-facing lab,‚Äù he says. ‚ÄúWhile we push technological frontiers, we‚Äôre equally committed to the responsibility of technology‚Äîensuring AI can help address society‚Äôs most pressing challenges.‚Äù



In a world shaped by global challenges, Xu sees collaboration and innovation as essential to real progress. With Singapore as a launchpad, he and his team are working to extend AI‚Äôs impact and value across Southeast Asia and beyond.



Xingxing Xu (center) with colleagues at Microsoft Research Asia &#8211; Singapore&nbsp;



Three essential strengths for the next generation of AI researchers



AI‚Äôs progress depends not only on technical breakthroughs but also on the growth and dedication of talent. At Microsoft Research Asia, there is a strong belief that bringing research into the real world requires more than technical coordination‚Äîit depends on unlocking the full creativity and potential of researchers.



In Singapore‚Äîa regional innovation hub that connects Southeast Asia‚ÄîXu and his colleagues are working to push AI beyond the lab and into fields like healthcare, finance, and manufacturing. For young researchers hoping to shape the future of AI, this is a uniquely powerful stage.



To help guide the next generation, Xu shares three pieces of advice:




Build a strong foundation ‚Äì ‚ÄúCore knowledge in machine learning, linear algebra, and probability and statistics is the bedrock of AI research,‚Äù Xu says. ‚ÄúA solid theoretical base is essential to remain competitive in a rapidly evolving field. Even today‚Äôs hottest trends in generative AI rely on longstanding principles of optimization and model architecture design.‚Äù While code generation tools are on the rise, Xu emphasizes that mathematical fundamentals remain essential for understanding and innovating in AI.



Understand real-world applications ‚Äì Technical skills alone aren‚Äôt enough. Xu encourages young researchers to deeply engage with the problems they‚Äôre trying to solve. Only by tightly integrating technology with its context can researchers create truly valuable solutions.‚ÄúIn healthcare, for example, researchers may need to follow doctors in clinics to gain a true understanding of clinical workflows. That context helps identify the best entry points for AI deployment. Framing research problems around real-world needs is often more impactful than just tuning model parameters,‚Äù Xu says.



Develop interdisciplinary thinking ‚Äì Cross-disciplinary collaboration is becoming essential to AI innovation. Xu advises young researchers to learn how to work with experts from other fields to explore new directions together. ‚ÄúThese kinds of interactions often spark fresh, creative ideas,‚Äù he says.Maintaining curiosity is just as important. ‚ÄúBeing open to new technologies and fields is what enables researchers to continually break new ground and produce original results.‚Äù




Xu extends an open invitation to aspiring researchers from all backgrounds to join Microsoft Research Asia ‚Äì Singapore. ‚ÄúWe offer a unique platform that blends cutting-edge research with real-world impact,‚Äù he says. ‚ÄúIt‚Äôs a place where you can work on the frontiers of AI‚Äîand see how your work can help transform industries and improve lives.‚Äù



To learn more about current openings at the Singapore lab, please visit our careers page (opens in new tab).¬†
Opens in a new tabThe post Xinxing Xu bridges AI research and real-world impact at Microsoft Research Asia &#8211; Singapore appeared first on Microsoft Research.
‚Ä¢ Technical approach for classifying human-AI interactions at scale
  As large language models (LLMs) become foundational to modern AI systems, the ability to run them at scale‚Äîefficiently, reliably, and in near real-time‚Äîis no longer a nice-to-have. It‚Äôs essential. The Semantic Telemetry project tackles this challenge by applying LLM-based classifiers to hundreds of millions of sampled, anonymized Bing Chat conversations each week. These classifiers extract signals like user expertise, primary topic, and satisfaction, enabling deeper insight into human-AI interactions and driving continuous system improvement.



But building a pipeline that can handle this volume isn‚Äôt just about plugging into an API. It requires a high-throughput, high-performance architecture that can orchestrate distributed processing, manage token and prompt complexity, and gracefully handle the unpredictability of remote LLM endpoints.



In this latest post in our series on Semantic Telemetry, we‚Äôll walk through the engineering behind that system‚Äîhow we designed for scale from the start, the trade-offs we made, and the lessons we learned along the way. From batching strategies and token optimization and orchestration, we‚Äôll share what it takes to build a real-time LLM classification pipeline.



For additional project background: Semantic Telemetry: Understanding how users interact with AI systems and Engagement, user expertise, and satisfaction: Key insights from the Semantic Telemetry Project.






	
		
						Blog
			
				Semantic Telemetry: Understanding how users interact with AI systems&nbsp;
			
					
	







	
		
						Blog
			
				Engagement, user expertise, and satisfaction: Key insights from the Semantic Telemetry Project&nbsp;
			
					
	






System architecture highlights



The Semantic Telemetry pipeline (opens in new tab) is a highly-scalable, highly-configurable, data transformation pipeline. While it follows a familiar ETL structure, several architectural innovations make it uniquely suited for high-throughput LLM integration:




Hybrid compute engineThe pipeline combines the distributed power of‚ÄØPySpark‚ÄØwith the speed and simplicity of‚ÄØPolars, enabling it to scale across large datasets or run lightweight jobs in Spark-less environments‚Äîwithout code changes.



LLM-centric transformation layerAt the core of the pipeline is a multi-stage transformation process tailored for running across multiple LLM endpoints such that:

Runs model agnostic. Provides a generic interface for LLMs and adopts model specific interfaces built from a generic interface.



Prompt templates are defined using the Prompty language specification for consistency and reuse, with options for users to include custom prompts.



Parsing and cleaning logic ensures structured, schema-aligned outputs, even when LLM responses are imperfect such as removing extra characters in output, resolving not-exact label matches (i.e. ‚Äúcreate‚Äù versus ‚Äúcreated‚Äù) and relabeling invalid classifications.






Figure 1. Architecture diagram



The pipeline supports multiple classification tasks (e.g., user expertise, topic, satisfaction) through modular prompt templates and configurable execution paths‚Äîmaking it easy to adapt to new use cases or environments.



Engineering challenges & solutions



Building a high-throughput, LLM-powered classification pipeline at scale introduced a range of engineering challenges‚Äîfrom managing latency and token limits to ensuring system resilience. Below are the key hurdles we encountered and how we addressed them.



LLM endpoint latency & variability



Challenge: LLM endpoints, especially those hosted remotely (e.g., Azure OpenAI), introduce unpredictable latency due to model load, prompt complexity, and network variability. This made it difficult to maintain consistent throughput across the pipeline.



Solution: We implemented a combination of:




Multiple Azure OpenAI endpoints in rotation to increase throughput and distribute workload. We can analyze throughput and redistribute as needed.



Saving output in intervals to write data asynchronously in case of network errors.



Utilizing models with higher tokens per minute (TPM) such as OpenAI‚Äôs GPT-4o mini. GPT-4o mini had a 2M TPM limit which is a 25x throughput increase from GPT-4 (80K TPM -> 2M TPM)



Timeouts and retries with exponential backoff.




Evolving LLM models & prompt alignment



Challenge: Each new LLM release‚Äîsuch as Phi, Mistral, DeepSeek, and successive generations of GPT (e.g., GPT-3.5, GPT-4, GPT-4 Turbo, GPT-4o)‚Äîbrings improvements, but also subtle behavioral shifts. These changes can affect classification consistency, output formatting, and even the interpretation of prompts. Maintaining alignment with baseline expectations across models became a moving target.



Solution: We developed a model evaluation workflow to test prompt alignment across LLM versions:




Small-sample testing: We ran the pipeline on a representative sample using the new model and compared the output distribution to a known baseline.



Distribution analysis: If the new model‚Äôs output aligned closely, we scaled up testing. If not, we iteratively‚ÄØtuned the prompts‚ÄØand re-ran comparisons.



Interpretation flexibility: We also recognized that a shift in distribution isn‚Äôt always a regression. Sometimes it reflects a more accurate or nuanced classification, especially as models improve.




To support this process, we used tools like Sammo (opens in new tab), which allowed us to compare outputs across multiple models and prompt variants. This helped us quantify the impact of prompt changes and model upgrades and make informed decisions about when to adopt a new model or adjust our classification schema.



Dynamic concurrency scaling for LLM calls



Challenge: LLM endpoints frequently encounter rate limits and inconsistent response times under heavy usage. The models&#8217; speeds can also vary, complicating the selection of optimal concurrency levels. Furthermore, users may choose suboptimal settings due to lack of familiarity, and default concurrency configurations are rarely ideal for every situation. Dynamic adjustments based on throughput, measured in various ways, can assist in determining optimal concurrency levels.



Solution: We implemented a dynamic concurrency control mechanism that proactively adjusts the number of parallel LLM calls based on real-time system behavior:




External task awareness: The system monitors the number of parallel tasks running across the pipeline (e.g., Spark executors or async workers) and uses this to inform the initial concurrency level.



Success/failure rate monitoring: The system tracks the rolling success and failure rates of LLM calls. A spike in failures triggers a temporary reduction in concurrency, while sustained success allows for gradual ramp-up.



Latency-based feedback loop: Instead of waiting for rate-limit errors, measure the‚ÄØresponse time‚ÄØof LLM calls. If latency increases, reduce concurrency; if latency decreases and success rates remain high, cautiously scale up.




	
		

		
		Spotlight: Event Series
	
	
	
						
				
					
				
			
			
			

									Microsoft Research Forum
				
								Join us for a continuous exchange of ideas about research in the era of general AI. Watch the first four episodes on demand.
				
								
					
						
							Watch on-demand						
					
				
							
	
Opens in a new tab	
	


Optimization experiments



To further improve throughput and efficiency, we ran a series of optimization experiments. Each approach came with trade-offs that we carefully measured.



Batch endpoints (Azure/OpenAI)



Batch endpoints are a cost-effective, moderately high-throughput way of executing LLM requests. Batch endpoints process large lists of LLM prompts over a 24-hour period, recording responses in a file. They are about 50% cheaper than non-batch endpoints and have separate token limits, enabling increased throughput when used alongside regular endpoints. However, they require at least 24 hours to complete requests and provide lower overall throughput compared to non-batch endpoints, making them unsuitable for situations needing quick results.



Conversation batching in prompts during pipeline runtime



Batching multiple conversations for classification at once can significantly increase throughput and reduce token usage, but it may impact the accuracy of results. In our experiment with a domain classifier, classifying 10 conversations simultaneously led to an average of 15-20% of domain assignments changing between repeated runs of the same prompt. To address this, one mitigation approach is to use a grader LLM prompt: first classify the batch, then have the LLM identify any incorrectly classified conversations, and finally re-classify those as needed. While batching offers efficiency gains, it is important to monitor for potential drops in classification quality.



Combining classifiers in a single prompt



Combining multiple classifiers into a single prompt increases throughput by allowing one call to the LLM instead of multiple calls. This not only multiplies the overall throughput by the number of classifiers processed but also reduces the total number of tokens used, since the conversation text is only passed in once. However, this approach may compromise classification accuracy, so results should be closely monitored.



Classification using text embeddings



An alternative approach is to train custom neural network models for each classifier using only the text embeddings of conversations. This method delivers both cost and time savings by avoiding making multiple LLM requests for every classifier and conversation‚Äîinstead, the system only needs to request conversation text embeddings once and can reuse these embeddings across all classifier models.



For example, starting with a set of conversations to validate and test the new model, run these conversations through the original prompt-based classifier to generate a set of golden classifications, then obtain text embeddings (using a tool like text-embedding-3-large) for each conversation. These embeddings and their corresponding classifications are used to train a model such as a multi-layer perceptron. In production, the workflow involves retrieving the text embedding for each conversation and passing it through the trained model; if there is a model for each classifier, a single embedding retrieval per conversation suffices for all classifiers.



The benefits of this approach include significantly increased throughput and cost savings‚Äîsince it‚Äôs not necessary to call the LLM for every classifier and conversation. However, this setup can require GPU compute which can increase costs and infrastructure complexity, and the resulting models may not achieve the same accuracy as prompt-based classification methods.



Prompt compression



Compressing prompts by eliminating unnecessary tokens or by using a tool such as LLMLingua (opens in new tab) to automate prompt compression can optimize classification prompts either ahead of time or in real-time. This approach increases overall throughput and results in cost savings due to a reduced number of tokens, but there are risks: changes to the classifier prompt or conversation text may impact classification accuracy, and depending on the compression technique, it could even decrease throughput if the compression process takes longer than simply sending uncompressed text to the LLM.



Text truncation



Truncating conversations to a specific length limits the overall number of tokens sent through an endpoint, offering cost savings and increased throughput like prompt compression. By reducing the number of tokens per request, throughput rises because more requests can be made before reaching the endpoint‚Äôs tokens-per-minute (TPM) limit, and costs decrease due to fewer tokens being processed. However, the ideal truncation length depends on both the classifiers and the conversation content, so it‚Äôs important to assess how truncation affects output quality before implementation. While this approach brings clear efficiency benefits, it also poses a risk: long conversations may have their most important content cut off, which can reduce classification accuracy.



Conclusion



Building a scalable, high-throughput pipeline for LLM-based classification is far from trivial. It requires navigating a constantly shifting landscape of model capabilities, prompt behaviors, and infrastructure constraints. As LLMs become faster, cheaper, and more capable, they‚Äôre unlocking new possibilities for real-time understanding of human-AI interactions at scale. The techniques we‚Äôve shared represent a snapshot of what‚Äôs working today. But more importantly, they offer a foundation for what‚Äôs possible tomorrow.
Opens in a new tabThe post Technical approach for classifying human-AI interactions at scale appeared first on Microsoft Research.
‚Ä¢ AI Testing and Evaluation: Reflections
  Generative AI presents a unique challenge and opportunity to reexamine governance practices for the responsible development, deployment, and use of AI. To advance thinking in this space, Microsoft has tapped into the experience and knowledge of experts across domains‚Äîfrom genome editing to cybersecurity‚Äîto investigate the role of testing and evaluation as a governance tool.&nbsp;AI Testing and Evaluation: Learnings from Science and Industry,&nbsp;hosted by Microsoft Research‚Äôs&nbsp;Kathleen Sullivan, explores what the technology industry and policymakers can learn from these fields and how that might help shape the course of AI development.



In the series finale, Amanda Craig Deckard, senior director of public policy in Microsoft‚Äôs Office of Responsible AI, rejoins Sullivan to discuss what Microsoft has learned about testing as a governance tool and what‚Äôs next for the company&#8217;s work in the AI governance space. The pair explores high-level takeaways (i.e., testing is important and challenging!); the roles of rigor, standardization, and interpretability in making testing a reliable governance tool; and the potential for public-private partnerships to help advance not only model-level evaluation but deployment-level evaluation, too.







Learn more:



Learning from other domains to advance AI evaluation and testing&nbsp;Microsoft Research Blog | June 2025&nbsp;



Responsible AI: Ethical policies and practices | Microsoft AI&nbsp;








	
		
			Subscribe to the Microsoft Research Podcast:		
		
							
					
						  
						Apple Podcasts
					
				
			
							
					
						
						Email
					
				
			
							
					
						
						Android
					
				
			
							
					
						
						Spotify
					
				
			
							
					
						
						RSS Feed
					
				
					
	




	
		
			
				
					

Transcript



[MUSIC]&nbsp;



KATHLEEN SULLIVAN: Welcome to AI Testing and Evaluation: Learnings from Science and Industry. I‚Äôm your host, Kathleen Sullivan.&nbsp;



As generative AI continues to advance, Microsoft has gathered a range of experts‚Äîfrom genome editing to cybersecurity‚Äîto share how their fields approach evaluation and risk assessment. Our goal is to learn from their successes and their stumbles to move the science and practice of AI testing forward. In this series, we‚Äôll explore how these insights might help guide the future of AI development, deployment, and responsible use.&nbsp;



[MUSIC ENDS]&nbsp;



For our final episode of the series, I‚Äôm thrilled to once again be joined by Amanda Craig Deckard, senior director of public policy in Microsoft‚Äôs Office of Responsible AI.&nbsp;



Amanda, welcome back to the podcast!



				
				
					



AMANDA CRAIG DECKARD: Thank you so much.



SULLIVAN: In our intro episode, you really helped set the stage for this series. And it‚Äôs been great, because since then, we‚Äôve had the pleasure of speaking with governance experts about genome editing, pharma, medical devices, cybersecurity, and we‚Äôve also gotten to spend some time with our own Microsoft responsible AI leaders and hear reflections from them.



And here‚Äôs what stuck with me, and I‚Äôd love to hear from you on this, as well: testing builds trust; context is shaping risk; and every field is really thinking about striking its own balance between pre-deployment testing and post-deployment monitoring.



So drawing on what you‚Äôve learned from the workshop and the case studies, what headline insights do you think matter the most for AI governance?



CRAIG DECKARD: It&#8217;s been really interesting to learn from all of these different domains, and there are, you know, lots of really interesting takeaways.&nbsp;



I think a starting point for me is actually pretty similar to where you landed, which is just that testing is really important for trust, and it&#8217;s also really hard [LAUGHS] to figure out exactly, you know, how to get it right, how to make sure that you&#8217;re addressing risks, that you&#8217;re not constraining innovation, that you are recognizing that a lot of the industry that&#8217;s impacted is really different. You have small organizations, you have large organizations, and you want to enable that opportunity that is enabled by the technology across the board.&nbsp;



And so it&#8217;s just difficult to, kind of, get all of these dynamics right, especially when, you know, I think we heard from other domains, testing is not some, sort of, like, oh, simple thing, right. There&#8217;s not this linear path from, like, A to B where you just test the one thing and you&#8217;re done.&nbsp;



SULLIVAN: Right.



CRAIG DECKARD: It&#8217;s complex, right. Testing is multistage. There&#8217;s a lot of testing by different actors. There are a lot of different purposes for which you might test. As I think it was Dan Carpenter who talked about it&#8217;s not just about testing for safety. It&#8217;s also about testing for efficacy and building confidence in the right dosage for pharmaceuticals, for example. And that&#8217;s across the board for all of these domains, right. That you&#8217;re really thinking about the performance of the technology. You&#8217;re thinking about safety. You&#8217;re trying to also calibrate for efficiency.



And so those tradeoffs, every expert shared that navigating those is really challenging. And also that there were real impacts to early choices in the, sort of, governance of risk in these different domains and the development of the testing, sort of, expectations, and that in some cases, this had been difficult to reverse, which also just layers on that complexity and that difficulty in a different way. So that‚Äôs the super high-level takeaway. But maybe if I could just quickly distill, like, three takeaways that I think really are applicable to AI in a bit more of a granular way.



You know, one is about, how is the testing exactly used? For what purpose? And the second is what emphasis there is on this pre- versus post-deployment testing and monitoring. And then the third is how rigid versus adaptive the, sort of, testing regimes or frameworks are in these different domains.&nbsp;



So on the first‚Äîhow is testing used?‚Äîso is testing something that impacts market entry, for example? Or is it something that might be used more for informing how risk is evolving in the domain and how broader risk management strategies might need to be applied? We have examples, like the pharmaceutical or medical device industry experts with whom you spoke, that&#8217;s really, you know, testing ‚Ä¶ there is a pre-deployment requirement. So that&#8217;s one question.&nbsp;



The second is this emphasis on pre- versus post-deployment testing and monitoring, and we really did see across domains that in many cases, there is a desire for both pre- and post-deployment, sort of, testing and monitoring, but also that, sort of, naturally in these different domains, a degree of emphasis on one or the other had evolved and that had a real impact on governance and tradeoffs.&nbsp;



And the third is just how rigid versus adaptive these testing and evaluation regimes or frameworks are in these different domains. We saw, you know, in some domains, the testing requirements were more rigid as you might expect in more of the pharmaceutical or medical devices industries, for example. And in other domains, there was this more, sort of, adaptive approach to how testing might get used. So, for example, in the case of our other general-purpose technologies, you know, you spoke with Alta Charo on genome editing, and in our case studies, we also explored this in the context of nanotechnology. In those general-purpose technology domains, there is more emphasis on downstream or application-context testing that is more, sort of, adaptive to the use scenario of the technology and, you know, having that work in conjunction with testing more at the, kind of, level of the technology itself.



SULLIVAN: I want to double-click on a number of the things we just talked about. But actually, before we go too much deeper, a question on if there&#8217;s anything that really surprised you or challenged maybe some of your own assumptions in this space from some of the discussions that we had over the series.&nbsp;



CRAIG DECKARD: Yeah. You know, I know I&#8217;ve already just mentioned this pre- versus post-deployment testing and monitoring issue, but it was something that was very interesting to me and in some ways surprised me or made me just realize something that I hadn&#8217;t fully connected before, about how these, sort of, regimes might evolve in different contexts and why. And in part, I couldn&#8217;t help but bring the context I have from cybersecurity policy into this, kind of, processing of what we learned and reflection because there was a real contrast for me between the pharmaceutical industry and the cybersecurity domain when I think about the emphasis on pre- versus post-deployment monitoring.



And on the one hand, we have in the pharmaceutical domain a real emphasis that has developed around pre-market testing. And there is also an expectation in some circumstances in the pharmaceutical domain for post-deployment testing, as well. But as we learned from our experts in that domain, there has naturally been a real, kind of, emphasis on the pre-market portion of that testing. And in reality, even where post-market monitoring is required and post-market testing is required, it does not always actually happen. And the experts really explained that, you know, part of it is just the incentive structure around the emphasis around, you know, the testing as a pre-market, sort of, entry requirement. And also just the resources that exist among regulators, right. There&#8217;s limited resources, right. And so there are just choices and tradeoffs that they need to make in their own, sort of, enforcement work.



And then on the other hand, you know, in cybersecurity, I never thought about the, kind of, emphasis on things like coordinated vulnerability disclosure and bug bounties that have really developed in the cybersecurity domain. But it&#8217;s a really important part of how we secure technology and enhance cybersecurity over time, where we have these norms that have developed where, you know, security researchers are doing really important research. They&#8217;re finding vulnerabilities in products. And we have norms developed where they report those to the companies that are in a position to address those vulnerabilities. And in some cases, those companies actually pay, through bug bounties, the researchers. And perhaps in some ways, the role of coordinated vulnerability disclosure and bug bounties has evolved the way that it has because there hasn&#8217;t been as much emphasis on the pre-market testing across the board at least in the context of software.



And so you look at those two industries and it was interesting to me to study them to some extent in contrast with each other as this way that the incentives and the resources that need to be applied to testing, sort of, evolve to address where there&#8217;s, kind of, more or less emphasis.



SULLIVAN: It&#8217;s a great point. I mean, I think what we&#8217;re hearing‚Äîand what you&#8217;re saying‚Äîis just exactly this choice ‚Ä¶ like, is there a binary choice between focusing on pre-deployment testing or post-deployment monitoring? And, you know, I think our assumption is that we need to do both. But I&#8217;d love to hear from you on that.&nbsp;



CRAIG DECKARD: Absolutely. I think we need to do both. I&#8217;m very persuaded by this inclination always that there&#8217;s value in trying to really do it all in a risk management context.&nbsp;



And also, we know one of the principles of risk management is you have to prioritize because there are finite resources. And I think that&#8217;s where we get to this challenge in really thinking deeply, especially as we&#8217;re in the early days of AI governance, and we need to be very thoughtful about, you know, tradeoffs that we may not want to be making but we are because, again, these are finite choices and we, kind of, can&#8217;t help but put our finger on the dial in different directions with our choices that, you know, it&#8217;s going to be very difficult to have, sort of, equal emphasis on both. And we need to invest in both, but we need to be very deliberate about the roles of each and how they complement each other and who does which and how we use what we learn from pre- versus post-deployment testing and monitoring.



SULLIVAN: Maybe just spending a little bit more time here ‚Ä¶ you know, a lot of attention goes into testing models upstream, but risk often shows up once they&#8217;re wired into real products and workflows. How much does deployment context change the risk picture from your perspective?&nbsp;



CRAIG DECKARD: Yeah, I ‚Ä¶ such an important question. I really agree that there has been a lot of emphasis to date on, sort of, testing models upstream, the AI model evaluation. And it&#8217;s also really important that we bring more attention into evaluation at the system or application level. And I actually see that in governance conversations, this is actually increasingly raised, this need to have system-level evaluation. We see this across regulation. We also see it in the context of just organizations trying to put in governance requirements for how their organization is going to operate in deploying this technology.&nbsp;



And there&#8217;s a gap today in terms of best practices around system-level testing, perhaps even more than model-level evaluation. And it&#8217;s really important because in a lot of cases, the deployment context really does impact the risk picture, especially with AI, which is a general-purpose technology, and we really saw this in our study of other domains that represented general-purpose technology.&nbsp;



So in the case study that you can find online on nanotechnology, you know, there&#8217;s a real distinction between the risk evaluation and the governance of nanotechnology in different deployment contexts. So the chapter that our expert on nanotechnology wrote really goes into incredibly interesting detail around, you know, deployment of nanotechnology in the context of, like, chemical applications versus consumer electronics versus pharmaceuticals versus construction and how the way that nanoparticles are basically delivered in all those different deployment contexts, as well as, like, what the risk of the actual use scenario is just varies so much. And so there&#8217;s a real need to do that kind of risk evaluation and testing in the deployment context, and this difference in terms of risks and what we learned in these other domains where, you know, there are these different approaches to trying to really think about and gain efficiencies and address risks at a horizontal level versus, you know, taking a real sector-by-sector approach. And to some extent, it seems like it&#8217;s more time intensive to do that sectoral deployment-specific work. And at the same time, perhaps there are efficiencies to be gained by actually doing the work in the context in which, you know, you have a better understanding of the risk that can result from really deploying this technology.&nbsp;



And ultimately, [LAUGHS] really what we also need to think about here is probably, in the end, just like pre- and post-deployment testing, you need both. Not probably; certainly!



So effectively we need to think about evaluation at the model level and the system level as being really important. And it&#8217;s really important to get system evaluation right so that we can actually get trust in this technology in deployment context so we enable adoption in low- and in high-risk deployments in a way that means that we&#8217;ve done risk evaluation in each of those contexts in a way that really makes sense in terms of the resources that we need to apply and ultimately we are able to unlock more applications of this technology in a risk-informed way.



SULLIVAN: That&#8217;s great. I mean, I couldn&#8217;t agree more. I think these contexts, the approaches are so important for trust and adoption, and I&#8217;d love to hear from you, what do we need to advance AI evaluation and testing in our ecosystem? What are some of the big gaps that you&#8217;re seeing, and what role can different stakeholders play in filling them? And maybe an add-on, actually: is there some sort of network effect that could 10x our testing capacity?&nbsp;



CRAIG DECKARD: Absolutely. So there&#8217;s a lot of work that needs to be done, and there&#8217;s a lot of work in process to really level up our whole evaluation and testing ecosystem. We learned, across domains, that there‚Äôs really a need to advance our thinking and our practice in three areas: rigor of testing; standardization of methodologies and processes; and interpretability of test results.&nbsp;



So what we mean by rigor is that we are ensuring that what we are ultimately evaluating in terms of risks is defined in a scientifically valid way and we are able to measure against that risk in a scientifically valid way.&nbsp;



By standardization, what we mean is that there&#8217;s really an accepted and well-understood and, again, a scientifically valid methodology for doing that testing and for actually producing artifacts out of that testing that are meeting those standards. And that sets us up for the final portion on interpretability, which is, like, really the process by which you can trust that the testing has been done in this rigorous and standardized way and that then you have artifacts that result from the testing process that can really be used in the risk management context because they can be interpreted, right.&nbsp;



We understand how to, like, apply weight to them for our risk-management decisions. We actually are able to interpret them in a way that perhaps they inform other downstream risk mitigations that address the risks that we see through the testing results and that we actually understand what limitations apply to the test results and why they may or may not be valid in certain, sort of, deployment contexts, for example, and especially in the context of other risk mitigations that we need to apply. So there&#8217;s a need to advance all three of those things‚Äîrigor, standardization, and interpretability‚Äîto level up the whole testing and evaluation ecosystem.&nbsp;



And when we think about what actors should be involved in that work ‚Ä¶ really everybody, which is both complex to orchestrate but also really important. And so, you know, you need to have the entire value chain involved in really advancing this work. You need the model developers, but you also need the system developers and deployers that are really engaged in advancing the science of evaluation and advancing how we are using these testing artifacts in the risk management process.&nbsp;



When we think about what could actually 10x our testing capacity‚Äîthat&#8217;s the dream, right? We all want to accelerate our progress in this space. You know,&nbsp;I think we need work across all three of those areas of rigor, standardization, and interpretability, but I think one that will really help accelerate our progress across the board is that standardization work, because ultimately, you&#8217;re going to need to have these tests be done and applied across so many different contexts, and ultimately, while we want the whole value chain engaged in the development of the thinking and the science and the standards in this space, we also need to realize that not every organization is necessarily going to have the capacity to, kind of, contribute to developing the ways that we create and use these tests. And there are going to be many organizations that are going to benefit from there being standardization of the methodologies and the artifacts that they can pick up and use.



One thing that I know we&#8217;ve heard throughout this podcast series from our experts in other domains, including Timo [Minssen] in the medical devices context and Ciaran [Martin] in the cybersecurity context, is that there&#8217;s been a recognition, as those domains have evolved, that there&#8217;s a need to calibrate our, sort of, expectations for different actors in the ecosystem and really understand that small businesses, for example, just cannot apply the same degree of resources that others may be able to, to do testing and evaluation and risk management. And so the benefit of having standardized approaches is that those organizations are able to, kind of, integrate into the broader supply chain ecosystem and apply their own, kind of, risk management practices in their own context in a way that is more efficient.&nbsp;



And finally, the last stakeholder that I think is really important to think about in terms of partnership across the ecosystem to really advance the whole testing and evaluation work that needs to happen is government partners, right, and thinking beyond the value chain, the AI supply chain, and really thinking about public-private partnership. That&#8217;s going to be incredibly important to advancing this ecosystem.



You know, I think there&#8217;s been real progress already in the AI evaluation and testing ecosystem in the public-private partnership context. We have been really supportive of the work of the International Network of AI Safety and Security Institutes (opens in new tab)[1] (opens in new tab) and the Center for AI Standards and Innovation (opens in new tab) that all allow for that kind of public-private partnership on actually testing and advancing the science and best practices around standards.&nbsp;



And there are other innovative, kind of, partnerships, as well, in the ecosystem. You know, Singapore has recently launched their Global AI Assurance Pilot (opens in new tab) findings. And that effort really paired application deployers and testers so that consequential impacts at deployment could really be tested. And that&#8217;s a really fruitful, sort of, effort that complements the work of these institutes and centers that are more focused on evaluation at the model level, for example.



And in general, you know, I think that there&#8217;s just really a lot of benefits for us thinking expansively about what we can accomplish through deep, meaningful public-private partnership in this space. I&#8217;m really excited to see where we can go from here with building on, you know, partnerships across AI supply chains and with governments and public-private partnerships.&nbsp;



SULLIVAN: I couldn&#8217;t agree more. I mean, this notion of more engagement across the ecosystem and value chain is super important for us and informs how we think about the space completely.&nbsp;



If you could invite any other industry to the next workshop, maybe quantum safety, space tech, even gaming, who&#8217;s on your wish list? And maybe what are some of the things you&#8217;d want to go deeper on?&nbsp;



CRAIG DECKARD: This is something that we really welcome feedback on if anyone listening has ideas about other domains that would be interesting to study. I will say, I think I shared at the outset of this podcast series, the domains that we added in this round of our efforts in studying other domains actually all came from feedback that we received from, you know, folks we‚Äôd engaged with our first study of other domains and multilateral, sort of, governance institutions. And so we&#8217;re really keen to think about what other domains could be interesting to study. And we are also keen to go deeper, building on what we learned in this round of effort going forward.&nbsp;



One of the areas that I am particularly really interested in is going deeper on, what, sort of, transparency and information sharing about risk evaluation and testing will be really useful to share in different contexts? So across the AI supply chain, what is the information that&#8217;s going to be really meaningful to share between developers and deployers of models and systems and those that are ultimately using this technology in particular deployment contexts? And, you know, I think that we could have much to learn from other general-purpose technologies like genome editing and nanotechnology and cybersecurity, where we could learn a bit more about the kinds of information that they have shared across the development and deployment life cycle and how that has strengthened risk management in general as well as provided a really strong feedback loop around testing and evaluation. What kind of testing is most useful to do at what point in the life cycle, and what artifacts are most useful to share as a result of that testing and evaluation work?



I&#8217;ll say, as Microsoft, we have been really investing in how we are sharing information with our various stakeholders. We also have been engaged with others in industry in reporting what we&#8217;ve done in the context of the Hiroshima AI Process, or HAIP, Reporting Framework (opens in new tab). This is an effort that is really just in its first round of really exploring how this kind of reporting can be really additive to risk management understanding. And again, I think there&#8217;s real opportunity here to look at this kind of reporting and understand, you know, what&#8217;s valuable for stakeholders and where is there opportunity to go further in really informing value chains and policymakers and the public about AI risk and opportunity and what can we learn again from other domains that have done this kind of work over decades to really refine that kind of information sharing.¬†



SULLIVAN: It&#8217;s really great to hear about all the advances that we&#8217;re making on these reports. I&#8217;m guessing a lot of the metrics in there are technical, but sociotechnical impacts‚Äîjobs, maybe misinformation, well-being‚Äîare harder to score. What new measurement ideas are you excited about, and do you have any thoughts on, like, who needs to pilot those?



CRAIG DECKARD: Yeah, it&#8217;s an incredibly interesting question that I think also just speaks to, you know, the breadth of, sort of, testing and evaluation that&#8217;s needed at different points along that AI life cycle and really not getting lost in one particular kind of testing or another pre- or post-deployment and thinking expansively about the risks that we&#8217;re trying to address through this testing.&nbsp;



You know, for example, even with the UK&#8217;s AI Security Institute (opens in new tab) that has just recently launched a new program, a new team, that&#8217;s focused on societal resilience research. I think it&#8217;s going to be a really important area from a sociotechnical impact perspective to bring some focus into as this technology is more widely deployed. Are we understanding the impacts over time as different people and different cultures adopt and use this technology for different purposes?&nbsp;



And I think that&#8217;s an area where there really is opportunity for greater public-private partnership in this research. Because we all share this long-term interest in ensuring that this technology is really serving people and we have to understand the impacts so that we understand, you know, what adjustments we can actually pursue sooner upstream to address those impacts and make sure that this technology is really going to work for all of us and in a way that is consistent with the societal values that we want.&nbsp;



SULLIVAN: So, Amanda, looking ahead, I would love to hear just what&#8217;s going to be on your radar? What&#8217;s top of mind for you in the coming weeks?



CRAIG DECKARD: Well, we are certainly continuing to process all the learnings that we&#8217;ve had from studying these domains. It‚Äôs really been a rich set of insights that we want to make sure we, kind of, fully take advantage of. And, you know, I think these hard questions and, you know, real opportunities to be thoughtful in these early days of AI governance are not, sort of, going away or being easily resolved soon. And so I think we continue to see value in really learning from others, thinking about what&#8217;s distinct in the AI context, but also what we can apply in terms of what other domains have learned.



SULLIVAN: Well, Amanda, it has been such a special experience for me to help illuminate the work of the Office of Responsible AI and our team in Microsoft Research, and [MUSIC] it&#8217;s just really special to see all of the work that we&#8217;re doing to help set the standard for responsible development and deployment of AI. So thank you for joining us today, and thanks for your reflections and discussion.



And to our listeners, thank you so much for joining us for the series. We really hope you enjoyed it!&nbsp;To check out all of our episodes, visit aka.ms/AITestingandEvaluation (opens in new tab), and if you want to learn more about how Microsoft approaches AI governance, you can visit microsoft.com/RAI (opens in new tab).&nbsp;



See you next time!&nbsp;



[MUSIC FADES]‚ÄØ

				
			
			
				Show more			
		
	





AI Testing and Evaluation podcast series








[1] (opens in new tab) Since the launch of the International Network of AI Safety Institutes, the UK renamed its institute the AI Security Institute (opens in new tab).
Opens in a new tabThe post AI Testing and Evaluation: Reflections appeared first on Microsoft Research.
‚Ä¢ Boost cold-start recommendations with vLLM on AWS Trainium
  Cold start in recommendation systems goes beyond just new user or new item problems‚Äîit‚Äôs the complete absence of personalized signals at launch. When someone first arrives, or when fresh content appears, there‚Äôs no behavioral history to tell the engine what they care about, so everyone ends up in broad generic segments. That not only dampens click-through and conversion rates, it can drive users away before a system ever gets a chance to learn their tastes. Standard remedies‚Äîcollaborative filtering, matrix factorization, or popularity lists‚Äîlack the nuance to bridge that signal gap, and their one-size-fits-all suggestions quickly feel stale. Imagine, instead, if you could generate detailed interest profiles from day one. By tapping into large language models (LLMs) for zero-shot reasoning, you can synthesize rich, context-aware user and item embeddings without waiting for weeks of interaction data‚Äîturning a cold start into a warm welcome. 
In this post, we demonstrate how to use vLLM for scalable inference and use AWS Deep Learning Containers (DLC) to streamline model packaging and deployment. We‚Äôll generate interest expansions through structured prompts, encode them into embeddings, retrieve candidates with FAISS, apply validation to keep results grounded, and frame the cold-start challenge as a scientific experiment‚Äîbenchmarking LLM and encoder pairings, iterating rapidly on recommendation metrics, and showing clear ROI for each configuration. 
Solution overview 
We build our cold-start solution on Amazon EC2 Trainium chips. To streamline model deployment, we use DLCs with the AWS Neuron SDK, which installs Neuron-optimized PyTorch modules and includes the latest AWS Trainium drivers and runtime pre-installed. 

 
 Figure : Cold-start recommendation pipeline on AWS Trainium with vLLM &amp; NxD
 
Sharding large models across multiple Trainium chips is handled by the distributed library used by Neuron, NeuronX Distributed (NxD), which integrates seamlessly with vLLM. NxD manages model partitions across multiple instances with minimal code changes, enabling parallel inference of even 70B parameter LLMs. This combination‚ÄîTrainium chips, Neuron Tools, and vLLM‚Äîgives machine learning (ML) engineers a flexible, cost-efficient, production-ready solution for experimenting with different LLM and encoder configurations and delivers rapid iteration on recommendation quality metrics without modifying core model code. 
In the next section, we orchestrate our experiments in a Jupyter notebook‚Äîproviding a reproducible, end-to-end workflow from loading data and engineering structured prompts to generating embeddings and retrieving candidates with FAISS‚Äîcomplete with interactive charts to visualize recommendation performance. Then, in the production deep-dive, we walk through a reference implementation that packages your Neuron-optimized LLM and encoder as DLC images and deploys them on Amazon Elastic Kubernetes Service (Amazon EKS) with autoscaling, so your inference layer automatically adapts to demand while optimizing cost and performance. 
Expanding user interest profiles with LLMs 
In this post, we use the Amazon Book Reviews dataset (mohamedbakhet/amazon-books-reviews) from Kaggle, which provides real-world user reviews and metadata for tens of thousands of books. This rich collection lets us simulate cold-start scenarios‚Äîwhere a brand-new user has only a single review or like‚Äîand evaluate how well our interest expansions, powered by distilled versions of Meta‚Äôs Llama 8B and 70B models, generate rich user profiles. We use an LLM to enrich a new user‚Äôs profile from minimal initial data. For example, if a user has only reviewed one science fiction novel, the LLM infers related subtopics‚Äîsuch as galactic empires, cyberpunk dystopias, or space exploration‚Äîthat the user is likely to enjoy. We use structured prompts that embed the user‚Äôs existing activity into a concise instruction to verify consistency and relevance, as demonstrated in the following example: 
 
 prompt = (
f"The user has shown interest in: {user_review_category}.\n"
"Suggest 3‚Äì5 related book topics they might enjoy.\n"
"Respond with a JSON list of topic keywords."
)
expanded_topics = llm.generate([prompt])[0].text 
 
By constraining the LLM‚Äôs output format‚Äîasking it to return a JSON array of topic keywords‚Äîwe avoid free‚Äëform tangents and obtain a predictable list of interest expansions. Modern generative models, such as Meta‚Äôs Llama, possess broad domain knowledge and human‚Äëlike reasoning, enabling them to connect related concepts and serve as powerful cold‚Äëstart boosters by inferring deep user preferences from a single review. These synthetic interests become new signals for our recommendation pipeline, allowing us to retrieve and rank books from the Amazon Reviews collection even with minimal user history. You can experiment with Llama variants ranging from one‚Äëbillion to seventy‚Äëbillion parameters to identify which model yields the most discriminative and relevant expansions. Those findings will guide our choice of model for production and determine the size and scale of the Amazon EC2 Trainium and Inferentia instances we provision, setting us up for live user A/B tests to validate performance in real‚Äëworld settings. 
Encoding user interests and retrieving relevant content 
After we have our expanded interests, the next step is to turn both those interests and our catalog of books into vectors that we can compare. We explore three sizes of the Google T5 encoder‚Äîbase, large and XL‚Äîto see how embedding dimensionality affects matching quality. The following are the steps: 
 
 Load the encoder for each size 
 Encode book summaries into a single NumPy matrix and normalize it 
 Build a FAISS index on those normalized vectors for fast nearest‚Äëneighbor search 
 Encode the expanded interest text the same way and query FAISS to retrieve the top k most similar books 
 
 
 from transformers import T5Tokenizer, T5EncoderModel
import faiss
import numpy as np

# Our dataset of book summaries
content_texts = df["review/summary"].tolist()
encoder_sizes = ["t5-base", "t5-large", "t5-xl"]
top_k = 5

for size in encoder_sizes:
    # 1. Load the tokenizer and encoder model for this size
    tokenizer = T5Tokenizer.from_pretrained(size)
    model = T5EncoderModel.from_pretrained(size)

    # 2. Encode all content into embeddings and normalize
    inputs = tokenizer(content_texts, return_tensors="pt", truncation=True, padding=True)
    outputs = model(**inputs)
    content_embs = outputs.last_hidden_state.mean(dim=1).detach().cpu().numpy().astype("float32")
    faiss.normalize_L2(content_embs)

    # 3. Build a FAISS index using inner-product (equivalent to cosine on unit vectors)
    index = faiss.IndexFlatIP(content_embs.shape[1])
    index.add(content_embs)

    # 4. Encode a single expanded interest and query the index
    interest = "space opera with political intrigue"
    enc = tokenizer([interest], return_tensors="pt", truncation=True, padding=True)
    interest_emb = model(**enc).last_hidden_state.mean(dim=1).detach().cpu().numpy().astype("float32")
    faiss.normalize_L2(interest_emb)

    distances, indices = index.search(interest_emb, top_k)
    recommendations = [content_texts[i] for i in indices[0]]

    print(f"\nTop {top_k} recommendations using {size}:")
    for title in recommendations:
        print(" -", title) 
 
You can compare how each encoder scale affects both the average FAISS distance (that is, how far apart your interest is from the content) and the actual recommended titles. Swapping in a different encoder family‚Äîsuch as SentenceTransformers‚Äîis as straightforward as replacing the model and tokenizer imports. 
Measuring and improving recommendation quality 
Now that we‚Äôve generated FAISS indexes for every LLM‚Äëencoder pairing and computed the mean distance between each expanded interest query and its top 10 neighbors, we know exactly how tightly or loosely each model‚Äôs embeddings cluster. The following chart shows those average distances for each combination‚Äîrevealing that 1B and 3B models collapse to almost zero, while 8B and 70B models (especially with larger encoders) produce progressively higher distances, signifying richer, more discriminative signals for recommendation. 

 
 Figure : Average FAISS distance by model and encoder
 
The chart shows that the 1B and 3B models yield an average FAISS distance of zero, meaning their expanded‚Äëinterest embeddings are essentially identical and offer no differentiation. By contrast, the 8B model produces a distance of about 0.5 with t5‚Äëbase, rising further with t5‚Äëlarge and t5‚Äëxl, which demonstrates that larger encoders capture more of the model‚Äôs nuance. The 70B model only adds a small boost‚Äîand only with the XL encoder‚Äîso its extra cost yields limited benefit. 
In practical terms, a Llama 8B LLM paired with a base or large T5 encoder delivers clear separation in embedding space without the higher inference time and resource usage of a 70B model. 
Comparing model and encoder impact on embedding spread 
To see how LLM size and encoder scale shape our embedding space, you can measure‚Äîfor each&nbsp;LLM and encoder &nbsp;pair‚Äîthe mean FAISS distance from a representative expanded interest vector to its top 10 neighbors. The following bar chart plots those averages side by side. You can instantly spot that 1B and 3B collapse to zero, 8B jumps to around 0.5 and rises with larger encoders, and 70B only adds a small extra spread at the XL scale. This helps you choose the smallest combination that still gives you the embedding diversity needed for effective cold‚Äëstart recommendations. 

 
 Figure : FAISS distance by LLM and encoder size
 
Evaluating recommendation overlap across Llama variations and encoders to balance consistency and novelty 
In the next analysis, you build a basic recommend_books&nbsp;helper that, for various LLM sizes and encoder choices, loads the corresponding expanded‚Äëinterest DataFrame, reads its FAISS index, reconstructs the first embedding as a stand‚Äëin query, and returns the top-k book titles. Using this helper, we first measure how much each pair of encoders agrees on recommendations for a single LLM‚Äîcomparing base compared to large, base compared to XL, and large compared XL‚Äîand then, separately, how each pair of LLM sizes aligns for a fixed encoder. Finally, we focus on the 8B model (shown in the following figure) and plot a heatmap of its encoder overlaps, which shows that base and large share about 40% of their top 5 picks while XL diverges more‚Äîillustrating how changing the encoder shifts the balance between consistency and novelty in the recommendations. 

 
 Figure : 8B model: encoder overlap heatmap
 
For the 8B model, the heatmap shows that t5_base and t5_large share 40% of their top 5 recommendations, t5_base and t5_xl also overlap 40%, while t5_large vs t5_xl overlap only 20%, indicating that the XL encoder introduces the greatest amount of novel titles compared to the other pairs. 
Tweaking tensor_parallel_size for optimal cost performance 
To balance inference speed against resource cost, we measured how increasing Neuron tensor parallelism affects latency when expanding user interests with the Llama 3.1 8B model on a trn1.32xlarge instance. We ran the same zero‚Äëshot expansion workload at&nbsp;tensor_parallel_size&nbsp;values of 2, 8, 16, and 32. As shown in the first chart, P50 Latency falls by 74 %‚Äîfrom 2,480 ms at TP = 2 to 650 ms at TP = 16‚Äîthen inches lower to 532 ms at TP = 32 (an additional 18 % drop). The following cost-to-performance chart shows that beyond TP = 16, doubling parallelism roughly doubles cost for only a 17 % further latency gain. 

 
 Figure : Latency compared to tensor parallel size
 
In practice, setting&nbsp;tensor_parallel_size&nbsp;to 16 delivers the best trade‚Äëoff: you capture most of the speed‚Äëup from model sharding while avoiding the sharply diminishing returns and higher core‚Äëhour costs that come with maximal parallelism, as shown in the following figure. 

 
 Figure : Cost-performance compared to tensor parallel size
 
The preceding figure visualizes the cost-to-performance ratio of the Llama 8B tests, emphasizing that TP=16 offers the most balanced efficiency before the benefits plateau. 
What‚Äôs next? 
Now that we have determined the models and encoders to use, as well as the optimal configuration to use with our dataset, such as sequence size and batch size, the next step is to deploy the models and define a production workflow that generates expanded interest that is encoded and ready for match with more content. 
Conclusion 
This post showed how AWS Trainium, the Neuron SDK, and scalable LLM inference can tackle cold-start challenges by enriching sparse user profiles for better recommendations from day one. 
Importantly, our experiments highlight that&nbsp;larger models and encoders don‚Äôt always mean better outcomes. While they can produce richer signals, the gains often don‚Äôt justify the added cost. You might find that an 8B LLM with a T5-large encoder strikes the best balance between performance and efficiency. 
Rather than assuming bigger is better, this approach helps teams identify the&nbsp;optimal model-encoder pair‚Äîdelivering high-quality recommendations with cost-effective infrastructure. 
 
About the authors 
Yahav Biran is a Principal Architect at AWS, focusing on large-scale AI workloads. He contributes to open-source projects and publishes in AWS blogs and academic journals, including the AWS compute and AI blogs and the Journal of Systems Engineering. He frequently delivers technical presentations and collaborates with customers to design Cloud applications. Yahav holds a Ph.D. in Systems Engineering from Colorado State University. 
 Nir Ozeri Nir is a Sr. Solutions Architect Manager with Amazon Web Services, based out of New York City. Nir leads a team of Solution Architects focused on ISV customers. Nir specializes in application modernization, application and product delivery, and scalable application architecture.

‚∏ª