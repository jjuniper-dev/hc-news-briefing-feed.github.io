‚úÖ Morning News Briefing ‚Äì September 30, 2025 10:44

üìÖ Date: 2025-09-30 10:44
üè∑Ô∏è Tags: #briefing #ai #publichealth #digitalgov

‚∏ª

üßæ Weather
‚Ä¢ Current Conditions:  7.4¬∞C
  Temperature: 7.4&deg;C Pressure / Tendency: 102.5 kPa rising Humidity: 92 % Dewpoint: 6.2&deg:C Wind: NW 3 km/h Air Quality Health Index: n/a . Observed at: Pembroke 6:00 AM EDT Tuesday 30 September 2025 Temperature: . Temperature: ¬†7.
‚Ä¢ Tuesday: Sunny. High 19.
  Forecast issued 5:00 AM EDT Tuesday 30 September 2025 . Sunny. Sunny. High 19.50% Sunny. 50% Sunny . High 19/30% Sunny, 50% sunny, 40% sunny . 50/50% sunny. 50/25% sunny sunny sunny, 25/50%. 50/20% sunny Tuesday, 30/30/50/50. 50
‚Ä¢ Tuesday night: Clear. Low plus 1 with patchy frost.
  Clear. Clear. Low plus 1 with patchy frost. Clear . Clear. Forecast issued 5:00 AM EDT Tuesday 30 September 2025 . Forecast forecast: Clear, clear, cold, sunny, sunny and warm . Low-freezing temperatures could be seen in the morning hours of Tuesday 30 Sept. 30 September 25, 20 Sept. 25, 25 Sept. 2015 . Fore

üåç International News
No updates.

üçÅ Canadian News
No updates.

üá∫üá∏ U.S. Top Stories
‚Ä¢ Germany thrived in the first China Shock. But the next one could prove catastrophic.
  The export-led industrial model that Germany has pursued for decades is now at a crossroads . Germany's export-driven industrial model has been hit by a series of challenges . The country's industrial model is now a 'crossroads' and a 'transport-led' model is at risk of being hit by an economic downturn . Germany has been in the middle of a recession for
‚Ä¢ Memphis and Portland, Ore. brace for troops. Why Chicago might be next
  Illinois Gov. JB Pritzker said the Department of Homeland Security has requested for 100 military personnel to help protect ICE agents and facilities in his state . The request is part of a request for 100 personnel to be deployed to protect agents in the state of Illinois . ICE agents are being held in federal prisons across the U.S. and around the world under a temporary order of temporary
‚Ä¢ 5 takeaways from the U.S. ceasefire proposal for Israel and Hamas
  Leaders of the U.S. and Israel say they have agreed to a broad plan that could end Israel's war in Gaza . But substantial uncertainties remain, including substantial uncertainties over the plan . The leaders of the two countries say they are working on a plan to end the Israeli-Gaza conflict . The plan is expected to be approved by both Israel and the United States on Monday night .
‚Ä¢ Here's what a government shutdown could affect around the country
  NPR's network of member stations explains how these effects will be felt nationwide . A federal shutdown will impact people across the U.S. NPR: The effects of the shutdown are expected to be felt across the country . NPR: How will the shutdown affect people in the United States? Please submit your story to iReport.com/report.com . Back to the page you came from
‚Ä¢ Government to shut down after midnight barring last minute breakthrough in Congress
  The government will shutdown at the end of the day on Tuesday barring a last-minute breakthrough . Democrats and Republicans have been unable to resolve an impasse over federal healthcare spending . The government is set to be shut down at end of Tuesday unless a breakthrough is reached . The shutdown is expected to last until Tuesday afternoon unless a last minute deal is reached by Republicans and Democrats to avoid it .

üß† Artificial Intelligence
No updates.

üíª Digital Strategy
‚Ä¢ 'Money-saving' UK procurement platform racks up monster tab
  Projected ¬£1.5M running costs balloon to ¬£12M under new contracts . Annual spending on procurement portal set to increase by more than eight times compared to projected plans . UK government is set to see annual spending on a procurement portal increase by eight times . Projected running costs to be more than ¬£12 million a year, compared to ¬£1 million a day in 2013 .
‚Ä¢ Britain's policing minister punts facial recog nationwide
  Met's Croydon cameras hailed as a triumph, guidance to be published later this year . Government to encourage police forces across England and Wales to adopt live facial recognition (LFR) technology . LFR technology is being used by the London's Metropolitan Police in a suburb in the south of the city . A minister has praised its use by the Met in the suburb of south London .
‚Ä¢ ¬£5.5B Bitcoin fraudster pleads guilty after years on the run
  Metropolitan Police has secured a "landmark conviction" following a record-busting Bitcoin seizure and seven-year investigation . Zhimin Qian recruited takeaway worker to launder funds through property overseas . London's Metropolitan Police secured a 'landmark' conviction after seven years of investigation . Police have secured record-breaking Bitcoin seizure, seven years' investigation and 7-year probe into launder
‚Ä¢ UK splurges ¬£4.4M on drones, e-planes, and other flights of fancy
  Taxpayer cash fuels 14 projects from NHS blood-hauling UAVs to posh eVTOL shuttles . The British government is splashing several million pounds on next-gen aviation projects . The projects will advance the use of unmanned aircraft for applications such as cargo delivery and infrastructure monitoring, as well as potential electric-powered light aircraft carrying passengers . The UK government is spending
‚Ä¢ Healthcare lags in Windows 11 upgrades ‚Äì and lives may depend on it
  Most orgs still on Windows 10, so maybe don't get ill after October 14 Interview . Enterprise plans for the end of Windows 10 should already be well underway . But some sectors are lagging, and there are other potential time bombs for administrators to worry about, according to Lansweeper.‚Ä¶‚Ä¶‚Ä¶ There are also potential time-bombing time bombs to administrators, says Lans

üè• Public Health
No updates.

üî¨ Science
‚Ä¢ Make cities more walkable, in the real world and in virtual reality
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Trends and patterns of work-related cyclist fatalities in Brazil, 2014‚Äì2022
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Public trust in science has declined since COVID ‚Äî virologists need to unite around safety standards
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Determinants of health information seeking behavior among people at risk of statelessness in Ghana
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Impact of vaccination timing and coverage on measles near elimination dynamics: a mathematical modelling analysis
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations

üßæ Government & Policy
No updates.

üèõÔ∏è Enterprise Architecture & IT Governance
No updates.

ü§ñ AI & Emerging Tech
‚Ä¢ The US may be heading toward a drone-filled future
  Police-tech giant Flock Safety is selling its drones to the private sector to track shoplifters . The company is in the middle of a federal lawsuit in Norfolk, Virginia, that alleges just that . Civil liberties advocates say it will expand the surveillance state created by police drones, license-plate readers, and other crime tech . In August, the FAA released a new proposed rule to allow drone operators to fly beyond visual line of sight .
‚Ä¢ Scientists can see Earth‚Äôs permafrost thawing from space
  Something is rotten in the city of Nunapitchuk. In recent years, a crack has formed in the middle of a house. Sewage has leached into the earth. Soil has eroded around buildings, leaving them perched atop precarious lumps of dirt. There are eternal puddles. And mold. The ground can feel squishy, sodden.&nbsp;



This small town in northern Alaska is experiencing a sometimes overlooked consequence of climate change: thawing permafrost. And Nunapitchuk is far from the only Arctic town to find itself in such a predicament.&nbsp;



Permafrost, which lies beneath about 15% of the land in the Northern Hemisphere, is defined as ground that has remained frozen for at least two years. Historically, much of the world‚Äôs permafrost has remained solid and stable for far longer, allowing people to build whole towns atop it. But as the planet warms, a process that is happening more rapidly near the poles than at more temperate latitudes, permafrost is thawing and causing a host of infrastructural and environmental problems.



Now scientists think they may be able to use satellite data to delve deep beneath the ground‚Äôs surface and get a better understanding of how the permafrost thaws, and which areas might be most severely affected because they had more ice to start with. Clues from the short-term behavior of those especially icy areas, seen from space, could portend future problems.



Using information gathered both from space and on the ground, they are working with affected communities to anticipate whether a house‚Äôs foundation will crack‚Äîand whether it is worth mending that crack or is better to start over in a new house on a stable hilltop. These scientists‚Äô permafrost predictions are already helping communities like Nunapitchuk make those tough calls.



But it‚Äôs not just civilian homes that are at risk. One of the top US intelligence agencies, the National Geospatial-Intelligence Agency (NGA), is also interested in understanding permafrost better. That‚Äôs because the same problems that plague civilians in the high north also plague military infrastructure, at home and abroad. The NGA is, essentially, an organization full of space spies‚Äîpeople who analyze data from surveillance satellites and make sense of it for the country‚Äôs national security apparatus.&nbsp;



Understanding the potential instabilities of the Alaskan military infrastructure‚Äîwhich includes radar stations that watch for intercontinental ballistic missiles, as well as military bases and National Guard posts‚Äîis key to keeping those facilities in good working order and planning for their strengthened future. Understanding the potential permafrost weaknesses that could affect the infrastructure of countries like Russia and China, meanwhile, affords what insiders might call ‚Äúsituational awareness‚Äù about competitors.&nbsp;



The work to understand this thawing will only become more relevant, for civilians and their governments alike, as the world continues to warm.&nbsp;



The ground beneath



If you live much below the Arctic Circle, you probably don‚Äôt think a lot about permafrost. But it affects you no matter where you call home.



In addition to the infrastructural consequences for real towns like Nunapitchuk, thawing permafrost contains sequestered carbon‚Äîtwice as much as currently inhabits the atmosphere. As the permafrost thaws, the process can release greenhouse gases into the atmosphere. That release can cause a feedback loop: Warmer temperatures thaw permafrost, which releases greenhouse gases, which warms the air more, which then‚Äîyou get it.&nbsp;



The microbes themselves, along with previously trapped heavy metals, are also set dangerously free.



For many years, researchers‚Äô primary options for understanding some of these freeze-thaw changes involved hands-on, on-the-ground surveys. But in the late 2000s, Kevin Schaefer, currently a senior scientist at the Cooperative Institute for Research in Environmental Sciences at the University of Colorado Boulder, started to investigate a less labor-intensive idea: using radar systems aboard satellites to survey the ground beneath.&nbsp;



This idea implanted itself in his brain in 2009, when he traveled to a place called Toolik Lake, southwest of the oilfields of Prudhoe Bay in Alaska. One day, after hours of drilling sample cores out of the ground to study permafrost, he was relaxing in the Quonset hut, chatting with colleagues. They began to discuss how&nbsp; space-based radar could potentially detect how the land sinks and heaves back up as temperatures change.&nbsp;



Huh, he thought. Yes, radar probably could do that.&nbsp;



Scientists call the ground right above permafrost the active layer. The water in this layer of soil contracts and expands with the seasons: during the summer, the ice suffusing the soil melts and the resulting decrease in volume causes the ground to dip. During the winter, the water freezes and expands, bulking the active layer back up. Radar can help measure that height difference, which is usually around one to five centimeters.&nbsp;



Schaefer realized that he could use radar to measure the ground elevation at the start and end of the thaw. The electromagnetic waves that bounce back at those two times would have traveled slightly different distances. That difference would reveal the tiny shift in elevation over the seasons and would allow him to estimate how much water had thawed and refrozen in the active layer and how far below the surface the thaw had extended.



With radar, Schaefer realized, scientists could cover a lot more literal ground, with less effort and at lower cost.





‚ÄúIt took us two years to figure out how to write a paper on it,‚Äù he says; no one had ever made those measurements before. He and colleagues presented the idea at the 2010 meeting of the American Geophysical Union and published a paper in 2012 detailing the method, using it to estimate the thickness of the active layer on Alaska‚Äôs North Slope.



When they did, they helped start a new subfield that grew as large-scale data sets started to become available around 5 to 10 years ago, says Roger Michaelides, a geophysicist at Washington University in St. Louis and a collaborator of Schaefer‚Äôs. Researchers‚Äô efforts were aided by the growth in space radar systems and smaller, cheaper satellites.&nbsp;



With the availability of global data sets (sometimes for free, from government-run satellites like the European Space Agency‚Äôs Sentinel) and targeted observations from commercial companies like Iceye, permafrost studies are moving from bespoke regional analyses to more automated, large-scale monitoring and prediction.



The remote view



Simon Zwieback, a geospatial and environmental expert at the University of Alaska Fairbanks, sees the consequences of thawing permafrost firsthand every day. His office overlooks a university parking lot, a corner of which is fenced off to keep cars and pedestrians from falling into a brand-new sinkhole. That area of asphalt had been slowly sagging for more than a year, but over a week or two this spring, it finally started to collapse inward.&nbsp;



Kevin Schaefer stands on top of a melting layer of ice near the Alaskan pipeline on the North Slope of Alaska.COURTESY OF KEVIN SCHAEFER




The new remote research methods are a large-scale version of Zwieback taking in the view from his window. Researchers look at the ground and measure how its height changes as ice thaws and refreezes. The approach can cover wide swaths of land, but it involves making assumptions about what‚Äôs going on below the surface‚Äînamely, how much ice suffuses the soil in the active layer and permafrost. Thawing areas with relatively low ice content could mimic thinner layers with more ice. And it‚Äôs important to differentiate the two, since more ice in the permafrost means more potential instability.&nbsp;



To check that they‚Äôre on the right track, scientists have historically had to go out into the field. But a few years ago, Zwieback started to explore a way to make better and deeper estimates of ice content using the available remote sensing data. Finding a way to make those kinds of measurements on a large scale was more than an academic exercise: Areas of what he calls ‚Äúexcess ice‚Äù are most liable to cause instability at the surface. ‚ÄúIn order to plan in these environments, we really need to know how much ice there is, or where those locations are that are rich in ice,‚Äù he says.





Zwieback, who did his undergraduate and graduate studies in Switzerland and Austria, wasn‚Äôt always so interested in permafrost, or so deeply affected by it. But in 2014, when he was a doctoral student in environmental engineering, he joined an environmental field campaign in Siberia, at the Lena River Delta, which resembles a gigantic piece of coral fanning out into the Arctic Ocean. Zwieback was near a town called Tiksi, one of the world‚Äôs northernmost settlements. It‚Äôs a military outpost and starting point for expeditions to the North Pole, featuring an abandoned plane near the ocean. Its Soviet-era concrete buildings sometimes bring it to the front page of the r/UrbanHell subreddit.&nbsp;



Here, Zwieback saw part of the coastline collapse, exposing almost pure ice. It looked like a subterranean glacier, but it was permafrost. ‚ÄúThat really had an indelible impact on me,‚Äù he says.&nbsp;



Later, as a doctoral student in Zurich and postdoc in Canada, he used his radar skills to understand the rapid changes that the activity of permafrost impressed upon the landscape.&nbsp;



And now, with his job in Fairbanks and his ideas about the use of radar sensing, he has done work funded by the NGA, which has an open Arctic data portal.&nbsp;



In his Arctic research, Zwieback started with the approach underlying most radar permafrost studies: looking at the ground‚Äôs seasonal subsidence and heave. ‚ÄúBut that‚Äôs something that happens very close to the surface,‚Äù he says. ‚ÄúIt doesn‚Äôt really tell us about these long-term destabilizing effects,‚Äù he adds.



In warmer summers, he thought, subtle clues would emerge that could indicate how much ice is buried deeper down.



For example, he expected those warmer-than-average periods to exaggerate the amount of change seen on the surface, making it easier to tell which areas are ice-rich. Land that was particularly dense with ice would dip more than it ‚Äúshould‚Äù‚Äîa precursor of bigger dips to come.



The first step, then, was to measure subsidence directly, as usual. But from there, Zwieback developed an algorithm to ingest data about the subsidence over time‚Äîas measured by radar‚Äîand other environmental information, like the temperatures at each measurement. He then created a digital model of the land that allowed him to adjust the simulated amount of ground ice and determine when it matched the subsidence seen in the real world. With that, researchers could infer the amount of ice beneath.



Next, he made maps of that ice that could potentially be useful to engineers‚Äîwhether they were planning a new subdivision or, as his funders might be, keeping watch on a military airfield.



‚ÄúWhat was new in my work was to look at these much shorter periods and use them to understand specific aspects of this whole system, and specifically how much ice there is deep down,‚Äù Zwieback says.&nbsp;



The NGA, which has also funded Schaefer‚Äôs work, did not respond to an initial request for comment but did later provide feedback for fact-checking. It removed an article on its website about Zwieback‚Äôs grant and its application to agency interests around the time that the current presidential administration began to ban mention of climate change in federal research. But the thawing earth is of keen concern.&nbsp;



To start, the US has significant military infrastructure in Alaska: It‚Äôs home to six military bases and 49 National Guard posts, as well as 21 missile-detecting radar sites. Most are vulnerable to thaw now or in the near future, given that 85% of the state is on permafrost.&nbsp;



Beyond American borders, the broader north is in a state of tension. Russia‚Äôs relations with Northern Europe are icy. Its invasion of Ukraine has left those countries fearing that they too could be invaded, prompting Sweden and Finland, for instance, to join NATO. The US has threatened takeovers of Greenland and Canada. And China‚Äîwhich has shipping and resource ambitions for the region‚Äîis jockeying to surpass the US as the premier superpower.&nbsp;



Permafrost plays a role in the situation. ‚ÄúAs knowledge has expanded, so has the understanding that thawing permafrost can affect things NGA cares about, including the stability of infrastructure in Russia and China,‚Äù read the NGA article. Permafrost covers 60% of Russia, and thaws have affected more than 40% of buildings in northern Russia already, according to statements from the country‚Äôs minister of natural resources in 2021. Experts say critical infrastructure like roads and pipelines is at risk, along with military installations. That could weaken both Russia‚Äôs strategic position and the security of its residents. In China, meanwhile, according to a report from the Council on Strategic Risks, important moving parts like the Qinghai-Tibet Railway, ‚Äúwhich allows Beijing to more quickly move military personnel near contested areas of the Indian border,‚Äù is susceptible to ground thaw‚Äîas are oil and gas pipelines linking Russia and China.&nbsp;



In the field



Any permafrost analysis that relies on data from space requires verification on Earth. The hope is that remote methods will become reliable enough to use on their own, but while they‚Äôre being developed, researchers must still get their hands muddy with more straightforward and longer tested physical methods. Some use a network called Circumpolar Active Layer Monitoring, which has existed since 1991, incorporating active-layer data from hundreds of measurement sites across the Northern Hemisphere.&nbsp;



Sometimes, that data comes from people physically probing an area; other sites use tubes permanently inserted into the ground, filled with a liquid that indicates freezing; still others use underground cables that measure soil temperature. Some researchers, like Schaefer, lug ground-penetrating radar systems around the tundra. He‚Äôs taken his system to around 50 sites and made more than 200,000 measurements of the active layer.



The field-ready ground-penetrating radar comes in a big box‚Äîthe size of a steamer trunk‚Äîthat emits radio pulses. These pulses bounce off the bottom of the active layer, or the top of the permafrost. In this case, the timing of that reflection reveals how thick the active layer is. With handles designed for humans, Schaefer‚Äôs team drags this box around the Arctic‚Äôs boggier areas.&nbsp;



The box floats. ‚ÄúI do not,‚Äù he says. He has vivid memories of tromping through wetlands, his legs pushing straight down through the muck, his body sinking up to his hips.



Andy Parsekian and Kevin Schaefer haul a ground penetrating radar unit through the tundra near Utqiagvik.COURTESY OF KEVIN SCHAEFER




Zwieback also needs to verify what he infers from his space data. And so in 2022, he went to the Toolik Field station, a National Science Foundation‚Äìfunded ecology research facility along the Dalton Highway and adjacent to Schaefer‚Äôs Toolik Lake. This road, which goes from Fairbanks up to the Arctic Ocean, is colloquially called the Haul Road; it was made famous in the TV show Ice Road Truckers. From this access point, Zwieback‚Äôs team needed to get deep samples of soil whose ice content could be analyzed in the lab.



Every day, two teams would drive along the Dalton Highway to get close to their field sites. Slamming their car doors, they would unload and hop on snow machines to travel the final distance. Often they would see musk oxen, looking like bison that never cut their hair. The grizzlies were also interested in these oxen, and in the nearby caribou.&nbsp;



At the sites they could reach, they took out a corer, a long, tubular piece of equipment driven by a gas engine, meant to drill deep into the ground. Zwieback or a teammate pressed it into the earth. The barrel‚Äôs two blades rotated, slicing a cylinder about five feet down to ensure that their samples went deep enough to generate data that can be compared with the measurements made from space. Then they pulled up and extracted the cylinder, a sausage of earth and ice.



All day every day for a week, they gathered cores that matched up with the pixels in radar images taken from space. In those cores, the ice was apparent to the eye. But Zwieback didn‚Äôt want anecdata. ‚ÄúWe want to get a number,‚Äù he says.



So he and his team would pack their soil cylinders back to the lab. There they sliced them into segments and measured their volume, in both their frozen and their thawed form, to see how well the measured ice content matched estimates from the space-based algorithm.&nbsp;





The initial validation, which took months, demonstrated the value of using satellites for permafrost work. The ice profiles that Zwieback‚Äôs algorithm inferred from the satellite data matched measurements in the lab down to about 1.1 feet, and farther in a warm year, with some uncertainty near the surface and deeper into the permafrost.&nbsp;



Whereas it cost tens of thousands of dollars to fly in on a helicopter, drive in a car, and switch to a snowmobile to ultimately sample a small area using your hands, only to have to continue the work at home, the team needed just a few hundred dollars to run the algorithm on satellite data that was free and publicly available.&nbsp;



Michaelides, who is familiar with Zwieback‚Äôs work, agrees that estimating excess ice content is key to making infrastructural decisions, and that historical methods of sussing it out have been costly in all senses. Zwieback‚Äôs method of using late-summer clues to infer what‚Äôs going on at that depth ‚Äúis a very exciting idea,‚Äù he says, and the results ‚Äúdemonstrate that there is considerable promise for this approach.‚Äù&nbsp;



He notes, though, that using space-based radar to understand the thawing ground is complicated: Ground ice content, soil moisture, and vegetation can differ even within a single pixel that a satellite can pick out. ‚ÄúTo be clear, this limitation is not unique to Simon&#8217;s work,‚Äù Michaelides says; it affects all space-radar methods. There is also excess ice below even where Zwieback‚Äôs algorithm can probe‚Äîsomething the labor-intensive on-ground methods can pick up that still can‚Äôt be seen from space.&nbsp;



Mapping out the future



After Zwieback did his fieldwork, NGA decided to do its own. The agency‚Äôs attempt to independently validate his work‚Äîin Prudhoe Bay, Utqiagvik, and Fairbanks‚Äîwas part of a project it called Frostbyte.&nbsp;



Its partners in that project‚Äîthe Army‚Äôs Cold Regions Research Engineering Laboratory and Los Alamos National Laboratory‚Äîdeclined requests for interviews. As far as Zwieback knows, they‚Äôre still analyzing data.&nbsp;



But the intelligence community isn‚Äôt the only group interested in research like Zwieback‚Äôs. He also works with Arctic residents, reaching out to rural Alaskan communities where people are trying to make decisions about whether to relocate or where to build safely. ‚ÄúThey typically can‚Äôt afford to do expensive coring,‚Äù he says. ‚ÄúSo the idea is to make these data available to them.‚Äù&nbsp;



Zwieback and his team haul their gear out to gather data from drilled core samples, a process which can be arduous  and costly.ANDREW JOHNSON




Schaefer is also trying to bridge the gap between his science and the people it affects. Through a company called Weather Stream, he is helping communities identify risks to infrastructure before anything collapses, so they can take preventative action.



Making such connections has always been a key concern for Erin Trochim, a geospatial scientist at the University of Alaska Fairbanks. As a researcher who works not just on permafrost but also on policy, she‚Äôs seen radar science progress massively in recent years‚Äîwithout commensurate advances on the ground.



For instance, it‚Äôs still hard for residents in her town of Fairbanks‚Äîor anywhere‚Äîto know if there‚Äôs permafrost on their property at all, unless they‚Äôre willing to do expensive drilling. She‚Äôs encountered this problem, still unsolved, on property she owns. And if an expert can‚Äôt figure it out, non-experts hardly stand a chance. ‚ÄúIt‚Äôs just frustrating when a lot of this information that we know from the science side, and [that‚Äôs] trickled through the engineering side, hasn‚Äôt really translated into the on-the-ground construction,‚Äù she says.&nbsp;



There is a group, though, trying to turn that trickle into a flood: Permafrost Pathways, a venture that launched with a $41 million grant through the TED Audacious Project. In concert with affected communities, including Nunapitchuk, it is building a data-gathering network on the ground, and combining information from that network with satellite data and local knowledge to help understand permafrost thaw and develop adaptation strategies.&nbsp;



‚ÄúI think about it often as if you got a diagnosis of a disease,‚Äù says Sue Natali, the head of the project. ‚ÄúIt‚Äôs terrible, but it‚Äôs also really great, because when you know what your problem is and what you‚Äôre dealing with, it‚Äôs only then that you can actually make a plan to address it.‚Äù&nbsp;



And the communities Permafrost Pathways works with are making plans. Nunapitchuk has decided to relocate, and the town and the research group have collaboratively surveyed the proposed new location: a higher spot on hardpacked sand. Permafrost Pathways scientists were able to help validate the stability of the new site‚Äîand prove to policymakers that this stability would extend into the future.&nbsp;



Radar helps with that in part, Natali says, because unlike other satellite detectors, it penetrates clouds. ‚ÄúIn Alaska, it‚Äôs extremely cloudy,‚Äù she says. ‚ÄúSo other data sets have been very, very challenging. Sometimes we get one image per year.‚Äù



And so radar data, and algorithms like Zwieback‚Äôs that help scientists and communities make sense of that data, dig up deeper insight into what‚Äôs going on beneath northerners‚Äô feet‚Äîand how to step forward on firmer ground.&nbsp;



Sarah Scoles is a freelance science journalist based in southern Colorado and the author, most recently, of the book Countdown: The Blinding Future of Nuclear Weapons.
‚Ä¢ Delivering a digital sixth sense with next-generation networks
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ The Download: AI to detect child abuse images, and what to expect from our 2025 Climate Tech Companies to Watch list
  This is today&#8217;s edition of¬†The Download,¬†our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



US investigators are using AI to detect child abuse images made by AIGenerative AI has enabled the production of child sexual abuse images to skyrocket. Now the leading investigator of child exploitation in the US is experimenting with using AI to distinguish AI-generated images from material depicting real victims, according to a new government filing.



The Department of Homeland Security‚Äôs Cyber Crimes Center, which investigates child exploitation across international borders, has awarded a $150,000 contract to San Francisco‚Äìbased Hive AI for its software, which can identify whether a piece of content was AI-generated. Read the full story.



‚ÄîJames O&#8217;Donnell







Coming soon: our 2025 list of Climate Tech Companies to Watch



The need to cut emissions and adapt to our warming world is growing more urgent. This year, we‚Äôve seen temperatures reach record highs, as they have nearly every year for the last decade. Climate-fueled natural disasters are affecting communities around the world, costing billions of dollars.¬†



That‚Äôs why, for the past two years, MIT Technology Review has curated a list of companies with the potential to make a meaningful difference in addressing climate change (you can revisit the 2024 list here). We‚Äôre excited to share that we‚Äôll publish our third edition of Climate Tech Companies to Watch on October 6. Here‚Äôs what you can expect from this year‚Äôs list.



‚ÄîCasey Crownhart







The must-reads



I‚Äôve combed the internet to find you today‚Äôs most fun/important/scary/fascinating stories about technology.



1 ChatGPT‚Äôs parental controls are now liveThe model can now alert parents and law enforcement when users under 18-years old discuss self harm or suicide. (Wired $)+ The feature launches as chatbot makers face increasing pressure to improve safety. (Bloomberg $)+ The looming crackdown on AI companionship. (MIT Technology Review)



2 Companies‚Äô AI spending is spiralling out of controlAnd it‚Äôs unclear whether they‚Äôll ever get returns on their investments. (WSJ $)+ Some VCs are convinced AI is the best way to make a quick buck. (TechCrunch)+ Investors are wondering what to invest in beyond AI. (Reuters)



3 Even oil executives are worried by Trump‚Äôs attacks on offshore windCracking down on renewables now is bad news for traditional energy down the line. (NYT $)+ The scale of the Trump administration‚Äôs intervention is wild. (The Guardian)+ How to make clean energy progress under Trump in the states. (MIT Technology Review)



4 How America is winning the war on city firesMaking homes fire-resistant isn‚Äôt glamorous‚Äîbut it‚Äôs essential. (Vox)+ How AI can help spot wildfires. (MIT Technology Review)



5 Top AI firms are going all-in on world modelsThey‚Äôre powered by videos and robotics data, not just language. (FT $)+ Experts are convinced they‚Äôre vital to creating the next wave of AI. (WSJ $)



6 China is rushing to electrify freight trucksNot content with dominating the electric car market, it‚Äôs eyeing bigger vehicles. (Rest of World)+ Sales of battery-powered cars are projected to plummet in the US. (NYT $)



7 What we lose when we rely on AI translationNuance and cultural context are among the first casualties. (WP $)+ How AI and Wikipedia have sent vulnerable languages into a doom spiral. (MIT Technology Review)



8 The tricky ethics of gene-editing the natural worldJust because we can, doesn‚Äôt mean we should. (Aeon)+ The short, strange history of gene de-extinction. (MIT Technology Review)



9 This robotics firm uses AI to clean the underside of giant ships Neptune Robotics has the lofty goal of becoming Uber for hull-cleaning. (Bloomberg $)10 Talent agents are desperate to sign this AI actressWe are living in the end times. (Deadline $)+ Why the Twin Peaks subreddit briefly became an AI slop dumping ground. (404 Media)+ An AI-powered hologram of Marvel Comics‚Äô creator Stan Lee isn‚Äôt very popular. (Ars Technica)







Quote of the day



‚ÄúThere‚Äôs so much pressure to be the company that went from zero to $100 million in X days.‚Äù



‚ÄîAn anonymous VC tells Fortune about the intense pressure on startups in the age of AI hype.







One more thing







How DeepSeek became a fortune teller for China‚Äôs youthAs DeepSeek has emerged as a homegrown challenger to OpenAI, young people across China have started using AI to revive fortune-telling practices that have deep roots in Chinese culture.



People are sharing AI-generated readings, experimenting with fortune-telling prompt engineering, and revisiting ancient spiritual texts‚Äîall with the help of DeepSeek.The surge in AI fortune-telling comes during a time of pervasive anxiety and pessimism in Chinese society. And as spiritual practices remain hidden underground thanks to the country‚Äôs regime, computers and phone screens are helping younger people to gain a sense of control over their lives. Read the full story.



‚ÄîCaiwen Chen







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)



+ This site lets you find out what was happening in the news on the day you were born.¬†+ This blistering track by South Korean band Silica Gel confirms rock is alive and well.+¬† Spend a few minutes exploring Hieronymus Bosch‚Äôs Garden of Earthly Delights.+ Play around with this map that allows you to explore movie settings around the world.
‚Ä¢ Coming soon: Our 2025 list of Climate Tech Companies to Watch
  MIT Technology Review has curated a list of companies with the potential to make a meaningful difference in addressing climate change . This year marks 10 years since the Paris Agreement, the UN treaty that aimed to limit global warming by setting a goal of cutting emissions so that temperatures would rise no more than 1.5 ¬∞C above preindustrial temperatures . The companies on this year‚Äôs list are inventing and scaling technologies that could help .

üîí Cybersecurity & Privacy
No updates.

üéì University AI
No updates.

üè¢ Corporate AI
‚Ä¢ Building health care agents using Amazon Bedrock AgentCore
  This blog was co-authored with Kuldeep Singh, Head of AI Platform at Innovaccer. 
The integration of agentic AI is ushering in a transformative era in health care, marking a significant departure from traditional AI systems. Agentic AI demonstrates autonomous decision-making capabilities and adaptive learning in complex medical environments, enabling it to monitor patient progress, coordinate care teams, and adjust treatment strategies in real time. These intelligent systems are becoming deeply embedded in healthcare operations, from enhancing diagnostic precision through advanced pattern recognition to optimizing clinical workflows and accelerating drug discovery processes. Agentic AI combines proactive problem-solving abilities with real-time adaptability so that healthcare professionals can focus on high-value, patient-centered activities while the AI handles routine tasks and complex data analysis. 
Innovaccer, a pioneering healthcare AI company, recently launched Innovaccer Gravity, built using Amazon Bedrock AgentCore, a new healthcare intelligence platform set to revolutionize data integration and AI-driven healthcare transformation. Building on their impressive track record‚Äîwhere their existing solutions serve more than 1,600 US care locations, manage more than 80 million unified health records, and have generated $1.5B in cost savings‚Äîthis exemplifies how AWS customers are leading the agentic AI evolution by creating intelligent solutions that transform healthcare delivery while delivering significant ROI. 
Health care demands precision and accountability. AI agents operating within this domain must handle sensitive patient data securely, adhere to rigorous compliance regulations (like HIPAA), and maintain consistent interoperability across diverse clinical workflows. Standard, generalized protocols fall short when dealing with complex healthcare systems and patient data protection requirements. Healthcare organizations need a robust service to convert their existing APIs into Model Context Protocol (MCP) compatible tools that can scale effectively while providing built-in authentication, authorization, encryption, and comprehensive audit trails. Amazon Bedrock AgentCore Gateway offers health care providers and digital health companies a straightforward and secure way to build, deploy, discover, and connect to tools at scale that they can use to create AI-powered healthcare solutions while maintaining the highest standards of security and compliance. 
Problem 
Healthcare organizations face significant data silo challenges because of diverse electronic health record (EHR) formats across different systems, often maintaining multiple systems to serve specialized departmental needs and legacy systems. FHIR (Fast Healthcare Interoperability Resources) solves these interoperability challenges by standardizing healthcare data into exchangeable resources (like patient records and lab results), enabling seamless communication between different systems while maintaining security and improving care coordination. However, implementing FHIR presents its own challenges, including technical complexity in integrating with legacy systems and the need for specialized expertise in healthcare informatics and API development. 
The implementation of AI agents introduces new layers of complexity, requiring careful design and maintenance of interfaces with existing systems. AI agents need secure access to the FHIR data and other healthcare tools with authentication (both inbound and outbound) and end-to-end encryption. MCP is a standardized communication framework that enables AI systems to seamlessly interact with external tools, data sources, and services through a unified interface. However, the development and scaling of MCP servers require substantial resources and expertise. Hosting these services demands ongoing development time and attention to maintain optimal performance and reliability. As healthcare organizations navigate this complex terrain, addressing these challenges becomes critical for achieving true interoperability and harnessing the full potential of modern healthcare technology. 
Deploy, enhance, and monitor AI agents at scale using Amazon Bedrock AgentCore 
By using Amazon Bedrock AgentCore, you can deploy and operate highly capable AI agents securely at scale. It offers infrastructure purpose-built for dynamic agent workloads, powerful tools to enhance agents, and essential controls for real-world deployment. Bedrock AgentCore offers a set of composable services with the services most relevant to the solution in this post mentioned in the following list. For more information, see the Bedrock AgentCore documentation. 
 
 AgentCore Runtime provides a secure, serverless runtime purpose-built for deploying and scaling dynamic AI agents and tools using any open source framework, protocol, and model. Runtime was built to work for agentic workloads with industry-leading extended runtime support, fast cold starts, true session isolation, built-in identity, and support for multi-modal payloads. 
 AgentCore Gateway provides a secure way for agents to discover and use tools along with straightforward transformation of APIs, AWS Lambda functions, and existing services into agent-compatible tools. Gateway speeds up custom code development, infrastructure provisioning, and security implementation so developers can focus on building innovative agent applications. 
 AgentCore Identity provides a secure, scalable agent identity and access management capability accelerating AI agent development. It is compatible with existing identity providers, avoiding the need to migrate uses or rebuild authentication flows. 
 AgentCore Observability helps developers trace, debug, and monitor agent performance in production through unified operational dashboards. With support for OpenTelemetry compatible telemetry and detailed visualizations of each step of the agent workflow. 
 
In this solution, we demonstrate how the user (a parent) can interact with a Strands or LangGraph agent in conversational style and get information about the immunization history and schedule of their child, inquire about the available slots, and book appointments. With some changes, AI agents can be made event-driven so that they can automatically send reminders, book appointments, and so on. This reduces the administrative burden on healthcare organizations and the parents who no longer need to keep track of the paperwork or make multiple calls to book appointments. 
 
As shown in the preceding diagram, the workflow for the healthcare appointment book built using Amazon Bedrock AgentCore is the following: 
 
 User interacts with Strands or LangGraph agent: The solution contains both Strands and LangGraph agents. You can also use other frameworks such as AutoGen and CrewAI. 
 Reasoning LLM from Amazon Bedrock: Claude 3.5 Sonnet large language model (LLM) is used from Amazon Bedrock. The model demonstrates advanced reasoning by grasping nuances and complex instructions, along with strong tool-calling capabilities that allow it to effectively integrate with external applications and services to automate various tasks such as web browsing, calculations, or data interactions. 
 Tools exposed using AgentCore Gateway: AgentCore Gateway provides secure access to the necessary tools required for the Strands or LangGraph agent using standard MCP clients. In this solution, REST APIs are hosted on Amazon API Gateway and exposed as MCP tools using AgentCore Gateway. 
 Ingress authentication for AgentCore Gateway: AgentCore Gateway is protected with oAuth 2.0 using Amazon Cognito as the identity provider. You can use other oAuth 2.0 compatible identity providers such as Auth0, and Keycloak as needed to fit your use case. 
 OpenAPI specs converted into tools with AgentCore Gateway: Amazon API Gateway is used as the backend to expose the APIs. By importing the OpenAPI specs, AgentCore Gateway provides an MCP compatible server without additional configuration for tool metadata. The following are the tools used in the solution. 
   
   get_patient_emr(): Gets the parent‚Äôs and child‚Äôs demographics information. 
   search_immunization_emr() ‚Äì Gets the immunization history and schedule for the child. 
   get_available_slots() ‚Äì Gets the pediatrician‚Äôs schedule around parent‚Äôs preferred date. 
   book_appointment() ‚Äì Books an appointment and returns the confirmation number. 
    
 AWS Healthlake as the FHIR server: HealthLake is used to manage patient data related to demographics, immunization history, schedule and appointments, and so on. HealthLake is a HIPAA-eligible service offering healthcare companies a complete view of individual and patient population health data using FHIR API-based transactions to securely store and transform their data into a queryable format at petabyte scale, and further analyze this data using machine learning (ML) models. 
 Egress authentication from AgentCore Gateway to tools: OAuth 2.0 with Amazon Cognito as the identity provider is used to do the authentication between AgentCore Gateway and the tools used in the solution. 
 
Solution setup 
 
  
   
   Important: The following code example is meant for learning and demonstration purposes only. For production implementations, it is recommended to add required error handling, input validation, logging, and security controls. 
   
  
 
The code and instructions to set up and clean up this example solution are available on GitHub. When set up, the solution looks like the following and is targeted towards parents to use the for immunization related appointments. 
 
Customizing the solution 
The solution can be customized to extend the same or a different use case through the following mechanisms: 
 
 OpenAPI specification: The solution uses a sample OpenAPI specification (named fhir-openapi-spec.yaml) with APIs hosted on API Gateway. The OpenAPI specification can be customized to add more tools or use entirely different tools by editing the YAML file. You must recreate the AgentCore gateway after making changes to the OpenAPI spec. 
 Agent instructions and LLM: The strands_agent.py or langgraph_agent.py can be modified to make changes to the goal or instructions for the Agent or to work with a different LLM. 
 
Future enhancements 
We‚Äôre already looking forward and planning future enhancements for this solution. 
 
 AgentCore Runtime: Host strands or a LangGraph agent on AgentCore Runtime. 
 AgentCore Memory: Use AgentCore Memory to preserve session information in short-term (in session) as well as long-term (across sessions) to provide a more personalized experience to the agent users. 
 
Innovaccer‚Äôs use case for Bedrock AgentCore 
Innovaccer‚Äôs gravity platform includes more than 400 connectors to unify data from EHRs from sources such as Epic, Oracle Cerner, and MEDITECH, more than 20 pre-trained models, 15 pre-built AI agents, 100 FHIR resources, and 60 out-of-the-box solutions with role based access control, comprehensive audit trail, end-to-end encryption, and secure personal health information (PHI) handling. They also provide a low-code or no-code interface to build additional AI agents with the tools exposed using Healthcare Model Context Protocol (HMCP) servers. 
Innovaccer uses Bedrock AgentCore for the following purposes: 
 
 AgentCore Gateway to turn their OpenAPI specifications into HMCP compatible tools without the heavy lifting required to build, secure, or scale MCP servers. 
 AgentCore Identity to handle the inbound and outbound authentication integrating with Innovaccer- or customer-provided OAuth servers. 
 AgentCore Runtime to deploy and scale the AI agents with multi-agent collaboration, along with logging, traceability and ability to plug in custom guardrails. 
 
Bedrock AgentCore supports enterprise-grade security with encryption in transit and at rest, complete session isolation, audit trails using AWS CloudTrail, and comprehensive controls to help Innovaccer agents operate reliably and securely at scale. 
Pricing for Bedrock AgentCore Gateway: 
AgentCore Gateway offers a consumption-based pricing model with billing based on API invocations (such as ListTools, InvokeTool and Search API), and indexing of tools. For more information, see the pricing page. 
Conclusion 
The integration of Amazon Bedrock AgentCore with healthcare systems represents a significant leap forward in the application of AI to improve patient care and streamline healthcare operations. By using the suite of services provided by Bedrock AgentCore, healthcare organizations can deploy sophisticated AI agents that securely interact with existing systems, adhere to strict compliance standards, and scale efficiently. 
The solution architecture presented in this post demonstrates the practical application of these technologies, showcasing how AI agents can simplify complex processes such as immunization scheduling and appointment booking. This can reduce administrative burdens on healthcare providers and enhance the patient experience by providing straightforward access to critical health information and services. 
As we look to the future, the potential for AI agents in the healthcare industry is vast. From improving diagnostic accuracy to personalizing treatment plans and streamlining clinical workflows, the possibilities are endless. Tools like Amazon Bedrock AgentCore can help healthcare organizations confidently navigate the complexities of implementing AI while maintaining the highest standards of security, compliance, and patient care. 
The healthcare industry stands at the cusp of a transformative era, where AI agents will play an increasingly central role in delivering efficient, personalized, and high-quality care. By embracing these technologies and continuing to innovate, we can create a healthcare network that is more responsive, intelligent, and patient-centric than ever before. 
 
About the Authors 
Kamal Manchanda is a Senior Solutions Architect at AWS with 17 years of experience in cloud, data, and AI technologies. He works closely with C-level executives and technical teams of AWS customers to drive cloud adoption and digital transformation initiatives. Prior to AWS, he led global teams delivering cloud-centric systems, data-driven applications, and AI/ML solutions across consulting and product organizations. Kamal specializes in translating complex business challenges into scalable, secure solutions that deliver measurable business value. 
Kuldeep Singh is AVP and Head of AI Platform at Innovaccer. He leads the work on AI agentic workflow layers for Gravity by Innovaccer, a healthcare intelligence platform designed to unify data, agents, and compliant workflows so health systems can deploy AI at scale. With deep experience in data engineering, AI, and product leadership, Kuldeep focuses on making healthcare more efficient, safe, and patient-centered. He plays a key role in building tools that allow care teams to automate complex, multi-step tasks (like integrating payer or EHR data, orchestrating clinical agents) without heavy engineering. He‚Äôs passionate about reducing clinician burnout, improving patient outcomes, and turning pilot projects into enterprise-wide AI solutions.
‚Ä¢ Build multi-agent site reliability engineering assistants with Amazon Bedrock AgentCore
  Site reliability engineers (SREs) face an increasingly complex challenge in modern distributed systems. During production incidents, they must rapidly correlate data from multiple sources‚Äîlogs, metrics, Kubernetes events, and operational runbooks‚Äîto identify root causes and implement solutions. Traditional monitoring tools provide raw data but lack the intelligence to synthesize information across these diverse systems, often leaving SREs to manually piece together the story behind system failures. 
With a generative AI solution, SREs can ask their infrastructure questions in natural language. For example, they can ask ‚ÄúWhy are the payment-service pods crash looping?‚Äù or ‚ÄúWhat‚Äôs causing the API latency spike?‚Äù and receive comprehensive, actionable insights that combine infrastructure status, log analysis, performance metrics, and step-by-step remediation procedures. This capability transforms incident response from a manual, time-intensive process into a time-efficient, collaborative investigation. 
In this post, we demonstrate how to build a multi-agent SRE assistant using Amazon Bedrock AgentCore, LangGraph, and the Model Context Protocol (MCP). This system deploys specialized AI agents that collaborate to provide the deep, contextual intelligence that modern SRE teams need for effective incident response and infrastructure management. We walk you through the complete implementation, from setting up the demo environment to deploying on Amazon Bedrock AgentCore Runtime for production use. 
Solution overview 
This solution uses a comprehensive multi-agent architecture that addresses the challenges of modern SRE operations through intelligent automation. The solution consists of four specialized AI agents working together under a supervisor agent to provide comprehensive infrastructure analysis and incident response assistance. 
The examples in this post use synthetically generated data from our demo environment. The backend servers simulate realistic Kubernetes clusters, application logs, performance metrics, and operational runbooks. In production deployments, these stub servers would be replaced with connections to your actual infrastructure systems, monitoring services, and documentation repositories. 
The architecture demonstrates several key capabilities: 
 
 Natural language infrastructure queries ‚Äì You can ask complex questions about your infrastructure in plain English and receive detailed analysis combining data from multiple sources 
 Multi-agent collaboration ‚Äì Specialized agents for Kubernetes, logs, metrics, and operational procedures work together to provide comprehensive insights 
 Real-time data synthesis ‚Äì Agents access live infrastructure data through standardized APIs and present correlated findings 
 Automated runbook execution ‚Äì Agents retrieve and display step-by-step operational procedures for common incident scenarios 
 Source attribution ‚Äì Every finding includes explicit source attribution for verification and audit purposes 
 
The following diagram illustrates the solution architecture. 
 
The architecture demonstrates how the SRE support agent integrates seamlessly with Amazon Bedrock AgentCore components: 
 
 Customer interface ‚Äì Receives alerts about degraded API response times and returns comprehensive agent responses 
 Amazon Bedrock AgentCore Runtime ‚Äì Manages the execution environment for the multi-agent SRE solution 
 SRE support agent ‚Äì Multi-agent collaboration system that processes incidents and orchestrates responses 
 Amazon Bedrock AgentCore Gateway ‚Äì Routes requests to specialized tools through OpenAPI interfaces: 
   
   Kubernetes API for getting cluster events 
   Logs API for analyzing log patterns 
   Metrics API for analyzing performance trends 
   Runbooks API for searching operational procedures 
    
 Amazon Bedrock AgentCore Memory ‚Äì Stores and retrieves session context and previous interactions for continuity 
 Amazon Bedrock AgentCore Identity ‚Äì Handles authentication for tool access using Amazon Cognito integration 
 Amazon Bedrock AgentCore Observability ‚Äì Collects and visualizes agent traces for monitoring and debugging 
 Amazon Bedrock LLMs ‚Äì Powers the agent intelligence through Anthropic‚Äôs Claude large language models (LLMs) 
 
The multi-agent solution uses a supervisor-agent pattern where a central orchestrator coordinates five specialized agents: 
 
 Supervisor agent ‚Äì Analyzes incoming queries and creates investigation plans, routing work to appropriate specialists and aggregating results into comprehensive reports 
 Kubernetes infrastructure agent ‚Äì Handles container orchestration and cluster operations, investigating pod failures, deployment issues, resource constraints, and cluster events 
 Application logs agent ‚Äì Processes log data to find relevant information, identifies patterns and anomalies, and correlates events across multiple services 
 Performance metrics agent ‚Äì Monitors system metrics and identifies performance issues, providing real-time analysis and historical trending 
 Operational runbooks agent ‚Äì Provides access to documented procedures, troubleshooting guides, and escalation procedures based on the current situation 
 
Using Amazon Bedrock AgentCore primitives 
The solution showcases the power of Amazon Bedrock AgentCore by using multiple core primitives. The solution supports two providers for Anthropic‚Äôs LLMs. Amazon Bedrock supports Anthropic‚Äôs Claude 3.7 Sonnet for AWS integrated deployments, and Anthropic API supports Anthropic‚Äôs Claude 4 Sonnet for direct API access. 
The Amazon Bedrock AgentCore Gateway component converts the SRE agent‚Äôs backend APIs (Kubernetes, application logs, performance metrics, and operational runbooks) into Model Context Protocol (MCP) tools. This enables agents built with an open-source framework supporting MCP (such as LangGraph in this post) to seamlessly access infrastructure APIs. 
Security for the entire solution is provided by Amazon Bedrock AgentCore Identity. It supports ingress authentication for secure access control for agents connecting to the gateway, and egress authentication to manage authentication with backend servers, providing secure API access without hardcoding credentials. 
The serverless execution environment for deploying the SRE agent in production is provided by Amazon Bedrock AgentCore Runtime. It automatically scales from zero to handle concurrent incident investigations while maintaining complete session isolation. Amazon Bedrock AgentCore Runtime supports both OAuth and AWS Identity and Access Management (IAM) for agent authentication. Applications that invoke agents must have appropriate IAM permissions and trust policies. For more information, see Identity and access management for Amazon Bedrock AgentCore. 
Amazon Bedrock AgentCore Memory transforms the SRE agent from a stateless system into an intelligent learning assistant that personalizes investigations based on user preferences and historical context. The memory component provides three distinct strategies: 
 
 User preferences strategy (/sre/users/{user_id}/preferences) ‚Äì Stores individual user preferences for investigation style, communication channels, escalation procedures, and report formatting. For example, Alice (a technical SRE) receives detailed systematic analysis with troubleshooting steps, whereas Carol (an executive) receives business-focused summaries with impact analysis. 
 Infrastructure knowledge strategy (/sre/infrastructure/{user_id}/{session_id}) ‚Äì Accumulates domain expertise across investigations, enabling agents to learn from past discoveries. When the Kubernetes agent identifies a memory leak pattern, this knowledge becomes available for future investigations, enabling faster root cause identification. 
 Investigation memory strategy (/sre/investigations/{user_id}/{session_id}) ‚Äì Maintains historical context of past incidents and their resolutions. This enables the solution to suggest proven remediation approaches and avoid anti-patterns that previously failed. 
 
The memory component demonstrates its value through personalized investigations. When both Alice and Carol investigate ‚ÄúAPI response times have degraded 3x in the last hour,‚Äù they receive identical technical findings but completely different presentations. 
Alice receives a technical analysis: 
 
 memory_client.retrieve_user_preferences(user_id="Alice")
# Returns: {"investigation_style": "detailed_systematic_analysis", "reports": "technical_exposition_with_troubleshooting_steps"} 
 
Carol receives an executive summary: 
 
 memory_client.retrieve_user_preferences(user_id="Carol") 
# Returns: {"investigation_style": "business_impact_focused","reports": "executive_summary_without_technical_details"} 
 
Adding observability to the SRE agent 
Adding observability to an SRE agent deployed on Amazon Bedrock AgentCore Runtime is straightforward using the Amazon Bedrock AgentCore Observability primitive. This enables comprehensive monitoring through Amazon CloudWatch with metrics, traces, and logs. Setting up observability requires three steps: 
 
 Add the OpenTelemetry packages to your pyproject.toml: 
   
   dependencies = [
    # ... other dependencies ...
    "opentelemetry-instrumentation-langchain",
    "aws-opentelemetry-distro~=0.10.1",
	] 
    
 Configure observability for your agents to enable metrics in CloudWatch. 
 Start your container using the opentelemetry-instrument utility to automatically instrument your application. 
 
The following command is added to the Dockerfile for the SRE agent: 
 
 # Run application with OpenTelemetry instrumentation 
CMD ["uv", "run", "opentelemetry-instrument", "uvicorn", "sre_agent.agent_runtime:app", "--host", "0.0.0.0", "--port", "8080"] 
 
As shown in the following screenshot, with observability enabled, you gain visibility into the following: 
 
 LLM invocation metrics ‚Äì Token usage, latency, and model performance across agents 
 Tool execution traces ‚Äì Duration and success rates for each MCP tool call 
 Memory operations ‚Äì Retrieval patterns and storage efficiency 
 End-to-end request tracing ‚Äì Complete request flow from user query to final response 
 
 
The observability primitive automatically captures these metrics without additional code changes, providing production-grade monitoring capabilities out of the box. 
Development to production flow 
The SRE agent follows a four-step structured deployment process from local development to production, with detailed procedures documented in Development to Production Flow in the accompanying GitHub repo: 
 
The deployment process maintains consistency across environments: the core agent code (sre_agent/) remains unchanged, and the deployment/ folder contains deployment-specific utilities. The same agent works locally and in production through environment configuration, with Amazon Bedrock AgentCore Gateway providing MCP tools access across different stages of development and deployment. 
Implementation walkthrough 
In the following section, we focus on how Amazon Bedrock AgentCore Gateway, Memory, and Runtime work together to build this multi-agent collaboration solution and deploy it end-to-end with MCP support and persistent intelligence. 
We start by setting up the repository and establishing the local runtime environment with API keys, LLM providers, and demo infrastructure. We then bring core AgentCore components online by creating the gateway for standardized API access, configuring authentication, and establishing tool connectivity. We add intelligence through AgentCore Memory, creating strategies for user preferences and investigation history while loading personas for personalized incident response. Finally, we configure individual agents with specialized tools, integrate memory capabilities, orchestrate collaborative workflows, and deploy to AgentCore Runtime with full observability. 
Detailed instructions for each step are provided in the repository: 
 
 Use Case Setup Guide ‚Äì Backend deployment and development setup 
 Deployment Guide ‚Äì Production containerization and Amazon Bedrock AgentCore Runtime deployment 
 
Prerequisites 
You can find the port forwarding requirements and other setup instructions in the README file‚Äôs Prerequisites section. 
Convert APIs to MCP tools with Amazon Bedrock AgentCore Gateway 
Amazon Bedrock AgentCore Gateway demonstrates the power of protocol standardization by converting existing backend APIs into MCP tools that agent frameworks can consume. This transformation happens seamlessly, requiring only OpenAPI specifications. 
Upload OpenAPI specifications 
The gateway process begins by uploading your existing API specifications to Amazon Simple Storage Service (Amazon S3). The create_gateway.sh script automatically handles uploading the four API specifications (Kubernetes, Logs, Metrics, and Runbooks) to your configured S3 bucket with proper metadata and content types. These specifications will be used to create API endpoint targets in the gateway. 
Create an identity provider and gateway 
Authentication is handled seamlessly through Amazon Bedrock AgentCore Identity. The main.py script creates both the credential provider and gateway: 
 
 # Create AgentCore Gateway with JWT authorization
def create_gateway(
    client: Any,
    gateway_name: str,
    role_arn: str,
    discovery_url: str,
    allowed_clients: list = None,
    description: str = "AgentCore Gateway created via SDK",
    search_type: str = "SEMANTIC",
    protocol_version: str = "2025-03-26",
) -&gt; Dict[str, Any]:
    
    # Build auth config for Cognito
    auth_config = {"customJWTAuthorizer": {"discoveryUrl": discovery_url}}
    if allowed_clients:
        auth_config["customJWTAuthorizer"]["allowedClients"] = allowed_clients
    
    protocol_configuration = {
        "mcp": {"searchType": search_type, "supportedVersions": [protocol_version]}
    }

    response = client.create_gateway(
        name=gateway_name,
        roleArn=role_arn,
        protocolType="MCP",
        authorizerType="CUSTOM_JWT",
        authorizerConfiguration=auth_config,
        protocolConfiguration=protocol_configuration,
        description=description,
        exceptionLevel='DEBUG'
    )
    return response 
 
Deploy API endpoint targets with credential providers 
Each API becomes an MCP target through the gateway. The solution automatically handles credential management: 
 
 def create_api_endpoint_target(
    client: Any,
    gateway_id: str,
    s3_uri: str,
    provider_arn: str,
    target_name_prefix: str = "open",
    description: str = "API Endpoint Target for OpenAPI schema",
) -&gt; Dict[str, Any]:
    
    api_target_config = {"mcp": {"openApiSchema": {"s3": {"uri": s3_uri}}}}

    # API key credential provider configuration
    credential_config = {
        "credentialProviderType": "API_KEY",
        "credentialProvider": {
            "apiKeyCredentialProvider": {
                "providerArn": provider_arn,
                "credentialLocation": "HEADER",
                "credentialParameterName": "X-API-KEY",
            }
        },
    }
    
    response = client.create_gateway_target(
        gatewayIdentifier=gateway_id,
        name=target_name_prefix,
        description=description,
        targetConfiguration=api_target_config,
        credentialProviderConfigurations=[credential_config],
    )
    return response 
 
Validate MCP tools are ready for agent framework 
Post-deployment, Amazon Bedrock AgentCore Gateway provides a standardized /mcp endpoint secured with JWT tokens. Testing the deployment with mcp_cmds.sh reveals the power of this transformation: 
 
 Tool summary:
================
Total tools found: 21

Tool names:
‚Ä¢ x_amz_bedrock_agentcore_search
‚Ä¢ k8s-api___get_cluster_events
‚Ä¢ k8s-api___get_deployment_status
‚Ä¢ k8s-api___get_node_status
‚Ä¢ k8s-api___get_pod_status
‚Ä¢ k8s-api___get_resource_usage
‚Ä¢ logs-api___analyze_log_patterns
‚Ä¢ logs-api___count_log_events
‚Ä¢ logs-api___get_error_logs
‚Ä¢ logs-api___get_recent_logs
‚Ä¢ logs-api___search_logs
‚Ä¢ metrics-api___analyze_trends
‚Ä¢ metrics-api___get_availability_metrics
‚Ä¢ metrics-api___get_error_rates
‚Ä¢ metrics-api___get_performance_metrics
‚Ä¢ metrics-api___get_resource_metrics
‚Ä¢ runbooks-api___get_common_resolutions
‚Ä¢ runbooks-api___get_escalation_procedures
‚Ä¢ runbooks-api___get_incident_playbook
‚Ä¢ runbooks-api___get_troubleshooting_guide
‚Ä¢ runbooks-api___search_runbooks 
 
Universal agent framework compatibility 
This MCP-standardized gateway can now be configured as a Streamable-HTTP server for MCP clients, including AWS Strands, Amazon‚Äôs agent development framework, LangGraph, the framework used in our SRE agent implementation, and CrewAI, a multi-agent collaboration framework. 
The advantage of this approach is that existing APIs require no modification‚Äîonly OpenAPI specifications. Amazon Bedrock AgentCore Gateway handles the following: 
 
 Protocol translation ‚Äì Between REST APIs to MCP 
 Authentication ‚Äì JWT token validation and credential injection 
 Security ‚Äì TLS termination and access control 
 Standardization ‚Äì Consistent tool naming and parameter handling 
 
This means you can take existing infrastructure APIs (Kubernetes, monitoring, logging, documentation) and instantly make them available to AI agent frameworks that support MCP‚Äîthrough a single, secure, standardized interface. 
Implement persistent intelligence with Amazon Bedrock AgentCore Memory 
Whereas Amazon Bedrock AgentCore Gateway provides seamless API access, Amazon Bedrock AgentCore Memory transforms the SRE agent from a stateless system into an intelligent, learning assistant. The memory implementation demonstrates how a few lines of code can enable sophisticated personalization and cross-session knowledge retention. 
Initialize memory strategies 
The SRE agent memory component is built on Amazon Bedrock AgentCore Memory‚Äôs event-based model with automatic namespace routing. During initialization, the solution creates three memory strategies with specific namespace patterns: 
 
 from sre_agent.memory.client import SREMemoryClient
from sre_agent.memory.strategies import create_memory_strategies

# Initialize memory client
memory_client = SREMemoryClient(
    memory_name="sre_agent_memory",
    region="us-east-1"
)

# Create three specialized memory strategies
strategies = create_memory_strategies()
for strategy in strategies:
    memory_client.create_strategy(strategy) 
 
The three strategies each serve distinct purposes: 
 
 User preferences (/sre/users/{user_id}/preferences) ‚Äì Individual investigation styles and communication preferences 
 Infrastructure Knowledge: /sre/infrastructure/{user_id}/{session_id} ‚Äì Domain expertise accumulated across investigations 
 Investigation Summaries: /sre/investigations/{user_id}/{session_id} ‚Äì Historical incident patterns and resolutions 
 
Load user personas and preferences 
The solution comes preconfigured with user personas that demonstrate personalized investigations. The manage_memories.py script loads these personas: 
 
 # Load Alice - Technical SRE Engineer
alice_preferences = {
    "investigation_style": "detailed_systematic_analysis",
    "communication": ["#alice-alerts", "#sre-team"],
    "escalation": {"contact": "alice.manager@company.com", "threshold": "15min"},
    "reports": "technical_exposition_with_troubleshooting_steps",
    "timezone": "UTC"
}

# Load Carol - Executive/Director
carol_preferences = {
    "investigation_style": "business_impact_focused",
    "communication": ["#carol-executive", "#strategic-alerts"],
    "escalation": {"contact": "carol.director@company.com", "threshold": "5min"},
    "reports": "executive_summary_without_technical_details",
    "timezone": "EST"
}

# Store preferences using memory client
memory_client.store_user_preference("Alice", alice_preferences)
memory_client.store_user_preference("Carol", carol_preferences) 
 
Automatic namespace routing in action 
The power of Amazon Bedrock AgentCore Memory lies in its automatic namespace routing. When the SRE agent creates events, it only needs to provide the actor_id‚ÄîAmazon Bedrock AgentCore Memory automatically determines which namespaces the event belongs to: 
 
 # During investigation, the supervisor agent stores context
memory_client.create_event(
    memory_id="sre_agent_memory-abc123",
    actor_id="Alice",  # AgentCore Memory routes this automatically
    session_id="investigation_2025_01_15",
    messages=[("investigation_started", "USER")]
)

# Memory system automatically:
# 1. Checks strategy namespaces &lt;!-- "all" is necessary here for technical accuracy --&gt;
# 2. Matches actor_id "Alice" to /sre/users/Alice/preferences
# 3. Stores event in User Preferences Strategy
# 4. Makes event available for future retrievals 
 
Validate the personalized investigation experience 
The memory component‚Äôs impact becomes clear when both Alice and Carol investigate the same issue. Using identical technical findings, the solution produces completely different presentations of the same underlying content. 
Alice‚Äôs technical report contains detailed systematic analysis for technical teams: 
 
 Technical Investigation Summary

Root Cause: Payment processor memory leak causing OOM kills

Analysis:
- Pod restart frequency increased 300% at 14:23 UTC
- Memory utilization peaked at 8.2GB (80% of container limit)
- JVM garbage collection latency spiked to 2.3s

Next Step:
1. Implement heap dump analysis (`kubectl exec payment-pod -- jmap`)
2. Review recent code deployments for memory management changes
3. Consider increasing memory limits and implementing graceful shutdown 
 
Carol‚Äôs executive summary contains business impact focused for executive stakeholders: 
 
 Business Impact Assessment
Status: CRITICAL - Customer payment processing degraded
Impact: 23% transaction failure rate, $47K revenue at risk
Timeline: Issue detected 14:23 UTC, resolution ETA 45 minutes
Business Actions: - Customer communication initiated via status page - Finance team alerted for revenue impact tracking - Escalating to VP Engineering if not resolved by 15:15 UTC 
 
The memory component enables this personalization while continuously learning from each investigation, building organizational knowledge that improves incident response over time. 
Deploy to production with Amazon Bedrock AgentCore Runtime 
Amazon Bedrock AgentCore makes it straightforward to deploy existing agents to production. The process involves three key steps: containerizing your agent, deploying to Amazon Bedrock AgentCore Runtime, and invoking the deployed agent. 
Containerize your agent 
Amazon Bedrock AgentCore Runtime requires ARM64 containers. The following code shows the complete Dockerfile: 
 
 # Use uv's ARM64 Python base image
FROM --platform=linux/arm64 ghcr.io/astral-sh/uv:python3.12-bookworm-slim

WORKDIR /app

# Copy uv files
COPY pyproject.toml uv.lock ./

# Install dependencies
RUN uv sync --frozen --no-dev

# Copy SRE agent module
COPY sre_agent/ ./sre_agent/

# Set environment variables
# Note: Set DEBUG=true to enable debug logging and traces
ENV PYTHONPATH="/app" \
    PYTHONDONTWRITEBYTECODE=1 \
    PYTHONUNBUFFERED=1

# Expose port
EXPOSE 8080

# Run application with OpenTelemetry instrumentation
CMD ["uv", "run", "opentelemetry-instrument", "uvicorn", "sre_agent.agent_runtime:app", "--host", "0.0.0.0", "--port", "8080"] 
 
Existing agents just need a FastAPI wrapper (agent_runtime:app) to become compatible with Amazon Bedrock AgentCore, and we add opentelemetry-instrument to enable observability through Amazon Bedrock AgentCore. 
Deploy to Amazon Bedrock AgentCore Runtime 
Deploying to Amazon Bedrock AgentCore Runtime is straightforward with the deploy_agent_runtime.py script: 
 
 import boto3

# Create AgentCore client
client = boto3.client('bedrock-agentcore', region_name=region)

# Environment variables for your agent
env_vars = {
    'GATEWAY_ACCESS_TOKEN': gateway_access_token,
    'LLM_PROVIDER': llm_provider,
    'ANTHROPIC_API_KEY': anthropic_api_key  # if using Anthropic
}

# Deploy container to AgentCore Runtime
response = client.create_agent_runtime(
    agentRuntimeName=runtime_name,
    agentRuntimeArtifact={
        'containerConfiguration': {
            'containerUri': container_uri  # Your ECR container URI
        }
    },
    networkConfiguration={"networkMode": "PUBLIC"},
    roleArn=role_arn,
    environmentVariables=env_vars
)

print(f"Agent Runtime ARN: {response['agentRuntimeArn']}") 
 
Amazon Bedrock AgentCore handles the infrastructure, scaling, and session management automatically. 
Invoke your deployed agent 
Calling your deployed agent is just as simple with invoke_agent_runtime.py: 
 
 # Prepare your query with user_id and session_id for memory personalization
payload = json.dumps({
    "input": {
        "prompt": "API response times have degraded 3x in the last hour",
        "user_id": "Alice",  # User for personalized investigation
        "session_id": "investigation-20250127-123456"  # Session for context
    }
})

# Invoke the deployed agent
response = agent_core_client.invoke_agent_runtime(
    agentRuntimeArn=runtime_arn,
    runtimeSessionId=session_id,
    payload=payload,
    qualifier="DEFAULT"
)

# Get the response
response_data = json.loads(response['response'].read())
print(response_data)  # Full response includes output with agent's investigation 
 
Key benefits of Amazon Bedrock AgentCore Runtime 
Amazon Bedrock AgentCore Runtime offers the following key benefits: 
 
 Zero infrastructure management ‚Äì No servers, load balancers, or scaling to configure 
 Built-in session isolation ‚Äì Each conversation is completely isolated 
 AWS IAM integration ‚Äì Secure access control without custom authentication 
 Automatic scaling ‚Äì Scales from zero to thousands of concurrent sessions 
 
The complete deployment process, including building containers and handling AWS permissions, is documented in the Deployment Guide. 
Real-world use cases 
Let‚Äôs explore how the SRE agent handles common incident response scenarios with a real investigation. 
When facing a production issue, you can query the system in natural language. The solution uses Amazon Bedrock AgentCore Memory to personalize the investigation based on your role and preferences: 
 
 export USER_ID=Alice
sre-agent --prompt "API response times have degraded 3x in the last hour" 
 
The supervisor retrieves Alice‚Äôs preferences from memory (detailed systematic analysis style) and creates an investigation plan tailored to her role as a Technical SRE: 
 
 Investigation Plan
1. Use metrics_agent to analyze API performance metrics including response times, error rates, and resource utilization to identify the extent and pattern of slowdown
2. Use logs_agent to examine application logs for errors related to slow API responses, focusing on database connection issues and memory errors
3. Use kubernetes_agent to check pod status and resource constraints, especially for web-service and database pods, looking for CrashLoopBackOff states and missing ConfigMaps
Complexity: Simple
Auto-execute: Yes
Agents involved: Metrics Agent, Logs Agent, Kubernetes Agent 
 
The agents investigate sequentially according to the plan, each contributing their specialized analysis. The solution then aggregates these findings into a comprehensive executive summary: 
 
 Executive Summary
Key Insights
- Root Cause: Database service failure due to missing ConfigMap 'database-config' in production namespace, causing database pod to crash repeatedly
- Impact: Severe performance degradation with API response times increased from 150ms to 5000ms (33x slower)
- Severity: High - Database unavailability, memory exhaustion (100%), and CPU saturation (95%) causing 75% error rate
Next Steps
1. Immediate (&lt; 1 hour): Create/update ConfigMap 'database-config' in production namespace and restart database pod
2. Short-term (&lt; 24 hours): 
   - Fix permissions on '/var/lib/postgresql/data' directory
   - Increase Java heap space for web-service to address OutOfMemoryErrors
   - Optimize UserService.loadAllUsers method causing memory issues
3. Long-term (&lt; 1 week): 
   - Implement resource monitoring with alerts for CPU (&gt;80%), memory (&gt;90%)
   - Optimize slow database queries, particularly "SELECT * FROM users WHERE status='active'"
   - Scale up resources or implement autoscaling for web-service
Critical Alerts
- Database pod (database-pod-7b9c4d8f2a-x5m1q) in CrashLoopBackOff state
- Web-service experiencing OutOfMemoryErrors in UserService.loadAllUsers(UserService.java:45)
- Node-3 experiencing memory pressure (&gt;85% usage)
- Web-app-deployment showing readiness probe failures with 503 errors
Troubleshooting Steps
1. Verify ConfigMap status: `kubectl get configmap database-config -n production`
2. Check database pod logs: `kubectl logs database-pod-7b9c4d8f2a-x5m1q -n production`
3. Create/update ConfigMap: `kubectl create configmap database-config --from-file=database.conf -n production`
4. Fix data directory permissions: `kubectl exec database-pod-7b9c4d8f2a-x5m1q -n production -- chmod -R 700 /var/lib/postgresql/data`
5. Restart database pod: `kubectl delete pod database-pod-7b9c4d8f2a-x5m1q -n production` 
 
This investigation demonstrates how Amazon Bedrock AgentCore primitives work together: 
 
 Amazon Bedrock AgentCore Gateway ‚Äì Provides secure access to infrastructure APIs through MCP tools 
 Amazon Bedrock AgentCore Identity ‚Äì Handles ingress and egress authentication 
 Amazon Bedrock AgentCore Runtime ‚Äì Hosts the multi-agent solution with automatic scaling 
 Amazon Bedrock AgentCore Memory ‚Äì Personalizes Alice‚Äôs experience and stores investigation knowledge for future incidents 
 Amazon Bedrock AgentCore Observability ‚Äì Captures detailed metrics and traces in CloudWatch for monitoring and debugging 
 
The SRE agent demonstrates intelligent agent orchestration, with the supervisor routing work to specialists based on the investigation plan. The solution‚Äôs memory capabilities make sure each investigation builds organizational knowledge and provides personalized experiences based on user roles and preferences. 
This investigation showcases several key capabilities: 
 
 Multi-source correlation ‚Äì It connects database configuration issues to API performance degradation 
 Sequential investigation ‚Äì Agents work systematically through the investigation plan while providing live updates 
 Source attribution ‚Äì Findings include the specific tool and data source 
 Actionable insights ‚Äì It provides a clear timeline of events and prioritized recovery steps 
 Cascading failure detection ‚Äì It can help show how one failure propagates through the system 
 
Business impact 
Organizations implementing AI-powered SRE assistance report significant improvements in key operational metrics. Initial investigations that previously took 30‚Äì45 minutes can now be completed in 5‚Äì10 minutes, providing SREs with comprehensive context before diving into detailed analysis. This dramatic reduction in investigation time translates directly to faster incident resolution and reduced downtime.The solution improves how SREs interact with their infrastructure. Instead of navigating multiple dashboards and tools, engineers can ask questions in natural language and receive aggregated insights from relevant data sources. This reduction in context switching enables teams to maintain focus during critical incidents and reduces cognitive load during investigations.Perhaps most importantly, the solution democratizes knowledge across the team. All team members can access the same comprehensive investigation techniques, reducing dependency on tribal knowledge and on-call burden. The consistent methodology provided by the solution makes sure investigation approaches remain uniform across team members and incident types, improving overall reliability and reducing the chance of missed evidence. 
The automatically generated investigation reports provide valuable documentation for post-incident reviews and help teams learn from each incident, building organizational knowledge over time. Furthermore, the solution extends existing AWS infrastructure investments, working alongside services like Amazon CloudWatch, AWS Systems Manager, and other AWS operational tools to provide a unified operational intelligence system. 
Extending the solution 
The modular architecture makes it straightforward to extend the solution for your specific needs. 
For example, you can add specialized agents for your domain: 
 
 Security agent ‚Äì For compliance checks and security incident response 
 Database agent ‚Äì For database-specific troubleshooting and optimization 
 Network agent ‚Äì For connectivity and infrastructure debugging 
 
You can also replace the demo APIs with connections to your actual systems: 
 
 Kubernetes integration ‚Äì Connect to your cluster APIs for pod status, deployments, and events 
 Log aggregation ‚Äì Integrate with your log management service (Elasticsearch, Splunk, CloudWatch Logs) 
 Metrics platform ‚Äì Connect to your monitoring service (Prometheus, Datadog, CloudWatch Metrics) 
 Runbook repository ‚Äì Link to your operational documentation and playbooks stored in wikis, Git repositories, or knowledge bases 
 
Clean up 
To avoid incurring future charges, use the cleanup script to remove the billable AWS resources created during the demo: 
 
 # Complete cleanup - deletes AWS resources and local files
./scripts/cleanup.sh 
 
This script automatically performs the following actions: 
 
 Stop backend servers 
 Delete the gateway and its targets 
 Delete Amazon Bedrock AgentCore Memory resources 
 Delete the Amazon Bedrock AgentCore Runtime 
 Remove generated files (gateway URIs, tokens, agent ARNs, memory IDs) 
 
For detailed cleanup instructions, refer to Cleanup Instructions. 
Conclusion 
The SRE agent demonstrates how multi-agent systems can transform incident response from a manual, time-intensive process into a time-efficient, collaborative investigation that provides SREs with the insights they need to resolve issues quickly and confidently. 
By combining the enterprise-grade infrastructure of Amazon Bedrock AgentCore with standardized tool access in MCP, we‚Äôve created a foundation that can adapt as your infrastructure evolves and new capabilities emerge. 
The complete implementation is available in our GitHub repository, including demo environments, configuration guides, and extension examples. We encourage you to explore the solution, customize it for your infrastructure, and share your experiences with the community. 
To get started building your own SRE assistant, refer to the following resources: 
 
 Automate tasks in your application using AI agents 
 Amazon Bedrock AgentCore Samples GitHub repository 
 Model Context Protocol documentation 
 LangGraph documentation 
 
 
About the authors 
Amit Arora is an AI and ML Specialist Architect at Amazon Web Services, helping enterprise customers use cloud-based machine learning services to rapidly scale their innovations. He is also an adjunct lecturer in the MS data science and analytics program at Georgetown University in Washington, D.C. 
Dheeraj Oruganty is a Delivery Consultant at Amazon Web Services. He is passionate about building innovative Generative AI and Machine Learning solutions that drive real business impact. His expertise spans Agentic AI Evaluations, Benchmarking and Agent Orchestration, where he actively contributes to research advancing the field. He holds a master‚Äôs degree in Data Science from Georgetown University. Outside of work, he enjoys geeking out on cars, motorcycles, and exploring nature.
‚Ä¢ DoWhile loops now supported in Amazon Bedrock Flows
  Today, we are excited to announce support for DoWhile loops in Amazon Bedrock Flows. With this powerful new capability, you can create iterative, condition-based workflows directly within your Amazon Bedrock flows, using Prompt nodes, AWS Lambda functions, Amazon Bedrock Agents, Amazon Bedrock Flows inline code, Amazon Bedrock Knowledge Bases, Amazon Simple Storage Service (Amazon S3), and other Amazon Bedrock nodes within the loop structure. This feature avoids the need for complex workarounds, enabling sophisticated iteration patterns that use the full range of Amazon Bedrock Flows components. Tasks like content refinement, recursive analysis, and multi-step processing can now seamlessly integrate AI model calls, custom code execution, and knowledge retrieval in repeated cycles. By providing loop support with diverse node types, this feature simplifies generative AI application development and accelerates enterprise adoption of complex, adaptive AI solutions. 
Organizations using Amazon Bedrock Flows can now use DoWhile loops to design and deploy workflows for building more scalable and efficient generative AI applications fully within the Amazon Bedrock environment while achieving the following: 
 
 Iterative processing ‚Äì Execute repeated operations until specific conditions are met, enabling dynamic content refinement and recursive improvements 
 Conditional logic ‚Äì Implement sophisticated decision-making within flows based on AI outputs and business rules 
 Complex use cases ‚Äì Manage multi-step generative AI workflows that require repeated execution and refinement 
 Builder-friendly ‚Äì Create and manage loops through both the Amazon Bedrock API and AWS Management Console in the traces 
 Observability ‚Äì Employ seamless tracking of loop iterations, conditions, and execution paths 
 
In this post, we discuss the benefits of this new feature, and show how to use DoWhile loops in Amazon Bedrock Flows. 
Benefits of DoWhile loops in Amazon Bedrock Flows 
Using DoWhile loops in Amazon Bedrock Flows offers the following benefits: 
 
 Simplified flow control ‚Äì Create sophisticated iterative workflows without complex orchestration or external services 
 Flexible processing ‚Äì Enable dynamic, condition-based execution paths that can adapt based on AI outputs and business rules 
 Enhanced development experience ‚Äì Help users build complex iterative workflows through an intuitive interface, without requiring external workflow management 
 
Solution overview 
In the following sections, we show how to create a simple Amazon Bedrock flow using Do-while loops with Lambda functions. Our example showcases a practical application where we construct a flow that generates a blog post on a given topic in an iterative manner until certain acceptance criteria are fulfilled. The flow demonstrates the power of combining different types of Amazon Bedrock Flows nodes within a loop structure, where Prompt nodes generate and fine-tune the blog post, Inline Code nodes allow writing custom Python code to analyze the outputs, and S3 Storage nodes enable storing each version of the blog post during the process for reference. The DoWhile loop continues to execute until the quality of the blog post meets the condition set in the loop controller. This example illustrates how different flow nodes can work together within a loop to progressively transform data until desired conditions are met, providing a foundation for understanding more complex iterative workflows with various node combinations. 
Prerequisites 
Before implementing the new capabilities, make sure you have the following: 
 
 An AWS account 
 Other Amazon Bedrock services in place: 
   
   Create and test your base prompts for customer service interactions in Amazon Bedrock Prompt Management 
   Create guardrails with relevant rules using Amazon Bedrock Guardrails 
    
 Resources in auxiliary AWS services needed for your workflow, such as Lambda, Amazon DynamoDB, and Amazon S3 
 Required AWS Identity and Access Management (IAM) permissions: 
   
   Access to Amazon Bedrock Flows 
   Appropriate access to large language models (LLMs) in Amazon Bedrock 
    
 
After these components are in place, you can proceed with using Amazon Bedrock Flows with DoWhile loop capabilities in your generative AI use case. 
Create your flow using DoWhile Loop nodes 
Complete the following steps to create your flow: 
 
 On the Amazon Bedrock console, choose Flows under Builder tools in the navigation pane. 
 Create a new flow, for example, dowhile-loop-demo. For detailed instructions on creating a flow, see Amazon Bedrock Flows is now generally available with enhanced safety and traceability. 
 Add a DoWhile loop node. 
 Add additional nodes according to the solution workflow (discussed in the next section). 
 
Amazon Bedrock provides different node types to build your prompt flow. For this example, we use a DoWhile Loop node for calling different types of nodes for a generative AI-powered application, which creates a blog post on a given topic and checks the quality in every loop. There is one DoWhile Loop node in the flow. This new node type is on the Nodes tab in the left pane, as shown in the following screenshot. 
 
DoWhile loop workflow 
A DoWhile loop consists of two parts: the loop and the loop controller. The loop controller validates the logic for the loop and decides whether to continue or exit the loop. In this example, it is executing Prompt, Inline Code, S3 Storage nodes each time the loop is executed. 
Let‚Äôs go through this flow step-by-step, as illustrated in the preceding screenshot: 
 
 A user asks to write a blog post on a specific topic (for example, using the following prompt: {‚Äútopic‚Äù: ‚ÄúAWS Lambda‚Äù, ‚ÄúAudience‚Äù: ‚ÄúChief Technology Officer‚Äù, ‚Äúword_count‚Äù:‚Äù500}). This prompt is sent to the Prompt node (Content_Generator). 
 The Prompt node (Content_Generator) writes a blog post based on the prompt using one of the Amazon Bedrock provided LLMs (such as Amazon Nova or Anthropic‚Äôs Claude) and is sent to the Loop Input node. This is the entry point to the DoWhile Loop node. 
 Three steps happen in tandem: 
   
   The Loop Input node forwards the blog post content to another Prompt node (Blog_Analysis_Rating) for rating the post based on criteria mentioned as part of the prompt. The output of this Prompt node is JSON code like the following example. The output of a Prompt node is always of type String. You can modify the prompt to get different types of output according to your needs. However, you can also ask the LLM to output a single rating number. 
     
     {
  "overall_rating": 8.5,
  "category_ratings": {
    "clarity_and_readability": 9,
    "value_to_target_audience": 8,
    "engagement_level": 8,
    "technical_accuracy": 9
  } 
      
   The blog post is sent to the flow output during every iteration. This is the final version whenever the loop condition is not met (exiting the loop) or the end of maximum loop iterations. 
   At the same time, the output of the previous Prompt node (Content_Generator) is forwarded to another Prompt node (Blog_Refinement) by the Loop Input node. This node recreates or modifies the blog post based on the feedback from the analysis. 
    
 The output of the Prompt node (Blog_Analysis_Rating) is fed into the Inline Code node to extract the necessary rating and return that as a number or other information required for checking the condition inside the loop controller as input variables (for example, a rating). 
 
 
 def __func(variable):
 return float(variable["overall_rating"])
__func(variable) 
 
Python code inside the Inline Code must be treated as untrusted, and appropriate parsing, validation, and data handling should be implemented. 
 
 The output of the Inline Code node is fed into the loop condition inside the loop controller to validate against the condition we set up inside the continue loop. In this example, we are checking for a rating less than or equal to 9 for the generated blog post. You can check up to five conditions. Additionally, a maximum loop iterations parameter makes sure that loop doesn‚Äôt continue infinitely. 
 The step consists of two parts: 
   
   A Prompt node (Blog_Refinement) forwards the newly generated blog post to loopinput inside the loop controller. 
   The loop controller stores the version of the post in Amazon S3 for future reference and comparing the different versions generated. 
    
 This path will execute if one of the conditions is met inside the continue loop and maximum loop iterations. If this continues, then the new modified blog post from earlier is forwarded to the input field in the Loop Input node as LoopInput and the loop continues. 
 The final output is produced after the DoWhile loop condition is met or maximum number of iterations are completed. The output will be final version of the blog post. 
 
You can see the output as shown in the following screenshot. The system also provides access to node execution traces, offering detailed insights into each processing step, real-time performance metrics, and highlighting issues that may have occurred during the flow‚Äôs execution. Traces can be enabled using an API and sent to an Amazon CloudWatch log. In the API, set the enableTrace field to true in an InvokeFlow request. Each flowOutputEvent in the response is returned alongside a flowTraceEvent. 
 
You have now successfully created and executed an Amazon Bedrock flow using DoWhile Loop nodes. You can also use Amazon Bedrock APIs to programmatically execute this flow. For additional details on how to configure flows, see Amazon Bedrock Flows is now generally available with enhanced safety and traceability. 
Considerations 
When working with DoWhile Loop nodes in Amazon Bedrock Flows, the following are the important things to note: 
 
 DoWhile Loop nodes don‚Äôt support nested loops (loops within loops) 
 Each loop controller can evaluate up to five input conditions for its exit criteria 
 A maximum iteration limit must be specified to help prevent infinite loops and enable controlled execution 
 
Conclusion 
The integration of DoWhile loops in Amazon Bedrock Flows marks a significant advancement in iterative workflow capabilities, enabling sophisticated loop-based processing that can incorporate Prompt nodes, Inline Code nodes, S3 Storage nodes, Lambda functions, agents, DoWhile Loop nodes, and Knowledge Base nodes. This enhancement responds directly to enterprise customers‚Äô needs for handling complex, repetitive tasks within their AI workflows, helping developers create adaptive, condition-based solutions without requiring external orchestration tools. By providing support for iterative processing patterns, DoWhile loops help organizations build more sophisticated AI applications that can refine outputs, perform recursive operations, and implement complex business logic directly within the Amazon Bedrock environment. This powerful addition to Amazon Bedrock Flows democratizes the development of advanced AI workflows, making iterative AI processing more accessible and manageable across organizations. 
DoWhile loops in Amazon Bedrock Flows are now available in all the AWS Regions where Amazon Bedrock Flows is supported, except for the AWS Gov Cloud (US) Region. To get started, open the Amazon Bedrock console or Amazon Bedrock APIs to begin building flows with Amazon Bedrock Flows. To learn more, refer to Create your first flow in Amazon Bedrock and Track each step in your flow by viewing its trace in Amazon Bedrock. 
We‚Äôre excited to see the innovative applications you will build with these new capabilities. As always, we welcome your feedback through AWS re:Post for Amazon Bedrock or your usual AWS contacts. Join the generative AI builder community at community.aws to share your experiences and learn from others. 
 
About the authors 
Shubhankar Sumar is a Senior Solutions Architect at AWS, where he specializes in architecting generative AI-powered solutions for enterprise software and SaaS companies across the UK. With a strong background in software engineering, Shubhankar excels at designing secure, scalable, and cost-effective multi-tenant systems on the cloud. His expertise lies in seamlessly integrating cutting-edge generative AI capabilities into existing SaaS applications, helping customers stay at the forefront of technological innovation. 
Jesse Manders is a Senior Product Manager on Amazon Bedrock, the AWS Generative AI developer service. He works at the intersection of AI and human interaction with the goal of creating and improving generative AI products and services to meet our needs. Previously, Jesse held engineering team leadership roles at Apple and Lumileds, and was a senior scientist in a Silicon Valley startup. He has an M.S. and Ph.D. from the University of Florida, and an MBA from the University of California, Berkeley, Haas School of Business. 
Eric Li is a Software Development Engineer II at AWS, where he builds core capabilities for Amazon Bedrock and SageMaker to support generative AI applications at scale. His work focuses on designing secure, observable, and cost-efficient systems that help developers and enterprises adopt generative AI with confidence. He is passionate about advancing developer experiences for building with large language models, making it easier to integrate AI into production-ready cloud applications.
‚Ä¢ How PropHero built an intelligent property investment advisor with continuous evaluation using Amazon Bedrock
  This post was written with Lucas Dahan, Dil Dolkun, and Mathew Ng from PropHero. 
PropHero is a leading property wealth management service that democratizes access to intelligent property investment advice through big data, AI, and machine learning (ML). For the Spanish and Australian consumer base, PropHero needed an AI-powered advisory system that could engage customers in accurate property investment discussions. The goal was to provide personalized investment insights and to guide and assist users at every stage of their investment journey: from understanding the process, gaining visibility into timelines, securely uploading documents, to tracking progress in real time. 
PropHero collaborated with the AWS Generative AI Innovation Center to implement an intelligent property investment advisor using AWS generative AI services with continuous evaluation. The solution helps users engage in natural language conversations about property investment strategies and receive personalized recommendations based on PropHero‚Äôs comprehensive market knowledge. 
In this post, we explore how we built a multi-agent conversational AI system using Amazon Bedrock that delivers knowledge-grounded property investment advice. We explore the agent architecture, model selection strategy, and comprehensive continuous evaluation system that facilitates quality conversations while facilitating rapid iteration and improvement. 
The challenge: Making property investment knowledge more accessible 
The area of property investment presents numerous challenges for both novice and experienced investors. Information asymmetry creates barriers where comprehensive market data remains expensive or inaccessible. Traditional investment processes are manual, time-consuming, and require extensive market knowledge to navigate effectively. For the Spanish and Australian consumers specifically, we needed to build a solution that could provide accurate, contextually relevant property investment advice in Spanish while handling complex, multi-turn conversations about investment strategies. The system needed to maintain high accuracy while delivering responses at scale, continuously learning and improving from customer interactions. Most importantly, it needed to assist users across every phase of their journey, from initial onboarding through to final settlement, ensuring comprehensive support throughout the entire investment process. 
Solution overview 
We built a complete end-to-end solution using AWS generative AI services, architected around a multi-agent AI advisor with integrated continuous evaluation. The system provides seamless data flow from ingestion through intelligent advisory conversations with real-time quality monitoring. The following diagram illustrates this architecture. 
 
The solution architecture consists of four virtual layers, each serving specific functions in the overall system design. 
Data foundation layer 
The data foundation provides the storage and retrieval infrastructure for system components: 
 
 Amazon DynamoDB ‚Äì Fast storage for conversation history, evaluation metrics, and user interaction data 
 Amazon Relational Database (Amazon RDS) for PostgreSQL ‚Äì A PostgreSQL database storing LangFuse observability data, including large language model (LLM) traces and latency metrics 
 Amazon Simple Storage Service (Amazon S3) ‚Äì A central data lake storing Spanish FAQ documents, property investment guides, and conversation datasets 
 
Multi-agent AI layer 
The AI processing layer encompasses the core intelligence components that power the conversational experience: 
 
 Amazon Bedrock ‚Äì Foundation models (FMs) such as LLMs and rerankers powering specialized agents 
 Amazon Bedrock Knowledge Bases ‚Äì Semantic search engine with semantic chunking for FAQ-style content 
 LangGraph ‚Äì Orchestration of multi-agent workflows and conversation state management 
 AWS Lambda ‚Äì Serverless functions executing multi-agent logic and retrival of user information for richer context 
 
Continuous evaluation layer 
The evaluation infrastructure facilitates continuous quality monitoring and improvement through these components: 
 
 Amazon CloudWatch ‚Äì Real-time monitoring of quality metrics with automated alerting and threshold management 
 Amazon EventBridge ‚Äì Real-time event triggers for conversation completion and quality assessment 
 AWS Lambda ‚Äì Automated evaluation functions measuring context relevance, response groundedness, and goal accuracy 
 Amazon QuickSight ‚Äì Interactive dashboards and analytics for monitoring the respective metrics 
 
Application and integration layer 
The integration layer provides secure interfaces for external communication: 
 
 Amazon API Gateway ‚Äì Secure API endpoints for conversational interface and evaluation webhooks 
 
Multi-agent AI advisor architecture 
The intelligent advisor uses a multi-agent system orchestrated through LangGraph, which sits in a single Lambda function, where each agent is optimized for specific tasks. The following diagram shows the communication flow among the various agents within the Lambda function. 
 
Agent composition and model selection 
Our model selection strategy involved extensive testing to match each component‚Äôs computational requirements with the most cost-effective Amazon Bedrock model. We evaluated factors including response quality, latency requirements, and cost per token to determine optimal model assignments for each agent type.Each component in the system uses the most appropriate model for its designated function, as outlined in the following table. 
 
  
   
   Component 
   Amazon Bedrock Model 
   Purpose 
   
   
   Router Agent 
   Anthropic Claude 3.5 Haiku 
   Query classification and routing 
   
   
   General Agent 
   Amazon Nova Lite 
   Common questions and conversation management 
   
   
   Advisor Agent 
   Amazon Nova Pro 
   Specialized property investment advice 
   
   
   Settlement agent 
   Anthropic Claude 3.5 Haiku 
   Customer support specialising on pre-settlement phase of investment 
   
   
   Response Agent 
   Amazon Nova Lite 
   Final response generation and formatting 
   
   
   Embedding 
   Cohere Embed Multilingual v3 
   Context retrieval 
   
   
   Retriever 
   Cohere Rerank 3.5 
   Context retrieval and ranking 
   
   
   Evaluator 
   Anthropic Claude 3.5 Haiku 
   Quality assessment and scoring 
   
  
 
End-to-end conversation flow 
The conversation processing follows a structured workflow that facilitates accurate responses while maintaining quality standards: 
 
 User queries enter through API Gateway and are routed to the router agent. 
 The router agent determines the appropriate specialized agent based on query analysis. 
 User information is retrieved at the start for richer context and knowledge-intensive queries trigger the retriever to access the Amazon Bedrock knowledge base. 
 Specialized agents process queries with retrieved user information and relevant context from the knowledge base. 
 The response agent formats and generates the final user-facing response with the appropriate tone. 
 Parallel evaluation processes assess context relevance, response groundedness, and goal accuracy. 
 Conversation data is stored in DynamoDB for analysis and improvement. 
 
Amazon Bedrock Knowledge Bases configuration 
The knowledge base uses several AWS services for optimal performance: 
 
 Data source ‚Äì S3 bucket 
 Chunking strategy ‚Äì Semantic chunking optimized for FAQ-style content 
 Embedding model ‚Äì Cohere Embed Multilingual v3 for Spanish understanding 
 Reranker ‚Äì Cohere Rerank 3.5 for reranking retrieved Spanish content 
 Vector database ‚Äì Amazon OpenSearch Serverless for scalable semantic search 
 
Sample conversation experience 
The system handles natural Spanish conversations seamlessly. For example: 
Usuario: ‚ÄúHola, ¬øqu√© es PropHero exactamente?‚Äù 
Asistente: ‚Äú¬°Hola! PropHero es una plataforma que te permite crear, optimizar y crecer tu patrimonio inmobiliario delegando todo el proceso en nuestro equipo de profesionales‚Ä¶‚Äù 
Usuario: ‚Äú¬øY c√≥mo funciona en la pr√°ctica?‚Äù 
Asistente: ‚ÄúEs muy sencillo. Comenzamos con una sesi√≥n con tu Property Coach para definir tu estrategia de inversi√≥n personalizada‚Ä¶‚Äù 
Integrated continuous evaluation system 
The continuous evaluation system is integrated into the architecture, operating as a core component. This approach supports quality monitoring in real-time alongside conversation processing. 
Evaluation metrics 
We used the following metrics from the Ragas library for evaluation: 
 
 Context Relevance (0‚Äì1) ‚Äì Measures the relevance of retrieved context to user queries, evaluating RAG system effectiveness 
 Response Groundedness (0‚Äì1) ‚Äì Makes sure responses are factually accurate and derived from PropHero‚Äôs official information 
 Agent Goal Accuracy (0‚Äì1) ‚Äì Binary measure of whether responses successfully address user investment goals 
 
Real-time evaluation workflow 
The evaluation system operates seamlessly within the conversation architecture: 
 
 Amazon DynamoDB Streams triggers ‚Äì Conversation data written to DynamoDB automatically triggers a Lambda function for evaluation through Amazon DynamoDB Streams 
 Parallel processing ‚Äì Lambda functions execute evaluation logic in parallel with response delivery 
 Multi-dimensional assessment ‚Äì Each conversation is evaluated across three key dimensions simultaneously 
 Intelligent scoring with LLM-as-a-judge ‚Äì Anthropic‚Äôs Claude 3.5 Haiku provides consistent evaluation as an LLM judge, offering standardized assessment criteria across conversations. 
 Monitoring and analytics ‚Äì CloudWatch captures metrics from the evaluation process, and QuickSight provides dashboards for trend analysis 
 
The following diagram provides an overview of the Lambda function responsible for continuous evaluation. 
 
Implementation insights and best practices 
Our development journey involved a 6-week iterative process with PropHero‚Äôs technical team. We conducted testing across different model combinations and evaluated chunking strategies using real customer FAQ data. This journey revealed several architectural optimizations that enhanced system performance, achieved significant cost reductions, and improved user experience. 
Model selection strategy 
Our approach to model selection demonstrates the importance of matching model capabilities to specific tasks. By using Amazon Nova Lite for simpler tasks and Amazon Nova Pro for complex reasoning, the solution achieves optimal cost-performance balance while maintaining high accuracy standards. 
Chunking and retrieval optimization 
Semantic chunking proved superior to hierarchical and fixed chunking approaches for FAQ-style content. The Cohere Rerank 3.5 model enabled the system to use fewer chunks (10 vs. 20) while maintaining accuracy, reducing latency and cost. 
Multilingual capabilities 
The system effectively handles Spanish and English queries by using FMs that support Spanish language on Amazon Bedrock. 
Business impact 
The PropHero AI advisor delivered measurable business value: 
 
 Enhanced customer engagement ‚Äì A 90% goal accuracy rate makes sure customers receive relevant, actionable property investment advice. Over 50% of our users (and over 70% of paid users) are actively using the AI advisor. 
 Operational efficiency ‚Äì Automated responses to common questions reduced customer service workload by 30%, freeing staff to focus on complex customer needs. 
 Scalable growth ‚Äì The serverless architecture automatically scales to handle increasing customer demand without manual intervention. 
 Cost optimization ‚Äì Strategic model selection achieved high performance while reducing AI costs by 60% compared to using premium models throughout. 
 Consumer base expansion ‚Äì Successful Spanish language support enabled PropHero‚Äôs expansion into the Spanish consumer base with localized expertise. 
 
Conclusion 
The PropHero AI advisor demonstrates how AWS generative AI services can be used to create intelligent, context-aware conversational agents that deliver real business value. By combining a modular agent architecture with a robust evaluation system, PropHero has created a solution that enhances customer engagement while providing accurate and relevant responses.The comprehensive evaluation pipeline has been particularly valuable, providing clear metrics for measuring conversation quality and guiding ongoing improvements. This approach makes sure the AI advisor will continue to evolve and improve over time.For more information about building multi-agent AI advisors with continuous evaluation, refer to the following resources: 
 
 Retrieve data and generate AI responses with Amazon Bedrock Knowledge Bases ‚Äì With Amazon Bedrock Knowledge Bases, you can implement semantic search with chunking strategies 
 LangGraph ‚Äì LangGraph can help you build multi-agent workflows 
 Ragas ‚Äì Ragas offers comprehensive LLM evaluation metrics, including context relevance, groundedness, and goal accuracy used in this implementation 
 
To learn more about the Generative AI Innovation Center, get in touch with your account team. 
 
About the authors 
Adithya Suresh is a Deep Learning Architect at the AWS Generative AI Innovation Center based in Sydney, where he collaborates directly with enterprise customers to design and scale transformational generative AI solutions for complex business challenges. He uses AWS generative AI services to build bespoke AI systems that drive measurable business value across diverse industries. 
Lucas Dahan was the Head of Data &amp; AI at PropHero at the time of writing. He leads the technology team that is transforming property investment through innovative digital solutions. 
 Dil Dolkun is the Data &amp; AI Engineer at PropHero‚Äôs tech team, and has been instrumental in designing data architectures and multi-agent workflows for PropHero‚Äôs generative AI property investment Advisor system. 
Mathew Ng is a Technical Lead at PropHero, who architected and scaled PropHero‚Äôs cloud-native, high-performance software solution from early stage start up to successful Series A funding. 
Aaron Su is a Solutions Architect at AWS, with a focus across AI and SaaS startups. He helps early-stage companies architect scalable, secure, and cost-effective cloud solutions.
‚Ä¢ Accelerate benefits claims processing with Amazon Bedrock Data Automation
  In the benefits administration industry, claims processing is a vital operational pillar that makes sure employees and beneficiaries receive timely benefits, such as health, dental, or disability payments, while controlling costs and adhering to regulations like HIPAA and ERISA. Businesses aim to optimize the workflow‚Äîcovering claim submission, validation, adjudication, payment, and appeals‚Äîto enhance employee satisfaction, strengthen provider relationships, and mitigate financial risks. The process includes specific steps like claim submission (through portals or paper), data validation (verifying eligibility and accuracy), adjudication (assessing coverage against plan rules), payment or denial (including check processing for reimbursements), and appeal handling. Efficient claims processing supports competitive benefits offerings, which is crucial for talent retention and employer branding, but requires balancing speed, accuracy, and cost in a highly regulated environment. 
Despite its importance, claims processing faces significant challenges in many organizations. Most notably, the reliance on legacy systems and manual processes results in frustratingly slow resolution times, high error rates, and increased administrative costs. Incomplete or inaccurate claim submissions‚Äîsuch as those with missing diagnosis codes or eligibility mismatches‚Äîfrequently lead to denials and rework, creating frustration for both employees and healthcare providers. Additionally, fraud, waste, and abuse continue to inflate costs, yet detecting these issues without delaying legitimate claims remains challenging. Complex regulatory requirements demand constant system updates, and poor integration between systems‚Äîsuch as Human Resource Information Systems (HRIS) and other downstream systems‚Äîseverely limits scalability. These issues drive up operational expenses, erode trust in benefits programs, and overburden customer service teams, particularly during appeals processes or peak claims periods. 
Generative AI can help address these challenges. With Amazon Bedrock Data Automation, you can automate generation of useful insights from unstructured multimodal content such as documents, images, audio, and video. Amazon Bedrock Data Automation can be used in benefits claims process to automate document processing by extracting and classifying documents from claims packets, policy applications, and supporting documents with industry-leading accuracy, reducing manual errors and accelerating resolution times. Amazon Bedrock Data Automation natural language processing capabilities interpret unstructured data, such as provider notes, supporting compliance with plan rules and regulations. By automating repetitive tasks and providing insights, Amazon Bedrock Data Automation helps reduce administrative burdens, enhance experiences for both employees and providers, and support compliance in a cost-effective manner. Furthermore, its scalable architecture enables seamless integration with existing systems, improving data flow across HRIS, claims systems, and provider networks, and advanced analytics help detect fraud patterns to optimize cost control. 
In this post, we examine the typical benefit claims processing workflow and identify where generative AI-powered automation can deliver the greatest impact. 
Benefit claims processing 
When an employee or beneficiary pays out of pocket for an expense covered under their health benefits, they submit a claim for reimbursement. This process requires several supporting documents, including doctor‚Äôs prescriptions and proof of payment, which might include check images, receipts, or electronic payment confirmations. 
The claims processing workflow involves several critical steps: 
 
 Document intake and processing ‚Äì The system receives and categorizes submitted documentation, including: 
   
   Medical records and prescriptions 
   Proof of payment documentation 
   Supporting forms and eligibility verification 
    
 Payment verification processing ‚Äì For check-based reimbursements, the system must complete the following steps: 
   
   Extract information from check images, including the account number and routing number contained in the MICR line 
   Verify payee and payer names against the information provided during the claim submission process 
   Confirm payment amounts match the claimed expenses 
   Flag discrepancies for human review 
    
 Adjudication and reimbursement ‚Äì When verification is complete, the system performs several actions: 
   
   Determine eligibility based on plan rules and coverage limits 
   Calculate appropriate reimbursement amounts 
   Initiate payment processing through direct deposit or check issuance 
   Provide notification to the claimant regarding the status of their reimbursement 
    
 
In this post, we walk through a real-world scenario to make the complexity of this multi-step process clearer. The following example demonstrates how Amazon Bedrock Data Automation can streamline the claims processing workflow, from initial submission to final reimbursement. 
Solution overview 
Let‚Äôs consider a scenario where a benefit plan participant seeks treatment and pays out of pocket for the doctor‚Äôs fee using a check. They then buy the medications prescribed by the doctor at the pharmacy store. Later, they log in to their benefit provider‚Äôs portal and submit a claim along with the image of the check and payment receipt for the medications. 
This solution uses Amazon Bedrock Data Automation to automate the two most critical and time-consuming aspects of this workflow: document intake and payment verification processing. The following diagram illustrates the benefits claims processing architecture. 
 
The end-to-end process works through four integrated stages: ingestion, extraction, validation, and integration. 
Ingestion 
When a beneficiary uploads supporting documents (check image and pharmacy receipt) through the company‚Äôs benefit claims portal, these documents are securely saved in an Amazon Simple Storage Service (Amazon S3) bucket, triggering the automated claims processing pipeline. 
Extraction 
After documents are ingested, the system immediately begins with intelligent data extraction: 
 
 The S3 object upload triggers an AWS Lambda function, which invokes the Amazon Bedrock Data Automation project. 
 Amazon Bedrock Data Automation uses blueprints for file processing and extraction. Blueprints are artifacts used to configure file processing business logic by specifying a list of field names for data extraction, along with their desired data formats (string, number, or Boolean) and natural language context for data normalization and validation rules. Amazon Bedrock Data Automation provides a catalog of sample blueprints out of the box. You can create a custom blueprint for your unique document types that aren‚Äôt predefined in the catalog. This solution uses two blueprints designed for different document types, as shown in the following screenshot: 
   
   The catalog blueprint US-Bank-Check for check processing. 
   The custom blueprint benefit-claims-pharmacy-receipt-blueprint for pharmacy-specific receipts. 
    
 
 
US-Bank-Check is a catalog blueprint provided out of the box by Amazon Bedrock Data Automation. The custom blueprint benefit-claims-pharmacy-receipt-blueprint is created using an AWS CloudFormation template to handle pharmacy receipt processing, addressing a specific document type that wasn‚Äôt available in the standard blueprint catalog. The benefit administrator wants to look for vendor-specific information such as name, address, and phone details for benefits claims processing. The custom blueprint schema contains natural language explanation of those fields, such as VendorName, VendorAddress, VendorPhone, and additional fields, explaining what the field represents, expected data types, and inference type for each extracted field (explained in Creating Blueprints for Extraction), as shown in the following screenshot. 
 
3. The two blueprints are added to the Amazon Bedrock Data Automation project. An Amazon Bedrock Data Automation project is a grouping of both standard and custom blueprints that you can use to process different types of files (like documents, audio, and images) using specific configuration settings, where you can control what kind of information you want to extract from each file type. When the project is invoked asynchronously, it automatically applies the appropriate blueprint, extracts information such as confidence scores and bounding box details for each field, and saves results in a separate S3 bucket. This intelligent classification alleviates the need for you to write complex document classification logic. 
The following screenshot illustrates the document classification by the standard catalog blueprint US-Bank-Check. 
 
The following screenshot shows the document classification by the custom blueprint benefit-claims-pharmacy-receipt-blueprint. 
 
Validation 
With the data extracted, the system moves to the validation and decision-making process using the business rules specific to each document type. 
The business rules are documented in standard operating procedure documents (AnyCompany Benefit Checks Standard Operating procedure.docx and AnyCompany Benefit Claims Standard Operating procedure.docx) and uploaded to an S3 bucket. Then the system creates a knowledge base for Amazon Bedrock with the S3 bucket as the source, as shown in the following screenshot. 
 
When the extracted Amazon Bedrock Data Automation results are saved to the configured S3 bucket, a Lambda function is triggered automatically. Based on the business rules retrieved from the knowledge base for the specific document type and the extracted Amazon Bedrock Data Automation output, an Amazon Nova Lite large langue model (LLM) makes the automated approve/deny decision for claims. 
The following screenshot shows the benefit claim adjudication automated decision for US-Bank-Check. 
 
The following screenshot shows the benefit claim adjudication automated decision for benefit-claims-pharmacy-receipt-blueprint. 
 
Integration 
The system seamlessly integrates with existing business processes. 
When validation is complete, an event is pushed to Amazon EventBridge, which triggers a Lambda function for downstream integration. In this implementation, we use an Amazon DynamoDB table and Amazon Simple Notification Service (Amazon SNS) email for downstream integration. A DynamoDB table is created as part of the deployment stack, which is used to populate details including document classification, extracted data, and automated decision. An email notification is sent for both check and receipts after the final decision is made by the system. The following screenshot shows an example email for pharmacy receipt approval. 
 
This flexible architecture helps you integrate with your existing applications through internal APIs or events to update claim status or trigger additional workflows when validation fails. 
Reducing manual effort through intelligent business rules management 
Beyond automating document processing, this solution addresses a common operational challenge: Traditionally, customers must write and maintain code for handling business rules around claims adjudication and processing. Every business rule change requires development effort and code updates, slowing time-to-market and increasing maintenance overhead. 
Our approach converts business rules and standard operating procedures (SOPs) into knowledge bases using Amazon Bedrock Knowledge Bases, which you can use for automated decision-making. This approach can dramatically reduce time-to-market when business rules change, because updates can be made through knowledge management rather than code deployment. 
In the following sections, we walk you through the steps to deploy the solution to your own AWS account. 
Prerequisites 
To implement the solution provided in this post, you must have the following: 
 
 An AWS account 
 Access to Amazon Titan Text Embeddings V2 and Amazon Nova Lite foundation models (FMs) enabled in Amazon Bedrock 
 
This solution uses Python 3.13 with Boto3 1.38. or later version, and the AWS Serverless Application Model Command Line Interface (AWS SAM CLI) version 1.138.0. We assume that you have installed these in your local machine already. If not, refer to the following instructions: 
 
 Python 3.13 installation 
 Install the AWS SAM CLI 
 
Set up code in your local machine 
To set up the code, clone the GitHub repository. After you have cloned the repository to your local machine, the project folder structure will look like the following code, as mentioned in the README file: 
 
Deploy the solution in your account 
The sample code comes with a CloudFormation template that creates necessary resources. To deploy the solution in your account, follow the deployment instructions in the README file. 
Clean up 
Deploying this solution in your account will incur costs. Follow the cleanup instructions in the README file to avoid charges when you are done. 
Conclusion 
Benefits administration companies can significantly enhance their operations by automating claims processing using the solution outlined in this post. This strategic approach directly addresses the industry‚Äôs core challenges and can deliver several key advantages: 
 
 Enhanced processing efficiency through accelerated claims resolution times, reduced manual error rates, and higher straight-through processing rates that minimize the frustrating delays and manual rework plaguing legacy systems 
 Streamlined document integration and fraud detection capabilities, where adding new supporting documents becomes seamless through new Amazon Bedrock Data Automation blueprints, while AI-powered analytics identify suspicious patterns without delaying legitimate claims, avoiding traditional months-long development cycles and reducing costly fraud, waste, and abuse 
 Agile business rule management that enables rapid adaptation to changing HIPAA and ERISA requirements and modification of business rules, significantly reducing administrative costs and time-to-market while improving scalability and integration with existing HRIS and claims, ultimately enhancing employee satisfaction, strengthening provider relationships, and supporting competitive benefits offerings that are crucial for talent retention and employer branding 
 
To get started with this solution, refer to the GitHub repo. For more information about Amazon Bedrock Data Automation, refer to Transform unstructured data into meaningful insights using Amazon Bedrock Data Automation and try the Document Processing Using Amazon Bedrock Data Automation workshop. 
 
About the authors 
Saurabh Kumar is a Senior Solutions Architect at AWS based out of Raleigh, NC, with expertise in Resilience Engineering, Chaos Engineering, and Generative AI solutions. He advises customers on fault-tolerance strategies and generative AI-driven modernization approaches, helping organizations build robust architectures while leveraging generative AI technologies to drive innovation. 
Kiran Lakkireddy is a Principal Solutions Architect at AWS with expertise in Financial Services, Benefits Management and HR Services industries. Kiran provides technology and architecture guidance to customers in their business transformation, with a specialized focus on GenAI security, compliance, and governance. He regularly speaks to customer security leadership on GenAI security, compliance, and governance topics, helping organizations navigate the complex landscape of AI implementation while maintaining robust security standards. 
Tamilmanam Sambasivam is a Solutions Architect and AI/ML Specialist at AWS. She helps enterprise customers to solve their business problems by recommending the right AWS solutions. Her strong back ground in Information Technology (24+ years of experience) helps customers to strategize, develop and modernize their business problems in AWS cloud. In the spare time, Tamil like to travel and gardening.

‚∏ª