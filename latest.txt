‚úÖ Morning News Briefing ‚Äì October 03, 2025 10:42

üìÖ Date: 2025-10-03 10:42
üè∑Ô∏è Tags: #briefing #ai #publichealth #digitalgov

‚∏ª

üßæ Weather
‚Ä¢ No watches or warnings in effect, Pembroke
  No watches or warnings in effect. No warnings or watches or watches in effect . Watch or warnings are no longer in effect in the U.S. No watches, warnings are in effect for the rest of the day . No watches and warnings are still in effect, but no watches are in place for the day's events . The weather is not expected to be affected by the weather .
‚Ä¢ Current Conditions:  1.7¬∞C
  Temperature: 1.7&deg;C Pressure / Tendency: 102.4 kPa falling Humidity: 97 % Humidity : 97 % Dewpoint: 1 .2&deg:C Wind: SSW calm km/h . Air Quality Health Index: n/a . Pembroke 6:00 AM EDT Friday 3 October 2025 . Weather forecast: Pem
‚Ä¢ Friday: Mainly sunny. High 25.
  Sunny. Becoming a mix of sun and cloud this afternoon . High 25. Humidex 26. UV index 5 or moderate . High for most of the day with a high of 25.50 degrees Fahrenheit in the early hours of Friday morning . Forecast issued 5:00 AM EDT Friday 3 October 2025. For more information on the weather, visit http://www.cnn

üåç International News
No updates.

üçÅ Canadian News
No updates.

üá∫üá∏ U.S. Top Stories
‚Ä¢ Sarah Mullally named first woman Archbishop of Canterbury
  Sarah Mullally has been named as the new Archbishop of Canterbury . She is the first woman to be chosen to lead the world's 85 million Anglicans . Mullally is also the first female to be named as Archbishop of the Canterbury Cathedral's first female head of the Anglican church . The archbishop will be in charge of the 85m Anglicans in the UK and the rest of
‚Ä¢ UK police release names of the 2 victims of the Manchester synagogue attack
  Greater Manchester Police named Adrian Daulby, 53, and Melvin Cravitz, 66 as the victims of Thursday's attack . Three more remain hospitalized in serious condition in Manchester . Greater Manchester police named the two victims of the attack on Thursday's 23rd anniversary of the terrorist attack in which three people were killed and injured in the attack in a car bomb blast in the city centre .
‚Ä¢ What to know about former U.K. leader Tony Blair, tapped by Trump for postwar Gaza role
  As Britain's prime minister, Tony Blair succeeded in negotiating peace in Northern Ireland in 1998 . Five years later, he joined the U.S.-led invasion of Iraq ‚Äî sullying his reputation ever since . Blair's reputation has since been tarnished by the Iraq invasion, which he joined in the invasion of the country's oil-rich territory . Blair was prime minister of Northern Ireland
‚Ä¢ Even a government shutdown can't stop the quiz. Can you score 11?
  This week's quiz also features bears . Check out the latest round of the weekly quiz to test your knowledge of bears . This week, please submit your answers to the quiz to the Daily Mail's weekly quiz . Back to the page you came from last week's version of this week's edition of The Daily Mail Online quiz . Please submit your quiz for next week's Newsquiz .
‚Ä¢ A public broadcaster's path after losing U.S. funds: Youth sports and less local news
  South Dakota Public Broadcasting says there's an ironic result to President Trump's successful attack on public media: It will have to rely more on NPR programs . The state will rely more heavily on NPR shows than other public media programs, the state's public broadcasting network says . South Dakota's public media network says it will need to rely on NPR for more time in the coming years . President Trump

üß† Artificial Intelligence
No updates.

üíª Digital Strategy
‚Ä¢ Retro nerd hacks LEGO's Game Boy into the real deal
  Modder crams working hardware into plastic shell and fires up Tetris . LEGO's new Game Boy creation, performed some suitably geeky magic, and turned it into a real Game Boy . LEGO‚Äôs new game Boy creation has been created by an enterprising nerd . It was created by a modder who used LEGO's plastic shell to create a Game Boy replica of the
‚Ä¢ Struggling to heat your home? How about 500 Raspberry Pi units?
  UK Power Networks trials Thermify's HeatHub boilers, swapping gas flames for clustered compute . UKPN is installing mini datacenters powered by Raspberry Pi hardware in customers homes to provide heating for families struggling with energy costs . The network is taking an unusual approach, installing mini-datacenters in customers' homes to heat their homes for their own use of energy efficient technology .
‚Ä¢ Criminals take Renault UK customer data for a joyride
  Names, numbers, and reg plates exposed in latest auto industry cyber-shunt . Renault UK customers are being warned their personal data may be in criminal hands after one of its supplier was hacked . Renault customers are warned their data may have been compromised . Names, number plates, reg plates and other data may also be in the hands of criminal criminals, it has been revealed . It is
‚Ä¢ How the ONS data-sharing dream ended in budget cuts and three rival platforms
  UK Treasury called time on troubled integration scheme after ¬£240M sunk Analysis . UK's Office for National Statistics (ONS) launched a plan to integrate government data and provide "high quality analysis that reflects the diversity of economic and social experience in our country" ONS provides data vital to form public sector policy and allocate resources in the UK . ONS: Data will be integrated into government data
‚Ä¢ Energy drink company punished ERP graybeard for going too fast
  On-Call is the weekly reader-contributed column that shares your tales of trying to deliver speedy tech support . Techies may need a jolt of energy to get through the final day of the working week, so we deliver it in the form of a new instalment of On Call . On Call is a weekly feature of the weekly readers' stories of dealing with tech support

üè• Public Health
No updates.

üî¨ Science
‚Ä¢ The impact of confounders, spillovers and interactions on social distancing policy effects estimates
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Unmasking hidden cardiovascular risk: masked hypertension, aortic stiffness and cardiac remodeling in women with prior preeclampsia
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Exclusive: ex-CDC director talks about why she was fired
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Monitoring changes in vitamin D levels during the COVID-19 pandemic with routinely-collected laboratory data
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Organization and delivery of medical services at the 19th Asian Games in Hangzhou China
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations

üßæ Government & Policy
No updates.

üèõÔ∏è Enterprise Architecture & IT Governance
No updates.

ü§ñ AI & Emerging Tech
‚Ä¢ Microsoft says AI can create ‚Äúzero day‚Äù threats in biology
  A team at Microsoft says it used artificial intelligence to discover a &#8220;zero day&#8221; vulnerability in the biosecurity systems used to prevent the misuse of DNA.



These screening systems are designed to stop people from purchasing genetic sequences that could be used to create deadly toxins or pathogens. But now researchers led by Microsoft‚Äôs chief scientist, Eric Horvitz, says they have figured out how to bypass the protections in a way previously unknown to defenders.&nbsp;



The team described its work today in the journal Science.



Horvitz and his team focused on generative AI algorithms that propose new protein shapes. These types of programs are already fueling the hunt for new drugs at well-funded startups like Generate Biomedicines and Isomorphic Labs, a spinout of Google.&nbsp;



The problem is that such systems are potentially ‚Äúdual use.‚Äù They can use their training sets to generate both beneficial molecules and harmful ones.



Microsoft says it began a ‚Äúred-teaming‚Äù test of AI‚Äôs dual-use potential in 2023 in order to determine whether ‚Äúadversarial AI protein design‚Äù could help bioterrorists manufacture harmful proteins.&nbsp;



The safeguard that Microsoft attacked is what‚Äôs known as biosecurity screening software. To manufacture a protein, researchers typically need to order a corresponding DNA sequence from a commercial vendor, which they can then install in a cell. Those vendors use screening software to compare incoming orders with known toxins or pathogens. A close match will set off an alert.





To design its attack, Microsoft used several generative protein models (including its own, called EvoDiff) to redesign toxins‚Äîchanging their structure in a way that let them slip past screening software but was predicted to keep their deadly function intact.



The researchers say the exercise was entirely digital and they never produced any toxic proteins. That was to avoid any perception that the company was developing bioweapons.&nbsp;



Before publishing the results, Microsoft says, it alerted the US government and software makers, who‚Äôve already patched their systems, although some AI-designed molecules can still escape detection.&nbsp;



‚ÄúThe patch is incomplete, and the state of the art is changing. But this isn‚Äôt a one-and-done thing. It‚Äôs the start of even more testing,‚Äù says Adam Clore, director of technology R&amp;D at Integrated DNA Technologies, a large manufacturer of DNA, who is a coauthor on the Microsoft report. ‚ÄúWe‚Äôre in something of an arms race.‚Äù



To make sure nobody misuses the research, the researchers say, they‚Äôre not disclosing some of their code and didn‚Äôt reveal what toxic proteins they asked the AI to redesign. However, some dangerous proteins are well known, like ricin‚Äîa poison found in castor beans‚Äîand the infectious prions that are the cause of mad-cow disease.



‚ÄúThis finding, combined with rapid advances in AI-enabled biological modeling, demonstrates the clear and urgent need for enhanced nucleic acid synthesis screening procedures coupled with a reliable enforcement and verification mechanism,‚Äù says Dean Ball, a fellow at the Foundation for American Innovation, a think tank in San Francisco.



Ball notes that the US government already considers screening of DNA orders a key line of security. Last May, in an executive order on biological research safety, President Trump called for an overall revamp of that system, although so far the White House hasn‚Äôt released new recommendations.



Others doubt that commercial DNA synthesis is the best point of defense against bad actors. Michael Cohen, an AI-safety researcher at the University of California, Berkeley, believes there will always be ways to disguise sequences and that Microsoft could have made its test harder.



‚ÄúThe challenge appears weak, and their patched tools fail a lot,‚Äù says Cohen. ‚ÄúThere seems to be an unwillingness to admit that sometime soon, we‚Äôre going to have to retreat from this supposed choke point, so we should start looking around for ground that we can actually hold.‚Äù&nbsp;



Cohen says biosecurity should probably be built into the AI systems themselves‚Äîeither directly or via controls over what information they give.&nbsp;



But Clore says monitoring gene synthesis is still a practical approach to detecting biothreats, since the manufacture of DNA in the US is dominated by a few companies that work closely with the government. By contrast, the technology used to build and train AI models is more widespread. ‚ÄúYou can‚Äôt put that genie back in the bottle,‚Äù says Clore. ‚ÄúIf you have the resources to try to trick us into making a DNA sequence, you can probably train a large language model.‚Äù
‚Ä¢ The Download: RIP EV tax credits, and OpenAI‚Äôs new valuation
  This is today&#8217;s edition of¬†The Download,¬†our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



EV tax credits are dead in the US. Now what?



Federal EV tax credits in the US officially came to an end yesterday.Those credits, expanded and extended in the 2022 Inflation Reduction Act, gave drivers up to $7,500 toward the purchase of a new electric vehicle. They‚Äôve been a major force in cutting the up-front costs of EVs, pushing more people toward purchasing them and giving automakers confidence that demand would be strong.The tax credits‚Äô demise comes at a time when battery-electric vehicles still make up a small percentage of new vehicle sales in the country. So what‚Äôs next for the US EV market?



‚ÄîCasey Crownhart



This article is from The Spark, MIT Technology Review‚Äôs weekly climate newsletter. To receive it in your inbox every Wednesday, sign up here.



If you‚Äôre interested in reading more about EVs and clean energy, take a look at:



+ The US could really use an affordable electric truck. Ford recently announced plans for a $30,000 electric pickup, which could be the shot in the arm that the slowing US EV market needs. Read the full story.+ What role should oil and gas companies play in climate tech, really?+ China is an EV-building powerhouse. These three charts explain its energy dominance. Read the full story.+ Supporting new technologies like EVs can be expensive, but deciding when to wean the public off incentives can be a difficult balancing act. Read the full story.







The must-reads



I‚Äôve combed the internet to find you today‚Äôs most fun/important/scary/fascinating stories about technology.



1 OpenAI has become the world‚Äôs most valuable startupMove aside, SpaceX. (Bloomberg $)+ OpenAI is now valued at an eye-watering $500 billion. (FT $)+ The valuation came after workers sold around $6.6 billion in shares. (Reuters)



2 Music labels are close to striking AI licensing dealsUniversal and Warner are trying their best to avoid the mis-steps of the internet era. (FT $)+ AI is coming for music, too. (MIT Technology Review)



3 Facebook‚Äôs political ads are full of spam and scamsAnd deepfake technology is making them more convincing than ever. (NYT $)+ Meta will start using conversations with its chatbots to personalize ads. (WSJ $)4 China is forging ahead with integrating AI tools into children‚Äôs livesBut educators worry they‚Äôll harm youngsters&#8217; learning and social skills. (Rest of World)+ Chinese universities want students to use more AI, not less. (MIT Technology Review)



5 The batteries of the future could be created by AI¬†Researchers including Microsoft are experimenting with materials suggested by models. (IEEE Spectrum)+ This startup wants to use the Earth as a massive battery. (MIT Technology Review)



6 A historian claims to have used AI to identify an anonymous NaziDigital tools helped J√ºrgen Matth√§us to pinpoint the person photographed beside a mass grave. (The Guardian)



7 The Pentagon is interested in AI-powered machine guns that shoot dronesSteven Simoni‚Äôs Allen Control Systems is part of Silicon Valley‚Äôs new military pivot. (Reuters)+ We saw a demo of the new AI system powering Anduril‚Äôs vision for war. (MIT Technology Review)



8 One of Saturn‚Äôs moons may have once hosted life Enceladus has all the necessary keystones to support life, and future missions could uncover it. (Scientific American $)+ Meanwhile, Blue Origin has won a NASA rover contract. (Wired $)+ The case against humans in space. (MIT Technology Review)



9 Chatbots exercise all sorts of tricks to keep you talkingThey don‚Äôt want the conversation to end, a new study has found. (Wired $)



10 What it‚Äôs like to become a viral memeDrew Scanlon, aka ‚ÄúBlinking Guy,‚Äù is leveraging his fame for a good cause. (SF Gate)







Quote of the day



‚ÄúI cannot overstate how disgusting I find this kind of ‚ÄòAI‚Äô dog shit in the first place, never mind under these circumstances.‚Äù



‚ÄîWriter Luke O‚ÄôNeil tells 404 Media his feelings about an AI-generated ‚Äúbiography‚Äù of journalist Kaleb Horton, who recently died.







One more thing







A day in the life of a Chinese robotaxi driver



When Liu Yang started his current job, he found it hard to go back to driving his own car: ‚ÄúI instinctively went for the passenger seat. Or when I was driving, I would expect the car to brake by itself,‚Äù says the 33-year-old Beijing native, who joined the Chinese tech giant Baidu in January 2021 as a robotaxi driver.



Liu is one of the hundreds of safety operators employed by Baidu, ‚Äúdriving‚Äù five days a week in Shougang Park. But despite having only worked for the company for 19 months, he already has to think about his next career move, as his job will likely be eliminated within a few years. Read the full story.



‚ÄîZeyi Yang







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)



+ Congratulations are in order for 32 Chunk, winner of this year‚Äôs highly prestigious Fat Bear Week competition + Here‚Äôs how 10 women artists got their days off to the best start possible.+ This Instagram account documenting the worldly travels of a cassette player is fab.+ Brb, I‚Äôm off to listen to Arctic Outpost Radio, spinning records from the very top of the world.
‚Ä¢ EV tax credits are dead in the US. Now what?
  On Wednesday, federal EV tax credits in the US officially came to an end.



Those credits, expanded and extended in the 2022 Inflation Reduction Act, gave drivers up to $7,500 in credits toward the purchase of a new electric vehicle. They‚Äôve been a major force in cutting the up-front costs of EVs, pushing more people toward purchasing them and giving automakers confidence that demand would be strong.





The tax credits‚Äô demise comes at a time when battery-electric vehicles still make up a small percentage of new vehicle sales in the country. And transportation is a major contributor to US climate pollution, with cars, trucks, ships, trains, and planes together making up roughly 30% of total greenhouse-gas emissions.



To anticipate what‚Äôs next for the US EV market, we can look to countries like Germany, which have ended similar subsidy programs. (Spoiler alert: It‚Äôs probably going to be a rough end to the year.)



When you factor in fuel savings, the lifetime cost of an EV can already be lower than that of a gas-powered vehicle today. But EVs can have a higher up-front cost, which is why some governments offer a tax credit or rebate that can help boost adoption for the technology.



In 2016, Germany kicked off a national incentive program to encourage EV sales. While the program was active, drivers could get grants of up to about ‚Ç¨6,000 toward the purchase of a new battery-electric or plug-in hybrid vehicle.



Eventually, the government began pulling back the credits. Support for plug-in hybrids ended in 2022, and commercial buyers lost eligibility in September 2023. Then the entire program came to a screeching halt in December 2023, when the government announced it would be ending the incentives with about one week‚Äôs notice.



Monthly sales data shows the fingerprints of those changes. In each case where there‚Äôs a contraction of public support, there‚Äôs a peak in sales just before a cutback, then a crash after. These short-term effects can be dramatic: There were about half as many battery-electric vehicles sold in Germany in January 2024 than there were in December 2023.&nbsp;





We‚Äôre already seeing the first half of this sort of boom-bust cycle in the US: EV sales ticked up in August, making up about 10% of all new vehicle sales, and analysts say September will turn out to be a record-breaking month. People rushed to take advantage of the credits while they still could.



Next comes the crash‚Äîthe next few months will probably be very slow for EVs. One analyst predicted to the Washington Post that the figure could plummet to the low single digits, ‚Äúlike 1 or 2%.‚Äù



Ultimately, it‚Äôs not terribly surprising that there are local effects around these policy changes. ‚ÄúThe question is really how long this decline will last, and how slowly any recovery in the growth will be,‚Äù Robbie Andrew, a senior researcher at the CICERO Center for International Climate Research in Norway who collects EV sales data, said in an email.&nbsp;



When I spoke to experts (including Andrew) for a story last year, several told me that Germany‚Äôs subsidies were ending too soon, and that they were concerned about what cutting off support early would mean for the long-term prospects of the technology in the country. And Germany was much further along than the US, with EVs making up 20% of new vehicle sales‚Äîtwice the American proportion.



EV growth did see a longer-term backslide in Germany after the end of the subsidies. Battery-electric vehicles made up 13.5% of new registrations in 2024, down from 18.5% the year before, and the UK also passed Germany to become Europe‚Äôs largest EV market.&nbsp;



Things have improved this year, with sales in the first half beating records set in 2023. But growth would need to pick up significantly for Germany to reach its goal of getting 15 million battery-electric vehicles registered in the country by 2030. As of January 2025, that number was just 1.65 million.&nbsp;



According to early projections, the end of tax credits in the US could significantly slow progress on EVs and, by extension, on cutting emissions. Sales of battery-electric vehicles could be about 40% lower in 2030 without the credits than what we‚Äôd see with them, according to one analysis by Princeton University‚Äôs Zero Lab.



Some US states still have their own incentive programs for people looking to buy electric vehicles. But without federal support, the US is likely to continue lagging behind global EV leaders like China.&nbsp;



As Andrew put it: ‚ÄúFrom a climate perspective, with road transport responsible for almost a quarter of US total emissions, leaving the low-hanging fruit on the tree is a significant setback.‚Äù&nbsp;



This article is from The Spark, MIT Technology Review‚Äôs weekly climate newsletter. To receive it in your inbox every Wednesday, sign up here.
‚Ä¢ Turning migration into modernization
  In late 2023, a long-trusted virtualization staple became the biggest open question on the enterprise IT roadmap . CIOs contending with pricing hikes and product roadmap opacity face a daunting choice: double‚Äëdown on a familiar but costlier stack, or use the disruption to rethink how‚Äîand where‚Äîcritical workloads should run . Forrester predicted that one in five large VMware customers would begin moving away from the platform in 2024 .
‚Ä¢ Roundtables: Trump‚Äôs Impact on the Next Generation of Innovators
  MIT Technology Review has honored dozens of young researchers on our Innovators Under 35 list . We checked back in with recent honorees to see how they‚Äôre faring amid sweeping changes . Learn about the complex realities of what life has been like for those aiming to build their labs and companies in today‚Äôs political climate . This was the third event in a special, three-part Roundtables series that also included: The Future of Birth Control Control .

üîí Cybersecurity & Privacy
No updates.

üéì University AI
No updates.

üè¢ Corporate AI
‚Ä¢ Enhance agentic workflows with enterprise search using Kore.ai and Amazon Q Business
  This post was written with Meghana Chintalapudi and Surabhi Sankhla of Kore.ai. 
As organizations struggle with exponentially growing volumes of data distributed across multiple repositories and applications, employees lose significant time‚Äîapproximately 30% according to the International Data Corporation (IDC)‚Äîsearching for information that could be spent on higher-value work. The complexity of modern enterprise data networks demands solutions that can efficiently integrate, process, and deliver actionable insights across disparate systems. 
In this post, we demonstrate how organizations can enhance their employee productivity by integrating Kore.ai‚Äôs AI for Work platform with Amazon Q Business. We show how to configure AI for Work as a data accessor for Amazon Q index for independent software vendors (ISVs), so employees can search enterprise knowledge and execute end-to-end agentic workflows involving search, reasoning, actions, and content generation. We explore the key benefits of this integration, including advanced search capabilities across more than 90 enterprise connectors and how to extend agentic experiences on top of a search foundation. The post includes a step-by-step implementation guide to help you set up this integration in your environment. 
Components of the integration 
Kore.ai is a leading Enterprise AI platform consistently recognized by Gartner as a leader in conversational AI. With three key Kore.ai offerings, AI for Work, AI for Process, and AI for Service, enterprises can build and deploy AI solutions based on their business needs. The AI for Work platform helps employees be more productive by making it possible to search across applications, take context-aware actions, generate content, and automate repetitive tasks. The platform goes beyond standalone search to deliver comprehensive agentic orchestration and workflows, helping employees follow up with clients, send weekly updates, or research and write marketing content with a single command. With AI for Work, your employees can create simple no-code agents while your admins have the flexibility to create more advanced low-code or pro-code agents. AI for Process, on the other hand, automates knowledge-intensive business processes end-to-end. AI for Service helps organizations deliver differentiated customer service experiences through self-service, proactive outreach campaigns, and agent assistance. 
Amazon Q index for ISVs is a powerful, managed vector search service that supports seamless integration of generative AI applications with customers‚Äô enterprise data through a unified, secure index. ISVs can access and retrieve relevant content through the SearchRelevantContent API for cross-application data retrieval without needing direct access or individual indexing of each data source, while customers retain full control over data access and governance. 
When combined with additional search connectors offered by AI for Work platform and its ability to create and orchestrate agents, organizations gain a complete solution that transforms how employees access enterprise data and execute tasks end-to-end. The following video shows one such agentic experience in action, where the AI for Work interface seamlessly orchestrates agents to help a sales executive prepare for a client meeting‚Äîcompiling information from Amazon Q index and AI for Work connectors, summarizing talking points, and sending them as an email, all from a single query. 

 
  
 
 
Benefits for enterprises 
Enterprises often struggle with fragmented data access and repetitive manual tasks that slow down critical business processes. For example, imagine a scenario where a product manager needs to compile quarterly feature requests‚Äîwith the integration of Kore.ai‚Äôs AI for Work and Amazon Q index, they can instantly gather requests from Salesforce, support tickets, and JIRA; automatically generate a structured roadmap; and schedule stakeholder meetings, all with a single query. This seamless integration changes the way enterprises interact with enterprise systems, through multiple key advantages: 
 
 Improved search capabilities ‚Äì Amazon Q index augments the generative AI experience by providing semantically relevant enterprise content across connected systems through its distributed vector database, delivering query responses at enterprise scale. Now, together with AI for Work, your employees can search data from over 90 connectors, integrating with enterprise systems like Microsoft 365, Salesforce, and Workday while also connecting with custom internal knowledge systems and third-party search providers. AI for Work‚Äôs orchestrator manages complex query processing and agent routing across multiple data sources, resulting in contextually appropriate and actionable results that significantly reduce search time while also enabling intelligent automations that extend far beyond traditional search capabilities. 
 Enhanced data processing ‚Äì The system continuously ingests and analyzes data through the document processing pipeline in Amazon Q index, which automatically handles multiple formats using intelligent chunking algorithms that preserve semantic context. The AI for Work platform unifies search, content generation, and actions in a single interface, to support the creation of multi-step agentic experiences grounded in search. Through real-time incremental indexing that processes only changed content, the system maintains data freshness while converting siloed raw data into actionable insights and multi-step business processes that can be saved and reused across the organization. 
 Cost optimization ‚Äì Organizations can achieve significant cost savings by streamlining routine tasks through agents that reduce operational overhead and improve resource allocation. AI for Work supports a wide range of agent-building options, from no-code and low-code to pro-code, for both non-technical employees and technical experts to build agents for themselves and to share across the organization, so teams can accomplish more with existing resources and benefit from sustained productivity improvements. 
 Security benefits ‚Äì Security remains paramount, with Amazon Q index implementing vector-level security through end-to-end encryption using AWS Key Management Service (AWS KMS) customer managed keys and document-level access controls that filter search results based on user identity and group membership. The joint solution implements robust role-based access control and audit trails. This zero-trust security approach maintains compliance with industry standards while providing granular control over sensitive enterprise data, making sure users only see information from documents they have explicit permissions to access while maintaining complete data sovereignty. With AI for Work‚Äôs robust security and governance tools enterprises can manage permissions and agent access, monitor usage, and enforce guardrails for secure, enterprise-wide deployment of AI solutions at scale. 
 
Solution overview 
The Amazon Q Business data accessor provides a secure interface that integrates Kore.ai‚Äôs AI for Work platform with Amazon Q index. The integration delivers a robust solution that uses enterprise data across multiple systems to power intelligent agentic actions and content generation capabilities that transform how organizations handle routine tasks and automate complex processes end-to-end. 
When a user submits a query through AI for Work, its orchestrator intelligently routes requests between Kore.ai‚Äôs native retrievers and Amazon Q index based on predefined routing rules and advanced intent recognition algorithms. For Amazon Q index requests, the architecture implements secure cross-account API calls using OAuth 2.0 tokens that transform into temporary AWS credentials, supporting both security and optimal performance while maintaining strict access controls throughout the entire system. With AI for Work‚Äôs agents, users can take follow up actions, such as drafting proposals or submitting tickets‚Äîdirectly on top of search results, for end-to-end task completion in a single interface. Users can also build personalized workflows of pre-defined steps and execute them from a single query to further save time. 
This supports use cases such as automated roadmap generation, where a product manager can query feature requests across multiple systems and receive a structured roadmap complete with stakeholder notifications, or RFP response automation, where sales executives can generate comprehensive proposals by pulling compliance documentation and tailoring responses based on client requirements. 
The following diagram illustrates the solution architecture. 
 
Prerequisites 
Before enabling the Amazon Q index integration with Kore.ai‚Äôs AI for Work, you must have the following components in place: 
 
 An AWS account with appropriate service access 
 Amazon Q Business set up with AWS IAM Identity Center for user authentication 
 Access to Kore.ai‚Äôs AI for Work (as a workspace admin) 
 
With these prerequisites met, you can complete the basic configuration steps on both the Amazon Q Business and Kore.ai consoles to get started. 
Add Kore.ai as a data accessor 
After creating an Amazon Q Business application with AWS IAM Identity Center, administrators can configure Kore.ai as a data accessor through the Amazon Q Business console. Complete the following steps: 
 
 On the Amazon Q Business console, choose Data accessors in the navigation pane. 
 Choose Add data accessor. 
 Choose Kore.ai as your data accessor. You must retrieve tenantID, a unique identifier for your application tenant. Refer to Prerequisites for instructions to retrieve the TenantId for your application. Similar instructions are also listed later in this post. 
 For Data source access, configure your level of access. You can select specific data sources from your Amazon Q index to be available through the data accessor. This makes it possible to control which content is surfaced in the AI for Work environment. 
 For User access, specify which users or groups can access the Amazon Q index through the data accessor. This option makes it possible to configure granular permissions for data accessor accessibility and manage organizational access controls. 
 
 
After you have added the data accessor, the Amazon Q Business console displays configuration details that you need to share with Kore.ai to complete the setup. 
 
 Note down the following information for the next step: 
   
   Amazon Q Business application ID 
   AWS Region of the Amazon Q Business application 
   Amazon Q Business retriever ID 
   Region for IAM Identity Center instance 
    
 
Configure Amazon Q index in Kore.ai‚Äôs AI for Work 
Kore.ai‚Äôs AI for Work supports flexible integration with Amazon Q index based on your enterprise search needs. There are two configuration options: configuring Amazon Q index as the primary enterprise knowledge source or configuring it as a search agent. We provide instructions for both options in this post. 
Option 1: Configure Amazon Q index as the primary enterprise knowledge source 
If you want Amazon Q index to act as the primary fallback search layer, coming into play, complete the following steps: 
 
 In AI for Work, go to Workspaces on the admin console. Then navigate to Enterprise Workspace, which is the default workspace. 
 
 
 
 Choose Configure to configure an enterprise knowledge data source. 
 On the Create New dropdown menu, choose Amazon Q. 
 
 
 
 Enter a source name and brief description. 
 Copy the tenant ID displayed‚Äîthis is required during the setup of the data accessor in AWS, as described in the previous section. 
 Enter the details captured earlier: 
   
   Amazon Q Business application ID 
   Region of the Amazon Q Business application 
   Amazon Q Business retriever ID 
   Region for IAM Identity Center instance 
    
 Choose Continue to save and complete the configuration. 
 
 
The new knowledge source now shows as Active. 
 
Option 2: Configure Amazon Q index as a search agent 
If you already have a primary search index, you can configure Amazon Q index as a search agent: 
 
 In AI for Work, go to Workspaces on the admin console. 
 Choose the workspace where you want to add Amazon Q index. (Enterprise Workspace is used by default). 
 Under AI Agents in the navigation pane, choose Search Agent 
 Choose Create agent. 
 
 
 
 Provide an agent name and purpose. This helps define when the search agent should be invoked. 
 Choose Continue to move to configuration. 
 For Select Search Index, choose Amazon Q. 
 
 
 
 Copy the tenant ID displayed‚Äîit is required during the setup of the data accessor in AWS. 
 
 
 
 Preview and test the agent. 
 After you have validated the agent, publish it to selected users or groups. 
 
Your integration is now complete. You can now access the assistant application and start asking questions in the AI for Work console. If you‚Äôve created a search agent, you can also access it from the list of agents and start interacting with it directly. 
Clean up 
When you are finished using this solution, clean up your resources to avoid additional costs: 
 
 Disable the Amazon Q index configuration within AI for Work‚Äôs settings. 
 Delete the Kore.ai data accessor from the Amazon Q Business console, which will remove permissions and access for users. 
 Delete the Amazon Q Business application to remove the associated index and data source connectors, on your AWS account. 
 
Conclusion 
The combination of Kore.ai‚Äôs AI for Work and Amazon Q index offers enterprises a transformative approach to boost employee productivity leveraging comprehensive search capabilities while streamlining repetitive tasks and processes. By integrating Kore.ai‚Äôs advanced agentic platform with the robust search infrastructure of Amazon Q index, organizations can now execute context aware actions by accessing relevant information across disparate systems while maintaining data ownership and security. This supports faster problem-solving, enhanced productivity, and better collaboration across the organization. 
In this post, we explored how enterprises can use the integration between Kore.ai‚Äôs AI for Work and Amazon Q Business to streamline their operational processes and unlock valuable productivity gains. We demonstrated how organizations can set up this integration using an Amazon Q data accessor, helping teams access critical information securely and cost-effectively. 
Unlock the full potential of your organization‚Äôs data and agentic workflows today with the Amazon Q index and Kore.ai‚Äôs AI for Work‚Äôs unified solution by following the steps in Amazon Q integration with AI for Work. 
 
About the authors 
Siddhant Gupta is a Software Development Manager on the Amazon Q team based in Seattle, WA. He is driving innovation and development in cutting-edge AI-powered solutions. 
Chinmayee Rane is a Generative AI Specialist Solutions Architect at AWS, with a core focus on generative AI. She helps ISVs accelerate the adoption of generative AI by designing scalable and impactful solutions. With a strong background in applied mathematics and machine learning, she specializes in intelligent document processing and AI-driven innovation. Outside of work, she enjoys salsa and bachata dancing. 
Bobby Williams is a Senior Solutions Architect at AWS. He has decades of experience designing, building, and supporting enterprise software solutions that scale globally. He works on solutions across industry verticals and horizontals and is driven to create a delightful experience for every customer. 
Santhosh Urukonda is a Senior PACE (Prototyping &amp; Cloud Engineering) Architect at AWSs with two decades of experience. He specializes in helping customers develop innovative, first-to-market solutions with a focus on generative AI. 
Nikhil Kumar Goddeti is a Cloud Support Engineer II at AWS. He specializes in AWS Data Analytics services with emphasis on Amazon OpenSearch Service, Amazon Q Business, Amazon Kinesis, Amazon MSK, Amazon AppFlow, and Amazon Kendra. He is a Subject Matter Expert of OpenSearch. Outside of work, he enjoys travelling with his friends and playing cricket. 
Meghana Chintalapudi is a Product Manager at Kore.ai, driving the development of search and agentic AI solutions for the AI for Work platform. She has led large-scale AI implementations for Fortune 500 clients, evolving from deterministic NLP and intent-detection models to advanced large language model deployments, with a strong emphasis on enterprise-grade security and scalability. Outside of work, Meghana is a dancer and takes movement workshops in Hyderabad, India. 
Surabhi Sankhla is a VP of Product at Kore.ai, where she leads the AI for Work platform to help enterprises boost employee productivity. With over 13 years of experience in product management and technology, she has launched AI products from the ground up and scaled them to millions of users. At Kore.ai, she drives product strategy, client implementations, and go-to-market execution in partnership with cross-functional teams. Based in San Francisco, Surabhi is passionate about making AI accessible and impactful for all.
‚Ä¢ Accelerate development with the Amazon Bedrock AgentCore MCP server
  Today, we‚Äôre excited to announce the Amazon Bedrock AgentCore Model Context Protocol (MCP) Server. With built-in support for runtime, gateway integration, identity management, and agent memory, the AgentCore MCP Server is purpose-built to speed up creation of components compatible with Bedrock AgentCore. You can use the AgentCore MCP server for rapid prototyping, production AI solutions, or to scale your agent infrastructure for your enterprise. 
Agentic IDEs like Kiro, Claude Code, GitHub Copilot, and Cursor, along with sophisticated MCP servers are transforming how developers build AI agents. What typically takes significant time and effort, for example learning about Bedrock AgentCore services, integrating Runtime and Tools Gateway, managing security configurations, and deploying to production can now be completed in minutes through conversational commands with your coding assistant. 
In this post we introduce the new AgentCore MCP server and walk through the installation steps so you can get started. 
AgentCore MCP server capabilities 
The AgentCore MCP server brings a new agentic development experience to AWS, providing specialized tools that automate the complete agent lifecycle, eliminate the steep learning curve, and reduce development friction that can slow innovation cycles.&nbsp;To address specific agent development challenges the AgentCore MCP server: 
 
 Transforms agents for AgentCore Runtime integration by providing guidance to your coding assistant on the minimum functionality changes needed‚Äîadding Runtime library imports, updating dependencies, initializing apps with BedrockAgentCoreApp(), converting entrypoints to decorators, and changing direct agent calls to payload handling‚Äîwhile preserving your existing agent logic and Strands Agents features. 
 Automates development environment provisioning by handling the complete setup process through your coding assistant: installing required dependencies (bedrock-agentcore SDK, bedrock-agentcore-starter-toolkit CLI helpers, strands-agents SDK), configuring AWS credentials and AWS Regions, defining execution roles with Bedrock AgentCore permissions, setting up ECR repositories, and creating .bedrock_agentcore.yaml configuration files. 
 Simplifies tool integration with Bedrock AgentCore Gateway for seamless agent-to-tool communication in the cloud environment. 
 Enables simple agent invocation and testing by providing natural language commands through your coding assistant to invoke provisioned agents on AgentCore Runtime and verify the complete workflow, including calls to AgentCore Gateway tools when applicable. 
 
Layered approach 
When using the AgentCore MCP server with your favorite client, we encourage you to consider a layered architecture designed to provide comprehensive AI agent development support: 
 
 Layer 1: Agentic IDE or client ‚Äì Use Kiro, Claude Code, Cursor, VS Code extensions, or another natural language interface for developers. For very simple tasks, agentic IDEs are equipped with the right tools to look up documentation and perform tasks specific to Bedrock AgentCore. However, with this layer alone, developers may observe sub-optimal performance across AgentCore developer paths. 
 Layer 2: AWS service documentation ‚Äì Install the AWS Documentation MCP Server&nbsp;for comprehensive AWS service documentation, including context about Bedrock AgentCore. 
 Layer 3: Framework documentation ‚Äì Install the Strands, LangGraph, or other framework docs MCP servers or use the llms.txt for framework-specific context. 
 Layer 4: SDK documentation ‚Äì Install the MCP or use the llms.txt for the Agent Framework SDK and Bedrock AgentCore SDK for a combined documentation layer that covers the Strands Agents SDK documentation and Bedrock AgentCore API references. 
 Layer 5: Steering files ‚Äì Task-specific guidance for more complex and repeated workflows. Each IDE has a different approach to using steering files (for example, see Steering in the Kiro documentation). 
 
Each layer builds upon the previous one, providing increasingly specific context so your coding assistant can handle everything from basic AWS operations to complex agent transformations and deployments. 
Installation 
To get started with the Amazon Bedrock AgentCore MCP server you can use the one-click install on the Github repository. 
Each IDE integrates with an MCP differently using the mcp.json file. Review the MCP documentation for your IDE, such as Kiro, Cursor, Q CLI, and Claude Code to determine the location of the mcp.json. 
 
  
   
   Client 
   Location of mcp.json 
   Documentation 
   
   
   Kiro 
   .kiro/settings/mcp.json 
   https://kiro.dev/docs/mcp/ 
   
   
   Cursor 
   .cursor/mcp.json 
   https://cursor.com/docs/context/mcp 
   
   
   Q CLI 
   ~/.aws/amazonq/mcp.json 
   https://docs.aws.amazon.com/amazonq/latest/qdeveloper-ug/qdev-mcp.html 
   
   
   Claude Code 
   ~/.claude/mcp.json 
   https://docs.claude.com/en/docs/claude-code/mcp 
   
  
 
Use the following in your mcp.json: 
 
 {
&nbsp;&nbsp;"mcpServers": {
&nbsp;&nbsp; &nbsp;"awslabs.amazon-bedrock-agentcore-mcp-server": {
&nbsp;&nbsp; &nbsp; &nbsp;"command": "uvx",
&nbsp;&nbsp; &nbsp; &nbsp;"args": ["awslabs.amazon-bedrock-agentcore-mcp-server@latest"],
&nbsp;&nbsp; &nbsp; &nbsp;"env": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"FASTMCP_LOG_LEVEL": "ERROR"
&nbsp;&nbsp; &nbsp; &nbsp;},
&nbsp;&nbsp; &nbsp; &nbsp;"disabled": false,
&nbsp;&nbsp; &nbsp; &nbsp;"autoApprove": []
&nbsp;&nbsp; &nbsp;}
&nbsp;&nbsp;}
} 
 
For example, here is what the IDE looks like on Kiro, with the AgentCore MCP server and the two tools, search_agentcore_docs and fetch_agentcore_doc, connected: 
 
Using the AgentCore MCP server for agent development 
While we show demos for various use cases below using the Kiro IDE, the AgentCore MCP server has also been tested to work on Claude Code, Amazon Q CLI, Cursor, and the VS Code Q plugin. First, let‚Äôs take a look at a typical agent development lifecycle using AgentCore services (remember that this is only one example with the tools available, and you are free to explore more such use cases simply by instructing the agent in your favorite Agentic IDE): 
 
The agent development lifecycle follows these steps: 
 
 The user takes a local set of tools or MCP servers and 
   
   Creates a lambda target for AgentCore Gateway; or 
   Deploys the MCP server as-is on AgentCore Runtime 
    
 The user prepares the actual agent code using a preferred framework like Strands Agents or LangGraph. The user can either: 
   
   Start from scratch (the server can fetch docs from the Strands Agents or LangGraph documentation) 
   Start from fully or partially working agent code 
    
 The user asks the agent to transform the code into a format compatible with AgentCore Runtime with the intention to deploy the agent later. This causes the agent to: 
   
   Write an appropriate requirements.txt&nbsp;file 
   import necessary libraries including bedrock_agentcore 
   decorate the main handler (or create one) to access the core agent calling logic or input handler 
    
 The user may then ask the agent to deploy to AgentCore Runtime. The agent can look up documentation and can use the AgentCore CLI to deploy the agent code to Runtime 
 The user can test the agent by asking the agent to do so. The AgentCore CLI command required for this is written and executed by the client 
 The user then asks to modify the code to use the deployed AgentCore Gateway MCP server within this AgentCore Runtime agent. 
   
   The agent modifies the original code to add an MCP client that can call the deployed gateway 
   The agent then deploys a new version v2 of the agent to Runtime 
   The agent then tests this integration with a new prompt 
    
 
Here is a demo of the MCP server working with Cursor IDE. We see the agent perform the following steps: 
 
 Transform the weather_agent.py to be compatible with AgentCore runtime 
 Use the AgentCore CLI to deploy the agent 
 Test the deployed agent with a successful prompt 
 
 

 
  
 

 
Here‚Äôs another example of deploying a LangGraph agent to AgentCore Runtime with the Cursor IDE performing similar steps as seen above. 

 
  
 
 
Clean up 
If you‚Äôd like to uninstall the MCP server, follow the MCP documentation for your IDE, such as Kiro, Cursor, Q CLI, and Claude Code for instructions. 
Conclusion 
In this post, we showed how you can use the AgentCore MCP server with your favorite Agentic IDE of choice to speed up your development workflows. 
We encourage you to review the Github repository, as well read through and use the following resources in your development: 
 
 Amazon Bedrock AgentCore CLI documentation 
 Strands Agents MCP Server 
 LangGraph llms.txt 
 
We encourage you to try out the AgentCore MCP server and provide any feedback through issues in our GitHub repository. 
 
About the authors 
 
  
  
   
   
  Shreyas Subramanian 
  Shreyas is a Principal Data Scientist and helps customers by using Generative AI to solve their business challenges using the AWS platform. Shreyas has a background in large scale optimization and Deep Learning, and he is a researcher studying the use of Machine Learning and Reinforcement Learning for accelerating learning and optimization tasks. Shreyas is also an Amazon best-selling book author with several research papers and patents to his name. 
  
  
  
   
   
  Primo Mu 
  Primo is a Software Development Engineer on the Agentic AI Foundation team at AWS, where he builds foundational systems and infrastructure that power intelligent AI applications. He has extensive experience working on backend stateless orchestration services behind products like Kiro and Q Dev CLI. He focuses on creating scalable frameworks and robust architectures that enable developers to build sophisticated agentic systems.
‚Ä¢ How Hapag-Lloyd improved schedule reliability with ML-powered vessel schedule predictions using Amazon SageMaker
  This post is cowritten with Thomas Voss and Bernhard Hersberger from Hapag-Lloyd. 
Hapag-Lloyd is one of the world‚Äôs leading shipping companies with more than 308 modern vessels, 11.9 million TEUs (twenty-foot equivalent units) transported per year, and 16,700 motivated employees in more than 400 offices in 139 countries. They connect continents, businesses, and people through reliable container transportation services on the major trade routes across the globe. 
In this post, we share how Hapag-Lloyd developed and implemented a machine learning (ML)-powered assistant predicting vessel arrival and departure times that revolutionizes their schedule planning. By using Amazon SageMaker AI and implementing robust MLOps practices, Hapag-Lloyd has enhanced its schedule reliability‚Äîa key performance indicator in the industry and quality promise to their customers. 
For Hapag-Lloyd, accurate vessel schedule predictions are crucial for maintaining schedule reliability, where schedule reliability is defined as percentage of vessels arriving within 1 calendar day (earlier or later) of their estimated arrival time, communicated around 3 to 4 weeks before arrival. 
Prior to developing the new ML solution, Hapag-Lloyd relied on simple rule-based and statistical calculations, based on historical transit patterns for vessel schedule predictions. While this statistical method provided basic predictions, it couldn‚Äôt effectively account for real-time conditions such as port congestion, requiring significant manual intervention from operations teams. 
Developing a new ML solution to replace the existing system presented several key challenges: 
 
 Dynamic shipping conditions ‚Äì The estimated time of arrival (ETA) prediction model needs to account for numerous variables that affect journey duration, including weather conditions, port-related delays such as congestion, labor strikes, and unexpected events that force route changes. For example, when the Suez Canal was blocked by the Ever Given container ship in March 2021, vessels had to be rerouted around Africa, adding approximately 10 days to their journey times. 
 Data integration at scale ‚Äì The development of accurate models requires integration of large volumes of historical voyage data with external real-time data sources including port congestion information and vessel position tracking (AIS). The solution needs to scale across 120 vessel services or lines and 1,200 unique port-to-port routes. 
 Robust MLOps infrastructure ‚Äì A robust MLOps infrastructure is required to continuously monitor model performance and quickly deploy updates whenever needed. This includes capabilities for regular model retraining to adapt to changing patterns, comprehensive performance monitoring, and maintaining real-time inference capabilities for immediate schedule adjustments. 
 
Hapag-Llyod‚Äôs previous approach to schedule planning couldn‚Äôt effectively address these challenges. A comprehensive solution that could handle both the complexity of vessel schedule prediction and provide the infrastructure needed to sustain ML operations at global scale was needed. 
The Hapag-Lloyd network consists of over 308 vessels and many more partner vessels that continuously circumnavigate the globe on predefined service routes, resulting in more than 3,500 port arrivals per month. Each vessel operates on a fixed service line, making regular round trips between a sequence of ports. For instance, a vessel might repeatedly sail a route from Southampton to Le Havre, Rotterdam, Hamburg, New York, and Philadelphia before starting the cycle again. For each port arrival, an ETA must be provided multiple weeks in advance to arrange critical logistics, including berth windows at ports and onward transportation of containers by sea, land or air transport. The following table shows an example where a vessel travels from Southampton to New York through Le Havre, Rotterdam, and Hamburg. The vessel‚Äôs time until arrival at the New York port can be calculated as the sum of ocean to port time to Southampton, and the respective berth times and port-to-port times for the intermediate ports called while sailing to New York. If this vessel encounters a delay in Rotterdam, it affects its arrival in Hamburg and cascades through the entire schedule, impacting arrivals in New York and beyond as shown in the following table. This ripple effect can disrupt carefully planned transshipment connections and require extensive replanning of downstream operations. 
 
  
   
   Port 
   Terminal call 
   Scheduled arrival 
   Scheduled departure 
   
   
   SOUTHAMPTON 
   1 
   2025-07-29 07:00 
   2025-07-29 21:00 
   
   
   LE HAVRE 
   2 
   2025-07-30 16:00 
   2025-07-31 16:00 
   
   
   ROTTERDAM 
   3 
   2025-08-03 18:00 
   2025-08-05 03:00 
   
   
   HAMBURG 
   4 
   2025-08-07 07:00 
   2025-08-08 07:00 
   
   
   NEW YORK 
   5 
   2025-08-18 13:00 
   2025-08-21 13:00 
   
   
   PHILADELPHIA 
   6 
   2025-08-22 06:00 
   2025-08-24 16:30 
   
   
   SOUTHAMPTON 
   7 
   2025-09-01 08:00 
   2025-09-02 20:00 
   
  
 
When a vessel departs Rotterdam with a delay, new ETAs must be calculated for the remaining ports. For Hamburg, we only need to estimate the remaining sailing time from the vessel‚Äôs current position. However, for subsequent ports like New York, the prediction requires multiple components: the remaining sailing time to Hamburg, the duration of port operations in Hamburg, and the sailing time from Hamburg to New York. 
Solution overview 
As an input to the vessel ETA prediction, we process the following two data sources: 
 
 Hapag-Lloyd‚Äôs internal data, which is stored in a data lake. This includes detailed vessel schedules and routes, port and terminal performance information, real-time port congestion and waiting times, and vessel characteristics datasets. This data is prepared for model training using AWS Glue jobs. 
 Automatic Identification System (AIS) data, which provides streaming updates on the vessel movements. This AIS data ingestion is batched every 20 minutes using AWS Lambda and includes crucial information such as latitude, longitude, speed, and direction of vessels. New batches are processed using AWS Glue and Iceberg to update the existing AIS database‚Äîcurrently holding around 35 million observations. 
 
These data sources are combined to create training datasets for the ML models. We carefully consider the timing of available data through temporal splitting to avoid data leakage. Data leakage occurs when using information that wouldn‚Äôt be available at prediction time in the real world. For example, when training a model to predict arrival time in Hamburg for a vessel currently in Rotterdam, we can‚Äôt use actual transit times that were only known after the vessel reached Hamburg. 
A vessel‚Äôs journey can be divided into different legs, which led us to develop a multi-step solution using specialized ML models for each leg, which are orchestrated as hierarchical models to retrieve the overall ETA: 
 
 The Ocean to Port (O2P) model predicts the time needed for a vessel to reach its next port from its current position at sea. The model uses features such as remaining distance to destination, vessel speed, journey progress metrics, port congestion data, and historical sea leg durations. 
 The Port to Port (P2P) model forecasts sailing time between any two ports for a given date, considering key features such as ocean distance between ports, recent transit time trends, weather, and seasonal patterns. 
 The Berth Time model estimates how long a vessel will spend at port. The model uses vessel characteristics (such as tonnage and load capacity), planned container load, and historical port performance. 
 The Combined model takes as input the predictions from the O2P, P2P, and Berth Time models, along with the original schedule. Rather than predicting absolute arrival times, it computes the expected deviation from the original schedule by learning patterns in historical prediction accuracy and specific voyage conditions. These computed deviations are then used to update ETAs for the upcoming ports in a vessel‚Äôs schedule. 
 
 
All four models are trained using the XGBoost algorithm built into SageMaker, chosen for its ability to handle complex relationships in tabular data and its robust performance with mixed numerical and categorical features. Each model has a dedicated training pipeline in SageMaker Pipelines, handling data preprocessing steps and model training. The following diagram shows the data processing pipeline, which generates the input datasets for ML training. 
 
As an example, this diagram shows the training pipeline of the Berth model. The steps in the SageMaker training pipelines of the Berth, P2P, O2P, and Combined models are identical. Therefore, the training pipeline is implemented once as a blueprint and re-used across the other models, enabling a fast turn-around time of the implementation. 
 
Because the Combined model depends on outputs from the other three specialized models, we use AWS Step Functions to orchestrate the SageMaker pipelines for training. This helps ensure that the individual models are updated in the correct sequence and maintains prediction consistency across the system. The orchestration of the training pipelines is shown in the following pipeline architecture. 
 The individual workflow begins with a data processing pipeline that prepares the input data (vessel schedules, AIS data, port congestion, and port performance metrics) and splits it into dedicated datasets. This feeds into three parallel SageMaker training pipelines for our base models (O2P, P2P, and Berth), each following a standardized process of feature encoding, hyperparameter optimization, model evaluation, and registration using SageMaker Processing and hyperparameter turning jobs and SageMaker Model Registry. After training, each base model runs a SageMaker batch transform job to generate predictions that serve as input features for the combined model training. The performance of the latest Combined model version is tested on the last 3 months of data with known ETAs, and performance metrics (R¬≤, mean absolute error (MAE)) are computed. If the model‚Äôs performance is below a set MAE threshold, the entire training process fails and the model version is automatically discarded, preventing the deployment of models that don‚Äôt meet the minimum performance threshold. 
All four models are versioned and stored as separate model package groups in the SageMaker Model Registry, enabling systematic version control and deployment. This orchestrated approach helps ensure that our models are trained in the correct sequence using parallel processing, resulting in an efficient and maintainable training process.The hierarchical model approach helps further ensure that a degree of explainability comparable to the current statistical and rule-based solution is maintained‚Äîavoiding ML black box behavior. For example, it becomes possible to highlight unusually long berthing time predictions when discussing predictions results with business experts. This helps increase transparency and build trust, which in turn increases acceptance within the company. 
Inference solution walkthrough 
The inference infrastructure implements a hybrid approach combining batch processing with real-time API capabilities as shown in Figure 5. Because most data sources update daily and require extensive preprocessing, the core predictions are generated through nightly batch inference runs. These pre-computed predictions are complemented by a real-time API that implements business logic for schedule changes and ETA updates. 
 
 Daily batch Inference: 
   
   Amazon EventBridge triggers a Step Functions workflow every day. 
   The Step Functions workflow orchestrates the data and inference process: 
     
     Lambda copies internal Hapag-Lloyd data from the data lake to Amazon Simple Storage Service (Amazon S3). 
     AWS Glue jobs combine the different data sources and prepare inference inputs 
     SageMaker inference executes in sequence: 
       
       Fallback predictions are computed from historical averages and written to Amazon Relational Database Service (Amazon RDS). Fallback predictions are used in case of missing data or a downstream inference failure. 
       Preprocessing data for the four specialized ML models. 
       O2P, P2P, and Berth model batch transforms. 
       The Combined model batch transform generates final ETA predictions, which are written to Amazon RDS. 
       Input features and output files are stored in Amazon S3 for analytics and monitoring. 
        
      
   For operational reliability, any failures in the inference pipeline trigger immediate email notifications to the on-call operations team through Amazon Simple Email Service (Amazon SES). 
    
 Real-time API: 
   
   Amazon API Gateway receives client requests containing the current schedule and an indication for which vessel-port combinations an ETA update is required. By receiving the current schedule through the client request, we can take care of intraday schedule updates while doing daily batch transform updates. 
   The API Gateway triggers a Lambda function calculating the response. The Lambda function constructs the response by linking the ETA predictions (stored in Amazon RDS) with the current schedule using custom business logic, so that we can take care of short-term schedule changes unknown at inference time. Typical examples of short-term schedule changes are port omissions (for example, due to port congestion) and one-time port calls. 
    
 
This architecture enables millisecond response times to custom requests while achieving a 99.5% availability (a maximum 3.5 hours downtime per month). 
 
Conclusion 
Hapag Lloyd‚Äôs ML powered vessel scheduling assistant outperforms the current solution in both accuracy and response time. Typical API response times are in the order of hundreds of milliseconds, helping to ensure a real-time user experience and outperforming the current solution by more than 80%. Low response times are crucial because, in addition to fully automated schedule updates, business experts require low response times to work with the schedule assistant interactively. In terms of accuracy, the MAE of the ML-powered ETA predictions outperform the current solution by approximately 12%, which translates into climbing by two positions in the international ranking of schedule reliability on average. This is one of the key performance metrics in liner shipping, and this is a significant improvement within the industry. 
To learn more about architecting and governing ML workloads at scale on AWS, see the AWS blog post Governing the ML lifecycle at scale, Part 1: A framework for architecting ML workloads using Amazon SageMaker and the accompanying AWS workshop AWS Multi-Account Data &amp; ML Governance Workshop. 
Acknowledgement 
We acknowledge the significant and valuable work of Michal Papaj and Piotr Zielinski from Hapag-Lloyd in the data science and data engineering areas of the project. 
About the authors 
Thomas Voss Thomas Voss works at Hapag-Lloyd as a data scientist. With his background in academia and logistics, he takes pride in leveraging data science expertise to drive business innovation and growth through the practical design and modeling of AI solutions. 
Bernhard Hersberger Bernhard Hersberger works as a data scientist at Hapag-Lloyd, where he heads the AI Hub team in Hamburg. He is enthusiastic about integrating AI solutions across the company, taking comprehensive responsibility from identifying business issues to deploying and scaling AI solutions worldwide. 
Gabija Pasiunaite At AWS, Gabija Pasiunaite was a Machine Learning Engineer at AWS Professional Services based in Zurich. She specialized in building scalable ML and data solutions for AWS Enterprise customers, combining expertise in data engineering, ML automation and cloud infrastructure. Gabija has contributed to the AWS MLOps Framework used by AWS customers globally. Outside work, Gabija enjoys exploring new destinations and staying active through hiking, skiing, and running. 
Jean-Michel Lourier Jean-Michel Lourier is a Senior Data Scientist within AWS Professional Services. He leads teams implementing data driven applications side by side with AWS customers to generate business value out of their data. He‚Äôs passionate about diving into tech and learning about AI, machine learning, and their business applications. He is also an enthusiastic cyclist. 
Mousam Majhi Mousam Majhi is a Senior ProServe Cloud Architect focusing on Data &amp; AI within AWS Professional Services. He works with Manufacturing and Travel, Transportation &amp; Logistics customers in DACH to achieve their business outcomes by leveraging data and AI powered solutions. Outside of work, Mousam enjoys hiking in the Bavarian Alps.
‚Ä¢ Rox accelerates sales productivity with AI agents powered by Amazon Bedrock
  This post was co-written with Shriram Sridharan, Taeuk Kang, and Santhosh Kumar Manavasi Lakshminarayanan from Rox. 
Rox is building a new revenue operating system for the applied AI era. 
Modern revenue teams rely on more data than ever before, such as Customer Relationship Management (CRM) systems, marketing automation, finance systems, support tickets, and live product usage. Though each serves its role, together they create silos that slow sellers down and leave insights untapped. 
Rox addresses this by providing a revenue operating system: a unified layer that brings these signals together and equips AI agents to execute go-to-market (GTM) workflows. Instead of reconciling reports or updating fields, sellers get real-time intelligence and automation in their daily flow. 
Today, we‚Äôre excited to announce that Rox is generally available, with Rox infrastructure built on AWS and delivered across web, Slack, macOS, and iOS. In this post, we share how Rox accelerates sales productivity with AI agents powered by Amazon Bedrock. 
Solution overview 
As noted in Rox is transforming revenue teams with AI-driven integration powered by AWS, modern GTM teams need more than a static database. Revenue data spans dozens of systems, such as product usage, finance, and support, and teams require a system that unifies context and acts on it in real time. 
Rox delivers this through a layered architecture on AWS: 
 
 System of record ‚Äì A unified, governed knowledge graph consolidates CRM, finance, support, product telemetry, and web data 
 Agent swarms ‚Äì Intelligent, account-aware agents reason over the graph and orchestrate multi-step workflows like research, outreach, opportunity management, and proposal generation 
 Interfaces across surfaces ‚Äì Sellers engage these workflows where they work, such as web application, Slack, iOS, and macOS 
 
This converts the CRM from a passive system of record into an active system of action, so teams can act on their data immediately and intelligently. 
The following diagram illustrates the solution architecture. 
 
Benefits and features of ROX 
Now generally available, Rox extends from intelligence to full execution with Command, a new conversational interface that orchestrates multi-agent workflows. Command coordinates with multiple specialized agents running in parallel. A single request (for example, ‚Äúprep me for the ACME renewal and draft follow-ups‚Äù) expands into a plan: research usage and support signals, identify missing stakeholders, refresh enrichment, propose next-best actions, draft outreach, update the opportunity, and assemble a proposal. Each step is completed through tool calls into your systems and is subject to guardrail approvals. Our comprehensive safety architecture employs a sophisticated multi-layer guardrail system as the first line of defense against inappropriate, harmful, or malicious requests. Incoming requests undergo rigorous analysis through our advanced filtering mechanisms before reaching the inference layer. This preprocessing stage evaluates multiple dimensions of safety and appropriateness, such as legal compliance assessment and business relevance evaluation, to make sure only legitimate, safe, and contextually appropriate requests proceed to model execution. 
Command decomposes the request, routes steps to the right agents, sequences external tool invocations (CRM, calendar, enrichment, email), reconciles results into the system of context, and returns one coherent thread that‚Äôs ready for consumption on the web, Slack, iOS, or macOS. Every suggestion is explainable (sources and traces), reversible (audit logs), and policy-aware (role-based access control, rate limits, required approvals). 
How Amazon Bedrock powers Rox 
Command demands a model capable of reasoning across multiple steps, orchestrating tools, and adapting dynamically. 
To meet these needs, Rox chose Anthropic‚Äôs Claude Sonnet 4 on Amazon Bedrock. Anthropic‚Äôs Claude Sonnet 4 has consistently demonstrated unmatched tool-calling and reasoning performance, allowing Rox agents to sequence workflows like account research, enrichment, outreach, opportunity management, and proposal generation with reliability. 
Amazon Bedrock provides the foundation to deliver Rox at enterprise scale, offering security, flexibility to integrate with the latest models, and scalability to handle thousands of concurrent agents reliably. 
In addition to Command, Rox includes the following features: 
 
 Research ‚Äì Offers deep account and market research, grounded in unified context (carried over from private beta) 
 Meet ‚Äì Makes it possible to record, transcribe, summarize, and turn meetings into actions (carried over from private beta) 
 Outreach ‚Äì Provides personalized prospect engagement, contextualized by unified data (new) 
 Revenue ‚Äì Helps you track, update, and advance pipelines in the flow of work (new) 
 Auto-fill proposals ‚Äì Helps you assemble tailored proposals in seconds from account context (new) 
 Rox apps ‚Äì Offers modular extensions that add purpose-built workflows (dashboards, trackers) directly into the system (new) 
 iOS app ‚Äì Delivers notifications and meeting prep on the go (new) 
 Mac app ‚Äì Brings the ability to transcribe calls and add them to the system of context (new) 
 Regional expansion ‚Äì Now live in the AWS Middle East (Bahrain) AWS Region, aligning with data residency and sovereignty needs (new) 
 
Early customer impact 
In beta, enterprises saw immediate gains: 
 
 50% higher representative productivity 
 20% faster sales velocity 
 Twofold revenue per rep 
 
For example, real Rox customers were able to sharpen their focus on high-value opportunities, driving a 40‚Äì50% increase in average selling price. Another customer saw 90% reduction in rep prep time and faster closes, plus 15% more six-figure deals uncovered through Rox insights. Rox also shortens ramp time for new reps, with customers reporting 50% quicker ramp time using Rox. 
Try Rox today 
Our vision is for revenue teams to run with an always-on agent swarm that continuously researches accounts, engages stakeholders, and moves the pipeline forward. 
Rox is now generally available. Get started at rox.com or visit the AWS Marketplace. Together with AWS, we will continue to build the AI-based operating system for modern revenue teams. 
 
About the authors 
Shriram Sridharan&nbsp;is the Co-Founder/Engineering Head of Rox, a Sequoia backed AI company. Before Rox, Shriram led the data infrastructure team at Confluent responsible for making Kafka faster and cheaper across clouds. Prior to that he was one of the early engineers in Amazon Aurora (pre-launch) re-imagining databases for the cloud. Aurora was the fastest growing AWS Service and a recipient of the 2019 SIGMOD systems award. 
Taeuk Kang is a Founding Engineer at Rox, working across AI research and engineering. He studied Computer Science at Stanford. Prior to Rox, he built large language model agents and retrieval-augmented generation systems at X (formerly Twitter) and designed the distributed LLM infrastructure powering core product features and Trust &amp; Safety, improving overall platform health. Earlier at Stripe, he developed high-performance streaming and batch data processing pipelines integrating Apache Flink, Spark, Kafka, and AWS SQS. 
Santhosh Kumar Manavasi Lakshminarayanan&nbsp;leads Platform at Rox. Before Rox he was Director of Engineering at StreamSets, acquired by IBM leading StreamSets Cloud Platform making it seamless for big enterprises to run their data pipeline at scale on modern cloud providers. Before StreamSets, he was an senior engineer at Platform Metadata team at Informatica. 
Andrew Brown&nbsp;is an Account Executive for AI Startups at Amazon Web Services (AWS) in San Francisco, CA. With a strong background in cloud computing and a focus on supporting startups, Andrew specializes in helping companies scale their operations using AWS technologies. 
Santhan Pamulapati&nbsp;is a Sr. Solutions Architect for GenAI startups at AWS, with deep expertise in designing and building scalable solutions that drives customer growth. He has strong background in building HPC systems leveraging AWS services and worked with strategic customers to solve business challenges.
‚Ä¢ Modernize fraud prevention: GraphStorm v0.5 for real-time inference
  Fraud continues to&nbsp;cause significant financial damage globally, with U.S. consumers alone losing $12.5 billion in 2024‚Äîa 25% increase from the previous year according to the Federal Trade Commission. This surge stems not from more frequent attacks, but from fraudsters‚Äô increasing sophistication. As fraudulent activities become more complex and interconnected, conventional machine learning approaches fall short by analyzing transactions in isolation, unable to capture the networks of coordinated activities that characterize modern fraud schemes. 
Graph neural networks (GNNs) effectively address this challenge by modeling relationships between entities‚Äîsuch as users sharing devices, locations, or payment methods. By analyzing both network structures and entity attributes, GNNs&nbsp;are effective at identifying sophisticated fraud schemes where perpetrators mask individual suspicious activities but leave traces in their relationship networks. However, implementing GNN-based online fraud prevention in production environments presents unique challenges: achieving sub-second inference responses, scaling to billions of nodes and edges, and maintaining operational efficiency for model updates.&nbsp;In this post, we show you how to overcome these challenges using GraphStorm, particularly the new real-time inference capabilities of GraphStorm v0.5. 
Previous solutions required tradeoffs between capability and simplicity. Our initial DGL approach provided comprehensive real-time capabilities but demanded intricate service orchestration‚Äîincluding manually updating endpoint configurations and payload formats after retraining with new hyperparameters. This approach also lacked model flexibility, requiring customization of GNN models and configurations when using architectures beyond relational graph convolutional networks (RGCN). Subsequent in-memory DGL implementations reduced complexity but&nbsp;encountered scalability limitations with enterprise data volumes. We built GraphStorm to bridge this gap, by introducing distributed training and high-level APIs that help simplify GNN development at enterprise scale. 
In a recent blog post, we illustrated GraphStorm‚Äôs enterprise-scale GNN model training and offline inference capability and simplicity. While offline GNN fraud detection can identify fraudulent transactions after they occur‚Äîpreventing financial loss requires stopping fraud before it happens. GraphStorm v0.5&nbsp;makes this possible through native real-time inference support through Amazon SageMaker AI. GraphStorm v0.5&nbsp;delivers two innovations: streamlined endpoint deployment that reduces weeks of custom engineering‚Äîcoding SageMaker entry point files, packaging model artifacts, and calling SageMaker deployment APIs‚Äîto a single-command operation, and standardized payload specification that helps simplify client integration with real-time inference services. These capabilities enable sub-second node classification tasks like fraud prevention, empowering organizations to proactively counter fraud threat with scalable, operationally straightforward GNN solutions. 
To showcase these capabilities, this post presents a fraud prevention solution. Through this solution, we show how a data scientist can transition a trained GNN model to production-ready inference endpoints with minimal operational overhead. If you‚Äôre interested in implementing GNN-based models for real-time fraud prevention or similar business cases, you can adapt the approaches presented here to create your own solutions. 
Solution overview 
Our proposed solution is a 4-step pipeline as shown in the following figure. The pipeline starts at step 1 with transaction graph export from an online transaction processing (OLTP) graph database to scalable storage (Amazon Simple Storage Service (Amazon S3) or Amazon EFS), followed by distributed model training in step 2. Step 3 is GraphStorm v0.5‚Äôs simplified deployment process that creates SageMaker&nbsp;real-time inference endpoints with one command. After SageMaker AI has deployed the endpoint successfully, a client application integrates with the OLTP graph database that processes live transaction streams in step 4. By querying the graph database, the client prepares subgraphs around to-be predicted transactions, convert the subgraph into standardized payload format, and invoke deployed endpoint for real-time prediction. 
 
To provide concrete implementation details for each step in the real-time inference solution, we demonstrate the complete workflow using the publicly available IEEE-CIS fraud detection task. 
Note: This example uses a Jupyter notebook as the controller of the overall four-step pipeline for simplicity. For more production-ready design, see the architecture described in Build a GNN-based real-time fraud detection solution. 
Prerequisites 
To run this example, you need an AWS account&nbsp;that&nbsp;the example‚Äôs AWS Cloud Development Kit (AWS CDK) code uses to create required resources, including Amazon Virtual Private Cloud (Amazon VPC), an Amazon Neptune database, Amazon SageMaker AI,&nbsp;Amazon Elastic Container Registry (Amazon ECR), Amazon S3, and related roles and permission. 
Note: These resources incur costs during execution (approximately $6 per hour with default settings). Monitor usage carefully and review pricing pages for these services before proceeding. Follow cleanup instructions at the end to avoid ongoing charges. 
Hands-on example: Real-time fraud prevention with IEEE-CIS dataset 
All implementation code for this example, including Jupyter notebooks and supporting Python scripts, is available in our public repository. The repository provides a complete end-to-end implementation that you can directly execute and adapt for your own fraud prevention use cases. 
Dataset and task overview 
This example uses the IEEE-CIS fraud detection dataset, containing 500,000 anonymized transactions with approximately 3.5% fraudulent cases. The dataset includes 392 categorical and numerical features, with key attributes like card types, product types, addresses, and email domains forming the graph structure shown in the following figure. Each transaction (with an isFraud&nbsp;label) connects to Card Type, Location, Product Type, and Purchaser and Recipient email domain entities, creating a heterogeneous graph that enables GNN models to detect fraud patterns through entity relationships. 
 
Unlike our previous post that demonstrated GraphStorm plus Amazon Neptune Analytics for offline analysis workflows, this example uses&nbsp;a Neptune database as the OLTP graph store, optimized for the quick subgraph extraction required during real-time inference. Following the graph design, the tabular IEEE-CIS data is converted to a set CSV files compatible with Neptune database format, allowing direct loading into both the Neptune database and GraphStorm‚Äôs GNN model training pipeline with a single set of files. 
Step 0: Environment setup 
Step 0 establishes the running environment required for the four-step fraud prevention pipeline. Complete setup instructions are available in the implementation repository. 
To run the example solution, you need to deploy an AWS CloudFormation stack through the AWS CDK. This stack creates the Neptune DB instance, the VPC to place it in, and appropriate roles and security groups. It additionally creates a SageMaker AI notebook instance, from which you run the example notebooks&nbsp;that come with the repository. 
 
 git clone https://github.com/aws-samples/amazon-neptune-samples.git
cd neptune-database-graphstorm-online-inference/neptune-db-cdk
# Ensure you have CDK installed and have appropriate credentials set up
cdk deploy 
 
When deployment is finished (it takes approximately 10 minutes for required resources to be ready), the AWS CDK prints a few outputs, one of which is the name of the SageMaker notebook instance you use to run through the notebooks: 
 
 # Example output
NeptuneInfraStack.NotebookInstanceName = arn:aws:sagemaker:us-east-1:012345678912:notebook-instance/NeptuneNotebook-9KgSB9XXXXXX 
 
You can navigate to the SageMaker AI notebook UI, find the corresponding notebook instance, and select its Open Jupyterlab link to access the notebook. 
Alternatively, you can use the AWS Command Line Interface (AWS CLI) to get a pre-signed URL to access the notebook. You will need to replace the &lt;notebook-instance-name&gt; with the actual notebook instance name. 
 
 aws sagemaker create-presigned-notebook-instance-url --notebook-instance-name &lt;notebook-instance-name&gt; 
 
When you‚Äôre in the notebook instance web console, open the first notebook, 0-Data-Preparation.ipynb, to start going through the example. 
Step 1: Graph construction 
In the Notebook 0-Data-Preparation, you transform the tabular IEEE-CIS dataset into the heterogeneous graph structure shown in the figure at the start of this section. The provided Jupyter Notebook extracts entities from transaction features, creating Card Type nodes from card1‚Äìcard6 features, Purchaser and Recipient nodes from email domains, Product Type nodes from product codes, and Location nodes from geographic information. The transformation establishes relationships between transactions and these entities, generating graph data in Neptune import format for direct ingestion into the OLTP graph store. The create_neptune_db_data() function orchestrates this entity extraction and relationship creation process across all node types (which takes approximately 30 seconds). 
 
 GRAPH_NAME&nbsp;= "ieee-cis-fraud-detection"
PROCESSED_PREFIX&nbsp;= f"./{GRAPH_NAME}"
ID_COLS&nbsp;= "card1,card2,card3,card4,card5,card6,ProductCD,addr1,addr2,P_emaildomain,R_emaildomain"
CAT_COLS&nbsp;= "M1,M2,M3,M4,M5,M6,M7,M8,M9"
# Lists of columns to keep from each file
COLS_TO_KEEP&nbsp;= {
&nbsp;&nbsp; &nbsp;"transaction.csv": (
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;ID_COLS.split(",")
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;+ CAT_COLS.split(",")
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;+
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;# Numerical features without missing values
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;[f"C{idx}"&nbsp;for&nbsp;idx&nbsp;in&nbsp;range(1, 15)]
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;+ ["TransactionID", "TransactionAmt", "TransactionDT", "isFraud"]
&nbsp;&nbsp; &nbsp;),
&nbsp;&nbsp; &nbsp;"identity.csv": ["TransactionID", "DeviceType"],
}

create_neptune_db_data(
&nbsp;&nbsp; &nbsp;data_prefix="./input-data/",
&nbsp;&nbsp; &nbsp;output_prefix=PROCESSED_PREFIX,
&nbsp;&nbsp; &nbsp;id_cols=ID_COLS,
&nbsp;&nbsp; &nbsp;cat_cols=CAT_COLS,
&nbsp;&nbsp; &nbsp;cols_to_keep=COLS_TO_KEEP,
&nbsp;&nbsp; &nbsp;num_chunks=1,
) 
 
This notebook also generates the JSON configuration file required by GraphStorm‚Äôs GConstruct command and executes the graph construction process. This GConstruct command transforms the Neptune-formatted data into a distributed binary graph format optimized for GraphStorm‚Äôs training pipeline, which partitions the heterogeneous graph structure across compute nodes to enable scalable model training on industry-scale graphs (measured in billions of nodes and edges). For the IEEE-CIS data, the GConstruct command takes 90 seconds to complete. 
In the Notebook&nbsp;1-Load-Data-Into-Neptune-DB, you load the CSV data into the Neptune database instance (takes approximately 9 minutes), which makes them available for online inference. During online inference, after selecting a transaction node, you query the Neptune database to get the graph neighborhood of the target node, retrieving the features of every node in the neighborhood and the subgraph structure around the target. 
Step 2: Model training 
After you have converted the data into the distributed binary graph format, it‚Äôs time to train a GNN model. GraphStorm provides command-line scripts to train a model without writing code. In the Notebook 2-Model-Training, you&nbsp;train a GNN model using GraphStorm‚Äôs node classification command with configuration managed through YAML files. The baseline configuration defines a two-layer RGCN model with 128-dimensional hidden layers, training for 4 epochs with a 0.001 learning rate and 1024 batch size, which takes approximately 100 seconds for 1 epoch of model training and evaluation in an ml.m5.4xlarge instance. To improve fraud detection accuracy, the notebook provides more advanced model configurations like the command below. 
 
 !python -m&nbsp;graphstorm.run.gs_node_classification \
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; --workspace ./&nbsp;\
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; --part-config ieee_gs/ieee-cis.json \
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; --num-trainers 1&nbsp;\
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; --cf ieee_nc.yaml \
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; --eval-metric roc_auc \
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; --save-model-path ./model-simple/&nbsp;\
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; --topk-model-to-save 1&nbsp;\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;--imbalance-class-weights 0.1,1.0 
 
Arguments in this command address the dataset‚Äôs label imbalance challenge where only 3.5% of transactions are fraudulent by using AUC-ROC as the evaluation metric and using class weights. The command also saves the best-performing model along with essential configuration files required for endpoint deployment. Advanced configurations can further enhance model performance through techniques like HGT encoders, multi-head attention, and class-weighted cross entropy loss function, though these optimizations increase computational requirements. GraphStorm enables these changes through run time arguments and YAML configurations, reducing the need for code modifications. 
Step 3: Real-time endpoint deployment 
In the Notebook 3-GraphStorm-Endpoint-Deployment, you deploy the real-time endpoint through GraphStorm v0.5‚Äôs straightforward launch script. The deployment requires three model artifacts generated during training: the saved model file that contains weights, the updated graph construction JSON file with feature transformation metadata, and the runtime-updated training configuration YAML file. These artifacts enable GraphStorm to recreate the exact training configurations and model for consistent inference behavior. Notably, the updated graph construction JSON and training configuration YAML file contains crucial configurations that are essential for restoring the trained model on the endpoint and processing incoming request payloads. It is crucial to use the updated JSON and YAML files for endpoint deployment.GraphStorm uses SageMaker AI bring your own container (BYOC) to deploy a consistent inference environment. You need to build and push the GraphStorm real-time Docker image to Amazon ECR using the provided shell scripts. This containerized approach provides consistent runtime environments compatible with the SageMaker AI managed infrastructure. The Docker image contains the necessary dependencies for GraphStorm‚Äôs real-time inference capabilities on the deployment environment. 
To deploy the endpoint, you can use the GraphStorm-provided launch_realtime_endpoint.py script that helps you gather required artifacts and creates the necessary SageMaker AI resources to deploy an endpoint. The script accepts the Amazon ECR image URI, IAM role, model artifact paths, and S3 bucket configuration, automatically handling endpoint provisioning and configuration. By default, the script waits for endpoint deployment to be complete before exiting. When completed, it prints the name and AWS Region of the deployed endpoint for subsequent inference requests. You will need to replace the fields enclosed by &lt;&gt; with the actual values of your environment. 
 
 !python ~/graphstorm/sagemaker/launch/launch_realtime_endpoint.py \
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;--image-uri &lt;account_id&gt;.dkr.ecr.&lt;aws_region&gt;.amazonaws.com/graphstorm:sagemaker-endpoint-cpu \
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;--role arn:aws:iam::&lt;account_id&gt;:role/&lt;your_role&gt; \
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;--region &lt;aws_region&gt; \
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;--restore-model-path &lt;restore-model-path&gt;/models/epoch-1/ \
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;--model-yaml-config-file &lt;restore-model-path&gt;/models/GRAPHSTORM_RUNTIME_UPDATED_TRAINING_CONFIG.yaml \
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;--graph-json-config-file &lt;restore-model-path&gt;/models/data_transform_new.json \
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;--infer-task-type&nbsp;node_classification \
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;--upload-tarfile-s3 s3://&lt;cdk-created-bucket&gt; \
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;--model-name ieee-fraud-detect 
 
Step 4: Real-time inference 
In the Notebook 4-Sample-Graph-and-Invoke-Endpoint, you build a basic client application that integrates with the deployed GraphStorm endpoint to perform real-time fraud prevention on incoming transactions. The inference process accepts transaction data through standardized JSON payloads, executes node classification predictions in a few hundreds of milliseconds, and returns fraud probability scores that enable immediate decision-making. 
An end-to-end inference call for a node that already exists in the graph has three distinct stages: 
 
 Graph sampling from the Neptune database. For a given target node that already exists in the graph, retrieve its k-hop neighborhood with a fanout limit, that is, limiting the number of neighbors retrieved at each hop by a threshold. 
 Payload preparation for inference. Neptune returns graphs using GraphSON, a specialized JSON-like data format used to describe graph data. At this step, you need to convert the returned GraphSON to GraphStorm‚Äôs own JSON specification. This step is performed on the inference client, in this case a SageMaker notebook instance. 
 Model inference using a SageMaker endpoint. After the payload is prepared, you send an inference request to a SageMaker endpoint that has loaded a previously trained model snapshot. The endpoint receives the request, performs any feature transformations needed (such as converting categorical features to one-hot encoding), creates the binary graph representation in memory, and makes a prediction for the target node using the graph neighborhood and trained model weights. The response is encoded to JSON and sent back to the client. 
 
An example response from the endpoint would look like: 
 
 {'status_code': 200,
&nbsp;'request_uid': '877042dbc361fc33',
&nbsp;'message': 'Request&nbsp;processed&nbsp;successfully.',
&nbsp;'error': '',
&nbsp;'data': {
&nbsp;&nbsp; &nbsp;'results': [
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;{
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;'node_type': 'Transaction',
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;'node_id': '2991260',
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;'prediction': [0.995966911315918, 0.004033133387565613]
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;]
&nbsp;&nbsp; &nbsp;}
} 
 
The data of interest for the single transaction you made a prediction for are in the prediction key and corresponding node_id. The prediction gives you the raw scores the model produces for class 0 (legitimate) and class 1 (fraudulent) at the corresponding 0 and 1 indexes of the predictions list. In this example, the model marks the transaction as most likely legitimate. You can find the full GraphStorm response specification in the GraphStorm documentation. 
Complete implementation examples, including client code and payload specifications, are provided in the repository to guide integration with production systems. 
Clean up 
To stop accruing costs on your account, you need to delete the AWS resources that you created with the AWS CDK at the Environment Setup step. 
You must first delete the SageMaker endpoint created during the Step 3 for cdk destroy to complete.&nbsp;See the Delete Endpoints and Resources for more options to delete an endpoint. When done, you can run the following from the repository‚Äôs root: 
 
 cd&nbsp;neptune-database-graphstorm-online-inference/neptune-db-cdk
cdk destroy 
 
See the AWS CDK docs for more information about how to use cdk destroy, or see the CloudFormation docs for how to delete a stack from the console UI.&nbsp;By default, the cdk destroy command does not delete the model artifacts and processed graph data stored in the S3 bucket during the training and deployment process. You must remove them manually. See Deleting a general purpose bucket for information about how to empty and delete an S3 bucket the AWS CDK has created. 
Conclusion 
Graph neural networks address complex fraud prevention challenges by modeling relationships between entities that traditional machine learning approaches miss when analyzing transactions in isolation. GraphStorm v0.5&nbsp;helps simplify deployment of GNN real-time inference with one command for endpoint creation that previously required coordination of multiple services and a standardized payload specification that helps simplify client integration with real-time inference services. Organizations can now deploy enterprise-scale fraud prevention endpoints through streamlined commands that reduce custom engineering from weeks to single-command operations. 
To implement GNN-based fraud prevention with your own data: 
 
 Review the GraphStorm documentation for model configuration options and deployment specifications. 
 Adapt this IEEE-CIS example to your fraud prevention dataset by modifying the graph construction and feature engineering steps using the complete source code and tutorials available in our GitHub repository. 
 Access step-by-step implementation guidance to build production-ready fraud prevention solutions with GraphStorm v0.5‚Äôs enhanced capabilities using your enterprise data. 
 
 
 
About the authors 
Jian Zhang&nbsp;is a Senior Applied Scientist who has been using machine learning techniques to help customers solve various problems, such as fraud detection, decoration image generation, and more. He has successfully developed graph-based machine learning, particularly graph neural network, solutions for customers in China, the US, and Singapore. As an enlightener of AWS graph capabilities, Zhang has given many public presentations about GraphStorm, the GNN, the Deep Graph Library (DGL), Amazon Neptune, and other AWS services. 
Theodore Vasiloudis&nbsp;is a Senior Applied Scientist at AWS, where he works on distributed machine learning systems and algorithms. He led the development of GraphStorm Processing, the distributed graph processing library for GraphStorm and is a core developer for GraphStorm. He received his PhD in Computer Science from KTH Royal Institute of Technology, Stockholm, in 2019. 
Xiang Song&nbsp;is a Senior Applied Scientist at AWS AI Research and Education (AIRE), where he develops deep learning frameworks including GraphStorm, DGL, and DGL-KE. He led the development of Amazon Neptune ML, a new capability of Neptune that uses graph neural networks for graphs stored in graph database. He is now leading the development of GraphStorm, an open source graph machine learning framework for enterprise use cases. He received his PhD in computer systems and architecture at the Fudan University, Shanghai, in 2014. 
Florian Saupe&nbsp;is a Principal Technical Product Manager at AWS AI/ML research supporting science teams like the graph machine learning group, and ML Systems teams working on large scale distributed training, inference, and fault resilience. Before joining AWS, Florian lead technical product management for automated driving at Bosch, was a strategy consultant at McKinsey &amp; Company, and worked as a control systems and robotics scientist‚Äîa field in which he holds a PhD. 
Ozan Eken&nbsp;is a Product Manager at AWS, passionate about building cutting-edge Generative AI and Graph Analytics products. With a focus on simplifying complex data challenges, Ozan helps customers unlock deeper insights and accelerate innovation. Outside of work, he enjoys trying new foods, exploring different countries, and watching soccer.

‚∏ª