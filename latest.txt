‚úÖ Morning News Briefing ‚Äì October 17, 2025 10:44

üìÖ Date: 2025-10-17 10:44
üè∑Ô∏è Tags: #briefing #ai #publichealth #digitalgov

‚∏ª

üßæ Weather
‚Ä¢ Current Conditions:  0.1¬∞C
  Temperature: 0.1&deg;C Pressure / Tendency: 102.0 kPa rising Humidity: 95 % Dewpoint: -0.7&deg:C Wind:  calm km/h Air Quality Health Index: n/a . Pembroke 6:00 AM EDT Friday 17 October 2025 Temperature:  0.01-0.1/deg
‚Ä¢ Friday: Chance of showers. High 12. POP 30%
  Increasing cloudiness this afternoon then 30 percent chance of showers late this afternoon . Sunny. Sunny. High 12. UV index 3 or moderate, with a slight risk of rain in the afternoon . Showers will be possible in the evening, but showers could linger late into the evening . Forecast issued 5:00 AM EDT Friday 17 October 2025. Forecast: "Showery"
‚Ä¢ Friday night: Chance of showers. Low plus 5. POP 30%
  Cloudy with 30 percent chance of showers this evening and after midnight . Wind becoming southeast 20 km/h before morning . Low plus 5.50 per cent chance of rain in the morning . Rainfall expected to drop to 30 per cent in the afternoon . Forecast issued 5:00 AM EDT Friday 17 October 2025 . Weather forecast: Rainy, cold, cloudy, breezy,

üåç International News
No updates.

üçÅ Canadian News
No updates.

üá∫üá∏ U.S. Top Stories
‚Ä¢ Former Japanese PM Murayama, known for apology over wartime aggression, dies at 101
  Tomiichi Murayama, Japan's prime minister from 1994, was best known for the "Murayama Statement," an apology delivered on the 50th anniversary of Japan's World War II surrender . The statement was delivered in Japan on the anniversary of the surrender of Japan to Japan in 1945 . Japan's current prime minister is the current president of the Kaidan Party, the
‚Ä¢ Zelenskyy is set to meet Trump at the White House. Here's what to expect
  Ukrainian President Volodymyr Zelenskyy meets with President Trump on Friday . The mood is expected to be very different from their first Oval Office encounter in February . The two leaders will meet for the first time on Friday in the Oval Office since their first meeting in February last year . The first meeting was held in February at the White House in Washington, D.C., in
‚Ä¢ Our quiz writer digs deep on politics and ... bowler hats. Can you score 11?
  What do you know about marathoning seniors, Diane Keaton's fashion choices and Taylor Swift sales? Find out! Share your knowledge of your favorite celebs and events in the U.S. next week's iReporter gallery . Visit CNN.com/Travel next Wednesday for a new look at the top 10 things you think about in-depth interviews with iReporters .
‚Ä¢ 'No Kings' organizers project a massive turnout for this weekend's protests
  Organizers say they expect millions of Americans to march this weekend against the policies of the Trump administration . The protests come amid National Guard deployments in several cities . National Guard forces are deployed to several cities in response to the events in Washington, D.C., New York, New York and New York . The National Guard is deployed to protect President Trump's National Security Council members in Washington .
‚Ä¢ In small towns and rural communities, young voters say they feel unseen by leaders
  Gen Z and millennial voters will make up more than half of the electorate in 2028 . They're a crucial bloc for both parties, but many are facing daunting economic realities and feel unseen by leaders . Many face daunting economic reality and feel invisible by leaders, according to a new poll by NPR's Kirsten Luce . The poll shows that millennials will be more likely to vote in 20

üß† Artificial Intelligence
No updates.

üíª Digital Strategy
‚Ä¢ Literal crossed wires sent cops after innocent neighbors in child abuse case
  Eight-year telco blunder had a profound impact on three wrongly accused in Wales . Details have emerged of a troubling case in which a basic engineering mistake wrecked a digital evidence investigation and led to wrongful accusations . The case has been described as a 'troubling case' and a 'disturbing case' involving a Welsh telco company in which three people were wrongly accused .
‚Ä¢ MIT boffins double precision of atomic clocks by taming quantum noise
  'Global phase spectroscopy' makes ultraprecise optical timekeepers even more precise . Researchers at MIT say they have discovered a way to double the precision of optical atomic clocks by quieting the quantum noise that clouds their ticking . The new technique could be used to make atomic clocks more precise and more precise with less noise in their clock clocks, MIT researchers say . Back to Mail
‚Ä¢ Britain's AI gold rush hits a wall ‚Äì not enough electricity
  Energy is essential for delivering the UK governments' AI ambitions, but Britain faces a critical question: how can it supply enough power for rapidly expanding datacenters without causing blackouts or inflating consumer bills?‚Ä¶ Britain faces the question: How can it provide enough power to rapidly expanding data-storage facilities without causing a blackout?‚Ä¶ Energy secretary Miliband promises renewable utopia for green and
‚Ä¢ AI boffins teach office supplies to predict your next move
  AI has now fixed its sights on once-benign household objects and desk fodder . What the world's been waiting for: a stapler with wheels to help humans afflicted by RSI . It was only a matter of time. Having invaded the software world, AI now has now moved on to household objects such as staplans and desk furniture . The world's most recent
‚Ä¢ 'Fax virus' panicked a manager and sparked job-killing Reply-All incident
  The Register celebrates each week in On Call ‚Äì the reader-contributed column that shares your tales of tech support trauma . The 1990s called with a reminder that in the time before ransomware, infosec panics could be quite quaint . On Call: The Register's On Call is a weekly look back at the working week with a certain nostalgia, an emotion we celebrate each week .

üè• Public Health
No updates.

üî¨ Science
‚Ä¢ Global landscape of kidney health across Indigenous populations
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Cardiovascular disease prevention in China: challenges and opportunities in the artificial intelligence-enabled digital health era
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Bacterial sexually transmitted infections and related antibiotic use among individuals eligible for doxycycline post-exposure prophylaxis in the United States
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Exploring sleep and vision impairment through a Patient and Public Involvement and Engagement (PPIE) lens: insights from multiple stakeholders
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Germany‚Äôs national genomDE strategy
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations

üßæ Government & Policy
No updates.

üèõÔ∏è Enterprise Architecture & IT Governance
No updates.

ü§ñ AI & Emerging Tech
‚Ä¢ From slop to Sotheby‚Äôs? AI art enters a new phase
  In this era of AI slop, the idea that generative AI tools like Midjourney and Runway could be used to make art can seem absurd: What possible artistic value is there to be found in the likes of Shrimp Jesus and Ballerina Cappuccina? But amid all the muck, there are people using AI tools with real consideration and intent. Some of them are finding notable success as AI artists: They are gaining huge online followings, selling their work at auction, and even having it exhibited in galleries and museums.&nbsp;





‚ÄúSometimes you need a camera, sometimes AI, and sometimes paint or pencil or any other medium,‚Äù says Jacob Adler, a musician and composer who won the top prize at the generative video company Runway‚Äôs third annual AI Film Festival for his work Total Pixel Space. ‚ÄúIt‚Äôs just one tool that is added to the creator‚Äôs toolbox.‚Äù¬†



One of the most conspicuous features of generative AI tools is their accessibility. With no training and in very little time, you can create an image of whatever you can imagine in whatever style you desire. That‚Äôs a key reason AI art has attracted so much criticism: It‚Äôs now trivially easy to clog sites like Instagram and TikTok with vapid nonsense, and companies can generate images and video themselves instead of hiring trained artists.



Henry Daubrez created these visuals for a bitcoin NFT titled The Order of Satoshi, which sold at Sotheby‚Äôs for $24,000.COURTESY OF THE ARTIST




Henry Daubrez, an artist and designer who created the AI-generated visuals for a bitcoin NFT that sold for $24,000 at Sotheby‚Äôs and is now Google‚Äôs first filmmaker in residence, sees that accessibility as one of generative AI‚Äôs most positive attributes. People who had long since given up on creative expression, or who simply never had the time to master a medium, are now creating and sharing art, he says.¬†



But that doesn‚Äôt mean the first AI-generated masterpiece could come from just anyone. ‚ÄúI don‚Äôt think [generative AI] is going to create an entire generation of geniuses,‚Äù says Daubrez, who has described himself as an ‚ÄúAI-assisted artist.‚Äù Prompting tools like DALL-E and Midjourney might not require technical finesse, but getting those tools to create something interesting, and then evaluating whether the results are any good, takes both imagination and artistic sensibility, he says: ‚ÄúI think we‚Äôre getting into a new generation which is going to be driven by taste.‚Äù&nbsp;



Kira Xonorika‚Äôs Trickster is the first piece to use generative AI in the Denver Art Museum‚Äôs permanent collection.COURTESY OF THE ARTIST




Even for artists who do have experience with other media, AI can be more than just a shortcut. Beth Frey, a trained fine artist who shares her AI art on an Instagram account with over 100,000 followers, was drawn to early generative AI tools because of the uncanniness of their creations‚Äîshe relished the deformed hands and haunting depictions of eating. Over time, the models‚Äô errors have been ironed out, which is part of the reason she hasn‚Äôt posted an AI-generated piece on Instagram in over a year. ‚ÄúThe better it gets, the less interesting it is for me,‚Äù she says. ‚ÄúYou have to work harder to get the glitch now.‚Äù



Beth Frey‚Äôs Instagram account @sentientmuppetfactory features uncanny AI creations.COURTESY OF THE ARTIST




Making art with AI can require relinquishing control‚Äîto the companies that update the tools, and to the tools themselves. For Kira Xonorika, a self-described ‚ÄúAI-collaborative artist‚Äù whose short film Trickster is the first generative AI piece in the Denver Art Museum‚Äôs permanent collection, that lack of control is part of the appeal. ‚Äú[What] I really like about AI is the element of unpredictability,‚Äù says Xonorika, whose work explores themes such as indigeneity and nonhuman intelligence. ‚ÄúIf you‚Äôre open to that, it really enhances and expands ideas that you might have.‚Äù



But the idea of AI as a co-creator‚Äîor even simply as an artistic medium‚Äîis still a long way from widespread acceptance. To many people, ‚ÄúAI art‚Äù and ‚ÄúAI slop‚Äù remain synonymous. And so, as grateful as Daubrez is for the recognition he has received so far, he‚Äôs found that pioneering a new form of art in the face of such strong opposition is an emotional mixed bag. ‚ÄúAs long as it‚Äôs not really accepted that AI is just a tool like any other tool and people will do whatever they want with it‚Äîand some of it might be great, some might not be‚Äîit‚Äôs still going to be sweet [and] sour,‚Äù he says.
‚Ä¢ Take our quiz: How much do you know about antimicrobial resistance?
  A growing number of harmful bacteria and fungi are becoming resistant to antibiotics . The WHO report shows that the problem is surging around the world . How much do you know about microbes, antibiotics, and the scale of the problem? Test your knowledge below to see how much you've heard about the AMR threat in this week‚Äôs Checkup . Read our weekly biotech newsletter from MIT Technology Review .
‚Ä¢ Unlocking the potential of SAF with book and claim in air freight
  Leaders discuss the urgency for reducing air freight emissions for freight forwarders and shippers . They also explain how companies can best make use of the book and claim model to support their emissions reduction strategies . In its neat form, SAF has the potential to reduce life cycle GHG emissions by up to 80% compared to conventional jet fuel . This webcast is presented by MIT Technology Review Insights in association with Avelia .
‚Ä¢ The Download: creating the perfect baby, and carbon removal‚Äôs lofty promises
  This is today&#8217;s edition of¬†The Download,¬†our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



The race to make the perfect baby is creating an ethical mess



An emerging field of science is seeking to use cell analysis to predict what kind of a person an embryo might eventually become.Some parents turn to these tests to avoid passing on devastating genetic disorders that run in their families. A much smaller group, driven by dreams of Ivy League diplomas or attractive, well-behaved offspring, are willing to pay tens of thousands of dollars to optimize for intelligence, appearance, and personality.But customers of the companies emerging to provide it to the public may not be getting what they‚Äôre paying for. Read the full story.



‚ÄîJulia Black



This story is from our forthcoming print issue, which is all about the body. If you haven‚Äôt already, subscribe now to receive future issues once they land. Plus, you&#8217;ll also receive a free digital report on nuclear power.







The problem with Big Tech‚Äôs favorite carbon removal tech



Sucking carbon pollution out of the atmosphere is becoming a big business‚Äîcompanies are paying top dollar for technologies that can cancel out their own emissions.



Tech giants like Microsoft are betting big on one technology: bioenergy with carbon capture and storage (BECCS). But there are a few potential problems with BECCS, as my colleague James Temple laid out in a new story. And some of the concerns echo similar problems with other climate technologies we cover, like carbon offsets and alternative jet fuels. Read the full story.



‚ÄîCasey Crownhart



This article is from The Spark, MIT Technology Review‚Äôs weekly climate newsletter. To receive it in your inbox every Wednesday, sign up here.







2025 climate tech companies to watch: Fervo Energy and its advanced geothermal power plants



Some places on Earth hit the geological jackpot for generating electricity. In those spots, three conditions naturally align: high temperatures, plentiful water, and rock that‚Äôs permeable enough for fluids to circulate through.Enhanced geothermal systems aim to replicate those conditions in far more places‚Äîproducing a steady supply of renewable energy wherever they‚Äôre deployed. Fervo Energy uses fracking techniques to create geothermal reservoirs capable of delivering enough electricity to power massive data centers and hundreds of thousands of homes. Read the full story.



‚ÄîCelina Zhao



Fervo Energy is one of our 10 climate tech companies to watch‚Äîour annual list of some of the most promising climate tech firms on the planet. Check out the rest of the list here.







The must-reads



I‚Äôve combed the internet to find you today‚Äôs most fun/important/scary/fascinating stories about technology.



1 Meta removed a Facebook group that shared ICE agent sightingsIt‚Äôs the latest tech company to acquiesce to US government pressure. (NYT $)+ Meta says the group violates its policies against ‚Äúcoordinated harm.‚Äù (NBC News)+ Another effort to track ICE raids was just taken offline. (MIT Technology Review)2 Loss-making AI startups are still soaring in valueIf it looks like a bubble, and sounds like a bubble‚Ä¶ (FT $)+ AI-backed energy firms have also ballooned in value. (WSJ $)+ Scaling isn‚Äôt always the answer, y&#8217;know. (Wired $)



3 Facial recognition is failing people with facial differencesYet it&#8217;s being embedded in everything from phone unlocking systems to public services. (Wired $)



4 Tech billionaires are backing a startup that treats tumors with sound wavesIt‚Äôs being touted as a less-invasive alternative to chemotherapy. (Bloomberg $)



5 Scam texts are a billion-dollar criminal enterpriseAnd we‚Äôre being inundated with more of them than ever before. (WSJ $)+ The people using humor to troll their spam texts. (MIT Technology Review)



6 South Korea has rolled back an AI textbook program for schoolsTurns out it was riddled with inaccuracies and added to teachers‚Äô workloads. (Rest of World)+ The country is considering allowing Google and Apple to make hi-res maps. (TechCrunch)



7 YouTube is setting its sights on sportsWhich makes sense, given that it‚Äôs conquered pretty much all the other TV genres. (Hollywood Reporter $)



8 Job hunting in the age of AI is bleakEven the best candidates are being overlooked. (The Atlantic $)+ The job market is a mess too. (Slate $)



9 A new channel broadcasts a livestream direct from the ISS If you‚Äôve ever wanted to be an astronaut, watching this is the next best thing. (The Guardian)10 The end of support for Windows 10 is an e-waste disasterUp to 400 million machines could be heading to the scrap heap. (404 Media)+ The US government has cut funding for a battery-metals recycler. (Bloomberg $)+ AI will add to the e-waste problem. Here‚Äôs what we can do about it. (MIT Technology Review)







Quote of the day



‚ÄúWe are not the elected moral police of the world.‚Äù



‚ÄîOpenAI CEO Sam Altman reacts to the outcry sparked by his company‚Äôs decision to relax its rules to let adults hold erotic conversations with ChatGPT, CNBC reports.







One more thing







Inside India‚Äôs scramble for AI independenceDespite its status as a global tech hub, India lags far behind the likes of the US and China when it comes to homegrown AI.That gap has opened largely because India has chronically underinvested in R&amp;D, institutions, and invention. Meanwhile, since no one native language is spoken by the majority of the population, training language models is far more complicated than it is elsewhere.So when the open-source foundation model DeepSeek-R1 suddenly outperformed many global peers, it struck a nerve. This launch by a Chinese startup prompted Indian policymakers to confront just how far behind the country was in AI infrastructure‚Äîand how urgently it needed to respond. Read the full story.



‚ÄîShadma Shaikh







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)



+ This haunting shot of a hyena is this year‚Äôs Wildlife Photographer of the Year award winner (thanks Laurel!)+ Madonna sure has a lot of famous friends.+¬† This little giraffe is so sleepy + Late ‚Äò80s dance heads, rise up!
‚Ä¢ Meet the man building a starter kit for civilization
  You live in a house you designed and built yourself. You rely on the sun for power, heat your home with a woodstove, and farm your own fish and vegetables. The year is 2025.&nbsp;



This is the life of Marcin Jakubowski, the 53-year-old founder of Open Source Ecology, an open collaborative of engineers, producers, and builders developing what they call the Global Village Construction Set (GVCS). It‚Äôs a set of 50 machines‚Äîeverything from a tractor to an oven to a circuit maker‚Äîthat are capable of building civilization from scratch and can be reconfigured however you see fit.&nbsp;



Jakubowski immigrated to the US from Slupca, Poland, as a child. His first encounter with what he describes as the ‚Äúprosperity of technology‚Äù was the vastness of the American grocery store. Seeing the sheer quantity and variety of perfectly ripe produce cemented his belief that abundant, sustainable living was within reach in the United States.&nbsp;





With a bachelor‚Äôs degree from Princeton and a doctorate in physics from the University of Wisconsin, Jakubowski had spent most of his life in school. While his peers kick-started their shiny new corporate careers, he followed a different path after he finished his degree in 2003: He bought a tractor to start a farm in Maysville, Missouri, eager to prove his ideas about abundance. ‚ÄúIt was a clear decision to give up the office cubicle or high-level research job, which is so focused on tiny issues that one never gets to work on the big picture,‚Äù he says. But in just a short few months, his tractor broke down‚Äîand he soon went broke.&nbsp;



Every time his tractor malfunctioned, he had no choice but to pay John Deere for repairs‚Äîeven if he knew how to fix the problem on his own. John Deere, the world‚Äôs largest manufacturer of agricultural equipment, continues to prohibit farmers from repairing their own tractors (except in Colorado, where farmers were granted a right to repair by state law in 2023). Fixing your own tractor voids any insurance or warranty, much like jailbreaking your iPhone.&nbsp;



Today, large agricultural manufacturers have centralized control over the market, and most commercial tractors are built with proprietary parts. Every year, farmers pay $1.2 billion in repair costs and lose an estimated $3 billion whenever their tractors break down, entirely because large agricultural manufacturers have lobbied against the right to repair since the ‚Äô90s. Currently there are class action lawsuits involving hundreds of farmers fighting for their right to do so.



‚ÄúThe machines own farmers. The farmers don‚Äôt own [the machines],‚Äù Jakubowski says. He grew certain that self-sufficiency relied on agricultural autonomy, which could be achieved only through free access to technology. So he set out to apply the principles of open-source software to hardware. He figured that if farmers could have access to the instructions and materials required to build their own tractors, not only would they be able to repair them, but they‚Äôd also be able to customize the vehicles for their needs. Life-changing technology should be available to all, he thought, not controlled by a select few. So, with an understanding of mechanical engineering, Jakubowski built his own tractor and put all his schematics online on his platform Open Source Ecology.&nbsp;&nbsp;



That tractor Jakubowski built is designed to be taken apart. It‚Äôs a critical part of the GVCS, a collection of plug-and-play machines that can ‚Äúbuild a thriving economy anywhere in the world ‚Ä¶ from scratch.‚Äù The GVCS includes a 3D printer, a self-contained hydraulic power unit called the Power Cube, and more, each designed to be reconfigured for multiple purposes. There‚Äôs even a GVCS micro-home. You can use the Power Cube to power a brick press, a sawmill, a car, a CNC mill, or a bioplastic extruder, and you can build wind turbines with the frames that are used in the home.&nbsp;



Jakubowski compares the GVCS to Lego blocks and cites the Linux ecosystem as his inspiration. In the same way that Linux‚Äôs source code is free to inspect, modify, and redistribute, all the instructions you need to build and repurpose a GVCS machine are freely accessible online. Jakubowski envisions a future in which the GVCS parallels the Linux infrastructure, with custom tools built to optimize agriculture, construction, and material fabrication in localized contexts. ‚ÄúThe [final form of the GVCS] must be proven to allow efficient production of food, shelter, consumer goods, cars, fuel, and other goods‚Äîexcept for exotic imports (coffee, bananas, advanced semiconductors),‚Äù he wrote on his Open Source Ecology wiki.&nbsp;



The ethos of GVCS is reminiscent of the Whole Earth Catalog, a countercultural publication that offered a combination of reviews, DIY manuals, and survival guides between 1968 and 1972. Founded by Stewart Brand, the publication had the slogan ‚ÄúAccess to tools‚Äù and was famous for promoting self-sufficiency. It heavily featured the work of R. Buckminster Fuller, an American architect known for his geodesic domes (lightweight structures that can be built using recycled materials) and for coining the term ‚Äúephemeralization,‚Äù which refers to the ability of technology to let us do more with less material, energy, and effort.&nbsp;



The schematics for Marcin Jakubowski&#8217;s designs are all available online.COURTESY OF OPEN SOURCE ECOLOGY




Jakubowski owns the publication‚Äôs entire printed output, but he offers a sharp critique of its legacy in our current culture of tech utopianism. ‚ÄúThe first structures we built were domes. Good ideas. But the open-source part of that was not really there yet‚ÄîFuller patented his stuff,‚Äù he says. Fuller and the Whole Earth Catalog may have popularized an important philosophy of self-reliance, but to Jakubowski, their failure to advocate for open collaboration stopped the ultimate vision of sustainability from coming to fruition. ‚ÄúThe failure of the techno-utopians to organize into a larger movement of collaborative, open, distributed production resulted in a miscarriage of techno-utopia,‚Äù he says.&nbsp;



With a background in physics and an understanding of mechanical engineering, Marcin Jakubowski built his own tractor.COURTESY OF OPEN SOURCE ECOLOGY




Unlike software, hardware can‚Äôt be infinitely reproduced or instantly tested. It requires manufacturing infrastructure and specific materials, not to mention exhaustive documentation. There are physical constraints‚Äîdifferent port standards, fluctuations in availability of materials, and more. And now that production chains are so globalized that manufacturing a hot tub can require parts from seven different countries and 14 states, how can we expect anything to be replicable in our backyard? The solution, according to Jakubowski, is to make technology ‚Äúappropriate.‚Äù&nbsp;



Appropriate technology is technology that‚Äôs designed to be affordable and sustainable for a specific local context. The idea comes from Gandhi‚Äôs philosophy of swadeshi (self-reliance) and sarvodaya (upliftment of all) and was popularized by the economist Ernst Friedrich ‚ÄúFritz‚Äù Schumacher‚Äôs book Small Is Beautiful, which discussed the concept of ‚Äúintermediate technology‚Äù: ‚ÄúAny intelligent fool can make things bigger, more complex, and more violent. It takes a touch of genius‚Äîand a lot of courage‚Äîto move in the opposite direction.‚Äù Because different environments operate at different scales and with different resources, it only makes sense to tailor technology for those conditions. Solar lamps, bikes, hand-¬≠powered water pumps‚Äîanything that can be built using local materials and maintained by the local community‚Äîare among the most widely cited examples of appropriate technology.&nbsp;





This concept has historically been discussed in the context of facilitating economic growth in developing nations and adapting capital-intensive technology to their needs. But Jakubowski hopes to make it universal. He believes technology needs to be appropriate even in suburban and urban places with access to supermarkets, hardware stores, Amazon deliveries, and other forms of infrastructure. If technology is designed specifically for these contexts, he says, end-to-end reproduction will be possible, making more space for collaboration and innovation.&nbsp;



What makes Jakubowski‚Äôs technology ‚Äúappropriate‚Äù is his use of reclaimed materials and off-the-shelf parts to build his machines. By using local materials and widely available components, he‚Äôs able to bypass the complex global supply chains that proprietary technology often requires. He also structures his schematics around concepts already familiar to most people who are interested in hardware, making his building instructions easier to follow.



Everything you need to build Jakubowski‚Äôs machines should be available around you, just as everything you need to know about how to repair or operate the machine is online‚Äîfrom blueprints to lists of materials to assembly instructions and testing protocols. ‚ÄúIf you‚Äôve got a wrench, you‚Äôve got a tractor,‚Äù his manual reads.&nbsp;&nbsp;



This spirit dates back to the ‚Äô70s, when the idea of building things ‚Äúmoved out of the retired person‚Äôs garage and into the young person‚Äôs relationship with the Volkswagen,‚Äù says Brand. He references John Muir‚Äôs 1969 book How to Keep Your Volkswagen Alive: A Manual of Step-by-Step Procedures for the Compleat Idiot and fondly recalls how the Beetle‚Äôs simple design and easily swapped parts made it common for owners to rebody their cars, combining the chassis of one with the body of another. He also mentions the impact of the Ford Model T cars that, with a few extra parts, were made into tractors during the Great Depression.&nbsp;



For Brand, the focus on repairability is critical in the modern context. There was a time when John Deere tractors were ‚Äúappropriate‚Äù in Jakubowski‚Äôs terms, Brand says: ‚ÄúA century earlier, John Deere took great care to make sure that his plowshares could be taken apart and bolted together, that you can undo and redo them, replace parts, and so on.‚Äù The company ‚Äúattracted insanely loyal customers because they looked out for the farmers so much,‚Äù Brand says, but ‚Äúthey‚Äôve really reversed the orientation.‚Äù Echoing Jakubowski‚Äôs initial motivation for starting OSE, Brand insists that technology is appropriate to the extent that it is repairable.&nbsp;



Even if you can find all the parts you need from Lowe‚Äôs, building your own tractor is still intimidating. But for some, the staggering price advantage is reason enough to take on the challenge: A GVCS tractor costs $12,000 to build, whereas a commercial tractor averages around $120,000 to buy, not including the individual repairs that might be necessary over its lifetime at a cost of $500 to $20,000 each. And gargantuan though it may seem, the task of building a GVCS tractor or other machine is doable: Just a few years after the project launched in 2008, more than 110 machines had been built by enthusiasts from Chile, Nicaragua, Guatemala, China, India, Italy, and Turkey, just to name a few places.&nbsp;



Of the many machines developed, what‚Äôs drawn the most interest from GVCS enthusiasts is the one nicknamed ‚ÄúThe Liberator,‚Äù which presses local soil into compressed earth blocks, or CEBs‚Äîa type of cost- and energy-¬≠efficient brick that can withstand extreme weather conditions. It‚Äôs been especially popular among those looking to build their own homes: A man named Aur√©lien Bielsa replicated the brick press in a small village in the south of France to build a house for his family in 2018, and in 2020 a group of volunteers helped a member of the Open Source Ecology community build a tiny home using blocks from one of these presses in a fishing village near northern Belize.&nbsp;



The CEB press, nicknamed ‚ÄúThe Liberator,‚Äù turns local soil into energy-efficient compressed earth blocks.COURTESY OF OPEN SOURCE ECOLOGY




Jakubowski recalls receiving an email about one of the first complete reproductions of the CEB press, built by a Texan named James Slate, who ended up starting a business selling the bricks: ‚ÄúWhen [James] sent me a picture [of our brick press], I thought it was a Photoshopped copy of our machine, but it was his. He just downloaded the plans off the internet. I knew nothing about it.‚Äù Slate described having a very limited background in engineering before building the brick press. ‚ÄúI had taken some mechanics classes back in high school. I mostly come from an IT computer world,‚Äù he said in an interview with Open Source Ecology. ‚ÄúPretty much anyone can build one, if they put in the effort.‚Äù&nbsp;





Andrew Spina, an early GVCS enthusiast, agrees. Spina spent five years building versions of the GVCS tractor and Power Cube, eager to create means of self-¬≠sufficiency at an individual scale. ‚ÄúI‚Äôm building my own tractor because I want to understand it and be able to maintain it,‚Äù he wrote in his blog, Machining Independence. Spina‚Äôs curiosity gestures toward the broader issue of technological literacy: The more we outsource to proprietary tech, the less we understand how things work‚Äîfurther entrenching our need for that proprietary tech. Transparency is critical to the open-source philosophy precisely because it helps us become self-sufficient.&nbsp;



Since starting Open Source Ecology, Jakubowski has been the main architect behind the dozens of machines available on his platform, testing and refining his designs on a plot of land he calls the Factor¬†e Farm in Maysville. Most GVCS enthusiasts reproduce Jakubowski‚Äôs machines for personal use; only a few have contributed to the set themselves. Of those select few, many made dedicated visits to the farm for weeks at a time to learn how to build Jakubowski‚Äôs GVCS collection. James Wise, one of the earliest and longest-term GVCS contributors, recalls setting up tents and camping out in his car to attend sessions at Jakubowski‚Äôs workshop, where visiting enthusiasts would gather to iterate on designs: ‚ÄúWe‚Äôd have a screen on the wall of our current best idea. Then we‚Äôd talk about it.‚Äù Wise doesn‚Äôt consider himself particularly experienced on the engineering front, but after working with other visiting participants, he felt more emboldened to contribute. ‚ÄúMost of [my] knowledge came from [my] peers,‚Äù he says.¬†



Jakubowski‚Äôs goal of bolstering collaboration hinges on a degree of collective proficiency. Without a community skilled with hardware, the organic innovation that the open-source approach promises will struggle to bear fruit, even if Jakubowski‚Äôs designs are perfectly appropriate and thoroughly documented.



‚ÄúThat‚Äôs why we‚Äôre starting a school!‚Äù said Jakubowski, when asked about his plan to build hardware literacy. Earlier this year, he announced the Future Builders Academy, an apprenticeship program where participants will be taught all the necessary skills to develop and build the affordable, self-sustaining homes that are his newest venture. Seed Eco Homes, as Jakubowski calls them, are ‚Äúhuman-sized, panelized‚Äù modular houses complete with a biodigester, a thermal battery, a geothermal cooling system, and solar electricity. Each house is entirely energy independent and can be built in five days, at a cost of around $40,000. Over eight of these houses have been built across the country, and Jakubowski himself lives in the earliest version of the design. Seed Eco Homes are the culmination of his work on the GVCS: The structure of each house combines parts from the collection and embodies its modular philosophy. The venture represents Jakubowski‚Äôs larger goal of making everyday technology accessible. ‚ÄúHousing [is the] single largest cost in one‚Äôs life‚Äîand a key to so much more,‚Äù he says.



The final goal of Open Source Ecology is a ‚Äúzero marginal cost‚Äù society, where producing an additional unit of a good or service costs little to nothing. Jakubowski‚Äôs interpretation of the concept (popularized by the American economist and social theorist Jeremy Rifkin) assumes that by eradicating licensing fees, decentralizing manufacturing, and fostering collaboration through education, we can develop truly equitable technology that allows us to be self-sufficient. Open-source hardware isn‚Äôt just about helping farmers build their own tractors; in Jakubowski‚Äôs view, it‚Äôs a complete reorientation of our relationship to technology.&nbsp;



In the first issue of the Whole Earth Catalog, a key piece of inspiration for Jakubowski‚Äôs project, Brand wrote: ‚ÄúWe are as gods and we might as well get good at it.‚Äù In 2007, in a book Brand wrote about the publication, he corrected himself: ‚ÄúWe are as gods and have to get good at it.‚Äù Today, Jakubowski elaborates: ‚ÄúWe‚Äôre becoming gods with technology. Yet technology has badly failed us. We‚Äôve seen great progress with civilization. But how free are people today compared to other times?‚Äù Cautioning against our reliance on the proprietary technology we use daily, he offers a new approach: Progress should mean not just achieving technological breakthroughs but also making everyday technology equitable.&nbsp;



‚ÄúWe don‚Äôt need more technology,‚Äù he says. ‚ÄúWe just need to collaborate with what we have now.‚Äù



Tiffany Ng is a freelance writer exploring the relationship between art, tech, and culture. She writes Cyber Celibate, a neo-Luddite newsletter on Substack.&nbsp;

üîí Cybersecurity & Privacy
‚Ä¢ Patch Tuesday, October 2025 ‚ÄòEnd of 10‚Äô Edition
  Microsoft today released software updates to plug a whopping 172 security holes in its Windows operating systems, including at least two vulnerabilities that are already being actively exploited. October&#8217;s Patch Tuesday also marks the final month that Microsoft will ship security updates for Windows 10 systems. If you&#8217;re running a Windows 10 PC and you&#8217;re unable or unwilling to migrate to Windows 11, read on for other options.

The first zero-day bug addressed this month (CVE-2025-24990) involves a third-party modem driver called Agere Modem that&#8217;s been bundled with Windows for the past two decades. Microsoft responded to active attacks on this flaw by completely removing the vulnerable driver from Windows.
The other zero-day is CVE-2025-59230, an elevation of privilege vulnerability in Windows Remote Access Connection Manager (also known as RasMan), a service used to manage remote network connections through virtual private networks (VPNs) and dial-up networks.
&#8220;While RasMan is a frequent flyer on Patch Tuesday, appearing more than 20 times since January 2022, this is the first time we&#8217;ve seen it exploited in the wild as a zero day,&#8221; said Satnam Narang, senior staff research engineer at Tenable.
Narang notes that Microsoft Office users should also take note of CVE-2025-59227 and CVE-2025-59234, a pair of remote code execution bugs that take advantage of &#8220;Preview Pane,‚Äù meaning that the target doesn‚Äôt even need to open the file for exploitation to occur. To execute these flaws, an attacker would social engineer a target into previewing an email with a malicious Microsoft Office document.
Speaking of Office, Microsoft quietly announced this week that Microsoft Word will now automatically save documents to OneDrive, Microsoft&#8217;s cloud platform. Users who are uncomfortable saving all of their documents to Microsoft&#8217;s cloud can change this in Word&#8217;s settings; ZDNet has a useful how-to on disabling this feature.
Kev Breen, senior director of threat research at Immersive, called attention to CVE-2025-59287, a critical remote code execution bug in the Windows Server Update Service¬† (WSUS) &#8212; the very same Windows service responsible for downloading security patches for Windows Server versions. Microsoft says there are no signs this weakness is being exploited yet. But with a threat score of 9.8 out of possible 10 and marked &#8220;exploitation more likely,&#8221; CVE-2025-59287 can be exploited without authentication and is an easy &#8220;patch now&#8221; candidate.
&#8220;Microsoft provides limited information, stating that an unauthenticated attacker with network access can send untrusted data to the WSUS server, resulting in deserialization and code execution,&#8221; Breen wrote. &#8220;As WSUS is a trusted Windows service that is designed to update privileged files across the file system, an attacker would have free rein over the operating system and could potentially bypass some EDR detections that ignore or exclude the WSUS service.&#8221;
For more on other fixes from Redmond today, check out the SANS Internet Storm Center monthly roundup, which indexes all of the updates by severity and urgency.
Windows 10 isn&#8217;t the only Microsoft OS that is reaching end-of-life today;¬†Exchange Server 2016, Exchange Server 2019, Skype for Business 2016, Windows 11 IoT Enterprise Version 22H2, and Outlook 2016 are some of the other products that Microsoft is sunsetting today.

If you&#8217;re running any Windows 10 systems, you&#8217;ve probably already determined whether your PC meets the technical hardware specs recommended for the Windows 11 OS. If you&#8217;re reluctant or unable to migrate a Windows 10 system to Windows 11, there are alternatives to simply continuing to use Windows 10 without ongoing security updates.
One option is to pay for another year&#8217;s worth of security updates through Microsoft&#8217;s Extended Security Updates (ESU) program. The cost is just $30 if you don&#8217;t have a Microsoft account, and apparently free if you register the PC to a Microsoft account. This video breakdown from Ask Your Computer Guy does a good job of walking Windows 10 users through this process. Microsoft emphasizes that ESU enrollment does not provide other types of fixes, feature improvements or product enhancements. It also does not come with technical support.
If your Windows 10 system is associated with a Microsoft account and signed in when you visit Windows Update, you should see an option to enroll in extended updates. Image: https://www.youtube.com/watch?v=SZH7MlvOoPM
Windows 10 users also have the option of installing some flavor of Linux instead. Anyone seriously considering this option should check out the website endof10.org, which includes a plethora of tips and a DIY installation guide.
Linux Mint is a great option for Linux newbies. Like most modern Linux versions, Mint will run on anything with a 64-bit CPU that has at least 2GB of memory, although 4GB is recommended. In other words, it will run on almost any computer produced in the last decade.
Linux Mint also is likely to be the most intuitive interface for regular Windows users, and it is largely configurable without any fuss at the text-only command-line prompt. Mint and other flavors of Linux come with LibreOffice, which is an open source suite of tools that includes applications similar to Microsoft Office, and it can open, edit and save documents as Microsoft Office files.
If you‚Äôd prefer to give Linux a test drive before installing it on a Windows PC, you can always just download it to a removable USB drive. From there, reboot the computer (with the removable drive plugged in) and select the option at startup to run the operating system from the external USB drive. If you don‚Äôt see an option for that after restarting, try restarting again and hitting the F8 button, which should open a list of bootable drives.¬†Here‚Äôs a fairly thorough tutorial¬†that walks through exactly how to do all this.
And if this is your first time trying out Linux, relax and have fun: The nice thing about a ‚Äúlive‚Äù version of Linux (as it‚Äôs called when the operating system is run from a removable drive such as a CD or a USB stick) is that none of your changes persist after a reboot. Even if you somehow manage to break something, a restart will return the system back to its original state.
As ever, if you experience any difficulties during or after applying this month&#8217;s batch of patches, please leave a note about it in the comments below.

üéì University AI
No updates.

üè¢ Corporate AI
‚Ä¢ Iterative fine-tuning on Amazon Bedrock for strategic model improvement
  Organizations often face challenges when implementing single-shot fine-tuning approaches for their generative AI models. The single-shot fine-tuning method involves selecting training data, configuring hyperparameters, and hoping the results meet expectations without the ability to make incremental adjustments. Single-shot fine-tuning frequently leads to suboptimal results and requires starting the entire process from scratch when improvements are needed. 
Amazon Bedrock now supports iterative fine-tuning, enabling systematic model refinement through controlled, incremental training rounds. With this capability you can build upon previously customized models, whether they were created through fine-tuning or distillation, providing a foundation for continuous improvement without the risks associated with complete retraining. 
In this post, we will explore how to implement the iterative fine-tuning capability of Amazon Bedrock to systematically improve your AI models. We‚Äôll cover the key advantages over single-shot approaches, walk through practical implementation using both the console and SDK, discuss deployment options, and share best practices for maximizing your iterative fine-tuning results. 
When to use iterative fine-tuning 
Iterative fine-tuning provides several advantages over single-shot approaches that make it valuable for production environments. Risk mitigation becomes possible through incremental improvements, so you can test and validate changes before committing to larger modifications. With this approach, you can make data-driven optimization based on real performance feedback rather than theoretical assumptions about what might work. The methodology also helps developers to apply different training techniques sequentially to refine model behavior. Most importantly, iterative fine-tuning accommodates evolving business requirements driven by continuous live data traffic. As user patterns change over time and new use cases emerge that weren‚Äôt present in initial training, you can leverage this fresh data to refine your model‚Äôs performance without starting from scratch. 
How to implement iterative fine-tuning on Amazon Bedrock 
Setting up iterative fine-tuning involves preparing your environment and creating training jobs that build upon your existing custom models, whether through the console interface or programmatically using the SDK. 
Prerequisites 
Before beginning iterative fine-tuning, you need a previously customized model as your starting point. This base model can originate from either fine-tuning or distillation processes and supports customizable models and variants available on Amazon Bedrock. You‚Äôll also need: 
 
 Standard IAM permissions for Amazon Bedrock model customization 
 Incremental training data focused on addressing specific performance gaps 
 S3 bucket for training data and job outputs 
 
Your incremental training data should target the specific areas where your current model needs improvement rather than attempting to retrain on all possible scenarios. 
Using the AWS Management Console 
The Amazon Bedrock console provides a straightforward interface for creating iterative fine-tuning jobs. 
Navigate to the Custom Models section and select Create fine-tuning job. The key difference in iterative fine-tuning lies in the base model selection, where you choose your previously customized model instead of a foundation model.  
During training, you can visit the Custom models page in the Amazon Bedrock console to track the job status.  
Once complete, you can monitor your jobs performance metrics on console through multiple metric charts, on the Training metrics and Validation metrics tabs.  
Using the SDK 
Programmatic implementation of iterative fine-tuning follows similar patterns to standard fine-tuning with one critical difference: specifying your previously customized model as the base model identifier. Here‚Äôs an example implementation: 
 
 import boto3
from datetime import datetime
import uuid

# Initialize Bedrock client
bedrock = boto3.client('bedrock')

# Define job parameters
job_name = f"iterative-finetuning-{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}"
custom_model_name = f"iterative-model-{str(uuid.uuid4())[:8]}"

# Key difference: Use your previously customized model ARN as base
# This could be from previous fine-tuning or distillation
base_model_id = "arn:aws:bedrock:&lt;Region&gt;:&lt;AccountID&gt;:custom-model/&lt;your-previous-custom-model-id&gt;"

# S3 paths for training data and outputs
training_data_uri = "s3://&lt;your-bucket&gt;/&lt;iterative-training-data&gt;"
output_path = "s3://&lt;your-bucket&gt;/&lt;iterative-output-folder&gt;/"

# Hyperparameters adjusted based on previous iteration learnings
hyperparameters = {
    "epochCount": "3" # Example
}

# Create the iterative fine-tuning job
response = bedrock.create_model_customization_job(
    customizationType="FINE_TUNING",
    jobName=job_name,
    customModelName=custom_model_name,
    roleArn=role_arn,
    baseModelIdentifier=base_model_id,  # Your previously customized model
    hyperParameters=hyperparameters,
    trainingDataConfig={
        "s3Uri": training_data_uri
    },
    outputDataConfig={
        "s3Uri": output_path
    }
)

job_arn = response.get('jobArn')
print(f"Iterative fine-tuning job created with ARN: {job_arn}")
 
 
Setting up inference for your iteratively fine-tuned model 
Once your iterative fine-tuning job completes, you have two primary options for deploying your model for inference, provisioned throughput and on-demand inference, each suited to different usage patterns and requirements. 
Provisioned Throughput 
Provisioned Throughput offers stable performance for predictable workloads where consistent throughput requirements exist. This option provides dedicated capacity so that the iteratively fine-tuned model maintains performance standards during peak usage periods. Setup involves purchasing model units based on expected traffic patterns and performance requirements. 
On-demand inference 
On-demand inference provides flexibility for variable workloads and experimentation scenarios. Amazon Bedrock now supports Amazon Nova Micro, Lite, and Pro models as well as Llama 3.3 models for on-demand inference with pay-per-token pricing. This option avoids the need for capacity planning so you can test your iteratively fine-tuned model without upfront commitments. The pricing model scales automatically with usage, making it cost-effective for applications with unpredictable or low-volume inference patterns. 
Best practices 
Successful iterative fine-tuning requires attention to several key areas. Most importantly, your data strategy should emphasize quality over quantity in incremental datasets. Rather than adding large volumes of new training examples, focus on high-quality data that addresses specific performance gaps identified in previous iterations. 
To track progress effectively, evaluation consistency across iterations allows meaningful comparison of improvements. Establish baseline metrics during your first iteration and maintain the same evaluation framework throughout the process. You can use Amazon Bedrock Evaluations to help you systematically identify where gaps exist in your model performance after each customization run. This consistency helps you understand whether changes are producing meaningful improvements. 
Finally, recognizing when to stop the iterative process helps to prevent diminishing returns on your investment. Monitor performance improvements between iterations and consider concluding the process when gains become marginal relative to the effort required. 
Conclusion 
Iterative fine-tuning on Amazon Bedrock provides a systematic approach to model improvement that reduces risks while enabling continuous refinement. With the iterative fine-tuning methodology organizations can build upon existing investments in custom models rather than starting from scratch when adjustments are needed. 
To get started with iterative fine-tuning, access the Amazon Bedrock console and navigate to the Custom models section. For detailed implementation guidance, refer to the Amazon Bedrock documentation. 
 
About the authors 
Yanyan Zhang is a Senior Generative AI Data Scientist at Amazon Web Services, where she has been working on cutting-edge AI/ML technologies as a Generative AI Specialist, helping customers use generative AI to achieve their desired outcomes. Yanyan graduated from Texas A&amp;M University with a PhD in Electrical Engineering. Outside of work, she loves traveling, working out, and exploring new things. 
Gautam Kumar is an Engineering Manager at AWS AI Bedrock, leading model customization initiatives across large-scale foundation models. He specializes in distributed training and fine-tuning. Outside work, he enjoys reading and traveling. 
Jesse Manders is a Senior Product Manager on Amazon Bedrock, the AWS Generative AI developer service. He works at the intersection of AI and human interaction with the goal of creating and improving generative AI products and services to meet our needs. Previously, Jesse held engineering team leadership roles at Apple and Lumileds, and was a senior scientist in a Silicon Valley startup. He has an M.S. and Ph.D. from the University of Florida, and an MBA from the University of California, Berkeley, Haas School of Business.
‚Ä¢ Voice AI-powered drive-thru ordering with Amazon Nova Sonic and dynamic menu displays
  Artificial Intelligence (AI) is transforming the quick-service restaurant industry, particularly in drive-thru operations where efficiency and customer satisfaction intersect. Traditional systems create significant obstacles in service delivery, from staffing limitations and order accuracy issues to inconsistent customer experiences across locations. These challenges, combined with rising labor costs and demand fluctuations, have pushed the industry to seek innovative solutions. 
In this post, we‚Äôll demonstrate how to implement a Quick Service Restaurants (QSRs) drive-thru solution using Amazon Nova Sonic and AWS services. We‚Äôll walk through building an intelligent system that combines voice AI with interactive menu displays, providing technical insights and implementation guidance to help restaurants modernize their drive-thru operations. 
For QSRs, the stakes are particularly high during peak hours, when long wait times and miscommunication between customers and staff can significantly impact business performance. Common pain points include order accuracy issues, service quality variations across different shifts, and limited ability to handle sudden spikes in customer demand. Modern consumers expect the same seamless, efficient service they experience with digital ordering systems, creating an unprecedented opportunity for voice AI technology to support 24/7 availability and consistent service quality. 
Amazon Nova Sonic is a foundation model (FM) within the Amazon Nova family, designed specifically for voice-enabled applications. Available through Amazon Bedrock, developers can use Nova Sonic to create applications that understand spoken language, process complex conversational interactions, and generate appropriate responses for real-time customer engagement. This innovative speech-to-speech model addresses traditional voice application challenges through: 
 
 Accurately recognizes streaming speech across accents with robustness to background noise 
 Adapts speech response to user‚Äôs tone and sentiment 
 Bidirectional streaming speech I/O with low user perceived latency 
 Graceful interruption handling and natural turn-taking in conversations 
 Industry-leading price-performance 
 
When integrated with AWS serverless services, Nova Sonic delivers natural, human-like voice interactions that helps improve the drive-thru experience. The architecture creates a cost-effective system that enhances both service consistency and operational efficiency through intelligent automation. 
Solution overview 
Our voice AI drive-thru solution creates an intelligent ordering system that combines real-time voice interaction with a robust backend infrastructure, delivering a natural customer experience. The system processes speech in real-time, understanding various accents, speaking styles, and handling background noise common in drive-thru environments. Integrating voice commands with interactive menu displays enhances user feedback while streamlining the ordering process by reducing verbal interactions. 
The system is built on AWS serverless architecture, integrating key components including Amazon Cognito for authentication with&nbsp;role-based access control, AWS Amplify for the digital menu board, Amazon API Gateway to facilitate access to Amazon DynamoDB tables, AWS Lambda functions with Amazon Nova Canvas for menu image generation, and Amazon Simple Storage Service (Amazon S3) with Amazon CloudFront for image storage and delivery. 
The following architecture diagram illustrates how these services interconnect to for natural conversations between customers and the digital menu board, orchestrating the entire customer journey from drive-thru entry to order completion. 
 
Let‚Äôs examine how each component works together to power this intelligent ordering system. 
Prerequisites 
You must have the following in place to complete the solution in this post: 
 
 An AWS account 
 FM access in Amazon Bedrock for Amazon Nova Sonic and Amazon Nova Canvas in the same AWS Region where you will deploy this solution 
 The accompanying AWS CloudFormation templates downloaded from the aws-samples GitHub repo 
 
Deploy solution resources using AWS CloudFormation 
Deploy the CloudFormation templates in an AWS Region where Amazon Bedrock is available and has support for the following models: Amazon Nova Sonic and Amazon Nova Canvas. 
This solution consists of two CloudFormation templates that work together to create a complete restaurant drive-thru ordering system. The nova-sonic-infrastructure-drivethru.yaml template establishes the foundational AWS infrastructure including Cognito user authentication, S3 storage with CloudFront CDN for menu images, DynamoDB tables for menu items and customer data, and API Gateway endpoints with proper CORS configuration. The nova-sonic-application-drivethru.yaml template builds upon this foundation by deploying a Lambda function that populates the system with a complete embedded drive-thru menu featuring burgers, wings, fries, drinks, sauces, and combo meals, while using the Amazon Nova Canvas AI model to automatically generate professional food photography for each menu item and storing them in the S3 bucket for delivery through CloudFront. 
During the deployment of the first CloudFormation template nova-sonic-infrastructure-drivethru.yaml, you will need to specify the following parameters: 
 
 Stack name 
 Environment ‚Äì Deployment environment: dev, staging, or prod (defaults to dev) 
 UserEmail ‚Äì Valid email address for the user account (required) 
 
Important: You must enable access to the selected Amazon Nova Sonic model and Amazon Nova Canvas model in the Amazon Bedrock console before deployment. 
AWS resource usage will incur costs. When deployment is complete, the following resources will be deployed: 
 
 Amazon Cognito resources: 
   
   User pool ‚Äì CognitoUserPool 
   App client ‚Äì AppClient 
   Identity pool ‚Äì CognitoIdentityPool 
   Groups ‚Äì AppUserGroup 
   User ‚Äì AppUser 
    
 
 
 AWS Identity and Access Management (IAM) resources: 
   
   IAM roles: 
     
     AuthenticatedRole 
     DefaultAuthenticatedRole 
     ApiGatewayDynamoDBRole 
     LambdaExecutionRole 
     S3BucketCleanupRole 
      
    
 
 
 Amazon DynamoDB tables: 
   
   MenuTable ‚Äì Stores menu items, pricing, and customization options 
   LoyaltyTable ‚Äì Stores customer loyalty information and points 
   CartTable ‚Äì Stores shopping cart data for active sessions 
   OrderTable ‚Äì Stores completed and pending orders 
   ChatTable ‚Äì Stores completed chat details 
    
 
 
 Amazon S3, CloudFront and AWS WAF resources: 
   
   MenuImagesBucket ‚Äì S3 bucket for storing menu item images 
   MenuImageCloudFrontDistribution ‚Äì CloudFront distribution for global content delivery 
   CloudFrontOriginAccessIdentity ‚Äì Secure access between CloudFront and S3 
   CloudFrontWebACL ‚Äì WAF protection for CloudFront distribution with security rules 
    
 
 
 Amazon API Gateway resources: 
   
   REST API ‚Äì app-api with Cognito authorization 
   API resources and methods: 
     
     /menu (GET, OPTIONS) 
     /loyalty (GET, OPTIONS) 
     /cart (POST, DELETE, OPTIONS) 
     /order (POST, OPTIONS) 
     /chat (POST, OPTIONS) 
      
   API deployment to specified environment stage 
    
 
 
 AWS Lambda function: 
   
   S3BucketCleanupLambda ‚Äì Cleans up S3 bucket on stack deletion 
    
 
 
 CloudFormation Custom Resource: 
   
   S3BucketCleanup ‚Äì Triggers S3BucketCleanupLambda 
    
 
After you deploy the CloudFormation template, copy the following from the Outputs tab on the AWS CloudFormation console to use during the configuration of your frontend application: 
 
 cartApiUrl 
 loyaltyApiUrl 
 menuApiUrl 
 orderApiUrl 
 chatApiUrl 
 UserPoolClientId 
 UserPoolId 
 IdentityPoolId 
 
The following screenshot shows you what the Outputs tab will look like. 
 
These output values are essential for configuring your frontend application (deployed via AWS Amplify) to connect with the backend services. The API URLs will be used for making REST API calls, while the Cognito IDs will be used for user authentication and authorization. 
During the deployment of the second CloudFormation template nova-sonic-application-drivethru.yaml you will need to specify the following parameters: 
 
 Stack name 
 InfrastructureStackName ‚Äì This stack name matches the one you previously deployed using nova-sonic-infrastructure-drivethru.yaml 
 
When deployment is complete, the following resources will be deployed: 
 
 AWS Lambda function: 
   
   DriveThruMenuLambda ‚Äì Populates menu data and generates AI images 
    
 
 
 CloudFormation Custom Resource: 
   
   DriveThruMenuPopulation ‚Äì Triggers DriveThruMenuLambda 
    
 
Once both CloudFormation templates are successfully deployed, you‚Äôll have a fully functional restaurant drive-thru ordering system with AI-generated menu images, complete authentication, and ready-to-use API endpoints for your Amplify frontend deployment. 
Deploy the Amplify application 
You need to manually deploy the Amplify application using the frontend code found on GitHub. Complete the following steps: 
 
 Download the frontend code NovaSonic-FrontEnd.zip from&nbsp;GitHub. 
 Use the .zip file to manually&nbsp;deploy the application in Amplify. 
 Return to the Amplify page and use the domain it automatically generated to access the application. 
 
User authentication 
The solution uses Amazon Cognito user pools and identity pools to implement secure, role-based access control for restaurant‚Äôs digital menu board. User pools handle authentication and group management through the AppUserGroup, and identity pools provide temporary AWS credentials mapped to specific IAM roles including AuthenticatedRole. The system makes sure that only verified digital menu board users can access the application and interact with the menu APIs, cart management, order processing, and loyalty services, while also providing secure access to Amazon Bedrock. This combines robust security with an intuitive ordering experience for both customers and restaurant operations. 
Serverless data management 
The solution implements a serverless API architecture using Amazon API Gateway to create a single REST API (app-api) that facilitates communication between the frontend interface and backend services. The API includes five resource endpoints (/menu, /loyalty, /cart, /chat,/order) with Cognito-based authentication and direct DynamoDB integration for data operations. The backend utilizes five DynamoDB tables: MenuTable for menu items and pricing, LoyaltyTable for customer profiles and loyalty points, CartTable for active shopping sessions, ChatTable for capturing chat history and OrderTable for order tracking and history. This architecture provides fast, consistent performance at scale with Global Secondary Indexes enabling efficient queries by customer ID and order status for optimal drive-thru operations. 
Menu and image generation and distribution 
The solution uses Amazon S3 and CloudFront for secure, global content delivery of menu item images. The CloudFormation template creates a MenuImagesBucket with restricted access through a CloudFront Origin Access Identity, making sure images are served securely using the CloudFront distribution for fast loading times worldwide. AWS Lambda powers the AI-driven content generation through the DriveThruMenuLambda function, which automatically populates sample menu data and generates high-quality menu item images using Amazon Nova Canvas. This serverless function executes during stack deployment to create professional food photography for the menu items, from classic burgers to specialty wings, facilitating consistent visual presentation across the entire menu. The Lambda function integrates with DynamoDB to store generated image URLs and uses S3 for persistent storage, creating a complete automated workflow that scales based on demand while optimizing costs through pay-per-use pricing. 
Voice AI processing 
The solution uses Amazon Nova Sonic as the core voice AI engine. The digital menu board establishes direct integration with Amazon Nova Sonic through secure WebSocket connections, for immediate processing of customer speech input and conversion to structured ordering data. The CloudFormation template configures IAM permissions for the AuthenticatedRole to access the amazon.nova-sonic-v1:0 foundation model, allowing authenticated users to interact with the voice AI service. Nova Sonic handles complex natural language understanding and intent recognition, processing customer requests like menu inquiries, order modifications, and item customizations while maintaining conversation context throughout the ordering process. This direct integration minimizes latency concerns and provides customers with a natural, conversational ordering experience that rivals human interaction while maintaining reliable service across drive-thru locations. 
Hosting the digital menu board 
AWS Amplify hosts and delivers the digital menu board interface as a scalable frontend application. The interface displays AI-generated menu images through CloudFront, with real-time pricing from DynamoDB, optimized for drive-thru environments. The React-based application automatically scales during peak hours, using the global content delivery network available in CloudFront for fast loading times. It integrates with Amazon Cognito for authentication, establishes WebSocket connections to Amazon Nova Sonic for voice processing, and uses API Gateway endpoints for menu and order management. This serverless solution maintains high availability while providing real-time visual updates as customers interact through voice commands. 
WebSocket connection flow 
The following sequence diagram illustrates the WebSocket connection setup enabling direct browser-to-Nova Sonic communication. This architecture leverages the AWS SDK update (client-bedrock-runtime v3.842.0), which introduces WebSocketHandler support in browsers, avoiding the need for a server. 
 
This advancement allows frontend applications to establish direct WebSocket connections to Nova Sonic, reducing latency and complexity while enabling real-time conversational AI in the browser. The initialization process includes credential validation, Bedrock client establishment, AI assistant configuration, and audio input setup (16kHz PCM). This direct client-to-service communication represents a shift from traditional architectures, offering more efficient and scalable conversational AI applications. 
Voice interaction and dynamic menu 
The following sequence diagram illustrates the flow of a customer‚Äôs burger query, demonstrating how natural language requests are processed to deliver synchronized audio responses and visual updates. 
 
This diagram shows how a query ("Can you show me what burgers you have?") is handled. Nova Sonic calls getMenuItems ({category: "burgers"}) to retrieve menu data, while Frontend App components fetch and structure burger items and prices. Nova Sonic generates a contextual response and triggers showCategory ({category: "burgers"}) to highlight the burger section in the UI. This process facilitates real-time synchronization between audio responses and visual menu updates, creating a seamless customer experience throughout the conversation. 
Drive-thru solution walkthrough 
After deploying your application in AWS Amplify, open the generated URL in your browser. You‚Äôll see two setup options: Choose Sample and Manual Setup. Select Choose Sample then pick AI Drive-Thru Experience from the sample list, and then select Load Sample. This will automatically import the system prompt, tools, and tool configurations for the drive-thru solution. We will configure these settings in the following steps. 
 
After selecting Load Sample, you‚Äôll be prompted to configure the connection settings. You‚Äôll need to use the Amazon Cognito and API Gateway information from your CloudFormation stack outputs. These values are required because they connect your digital menu board to backend services. 
Enter the configuration values you copied from the CloudFormation outputs (nova-sonic-infrastructure-drivethru.yaml). These are organized into two sections, as demonstrated in the following videos. After you enter the configuration details in each section, select Save button at the top of the screen. 
Amazon Cognito configuration: 
 
 UserPoolId 
 UserPoolClientId 
 IdentityPoolId 
 
 
Agent configuration: 
 
 Auto-Initiate Conversation ‚Äì Nova Sonic is initially set to wait for you to start the conversation. However, you can enable automatic conversation initiation by checking the ‚ÄòEnable auto-initiate‚Äô box. There is a pre-recorded ‚ÄòHello‚Äô that you can use that‚Äôs stored locally. 
 
 
 
 Tools global parameters: 
   
   menuAPIURL 
   cartAPIURL 
   orderAPIUR 
   loyaltyAPIURL 
   chatAPIURL 
    
 
 
After completing the configuration, click the Save and Exit button located at the top of the page. This action will redirect you to a sign-in screen. To access the system, use the username appuser and the password automatically generated and emailed to you to the email that was provided during the CloudFormation deployment. 
After entering the temporary password, you‚Äôll be asked to verify your account through a temporary code sent to your email. 
Upon your initial login attempt, you‚Äôll be required to create a new password to replace the temporary one, as demonstrated in the following video. 
 
Begin your drive-thru experience by clicking the microphone icon. The AI assistant welcomes you and guides you through placing your order while dynamically updating the digital menu board to highlight relevant items. The system intelligently suggests complementary items and adapts its communication style to enhance your ordering experience. 
 
 
  
   
  
  
 
Clean up 
If you decide to discontinue using the solution, you can follow these steps to remove it, its associated resources deployed using AWS CloudFormation, and the Amplify deployment: 
 
 Delete the CloudFormation stack: 
   
   On the AWS CloudFormation console, choose&nbsp;Stacks&nbsp;in the navigation pane. 
   Locate the stack you created during the deployment process of nova-sonic-application-drivethru.yaml (you assigned a name to it). 
   Select the stack and choose&nbsp;Delete. 
   Repeat this for nova-sonic-infrastructure-drivethru.yaml 
    
 
 
 Delete the Amplify application and its resources. For instructions, refer to&nbsp;Clean Up Resources. 
 
Conclusion 
The voice AI-powered drive-thru ordering system using Amazon Nova Sonic provides restaurants with a practical solution to common operational challenges including staffing constraints, order accuracy issues, and peak-hour bottlenecks. The serverless architecture built on AWS services‚ÄîAmazon Cognito for authentication, API Gateway for data communication, DynamoDB for storage, and AWS Amplify for hosting, creates a scalable system that handles varying demand while maintaining consistent performance. The system supports essential restaurant operations including menu management, cart functionality, loyalty programs, and order processing through direct API Gateway and DynamoDB integration. For restaurants looking to modernize their drive-thru operations, this solution offers measurable benefits including reduced wait times, improved order accuracy, and operational efficiency gains. The pay-per-use pricing model and automated scaling help control costs while supporting business growth. As customer expectations shift toward more efficient service experiences, implementing voice AI technology provides restaurants with a competitive advantage and positions them well for future technological developments in the food service industry. 
Additional resources 
To learn more about Amazon Nova Sonic and additional solutions, refer to the following resources: 
 
 Introducing Amazon Nova Sonic: Human-like voice conversations for generative AI applications 
 Frontend application source code used in this blog is available on GitHub 
 Voice AI-Powered Hotel In-Room Service with Amazon Nova Sonic 
 
 
 
About the Authors 
 
  
 Salman Ahmed 
 Salman is a Senior Technical Account Manager in AWS Enterprise Support. He specializes in guiding customers through the design, implementation, and support of AWS solutions. Combining his networking expertise with a drive to explore new technologies, he helps organizations successfully navigate their cloud journey. Outside of work, he enjoys photography, traveling, and watching his favorite sports teams. 
 
 
  
 Sergio Barraza 
 Sergio is a Senior Technical Account Manager at AWS, helping customers on designing and optimizing cloud solutions. With more than 25 years in software development, he guides customers through AWS services adoption. Outside of work, Sergio is a multi-instrument musician playing guitar, piano, and drums, and he also practices Wing Chun Kung Fu. 
 
 
  
 Ravi Kumar 
 Ravi is a Senior Technical Account Manager in AWS Enterprise Support who helps customers in the travel and hospitality industry to streamline their cloud operations on AWS. He is a results-driven IT professional with over 20 years of experience. Ravi is passionate about generative AI and actively explores its applications in cloud computing. In his free time, Ravi enjoys creative activities like painting. He also likes playing cricket and traveling to new places. 
 
 
  
 Ankush Goyal 
 Ankush is a Senior Technical Account Manager at AWS Enterprise Support, specializing in helping customers in the travel and hospitality industries optimize their cloud infrastructure. With over 20 years of IT experience, he focuses on leveraging AWS networking services to drive operational efficiency and cloud adoption. Ankush is passionate about delivering impactful solutions and enabling clients to streamline their cloud operations. 
 
 
  
 Leland Johnson 
 Leland is a Sr. Solutions Architect for AWS focusing on travel and hospitality. As a Solutions Architect, he plays a crucial role in guiding customers through their cloud journey by designing scalable and secure cloud solutions. Outside of work, he enjoys playing music and flying light aircraft.
‚Ä¢ Optimizing document AI and structured outputs by fine-tuning Amazon Nova Models and on-demand inference
  Multimodal fine-tuning represents a powerful approach for customizing vision large language models (LLMs) to excel at specific tasks that involve both visual and textual information. Although base multimodal models offer impressive general capabilities, they often fall short when faced with specialized visual tasks, domain-specific content, or output formatting requirements. Fine-tuning addresses these limitations by adapting models to your specific data and use cases, dramatically improving performance on tasks that matter to your business. 
A common use case is document processing, which includes extracting structured information from complex layouts including invoices, purchase orders, forms, tables, or technical diagrams.&nbsp;Although off-shelf LLMs often struggle with specialized documents like tax forms, invoices, and loan applications, fine-tuned models can learn from high data variations and can deliver significantly higher accuracy while reducing processing costs. 
This post provides a comprehensive hands-on guide to fine-tune Amazon Nova Lite for document processing tasks, with a focus on tax form data extraction. Using our open-source GitHub repository code sample, we demonstrate the complete workflow from data preparation to model deployment.&nbsp;Since Amazon Bedrock provides on-demand inference with pay-per-token pricing for Amazon Nova, we can benefit from the accuracy improvement from model customization and maintain the pay-as-you-go cost structure. 
The document processing challenge 
Given a single or multi-page document, the goal is to extract or derive specific structured information from the document so that it can be used for downstream systems or additional insights. The following diagram shows how a vision LLM can be used to derive the structured information based on a combination of text and vision capabilities. 
 
The key challenges for enterprises in workflow automation when processing&nbsp;documents, like invoices or W2 tax forms, are the following: 
 
 Complex layouts: Specialized forms contain multiple sections with specific fields arranged in a structured format. 
 Variability of document types: Many diverse document types exist (invoices, contracts, forms). 
 Variability within a single document type: Each vendor can send a different invoice format and style or type. 
 Data quality variations: Scanned documents vary in quality, orientation, and completeness. 
 Language barriers: Documents can be in multiple languages. 
 Critical accuracy requirements: Tax-related data extraction demands extremely high accuracy. 
 Structured output needs: Extracted data must be formatted consistently for downstream processing. 
 Scalability and integration: Grow with business needs and integrate with existing systems; for example, Enterprise Resource Planning&nbsp;(ERP) systems. 
 
Approaches for intelligent document processing that use LLMs or vision LLMs fall into three main categories: 
 
 Zero-shot prompting:&nbsp;An LLM or vision LLM is used to derive the structured information based on the input document, instructions, and the target schema. 
 Few-shot prompting: A technique used with LLMs or vision LLMs where a few of other additional examples (document + target output) are provided within the prompt to guide the model in completing a specific task. Unlike zero-shot prompting, which relies solely on natural language instructions, few-shot prompting can improve accuracy and consistency by demonstrating the desired input-output behavior through a set of examples. 
 Fine-tuning: Customize or fine-tune the weights of a given LLM or vision LLM by providing larger amounts of annotated documents (input/output pairs), to teach the model exactly how to extract or interpret relevant information. 
 
For the first two approaches, refer to the amazon-nova-samples repository, which contains sample code on how to use the Amazon Bedrock Converse API for structured output by using tool calling. 
Off-shelf LLMs excel at general document understanding, but they might not optimally handle domain-specific challenges. A fine-tuned Nova model can enhance performance by: 
 
 Learning document-specific layouts and field relationships 
 Adapting to common quality variations in your document dataset 
 Providing consistent, structured outputs 
 Maintaining high accuracy across different document variations. For example, invoice documents can have hundreds of different vendors, each with different formats, layouts or even different languages. 
 
Creating the annotated dataset and selecting the customization technique 
While there are various methods for customization&nbsp;of Amazon Nova models available, the most relevant for document processing are the following: 
 
 Fine-tune for specific tasks:&nbsp;Adapt Nova models for specific tasks using supervised fine-tuning (SFT). Choose between Parameter-Efficient Fine-Tuning (PEFT) for light-weight adaptation with limited data, or full fine-tuning when you have extensive training datasets to update all parameters of the model. 
 Distill to create smaller, faster models:&nbsp;Use knowledge distillation to transfer knowledge from a larger, more intelligent model, like Nova Premier (teacher) to a smaller, faster, more cost-efficient model (student), ideal for when you don‚Äôt have enough annotated training datasets and the teacher model provides the accuracy that meets your requirement. 
 
To be able to learn from previous examples, you need to either have an annotated dataset from which we can learn or a model that is good enough for your task so that you can use it as a teacher model. 
 
 Automated dataset annotation with historic data from Enterprise Resource Planning&nbsp;(ERP) systems, such as SAP: Many customers have already historic documents that have been manually processed and consumed by downstream systems, like ERP or customer relationship management (CRM) systems.&nbsp;Explore existing downstream systems like SAP and the data they contain. This data can often be mapped back to the original source document it has been derived from and helps you to bootstrap an annotated dataset very quickly. 
 Manual dataset annotation: Identify the most relevant documents and formats, and annotate them using human annotators, so that you have document/JSON pairs where the JSON contains the target information that you want to extract or derive from your source documents. 
 Annotate with the teacher model: Explore if a larger model like Nova Premier can provide accurate enough results using prompt engineering. If that is the case, you can also use distillation. 
 
For the first and second options, we recommend supervised model fine-tuning. For the third, model distillation is the right approach. 
Amazon Bedrock currently provides both&nbsp;fine-tuning and distillation techniques, so that anyone with a basic data science skillset can very easily submit jobs. They run on compute completely managed by Amazon, so you don‚Äôt have worry about instance sizes or capacity limits. 
Nova customization is also available with Amazon SageMaker&nbsp;with more options and controls. For example,&nbsp;if you have&nbsp;sufficient high-quality labeled data and you want deeper customization for your use case, full rank fine-tuning might produce higher accuracy. Full rank fine tuning is supported with SageMaker training jobs and SageMaker HyperPod. 
Data preparation best practices 
The quality and structure of your training data fundamentally determine the success of fine-tuning. Here are key steps and considerations for preparing effective multimodal datasets and configuring your fine-tuning job: 
Dataset analysis and base model evaluation 
Our demonstration uses a synthetic dataset of W2 tax forms: the Fake W-2 (US Tax Form) Dataset. This public dataset comprises simulated US tax returns (W-2 statements for years 2016-19), including noisy images that mimic low-quality scanned W2 tax forms. 
Before fine-tuning, it‚Äôs crucial to: 
 
 Analyze dataset characteristics (image quality, field completeness, class distribution), define use-case-specific evaluation metrics, and establish baseline model performance. 
 Compare each predicted field value against the ground truth, calculating precision, recall, and F1 scores for individual fields and overall performance. 
 
Prompt optimization 
Crafting an effective prompt is essential for aligning the model with task requirements. Our system comprises two key components: 
 
 System prompt: Defines the task, provides detailed instructions for each field to be extracted, and specifies the output format. 
 User prompt: Follows Nova vision understanding best practices, utilizing the {media_file}-then-{text} structure as outlined in the Amazon Nova model user guide. 
 
Iterate on your prompts using the base model to optimize performance before fine-tuning. 
Dataset preparation 
Prepare your dataset in JSONL format and split it into training, validation, and test sets: 
 
 Training set: 70-80% of data 
 Validation set: 10-20% of data 
 Test set: 10-20% of data 
 
Fine-tuning job configuration and monitoring 
Once the dataset is prepared and uploaded to an Amazon Simple Storage Service (Amazon S3) bucket, we can configure and submit the fine-tuning job on Bedrock.&nbsp;When configuring your fine-tuning job on Amazon Bedrock, key parameters include: 
 
  
   
   Parameter 
   Definition 
   Purpose 
   
  
  
   
   Epochs 
   Number of complete passes through the training dataset 
   Determines how many times the model sees the entire dataset during training 
   
   
   Learning rate 
   Step size for gradient descent optimization 
   Controls how much model weights are adjusted in response to estimated error 
   
   
   Learning rate warmup steps 
   Number of steps to gradually increase the learning rate 
   Prevents instability by slowly ramping up the learning rate from a small value to the target rate 
   
  
 
Amazon Bedrock customization provides validation loss metrics throughout the training process. Monitor these metrics to: 
 
 Assess model convergence 
 Detect potential overfitting 
 Gain early insights into model performance on unseen data 
 
The following graph shows an example metric analysis: 
 
When analyzing the training and validation loss curves, the relative behavior between these metrics provides crucial insights into the model‚Äôs learning dynamics. Optimal learning patterns can be observed as: 
 
 Both training and validation losses decrease steadily over time 
 The curves maintain relatively parallel trajectories 
 The gap between training and validation loss remains stable 
 Final loss values converge to similar ranges 
 
Model inference options for customized models 
Once your custom model has been created in Bedrock, you have two main ways to make inferences to that model: use on-demand custom model inference (ODI) deployments, or use Provisioned Throughput endpoints. Let‚Äôs talk about why and when to choose one over the other. 
On-demand custom model deployments provide a flexible and cost-effective way to leverage your custom Bedrock models. With on-demand deployments, you only pay for the compute resources you use, based on the number of tokens processed during inference. This makes on-demand a great choice for workloads with variable or unpredictable usage patterns, where you want to avoid over-provisioning resources. The on-demand approach also offers automatic scaling, so you don‚Äôt have to worry about managing infrastructure capacity. Bedrock will automatically provision the necessary compute power to handle your requests in near real time. This self-service, serverless experience can simplify your operations and deployment workflows. 
Alternatively, Provisioned Throughput endpoints are recommended for workloads with steady traffic patterns and consistent high-volume requirements, offering predictable performance and cost benefits over on-demand scaling. 
This example uses the ODI option to leverage per-token based pricing; the following code snippet is how you can create an ODI endpoint for your custom model: 
 
 # Function to create on-demand inferencing deployment for custom model
def create_model_deployment(custom_model_arn):
&nbsp;&nbsp; &nbsp;"""
&nbsp;&nbsp; &nbsp;Create an on-demand inferencing deployment for the custom model
&nbsp;&nbsp; &nbsp;
&nbsp;&nbsp; &nbsp;Parameters:
&nbsp;&nbsp; &nbsp;-----------
&nbsp;&nbsp; &nbsp;custom_model_arn : str
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;ARN of the custom model to deploy
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;
&nbsp;&nbsp; &nbsp;Returns:
&nbsp;&nbsp; &nbsp;--------
&nbsp;&nbsp; &nbsp;deployment_arn : str
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;ARN of the created deployment
&nbsp;&nbsp; &nbsp;"""
&nbsp;&nbsp; &nbsp;try:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;print(f"Creating on-demand inferencing deployment for model: {custom_model_arn}")
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;# Generate a unique name for the deployment
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;deployment_name = f"nova-ocr-deployment-{time.strftime('%Y%m%d-%H%M%S')}"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;# Create the deployment
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;response = bedrock.create_custom_model_deployment(
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;modelArn=custom_model_arn,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;modelDeploymentName=deployment_name,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;description=f"on-demand inferencing deployment for model: {custom_model_arn}",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;)
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;# Get the deployment ARN
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;deployment_arn = response.get('customModelDeploymentArn')
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;print(f"Deployment request submitted. Deployment ARN: {deployment_arn}")
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;return deployment_arn
&nbsp;&nbsp; &nbsp;
&nbsp;&nbsp; &nbsp;except Exception as e:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;print(f"Error creating deployment: {e}")
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;return None 
 
Evaluation:&nbsp;Accuracy improvement with fine-tuning 
Our evaluation of the base model and the fine-tuned Nova model shows significant improvements across all field categories. Let‚Äôs break down the performance gains: 
 
  
   
   Field category 
   Metric 
   Base model 
   Fine-tuned model 
   Improvement 
   
   
   Employee information 
   Accuracy 
   58% 
   82.33% 
   24.33% 
   
   
   Precision 
   57.05% 
   82.33% 
   25.28% 
   
   
   Recall 
   100% 
   100% 
   0% 
   
   
   F1 score 
   72.65% 
   90.31% 
   17.66% 
   
   
   Employer information 
   Accuracy 
   58.67% 
   92.67% 
   34% 
   
   
   Precision 
   53.66% 
   92.67% 
   39.01% 
   
   
   Recall 
   100% 
   100% 
   0% 
   
   
   F1 score 
   69.84% 
   96.19% 
   26.35% 
   
   
   Earnings 
   Accuracy 
   62.71% 
   85.57% 
   22.86% 
   
   
   Precision 
   60.97% 
   85.57% 
   24.60% 
   
   
   Recall 
   99.55% 
   100% 
   0.45% 
   
   
   F1 score 
   75.62% 
   92.22% 
   16.60% 
   
   
   Benefits 
   Accuracy 
   45.50% 
   60% 
   14.50% 
   
   
   Precision 
   45.50% 
   60% 
   14.50% 
   
   
   Recall 
   93.81% 
   100% 
   6.19% 
   
   
   F1 score 
   61.28% 
   75% 
   13.72% 
   
   
   Multi-state employment 
   Accuracy 
   58.29% 
   94.19% 
   35.90% 
   
   
   Precision 
   52.14% 
   91.83% 
   39.69% 
   
   
   Recall 
   99.42% 
   100% 
   0.58% 
   
   
   F1 score 
   68.41% 
   95.74% 
   27.33% 
   
  
 
The following graphic shows a bar chart comparing the F1 scores of the base model and fine-tuned model for each field category, with the improvement percentage shown in the previous table: 
 
Key observations: 
 
 Substantial improvements across all categories, with the most significant gains in employer information and multi-state employment 
 Consistent 100% recall maintained or achieved in the fine-tuned model, indicating comprehensive field extraction 
 Notable precision improvements, particularly in categories that were challenging for the base model 
 
Clean up 
To avoid incurring unnecessary costs when you‚Äôre no longer using your custom model, it‚Äôs important to properly clean up the resources. Follow these steps to remove both the deployment and the custom model: 
 
 Delete the custom model deployment 
 Delete the custom model 
 
Cost analysis 
In our example, we chose to use Bedrock fine-tuning job which is PEFT and ODI is available.&nbsp;PEFT fine tuning Nova Lite paired with on-demand inference capabilities offers a cost-effective and scalable solution for enhanced document processing. The cost structure is straightforward and transparent: 
One-time cost: 
 
 Model training: $0.002 per 1,000 tokens √ó number of epochs 
 
Ongoing costs: 
 
 Storage: $1.95 per month per custom model 
 On-demand Inference: Same per-token pricing as the base model 
   
   Example 1 page from above dataset: 1895 tokens/1000 * $0.00006 + 411 tokens/1000 * $0.00024 = $0.00021 
    
 
On-demand inference allows you to run your custom Nova models without maintaining provisioned endpoints, enabling pay-as-you-go pricing based on actual token usage. This approach eliminates the need for capacity planning while ensuring cost-efficient scaling. 
Conclusion 
In this post, we‚Äôve demonstrated how fine-tuning Amazon Nova Lite can transform document processing accuracy while maintaining cost efficiency. Our evaluation shows significant performance gains, with up to 39% improvement in precision for critical fields and perfect recall across key document categories. While our implementation did not require constrained decoding, tool calling with Nova can provide additional reliability for more complex structured outputs, especially when working with intricate JSON schemas. Please refer to the resource on structured output with tool calling for further information. 
The flexible deployment options, including on-demand inference with pay-per-use pricing, eliminate infrastructure overhead while maintaining the same inference costs as the base model. With the dataset we used for this example, runtime inference per page cost was $0.00021, making it a cost-effective solution.&nbsp;Through practical examples and step-by-step guides, we‚Äôve shown how to prepare training data, fine-tune models, and evaluate performance with clear metrics. 
To get started with your own implementation, visit our GitHub repository for complete code samples and detailed documentation. 
 
About the authors 
Sharon Li is an AI/ML Specialist Solutions Architect at Amazon Web Services (AWS) based in Boston, Massachusetts. With a passion for leveraging cutting-edge technology, Sharon is at the forefront of developing and deploying innovative generative AI solutions on the AWS cloud platform. 
Arlind Nocaj is a GTM Specialist Solutions Architect for AI/ML and Generative AI for europe central based in AWS Zurich Office, who guides enterprise customers through their digital transformation journeys. With a PhD in network analytics and visualization (Graph Drawing) and over a decade of experience as a research scientist and software engineer, he brings a unique blend of academic rigor and practical expertise to his role. His primary focus lies in using the full potential of data, algorithms, and cloud technologies to drive innovation and efficiency. His areas of expertise include Machine Learning, Generative AI and in particular Agentic systems with Multi-modal LLMs for document processing and structured insights. 
Pat Reilly&nbsp;is a Sr. Specialist Solutions Architect on the Amazon Bedrock Go-to-Market team. Pat has spent the last 15 years in analytics and machine learning as a consultant. When he‚Äôs not building on AWS, you can find him fumbling around with wood projects. 
Malte Reimann is a Solutions Architect based in Zurich, working with customers across Switzerland and Austria on their cloud initiatives. His focus lies in practical machine learning applications‚Äîfrom prompt optimization to fine-tuning vision language models for document processing. The most recent example, working in a small team to provide deployment options for Apertus on AWS. An active member of the ML community, Malte balances his technical work with a disciplined approach to fitness, preferring early morning gym sessions when it‚Äôs empty. During summer weekends, he explores the Swiss Alps on foot and enjoying time in nature. His approach to both technology and life is straightforward: consistent improvement through deliberate practice, whether that‚Äôs optimizing a customer‚Äôs cloud deployment or preparing for the next hike in the clouds.
‚Ä¢ Transforming enterprise operations: Four high-impact use cases with Amazon Nova
  Since the launch of Amazon Nova at AWS re:Invent 2024, we have seen adoption trends across industries, with notable gains in operational efficiency, compliance, and customer satisfaction. With its capabilities in secure, multimodal AI and domain customization, Nova is enhancing workflows and enabling cost efficiencies across core use cases. 
In this post, we share four high-impact, widely adopted use cases built with Nova in Amazon Bedrock, supported by real-world customers deployments, offerings available from AWS partners, and experiences. These examples are ideal for organizations researching their own AI adoption strategies and use cases across industries. 
Customer service 
Traditional chatbots often frustrate users with scripted, inflexible responses that fail to understand context or intent. For enterprises, these are missed opportunities to resolve issues quickly, lower support costs, and drive customer loyalty. AI-powered applications can understand natural language, adapt to individual customer needs, and integrate with backend systems in real time. Organizations are transforming support from a cost center into a strategic driver of satisfaction and retention. These are often high-volume and interactive scenarios, so the balance of cost, speed, and intelligence is critical. 
Customer service applications built with Nova in Amazon Bedrock can seamlessly integrate with business data stored with AWS, and offer the security, privacy, and reliability for production use in enterprise environments. 
 
 Infosys, a leading global IT services and consulting organization, developed Infosys Event AI for real-time transcription, multilingual translation, and intelligent summarization of live event content. Infosys Event AI is built with Amazon Nova Pro in Amazon Bedrock. During a recent event in Bangalore, the AI assistant handled around 230 users per minute and was queried an average of 57 times per minute, generating more than 9,000 session summaries. This solution enhanced knowledge retention, engagement, and inclusivity by making event insights instantly accessible in multiple languages and formats for hearing-impaired and remote participants. By transforming event content into a persistent, searchable multilingual knowledge asset, Infosys Event AI accelerates learning and collaboration. 
 Fortinet, an AWS Partner and cybersecurity company, uses Amazon Nova Micro to power its AI support assistant, delivering significant performance improvements at a fraction of the cost. By switching to Nova Micro in Amazon Bedrock, Fortinet achieved an 85 times reduction in inference costs, dramatically lowering TCO while maintaining rapid response times. The assistant now helps users quickly navigate complex documentation across more than 60 products, improving support efficiency and elevating customer satisfaction. 
 Amazon Customer Service uses Nova with its AI-driven issue resolution system. The system is a two-step approach combining intent detection and issue resolution. Amazon Customer Service customized Nova Micro, resulting in 76.9% accuracy for in-domain issues and 69.2% in generalization testing, surpassing current baselines by 5.4% and 7.3%, respectively. Additionally, Nova Lite is used for tool selection, achieving 86.1% accuracy and 4.8% improvement over existing systems. 
 AWS Summit New York City 2025 was attended by 18,000 participants, featuring the AI assistant Diana for customer service developed with Nova Sonic. By dialing a phone number, the Sonic-powered voice assistant answered hundreds of queries about the event, including session details, location, and FAQs. 
 
Search 
Large enterprises face slow, siloed, and inefficient search across vast stores of structured and unstructured data, costing time, productivity, and customer responsiveness. By adopting AI-powered, multimodal search that understands natural language and enforces secure access, organizations can deliver instant, relevant answers from documents, images, and technical files. This accelerates decision-making, shortens deal cycles, improves customer satisfaction, and reduces the cost of knowledge discovery at scale. Search applications increasingly rely on a mix of information across modalities, including text, documents, images, and video. 
Nova is among the fastest and most cost-effective multimodal models, offering vision fine-tuning capabilities. Nova also integrates with broader Amazon models including Amazon Titan Multimodal Embeddings and data services including Amazon OpenSearch Service for more robust search capabilities and performance. 
 
 Siemens faced growing performance bottlenecks as its massive datasets strained traditional search systems, slowing retrieval speeds and impacting productivity across its global operations. To address this, Siemens integrated Amazon Nova, achieving a threefold boost in search performance that dramatically accelerated data retrieval and improved workflow efficiency. Amazon Nova delivers high-speed, scalable search capabilities, and Siemens‚Äôs implementation facilitates seamless integration with existing systems, maintaining business continuity with minimal disruption. This enhanced user experience and positioned Siemens to handle future data growth with ease, supported by continuous performance monitoring and tight infrastructure alignment. 
 
Video understanding and analysis 
Organizations are adopting video understanding applications to drive business value across multiple fronts, including customer behavior analysis, traffic patterns, and manufacturing quality control. Security and safety benefits are realized through real-time threat detection and workplace safety monitoring, and customer experience is enhanced through personalized content recommendations and improved content searchability. Organizations gain competitive advantage through data-driven decision-making and innovation in service delivery, while reducing costs by minimizing manual review processes and decreasing security incidents. This comprehensive approach to video analysis helps companies extract insights from their video data, ultimately leading to improved operations, better decision-making, and enhanced customer experiences. As developers build, iterate, and evolve these applications, there is a growing demand to natively understand video as opposed to dealing with the overhead of frames, time stamps, and synchronization. 
Amazon Nova models can analyze, classify, and summarize information in the video based on provided instructions. Applications built with Nova understanding models in Amazon Bedrock offer comprehensive analysis of multiple video formats through flexible input methods, with the ability to analyze, classify, and summarize video content while handling files up to 1 GB through Amazon Simple Storage Service (Amazon S3) integration. 
 
 Bitcentral partnered with Caylent to transform how archived content is discovered, accessed, and reused. Using Nova Pro in Amazon Bedrock, Caylent deployed a solution that aligned with the needs of journalists, producers, and broadcasters across more than 1,600 client sites. By embedding semantic video search, contextual metadata generation, and AI-powered content analysis into its workflows, Bitcentral redefined how archived footage is indexed, discovered, and reused. Journalists and producers can now surface high-value content in real time and unlock new revenue streams. 
 Loka, an AWS Premier Partner, built a video surveillance offering to automatically identify and classify millions of visual events in video footage. This system effectively distinguishes between routine events and critical incidents, helping filter out non-essential activities and alerts. The solution proved highly successful, reducing irrelevant alerts by 55% while maintaining a threat detection rate above 97%. By implementing this automated filtering system, Loka doubled video monitoring efficiency for their client. The tool, built on Amazon Bedrock using Amazon Nova Pro, significantly reduced the workload for human operators while improving overall threat detection capabilities. 
 Accenture Spotlight can analyze long-form videos and automatically generate personalized short-form clips and highlights, which are particularly useful for sports content like soccer, Formula 1, and rugby. Spotlight is capable of matching content to specific audience demographics and can process real-time CCTV footage in retail settings to create personalized offers. The system is built with Amazon Nova in Amazon Bedrock and operates through three specialized super agents working under a central orchestrator. Spotlight can process videos in minutes rather than the traditional hours or days, while achieving cost savings that are 10 times better than conventional methods. The solution is versatile enough to be used across different industries, from media and entertainment to retail, while maintaining high quality standards and brand alignment through its human-in-the-loop quality assurance option. 
 
Creative content generation 
Organizations are seeking ways to revolutionize creative content generation including stock imagery, marketing campaign assets, and product visualizations. It is often slowed down by fragmented workflows, high production costs, and the need to continuously balance scale with personalization. Marketing teams struggle to keep up with the demand for fresh, high-quality assets across multiple channels, while creative fatigue and long lead times limit their agility. 
Amazon Nova addresses these challenges with Nova Canvas and Nova Reel: high-quality creative models that transform text and image inputs into professional-grade images and videos. Nova creative models are designed to deliver customizable visual content with control features, making creative content generation accessible and efficient for media, entertainment, retail, marketing, and advertising industries. 
 
 Dentsu is reimagining how ads come to life with Amazon Nova creative generation models. What used to take weeks of brainstorming, filming, and editing now happens in days. Their creative teams can sketch out an idea in plain language and watch it turn into polished videos and custom images, ready for markets across the globe in over 200 languages. Built-in safeguards like moderation, watermarking, and IP indemnity mean every piece stays brand safe. For Co-op, Dentsu went a step further‚Äîpairing Nova with Amazon Ads to design custom audience profiles that delivered a +4-point lift in brand preference among 25‚Äì34-year-olds and a +5-point lift in favorability among affluent shoppers. 
 Quantiphi, an AWS Premier Global Consulting Partner, developed Qreator, a generative AI-powered marketing content creation service built on AWS. Their service helps marketers create content through natural language prompts while maintaining brand consistency and cross-channel adaptability. With Qreator, business can achieve an approximate 30% reduction in content creation time and get to market approximately 40% faster, automating what was a manual process, and improving consistency across formats and channels. 
 The Fragrance Lab is a unique AWS activation that was showcased at the Cannes Lions International Festival of Creativity. It demonstrates how to build personalized products and campaign assets using Amazon Nova foundation models in Amazon Bedrock. Although our activation at Cannes Lions focused on personalized fragrance development and ad campaign creation, the underlying architecture and methodology can be adapted across diverse categories, such as fashion, food, and beverage, opening endless possibilities for customized customer experiences. The Fragrance Lab activation won two International Business Awards: Gold for Exhibition Event Experience and Silver for Experiential Event. 
 
Conclusion 
The four use cases presented in this post demonstrate the utility of Amazon Nova across industries and applications. From Infosys‚Äôs Event AI improving accessibility and engagement, to Loka‚Äôs intelligent video surveillance, and Dentsu‚Äôs creative content generation, each implementation showcases significant, measurable improvements in efficiency, cost reduction, and customer satisfaction. 
Organizations using Amazon Nova are achieving tangible business outcomes through evidence-based adoption strategies. By partnering with Amazon and AWS Partners, organizations are accelerating their AI transformation while maintaining strong foundations in security, compliance, and privacy-by-design principles. 
To get started building with Nova, visit the Amazon Nova user guide or visit the AWS console. 
 
About the Authors 
Abhinav Bhargava is a Sr Product Marketing Manager at AWS on the Amazon Nova team, where he focuses on scaling generative AI adoption through customer-centric solutions. With a background in design and sustainability, he brings a unique perspective to connecting technology and creativity to drive enterprise innovation. Based in Seattle, Abhinav enjoys playing volleyball, traveling, and learning about new cultures. 
Raechel Frick is a Sr Product Marketing Manager at AWS. With over 20 years of experience in the tech industry, she brings a customer-first approach and growth mindset to building integrated marketing programs. Based in the greater Seattle area, Raechel balances her professional life with being a soccer mom and after-school carpool manager, demonstrating her ability to excel both in the corporate world and family life.
‚Ä¢ Building smarter AI agents: AgentCore long-term memory deep dive
  Building AI agents that remember user interactions requires more than just storing raw conversations. While Amazon Bedrock AgentCore short-term memory captures immediate context, the real challenge lies in transforming these interactions into persistent, actionable knowledge that spans across sessions. This is the information that transforms fleeting interactions into meaningful, continuous relationships between users and AI agents. In this post, we‚Äôre pulling back the curtain on how the Amazon Bedrock AgentCore Memory long-term memory system works. 
If you‚Äôre new to AgentCore Memory, we recommend reading our introductory blog post first:&nbsp;Amazon Bedrock AgentCore Memory: Building context-aware agents. In brief, AgentCore Memory is a fully managed service that enables developers to build context-aware AI agents by providing both short-term working memory and long-term intelligent memory capabilities. 
The challenge of persistent memory 
When humans interact, we don‚Äôt just remember exact conversations‚Äîwe extract meaning, identify patterns, and build understanding over time. Teaching AI agents to respond the same requires solving several complex challenges: 
 
 Agent memory systems must distinguish between meaningful insights and routine chatter, determining which utterances deserve long-term storage versus temporary processing. A user saying ‚ÄúI‚Äôm vegetarian‚Äù should be remembered, but ‚Äúhmm, let me think‚Äù should not. 
 Memory systems need to recognize related information across time and merge it without creating duplicates or contradictions. When a user mentions they‚Äôre allergic to shellfish in January and mentions ‚Äúcan‚Äôt eat shrimp‚Äù in March, these needs to be recognized as related facts and consolidated with existing knowledge without creating duplicates or contradictions. 
 Memories must be processed in order of temporal context. Preferences that change over time (for example, the user loved spicy chicken in a restaurant last year, but today, they prefer mild flavors) require careful handling to make sure the most recent preference is respected while maintaining historical context. 
 As memory stores grow to contain thousands or millions of records, finding relevant memories quickly becomes a significant challenge. The system must balance comprehensive memory retention with efficient retrieval. 
 
Solving these problems requires sophisticated extraction, consolidation, and retrieval mechanisms that go beyond simple storage.&nbsp;Amazon Bedrock AgentCore Memory tackles these complexities by implementing a research-backed long-term memory pipeline that mirrors human cognitive processes while maintaining the precision and scale required for enterprise applications. 
How AgentCore long-term memory works 
When the agentic application sends conversational events to AgentCore Memory, it initiates a pipeline to transform raw conversational data into structured, searchable knowledge through a multi-stage process. Let‚Äôs explore each component of this system.&nbsp; 
1. Memory extraction: From conversation to insights 
When new events are stored in short-term memory, an asynchronous extraction process analyzes the conversational content to identify meaningful information. This process leverages large language models (LLMs) to understand context and extract relevant details that should be preserved in long-term memory.&nbsp;The extraction engine processes incoming messages alongside prior context to generate memory records in a predefined schema. As a developer, you can configure one or more Memory strategies to extract only the information types relevant to your application needs. The extraction process supports three built-in memory strategies: 
 
 Semantic memory: Extracts facts and knowledge. Example: 
   
   "The customer's company has 500 employees across Seattle, Austin, and Boston" 
    
 User preferences: Captures explicit and implicit preferences given context. Example: 
   
   {‚Äúpreference‚Äù: "Prefers Python for development work", ‚Äúcategories‚Äù: [‚Äúprogramming‚Äù, ‚Äùcode-style‚Äù], ‚Äúcontext‚Äù: ‚ÄúUser wants to write a student enrollment website‚Äù} 
    
 Summary memory: Creates running narratives of conversations under different topics scoped to sessions and preserves the key information in a structured XML format. Example: 
   
   &lt;topic=‚ÄúMaterial-UI TextareaAutosize inputRef Warning Fix Implementation‚Äù&gt; A developer successfully implemented a fix for the issue in Material-UI where the TextareaAutosize component gives a "Does not recognize the 'inputRef' prop" warning when provided to OutlinedInput through the 'inputComponent' prop. &lt;/topic&gt; 
    
 
For each strategy, the system processes events with timestamps for&nbsp;maintaining the continuity of context and conflict resolution. Multiple memories can be extracted from a single event, and each memory strategy operates independently, allowing parallel processing. 
2. Memory consolidation 
Rather than simply adding new memories to existing storage, the system performs intelligent consolidation to merge related information, resolve conflicts, and minimize redundancies. This consolidation makes sure the agent‚Äôs memory remains coherent and up to date as new information arrives. 
The consolidation process works as follows: 
 
 Retrieval: For each newly extracted memory, the system retrieves the top most semantically similar existing memories from the same namespace and strategy. 
 Intelligent processing: The new memory and retrieved memories are sent to the LLM with a consolidation prompt. The prompt preserves the semantic context, thus avoiding unnecessary updates (for example, ‚Äúloves pizza‚Äù and ‚Äúlikes pizza‚Äù are considered essentially the same information). Preserving these core principles, the prompt is designed to handle various scenarios: 
   
   You are an expert in managing data. Your job is to manage memory store.&nbsp;
Whenever a new input is given, your job is to decide which operation to perform.

Here is the new input text.
TEXT: {query}

Here is the relevant and existing memories
MEMORY: {memory}

You can call multiple tools to manage the memory stores... 
   Based on this prompt, the LLM determines the appropriate action: 
   
   ADD: When the new information is distinct from existing memories 
   UPDATE: Enhance existing memories when the new knowledge complements or updates the existing memories 
   NO-OP: When the information is redundant 
    
 Vector store updates: The system applies the determined actions, maintaining an immutable audit trail by marking the outdated memories as INVALID instead of instantly deleting them. 
 
This approach makes sure that contradictory information is resolved (prioritizing recent information), duplicates are minimized, and related memories are appropriately merged. 
Handling edge cases 
The consolidation process gracefully handles several challenging scenarios: 
 
 Out-of-order events: Although the system processes events in temporal order within sessions, it can handle late-arriving events through careful timestamp tracking and consolidation logic. 
 Conflicting information: When new information contradicts existing memories, the system prioritizes recency while maintaining a record of previous states: 
   
   Existing: "Customer budget is \$500"
New: "Customer mentioned budget increased to \$750"
Result: New active memory with \$750, previous memory marked inactive 
    
 Memory failures: If consolidation fails for one memory, it doesn‚Äôt impact others. The system uses exponential backoff and retry mechanisms to handle transient failures. If consolidation ultimately fails, the memory is added to the system to help prevent potential loss of information. 
 
Advanced custom memory strategy configurations 
While built-in memory strategies cover common use cases, AgentCore Memory recognizes that different domains require tailored approaches for memory extraction and consolidation. The system supports built-in strategies with overrides for custom prompts that extend the built-in extraction and consolidation logic, letting teams adapt memory handling to their specific requirements. To maintain system compatibility and focus on criteria and logic rather than output formats, custom prompts help developers customize what information gets extracted or filtered out, how memories should be consolidated, and how to resolve conflicts between contradictory information. 
AgentCore Memory also supports custom model selection for memory extraction and consolidation. This flexibility helps developers balance accuracy and latency based on their specific needs. You can define them via the APIs when you create the memory_resource as a strategy override or via the console (as shown below in the console screenshot). 
 
Apart from override functionality, we also offer self-managed strategies that provide complete control over your memory processing pipeline. With self-managed strategies, you can implement custom extraction and consolidation algorithms using any models or prompts while leveraging AgentCore Memory for storage and retrieval. Also, using the Batch APIs, you can directly ingest extracted records into AgentCore Memory while maintaining full ownership of the processing logic. 
Performance characteristics 
We evaluated our built-in memory strategy across three public benchmarking datasets to assess different aspects of long-term conversational memory: 
 
 LoCoMo: Multi-session conversations generated through a machine-human pipeline with persona-based interactions and temporal event graphs. Tests long-term memory capabilities across realistic conversation patterns. 
 LongMemEval: Evaluates memory retention in long conversations across multiple sessions and extended time periods. We randomly sampled 200 QA pairs for evaluation efficiency. 
 PrefEval: Tests preference memory across 20 topics using 21-session instances to evaluate the system‚Äôs ability to remember and consistently apply user preferences over time. 
 PolyBench-QA: A question-answering dataset containing 807 Question Answer (QA) pairs across 80 trajectories, collected from a coding agent solving tasks in PolyBench. 
 
We use two standard metrics: correctness and compression rate. LLM-based correctness evaluates whether the system can correctly recall and use stored information when needed. Compression rate is defined as output memory token count / full context token count, and evaluates how effectively the memory system stores information. Higher compression rates indicate the system maintains essential information while reducing storage overhead. This compression rate directly translates to faster inference speeds and lower token consumption‚Äìthe most critical consideration for deploying agents at scale because it enables more efficient processing of large conversational histories and reduces operational costs. 
 
  
   
   Memory Type 
   Dataset 
   Correctness 
   Compression Rate 
   
   
   RAG baseline (full conversation history) 
   LoCoMo 
   77.73% 
   0% 
   
   
   LongMemEval-S 
   75.2% 
   0% 
   
   
   PrefEval 
   51% 
   0% 
   
   
   Semantic Memory 
   LoCoMo 
   70.58% 
   89% 
   
   
   LongMemEval-S 
   73.60% 
   94% 
   
   
   Preference Memory 
   PrefEval 
   79% 
   68% 
   
   
   Summarization 
   PolyBench-QA 
   83.02% 
   95% 
   
  
 
The retrieval-augmented-generation (RAG) baseline performs well on factual QA tasks due to complete conversation history access, but struggles with preference inference. The memory system achieves strong practical trade-offs: though information compression leads to slightly lower correctness on some factual tasks, it provides 89-95% compression rates for scalable deployment, maintaining bounded context sizes, and performs effectively at their specialized use cases. 
For more complex tasks requiring inference (understanding user preferences or behavioral patterns), memory demonstrates clear advantages in both performance accuracy and storage efficiency‚Äîthe extracted insights are more valuable than raw conversational data for these use cases. 
Beyond accuracy metrics, AgentCore Memory delivers the performance characteristics necessary for production deployment. 
 
 Extraction and consolidation operations complete within 20-40 seconds for standard conversations after the extraction is triggered. 
 Semantic search retrieval (retrieve_memory_records API) returns results in approximately 200 milliseconds. 
 Parallel processing architecture enables multiple memory strategies to process independently; thus, different memory types can be processed simultaneously without blocking each other. 
 
These latency characteristics, combined with the high compression rates, enable the system to maintain responsive user experiences while managing extensive conversational histories efficiently across large-scale deployments. 
Best practices for long-term memory 
To maximize the effectiveness of long-term memory in your agents: 
 
 Choose the right memory strategies: Select built-in strategies that align with your use case or create custom strategies for domain-specific needs.&nbsp;Semantic memory captures factual knowledge, preference memory tailors towards&nbsp;individual preference, and&nbsp;summarization memory distills complex information for better context management. For example, a customer support agent might use semantic memory to capture customer transaction history and past issues, while summarization memory creates short narratives of current support conversations and troubleshooting workflows across different topics. 
 Design meaningful namespaces: Structure your namespaces to reflect your application‚Äôs hierarchy. This also enables precise memory isolation and efficient retrieval. For example, use customer-support/user/john-doe for individual agent memories and customer-support/shared/product-knowledge for team-wide information. 
 Monitor consolidation patterns: Regularly review what memories are being created (using list_memories or retrieve_memory_records API), updated, or skipped. This helps refine your extraction strategies and helps the system capture relevant information that‚Äôs better fitted to your use case. 
 Plan for async processing: Remember that long-term memory extraction is asynchronous. Design your application to handle the delay between event ingestion and memory availability. Consider using short-term memory for immediate retrieval needs while long-term memories are being processed and consolidated in the background. You might also want to implement fallback mechanisms or loading states to manage user expectations during processing delays. 
 
Conclusion 
The Amazon Bedrock AgentCore Memory long-term memory system represents a significant advancement in building AI agents. By combining sophisticated extraction algorithms, intelligent consolidation processes, and immutable storage designs, it provides a robust foundation for agents that learn, adapt, and improve over time. 
The science behind this system, from research-backed prompts to innovative consolidation workflow, makes sure that your agents don‚Äôt just remember, but understand. This transforms one-time interactions into continuous learning experiences, creating AI agents that become more helpful and personalized with every conversation. 
Resources: ‚Äì AgentCore Memory Docs ‚Äì AgentCore Memory code samples ‚Äì Getting started with AgentCore ‚Äì Workshop 
 
About the authors 
Akarsha Sehwag is a Generative AI Data Scientist for Amazon Bedrock AgentCore GTM team. With over six years of expertise in AI/ML, she has built production-ready enterprise solutions across diverse customer segments in Generative AI, Deep Learning and Computer Vision domains. Outside of work, she likes to hike, bike or play Badminton. 
Jiarong Jiang is a Principal Applied Scientist at AWS, driving innovations in Retrieval-Augmented Generation (RAG) and agent memory systems to improve the accuracy and intelligence of enterprise AI. She‚Äôs passionate about enabling customers to build context-aware, reasoning-driven applications that leverage their own data effectively. 
Jay Lopez-Braus is a Senior Technical Product Manager at AWS. He has over ten years of product management experience. In his free time, he enjoys all things outdoors. 
Dani Mitchell is a Generative AI Specialist Solutions Architect at Amazon Web Services (AWS). He is focused on helping accelerate enterprises across the world on their generative AI journeys with Amazon Bedrock and Bedrock AgentCore. 
Peng Shi is a Senior Applied Scientist at AWS, where he leads advancements in agent memory systems to enhance the accuracy, adaptability, and reasoning capabilities of AI. His work focuses on creating more intelligent and context-aware applications that bridge cutting-edge research with real-world impact.

‚∏ª