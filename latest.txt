âœ… Morning News Briefing â€“ August 14, 2025 10:48

ğŸ“… Date: 2025-08-14 10:48
ğŸ·ï¸ Tags: #briefing #ai #publichealth #digitalgov

â¸»

ğŸ§¾ Weather
â€¢ No watches or warnings in effect, Pembroke
  No watches or warnings in effect. No warnings or watches or watches in effect . Watch or warnings are no longer in effect in the U.S. No watches, warnings are in effect for the rest of the day . No watches and warnings are still in effect, but no watches are in place for the day's events . The weather is not expected to be affected by the weather .
â€¢ Current Conditions:  15.8Â°C
  Temperature: 15.8&deg;C Pressure / Tendency: 101.6 kPa rising Humidity: 86 % Dewpoint: 13.5&deg:C Wind: NNW 8 km/h . Air Quality Health Index: n/a . Pembroke 6:00 AM EDT Thursday 14 August 2025 . Weather forecast: 15/8Â°C Pem
â€¢ Thursday: Sunny. High 26.
  Sunny. Sunny. High 26. Humidex 29. UV index 7 or high . UV index is 7 or . high . Forecast issued 5:00 AM EDT Thursday 14 August 2025. Forecast: Sunny, sunny, breezy, sunny and breezy in the coming days of the next few days . Rainy showers could make it feel like it was in the middle of

ğŸŒ International News
No updates.

ğŸ Canadian News
No updates.

ğŸ‡ºğŸ‡¸ U.S. Top Stories
â€¢ New details emerge about Japan's notorious WWII germ warfare program
  The release of WWII-era military documents this year has given a boost to researchers digging into Japan's germ warfare program . Japan's government has never apologized for the atrocities . The release has given researchers a boost in their efforts to find out what Japan did to develop germ warfare . Japan has never apologised for its atrocities, despite the release of military documents in WWII-style documents this month .
â€¢ Trump is tightening the screws on corporate America â€” and CEOs are staying mum
  Critics warn that Trump's demands for business leaders to step down, and for the government to take a cut of sales, threaten American-style capitalism . Critics say the demands threaten the future of American capitalism . Trump has been criticized for his demands that business leaders step down and for taking a cut from sales sales to the government . Trump is expected to be the next president of the United States
â€¢ The fight is on. How redistricting could unfold in 8 entangled states
  State leaders in both parties say they're ready to redraw political lines ahead of 2026 . But state laws and constitutions make mid-decade redistricting virtually impossible in many places . Redrawing political lines is difficult in many states, but state laws make it impossible in some places . State leaders say they'll redraw lines in 2026, but many states have laws that
â€¢ Counting steps for health? Here's how many you really need
  Counting steps is easy using a phone, a wearable or a fitness tracker . Scientists have lots of data to figure out how many daily steps you need to take to improve your health . Here's what they've found out about how many steps needed for a person to get enough to take daily steps to improve their health, and how much steps needed to be taken to make sure you get
â€¢ Crime is down in Washington, D.C., but still a reality in some neighborhoods
  Overwhelmingly, however, people opposed President Trump's takeover of the city with federal agents and National Guard troops . Residents across Washington have different takes on crime in their communities . People opposed the takeover of Washington by the federal government . People in Washington opposed the move, calling for the National Guard and federal agents to take control of the streets of the nation's capital . People also opposed the

ğŸ§  Artificial Intelligence
No updates.

ğŸ’» Digital Strategy
â€¢ .NET 10 preview out now, likely to be near feature-complete
  Microsoft has released Preview 7 of its .NET 10 runtime and frameworks . New features include wrapping WebSocket connections as streams, improved passkey authentication in ASP.NET . MAUI (Multi-platform App UI) fixes and more as latest LTS version nears release candidate stage . Microsoft has also released new features and fixes for MAUI, including improvements to the multi-platform app UI
â€¢ Stock in the Channel pulls website amid cyberattack
  Intruders accessed important systems but tell customers their data is safe . UK-based multinational that provides tech stock availability tools is telling customers that its website outage is due to a cyber attack . Intruder accessed system but tells customers that their data was safe . The attack is not the first time a major tech company has been hit by an attack on its website, the company says .
â€¢ Social media users rubbish at spotting sneaky ads, say boffins
  Social media marketeers are getting better at concealing promos in posts . Boffins have peered deep into the eyes of social media users and come to the conclusion that they're not great at spotting when an influencer is trying to sell them something .â€¦â€¦â€¦...â€¦â€¦ Social media users aren't great at seeing when influencers try to sell something, they say .
â€¢ The plan for Linux after Torvalds has a kernel of truth: There isnâ€™t one
  Linux kernel has achieved a fundamental status in the industry, and thus the world, unmatched in scope, stability, and reputation . It powers lightbulbs to supercomputers, not to mention the billion-plus global army of Android . Success does not guarantee succession, says John Defterios of the Linux kernel . Linux kernel covers a host of processors, a massive array of supported devices
â€¢ The Â£9 billion question: To Microsoft or not to Microsoft?
  UK government's five-year Strategic Partnership Agreement (SPA24) with Microsoft is set to see public sector bodies spend around Â£1.9 billion each year . Are UK taxpayers getting real value from SPA24 â€” or just high cost convenience? Register debate series to see how much money is spent on Microsoft software and services it will cost taxpayers in the UK and the public sector will spend

ğŸ¥ Public Health
No updates.

ğŸ”¬ Science
â€¢ Cured but breathless: the growing burden of DIILD in cancer survivors
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
â€¢ Optimization-based dietary recommendations for healthy eating
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
â€¢ Air pollution could be reduced by incentivizing local government officials to control crop burning
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
â€¢ Identifying key factors for organizational resilience among medical alliance using the analytic hierarchy process method
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
â€¢ Birth weight impacts physical and motor performance of school-aged children in Matola, Mozambique
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations

ğŸ§¾ Government & Policy
No updates.

ğŸ›ï¸ Enterprise Architecture & IT Governance
No updates.

ğŸ¤– AI & Emerging Tech
â€¢ The US could really use an affordable electric truck
  On Monday, Ford announced plans for an affordable electric truck with a 2027 delivery date and an expected price tag of about $30,000, thanks in part to a new manufacturing process that it says will help cut costs.



This could be the shot in the arm that the slowing US EV market needs. Sales are slowing, and Ford in particular has struggled recentlyâ€”the automaker has lost $12 billion over the last two and a half years on its EV division. And the adoption barriers continue to mount, with the Trump administration cutting tax credits as well as rules designed to push automakers toward zero-emissions vehicles. And thatâ€™s not to mention tariffs.



But if anything can get Americans excited, itâ€™s a truck, especially an affordable one. (There was a ton of buzz over the announcement of a bare-bones truck from Bezos-backed Slate Auto earlier this year, for example.) The big question is whether the company can deliver in this environment.



One key thing to note here: This is not the first time that thereâ€™s been a big splashy truck announcement from Ford that was supposed to change everything. The F-150 Lightning was hailed as a turning point for vehicle electrification, a signal that decarbonization had entered a new era. We cited the truck when we put â€œThe Inevitable EVâ€ on our 10 Breakthrough Technologies list in 2023.Â 



Things havenâ€™t quite turned out that way. One problem is that the Lightning was supposed to be relatively affordable, with a price tag of about $40,000 when it was first announced in 2021. The starting price inflated to $52,000 when it actually went on sale in 2022.



The truck was initially popular and became quite hard to find at dealerships. But prices climbed and interest leveled off. The base model hit nearly $60,000 by 2023. For the past few years, Ford has cut Lightning production several times and laid off employees who assembled the trucks.



Now, though, Ford is once again promising an affordable truck, and itâ€™s supposed to be even cheaper this time. To help cut costs, the company says itâ€™s simplifying, creating one universal platform for a new set of EVs. Using a common structure and set of components will help produce not only a midsize truck but also other trucks, vans, and SUVs. There are also planned changes to the manufacturing process (rather than one assembly line, multiple lines will join together to form what theyâ€™re calling an assembly tree).&nbsp;





Another supporting factor for cost savings is the battery. The company plans to use lithium-iron phosphate (or LFP) cellsâ€”a type of lithium-ion battery that doesnâ€™t contain nickel or cobalt. Leaving out those relatively pricey metals means lower costs.



Side note here: That battery could be surprisingly small. In a media briefing, a Ford official reportedly said that the truckâ€™s battery would be 15% smaller than the one in the Atto crossover from the Chinese automaker BYD. Since that model has a roughly 60-kilowatt-hour pack, that could put this new battery at 51 kilowatt-hours. Thatâ€™s only half the capacity of the Ford Lightningâ€™s battery and similar to the smallest pack offered in a Tesla Model 3 today. (This could mean the truck has a relatively limited range, though the company hasnâ€™t shared any details on that front yet.)Â 



A string of big promises isnâ€™t too unusual for a big company announcement. What was unusual was the tone from officials during the event on Monday.



As Andrew Hawkins pointed out in The Verge this week, â€œFord seems to realize its timing is unfortunate.â€ During the announcement, executives emphasized that this was a bet, one that might not work out.



CEO Jim Farley put it bluntly: â€œThe automotive industry has a graveyard littered with affordable vehicles that were launched in our country with all good intentions, and they fizzled out with idle plants, laid-off workers, and red ink.â€ Woof.



From where Iâ€™m standing, itâ€™s hard to be optimistic that this announcement will turn out differently from all those failed ones, given where the US EV market is right now.&nbsp;&nbsp;&nbsp;



In a new report published in June, the energy consultancy BNEF slashed its predictions for future EV uptake. Last year, the organization predicted that 48% of new vehicles sold in the US in 2030 would be electric. In this yearâ€™s edition, that number got bumped down to just 27%.



To be clear: BNEF and other organizations are still expecting more EVs on the roads in the future than today, since the vehicles make up less than 10% of new sales in the US. But expectations are way down, in part because of a broad cut in public support for EVs.&nbsp;



The tax credits that gave drivers up to $7,500 off the purchase of a new EV end in just over a month. Tariffs are going to push costs up even for domestic automakers like Ford, which still rely on imported steel and aluminum.



A revamped manufacturing process and a cheaper, desirable vehicle could be exactly the sort of move that automakers need to make for the US EV market. But Iâ€™m skeptical that this truck will be able to turn it all around.&nbsp;



This article is from The Spark, MIT Technology Reviewâ€™s weekly climate newsletter. To receive it in your inbox every Wednesday, sign up here.
â€¢ The road to artificial general intelligence
  Can todayâ€™s AI revolution produce models that rival or surpass human intelligence across all domains? If so, what underlying enablers would be needed to power them? Dario Amodei, co-founder of Anthropic, predicts some form of â€œpowerful AIâ€ could come as early as 2026 . Sam Altman, chief executive of OpenAI, believes AGI-like properties are already â€œcoming into view,â€ unlocking a societal transformation on par with electricity and the internet .
â€¢ The Download: Trumpâ€™s golden dome, and fueling AI with nuclear power
  This is today&#8217;s edition ofÂ The Download,Â our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



Why Trumpâ€™s â€œgolden domeâ€ missile defense idea is another ripped straight from the movies



Within a week of his inauguration, President Trump issued an executive order to develop â€œThe Iron Dome for Americaâ€ (rebranded the â€œGolden Domeâ€ a month later.) The eruption of a revived conflict between Israel and Iran in June has only strengthened the case for an American version of the Iron Dome in the eyes of the administration.



Trump has often expressed admiration for Israelâ€™s Iron Dome, an air defense system that can intercept short-range rockets and artillery over the small nation and that is funded in part by the United States.



But in the complicated security landscape confronting the world today, is spectacle the same as safety? Read the full story.



â€”Becky Ferreira



This story is from our forthcoming print issue, which is all about security. If you havenâ€™t already, subscribe now to receive future issues once they land.







MIT Technology Review Narrated: Can nuclear power really fuel the rise of AI?In the AI arms race, all the major players say they want to go nuclear.



Over the past year, the likes of Meta, Amazon, Microsoft, and Google have sent out a flurry of announcements related to nuclear energy. Some are about agreements to purchase power from existing plants, while others are about investments looking to boost unproven advanced technologies.



These somewhat unlikely partnerships could be a win for both the nuclear power industry and large tech companies. But thereâ€™s one glaring potential roadblock: timing.



This is our latest story to be turned into a MIT Technology Review Narrated podcast, which weâ€™re publishing each week on Spotify and Apple Podcasts. Just navigate to MIT Technology Review Narrated on either platform, and follow us to get all our new content as itâ€™s released.







The must-reads



Iâ€™ve combed the internet to find you todayâ€™s most fun/important/scary/fascinating stories about technology.



1 OpenAI has restored GPT-4o as the default for paying usersAnd Sam Altman has promised â€œplenty of noticeâ€ if other changes are made. (VentureBeat)+ The GPT-5 rollout has been plagued with issues. (WSJ $)



2 Perplexity has offered to buy Chrome for $34.5 billionThatâ€™s way more money than Perplexity itself is worth. (WSJ $)+ The company is definitely shooting its shot. (TechCrunch)+ However, none of the deals it floats generally come to fruition. (The Information $)



3 A US appeals court has permitted DOGE to access sensitive citizen dataIt rejected unionsâ€™ attempt to block it on privacy grounds. (WP $)+ DOGEâ€™s tech takeover threatens the safety and stability of our critical data. (MIT Technology Review)



4 US military officials are preparing to launch security satellites into spaceAfter over a decade in highly secretive development and testing. (Ars Technica)



5 Scientists want to test a carbon removal project in the Gulf of MaineTo see whether the ocean can be engineered to absorb more carbon. (Undark)+ Seaweed farming for carbon dioxide capture would take up too much of the ocean. (MIT Technology Review)



6 UK traffic to porn sites has plummetedEver since the country introduced age-checking measures. (FT $)



7 AI eroded doctorsâ€™ ability to spot cancerTheir ability to spot tumors fell by around 20% within just a few months of adopting it. (Bloomberg $)+ And their skills degraded pretty quicklyâ€”within months. (Time $)+ Why itâ€™s so hard to use AI to diagnose cancer. (MIT Technology Review)



8 The UK is asking residents to delete emails during a drought In a bid to save water used to cool data centers. (404 Media)+ Needless to say, there are far easier ways to save water. (The Verge)+ The data center boom in the desert. (MIT Technology Review)



9 Hair loss may be becoming a thing of the past Whatâ€™s next for Jeff Bezos? (NY Mag $)



10 Your old electronics could contain a tiny hidden doodleA passionate group of collectors are trying to seek out the chip etchings. (NYT $)







Quote of the day



â€œIt&#8217;s so sad to hear users say, &#8216;Please can I have it back? I&#8217;ve never had anyone in my life be supportive of me. I never had a parent tell me I was doing a good job.'&#8221;



â€”Sam Altman explains why some users have requested the company return ChatGPT to its previous more sycophantic ways, Insider reports.







One more thing







Palmer Luckey on the Pentagonâ€™s future of mixed realityPalmer Luckey has, in some ways, come full circle.His first experience with virtual-reality headsets was as a teenage lab technician at a defense research center in Southern California, studying their potential to curb PTSD symptoms in veterans. He then built Oculus, sold it to Facebook for $2 billion, left Facebook after a highly public ousting, and founded Anduril, which focuses on drones, cruise missiles, and other AI-enhanced technologies for the US Department of Defense. The company is now valued at $14 billion.Now Luckey is redirecting his energy again, to headsets for the military. He spoke to MIT Technology Review about his plans. Read the full interview.



â€”James Oâ€™Donnell







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)



+ Take a look at these ancient sites that might, just might, have been built by aliens + What to take to a BBQ if you fancy emulating the cooking greats.+ This fun site swaps the captions on different comics.+ Bring back the drumulator, I say.
â€¢ Why Trumpâ€™s â€œgolden domeâ€ missile defense idea is another ripped straight from the movies
  In 1940, a fresh-faced Ronald Reagan starred as US Secret Service agent Brass Bancroft in Murder in the Air, an action film centered on a fictional â€œsuperweaponâ€ that could stop enemy aircraft midflight. A mock newspaper in the movie hails it as the â€œgreatest peace argument ever invented.â€ The experimental weapon is â€œthe exclusive property of Uncle Sam,â€ Reaganâ€™s character declares.



More than 40 years later, this cinematic visionâ€”an American superweapon capable of neutralizing assaults and ushering in global peaceâ€”became a real-life centerpiece of Reaganâ€™s presidency. Some have suggested that Reaganâ€™s Strategic Defense Initiative (SDI), a quixotic plan for a space-based missile shield, may have been partly inspired by his silver-screen past; indeed, the concept was so fantastical itâ€™s now better known by its Hollywood-referencing nickname, â€œStar Wars.â€



In January 2024, Donald Trump revived the space-shield dream at a primary campaign rally in Laconia, New Hampshire, using the Star Wars nickname that Reagan hated. It didnâ€™t work in the 1980s, Trump said, because the technology wasnâ€™t there. But times have changed.&nbsp;




Whether in Golden Age Hollywood or Trumpâ€™s impromptu dramatizations, the dream of a missile shield is animated by its sheer cinematic allure.




â€œIâ€™ve seen so many things. Iâ€™ve seen shots that you wouldnâ€™t even believe,â€ Trump said. He acted out a scene of missile defense experts triangulating the path of an incoming weapon. â€œDing, ding, ding, ding,â€ he said, as he mimed typing on a keyboard. â€œMissile launch? Psshing!!â€ He raised his hand to indicate the rising missile, then let it fall to signal the successful interception: â€œBoom.â€&nbsp;



Trump has often expressed admiration for Israelâ€™s Iron Dome, an air defense system that can intercept short-range rockets and artillery over the small nation and that is funded in part by the United States. At the rally, he pledged to â€œbuild an Iron Dome over our country, a state-of-the-art missile defense shield made in the USA â€¦ a lot of it right here in New Hampshire, actually.â€&nbsp;



Within a week of his inauguration, President Trump began working toward this promise by issuing an executive order to develop â€œThe Iron Dome for America,â€ which was rebranded the â€œGolden Domeâ€ a month later. The eruption of a revived conflict between Israel and Iran in Juneâ€”including Trumpâ€™s decision to strike Iranâ€™s nuclear facilitiesâ€”has only strengthened the case for an American version of the Iron Dome in the eyes of the administration.



CHIP SOMODEVILLA/GETTY IMAGES




The Golden Dome has often been compared to SDI for its futuristic sheen, its aggressive form of protection, and its reflection of the belief that an impenetrable shield is the cheat code to global peace. Both efforts demonstrate the performative power of spectacle in defense policy, especially when wielded by deft showmen like Reagan and Trump. Whether in Golden Age Hollywood or Trumpâ€™s impromptu dramatizations, the dream of a missile shield is animated by its sheer cinematic allure, often rendered in deceptively simple concept art depicting a society made immune to catastrophic strikes.&nbsp;



But in the complicated security landscape confronting the world today, is spectacle the same as safety?



â€œMissile defense is an area where facts and fiction blend,â€ says Anette Stimmer, a lecturer in international relations at the University of St Andrews who has researched SDI. â€œA lot is up to interpretation by all the actors involved.â€







Trumpâ€™s view is simple: Space is as much a warfighting domain as land, air, and ocean, and therefore the US must assert its dominance there with advanced technologies. This position inspired the creation of the US Space Force in his first term, and Trump has now redoubled his efforts with the ongoing development of the Golden Dome.&nbsp;&nbsp;





General Michael Guetlein, who Trump has appointed to lead the Golden Dome project, argued that Americaâ€™s foes, including China and Russia, have forced the nationâ€™s hand by continually pushing limits in their own weapons programs. â€œWhile we have been focused on peace overseas, our adversaries have been quickly modernizing their nuclear forces, building out ballistic missiles capable of hosting multiple warheads; building out hypersonic missiles capable of attacking the United States within an hour and traveling at 6,000 miles an hour; building cruise missiles that can navigate around our radar and our defenses; and building submarines that can sneak up on our shores; and, worse yet, building space weapons,â€ Guetlein said in May.



â€œIt is time that we change that equation and start doubling down on the protection of the homeland,â€ he said. â€œGolden Dome is a bold and aggressive approach to hurry up and protect the homeland from our adversaries. We owe it to our children and our childrenâ€™s children to protect them and afford them a quality of life that we have all grown up enjoying.â€



With that vision in mind, Trumpâ€™s executive order outlines a host of goals for missile defense, some of which support bipartisan priorities like protecting supply chains and upgrading sensor arrays. The specific architecture of the Golden Dome is still being hammered out, but the initial executive order envisions a multi-tiered system of new sensors and interceptorsâ€”on the ground, in the air, and in spaceâ€”that would work together to counter the threat of attacks from ballistic, hypersonic, and cruise missiles. The system would be coordinated in part by artificial-intelligence models trained for real-time threat detection and response.Â 



The technology that links the Golden Dome directly to SDI hinges on one key bullet point in the order that demands the â€œdevelopment and deployment of proliferated space-based interceptors capable of boost-phase intercept.â€ This language revives Reaganâ€™s dream of deploying hundreds of missile interceptors in orbit to target missiles in the boost phase right after liftoff, a window of just a few minutes when the projectiles are slower and still near the attackerâ€™s territory.



Space weapons are an attractive option for targeting the boost phase because interceptors need to be close enough to the launching missile to hit it. If a nation fired off long-range missiles from deep in its territory, the nearest ground- or air-based interceptors could be thousands of miles from the launch site. Space interceptors, in contrast, would be just a few hundred miles overhead of the ascending missiles, allowing for a much faster reaction time. But though the dream of boost-phase interception dates back decades, these maneuvers have never been operationally demonstrated from ground, air, or space.





â€œItâ€™s a really hard problem that hasnâ€™t been solved,â€ says Laura Grego, senior scientist and research director at the Union of Concerned Scientistsâ€™ global security program.



The US is currently protected by the Ground-Based Midcourse Defense (GMD), which consists of 44 interceptor missiles split between bases in Alaska and California, along with a network of early-Â­warning sensors on the ground, at sea, and in orbit. Tests suggest that the GMD would have about a 50% success rate at intercepting missiles.



Initiated by President Bill Clinton in the late â€™90s and accelerated by President George W. Bush in the 2000s, the GMD is intended mainly to defend against rogue states like North Korea, which has nuclear weapons and intercontinental ballistic missiles (ICBMs) capable of reaching the US. A secondary focus is Iran, which does not currently have a nuclear weapon or ICBMs. Still, the GMD is built to anticipate a possible future where it develops those capabilities.&nbsp;



The GMD is not designed to protect the US from the sort of large-scale and coordinated missile attacks that Russia and China could lob across the world. The Bush administration instead favored a focus on strategic deterrence with these peer nations, an approach that the Obama and Biden administrations continued. In addition to the GMD, the Pentagon and its international partners maintain regional defense systems to counter threats in conflict hot spots or attacks on critical infrastructure. All these networks are designed to intercept missiles during their midcourse cruise phase, as they hurtle through the sky or space, or during their terminal or reentry phase, as they approach their targets. The GMD has cost upward of $63 billion since it was initiated, and the US spends about an additional $20 billion to $30 billion annually on its array of other missile defense systems.&nbsp;



In May, Trump was presented with several design options for the Golden Dome and selected a plan with a price tag of $175 billion and a schedule for full deployment by the end of his term. The One Big Beautiful Bill, signed into law on July 4, approved an initial $24.4 billion in funding for it. Space technologies and launch access have become much more affordable since the 1980s, but many analysts still think the projected cost and timeline are not realistic. The Congressional Budget Office, a nonpartisan federal agency, projected that the cost of the space-based interceptors could total from $161 billion to $542 billion over the course of 20 years. The wide range can be explained by the current lack of specifics on those orbital interceptorsâ€™ design and number.



Reintroducing the idea of space-based interceptors is â€œprobably the most controversial piece of Golden Dome,â€ says Leonor Tomero, who served as deputy assistant secretary of defense for nuclear and missile defense policy in the Biden administration.&nbsp;



â€œThere are a lot of improvements that we can and should make on missile defense,â€ she continues. â€œThereâ€™s a lot of capability gaps I think we do need to address. My concern is the focus on reviving Star Wars and SDI. Itâ€™s got very significant policy implications, strategic stability implications, in addition to cost implications and technology feasibility challenges.â€&nbsp;



Indeed. Regardless of whether the Golden Dome materializes, the program is already raising geopolitical anxieties reminiscent of the Cold War era. Back then, the US had one main adversary: the Soviet Union. Now, it confronts a roiling multipolarity of established and nascent nuclear powers. Many of them have expressed dismay over the about-face on American missile defense strategy, which was previously predicated on arms reduction and deterrence.



â€œHere we are, despite years of saying we are not going to do thisâ€”that it is technically out of reach, economically unsustainable, and strategically unwise,â€ Grego says. â€œOvernight, weâ€™re like, â€˜No, actually, weâ€™re doing it.â€™â€&nbsp;





The fact that we â€œblew up that logicâ€ will â€œhave a big impact on whether or not the program actually succeeds in creating the vision that it lays out,â€ she adds.



Russian and Chinese officials called the Golden Dome â€œdeeply destabilizing in natureâ€ in a joint statement in May, and North Koreaâ€™s foreign ministry warned it could â€œturn outer space into a potential nuclear war field.â€&nbsp;&nbsp;



Reagan, by all accounts, believed that SDI would be the ultimate tool of peace for all nations, and he even offered to share the technology with the Soviet leader, Mikhail Gorbachev. Trump, in contrast, sees Golden Dome as part of his â€œAmerica Firstâ€ brand. He has lamented that past American leaders supported the development of other missile defense projects abroad while neglecting to build similar security measures for their own country. The Golden Dome is both an expression of Trumpâ€™s belief that the world is leeching off America and a bargaining chip in negotiations toward a new power balance; Canada could be covered by the shield for free, he has saidâ€”in exchange for becoming the 51st state.



Trump has argued that America has been both demographically diluted by unchecked immigration and financially depleted by freeloading allied nationsâ€”undermining its security on both internal and external fronts. His first termâ€™s marquee promise to build a wall on the southern US border, paid for by Mexico, aimed to address the former problem. That administration did build more physical barriers along the border (though US taxpayers, not Mexico, footed the bill). But just as important, the wall emerged as a symbolic shorthand for tougher immigration control.&nbsp;



The Golden Dome is the second-term amplification of that promise, a wall that expands the concept of the â€œborderâ€ to the entire American airspace. Trump has projected an image of his envisioned space missile shield as a literal dome that could ward off coordinated attacks, including boost-phase interceptors from space and cruise- and terminal-phase interception by ground and air assets. When he announced the selected plan from the Resolute Desk in May, he sat in front of a mockup that depicted a barrage of incoming missiles being thwarted by the nationwide shield, depicted with a golden glow.



The Golden Domeâ€™s orbital interceptors are supposedly there to target the early boost phase of missiles on or near the launch site, not over the United States. But the image of a besieged America, repelling enemy fire from the heavens, provides the visual and cinematic idea of both threat and security that Trump hopes to impress on the public.&nbsp;&nbsp;



â€œThis administration, and MAGA world, thinks about itself as being victimized by immigrants, government waste, leftist professors, and so on,â€ says Edward Tabor Linenthal, a historian who examined public narratives about SDI in his 1989 book Symbolic Defense: The Cultural Significance of the Strategic Defense Initiative. â€œItâ€™s not much of a jump to be victimized by too many nations getting nuclear weapons.â€&nbsp;







Even in our era of entrenched political polarization, there is support across party lines for upgrading and optimizing Americaâ€™s missile defense systems. No long-range missile has ever struck US soil, but an attack would be disastrous for the nation and the world.&nbsp;



â€œWeâ€™ve come a long way in terms of missile defense,â€ says Tomero. â€œThere has been a lot of bipartisan consensus on increasing regional missile defense, working with our allies, and making sure that the missile defense interceptors we have work.â€



SHOUT




Trump has challenged that consensus with his reversion to the dream of a space shield. He is correct that SDI failed to materialize in part because its envisioned technologies were out of reach, from a financial and engineering standpoint, in the 1980s. But the controversy that erupted around SDIâ€”and that tarnished it with the derisive name â€œStar Warsâ€â€”stemmed just as much from its potential geopolitical disruptiveness as from its fantastical techno-optimism.&nbsp;



â€œThis idea of a missile shield, also back when Reagan proposed it, has a huge popular appeal, because who wouldnâ€™t want to be able to defend your country from nuclear weapons? It is a universal dream,â€ says Stimmer. â€œIt requires a bit more digging in and understanding to see that actually, this vision depends a lot on technological feasibility and on how others perceive it.â€&nbsp;



Reagan maintained a steadfast conviction that this shield of space-based interceptors would render nuclear weapons â€œimpotent and obsolete,â€ ushering in â€œworld peace,â€ as he said in his March 1983 speech announcing SDI. The doctrine of mutually assured destruction could be replaced by mutually assured survival, he argued.




Amid nuclear tensions, J. Robert Oppenheimer compared the US and the Soviet Union to â€œtwo scorpions in a bottle.â€ Now there are many more scorpions.




But Gorbachev saw the space-based shield as an offensive weapon, since it would give the US a first-strike advantage. The imbalance, he warned, could spark a weapons race in space, a domain that had been spared from overt military conflicts. As a result, the initiative would only destabilize the world order and interrupt the progress of arms control and nuclear de-proliferation efforts.&nbsp;



Reaganâ€™s insistence on SDI as the only route to world peace may have blocked opportunities to advance that goal through more practical and cost-effective avenues, such as diplomacy and arms control. At the 1986 Reykjavik Summit, Reagan and Gorbachev came very close to an arms control agreement that might have eliminated all ballistic missiles and nuclear weapons. The sticking point was Reaganâ€™s refusal to give up SDI.&nbsp;





â€œIt is not the Strategic Defense Initiative; itâ€™s a strategic defense ideology,â€ says Linenthal. He mentions the famous metaphor used by J. Robert Oppenheimer, a central figure of the Manhattan Project, who compared the United States and the Soviet Union to â€œtwo scorpions in a bottle.â€ Either scorpion could kill the other, but only at the probable cost of its own life.&nbsp;



Reagan felt a â€œtremendously powerful impetusâ€ to escape Oppenheimerâ€™s metaphor, Linenthal noted: â€œIt was a new kind of deliverance that would resolve it all. Of course, now there are many more scorpions, so it has to be a bigger bottle.â€



A true believer, Reagan never abandoned SDI in spite of cost overruns and public backlash. President Bill Clinton redirected the program in 1993 by shifting gears from global to regional missile defense, a focus that remained fairly consistent for decadesâ€”until Trump took center stage. Now, the Golden Dome has flipped that logic on its head, risking a possible escalation of military tensions in outer space.



Tomero describes a â€œnightmare scenarioâ€ in which adversaries attack the Golden Domeâ€™s space infrastructure, leaving the orbital environment filled with debris that renders the defense system, among countless other space assets, inoperable.&nbsp;



â€œHaving a one-sided capability that is very threatening to our adversaries is obviously going to create very dangerous stability issues,â€ she says. It could â€œlead to inadvertent escalation and miscalculation and, I think, lower the threshold to conflict and nuclear war.â€&nbsp;







As president, Trump has channeled the boardroom antics that once resuscitated his celebrity status on The Apprentice. But armed adversaries, long wary of Americaâ€™s position on missile defense, donâ€™t have the luxury of wondering whether itâ€™s all real or just more stagecraft.&nbsp;



â€œWhat makes Trump so difficult to read for others is his unpredictability,â€ Stimmer says. â€œThis, just by itself, destabilizes things, because no one knows what heâ€™ll actually do.â€



Trump has described the Golden Dome as nearly impenetrable by missile attacks, evoking a clear symbolic return to an American golden age where we can all feel safe again.



â€œAll of them will be knocked out of the air,â€ as â€œthe success rate is very close to 100%,â€ he said at the projectâ€™s official launch in May. â€œWe will truly be completing the job that President Reagan started 40 years ago, forever ending the missile threat to the American homeland.â€



Becky Ferreira is a science reporter based in upstate New York, and author of First Contact, a book about the search for alien life, which will be published in September.&nbsp;
â€¢ The Download: meet the judges using AI, and GPT-5â€™s health promises
  This is today&#8217;s edition ofÂ The Download,Â our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



Meet the early-adopter judges using AI



The propensity for AI systems to make mistakes that humans miss has been on full display in the US legal system as of late. The follies began when lawyers submitted documents citing cases that didnâ€™t exist. Similar mistakes soon spread to other roles in the courts. Last December, a Stanford professor submitted sworn testimony containing hallucinations and errors in a case about deepfakes, despite being an expert on AI and misinformation himself.Now, judges are experimenting with generative AI too. Some believe that with the right precautions, the technology can expedite legal research, summarize cases, draft routine orders, and overall help speed up the court system, which is badly backlogged in many parts of the US. Are they right to be so confident in it? Read the full story.



â€”James Oâ€™Donnell







What you may have missed about GPT-5



OpenAIâ€™s new GPT-5 model was supposed to give a glimpse of AIâ€™s newest frontier. It was meant to mark a leap toward the â€œartificial general intelligenceâ€ that techâ€™s evangelists have promised will transform humanity for the better.&nbsp;



Against those expectations, the model has mostly underwhelmed. But thereâ€™s one other thing to take from all this. Among other suggestions for potential uses of its models, OpenAI has begun explicitly telling people to use them for health advice. Itâ€™s a change in approach that signals the company is wading into dangerous waters. Read the full story.



â€”James Oâ€™Donnell



This story originally appeared in The Algorithm, our weekly newsletter on AI. To get stories like this in your inbox first, sign up here.







The must-reads



Iâ€™ve combed the internet to find you todayâ€™s most fun/important/scary/fascinating stories about technology.



1 The US has extended its China tariff truce by another 90 daysÂ Itâ€™s yet again another example of Trumpâ€™s on-again, off-again policies. (CNBC)+ China has succeeded in finding other markets to sell to anyway. (CNN)+ Now weâ€™ve got to wait until November 10 for the next round of tariffs. (BBC)2 Europeâ€™s arms factories are rapidly expandingAs EU governments debate how to sustain weapons deliveries to Ukraine. (FT $)+ Trump is due to meet with Vladimir Putin on Friday. (The Guardian)+ Generative AI is learning to spy for the US military. (MIT Technology Review)Â 



3 China has urged companies to avoid using Nvidiaâ€™s H20 chipsWhich comes as a blow to the firm after it made a deal with the US government. (Bloomberg $)+ Chinese officials fear that the US could embed â€œback doorsâ€ into them. (SCMP $)



4 Elon Musk has threatened legal action against AppleHe claims that OpenAI is the only AI firm able to top its App Store charts. (Reuters)+ Grok is ranked a lowly sixth on its free listings. (FT $)



5 AI is making sharing photos of children even riskierNudifying tools are making it easier than ever to manipulate images. (NYT $)+ You need to talk to your kid about AI. Here are 6 things you should say. (MIT Technology Review)



6 The future of food hinges on our land useCan factory farming ease the burden? (Vox)+ Africa fights rising hunger by looking to foods of the past. (MIT Technology Review)



7 What does Palantir really do?Even former workers donâ€™t seem entirely sure. (Wired $)



8 Interest in AI majors is explodingAmong young students and older workers alike. (WP $)+ Weâ€™re reliving a new dot com bubble updated for the AI age. (New Yorker $)



9 The in-person job interview is staging a comebackAI has made it too easy to cheat remotely. (WSJ $)



10 This YouTube show attempts to turn internet discourse into live debateSounds absolutely terrible. (New Yorker $)







Quote of the day



â€œItâ€™s not banned but has kind of become a politically incorrect thing to do.â€



â€”An anonymous Chinese data center operator tells the Financial Times why purchasing Nvidiaâ€™s H20 chips has become so fraught in China.







One more thing







The search for extraterrestrial life is targeting Jupiterâ€™s icy moon EuropaEuropa, Jupiterâ€™s fourth-largest moon, is nothing like ours. Its surface is a vast saltwater ocean, encased in a blanket of cracked ice, one that seems to occasionally break open and spew watery plumes into the moonâ€™s thin atmosphere.For these reasons, Europa captivates planetary scientists. All that water and energyâ€”and hints of elements essential for building organic molecules â€”point to another extraordinary possibility. Jupiterâ€™s big, bright moon could host life.They may eventually get some answers thanks to Europa Clipper, scheduled to reach Jupiter in 2030. Read the full story.



â€”Stephen Ornes







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)+ Thereâ€™s still plenty of time to decide on a song of the summer.+ Why we love to love horrendously bad filmsâ€”particularly The Room.+ Lock up your daughters: these medieval bards were dangerously charismatic.+ How to instantly become better at pretty much anything.

ğŸ”’ Cybersecurity & Privacy
â€¢ Microsoft Patch Tuesday, August 2025 Edition
  Microsoft today released updates to fix more than 100 security flaws in its Windows operating systems and other software. At least 13 of the bugs received Microsoft&#8217;s most-dire &#8220;critical&#8221; rating, meaning they could be abused by malware or malcontents to gain remote access to a Windows system with little or no help from users.

August&#8217;s patch batch from Redmond includes an update for CVE-2025-53786, a vulnerability that allows an attacker to pivot from a compromised Microsoft Exchange Server directly into an organization&#8217;s cloud environment, potentially gaining control over Exchange Online and other connected Microsoft Office 365 services. Microsoft first warned about this bug on Aug. 6, saying it affects Exchange Server 2016 and Exchange Server 2019, as well as its flagship Exchange Server Subscription Edition.
Ben McCarthy, lead cyber security engineer at Immersive, said a rough search reveals approximately 29,000 Exchange servers publicly facing on the internet that are vulnerable to this issue, with many of them likely to have even older vulnerabilities.
McCarthy said the fix for CVE-2025-53786 requires more than just installing a patch, such as following Microsoft&#8217;s manual instructions for creating a dedicated service to oversee and lock down the hybrid connection.
&#8220;In effect, this vulnerability turns a significant on-premise Exchange breach into a full-blown, difficult-to-detect cloud compromise with effectively living off the land techniques which are always harder to detect for defensive teams,&#8221; McCarthy said.
CVE-2025-53779 is a weakness in the Windows Kerberos authentication system that allows an unauthenticated attacker to gain domain administrator privileges. Microsoft credits the discovery of the flaw to Akamai researcher Yuval Gordon, who dubbed it &#8220;BadSuccessor&#8221; in a May 2025 blog post. The attack exploits a weakness in &#8220;delegated Managed Service Account&#8221; or dMSA &#8212; a feature that was introduced in Windows Server 2025.
Some of the critical flaws addressed this month with the highest severity (between 9.0 and 9.9 CVSS scores) include a remote code execution bug in the Windows GDI+ component that handles graphics rendering (CVE-2025-53766) and CVE-2025-50165, another graphics rendering weakness. Another critical patch involves CVE-2025-53733, a vulnerability in Microsoft Word that can be exploited without user interaction and triggered through the Preview Pane.
One final critical bug tackled this month deserves attention: CVE-2025-53778, a bug in Windows NTLM, a core function of how Windows systems handle network authentication. According to Microsoft, the flaw could allow an attacker with low-level network access and basic user privileges to exploit NTLM and elevate to SYSTEM-level access â€” the highest level of privilege in Windows. Microsoft rates the exploitation of this bug as &#8220;more likely,&#8221; although there is no evidence the vulnerability is being exploited at the moment.
Feel free to holler in the comments if you experience problems installing any of these updates. As ever, the SANS Internet Storm Center has its useful breakdown of the Microsoft patches indexed by severity and CVSS score, and AskWoody.com is keeping an eye out for Windows patches that may cause problems for enterprises and end users.
GOOD MIGRATIONS
Windows 10 users out there likely have noticed by now that Microsoft really wants you to upgrade to Windows 11. The reason is that after the Patch Tuesday on October 14, 2025, Microsoft will stop shipping free security updates for Windows 10 computers. The trouble is, many PCs running Windows 10 do not meet the hardware specifications required to install Windows 11Â (or they do, but just barely).
If the experience with Windows XP is any indicator, many of these older computers will wind up in landfills or else will be left running in an unpatched state. But if your Windows 10 PC doesn&#8217;t have the hardware chops to run Windows 11 and you&#8217;d still like to get some use out of it safely, consider installing a newbie-friendly version of Linux, like Linux Mint.
Like most modern Linux versions, Mint will run on anything with a 64-bit CPU that has at least 2GB of memory, although 4GB is recommended. In other words, it will run on almost any computer produced in the last decade.
There are many versions of Linux available, but Linux Mint is likely to be the most intuitive interface for regular Windows users, and it is largely configurable without any fuss at the text-only command-line prompt. Mint and other flavors of Linux come with LibreOffice, which is an open source suite of tools that includes applications similar to Microsoft Office, and it can open, edit and save documents as Microsoft Office files.
If you&#8217;d prefer to give Linux a test drive before installing it on a Windows PC, you can always just download it to a removable USB drive. From there, reboot the computer (with the removable drive plugged in) and select the option at startup to run the operating system from the external USB drive. If you don&#8217;t see an option for that after restarting, try restarting again and hitting the F8 button, which should open a list of bootable drives. Here&#8217;s a fairly thorough tutorial that walks through exactly how to do all this.
And if this is your first time trying out Linux, relax and have fun: The nice thing about a &#8220;live&#8221; version of Linux (as it&#8217;s called when the operating system is run from a removable drive such as a CD or a USB stick) is that none of your changes persist after a reboot. Even if you somehow manage to break something, a restart will return the system back to its original state.

ğŸ“ University AI
No updates.

ğŸ¢ Corporate AI
â€¢ Dion: the distributed orthonormal update revolution is here
  Training AI models requires choosing an optimizer and for nearly a decade, Adam( (opens in new tab)&#8211;W) (opens in new tab) has been the optimizer of choice. Given that durability and success, it was fair to doubt that any further improvement was possible. And yet, last December, a new optimizer called Muon (opens in new tab) showed serious promise by powering a nanoGPT speedrun (opens in new tab). This proved out, with multiple AI labs (e.g., Kimi-AI (opens in new tab) and Essential-AI (opens in new tab)) reporting 2x scale improvements and the release of the 1T parameter Kimi K2 (opens in new tab) model.&nbsp;Restated: you can train a model to similar performance with half as many GPUs.



Thereâ€™s one fly in the ointment: Muon requires large matrix multiplications in the optimizer, which requires heavy communication in large models at the scale where FSDP and TP parallelization becomes desirable.&nbsp;Going back to the inspiration for Muon, the key idea is an orthonormal update, which sparked the search&nbsp;for more scalable alternative linear algebras realizing the same goal. Thatâ€™s exactly what Dion is. We have open-sourced this new optimizer to enable anyone to train large models more efficiently at scale. &nbsp;



Whatâ€™s an orthonormal update?



Figure1. Illustration of matrix parameters



At the core of Transformers, a set of input activations is multiplied by a learned weight matrix to produce a new set of output activations. When the weight matrix is updated during training, the resulting change in the output activations generally depends on the direction of the input activations. As a result, the learning rate must be chosen conservatively to accommodate the input direction that induces the largest change. Orthonormalized updates alter this behavior by (approximately) making the change in output activations invariant to the direction of the input. This is achieved by enforcing orthonormality (opens in new tab) on the update matrix, thereby equalizing its effect across all input directions.



What is Dion?



While Muon has shown strong empirical results, scaling it to very large models poses challenges. As reported by Essential AI (opens in new tab), applying Muon to large architectures like LLaMA-3 becomes compute-boundâ€”and potentially communication-boundâ€”due to the cost of the Newtonâ€“Schulz orthonormalization steps (opens in new tab).



Figure 2. Pseudocode of the centralized version of Dion



This is where Dion enters. At a high level, Dion introduces a new axis for scalability: the rank. Specifically, for a given rank r, Dion orthonormalizes only the top r of the singular vector space, reducing communication and compute overhead while preserving performance.&nbsp;Empirically, we observe that the necessary rank for good performance grows much more slowly than the number of parameters in larger models.




	
		
						Download
			
				Dion optimizer&nbsp;
			
					
	




Dion implements orthonormalization using amortized power iteration (opens in new tab).&nbsp;Power iteration typically pulls out the largest singular value by repeated matrix multiplication.&nbsp;By amortizing this process over optimization stepsâ€”applied to the slowly-evolving momentum matrixâ€”we reduce the cost to just two matrix multiplications per step. Incorporating a QR decomposition allows us to extract an approximate orthonormal basis spanning the top singular directions, rather than just the leading one.&nbsp;This amortized power iteration is fully compatible with standard distributed training techniques such as FSDP and tensor parallelism.&nbsp;Here, we show a simple centralized version, but the technique works for more complex forms of parallelization as presented in the paper. In other words, we can orthogonalize a matrix without ever seeing a full row or column of it.&nbsp;



Low-rank approximation would ordinarily introduce error, but Dion overcomes this through an error feedback mechanism. This keeps the residual of low rank approximation in the momentum matrix so that any systematic gradient structure not initially captured accumulates to eventually be applied in a future update.



	
		

		
		PODCAST SERIES
	
	
	
						
				
					
				
			
			
			

									AI Testing and Evaluation: Learnings from Science and Industry
				
								Discover how Microsoft is learning from other domains to advance evaluation and testing as a pillar of AI governance.
				
								
					
						
							Listen now						
					
				
							
	
Opens in a new tab	
	


How does it work?



Something very strange happened in our experiments. Usually, adding an extra constraint on the way an algorithm works can be expected to decrease overall performance. And indeed, at the 120M parameter scale of the speedrun, we see Dionâ€™s update taking more time than Muon, while not yielding any significant gains. But at larger scales, we observed a different trend: Dion began to outperform Muon.



Figure 3. Wall-clock time speedup of Dion for 3B model training



Why would adding a constraint improve the update rule? The answer lies in what the constraint enforces. Dion achieves a much closer approximation to true orthonormalization than Muon. This precision, initially subtle, becomes increasingly important as the number of singular vectors grows. Over increasing model scale and training steps, this small advantage accumulatesâ€”leading to a measurable improvement in performance.



This edge further grows with batch sizeâ€”with larger batches the update quality tends to degrade, but notably more slowly with Dion than Muon (and Muon is already a significant improvement over AdamW).



Figure 4. Scaling of Dion across different batch sizes



Here you can see how the number of steps to reach a pretraining loss compared to AdamW varies as batch size grows with full rank and Â¼ rank Dion (in orange) and Muon (in blue).&nbsp;&nbsp;&nbsp;



In our experiments, these benefits extend to various post-training regimes as well.



We also experimented with rank, discovering empirically that larger models tolerate smaller rank well.



Figure 5. Low-rank Dion across different model sizes



Projecting this trend out to the scale of the LLaMA-3 (opens in new tab) 405B parameter models suggests that Dion is fully effective even with rank fractions as low as 1/16 or 1/64 for large dense models like LLaMA-3.&nbsp;&nbsp;&nbsp;&nbsp;



Using hardware timings of the individual update steps suggests a story that looks this:



Figure 6. Estimated wall-clock time of each optimizer step for Llama 3 405B. Lower is better. Muon is highlighted in orange as our baseline, next to Dion with varying rank fractions. Suggested rank fractions for a 405B parameter model are shown in blue. Using Dion with rank fraction 1/16 or lower offers an order-of-magnitude speedup over Muon.



Weâ€™ve open-sourced a PyTorch FSDP2 + Tensor Parallel (TP) implementation of Dion, available via a simple pip install. Our goal is to make faster training with Dion accessible to everyone. As a bonus, the repository also includes a PyTorch FSDP2 implementation of Muon.




Dion optimizer




Acknowledgements



We thank Riashat Islam and Pratyusha Sharma for their helpful feedback on the writing and presentation.
Opens in a new tabThe post Dion: the distributed orthonormal update revolution is here appeared first on Microsoft Research.
â€¢ Securely launch and scale your agents and tools on Amazon Bedrock AgentCore Runtime
  Organizations are increasingly excited about the potential of AI agents, but many find themselves stuck in what we call â€œproof of concept purgatoryâ€â€”where promising agent prototypes struggle to make the leap to production deployment. In our conversations with customers, weâ€™ve heard consistent challenges that block the path from experimentation to enterprise-grade deployment: 
â€œOur developers want to use different frameworks and models for different use casesâ€”forcing standardization slows innovation.â€ 
â€œThe stochastic nature of agents makes security more complex than traditional applicationsâ€”we need stronger isolation between user sessions.â€ 
â€œWe struggle with identity and access control for agents that need to act on behalf of users or access sensitive systems.â€ 
â€œOur agents need to handle various input typesâ€”text, images, documentsâ€”often with large payloads that exceed typical serverless compute limits.â€ 
â€œWe canâ€™t predict the compute resources each agent will need, and costs can spiral when overprovisioning for peak demand.â€ 
â€œManaging infrastructure for agents that may be a mix of short and long-running requires specialized expertise that diverts our focus from building actual agent functionality.â€ 
Amazon Bedrock AgentCore Runtime addresses these challenges with a secure, serverless hosting environment specifically designed for AI agents and tools. Whereas traditional application hosting systems werenâ€™t built for the unique characteristics of agent workloadsâ€”variable execution times, stateful interactions, and complex security requirementsâ€”AgentCore Runtime was purpose-built for these needs. 
The service alleviates the infrastructure complexity that has kept promising agent prototypes from reaching production. It handles the undifferentiated heavy lifting of container orchestration, session management, scalability, and security isolation, helping developers focus on creating intelligent experiences rather than managing infrastructure. In this post, we discuss how to accomplish the following: 
 
 Use different agent frameworks and different models 
 Deploy, scale, and stream agent responses in four lines of code 
 Secure agent execution with session isolation and embedded identity 
 Use state persistence for stateful agents along with Amazon Bedrock AgentCore Memory 
 Process different modalities with large payloads 
 Operate asynchronous multi-hour agents 
 Pay only for used resources 
 
Use different agent frameworks and models 
One advantage of AgentCore Runtime is its framework-agnostic and model-agnostic approach to agent deployment. Whether your team has invested in LangGraph for complex reasoning workflows, adopted CrewAI for multi-agent collaboration, or built custom agents using Strands, AgentCore Runtime can use your existing code base without requiring architectural changes or any framework migrations. Refer to these samples on Github for examples. 
With AgentCore Runtime, you can integrate different large language models (LLMs) from your preferred provider, such as Amazon Bedrock managed models, Anthropicâ€™s Claude, OpenAIâ€™s API, or Googleâ€™s Gemini. This makes sure your agent implementations remain portable and adaptable as the LLM landscape continues to evolve while helping you pick the right model for your use case to optimize for performance, cost, or other business requirements. This gives you and your team the flexibility to choose your favorite or most useful framework or model using a unified deployment pattern. 
Letâ€™s examine how AgentCore Runtime supports two different frameworks and model providers: 
 
  
   
   LangGraph agent using Anthropicâ€™s Claude Sonnet on Amazon Bedrock 
   Strands agent using GPT 4o mini through the OpenAI API 
   
   
    
   
  
 
For the full code examples, refer to langgraph_agent_web_search.py and strands_openai_identity.py on GitHub. 
Both examples above show how you can use AgentCore SDK, regardless of the underlying framework or model choice. After you have modified your code as shown in these examples, you can deploy your agent with or without the AgentCore Runtime starter toolkit, discussed in the next section. 
Note that there are minimal additions, specific to AgentCore SDK, to the example code above. Let us dive deeper into this in the next section. 
Deploy, scale, and stream agent responses with four lines of code 
Letâ€™s examine the two examples above. In both examples, we only add four new lines of code: 
 
 Import â€“ from bedrock_agentcore.runtime import BedrockAgentCoreApp 
 Initialize â€“ app = BedrockAgentCoreApp() 
 Decorate â€“ @app.entrypoint 
 Run â€“ app.run() 
 
Once you have made these changes, the most straightforward way to get started with agentcore is to use the AgentCore Starter toolkit. We suggest using uv to create and manage local development environments and package requirements in python. To get started, install the starter toolkit as follows: 
 
 uv pip install bedrock-agentcore-starter-toolkit 
 
Run the appropriate commands to configure, launch, and invoke to deploy and use your agent. The following video provides a quick walkthrough. 

 
  
 
 
For your chat style applications, AgentCore Runtime supports streaming out of the box. For example, in Strands, locate the following synchronous code: 
 
 result = agent(user_message) 
 
Change the preceding code to the following and deploy: 
 
 agent_stream =&nbsp;agent.stream_async(user_message)
&nbsp;&nbsp; &nbsp;async&nbsp;for&nbsp;event in&nbsp;agent_stream:
&nbsp; &nbsp; &nbsp; &nbsp; yield&nbsp;event&nbsp;#you can process/filter these events before yielding 
 
For more examples on streaming agents, refer to the following GitHub repo. The following is an example streamlit application streaming back responses from an AgentCore Runtime agent. 

 
  
 
 
Secure agent execution with session isolation and embedded identity 
AgentCore Runtime fundamentally changes how we think about serverless compute for agentic applications by introducing persistent execution environments that can maintain an agentâ€™s state across multiple invocations. Rather than the typical serverless model where functions spin up, execute, and immediately terminate, AgentCore Runtime provisions dedicated microVMs that can persist for up to 8 hours. This enables sophisticated multi-step agentic workflows where each subsequent call builds upon the accumulated context and state from previous interactions within the same session. The practical implication of this is that you can now implement complex, stateful logic patterns that would previously require external state management solutions or cumbersome workarounds to maintain context between function executions. This doesnâ€™t obviate the need for external state management (see the following section on using AgentCore Runtime with AgentCore Memory), but is a common need for maintaining local state and files temporarily, within a session context. 
Understanding the session lifecycle 
The session lifecycle operates through three distinct states that govern resource allocation and availability (see diagram below for a high level view of this session lifecycle). When you first invoke a runtime with a unique session identifier, AgentCore provisions a dedicated execution environment and transitions it to an Active state during request processing or when background tasks are running. 
The system automatically tracks synchronous invocation activity, while background processes can signal their status through HealthyBusy responses to health check pings from the service (see the later section on asynchronous workloads). Sessions transition to Idle when not processing requests but remain provisioned and ready for immediate use, reducing cold start penalties for subsequent invocations. 
 
Finally, sessions reach a Terminated state when they currently exceed a 15-minute inactivity threshold, hit the 8-hour maximum duration limit, or fail health checks. Understanding these state transitions is crucial for designing resilient workflows that gracefully handle session boundaries and resource cleanup. For more details on session lifecycle-related quotas, refer to AgentCore Runtime Service Quotas. 
The ephemeral nature of AgentCore sessions means that runtime state exists solely within the boundaries of the active session lifecycle. The data your agent accumulates during executionâ€”such as conversation context, user preference mappings, intermediate computational results, or transient workflow stateâ€”remains accessible only while the session persists and is completely purged when the session terminates. 
For persistent data requirements that extend beyond individual session boundaries, AgentCore Memory provides the architectural solution for durable state management. This purpose-built service is specifically engineered for agent workloads and offers both short-term and long-term memory abstractions that can maintain user conversation histories, learned behavioral patterns, and critical insights across session boundaries. See documentation here for more information on getting started with AgentCore Memory. 
True session isolation 
Session isolation in AI agent workloads addresses fundamental security and operational challenges that donâ€™t exist in traditional application architectures. Unlike stateless functions that process individual requests independently, AI agents maintain complex contextual state throughout extended reasoning processes, handle privileged operations with sensitive credentials and files, and exhibit non-deterministic behavior patterns. This creates unique risks where one userâ€™s agent could potentially access anotherâ€™s dataâ€”session-specific information could be used across multiple sessions, credentials could leak between sessions, or unpredictable agent behavior could compromise system boundaries. Traditional containerization or process isolation isnâ€™t sufficient because agents need persistent state management while maintaining absolute separation between users. 
Letâ€™s explore a case study: In May 2025, Asana deployed a new MCP server to power agentic AI features (integrations with ChatGPT, Anthropicâ€™s Claude, Microsoft Copilot) across its enterprise software as a service (SaaS) offering. Due to a logic flaw in MCPâ€™s tenant isolation and relying solely on user but not agent identity, requests from one organizationâ€™s user could inadvertently retrieve cached results containing another organizationâ€™s data. This cross-tenant contamination wasnâ€™t triggered by a targeted exploit but was an intrinsic security fault in handling context and cache separation across agentic AI-driven sessions. 
The exposure silently persisted for 34 days, impacting roughly 1,000 organizations, including major enterprises. After it was discovered, Asana halted the service, remediated the bug, notified affected customers, and released a fix. 
AgentCore Runtime solves these challenges through complete microVM isolation that goes beyond simple resource separation. Each session receives its own dedicated virtual machine with isolated compute, memory, and file system resources, making sure agent state, tool operations, and credential access remain completely compartmentalized. When a session ends, the entire microVM is terminated and memory sanitized, minimizing the risk of data persistence or cross-contamination. This architecture provides the deterministic security boundaries that enterprise deployments require, even when dealing with the inherently probabilistic and non-deterministic nature of AI agents, while still enabling the stateful, personalized experiences that make agents valuable. Although other offerings might provide sandboxed kernels, with the ability to manage your own session state, persistence, and isolation, this should not be treated a strict security boundary. AgentCore Runtime provides consistent, deterministic isolation boundaries regardless of agent execution patterns, delivering the predictable security properties required for enterprise deployments. The following diagram shows how two separate sessions run in isolated microVM kernels. 
 
AgentCore Runtime embedded identity 
Traditional agent deployments often struggle with identity and access management, particularly when agents need to act on behalf of users or access external services securely. The challenge becomes even more complex in multi-tenant environmentsâ€”for example, where you need to make sure Agent A accessing Google Drive on behalf of User 1 can never accidentally retrieve data belonging to User 2. 
AgentCore Runtime addresses these challenges through its embedded identity system that seamlessly integrates authentication and authorization into the agent execution environment. First, each runtime is associated with a unique workload identity (you can treat this as a unique agent identity). The service supports two primary authentication mechanisms for agents using this unique agent identity: IAM SigV4 Authentication for agents operating within AWS security boundaries, and OAuth based (JWT Bearer Token Authentication) integration with existing enterprise identity providers like Amazon Cognito, Okta, or Microsoft Entra ID. 
When deploying an agent with AWS Identity and Access Management (IAM) authentication, users donâ€™t have to incorporate other Amazon Bedrock AgentCore Identity specific settings or setupâ€”simply configure with IAM authorization, launch, and invoke with the right user credentials. 
When using JWT authentication, you configure the authorizer during the CreateAgentRuntime operation, specifying your identity provider (IdP)-specific discovery URL and allowed clients. Your existing agent code requires no modificationâ€”you simply add the authorizer configuration to your runtime deployment. When a calling entity or user invokes your agent, they pass their IdP-specific access token as a bearer token in the Authorization header. AgentCore Runtime uses AgentCore Identity to automatically validate this token against your configured authorizer and rejects unauthorized requests. The following diagram shows the flow of information between AgentCore runtime, your IdP, AgentCore Identity, other AgentCore services, other AWS services (in orange), and other external APIs or resources (in purple). 
 
Behind the scenes, AgentCore Runtime automatically exchanges validated user tokens for workload access tokens (through the bedrock-agentcore:GetWorkloadAccessTokenForJWT API). This provides secure outbound access to external services through the AgentCore credential provider system, where tokens are cached using the combination of agent workload identity and user ID as the binding key. This cryptographic binding makes sure, for example, User 1â€™s Google token can never be accessed when processing requests for User 2, regardless of application logic errors. Note that in the preceding diagram, connecting to AWS resources can be achieved simply by editing the AgentCore Runtime execution role, but connections to Amazon Bedrock AgentCore Gateway or to another runtime will require reauthorization with a new access token. 
The most straightforward way to configure your agent with OAuth-based inbound access is to use the AgentCore starter toolkit: 
 
 With the AWS Command Line Interface (AWS CLI), follow the prompts to interactively enter your OAuth discovery URL and allowed Client IDs (comma-separated). 
 
 
 
 With Python, use the following code: 
 
 
  bedrock_agentcore_starter_toolkit&nbsp;&nbsp;Runtime
&nbsp;boto3.session&nbsp;&nbsp;Session
boto_session&nbsp;&nbsp;Session()
region&nbsp;&nbsp;boto_sessionregion_name
region

discovery_url&nbsp;&nbsp;'&lt;your-cognito-user-pool-discovery-url&gt;'

client_id&nbsp;&nbsp;'&lt;your-cognito-app-client-id&gt;'

agentcore_runtime&nbsp;&nbsp;Runtime()
response&nbsp;&nbsp;agentcore_runtimeconfigure(
&nbsp; &nbsp; entrypoint"strands_openai.py",
&nbsp; &nbsp; auto_create_execution_role,
&nbsp; &nbsp; auto_create_ecr,
&nbsp; &nbsp; requirements_file"requirements.txt",
&nbsp; &nbsp; regionregion,
&nbsp; &nbsp; agent_nameagent_name,
&nbsp; &nbsp; authorizer_configuration{
&nbsp; &nbsp; &nbsp; &nbsp; "customJWTAuthorizer":&nbsp;{
&nbsp; &nbsp; &nbsp; &nbsp; "discoveryUrl":&nbsp;discovery_url,
&nbsp; &nbsp; &nbsp; &nbsp; "allowedClients":&nbsp;[client_id]
&nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; }
&nbsp; &nbsp; ) 
 
 
 For outbound access (for example, if your agent uses OpenAI APIs), first set up your keys using the API or the Amazon Bedrock console, as shown in the following screenshot. 
 
 
 
 Then access your keys from within your AgentCore Runtime agent code: 
 
 
 from&nbsp;bedrock_agentcore.identity.auth import&nbsp;requires_api_key

@requires_api_key(
&nbsp;&nbsp; &nbsp;provider_name="openai-apikey-provider" # replace with your own credential provider name
)
async def need_api_key(*, api_key: str):
&nbsp;&nbsp; &nbsp;print(f'received api key for async func: {api_key}')
&nbsp;&nbsp; &nbsp;os.environ["OPENAI_API_KEY"] = api_key 
 
For more information on AgentCore Identity, refer to Authenticate and authorize with Inbound Auth and Outbound Auth and Hosting AI Agents on AgentCore Runtime. 
Use AgentCore Runtime state persistence with AgentCore Memory 
AgentCore Runtime provides ephemeral, session-specific state management that maintains context during active conversations but doesnâ€™t persist beyond the session lifecycle. Each user session preserves conversational state, objects in memory, and local temporary files within isolated execution environments. For short-lived agents, you can use the state persistence offered by AgentCore Runtime without needing to save this information externally. However, at the end of the session lifecycle, the ephemeral state is permanently destroyed, making this approach suitable only for interactions that donâ€™t require knowledge retention across separate conversations. 
AgentCore Memory addresses this challenge by providing persistent storage that survives beyond individual sessions. Short-term memory captures raw interactions as events using create_event, storing the complete conversation history that can be retrieved with get_last_k_turns even if the runtime session restarts. Long-term memory uses configurable strategies to extract and consolidate key insights from these raw interactions, such as user preferences, important facts, or conversation summaries. Through retrieve_memories, agents can access this persistent knowledge across completely different sessions, enabling personalized experiences. The following diagram shows how AgentCore Runtime can use specific APIs to interact with Short-term and Long-term memory in AgentCore Memory. 
 
This basic architecture, of using a runtime to host your agents, and a combination of short- and long-term memory has become commonplace in most agentic AI applications today. Invocations to AgentCore Runtime with the same session ID lets you access the agent state (for example, in a conversational flow) as though it were running locally, without the overhead of external storage operations, and AgentCore Memory selectively captures and structures the valuable information worth preserving beyond the session lifecycle. This hybrid approach means agents can maintain fast, contextual responses during active sessions while building cumulative intelligence over time. The automatic asynchronous processing of long-term memories according to each strategy in AgentCore Memory makes sure insights are extracted and consolidated without impacting real-time performance, creating a seamless experience where agents become progressively more helpful while maintaining responsive interactions. This architecture avoids the traditional trade-off between conversation speed and long-term learning, enabling agents that are both immediately useful and continuously improving. 
Process different modalities with large payloads 
Most AI agent systems struggle with large file processing due to strict payload size limits, typically capping requests at just a few megabytes. This forces developers to implement complex file chunking, multiple API calls, or external storage solutions that add latency and complexity. AgentCore Runtime removes these constraints by supporting payloads up to 100 MB in size, enabling agents to process substantial datasets, high-resolution images, audio, and comprehensive document collections in a single invocation. 
Consider a financial audit scenario where you need to verify quarterly sales performance by comparing detailed transaction data against a dashboard screenshot from your analytics system. Traditional approaches would require using external storage such as Amazon Simple Storage Service (Amazon S3) or Google Drive to download the Excel file and image into the container running the agent logic. With AgentCore Runtime, you can send both the comprehensive sales data and the dashboard image in a single payload from the client: 
 
 large_payload = {
"prompt": "Compare the Q4 sales data with the dashboard metrics and identify any discrepancies",
"sales_data": base64.b64encode(excel_sales_data).decode('utf-8'),
"dashboard_image": base64.b64encode(dashboard_screenshot).decode('utf-8')
} 
 
The agentâ€™s entrypoint function can be modified to process both data sources simultaneously, enabling this cross-validation analysis: 
 
 @app.entrypoint
def audit_analyzer(payload, context):
&nbsp;&nbsp; &nbsp;inputs = [
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;{"text": payload.get("prompt", "Analyze the sales data and dashboard")},
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;{"document": {"format": "xlsx", "name": "sales_data", 
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "source": {"bytes": base64.b64decode(payload["sales_data"])}}},
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;{"image": {"format": "png", 
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"source": {"bytes": base64.b64decode(payload["dashboard_image"])}}}
&nbsp;&nbsp; &nbsp;]
&nbsp;&nbsp; &nbsp;
&nbsp;&nbsp; &nbsp;response = agent(inputs)
&nbsp;&nbsp; &nbsp;return response.message['content'][0]['text'] 
 
To test out an example of using large payloads, refer to the following GitHub repo. 
Operate asynchronous multi-hour agents 
As AI agents evolve to tackle increasingly complex tasksâ€”from processing large datasets to generating comprehensive reportsâ€”they often require multi-step processing that can take significant time to complete. However, most agent implementations are synchronous (with response streaming) that block until completion. While synchronous, streaming agents are a common way to expose agentic chat applications to users, users cannot interact with the agent when a task or tool is still running, view the status of, or cancel background operations, or start more concurrent tasks while others have still not completed. 
Building asynchronous agents forces developers to implement complex distributed task management systems with state persistence, job queues, worker coordination, failure recovery, and cross-invocation state management while also navigating serverless system limitations like execution timeouts (tens of minutes), payload size restrictions, and cold start penalties for long-running compute operationsâ€”a significant heavy lift that diverts focus from core functionality. 
AgentCore Runtime alleviates this complexity through stateful execution sessions that maintain context across invocations, so developers can build upon previous work incrementally without implementing complex task management logic. The AgentCore SDK provides ready-to-use constructs for tracking asynchronous tasks and seamlessly managing compute lifecycles, and AgentCore Runtime supports execution times up to 8 hours and request/response payload sizes of 100 MB, making it suitable for most asynchronous agent tasks. 
Getting started with asynchronous agents 
You can get started with just a couple of code changes: 
 
 pip install bedrock-agentcore 
 
To build interactive agents that perform asynchronous tasks, simply call add_async_task when starting a task and complete_async_task when finished. The SDK automatically handles task tracking and manages compute lifecycle for you. 
 
 # Start tracking a task
task_id = app.add_async_task("data_processing")

# Do your work...
# (your business logic here)

# Mark task as complete
app.complete_async_task(task_id) 
 
These two method calls transform your synchronous agent into a fully asynchronous, interactive system. Refer to this sample for more details. 
The following example shows the difference between a synchronous agent that streams back responses to the user immediately vs. a more complex multi-agent scenario where longer running, asynchronous background shopping agents use Amazon Bedrock AgentCore Browser to automate a shopping experience on amazon.com on behalf of the user. 

 
  
 
 
Pay only for Used Resources 
Amazon Bedrock AgentCore Runtime introduces a consumption-based pricing model that fundamentally changes how you pay for AI agent infrastructure. Unlike traditional compute models that charge for allocated resources regardless of utilization, AgentCore Runtime bills you only for what you actually use however long you use it; said differently, you donâ€™t have to pre-allocate resources like CPU or GB Memory, and you donâ€™t pay for CPU resources during I/O wait periods. This distinction is particularly valuable for AI agents, which typically spend significant time waiting for LLM responses or external API calls to complete. Here is a typical Agent event loop, where we only expect the purple boxes to be processed within Runtime: 
 
The LLM call (light blue) and tool call (green) boxes take time, but are run outside the context of AgentCore Runtime; users only pay for processing that happens in Runtime itself (purple boxes). Letâ€™s look at some real-world examples to understand the impact: 
Customer support agent example 
Consider a customer support agent that handles 10,000 user inquiries per day. Each interaction involves initial query processing, knowledge retrieval from Retrieval Augmented Generation (RAG) systems, LLM reasoning for response formulation, API calls to order systems, and final response generation. In a typical session lasting 60 seconds, the agent could actively use CPU for only 18 seconds (30%) while spending the remaining 42 seconds (70%) waiting for LLM responses or API calls to complete. Memory usage can fluctuate between 1.5 GB to 2.5 GB depending on the complexity of the customer query and the amount of context needed. With traditional compute models, you would pay for the full 60 seconds of CPU time and peak memory allocation. With AgentCore Runtime, you only pay for the 18 seconds of active CPU processing and the actual memory consumed moment-by-moment: 
 
 CPU cost: 18 seconds Ã— 1 vCPU Ã— ($0.0895/3600) = $0.0004475
 Memory cost: 60 seconds Ã— 2GB average Ã— ($0.00945/3600) = $0.000315
 Total per session: $0.0007625 
 
For 10,000 daily sessions, this represents a 70% reduction in CPU costs compared to traditional models that would charge for the full 60 seconds. 
Data analysis agent example 
The savings become even more dramatic for data processing agents that handle complex workflows. A financial analysis agent processing quarterly reports might run for three hours but have highly variable resource needs. During data loading and initial parsing, it might use minimal resources (0.5 vCPU, 2 GB memory). When performing complex calculations or running statistical models, it might spike to 2 vCPU and 8 GB memory for just 15 minutes of the total runtime, while spending the remaining time waiting for batch operations or model inferences at much lower resource utilization. By charging only for actual resource consumption while maintaining your session state during I/O waits, AgentCore Runtime aligns costs directly with value creation, making sophisticated agent deployments economically viable at scale. 
Conclusion 
In this post, we explored how AgentCore Runtime simplifies the deployment and management of AI agents. The service addresses critical challenges that have traditionally blocked agent adoption at scale, offering framework-agnostic deployment, true session isolation, embedded identity management, and support for large payloads and long-running, asynchronous agents, all with a consumption based model where you pay only for the resources you use. 
With just four lines of code, developers can securely launch and scale their agents while using AgentCore Memory for persistent state management across sessions. For hands-on examples on AgentCore Runtime covering simple tutorials to complex use cases, and demonstrating integrations with various frameworks such as LangGraph, Strands, CrewAI, MCP, ADK, Autogen, LlamaIndex, and OpenAI Agents, refer to the following examples on GitHub: 
 
 Amazon Bedrock AgentCore Runtime 
 Amazon Bedrock AgentCore Samples: Agentic Frameworks 
 Hosting MCP server on AgentCore Runtime 
 Amazon Bedrock AgentCore Starter Toolkit 
 Runtime QuickStart guide 
 
 
About the authors 
Shreyas Subramanian is a Principal Data Scientist and helps customers by using Generative AI and deep learning to solve their business challenges using AWS services like Amazon Bedrock and AgentCore. Dr. Subramanian contributes to cutting-edge research in deep learning, Agentic AI, foundation models and optimization techniques with several books, papers and patents to his name. In his current role at Amazon, Dr. Subramanian works with various science leaders and research teams within and outside Amazon, helping to guide customers to best leverage state-of-the-art algorithms and techniques to solve business critical problems. Outside AWS, Dr. Subramanian is a expert reviewer for AI papers and funding via organizations like Neurips, ICML, ICLR, NASA and NSF. 
Kosti Vasilakakis is a Principal PM at AWS on the Agentic AI team, where he has led the design and development of several Bedrock AgentCore services from the ground up, including Runtime. He previously worked on Amazon SageMaker since its early days, launching AI/ML capabilities now used by thousands of companies worldwide. Earlier in his career, Kosti was a data scientist. Outside of work, he builds personal productivity automations, plays tennis, and explores the wilderness with his family. 
Vivek Bhadauria is a Principal Engineer at Amazon Bedrock with almost a decade of experience in building AI/ML services. He now focuses on building generative AI services such as Amazon Bedrock Agents and Amazon Bedrock Guardrails. In his free time, he enjoys biking and hiking.
â€¢ PwC and AWS Build Responsible AI with Automated Reasoning on Amazon Bedrock
  This is a guest post co-written with Scott Likens, Ambuj Gupta, Adam Hood, Chantal Hudson, Priyanka Mukhopadhyay, Deniz Konak Ozturk, and Kevin Paul from PwC 
Organizations are deploying generative AI solutions while balancing accuracy, security, and compliance. In this globally competitive environment, scale matters less, speed matters more, and innovation matters most of all, according to recent PwC 2025 business insights on AI agents. To maintain a competitive advantage, organizations must support rapid deployment and verifiable trust in AI outputs. Particularly within regulated industries, mathematical verification of results can transform the speed of innovation from a potential risk into a competitive advantage. 
This post presents how AWS and PwC are developing new reasoning checks that combine deep industry expertise with Automated Reasoning checks in Amazon Bedrock Guardrails to support innovation. Automated Reasoning is a branch of AI focused on algorithmic search for mathematical proofs. Automated Reasoning checks in Amazon Bedrock Guardrails, which encode knowledge into formal logic to validate if large language models (LLM) outputs are possible, are generally available as of August 6, 2025. 
This new guardrail policy maintains accuracy within defined parameters, unlike traditional probabilistic reasoning methods. The system evaluates AI-generated content against rules derived from policy documents, including company guidelines and operational standards. Automated Reasoning checks produce findings that provide insights into whether the AI-generated content aligns with the rules extracted from the policy, highlights ambiguity that exists in the content, and provides suggestions on how to remove assumptions. 
â€œIn a field where breakthroughs are happening at incredible speed, reasoning is one of the most important technical advances to help our joint customers succeed in generative AI,â€ says Matt Wood, Global CTIO at PwC, at AWS Re:Invent 2024. 
Industry-transforming use cases using Amazon Bedrock Automated Reasoning checks 
The strategic alliance combining PwCâ€™s proven, deep expertise and the innovative technology from AWS is set to transform how businesses approach AI-driven innovation. The following diagram illustrates PWCâ€™s Automated Reasoning implementation. We initially focus on highly regulated industries such as pharmaceuticals, financial services, and energy. 
 
In the following sections, we present three groundbreaking use cases developed by PwC teams. 
EU AI Act compliance for financial services risk management 
The European Union (EU) AI Act requires organizations to classify and verify all AI applications according to specific risk levels and governance requirements. PwC has developed a practical approach to address this challenge using Automated Reasoning checks in Amazon Bedrock Guardrails, which transforms EU AI Act compliance from a manual burden into a systematic, verifiable process. Given a description of an AI applicationâ€™s use case, the solution converts risk classification criteria into defined guardrails, enabling organizations to consistently assess and monitor AI applications while supporting expert human judgment through automated compliance verification with auditable artifacts.The key benefits of using Automated Reasoning checks include: 
 
 Automated classification of AI use cases into risk categories 
 Verifiable logic trails for AI-generated classifications 
 Enhanced speed in identifying the required governance controls 
 
The following diagram illustrates the workflow for this use case. 
 
Pharmaceutical content review 
PwCâ€™s Regulated Content Orchestrator (RCO) is a globally scalable, multi-agent capabilityâ€”powered by a core rules engine customized to company, region, product, and indication for useâ€”that automates medical, legal, regulatory, and brand compliance. The RCO team was an early incubating collaborator of Amazon Bedrock Automated Reasoning checks, implementing it as a secondary validation layer in the marketing content generation process. This enhanced defense strengthened existing content controls, resulting in accelerated content creation and review processes while enhancing compliance standards.Key benefits of Automated Reasoning Checks in Amazon Bedrock Guardrails include: 
 
 Applies automated, mathematically based safeguards for verifying RCOâ€™s analysis 
 Enables transparent QA with traceable, audit-ready reasoning 
 Safeguards against potentially unsupported or hallucinated outputs 
 
The following diagram illustrates the workflow for this use case. 
 
Utility outage management for real-time decision support 
Utility outage management applies Automated Reasoning checks in Amazon Bedrock Guardrails to enhance response times and operational efficiency of utility companies. The solution can generate standardized protocols from regulatory guidelines, creates procedures based on NERC and FERC requirements, and verifies AI-produced outage classifications. Through an integrated cloud-based architecture, this solution applies severity-based verification workflows to dispatch decisionsâ€”normal outages (3-hour target) assign tickets to available crews, medium severity (6-hour target) triggers expedited dispatch, and critical incidents (12-hour target) activate emergency procedures with proactive messaging. 
The key benefits of using Automated Reasoning checks include: 
 
 Effective and enhanced responses to customers 
 Real-time operational insights with verified regulatory alignment 
 Accelerated decision-making with mathematical certainty 
 
The following diagram illustrates the workflow for this use case. 
 
Looking ahead 
As the adoption of AI continues to evolve, particularly with agentic AI, the AWS and PwC alliance is focused on the following: 
 
 Expanding Automated Reasoning checks integrated solutions across more industries 
 Developing industry-specific agentic AI solutions with built-in compliance verification 
 Enhancing explainability features to provide greater transparency 
 
Conclusion 
The integration of Automated Reasoning checks in Amazon Bedrock Guardrails with PwCâ€™s deep industry expertise offers a powerful avenue to help deploy AI-based solutions. As an important component of responsible AI, Automated Reasoning checks provides safeguards that help improve the trustworthiness of AI applications. With the expectation of mathematical certainty and verifiable trust in AI outputs, organizations can now innovate without compromising on accuracy, security, or compliance. To learn more about how Automated Reasoning checks works, refer to Minimize AI hallucinations and deliver up to 99% verification accuracy with Automated Reasoning checks: Now available and Improve accuracy by adding Automated Reasoning checks in Amazon Bedrock Guardrails. 
Explore how Automated Reasoning checks in Amazon Bedrock can improve the trustworthiness of your generative AI applications. To learn more about using this capability or to discuss custom solutions for your specific needs, contact your AWS account team or an AWS Solutions Architect. Contact the PwC team to learn how you can use the combined power of AWS and PwC to drive innovation in your industry. 
 
About the authors 
Nafi Diallo&nbsp;is a Senior Automated Reasoning Architect at Amazon Web Services, where she advances innovations in AI safety and Automated Reasoning systems for generative AI applications. Her expertise is in formal verification methods, AI guardrails implementation, and helping global customers build trustworthy and compliant AI solutions at scale. She holds a PhD in Computer Science with research in automated program repair and formal verification, and an MS in Financial Mathematics from WPI. 
Adewale Akinfaderin&nbsp;is a Sr. Data Scientistâ€“Generative AI, Amazon Bedrock, where he contributes to cutting edge innovations in foundational models and generative AI applications at AWS. His expertise is in reproducible and end-to-end AI/ML methods, practical implementations, and helping global customers formulate and develop scalable solutions to interdisciplinary problems. He has two graduate degrees in physics and a doctorate in engineering. 
 Bharathi Srinivasan is a Generative AI Data Scientist at the AWS Worldwide Specialist Organization. She works on developing solutions for Responsible AI, focusing on algorithmic fairness, veracity of large language models, and explainability. Bharathi guides internal teams and AWS customers on their responsible AI journey. She has presented her work at various learning conferences. 
Dan Spillane, a Principal at Amazon Web Services (AWS), leads global strategic initiatives in the Consulting Center of Excellence (CCOE). He works with customers and partners to solve their critical business challenges using innovative technologies. Dan specializes in generative AI and responsible AI, including automated reasoning. He applies these tools to deliver measurable business value at scale. As a lifelong learner, Dan actively studies global cultures and business mechanisms, which enhances his ability to mentor others and drive cross-cultural initiatives. 
Aartika Sardana Chandras is a Senior Product Marketing Manager for AWS Generative AI solutions, with a focus on Amazon Bedrock. She brings over 15 years of experience in product marketing, and is dedicated to empowering customers to navigate the complexities of the AI lifecycle. Aartika is passionate about helping customers leverage powerful AI technologies in an ethical and impactful manner. 
Rama Lankalapalli is a Senior Partner Solutions Architect (PSA) at AWS where he leads a global team of PSAs supporting PwC, a major global systems integrator. Working closely with PwCâ€™s global practice he champions enterprise cloud adoption by leveraging the breadth and depth of AWS services across migrations, modernization, security, AI/ML, and analytics. Rama architects scalable solutions that help organizations accelerate their digital transformation while delivering measurable business outcomes. His leadership combines deep technical expertise with strategic insight to drive customer success through innovative, industry-specific cloud solutions. 
Scott Likens serves as the Chief AI Engineer over Global and US teams at PwC and leads the AI Engineering and Emerging Technology R&amp;D teams domestically, driving the firmâ€™s strategy around AI, Blockchain, VR, Quantum Computing, and other disruptors. With over 30 years of emerging technology expertise, he has helped clients transform customer experience, digital strategy, and operations for various industries. 
Ambuj Gupta&nbsp;is a Director in PwCâ€™s&nbsp;AI and Digital Contacts &amp; Service&nbsp;practice, based in&nbsp;Chicago. With over&nbsp;15 years of experience, Ambuj brings deep expertise in&nbsp;Artificial Intelligence,&nbsp;Agentic and Generative AI,&nbsp;Digital Contact Solutions, and&nbsp;Cloud Innovation&nbsp;across a broad spectrum of platforms and industries. He is recognized for driving&nbsp;strategic transformation&nbsp;through&nbsp;Cloud Native AI Automation&nbsp;and emerging technologiesâ€”including&nbsp;GenAI-powered agents,&nbsp;Intelligent Agent Assists, and&nbsp;Customer Data Platformsâ€”to enhance channel performance and employee effectiveness. 
Adam Hood is a Partner and AWS Data and AI Leader at PwC US. As a strategic and results-oriented technology leader, Adam specializes in driving enterprise-wide transformation and unlocking business value through the strategic application of digital systems, data, and GenAI/AI/ML including building agentic workflows. With a track record of success in industry and consulting, he has guided organizations through complex digital, finance, and ERP modernizations, from initial strategy and business case development to seamless execution and global rollout. 
Chantal Hudson is a Manager in PwC UKâ€™s AI and Modelling team. She has been with PwC for just over five years, starting her career in the South African firm. Chantal works primarily with large banks on credit risk modelling, and is particularly interested in the application of AI applying AI to advance modelling practices. 
Priyanka Mukhopadhyay is a Manager in PwCâ€™s Cloud and Digital Engineering practice. She is an AWS Certified Solution Architect â€“ Associate with over 13 years of experience in Data Engineering. Over the past decade, she has honed her expertise in AWS services and has more than 12 years of experience in developing and delivering robust projects following Agile Methodologies. 
Deniz Konak Ozturk is a Senior Manager within PwCâ€™s AI &amp;Modelling team. She has around 15 years of experience in AI/Gen AI and traditional model development, implementation and validation across UK and EU/non-EU territories and compliance assessment with EU regulations as well as IFRS9 audits. Over the past 6 years, her focus has been primarily on AI/Gen AI, highlighted by her involvement in AI Validation framework development, implementation of this framework into different clients, product management for an automated ML platform, and leading research and product ownership in an R&amp;D initiative on Alternative Data Usage for ML based Risk Models targeting the financially underserved segment. 
Kevin Paul is a Director within the AI Engineering group at PwC. He specializes in Applied AI, and has extensive of experience across the AI lifecycle, building and maintaining solutions across industries.
â€¢ How Amazon scaled Rufus by building multi-node inference using AWS Trainium chips and vLLM
  At Amazon, our team builds Rufus, a generative AI-powered shopping assistant that serves millions of customers at immense scale. However, deploying Rufus at scale introduces significant challenges that must be carefully navigated. Rufus is powered by a custom-built large language model (LLM). As the modelâ€™s complexity increased, we prioritized developing scalable multi-node inference capabilities that maintain high-quality interactions while delivering low latency and cost-efficiency. 
In this post, we share how we developed a multi-node inference solution using Amazon Trainium and vLLM, an open source library designed for efficient and high-throughput serving of LLMs. We also discuss how we built a management layer on top of Amazon Elastic Container Service (Amazon ECS) to host models across multiple nodes, facilitating robust, reliable, and scalable deployments. 
Challenges with multi-node inference 
As our Rufus model grew bigger in size, we needed multiple accelerator instances because no single chip or instance had enough memory for the entire model. We first needed to engineer our model to be split across multiple accelerators. Techniques such as tensor parallelism can be used to accomplish this, which can also impact various metrics such as time to first token. At larger scale, the accelerators on a node might not be enough and require you to use multiple hosts or nodes. At that point, you must also address managing your nodes as well as how your model is sharded across them (and their respective accelerators). We needed to address two major areas: 
 
 Model performance â€“ Maximize compute and memory resources utilization across multiple nodes to serve models at high throughput, without sacrificing low latency. This includes designing effective parallelism strategies and model weight-sharding approaches to partition computation and memory footprint both within the same node and across multiple nodes, and an efficient batching mechanism that maximizes hardware resource utilization under dynamic request patterns. 
 Multi-node inference infrastructure â€“ Design a containerized, multi-node inference abstraction that represents a single model running across multiple nodes. This abstraction and underlying infrastructure needs to support fast inter-node communication, maintain consistency across distributed components, and allow for deployment and scaling as a single, deployable unit. In addition, it must support continuous integration to allow rapid iteration and safe, reliable rollouts in production environments. 
 
Solution overview 
Taking these requirements into account, we built multi-node inference solution designed to overcome the scalability, performance, and reliability challenges inherent in serving LLMs at production scale using tens of thousands of TRN1 instances. 
To create a multi-node inference infrastructure, we implemented a leader/follower multi-node inference architecture in vLLM. In this configuration, the leader node uses vLLM for request scheduling, batching, and orchestration, and follower nodes execute distributed model computations. Both leader and follower nodes share the same NeuronWorker implementation in vLLM, providing a consistent model execution path through seamless integration with the AWS Neuron SDK. 
To address how we split the model across multiple instances and accelerators, we used hybrid parallelism strategies supported in the Neuron SDK. Hybrid parallelism strategies such as tensor parallelism and data parallelism are selectively applied to maximize cross-node compute and memory bandwidth utilization, significantly improving overall throughput. 
Being aware of how the nodes are connected is also important to avoid latency penalties. We took advantage of network topology-aware node placement. Optimized placement facilitates low-latency, high-bandwidth cross-node communication using Elastic Fabric Adapter (EFA), minimizing communication overhead and improving collective operation efficiency. 
Lastly, to manage models across multiple nodes, we built a multi-node inference unit abstraction layer on Amazon ECS. This abstraction layer supports deploying and scaling multiple nodes as a single, cohesive unit, providing robust and reliable large-scale production deployments. 
By combining a leader/follower orchestration model, hybrid parallelism strategies, and a multi-node inference unit abstraction layer built on top of Amazon ECS, this architecture deploys a single model replica to run seamlessly across multiple nodes, supporting large production deployments.In the following sections, we discuss the architecture and key components of the solution in more detail. 
Inference engine design 
We built an architecture on Amazon ECS using Trn1 instances that supports scaling inference beyond a single node to fully use distributed hardware resources, while maintaining seamless integration with NVIDIA Triton Inference Server, vLLM, and the Neuron SDK. 
Although the following diagram illustrates a two-node configuration (leader and follower) for simplicity, the architecture is designed to be extended to support additional follower nodes as needed. 
 
In this architecture, the leader node runs the Triton Inference Server and vLLM engine, serving as the primary orchestration unit for inference. By integrating with vLLM, we can use continuous batchingâ€”a technique used in LLM inference to improve throughput and accelerator utilization by dynamically scheduling and processing inference requests at the token level. The vLLM scheduler handles batching based on the global batch size. It operates in a single-node context and is not aware of multi-node model execution. After the requests are scheduled, theyâ€™re handed off to the NeuronWorker component in vLLM, which handles broadcasting model inputs and executing the model through integration with the Neuron SDK. 
The follower node operates as an independent process and acts as a wrapper around the vLLM NeuronWorker component. It continuously listens to model inputs broadcasted from the leader node and executes the model using the Neuron runtime in parallel with the leader node. 
For nodes to communicate with each other with the proper information, two mechanisms are required: 
 
 Cross-node model inputs broadcasting on CPU â€“ Model inputs are broadcasted from the leader node to follower nodes using the torch.distributed communication library with the Gloo backend. A distributed process group is initialized during NeuronWorker initialization on both the leader and follower nodes. This broadcast occurs on CPU over standard TCP connections, allowing follower nodes to receive the full set of model inputs required for model execution. 
 Cross-node collectives communication on Trainium chips â€“ During model execution, cross-node collectives (such as all gather or all reduce) are managed by the Neuron Distributed Inference (NxDI) library, which uses EFA to deliver high-bandwidth, low-latency inter-node communication. 
 
Model parallelism strategies 
We adopted hybrid model parallelism strategies through integration with the Neuron SDK to maximize cross-node memory bandwidth utilization (MBU) and model FLOPs utilization (MFU), while also reducing memory pressure on each individual node. For example, during the context encoding (prefill) phase, we use context parallelism by splitting inputs along the sequence dimension, facilitating parallel computation of attention layers across nodes. In the decoding phase, we adopt data parallelism by partitioning the input along the batch dimension, so each node can serve a subset of batch requests independently. 
Multi-node inference infrastructure 
We also designed a distributed LLM inference abstraction: the multi-node inference unit, as illustrated in the following diagram. This abstraction serves as a unit of deployment for inference service, supporting consistent and reliable rolling deployments on a cell-by-cell basis across the production fleet. This is important so you only have a minimal number of nodes offline during upgrades without impacting your entire service. Both the leader and follower nodes described earlier are fully containerized, so each node can be independently managed and updated while maintaining a consistent execution environment across the entire fleet. This consistency is critical for reliability, because the leader and follower nodes must run with identical software stacksâ€”including Neuron SDKs, Neuron drivers, EFA software, and other runtime dependenciesâ€”to achieve correct and reliable multi-node inference execution. The inference containers are deployed on Amazon ECS. 
 
A crucial aspect of achieving high-performance distributed LLM inference is minimizing the latency of cross-node collective operations, which rely on Remote Direct Memory Access (RDMA). To enable this, optimized node placement is essential: the deployment management system must compose a cell by pairing nodes based on their physical location and proximity. With this optimized placement, cross-node operations can utilize the high-bandwidth, low-latency EFA network available to instances. The deployment management system gathers this information using the Amazon EC2 DescribeInstanceTopology API to pair nodes based on their underlying network topology. 
To maintain high availability for customers (making sure Rufus is always online and ready to answer a question), we developed a proxy layer positioned between the systemâ€™s ingress or load-balancing layer and the multi-node inference unit. This proxy layer is responsible for continuously probing and reporting the health of all worker nodes. Rapidly detecting unhealthy nodes in a distributed inference environment is critical for maintaining availability because it makes sure the system can immediately route traffic away from unhealthy nodes and trigger automated recovery processes to restore service stability. 
The proxy also monitors real-time load on each multi-node inference unit and reports it to the ingress layer, supporting fine-grained, system-wide load visibility. This helps the load balancer make optimized routing decisions that maximize per-cell performance and overall system efficiency. 
Conclusion 
As Rufus continues to evolve and become more capable, we must continue to build systems to host our model. Using this multi-node inference solution, we successfully launched a much larger model across over tens of thousands of AWS Trainium chips to Rufus customers, supporting Prime Day traffic. This increased model capacity has enabled new shopping experiences and significantly improved user engagement. This achievement marks a major milestone in pushing the boundaries of large-scale AI infrastructure for Amazon, delivering a highly available, high-throughput, multi-node LLM inference solution at industry scale. 
AWS Trainium in combination with solutions such as NVIDIA Triton and vLLM can help you enable large inference workloads at scale with great cost performance. We encourage you to try these solutions to host large models for your workloads. 
 
About the authors 
James Park is a ML Specialist Solutions Architect at Amazon Web Services. He works with Amazon.com to design, build, and deploy technology solutions on AWS, and has a particular interest in AI and machine learning. In his spare time he enjoys seeking out new cultures, new experiences, and staying up to date with the latest technology trends. 
Faqin Zhong&nbsp;is a Software Engineer at Amazon Stores Foundational AI, working on LLM inference infrastructure and optimizations. Passionate about generative AI technology, Faqin collaborates with leading teams to drive innovation, making LLMs more accessible and impactful, and ultimately enhancing customer experiences across diverse applications. Outside of work, she enjoys cardio exercise and baking with her son. 
Charlie Taylor&nbsp;is a Senior Software Engineer within Amazon Stores Foundational AI, focusing on developing distributed systems for high performance LLM inference. He builds inference systems and infrastructure to help larger, more capable models respond to customers faster. Outside of work, he enjoys reading and surfing. 
Yang Zhou&nbsp;is a Software Engineer working on building and optimizing machine learning systems. His recent focus is enhancing the performance and cost-efficiency of generative AI inference. Beyond work, he enjoys traveling and has recently discovered a passion for running long distances. 
Nicolas Trown&nbsp;is a Principal Engineer in Amazon Stores Foundational AI. His recent focus is lending his systems expertise across Rufus to aid the Rufus Inference team and efficient utilization across the Rufus experience. Outside of work, he enjoys spending time with his wife and taking day trips to the nearby coast, Napa, and Sonoma areas. 
Michael Frankovich&nbsp;is a Principal Software Engineer at Amazon Core Search, where he supports the ongoing development of their cellular deployment management system used to host Rufus, among other search applications. Outside of work, he enjoys playing board games and raising chickens. 
Adam (Hongshen) Zhao is a Software Development Manager at Amazon Stores Foundational AI. In his current role, Adam is leading the Rufus Inference team to build generative AI inference optimization solutions and inference system at scale for fast inference at low cost. Outside of work, he enjoys traveling with his wife and creating art. 
Bing Yin is a Director of Science at Amazon Stores Foundational AI. He leads the effort to build LLMs that are specialized for shopping use cases and optimized for inference at Amazon scale. Outside of work, he enjoys running marathon races. 
Parthasarathy Govindarajen is Director of Software Development at Amazon Stores Foundational AI. He leads teams that develop advanced infrastructure for large language models, focusing on both training and inference at scale. Outside of work, he spends his time playing cricket and exploring new places with his family.
â€¢ Build an intelligent financial analysis agent with LangGraph and Strands Agents
  Agentic AI is revolutionizing the financial services industry through its ability to make autonomous decisions and adapt in real time, moving well beyond traditional automation. Imagine an AI assistant that can analyze quarterly earnings reports, compare them against industry expectations, and generate insights about future performance. This seemingly straightforward task involves multiple complex steps: document processing, data extraction, numerical analysis, context integration, and insight generation. 
Financial analysis workflows present unique technical challenges for generative AI that push the boundaries of traditional large language model (LLM) implementations. This domain requires architectural patterns designed to handle the inherent complexities of financial workflows to assist analysts. Although agentic AI systems drive substantial improvements in operational efficiency and customer experience, delivering measurable productivity gains across operations, they also present unique implementation challenges around governance, data privacy, and regulatory compliance. Financial institutions must carefully balance the transformative potential of agentic AIâ€”from dynamic financial coaching to real-time risk assessmentâ€”with the need for robust oversight and control frameworks. 
This post describes an approach of combining three powerful technologies to illustrate an architecture that you can adapt and build upon for your specific financial analysis needs: LangGraph for workflow orchestration, Strands Agents for structured reasoning, and Model Context Protocol (MCP) for tool integration. 
The following screenshot figure demonstrates how this solution operates in practice: 
 
The reference architecture discussed in this post emerged from experimenting with different patterns for financial domain applications. We hope these insights help you navigate similar challenges in your own projects, whether in finance or other complex analytical domains. 
Understanding the challenges in financial analysis workflows 
Before diving into the solution implementation details, itâ€™s worth understanding the core challenges that informed our architectural decisions. These challenges arenâ€™t unique to our project, theyâ€™re inherent to the nature of financial analysis and appear in many complex analytical applications.Our first challenge involved dynamic and adaptive analysis flows. Financial analysis workflows are inherently dynamic, with analysts constantly adjusting their approach based on observed patterns and intuition. An analyst might shift focus from revenue analysis to operational metrics, or dive deeper into specific industry segments based on emerging insights. This requires an orchestration strategy that can handle flexible, nonlinear execution paths while maintaining analytical coherence and context throughout the process.In our second challenge, we were faced with complex integration across multiple data sources. Financial analysis requires seamless integration with various internal and external systems, from proprietary databases to public industry data APIs. Each integration point introduces potential compatibility issues and architectural complexity. The challenge lies in maintaining manageable system coupling while enabling access to diverse data sources, each with their own interfaces, authentication methods, and data formats. A robust integration strategy is needed to keep system complexity at a sustainable level while maintaining reliable data access across sources. 
Solution overview 
To address these challenges, we developed an architectural pattern combining three complementary technologies: 
 
 LangGraph â€“ Provides the foundation for handling dynamic analysis flows through structured workflow orchestration, enabling flexible execution paths while maintaining state and context 
 Strands Agents â€“ Serves as an intermediary layer, coordinating between foundation models (FMs) and specialized tools to execute complex analytical tasks 
 MCP â€“ Standardizes the integration of diverse data sources and tools, simplifying the complexity of connecting to multiple financial systems and services. 
 
This pattern is illustrated in the following diagram. 
 
The combination of these frameworks serves distinct but complementary purposes in the architecture. LangGraph excels at orchestrating high-level workflows and directional processes, and Strands Agents optimizes autonomous agent interactions and decision-making at a more granular level. We used this dual-framework approach to make the most of the strengths of each technology: LangGraphâ€™s structured workflow capabilities for macro-level orchestration, and the specialized handling by Strands Agents of agent-tool interactions. Rather than being redundant, this architecture creates a more robust and flexible system capable of handling both complex workflow management and intelligent agent operations effectively. This modular, maintainable system can handle the complexity of financial analysis while remaining flexible and extensible. 
LangGraph: Structuring financial analysis workflows 
One effective design principle when implementing multi-agent systems is breaking down complex problems into simpler tasks. Instead of asking an agent to solve an entire financial problem at one time, consider decomposing it into discrete analytical steps.Consider the following example. When a user wants to analyze the performance of company A compared to company B, the process works as follows: 
 
 The user submits a natural language query: â€œCompare the quarterly revenue growth of Company A and Company B for the past year and explain whatâ€™s driving the differences.â€ 
 The router node analyzes this query and determines that it requires financial data retrieval followed by comparative analysis. It routes the request to the agent specialized for financial analysis. 
 The specialized agent processes the request and generates a comprehensive response that addresses the userâ€™s query. (The specific tools and reasoning process that the agent uses to generate its response will be covered in the next section.) 
 
This workflow is illustrated in the following diagram. 
 
In a multi-agent architecture design, LangGraph provides powerful workflow orchestration capabilities that align well with the dynamic nature of financial analysis. It supports both directional and recursive workflows, so you can model everything from straightforward sequential processes to complex iterative analyses that evolve based on intermediate findings.A key strength of LangGraph is how it combines flexible workflow patterns with precise programmatic control. Rather than leaving all flow decisions to LLM reasoning, you can implement specific business logic to guide the analysis process. For instance, our architectureâ€™s router node can direct requests to specialized agents based on concrete criteria, determining whether a request requires file processing, numerical analysis, or industry data integration.Additionally, LangGraphâ€™s GraphState primitive elegantly solves the challenge of maintaining context across distributed agent nodes. This shared state mechanism makes sure that each agent in the workflow can access and build upon previous analysis results, minimizing redundant processing while maintaining analytical coherence throughout the entire process. 
Strands Agents: Orchestrating financial reasoning 
While LangGraph manages the overall workflow, Strands Agent handles the specialized reasoning and tool usage within each node. When the agent receives the request to compare companies, it first identifies the specific data needed: 
 
 Initial reasoning â€“ The agent determines that it needs quarterly revenue figures for both companies, year-over-year growth percentages, and industry benchmarks for context 
 Data retrieval â€“ The agent retrieves financial news mentioning both companies and pulls fundamental metrics such as quarterly revenue data and industry averages 
 Analysis and synthesis â€“ The agent analyzes growth trends, identifies correlations between news events and performance changes, and synthesizes findings into a comprehensive analysis explaining not only the revenue differences but also potential driving factors 
 
This reasoning-tool execution cycle, shown in the following diagram, allows the agent to dynamically gather information and refine its analysis as needed. 
 
For a different use case, consider a request to â€œCreate a Word document summarizing the financial performance of Amazon for investorsâ€: 
 
 The documentation agent uses a document generation tool to create a Word document with proper formatting 
 The agent structures the content with appropriate sections, such as â€œExecutive Summary,â€ â€œRevenue Analysis,â€ and â€œIndustry Positionâ€ 
 Using specialized formatting tools, it creates data tables for quarterly comparisons, bullet points for key insights, and embedded charts visualizing performance trends 
 Finally, it delivers a professionally formatted document ready with download links 
 
Strands agents are designed to flexibly iterate through their specialized tasks using the tools provided to them, gathering necessary information for comprehensive responses. Itâ€™s crucial to connect these agents with appropriate tools that match the nature of their assigned tasksâ€”whether itâ€™s financial data analysis tools, document generation capabilities, or other specialized functionalities. This alignment between agent capabilities and their toolsets facilitates optimal performance in executing their designated responsibilities. 
MCP: Interaction with financial tools 
The MCP provides a foundation for creating extensible, standardized financial tools. Rather than building monolithic applications where M clients need to connect to N servers (creating MÃ—N integration complexity), MCP standardizes the communication protocol, reducing this to M+N connections. This creates a modular system where financial analysts can focus on creating specialized tools, agent developers can concentrate on reasoning and orchestration, and new capabilities can be added without modifying existing components. 
Modular server architecture 
Our architecture uses specialized MCP servers that provide focused financial capabilities. Each server exposes self-documenting tools through the standardized MCP protocol: 
 
 Stock analysis â€“ Real-time quotes and historical market data from Yahoo Finance 
 Financial analysis â€“ Combines fundamental metrics, such as price-to-earnings (P/E) ratios and revenue growth, with technical indicators, such as relative strength index (RSI) and moving average convergence/divergence (MACD) for investment recommendations 
 Web and news search â€“ Aggregates sentiment from news sources and social media with theme extraction 
 
These capabilities are shown in the following diagram. 
 
Hereâ€™s how a typical MCP tool is structured: 
 
 @mcp.tool()
async def comprehensive_analysis(equity: str) -&gt; str:
    """
    Get complete investment analysis combining both fundamental and technical factors.
    Provides a holistic view of a stock with interpreted signals, valuation assessment,
    growth metrics, financial health indicators, and momentum analysis with clear buy/sell signals.
    
    Args:
        equity: Stock ticker symbol (e.g., AAPL, MSFT, TSLA)
    """
    try:
        data = await fetch_comprehensive_analysis(equity)
        return format_analysis_results(data)
    except Exception as e:
        return f"Error retrieving comprehensive analysis: {str(e)}" 
 
Client-side integration 
The Strands Agents MCP client connects to available servers and provides unified tool access. However, because individual MCP servers can expose multiple tools, having too many tools in context can make the agentâ€™s reasoning process inefficient. To address this, our implementation provides flexible server selection, so users can connect only to relevant MCP servers based on their analysis needs: 
 
 # Server name mapping for easy selection
SERVER_NAME_TO_URL = {
    "word": "http://localhost:8089/mcp",
    "stock": "http://localhost:8083/mcp", 
    "financial": "http://localhost:8084/mcp",
    "news": "http://localhost:8085/mcp"
}

async def execute_financial_analysis(state: GraphState) -&gt; GraphState:
    # Select relevant servers based on analysis type
    selected_servers = [
        SERVER_NAME_TO_URL["stock"],
        SERVER_NAME_TO_URL["financial"],
        SERVER_NAME_TO_URL["news"]  # Skip document generation for basic analysis
    ]
    
    mcp_clients, all_tools = await create_mcp_clients(selected_servers)
    
    # Create agent with focused tool access
    agent = Agent(
        model=model,
        tools=all_tools,  # Only tools from selected servers
        system_prompt=FINANCIAL_SYSTEM_PROMPT
    ) 
 
This selective approach significantly improves analysis quality by reducing tool context noise and helping the agent focus on relevant capabilities. The implementation also supports connecting to individual servers when specific functionality is needed: 
 
 # Connect to a single server for specialized tasks
financial_client = get_mcp_client("financial")
available_tools = get_all_available_tools()  # Discover all tools across servers 
 
Although our examples use localhost endpoints for development, MCPâ€™s streamable HTTP protocol enables seamless connection to remote servers by simply updating the URL mappings: 
 
 # Production setup with remote MCP servers
PRODUCTION_SERVER_URLS = {
    "stock": "https://stock-analysis-api.company.com/mcp",
    "financial": "https://financial-analysis-engine.company.com/mcp",
    "news": "https://news-sentiment-service.company.com/mcp"
} 
 
The following is an example request that asks, â€œAnalyze Amazon financial performance and market position for Q3 2025.â€ 
The workflow is as follows: 
 
 LangGraph receives the request and routes to the financial_analysis node 
 MCP servers provide the necessary tools: 
   
   Stock server â€“ yahoo_stock_quote() for current prices 
   Financial server â€“ comprehensive_analysis() for detailed metrics 
   News server â€“ get_market_sentiment() for recent sentiment data 
    
 Strands Agents orchestrates the analysis and returns the final answer about the financial performance and market position of Amazon in Q3 2025: 
   
   Stock server â€“ Current price: $220, P/E: 47.8, market cap: $1.6T 
   Financial server â€“ Revenue growth: +11.2% YoY, AWS: +19% YoY, operating margin: 7.8% 
   News server â€“ Sentiment: Positive (0.74), key themes: AI investments, logistics efficiency, holiday retail prep 
    
 
The agent returns the following output: 

 Amazon demonstrates strong performance across key segments in Q3 2025. AWS continues robust growth at +19% YoY, driving improved operating margins to 7.8%. The companyâ€™s AI investments are positioning it well for future growth, while logistics improvements support retail competitiveness heading into holiday season. Overall sentiment remains positive despite some margin pressure from continued infrastructure investments.
 
Deploy the financial analysis agent 
The deployment of our financial analysis agent follows two main paths: local development for testing and iteration, followed by production deployment options for real-world use. This section focuses on getting the application running in your development environment, with production deployment covered separately. 
Local deployment enables rapid iteration and testing of your financial analysis workflows before moving to production. Our application uses Amazon Bedrock for its foundational AI capabilities, powering the Strands Agents components. Make sure your AWS account has Amazon Bedrock access enabled with appropriate model permissions for the models specified in your configuration. The solution can be found on GitHub in the sample-agentic-frameworks-on-aws repository. Follow these steps: 
Development environment deployment 
 
 To set up your development environment, clone the repository and install dependencies for both frontend and backend components: 
 
 
 # Clone the repository
git clone https://github.com/your-organization/financial-agent.git
cd financial-agent

# Install frontend dependencies
npm install 
 
 
 To set up the Python backend, install the Python dependencies required for the backend services: 
 
 
 # Navigate to the backend directory
cd py-backend

# Install dependencies
pip install -r requirements.txt

# Return to the main project directory
cd .. 
 
 
 To launch the application, use the convenient script configured in the project, which launches both the frontend and backend services simultaneously: 
 
 
 # Launch both frontend and backend
npm run dev 
 
This command Starts the Next.js frontend on port 3000 and launches the FastAPI backend on port 8000. 
You can now access the financial analysis agent in your browser at http://localhost:3000. 
 
 To make sure your local deployment is functioning correctly, 
 
 
 
 To configure the MCP tool integration, on the MCP console under Configuration, choose Tool. You can connect or disconnect necessary tools when you want to change MCP settings. This flexibility means you can: 
   
   Tailor the agentâ€™s capabilities to specific financial analysis tasks 
   Add specialized data sources for industry segments 
   Control which external APIs the agent can access 
   Create custom tool combinations for different analytical scenarios 
    
 
 
This configuration-driven approach embodies the flexible architecture weâ€™ve discussed throughout this article, allowing the financial analysis agent to be adapted to various use cases without code changes. Whether you need stock market analysis, portfolio evaluation, industry news integration, or financial documentation generation, the appropriate combination of MCP tools can be selected through this interface. 
Production deployment 
Local development is sufficient for testing and iteration, but deploying the financial analysis agent to production requires a more robust architecture. This section outlines our production deployment approach, which uses AWS services to create a scalable, resilient, and secure solution. 
The production deployment follows a distributed architecture pattern that separates concerns while maintaining efficient communication paths. As shown in the following diagram, the production architecture consists of: 
 
 Web frontend â€“ A Next.js application deployed on AWS Amplify that provides the user interface for interacting with the financial agent 
 Backend application â€“ A containerized FastAPI service running on Amazon Elastic Container Service (Amazon ECS) that orchestrates the LangGraph workflow and Strands Agents for financial analysis and documentation 
 MCP server farm â€“ A collection of specialized microservices that provide financial tools and data connectors 
 Amazon Bedrock â€“ The FM service that powers the agentsâ€™ reasoning capabilities 
 Amazon DynamoDB â€“ A persistent storage layer for conversation history and state management 
 
This distributed approach enables scalable performance while preserving the fundamental architectural patterns weâ€™ve discussed throughout this post. The deployment strategy can be adapted to your specific organizational requirements and infrastructure preferences. 
Conclusion: Beyond implementation to architectural thinking 
In this post, weâ€™ve demonstrated how combining LangGraph, Strands Agents, and MCP creates a powerful architectural pattern for financial analysis applications. This approach directly addresses the key challenges in financial workflows: 
 
 Decomposition of complex problems â€“ Breaking financial analysis into manageable components with LangGraph allows for focused, accurate reasoning while maintaining overall context 
 Reasoning-tool execution cycles â€“ The separation between reasoning (using Strands Agents) and execution (using MCP) creates a more modular system that mirrors how financial analysts actually work 
 Flexible tool integration â€“ The Strands Agents built-in MCP support enables seamless connection to financial tools and document generation capabilities, allowing continuous expansion of functionality without disrupting the core architecture 
 
This architecture serves as a foundation that you can extend and adapt to your specific financial analysis needs. Whether youâ€™re building portfolio analysis tools, equity research assistants, investment advisory solutions, or financial documentation generators, these patterns provide a robust starting point.This post offers building blocks that you can adapt to your specific financial analysis needs. We hope these architectural insights help you navigate similar challenges in your own projects, whether in finance or other complex analytical domains. 
Next steps 
We invite you to build upon our LangGraph, Strands, and MCP architecture to transform your financial workflows. Whether youâ€™re building for a portfolio manager, equity analyst, risk professional, or financial advisor, you can create specialized tools that connect to your unique data sources. Start with a single MCP server addressing a specific pain point, then gradually expand as you experience the benefits. 
Join our community. Weâ€™re building a community of generative AI practitioners who are pushing the boundaries of whatâ€™s possible with these technologies. Share your tools by contributing to our Agentic Frameworks open source GitHub repository. 
About the Authors 
 
Evan Grenda Sr. GenAI Specialist at AWS, where he works with top-tier third-party foundation model and agentic frameworks providers to develop and execute joint go-to-market strategies, enabling customers to effectively deploy and scale solutions to solve enterprise agentic AI challenges. Evan holds a BA in Business Administration from the University of South Carolina, a MBA from Auburn University, and an MS in Data Science from St. Josephâ€™s University. 
Karan Singh is a Agentic AI leader at AWS, where he works with top-tier third-party foundation model and agentic frameworks providers to develop and execute joint go-to-market strategies, enabling customers to effectively deploy and scale solutions to solve enterprise agentic AI challenges. Karan holds a BS in Electrical Engineering from Manipal University, a MS in Electrical Engineering from Northwestern University, and an MBA from the Haas School of Business at University of California, Berkeley. 
Sayan Chakraborty is a Sr. Solutions Architect at AWS. He helps large enterprises build secure, scalable, and performant solutions in the AWS Cloud. With a background of Enterprise and Technology Architecture, he has experience delivering large scale digital transformation programs across a wide range of industry verticals. He holds a B. Tech. degree in Computer Engineering from Manipal University, Sikkim, India. 
Kihyeon Myung is a Sr. Applied AI Architect at AWS, where he helps enterprise customers build and deploy agent applications and RAG pipelines. With over 3 years of experience in AI and GenAI, Kihyeon specializes in designing and implementing agentic AI systems, combining his development background with expertise in Machine Learning and Generative AI.

â¸»