‚úÖ Morning News Briefing ‚Äì July 23, 2025 10:50

üìÖ Date: 2025-07-23 10:50
üè∑Ô∏è Tags: #briefing #ai #publichealth #digitalgov

‚∏ª

üßæ Weather
‚Ä¢ Current Conditions: Partly Cloudy, 9.6¬∞C
  Temperature: 9.6&deg;C Pressure: 102.2 kPa  Visibility: 24 km Visibility : 24 km Humidity: 95 % Dewpoint: 8.9&deg:C Wind: S 8 km/h Air Quality Health Index: n/a . Observed at: Garrison Petawawa 6:00 AM EDT Wednesday 23 July 2025 Condition:
‚Ä¢ Wednesday: A mix of sun and cloud. High 29.
  Sunny. Becoming a mix of sun and cloud this morning . High 29. Humidex 34. UV index 9 or very high. Sunny. High 29 in the morning, with a high of 34.50 in the afternoon . High of 29.50 is 29.60 in the early morning of July 23, 2025 . Forecast issued 5:00 AM EDT Wednesday 23 July
‚Ä¢ Wednesday night: A few clouds. Low 18.
  Clear. Clear. Becoming partly cloudy overnight. Clear . Clear. Low 18.50/60 . Clear skies . Clear weather forecast for the next few days . Forecast issued 5:00 a.m. Wednesday 23 July 2025. Forecast: Clear, cloudy, sunny, breezy, warm, cloudy and breezy in the coming days. Clear skies. Clear weather.

üåç International News
No updates.

üçÅ Canadian News
No updates.

üá∫üá∏ U.S. Top Stories
‚Ä¢ How China came to rule the world of rare earth elements
  The U.S. once controlled the market on rare earth elements, sought after for a range of technologies . But in the last few decades, China has cornered that market and surpassed the U.N. China has surpassed the United States in the rare earth element market, which is sought after by technology makers . The market for rare earths has grown in China in recent decades .
‚Ä¢ How effective is 'precooling' your home during off-peak hours? It depends
  Do precooling your home work? Yes, no or maybe ‚Äî depending on who you ask . But don't worry, we also have some other tips to keep you cool this summer . Precooling is a good way to stay cool in the summer, according to George Frey, the author of the book, "The Summer of the Summer" Do you know how to get
‚Ä¢ U.S. probes foreign links to agriculture research to protect food supply
  Researchers agree food security is important, but they say scrutiny of foreign collaboration could hurt U.S. innovation . They say scrutiny could hurt innovation in the United States . Food security is a key component of the study, but researchers say it is not necessary to rely on foreign sources to help develop food security in the U.K. Researchers say it could be a problem for U.N
‚Ä¢ Protests are near constant. Do they work?
  The U.S. has been in a constant state of protest in recent years . But some are starting to wonder: How effective are any of those protests? When it comes to achieving lasting social change, do any of them work? Do any of the protests work? We ask: Do you know how effective protests have been? Share your thoughts with CNN iReport.com/Her
‚Ä¢ The U.N.'s highest court will decide on the climate obligations of countries
  After years of lobbying by vulnerable island nations, the U.N. asked the International Court of Justice for an advisory opinion . The advisory opinion is non-binding but important basis for international obligations . The island nations have been lobbying for years to get the opinion from the court of justice . The U.S. is seeking the opinion of the court, which is an important part of international

üß† Artificial Intelligence
No updates.

üíª Digital Strategy
‚Ä¢ Building the Apollo Soyuz Test Project out of LEGO¬Æ bricks
  Lego bricks are being dusted off for a final time ‚Äì at least for the Apollo program ‚Äì to recreate the Apollo Soyuz Test Project via the medium of the plaything . In space, no one can hear you step on a plastic brick in space . The Lego bricks will be used for the final time in a final attempt to recreate a final version of the Soyuz test project .
‚Ä¢ Musk is messing with the Cosmic Dawn. Will alien hunters save the day for all mankind?
  Elon Musk's Starlink is a multi-thousand satellite global internet provider, growing by hundreds of new orbiting relays a month . It's part of intricate geopolitical power games between the Pentagon, the US government, Ukraine, and Musk himself . It is part of an intricate geopolitical game between Pentagon, US government and Ukraine, Musk says . SpaceX is a 'spacey' company that
‚Ä¢ Sh!t happens, so Microsoft is paying biz to flush its carbon sins underground
  Microsoft has signed a contract with a company that will pump shit underground for it in exchange for carbon credits . Organic waste to be pumped out of sight as part of 4.9M-tonne CO2 removal deal . Microsoft will pump organic waste underground for the company's carbon credits in return for carbon dioxide removal . Microsoft signed a deal with the company to pump the waste underground in exchange
‚Ä¢ COVID-19 pandemic accelerated brain aging ‚Äì even if you didn‚Äôt catch the virus
  A longitudinal study of nearly 996 healthy adults found that the COVID-19 pandemic accelerated ageing of their brains . Boffins think worry and stress during the plague years changed our wetware, and that the damage can be reversed . The study found that worry, stress and worry caused the rapid ageing of the brains of those who suffered from the plague, which accelerated their brain ageing
‚Ä¢ Stop flooding us with AI-based grant applications, begs Health Institute
  Already reeling from staff cuts, the health agency is choking on slop ai-pocalypse . The Trump administration will introduce an AI Action Plan on Wednesday to boost the use of AI in government . The US National Institutes of Health (NIH) are pleading for less of it . The agency is already reeling from Staff cuts, and is desperate for less AI to be used in government

üè• Public Health
No updates.

üî¨ Science
‚Ä¢ Maximizing cascade genetic testing for disease prevention through direct notification of at-risk relatives
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Protect the integrity of the US National Institutes of Health
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Large language model-based biological age prediction in large-scale populations
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Role of Allium cepa and Allium sativum extracts in oxidative stress, sperm parameters and histological abnormalities induced by deep-frying oil in testis
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Help save 2 million lives: close the vaccine funding gap
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations

üßæ Government & Policy
No updates.

üèõÔ∏è Enterprise Architecture & IT Governance
No updates.

ü§ñ AI & Emerging Tech
‚Ä¢ Fighting forever chemicals and startup fatigue
  What if we could permanently remove the toxic ‚Äúforever chemicals‚Äù contaminating our water? That‚Äôs the driving force behind Michigan-based startup Enspired Solutions, founded by environmental toxicologist Denise Kay and chemical engineer Meng Wang. The duo left corporate consulting in the rearview mirror to take on one of the most pervasive environmental challenges: PFAS.







&#8220;PFAS is referred to as a forever chemical because it is so resistant to break down,‚Äù says Kay. ‚ÄúIt does not break down naturally in the environment, so it just circles around and around. This chemistry, which would break that cycle and break the molecule apart, could really support the health of all of us.‚Äù



Basing the company in Michigan was both a strategic and a practical strategy. The state has been a leader in PFAS regulation with a startup infrastructure‚Äîbuoyed by the Michigan Economic Development Corporation (MEDC)‚Äîthat helped turn an ambitious vision into a viable business.



From intellectual property analyses to forecasting finances and fundraising guidance, the MEDC‚Äôs programs offered Kay and Wang the resources to focus on building their PFASigator: a machine the size of two large refrigerators that uses ultraviolet light and chemistry to break down PFAS in water. In other words, ‚Äúit essentially eats PFAS.‚Äù



Despite the support from the MEDC, the journey has been far from smooth. &#8220;As people say, being an entrepreneur and running a startup is like a rollercoaster,‚Äù Kay says. ‚ÄúYou have high moments, and you have very low moments when you think nothing&#8217;s ever going to move forward.&#8221;



Without revenue or salaries in the early days, the co-founders had to be sustained by something greater than financial incentive.



&#8220;If problem solving and learning new talents do not provide sufficient intrinsic reward for a founder to be satisfied throughout what I guarantee will be a long duration effort, then that founder may need to reset their expectations. Because the financial rewards of entrepreneurship are small throughout the process.&#8221;



Still, Kay remains optimistic about the road ahead for Enspired Solutions, for clean water innovation, and for other founders walking down a similar path. &#8220;Often, founders are coached about formulas for fundraising, formulas for startup success. Learning those formulas and expectations is important, but it&#8217;s also important to not forget that it&#8217;s your creativity and innovation and foresight that got you to the place you&#8217;re in and drove you to start a company. Ultimately, people still want to see that shine through.&#8221;



This episode of Business Lab is produced in partnership with the Michigan Economic Development Corporation.



Full Transcript



Megan Tatum: From MIT Technology Review, I&#8217;m Megan Tatum. This is Business Lab, the show that helps business leaders make sense of new technologies coming out of the lab and into the marketplace.



Today&#8217;s episode is brought to you in partnership with the Michigan Economic Development Corporation.Our topic today is launching a technology startup in the US state of Michigan. Building out an innovative idea into a viable product and company requires knowledge and resources that individuals might not have. That&#8217;s why the Michigan Economic Development Corporation, or the MEDC, has launched an innovation campaign to support technology entrepreneurs.Two words for you: startup ecosystem.My guest is Dr. Denise Kay, the co-founder and CEO at Enspired Solutions, a Michigan-based startup focused on removing synthetic forever chemicals called PFAS from water.Welcome, Denise.



Dr. Denise Kay: Hi, Megan.



Megan: Hi. Thank you so much for joining us. To get us started, Denise, I wondered if we could talk about Enspired Solutions a bit more. How did the idea come about, and what does your company do?



Denise: Well, my co-founder, Meng, and I had careers in consulting, advising clients on the fate and toxicity of chemicals in the environment. What we did was evaluate how chemicals moved through soil, water, and air, and what toxic impact they might have on humans and wildlife. That put us in a really unique position to see early on¬†the environmental and health ramifications of the manmade chemical PFAS in our environment.



When we learned of a very novel and elegant chemistry that could effectively destroy PFAS, we could foresee the value in making this chemistry available for commercial use and the potential for a significant positive impact on maintaining healthy water resources for all of us.Like you mentioned, PFAS is referred to as a forever chemical because it is so resistant to break down. It does not break down naturally in the environment, so it just circles around and around. This chemistry, which would break that cycle and break the molecule apart, could really support the health of all of us.Ultimately, Meng and I quit our jobs, and we founded Enspired Solutions. Our objective was to design, manufacture, and sell commercial-scale equipment that destroys PFAS in water based on this laboratory bench-scale chemistry that had been discovered, the goal being that this toxic contaminant does not continue to circulate in our natural resources.At this point, we have won an award from the EPA and Department of Defense, and proven our technology in over 200 different water samples ranging from groundwater, surface water, landfill leachate, industrial wastewater, [and] municipal wastewater. It&#8217;s really everywhere. What we&#8217;re seeing traction in right now is customer applications managing semiconductor waste. Groundwater and surface water around airports tend to be high in PFAS. Centralized waste disposal facilities that collect and manage PFAS-contaminated liquids. And also, even transitioning firetrucks to PFAS-free firefighting foams.



Megan: Fantastic. That&#8217;s a huge breadth of applications, incredible stuff.



Denise: Yeah.



Megan: You launched about four years ago now. I wondered what factors made Michigan the right place to build and grow the company?



Denise: That is something we put a lot of thought into, because I live in Michigan, and Meng lives in Illinois, so when it was just the two of us, there was even that, &#8220;Okay, what is going to be our headquarters?&#8221; We looked at a number of factors.



Some of the things we considered were rentable incubator space. By incubator, I mean startup incubators or innovation centers. The startup support network, a pool of future employees, and what position the state agencies were taking regarding PFAS.While thinking about all those things and investigating our communities, in Michigan, we found a space to rent where we could do chemistry experiments in an incubator environment. Somewhere where we were surrounded by other entrepreneurs, which we knew was something we had to learn how to do. We were great chemists, but we knew that surrounding ourselves with those skills that could be a gap for us was going to be helpful.Also, we know that Michigan has moved much faster than other states in identifying PFAS sources in the environment and regulating its presence. This combination was something we knew would be the right place for starting our business and having success.



Megan: It was a perfect setting for those two reasons. What were the first stages of your journey working with the Michigan Economic Development Corporation, the MEDC?



Denise: Well, both my co-founder, Meng, and I are first-time entrepreneurs. MEDC was one of the first resources I reached out to, starting from a Google search. They were an information resource we turned to initially, and then again and again for learning some fundamental skills. And receiving one-on-one expert mentorship for things like business contracts, understanding intellectual property landscapes, tracking and forecasting our business finances, and even how to approach fundraising.



Megan: Wow. It sounds like they were an invaluable resource in those early days. How did early-stage research and development progress from that point? What were the key MEDC services and programs you used to get started?



Denise: Well, our business is based on cutting-edge science, truly cutting-edge science. Understanding the intellectual property landscape, which is a term used to describe intellectual property, patents, trademarks, trade secrets that are related to the science we were founding our business on, it was very important. So that we knew we were starting on a path, that we wouldn&#8217;t hit a wall three years from now.



The MEDC performed an IP landscape survey for us. They searched the breadth of patents, and patent applications, and trademarks, and those things, and provided that for Meng and me to review and consider our position before really, really digging in and spending a lot of emotional time and money on the business.



The MEDC also helped us early on create a model in Excel for tracking business financing and forecasting, forecasting our future financial needs, so that we could be proactive instead of reactive to financial limitations. We knew it wasn&#8217;t going to be inexpensive to design and build a piece of equipment that&#8217;s the size of two very large refrigerators that had never been built before. That type of financial-forward modeling helped us figure out when we would need to start fundraising and taking in investments. As we progressed along that, the MEDC also provided support of an attorney who reviewed contract language to make sure that we really understood various agreements that we were signing.



Megan: Right. You mentioned that you and your co-founder were first-time entrepreneurs, as you put it. Tech acumen and business acumen are very different sets of skills. I wondered, what was the process like, developing this innovative technology¬†while also building out a viable business plan?



Denise: Well, Meng is a brilliant individual. She is a chemical engineer who also has an MBA. Meng had fantastic training to help understand the basis of how businesses function, in addition to understanding both the engineering and the chemistry behind what we were trying to do.I am an environmental toxicologist by training. I&#8217;ve had a longer career than Meng in that field. Over time, I have grown new offices and¬†established new offices for different consulting firms I&#8217;ve worked for. I had the experience with people, space, culture, and running a business from that side. Meng has the financial MBA knowledge basis for a business. We&#8217;re both excellent chemists and engineers, and those types of things.We had much of the necessary knowledge, at least to take the first steps forward. The challenge became the hard limit of 24 hours in a day and no revenue to hire any support. That&#8217;s when the startup support networks like the MEDC became invaluable.



It was simply impossible to do everything that needed to be done, especially while we were learning what we were doing. The MEDC and other programs provided support to take some of that load off us, but also helped us to learn to implement the new skills in an efficient manner, less stumbling.



Megan: So many things to juggle, isn&#8217;t there, in starting a company. I wondered, in that vein, could you share some successes and highlights from your journey so far? Any partnerships or projects that you&#8217;re excited about that you could share with us?



Denise: As people say, being an entrepreneur and running a startup is like a rollercoaster. You have high moments and you have very low moments when you think nothing&#8217;s ever going to move forward. I&#8217;d love to talk about some of the highlights. Our machine, which we call the PFASigator.



First of all, coming up with that name has a fun story behind it. The machine is, like I said, about the size of two large refrigerators. It&#8217;s very large, and it breaks down PFAS in water. The machine takes in water that has PFAS in it, we add a couple of liquid chemicals, then¬†a very intense ultraviolet light shines on that water, which catalyzes a chemical reaction called reductive defluorination. When all of this is happening and the PFAS molecules are being broken apart to nontoxic compounds, to an outsider, it all still just looks like water with a light shining on it. But the machine is big, and it essentially eats PFAS.



Meng and I were bantering, and her young, six-year-old son was in the background at the time. We were throwing names around. Thomas called out, &#8220;The PFASigator!&#8221; We were like, &#8220;Ooh, there&#8217;s something there.&#8221;



Megan: It&#8217;s a great name.



Denise: It matches what we do, and it&#8217;s a memorable name. We&#8217;ve really had fun with that throughout. That was an early highlight, and we&#8217;ve stuck with that name.



The next highlight I&#8217;d say was standing next to our first fully functioning PFASigator. It was big. It was all stainless steel. Meng and I had never been part of building a physical, large object like that. Just standing there, and the picture we have of us, it was exhilarating. That was a magnificent feeling.



Selling our first machine was a day that everyone in the company, I think we were about eight at that point, received a bottle of champagne.



Megan: Fantastic.



Denise: For a startup to go from zero to one, they call it, you&#8217;ve sold nothing to you&#8217;ve sold something. That&#8217;s a real strong milestone and was a celebration for us.



I&#8217;d say most recently, Enspired has been awarded a very exciting project in Michigan. It is in the contracting phase, so I can&#8217;t reveal too many details. But it is with a progressive municipality that will have our PFASigator permanently installed, destroying PFAS. That kind of movement from zero to one, and then a significant contract that will raise the visibility of the effectiveness of our approach and machine, has really buoyed our energy and is pushing us forward. It&#8217;s amazing to know we are now having an impact on the sustainability of water resources. That&#8217;s what we started the company for.



Megan: Awesome. You have some incredible milestones there. But it&#8217;s a hard journey, as you&#8217;ve said as well, being an entrepreneur. I wondered, finally, what advice would you offer to burgeoning entrepreneurs given your own experience?



Denise: I would advise that if problem solving and learning new talents do not provide sufficient intrinsic reward for a founder to be satisfied throughout what I guarantee will be a long duration effort, then that founder may need to reset their expectations, because the financial rewards of entrepreneurship are small throughout the process.Meng and I put [in] some of our personal funds and took no salary, and worked harder than we ever had in our lives for at least a year and a half before we were able to take a small salary. The financial rewards are small throughout the process of being a startup. The rewards are delayed, and in many cases, for many startups, the financial rewards never materialize.It&#8217;s a tough journey, and you have to love being on that journey, and be intrinsically rewarded for that for the sake of the journey itself, or you&#8217;ll be a very unhappy founder.Megan: It needs to be something you&#8217;re as passionate about as I can tell you are about the work you&#8217;re doing at Enspired Solutions.



Denise: There&#8217;s probably one other thing I&#8217;d like to add to that.



Megan: Of course.



Denise: Often, founders are coached about formulas for fundraising, formulas for startup success. Learning those formulas and expectations is important, but it&#8217;s also important to not forget that it&#8217;s your creativity and innovation and foresight that got you to the place you&#8217;re in and drove you to start a company. Ultimately, people still want to see that shine through.&#8221;



Megan: That&#8217;s fantastic advice. Thank you so much, Denise.



That was Dr. Denise Kay, the co-founder and CEO at Enspired Solutions, whom I spoke with from an unexpectedly sunny Brighton, England.That&#8217;s it for this episode of Business Lab. I&#8217;m your host, Megan Tatum. I&#8217;m a contributing editor and host for Insights, the custom publishing division of MIT Technology Review. We were founded in 1899 at the Massachusetts Institute of Technology. You can find us in print, on the web, and at events each year around the world. For more information about us and the show, please check out our website at technologyreview.com.



This show is available wherever you get your podcasts. If you enjoyed this episode, we hope you&#8217;ll take a moment to rate and review us. Business Lab is a production of MIT Technology Review, and this episode was produced by Giro Studios. Thanks for listening.



This content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review‚Äôs editorial staff.



This content was researched, designed, and written entirely by human writers, editors, analysts, and illustrators. This includes the writing of surveys and collection of data for surveys. AI tools that may have been used were limited to secondary production processes that passed thorough human review.
‚Ä¢ The Download: how to melt rocks, and what you need to know about AI
  This is today&#8217;s edition of¬†The Download,¬†our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



This startup wants to use beams of energy to drill geothermal wells



Geothermal startup Quaise certainly has an unconventional approach when it comes to destroying rocks: it uses a new form of drilling technology to melt holes through them. The company hopes it‚Äôs the key to unlocking geothermal energy and making it feasible anywhere.Quaise‚Äôs technology could theoretically be used to tap into the Earth‚Äôs heat from anywhere on the globe. But some experts caution that reinventing drilling won‚Äôt be as simple, or as fast, as Quaise‚Äôs leadership hopes. Read the full story.



‚ÄîCasey Crownhart







Five things you need to know about AI right now



‚ÄîWill Douglas Heaven, senior editor for AI



Last month I gave a talk at SXSW London called ‚ÄúFive things you need to know about AI‚Äù‚Äîmy personal picks for the five most important ideas in AI right now.&nbsp;



I aimed the talk at a general audience, and it serves as a quick tour of how I‚Äôm thinking about AI in 2025. There‚Äôs some fun stuff in there. I even make jokes!&nbsp;



You can now watch the video of my talk, but if you want to see the five I chose right now, here is a quick look at them.



This story originally appeared in The Algorithm, our weekly newsletter on AI. To get stories like this in your inbox first, sign up here.







Why it‚Äôs so hard to make welfare AI fair



There are plenty of stories about AI that‚Äôs caused harm when deployed in sensitive situations, and in many of those cases, the systems were developed without much concern to what it meant to be fair or how to implement fairness.But the city of Amsterdam spent a lot of time and money to try to create ethical AI‚Äîin fact, it followed every recommendation in the responsible AI playbook. But when it deployed it in the real world, it still couldn‚Äôt remove biases. So why did Amsterdam fail? And more importantly: Can this ever be done right?Join our editor Amanda Silverman, investigative reporter Eileen Guo and Gabriel Geiger, an investigative reporter from Lighthouse Reports, for a subscriber-only Roundtables conversation at 1pm ET on Wednesday July 30 to explore if algorithms can ever be fair. Register here!







The must-reads



I‚Äôve combed the internet to find you today‚Äôs most fun/important/scary/fascinating stories about technology.



1 America‚Äôs grand data center ambitions aren‚Äôt being realized¬†A major partnership between SoftBank and OpenAI hasn‚Äôt got off to a flying start. (WSJ $)+ The setback hasn‚Äôt stopped OpenAI opening its first DC office. (Semafor)



2 OpenAI is partnering with the UK governmentIn a bid to increase its public services‚Äô productivity and to drive economic growth. (BBC)+ It all sounds pretty vague. (Engadget)



3 The battle for AI math supremacy is heating upGoogle and OpenAI went head to head in a math competition‚Äîbut only one played by the rules. (Axios)+ The International Math Olympiad poses a unique challenge to AI models. (Ars Technica)+ What‚Äôs next for AI and math. (MIT Technology Review)



4 Mark Zuckerberg‚Äôs secretive Hawaiian compound is getting biggerThe multi-billionaire is sinking millions of dollars into the project. (Wired $)



5 India‚Äôs back offices are meeting global demand for AI expertise¬†New ‚Äòcapability centers‚Äô could help to improve the country‚Äôs technological prospects. (FT $)+ The founder of Infosys believes the future of AI will be more democratic. (Rest of World)+ Inside India‚Äôs scramble for AI independence. (MIT Technology Review)



6 A crime-tracking app will share videos with the NYPDPublic safety agencies will have access to footage shared on Citizen. (The Verge)+ AI was supposed to make police bodycams better. What happened? (MIT Technology Review)



7 China has a problem with competition: there‚Äôs too much of itIts government is making strides to crack down on price wars within sectors. (NYT $)+ China‚Äôs Xiaomi is making waves across the world. (Economist $)



8 The metaverse is a tobacco marketer‚Äôs playground Fed up of legal constraints, they‚Äôre already operating in unregulated spaces. (The Guardian)+ Welcome to the oldest part of the metaverse. (MIT Technology Review)



9 How AI is shaking up physicsModels are suggesting outlandish ideas that actually work. (Quanta Magazine)



10 Tesla has opened a diner that resembles a spaceshipIt‚Äôs technically a drive-thru that happens to sell Tesla merch. (TechCrunch)







Quote of the day



&nbsp;&#8220;If you can pick off the individuals for $100 million each and they&#8217;re good, it&#8217;s actually a bargain.&#8221;



‚ÄîEntrepreneur Laszlo Bock tells Insider why he thinks the eye-watering sums Meta is reportedly offering top AI engineers is money well spent.







One more thing







The world‚Äôs first industrial-scale plant for green steel promises a cleaner futureAs of 2023, nearly 2 billion metric tons of steel were being produced annually, enough to cover Manhattan in a layer more than 13 feet thick.Making this metal produces a huge amount of carbon dioxide. Overall, steelmaking accounts for around 8% of the world‚Äôs carbon emissions‚Äîone of the largest industrial emitters and far more than such sources as aviation.A handful of groups and companies are now making serious progress toward low- or zero-emission steel. Among them, the Swedish company Stegra stands out. The startup is currently building the first industrial-scale plant in the world to make green steel. But can it deliver on its promises? Read the full story.‚ÄîDouglas Main







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)



+ Spoiler haters look away now: these are the best movie endings of all.+ 27 years on, this bop from the Godzilla soundtrack still sounds like the future.+ Inside the race to preserve the very first color photographs for generations to come.+ Origami space planes sound very cool.
‚Ä¢ This startup wants to use beams of energy to drill geothermal wells
  A beam of energy hit the slab of rock, which quickly began to glow. Pieces cracked off, sparks ricocheted, and dust whirled around under a blast of air.&nbsp;



From inside a modified trailer, I peeked through the window as a millimeter-wave drilling rig attached to an unassuming box truck melted a hole into a piece of basalt in less than two minutes. After the test was over, I stepped out of the trailer into the Houston heat. I could see a ring of black, glassy material stamped into the slab fragments, evidence of where the rock had melted.¬†¬†



This rock-melting drilling technology from the geothermal startup Quaise is certainly unconventional. The company hopes it‚Äôs the key to unlocking geothermal energy and making it feasible anywhere.



Geothermal power tends to work best in those parts of the world that have the right geology and heat close to the surface. Iceland and the western US, for example, are hot spots for this always-available renewable energy source because they have all the necessary ingredients. But by digging deep enough, companies could theoretically tap into the Earth‚Äôs heat from anywhere on the globe.



That‚Äôs a difficult task, though. In some places, accessing temperatures high enough to efficiently generate electricity would require drilling miles and miles beneath the surface. Often, that would mean going through very hard rock, like granite.



Quaise‚Äôs proposed solution is a new mode of drilling that eschews the traditional technique of scraping into rock with a hard drill bit. Instead, the company plans to use a gyrotron, a device that emits high-frequency electromagnetic radiation. Today, the fusion power industry uses gyrotrons to heat plasma to 100 million ¬∞C, but Quaise plans to use them to blast, melt, and vaporize rock. This could, in theory, make drilling faster and more economical, allowing for geothermal energy to be accessed anywhere.&nbsp;&nbsp;



Since Quaise‚Äôs founding in 2018, the company has demonstrated that its systems work in the controlled conditions of the laboratory, and it has started trials in a semi-controlled environment, including the backyard of its Houston headquarters. Now these efforts are leaving the lab, and the team is taking gyrotron drilling technology to a quarry to test it in real-world conditions.&nbsp;



Some experts caution that reinventing drilling won‚Äôt be as simple, or as fast, as Quaise‚Äôs leadership hopes. The startup is also attempting to raise a large funding round this year, at a time when economic uncertainty is slowing investment and the US climate technology industry is in a difficult spot politically because of policies like tariffs and a slowdown in government support. Quaise‚Äôs big idea aims to accelerate an old source of renewable energy. This make-or-break moment might determine how far that idea can go.¬†



Blasting through



Rough calculations from the geothermal industry suggest that enough energy is stored inside the Earth to meet our energy demands for tens or even hundreds of thousands of years, says Matthew Houde, cofounder and chief of staff at Quaise. After that, other sources like fusion should be available, ‚Äúassuming we continue going on that long, so to speak,‚Äù he quips.¬†



‚ÄúWe want to be able to scale this style of geothermal beyond the locations where we‚Äôre able to readily access those temperatures today with conventional drilling,‚Äù Houde says. The key, he adds, is simply going deep enough: ‚ÄúIf we can scale those depths to 10 to 20 kilometers, then we can enable super-hot geothermal to be worldwide accessible.‚Äù



Though that‚Äôs technically possible, there are few examples of humans drilling close to this depth. One research project that began in 1970 in the former Soviet Union reached just over 12 kilometers, but it took nearly 20 years and was incredibly expensive.&nbsp;



Quaise hopes to speed up drilling and cut its cost, Houde says. The company‚Äôs goal is to drill through rock at a rate of between three and five meters per hour of steady operation.



One key factor slowing down many operations that drill through hard rocks like granite is nonproductive time. For example, equipment frequently needs to be brought all the way back up to the surface for repairs or to replace drill bits.



Quaise‚Äôs key to potentially changing that is its gyrotron. The device emits millimeter waves, beams of energy with wavelengths that fall between microwaves and infrared waves. It‚Äôs a bit like a laser, but the beam is not visible to the human eye.¬†



Quaise‚Äôs goal is to heat up the target rock, effectively drilling it away. The gyrotron beams waves at a target rock via a waveguide, a hollow metal tube that directs the energy to the right spot. (One of the company‚Äôs main technological challenges is to avoid accidentally making plasma, an ionized, superheated state of matter, as it can waste energy and damage key equipment like the waveguide.)



Here‚Äôs how it works in practice: When Quaise‚Äôs rig is drilling a hole, the tip of the waveguide is positioned a foot or so away from the rock it‚Äôs targeting. The gyrotron lets out a burst of millimeter waves for about a minute. They travel down the waveguide and hit the target rock, which heats up and then cracks, melts, or even vaporizes.



Then the beam stops, and the drill bit at the end of the waveguide is lowered to the surface of the rock, rotating and scraping off broken shards and melted bits of rock as it descends. A steady blast of air carries the debris up to the surface, and the process repeats. The energy in the millimeter waves does the hard work, and the scraping and compressed air help remove the fractured or melted material away.



This system is what I saw in action at the company‚Äôs Houston headquarters. The drilling rig in the yard is a small setup, something like what a construction company might use to drill micro piles for a foundation or what researchers would use to take geological samples. In total, the gyrotron has a power of 100 kilowatts. A cooling system helps the superconducting magnet in the gyrotron reach the necessary temperature (about -200 ¬∞C), and a filtration system catches the debris that sloughs off samples.¬†



CASEY CROWNHART




Soon after my visit, this backyard setup was packed up and shipped to central Texas to be used for further field testing in a rock quarry. The company announced in July that it had used that rig to drill a 100-meter-deep hole at that field test site.&nbsp;



Quaise isn‚Äôt the first to develop nonmechanical drilling, says Roland Horne, head of the geothermal program at Stanford University. ‚ÄúBurning holes in rocks is impressive. However, that‚Äôs not the whole of what‚Äôs involved in drilling,‚Äù he says. The operation will need to be able to survive the high temperatures and pressures at the bottom of wells as they‚Äôre drilled, he says.



So far, the company has found success drilling holes into columns of rock inside metal casings, as well as the quarry in its field trials. But there‚Äôs a long road between drilling into predictable material in a relatively predictable environment and creating a miles-deep geothermal well.&nbsp;



Rocky roads



In April, Quaise fully integrated its second 100-kilowatt gyrotron onto an oil and gas rig owned by the company‚Äôs investor and technology partner Nabors. This rig is the sort that would typically be used for training or engineering development, and it‚Äôs set up along with a row of other rigs at the Nabors headquarters, just across town from the Quaise lab. At 182 feet high, the top is visible above the office building from the parking lot.



When I visited in April, the company was still completing initial tests, using special thermal paper and firing short blasts to test the setup. In May the company tested this integrated rig, drilling a hole four inches in diameter and 30 feet deep. Another test in June reached a depth of 40 feet. These holes were drilled into columns of basalt that had been lowered into the ground as a test material.



While the company tests its 100-kilowatt systems at the rig and the quarry, the next step is an even larger system, which features a gyrotron that‚Äôs 10 times more powerful. This one-megawatt system will drill larger holes, over eight inches across, and represents the commercial-scale version of the company‚Äôs technology. Drilling tests are set to begin with this larger drill in 2026.&nbsp;



The one-megawatt system actually needs a little over three megawatts of power overall, including the energy needed to run support equipment like cooling systems and the compressor that blows air into the hole, carrying the rock dust back up to the surface. That power demand is similar to what an oil and gas rig requires today.&nbsp;



Quaise is in the process of setting up a pilot plant in Oregon, basically on the side of a volcano, says Trenton Cladouhos, the company‚Äôs vice president of geothermal resource development. This project will use conventional drilling, and its main purpose is to show that Quaise can build and run a geothermal plant, Cladouhos says.&nbsp;



The company is building an exploration well this year and plans to begin drilling production wells (those that can eventually be used to generate electricity) in 2026. That pilot project will reach about 20 megawatts of power with the first few wells, operating on rock that‚Äôs around 350 ¬∞C. The company plans to have it operational as early as 2028.



Quaise‚Äôs strategy with the Oregon project is to show that it can use super-hot rocks to produce geothermal power efficiently, says CEO Carlos Araque. After it fires up the plant and begins producing electricity, the company can go back in and deepen the holes with millimeter-wave drilling in the future, he adds.



A drilling test shows Quaise&#8217;s millimeter-wave technology drilling into a piece of granite. QUAISE




Araque says the company already has some customers lined up for the energy it‚Äôll produce, though he declined to name them, saying only that one was a big tech company, and there‚Äôs a utility involved as well.



But the startup will need more capital to finish this project and complete its testing with the larger, one-megawatt gyrotron. And uncertainty is floating around in climate tech, given the Trump administration‚Äôs tariffs and rollback of financial support for climate tech (though geothermal has been relatively unscathed).¬†



Quaise still has some technical barriers to overcome before it begins building commercial power plants.&nbsp;



One potential hurdle: drilling in different directions. Right now, millimeter-wave drilling can go in a straight line, straight down. Developing a geothermal plant like the one at the Oregon site will likely require what‚Äôs called directional drilling, the ability to drill in directions other than vertical.



And the company will likely face challenges as it transitions from lab testing to field trials. One key challenge for geothermal technology companies attempting to operate at this depth will be&nbsp; keeping wells functional for a long time to keep a power plant operating, says Jefferson Tester, a professor at Cornell University and an expert in geothermal energy.



Quaise‚Äôs technology is very aspirational, Tester says, and it can be difficult for new ideas in geothermal to compete economically. ‚ÄúIt‚Äôs eventually all about cost,‚Äù he says. And companies with ambitious ideas run the risk that their investors will run out of patience before they can develop their technology enough to make it onto the grid.



‚ÄúThere‚Äôs a lot more to learn‚ÄîI mean, we‚Äôre reinventing drilling,‚Äù says Steve Jeske, a project manager at Quaise. ‚ÄúIt seems like it shouldn‚Äôt work, but it does.‚Äù
‚Ä¢ Five things you need to know about AI right now
  Last month I gave a talk at SXSW London called ‚ÄúFive things you need to know about AI‚Äù‚Äîmy personal picks for the five most important ideas in AI right now.&nbsp;



I aimed the talk at a general audience, and it serves as a quick tour of how I‚Äôm thinking about AI in 2025. I‚Äôm sharing it here in case you‚Äôre interested. I think the talk has something for everyone. There‚Äôs some fun stuff in there. I even make jokes!



The¬†video¬†is now available (thank you, SXSW London).¬†Below is a quick look at my top five. Let me know if you would have picked different ones!



1. Generative AI is now so good it‚Äôs scary.



Maybe you think that‚Äôs obvious. But I am constantly having to check my assumptions about how fast this technology is progressing‚Äîand it‚Äôs my job to keep up.¬†A few months ago, my colleague‚Äîand your regular Algorithm writer‚ÄîJames O‚ÄôDonnell shared 10 music tracks with the¬†MIT Technology Review¬†editorial team and challenged us to pick which ones had been produced using generative AI and which had been made by people. Pretty much everybody did worse than chance.What‚Äôs happening with music is happening across media, from code to robotics to protein synthesis to video. Just look at what people are doing with new video-generation tools like Google DeepMind‚Äôs Veo 3. And this technology is being put into everything.My point here? Whether you think AI is the best thing to happen to us or the worst, do not underestimate it. It‚Äôs good, and it‚Äôs getting better.



2. Hallucination is a feature, not a bug.



Let‚Äôs not forget the fails. When AI makes up stuff, we call it hallucination. Think of customer service bots offering nonexistent refunds, lawyers submitting briefs filled with nonexistent cases, or RFK Jr.‚Äôs government department publishing a report that cites nonexistent academic papers.¬†You‚Äôll hear a lot of talk that makes hallucination sound like it‚Äôs a problem we need to fix. The more accurate way to think about hallucination is that this is exactly what generative AI does‚Äîwhat it‚Äôs meant to do‚Äîall the time. Generative models are trained to make things up.What‚Äôs remarkable is not that they make up nonsense, but that the nonsense they make up so often matches reality.¬†Why does this matter? First, we need to be aware of what this technology can and can‚Äôt do. But also: Don‚Äôt hold out for a future version that doesn‚Äôt hallucinate.





3. AI is power hungry and getting hungrier.



You‚Äôve probably heard that AI is power hungry. But a lot of that reputation comes from the amount of electricity it takes to train these giant models, though giant models only get trained every so often.What‚Äôs changed is that these models are now being used by hundreds of millions of people every day.¬†And while using a model takes far less energy than training one, the energy costs ramp up massively with those kinds of user numbers.¬†ChatGPT, for example, has 400 million weekly users. That makes it the fifth-most-visited website in the world, just after Instagram and ahead of X. Other chatbots are catching up.¬†So it‚Äôs no surprise that tech companies are racing to build new data centers in the desert and revamp power grids.The truth is we‚Äôve been in the dark about exactly how much energy it takes to fuel this boom because none of the major companies building this technology have shared much information about it.¬†That‚Äôs starting to change, however. Several of my colleagues spent months working with researchers to crunch the numbers for some open source versions of this tech. (Do check out what they found.)



4. Nobody knows exactly how large language models work.



Sure, we know how to build them. We know how to make them work really well‚Äîsee no. 1 on this list.But how they do what they do is still an unsolved mystery. It‚Äôs like these things have arrived from outer space and scientists are poking and prodding them from the outside to figure out what they really are.It‚Äôs incredible to think that never before has a mass-market technology used by billions of people been so little understood.Why does that matter? Well, until we understand them better we won‚Äôt know exactly what they can and can‚Äôt do. We won‚Äôt know how to control their behavior. We won‚Äôt fully understand hallucinations.



5. AGI doesn‚Äôt mean anything.



Not long ago, talk of AGI was fringe, and mainstream researchers were embarrassed to bring it up.¬†But as AI has got better and far more lucrative, serious people are happy to insist they‚Äôre about to create it. Whatever it is.AGI‚Äîor artificial general intelligence‚Äîhas come to mean something like: AI that can match the performance of humans on a wide range of cognitive tasks.But what does that mean? How do we measure performance? Which humans? How wide a range of tasks? And performance on cognitive tasks is just another way of saying intelligence‚Äîso the definition is circular anyway.Essentially, when people refer to AGI they now tend to just mean AI, but better than what we have today.There‚Äôs this absolute faith in the progress of AI. It‚Äôs gotten better in the past, so it will continue to get better. But there is zero evidence that this will actually play out.¬†So where does that leave us? We are building machines that are getting very good at mimicking some of the things people do, but the technology still has serious flaws. And we‚Äôre only just figuring out how it actually works.



Here‚Äôs how I think about AI: We have built machines with humanlike behavior, but we haven‚Äôt shrugged off the habit of imagining a humanlike mind behind them. This leads to exaggerated assumptions about what AI can do and plays into the wider culture wars between techno-optimists and techno-skeptics.It‚Äôs right to be amazed by this technology. It‚Äôs also right to be skeptical of many of the things said about it. It‚Äôs still very early days, and it‚Äôs all up for grabs.



This story originally appeared in The Algorithm, our weekly newsletter on AI. To get stories like this in your inbox first, sign up here.
‚Ä¢ The Download: how your data is being used to train AI, and why chatbots aren‚Äôt doctors
  This is today&#8217;s edition of¬†The Download,¬†our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



A major AI training data set contains millions of examples of personal data



Millions of images of passports, credit cards, birth certificates, and other documents containing personally identifiable information are likely included in one of the biggest open-source AI training sets, new research has found.Thousands of images‚Äîincluding identifiable faces‚Äîwere found in a small subset of DataComp CommonPool, a major AI training set for image generation scraped from the web. Because the researchers audited just 0.1% of CommonPool‚Äôs data, they estimate that the real number of images containing personally identifiable information, including faces and identity documents, is in the hundreds of millions.&nbsp;



The bottom line? Anything you put online can be and probably has been scraped. Read the full story.



‚ÄîEileen Guo







AI companies have stopped warning you that their chatbots aren‚Äôt doctors



AI companies have now mostly abandoned the once-standard practice of including medical disclaimers and warnings in response to health questions, new research has found. In fact, many leading AI models will now not only answer health questions but even ask follow-ups and attempt a diagnosis.Such disclaimers serve an important reminder to people asking AI about everything from eating disorders to cancer diagnoses, the authors say, and their absence means that users of AI are more likely to trust unsafe medical advice. Read the full story.



‚ÄîJames O‚ÄôDonnell







The must-reads



I‚Äôve combed the internet to find you today‚Äôs most fun/important/scary/fascinating stories about technology.



1 Hackers exploited a flaw in Microsoft‚Äôs software to attack government agenciesEngineers across the world are racing to mitigate the risk it poses. (Bloomberg $)+ The attack hones in on servers housed within an organization, not the cloud. (WP $)¬†2 The French government has launched a criminal probe into XIt‚Äôs investigating the company‚Äôs recommendation algorithm‚Äîbut X isn‚Äôt cooperating. (FT $)+ X says French lawmaker Eric Bothorel has accused it of manipulating its algorithm for foreign interference purposes. (Reuters)¬†



3 Trump aides explored ending contracts with SpaceXBut they quickly found most of them are vital to the Defense Department and NASA. (WSJ $)+ But that doesn‚Äôt mean it‚Äôs smooth sailing for SpaceX right now. (NY Mag $)+ Rivals are rising to challenge the dominance of SpaceX. (MIT Technology Review)



4 Meta has refused to sign the EU‚Äôs AI code of practiceIts new global affairs chief claims the rules with throttle growth. (CNBC)+ The code is voluntary‚Äîbut declining to sign it sends a clear message. (Bloomberg $)



5 A Polish programmer beat an OpenAI model in a coding competitionBut only narrowly. (Ars Technica)+ The second wave of AI coding is here. (MIT Technology Review)



6 Nigeria has dreams of becoming a major digital worker hubThe rise of AI means there‚Äôs less outsourcing work to go round. (Rest of World)+ What Africa needs to do to become a major AI player. (MIT Technology Review)



7 Microsoft is building a digital twin of the Notre-Dame CathedralThe replica can help support its ongoing maintenance, apparently. (Reuters)



8 How funny is AI, really?Not all senses of humor are made equal. (Undark)+ What happened when 20 comedians got AI to write their routines. (MIT Technology Review)



9 What it‚Äôs like to forge a friendship with an AIStudent MJ Cocking found the experience incredibly helpful. (NYT $)+ But chatbots can also fuel vulnerable people‚Äôs dangerous delusions. (WSJ $)+ The AI relationship revolution is already here. (MIT Technology Review)



10 Work has begun on the first space-based gravitational wave detectorThe waves are triggered when massive objects like black holes collide. (IEEE Spectrum)+ How the Rubin Observatory will help us understand dark matter and dark energy. (MIT Technology Review)







Quote of the day



&#8220;There was just no way I was going to make it through four years of this.&#8221;



‚ÄîEgan Reich, a former worker in the US Department of Labor, explains why he accepted the agency&#8217;s second deferred resignation offer in April after DOGE‚Äôs rollout, Insider reports.







One more thing







The world is moving closer to a new cold war fought with authoritarian techA cold war is brewing between the world‚Äôs autocracies and democracies‚Äîand technology is fueling it.Authoritarian states are following China‚Äôs lead and are trending toward more digital rights abuses by increasing the mass digital surveillance of citizens, censorship, and controls on individual expression.And while democracies also use massive amounts of surveillance technology, it‚Äôs the tech trade relationships between authoritarian countries that‚Äôs enabling the rise of digitally enabled social control. Read the full story.‚ÄîTate Ryan-Mosley







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)+ I need to sign up for Minneapolis‚Äô annual cat tour immediately.+ What are the odds? This mother has had four babies, all born on July 7 in different years.+ Not content with being a rap legend, Snoop Dogg has become a co-owner of a Welsh soccer club.+ Appetite for Destruction, Guns n‚Äô Roses‚Äô outrageous debut album, was released on this day 38 years ago.

üîí Cybersecurity & Privacy
‚Ä¢ Microsoft Fix Targets Attacks on SharePoint Zero-Day
  Microsoft Corp. issued an emergency security update for a vulnerability in SharePoint Server that is actively being exploited to compromise vulnerable organizations . The patch comes amid reports that malicious hackers have used the SharePoint flaw to breach U.S. federal and state agencies, universities, and energy companies . The Washington Post reported on Sunday that the government and partners in Canada and Australia are investigating the hack of SharePoint servers, which provide a platform for sharing and managing documents .

üéì University AI
No updates.

üè¢ Corporate AI
‚Ä¢ AI Testing and Evaluation: Reflections
  Generative AI presents a unique challenge and opportunity to reexamine governance practices for the responsible development, deployment, and use of AI. To advance thinking in this space, Microsoft has tapped into the experience and knowledge of experts across domains‚Äîfrom genome editing to cybersecurity‚Äîto investigate the role of testing and evaluation as a governance tool.&nbsp;AI Testing and Evaluation: Learnings from Science and Industry,&nbsp;hosted by Microsoft Research‚Äôs&nbsp;Kathleen Sullivan, explores what the technology industry and policymakers can learn from these fields and how that might help shape the course of AI development.



In the series finale, Amanda Craig Deckard, senior director of public policy in Microsoft‚Äôs Office of Responsible AI, rejoins Sullivan to discuss what Microsoft has learned about testing as a governance tool and what‚Äôs next for the company&#8217;s work in the AI governance space. The pair explores high-level takeaways (i.e., testing is important and challenging!); the roles of rigor, standardization, and interpretability in making testing a reliable governance tool; and the potential for public-private partnerships to help advance not only model-level evaluation but deployment-level evaluation, too.







Learn more:



Learning from other domains to advance AI evaluation and testing&nbsp;Microsoft Research Blog | June 2025&nbsp;



Responsible AI: Ethical policies and practices | Microsoft AI&nbsp;








	
		
			Subscribe to the Microsoft Research Podcast:		
		
							
					
						  
						Apple Podcasts
					
				
			
							
					
						
						Email
					
				
			
							
					
						
						Android
					
				
			
							
					
						
						Spotify
					
				
			
							
					
						
						RSS Feed
					
				
					
	




	
		
			
				
					

Transcript



[MUSIC]&nbsp;



KATHLEEN SULLIVAN: Welcome to AI Testing and Evaluation: Learnings from Science and Industry. I‚Äôm your host, Kathleen Sullivan.&nbsp;



As generative AI continues to advance, Microsoft has gathered a range of experts‚Äîfrom genome editing to cybersecurity‚Äîto share how their fields approach evaluation and risk assessment. Our goal is to learn from their successes and their stumbles to move the science and practice of AI testing forward. In this series, we‚Äôll explore how these insights might help guide the future of AI development, deployment, and responsible use.&nbsp;



[MUSIC ENDS]&nbsp;



For our final episode of the series, I‚Äôm thrilled to once again be joined by Amanda Craig Deckard, senior director of public policy in Microsoft‚Äôs Office of Responsible AI.&nbsp;



Amanda, welcome back to the podcast!



				
				
					



AMANDA CRAIG DECKARD: Thank you so much.



SULLIVAN: In our intro episode, you really helped set the stage for this series. And it‚Äôs been great, because since then, we‚Äôve had the pleasure of speaking with governance experts about genome editing, pharma, medical devices, cybersecurity, and we‚Äôve also gotten to spend some time with our own Microsoft responsible AI leaders and hear reflections from them.



And here‚Äôs what stuck with me, and I‚Äôd love to hear from you on this, as well: testing builds trust; context is shaping risk; and every field is really thinking about striking its own balance between pre-deployment testing and post-deployment monitoring.



So drawing on what you‚Äôve learned from the workshop and the case studies, what headline insights do you think matter the most for AI governance?



CRAIG DECKARD: It&#8217;s been really interesting to learn from all of these different domains, and there are, you know, lots of really interesting takeaways.&nbsp;



I think a starting point for me is actually pretty similar to where you landed, which is just that testing is really important for trust, and it&#8217;s also really hard [LAUGHS] to figure out exactly, you know, how to get it right, how to make sure that you&#8217;re addressing risks, that you&#8217;re not constraining innovation, that you are recognizing that a lot of the industry that&#8217;s impacted is really different. You have small organizations, you have large organizations, and you want to enable that opportunity that is enabled by the technology across the board.&nbsp;



And so it&#8217;s just difficult to, kind of, get all of these dynamics right, especially when, you know, I think we heard from other domains, testing is not some, sort of, like, oh, simple thing, right. There&#8217;s not this linear path from, like, A to B where you just test the one thing and you&#8217;re done.&nbsp;



SULLIVAN: Right.



CRAIG DECKARD: It&#8217;s complex, right. Testing is multistage. There&#8217;s a lot of testing by different actors. There are a lot of different purposes for which you might test. As I think it was Dan Carpenter who talked about it&#8217;s not just about testing for safety. It&#8217;s also about testing for efficacy and building confidence in the right dosage for pharmaceuticals, for example. And that&#8217;s across the board for all of these domains, right. That you&#8217;re really thinking about the performance of the technology. You&#8217;re thinking about safety. You&#8217;re trying to also calibrate for efficiency.



And so those tradeoffs, every expert shared that navigating those is really challenging. And also that there were real impacts to early choices in the, sort of, governance of risk in these different domains and the development of the testing, sort of, expectations, and that in some cases, this had been difficult to reverse, which also just layers on that complexity and that difficulty in a different way. So that‚Äôs the super high-level takeaway. But maybe if I could just quickly distill, like, three takeaways that I think really are applicable to AI in a bit more of a granular way.



You know, one is about, how is the testing exactly used? For what purpose? And the second is what emphasis there is on this pre- versus post-deployment testing and monitoring. And then the third is how rigid versus adaptive the, sort of, testing regimes or frameworks are in these different domains.&nbsp;



So on the first‚Äîhow is testing used?‚Äîso is testing something that impacts market entry, for example? Or is it something that might be used more for informing how risk is evolving in the domain and how broader risk management strategies might need to be applied? We have examples, like the pharmaceutical or medical device industry experts with whom you spoke, that&#8217;s really, you know, testing ‚Ä¶ there is a pre-deployment requirement. So that&#8217;s one question.&nbsp;



The second is this emphasis on pre- versus post-deployment testing and monitoring, and we really did see across domains that in many cases, there is a desire for both pre- and post-deployment, sort of, testing and monitoring, but also that, sort of, naturally in these different domains, a degree of emphasis on one or the other had evolved and that had a real impact on governance and tradeoffs.&nbsp;



And the third is just how rigid versus adaptive these testing and evaluation regimes or frameworks are in these different domains. We saw, you know, in some domains, the testing requirements were more rigid as you might expect in more of the pharmaceutical or medical devices industries, for example. And in other domains, there was this more, sort of, adaptive approach to how testing might get used. So, for example, in the case of our other general-purpose technologies, you know, you spoke with Alta Charo on genome editing, and in our case studies, we also explored this in the context of nanotechnology. In those general-purpose technology domains, there is more emphasis on downstream or application-context testing that is more, sort of, adaptive to the use scenario of the technology and, you know, having that work in conjunction with testing more at the, kind of, level of the technology itself.



SULLIVAN: I want to double-click on a number of the things we just talked about. But actually, before we go too much deeper, a question on if there&#8217;s anything that really surprised you or challenged maybe some of your own assumptions in this space from some of the discussions that we had over the series.&nbsp;



CRAIG DECKARD: Yeah. You know, I know I&#8217;ve already just mentioned this pre- versus post-deployment testing and monitoring issue, but it was something that was very interesting to me and in some ways surprised me or made me just realize something that I hadn&#8217;t fully connected before, about how these, sort of, regimes might evolve in different contexts and why. And in part, I couldn&#8217;t help but bring the context I have from cybersecurity policy into this, kind of, processing of what we learned and reflection because there was a real contrast for me between the pharmaceutical industry and the cybersecurity domain when I think about the emphasis on pre- versus post-deployment monitoring.



And on the one hand, we have in the pharmaceutical domain a real emphasis that has developed around pre-market testing. And there is also an expectation in some circumstances in the pharmaceutical domain for post-deployment testing, as well. But as we learned from our experts in that domain, there has naturally been a real, kind of, emphasis on the pre-market portion of that testing. And in reality, even where post-market monitoring is required and post-market testing is required, it does not always actually happen. And the experts really explained that, you know, part of it is just the incentive structure around the emphasis around, you know, the testing as a pre-market, sort of, entry requirement. And also just the resources that exist among regulators, right. There&#8217;s limited resources, right. And so there are just choices and tradeoffs that they need to make in their own, sort of, enforcement work.



And then on the other hand, you know, in cybersecurity, I never thought about the, kind of, emphasis on things like coordinated vulnerability disclosure and bug bounties that have really developed in the cybersecurity domain. But it&#8217;s a really important part of how we secure technology and enhance cybersecurity over time, where we have these norms that have developed where, you know, security researchers are doing really important research. They&#8217;re finding vulnerabilities in products. And we have norms developed where they report those to the companies that are in a position to address those vulnerabilities. And in some cases, those companies actually pay, through bug bounties, the researchers. And perhaps in some ways, the role of coordinated vulnerability disclosure and bug bounties has evolved the way that it has because there hasn&#8217;t been as much emphasis on the pre-market testing across the board at least in the context of software.



And so you look at those two industries and it was interesting to me to study them to some extent in contrast with each other as this way that the incentives and the resources that need to be applied to testing, sort of, evolve to address where there&#8217;s, kind of, more or less emphasis.



SULLIVAN: It&#8217;s a great point. I mean, I think what we&#8217;re hearing‚Äîand what you&#8217;re saying‚Äîis just exactly this choice ‚Ä¶ like, is there a binary choice between focusing on pre-deployment testing or post-deployment monitoring? And, you know, I think our assumption is that we need to do both. But I&#8217;d love to hear from you on that.&nbsp;



CRAIG DECKARD: Absolutely. I think we need to do both. I&#8217;m very persuaded by this inclination always that there&#8217;s value in trying to really do it all in a risk management context.&nbsp;



And also, we know one of the principles of risk management is you have to prioritize because there are finite resources. And I think that&#8217;s where we get to this challenge in really thinking deeply, especially as we&#8217;re in the early days of AI governance, and we need to be very thoughtful about, you know, tradeoffs that we may not want to be making but we are because, again, these are finite choices and we, kind of, can&#8217;t help but put our finger on the dial in different directions with our choices that, you know, it&#8217;s going to be very difficult to have, sort of, equal emphasis on both. And we need to invest in both, but we need to be very deliberate about the roles of each and how they complement each other and who does which and how we use what we learn from pre- versus post-deployment testing and monitoring.



SULLIVAN: Maybe just spending a little bit more time here ‚Ä¶ you know, a lot of attention goes into testing models upstream, but risk often shows up once they&#8217;re wired into real products and workflows. How much does deployment context change the risk picture from your perspective?&nbsp;



CRAIG DECKARD: Yeah, I ‚Ä¶ such an important question. I really agree that there has been a lot of emphasis to date on, sort of, testing models upstream, the AI model evaluation. And it&#8217;s also really important that we bring more attention into evaluation at the system or application level. And I actually see that in governance conversations, this is actually increasingly raised, this need to have system-level evaluation. We see this across regulation. We also see it in the context of just organizations trying to put in governance requirements for how their organization is going to operate in deploying this technology.&nbsp;



And there&#8217;s a gap today in terms of best practices around system-level testing, perhaps even more than model-level evaluation. And it&#8217;s really important because in a lot of cases, the deployment context really does impact the risk picture, especially with AI, which is a general-purpose technology, and we really saw this in our study of other domains that represented general-purpose technology.&nbsp;



So in the case study that you can find online on nanotechnology, you know, there&#8217;s a real distinction between the risk evaluation and the governance of nanotechnology in different deployment contexts. So the chapter that our expert on nanotechnology wrote really goes into incredibly interesting detail around, you know, deployment of nanotechnology in the context of, like, chemical applications versus consumer electronics versus pharmaceuticals versus construction and how the way that nanoparticles are basically delivered in all those different deployment contexts, as well as, like, what the risk of the actual use scenario is just varies so much. And so there&#8217;s a real need to do that kind of risk evaluation and testing in the deployment context, and this difference in terms of risks and what we learned in these other domains where, you know, there are these different approaches to trying to really think about and gain efficiencies and address risks at a horizontal level versus, you know, taking a real sector-by-sector approach. And to some extent, it seems like it&#8217;s more time intensive to do that sectoral deployment-specific work. And at the same time, perhaps there are efficiencies to be gained by actually doing the work in the context in which, you know, you have a better understanding of the risk that can result from really deploying this technology.&nbsp;



And ultimately, [LAUGHS] really what we also need to think about here is probably, in the end, just like pre- and post-deployment testing, you need both. Not probably; certainly!



So effectively we need to think about evaluation at the model level and the system level as being really important. And it&#8217;s really important to get system evaluation right so that we can actually get trust in this technology in deployment context so we enable adoption in low- and in high-risk deployments in a way that means that we&#8217;ve done risk evaluation in each of those contexts in a way that really makes sense in terms of the resources that we need to apply and ultimately we are able to unlock more applications of this technology in a risk-informed way.



SULLIVAN: That&#8217;s great. I mean, I couldn&#8217;t agree more. I think these contexts, the approaches are so important for trust and adoption, and I&#8217;d love to hear from you, what do we need to advance AI evaluation and testing in our ecosystem? What are some of the big gaps that you&#8217;re seeing, and what role can different stakeholders play in filling them? And maybe an add-on, actually: is there some sort of network effect that could 10x our testing capacity?&nbsp;



CRAIG DECKARD: Absolutely. So there&#8217;s a lot of work that needs to be done, and there&#8217;s a lot of work in process to really level up our whole evaluation and testing ecosystem. We learned, across domains, that there‚Äôs really a need to advance our thinking and our practice in three areas: rigor of testing; standardization of methodologies and processes; and interpretability of test results.&nbsp;



So what we mean by rigor is that we are ensuring that what we are ultimately evaluating in terms of risks is defined in a scientifically valid way and we are able to measure against that risk in a scientifically valid way.&nbsp;



By standardization, what we mean is that there&#8217;s really an accepted and well-understood and, again, a scientifically valid methodology for doing that testing and for actually producing artifacts out of that testing that are meeting those standards. And that sets us up for the final portion on interpretability, which is, like, really the process by which you can trust that the testing has been done in this rigorous and standardized way and that then you have artifacts that result from the testing process that can really be used in the risk management context because they can be interpreted, right.&nbsp;



We understand how to, like, apply weight to them for our risk-management decisions. We actually are able to interpret them in a way that perhaps they inform other downstream risk mitigations that address the risks that we see through the testing results and that we actually understand what limitations apply to the test results and why they may or may not be valid in certain, sort of, deployment contexts, for example, and especially in the context of other risk mitigations that we need to apply. So there&#8217;s a need to advance all three of those things‚Äîrigor, standardization, and interpretability‚Äîto level up the whole testing and evaluation ecosystem.&nbsp;



And when we think about what actors should be involved in that work ‚Ä¶ really everybody, which is both complex to orchestrate but also really important. And so, you know, you need to have the entire value chain involved in really advancing this work. You need the model developers, but you also need the system developers and deployers that are really engaged in advancing the science of evaluation and advancing how we are using these testing artifacts in the risk management process.&nbsp;



When we think about what could actually 10x our testing capacity‚Äîthat&#8217;s the dream, right? We all want to accelerate our progress in this space. You know,&nbsp;I think we need work across all three of those areas of rigor, standardization, and interpretability, but I think one that will really help accelerate our progress across the board is that standardization work, because ultimately, you&#8217;re going to need to have these tests be done and applied across so many different contexts, and ultimately, while we want the whole value chain engaged in the development of the thinking and the science and the standards in this space, we also need to realize that not every organization is necessarily going to have the capacity to, kind of, contribute to developing the ways that we create and use these tests. And there are going to be many organizations that are going to benefit from there being standardization of the methodologies and the artifacts that they can pick up and use.



One thing that I know we&#8217;ve heard throughout this podcast series from our experts in other domains, including Timo [Minssen] in the medical devices context and Ciaran [Martin] in the cybersecurity context, is that there&#8217;s been a recognition, as those domains have evolved, that there&#8217;s a need to calibrate our, sort of, expectations for different actors in the ecosystem and really understand that small businesses, for example, just cannot apply the same degree of resources that others may be able to, to do testing and evaluation and risk management. And so the benefit of having standardized approaches is that those organizations are able to, kind of, integrate into the broader supply chain ecosystem and apply their own, kind of, risk management practices in their own context in a way that is more efficient.&nbsp;



And finally, the last stakeholder that I think is really important to think about in terms of partnership across the ecosystem to really advance the whole testing and evaluation work that needs to happen is government partners, right, and thinking beyond the value chain, the AI supply chain, and really thinking about public-private partnership. That&#8217;s going to be incredibly important to advancing this ecosystem.



You know, I think there&#8217;s been real progress already in the AI evaluation and testing ecosystem in the public-private partnership context. We have been really supportive of the work of the International Network of AI Safety and Security Institutes (opens in new tab)[1] (opens in new tab) and the Center for AI Standards and Innovation (opens in new tab) that all allow for that kind of public-private partnership on actually testing and advancing the science and best practices around standards.&nbsp;



And there are other innovative, kind of, partnerships, as well, in the ecosystem. You know, Singapore has recently launched their Global AI Assurance Pilot (opens in new tab) findings. And that effort really paired application deployers and testers so that consequential impacts at deployment could really be tested. And that&#8217;s a really fruitful, sort of, effort that complements the work of these institutes and centers that are more focused on evaluation at the model level, for example.



And in general, you know, I think that there&#8217;s just really a lot of benefits for us thinking expansively about what we can accomplish through deep, meaningful public-private partnership in this space. I&#8217;m really excited to see where we can go from here with building on, you know, partnerships across AI supply chains and with governments and public-private partnerships.&nbsp;



SULLIVAN: I couldn&#8217;t agree more. I mean, this notion of more engagement across the ecosystem and value chain is super important for us and informs how we think about the space completely.&nbsp;



If you could invite any other industry to the next workshop, maybe quantum safety, space tech, even gaming, who&#8217;s on your wish list? And maybe what are some of the things you&#8217;d want to go deeper on?&nbsp;



CRAIG DECKARD: This is something that we really welcome feedback on if anyone listening has ideas about other domains that would be interesting to study. I will say, I think I shared at the outset of this podcast series, the domains that we added in this round of our efforts in studying other domains actually all came from feedback that we received from, you know, folks we‚Äôd engaged with our first study of other domains and multilateral, sort of, governance institutions. And so we&#8217;re really keen to think about what other domains could be interesting to study. And we are also keen to go deeper, building on what we learned in this round of effort going forward.&nbsp;



One of the areas that I am particularly really interested in is going deeper on, what, sort of, transparency and information sharing about risk evaluation and testing will be really useful to share in different contexts? So across the AI supply chain, what is the information that&#8217;s going to be really meaningful to share between developers and deployers of models and systems and those that are ultimately using this technology in particular deployment contexts? And, you know, I think that we could have much to learn from other general-purpose technologies like genome editing and nanotechnology and cybersecurity, where we could learn a bit more about the kinds of information that they have shared across the development and deployment life cycle and how that has strengthened risk management in general as well as provided a really strong feedback loop around testing and evaluation. What kind of testing is most useful to do at what point in the life cycle, and what artifacts are most useful to share as a result of that testing and evaluation work?



I&#8217;ll say, as Microsoft, we have been really investing in how we are sharing information with our various stakeholders. We also have been engaged with others in industry in reporting what we&#8217;ve done in the context of the Hiroshima AI Process, or HAIP, Reporting Framework (opens in new tab). This is an effort that is really just in its first round of really exploring how this kind of reporting can be really additive to risk management understanding. And again, I think there&#8217;s real opportunity here to look at this kind of reporting and understand, you know, what&#8217;s valuable for stakeholders and where is there opportunity to go further in really informing value chains and policymakers and the public about AI risk and opportunity and what can we learn again from other domains that have done this kind of work over decades to really refine that kind of information sharing.¬†



SULLIVAN: It&#8217;s really great to hear about all the advances that we&#8217;re making on these reports. I&#8217;m guessing a lot of the metrics in there are technical, but sociotechnical impacts‚Äîjobs, maybe misinformation, well-being‚Äîare harder to score. What new measurement ideas are you excited about, and do you have any thoughts on, like, who needs to pilot those?



CRAIG DECKARD: Yeah, it&#8217;s an incredibly interesting question that I think also just speaks to, you know, the breadth of, sort of, testing and evaluation that&#8217;s needed at different points along that AI life cycle and really not getting lost in one particular kind of testing or another pre- or post-deployment and thinking expansively about the risks that we&#8217;re trying to address through this testing.&nbsp;



You know, for example, even with the UK&#8217;s AI Security Institute (opens in new tab) that has just recently launched a new program, a new team, that&#8217;s focused on societal resilience research. I think it&#8217;s going to be a really important area from a sociotechnical impact perspective to bring some focus into as this technology is more widely deployed. Are we understanding the impacts over time as different people and different cultures adopt and use this technology for different purposes?&nbsp;



And I think that&#8217;s an area where there really is opportunity for greater public-private partnership in this research. Because we all share this long-term interest in ensuring that this technology is really serving people and we have to understand the impacts so that we understand, you know, what adjustments we can actually pursue sooner upstream to address those impacts and make sure that this technology is really going to work for all of us and in a way that is consistent with the societal values that we want.&nbsp;



SULLIVAN: So, Amanda, looking ahead, I would love to hear just what&#8217;s going to be on your radar? What&#8217;s top of mind for you in the coming weeks?



CRAIG DECKARD: Well, we are certainly continuing to process all the learnings that we&#8217;ve had from studying these domains. It‚Äôs really been a rich set of insights that we want to make sure we, kind of, fully take advantage of. And, you know, I think these hard questions and, you know, real opportunities to be thoughtful in these early days of AI governance are not, sort of, going away or being easily resolved soon. And so I think we continue to see value in really learning from others, thinking about what&#8217;s distinct in the AI context, but also what we can apply in terms of what other domains have learned.



SULLIVAN: Well, Amanda, it has been such a special experience for me to help illuminate the work of the Office of Responsible AI and our team in Microsoft Research, and [MUSIC] it&#8217;s just really special to see all of the work that we&#8217;re doing to help set the standard for responsible development and deployment of AI. So thank you for joining us today, and thanks for your reflections and discussion.



And to our listeners, thank you so much for joining us for the series. We really hope you enjoyed it!&nbsp;To check out all of our episodes, visit aka.ms/AITestingandEvaluation (opens in new tab), and if you want to learn more about how Microsoft approaches AI governance, you can visit microsoft.com/RAI (opens in new tab).&nbsp;



See you next time!&nbsp;



[MUSIC FADES]‚ÄØ

				
			
			
				Show more			
		
	





AI Testing and Evaluation podcast series








[1] (opens in new tab) Since the launch of the International Network of AI Safety Institutes, the UK renamed its institute the AI Security Institute (opens in new tab).
Opens in a new tabThe post AI Testing and Evaluation: Reflections appeared first on Microsoft Research.
‚Ä¢ Beyond accelerators: Lessons from building foundation models on AWS with Japan‚Äôs GENIAC program
  In 2024, the Ministry of Economy, Trade and Industry (METI) launched the Generative AI Accelerator Challenge (GENIAC)‚Äîa Japanese national program to boost generative AI by providing companies with funding, mentorship, and massive compute resources for foundation model (FM) development. AWS was selected as the cloud provider for GENIAC‚Äôs second cycle (cycle 2). It provided infrastructure and technical guidance for 12 participating organizations. On paper, the challenge seemed straightforward: give each team access to hundreds of GPUs/Trainium chips and let innovation ensue. In practice, successful FM training required far more than raw hardware. 
AWS discovered that allocating over 1,000 accelerators was merely the starting point‚Äîthe real challenge lay in architecting a reliable system and overcoming distributed training obstacles. During GENIAC cycle 2, 12 customers successfully deployed 127 Amazon EC2 P5 instances (NVIDIA H100 TensorCore GPU servers) and 24 Amazon EC2 Trn1 instances (AWS Trainium1 servers) in a single day. Over the following 6 months, multiple large-scale models were trained, including notable projects like Stockmark-2-100B-Instruct-beta, Llama 3.1 Shisa V2 405B, and Llama-3.1-Future-Code-Ja-8B, and others. 
This post shares the key insights from this engagement and valuable lessons for enterprises or national initiatives aiming to build FMs at scale. 
Cross-functional engagement teams 
A crucial early lesson from technical engagement for the GENIAC was that running a multi-organization, national-scale machine learning (ML) initiative requires coordinated support across diverse internal teams. AWS established a virtual team that brought together account teams, specialist Solutions Architects, and service teams. The GENIAC engagement model thrives on close collaboration between customers and a multi-layered AWS team structure, as illustrated in the following figure. 
 
Customers (Cx) typically consist of business and technical leads, including ML and platform engineers, and are responsible for executing training workloads. AWS account teams (Solutions Architects and Account Managers) manage the relationship, maintain documentation, and maintain communication flows with customers and internal specialists. The World Wide Specialist Organization (WWSO) Frameworks team specializes in large-scale ML workloads, with a focus on core HPC and container services such as AWS ParallelCluster, Amazon Elastic Kubernetes Service (Amazon EKS), and Amazon SageMaker HyperPod. The WWSO Frameworks team is responsible for establishing this engagement structure and supervising technical engagements in this program. They lead the engagement in partnership with other stakeholders and serve as an escalation point for other stakeholders. They work directly with the service teams‚ÄîAmazon Elastic Compute Cloud (Amazon EC2), Amazon Simple Storage Service (Amazon S3), Amazon FSx, and SageMaker HyperPod‚Äîto help navigate engagements, escalations (business and technical), and make sure the engagement framework is in working order. They provide guidance on training and inference to customers and educate other teams on the technology. The WWSO Frameworks team worked closely with Lead Solutions Architects (Lead SAs), a role specifically designated to support GENIAC engagements. These Lead SAs serve as a cornerstone of this engagement. They are an extension of the Frameworks specialist team and work directly with customers and the account teams. They interface with customers and engage their Framework specialist counterparts when clarification or further expertise is required for in-depth technical discussions or troubleshooting. With this layered structure, AWS can scale technical guidance effectively across complex FM training workloads. 
Another critical success factor for GENIAC was establishing robust communication channels between customers and AWS members. The foundation of our communication strategy was a dedicated internal Slack channel for GENIAC program coordination, connecting AWS account teams with lead SAs. This channel enabled real-time troubleshooting, knowledge sharing, and rapid escalation of customer issues to the appropriate technical specialists and service team members. Complementing this was an external Slack channel that bridged AWS teams with customers, creating a collaborative environment where participants could ask questions, share insights, and receive immediate support. This direct line of communication significantly reduced resolution times and fostered a community of practice among participants. 
AWS maintained comprehensive workload tracking documents, which clarifies each customer‚Äôs training implementation details (model architecture, distributed training frameworks, and related software components) alongside infrastructure specifications (instance types and quantities, cluster configurations for AWS ParallelCluster or SageMaker HyperPod deployments, and storage solutions including Amazon FSx for Lustre and Amazon S3). This tracking system also maintained a chronological history of customer interactions and support cases. In addition, the engagement team held weekly review meetings to track outstanding customer inquiries and technical issues. This regular cadence made it possible for team members to share lessons learned and apply them to their own customer engagements, fostering continuous improvement and knowledge transfer across the program. 
With a structured approach to communication and documentation, we could identify common challenges, such as misconfigured NCCL library impacting multi-node performance, share solutions across teams, and continuously refine our engagement model. The detailed tracking system provided valuable insights for future GENIAC cycles, helping us anticipate customer needs and proactively address potential bottlenecks in the FM development process. 
Reference architectures 
Another early takeaway was the importance of solid reference architectures. Rather than let each team configure their own cluster from scratch, AWS created pre-validated templates and automation for two main approaches: AWS ParallelCluster (for a user-managed HPC cluster) and SageMaker HyperPod (for a managed, resilient cluster service). These reference architectures covered the full stack‚Äîfrom compute, network, and storage to container environments and monitoring‚Äîand were delivered as a GitHub repository so teams could deploy them with minimal friction. 
AWS ParallelCluster proved invaluable as an open source cluster management tool for multi-node GPU training. AWS ParallelCluster automates the setup of a Slurm-based HPC cluster on AWS. AWS ParallelCluster simplifies cluster provisioning based on the open source Slurm scheduler, using a simple YAML config to stand up the environment. For the GEINIAC program, AWS also offered SageMaker HyperPod as another option for some teams. SageMaker HyperPod is a managed service that provisions GPU and Trainium clusters for large-scale ML. HyperPod integrates with orchestrators like Slurm or Kubernetes (Amazon EKS) for scheduling, providing additional managed functionality around cluster resiliency. By including reference architectures for both AWS ParallelCluster and SageMaker HyperPod, the GENIAC program gave participants flexibility‚Äîsome opted for the fine-grained control of managing their own HPC cluster, whereas others preferred the convenience and resilience of a managed SageMaker HyperPod cluster. 
The reference architecture (shown in the following diagram) seamlessly combines compute, networking, storage, and monitoring into an integrated system specifically designed for large-scale FM training. 
 
The base infrastructure stack is available as an AWS CloudFormation template that provisions the complete infrastructure stack with minimal effort. This template automatically configures a dedicated virtual private cloud (VPC) with optimized networking settings and implements a high-performance FSx for Lustre file system for training data (complemented by optional Amazon FSx for OpenZFS support for shared home directories). The architecture is completed with an S3 bucket that provides durable, long-term storage for datasets and model checkpoints, maintaining data availability well beyond individual training cycles. This reference architecture employs a hierarchical storage approach that balances performance and cost-effectiveness. It uses Amazon S3 for durable, long-term storage of training data and checkpoints, and links this bucket to the Lustre file system through a data repository association (DRA). The DRA enables automatic and transparent data transfer between Amazon S3 and FSx for Lustre, allowing high-performance access without manual copying. You can use the following CloudFormation template to create the S3 bucket used in this architecture. 
The optional monitoring infrastructure combines Amazon Managed Service for Prometheus and Amazon Managed Grafana (or self-managed Grafana service running on Amazon EC2) to provide comprehensive observability. It integrated DCGM Exporter for GPU metrics and EFA Exporter for network metrics, enabling real-time monitoring of system health and performance. This setup allows for continuous tracking of GPU health, network performance, and training progress, with automated alerting for anomalies through Grafana Dashboards. For example, the GPU Health Dashboard (see the following screenshot) provides metrics of common GPU errors, including Uncorrectable Remapped Rows, Correctable Remapped Rows, XID Error Codes, Row Remap Failure, Thermal Violations, and Missing GPUs (from Nvidia-SMI), helping users identify hardware failures as quickly as possible. 
 
Reproducible deployment guides and structured enablement sessions 
Even the best reference architectures are only useful if teams know how to use them. A critical element of GENIAC‚Äôs success was reproducible deployment guides and structured enablement through workshops.On October 3, 2024, AWS Japan and the WWSO Frameworks team conducted a mass enablement session for GENIAC Cycle 2 participants, inviting Frameworks team members from the United States to share best practices for FM training on AWS. 
The enablement session welcomed over 80 participants and provided a comprehensive mix of lectures, hands-on labs, and group discussions‚Äîearning a CSAT score of 4.75, reflecting its strong impact and relevance to attendees. The lecture sessions covered infrastructure fundamentals, exploring orchestration options such as AWS ParallelCluster, Amazon EKS, and SageMaker HyperPod, along with the software components necessary to build and train large-scale FMs using AWS. The sessions highlighted practical challenges in FM development‚Äîincluding massive compute requirements, scalable networking, and high-throughput storage‚Äîand mapped them to appropriate AWS services and best practices. (For more information, see the slide deck from the lecture session.) Another session focused on best practices, where attendees learned to set up performance dashboards with Prometheus and Grafana, monitor EFA traffic, and troubleshoot GPU failures using NVIDIA‚Äôs DCGM toolkit and custom Grafana dashboards based on the Frameworks team‚Äôs experience managing a cluster with 2,000 P5 instances. 
Additionally, the WWSO team prepared workshops for both AWS ParallelCluster (Machine Learning on AWS ParallelCluster) and SageMaker HyperPod (Amazon SageMaker HyperPod Workshop), providing detailed deployment guides for the aforementioned reference architecture. Using these materials, participants conducted hands-on exercises deploying their training clusters using Slurm with file systems including FSx for Lustre and FSx for OpenZFS, running multi-node PyTorch distributed training. Another segment of the workshop focused on observability and performance tuning, teaching participants how to monitor resource utilization, network throughput (EFA traffic), and system health. By the end of these enablement sessions, customers and supporting AWS engineers had established a shared baseline of knowledge and a toolkit of best practices. Using the assets and knowledge gained during the workshops, customers participated in onboarding sessions‚Äîstructured, hands-on meetings with their Lead SAs. These sessions differed from the earlier workshops by focusing on customer-specific cluster deployments tailored to each team‚Äôs unique use case. During each session, Lead SAs worked directly with teams to deploy training environments, validate setup using NCCL tests, and resolve technical issues in real time. 
Customer feedback 

 ‚ÄúTo fundamentally solve data entry challenges, we significantly improved processing accuracy and cost-efficiency by applying two-stage reasoning and autonomous learning with SLM and LLM for regular items, and visual learning with VLM using 100,000 synthetic data samples for detailed items. We also utilized Amazon EC2 P5 instances to enhance research and development efficiency. These ambitious initiatives were made possible thanks to the support of many people, including AWS. We are deeply grateful for their extensive support.‚Äù 
 ‚Äì Takuma Inoue, Executive Officer, CTO at AI Inside 
 ‚ÄúFuture chose AWS to develop large-scale language models specialized for Japanese and software development at GENIAC. When training large-scale models using multiple nodes, Future had concerns about environment settings such as inter-node communication, but AWS had a wide range of tools, such as AWS ParallelCluster, and we received strong support from AWS Solutions Architects, which enabled us to start large-scale training quickly.‚Äù 
 ‚Äì Makoto Morishita, Chief Research Engineer at Future
 
Results and looking ahead 
GENIAC has demonstrated that training FMs at scale is fundamentally an organizational challenge, not merely a hardware one. Through structured support, reproducible templates, and a cross-functional engagement team (WWSO Frameworks Team, Lead SAs, and Account Teams), even small teams can successfully execute massive workloads in the cloud. Thanks to this structure, 12 customers launched over 127 P5 instances and 24 Trn1 instances across multiple AWS Regions, including Asia Pacific (Tokyo), in a single day. Multiple large language models (LLMs) and custom models were trained successfully, including a 32B multimodal model on Trainium and a 405B tourism-focused multilingual model.The technical engagement framework established through GENIAC Cycle 2 has provided crucial insights into large-scale FM development. Building on this experience, AWS is advancing improvements across multiple dimensions: engagement models, technical assets, and implementation guidance. We are strengthening cross-functional collaboration and systematizing knowledge sharing to establish a more efficient support structure. Reference architectures and automated training templates continue to be enhanced, and practical technical workshops and best practices are being codified based on lessons learned.AWS has already begun preparations for the next cycle of GENIAC. As part of the onboarding process, AWS hosted a comprehensive technical event in Tokyo on April 3, 2025, to equip FM builders with hands-on experience and architectural guidance. The event, attended by over 50 participants, showcased the commitment AWS has to supporting scalable, resilient generative AI infrastructure. 
 
The event highlighted the technical engagement model of AWS for GENIAC, alongside other support mechanisms, including the LLM Development Support Program and Generative AI Accelerator. The day featured an intensive workshop on SageMaker HyperPod and Slurm, where participants gained hands-on experience with multi-node GPU clusters, distributed PyTorch training, and observability tools. Sessions covered essential topics, including containerized ML, distributed training strategies, and AWS purpose-built silicon solutions. Classmethod Inc. shared practical SageMaker HyperPod insights, and AWS engineers demonstrated architectural patterns for large-scale GPU workloads. The event showcased AWS‚Äôs end-to-end generative AI support landscape, from infrastructure to deployment tools, setting the stage for GENIAC Cycle 3. As AWS continues to expand its support for FM development, the success of GENIAC serves as a blueprint for enabling organizations to build and scale their AI capabilities effectively. 
Through these initiatives, AWS will continue to provide robust technical support, facilitating the smooth execution of large-scale FM training. We remain committed to contributing to the advancement of generative AI development all over the world through our technical expertise. 
This post was contributed by AWS GENIAC Cycle 2 core members Masato Kobayashi, Kenta Ichiyanagi, and Satoshi Shirasawa, Accelerated Computing Specialist Mai Kiuchi, as well as Lead SAs Daisuke Miyamoto, Yoshitaka Haribara, Kei Sasaki, Soh Ohara, and Hiroshi Tokoyo with Executive Sponsorship from Toshi Yasuda. Hiroshi Hata and Tatsuya Urabe also provided support as core member and Lead SA during their time at AWS. 
The authors extend their gratitude to WWSO Frameworks members Maxime Hugues, Matthew Nightingale, Aman Shanbhag, Alex Iankoulski, Anoop Saha, Yashesh Shroff, Natarajan Chennimalai Kumar, Shubha Kumbadakone, and Sundar Ranganathan for their technical contributions. Pierre-Yves Aquilanti provided in-depth support during his time at AWS. 
 
About the authors 
Keita Watanabe is a Senior Specialist Solutions Architect on the AWS WWSO Frameworks team. His background is in machine learning research and development. Prior to joining AWS, Keita worked in the ecommerce industry as a research scientist developing image retrieval systems for product search. He leads GENIAC technical engagements. 
Masaru Isaka is a Principal Business Development on the AWS WWSO Frameworks team, specializing in machine learning and generative AI solutions. Having engaged with GENIAC since its inception, he leads go-to-market strategies for AWS‚Äôs generative AI offerings.
‚Ä¢ Streamline deep learning environments with Amazon Q Developer and MCP
  Data science teams working with artificial intelligence and machine learning (AI/ML) face a growing challenge as models become more complex. While Amazon Deep Learning Containers (DLCs) offer robust baseline environments out-of-the-box, customizing them for specific projects often requires significant time and expertise. 
In this post, we explore how to use Amazon Q Developer and Model Context Protocol (MCP) servers to streamline DLC workflows to automate creation, execution, and customization of DLC containers. 
AWS DLCs 
AWS DLCs provide generative AI practitioners with optimized Docker environments to train and deploy large language models (LLMs) in their pipelines and workflows across Amazon Elastic Compute Cloud (Amazon EC2), Amazon Elastic Kubernetes Service (Amazon EKS), and Amazon Elastic Container Service (Amazon ECS). AWS DLCs are targeted for self-managed machine learning (ML) customers who prefer to build and maintain their AI/ML environments on their own, want instance-level control over their infrastructure, and manage their own training and inference workloads. Provided at no additional cost, the DLCs come pre-packaged with CUDA libraries, popular ML frameworks, and the Elastic Fabric Adapter (EFA) plug-in for distributed training and inference on AWS. They automatically configure a stable connected environment, which eliminates the need for customers to troubleshoot common issues such as version incompatibilities. DLCs are available as Docker images for training and inference with PyTorch and TensorFlow on Amazon Elastic Container Registry (Amazon ECR). 
The following figure illustrates the ML software stack on AWS. 
 
DLCs are kept current with the latest version of frameworks and drivers, tested for compatibility and security, and offered at no additional cost. They are also straightforward to customize by following our recipe guides. Using AWS DLCs as a building block for generative AI environments reduces the burden on operations and infrastructure teams, lowers TCO for AI/ML infrastructure, accelerates the development of generative AI products, and helps generative AI teams focus on the value-added work of deriving generative AI-powered insights from the organization‚Äôs data. 
Challenges with DLC customization 
Organizations often encounter a common challenge: they have a DLC that serves as an excellent foundation, but it requires customization with specific libraries, patches, or proprietary toolkits. The traditional approach to this customization involves the following steps: 
 
 Rebuilding containers manually 
 Installing and configuring additional libraries 
 Executing extensive testing cycles 
 Creating automation scripts for updates 
 Managing version control across multiple environments 
 Repeating this process several times annually 
 
This process often requires days of work from specialized teams, with each iteration introducing potential errors and inconsistencies. For organizations managing multiple AI projects, these challenges compound quickly, leading to significant operational overhead and potential delays in development cycles. 
Using the Amazon Q CLI with a DLC MCP server 
Amazon Q acts as your AI-powered AWS expert, offering real-time assistance to help you build, extend, and operate AWS applications through natural conversations. It combines deep AWS knowledge with contextual understanding to provide actionable guidance when you need it. This tool can help you navigate AWS architecture, manage resources, implement best practices, and access documentation‚Äîall through natural language interactions. 
The Model Context Protocol (MCP) is an open standard that enables AI assistants to interact with external tools and services. Amazon Q Developer CLI now supports MCP, allowing you to extend Q‚Äôs capabilities by connecting it to custom tools and services. 
By taking advantage of the benefits of both Amazon Q and MCP, we have implemented a DLC MCP server that transforms container management from complex command line operations into simple conversational instructions. Developers can securely create, customize, and deploy DLCs using natural language prompts. This solution potentially reduces the technical overhead associated with DLC workflows. 
Solution overview 
The following diagram shows the interaction between users using Amazon Q with a DLC MCP server. 
 
The DLC MCP server provides six core tools: 
 
 Container management service ‚Äì This service helps with core container operations and DLC image management: 
   
   Image discovery ‚Äì List and filter available DLC images by framework, Python version, CUDA version, and repository type 
   Container runtime ‚Äì Run DLC containers locally with GPU support 
   Distributed training setup ‚Äì Configure multi-node distributed training environments 
   AWS integration ‚Äì Automatic Amazon ECR authentication and AWS configuration validation 
   Environment setup ‚Äì Check GPU availability and Docker configuration 
    
 Image building service ‚Äì This service helps create and customize DLC images for specific ML workloads: 
   
   Base image selection ‚Äì Browse available DLC base images by framework and use case 
   Custom Dockerfile generation ‚Äì Create optimized Dockerfiles with custom packages and configurations 
   Image building ‚Äì Build custom DLC images locally or push to Amazon ECR 
   Package management ‚Äì Install system packages, Python packages, and custom dependencies 
   Environment configuration ‚Äì Set environment variables and custom commands 
    
 Deployment service ‚Äì This service helps with deploying DLC images across AWS compute services: 
   
   Multi-service deployment ‚Äì Support for Amazon EC2, Amazon SageMaker, Amazon ECS, and Amazon EKS 
   SageMaker integration ‚Äì Create models and endpoints for inference 
   Container orchestration ‚Äì Deploy to ECS clusters and EKS clusters 
   Amazon EC2 deployment ‚Äì Launch EC2 instances with DLC images 
   Status monitoring ‚Äì Check deployment status and endpoint health 
    
 Upgrade service ‚Äì This service helps upgrade or migrate DLC images to newer framework versions: 
   
   Upgrade path analysis ‚Äì Analyze compatibility between current and target framework versions 
   Migration planning ‚Äì Generate upgrade strategies with compatibility warnings 
   Dockerfile generation ‚Äì Create upgrade Dockerfiles that preserve customizations 
   Version migration ‚Äì Upgrade PyTorch, TensorFlow, and other frameworks 
   Custom file preservation ‚Äì Maintain custom files and configurations during upgrades 
    
 Troubleshooting service ‚Äì This service helps diagnose and resolve DLC-related issues: 
   
   Error diagnosis ‚Äì Analyze error messages and provide specific solutions 
   Framework compatibility ‚Äì Check version compatibility and requirements 
   Performance optimization ‚Äì Get framework-specific performance tuning tips 
   Common issues ‚Äì Maintain a database of solutions for frequent DLC problems 
   Environment validation ‚Äì Verify system requirements and configurations 
    
 Best practices service ‚Äì This service provides best practices on the following: 
   
   Security guidelines ‚Äì Comprehensive security best practices for DLC deployments 
   Cost optimization ‚Äì Strategies to reduce costs while maintaining performance 
   Deployment patterns ‚Äì System-specific deployment recommendations 
   Framework guidance ‚Äì Framework-specific best practices and optimizations 
   Custom image guidelines ‚Äì Best practices for creating maintainable custom images 
    
 
Prerequisites 
Follow the installation steps in the GitHub repo to set up the DLC MCP server and Amazon Q CLI in your workstation. 
Interact with the DLC MPC server 
You‚Äôre now ready to start using the Amazon Q CLI with DLC MCP server. Let‚Äôs start with the CLI, as shown in the following screenshot. You can also check the default tools and loaded server tools in the CLI with the /tools command. 
 
In the following sections, we demonstrate three separate use cases using the DLC MPC server. 
Run a DLC training container 
In this scenario, our goal is to identify a PyTorch base image, launch the image in a local Docker container, and run a simple test script to verify the container. 
We start with the prompt ‚ÄúRun Pytorch container for training.‚Äù 
The MCP server automatically handles the entire workflow: it authenticates with Amazon ECR and pulls the appropriate PyTorch DLC image. 
 
Amazon Q used the GPU image because we didn‚Äôt specify the device type. Let‚Äôs try asking for a CPU image and see its response. After identifying the image, the server pulls the image from the ECR repository successfully and runs the container in your environment. Amazon Q has built-in tools that handle bash scripting and file operations, and a few other standard tools that speed up the runtime. 
 
After the image is identified, the run_the_container tool from the DLC MCP server is used to start the container locally, and Amazon Q tests it with simple scripts to make sure the container is loading and running the scripts as expected. In our example, our test script checks the PyTorch version. 
 
We further prompt the server to perform a training task on the PyTorch CPU training container using a popular dataset. Amazon Q autonomously selects the CIFAR-10 dataset for this example. Amazon Q gathers the dataset and model information based on its pretrained knowledge without human intervention. Amazon Q prompts the user about the choices it‚Äôs making on your behalf. If needed, you can specify the required model or dataset directly in the prompt. 
 
When the scripts are ready for execution, the server runs the training job on the container. After successfully training, it summarizes the training job results along with the model path. 
 
Create a custom DLC with NVIDIA‚Äôs NeMO Toolkit 
In this scenario, we walk through the process of enhancing an existing DLC with NVIDIA‚Äôs NeMo toolkit. NeMo, a powerful framework for conversational AI, is built on PyTorch Lightning and is designed for efficient development of AI models. Our goal is to create a custom Docker image that integrates NeMo into the existing PyTorch GPU training container. This section demonstrates how to create a custom DLC image that combines the PyTorch GPU environment with the advanced capabilities of the NeMo toolkit. 
The server invokes the create_custom_dockerfile tool from our MCP server‚Äôs image building module. We can use this tool to specify our base image from Amazon ECR and add custom commands to install NeMo. 
 
This Dockerfile serves as a blueprint for our custom DLC image, making sure the necessary components are in place. Refer to the Dockerfile in the GitHub repo. 
 
After the custom Dockerfile is created, the server starts building our custom DLC image. To achieve this, Amazon Q uses the build_custom_dlc_image tool in the image building module. This tool streamlines the process by setting up the build environment with specified arguments. This step transforms our base image into a specialized container tailored for NeMo-based AI development. 
 
The build command pulls from a specified ECR repository, making sure we‚Äôre working with the most up-to-date base image. The image also comes with related packages and libraries to test NeMo; you can specify the requirements in the prompt if required. 
 
NeMo is now ready to use with a quick environment check to make sure our tools are in the toolbox before we begin. You can run a simple Python script in the Docker container that shows you everything you want to know. In the following screenshot, you can see the PyTorch version 2.7.1+cu128 and PyTorch Lightning version 2.5.2. The NeMo modules are loaded and ready for use. 
 
The DLC MCP server has transformed the way we create custom DLC images. Traditionally, setting up environments, managing dependencies, and writing Dockerfiles for AI development was a time-consuming and error-prone process. It often took hours, if not days, to get everything just right. But now, with Amazon Q along with the DLC MCP server, you can accomplish this in just a few minutes. 
For NeMo-based AI applications, you can focus more on model development and less on infrastructure setup. The standardized process makes it straightforward to move from development to production, and you can be confident that your container will work the same way each time it‚Äôs built. 
Add the latest version of the DeepSeek model to a DLC 
In this scenario, we explore how to enhance an existing PyTorch GPU DLC by adding the DeepSeek model. Unlike our previous example where we added the NeMo toolkit, here we integrate a powerful language model using the latest PyTorch GPU container as our base. Let‚Äôs start with the prompt shown in the following screenshot. 
 
Amazon Q interacts with DLC MCP server to list the DLC images and check for available PyTorch GPU images. After the base image is picked, multiple tools from the DLC MCP server, such as create_custom_dockerfile and build_custom_dlc_image, are used to create and build the Dockerfile. The key components in Dockerfile for this example are: 
 
 
{
    "base_image": "763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-inference:2.6.0-gpu-py312-cu124-ubuntu22.04-ec2",
    "custom_commands": [
        "mkdir -p /opt/ml/model",
        "mkdir -p /opt/ml/code",
        "pip install --upgrade torch torchvision torchaudio"
    ],
    "environment_variables": {
        "CUDA_VISIBLE_DEVICES": "0",
        "HF_HOME": "/opt/ml/model",
        "MODEL_NAME": "deepseek-ai/deepseek-coder-6.7b-instruct"
    }
}
 
 
This configuration sets up our working directories, handles the PyTorch upgrade to 2.7.1 (latest), and sets essential environment variables for DeepSeek integration. The server also includes important Python packages like transformers, accelerate, and Flask for a production-ready setup. 
Before diving into the build process, let‚Äôs understand how the MCP server prepares the groundwork. When you initiate the process, the server automatically generates several scripts and configuration files. This includes: 
 
 A custom Dockerfile tailored to your requirements 
 Build scripts for container creation and pushing to Amazon ECR 
 Test scripts for post-build verification 
 Inference server setup scripts 
 Requirement files listing necessary dependencies 
 
The build process first handles authentication with Amazon ECR, establishing a secure connection to the AWS container registry. Then, it either locates your existing repository or creates a new one if needed. In the image building phase, the base PyTorch 2.6.0 image gets transformed with an upgrade to version 2.7.1, complete with CUDA 12.8 support. The DeepSeek Coder 6.7B Instruct model integration happens seamlessly. 
 
After the build is successful, we move to the testing phase using the automatically generated test scripts. These scripts help verify both the basic functionality and production readiness of the DeepSeek container. To make sure our container is ready for deployment, we spin it up using the code shown in the following screenshot. 
 
The container initialization takes about 3 seconds‚Äîa remarkably quick startup time that‚Äôs crucial for production environments. The server performs a simple inference check using a curl command that sends a POST request to our local endpoint. This test is particularly important because it verifies not just the model‚Äôs functionality, but also the entire infrastructure we‚Äôve set up. 
 
We have successfully created a powerful inference image that uses the DLC PyTorch container‚Äôs performance optimizations and GPU acceleration while seamlessly integrating DeepSeek‚Äôs advanced language model capabilities. The result is more than just a development tool‚Äîit‚Äôs a production-ready solution complete with health checks, error handling, and optimized inference performance. This makes it ideal for deployment in environments where reliability and performance are critical. This integration creates new opportunities for developers and organizations looking to implement advanced language models in their applications. 
Conclusion 
The combination of DLC MCP and Amazon Q transforms what used to be weeks of DevOps work into a conversation with your tools. This not only saves time and reduces errors, but also helps teams focus on their core ML tasks rather than infrastructure management. 
For more information about Amazon Q Developer, refer to the Amazon Q Developer product page to find video resources and blog posts. You can share your thoughts with us in the comments section or in the issues section of the project‚Äôs GitHub repository. 
 
About the authors 
Sathya Balakrishnan is a Sr. Cloud Architect in the Professional Services team at AWS, specializing in data and ML solutions. He works with US federal financial clients. He is passionate about building pragmatic solutions to solve customers‚Äô business problems. In his spare time, he enjoys watching movies and hiking with his family. 
Jyothirmai Kottu is a Software Development Engineer in the Deep Learning Containers team at AWS, specializing in building and maintaining robust AI and ML infrastructure. Her work focuses on enhancing the performance, reliability, and usability of DLCs, which are crucial tools for AI/ML practitioners working with AI frameworks. She is passionate about making AI/ML tools more accessible and efficient for developers around the world. Outside of her professional life, she enjoys a good coffee, yoga, and exploring new places with family and friends. 
Arindam Paul is a Sr. Product Manager in SageMaker AI team at AWS responsible for Deep Learning workloads on SageMaker, EC2, EKS, and ECS. He is passionate about using AI to solve customer problems. In his spare time, he enjoys working out and gardening.
‚Ä¢ Build an AI-powered automated summarization system with Amazon Bedrock and Amazon Transcribe using Terraform
  Extracting meaningful insights from unstructured data presents significant challenges for many organizations. Meeting recordings, customer interactions, and interviews contain invaluable business intelligence that remains largely inaccessible due to the prohibitive time and resource costs of manual review. Organizations frequently struggle to efficiently capture and use key information from these interactions, resulting in not only productivity gaps but also missed opportunities to use critical decision-making information. 
This post introduces a serverless meeting summarization system that harnesses the advanced capabilities of Amazon Bedrock and Amazon Transcribe to transform audio recordings into concise, structured, and actionable summaries. By automating this process, organizations can reclaim countless hours while making sure key insights, action items, and decisions are systematically captured and made accessible to stakeholders. 
Many enterprises have standardized on infrastructure as code (IaC) practices using Terraform, often as a matter of organizational policy. These practices are typically driven by the need for consistency across environments, seamless integration with existing continuous integration and delivery (CI/CD) pipelines, and alignment with broader DevOps strategies. For these organizations, having AWS solutions implemented with Terraform helps them maintain governance standards while adopting new technologies. Enterprise adoption of IaC continues to grow rapidly as organizations recognize the benefits of automated, version-controlled infrastructure deployment. 
This post addresses this need by providing a complete Terraform implementation of a serverless audio summarization system. With this solution, organizations can deploy an AI-powered meeting summarization solution while maintaining their infrastructure governance standards. The business benefits are substantial: reduced meeting follow-up time, improved knowledge sharing, consistent action item tracking, and the ability to search across historical meeting content. Teams can focus on acting upon meeting outcomes rather than struggling to document and distribute them, driving faster decision-making and better organizational alignment. 
What are Amazon Bedrock and Amazon Transcribe? 
Amazon Bedrock is a fully managed service that offers a choice of high-performing foundation models (FMs) from leading AI companies like AI21 Labs, Anthropic, Cohere, DeepSeek, Luma, Meta, Mistral AI, poolside (coming soon), Stability AI, TwelveLabs (coming soon), Writer, and Amazon Nova through a single API, along with a broad set of capabilities to build generative AI applications with security, privacy, and responsible AI. With Amazon Bedrock, you can experiment with and evaluate top FMs for your use case, customize them with your data using techniques such as fine-tuning and Retrieval Augmented Generation (RAG), and build agents that execute tasks using your enterprise systems and data sources. 
Amazon Transcribe is a fully managed, automatic speech recognition (ASR) service that makes it straightforward for developers to add speech to text capabilities to their applications. It is powered by a next-generation, multi-billion parameter speech FM that delivers high-accuracy transcriptions for streaming and recorded speech. Thousands of customers across industries use it to automate manual tasks, unlock rich insights, increase accessibility, and boost discoverability of audio and video content. 
Solution overview 
Our comprehensive audio processing system combines powerful AWS services to create a seamless end-to-end solution for extracting insights from audio content. The architecture consists of two main components: a user-friendly frontend interface that handles customer interactions and file uploads, and a backend processing pipeline that transforms raw audio into valuable, structured information. This serverless architecture facilitates scalability, reliability, and cost-effectiveness while delivering insightful AI-driven analysis capabilities without requiring specialized infrastructure management. 
The frontend workflow consists of the following steps: 
 
 Users upload audio files through a React-based frontend delivered globally using Amazon CloudFront. 
 Amazon Cognito provides secure authentication and authorization for users. 
 The application retrieves meeting summaries and statistics through AWS AppSync GraphQL API, which invokes AWS Lambda functions to query. 
 
The processing consists of the following steps: 
 
 Audio files are stored in an Amazon Simple Storage Service (Amazon S3) bucket. 
 When an audio file is uploaded to Amazon S3 in the audio/{user_id}/ prefix, an S3 event notification sends a message to an Amazon Simple Queue Service (Amazon SQS) queue. 
 The SQS queue triggers a Lambda function, which initiates the processing workflow. 
 AWS Step Functions orchestrates the entire transcription and summarization workflow with built-in error handling and retries. 
 Amazon Transcribe converts speech to text with high accuracy. 
 uses an FM (specifically Anthropic‚Äôs Claude) to generate comprehensive, structured summaries. 
 Results are stored in both Amazon S3 (raw data) and Amazon DynamoDB (structured data) for persistence and quick retrieval. 
 
For additional security, AWS Identity and Access Management helps manage identities and access to AWS services and resources. 
The following diagram illustrates this architecture. 
 
This architecture provides several key benefits: 
 
 Fully serverless ‚Äì Automatic scaling and no infrastructure to manage 
 Event-driven ‚Äì Real-time responses from components based on events 
 Resilient ‚Äì Built-in error handling and retry mechanism 
 Secure ‚Äì Authentication, authorization, and encryption throughout 
 Cost-effective ‚Äì Pay-per-use price model 
 Globally available ‚Äì Content delivery optimized for users worldwide 
 Highly extensible ‚Äì Seamless integration with additional services 
 
Let‚Äôs walk through the key components of our solution in more detail. 
Project structure 
Our meeting audio summarizer project follows a structure with frontend and backend components: 
 
 sample-meeting-audio-summarizer-in-terraform/&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
‚îú‚îÄ‚îÄ backend/&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
‚îÇ &nbsp; ‚îú‚îÄ‚îÄ functions/ &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; # Lambda function code&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
‚îÇ &nbsp; ‚îÇ &nbsp; ‚îú‚îÄ‚îÄ audio-processing/ &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# Audio processing functions&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
‚îÇ &nbsp; ‚îÇ &nbsp; ‚îú‚îÄ‚îÄ authentication/ &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# Authentication functions&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
‚îÇ &nbsp; ‚îÇ &nbsp; ‚îú‚îÄ‚îÄ data-access/ &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; # Data access functions&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;
‚îÇ &nbsp; ‚îÇ &nbsp; ‚îú‚îÄ‚îÄ queue-processing/ &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# SQS queue processing functions&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
‚îÇ &nbsp; ‚îÇ &nbsp; ‚îú‚îÄ‚îÄ summarization/ &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; # Summarization functions&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
‚îÇ &nbsp; ‚îÇ &nbsp; ‚îú‚îÄ‚îÄ transcription/ &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; # Transcription functions&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
‚îÇ &nbsp; ‚îÇ &nbsp; ‚îî‚îÄ‚îÄ zipped/ &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# Zipped Lambda functions for deployment&nbsp;
‚îÇ &nbsp; ‚îî‚îÄ‚îÄ terraform/ &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; # Infrastructure as Code&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
‚îÇ &nbsp; &nbsp; &nbsp; ‚îú‚îÄ‚îÄ modules/ &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; # Terraform modules&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
‚îÇ &nbsp; &nbsp; &nbsp; ‚îÇ &nbsp; ‚îú‚îÄ‚îÄ api/ &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; # AppSync GraphQL API&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
‚îÇ &nbsp; &nbsp; &nbsp; ‚îÇ &nbsp; ‚îú‚îÄ‚îÄ auth/ &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# Cognito authentication&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
‚îÇ &nbsp; &nbsp; &nbsp; ‚îÇ &nbsp; ‚îú‚îÄ‚îÄ compute/ &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; # Lambda functions&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
‚îÇ &nbsp; &nbsp; &nbsp; ‚îÇ &nbsp; ‚îú‚îÄ‚îÄ messaging/ &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; # SQS queues and S3 notifications&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;
‚îÇ &nbsp; &nbsp; &nbsp; ‚îÇ &nbsp; ‚îú‚îÄ‚îÄ network/ &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; # CloudFront and S3 website&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
‚îÇ &nbsp; &nbsp; &nbsp; ‚îÇ &nbsp; ‚îú‚îÄ‚îÄ orchestration/ &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; # Step Functions&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
‚îÇ &nbsp; &nbsp; &nbsp; ‚îÇ &nbsp; ‚îú‚îÄ‚îÄ queue-processor/ &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; # Queue processing Lambda&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
‚îÇ &nbsp; &nbsp; &nbsp; ‚îÇ &nbsp; ‚îî‚îÄ‚îÄ storage/ &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; # S3 and DynamoDB&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
‚îÇ &nbsp; &nbsp; &nbsp; ‚îú‚îÄ‚îÄ main.tf &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# Main Terraform configuration&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
‚îÇ &nbsp; &nbsp; &nbsp; ‚îú‚îÄ‚îÄ outputs.tf &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; # Output values&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
‚îÇ &nbsp; &nbsp; &nbsp; ‚îú‚îÄ‚îÄ variables.tf &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; # Input variables&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
‚îÇ &nbsp; &nbsp; &nbsp; ‚îî‚îÄ‚îÄ terraform.tfvars &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; # Variable values&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
‚îú‚îÄ‚îÄ docs/ &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# Documentation and architecture diagrams
‚îú‚îÄ‚îÄ frontend/ &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# React web application&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
‚îÇ &nbsp; ‚îú‚îÄ‚îÄ public/ &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# Public assets&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
‚îÇ &nbsp; ‚îî‚îÄ‚îÄ src/ &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; # React application source&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
‚îÇ &nbsp; &nbsp; &nbsp; ‚îú‚îÄ‚îÄ components/ &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# React components&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
‚îÇ &nbsp; &nbsp; &nbsp; ‚îú‚îÄ‚îÄ graphql/ &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; # GraphQL queries and mutations&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
‚îÇ &nbsp; &nbsp; &nbsp; ‚îú‚îÄ‚îÄ pages/ &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; # Page components&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
‚îÇ &nbsp; &nbsp; &nbsp; ‚îî‚îÄ‚îÄ services/ &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# Service integrations&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
‚îî‚îÄ‚îÄ scripts/ &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; # Deployment and utility scripts&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
‚îú‚îÄ‚îÄ deploy.sh&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; # Main deployment script&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
‚îî‚îÄ‚îÄ zip-lambdas.sh&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# Script to zip all backend lambdas&nbsp;  
 
Infrastructure setup Terraform 
Our solution uses Terraform to define and provision the AWS infrastructure in a consistent and repeatable way. The main Terraform configuration orchestrates the various modules. The following code shows three of them: 
 
 # Compute Module - Lambda functions
module "compute" {
&nbsp;&nbsp;source = "./modules/compute"
&nbsp;&nbsp;
&nbsp;&nbsp;aws_region &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;= var.aws_region
&nbsp;&nbsp;aws_account &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; = data.aws_caller_identity.current.account_id
&nbsp;&nbsp;meeting_statistics_table_name &nbsp; &nbsp; = var.meeting_statistics_table_name
&nbsp;&nbsp;meeting_summaries_table_name &nbsp; &nbsp; &nbsp;= var.meeting_summaries_table_name
&nbsp;&nbsp;cognito_user_pool_id &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;= module.auth.cognito_user_pool_id
&nbsp;&nbsp;iam_roles &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; = module.auth.iam_roles
&nbsp;&nbsp;storage_bucket &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;= module.storage.storage_bucket
&nbsp;&nbsp;model_id &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;= var.model_id
&nbsp;&nbsp;inference_profile_prefix &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;= var.inference_profile_prefix
}

# Orchestration Module - Step Functions
module "orchestration" {
&nbsp;&nbsp;source = "./modules/orchestration"
&nbsp;&nbsp;
&nbsp;&nbsp;aws_region &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;= var.aws_region
&nbsp;&nbsp;aws_account &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; = data.aws_caller_identity.current.account_id
&nbsp;&nbsp;storage_bucket &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;= module.storage.storage_bucket
&nbsp;&nbsp;iam_roles &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; = module.auth.iam_roles
&nbsp;&nbsp;lambda_functions &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;= module.compute.lambda_functions
}

# Queue Processor Module - ProcessTranscriptionQueueFunction Lambda
module "queue_processor" {
&nbsp;&nbsp;source = "./modules/queue-processor"
&nbsp;&nbsp;
&nbsp;&nbsp;storage_bucket &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;= module.storage.storage_bucket
&nbsp;&nbsp;state_machine_arn &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; = module.orchestration.state_machine_arn
&nbsp;&nbsp;lambda_function_transcription_role = module.auth.iam_roles.lambda_function_transcription_role
&nbsp;&nbsp;
&nbsp;&nbsp;depends_on = [
&nbsp;&nbsp; &nbsp;module.storage,
&nbsp;&nbsp; &nbsp;module.orchestration
&nbsp;&nbsp;]
} 
 
Audio processing workflow 
The core of our solution is a Step Functions workflow that orchestrates the processing of audio files. The workflow handles language detection, transcription, summarization, and notification in a resilient way with proper error handling. 
Amazon Bedrock for summarization 
The summarization component is powered by Amazon Bedrock, which provides access to state-of-the-art FMs. Our solution uses Anthropic‚Äôs Claude 3.7 Sonnet version 1 to generate comprehensive meeting summaries: 
prompt =&nbsp;f"""Even if it is a raw transcript of a meeting discussion, lacking clear structure and context and containing multiple speakers, incomplete sentences, and tangential topics, PLEASE PROVIDE a clear and thorough analysis as detailed as possible of this conversation. DO NOT miss any information. CAPTURE as much information as possible. Use bullet points instead of dashes in your summary. IMPORTANT: For ALL section headers, use plain text with NO markdown formatting (no #, ##, **, or * symbols). Each section header should be in ALL CAPS followed by a colon. For example: "TITLE:" not "# TITLE" or "## TITLE". 
CRITICAL INSTRUCTION: DO NOT use any markdown formatting symbols like #, ##, **, or * in your response, especially for the TITLE section. The TITLE section MUST start with "TITLE:" and not "# TITLE:" or any variation with markdown symbols. 
FORMAT YOUR RESPONSE EXACTLY AS FOLLOWS: 
TITLE: Give the meeting a short title 2 or 3 words that is related to the overall context of the meeting, find a unique name such a company name or stakeholder and include it in the title &nbsp; &nbsp; &nbsp; 
TYPE: Depending on the context of the meeting, the conversation, the topic, and discussion, ALWAYS assign a type of meeting to this summary. Allowed Meeting types are: Client meeting, Team meeting, Technical meeting, Training Session, Status Update, Brainstorming Session, Review Meeting, External Stakeholder Meeting, Decision Making Meeting, and Problem Solving Meeting. This is crucial, don't overlook this. 
STAKEHOLDERS: Provide a list of the participants in the meeting, their company, and their corresponding roles. If the name is not provided or not understood, please replace the name with the word 'Not stated'. If a speaker does not introduce themselves, then don't include them in the STAKEHOLDERS section. &nbsp; 
CONTEXT: provide a 10-15 summary or context sentences with the following information: Main reason for contact, Resolution provided, Final outcome, considering all the information above 
MEETING OBJECTIVES: provide all the objectives or goals of the meeting. Be thorough and detailed. 
CONVERSATION DETAILS: Customer's main concerns/requests Solutions discussed Important information verified Decisions made 
KEY POINTS DISCUSSED (Elaborate on each point, if applicable): List all significant topics and issues Important details or numbers mentioned Any policies or procedures explained Special requests or exceptions 
ACTION ITEMS &amp; NEXT STEPS (Elaborate on each point, if applicable): What the customer needs to do: Immediate actions required Future steps to take Important dates or deadlines What the company will do (Elaborate on each point, if applicable): Processing or handling steps Follow-up actions promised Timeline for completion 
ADDITIONAL NOTES (Elaborate on each point, if applicable): Any notable issues or concerns Follow-up recommendations Important reminders 
TECHNICAL REQUIREMENTS &amp; RESOURCES (Elaborate on each point, if applicable): Systems or tools discussed/needed Technical specifications mentioned Required access or permissions Resource allocation details 
Frontend implementation 
The frontend is built with React and provides the following features: 
 
 User authentication and authorization using Amazon Cognito 
 Audio file upload interface with progress indicators 
 Summary viewing with formatted sections (stakeholders, key points, action items) 
 Search functionality across meeting summaries 
 Meeting statistics visualization 
 
The frontend communicates with the backend through the AWS AppSync GraphQL API, which provides a unified interface for data operations. 
Security considerations 
Security is a top priority in our solution, which we address with the following measures: 
 
 User authentication is handled by Amazon Cognito 
 API access is secured with Amazon Cognito user pools 
 S3 bucket access is restricted to authenticated users 
 IAM roles follow the principle of least privilege 
 Data is encrypted at rest and in transit 
 Step Functions provide secure orchestration with proper error handling 
 
Benefits of using Amazon Bedrock 
Amazon Bedrock offers several key advantages for our meeting summarization system: 
 
 Access to state-of-the-art models ‚Äì Amazon Bedrock provides access to leading FMs like Anthropic‚Äôs Claude 3.7 Sonnet version 1, which delivers high-quality summarization capabilities without the need to train custom models. 
 Fully managed integration ‚Äì Amazon Bedrock integrates seamlessly with other AWS services, allowing for a fully serverless architecture that scales automatically with demand. 
 Cost-efficiency ‚Äì On-Demand pricing means you only pay for the actual processing time, making it cost-effective for variable workloads. 
 Security and compliance ‚Äì Amazon Bedrock maintains data privacy and security, making sure sensitive meeting content remains protected within your AWS environment. 
 Customizable prompts ‚Äì The ability to craft detailed prompts allows for tailored summaries that extract exactly the information your organization needs from meetings. Amazon Bedrock also provides prompt management and optimization, as well as the playground for quick prototyping. 
 Multilingual support ‚Äì Amazon Bedrock can process content in multiple languages, making it suitable for global organizations. 
 Reduced development time ‚Äì Pre-trained models minimize the need for extensive AI development expertise and infrastructure. 
 Continuous improvement ‚Äì Amazon Bedrock provides a model choice, and the user can update the existing models with a single string change. 
 
Prerequisites 
Before implementing this solution, make sure you have: 
 
 An AWS account with permissions to create and manage the required services, such as Amazon S3, DynamoDB, Lambda, Amazon Transcribe, Amazon Bedrock, Step Functions, AWS AppSync, Amazon CloudWatch, and IAM 
  Terraform v1.5.0 or later installed 
 The AWS Command Line Interface (AWS CLI) configured with appropriate credentials 
 Access to Amazon Bedrock FMs (Anthropic‚Äôs Claude 3.7 Sonnet version 1 recommended 
 Basic familiarity with Terraform and AWS services 
 
In the following sections, we walk through the steps to deploy the meeting audio summarizer solution. 
Clone the repository 
First, clone the repository containing the Terraform code: 
git clone https://github.com/aws-samples/sample-meeting-audio-summarizer-in-terraform
cd sample-meeting-audio-summarizer-in-terraform 
Configure AWS credentials 
Make sure your AWS credentials are properly configured. You can use the AWS CLI to set up your credentials: 
aws configure --profile meeting-summarizer 
You will be prompted to enter your AWS access key ID, secret access key, default AWS Region, and output format. 
Install frontend dependencies 
To set up the frontend development environment, navigate to the frontend directory and install the required dependencies: 
cd frontend
npm install 
Create configuration files 
Move to the terraform directory: 
cd ../backend/terraform/ &nbsp; 
Update the terraform.tfvars file in the backend/terraform directory with your specific values. This configuration supplies values for the variables previously defined in the variables.tf file. 
You can customize other variables defined in variables.tf according to your needs. In the terraform.tfvars file, you provide actual values for the variables declared in variables.tf, so you can customize the deployment without modifying the core configuration files: 
 
 aws_region&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;= "us-east-1"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
aws_profile&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; = "YOUR-AWS-PROFILE"&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
environment&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; = "prod"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
app_name&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;= "meeting-audio-summarizer"&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
dynamodb_read_capacity&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;= 5&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
dynamodb_write_capacity&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; = 5&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
cognito_allowed_email_domains&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; = ["example.com"]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
model_id&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;= "anthropic.claude-3-7-sonnet-20250219-v1:0"&nbsp;&nbsp;
inference_profile_prefix&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;= "us"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
frontend_bucket_name&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;= "a-unique-bucket-name"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
storage_bucket&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;= "a-unique-bucket-name"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
cognito_domain_prefix&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; = "meeting-summarizer"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
meeting_statistics_table_name&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; = "MeetingStatistics"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
meeting_summaries_table_name&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;= "MeetingSummaries"&nbsp;  
 
For a-unique-bucket-name, choose a unique name that is meaningful and makes sense to you. 
Initialize and apply Terraform 
Navigate to the terraform directory and initialize the Terraform environment: 
terraform init 
To upgrade the previously selected plugins to the newest version that complies with the configuration‚Äôs version constraints, use the following command: 
terraform init&nbsp;-upgrade 
This will cause Terraform to ignore selections recorded in the dependency lock file and take the newest available version matching the configured version constraints. 
Review the planned changes: 
terraform plan 
Apply the Terraform configuration to create the resources: 
terraform apply 
When prompted, enter yes to confirm the deployment. You can run terraform apply -auto-approve to skip the approval question. 
Deploy the solution 
After the backend deployment is complete, deploy the entire solution using the provided deployment script: 
cd ../../scripts
sudo chmod&nbsp;+x deploy.sh
./deploy.sh 
This script handles the entire deployment process, including: 
 
 Deploying the backend infrastructure using Terraform 
 Automatically configuring the frontend with backend resource information 
 Building and deploying the frontend application 
 Setting up CloudFront distribution 
 Invalidating the CloudFront cache to make sure the latest content is served 
 
Verify the deployment 
After the entire solution (both backend and frontend) is deployed, in your terminal you should see something similar to the following text: 
 
 Deployment complete! :)

============================================================================
Your app is available at: https://d1e5vh2t5qryy2.cloudfront.net.
============================================================================ 
 
The CloudFront URL (*.cloudfront.net/) is unique, so yours will not be the same. 
Enter the URL into your browser to open the application. You will see a login page like the following screenshot. You must create an account to access the application. 
 
Start by uploading a file: 
 
View generated summaries in a structured format: 
 
See meeting statistics: 
 
Clean up 
To cleanup the solution you must run this command. 
terraform destroy 
This command will completely remove the AWS resources provisioned by Terraform in your environment. When executed, it will display a detailed plan showing the resources that will be destroyed, and prompt for confirmation before proceeding. The process may take several minutes as it systematically removes infrastructure components in the correct dependency order. 
Remember to verify the destruction is complete by checking your AWS Console to make sure no billable resources remain active. 
Cost considerations 
When implementing this solution, it‚Äôs important to understand the cost implications of each component. Let‚Äôs analyze the costs based on a realistic usage scenario, based on the following assumptions: 
 
 50 hours of audio processing per month 
 Average meeting length of 30 minutes 
 100 active users accessing the system 
 5 million API queries per month 
 
The majority of the cost comes from Amazon Transcribe (approximately 73% of total cost at $72.00), with AWS AppSync being the second largest cost component (approximately 20% at $20.00). Despite providing the core AI functionality, Amazon Bedrock costs approximately 3% of total at $3.00, and DynamoDB, CloudFront, Lambda, Step Functions, Amazon SQS, and Amazon S3 make up the remaining 4%. 
We can take advantage of the following cost optimization opportunities: 
 
 Implement audio compression to reduce storage and processing costs 
 Use Amazon Transcribe Medical for medical meetings (if applicable) for higher accuracy 
 Implement caching strategies for frequently accessed summaries to reduce AppSync and DynamoDB costs 
 Consider reserved capacity for DynamoDB if usage patterns are predictable 
 
The following table summarizes these prices. Refer the AWS pricing pages for each service to learn more about the AWS pricing model. 
 
  
   
   Service 
   Usage 
   Unit Cost 
   Monthly Cost 
   
   
   Amazon Bedrock 
   500K input tokens100K output tokens 
   $3.00 per million tokens$15.00 per million tokens 
   $3 
   
   
   Amazon CloudFront 
   5GB data transfer 
   $0.085 per GB 
   $0.43 
   
   
   Amazon Cognito 
   100 Monthly Active Users (MAU) 
   Free tier (first 50K users) 
   $0 
   
   
   Amazon DynamoDB 
   5 RCU/WCU, ~ 1GB storage 
   $0.25 per RCU/WCU + $0.25/GB 
   $2.75 
   
   
   Amazon SQS 
   1,000 messages 
   $0.40 per million 
   $0.01 
   
   
   Amazon S3 Storage 
   3GB audio + 12MB transcripts/summaries 
   $0.023 per GB 
   $0.07 
   
   
   AWS Step Functions 
   1,000 state transitions 
   $0.025 per 1,000 
   $0.03 
   
   
   AWS AppSync 
   5M queries 
   $4.00 per million 
   $20 
   
   
   AWS Lambda 
   300 invocations, 5s avg. runtime, 256MB 
   Various 
   $0.10 
   
   
   Amazon Transcribe 
   50 hours of audio 
   $1.44 per hour 
   $72 
   
   
    
    
   TOTAL 
   98.39 
   
  
 
Next steps  
The next phase of our meeting summarization solution will incorporate several advanced AI technologies to deliver greater business value. Amazon Sonic Model can improve transcription accuracy by better handling multiple speakers, accents, and technical terminology‚Äîaddressing a key pain point for global organizations with diverse teams. Meanwhile, Amazon Bedrock Flows can enhance the system‚Äôs analytical capabilities by implementing automated meeting categorization, role-based summary customization, and integration with corporate knowledge bases to provide relevant context. These improvements can help organizations extract actionable insights that would otherwise remain buried in conversation. 
The addition of real-time processing capabilities helps teams see key points, action items, and decisions as they emerge during meetings, enabling immediate clarification and reducing follow-up questions. Enhanced analytics functionality track patterns across multiple meetings over time, giving management visibility into communication effectiveness, decision-making processes, and project progress. By integrating with existing productivity tools like calendars, daily agenda, task management systems, and communication services, this solution makes sure that meeting intelligence flows directly into daily workflows, minimizing manual transfer of information and making sure critical insights drive tangible business outcomes across departments. 
Conclusion 
Our meeting audio summarizer combines AWS serverless technologies with generative AI to solve a critical productivity challenge. It automatically transcribes and summarizes meetings, saving organizations thousands of hours while making sure insights and action items are systematically captured and shared with stakeholders. 
The serverless architecture scales effortlessly with fluctuating meeting volumes, costs just $0.98 per meeting on average, and minimizes infrastructure management and maintenance overhead. Amazon Bedrock provides enterprise-grade AI capabilities without requiring specialized machine learning expertise or significant development resources, and the Terraform-based infrastructure as code enables rapid deployment across environments, customization to meet specific organizational requirements, and seamless integration with existing CI/CD pipelines. 
As the field of generative AI continues to evolve and new, better-performing models become available, the solution‚Äôs ability to perform its tasks will automatically improve on performance and accuracy without additional development effort, enhancing summarization quality, language understanding, and contextual awareness. This makes the meeting audio summarizer an increasingly valuable asset for modern businesses looking to optimize meeting workflows, enhance knowledge sharing, and boost organizational productivity. 
Additional resources 
Refer to Amazon Bedrock Documentation for more details on model selection, prompt engineering, and API integration for your generative AI applications. Additionally, see Amazon Transcribe Documentation for information about the speech-to-text service‚Äôs features, language support, and customization options for achieving accurate audio transcription. For infrastructure deployment needs, see Terraform AWS Provider Documentation for detailed explanations of resource types, attributes, and configuration options for provisioning AWS resources programmatically. To enhance your infrastructure management skills, see Best practices for using the Terraform AWS Provider, where you can find recommended approaches for module organization, state management, security configurations, and resource naming conventions that will help make sure your AWS infrastructure deployments remain scalable and maintainable. 
 
About the authors 
Dunieski Otano is a Solutions Architect at Amazon Web Services based out of Miami, Florida. He works with World Wide Public Sector MNO (Multi-International Organizations) customers. His passion is Security, Machine Learning and Artificial Intelligence, and Serverless. He works with his customers to help them build and deploy high available, scalable, and secure solutions. Dunieski holds 14 AWS certifications and is an AWS Golden Jacket recipient. In his free time, you will find him spending time with his family and dog, watching a great movie, coding, or flying his drone. 
Joel Asante, an Austin-based Solutions Architect at Amazon Web Services (AWS), works with GovTech (Government Technology) customers. With a strong background in data science and application development, he brings deep technical expertise to creating secure and scalable cloud architectures for his customers. Joel is passionate about data analytics, machine learning, and robotics, leveraging his development experience to design innovative solutions that meet complex government requirements. He holds 13 AWS certifications and enjoys family time, fitness, and cheering for the Kansas City Chiefs and Los Angeles Lakers in his spare time. 
Ezzel Mohammed is a Solutions Architect at Amazon Web Services (AWS) based in Dallas, Texas. He works on the International Organizations team within the World Wide Public Sector, collaborating closely with UN agencies to deliver innovative cloud solutions. With a Computer Science background, Ezzeldien brings deep technical expertise in system design, helping customers architect and deploy highly available and scalable solutions that meet international compliance requirements. He holds 9 AWS certifications and is passionate about applying AI Engineering and Machine Learning to address global challenges. In his free time, he enjoys going on walks, watching soccer with friends and family, playing volleyball, and reading tech articles.
‚Ä¢ Kyruus builds a generative AI provider matching solution on AWS
  This post was written with Zach Heath of Kyruus Health. 
When health plan members need care, they shouldn‚Äôt need a dictionary. Yet millions face this exact challenge‚Äîdescribing symptoms in everyday language while healthcare references clinical terminology and complex specialty classifications. This disconnect forces members to become amateur medical translators, attempting to convert phrases like ‚Äúmy knee hurts when I climb stairs‚Äù into specialized search criteria such as orthopedics or physical medicine. Traditional provider directories compound this problem with overwhelming filter options and medical jargon, leading to frustrated members, delayed care access, and ultimately higher costs for both individuals and health plans. 
Kyruus Health, a leading provider of care access solutions, serves over 1,400 hospitals, 550 medical groups, and 100 health plan brands‚Äîconnecting more than 500,000 providers with patients seeking care and facilitating over 1 million appointments annually. To address the challenges of healthcare navigation, they developed Guide, an AI-powered solution that understands natural language and connects members with the right providers. With Guide, members can express health concerns in their own words and receive personalized provider matches without requiring clinical knowledge. Health plans implementing this solution have reported enhanced member experience and higher Net Promoter Scores (NPS), along with improved care access conversion and appointment scheduling rates. 
In this post, we demonstrate how Kyruus Health uses AWS services to build Guide. We show how Amazon Bedrock, a fully managed service that provides access to foundation models (FMs) from leading AI companies and Amazon through a single API, and Amazon OpenSearch Service, a managed search and analytics service, work together to understand everyday language about health concerns and connect members with the right providers. We explore the solution architecture, share implementation insights, and examine how this approach delivers measurable business value for health plans and their members. 
Solution overview 
Guide transforms healthcare provider search by translating natural language health concerns into precisely matched provider recommendations. The solution uses Amazon Bedrock with Anthropic‚Äôs Claude 3.5 Sonnet to understand everyday descriptions of health concerns and convert them into structured medical parameters. Then it uses OpenSearch Service to match these parameters against comprehensive provider data and deliver targeted recommendations. 
This architecture makes it possible for members to express health needs in plain language while making sure provider matches meet clinical requirements. The entire solution maintains HIPAA compliance through end-to-end encryption and fine-grained access controls, so Kyruus Health to focus on improving the member experience instead of managing complex infrastructure. 
The following diagram illustrates the solution architecture. 
 
This architecture translates natural language queries into structured healthcare parameters through the following steps: 
 
 A member enters a query like ‚ÄúI‚Äôve been having shooting pain down my leg for two weeks‚Äù through the health plan application. Amazon API Gateway securely receives the member‚Äôs query request. 
 API Gateway routes the request to Guide‚Äôs conversation service running on Amazon Elastic Container Service (Amazon ECS). 
 Guide‚Äôs conversation service calls Amazon Bedrock, where Anthropic‚Äôs Claude 3.5 Sonnet processes the natural language. The model identifies potential sciatica and translates this everyday description into structured medical parameters, including appropriate specialties like neurology or orthopedics. 
 The health plan application initiates a new API call through API Gateway to the Provider Search Service running on Amazon ECS, using the structured parameters derived from the previous steps. 
 The Provider Search Service queries OpenSearch Service, which contains comprehensive provider data previously ingested from Amazon Simple Storage Service (Amazon S3), including specialties, clinical focus areas, locations, and insurance network participation. 
 
Matched providers are then returned to the health plan application and presented to the member through an intuitive conversational interface. This architecture demonstrates the powerful combination of Amazon Bedrock FMs with purpose-built AWS services like OpenSearch Service, creating an end-to-end solution that bridges the gap between complex healthcare data and intuitive member experiences. 
Building with Tribe AI 
To accelerate their AI transformation, Kyruus Health partnered with Tribe AI, an AWS Partner with extensive experience in building and implementing enterprise-grade generative AI solutions at scale. Tribe AI‚Äôs proven track record in deploying FMs in complex, regulatory environments like healthcare helped de-risk the adoption of generative AI for Kyruus. This partnership allowed Kyruus to focus on their healthcare domain expertise while using Tribe AI‚Äôs technical implementation knowledge to bring Guide from concept to production. 
Implementation insights 
Kyruus Health‚Äôs successful implementation of Guide yielded key insights that can help organizations building healthcare AI initiatives: 
 
 Healthcare-specific testing infrastructure is essential ‚Äì Kyruus Health prioritized testing with real healthcare scenarios from the start. This process made sure Guide could accurately translate everyday descriptions into appropriate provider specialties, maintaining reliability where matching decisions directly impact health outcomes and plan costs. 
 User-centered design principles must guide AI implementation ‚Äì By focusing first on member needs rather than technical capabilities, Kyruus Health made sure their solution addressed the actual friction points in healthcare navigation. This approach led directly to significant improvements in satisfaction and reduced search abandonment rates, demonstrating how AI implementations should start with human needs rather than technical possibilities. 
 Strategic model selection drives business outcomes ‚Äì Rather than using a single model for all tasks, Kyruus Health discovered the power of strategically deploying specialized models for different aspects of healthcare navigation‚Äîincluding complex symptom interpretation and clinical specialty mapping. This targeted approach improved provider match accuracy by aligning specific AI capabilities to distinct parts of the matching process, optimizing both performance and cost while delivering more precise provider recommendations. 
 
These insights demonstrate how a thoughtful implementation approach can transform complex healthcare navigation challenges into intuitive member experiences that deliver measurable business results. 
Guide member experience in action 
The following screenshot shows how the AWS architecture translates into the real-world member experience. When a member enters their symptom description and location preference, Guide processes this natural language input through Amazon Bedrock and identifies appropriate specialists using OpenSearch Service. The system interprets the medical concern and location requirements, responding with relevant specialists within the requested distance who are accepting new patients. This streamlined experience has delivered higher match rates and increased appointment completion for health plans. 
 
Conclusion 
Guide demonstrates how generative AI powered by AWS transforms healthcare navigation by bridging the gap between everyday language and clinical terminology. In this post, we explored how an architecture combining Amazon Bedrock and OpenSearch Service processes natural language queries into personalized provider matches, helping members find appropriate healthcare providers using natural language descriptions of their symptoms. 
For health plans evaluating digital initiatives, Guide offers a blueprint for solving complex healthcare challenges while delivering measurable improvements in member satisfaction and appointment conversion rates. To build your own generative AI solutions, explore Amazon Bedrock for managed access to FMs. For healthcare-specific guidance, check out the AWS Healthcare Industry Lens and browse implementation examples, use cases, and technical guidance in the AWS Healthcare and Life Sciences Blog. 
 
About the authors 
Zach Heath is a Senior Staff Software Engineer at Kyruus Health. A passionate technologist, he specializes in architecting and implementing robust, scalable software solutions that transform healthcare search experiences by connecting patients with the right care through innovative technology. 
Anil Chinnam is a Solutions Architect at AWS. He is a generative AI enthusiast passionate about translating cutting-edge technologies into tangible business value for healthcare customers. As a trusted technical advisor, he helps customers drive cloud adoption and business transformation outcomes.

‚∏ª