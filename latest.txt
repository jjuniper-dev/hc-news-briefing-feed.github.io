‚úÖ Morning News Briefing ‚Äì September 09, 2025 10:44

üìÖ Date: 2025-09-09 10:44
üè∑Ô∏è Tags: #briefing #ai #publichealth #digitalgov

‚∏ª

üßæ Weather
‚Ä¢ No watches or warnings in effect, Pembroke
  No watches or warnings in effect. No warnings or watches or watches in effect . Watch or warnings are no longer in effect in the U.S. No watches, warnings are in effect for the rest of the day . No watches and warnings are still in effect, but no watches are in place for the day's events . The weather is not expected to be affected by the weather .
‚Ä¢ Current Conditions:  4.9¬∞C
  Temperature: 4.9&deg;C Pressure / Tendency: 102.7 kPa rising Humidity: 99 % Humidity : 99 % Dewpoint: 4 .8&deg:C Wind: S 3 km/h . Air Quality Health Index: n/a . Pembroke 6:00 AM EDT Tuesday 9 September 2025 . Weather forecast: 10 September 2025
‚Ä¢ Tuesday: Sunny. High 22.
  Fog patches dissipating this morning . Sunny. Sunny. High 22. UV index 6 or high . High 22; UV index will be 6 or very high in the morning . Forecast issued 5:00 AM EDT Tuesday 9 September 2025 . For more information on the weather, visit CNN.com/suspective-watcher.com or click here for more details . For

üåç International News
No updates.

üçÅ Canadian News
No updates.

üá∫üá∏ U.S. Top Stories
‚Ä¢ The 2025 Student Podcast Challenge Honorable Mentions
  The honorable mentions were chosen from nearly 2,000 entries in this year's Student Podcast Challenge . Here are the honorable mentions from the competition, chosen from more than 2,200 entries . The honorable mention is the first honorable mention from the Student podcast Challenge . Read the rest of the competition's posts on iReport.com/studentpod.com and iReport@mailonline.com
‚Ä¢ Trump's Medicaid cuts will hurt children's hospitals
  GOP said its overhaul of Medicaid was aimed at reducing fraud and getting more adult beneficiaries to work . Among the likely side effects: fewer services and doctors for treating sick children . The GOP said it was trying to reduce fraud and get more adults to work in Medicaid . The likely side effect is fewer services for sick children, including fewer doctors and fewer doctors for those treating them sickly children .
‚Ä¢ A new Nation's Report Card shows drops in science, math and reading scores
  Nation's Report Card is the first since the Trump administration began making cuts to the U.S. Education Department . The scores reflect the state of student achievement in early 2024 . It's the first nation's report card since the cuts were made to the Education Department in the early years of the Obama administration . The report card reflects the state-of-the-art achievement of students in
‚Ä¢ Here they are: The best student podcasts in America
  For the 2025 NPR Student Podcast Challenge, we've listened to nearly 2,000 entries from around the U.S. and narrowed them down to 11 middle school and 10 high school finalists . We've narrowed the list of finalists to 11 high school and 11 middle and 10 middle school students . For more videos, visit http://www.dailymailonline.com/the2525 .
‚Ä¢ Nepal's prime minister resigns following violent protests against social media ban
  Nepal's prime minister resigned Tuesday following violent protests against a ban on social media platforms and government corruption . Protests were held in Nepal's capital, Kathmandu, on Tuesday . Protesters were angered by a government ban on the use of social media and corruption . Nepal's PM resigned Tuesday after violent protests over the ban and corruption of the government . Nepal is the most populous country in the

üß† Artificial Intelligence
No updates.

üíª Digital Strategy
‚Ä¢ Nokia successor HMD spawns secure device biz with Euro-made smartphone
  Ivalo XE handset targets governments and security critical sectors, though Qualcomm silicon keeps it tied to the US . Finnish phone maker HMD Global is launching a business unit called HMD Secure to target governments and other security-critical customers, and has its first device ready to go . It has its own device, HMD XE, ready to launch in the US, but Qualcomm
‚Ä¢ Anthropic's Claude Code runs code to test if it is safe ‚Äì which might be a big mistake
  Checkmarx says automated reviews in Anthropic's Claude Code can catch some bugs but miss others . It can sometimes create new risks by executing code while testing it, researchers say . AI security reviews add new risks, say researchers . The reviews can catch bugs but also miss others, they say, and sometimes create risks by testing it while it's in place of testing it . Checkmar
‚Ä¢ AI Darwin Awards launch to celebrate spectacularly bad deployments
  Darwin Awards are being extended to include examples of misadventures involving overzealous applications of AI . From fast food fiascos to botched databases, there are fresh honors for machine learning failures . The Darwin Awards will also be extended to feature examples of over-zealous AI applications of the past and present, including those from the U.S. and Canada, as well as the
‚Ä¢ Legacy tech blunts UK top cops' fight against serious crime, inspectors find
  Report warns creaking infrastructure undermines National Crime Agency's efficiency and effectiveness . NCA clings to legacy systems and relies on IT strategy that lacks clarity, it has been found . Report: 'IT strategy is lacking clarity' and 'clarity' is needed to improve efficiency of the agency's IT systems, it is claimed . The NCA's IT strategy has been criticised by policing watchdog
‚Ä¢ Microsoft veteran's worst Windows bug was Pinball running at 5,000 FPS
  Dev admits the game once ate an entire CPU core . Former Microsoft engineer Dave Plummer has come clean and admitted that the worst bug he ever shipped was in... Pinball.‚Ä¶‚Ä¶‚Ä¶ Pinball. The worst bug in Pinball was in Pinball, which once ate a CPU core core . The bug was a bug in the game that ate a core core of the game's CPU core, Pl

üè• Public Health
No updates.

üî¨ Science
‚Ä¢ Enhancing Mindfulness-Based Cognitive Therapy in a Virtual Reality: A Prospective Interventional Study
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Changes in cholera burden across Africa
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Nature goes inside the world‚Äôs largest ‚Äòmosquito factory‚Äô ‚Äî here‚Äôs the buzz
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Feasibility and clinical utility of expanded genomic newborn screening in the Early Check program
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Longitudinal associations between 24-hour movement behaviors and physical fitness in preschoolers: a compositional isotemporal substitution analysis
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations

üßæ Government & Policy
No updates.

üèõÔ∏è Enterprise Architecture & IT Governance
No updates.

ü§ñ AI & Emerging Tech
‚Ä¢ Help! My therapist is secretly using ChatGPT
  In Silicon Valley‚Äôs imagined future, AI models are so empathetic that we‚Äôll use them as therapists. They‚Äôll provide mental-health care for millions, unimpeded by the pesky requirements for human counselors, like the need for graduate degrees, malpractice insurance, and sleep. Down here on Earth, something very different has been happening.&nbsp;



Last week, we published a story about people finding out that their therapists were secretly using ChatGPT during sessions. In some cases it wasn‚Äôt subtle; one therapist accidentally shared his screen during a virtual appointment, allowing the patient to see his own private thoughts being typed into ChatGPT in real time. The model then suggested responses that his therapist parroted.&nbsp;



It‚Äôs my favorite AI story as of late, probably because it captures so well the chaos that can unfold when people actually use AI the way tech companies have all but told them to.



As the writer of the story, Laurie Clarke, points out, it‚Äôs not a total pipe dream that AI could be therapeutically useful. Early this year, I wrote about the first clinical trial of an AI bot built specifically for therapy. The results were promising! But the secretive use by therapists of AI models that are not vetted for mental health is something very different. I had a conversation with Clarke to hear more about what she found.&nbsp;



I have to say, I was really fascinated that people called out their therapists after finding out they were covertly using AI. How did you interpret the reactions of these therapists? Were they trying to hide it?



In all the cases mentioned in the piece, the therapist hadn‚Äôt provided prior disclosure of how they were using AI to their patients. So whether or not they were explicitly trying to conceal it, that‚Äôs how it ended up looking when it was discovered. I think for this reason, one of my main takeaways from writing the piece was that therapists should absolutely disclose when they‚Äôre going to use AI and how (if they plan to use it). If they don‚Äôt, it raises all these really uncomfortable questions for patients when it‚Äôs uncovered and risks irrevocably damaging the trust that‚Äôs been built.





In the examples you‚Äôve come across, are therapists turning to AI simply as a time-saver? Or do they think AI models can genuinely give them a new perspective on what‚Äôs bothering someone?



Some see AI as a potential time-saver. I heard from a few therapists that notes are the bane of their lives. So I think there is some interest in AI-powered tools that can support this. Most I spoke to were very skeptical about using AI for advice on how to treat a patient. They said it would be better to consult supervisors or colleagues, or case studies in the literature. They were also understandably very wary of inputting sensitive data into these tools.



There is some evidence AI can deliver more standardized, &#8220;manualized&#8221; therapies like CBT [cognitive behavioral therapy] reasonably effectively. So it‚Äôs possible it could be more useful for that. But that is AI specifically designed for that purpose, not general-purpose tools like ChatGPT.



What happens if this goes awry? What attention is this getting from ethics groups and lawmakers?



At present, professional bodies like the American Counseling Association advise against using AI tools to diagnose patients. There could also be more stringent regulations preventing this in future. Nevada and Illinois, for example, have recently passed laws prohibiting the use of AI in therapeutic decision-making. More states could follow.



OpenAI‚Äôs Sam Altman said last month that ‚Äúa lot of people effectively use ChatGPT as a sort of therapist,‚Äù and that to him, that‚Äôs a good thing. Do you think tech companies are overpromising on AI‚Äôs ability to help us?



I think that tech companies are subtly encouraging this use of AI because clearly it‚Äôs a route through which some people are forming an attachment to their products. I think the main issue is that what people are getting from these tools isn‚Äôt really ‚Äútherapy‚Äù by any stretch. Good therapy goes far beyond being soothing and validating everything someone says. I‚Äôve never in my life looked forward to a (real, in-person) therapy session. They‚Äôre often highly uncomfortable, and even distressing. But that‚Äôs part of the point. The therapist should be challenging you and drawing you out and seeking to understand you. ChatGPT doesn‚Äôt do any of these things.&nbsp;



Read the full story from Laurie Clarke.&nbsp;



This story originally appeared in The Algorithm, our weekly newsletter on AI. To get stories like this in your inbox first,¬†sign up here.
‚Ä¢ AI is changing the grid. Could it help more than it harms?
  The rising popularity of AI is driving an increase in electricity demand so significant it has the potential to reshape our grid. Energy consumption by data centers has gone up by 80% from 2020 to 2025 and is likely to keep growing. Electricity prices are already rising, especially in places where data centers are most concentrated.¬†



Yet many people, especially in Big Tech, argue that AI will be, on balance, a positive force for the grid. They claim that the technology could help get more clean power online faster, run our power system more efficiently, and predict and prevent failures that cause blackouts.&nbsp;



There are early examples where AI is helping already, including AI tools that utilities are using to help forecast supply and demand. The question is whether these big promises will be realized fast enough to outweigh the negative effects of AI on local grids and communities.&nbsp;



A delicate balance



One area where AI is already being used for the grid is in forecasting, says Utkarsha Agwan, a member of the nonprofit group Climate Change AI.



Running the grid is a balancing act: Operators have to understand how much electricity demand there is and turn on the right combination of power plants to meet it. They optimize for economics along the way, choosing the sources that will keep prices lowest for the whole system.



That makes it necessary to look ahead hours and in some cases days. Operators consider factors such as historical data (holidays often see higher demand) and the weather (a hot day means more air conditioners sucking up power). These predictions also consider what level of supply is expected from intermittent sources like solar panels.



There‚Äôs little risk in using AI tools in forecasting; it‚Äôs often not as time sensitive as other applications, which can require reactions within seconds or even milliseconds. A grid operator might use a forecast to determine which plants will need to turn on. Other groups might run their own forecasts as well, using AI tools to decide how to staff a plant, for example. The tools also can‚Äôt physically control anything. Rather, they can be used alongside more conventional methods to provide more data.&nbsp;&nbsp;





Today, grid operators make a lot of approximations to model the grid, because the system is so incredibly complex that it‚Äôs impossible to truly know what‚Äôs going on in every place at every time. Not only are there a whole host of power plants and consumers to think about, but there are considerations like making sure power lines don‚Äôt get overloaded.



Working with those estimates can lead to some inefficiencies, says Kyri Baker, a professor at the University of Colorado Boulder. Operators tend to generate a bit more electricity than the system uses, for example. Using AI to create a better model could reduce some of those losses and allow operators to make decisions about how to control infrastructure in real time to reach a closer match of supply and demand.



She gives the example of a trip to the airport. Imagine there‚Äôs a route you know will get you there in about 45 minutes. There might be another, more complicated route that could save you some time in ideal conditions‚Äîbut you‚Äôre not sure whether it‚Äôs better on any particular day. What the grid does now is the equivalent of taking the reliable route.



‚ÄúSo that‚Äôs the gap that AI can help close. We can solve this more complex problem, fast enough and reliably enough that we can possibly use it and shave off emissions,‚Äù Baker says.&nbsp;



In theory, AI could be used to operate the grid entirely without human intervention. But that work is largely still in the research phase. Grid operators are running some of the most critical infrastructure in this country, and the industry is hesitant to mess with something that‚Äôs already working, Baker says. If this sort of technology is ever used in grid operations, there will still be humans in the loop to help make decisions, at least when it‚Äôs first deployed.&nbsp;&nbsp;



Planning ahead



Another fertile area for AI is planning future updates to the grid. Building a power plant can take a very long time‚Äîthe typical time from an initial request to commercial operation in the US is roughly four years. One reason for the lengthy wait is that new power plants have to demonstrate how they might affect the rest of the grid before they can connect.¬†



An interconnection study examines whether adding a new power plant of a particular type in a particular place would require upgrades to the grid to prevent problems. After regulators and utilities determine what upgrades might be needed, they estimate the cost, and the energy developer generally foots the bill.&nbsp;



Today, those studies can take months. They involve trying to understand an incredibly complicated system, and because they rely on estimates of other existing and proposed power plants, only a few can happen in an area at any given time. This has helped create the years-long interconnection queue, a long line of plants waiting for their turn to hook up to the grid in markets like the US and Europe. The vast majority of projects in the queue today are renewables, which means there‚Äôs clean power just waiting to come online.&nbsp;



AI could help speed this process, producing these reports more quickly. The Midcontinent Independent System Operator, a grid operator that covers 15 states in the central US, is currently working with a company called Pearl Street to help automate these reports.



AI won‚Äôt be a cure-all for grid planning; there are other steps to clearing the interconnection queue, including securing the necessary permits. But the technology could help move things along. ‚ÄúThe sooner we can speed up interconnection, the better off we‚Äôll be,‚Äù says Rob Gramlich, president of Grid Strategies, a consultancy specializing in transmission and power markets.



There‚Äôs a growing list of other potential uses for AI on the grid and in electricity generation. The technology could monitor and plan ahead for failures in equipment ranging from power lines to gear boxes. Computer vision could help detect everything from wildfires to faulty lines. AI could also help balance supply and demand in virtual power plants, systems of distributed resources like EV chargers or smart water heaters.&nbsp;



While there are early examples of research and pilot programs for AI from grid planning to operation, some experts are skeptical that the technology will deliver at the level some are hoping for. ‚ÄúIt‚Äôs not that AI has not had some kind of transformation on power systems,‚Äù Climate Change AI‚Äôs Agwan says. ‚ÄúIt‚Äôs that the promise has always been bigger, and the hope has always been bigger.‚Äù



Some places are already seeing higher electricity prices because of power needs from data centers. The situation is likely to get worse. Electricity demand from data centers is set to double by the end of the decade, reaching 945 terawatt-hours, roughly the annual demand from the entire country of Japan.¬†



The infrastructure growth needed to support AI load growth has outpaced the promises of the technology, ‚Äúby quite a bit,‚Äù says Panayiotis Moutis, an assistant professor of electrical engineering at the City University of New York. Higher bills caused by the increasing energy needs of AI aren‚Äôt justified by existing ways of using the technology for the grid, he says.&nbsp;



‚ÄúAt the moment, I am very hesitant to lean on the side of AI being a silver bullet,‚Äù Moutis says.&nbsp;
‚Ä¢ Three big things we still don‚Äôt know about AI‚Äôs energy burden
  Earlier this year, when my colleague Casey Crownhart and I spent six months researching the climate and energy burden of AI, we came to see one number in particular as our white whale: how much energy the leading AI models, like ChatGPT or Gemini, use up when generating a single response.&nbsp;



This fundamental number remained elusive even as the scramble to power AI escalated to the White House and the Pentagon, and as projections showed that in three years AI could use as much electricity as 22% of all US households.&nbsp;



The problem with finding that number, as we explain in our piece published in May, was that AI companies are the only ones who have it. We pestered Google, OpenAI, and Microsoft, but each company refused to provide its figure. Researchers we spoke to who study AI‚Äôs impact on energy grids compared it to trying to measure the fuel efficiency of a car without ever being able to drive it, making guesses based on rumors of its engine size and what it sounds like going down the highway.



But then this summer, after we published, a strange thing started to happen. In June, OpenAI‚Äôs Sam Altman wrote that an average ChatGPT query uses 0.34 watt-hours of energy. In July, the French AI startup Mistral didn‚Äôt publish a number directly but released an estimate of the emissions generated. In August, Google revealed that answering a question to Gemini uses about 0.24 watt-hours of energy. The figures from Google and OpenAI were similar to what Casey and I estimated for medium-size AI models.&nbsp;



So with this newfound transparency, is our job complete? Did we finally harpoon our white whale, and if so, what happens next for people studying the climate impact of AI? I reached out to some of our old sources, and some new ones, to find out.



The numbers are vague and chat-only



The first thing they told me is that there‚Äôs a lot missing from the figures tech companies published this summer.&nbsp;



OpenAI‚Äôs number, for example, did not appear in a detailed technical paper but rather in a blog post by Altman that leaves lots of unanswered questions, such as which model he was referring to, how the energy use was measured, and how much it varies. Google‚Äôs figure, as Crownhart points out, refers to the median amount of energy per query, which doesn‚Äôt give us a sense of the more energy-demanding Gemini responses, like when it uses a reasoning model to ‚Äúthink‚Äù through a hard problem or generates a really long response.&nbsp;



The numbers also refer only to interactions with chatbots, not the other ways that people are becoming increasingly reliant on generative AI.&nbsp;



‚ÄúAs video and image becomes more prominent and used by more and more people, we need the numbers from different modalities and how they measure up,‚Äù says Sasha Luccioni, AI and climate lead at the AI platform Hugging Face.&nbsp;



This is also important because the figures for asking a question to a chatbot are, as expected, undoubtedly small‚Äîthe same amount of electricity used by a microwave in just seconds. That‚Äôs part of the reason AI and climate researchers don‚Äôt suggest that any one individual‚Äôs AI use creates a significant climate burden.&nbsp;



A full accounting of AI‚Äôs energy demands‚Äîone that goes beyond what‚Äôs used to answer an individual query to help us understand its full net impact on the climate‚Äîwould require application-specific information on how all this AI is being used. Ketan Joshi, an analyst for climate and energy groups, acknowledges that researchers don‚Äôt usually get such specific information from other industries but says it might be justified in this case.



‚ÄúThe rate of data center growth is inarguably unusual,‚Äù Joshi says. ‚ÄúCompanies should be subject to significantly more scrutiny.‚Äù





We have questions about energy efficiency



Companies making billion-dollar investments into AI have struggled to square this growth in energy demand with their sustainability goals. In May, Microsoft said that its emissions have soared by over 23% since 2020, owing largely to AI, while the company has promised to be carbon negative by 2030. ‚ÄúIt has become clear that our journey towards being carbon negative is a marathon, not a sprint,‚Äù Microsoft wrote.



Tech companies often justify this emissions burden by arguing that soon enough, AI itself will unlock efficiencies that will make it a net positive for the climate. Perhaps the right AI system, the thinking goes, could design more efficient heating and cooling systems for a building, or help discover the minerals required for electric-vehicle batteries.&nbsp;



But there are no signs that AI has been usefully used to do these things yet. Companies have shared anecdotes about using AI to find methane emission hot spots, for example, but they haven‚Äôt been transparent enough to help us know if these successes outweigh the surges in electricity demand and emissions that Big Tech has produced in the AI boom. In the meantime, more data centers are planned, and AI‚Äôs energy demand continues to rise and rise.&nbsp;



The ‚Äòbubble‚Äô question



One of the big unknowns in the AI energy equation is whether society will ever adopt AI at the levels that figure into tech companies‚Äô plans. OpenAI has said that ChatGPT receives 2.5 billion prompts per day. It‚Äôs possible that this number, and the equivalent numbers for other AI companies, will continue to soar in the coming years. Projections released last year by the Lawrence Berkeley National Laboratory suggest that if they do, AI alone could consume as much electricity annually as 22% of all US households by 2028.



But this summer also saw signs of a slowdown that undercut the industry‚Äôs optimism. OpenAI‚Äôs launch of GPT-5 was largely considered a flop, even by the company itself, and that flop led critics to wonder if AI may be hitting a wall. When a group at MIT found that 95% of businesses are seeing no return on their massive AI investments, stocks floundered. The expansion of AI-specific data centers might be an investment that‚Äôs hard to recoup, especially as revenues for AI companies remain elusive.&nbsp;



One of the biggest unknowns about AI‚Äôs future energy burden isn‚Äôt how much a single query consumes, or any other figure that can be disclosed. It‚Äôs whether demand will ever reach the scale companies are building for or whether the technology will collapse under its own hype. The answer will determine whether today‚Äôs buildout becomes a lasting shift in our energy system or a short-lived spike.
‚Ä¢ The Download: introducing our 35 Innovators Under 35 list for 2025
  This is today&#8217;s edition of¬†The Download,¬†our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



Introducing: our 35 Innovators Under 35 list for 2025



The world is full of extraordinary young people brimming with ideas for how to crack tough problems. Every year, we recognize 35 such individuals from around the world‚Äîall of whom are under the age of 35.



These scientists, inventors, and entrepreneurs are working to help mitigate climate change, accelerate scientific progress, and alleviate human suffering from disease. Some are launching companies while others are hard at work in academic labs. They were selected from hundreds of nominees by expert judges and our newsroom staff.&nbsp;



Get to know them all‚Äîincluding our 2025 Innovator of the Year‚Äîin these profiles.







Why basic science deserves our boldest investment



‚ÄîJulia R. Greer is a materials scientist at the California Institute of Technology, a judge for MIT Technology Review‚Äôs Innovators Under 35 and a former honoree (in 2008).



A modern chip the size of a human fingernail contains tens of billions of silicon transistors, each measured in nanometers‚Äîsmaller than many viruses. These tiny switches form the infrastructure behind nearly every digital device in use today.



Much of the fundamental understanding that moved transistor technology forward came from federally funded university research. But that funding is under increasing pressure, thanks to deep budget cuts proposed by the White House.These losses have forced some universities to freeze graduate student admissions, cancel internships, and scale back summer research opportunities‚Äîmaking it harder for young people to pursue scientific and engineering careers.&nbsp;



In an age dominated by short-term metrics and rapid returns, it can be difficult to justify research whose applications may not materialize for decades. But those are precisely the kinds of efforts we must support if we want to secure our technological future. Read the full story.







The must-reads



I‚Äôve combed the internet to find you today‚Äôs most fun/important/scary/fascinating stories about technology.



1 The US is considering annual chip supply permits in ChinaFor South Korean companies Samsung and SK Hynix, specifically. (Bloomberg $)+ US lawmakers still hold power over chips in China. (CNN)



2 America has recorded its first case of screwworm in over 50 yearsAnd the warming climate is making it easier for the flies to thrive. (Vox)+ Experts fear an approaching public health emergency. (The Guardian)3 Drone warfare is dominating Ukraine‚Äôs frontlineAmid relentless assaults, overhead and land drones are being put to work. (The Guardian)+ How cutting-edge drones forced land-locked tanks to evolve. (NYT $)+ On the ground in Ukraine‚Äôs largest Starlink repair shop. (MIT Technology Review)



4 OpenAI is working out why chatbots hallucinate so muchExamining a model‚Äôs incentives provides some clues. (Insider $)+ Models‚Äô tendency to confidently present falsehoods as fact is a big problem. (TechCrunch)+ Why does AI hallucinate? (MIT Technology Review)



5 How one man is connecting Silicon Valley to the Middle East‚Äôs AI boomIf you want to build a data center, Zachary Cefaratti is your man. (FT $)+ The data center boom in the desert. (MIT Technology Review)



6 The first OpenAI-backed movie is coming to theaters next yearThe animated Critterz is hoping for a Cannes Film Festival debut. (WSJ $)+ A Disney director tried‚Äîand failed‚Äîto use an AI Hans Zimmer to create a soundtrack. (MIT Technology Review)



7 Who wants to live forever?These billionaires are confident their cash will pave the way to longer lives. (WSJ $)+ Putin says organ transplants could grant immortality. Not quite. (MIT Technology Review)



8 Tesla isn‚Äôt focused on selling cars any moreThe company‚Äôs latest Master Plan is all about humanoid robots. (The Atlantic $)+ The board is willing to offer Musk a $1 trillion pay package if he delivers. (Wired $)+ Uber is gearing up to test driverless cars in Germany. (The Verge)+ China‚Äôs EV giants are betting big on humanoid robots. (MIT Technology Review)



9 Do aliens go on holiday?Scientists wonder whether tourism could be a potential drive for them to visit us. (New Yorker $)+ How these two UFO hunters became go-to experts on America‚Äôs ‚Äúmystery drone‚Äù invasion. (MIT Technology Review)



10 Vodafone‚Äôs new TikTok influencer isn‚Äôt realIt‚Äôs yet another example of AI avatars being used in ads. (The Verge)+ Synthesia‚Äôs AI clones are more expressive than ever. Soon they‚Äôll be able to talk back. (MIT Technology Review)







Quote of the day



‚ÄúSilicon Valley totally effed up in overhyping LLMs.‚Äù



‚ÄîPalantir CEO Alex Karp criticizes those who fueled the AI hype around large language models, Semafor reports.







One more thing







Puerto Rico‚Äôs power struggles



On the southeastern coast of Puerto Rico lies the country‚Äôs only coal-fired power station, flanked by a mountain of toxic ash. The plant, owned by the utility giant AES, has long plagued this part of Puerto Rico with air and water pollution.Before the coal plant opened Guayama had on average just over 103 cancer cases per year. In 2003, the year after the plant opened, the number of cancer cases in the municipality surged by 50%, to 167.&nbsp;



In 2022, the most recent year with available data, cases hit a new high of 209. The question is: How did it get this bad? Read the full story.



‚ÄîAlexander C. Kaufman







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)



+ What‚Äôs up with tennis players‚Äô strange serving rituals?+ If constant scrolling is turning your hands into gnarled claws, this stretch should help.+ How to land a genuine bargain on Facebook Marketplace.+ This photographer tracks down people who featured in pictures decades before, and persuades them to recreate their poses. Heartwarming stuff
‚Ä¢ How Trump‚Äôs policies are affecting early-career scientists‚Äîin their own words
  This story is part of MIT Technology Review‚Äôs &#8220;America Undone‚Äù series, examining how the foundations of US success in science and innovation are currently under threat.¬†You can read the rest here.



Every year MIT Technology Review celebrates accomplished young scientists, entrepreneurs, and inventors from around the world in our Innovators Under 35 list. We‚Äôve just published the 2025 edition. This year, though, the context is pointedly different: The US scientific community finds itself in an unprecedented position, with the very foundation of its work under attack.&nbsp;



Since Donald Trump took office in January, his administration has fired top government scientists, targeted universities individually and academia more broadly, and made substantial funding cuts to the country‚Äôs science and technology infrastructure. It has also upended longstanding rights and norms related to free speech, civil rights, and immigration‚Äîall of which further affects the overall environment for research and innovation in science and technology.&nbsp;



We wanted to understand how these changes are affecting the careers and work of our most recent classes of innovators. The US government is the largest source of research funding at US colleges and universities, and many of our honorees are new professors and current or recent graduate or PhD students, while others work with government-funded entities in other ways. Meanwhile, about 16% of those in US graduate programs are international students.&nbsp;



We sent surveys to the six most recent cohorts, which include 210 people. We asked people about both positive and negative impacts of the administration‚Äôs new policies and invited them to tell us more in an optional interview. Thirty-seven completed our survey, and we spoke with 14 of them in follow-up calls. Most respondents are academic researchers (about two-thirds) and are based in the US (81%); 11 work in the private sector (six of whom are entrepreneurs). Their responses provide a glimpse into the complexities of building their labs, companies, and careers in today‚Äôs political climate.&nbsp;



Twenty-six people told us that their work has been affected by the Trump administration‚Äôs changes; only one of them described those effects as ‚Äúmostly positive.‚Äù The other 25 reported primarily negative effects. While a few agreed to be named in this story, most asked to be identified only by their job titles and general areas of work, or wished to remain anonymous, for fear of retaliation. ‚ÄúI would not want to flag the ire of the US government,‚Äù one interviewee told us.&nbsp;



Across interviews and surveys, certain themes appeared repeatedly: the loss of jobs, funding, or opportunities; restrictions on speech and research topics; and limits on who can carry out that research. These shifts have left many respondents deeply concerned about the ‚Äúlong-term implications in IP generation, new scientists, and spinout companies in the US,‚Äù as one respondent put it.&nbsp;





One of the things we heard most consistently is that the uncertainty of the current moment is pushing people to take a more risk-averse approach to their scientific work‚Äîeither by selecting projects that require fewer resources or that seem more in line with the administration‚Äôs priorities, or by erring on the side of hiring fewer people. ‚ÄúWe‚Äôre not thinking so much about building and enabling ‚Ä¶ we‚Äôre thinking about surviving,‚Äù said one respondent.&nbsp;



Ultimately, many are worried that all the lost opportunities will result in less innovation overall‚Äîand caution that it will take time to grasp the full impact.&nbsp;



‚ÄúWe‚Äôre not going to feel it right now, but in like two to three years from now, you will feel it,‚Äù said one entrepreneur with a PhD who started his company directly from his area of study. ‚ÄúThere are just going to be fewer people that should have been inventing things.‚Äù



The money: ‚ÄúFolks are definitely feeling the pressure‚Äù



The most immediate impact has been financial. Already, the Trump administration has pulled back support for many areas of science‚Äîending more than a thousand awards by the National Institutes of Health and over 100 grants for climate-related projects by the National Science Foundation. The rate of new awards granted by both agencies has slowed, and the NSF has cut the number of graduate fellowships it‚Äôs funding by half for this school year.&nbsp;



The administration has also cut or threatened to cut funding from a growing number of universities, including Harvard, Columbia, Brown, and UCLA, for supposedly not doing enough to combat antisemitism.



As a result, our honorees said that finding funding to support their work has gotten much harder‚Äîand it was already a big challenge before.&nbsp;







A biochemist at a public university told us she‚Äôd lost a major NIH grant. Since it was terminated earlier this year, she‚Äôs been spending less time in the lab and more on fundraising.&nbsp;



Others described uncertainty about the status of grants from a wide range of agencies, including NSF, the Advanced Research Projects Agency for Health, the Department of Energy, and the Centers for Disease Control and Prevention, which collectively could pay out more than $44 million to the researchers we‚Äôve recognized. Several had waited months for news on an application‚Äôs status or updates on when funds they had already won would be disbursed. One AI researcher who studies climate-related issues is concerned that her multiyear grant may not be renewed, even though renewal would have been ‚Äúfairly standard‚Äù in the past.



Two individuals lamented the cancellation of 24 awards in May by the DOE‚Äôs Office of Clean Energy Demonstrations, including grants for carbon capture projects and a clean cement plant. One said the decision had ‚Äúseverely disrupted the funding environment for climate-tech startups‚Äù by creating ‚Äúwidespread uncertainty,‚Äù ‚Äúundermining investor confidence,‚Äù and ‚Äúcomplicating strategic planning.‚Äù&nbsp;





Climate research and technologies have been a favorite target of the Trump administration: The recently passed tax and spending bill put stricter timelines in place that make it harder for wind and solar installations to qualify for tax credits via the Inflation Reduction Act. Already, at least 35 major commercial climate-tech projects have been canceled or downsized this year.&nbsp;



In response to a detailed list of questions, a DOE spokesperson said, ‚ÄúSecretary [Chris] Wright and President Trump have made it clear that unleashing American scientific innovation is a top priority.‚Äù They pointed to ‚Äúrobust investments in science‚Äù in the president‚Äôs proposed budget and the spending bill and cited special areas of focus ‚Äúto maintain America‚Äôs global competitiveness,‚Äù including nuclear fusion, high-performance computing, quantum computing, and AI.&nbsp;







Other respondents cited tighter budgets brought on by a change in how the government calculates indirect costs, which are funds included in research grants to cover equipment, institutional overhead, and in some cases graduate students‚Äô salaries. In February, the NIH instituted a 15% cap on indirect costs‚Äîwhich ran closer to 28% of the research funds the NIH awarded in 2023. The DOE, DOD, and NSF all soon proposed similar caps. This collective action has sparked lawsuits, and indirect costs remain in limbo. (MIT, which owns MIT Technology Review, is involved in several of these lawsuits; MIT Technology Review is editorially independent from the university.)&nbsp;



Looking ahead, an academic at a public university in Texas, where the money granted for indirect costs funds student salaries, said he plans to hire fewer students for his own lab. ‚ÄúIt‚Äôs very sad that I cannot promise [positions] at this point because of this,‚Äù he told us, adding that the cap could also affect the competitiveness of public universities in Texas, since schools elsewhere may fund their student researchers differently.&nbsp;



At the same time, two people with funding through the Defense Department‚Äîwhich could see a surge of investment under the president‚Äôs proposed budget‚Äîsaid their projects were moving forward as planned. A biomedical engineer at a public university in the Midwest expressed excitement about what he perceives as a fresh surge of federal interest in industrial and defense applications of synthetic biology.&nbsp;Still, he acknowledged colleagues working on different projects don&#8217;t feel as optimistic: ‚ÄúFolks are definitely feeling the pressure.‚Äù



Many who are affected by cuts or delays are now looking for new funding sources in a bid to become less reliant on the federal government. Eleven people said they are pursuing or plan to pursue philanthropic and foundation funding or to seek out industry support. However, the amount of private funding available can‚Äôt begin to make up the difference in federal funds lost, and investors often focus more on low-risk, short-term applications than on open scientific questions.&nbsp;



The NIH responded to a detailed list of questions with a statement pointing to unspecified investments in early-career researchers. ‚ÄúRecent updates to our priorities and processes are designed to broaden scientific opportunity rather than restrict it, ensuring that taxpayer-funded research is rigorous, reproducible, and relevant to all Americans,‚Äù it reads. The NSF declined a request for comment from MIT Technology Review.&nbsp;







Further complicating this financial picture are tariffs‚Äîsome of which are already in effect, and many more of which have been threatened. Nine people who responded to our survey said their work is already being affected by these taxes imposed on goods imported into the US. For some scientists, this has meant higher operating costs for their labs: An AI researcher said tariffs are making computational equipment more expensive, while the Texas academic said the cost of buying microscopes from a German firm had gone up by thousands of dollars since he first budgeted for them. (Neither the White House press office nor the White House Office of Science and Technology Policy responded to requests for comment.)&nbsp;



One cleantech entrepreneur saw a positive impact on his business as more US companies reevaluated their supply chains and sought to incorporate more domestic suppliers. The entrepreneur‚Äôs firm, which is based in the US, has seen more interest for its services from potential customers seeking ‚Äútariff-proof vendors.‚Äù&nbsp;&nbsp;



‚ÄúEverybody is proactive on tariffs and we‚Äôre one of these solutions‚Äîwe‚Äôre made in America,‚Äù he said.&nbsp;



Another person, who works for a European firm, is factoring potential tariffs into decisions about where to open new production facilities. Though the Trump administration has said the taxes are meant to reinvigorate US manufacturing, she‚Äôs now less inclined to build out a significant presence in the US because, she said, tariffs may drive up the costs of importing raw materials that are required to make the company‚Äôs product.&nbsp;



What‚Äôs more, financial backers have encouraged her company to stay rooted abroad because of the potential impact of tariffs for US-based facilities: ‚ÄúPeople who invest worldwide‚Äîthey are saying it‚Äôs reassuring for them right now to consider investing in Europe,‚Äù she said.



The climate of fear: ‚ÄúIt will impact the entire university if there is retaliation‚Äù&nbsp;



Innovators working in both academia and the private sector described new concerns about speech and the politicization of science. Many have changed how they describe their work in order to better align with the administration‚Äôs priorities‚Äîfearing funding cuts, job terminations, immigration action, and other potential retaliation.&nbsp;



This is particularly true for those who work at universities. The Trump administration has reached deals with some institutions, including Columbia and Brown, that would restore part of the funding it slashed‚Äîbut only after the universities agreed to pay hefty fines and abide by terms that, critics say, hand over an unprecedented level of oversight to administration officials.&nbsp;



Some respondents had received guidance on what they could or couldn‚Äôt say from program managers at their funding agencies or their universities or investors; others had not received any official guidance but made personal decisions on what to say and share publicly based on recent news of grant cancellations.



Both on and off campus, there is substantial pressure on diversity, equity, and inclusion (DEI) initiatives, which have been hit particularly hard as the administration seeks to eliminate what it called ‚Äúillegal and immoral discrimination programs‚Äù in one of the first executive orders of President Trump‚Äôs second term.&nbsp;&nbsp;



One respondent, whose work focuses on fighting child sexual abuse materials, recalled rewriting a grant abstract ‚Äú3x to remove words banned‚Äù by Senator Ted Cruz of Texas, an administration ally; back in February, Cruz identified 3,400 NSF grants as ‚Äúwoke DEI‚Äù research advancing ‚Äúneo-Marxist class warfare propaganda.‚Äù (His list includes grants to research self-driving cars and solar eclipses. His office did not respond to a request for comment.)&nbsp;




Many other researchers we spoke with are also taking steps to avoid being put in the DEI bucket. A technologist at a Big Tech firm whose work used to include efforts to provide more opportunities for marginalized communities to get into computing has stopped talking about those recruiting efforts. One biologist described hearing that grant applications for the NIH now have to avoid words like ‚Äúcell type diversity‚Äù for ‚ÄúDEI reasons‚Äù‚Äîno matter that ‚Äúcell type diversity&#8221; is, she said, a common and ‚Äúneutral‚Äù scientific term in microbiology. (In its statement, the NIH said: ‚ÄúTo be clear, no scientific terms are banned, and commonly used terms like ‚Äòcell type diversity‚Äô are fully acceptable in applications and research proposals.‚Äù)&nbsp;



Plenty of other research has also gotten caught up in the storm.&nbsp;






One person who works in climate technology said that she now talks about ‚Äúcritical minerals,‚Äù ‚Äúsovereignty,‚Äù and ‚Äúenergy independence‚Äù or ‚Äúdominance‚Äù rather than ‚Äúclimate‚Äù or ‚Äúindustrial decarbonization.‚Äù (Trump‚Äôs Energy Department has boosted investment in critical minerals, pledging nearly $1 billion to support related projects.) Another individual working in AI said she has been instructed to talk less about ‚Äúregulation,‚Äù ‚Äúsafety,‚Äù or ‚Äúethics‚Äù as they relate to her work. One survey respondent described the language shift as ‚Äúdefinitely more red-themed.‚Äù




Some said that shifts in language won‚Äôt change the substance of their work, but others feared they will indeed affect the research itself.&nbsp;



Emma Pierson, an assistant professor of computer science at the University of California, Berkeley, worried that AI companies may kowtow to the administration, which could in turn ‚Äúinfluence model development.‚Äù While she noted that this fear is speculative, the Trump administration‚Äôs AI Action Plan contains language that directs the federal government to purchase large language models that generate ‚Äútruthful responses‚Äù (by the administration‚Äôs definition), with a goal of ‚Äúpreventing woke AI in the federal government.‚Äù&nbsp;




And one biomedical researcher fears that the administration‚Äôs effective ban on DEI will force an end to outreach ‚Äúfavoring any one community‚Äù and hurt efforts to improve the representation of women and people of color in clinical trials. The NIH and the Food and Drug Administration had been working for years to address the historic underrepresentation of these groups through approaches including specific funding opportunities to address health disparities; many of these efforts have recently been cut.&nbsp;




Respondents from both academia and the private sector told us they‚Äôre aware of the high stakes of speaking out.&nbsp;



‚ÄúAs an academic, we have to be very careful about how we voice our personal opinion because it will impact the entire university if there is retaliation,‚Äù one engineering professor told us.&nbsp;



‚ÄúI don‚Äôt want to be a target,‚Äù said one cleantech entrepreneur, who worries not only about reprisals from the current administration but also about potential blowback from Democrats if he cooperates with it.&nbsp;



‚ÄúI‚Äôm not a Trumper!‚Äù he said. ‚ÄúI‚Äôm just trying not to get fined by the EPA.‚Äù&nbsp;




The people: ‚ÄúThe adversarial attitude against immigrants ‚Ä¶ is posing a brain drain‚Äù



Immigrants are crucial to American science, but what one respondent called a broad ‚Äúpersecution of immigrants,‚Äù and an increasing climate of racism and xenophobia, are matters of growing concern.&nbsp;



Some people we spoke with feel vulnerable, particularly those who are immigrants themselves. The Trump administration has revoked 6,000 international student visas (causing federal judges to intervene in some cases) and threatened to ‚Äúaggressively‚Äù revoke the visas of Chinese students in particular. In recent months, the Justice Department has prioritized efforts to denaturalize certain citizens, while similar efforts to revoke green cards granted decades ago were shut down by court order. One entrepreneur who holds a green card told us, ‚ÄúI find myself definitely being more cognizant of what I‚Äôm saying in public and certainly try to stay away from anything political as a result of what‚Äôs going on, not just in science but in the rest of the administration‚Äôs policies.‚Äù&nbsp;



On top of all this, federal immigration raids and other enforcement actions‚Äîauthorities have turned away foreign academics upon arrival to the US and detained others with valid academic visas, sometimes because of their support for Palestine‚Äîhave created a broad climate of fear.&nbsp;&nbsp;



Four respondents said they were worried about their own immigration status, while 16 expressed concerns about their ability to attract or retain talent, including international students. More than a million international students studied in the US last year, with nearly half of those enrolling in graduate programs, according to the Institute of International Education.&nbsp;



‚ÄúThe adversarial attitude against immigrants, especially those from politically sensitive countries, is posing a brain drain,‚Äù an AI researcher at a large public university on the West Coast told us.&nbsp;



This attack on immigration in the US can be compounded by state-level restrictions. Texas and Florida both restrict international collaborations with and recruitment of scientists from countries&nbsp;including China, even though researchers told us that international collaborations could help mitigate the impacts of decreased domestic funding. ‚ÄúI cannot collaborate at this point because there‚Äôs too many restrictions and Texas also can limit us from visiting some countries,‚Äù the Texas academic said. ‚ÄúWe cannot share results. We cannot visit other institutions ‚Ä¶ and we cannot give talks.‚Äù



All this is leading to more interest in positions outside the United States. One entrepreneur, whose business is multinational, said that their company has received a much higher share of applications from US-based candidates to openings in Europe than it did a year ago, despite the lower salaries offered there.&nbsp;



‚ÄúIt is becoming easier to hire good people in the UK,‚Äù confirmed Karen Sarkisyan, a synthetic biologist based in London.&nbsp;



At least one US-based respondent, an academic in climate technology, accepted a tenured position in the United Kingdom. Another said that she was looking for positions in other countries, despite her current job security and ‚Äúvery good‚Äù salary. ‚ÄúI can tell more layoffs are coming, and the work I do is massively devalued. I can‚Äôt stand to be in a country that treats their scientists and researchers and educated people like this,‚Äù she told us.&nbsp;



Some professors reported in our survey and interviews that their current students are less interested in pursuing academic careers because graduate and PhD students are losing offers and opportunities as a result of grant cancellations. So even as the number of international students dwindles, there may also be ‚Äúshortages in domestic grad students,‚Äù one mechanical engineer at a public university said, and ‚Äúresearch will fall behind.‚Äù&nbsp;&nbsp;




Have more information on this story or a tip for something else that we should report? Using a non-work device, reach the reporter on Signal at eileenguo.15 or tips@technologyreview.com.




In the end, this will affect not just academic research but also private-sector innovation. One biomedical entrepreneur told us that academic collaborators frequently help his company generate lots of ideas: ‚ÄúWe hope that some of them will pan out and become very compelling areas for us to invest in.‚Äù Particularly for small startups without large research budgets, having fewer academics to work with will mean that ‚Äúwe just invest less, we just have fewer options to innovate,‚Äù he said. ‚ÄúThe level of risk that industry is willing to take is generally lower than academia, and you can‚Äôt really bridge that gap.‚Äù&nbsp;



Despite it all, a number of researchers and entrepreneurs who generally expressed frustration about the current political climate said they still consider the US the best place to do science.&nbsp;



Pierson, the AI researcher at Berkeley, described staying committed to her research into social inequities despite the political backlash: ‚ÄúI‚Äôm an optimist. I do believe this will pass, and these problems are not going to pass unless we work on them.‚Äù&nbsp;



And a biotech entrepreneur pointed out that US-based scientists can still command more resources than those in most other countries. ‚ÄúI think the US still has so much going for it. Like, there isn‚Äôt a comparable place to be if you‚Äôre trying to be on the forefront of innovation‚Äîtrying to build a company or find opportunities,‚Äù he said.



Several academics and founders who came to the US to pursue scientific careers spoke about still being drawn to America‚Äôs spirit of invention and the chance to advance on their own merits. ‚ÄúFor me, I‚Äôve always been like, the American dream is something real,‚Äù said one. They said they‚Äôre holding fast to those ideals‚Äîfor now.

üîí Cybersecurity & Privacy
‚Ä¢ 18 Popular Code Packages Hacked, Rigged to Steal Crypto
  At least 18 popular JavaScript code packages that are collectively downloaded more than two billion times each week were briefly compromised with malicious software today, after a developer involved in maintaining the projects was phished. The attack appears to have been quickly contained and was narrowly focused on stealing cryptocurrency. But experts warn that a similar attack with a slightly more nefarious payload could lead to a disruptive malware outbreak that is far more difficult to detect and restrain.
This phishing email lured a developer into logging in at a fake NPM website and supplying a one-time token for two-factor authentication. The phishers then used that developer&#8217;s NPM account to add malicious code to at least 18 popular JavaScript code packages.
Aikido is a security firm in Belgium that monitors new code updates to major open-source code repositories, scanning any code updates for suspicious and malicious code. In a blog post published today, Aikido said its systems found malicious code had been added to at least 18 widely-used code libraries available on NPM (short for) &#8220;Node Package Manager,&#8221; which acts as a central hub for JavaScript development and the latest updates to widely-used JavaScript components.
JavaScript is a powerful web-based scripting language used by countless websites to build a more interactive experience with users, such as entering data into a form. But there&#8217;s no need for each website developer to build a program from scratch for entering data into a form when they can just reuse already existing packages of code at NPM that are specifically designed for that purpose.
Unfortunately, if cybercriminals manage to phish NPM credentials from developers, they can introduce malicious code that allows attackers to fundamentally control what people see in their web browser when they visit a website that uses one of the affected code libraries.
According to Aikido, the attackers injected a piece of code that silently intercepts cryptocurrency activity in the browser, &#8220;manipulates wallet interactions, and rewrites payment destinations so that funds and approvals are redirected to attacker-controlled accounts without any obvious signs to the user.&#8221;
&#8220;This malware is essentially a browser-based interceptor that hijacks both network traffic and application APIs,&#8221; Aikido researcher Charlie Eriksen wrote. &#8220;What makes it dangerous is that it operates at multiple layers: Altering content shown on websites, tampering with API calls, and manipulating what users‚Äô apps believe they are signing. Even if the interface looks correct, the underlying transaction can be redirected in the background.&#8221;
Aikido said it used the social network Bsky to notify the affected developer, Josh Junon, who quickly replied that he was aware of having just been phished. The phishing email that Junon fell for was part of a larger campaign that spoofed NPM and told recipients they were required to update their two-factor authentication (2FA) credentials. The phishing site mimicked NPM&#8217;s login page, and intercepted Junon&#8217;s credentials and 2FA token. Once logged in, the phishers then changed the email address on file for Junon&#8217;s NPM account, temporarily locking him out.
Aikido notified the maintainer on Bluesky, who replied at 15:15 UTC that he was aware of being compromised, and starting to clean up the compromised packages.
Junon also issued a mea culpa on HackerNews, telling the community&#8217;s coder-heavy readership, &#8220;Hi, yep I got pwned.&#8221;
&#8220;It looks and feels a bit like a targeted attack,&#8221; Junon wrote. &#8220;Sorry everyone, very embarrassing.&#8221;
Philippe Caturegli, &#8220;chief hacking officer&#8221; at the security consultancy Seralys, observed that the attackers appear to have registered their spoofed website &#8212; npmjs[.]help &#8212; just two days before sending the phishing email. The spoofed website used services from dnsexit[.]com, a &#8220;dynamic DNS&#8221; company that also offers &#8220;100% free&#8221; domain names that can instantly be pointed at any IP address controlled by the user.
Junon&#8217;s mea cupla on Hackernews today listed the affected packages.
Caturegli said it&#8217;s remarkable that the attackers in this case were not more ambitious or malicious with their code modifications.
&#8220;The crazy part is they compromised billions of websites and apps just to target a couple of cryptocurrency things,&#8221; he said. &#8220;This was a supply chain attack, and it could easily have been something much worse than crypto harvesting.&#8221;
Akito&#8217;s Eriksen agreed, saying countless websites dodged a bullet because this incident was handled in a matter of hours. As an example of how these supply-chain attacks can escalate quickly, Eriksen pointed to another compromise of an NPM developer in late August that added malware to &#8220;nx,&#8221; an open-source code development toolkit with as many as six million weekly downloads.
In the nx compromise, the attackers introduced code that scoured the user&#8217;s device for authentication tokens from programmer destinations like GitHub and NPM, as well as SSH and API keys. But instead of sending those stolen credentials to a central server controlled by the attackers, the malicious code created a new public repository in the victim&#8217;s GitHub account, and published the stolen data there for all the world to see and download.
Eriksen said coding platforms like GitHub and NPM should be doing more to ensure that any new code commits for broadly-used packages require a higher level of attestation that confirms the code in question was in fact submitted by the person who owns the account, and not just by that person&#8217;s account.
&#8220;More popular packages should require attestation that it came through trusted provenance and not just randomly from some location on the Internet,&#8221; Eriksen said. &#8220;Where does the package get uploaded from, by GitHub in response to a new pull request into the main branch, or somewhere else? In this case, they didn&#8217;t compromise the target&#8217;s GitHub account. They didn&#8217;t touch that. They just uploaded a modified version that didn&#8217;t come where it&#8217;s expected to come from.&#8221;
Eriksen said code repository compromises can be devastating for developers, many of whom end up abandoning their projects entirely after such an incident.
&#8220;It&#8217;s unfortunate because one thing we&#8217;ve seen is people have their projects get compromised and they say, &#8216;You know what, I don&#8217;t have the energy for this and I&#8217;m just going to deprecate the whole package,'&#8221; Eriksen said.
Kevin Beaumont, a frequently quoted security expert who writes about security incidents at the blog doublepulsar.com, has been following this story closely today in frequent updates to his account on Mastodon. Beaumont said the incident is a reminder that much of the planet still depends on code that is ultimately maintained by an exceedingly small number of people who are mostly overburdened and under-resourced.
&#8220;For about the past 15 years every business has been developing apps by pulling in 178 interconnected libraries written by 24 people in a shed in Skegness,&#8221; Beaumont wrote on Mastodon. &#8220;For about the past 2 years orgs have been buying AI vibe coding tools, where some exec screams &#8216;make online shop&#8217; into a computer and 389 libraries are added and an app is farted out. The output = if you want to own the world&#8217;s companies, just phish one guy in Skegness.&#8221;
Image: https://infosec.exchange/@GossiTheDog@cyberplace.social.
Aikido recently launched a product that aims to help development teams ensure that every code library used is checked for malware before it can be used or installed. Nicholas Weaver, a researcher with the International Computer Science Institute, a nonprofit in Berkeley, Calif., said Aikido&#8217;s new offering exists because many organizations are still one successful phishing attack away from a supply-chain nightmare.
Weaver said these types of supply-chain compromises will continue as long as people responsible for maintaining widely-used code continue to rely on phishable forms of 2FA.
&#8220;NPM should only support phish-proof authentication,&#8221; Weaver said, referring to physical security keys that are phish-proof &#8212; meaning that even if phishers manage to steal your username and password, they still can&#8217;t log in to your account without also possessing that physical key.
&#8220;All critical infrastructure needs to use phish-proof 2FA, and given the dependencies in modern software, archives such as NPM are absolutely critical infrastructure,&#8221; Weaver said. &#8220;That NPM does not require that all contributor accounts use security keys or similar 2FA methods should be considered negligence.&#8221;
‚Ä¢ GOP Cries Censorship Over Spam Filters That Work
  The chairman of the Federal Trade Commission (FTC) last week sent a letter to Google&#8217;s CEO demanding to know why Gmail was blocking messages from Republican senders while allegedly failing to block similar missives supporting Democrats. The letter followed media reports accusing Gmail of disproportionately flagging messages from the GOP fundraising platform WinRed and sending them to the spam folder. But according to experts who track daily spam volumes worldwide, WinRed&#8217;s messages are getting blocked more because its methods of blasting email are increasingly way more spammy than that of ActBlue, the fundraising platform for Democrats.
Image: nypost.com
On Aug. 13, The New York Post ran an &#8220;exclusive&#8221; story titled, &#8220;Google caught flagging GOP fundraiser emails as &#8216;suspicious&#8217; &#8212; sending them directly to spam.&#8221; The story cited a memo from Targeted Victory ‚Äì whose clients include the National Republican Senatorial Committee (NRSC), Rep. Steve Scalise and Sen. Marsha Blackburn ‚Äì which said it observed that the &#8220;serious and troubling&#8221; trend was still going on as recently as June and July of this year.
‚ÄúIf Gmail is allowed to quietly suppress WinRed links while giving ActBlue a free pass, it will continue to tilt the playing field in ways that voters never see, but campaigns will feel every single day,‚Äù the memo reportedly said.
In an August 28 letter to Google CEO Sundar Pichai, FTC Chairman Andrew Ferguson cited the New York Post story and warned that Gmail&#8217;s parent Alphabet may be engaging in unfair or deceptive practices.
&#8220;Alphabet‚Äôs alleged partisan treatment of comparable messages or messengers in Gmail to achieve political objectives may violate both of these prohibitions under the FTC Act,&#8221; Ferguson wrote. &#8220;And the partisan treatment may cause harm to consumers.&#8221;
However, the situation looks very different when you ask spam experts what&#8217;s going on with WinRed&#8217;s recent messaging campaigns. Atro Tossavainen and Pekka Jalonen are co-founders at Koli-L√µks O√ú, an email intelligence company in Estonia. Koli-L√µks taps into real-time intelligence about daily spam volumes by monitoring large numbers of &#8220;spamtraps&#8221; &#8212; email addresses that are intentionally set up to catch unsolicited emails.
Spamtraps are generally not used for communication or account creation, but instead are created to identify senders exhibiting spammy behavior, such as scraping the Internet for email addresses or buying unmanaged distribution lists. As an email sender, blasting these spamtraps over and over with unsolicited email is the fastest way to ruin your domain&#8217;s reputation online. Such activity also virtually ensures that more of your messages are going to start getting listed on spam blocklists that are broadly shared within the global anti-abuse community.
Tossavainen told KrebsOnSecurity that WinRed&#8217;s emails hit its spamtraps in the .com, .net, and .org space far more frequently than do fundraising emails sent by ActBlue. Koli-L√µks published a graph of the stark disparity in spamtrap activity for WinRed versus ActBlue, showing a nearly fourfold increase in spamtrap hits from WinRed emails in the final week of July 2025.
Image: Koliloks.eu
&#8220;Many of our spamtraps are in repurposed legacy-TLD domains (.com, .org, .net) and therefore could be understood to have been involved with a U.S. entity in their pre-zombie life,&#8221; Tossavainen explained in the LinkedIn post.
Raymond Dijkxhoorn is the CEO and a founding member of SURBL, a widely-used blocklist that flags domains and IP addresses known to be used in unsolicited messages, phishing and malware distribution. Dijkxhoorn said their spamtrap data mirrors that of Koli-L√µks, and shows that WinRed has consistently been far more aggressive in sending email than ActBlue.
Dijkxhoorn said the fact that WinRed&#8217;s emails so often end up dinging the organization&#8217;s sender reputation is not a content issue but rather a technical one.
&#8220;On our end we don‚Äôt really care if the content is political or trying to sell viagra or penis enlargements,&#8221; Dijkxhoorn said. &#8220;It‚Äôs the mechanics, they should not end up in spamtraps. And that‚Äôs the reason the domain reputation is tempered. Not ‚Äòbecause domain reputation firms have a political agenda.&#8217; We really don&#8217;t care about the political situation anywhere. The same as we don&#8217;t mind people buying penis enlargements. But when either of those land in spamtraps it will impact sending experience.&#8221;
The FTC letter to Google&#8217;s CEO also referenced a debunked 2022 study (PDF) by political consultants who found Google caught more Republican emails in spam filters. Techdirt editor Mike Masnick notes that while the 2022 study also found that other email providers caught more Democratic emails as spam, &#8220;Republicans laser-focused on Gmail because it fit their victimization narrative better.&#8221;
Masnick said GOP lawmakers then filed both lawsuits and complaints with the Federal Election Commission (both of which failed easily), claiming this was somehow an ‚Äúin-kind contribution‚Äù to Democrats.
&#8220;This is political posturing designed to keep the White House happy by appearing to &#8216;do something&#8217; about conservative claims of &#8216;censorship,'&#8221; Masnick wrote of the FTC letter. &#8220;The FTC has never policed &#8216;political bias&#8217; in private companies‚Äô editorial decisions, and for good reason‚Äîthe First Amendment prohibits exactly this kind of government interference.&#8221;
WinRed did not respond to a request for comment.
The WinRed website says it is an online fundraising platform supported by a united front of the Trump campaign, the Republican National Committee (RNC), the NRSC,¬†and the National Republican Congressional Committee (NRCC).
WinRed has recently come under fire for aggressive fundraising via text message as well. In June, 404 Media reported on a lawsuit filed by a family in Utah against the RNC for allegedly bombarding their mobile phones with text messages seeking donations after they&#8217;d tried to unsubscribe from the missives dozens of times.
One of the family members said they received 27 such messages from 25 numbers, even after sending 20 stop requests. The plaintiffs in that case allege the texts from WinRed and the RNC &#8220;knowingly disregard stop requests and purposefully use different phone numbers to make it impossible to block new messages.&#8221;
Dijkxhoorn said WinRed did inquire recently about why some of its assets had been marked as a risk by SURBL, but he said they appeared to have zero interest in investigating the likely causes he offered in reply.
&#8220;They only replied with, &#8216;You are interfering with U.S. elections,'&#8221; Dijkxhoorn said, noting that many of SURBL&#8217;s spamtrap domains are only publicly listed in the registration records for random domain names.
&#8220;They‚Äôre at best harvested by themselves but more likely [they] just went and bought lists,&#8221; he said. &#8220;It&#8217;s not like ‚ÄòOh Google is filtering this and not the other,‚Äô the reason isn&#8217;t the provider. The reason is the fundraising spammers and the lists they send to.&#8221;

üéì University AI
No updates.

üè¢ Corporate AI
‚Ä¢ Maximize HyperPod Cluster utilization with HyperPod task governance fine-grained quota allocation
  We are excited to announce the general availability of fine-grained&nbsp;compute and memory quota allocation&nbsp;with HyperPod task governance. With this capability, customers can optimize Amazon SageMaker HyperPod cluster utilization on Amazon Elastic Kubernetes Service (Amazon EKS), distribute fair usage, and support efficient resource allocation across different teams or projects.&nbsp;For more information, see HyperPod task governance best practices for maximizing the value of SageMaker HyperPod task governance. 
Compute quota management is an administrative mechanism that sets and controls compute resource limits across users, teams, and projects. It controls fair resource distribution, preventing a single entity from monopolizing cluster resources, thereby optimizing overall computational efficiency. 
Because of budget constraints, customers might want to allocate compute resources across multiple teams fairly. For example, a data scientist might need some GPUs (for example, four H100 GPUs) for model development, but not the entire instance‚Äôs compute capacity. In other cases, customers have limited compute resources but many teams, and they want to fairly share compute resources across these teams, so that no idle capacity is left unused. 
With HyperPod task governance, administrators can now allocate granular GPU, vCPU, and vCPU memory to teams and projects‚Äîin addition to the entire instance resources‚Äîbased on their preferred strategy. Key capabilities include GPU-level quota allocation by instance type and family, or hardware type‚Äîsupporting both Trainium and NVIDIA GPUs‚Äîand optional CPU and memory allocation for fine-tuned resource control. Administrators can also define&nbsp;the weight (or priority level) a team is given for fair-share idle compute allocation. 

 ‚ÄúWith a wide variety of frontier AI data experiments and production pipelines, being able to maximize SageMaker HyperPod Cluster utilization is extremely high impact. This requires fair and controlled access to shared resources like state-of-the-art GPUs, granular hardware allocation, and more. This is exactly what HyperPod task governance is built for, and we‚Äôre excited to see AWS pushing efficient cluster utilization for a variety of AI use cases.‚Äù 
 ‚Äì Daniel Xu, Director of Product at Snorkel AI, whose AI data technology platform empowers enterprises to build specialized AI applications by leveraging their organizational expertise at scale.
 
In this post, we dive deep into how to define quotas for teams or projects based on granular or instance-level allocation. We discuss different methods to define such policies, and how data scientists can schedule their jobs seamlessly with this new capability. 
Solution overview 
Prerequisites 
To follow the examples in this blog post, you need to meet the following prerequisites: 
 
 An AWS account with access to SageMaker HyperPod. 
 A running SageMaker HyperPod (EKS-orchestrated) cluster. For more information on how to create and configured a new HyperPod cluster, see the HyperPod workshop or the SageMaker HyperPod cluster creation with Amazon EKS orchestration. 
 HyperPod task governance addon version 1.3 or later installed in the cluster. For more information, see set up HyperPod task governance 
 
To schedule and execute the example jobs in the Submitting Tasks section, you will also need: 
 
 A local environment (either your local machine or a cloud-based compute environment), from which to run the HyperPod CLI and kubectl commands, configured as follows: 
   
   OS based on Linux or MacOS 
   Python&nbsp;3.8, 3.9, 3.10, or 3.11 installed 
   AWS Command Line Interface (AWS CLI) configured with the appropriate credentials to use the above services 
   HyperPod CLI version 3.1.0 
   Kubernetes command-line tool, kubectl 
    
 HyperPod Training Operator installed in the cluster 
 
Allocating granular compute and memory quota using the AWS console 
Administrators are the primary persona interacting with SageMaker HyperPod task governance and are responsible for managing cluster compute allocation in alignment with the organization‚Äôs strategic priorities and goals. 
Implementing this feature follows the familiar compute allocation&nbsp;creation workflow of HyperPod task governance.&nbsp;To get started, sign in to the AWS Management Console and navigate to Cluster Management under HyperPod Clusters in the Amazon SageMaker AI console. After selecting your HyperPod cluster, select the Policies tab in the cluster detail page. Navigate to&nbsp;Compute allocations and choose Create. 
 
As with existing functionality, you can enable task prioritization and fair-share resource allocation through cluster policies that prioritize critical workloads and distribute idle compute across teams. By using HyperPod task governance, you can define queue admission policies (first-come-first-serve by default or task ranking) and idle compute allocation methods (first-come-first-serve or fair-share by default). In the Compute allocation section, you can create and edit allocations to distribute resources among teams, enable lending and borrowing of idle compute, configure preemption of low-priority tasks, and assign fair-share weights. 
The key innovation is in the Allocations section shown in the following figure, where you‚Äôll now find fine-grained options for resource allocation. In addition to the existing instance-level quotas, you can now directly specify GPU quotas by instance type and family or by hardware type. When you define GPU allocations, HyperPod task governance intelligently calculates appropriate default values for vCPUs and memory which are set proportionally. 
For example, when allocating 2 GPUs from a single p5.48xlarge instance (which has 8 GPUs, 192 vCPUs, and 2 TiB memory) in your HyperPod cluster, HyperPod task governance assigns 48 vCPUs and 512 GiB memory as default values‚Äîwhich is equivalent to one quarter of the instance‚Äôs total resources. Similarly, if your HyperPod cluster contains 2 ml.g5.2xlarge instances (each with 1 GPU, 8 vCPUs, and 32 GiB memory), allocating 2 GPUs would automatically assign 16 vCPUs and 64 GiB memory from both instances as shown in the following image. 
 
You can either proceed with these automatically calculated default values or customize the allocation by manually adjusting the vCPUs and vCPU memory fields as seen in the following image. 
 
Amazon SageMaker HyperPod supports clusters that include CPU-based instances, GPU-based instances, and AWS Neuron-based hardware (AWS Inferentia and AWS Trainium chips). You can specify resource allocation for your team by instances, GPUs, vCPUs, vCPU memory, or Neuron devices, as shown in the following image. 
 
Quota allocation can be more than capacity. Resources added to the compute allocation policy that aren‚Äôt currently available in the cluster represent planning for future capacity upgrades. Jobs that require these unprovisioned resources will be automatically queued and remain in a pending state until the necessary resources become available. It‚Äôs important to understand that in SageMaker HyperPod, compute allocations function as quotas, which are verified during workload scheduling to understand if a workload should be admitted or not, regardless of actual capacity availability. When resource requests are within these defined allocation limits and current utilization, the Kubernetes scheduler (kube-scheduler) handles the actual distribution and placement of pods across the HyperPod cluster nodes. 
Allocating granular compute and memory quota using AWS CLI 
You can also create or update compute quotas using the AWS CLI. The following is an example for creating a compute quota with only GPU count specification using the AWS CLI: 
 
 aws sagemaker \
create-compute-quota \
--region &lt;aws_region&gt;&nbsp;\
--name "only-gpu-quota" \
--cluster-arn "arn:aws:sagemaker: &lt;aws_region&gt;:&lt;account_id&gt;:cluster/&lt;cluster_id&gt;" \
--description "test description" \
--compute-quota-config "ComputeQuotaResources=[{InstanceType=ml.g6.12xlarge,Accelerators=2}],ResourceSharingConfig={Strategy=LendAndBorrow,BorrowLimit=10}" \
--activation-state "Enabled" \
--compute-quota-target "TeamName=onlygputeam2,FairShareWeight=10"&nbsp; 
 
Compute quotas can also be created with mixed quota types, including a certain number of instances and granular compute resources, as shown in the following example: 
 
 aws sagemaker \
create-compute-quota \
--region &lt;aws_region&gt; \
--name "mix-quota-type" \
--cluster-arn "arn:aws:sagemaker:&lt;aws_region&gt;:&lt;account_id&gt;:cluster/&lt;cluster_id&gt;" \
--description "Mixed quota allocation" \
--compute-quota-config "ComputeQuotaResources=[{InstanceType=ml.g6.12xlarge,Accelerators=2}, {InstanceType=ml.p5.48xlarge,Count=3}, {InstanceType=ml.c5.2xlarge,VCpu=2}],ResourceSharingConfig={Strategy=LendAndBorrow,BorrowLimit=10}" \
--activation-state "Enabled" \
--compute-quota-target "TeamName=mixquotatype,FairShareWeight=10"&nbsp; 
 
HyperPod task governance deep dive 
SageMaker HyperPod task governance enables allocation of GPU, CPU, and memory resources by integrating with Kueue, a Kubernetes-native system for job queueing. 
Kueue doesn‚Äôt replace existing Kubernetes scheduling components, but rather integrates with the kube-scheduler, such that Kueue decides whether a workload should be admitted based on the resource quotas and current utilization, and then the kube-scheduler takes care of pod placement on the nodes. 
When a workload requests specific resources, Kueue selects an appropriate resource flavor based on availability, node affinity, and job priority. The scheduler then injects the corresponding node labels and tolerations into the PodSpec, allowing Kubernetes to place the pod on nodes with the requested hardware configuration. This supports precise resource governance and efficient allocation for multi-tenant clusters. 
When a SageMaker HyperPod task governance compute allocation is created, Kueue creates ClusterQueues that define resource quotas and scheduling policies, along with ResourceFlavors for the selected instance types with their unique resource characteristics. 
For example, the following compute allocation policy allocates ml.g6.12xlarge instances with 2 GPUs and 48 vCPUs to the onlygputeam team, implementing a LendAndBorrow strategy with an up to 50% borrowing limit. This configuration enables flexible resource sharing while maintaining priority through a fair share weight of 10 and the ability to preempt lower priority tasks from other teams. 
 
 aws sagemaker describe-compute-quota \ &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 
--region &lt;aws_region&gt; \
--compute-quota-id &lt;compute_quota_id&gt;

#output
{
&nbsp;&nbsp; &nbsp;"ComputeQuotaArn": "arn:aws:sagemaker:&lt;aws_region&gt;:&lt;account_id&gt;:compute-quota/&lt;compute_quota_id&gt;",
&nbsp;&nbsp; &nbsp;"ComputeQuotaId": "&lt;compute_quota_id&gt;",
&nbsp;&nbsp; &nbsp;"Name": "only-gpu-quota",
&nbsp;&nbsp; &nbsp;"Description": "Only GPU quota allocation",
&nbsp;&nbsp; &nbsp;"ComputeQuotaVersion":&nbsp;1,
&nbsp;&nbsp; &nbsp;"Status": "Created",
&nbsp;&nbsp; &nbsp;"ClusterArn": "arn:aws:sagemaker:&lt;aws_region&gt;:&lt;account_id&gt;:cluster/&lt;cluster_id&gt;",
&nbsp;&nbsp; &nbsp;"ComputeQuotaConfig": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"ComputeQuotaResources": [
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;{
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"InstanceType": "ml.g6.12xlarge",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"Accelerators": 2,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"VCpu": 48.0
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;],
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"ResourceSharingConfig": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"Strategy": "LendAndBorrow",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"BorrowLimit": 50
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;},
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"PreemptTeamTasks": "LowerPriority"
&nbsp;&nbsp; &nbsp;},
&nbsp;&nbsp; &nbsp;"ComputeQuotaTarget": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"TeamName": "onlygputeam",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"FairShareWeight": 10
&nbsp;&nbsp; &nbsp;},
&nbsp;&nbsp; &nbsp;"ActivationState": "Enabled",
&nbsp;&nbsp; &nbsp;"CreationTime": "2025-07-24T11:12:12.021000-07:00",
&nbsp;&nbsp; &nbsp;"CreatedBy": {},
&nbsp;&nbsp; &nbsp;"LastModifiedTime": "2025-07-24T11:15:45.205000-07:00",
&nbsp;&nbsp; &nbsp;"LastModifiedBy": {}
} 
 
The corresponding Kueue ClusterQueue is configured with the&nbsp;ml.g6.12xlarge flavor, providing quotas for 2 NVIDIA GPUs, 48 CPU cores, and 192 Gi memory. 
 
 kubectl describe clusterqueue hyperpod-ns-onlygputeam-clusterqueue

# output
Name: &nbsp; &nbsp; &nbsp; &nbsp; hyperpod-ns-onlygputeam-clusterqueue
Namespace:
Labels: &nbsp; &nbsp; &nbsp; sagemaker.amazonaws.com/quota-allocation-id=onlygputeam
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;sagemaker.amazonaws.com/sagemaker-managed-queue=true
Annotations: &nbsp;&lt;none&gt;
API Version: &nbsp;kueue.x-k8s.io/v1beta1
Kind: &nbsp; &nbsp; &nbsp; &nbsp; ClusterQueue
Metadata:
&nbsp; ...
Spec:
&nbsp;&nbsp;Cohort: &nbsp;shared-pool
&nbsp;&nbsp;Fair Sharing:
&nbsp;&nbsp; &nbsp;Weight: &nbsp;10
&nbsp;&nbsp;Flavor Fungibility:
&nbsp;&nbsp; &nbsp;When Can Borrow: &nbsp; TryNextFlavor
&nbsp;&nbsp; &nbsp;When Can Preempt: &nbsp;TryNextFlavor
&nbsp;&nbsp;Namespace Selector:
&nbsp;&nbsp; &nbsp;Match Labels:
&nbsp;&nbsp; &nbsp; &nbsp;kubernetes.io/metadata.name: &nbsp;hyperpod-ns-onlygputeam
&nbsp;&nbsp;Preemption:
&nbsp;&nbsp; &nbsp;Borrow Within Cohort:
&nbsp;&nbsp; &nbsp; &nbsp;Policy: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; LowerPriority
&nbsp;&nbsp; &nbsp;Reclaim Within Cohort: &nbsp;Any
&nbsp;&nbsp; &nbsp;Within Cluster Queue: &nbsp; LowerPriority
&nbsp;&nbsp;Queueing Strategy: &nbsp; &nbsp; &nbsp; &nbsp;BestEffortFIFO
&nbsp;&nbsp;Resource Groups:
&nbsp;&nbsp; &nbsp;Covered Resources:
&nbsp;&nbsp; &nbsp; &nbsp;nvidia.com/gpu
&nbsp;&nbsp; &nbsp; &nbsp;aws.amazon.com/neurondevice
&nbsp;&nbsp; &nbsp; &nbsp;cpu
&nbsp;&nbsp; &nbsp; &nbsp;memory
&nbsp;&nbsp; &nbsp; &nbsp;vpc.amazonaws.com/efa
&nbsp;&nbsp; &nbsp;Flavors:
&nbsp;&nbsp; &nbsp; &nbsp;Name: &nbsp;ml.g6.12xlarge
&nbsp;&nbsp; &nbsp; &nbsp;Resources:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;Borrowing Limit: &nbsp;1
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;Name: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; nvidia.com/gpu
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;Nominal Quota: &nbsp; &nbsp;2
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;Borrowing Limit: &nbsp;0
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;Name: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; aws.amazon.com/neurondevice
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;Nominal Quota: &nbsp; &nbsp;0
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;Borrowing Limit: &nbsp;24
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;Name: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; cpu
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;Nominal Quota: &nbsp; &nbsp;48
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;Borrowing Limit: &nbsp;96Gi
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;Name: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; memory
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;Nominal Quota: &nbsp; &nbsp;192Gi
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;Borrowing Limit: &nbsp;0
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;Name: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; vpc.amazonaws.com/efa
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;Nominal Quota: &nbsp; &nbsp;1
&nbsp;&nbsp; &nbsp;... 
 
A Kueue LocalQueue will be also created, and will reference the corresponding ClusterQueue. The LocalQueue acts as the namespace-scoped resource through which users can submit workloads, and these workloads are then admitted and scheduled according to the quotas and policies defined in the ClusterQueue. 
 
 kubectl describe localqueue hyperpod-ns-onlygputeam-localqueue -n hyperpod-ns-onlygputeam

# output
Name: &nbsp; &nbsp; &nbsp; &nbsp; hyperpod-ns-onlygputeam-localqueue
Namespace: &nbsp; &nbsp;hyperpod-ns-onlygputeam
Labels: &nbsp; &nbsp; &nbsp; sagemaker.amazonaws.com/quota-allocation-id=onlygputeam
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;sagemaker.amazonaws.com/sagemaker-managed-queue=true
Annotations: &nbsp;&lt;none&gt;
API Version: &nbsp;kueue.x-k8s.io/v1beta1
Kind: &nbsp; &nbsp; &nbsp; &nbsp; LocalQueue
Metadata:
&nbsp; &nbsp; ...
Spec:
&nbsp;&nbsp;Cluster Queue: &nbsp;hyperpod-ns-onlygputeam-clusterqueue
&nbsp;&nbsp;Stop Policy: &nbsp; &nbsp;None
Status:
&nbsp;&nbsp;Admitted Workloads: &nbsp;0 
 
Submitting tasks 
There are two ways to submit tasks on Amazon EKS orchestrated SageMaker HyperPod clusters: the SageMaker HyperPod CLI and the Kubernetes command-line tool, kubectl.&nbsp;With both options, data scientists need to reference their team‚Äôs namespace and task priority class‚Äîin addition to the requested GPU and vCPU compute and memory resources‚Äîto use their granular allocated quota with appropriate prioritization. If the user doesn‚Äôt specify a priority class, then SageMaker HyperPod task governance will automatically assume the lowest priority.&nbsp;The specific GPU type comes from an instance type selection, because data scientists want to use GPUs with certain capabilities (for example, H100 instead of H200) to perform their tasks efficiently. 
HyperPod CLI 
The HyperPod CLI was created to abstract the complexities of working with kubectl and so that developers using SageMaker HyperPod can iterate faster with custom commands.The following is an example of a job submission with the HyperPod CLI requesting both compute and memory resources: 
 
 hyp create hyp-pytorch-job \
--job-name sample-job1 \
--image  &lt;account_id&gt;.dkr.ecr.&lt;aws_region&gt;.amazonaws.com/&lt;image_name&gt;:&lt;tag&gt; \
--pull-policy "Always" \
--tasks-per-node 1 \
--max-retry 1 \
--priority high-priority \
--namespace hyperpod-ns-team1 \
--queue-name hyperpod-ns-team1-localqueue \
--instance-type ml.g5.8xlarge \
--accelerators 1 \
--vcpu 4 \
--memory 1 \
--accelerators-limit 1 \
--vcpu-limit 5 \
--memory-limit 2 
 
The highlighted parameters enable requesting granular compute and memory resources. The HyperPod CLI requires to install the HyperPod Training Operator in the cluster and then build a container image that includes the HyperPod Elastic Agent. For further instructions on how to build such container image, please refer to the HyperPod Training Operator documentation. 
For more information on the supported HyperPod CLI arguments and related description, see the SageMaker HyperPod CLI reference documentation. 
Kubectl 
The following is an example of a kubectl command to submit a job to the HyperPod cluster using the specified queue. This is a simple example of a PyTorch job that will check for GPU availability and then sleep for 5 minutes. Compute and memory resources are requested using the standard Kubernetes resource management constructs. 
 
  
  apiVersion: batch/v1 
kind: Job 
metadata: 
  name: gpu-training-job 
  namespace: hyperpod-ns-team1 
spec: 
  parallelism: 1 
  completions: 1 
  suspend: true 
  template: 
    metadata: 
      labels: 
        kueue.x-k8s.io/queue-name: hyperpod-ns-team1-localqueue
        kueue.x-k8s.io/priority-class: high-priority
    spec: 
      containers: 
      - name: training-container 
        image: pytorch/pytorch:2.0.0-cuda11.7-cudnn8-runtime
        command: 
        - "python"
        - "-c"
        - "import torch; print('GPU available:', torch.cuda.is\_available()); import time; time.sleep(15)" 
        resources: 
          requests:  
            nvidia.com/gpu: 1  
            cpu: "4"  
            memory: "1Gi"  
          limits:  
            nvidia.com/gpu: 1         
      restartPolicy: Never 
  
 Sample commands 
 
Following is a short reference guide for helpful commands when interacting with SageMaker HyperPod task governance: 
 
 Describing cluster policy with the AWS CLI ‚Äì This AWS CLI command is useful for viewing the cluster policy settings for your cluster. 
 List compute quota allocations with the AWS CLI ‚Äì Use this AWS CLI command to view the different teams and set up task governance and their respective quota allocation settings. 
 HyperPod CLI ‚Äì The HyperPod CLI abstracts&nbsp;common kubectl commands used to interact with SageMaker HyperPod clusters such as submitting, listing, and cancelling tasks. See the SageMaker HyperPod CLI reference documentation&nbsp;for a full list of commands. 
 kubectl ‚Äì You can also use kubectl&nbsp;to interact with task governance; some useful commands are: 
 
kubectl get workloads -n hyperpod-ns-&lt;team-name&gt; kubectl describe workload &lt;workload-name&gt; -n hyperpod-ns-&lt;team-name&gt;. These commands show the workloads running in your cluster per namespace and provide detailed reasonings on Kueue admission. You can use these commands to answer questions such as ‚ÄúWhy was my task preempted?‚Äù or ‚ÄúWhy did my task get admitted?‚Äù 
Common scenarios 
A common use case for more granular allocation of GPU compute is fine-tuning small and medium sized large language models (LLMs). A single H100 or H200 GPU might be sufficient to address such a use case (also depending on the chosen batch size and other factors), and machine learning (ML) platform administrators can choose to allocate a single GPU to each data scientist or ML researcher to optimize the utilization of an instance like ml.p5.48xlarge, which comes with 8 H100 GPUs onboard. 
Small language models (SLMs) have emerged as a significant advancement in generative AI, offering lower latency, decreased deployment costs, and enhanced privacy capabilities while maintaining impressive performance on targeted tasks, making them increasingly vital for agentic workflows and edge computing scenarios. The new SageMaker HyperPod task governance with fine-grained GPU, CPU, and memory allocation significantly enhances SLM development by enabling precise matching of resources to model requirements, allowing teams to efficiently run multiple experiments concurrently with different architectures. This resource optimization is particularly valuable as organizations develop specialized SLMs for domain-specific applications, with priority-based scheduling so that critical model training jobs receive resources first while maximizing overall cluster utilization. By providing exactly the right resources at the right time, HyperPod accelerates the development of specialized, domain-specific SLMs that can be deployed as efficient agents in complex workflows, enabling more responsive and cost-effective AI solutions across industries. 
With the growing popularity of SLMs, organizations can use granular quota allocation to create targeted quota policies that prioritize GPU resources, addressing the budget-sensitive nature of ML infrastructure where GPUs represent the most significant cost and performance factor. Organizations can now selectively apply CPU and memory limits where needed, creating a granular resource management approach that efficiently supports diverse machine learning workloads regardless of model size. 
Similarly, to support inference workloads, multiple teams might not require an entire instance to deploy their models, helping to avoid having entire instances equipped with multiple GPUs allocated to each team and leaving GPU compute sitting idle. 
Finally, during experimentation and algorithm development, data scientists and ML researchers can choose to deploy a container hosting their preferred IDE on HyperPod, like JupyterLab or Code-OSS (Visual Studio Code open source). In this scenario, they often experiment with smaller batch sizes before scaling to multi-GPU configurations, hence not needing entire multi-GPU instances to be allocated.Similar considerations apply to CPU instances; for example, an ML platform administrator might decide to use CPU instances for IDE deployment, because data scientists prefer to scale their training or fine-tuning with jobs rather than experimenting with the local IDE compute. In such cases, depending on the instances of choice, partitioning CPU cores across the team might be beneficial. 
Conclusion 
The introduction of fine-grained compute quota allocation in SageMaker HyperPod represents a significant advancement in ML infrastructure management. By enabling GPU-level resource allocation alongside instance-level controls, organizations can now precisely tailor their compute resources to match their specific workloads and team structures. 
This granular approach to resource governance addresses critical challenges faced by ML teams today, balancing budget constraints, maximizing expensive GPU utilization, and ensuring fair access across data science teams of all sizes. Whether fine-tuning SLMs that require single GPUs, running inference workloads with varied resource needs, or supporting development environments that don‚Äôt require full instance power, this flexible capability helps ensure that no compute resources sit idle unnecessarily. 
ML workloads continue to diversify in their resource requirements and SageMaker HyperPod task governance now provides the adaptability organizations need to optimize their GPU capacity investments. To learn more, visit the&nbsp;SageMaker HyperPod product page and HyperPod task governance documentation. 
Give this a try in the&nbsp;Amazon SageMaker AI console and leave your comments here. 
 
About the authors 
Siamak Nariman is a Senior Product Manager at AWS. He is focused on AI/ML technology, ML model management, and ML governance to improve overall organizational efficiency and productivity. He has extensive experience automating processes and deploying various technologies. 
Zhenshan Jin is a Senior Software Engineer at Amazon Web Services (AWS), where he leads software development for task governance on SageMaker HyperPod. In his role, he focuses on empowering customers with advanced AI capabilities while fostering an environment that maximizes engineering team efficiency and productivity. 
Giuseppe Angelo Porcelli is a Principal Machine Learning Specialist Solutions Architect for Amazon Web Services. With several years of software engineering and an ML background, he works with customers of any size to understand their business and technical needs and design AI and ML solutions that make the best use of the AWS Cloud and the Amazon Machine Learning stack. He has worked on projects in different domains, including MLOps, computer vision, and NLP, involving a broad set of AWS services. In his free time, Giuseppe enjoys playing football. 
Sindhura Palakodety is a Solutions Architect at AWS. She is passionate about helping customers build enterprise-scale Well-Architected solutions on the AWS platform and specializes in the data analytics domain.
‚Ä¢ Build and scale adoption of AI agents for education with Strands Agents, Amazon Bedrock AgentCore, and LibreChat
  Basic AI chat isn‚Äôt enough for most business applications. Institutions need AI that can pull from their databases, integrate with their existing tools, handle multi-step processes, and make decisions independently. 
This post demonstrates how to quickly build sophisticated AI agents using Strands Agents, scale them reliably with Amazon Bedrock AgentCore, and make them accessible through LibreChat‚Äôs familiar interface to drive immediate user adoption across your institution. 
Challenges with basic AI chat interfaces 
Although basic AI chat interfaces can answer questions and generate content, educational institutions need capabilities that simple chat can‚Äôt provide: 
 
 Contextual decision-making ‚Äì A student asking ‚ÄúWhat courses should I take?‚Äù needs an agent that can access their transcript, check prerequisites, verify graduation requirements, and consider schedule conflicts‚Äînot just generic course descriptions 
 Multi-step workflows ‚Äì Degree planning requires analyzing current progress, identifying remaining requirements, suggesting course sequences, and updating recommendations as students make decisions 
 Institutional data integration ‚Äì Effective educational AI must connect to student information systems, learning management services, academic databases, and institutional repositories to provide relevant, personalized guidance 
 Persistent memory and learning ‚Äì Agents need to remember previous interactions with students, track their academic journey over semesters, and build understanding of individual learning patterns and needs 
 
Combining open source flexibility with enterprise infrastructure 
The integration presented in this post demonstrates how three technologies can work together to address these challenges: 
 
 Strands Agents ‚Äì Build sophisticated multi-agent workflows in just a few lines of code 
 Amazon Bedrock AgentCore ‚Äì Scale agents reliably with serverless, pay-per-use deployment 
 LibreChat ‚Äì Provide users with a familiar chat interface that drives immediate adoption 
 
Strands Agents overview 
Strands Agents is an open source SDK that takes a model-driven approach to building and running AI agents in just a few lines of code. Unlike LibreChat‚Äôs simple agent implementation, Strands supports sophisticated patterns including multi-agent orchestration through workflow, graph, and swarm tools; semantic search for managing thousands of tools; and advanced reasoning capabilities with deep analytical thinking cycles. The framework simplifies agent development by embracing the capabilities of state-of-the-art models to plan, chain thoughts, call tools, and reflect, while scaling from local development to production deployment with flexible architectures and comprehensive observability. 
Amazon Bedrock AgentCore overview 
Amazon Bedrock AgentCore is a comprehensive set of enterprise-grade services that help developers quickly and securely deploy and operate AI agents at scale using the framework and model of your choice, hosted on Amazon Bedrock or elsewhere. The services are composable and work with popular open source frameworks and many models, so you don‚Äôt have to choose between open source flexibility and enterprise-grade security and reliability. 
Amazon Bedrock AgentCore includes modular services that can be used together or independently: Runtime (secure, serverless runtime for deploying and scaling dynamic agents), Gateway (converts APIs and AWS Lambda functions into agent-compatible tools), Memory (manages both short-term and long-term memory), Identity (provides secure access management), and Observability (offers real-time visibility into agent performance). 
The key Amazon Bedrock AgentCore service used in this integration is Amazon Bedrock AgentCore Runtime, a secure, serverless runtime purpose-built for deploying and scaling dynamic AI agents and tools using an open source framework including LangGraph, CrewAI, and Strands Agents; a protocol; and a model of your choosing. Amazon Bedrock AgentCore Runtime was built to work for agentic workloads with industry-leading extended runtime support, fast cold starts, true session isolation, built-in identity, and support for multimodal payloads. Rather than the typical serverless model where functions spin up, execute, and immediately terminate, Amazon Bedrock AgentCore Runtime provisions dedicated microVMs that can persist for up to 8 hours, enabling sophisticated multi-step agentic workflows where each subsequent call builds upon the accumulated context and state from previous interactions within the same session. 
LibreChat overview 
LibreChat has emerged as a leading open source alternative to commercial AI chat interfaces, offering educational institutions a powerful solution for deploying conversational AI at scale. Built with flexibility and extensibility in mind, LibreChat provides several key advantages for higher education: 
 
 Multi-model support ‚Äì LibreChat supports integration with multiple AI providers, so institutions can choose the most appropriate models for different use cases while avoiding vendor lock-in 
 User management ‚Äì Robust authentication and authorization systems help institutions manage access across student populations, faculty, and staff with appropriate permissions and usage controls 
 Conversation management ‚Äì Students and faculty can organize their AI interactions into projects and topics, creating a more structured learning environment 
 Customizable interface ‚Äì The solution can be branded and customized to match institutional identity and specific pedagogical needs 
 
Integration benefits 
Integrating Strands Agents with Amazon Bedrock AgentCore and LibreChat creates unique benefits that extend the capabilities of both services far beyond what either could achieve independently: 
 
 Seamless agent experience through familiar interface ‚Äì LibreChat‚Äôs intuitive chat interface becomes a gateway to sophisticated agentic workflows. Users can trigger complex multi-step processes, data analysis, and external system integrations through natural conversation, without needing to learn new interfaces or complex APIs. 
 Dynamic agent loading and management ‚Äì Unlike static AI chat implementations, this integration supports dynamic agent loading with access management. New agentic applications can be deployed separately and made available to users without requiring LibreChat updates or downtime, enabling rapid agent development. 
 Enterprise-grade security and scaling ‚Äì Amazon Bedrock AgentCore Runtime provides complete session isolation for each user session, where each session runs with isolated CPU, memory, and filesystem resources. This creates complete separation between user sessions, safeguarding stateful agent reasoning processes and helping prevent cross-session data contamination. The service can scale up to thousands of agent sessions in seconds while developers only pay for actual usage, making it ideal for educational institutions that need to support large student populations with varying usage patterns. 
 Built-in AWS resource integration ‚Äì Organizations already running infrastructure on AWS can seamlessly connect their existing resources‚Äîdatabases, data lakes, Lambda functions, and applications‚Äîto Strands Agents without complex integrations or data movement. Agents can directly access and surface insights through the LibreChat interface, turning existing AWS investments into intelligent, conversational experiences, such as querying an Amazon Relational Database Service (Amazon RDS) database, analyzing data in Amazon Simple Storage Service (Amazon S3), or integrating with existing microservices. 
 Cost-effective agentic computing ‚Äì By using LibreChat‚Äôs efficient architecture with the Amazon Bedrock AgentCore pay-per-use model, organizations can deploy sophisticated agentic applications without the high fixed costs typically associated with enterprise AI systems. Users only pay for actual agent computation and tool usage. 
 
Agent use cases in higher education settings 
The integration of LibreChat with Strands Agents enables numerous educational applications that demonstrate the solution‚Äôs versatility and power: 
 
 A course recommendation agent can analyze a student‚Äôs academic history, current enrollment, and career interests to suggest relevant courses. By integrating with the student information system, the agent can make sure recommendations consider prerequisites, schedule conflicts, and graduation requirements. 
 A degree progress tracking agent can interact with students and help them understand their specific degree requirements and provide guidance on remaining coursework, elective options, and timeline optimization. 
 Agents can be configured with access to academic databases and institutional repositories, helping students and faculty discover relevant research papers and resources, providing guidance on academic writing, citation formats, and research methodology specific to different disciplines. 
 Agents can handle routine student inquiries about registration, deadlines, and campus resources, freeing up staff time for more complex student support needs. 
 
Refer to the following GitHub repo for Strands Agent code examples for educational use cases. 
Solution overview 
The following architecture diagram illustrates the overall system design for deploying LibreChat with Strands Agents integration. Strands Agents is deployed using Amazon Bedrock AgentCore Runtime, a secure, serverless runtime purpose-built for deploying and scaling dynamic AI agents and tools using an open source framework including Strands Agents. 
 
The solution architecture includes several key components: 
 
 LibreChat core services ‚Äì The core chat interface runs in an Amazon Elastic Container Service (Amazon ECS) with AWS Fargate cluster, including LibreChat for the user-facing experience, Meilisearch for enhanced search capabilities, and Retrieval Augmented Generation (RAG) API services for document retrieval. 
 LibreChat supporting infrastructure ‚Äì This solution uses Amazon Elastic File System (Amazon EFS) for storing Meilisearch‚Äôs indexes and user uploaded files; Amazon Aurora PostgreSQL-Compatible Edition for vector database used by the RAG API; Amazon S3 for storing LibreChat configurations; Amazon DocumentDB for user, session, conversation data management; and AWS Secrets Manager for managing access to the resources. 
 Strands Agents integration ‚Äì This solution integrates Strands Agents (hosted by Amazon Bedrock AgentCore Runtime) with LibreChat through custom endpoints using Lambda and Amazon API Gateway. This integration pattern enables dynamic loading of agents in LibreChat for advanced generative AI capabilities. In particularly, the solution showcases a user activity analysis agent that draws insights from LibreChat logs. 
 Authentication and security ‚Äì The integration between LibreChat and Strands Agents implements a multi-layered authentication approach that maintains security without compromising user experience or administrative simplicity. When a student or faculty member selects a Strands Agent from LibreChat‚Äôs interface, the authentication flow operates seamlessly in the background through several coordinated layers: 
   
   User authentication ‚Äì LibreChat handles user login through your institution‚Äôs existing authentication system, with comprehensive options including OAuth, LDAP/AD, or local accounts as detailed in the LibreChat authentication documentation. 
   API Gateway security ‚Äì After users are authenticated to LibreChat, the system automatically handles API Gateway security by authenticating each request using preconfigured API keys. 
   Service-to-service authentication ‚Äì The underlying Lambda function uses AWS Identity and Access Management (IAM) roles to securely invoke Amazon Bedrock AgentCore Runtime where the Strands Agent is deployed. 
   Resource access control ‚Äì Strands Agents operate within defined permissions to access only authorized resources. 
    
 
Deployment process 
This solution uses the AWS Cloud Development Kit (AWS CDK) and AWS CloudFormation to handle the deployment through several automated phases. We will use a log analysis agent as an example to demonstrate the deployment process. The agent makes it possible for the admin to perform LibreChat log analysis through natural language queries. 
LibreChat is deployed as a containerized service with ECS Fargate clusters and is integrated with supporting services, including virtual private cloud (VPC) networking, Application Load Balancer (ALB), and the complete data layer with Aurora PostgreSQL-Compatible, DocumentDB, Amazon EFS, and Amazon S3 storage. Security is built in with appropriate IAM roles, security groups, and secrets management. 
The user activity analysis agent provides valuable insights into how students interact with AI tools, identifying peak usage times, popular topics, and potential areas where students might need additional support. The agent is automatically provisioned using the following CloudFormation template, which deploys Strands Agents using Amazon Bedrock AgentCore Runtime, provisions a Lambda function that invokes the agent, API Gateway to make the agent a URL endpoint, and a second Lambda function that accesses LibreChat logs stored in DocumentDB. The second Lambda is used as a tool of the agent. 
The following code shows how to configure LibreChat to make the agent a custom endpoint: 
 
 custom:
    - name: 'log-analysis-assitant'
      apiKey: '{AWS_API_GATEWAY_KEY}'
      baseURL: '{AWS_API_GATEWAY_URL}'
      models:
        default: ['Strands Agent']
        fetch: false
      headers:
        x-api-key: '{AWS_API_GATEWAY_KEY}'
      titleConvo: true
      titleModel: 'us.amazon.nova-lite-v1:0'
      modelDisplayLabel: 'log-analysis-assitant'
      forcePrompt: false
      stream: false
      iconURL: 'https://d1.awsstatic.com/onedam/marketing-channels/website/aws/en_US/
             product-categories/ai-ml/machine-learning/approved/images/256f3da1-3193-
             441c-b2641f33fdd6.a045b9b4c4f34545e1c79a405140ac0146699835.jpeg' 
 
 After the stack is deployed successfully, you can log in to LibreChat, select the agent, and start chatting. The following screenshot shows an example question that the user activity analysis agent can help answer, where it reads the LibreChat user activities from DocumentDB and generates an answer.
 
 
Deployment considerations and best practices 
When deploying this LibreChat and Strands Agents integration, organizations should carefully consider several key factors that can significantly impact both the success of the implementation and its long-term sustainability. 
Security and compliance form the foundation of any successful deployment, particularly in educational environments where data protection is paramount. Organizations must implement robust data classification schemes to maintain appropriate handling of sensitive information, and role-based access controls make sure users only access AI capabilities and data appropriate to their roles. Beyond traditional perimeter security, a layered authorization approach becomes critical when deploying AI systems that might access multiple data sources with varying sensitivity levels. This involves implementing multiple authorization checks throughout the application stack, including service-to-service authorization, trusted identity propagation that carries the end-user‚Äôs identity through the system components, and granular access controls that evaluate permissions at each data access point rather than relying solely on broad service-level permissions. Such layered security architectures help mitigate risks like prompt injection vulnerabilities and unauthorized cross-tenant data access, making sure that even if one security layer is compromised, additional controls remain in place to protect sensitive educational data. Regular compliance monitoring becomes essential, with automated audits and checks maintaining continued adherence to relevant data protection regulations throughout the system‚Äôs lifecycle, while also validating that layered authorization policies remain effective as the system evolves. 
Cost management requires a strategic approach that balances functionality with financial sustainability. Organizations must prioritize their generative AI spending based on business impact and criticality while maintaining cost transparency across customer and user segments. Implementing comprehensive usage monitoring helps organizations track AI service consumption patterns and identify optimization opportunities before costs become problematic. The human element of deployment often proves more challenging than the technical implementation. Faculty training programs should provide comprehensive guidance on integrating AI tools into teaching practices, focusing not just on how to use the tools but how to use them effectively for educational outcomes. Student onboarding requires clear guidelines and tutorials that promote both effective AI interaction and academic integrity. Perhaps most importantly, establishing continuous feedback loops makes sure the system evolves based on actual user experiences and measured educational outcomes rather than assumptions about what users need.Successful deployments also require careful attention to the dynamic nature of AI technology. The architecture‚Äôs support for dynamic agent loading enables organizations to add specialized agents for new departments or use cases without disrupting existing services. Version control systems should maintain different agent versions for testing and gradual rollout of improvements, and performance monitoring tracks both technical metrics and user satisfaction to guide continuous improvement efforts. 
Conclusion 
The integration of LibreChat with Strands Agents represents a significant step forward in democratizing access to advanced AI capabilities in higher education. By combining the accessibility and customization of open source systems with the sophistication and reliability of enterprise-grade AI services, institutions can provide students and faculty with powerful tools that enhance learning, research, and academic success.This architecture demonstrates that educational institutions don‚Äôt need to choose between powerful AI capabilities and institutional control. Instead, they can take advantage of the innovation and flexibility of open source solutions with the scalability and reliability of cloud-based AI services. The integration example showcased in this post illustrates the solution‚Äôs versatility and potential for customization as institutions expand and adapt the solution to meet evolving educational needs. 
For future work, the LibreChat system‚Äôs Model Context Protocol (MCP) server integration capabilities offer exciting possibilities for enhanced agent architectures. A particularly promising avenue involves wrapping agents as MCP servers, transforming them into standardized tools that can be seamlessly integrated alongside other MCP-enabled agents. This approach would enable educators to compose sophisticated multi-agent workflows, creating highly personalized educational experiences tailored to individual learning styles. 
The future of education is about having the right AI tools, properly integrated and ethically deployed, to enhance human learning and achievement through flexible, interoperable, and extensible solutions that can evolve with educational needs. 
Acknowledgement 
The authors extend their gratitude to Arun Thangavel, Ashish Rawat and Kosti Vasilakakis for their insightful feedback and review of the post. 
 
About the authors 
Dr. Changsha Ma is a Senior AI/ML Specialist at AWS. She is a technologist with a PhD in Computer Science, a master‚Äôs degree in Education Psychology, and years of experience in data science and independent consulting in AI/ML. She is passionate about researching methodological approaches for machine and human intelligence. Outside of work, she loves hiking, cooking, and spending time with friends and families. 
Sudheer Manubolu&nbsp;is a Solutions Architect at Amazon Web Services (AWS). He specializes in cloud architecture, enterprise solutions, and AI/ML implementations. He provides technical and architectural guidance to customers building transformative solutions on AWS, with particular emphasis on leveraging AWS‚Äôs AI/ML and container services to drive innovation and operational excellence. 
Abhilash Thallapally is a Solutions Architect at AWS helping public sector customers design and build scalable AI/ML solutions using Amazon SageMaker. His work covers a wide range of ML use cases, with a primary interest in computer vision, Generative AI, IoT, and deploying cost optimized solutions on AWS. 
Mary Strain leads strategy for artificial intelligence and machine learning for US education at AWS. Mary began her career as a middle school teacher in the Bronx, NY. Since that time, she has held leadership roles in education and public sector technology organizations. Mary has advised K12, higher education, and state and local government on innovative policies and practices in competency based assessment, curriculum design, micro credentials and workforce development initiatives. As an advisor to The Education Design Studio at The University of Pennsylvania, The Coalition of Schools Educating Boys of Color, and The State of NJ AI Task Force, Mary has been on the leading edge of bringing innovative solutions to education for two decades.
‚Ä¢ Skai uses Amazon Bedrock Agents to significantly improve customer insights by revolutionized data access and analysis
  This post was written with Lior Heber and Yarden Ron of Skai. 
Skai (formerly Kenshoo) is an AI-driven omnichannel advertising and analytics platform designed for brands and agencies to plan, launch, optimize, and measure paid media across search, social, retail media marketplaces and other ‚Äúwalled-garden‚Äù channels from a single interface. By unifying data from over 100 publishers and retail networks, Skai applies real-time analytics, predictive modeling, and incremental testing to surface budget and bidding recommendations, connect media spend to sales outcomes, and reduce channel silos, giving marketers full-funnel visibility and higher return on ad spend at scale. 
Skai recognized that our customers were spending days (sometimes weeks) manually preparing reports, struggling to query complex datasets, and lacking intuitive visualization tools. Traditional analytics platforms required technical expertise, leaving many users overwhelmed by untapped data potential. But through the partnership with AWS and adoption of Amazon Bedrock Agents AI assistants that can autonomously perform complex, multi-step tasks by orchestrating calls to APIs, we‚Äôve redefined what‚Äôs possible. Now, customers can analyze their data in natural language, generate reports in minutes instead of days, and visualize insights through natural language conversation. 
In this post, we share how Skai used Amazon Bedrock Agents to improve data access and analysis and improve customer insights. 
Challenges with data analytics 
Before adopting Amazon Bedrock Agents, Skai‚Äôs customers accessed their data through tables, charts, and predefined business questions. Campaign manager teams, looking to do deep research on their data, would spend around 1.5 days a week preparing static reports, while individual users struggled to connect the dots between their massive amount of data points. Critical business questions, like where should a client spend their time optimizing campaigns, and how, remained hidden in unstructured knowledge and siloed data points. 
We identified three systematic challenges: 
 
 Time-consuming report generation ‚Äì Grids display flat and grouped data at specific entity levels, like campaigns, ads, products, and keywords. However, gaining a comprehensive understanding by connecting these different entities and determining relevant time frames is time-consuming. Users must manipulate raw data to construct a complete narrative. 
 Summarization ‚Äì Analyzing extracted raw data posed significant challenges in understanding, identifying key patterns, summarizing complex datasets, and drawing insightful conclusions. Users lacked intuitive tools to dynamically explore data dimensions, hindering their ability to gain a holistic view and extract crucial insights for informed decisions. 
 Recommendations ‚Äì Presenting data-driven recommendations to stakeholders with varying understanding requires deep data analysis, anticipating perspectives, and clear, persuasive communication to demonstrate ROI and facilitate informed decisions. 
 
How Celeste powered transformation 
To address the challenges of time-consuming report generation, the difficulty in summarizing complex data, and the need for data-driven recommendations, Skai used AWS to build Celeste, a generative AI agent. With AI agents, users can ask questions in natural language, and the agent automatically collects data from multiple sources, synthesizes it into a cohesive narrative with actionable insights, and provides data-oriented recommendations. 
The Skai Platform absorbs an enormous amount of data about product searches across many retailers and traditional search engines. Sorting through this data can be time-consuming, but the capabilities in Celeste can make this type of exploratory research much easier. 
Skai‚Äôs solution leverages Amazon Bedrock Agents to create an AI-driven analytics assistant that transforms how users interact with complex advertising data. The system processes natural language queries like ‚ÄòCompare ad group performance across low-performing campaigns in Q1,‚Äô eliminating the need for a database specialist. Agent automatically joins Skai‚Äôs datasets from profiles, campaigns, ads, products, keywords, and search terms across multiple advertising publishers. Beyond simple data retrieval, the assistant generates comprehensive insights and case studies while providing actionable recommendations on campaign activity, complete with detailed analytical approaches and ready-to-present stakeholder materials. 
For example, consider the following question: ‚ÄúI‚Äôm launching a new home security product and want to activate 3 new Sponsored Product campaigns and 2 new Sponsored Brand campaigns on Amazon. What high-performing keywords and their match types are already running in other campaigns that would be good to include in these new activations?‚Äù 
When asked this question with real client data, Celeste answered quickly, finding a combination of branded and generic category terms that the manufacturer might consider for this new product launch. With just a few follow-up questions, Celeste was able to provide estimated CPCs, budgets, and a high-level testing plan for these hypothetical campaigns, complete with negative keywords to reduce unnecessary conflict with their existing campaigns. 
This is a great example of an exploratory question that requires summary analysis, identification of trends and insights, and recommendations. Skai data directly supports these kinds of analyses, and the capabilities within Celeste give the agent the intelligence to provide smart recommendations. Amazon Bedrock makes this possible because it gives Celeste access to strong foundation models (FMs) without exposing clients to the risk of having those models‚Äô vendors use sensitive questions for purposes outside of supporting the client directly. Celeste reduces 75% on average the time needed to build client case studies, transforming a process that often took weeks into one requiring only minutes. 
Accelerating time-to-value through managed AI using Amazon Bedrock 
One critical element of Skai‚Äôs success story was our deliberate choice of Amazon Bedrock as the foundational AI service. Unlike alternatives requiring extensive infrastructure setup and model management, Amazon Bedrock provided a frictionless path from concept to production. 
The journey began with a simple question: How can we use generative AI to provide our clients a new and improved experience without building AI infrastructure from scratch? With Amazon Bedrock, Skai could experiment within hours and deliver a working proof of concept in days. The team could test multiple FMs (Anthropic‚Äôs Claude, Meta‚Äôs Llama, and Amazon Nova) without managing separate environments and iterate rapidly through Amazon Bedrock Agents. 
One developer noted, ‚ÄúWe went from whiteboard to a working prototype in a single sprint. With traditional approaches, we‚Äôd still be configuring infrastructure.‚Äù 
With Amazon Bedrock Agents, Skai could prioritize customer value and rapid iteration over infrastructure complexity. The managed service minimized DevOps overhead for model deployment and scaling while alleviating the need for specialized ML expertise in FM tuning. This helped the team concentrate on data integration and customer-specific analytics patterns, using cost-effective on-demand models at scale while making sure client data remained private and secure.With Amazon Bedrock Agents, domain experts can focus exclusively on what matters most: translating customer data challenges into actionable insights. 
Benefits of Amazon Bedrock Agents 
The introduction of Amazon Bedrock Agents dramatically simplified Skai‚Äôs architecture while reducing the need to build custom code. Built-in action groups replaced thousands of lines of custom integration code that would have required weeks of development time. The platform‚Äôs native memory and session management capabilities meant the team could focus on business logic rather than infrastructure concerns. Declarative API definitions reduced integration time from weeks to hours. Additionally, the integrated code interpreter simplified math problem management and facilitated accuracy and scale issues. 
As a solution provider serving many customers, security and compliance were non-negotiable. Amazon Bedrock addressed these security requirements by inheriting AWS‚Äôs comprehensive compliance certifications including HIPAA, SOC2, and ISO27001. Commitment to not retaining data for model training proved critical for protecting sensitive customer information, while its seamless integration with existing AWS Identity and Access Management (IAM) policies and VPC configurations simplified deployment. 
During every client demonstration of Celeste, initial inquiries consistently centered on privacy, security, and the protection of proprietary data. With an AWS infrastructure, Skai confidently assured clients that their data would not be used to train any models, effectively distinguishing Skai from its competitors.With pay-as-you-go model, Skai scaled economically without AI infrastructure investment. The team avoided costly upfront commitments to GPU clusters or specialized instances, instead leveraging automatic scaling based on actual usage patterns. This approach provided granular cost attribution to specific agents, allowing Skai to understand and optimize spending at a detailed level. The flexibility to select the most appropriate model for each specific task further optimized both performance and costs, ensuring resources aligned precisely with business needs. 
AWS Enterprise Support as a strategic partner in AI innovation 
Working with cutting-edge generative AI agents presents unique challenges that extend far beyond traditional technical support needs. When building Celeste, Skai encountered complex scenarios where solutions didn‚Äôt emerge as expected, from managing 200,000-token conversations to optimizing latency in multi-step agent workflows. AWS Enterprise Support proved invaluable as a strategic partner rather than just a support service. 
AWS Enterprise Support provided dedicated Technical Account Management (TAM) and Solutions Architect (SA) services that went well beyond reactive problem-solving. Our TAM and SA became an extension of our engineering team, offering the following: 
 
 Regular architectural reviews to optimize our Amazon Bedrock Agents implementation 
 Proactive monitoring recommendations that helped us identify potential bottlenecks before they impacted customer experience 
 Direct access to AWS service teams when we needed deep technical expertise on the advanced features of Amazon Bedrock Agents 
 Strategic guidance and optimization as we scaled from prototype to production 
 
When complex issues arose, such as our initial 90-second (or more) latency challenges or session management complexities, Enterprise Support provided immediate escalation paths and expert consultation. 
This comprehensive support framework was instrumental in achieving our aggressive KPIs and time-to-market goals. The combination of proactive guidance, rapid issue resolution, and strategic partnership helped us achieve the following: 
 
 Reduce proof of concept to production timeline by 50% 
 Maintain 99.9% uptime during critical customer demonstrations 
 Scale confidently, knowing we had enterprise-grade support backing our innovation 
 
The value of Enterprise Support provided the confidence and partnership necessary to build our product roadmap on emerging AI technologies, knowing AWS was fully committed to the success of Celeste. 
Solution overview 
The following diagram illustrates the solution architecture. 
 
Our Amazon Bedrock Agent operates on several core components. 
First, a custom layer comprises the following: 
 
 Customer Experience UI (CX UI) ‚Äì The frontend interface that users interact with to submit questions and view responses 
 Chat Manager ‚Äì Orchestrates the conversation flow, manages session state, and handles the communication between the UI and the processing layer 
 Chat Executor ‚Äì Receives processed requests from Chat Manager, interfaces with Amazon Bedrock Agent and handles the business logic for determining when and how to invoke the agent, and manages the overall conversation workflow and short memory 
 
Second, we used the following in conjunction with Amazon Bedrock: 
 
 Amazon Bedrock agent ‚Äì An orchestrator that receives queries from Chat Executor, determines which tools to invoke based on the query, and manages the tool invocation process. 
 Anthropic‚Äôs Claude 3.5 Sonnet V2 ‚Äì The FM that generates natural language responses. The model generates queries for the API and processes the structured data returned by tools. It creates coherent, contextual answers for users. 
 
Finally, the data layer consists of the following: 
 
 Tool API ‚Äì A custom API that receives tool invocation requests from the Amazon Bedrock agent and queries the customer data 
 Customer data ‚Äì The data storage containing sensitive customer information that remains isolated from Amazon Bedrock 
 
The solution also includes the following key security measures: 
 
 Data isolation is enforced between the Tool API and Amazon Bedrock agent 
 Raw customer data is never shared 
 Skai can maintain data privacy and compliance requirements 
 
Overcoming critical challenges 
Implementing the solution brought with it a few key challenges. 
Firstly, early prototypes suffered from 90-second (or more) response times when chaining multiple agents and APIs. By adopting a custom orchestrator and streaming, we reduced median latency by 30%, as illustrated in the following table. 
 
  
   
   Approach 
   Average Latency (seconds) 
   P90 
   P99 
   
   
   Baseline 
   136 
   194 
   215 
   
   
   Optimized Workflow 
   44 
   102 
   102 
   
  
 
Secondly, customers frequently analyzed multi-year datasets, exceeding Anthropic Claude‚Äôs 50,000-token window. Our solution uses dynamic session chunking to split conversations while retaining key context, and employs Retrieval Augmented Generation (RAG)-based memory retrieval. 
Lastly, we implemented the following measures for error handling at scale: 
 
 Real-time tracing using WatchDog with Amazon CloudWatch Logs Insights to monitor more than 230 agent metrics 
 A retry mechanism, in which failed API calls with 500 error: ‚ÄúBEDROCK_MODEL_INVOCATION_SERVICE_UNAVAILABLE‚Äù are automatically retried 
 Amazon CloudWatch monitoring and alerting 
 
Business results 
Since deploying with AWS, Skai‚Äôs platform has achieved significant results, as shown in the following table. 
 
  
   
   Metric 
   Improvement 
   
   
   Report Generation Time 
   50% Faster 
   
   
   Case Study Generation Time 
   75% Faster 
   
   
   QBR Composition Time 
   80% Faster 
   
   
   Report to Recommendation Time 
   90% Faster 
   
  
 
While the metrics above demonstrate measurable improvements, the true business impact becomes clear through customer feedback. The core challenges Skai addressed‚Äîtime-consuming report generation, complex data analysis, and the need for actionable recommendations, have been resolved in ways that fundamentally changed how users work with advertising data. 
Customer testimonials 
‚ÄúIt‚Äôs made my life easier. It‚Äôs made my team‚Äôs life easier. It‚Äôs made my clients‚Äô lives easier and better. So we all work in jobs where there‚Äôs millions and millions of data points to scour through every day, and being able to do that as efficiently as possible and as fluidly as possible with Celeste AI is always a welcome addition to Skai.‚Äù ‚Äì Aram Howard, Amazon Advertising Executive, Data Analyst | Channel Bakers 
‚ÄúCeleste is saving hours of time. It‚Äôs like having another set of eyes to give suggestions. I‚Äôm so stoked to see where this could take us.‚Äù ‚Äì Erick Rudloph, Director of Search Marketing, Xcite Media Group 
‚ÄúIt truly feels like having a data scientist right next to me to answer questions, even with recommendations for starting an optimization or looking at an account‚Äôs performance.‚Äù ‚Äì Director of Search Marketing at Media Agency 
Looking ahead: The future of Celeste 
We‚Äôre expanding Celeste‚Äôs capabilities in the following areas: 
 
 Personalizing the user experience, retaining memories and preferences across multiple sessions. 
 Ingestion of custom data assets, so the client can bring their own data into Celeste and seamlessly connect it to Celeste‚Äôs existing data and knowledge. 
 New tools for seamless team integration. These tools will allow Celeste to generate client presentations, build data dashboards, and provide timely notifications. 
 
Conclusion 
With Amazon Bedrock Agents, Skai transformed raw data into strategic assets, helping customers make faster, smarter decisions without technical bottlenecks. By combining a robust AWS AI/ML infrastructure with our domain expertise, we‚Äôve created a blueprint other organizations can follow to democratize data analytics. 
What truly set our journey apart was the ease with which Amazon Bedrock helped us transition from concept to production. Rather than building complex AI infrastructure, we used a fully managed service that let us focus on our core strengths: understanding customer data challenges and delivering insights at scale. The decision to use Amazon Bedrock resulted in considerable business acceleration, helping us deliver value in weeks rather than quarters while maintaining production grade security, performance, and reliability. 
Skai‚Äôs journey with Amazon Bedrock continues‚Äîfollow our series for updates on multi-agent systems and other generative innovations. 
 
About the authors 
Lior Heber is the Al Lead Architect at Skai, where he has spent over a decade shaping the company‚Äôs technology with a focus on innovation, developer experience, and intelligent Ul design. With a strong background in software architecture and Al-driven solutions, Lior has led transformative projects that push the boundaries of how teams build and deliver products. Beyond his work in tech, he co-founded Colorful Family, a project creating children‚Äôs books for diverse families. Lior combines technical expertise with creativity, always looking for ways to bridge technology and human experience. 
Yarden Ron is a Software Development Team Lead at Skai, bringing over four years of leadership and engineering experience to the AI-powered commerce media platform. He recently spearheaded the launch of Celeste AI ‚Äì a GenAI agent designed to revolutionize how marketers engage with their platforms by making insights faster, smarter, and more intuitive. Based in Israel, Yarden blends technical acumen with collaborative drive, leading teams that turn innovative ideas into impactful products. 
Tomer Berkovich is a Technical Account Manager at AWS with a specialty focus on Generative AI and Machine Learning. He brings over two decades of technology, engineering, and architecture experience to help organizations navigate their AI/ML journey on AWS. When he isn‚Äôt working, he enjoys spending time with his family, exploring emerging technologies, and powerlifting while chasing new personal records. 
Dov Amir is a Senior Solutions Architect at AWS, bringing over 20 years of experience in Software, cloud and architecture. In his current role, Dov helps customers accelerate cloud adoption and application modernization by leveraging cloud-native technologies and generative AI. 
Gili Nachum is a Principal AI/ML Specialist Solutions Architect who works as part of the EMEA Amazon Machine Learning team. Gili is passionate about the challenges of training deep learning models, and how machine learning is changing the world as we know it. In his spare time, Gili enjoys playing table tennis.
‚Ä¢ The power of AI in driving personalized product discovery at Snoonu
  This post was written with Felipe Monroy, Ana Jaime, and Nikita Gordeev from Snoonu. 
Managing a massive product catalog in the ecommerce space has introduced new hurdles for retailers who are trying to efficiently connect customers with the items they truly want. Traditional one-size-fits-all approaches often result in lost opportunities and reduced customer engagement. For marketplace apps like Snoonu, personalization is crucial for driving customer engagement, improving conversion rates, and maximizing revenue per user while building lasting customer loyalty. 
From the customer perspective, Snoonu‚Äôs users expect seamless product discovery experiences that save time and surface items that match their preferences. They want recommendations that are contextually appropriate and adapt to their changing needs. This makes personalization more than just a business imperative, but a customer satisfaction requirement. 
In this post, we share how Snoonu, a leading ecommerce platform in the Middle East, transformed their product discovery experience using AI-powered personalization. 
Challenges with ranking and recommendations 
At Snoonu, innovation and excellence are guiding principles, as they strive to deliver the best shopping and delivery experience in Qatar. As they scale, optimizing ranking and recommending products as well as merchants to their customers is essential in enhancing user experience and driving business growth. Initially, Snoonu relied on basic rules like popularity-based ranking, which resulted in a recommendation system that lacked personalization and depth. To overcome this barrier, Snoonu turned to Amazon Personalize in 2024. They began with real-time recommendations for the entire platform by having a single, unified global model and progressively advanced toward recommendation models per vertical. This evolution showcases the agility and iterative approach of a startup mindset focused on continuously delivering incremental value. As a result, Snoonu achieved measurable impact through smarter, data-driven experiences that drastically boosted customer engagement and conversion rates. 
The evolution of personalization at Snoonu 
Snoonu began its personalization journey using static rules, primarily relying on popularity-based ranking. Although this method was straightforward to implement, it showed the same top products to all users, ignoring individual preferences, which lead to low engagement with recommended items. Perhaps most importantly, it failed to facilitate the discovery of long-tail products that could have been perfect matches for specific customers. 
To address these limitations, Snoonu implemented real-time recommendations by developing a unified global model across all verticals (Marketplace, Food, and Groceries). The system generated daily recommendation lists for users. Although this approach improved relevance compared to static rules, it had its own challenges. Recommendations would become stale quickly, unable to capture nuanced user behavior or react to inventory changes. Furthermore, because recommendations remained unchanged throughout the week, the system‚Äôs inability to dynamically adapt to user actions and inventory updates limited its effectiveness in driving conversions. 
The real breakthrough came with the decision to implement separate models for each vertical rather than maintaining a single, unified global model. This strategic shift acknowledged that customer behaviors and decision-making processes vary significantly across verticals‚Äîfor instance, a customer ordering lunch exhibits different patterns compared to someone planning weekly groceries. By developing specialized models for food delivery, grocery shopping, and marketplace purchases, Snoonu could better capture the unique nuances of each vertical‚Äôs customer journey, despite some overlapping behaviors. This vertical-based approach produced more precise recommendations, ultimately enhancing customer experience and satisfaction. Additionally, Snoonu transformed their training strategy from a 3-week cycle to daily updates‚Äîa crucial change that we explore later in this post. 
The system was further enhanced with sophisticated filter expressions to improve recommendation relevance. One key implementation of this feature is used to exclude items that were was already added to a basket to reduce the exploration list. Additionally, the filter expressions support recommendations on the user‚Äôs current browsing category. For example, when a user searches for laptops, the system intelligently suggests related electronics items. 
To optimize performance and scalability, Snoonu implemented Redis caching through Amazon ElastiCache. This addition significantly reduced API latency, improved system scalability, and optimized costs while enhancing the overall user experience. 
Solution overview 
Snoonu‚Äôs architecture uses Amazon Personalize to deliver real-time recommendations by seamlessly capturing user interactions, continuously updating its personalization models, and filtering by available products to its customers. The following diagram illustrates the solution architecture. 
 
The technical solution architecture comprises several key stages using AWS services and external tools: data preparation and collection, model training, and real-time recommendations. In the following sections, we discuss these stages in more detail. 
Data preparation and collection 
Snoonu implemented a dual-pipeline data collection strategy for their recommendation system. Amplitude captures all user interactions across the app, which feed into Snoonu‚Äôs primary pipeline that exports daily data to BigQuery for historical storage and model retraining. The historical storage comprises structured datasets of user-item interactions, items metadata, and users metadata, which are processed daily in Databricks. After validation and feature transformation, the data is exported to Amazon Simple Storage Service (Amazon S3) and used to retrain the model daily through Amazon Personalize. For more details about the different data types used and how they power personalization, see Preparing training data for Amazon Personalize. 
Daily dataset updates, collected by this data preparation and collection pipeline, support fresh item catalogs and updated business rules. 
Model training and update 
Snoonu implemented multiple Amazon Personalize recipes for different use cases. The aws-user-personalization recipe powers homepage ranking and cart and product detail page suggestions. The aws-similar-items recipe handles ‚Äúfrequently bought together‚Äù recommendations, and the aws-personalized-ranking recipe enhances category and subcategory pages ranking. 
Although Amazon Personalize typically recommends weekly training schedules, through extensive experimentation, Snoonu discovered their use case required frequent updates. They transitioned from a 3-week training cycle to weekly, and finally settled on daily training to maintain relevancy across all verticals. 
Each vertical uses different Amazon Personalize recipes optimized for their specific needs. For instance, Snoonu‚Äôs Marketplace implements both user personalization and reranking capabilities to handle the extensive catalog effectively. Their grocery vertical primarily uses similar items that recommends ‚Äúfrequently bought together‚Äù items to enhance basket building, and the food delivery service relies on user personalization to capture ordering patterns. To learn more about choosing the right Amazon Personalize recipe, see Choosing a recipe. 
To complement this and update the real-time recommendation system, they stream Amplitude events that are filtered by business verticals through Amazon Kinesis Data Streams, which then flow into AWS Lambda. The Lambda function performs three critical tasks: it validates the schema to make sure incoming Kinesis Data Streams data is well-formed, processes these events before sending to Amazon Personalize using the PutEvents API to update the event tracker, and triggers recommendation updates by sending HTTP POST requests with userIds to their intermediary service. This enables the system to continuously update and refine recommendations based on immediate user behavior. 
Real-time recommendations 
Given that Snoonu is using a custom recipe, they built a campaign that hosts a solution version to return real-time recommendations. To learn more about this process, see Custom resources for training and deploying Amazon Personalize models. 
To effectively serve these recommendations at scale, Snoonu developed an intermediary service that acts as a smart bridge between the frontend and their campaign solution. This service addresses the challenges of instantly responding to user behavior while efficiently managing caching, API usage, and response times at scale. To achieve this, they use Amazon ElastiCache for caching recommendations, implementing a live trigger cache invalidation strategy instead of traditional Time to Live (TTL) logic. When a user interacts, a Lambda function triggers the recommendation service, potentially clearing the cache and fetching new recommendations only if the action is likely to change the user‚Äôs preferences. 
The service also handles postprocessing of Amazon Personalize results, filtering by factors not natively supported such as geographical availability and real-time stock status. This architecture makes sure the system can instantly respond to user behavior while efficiently managing caching, API usage, and response times at scale, providing personalized recommendations that stay fresh and relevant to users. 
Finally, all recommendation requests are logged through Amazon MQ (using RabbitMQ), so Snoonu can compare pre-and post-filtering recommendations and monitor system performance. 
Business outcomes 
The implementation of Amazon Personalize delivered remarkable business results for Snoonu, demonstrating the powerful impact of personalized recommendations in ecommerce. In the Groceries vertical, the platform witnessed a striking 1,600% increase in add-to-cart events from cart recommendations, indicating substantially improved customer engagement and product discovery. The personalization strategy proved to be highly cost-effective, generating a Gross Merchandise Value (GMV) that was 47 times the total model investment over a 6-month period. This translated to QR 2.6 million (USD $715,000) in accumulated incremental GMV directly attributed to recommendation-driven conversions. Most notably, personalized recommendations contributed 30% to customer basket size in orders with at least one recommended product. These results also showcase how effectively personalization can create a multiplier effect, driving immediate sales and encouraging customers to explore broader product categories and make more informed purchasing decisions. The success in the marketplace paved the way for Snoonu to expand this personalization strategy across other verticals, using customer insights to create more engaging and relevant shopping experiences. 
Key learnings 
The journey to implementing AI-driven personalization yielded valuable insights. Starting small and scaling fast proved crucial, helping Snoonu validate impact before expanding to more complex use cases. Data quality emerged as a critical success factor, requiring significant investment in preparation and maintenance of consistent schemas. 
Snoonu‚Äôs journey with personalization began with a single, unified model across all verticals, which provided a foundation for understanding customer behavior patterns. As the platform matured, the team strategically evolved their approach by developing specialized models for different verticals and implementing specific recommendation recipes tailored to various use cases. 
Conclusion 
Snoonu‚Äôs journey with Amazon Personalize demonstrates the transformative power of AI-driven personalization in ecommerce. Snoonu‚Äôs phased approach to personalization substantially impacted customer behavior. This solution optimized performance within specific verticals and created a multiplier effect, enhancing customer engagement and encouraging cross-category exploration. Amazon Personalize helped Snoonu achieve substantial business value while significantly improving customer satisfaction. This success story opened the door to explore even more innovative AI-powered capabilities, such as using generative AI models on Amazon Bedrock, to drive further enhancements to their personalization and discovery experiences. 
Learn more about how Snoonu used Amazon Bedrock to categorize over 1 million products. 
About Snoonu 
Snoonu is Qatar‚Äôs first All-in-One delivery app. Since their founding in 2019 by Mr. Hamad Al-Hajri, Snoonu has evolved into the country‚Äôs leading tech-driven super-app, offering a wide range of delivery services‚Äîincluding food, groceries, electronics, pharmaceuticals, and more. 
 
About the Authors 
Felipe Monroy is a Senior Data Scientist at Snoonu. He has extensive experience in data science and ML engineering, holding diverse roles at global companies such as McKinsey &amp; Company and Rappi across Latin America and Australia. With a master‚Äôs degree in Data Science and Innovation from the University of Technology Sydney. He specializes in advanced analytics, leveraging expertise in MLflow, AWS, and Databricks. He is particularly passionate about MLOps and LLMs, frequently contributing to discussions and mentoring others in these areas. 
Ana Jaime is the Head of AI &amp; Data Science at Snoonu, with a strong foundation in both technical execution and strategic leadership. Ana held key leadership roles at Rappi before joining Snoonu. She is now dedicated to building and leading a high-performing AI &amp; Data Science team, with projects spanning personalization, ETAs, demand forecasting, content moderation, and item categorization. Through her leadership and the team‚Äôs contributions, Snoonu was recognized as the Best Emerging Tech Company in AI in 2024. Ana is committed to fostering talent with a keen interest in ground-breaking technologies, passion, and dedication. 
Nikita Gordeev is the Chief Technology Officer at Snoonu. He has over 10 years of experience in software development and leadership roles in the banking, telecommunication, and ecommerce sectors. He holds a master‚Äôs degree in Information Security and Advanced Studies at MIT. He is always open and eager to share knowledge and insights with those who are interested. 
Saubia Khan&nbsp;is a Startup Solutions Architect residing in the sunny city of Dubai. Her focus is AI, ML, and generative AI, where she is passionate about making it easier for her customers to build and deploy their AI solutions on AWS. 
Ahmed Azzam  is a Senior Solutions Architect at AWS. He is passionate about helping startups not only architect and develop scalable applications, but also think big on innovative solutions using AWS services.
‚Ä¢ Accelerating HPC and AI research in universities with Amazon SageMaker HyperPod
  This post was written with Mohamed Hossam of Brightskies. 
Research universities engaged in large-scale AI and high-performance computing (HPC) often face significant infrastructure challenges that impede innovation and delay research outcomes. Traditional on-premises HPC clusters come with long GPU procurement cycles, rigid scaling limits, and complex maintenance requirements. These obstacles restrict researchers‚Äô ability to iterate quickly on AI workloads such as natural language processing (NLP), computer vision, and foundation model (FM) training. Amazon SageMaker HyperPod alleviates the undifferentiated heavy lifting involved in building AI models. It helps quickly scale model development tasks such as training, fine-tuning, or inference across a cluster of hundreds or thousands of AI accelerators (NVIDIA GPUs H100, A100, and others) integrated with preconfigured HPC tools and automated scaling. 
In this post, we demonstrate how a research university implemented SageMaker HyperPod to accelerate AI research by using dynamic SLURM partitions, fine-grained GPU resource management, budget-aware compute cost tracking, and multi-login node load balancing‚Äîall integrated seamlessly into the SageMaker HyperPod environment. 
Solution overview 
Amazon SageMaker HyperPod is designed to support large-scale machine learning operations for researchers and ML scientists. The service is fully managed by AWS, removing operational overhead while maintaining enterprise-grade security and performance. 
The following architecture diagram illustrates how to access SageMaker HyperPod to submit jobs. End users can use AWS Site-to-Site VPN, AWS Client VPN, or AWS Direct Connect to securely access the SageMaker HyperPod cluster. These connections terminate on the Network Load Balancer that efficiently distributes SSH traffic to login nodes, which are the primary entry points for job submission and cluster interaction. At the core of the architecture is SageMaker HyperPod compute, a controller node that orchestrates cluster operations, and multiple compute nodes arranged in a grid configuration. This setup supports efficient distributed training workloads with high-speed interconnects between nodes, all contained within a private subnet for enhanced security. 
The storage infrastructure is built around two main components: Amazon FSx for Lustre provides high-performance file system capabilities, and Amazon S3 for dedicated storage for datasets and checkpoints. This dual-storage approach provides both fast data access for training workloads and secure persistence of valuable training artifacts. 
 
The implementation consisted of several stages. In the following steps, we demonstrate how to deploy and configure the solution. 
Prerequisites 
Before deploying Amazon SageMaker HyperPod, make sure the following prerequisites are in place: 
 
 AWS configuration: 
   
   The AWS Command Line Interface (AWS CLI) configured with appropriate permissions 
   Cluster configuration files prepared: cluster-config.json and provisioning-parameters.json 
    
 Network setup: 
   
   A virtual private cloud (VPC) configured for cluster resources. 
   Security groups with Elastic Fabric Adapter (EFA) communication enabled. 
   An Amazon FSx for Lustre file system for shared, high-performance storage 
    
 An AWS Identity and Management (IAM) role with permissions for the following: 
   
   Amazon Elastic Compute Cloud (Amazon EC2) instance and Amazon SageMaker cluster management 
   FSx for Lustre and Amazon Simple Storage Service (Amazon S3) access 
   Amazon CloudWatch Logs and AWS Systems Manager integration 
   EFA network configuration 
    
 
Launch the CloudFormation stack 
We launched an AWS CloudFormation stack to provision the necessary infrastructure components, including a VPC and subnet, FSx for Lustre file system, S3 bucket for lifecycle scripts and training data, and IAM roles with scoped permissions for cluster operation. Refer to the Amazon SageMaker HyperPod workshop for CloudFormation templates and automation scripts. 
Customize SLURM cluster configuration 
To align compute resources with departmental research needs, we created SLURM partitions to reflect the organizational structure, for example NLP, computer vision, and deep learning teams. We used the SLURM partition configuration to define slurm.conf with custom partitions. SLURM accounting was enabled by configuring slurmdbd and linking usage to departmental accounts and supervisors. 
To support fractional GPU sharing and efficient utilization, we enabled Generic Resource (GRES) configuration. With GPU stripping, multiple users can access GPUs on the same node without contention. The GRES setup followed the guidelines from the Amazon SageMaker HyperPod workshop. 
Provision and validate the cluster 
We validated the cluster-config.json and provisioning-parameters.json files using the AWS CLI and a SageMaker HyperPod validation script: 
 
 $curl -O https://raw.githubusercontent.com/aws-samples/awsome-distributed-training/main/1.architectures/5.sagemaker-hyperpod/validate-config.py

$pip3 install boto3

$python3 validate-config.py --cluster-config cluster-config.json --provisioning-parameters provisioning-parameters.json 
 
Then we created the cluster: 
 
 $aws sagemaker create-cluster \
  --cli-input-json file://cluster-config.json \
  --region us-west-2 
 
Implement cost tracking and budget enforcement 
To monitor usage and control costs, each SageMaker HyperPod resource (for example, Amazon EC2, FSx for Lustre, and others) was tagged with a unique ClusterName tag. AWS Budgets and AWS Cost Explorer reports were configured to track monthly spending per cluster. Additionally, alerts were set up to notify researchers if they approached their quota or budget thresholds. 
This integration helped facilitate efficient utilization and predictable research spending. 
Enable load balancing for login nodes 
As the number of concurrent users increased, the university adopted a multi-login node architecture. Two login nodes were deployed in EC2 Auto Scaling groups. A Network Load Balancer was configured with target groups to route SSH and Systems Manager traffic. Lastly, AWS Lambda functions enforced session limits per user using Run-As tags with Session Manager, a capability of Systems Manager. 
For details about the full implementation, see Implementing login node load balancing in SageMaker HyperPod for enhanced multi-user experience. 
Configure federated access and user mapping 
To facilitate secure and seamless access for researchers, the institution integrated AWS IAM Identity Center with their on-premises Active Directory (AD) using AWS Directory Service. This allowed for unified control and administration of user identities and access privileges across SageMaker HyperPod accounts. The implementation consisted of the following key components: 
 
 Federated user integration ‚Äì We mapped AD users to POSIX user names using Session Manager run-as tags, allowing fine-grained control over compute node access 
 Secure session management ‚Äì We configured Systems Manager to make sure users access compute nodes using their own accounts, not the default ssm-user 
 Identity-based tagging ‚Äì Federated user names were automatically mapped to user directories, workloads, and budgets through resource tags 
 
For full step-by-step guidance, refer to the Amazon SageMaker HyperPod workshop. 
This approach streamlined user provisioning and access control while maintaining strong alignment with institutional policies and compliance requirements. 
Post-deployment optimizations 
To help prevent unnecessary consumption of compute resources by idle sessions, the university configured SLURM with Pluggable Authentication Modules (PAM). This setup enforces automatic logout for users after their SLURM jobs are complete or canceled, supporting prompt availability of compute nodes for queued jobs. 
The configuration improved job scheduling throughput by freeing idle nodes immediately and reduced administrative overhead in managing inactive sessions. 
Additionally, QoS policies were configured to control resource consumption, limit job durations, and enforce fair GPU access across users and departments. For example: 
 
 MaxTRESPerUser ‚Äì Makes sure GPU or CPU usage per user stays within defined limits 
 MaxWallDurationPerJob ‚Äì Helps prevent excessively long jobs from monopolizing nodes 
 Priority weights ‚Äì Aligns priority scheduling based on research group or project 
 
These enhancements facilitated an optimized, balanced HPC environment that aligns with the shared infrastructure model of academic research institutions. 
Clean up 
To delete the resources and avoid incurring ongoing charges, complete the following steps: 
 
 Delete the SageMaker HyperPod cluster: 
 
 
 $aws sagemaker delete-cluster --cluster-name &lt;name&gt; 
 
 
 Delete the CloudFormation stack used for the SageMaker HyperPod infrastructure: 
 
 
 $aws cloudformation delete-stack --stack-name &lt;stack-name&gt; --region &lt;region&gt; 
 
This will automatically remove associated resources, such as the VPC and subnets, FSx for Lustre file system, S3 bucket, and IAM roles. If you created these resources outside of CloudFormation, you must delete them manually. 
Conclusion 
SageMaker HyperPod provides research universities with a powerful, fully managed HPC solution tailored for the unique demands of AI workloads. By automating infrastructure provisioning, scaling, and resource optimization, institutions can accelerate innovation while maintaining budget control and operational efficiency. Through customized SLURM configurations, GPU sharing using GRES, federated access, and robust login node balancing, this solution highlights the potential of SageMaker HyperPod to transform research computing, so researchers can focus on science, not infrastructure. 
For more details on making the most of SageMaker HyperPod, check out the SageMaker HyperPod workshop and explore further blog posts about SageMaker HyperPod. 
 
About the authors 
Tasneem Fathima is Senior Solutions Architect at AWS. She supports Higher Education and Research customers in the United Arab Emirates to adopt cloud technologies, improve their time to science, and innovate on AWS. 
Mohamed Hossam is a Senior HPC Cloud Solutions Architect at Brightskies, specializing in high-performance computing (HPC) and AI infrastructure on AWS. He supports universities and research institutions across the Gulf and Middle East in harnessing GPU clusters, accelerating AI adoption, and migrating HPC/AI/ML workloads to the AWS Cloud. In his free time, Mohamed enjoys playing video games.

‚∏ª