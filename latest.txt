‚úÖ Morning News Briefing ‚Äì September 01, 2025 10:46

üìÖ Date: 2025-09-01 10:46
üè∑Ô∏è Tags: #briefing #ai #publichealth #digitalgov

‚∏ª

üßæ Weather
‚Ä¢ Current Conditions:  9.1¬∞C
  Temperature: 9.1&deg;C Pressure / Tendency: 102.6 kPa rising Humidity: 98 % Dewpoint: 8.8&deg:C Wind:  calm km/h Air Quality Health Index: n/a Observed at: Pembroke 6:00 AM EDT Monday 1 September 2025 . Weather:    Wind: calm km
‚Ä¢ Monday: A mix of sun and cloud. High 25.
  Fog patches dissipating this morning . High 25. Humidex 27. UV index 6 or high . A mix of sun and cloud expected to be a high of 25 degrees in the early hours of Monday morning . Forecast issued 5:00 AM EDT Monday 1 September 2025. Forecast: "Fog patches dissipated this morning. High 25" Forecast forecast: "A
‚Ä¢ Monday night: Clear. Low 13.
  Forecast issued 5:00 AM EDT Monday 1 September 2025 . Clear . Clear. Clear. Low 13.70s . Clear skies . Clear seaside weather. Clear skies. Clear seasides. Low seaside seaside forecast . Forecast: Monday 1 Sept. 1 2025. Clear and sunny skies. Forecast for Monday 1 Sep. 1 September 25th, 25th .

üåç International News
No updates.

üçÅ Canadian News
No updates.

üá∫üá∏ U.S. Top Stories
‚Ä¢ Has Trump kept his campaign promises to American workers? Here's what some say.
  Seven months into his second term, we take a look at how he is doing . Trump made many promises to American workers during the campaign trail . Trump has been in office seven months since taking over the White House . He has been criticized for his lack of experience in the U.S. in the past seven months of his presidency . Trump is the first president to be in office since
‚Ä¢ More students head back to class without one crucial thing: their phones
  This back to school season, more districts than ever have cell phone bans in place . Teachers and legislators alike say the restrictions help kids focus in class . Restrictions help kids stay focused in class, lawmakers say . Back to Mail Online home: Share your photos with us at iReport.com/the-Report-of-the-Show . Back To Mail Online: Share a photo
‚Ä¢ How Trump is decimating federal employee unions one step at a time
  President Trump has ended collective bargaining rights for more than one million federal workers . Unions have sued to block the move, but agencies are terminating contracts as litigation continues . Agency contracts are being terminated while litigation continues to be heard from the White House continues to play a role in the end of collective bargaining in federal workers' contracts . The White House has been criticized by unions for ending collective bargaining
‚Ä¢ Earthquake devastates eastern Afghanistan, killing hundreds and destroying villages
  The 6.0 magnitude quake late Sunday hit a series of towns in the province of Kunar, near the city of Jalalabad in neighboring Nangahar province, causing extensive damage . The quake caused extensive damage to a number of towns, including several towns in Kunar province . The earthquake was the largest in the country's history, causing damage to the region's infrastructure . The
‚Ä¢ Guatemala says it suggested that U.S. send its unaccompanied migrant children home
  A U.S. federal judge blocked the deportation of unaccompanied Guatemalan children . Guatemala says in a statement it suggested the government of Guatemala return its children to their home country . Guatemala has suggested that the children be returned to their native country . A judge has blocked the deportations of unaccompanied children from the United States, citing a federal judge's decision to stay in the U.K.

üß† Artificial Intelligence
No updates.

üíª Digital Strategy
‚Ä¢ LegalPwn: Tricking LLMs by burying badness in lawyerly fine print
  Researchers at security firm Pangea have discovered yet another way to trivially trick large language models into ignoring their guardrails . Stick your adversarial instructions somewhere in a legal document to give them an air of unearned legitimacy ‚Äì a trick familiar to lawyers the world over . The trick is similar to the trick used by lawyers to make it appear as though it is a legal statement .
‚Ä¢ ESA's Solar Orbiter will help space boffins predict destructive coronal ejections
  Superfast electrons traced back to the Sun have been traced to the source of electrons expelled by the Sun . This could have implications for forecasting space weather . The European Space Agency's Solar Orbiter probe has pinpointed the source . It has implications for predicting space weather, with implications for space weather forecasts, ESA says . The solar orbiter probe is currently in orbit around the solar system .
‚Ä¢ I was a part-time DBA. After this failover foul-up, they hired a full-time DBA
  RTFM Who, Me? ‚Äì the reader-contributed column in which we share your most magnificent messes, and your means of making it out alive . No two mistakes are the same, but The Register thinks they're all worth celebrating each Monday when we serve up a fresh edition of the column each Monday .‚Ä¶ At last, enough hours in the day to . RTF
‚Ä¢ Traffic to government domains often crosses national borders, or flows through risky bottlenecks
  Internet traffic to government domains often flows across borders, relies on a worryingly small number of network connections, or does not require encryption . Sites at yourcountry.gov may also not bother with HTTPs, according to new research . Your country.gov sites also don't bother to bother with HTTPS or require encryption, new research says . The government domains are often located in the U.
‚Ä¢ China launches new ‚ÄòAI+‚Äô policy to ‚Äòdeepen information technology revolution‚Äô
  China‚Äôs State Council last week announced a new IT policy called ‚ÄúAI +‚Äù The new policy is the successor to 2015's ‚ÄúInternet +‚Äô‚Äù and will be called AI + . Plus: Spain cancels Huawei deal; Sony wants to use only recycled gold . Video of Alibaba's uncanny FOSS digital humans; and more Asia In Brief . Plus

üè• Public Health
No updates.

üî¨ Science
‚Ä¢ Do social-media bans benefit young people? These data could offer clues
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Polydoctoring and health outcomes among the very old population with multimorbidity: a retrospective cohort study in Japan
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Knowledge attitudes and practices regarding the use of prebiotics and probiotics for respiratory infections among Saudi healthcare students
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Cultivating an equity-oriented data sharing culture for African health research initiatives
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Aluminum in childhood vaccines not linked to chronic diseases
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations

üßæ Government & Policy
No updates.

üèõÔ∏è Enterprise Architecture & IT Governance
No updates.

ü§ñ AI & Emerging Tech
‚Ä¢ The Download: humans in space, and India‚Äôs thorium ambitions
  This is today&#8217;s edition of¬†The Download,¬†our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



The case against humans in space



Elon Musk and Jeff Bezos are bitter rivals in the commercial space race, but they agree on one thing: Settling space is an existential imperative. Space is the place. The final frontier. It is our human destiny to transcend our home world and expand our civilization to extraterrestrial vistas.This belief has been mainstream for decades, but its rise has been positively meteoric in this new gilded age of astropreneurs.But as visions of giant orbital stations and Martian cities dance in our heads, a case against human space colonization has found its footing in a number of recent books, from doubts about the practical feasibility of off-Earth communities, to realism about the harsh environment of space and the enormous tax it would exact on the human body. Read the full story.‚ÄîBecky Ferreira



This story is from our new print edition, which is all about the future of security. Subscribe here to catch future copies when they land.







This American nuclear company could help India‚Äôs thorium dream



For just the second time in nearly two decades, the United States has granted an export license to an American company planning to sell nuclear technology to India, MIT Technology Review has learned.&nbsp;



The decision to greenlight Clean Core Thorium Energy‚Äôs license is a major step toward closer cooperation between the two countries on atomic energy and marks a milestone in the development of thorium as an alternative to uranium for fueling nuclear reactors. Read more about why it‚Äôs such a big deal.



‚ÄîAlexander C. Kaufman







RFK Jr‚Äôs plan to improve America‚Äôs diet is missing the point



A lot of Americans don‚Äôt eat well. And they‚Äôre paying for it with their health. A diet high in sugar, sodium, and saturated fat can increase the risk of problems like diabetes, heart disease, and kidney disease, to name a few. And those are among the leading causes of death in the US.



This is hardly news. But this week Robert F Kennedy Jr., who heads the US Department of Health and Human Services, floated a new solution to the problem: teaching medical students more about the role of nutrition in health could help turn things around.



It certainly sounds like a good idea. If more Americans ate a healthier diet, we could expect to see a decrease in those diseases.&nbsp;



But this framing of America‚Äôs health crisis is overly simplistic, especially given that plenty of the administration‚Äôs other actions have directly undermined health in multiple ways‚Äîincluding by canceling a vital nutrition education program. And at any rate, there are other, more effective ways to tackle the chronic-disease crisis. Read the full story.



‚ÄîJessica Hamzelou



This article first appeared in The Checkup, MIT Technology Review‚Äôs weekly biotech newsletter. To receive it in your inbox every Thursday, and read articles like this first, sign up here.







The must-reads



I‚Äôve combed the internet to find you today‚Äôs most fun/important/scary/fascinating stories about technology.



1 RFK Jr‚Äôs deputy has been chosen to be the new acting head of the CDCJim O‚ÄôNeill is likely to greenlight his boss‚Äôs federal vaccine policy plans. (WP $)+ The future of the department looks decidedly precarious. (The Atlantic $)+ Everything you need to know about Jim O‚ÄôNeill, the longevity enthusiast who is now RFK Jr.‚Äôs right-hand man. (MIT Technology Review)



2 A man killed his mother and himself after conversing with ChatGPTThe chatbot encouraged Stein-Erik Soelberg‚Äôs paranoia while repeatedly assuring him he was sane. (WSJ $)+ An AI chatbot told a user how to kill himself‚Äîbut the company doesn‚Äôt want to ‚Äúcensor‚Äù it. (MIT Technology Review)



3 China is cracking down on excess competition in its AI sectorThe country is hellbent on avoiding wasteful investment. (Bloomberg $)+ China is laser-focused on engineering, not so much on litigating. (Wired $)+ China built hundreds of AI data centers to catch the AI boom. Now many stand unused. (MIT Technology Review)



4 The EU should be prepared to walk away from a US trade dealIts competition commissioner worries Trump may act on his threats to target the bloc. (FT $)+ The French President had a similar warning for his ministers. (Politico)



5 xAI has released a new Grok agentic coding modelAt a significantly lower price than its rivals. (Reuters)+ This no-code website builder has been valued at $2 billion. (TechCrunch)+ The second wave of AI coding is here. (MIT Technology Review)



6 A US mail change has thrown online businesses into turmoilAll package deliveries are due to face duties from this week. (Insider $)



7 A former DOGE official is running America‚Äôs biggest MDMA companyAnd Antonio Gracias is not the only member of the department with ties to the psychedelics industry. (The Guardian)+ Other DOGE workers are joining Trump‚Äôs new National Design Studio. (Wired $)+ The FDA said no to the use of MDMA as a therapy last year. (MIT Technology Review)



8 How chatbots fake having personalitiesThey have no persistent self‚Äîdespite what they may tell you. (Ars Technica)+ What is AI? (MIT Technology Review)



9 The future of podcasting is murkyHundreds of shows have folded. The medium is in desperate need of an archive. (NY Mag $)+ The race to save our online lives from a digital dark age. (MIT Technology Review)10 Do we even know what we want to watch anymore?We‚Äôre so reliant on algorithms, it‚Äôs hard to know. (New Yorker $)







Quote of the day



‚ÄúWe‚Äôre scared for ourselves and for the country.‚Äù&nbsp;



‚ÄîAn anonymous CDC worker tells the New York Times about the mood inside the agency following the firing of their new director Susan Monarez.







One more thing







How a tiny Pacific Island became the global capital of cybercrimeTokelau, a string of three isolated atolls strung out across the Pacific, is so remote that it was the last place on Earth to be connected to the telephone‚Äîonly in 1997. Just three years later, the islands received a fax with an unlikely business proposal that would change everything.



It was from an early internet entrepreneur from Amsterdam, named Joost Zuurbier. He wanted to manage Tokelau‚Äôs country-code top-level domain, or ccTLD‚Äîthe short string of characters that is tacked onto the end of a URL‚Äîin exchange for money.



In the succeeding years, tiny Tokelau became an unlikely internet giant‚Äîbut not in the way it may have hoped. Until recently, its .tk domain had more users than any other country‚Äôs: a staggering 25 million‚Äîbut the vast majority were spammers, phishers, and cybercriminals.



Now the territory is desperately trying to clean up .tk. Its international standing, and even its sovereignty, may depend on it. Read the full story.¬†‚ÄîJacob Judah







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)+ Scientists are using yeast to help save the bees.+ How to become super productive + Why North American mammoths were genetic freaks of nature.+ I love Seal‚Äôs steadfast refusal to explain his lyrics to Kiss from a Rose.
‚Ä¢ This American nuclear company could help India‚Äôs thorium dream
  For just the second time in nearly two decades, the United States has granted an export license to an American company planning to sell nuclear technology to India, MIT Technology Review has learned. The decision to greenlight Clean Core Thorium Energy‚Äôs license is a major step toward closer cooperation between the two countries on atomic energy and marks a milestone in the development of thorium as an alternative to uranium for fueling nuclear reactors.&nbsp;



Starting from the issuance last week, the thorium fuel produced by the Chicago-based company can be shipped to reactors in India, where it could be loaded into the cores of existing reactors. Once Clean Core receives final approval from Indian regulators, it will become one of the first American companies to sell nuclear technology to India, just as the world‚Äôs most populous nation has started relaxing strict rules that have long kept the US private sector from entering its atomic power industry.&nbsp;



‚ÄúThis license marks a turning point, not just for Clean Core but for the US-India civil nuclear partnership,‚Äù says Mehul Shah, the company&#8217;s chief executive and founder. ‚ÄúIt places thorium at the center of the global energy transformation.‚Äù



Thorium has long been seen as a good alternative to uranium because it‚Äôs more abundant, produces both smaller amounts of long-lived radioactive waste and fewer byproducts with centuries-long half-lives, and reduces the risk that materials from the fuel cycle will be diverted into weapons manufacturing.&nbsp;



But at least some uranium fuel is needed to make thorium atoms split, making it an imperfect replacement. It‚Äôs also less well suited for use in the light-water reactors that power the vast majority of commercial nuclear plants worldwide. And in any case, the complex, highly regulated nuclear industry is extremely resistant to change.



For India, which has scant uranium reserves but abundant deposits of thorium, the latter metal has been part of a long-term strategy for reducing dependence on imported fuels. The nation started negotiating a nuclear export treaty with the US in the early 2000s, and a 123 Agreement‚Äîa special, Senate-approved treaty the US requires with another country before sending it any civilian nuclear products‚Äîwas approved in 2008.



A new approach



While most thorium advocates have envisioned new reactors designed to run on this fuel, which would mean rebuilding the nuclear industry from the ground up, Shah and his team took a different approach. Clean Core created a new type of fuel that blends thorium with a more concentrated type of uranium called HALEU (high-assay low-enriched uranium). This blended fuel can be used in India‚Äôs pressurized heavy-water reactors, which make up the bulk of the country‚Äôs existing fleet and many of the new units under development now.&nbsp;



Thorium isn‚Äôt a fissile material itself, meaning its atoms aren‚Äôt inherently unstable enough for an extra neutron to easily split the nuclei and release energy. But the metal has what‚Äôs known as ‚Äúfertile properties,‚Äù meaning it can absorb neutrons and transform into the fissile material uranium-233. Uranium-233 produces fewer long-lived radioactive isotopes than the uranium-235 that makes up the fissionable part of traditional fuel pellets. Most commercial reactors run on low-enriched uranium, which is about 5% U-235. When the fuel is spent, roughly 95% of the energy potential is left in the metal. And what remains is a highly toxic cocktail of long-lived radioactive isotopes such as cesium-137 and plutonium-239, which keep the waste dangerous for tens of thousands of years. Another concern is that the plutonium could be extracted for use in weapons.&nbsp;



Enriched up to 20%, HALEU allows reactors to extract more of the available energy and thus reduce the volume of waste. Clean Core‚Äôs fuel goes further: The HALEU provides the initial spark to ignite fertile thorium and triggers a reaction that can burn much hotter and utilize the vast majority of the material in the core, as a study published last year in the journal Nuclear Engineering and Design showed.





‚ÄúThorium provides attributes needed to achieve higher burnups,‚Äù says Koroush Shirvan, an MIT professor of nuclear science and engineering who helped design Clean Core‚Äôs fuel assemblies. ‚ÄúIt is enabling technology to go to higher burnups, which reduces your spent fuel volume, increases your fuel efficiency, and reduces the amount of uranium that you need.‚Äù&nbsp;



Compared with traditional uranium fuel, Clean Core says, its fuel reduces waste by more than 85% while avoiding the most problematic isotopes produced during fission. ‚ÄúThe result is a safer, more sustainable cycle that reframes nuclear power not as a source of millennia-long liabilities but as a pathway to cleaner energy and a viable future fuel supply,‚Äù says Milan Shah, Clean Core‚Äôs chief operating officer and Mehul‚Äôs son.



Pressurized heavy-water reactors are particularly well suited to thorium because heavy water‚Äîa version of H2O that has an extra neutron on the hydrogen atom‚Äîabsorbs fewer neutrons during the fission process, increasing efficiency by allowing more neutrons to be captured by the thorium. 



There are 46 so-called PHWRs operating worldwide: 17 in Canada, 19 in India, three each in Argentina and South Korea, and two each in China and Romania, according to data from the International Atomic Energy Agency. In 1954, India set out a three-stage development plan for nuclear power that involved eventually phasing thorium into the fuel cycle for its fleet.¬†



Yet in the 56 years since India built its first commercial nuclear plant, its state-controlled industry has remained relatively shut off to the private sector and the rest of the world. When the US signed the 123 Agreement with India in 2008, the moment heralded an era in which the subcontinent could become a testing ground for new American reactor designs.&nbsp;



In 2010, however, India passed the Civil Liability for Nuclear Damage Act. The legislation was based on what lawmakers saw as legal shortcomings in the wake of the 1984 Bhopal chemical factory disaster, when a subsidiary of the American industrial giant Dow Chemical avoided major payouts to the victims of a catastrophe that killed thousands. Under this law, responsibility for an accident at an Indian nuclear plant would fall on suppliers. The statute effectively killed any exports to India, since few companies could shoulder that burden. Only Russia‚Äôs state-owned Rosatom charged ahead with exporting reactors to India.



But things are changing. In a joint statement issued after a February 2025 summit, Prime Minister Narendra Modi and President Donald Trump &#8220;announced their commitment to fully realise the US-India 123 Civil Nuclear Agreement by moving forward with plans to work together to build US-designed nuclear reactors in India through large scale localisation and possible technology transfer.‚Äù¬†



In March 2025, US federal officials gave the nuclear developer Holtec International an export license to sell Indian companies its as-yet-unbuilt small modular reactors, which are based on the light-water reactor design used in the US. In April, the Indian government suggested it would reform the nuclear liability law to relax rules on foreign companies in hopes of drawing more overseas developers. Last month, a top minister confirmed that the Modi administration would overhaul the law.¬†



‚ÄúFor India, the thing they need to do is get another international vendor in the marketplace,‚Äù says Chris Gadomski, the chief nuclear analyst at the consultancy BloombergNEF.



Path of least resistance



But Shah sees larger potential for Clean Core. Unlike Holtec, whose export license was endorsed by the two Mumbai-based industrial giants Larsen &amp; Toubro and Tata Consulting Engineers, Clean Core had its permit approved by two of India‚Äôs atomic regulators and its main state-owned nuclear company. By focusing on fuel rather than new reactors, Clean Core could become a vendor to the majority of the existing plants already operating in India.&nbsp;



Its technology diverges not only from that of other US nuclear companies but also from the approach used in China. Last year, China made waves by bringing its first thorium-fueled reactor online. This enabled it to establish a new foothold in a technology the US had invented and then abandoned, and it gave Beijing another leg up in atomic energy. 



But scaling that technology will require building out a whole new kind of reactor. That comes at a cost. A recent Johns Hopkins University study found that China‚Äôs success in building nuclear reactors stemmed in large part from standardization and repetition of successful designs, virtually all of which have been light-water reactors. Using thorium in existing heavy-water reactors lowers the bar for popularizing the fuel, according to the younger Shah.¬†



‚ÄúWe think ours is the path of least resistance,‚Äù Milan Shah says. ‚ÄúMaybe not being completely revolutionary in the way you look at nuclear today, but incredibly evolutionary to progress humanity forward.‚Äù&nbsp;



The company has plans to go beyond pressurized heavy-water reactors. Within two years, the elder Shah says, Clean Core plans to design a version of its fuel that could work in the light-water reactors that make up the entire US fleet of 94. But it‚Äôs not a simple conversion. For starters, there‚Äôs the size: While the PHWR fuel rods are about 50 centimeters in length, the rods that go into light-water reactors are roughly four meters long. Then there‚Äôs the history of challenges with light water‚Äôs absorption of neutrons that could otherwise be captured to induce fission in the thorium.&nbsp;



For Anil Kakodkar, the former chairman of India‚Äôs Atomic Energy Commission and a mentor to Shah, popularizing thorium could help rectify one of the darker chapters in his country‚Äôs nuclear development. In 1974, India became the first country since the signing of the first global Treaty on the Non-Proliferation of Nuclear Weapons to successfully test an atomic weapon. New Delhi was never a signatory to the pact. But the milestone prompted neighboring Pakistan to develop its own weapons.&nbsp;



In response, President Jimmy Carter tried to demonstrate Washington‚Äôs commitment to reversing the Cold War arms race by sacrificing the first US effort to commercialize nuclear waste recycling, since the technology to separate plutonium and other radioisotopes from uranium in spent fuel was widely seen as a potential new source of weapons-grade material. By running its own reactors on thorium, Kakodkar says, India can chart a new path for newcomer nations that want to harness the power of the atom without stoking fears that nuclear weapons capability will spread.&nbsp;



‚ÄúThe proliferation concerns will be dismissed to a significant extent, allowing more rapid growth of nuclear power in emerging countries,‚Äù he says. ‚ÄúThat will be a good thing for the world at large.‚Äù&nbsp;



Alexander C. Kaufman is a reporter who has covered energy, climate change, pollution, business, and geopolitics for more than a decade.&nbsp;
‚Ä¢ RFK Jr‚Äôs plan to improve America‚Äôs diet is missing the point
  A lot of Americans don‚Äôt eat well. And they‚Äôre paying for it with their health. A diet high in sugar, sodium, and saturated fat can increase the risk of problems like diabetes, heart disease, and kidney disease, to name a few. And those are among¬†the leading causes of death in the US.



This is hardly news. But this week Robert F Kennedy Jr., who heads the US Department of Health and Human Services, floated a new solution to the problem. Kennedy and education secretary Linda McMahon think that teaching medical students more about the role of nutrition in health could help turn things around.





‚ÄúI‚Äôm working with Linda on forcing medical schools ‚Ä¶ to put nutrition into medical school education,‚Äù Kennedy said during¬†a cabinet meeting on August 26. The next day, HHS released¬†a statement calling for ‚Äúincreased nutrition education‚Äù for medical students.



‚ÄúWe can reverse the chronic-disease epidemic simply by changing our diets and lifestyles,‚Äù Kennedy said in¬†an accompanying video statement. ‚ÄúBut to do that, we need nutrition to be a basic part of every doctor‚Äôs training.‚Äù



It certainly sounds like a good idea. If more Americans ate a healthier diet, we could expect to see a decrease in those diseases. But this framing of America‚Äôs health crisis is overly simplistic, especially given that plenty of the administration‚Äôs other actions have directly undermined health in multiple ways‚Äîincluding by canceling a vital nutrition education program.



At any rate, there are other, more effective ways to tackle the chronic-disease crisis.



The biggest killers, heart disease and stroke,¬†are responsible for more than a third of deaths, according to the US Centers for Disease Control and Prevention. A healthy diet can reduce your risk of developing those conditions. And it makes total sense to educate the future doctors of America about nutrition.



Medical bodies are on board with the idea, too. ‚ÄúThe importance of nutrition in medical education is increasingly clear, and we support expanded, evidence-based instruction to better equip physicians to prevent and manage chronic disease and improve patient outcomes,‚Äù David H. Aizuss, chair of the American Medical Association‚Äôs board of trustees, said in a statement.



But it‚Äôs not as though medical students aren‚Äôt getting any nutrition education. And that training has increased in the last five years, according to¬†surveys carried out by the American Association of Medical Colleges.



Kennedy has referred to¬†a 2021 survey suggesting that medical students in the US get only around one hour of nutrition education per year. But the AAMC argues that nutrition education increasingly happens through ‚Äúintegrated experiences‚Äù rather than stand-alone lectures.





‚ÄúMedical schools understand the critical role that nutrition plays in preventing, managing, and treating chronic health conditions, and incorporate significant nutrition education across their required curricula,‚Äù Alison J. Whelan, AAMC‚Äôs chief academic officer, said in a statement.



That‚Äôs not to say there isn‚Äôt room for improvement. Gabby Headrick, a food systems dietician and associate director of food and nutrition policy at George Washington University‚Äôs Institute for Food Safety &amp; Nutrition Security, thinks nutritionists could take a more prominent role in patient care, too.



But it‚Äôs somewhat galling for the administration to choose medical education as its focus given the recent cuts in federal funding that will affect health. For example, funding for the National Diabetes Prevention Program, which offers support and guidance to help thousands of people adopt healthy diets and exercise routines, was canceled by the Trump administration in March.



The focus on medical schools also overlooks one of the biggest factors behind poor nutrition in the US: access to healthy food. A recent¬†survey by the Pew Research Center found that increased costs make it harder for most Americans to eat well. Twenty percent of the people surveyed acknowledged that their diets were not healthy.



‚ÄúSo many people know what a healthy diet is, and they know what should be on their plate every night,‚Äù says Headrick, who has researched this issue. ‚ÄúBut the vast majority of folks just truly do not have the money or the time to get the food on the plate.‚Äù



The Supplemental Nutrition Assistance Program (SNAP) has been helping low-income Americans afford some of those healthier foods. It¬†supported over 41 million people in 2024. But under the Trump administration‚Äôs tax and spending bill,¬†the program is set to lose around $186 billion in funding over the next 10 years.



Kennedy‚Äôs focus is on education. And it just so happens that there is a nutrition education program in place‚Äîone that helps people of all ages learn not only what healthy foods are, but how to source them on a budget and use them to prepare meals.



SNAP-Ed, as it‚Äôs known, has already provided this support to millions of Americans. Under the Trump administration, it is¬†set to be eliminated.



It is difficult to see how these actions are going to help people adopt healthier diets. What might be a better approach? I put the question to Headrick: If she were in charge, what policies would she enact?



‚ÄúUniversal health care,‚Äù she told me. Being able to access health care without risking financial hardship not only¬†improves health outcomes and life expectancy; it also spares people from medical debt‚Äîsomething that¬†affects around 40% of adults in the US, according to a recent survey.



And the Trump administration‚Äôs plans to¬†cut federal health spending by about a trillion dollars over the next decade certainly aren‚Äôt going to help with that. All told, around 16 million people could lose their health insurance by 2034, according to¬†estimates by the Congressional Budget Office.



‚ÄúThe evidence suggests that if we cut folks‚Äô social benefit programs, such as access to health care and food, we are going to see detrimental impacts,‚Äù says Headrick. ‚ÄúAnd it‚Äôs going to cause an increased burden of preventable disease.‚Äù



This article first appeared in The Checkup,¬†MIT Technology Review‚Äôs¬†weekly biotech newsletter. To receive it in your inbox every Thursday, and read articles like this first,¬†sign up here.
‚Ä¢ The Download: Google‚Äôs AI energy use, and the AI Hype Index
  This is today&#8217;s edition of¬†The Download,¬†our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



Google‚Äôs still not giving us the full picture on AI energy use&nbsp;



‚ÄîCasey Crownhart



Google just announced that a typical query to its Gemini app uses about 0.24 watt-hours of electricity. That‚Äôs about the same as running a microwave for one second‚Äîsomething that feels insignificant. I run the microwave for many more seconds than that most days.I welcome more openness from major AI players about their estimated energy use per query. But I‚Äôve noticed that some folks are taking this number and using it to conclude that we don‚Äôt need to worry about AI‚Äôs energy demand. That‚Äôs not the right takeaway here. Let‚Äôs dig into why.



This article is from The Spark, MIT Technology Review‚Äôs weekly climate newsletter. To receive it in your inbox every Wednesday, sign up here.



+ If you‚Äôre interested in AI‚Äôs energy footprint, earlier this year, MIT Technology Review published Power Hungry: a comprehensive series on AI and energy.







The AI Hype Index: AI-designed antibiotics show promise



Separating AI reality from hyped-up fiction isn‚Äôt always easy. That‚Äôs why we‚Äôve created the AI Hype Index‚Äîa simple, at-a-glance summary of everything you need to know about the state of the industry. Take a look at this month‚Äôs edition here.







The must-reads



I‚Äôve combed the internet to find you today‚Äôs most fun/important/scary/fascinating stories about technology.



1 The White House has fired the director of the CDCBut Susan Monarez is refusing to go quietly. (WP $)+ Monarez is said to have clashed with RFK Jr over vaccine policy. (NYT $)+ She was confirmed by the Senate to the position just last month. (The Guardian)+ Vaccine consensus is splintering across the US. (Vox)



2 A Chinese hacking campaign hit at least 200 US organizationsIntelligence agencies say the breaches are among the most significant ever. (WP $)+ AI-generated ransomware is on the rise. (Wired $)



3 Ukraine‚Äôs new Flamingo cruise missile took just months to buildRussia‚Äôs air defenses are weakening. Can this missile exploit the gaps? (Economist $)+ 14 people were killed in an overnight bombardment of Kyiv. (BBC)+ On the ground in Ukraine‚Äôs largest Starlink repair shop. (MIT Technology Review)



4 AI infrastructure spending is boosting the US economyCompanies are throwing so much money at AI hardware it‚Äôs lifting the real economy, not just the stock market. (NYT $)+ How to fine-tune AI for prosperity. (MIT Technology Review)5 OpenAI and Anthropic safety-tested each other‚Äôs AIThey found Claude is a lot more cautious than OpenAI‚Äôs mini models. (Engadget)+ Sycophancy was a repeated issue among OpenAI‚Äôs models. (TechCrunch)+ This benchmark used Reddit‚Äôs AITA to test how much AI models suck up to us. (MIT Technology Review)



6 Climate change exacerbated Europe‚Äôs deadly wildfiresAnd fires across the Mediterranean are likely to become more frequent and severe. (BBC)+ What the collapse of a glacier can teach us. (New Yorker $)+ How AI can help spot wildfires. (MIT Technology Review)



7 911 centers are using AI to answer callsIt‚Äôs helping to triage anything that isn‚Äôt urgent. (TechCrunch)



8 Wikipedia has compiled a list of AI writing tropesBut their presence still isn‚Äôt a dead giveaway a text has been written by AI. (Fast Company $)+ AI-text detection tools are really easy to fool. (MIT Technology Review)



9 Melania Trump has launched the Presidential AI Challenge¬†But it‚Äôs not all that clear what the competition actually is. (NY Mag $)



10 Netflix‚Äôs algorithm-appeasing movies are bland and boringBut millions of people will watch them anyway. (The Guardian)







Quote of the day



&#8220;The more you buy, the more you grow.&#8221;



‚ÄîNvidia CEO Jensen Huang conveniently sees no end to the AI chip spending boom, Reuters reports.







One more thinghttps://www.technologyreview.com/2025/01/13/1109922/inside-the-strange-limbo-facing-ivf-embryos/?utm_source=the_download&amp;utm_medium=email&amp;utm_campaign=the_download.unpaid.engagement&amp;utm_term=*|SUBCLASS|*&amp;utm_content=*|DATE:m-d-Y|*







Inside the strange limbo facing millions of IVF embryosMillions of embryos created through IVF sit frozen in time, stored in cryopreservation tanks around the world, and the number is only growing.At a basic level, an embryo is simply a tiny ball of a hundred or so cells. But unlike other types of body tissue, it holds the potential for life. Many argue that this endows embryos with a special moral status, one that requires special protections.The problem is that no one can really agree on what that status is. What do these embryos mean to us? And who should be responsible for them? Read the full story.



‚ÄîJessica Hamzelou







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)+ Wow, that is one seriously orange shark!+ TikTok is a proven way to introduce younger generations to older music‚Äîand now it‚Äôs Radiohead‚Äôs turn.+ Why we‚Äôre still going bananas for Donkey Kong after all these years+ This photo perfectly captures the joy of letting loose at a wedding.
‚Ä¢ Creating a qubit fit for a quantum future
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations

üîí Cybersecurity & Privacy
‚Ä¢ Affiliates Flock to ‚ÄòSoulless‚Äô Scam Gambling Machine
  Last month, KrebsOnSecurity tracked the sudden emergence of hundreds of polished online gaming and wagering websites that lure people with free credits and eventually abscond with any cryptocurrency funds deposited by players. We&#8217;ve since learned that these scam gambling sites have proliferated thanks to a new Russian affiliate program called &#8220;Gambler Panel&#8221; that bills itself as a &#8220;soulless project that is made for profit.&#8221;
A machine-translated version of Gambler Panel&#8217;s affiliate website.
The scam begins with deceptive ads posted on social media that claim the wagering sites are working in partnership with popular athletes or social media personalities. The ads invariably state that by using a supplied &#8220;promo code,&#8221; interested players can claim a $2,500 credit on the advertised gaming website.
The gaming sites ask visitors to create a free account to claim their $2,500 credit, which they can use to play any number of extremely polished video games that ask users to bet on each action. However, when users try to cash out any &#8220;winnings&#8221; the gaming site will reject the request and prompt the user to make a ‚Äúverification deposit‚Äù of cryptocurrency ‚Äî typically around $100 ‚Äî before any money can be distributed.
Those who deposit cryptocurrency funds are soon pressed into more wagering and making additional deposits. And &#8212; shocker alert &#8212; all players eventually lose everything they&#8217;ve invested in the platform.
The number of scam gambling or &#8220;scambling&#8221; sites has skyrocketed in the past month, and now we know why: The sites all pull their gaming content and detailed strategies for fleecing players straight from the playbook created by Gambler Panel, a Russian-language affiliate program that promises affiliates up to 70 percent of the profits.

Gambler Panel&#8217;s website gambler-panel[.]com links to a helpful wiki that explains the scam from cradle to grave, offering affiliates advice on how best to entice visitors, keep them gambling, and extract maximum profits from each victim.
&#8220;We have a completely self-written from scratch FAKE CASINO engine that has no competitors,&#8221; Gambler Panel&#8217;s wiki enthuses. &#8220;Carefully thought-out casino design in every pixel, a lot of audits, surveys of real people and test traffic floods were conducted, which allowed us to create something that has no doubts about the legitimacy and trustworthiness even for an inveterate gambling addict with many years of experience.&#8221;
Gambler Panel explains that the one and only goal of affiliates is to drive traffic to these scambling sites by any and all means possible.
A machine-translated portion of Gambler Panel&#8217;s singular instruction for affiliates: Drive traffic to these scambling sites by any means available.
&#8220;Unlike white gambling affiliates, we accept absolutely any type of traffic, regardless of origin, the only limitation is the CIS countries,&#8221; the wiki continued, referring to a common prohibition against scamming people in Russia and former Soviet republics in the Commonwealth of Independent States.
The program&#8217;s website claims it has more than 20,000 affiliates, who earn a minimum of $10 for each verification deposit. Interested new affiliates must first get approval from the group&#8217;s Telegram channel, which currently has around 2,500 active users.
The Gambler Panel channel is replete with images of affiliate panels showing the daily revenue of top affiliates, scantily-clad young women promoting the Gambler logo, and fast cars that top affiliates claimed they bought with their earnings.
A machine-translated version of the wiki for the affiliate program Gambler Panel.
The apparent popularity of this scambling niche is a consequence of the program&#8217;s ease of use and detailed instructions for successfully reproducing virtually every facet of the scam. Indeed, much of the tutorial focuses on advice and ready-made templates to help even novice affiliates drive traffic via social media websites, particularly on Instagram and TikTok.
Gambler Panel also walks affiliates through a range of possible responses to questions from users who are trying to withdraw funds from the platform. This section, titled &#8220;Rules for working in Live chat,&#8221; urges scammers to respond quickly to user requests (1-7 minutes), and includes numerous strategies for keeping the conversation professional and the user on the platform as long as possible.
A machine-translated version of the Gambler Panel&#8217;s instructions on managing chat support conversations with users.
The connection between Gambler Panel and the explosion in the number of scambling websites was made by a 17-year-old developer who operates multiple Discord servers that have been flooded lately with misleading ads for these sites.
The researcher, who asked to be identified only by the nickname &#8220;Thereallo,&#8221; said Gambler Panel has built a scalable business product for other criminals.
&#8220;The wiki is kinda like a &#8216;how to scam 101&#8217; for criminals written with the clarity you would expect from a legitimate company,&#8221; Thereallo said. &#8220;It&#8217;s clean, has step by step guides, and treats their scam platform like a real product. You could swap out the content, and it could be any documentation for startups.&#8221;
&#8220;They&#8217;ve minimized their own risk &#8212; spreading the links on Discord / Facebook / YT Shorts, etc. &#8212; and outsourced it to a hungry affiliate network, just like a franchise,&#8221; Thereallo wrote in response to questions.
&#8220;A centralized platform that can serve over 1,200 domains with a shared user base, IP tracking, and a custom API is not at all a trivial thing to build,&#8221; Thereallo said. &#8220;It&#8217;s a scalable system designed to be a resilient foundation for thousands of disposable scam sites.&#8221;
The security firm Silent Push has compiled a list of the latest domains associated with the Gambler Panel, available here (.csv).

üéì University AI
No updates.

üè¢ Corporate AI
‚Ä¢ Detect Amazon Bedrock misconfigurations with Datadog Cloud Security
  This post was co-written with Nick Frichette and Vijay George from Datadog.&nbsp; 
As organizations increasingly adopt Amazon Bedrock for generative AI applications, protecting against misconfigurations that could lead to data leaks or unauthorized model access becomes critical. The AWS Generative AI Adoption Index, which surveyed 3,739 senior IT decision-makers across nine countries, revealed that 45% of organizations selected generative AI tools as their top budget priority in 2025. As more AWS and Datadog customers accelerate their adoption of AI, building AI security into existing processes will become essential, especially as more stringent regulations emerge. But looking at AI risks in a silo isn‚Äôt enough; AI risks must be contextualized alongside other risks such as identity exposures and misconfigurations. The combination of Amazon Bedrock and Datadog‚Äôs comprehensive security monitoring helps organizations innovate faster while maintaining robust security controls. 
Amazon Bedrock delivers enterprise-grade security by incorporating built-in protections across data privacy, access controls, network security, compliance, and responsible AI safeguards. Customer data is encrypted both in transit using TLS 1.2 or above and at rest with AWS Key Management Service (AWS KMS), and organizations have full control over encryption keys. Data privacy is central: your input, prompts, and outputs are not shared with model providers nor used to train or improve foundation models (FMs). Fine-tuning and customizations occur on private copies of models, providing data confidentiality. Access is tightly governed through AWS Identity and Access Management (IAM) and resource-based policies, supporting granular authorization for users and roles. Amazon Bedrock integrates with AWS PrivateLink and supports virtual private cloud (VPC) endpoints for private, internal communication, so traffic doesn‚Äôt leave the Amazon network. The service complies with key industry standards such as ISO, SOC, CSA STAR, HIPAA eligibility, GDPR, and FedRAMP High, making it suitable for regulated industries. Additionally, Amazon Bedrock includes configurable guardrails to filter sensitive or harmful content and promote responsible AI use. Security is structured under the AWS Shared Responsibility Model, where AWS manages infrastructure security and customers are responsible for secure configurations and access controls within their Amazon Bedrock environment. 
Building on these robust AWS security features, Datadog and AWS have partnered to provide a holistic view of AI infrastructure risks, vulnerabilities, sensitive data exposure, and other misconfigurations. Datadog Cloud Security employs both agentless and agent-based scanning to help organizations identify, prioritize, and remediate risks across cloud resources. This integration helps AWS users prioritize risks based on business criticality, with security findings enriched by observability data, thereby enhancing their overall security posture in AI implementations. 
We‚Äôre excited to announce new security capabilities in Datadog Cloud Security that can help you detect and remediate Amazon Bedrock misconfigurations before they become security incidents. This integration helps organizations embed robust security controls and secure their use of the powerful capabilities of Amazon Bedrock by offering three critical advantages: holistic AI security by integrating AI security into your broader cloud security strategy, real-time risk detection through identifying potential AI-related security issues as they emerge, and simplified compliance to help meet evolving AI regulations with pre-built detections. 
AWS and Datadog: Empowering customers to adopt AI securely 
The partnership between AWS and Datadog is focused on helping customers operate their cloud infrastructure securely and efficiently. As organizations rapidly adopt AI technologies, extending this partnership to include Amazon Bedrock is a natural evolution. Amazon Bedrock is a fully managed service that makes high-performing FMs from leading AI companies and Amazon available through a unified API, making it an ideal starting point for Datadog‚Äôs AI security capabilities. 
The decision to prioritize Amazon Bedrock integration is driven by several factors, including strong customer demand, comprehensive security needs, and the existing integration foundation. With over 900 integrations and a partner-built Marketplace, Datadog‚Äôs long-standing partnership with AWS and deep integration capabilities have helped Datadog quickly develop comprehensive security monitoring for Amazon Bedrock while using their existing cloud security expertise. 
Throughout Q4 2024, Datadog Security Research observed increasing threat actor interest in cloud AI environments, making this integration particularly timely. By combining the powerful AI capabilities of AWS with Datadog‚Äôs security expertise, you can safely accelerate your AI adoption while maintaining robust security controls. 
How Datadog Cloud Security helps secure Amazon Bedrock resources 
After adding the AWS integration to your Datadog account and enabling Datadog Cloud Security, Datadog Cloud Security continuously monitors your AWS environment, identifying misconfigurations, identity risks, vulnerabilities, and compliance violations. These detections use the Datadog Severity Scoring system to prioritize them based on infrastructure context. The scoring considers a variety of variables, including if the resource is in production, is publicly accessible, or has access to sensitive data. This multi-layer analysis can help you reduce noise and focus your attention to the most critical misconfigurations by considering runtime behavior. 
Partnering with AWS, Datadog is excited to offer detections for Datadog Cloud Security customers, such as: 
 
 Amazon Bedrock custom models should not output model data to publicly accessible S3 buckets 
 Amazon Bedrock custom models should not train from publicly writable S3 buckets 
 Amazon Bedrock guardrails should have a prompt attack filter enabled and block prompt attacks at high sensitivity 
 Amazon Bedrock agent guardrails should have the sensitive information filter enabled and block highly sensitive PII entities 
 
Detect AI misconfigurations with Datadog Cloud Security 
To understand how these detections can help secure your Amazon Bedrock infrastructure, let‚Äôs look at a specific use case, in which Amazon Bedrock custom models should not train from publicly writable Amazon Simple Storage Service (Amazon S3) buckets. 
With Amazon Bedrock, you can customize AI models by fine-tuning on domain specific data. To do this, that data is stored in an S3 bucket. Threat actors are constantly evaluating the configuration of S3 buckets, looking for the potential to access sensitive data or even the ability to write to S3 buckets. 
If a threat actor finds an S3 bucket that was misconfigured to permit public write access, and that same bucket contained data that was used to train an AI model, a bad actor could poison that dataset and introduce malicious behavior or output to the model. This is known as a data poisoning attack. 
Normally, detecting these types of misconfigurations requires multiple steps: one to identify the S3 bucket misconfigured with write access, and one to identify that the bucket is being used by Amazon Bedrock. With Datadog Cloud Security, this detection is one of hundreds that are activated out of the box. 
In the Datadog Cloud Security system, you can view this issue alongside surrounding infrastructure using Cloud Map. This provides live diagrams of your cloud architecture, as shown in the following screenshot. AI risks are then contextualized alongside sensitive data exposure, identity risks, vulnerabilities, and other misconfigurations to give you a 360-view of risks. 
 
For example, you might see that your application is using Anthropic‚Äôs Claude 3.7 on Amazon Bedrock and accessing training or prompt data stored in an S3 bucket that also has public write access. This could inadvertently impact model integrity by introducing unapproved data to the large language model (LLM), so you will want to update this configuration. Though basic, the first step for most security initiatives is identifying the issue. With agentless scanning, Datadog scans your AWS environment at intervals between 15 minutes and 2 hours, so users can identify misconfigurations as they are introduced to their environment. The next step is to remediate this risk. Datadog Cloud Security offers automatically generated remediation guidance, specifically for each risk (see the following screenshot). You will get a step-by-step explanation of how to fix each finding. In this situation, we can remediate this issue by modifying the S3 bucket‚Äôs policy, helping prevent public write access. You can do this directly in AWS, create a JIRA ticket, or use the built-in workflow automation tools. From here, you can apply remediation steps directly within Datadog and confirm that the misconfiguration has been resolved. 
 
Resolving this issue will positively impact your compliance posture, as illustrated by the posture score in Datadog Cloud Security, helping teams meet internal benchmarks and regulatory standards. Teams can also create custom frameworks or iterate on existing ones for tailored compliance controls. 
 
As generative AI is embraced across industries, the regulatory environment will evolve. Datadog will continue partnering with AWS to expand their detection library and support secure AI adoption and compliance. 
How Datadog Cloud Security detects misconfigurations in your cloud environment 
You can deploy Datadog Cloud Security either with the Datadog agent, agentlessly, or both to maximize security coverage in your cloud environment. Datadog customers can start monitoring their AWS accounts for misconfigurations by first adding the AWS integration to Datadog. This enables Datadog to crawl cloud resources in customer AWS accounts. 
As the Datadog system finds resources, it runs through a catalog of hundreds of out-of-the-box detection rules against these resources, looking for misconfigurations and threat paths that adversaries can exploit. 
Secure your AI infrastructure with Datadog 
Misconfigurations in AI systems can be risky, but with the right tools, you can have the visibility and context needed to manage them. With Datadog Cloud Security, teams gain visibility into these risks, detect threats early, and remediate issues with confidence. In addition, Datadog has also released numerous agentic AI security features, designed to help teams gain visibility into the health and security of critical AI workload, which includes new announcements made to Datadog‚Äôs LLM observability features. 
Lastly, Datadog announced Bits AI Security Analyst alongside other Bits AI agents at DASH. Included as part of Cloud SIEM, Bits is an agentic AI security analyst that automates triage for AWS CloudTrail signals. Bits investigates each alert like a seasoned analyst: pulling in relevant context from across your Datadog environment, annotating key findings, and offering a clear recommendation on whether the signal is likely benign or malicious. By accelerating triage and surfacing real threats faster, Bits helps reduce mean time to remediation (MTTR) and frees analysts to focus on important threat hunting and response initiatives. This helps across different threats, including AI-related threats. 
To learn more about how Datadog helps secure your AI infrastructure, see Monitor Amazon Bedrock with Datadog or check out our security documentation. If you‚Äôre not already using Datadog, you can get started with Datadog Cloud Security with a 14-day free trial. 
 
About the Authors 
Nina Chen is a Customer Solutions Manager at AWS specializing in leading software companies to use the power of the AWS Cloud to accelerate their product innovation and growth. With over 4 years of experience working in the strategic independent software vendor (ISV) vertical, Nina enjoys guiding ISV partners through their cloud transformation journeys, helping them optimize their cloud infrastructure, driving product innovation, and delivering exceptional customer experiences. 
 Sujatha Kuppuraju is a Principal Solutions Architect at AWS, specializing in cloud and generative AI security. She collaborates with software companies‚Äô leadership teams to architect secure, scalable solutions on AWS and guide strategic product development. Using her expertise in cloud architecture and emerging technologies, Sujatha helps organizations optimize offerings, maintain robust security, and bring innovative products to market in an evolving tech landscape. 
Nick Frichette is a Staff Security Researcher for Cloud Security Research at Datadog. 
Vijay George is a Product Manager for AI Security at Datadog.
‚Ä¢ Set up custom domain names for Amazon Bedrock AgentCore Runtime agents
  When deploying AI agents to Amazon Bedrock AgentCore Runtime (currently in preview), customers often want to use custom domain names to create a professional and seamless experience. 
By default, AgentCore Runtime agents use endpoints like https://bedrock-agentcore.{region}.amazonaws.com/runtimes/{EncodedAgentARN}/invocations. 
In this post, we discuss how to transform these endpoints into user-friendly custom domains (like https://agent.yourcompany.com) using Amazon CloudFront as a reverse proxy. The solution combines CloudFront, Amazon Route 53, and AWS Certificate Manager (ACM) to create a secure, scalable custom domain setup that works seamlessly with your existing agents. 
Benefits of Amazon Bedrock AgentCore Runtime 
If you‚Äôre building AI agents, you have probably wrestled with hosting challenges: managing infrastructure, handling authentication, scaling, and maintaining security. Amazon Bedrock AgentCore Runtime helps address these problems. 
Amazon Bedrock AgentCore Runtime is framework agnostic; you can use it with LangGraph, CrewAI, Strands Agents, or custom agents you have built from scratch. It supports extended execution times up to 8 hours, perfect for complex reasoning tasks that traditional serverless functions can‚Äôt handle. Each user session runs in its own isolated microVM, providing security that‚Äôs crucial for enterprise applications. 
The consumption-based pricing model means you only pay for what you use, not what you provision. And unlike other hosting solutions, Amazon Bedrock AgentCore Runtime includes built-in authentication and specialized observability for AI agents out of the box. 
Benefits of custom domains 
When using Amazon Bedrock AgentCore Runtime with Open Authorization (OAuth) authentication, your applications make direct HTTPS requests to the service endpoint. Although this works, custom domains offer several benefits: 
 
 Custom branding ‚Äì Client-side applications (web browsers, mobile apps) display your branded domain instead of AWS infrastructure details in network requests 
 Better developer experience ‚Äì Development teams can use memorable, branded endpoints instead of copying and pasting long AWS endpoints across code bases and configurations 
 Simplified maintenance ‚Äì Custom domains make it straightforward to manage endpoints when deploying multiple agents or updating configurations across environments 
 
Solution overview 
In this solution, we use CloudFront as a reverse proxy to transform requests from your custom domain into Amazon Bedrock AgentCore Runtime API calls. Instead of using the default endpoint, your applications can make requests to a user-friendly URL like https://agent.yourcompany.com/. 
The following diagram illustrates the solution architecture. 
 
The workflow consists of the following steps: 
 
 A client application authenticates with Amazon Cognito and receives a bearer token. 
 The client makes an HTTPS request to your custom domain. 
 Route 53 resolves the DNS request to CloudFront. 
 CloudFront forwards the authenticated request to the Amazon Bedrock Runtime agent. 
 The agent processes the request and returns the response through the same path. 
 
You can use the same CloudFront distribution to serve both your frontend application and backend agent endpoints, avoiding cross-origin resource sharing (CORS) issues because everything originates from the same domain. 
Prerequisites 
To follow this walkthrough, you must have the following in place: 
 
 An AWS account with appropriate permissions 
 The AWS Cloud Development Kit (AWS CDK) version 2.x or later 
 An AWS Identity and Access Management (IAM) execution role with appropriate permissions for Amazon Bedrock AgentCore Runtime 
 
Although Amazon Bedrock AgentCore Runtime can be in other supported AWS Regions, CloudFront requires SSL certificates to be in the us-east-1 Region. 
You can choose from the following domain options: 
 
 Use an existing domain ‚Äì Add a subdomain like agent.yourcompany.com 
 Register a new domain ‚Äì Use Route 53 to register a domain if you don‚Äôt have one 
 Use the default URL from CloudFront ‚Äì No domain registration or configuration required 
 
Choose the third option if you want to test the solution quickly before setting up a custom domain. 
Create an agent with inbound authentication 
If you already have an agent deployed with OAuth authentication, you can skip to the next section to set up the custom domain. Otherwise, follow these steps to create a new agent using Amazon Cognito as your OAuth provider: 
 
 Create a new directory for your agent with the following structure: 
 
 
 your_project_directory/
‚îú‚îÄ‚îÄ agent_example.py # Your main agent code
‚îú‚îÄ‚îÄ requirements.txt # Dependencies for your agent
‚îî‚îÄ‚îÄ __init__.py # Makes the directory a Python package 
 
 
 Create the main agent code in agent_example.py: 
 
 
 # agent_example.py
from strands import Agent
from bedrock_agentcore.runtime import BedrockAgentCoreApp

agent = Agent()
app = BedrockAgentCoreApp()
@app.entrypoint
def invoke(payload):
    """Process user input and return a response"""
    user_message = payload.get("prompt", "Hello")
    response = agent(user_message)
    return str(response) # response should be json serializable
if __name__ == "__main__":
    app.run() 
 
 
 Add dependencies to requirements.txt: 
 
 
 # requirements.txt
strands-agents
bedrock-agentcore 
 
 
 Run the following commands to create an Amazon Cognito user pool and test user: 
 
 
 # Create User Pool and capture Pool ID
export POOL_ID=$(aws cognito-idp create-user-pool \
  --pool-name "MyUserPool" \
  --policies '{"PasswordPolicy":{"MinimumLength":8}}' \
  --region us-east-1 | jq -r '.UserPool.Id')

# Create App Client and capture Client ID
export CLIENT_ID=$(aws cognito-idp create-user-pool-client \
  --user-pool-id $POOL_ID \
  --client-name "MyClient" \
  --no-generate-secret \
  --explicit-auth-flows "ALLOW_USER_PASSWORD_AUTH" "ALLOW_REFRESH_TOKEN_AUTH" \
  --region us-east-1 | jq -r '.UserPoolClient.ClientId')

# Create and configure a test user
aws cognito-idp admin-create-user \
  --user-pool-id $POOL_ID \
  --username "testuser" \
  --temporary-password "Temp1234" \
  --region us-east-1 \
  --message-action SUPPRESS

aws cognito-idp admin-set-user-password \
  --user-pool-id $POOL_ID \
  --username "testuser" \
  --password "MyPassword123" \
  --region us-east-1 \
  --permanent

echo "Pool ID: $POOL_ID"
echo "Discovery URL: https://cognito-idp.us-east-1.amazonaws.com/$POOL_ID/.well-known/openid-configuration"
echo "Client ID: $CLIENT_ID" 
 
 
 Deploy the agent using the Amazon Bedrock AgentCore command line interface (CLI) provided by the starter toolkit: 
 
 
 pip install bedrock-agentcore-starter-toolkit #install the starter toolkit

agentcore configure --entrypoint agent_example.py \
--name my_agent \
--execution-role your-execution-role-arn \
--requirements-file requirements.txt \
--authorizer-config "{\"customJWTAuthorizer\":{\"discoveryUrl\":\"https://cognito-idp.us-east-1.amazonaws.com/$POOL_ID/.well-known/openid-configuration\",\"allowedClients\":[\"$CLIENT_ID\"]}}"

agentcore launch 
 
Make note of your agent runtime Amazon Resource Name (ARN) after deployment. You will need this for the custom domain configuration. 
For additional examples and details, see Authenticate and authorize with Inbound Auth and Outbound Auth. 
Set up the custom domain solution 
Now let‚Äôs implement the custom domain solution using the AWS CDK. This section shows you how to create the CloudFront distribution that proxies your custom domain requests to Amazon Bedrock AgentCore Runtime endpoints. 
 
 Create a new directory and initialize an AWS CDK project: 
 
 
 mkdir agentcore-custom-domain
cd agentcore-custom-domain
cdk init app --language python
source .venv/bin/activate
pip install aws-cdk-lib constructs 
 
 
 Encode the agent ARN and prepare the CloudFront origin configuration: 
 
 
 # agentcore_custom_domain_stack.py 
import urllib.parse

agent_runtime_arn = "arn:aws:bedrock-agentcore:us-east-1:accountId:runtime/my_agent-xbcDkz4FR9"
encoded_arn = urllib.parse.quote(agent_runtime_arn, safe='') # URL-encode the ARN
region = agent_runtime_arn.split(':')[3]  # Extract region from ARN 
 
If your frontend application runs on a different domain than your agent endpoint, you must configure CORS headers. This is common if your frontend is hosted on a different domain (for example, https://app.yourcompany.com calling https://agent.yourcompany.com), or if you‚Äôre developing locally (for example, http://localhost:3000 calling your production agent endpoint). 
 
 To handle CORS requirements, create a CloudFront response headers policy: 
 
 
 # agentcore_custom_domain_stack.py 
from aws_cdk.aws_cloudfront import ResponseHeadersPolicy, ResponseHeadersCorsBehavior

# Create CORS response headers policy
cors_policy = ResponseHeadersPolicy(self, 'CorsPolicy',
    cors_behavior=ResponseHeadersCorsBehavior(
        access_control_allow_origins=['*'], # Or specify your frontend domains
        access_control_allow_headers=[
            'Authorization',
            'Content-Type', 
            'X-Amzn-*',
            'X-Requested-With'
        ],
        access_control_allow_methods=['GET', 'POST', 'OPTIONS'],
        access_control_allow_credentials=False,
        access_control_expose_headers=['*'],
        origin_override=True # Overrides CORS headers from origin
    )
) 
 
 
 Create a CloudFront distribution to act as a reverse proxy for your agent endpoints: 
 
 
 # agentcore_custom_domain_stack.py
 from aws_cdk.aws_cloudfront import (
    Distribution, BehaviorOptions, CachePolicy, 
    AllowedMethods, ViewerProtocolPolicy,
    OriginProtocolPolicy, OriginRequestPolicy
)
from aws_cdk.aws_cloudfront_origins import HttpOrigin

bedrock_agentcore_hostname = f"bedrock-agentcore.{region}.amazonaws.com"
origin_path = f"/runtimes/{encoded_arn}/invocations"

distribution = Distribution(self, 'Distribution',
    default_behavior=BehaviorOptions(
        origin=HttpOrigin(
            bedrock_agentcore_hostname,
            origin_path=origin_path, 
            protocol_policy=OriginProtocolPolicy.HTTPS_ONLY,
            read_timeout=Duration.seconds(120) # Optional: for responses &gt;30s, adjust as needed
        ),
        viewer_protocol_policy=ViewerProtocolPolicy.REDIRECT_TO_HTTPS,
        cache_policy=CachePolicy.CACHING_DISABLED,  # Critical for dynamic APIs
        allowed_methods=AllowedMethods.ALLOW_ALL,
        response_headers_policy=cors_policy,  # Add CORS policy if created
        origin_request_policy=OriginRequestPolicy.ALL_VIEWER,  # Forward headers for MCP
    ),
    # Add domain configuration if using custom domains
    domain_names=[domain_name] if domain_name else None,
    certificate=certificate if domain_name else None,
) 
 
Set cache_policy=CachePolicy.CACHING_DISABLED to make sure your agent responses remain dynamic and aren‚Äôt cached by CloudFront. 
 
 If you‚Äôre using a custom domain, add an SSL certificate and DNS configuration to your stack: 
 
 
 # agentcore_custom_domain_stack.py 
from aws_cdk.aws_certificatemanager import Certificate, CertificateValidation
from aws_cdk.aws_route53 import HostedZone, ARecord, RecordTarget
from aws_cdk.aws_route53_targets import CloudFrontTarget

# For existing domains
hosted_zone = HostedZone.from_lookup(self, 'HostedZone',
    domain_name='yourcompany.com'
)
# SSL certificate with automatic DNS validation
certificate = Certificate(self, 'Certificate',
    domain_name='my-agent.yourcompany.com',
    validation=CertificateValidation.from_dns(hosted_zone),
)
# DNS record pointing to CloudFront
ARecord(self, 'AliasRecord',
    zone=hosted_zone,
    record_name='my-agent.yourcompany.com',
    target=RecordTarget.from_alias(CloudFrontTarget(distribution)),
) 
 
The following code is the complete AWS CDK stack that combines all the components: 
 
 # agentcore_custom_domain_stack.py
import urllib.parse
from aws_cdk import Stack, CfnOutput, Duration
from aws_cdk.aws_cloudfront import (
    Distribution, BehaviorOptions,
    CachePolicy, AllowedMethods,
    ViewerProtocolPolicy, OriginProtocolPolicy,
    ResponseHeadersPolicy, ResponseHeadersCorsBehavior,
    OriginRequestPolicy
)
from aws_cdk.aws_cloudfront_origins import HttpOrigin
from aws_cdk.aws_certificatemanager import Certificate, CertificateValidation
from aws_cdk.aws_route53 import HostedZone, ARecord, RecordTarget
from aws_cdk.aws_route53_targets import CloudFrontTarget
from constructs import Construct

class AgentcoreCustomDomainStack(Stack):
    def __init__(self, scope: Construct, construct_id: str, **kwargs) -&gt; None:
        super().__init__(scope, construct_id, **kwargs)

        # Configuration - Update these for your setup
        agent_runtime_arn = "arn:aws:bedrock-agentcore:us-east-1:accountId:runtime/my_agent-xbcDkz4FR9"
        region = agent_runtime_arn.split(':')[3]  # Extract region from ARN
        domain_name = "agent.yourcompany.com"  # Using your hosted zone
        hosted_zone_id = "Z1234567890ABC"  # Your hosted zone ID
        enable_cors = True  # Set to False if serving frontend and backend from same domain

        # Encode the agent ARN for the origin path
        encoded_arn = urllib.parse.quote(agent_runtime_arn, safe='')
        bedrock_agentcore_hostname = f"bedrock-agentcore.{region}.amazonaws.com"
        origin_path = f"/runtimes/{encoded_arn}/invocations"

        # Create CORS response headers policy if needed
        cors_policy = None
        if enable_cors:
            cors_policy = ResponseHeadersPolicy(self, 'CorsPolicy',
                cors_behavior=ResponseHeadersCorsBehavior(
                    access_control_allow_origins=['*'],  # Or specify your frontend domains
                    access_control_allow_headers=[
                        'Authorization',
                        'Content-Type', 
                        'X-Amzn-*',
                        'X-Requested-With'
                    ],
                    access_control_allow_methods=['GET', 'POST', 'OPTIONS'],
                    access_control_expose_headers=['*'],
                    access_control_allow_credentials=False,
                    origin_override=True  # Overrides CORS headers from origin
                )
            )

        # Base distribution configuration
        distribution_props = {
            "default_behavior": BehaviorOptions(
                origin=HttpOrigin(
                    bedrock_agentcore_hostname,
                    origin_path=origin_path,  # Direct path to agent endpoint
                    protocol_policy=OriginProtocolPolicy.HTTPS_ONLY,
                    read_timeout=Duration.seconds(120) # Optional: for responses &gt;30s, adjust as needed
                ),
                viewer_protocol_policy=ViewerProtocolPolicy.REDIRECT_TO_HTTPS,
                cache_policy=CachePolicy.CACHING_DISABLED,
                allowed_methods=AllowedMethods.ALLOW_ALL,
                response_headers_policy=cors_policy,  # Add CORS policy if enabled
                origin_request_policy=OriginRequestPolicy.ALL_VIEWER,  # Forward headers for MCP
            )
        }

        # Optional: Add custom domain
        if domain_name:
            # Use from_hosted_zone_attributes for specific zone
            hosted_zone = HostedZone.from_hosted_zone_attributes(self, 'HostedZone',
                                                                 zone_name='yourcompany.com',  # Your root domain
                                                                 hosted_zone_id=hosted_zone_id
                                                                 )

            certificate = Certificate(self, 'Certificate',
                                      domain_name=domain_name,
                                      validation=CertificateValidation.from_dns(
                                          hosted_zone),
                                      )

            # Add custom domain to distribution
            distribution_props["domain_names"] = [domain_name]
            distribution_props["certificate"] = certificate

        distribution = Distribution(self, 'Distribution', **distribution_props)

        # Create DNS record if using custom domain
        if domain_name:
            ARecord(self, 'AliasRecord',
                    zone=hosted_zone,
                    record_name=domain_name,
                    target=RecordTarget.from_alias(
                        CloudFrontTarget(distribution)),
                    )

        # Outputs
        if domain_name:
            domain_url = f"https://{domain_name}/"
            CfnOutput(self, "AgentEndpoint",
                      value=domain_url,
                      description="Your custom domain endpoint"
                      )

        CfnOutput(self, "CloudFrontDistribution",
                  value=f"https://{distribution.distribution_domain_name}/",
                  description="CloudFront default domain (works without custom domain)"
                  ) 
 
 
 Configure the AWS CDK app entry point: 
 
 
 # app.py
#!/usr/bin/env python3
import aws_cdk as cdk
from agentcore_custom_domain.agentcore_custom_domain_stack import AgentCoreCustomDomainStack

app = cdk.App()
AgentcoreCustomDomainStack(app, "AgentCoreCustomDomainStack",
    # CloudFront requires certificates in us-east-1
    env=cdk.Environment(region='us-east-1'),
)
app.synth() 
 
Deploy your custom domain 
Now you can deploy the solution and verify it works with both custom and default domains. Complete the following steps: 
 
 Update the following values in agentcore_custom_domain_stack.py: 
   
   Your Amazon Bedrock AgentCore Runtime ARN 
   Your domain name (if using a custom domain) 
   Your hosted zone ID (if using a custom domain) 
    
 Deploy using the AWS CDK: 
 
 
 cdk deploy 
 
Test your endpoint 
After you deploy the custom domain, you can test your endpoints using either the custom domain or the CloudFront default domain.First, get a JWT token from Amazon Cognito: 
 
 export TOKEN=$(aws cognito-idp initiate-auth \
  --client-id "your-client-id" \
  --auth-flow USER_PASSWORD_AUTH \
  --auth-parameters USERNAME='testuser',PASSWORD='MyPassword123' \
  --region us-east-1 | jq -r '.AuthenticationResult.AccessToken') 
 
Use the following code to test with your custom domain: 
 
 curl -X POST "https://my-agent.yourcompany.com/" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -H "X-Amzn-Bedrock-AgentCore-Runtime-Session-Id: session-12345678901234567890123456789012345" \
  -d '{"prompt": "Hello, how can you help me today?"}' 
 
Alternatively, use the following code to test with the CloudFront default domain: 
 
 curl -X POST "https://d1234567890123.cloudfront.net/" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -H "X-Amzn-Bedrock-AgentCore-Runtime-Session-Id: session-12345678901234567890123456789012345" \
  -d '{"prompt": "Hello, how can you help me today?"}' 
 
 
 If everything works correctly, you will receive a response from your agent through either endpoint. You‚Äôve successfully created a custom domain for your Amazon Bedrock AgentCore Runtime agent! 
 
Considerations 
As you implement this solution in production, the following are some important considerations: 
 
 Cost implications ‚Äì CloudFront adds costs for data transfer and requests. Review Amazon CloudFront pricing to understand the impact for your usage patterns. 
 Security enhancements ‚Äì Consider implementing the following security measures: 
   
   AWS WAF rules to help protect against common web exploits. 
   Rate limiting to help prevent abuse. 
   Geo-restrictions if your agent should only be accessible from specific Regions. 
    
 Monitoring ‚Äì Enable CloudFront access logs and set up Amazon CloudWatch alarms to monitor error rates, latency, and request volume. 
 
Clean up 
To avoid ongoing costs, delete the resources when you no longer need them: 
 
 cdk destroy 
 
You might need to manually delete the Route 53 hosted zones and ACM certificates from their respective service consoles. 
Conclusion 
In this post, we showed you how to create custom domain names for your Amazon Bedrock AgentCore Runtime agent endpoints using CloudFront as a reverse proxy. This solution provides several key benefits: simplified integration for development teams, custom domains that align with your organization, cleaner infrastructure abstraction, and straightforward maintenance when endpoints need updates. By using CloudFront as a reverse proxy, you can also serve both your frontend application and backend agent endpoints from the same domain, avoiding common CORS challenges. 
We encourage you to explore this solution further by adapting it to your specific needs. You might want to enhance it with additional security features, set up monitoring, or integrate it with your existing infrastructure. 
To learn more about building and deploying AI agents, see the Amazon Bedrock AgentCore Developer Guide. For advanced configurations and best practices with CloudFront, refer to the Amazon CloudFront documentation. You can find detailed information about SSL certificates in the AWS Certificate Manager documentation, and domain management in the Amazon Route 53 documentation. 
Amazon Bedrock AgentCore is currently in preview and subject to change. Standard AWS pricing applies to additional services used, such as CloudFront, Route 53, and Certificate Manager. 
 
About the authors 
Rahmat Fedayizada is a Senior Solutions Architect with the AWS Energy and Utilities team. He works with energy companies to design and implement scalable, secure, and highly available architectures. Rahmat is passionate about translating complex technical requirements into practical solutions that drive business value. 
Paras Bhuva is a Senior Manager of Solutions Architecture at AWS, where he leads a team of solution architects helping energy customers innovate and accelerate their transformation. Having started as a Solution Architect in 2012, Paras is passionate about architecting scalable solutions and building organizations focused on application modernization and AI initiatives.
‚Ä¢ Introducing auto scaling on Amazon SageMaker HyperPod
  Today, we‚Äôre excited to announce that Amazon SageMaker HyperPod now supports managed node automatic scaling with Karpenter, so you can efficiently scale your SageMaker HyperPod clusters to meet your inference and training demands. Real-time inference workloads require automatic scaling to address unpredictable traffic patterns and maintain service level agreements (SLAs). As demand spikes, organizations must rapidly adapt their GPU compute without compromising response times or cost-efficiency. Unlike self-managed Karpenter deployments, this service-managed solution alleviates the operational overhead of installing, configuring, and maintaining Karpenter controllers, while providing tighter integration with the resilience capabilities of SageMaker HyperPod. This managed approach supports scale to zero, reducing the need for dedicated compute resources to run the Karpenter controller itself, improving cost-efficiency. 
SageMaker HyperPod offers a resilient, high-performance infrastructure, observability, and tooling optimized for large-scale model training and deployment. Companies like Perplexity, HippocraticAI, H.AI, and Articul8 are already using SageMaker HyperPod for training and deploying models. As more customers transition from training foundation models (FMs) to running inference at scale, they require the ability to automatically scale their GPU nodes to handle real production traffic by scaling up during high demand and scaling down during periods of lower utilization. This capability necessitates a powerful cluster auto scaler. Karpenter, an open source Kubernetes node lifecycle manager created by AWS, is a popular choice among Kubernetes users for cluster auto scaling due to its powerful capabilities that optimize scaling times and reduce costs. 
This launch provides a managed Karpenter-based solution for automatic scaling that is installed and maintained by SageMaker HyperPod, removing the undifferentiated heavy lifting of setup and management from customers. The feature is available for SageMaker HyperPod EKS clusters, and you can enable auto scaling to transform your SageMaker HyperPod cluster from static capacity to a dynamic, cost-optimized infrastructure that scales with demand. This combines Karpenter‚Äôs proven node lifecycle management with the purpose-built and resilient infrastructure of SageMaker HyperPod, designed for large-scale machine learning (ML) workloads. In this post, we dive into the benefits of Karpenter, and provide details on enabling and configuring Karpenter in your SageMaker HyperPod EKS clusters. 
New features and benefits 
Karpenter-based auto scaling in your SageMaker HyperPod clusters provides the following capabilities: 
 
 Service managed lifecycle ‚Äì SageMaker HyperPod handles Karpenter installation, updates, and maintenance, alleviating operational overhead 
 Just-in-time provisioning ‚Äì Karpenter observes your pending pods and provisions the required compute for your workloads from an on-demand pool 
 Scale to zero ‚Äì You can scale down to zero nodes without maintaining dedicated controller infrastructure 
 Workload-aware node selection ‚Äì Karpenter chooses optimal instance types based on pod requirements, Availability Zones, and pricing to minimize costs 
 Automatic node consolidation ‚Äì Karpenter regularly evaluates clusters for optimization opportunities, shifting workloads to avoid underutilized nodes 
 Integrated resilience ‚Äì Karpenter uses the built-in fault tolerance and node recovery mechanisms of SageMaker HyperPod 
 
These capabilities are built on top of recently launched continuous provisioning capabilities, which enables SageMaker HyperPod to automatically provision remaining capacity in the background while workloads start immediately on available instances. When node provisioning encounters failures due to capacity constraints or other issues, SageMaker HyperPod automatically retries in the background until clusters reach their desired scale, so your auto scaling operations remain resilient and non-blocking. 
Solution overview 
The following diagram illustrates the solution architecture. 
 
Karpenter works as a controller in the cluster and operates in the following steps: 
 
 Watching ‚Äì Karpenter watches for un-schedulable pods in the cluster through the Kubernetes API server. These could be pods that go into pending state when deployed or automatically scaled to increase the replica count. 
 Evaluating ‚Äì When Karpenter finds such pods, it computes the shape and size of a NodeClaim to fit the set of pods requirements (GPU, CPU, memory) and topology constraints, and checks if it can pair them with an existing NodePool. For each NodePool, it queries the SageMaker HyperPod APIs to get the instance types supported by the NodePool. It uses the information about instance type metadata (hardware requirements, zone, capacity type) to find a matching NodePool. 
 Provisioning ‚Äì If Karpenter finds a matching NodePool, it creates a NodeClaim and tries to provision a new instance to be used as the new node. Karpenter internally uses the sagemaker:UpdateCluster API to increase the capacity of the selected instance group. 
 Disrupting ‚Äì Karpenter periodically checks if a new node is needed or not. If it‚Äôs not needed, Karpenter deletes it, which internally translates to a delete node request to the SageMaker HyperPod cluster. 
 
Prerequisites 
Verify you have the required quotas for the instances you will create in the SageMaker HyperPod cluster. To review your quotas, on the Service Quotas console, choose AWS services in the navigation pane, then choose SageMaker. For example, the following screenshot shows the available quota for g5.12xlarge instances (three). 
 
To update the cluster, you must first create AWS Identity and Access Management (IAM) permissions for Karpenter. For instructions, see Create an IAM role for HyperPod autoscaling with Karpenter. 
Create and configure a SageMaker HyperPod cluster 
To begin, launch and configure your SageMaker HyperPod EKS cluster and verify that continuous provisioning mode is enabled on cluster creation. Complete the following steps: 
 
 On the SageMaker AI console, choose HyperPod clusters in the navigation pane. 
 Choose Create HyperPod cluster and Orchestrated on Amazon EKS. 
 For Setup options, select Custom setup. 
 For Name, enter a name. 
 For Instance recovery, select Automatic. 
 For Instance provisioning mode, select Use continuous provisioning. 
 Choose Submit. 
 
 
This setup creates the necessary configuration such as virtual private cloud (VPC), subnets, security groups, and EKS cluster, and installs operators in the cluster. You can also provide existing resources such as an EKS cluster if you want to use an existing cluster instead of creating a new one. This setup will take around 20 minutes. 
Verify that each InstanceGroup is limited to one zone by opting for the OverrideVpcConfig and selecting only one subnet per each InstanceGroup. 
 
After you create the cluster, you must update it to enable Karpenter. You can do this using Boto3 or the AWS Command Line Interface (AWS CLI) using the UpdateCluster API command (after configuring the AWS CLI to connect to your AWS account). 
The following code uses Python Boto3: 
 
 import boto3
client = boto3.client('sagemaker')
response = client.update_cluster(
    ClusterName=&lt;Your_Cluster_Name&gt;,
    AutoScaling = { "Mode": "Enable", "AutoScalerType": "Karpenter" },
    ClusterRole = &lt;Cluster_Role_ARN&gt;,
) 
 
 
 The following code uses the AWS CLI: 
 
 
 aws sagemaker update-cluster \
&nbsp; &nbsp; --cluster-name &lt;clustername&gt;&nbsp;\
&nbsp; &nbsp; --auto-scaling '{ "Mode": "Enable", "AutoScalerType": "Karpenter" }` \
&nbsp; &nbsp; --cluster-role &lt;clusterrole&gt; 
 
After you run this command and update the cluster, you can verify that Karpenter has been enabled by running the DescribeCluster API. 
The following code uses Python: 
 
 import&nbsp;boto3
client&nbsp;=&nbsp;boto3.client('sagemaker')
print(sagemaker_client.describe_cluster(ClusterName=&lt;Your_Cluster_Name&gt;).get("AutoScaling")) 
 
The following code uses the AWS CLI: 
 
 aws sagemaker describe-cluster --cluster-name &lt;clustername&gt; --query AutoScaling 
 
The following code shows our output: 
 
 {'Mode': 'Enable',
&nbsp;'AutoScalerType': 'Karpenter',
&nbsp;'Status': 'Enabled'} 
 
Now you have a working cluster. The next step is to set up some custom resources in your cluster for Karpenter. 
Create HyperpodNodeClass 
HyperpodNodeClass is a custom resource that maps to pre-created instance groups in SageMaker HyperPod, defining constraints around which instance types and Availability Zones are supported for Karpenter‚Äôs auto scaling decisions. To use HyperpodNodeClass, simply specify the names of the InstanceGroups of your SageMaker HyperPod cluster that you want to use as the source for the AWS compute resources to use to scale up your pods in your NodePools. 
The HyperpodNodeClass name that you use here is carried over to the NodePool in the next section where you reference it. This tells the NodePool which HyperpodNodeClass to draw resources from. To create a HyperpodNodeClass, complete the following steps: 
 
 Create a YAML file (for example, nodeclass.yaml) similar to the following code. Add InstanceGroup names that you used at the time of the SageMaker HyperPod cluster creation. You can also add new instance groups to an existing SageMaker HyperPod EKS cluster. 
 Reference the HyperPodNodeClass name in your NodePool configuration. 
 
The following is a sample HyperpodNodeClass that uses ml.g6.xlarge and ml.g6.4xlarge instance types: 
 
 apiVersion: karpenter.sagemaker.amazonaws.com/v1
kind: HyperpodNodeClass
metadata:
&nbsp;&nbsp;name:&nbsp;multiazg6
spec:
&nbsp;&nbsp;instanceGroups:
&nbsp; &nbsp; #&nbsp;name of&nbsp;InstanceGroup in HyperPod cluster. InstanceGroup needs to pre-created
&nbsp; &nbsp; # before this step can be completed.
&nbsp; &nbsp; #&nbsp;MaxItems: 10
&nbsp;&nbsp; &nbsp;- auto-g6-az1
&nbsp;&nbsp; &nbsp;- auto-g6-4xaz2 
 
 
 Apply the configuration to your EKS cluster using kubectl: 
 
 
 kubectl apply -f nodeclass.yaml 
 
 
 Monitor the HyperpodNodeClass status to verify the Ready condition in status is set to True to ensure it was successfully created: 
 
 
 kubectl get hyperpodnodeclass multiazc5 -oyaml 
 
The SageMaker HyperPod cluster must have AutoScaling enabled and the AutoScaling status must change to InService before the HyperpodNodeClass can be applied. 
For more information and key considerations, see Autoscaling on SageMaker HyperPod EKS. 
Create NodePool 
The NodePool sets constraints on the nodes that can be created by Karpenter and the pods that can run on those nodes. The NodePool can be set to perform various actions, such as: 
 
 Define labels and taints to limit the pods that can run on nodes Karpenter creates 
 Limit node creation to certain zones, instance types, and computer architectures, and so on 
 
For more information about NodePool, refer to NodePools. SageMaker HyperPod managed Karpenter supports a limited set of well-known Kubernetes and Karpenter requirements, which we explain in this post. 
To create a NodePool, complete the following steps: 
 
 Create a YAML file named nodepool.yaml with your desired NodePool configuration. 
 
The following code is a sample configuration to create a sample NodePool. We specify the NodePool to include our ml.g6.xlarge SageMaker instance type, and we additionally specify it for one zone. Refer to NodePools for more customizations. 
 
 apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
&nbsp;name:&nbsp;gpunodepool
spec:
&nbsp;template:
&nbsp;&nbsp; spec:
&nbsp; &nbsp; &nbsp;nodeClassRef:
&nbsp;&nbsp; &nbsp; &nbsp;group: karpenter.sagemaker.amazonaws.com
&nbsp;&nbsp; &nbsp; &nbsp;kind: HyperpodNodeClass
&nbsp;&nbsp; &nbsp; &nbsp;name: multiazg6
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;expireAfter:&nbsp;Never
&nbsp;&nbsp; &nbsp; requirements:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;- key: node.kubernetes.io/instance-type
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;operator: Exists
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;- key: "node.kubernetes.io/instance-type"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;operator: In
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;values: ["ml.g6.xlarge"]
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;- key: "topology.kubernetes.io/zone"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;operator: In
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;values: ["us-west-2a"] 
 
 
 Apply the NodePool to your cluster: 
 
 
 kubectl apply -f nodepool.yaml 
 
 
 Monitor the NodePool status to ensure the Ready condition in the status is set to True: 
 
 
 kubectl get nodepool gpunodepool -oyaml 
 
This example shows how a NodePool can be used to specify the hardware (instance type) and placement (Availability Zone) for pods. 
Launch a simple workload 
The following workload runs a Kubernetes deployment where the pods in deployment are requesting for 1 CPU and 256 MB memory per replica, per pod. The pods have not been spun up yet. 
 
 kubectl apply -f https://raw.githubusercontent.com/aws/karpenter-provider-aws/refs/heads/main/examples/workloads/inflate.yaml 
 
When we apply this, we can see a deployment and a single node launch in our cluster, as shown in the following screenshot. 
 
To scale this component, use the following command: 
 
 kubectl scale deployment inflate --replicas 10 
 
Within a few minutes, we can see Karpenter add the requested nodes to the cluster. 
 
Implement advanced auto scaling for inference with KEDA and Karpenter 
To implement an end-to-end auto scaling solution on SageMaker HyperPod, you can set up Kubernetes Event-driven Autoscaling (KEDA) along with Karpenter. KEDA enables pod-level auto scaling based on a wide range of metrics, including Amazon CloudWatch metrics, Amazon Simple Queue Service (Amazon SQS) queue lengths, Prometheus queries, and resource utilization patterns. By configuring Keda ScaledObject resources to target your model deployments, KEDA can dynamically adjust the number of inference pods based on real-time demand signals. 
When integrating KEDA and Karpenter, this combination creates a powerful two-tier auto scaling architecture. As KEDA scales your pods up or down based on workload metrics, Karpenter automatically provisions or deletes nodes in response to changing resource requirements. This integration delivers optimal performance while controlling costs by making sure your cluster has precisely the right amount of compute resources available at all times. For effective implementation, consider the following key factors: 
 
 Set appropriate buffer thresholds in KEDA to accommodate Karpenter‚Äôs node provisioning time 
 Configure cooldown periods carefully to prevent scaling oscillations 
 Define clear resource requests and limits to help Karpenter make optimal node selections 
 Create specialized NodePools tailored to specific workload characteristics 
 
The following is a sample spec of a KEDA ScaledObject file that scales the number of pods based on CloudWatch metrics of Application Load Balancer (ALB) request count: 
 
 apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
&nbsp;&nbsp;name: nd-deepseek-llm-scaler
&nbsp;&nbsp;namespace: default
spec:
&nbsp;&nbsp;scaleTargetRef:
&nbsp;&nbsp; &nbsp;name: nd-deepseek-llm-r1-distill-qwen-1-5b
&nbsp;&nbsp; &nbsp;apiVersion: apps/v1
&nbsp;&nbsp; &nbsp;kind: Deployment
&nbsp;&nbsp;minReplicaCount: 1
&nbsp;&nbsp;maxReplicaCount: 3
&nbsp;&nbsp;pollingInterval: 30 &nbsp; &nbsp; # seconds between checks
&nbsp;&nbsp;cooldownPeriod: 300 &nbsp; &nbsp; # seconds before scaling down
&nbsp;&nbsp;triggers:
&nbsp;&nbsp; &nbsp;- type: aws-cloudwatch
&nbsp;&nbsp; &nbsp; &nbsp;metadata:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;namespace: AWS/ApplicationELB &nbsp; &nbsp; &nbsp; &nbsp;# or your metric namespace
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;metricName: RequestCount &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# or your metric name
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;dimensionName: LoadBalancer &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; # or your dimension key
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;dimensionValue: app/k8s-default-albnddee-cc02b67f20/0991dc457b6e8447
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;statistic: Sum
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;threshold: "3" &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# change to your desired threshold
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;minMetricValue: "0" &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; # optional floor
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;region: us-east-2 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; # your AWS region
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;identityOwner: operator &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; # use the IRSA SA bound to keda-operator 
 
Clean up 
To clean up your resources to avoid incurring more charges, delete your SageMaker HyperPod cluster. 
Conclusion 
With the launch of Karpenter node auto scaling on SageMaker HyperPod, ML workloads can automatically adapt to changing workload requirements, optimize resource utilization, and help control costs by scaling precisely when needed. You can also integrate it with event-driven pod auto scalers such as KEDA to scale based on custom metrics. 
To experience these benefits for your ML workloads, enable Karpenter in your SageMaker HyperPod clusters. For detailed implementation guidance and best practices, refer to Autoscaling on SageMaker HyperPod EKS. 
 
About the authors 
Vivek Gangasani is a Worldwide Lead GenAI Specialist Solutions Architect for SageMaker Inference. He drives Go-to-Market (GTM) and Outbound Product strategy for SageMaker Inference. He also helps enterprises and startups deploy, manage, and scale their GenAI models with SageMaker and GPUs. Currently, he is focused on developing strategies and content for optimizing inference performance and GPU efficiency for hosting Large Language Models. In his free time, Vivek enjoys hiking, watching movies, and trying different cuisines. 
Adam Stanley is a Solution Architect for Software, Internet and Model Provider customers at Amazon Web Services (AWS). He supports customers adopting all AWS services, but focuses primarily on Machine Learning training and inference infrastructure. Prior to AWS, Adam went to the University of New South Wales and graduated with degrees in Mathematics and Accounting. You can connect with him on LinkedIn. 
Kunal Jha is a Principal Product Manager at AWS, where he focuses on building Amazon SageMaker HyperPod to enable scalable distributed training and fine-tuning of foundation models. In his spare time, Kunal enjoys skiing and exploring the Pacific Northwest. You can connect with him on LinkedIn. 
Ty Bergstrom is a Software Engineer at Amazon Web Services. He works on the HyperPod Clusters platform for Amazon SageMaker.
‚Ä¢ Meet Boti: The AI assistant transforming how the citizens of Buenos Aires access government information with Amazon Bedrock
  This post is co-written with Julieta Rappan, Macarena Blasi, and Mar√≠a Candela Blanco from the Government of the City of Buenos Aires. 
The Government of the City of Buenos Aires continuously works to improve citizen services. In February 2019, it introduced an AI assistant named Boti available through WhatsApp, the most widely used messaging service in Argentina. With Boti, citizens can conveniently and quickly access a wide variety of information about the city, such as renewing a driver‚Äôs license, accessing healthcare services, and learning about cultural events. This AI assistant has become a preferred communication channel and facilitates more than 3 million conversations each month. 
As Boti grows in popularity, the Government of the City of Buenos Aires seeks to provide new conversational experiences that harness the latest developments in generative AI. One challenge that citizens often face is navigating the city‚Äôs complex bureaucratic landscape. The City Government‚Äôs website includes over 1,300 government procedures, each of which has its own logic, nuances, and exceptions. The City Government recognized that Boti could improve access to this information by directly answering citizens‚Äô questions and connecting them to the right procedure. 
To pilot this new solution, the Government of the City of Buenos Aires partnered with the AWS Generative AI Innovation Center (GenAIIC). The teams worked together to develop an agentic AI assistant using LangGraph and Amazon Bedrock. The solution includes two main components: an input guardrail system and a government procedures agent. The input guardrail uses a custom LLM classifier to analyze incoming user queries, determining whether to approve or block requests based on their content. Approved requests are handled by the government procedures agent, which retrieves relevant procedural information and generates responses. Since most user queries focus on a single procedure, we developed a novel reasoning retrieval system to improve retrieval accuracy. This system initially retrieves comparative summaries that disambiguate similar procedures and then applies a large language model (LLM) to select the most relevant results. The agent uses this information to craft responses in Boti‚Äôs characteristic style, delivering short, helpful, and expressive messages in Argentina‚Äôs Rioplatense Spanish dialect. We focused on distinctive linguistic features of this dialect including the voseo (using ‚Äúvos‚Äù instead of ‚Äút√∫‚Äù) and periphrastic future (using ‚Äúir a‚Äù before verbs). 
In this post, we dive into the implementation of the agentic AI system. We begin with an overview of the solution, explaining its design and main features. Then, we discuss the guardrail and agent subcomponents and assess their performance. Our evaluation shows that the guardrails effectively block harmful content, including offensive language, harmful opinions, prompt injection attempts, and unethical behaviors. The agent achieves up to 98.9% top-1 retrieval accuracy using the reasoning retriever, which marks a 12.5‚Äì17.5% improvement over standard retrieval-augmented generation (RAG) methods. Subject matter experts found that Boti‚Äôs responses were 98% accurate in voseo usage and 92% accurate in periphrastic future usage. The promising results of this solution establish a new era of citizen-government interaction. 
Solution overview 
The Government of the City of Buenos Aires and the GenAIIC built an agentic AI assistant using Amazon Bedrock and LangGraph that includes an input guardrail system to enable safe interactions and a government procedures agent to respond to user questions. The workflow is shown in the following diagram. 
 
The process begins when a user submits a question. In parallel, the question is passed to the input guardrail system and government procedures agent. The input guardrail system determines whether the question contains harmful content. If triggered, it stops graph execution and redirects the user to ask questions about government procedures. Otherwise, the agent continues to formulate its response. The agent either calls a retrieval tool, which allows it to obtain relevant context and metadata from government procedures stored in Amazon Bedrock Knowledge Bases, or responds to the user. Both the input guardrail and government procedures agent use the Amazon Bedrock Converse API for LLM inference. This API provides access to a wide selection of LLMs, helping us optimize performance and latency across different subtasks. 
Input guardrail system 
Input guardrails help prevent the LLM system from processing harmful content. Although Amazon Bedrock Guardrails offers one implementation approach with filters for specific words, content, or sensitive information, we developed a custom solution. This provided us greater flexibility to optimize performance for Rioplatense Spanish and monitor specific types of content. The following diagram illustrates our approach, in which an LLM classifier assigns a primary category (‚Äúapproved‚Äù or ‚Äúblocked‚Äù) as well as a more detailed subcategory. 
 
Approved queries are within the scope of the government procedures agent. They consist of on-topic requests, which focus on government procedures, and off-topic requests, which are low-risk conversation questions that the agent responds to directly. Blocked queries contain high-risk content that Boti should avoid, including offensive language, harmful opinions, prompt injection attacks, or unethical behaviors. 
We evaluated the input guardrail system on a dataset consisting of both normal and harmful user queries. The system successfully blocked 100% of harmful queries, while occasionally flagging normal queries as harmful. This performance balance makes sure that Boti can provide helpful information while maintaining safe and appropriate interactions for users. 
Agent system 
The government procedures agent is responsible for answering user questions. It determines when to retrieve relevant procedural information using its retrieval tool and generates responses in Boti‚Äôs characteristic style. In the following sections, we examine both processes. 
Reasoning retriever 
The agent can use a retrieval tool to provide accurate and up-to-date information about government procedures. Retrieval tools typically employ a RAG framework to perform semantic similarity searches between user queries and a knowledge base containing document chunks stored as embeddings, and then provide the most relevant samples as context to the LLM. Government procedures, however, present challenges to this standard approach. Related procedures, such as renewing and reprinting drivers‚Äô licenses, can be difficult to disambiguate. Additionally, each user question typically requires information from one specific procedure. The mixture of chunks returned from standard RAG approaches increases the likelihood of generating incorrect responses. 
To better disambiguate government procedures, the Buenos Aires and GenAIIC teams developed a reasoning retrieval method that uses comparative summaries and LLM selection. An overview of this approach is shown in the following diagram. 
 
A necessary preprocessing step before retrieval is the creation of a government procedures knowledge base. To capture both the key information contained in procedures and how they related to each other, we created comparative summaries. Each summary contains basic information, such as the procedure‚Äôs purpose, intended audience, and content, such as costs, steps, and requirements. We clustered the base summaries into small groups, with an average cluster size of 5, and used an LLM to generate descriptions about what made each procedure different from its neighbors. We appended the distinguishing descriptions to the base information to create the final summary. We note that this approach shares similarities to Anthropic‚Äôs Contextual Retrieval, which prepends explanatory context to document chunk. 
With the knowledge base in place, we are able to retrieve relevant government procedures based on the user query. The reasoning retriever completes three steps: 
 
 Retrieve M Summaries: We retrieve between 1 and M comparative summaries using semantic search. 
 Optional Reasoning: In some cases, the initial retrieval surfaces similar procedures. To make sure that the most relevant procedures are returned to the agent, we apply an optional LLM reasoning step. The condition for this step occurs when the ratio of the first and second retrieval scores falls below a threshold value. An LLM follows a chain-of-thought (CoT) process in which it compares the user query to the retrieved summaries. It discards irrelevant procedures and reorders the remaining ones based on relevance. If the user query is specific enough, this process typically returns one result. By applying this reasoning step selectively, we minimize latency and token usage while maintaining high retrieval accuracy. 
 Retrieve N Full-Text Procedures: After the most relevant procedures are identified, we fetch their complete documents and metadata from an Amazon DynamoDB table. The metadata contains information like the source URL and the sentiment of the procedure. The agent typically receives between 1 and N results, where N ‚â§ M. 
 
The agent receives the retrieved full text procedures in its context window. It follows its own CoT process to determine the relevant content and URL source attributions when generating its answer. 
We evaluated our reasoning retriever against standard RAG techniques using a synthetic dataset of 1,908 questions derived from known source procedures. The performance was measured by determining whether the correct procedure appeared in the top-k retrieved results for each question. The following plot compares the top-k retrieval accuracy for each approach across different models, arranged in order of ascending performance from left to right. The metrics are proportionally weighted based on each procedure‚Äôs webpage visit frequency, making sure that our evaluation reflects real-world usage patterns. 
 
The first three approaches represent standard vector-based retrieval methods. The first method, Section Titan, involved chunking procedures by document sections, targeting approximately 250 words per chunk, and then embedding the chunks using Amazon Titan Text Embeddings v2. The second method, Summaries Titan, consisted of embedding the procedure summaries using the same embedding model. By embedding summaries rather than document text, the retrieval accuracy improved by 7.8‚Äì15.8%. The third method, Summaries Cohere, involved embedding procedure summaries using Cohere Multilingual v3 on Amazon Bedrock. The Cohere Multilingual embedding model provided a noticeable improvement in retrieval accuracy compared to the Amazon Titan embedding models, with all top-k values above 90%. 
The next three approaches use the reasoning retriever. We embedded the procedure summaries using the Cohere Multilingual model, retrieved 10 summaries during the initial retrieval step, and optionally applied the LLM-based reasoning step using either Anthropic‚Äôs Haiku 3, Claude 3 Sonnet, or Claude 3.5 Sonnet on Amazon Bedrock. All three reasoning retrievers consistently outperform standard RAG techniques, achieving 12.5‚Äì17.5% higher top-k accuracies. Anthropic‚Äôs Claude 3.5 Sonnet delivered the highest performance with 98.9% top-1 accuracy. These results demonstrate how combining embedding-based retrieval with LLM-powered reasoning can improve RAG performance. 
Answer generation 
After collecting the necessary information, the agent responds using Boti‚Äôs distinctive communication style: concise, helpful messages in Rioplatense Spanish. We maintained this voice through prompt engineering that specified the following: 
 
 Personality ‚Äì Convey a warm and friendly tone, providing quick solutions to everyday problems 
 Response length ‚Äì Limit responses to a few sentences 
 Structure ‚Äì Organize content using lists and highlights key information using bold text 
 Expression ‚Äì Use emojis to mark important requirements and add visual cues 
 Dialect ‚Äì Incorporate Rioplatense linguistic features, including voseo, periphrastic future, and regional vocabulary (for example, ‚Äúacordate,‚Äù ‚Äúentrar,‚Äù ‚Äúac√°,‚Äù and ‚Äúall√°‚Äù). 
 
Government procedures often address sensitive topics, like accidents, health, or security. To facilitate appropriate responses, we incorporated sentiment analysis into our knowledge base as metadata. This allows our system to route to different prompt templates. Sensitive topics are directed to prompts with reduced emoji usage and more empathetic language, whereas neutral topics receive standard templates. 
The following figure shows a sample response to a question about borrowing library books. It has been translated to English for convenience. 
 
To validate our prompt engineering approach, subject matter experts at the Government of the City of Buenos Aires reviewed a sample of Boti‚Äôs responses. Their analysis confirmed high fidelity to Rioplatense Spanish, with 98% accuracy in voseo usage and 92% in periphrastic future usage. 
Conclusion 
This post described the agentic AI assistant built by the Government of the City of Buenos Aires and the GenAIIC to respond to citizens‚Äô questions about government procedures. The solution consists of two primary components: an input guardrail system that helps prevent the system from responding to harmful user queries and a government procedures agent that retrieves relevant information and generates responses. The input guardrails effectively block harmful content, including queries with offensive language, harmful opinions, prompt injection, and unethical behaviors. The government procedures agent employs a novel reasoning retrieval method that disambiguates similar government procedures, achieving up to 98.9% top-1 retrieval accuracy and a 12.5‚Äì17.5% improvement over standard RAG methods. Through prompt engineering, responses are delivered in Rioplatense Spanish using Boti‚Äôs voice. Subject matter experts rated Boti‚Äôs linguistic performance highly, with 98% accuracy in voseo usage and 92% in periphrastic future usage. 
As generative AI advances, we expect to continuously improve our solution. The expanding catalog of LLMs available in Amazon Bedrock makes it possible to experiment with newer, more powerful models. This includes models that process text, as explored in the solution in this post, as well as models that process speech, allowing for direct speech-to-speech interactions. We might also explore the fine-tuning capabilities of Amazon Bedrock to customize models so that they better capture the linguistic features of Rioplatense Spanish. Beyond model improvements, we can iterate on our agent framework. The agent‚Äôs tool set can be expanded to support other tasks associated with government procedures like account creation, form completion, and appointment scheduling. As the City Government develops new experiences for citizens, we can consider implementing multi-agent frameworks in which specialist agents, like the government procedures agent, handle specific tasks. 
To learn more about Boti and AWS‚Äôs generative AI capabilities, check out the following resources: 
 
 Boti: The City Chatbot 
 Government of the City of Buenos Aires: Procedures 
 Amazon Bedrock 
 Amazon Bedrock Knowledge Bases 
 
 
 
About the authors 
Julieta Rappan is Director of the Digital Channels Department of the Buenos Aires City Government, where she coordinates the landscape of digital and conversational interfaces. She has extensive experience in the comprehensive management of strategic and technological projects, as well as in leading high-performance teams focused on the development of digital products and services. Her leadership drives the implementation of technological solutions with a focus on scalability, coherence, public value, and innovation‚Äîwhere generative technologies are beginning to play a central role. 
Macarena Blasi is Chief of Staff at the Digital Channels Department of the Buenos Aires City Government, working across the city‚Äôs main digital services, including Boti‚Äîthe WhatsApp-based virtual assistant‚Äîand the official Buenos Aires website. She began her journey working in conversational experience design, later serving as product owner and Operations Manager and then as Head of Experience and Content, leading multidisciplinary teams focused on improving the quality, accessibility, and usability of public digital services. Her work is driven by a commitment to building clear, inclusive, and human-centered experiences in the public sector. 
Mar√≠a Candela Blanco is Operations Manager for Quality Assurance, Usability, and Continuous Improvement at the Buenos Aires Government, where she leads the content, research, and conversational strategy across the city‚Äôs main digital channels, including the Boti AI assistant and the official Buenos Aires website. Outside of tech, Candela studies literature at UNSAM and is deeply passionate about language, storytelling, and the ways they shape our interactions with technology. 
Leandro Micchele is a Software Developer focused on applying AI to real-world use cases, with expertise in AI assistants, voice, and vision solutions. He serves as the technical lead and consultant for the Boti AI assistant at the Buenos Aires Government and works as a Software Developer at Telecom Argentina. Beyond tech, his discipline extends to martial arts: he has over 20 years of experience and currently teaches Aikido. 
Hugo Albuquerque is a Deep Learning Architect at the AWS Generative AI Innovation Center. Before joining AWS, Hugo had extensive experience working as a data scientist in the media and entertainment and marketing sectors. In his free time, he enjoys learning other languages like German and practicing social dancing, such as Brazilian Zouk. 
Enrique Balp is a Senior Data Scientist at the AWS Generative AI Innovation Center working on cutting-edge AI solutions. With a background in the physics of complex systems focused on neuroscience, he has applied data science and machine learning across healthcare, energy, and finance for over a decade. He enjoys hikes in nature, meditation retreats, and deep friendships. 
Diego Galaviz is a Deep Learning Architect at the AWS Generative AI Innovation Center. Before joining AWS, he had over 8 years of expertise as a data scientist across diverse sectors, including financial services, energy, big tech, and cybersecurity. He holds a master‚Äôs degree in artificial intelligence, which complements his practical industry experience. 
Laura Kulowski is a Senior Applied Scientist at the AWS Generative AI Innovation Center, where she works with customers to build generative AI solutions. Before joining Amazon, Laura completed her PhD at Harvard‚Äôs Department of Earth and Planetary Sciences and investigated Jupiter‚Äôs deep zonal flows and magnetic field using Juno data. 
Rafael Fernandes is the LATAM leader of the AWS Generative AI Innovation Center, whose mission is to accelerate the development and implementation of generative AI in the region. Before joining Amazon, Rafael was a co-founder in the financial services industry space and a data science leader with over 12 years of experience in Europe and LATAM.
‚Ä¢ Empowering air quality research with secure, ML-driven predictive analytics
  Air pollution remains one of Africa‚Äôs most pressing environmental health crises, causing widespread illness across the continent. Organizations like sensors.AFRICA have deployed hundreds of air quality sensors to address this challenge, but face a critical data problem: significant gaps in PM2.5 (particulate matter with diameter less than or equal to 2.5 micrometers) measurement records because of power instability and connectivity issues in high-risk regions where physical maintenance is limited. Missing data in PM2.5 datasets reduces statistical power and introduces bias into parameter estimates, leading to unreliable trend detection and flawed conclusions about air quality patterns. These data gaps ultimately compromise evidence-based decision-making for pollution control strategies, health impact assessments, and regulatory compliance. 
In this post, we demonstrate&nbsp;the time-series forecasting capability of&nbsp;Amazon SageMaker Canvas,&nbsp;a&nbsp;low-code no-code (LCNC) machine learning (ML) platform to predict PM2.5 from incomplete datasets. PM2.5 exposure contributes to millions of premature deaths globally through cardiovascular disease, respiratory illness, and systemic health effects, making accurate air quality forecasting a critical public health tool. A key advantage of the forecasting capability of SageMaker Canvas is its robust handling of incomplete data. Traditional air quality monitoring systems often require complete datasets to function properly, meaning they can‚Äôt be relied on when sensors malfunction or require maintenance. In contrast, SageMaker Canvas can generate reliable predictions even when faced with gaps in sensor data. This resilience enables continuous operation of air quality monitoring networks despite inevitable sensor failures or maintenance periods, eliminating costly downtime and data gaps. Environmental agencies and public health officials benefit from uninterrupted access to critical air quality information, enabling timely pollution alerts and more comprehensive long-term analysis of air quality trends. By maintaining operational continuity even with imperfect data inputs, SageMaker Canvas significantly enhances the reliability and practical utility of environmental monitoring systems. 
In this post, we provide a data imputation solution using Amazon SageMaker AI, AWS Lambda, and AWS Step Functions. This solution is designed for environmental analysts, public health officials, and business intelligence professionals who need reliable PM2.5 data for trend analysis, reporting, and decision-making. We sourced our sample training dataset from openAFRICA. Our solution predicts PM2.5 values using time-series forecasting. The sample training dataset contained over 15 million records from March 2022 to Oct 2022 in various parts of Kenya and Nigeria‚Äîdata coming from 23 sensor devices from 15 unique locations. The sample code and workflows can be adapted to create prediction models for your PM2.5 datasets. See our solution‚Äôs README for detailed instructions. 
Solution overview 
The solution consists of two main ML components: a training workflow and an inference workflow. These workflows are built using the following services: 
 
 SageMaker Canvas is used to prepare data and train the prediction model through its no-code interface 
 Batch Transform for inference with Amazon SageMaker AI&nbsp;is used for inference, processing the dataset in bulk to generate predictions 
 Step Functions orchestrates the inferencing process by coordinating the workflow between data retrieval, batch transforming, and database updates, managing workflow state transitions, and making sure that data flows properly through each processing stage 
 Lambda functions perform critical operations at each workflow step: retrieving sensor data from the database in required format, transforming data for model input, sending batches to SageMaker for inferencing, and updating the database with prediction results after processing is complete 
 
At a high level, the solution works by taking a set of PM2.5 data with gaps and predicts the missing values within the range of plus or minus 4.875 micrograms per cubic meter of the actual PM2.5 concentration. It does this by first training a model on the data using inputs for the specific schema and a historical set of values from the user to guide the training process, which is completed with SageMaker Canvas. After the model is trained on a representative dataset and schema, SageMaker Canvas exports the model for use with batch processing. The Step Functions orchestration calls a Lambda function every 24 hours that takes a dataset of new sensor data that has gaps and initiates a SageMaker batch transform job to predict the missing values. The batch transform job processes the entire dataset at once, and the Lambda function then updates the existing dataset with the results. The new completed dataset with predicted values can now be distributed to&nbsp;public health decision-makers who need complete datasets to effectively analyze the patterns of PM2.5 data. 
We dive into each of these steps in later sections of this post. 
Solution walkthrough 
The following diagram shows our solution architecture: 
 
Let‚Äôs explore the architecture step by step: 
 
 To&nbsp;systematically collect, identify, and fill PM2.5 data gaps caused by sensor limitations and connectivity issues,&nbsp;Amazon EventBridge Scheduler invokes a Step Functions state machine every 24 hours.&nbsp;Step Functions orchestrates the calling of various Lambda functions to perform different steps without handling the complexities of error handling, retries, and state management, providing a serverless workflow that seamlessly coordinates the PM2.5 data imputation process. 
 The State Machine invokes a Lambda function in your&nbsp;Amazon Virtual Private Cloud (Amazon VPC) that retrieves records containing missing air quality values from the user‚Äôs air quality database on Amazon Aurora PostgreSQL-Compatible Edition and stores the records in a CSV file in an Amazon Simple Storage Service (Amazon S3) bucket. 
 The State Machine then runs a Lambda function that retrieves the records from Amazon S3 and initiates the SageMaker batch transform job in your VPC using your SageMaker model created from your SageMaker Canvas predictive model trained on historical PM2.5 data. 
 To streamline the batch transform workflow, this solution uses an event-driven approach with EventBridge and Step Functions. EventBridge captures completion events from SageMaker batch transform jobs, while the task token functionality of Step Functions enables extended waiting periods beyond the time limits of Lambda. After processing completes, SageMaker writes the prediction results directly to an S3 bucket. 
 The final step in the state machine retrieves the predicted values from the S3 bucket and then updates the database in&nbsp;Aurora PostgreSQL-Compatible with the values including a predicted label set to true. 
 
Prerequisites 
To implement the PM2.5 data imputation solution, you must have the following: 
 
 An AWS account with&nbsp;AWS Identity and Access Management (IAM)&nbsp;permissions sufficient to deploy the solution and interact with the database. 
 The following AWS services: 
   
   Amazon SageMaker AI 
   AWS Lambda 
   AWS Step Functions 
   Amazon S3 
   Aurora&nbsp;PostgreSQL-Compatible 
   Amazon CloudWatch 
   AWS CloudFormation 
   Amazon Virtual Private Cloud (VPC) 
   Amazon EventBridge 
   IAM for authentication to&nbsp;Aurora&nbsp;PostgreSQL-Compatible 
   AWS Systems Manager Parameter Store 
    
 A local desktop set up with AWS Command Line Interface (AWS CLI) version 2, Python 3.10, AWS Cloud Development Kit (AWS CDK) v2.x, and Git version 2.x. 
 The AWS CLI set up with the necessary credentials in the desired AWS Region. 
 Historical air quality sensor data. Note that our solution requires a fixed schema described in the GitHub repo‚Äôs README. 
 
Deploy the solution 
You will run the following steps to complete the deployment: 
 
 Prepare your environment by building Python modules locally for Lambda layers, deploying infrastructure using the AWS CDK, and initializing your Aurora PostgreSQL database with sensor data. 
 Perform steps in the Build your air quality prediction model section to configure a SageMaker Canvas application, followed by training and registering your model in Amazon SageMaker Model Registry. 
 Create SageMaker model using your registered SageMaker Canvas model by updating infrastructure using the AWS CDK. 
 Manage future configuration changes using the AWS CDK. 
 
Step 1: Deploy AWS infrastructure and upload air quality sensor data 
Complete the following steps to deploy the PM2.5 data imputation solution AWS Infrastructure and upload air quality sensor data to Amazon Aurora RDS: 
 
 Clone the repository to your local desktop environment using the following command: 
 
 
 git clone git@github.com:aws-samples/sample-empowering-air-quality-research-secure-machine-learning-predictive-analytics.git 
 
 
 Change to the project directory: 
 
cd &lt;BASE_PROJECT_FOLDER&gt; 
 
 Follow the deployment steps in the README&nbsp;file up to Model Setup for Batch Transform Inference. 
 
Step 2: Build your air quality prediction model 
After you create the SageMaker AI domain and the SageMaker AI user profile as part of the CDK deployment steps, follow these steps to build your air quality prediction model 
Configure your SageMaker Canvas application 
 
 On the AWS Management Console, go to the&nbsp;SageMaker AI&nbsp;console and select the domain and the user profile that was created under Admin, Configurations, and Domains. 
 Choose the App Configurations tab, scroll down to the Canvas section, and select Edit. 
 In Canvas storage configuration, select Encryption and select the dropdown for aws/s3. 
 In the ML Ops Configuration, turn on the option to Enable Model Registry registration permissions for this user profile. 
   
   Optionally, in the Local file upload configuration&nbsp;section in your domain‚Äôs Canvas App Configuration, you can turn on Enable local file upload. 
    
 Choose Submit to save your configuration choices. 
 In your Amazon SageMaker AI home page, go to the Applications and IDEs section and select Canvas. 
 Select the SageMaker AI user profile that was created for you by the CDK deployment and choose Open Canvas. 
 In a new tab, SageMaker Canvas will start creating your application. This takes a few minutes. 
 
Create and register your prediction model 
In this phase, you develop a prediction model using your historical air quality sensor data. 
 
The preceding architecture diagram illustrates the end-to-end process for training the SageMaker Canvas prediction model, registering that model and creating a SageMaker model for running inference on newly found PM2.5 data gaps.&nbsp;The training process starts by extracting air quality sensor dataset from the database. The dataset is imported into SageMaker Canvas for predictive analysis. This training dataset is transformed and prepared through data wrangling steps implemented by SageMaker Canvas for building and training ML models. 
Prepare data 
Our solution supports a SageMaker Canvas model trained for a single-target variable prediction based on historical data and performs corresponding data imputation for PM2.5 data gaps. To train your model for predictive analysis, follow the comprehensive End to End Machine Learning workflow in the AWS Canvas Immersion Day workshop, adapting each step to prepare your air quality sensor dataset. Begin with the standard workflow until you reach the data preparation section. Here, you can make several customizations: 
 
 Filter dataset for single-target value prediction: Your air quality dataset might contain multiple sensor parameters. For single-target value prediction using this solution, filter the dataset to include only PM2.5 measurements. 
 Clean sensor data: Remove records containing sensor fault values. For example, we filtered out values that equal 65535, because 65535 is a common error code for malfunctioning sensors. Adjust this filtering based on the specific error codes your air quality monitoring equipment produces. 
 
The following image shows our data wrangling Data Flow implemented using above guidance: 
Data Wrangler &gt; Data Flow 
 
 
 Review generated insights and remove irrelevant data: Review the SageMaker Canvas generated insights and analyses. Evaluate them based on time-series forecasting and geospatial temporal data for air quality patterns and relationships between other columns of impact. See chosen columns of impact in GitHub for guidance. Analyze your dataset to identify rows and columns that impact the prediction and remove data that can reduce prediction accuracy. 
 
The following image shows our data wrangling&nbsp;Analyses obtained with implementing the above guidance: 
Data Wrangler &gt; Analyses 
 
Training your prediction model 
After completing your data preparation, proceed to the Train the Model section of the workshop and continue with these specifications: 
 
 Select problem type: Select Predictive Analysis as your ML approach. Because our dataset is tabular and contains a timestamp, a target column that has values we‚Äôre using to forecast future values, and a device ID column, SageMaker Canvas will choose time series forecasting. 
 Define target column: Set Value as your target column for predicting PM2.5 values. 
 Build configuration: Use the Standard Build option for model training because it generally has a higher accuracy. See What happens when you build a model in How custom models work&nbsp;for more information. 
 
By following these steps, you can create a model optimized for PM2.5 dataset predictive analysis, capable of generating valuable insights. Note that SageMaker Canvas supports retraining the ML model for updated PM2.5 datasets. 
Evaluate the model 
After training your model, proceed to Evaluate the model and review column impact, root mean square error (RMSE) score and other advanced metrics to understand your model‚Äôs performance for generating predictions for PM2.5. 
The following image shows our model evaluation statistics achieved. 
 
Add the model to the registry 
Once you are satisfied with your model performance, follow the steps in Register a model version to the SageMaker AI model registry. Make sure to change the approval status to Approved before continuing to run this solution. At the time of this post‚Äôs publication, the approval must be updated in Amazon SageMaker Studio. 
Log out of SageMaker Canvas 
After completing your work in SageMaker Canvas, you can log out or configure your application to automatically terminate the&nbsp;workspace instance. A workspace instance is dedicated for your use every time you launch a Canvas application, and you are billed for as long as the instance runs. Logging out or terminating the workspace instance stops the workspace instance billing. For more information, see billing and cost in SageMaker Canvas. 
Step 3: Create a SageMaker model using your registered SageMaker Canvas model 
In the previous steps, you created a SageMaker domain and user profile through CDK deployment (Step 1) and successfully registered your model (Step 2). Now, it‚Äôs time to create the SageMaker model in your VPC using the SageMaker Canvas model you registered. Follow Model Setup for Batch Inference and Re-Deploy with Updated Configuration sections in the code README for creating SageMaker model. 
Step 4: Manage future configuration changes 
The same deployment pattern applies to any future configuration modifications you might require, including: 
 
 Batch transform instance type optimizations 
 Transform job scheduling changes 
 
Update the relevant parameters in your configuration and run cdk deploy to propagate these changes throughout your solution architecture. 
For a comprehensive list of configurable parameters and their default values, see the configuration file in the repository. 
Execute cdk deploy again to update your infrastructure stack with the your model ID for batch transform operations, replacing the placeholder value initially deployed. This infrastructure-as-code approach helps ensure consistent, version-controlled updates to your data imputation workflow. 
Security best practices 
Security and compliance is a shared responsibility between AWS and the customer, as outlined in the Shared Responsibility Model. We encourage you to review this model for a comprehensive understanding of the respective responsibilities. 
In this solution, we enhanced security by implementing encryption at rest for Amazon S3, Aurora PostgreSQL-Compatible database, and the SageMaker Canvas application. We also enabled encryption in transit by requiring SSL/TLS for all connections from the Lambda functions. We implemented secure database access by providing temporary dynamic credentials through IAM authentication for Amazon RDS, eliminating the need for static passwords. Each Lambda function operates with least privilege access, receiving only the minimal permissions required for its specific function. Finally, we deployed the Lambda functions, Aurora PostgreSQL-Compatible instance, and SageMaker Batch Transform jobs in private subnets of the VPC that do not traverse the public internet. This private network architecture is enabled through VPC endpoints for Amazon S3, SageMaker AI, and&nbsp;AWS Secrets Manager. 
Results 
As shown in the following image, our model, developed using SageMaker Canvas, predicts PM2.5 values with an R-squared of 0.921. Because ML models for PM2.5 prediction frequently achieve R-squared values between 0.80 and 0.98 (see this example from ScienceDirect), our solution is within the range of higher-performing PM2.5 prediction models available today. SageMaker Canvas delivers this performance through its no-code experience, automatically handling model training and optimization without requiring ML expertise from users. 
 
Clean up 
Complete the following steps to clean up your resources: 
 
 SageMaker Canvas application cleanup: 
   
   On the go to the&nbsp;SageMaker AI&nbsp;console and select the domain that was created under Admin&nbsp;Configurations, and Domains. 
   Select the user created under User Profiles&nbsp;for that domain. 
   On the User Details page, navigate to&nbsp;Spaces and Apps, and choose Delete to manually delete your SageMaker AI canvas application and clean up resources. 
    
 SageMaker Domain EFS storage cleanup: 
   
   Open Amazon EFS&nbsp;and in File systems, delete filesystem tagged as ManagedByAmazonSageMakerResource. 
   Open VPC and under Security, navigate to Security groups. 
   On Security groups, select security-group-for-inbound-nfs-&lt;your-sagemaker-domain-id&gt; and delete all Inbound rules associated with that group. 
   On Security groups, select security-group-for-outbound-nfs-&lt;your-sagemaker-domain-id&gt; and delete all associated Outbound rules. 
   Finally, delete both the security groups:&nbsp;security-group-for-inbound-nfs-&lt;your-sagemaker-domain-id&gt; and&nbsp;security-group-for-outbound-nfs-&lt;your-sagemaker-domain-id&gt;. 
    
 Use the AWS CDK to clean up the remaining AWS resources: 
   
   After the preceding steps are complete, return to your local desktop environment where the GitHub repo was cloned, and change to the project‚Äôs infra directory: cd &lt;BASE_PROJECT_FOLDER&gt;/infra 
   Destroy the resources created with AWS CloudFormation using the AWS CDK: cdk destroy 
   Monitor the AWS CDK process deleting resources created by the solution. If there are any errors, troubleshoot using the CloudFormation console and then retry deletion. 
    
 
Conclusion 
The development of accurate PM2.5 prediction models has traditionally required extensive technical expertise, presenting significant challenges for public health researchers studying air pollution‚Äôs impact on disease outcomes. From data preprocessing and feature engineering to model selection and hyperparameter tuning, these technical requirements diverted substantial time and effort away from researchers‚Äô core work of analyzing health outcomes and developing evidence-based interventions.SageMaker Canvas transforms this landscape by dramatically reducing the effort required to develop high-performing PM2.5 prediction models. Public health researchers can now generate accurate predictions without mastering complex ML algorithms, iterate quickly through an intuitive interface, and validate models across regions without manual hyperparameter tuning. With this shift to streamlined, accessible prediction capabilities, researchers can dedicate more time to interpreting results, understanding air pollution‚Äôs impact on community health, and developing protective interventions for vulnerable populations. The result is more efficient research that responds quickly to emerging air quality challenges and informs timely public health decisions. We invite you to implement this solution for your air quality research or ML-based predictive analytics projects. Our comprehensive deployment steps and customization guidance will help you launch quickly and efficiently. As we continue enhancing this solution, your feedback is invaluable for improving its capabilities and maximizing its impact. 
 
About the authors 
Nehal Sangoi is a Senior Technical Account Manager at Amazon Web Services. She provides strategic technical guidance to help independent software vendors plan and build solutions using AWS best practices. Connect with Nehal on LinkedIn. 
Ben Peterson is a Senior Technical Account Manager with AWS. He is passionate about enhancing the developer experience and driving customer success. In his role, he provides strategic guidance on using the comprehensive AWS suite of services to modernize legacy systems, optimize performance, and unlock new capabilities. Connect with Ben on LinkedIn. 
Shashank Shrivastava is a Senior Delivery Consultant and Serverless TFC member at AWS. He is passionate about helping customers and developers build modern applications on serverless architecture. As a pragmatic developer and blogger, he promotes community-driven learning and sharing of technology. His interests are software architecture, developer tools, GenAI, and serverless computing. Connect with Shashank on LinkedIn. 
Akshay Singhal is a Senior Technical Account Manager at Amazon Web Services supporting Enterprise Support customers focusing on the Security ISV segment. He provides technical guidance for customers to implement AWS solutions, with expertise spanning serverless architectures and cost optimization. Outside of work, Akshay enjoys traveling, Formula 1, making short movies, and exploring new cuisines. Connect with Akshay on LinkedIn.

‚∏ª