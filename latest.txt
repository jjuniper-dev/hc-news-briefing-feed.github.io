‚úÖ Morning News Briefing ‚Äì June 28, 2025

üìÖ Date: 2025-06-28
üè∑Ô∏è Tags: #briefing #ai #publichealth #digitalgov

‚∏ª

üßæ Weather
‚Ä¢ No watches or warnings in effect, Pembroke
  
‚Ä¢ Current Conditions: Fog, 18.7¬∞C
  Observed at Garrison Petawawa 7:00 AM EDT Saturday 28
‚Ä¢ Saturday: Chance of showers. High 24. POP 40%
  Cloudy with 40 percent chance of showers . Risk of a thunderstorm this morning
‚Ä¢ Saturday night: Overcast. Low 15.
  Fog patches developing after midnight
‚Ä¢ Sunday: Clearing. High 30.
  Fog patches dissipating in the morning . Clearing near noon . High 30.
‚Ä¢ Sunday night: Clear. Low 16.
  Clear. Clear
‚Ä¢ Monday: Cloudy. High 29.
  Cloudy.
‚Ä¢ Monday night: Chance of showers. Low 17. POP 40%
  Cloudy with 40 percent chance
‚Ä¢ Tuesday: Chance of showers. High 27. POP 60%
  A mix of sun and cloud with 60
‚Ä¢ Tuesday night: Chance of showers. Low 14. POP 30%
  Cloudy periods with 30 percent
‚Ä¢ Wednesday: A mix of sun and cloud. High 27.
  A mix of sun and
‚Ä¢ Wednesday night: Cloudy periods. Low 13.
  Cloudy periods
‚Ä¢ Thursday: A mix of sun and cloud. High 28.
  A mix of sun and
‚Ä¢ Thursday night: Chance of showers. Low 15. POP 30%
  Cloudy periods with 30 percent
‚Ä¢ Friday: A mix of sun and cloud. High 28.
  A mix of sun and

üåç International & Canadian News
No updates.

üá∫üá∏ U.S. Top Stories
‚Ä¢ Hard to imagine a worse time to deport Afghan refugees, human rights advocates say
  There were 71,000 deportations in the first half of June
‚Ä¢ 'Where's our money?' CDC grant funding is moving so slowly layoffs are happening
  The state and local health departments that rely on CDC funding say the money
‚Ä¢ Defense Department will stop providing crucial satellite weather data
  Hurricane forecasters rely on weather data collected and processed by Department
‚Ä¢ Israel's leaders slam a news report on a Gaza 'killing field' near food sites
  Israel's prime minister denounced a report in Israeli newspaper Haaretz
‚Ä¢ In an about-face, Germany plans to build Europe's biggest military
  A new Trump presidency and the Ukraine war have pushed Germany to change its constitution
‚Ä¢ Slain Minnesota lawmaker Melissa Hortman to be laid to rest in private funeral service
  Minnesota lawmaker was killed during a string
‚Ä¢ Big beautiful bottleneck: Trump's high-stakes week at home and abroad
  What happens over the next 10 days could come to define Trump
‚Ä¢ Where the Voting Rights Act stands after the Supreme Court punts on a Louisiana case
  The Supreme Court delayed ruling on a Louisiana congressional redistricting case that
‚Ä¢ Looking forward to July 4 fireworks? Tariffs may take the spark out of your fun
  Most fireworks are made in China and importers now have to pay tariffs
‚Ä¢ What is birthright citizenship and what happens after the Supreme Court ruling?
  A Supreme Court ruling limits the ability of federal courts to impose universal injunctions

üß† Artificial Intelligence & Digital Strategy
‚Ä¢ How to get free software from yesteryear's IT crowd ‚Äì trick code into thinking it's running on a rival PC
  'This is not a copyright message' Windows 95 engineers were grappling with the technology when they found some unusual text
‚Ä¢ Anthropic chucks chump change at studies on job-killing tech
  Anthropic offers $10K‚Äì$50K grants to
‚Ä¢ Crims are posing as insurance companies to steal health records and payment info
  Criminals are tricking patients and healthcare providers into handing over medical records and bank
‚Ä¢ Supremes uphold Texas law that forces age-check before viewing adult material
  US Supreme Court has ruled that Texas' age certification law for viewing sexually
‚Ä¢ How Broadcom is quietly plotting a takeover of the AI infrastructure market
  GPUs dominate the conversation when it comes to AI infrastructure . While they're an essential piece of the puzzle, it's the inter
‚Ä¢ Uncle Sam wants you ‚Äì to use memory-safe programming languages
  'Memory vulnerabilities pose serious risks to national security and critical infrastructure,' say CISA and
‚Ä¢ Fed chair Powell says AI is coming for your job
  Jerome Powell: AI will make 'significant changes' to economy, labor market . US Federal
‚Ä¢ Palantir jumps aboard tech-nuclear bandwagon with software deal
  AI boom needs power, and startup The Nuclear Company aims to help build Palantir . Data analytics software will help build nuclear plants faster
‚Ä¢ Mars Reconnaissance Orbiter learns new trick at the age of 19: ‚Äòvery large rolls‚Äô
  NASA's 19-year-old Mars Reconnaissance Orbiter has been busy teaching an old spacecraft new tricks .
‚Ä¢ Cisco punts network-security integration as key for agentic AI
  Cisco is talking up integration of security into network infrastructure such as its latest Catalyst switches . Getting it in
‚Ä¢ Aloha, you‚Äôve been pwned: Hawaiian Airlines discloses ‚Äòcybersecurity event‚Äô
  Hawaiian Airlines said a "cybersecurity incident" affected some IT systems, but flights are operating as
‚Ä¢ US Department of Defense will stop sending critical hurricane satellite data
  Satellite data used for hurricane forecasting is to be abruptly cut off from the end of
‚Ä¢ So you CAN turn an entire car into a video game controller
  Pen Test Partners hijack data from Renault Clio to steer, brake, and
‚Ä¢ Before the megabit: A trip through vintage datacenter networking
  Datacenter networking is crammed with exotic technology and capabilities beyond the imaginings of administrators charged with running big iron
‚Ä¢ Data spill in aisle 5: Grocery giant Ahold Delhaize says 2.2M affected after cyberattack
  Ahold Delhaize says upwards of 2.2 million people had their
‚Ä¢ There's no international protocol on what to do if an asteroid strikes Earth
  UK lawmakers learn there is no international protocol for making decisions over how
‚Ä¢ The network is indeed trying to become the computer
  Moore's Law has run out of gas and AI workloads need massive amounts of parallel compute and high bandwidth memory right next to it . AI
‚Ä¢ The year of the European Union Linux desktop may finally arrive
  Microsoft has extended Windows 10 support for another year . Microsoft tactically admits it has failed to
‚Ä¢ Fresh UK postcode tool points out best mobile network in your area
  UK's telecoms regulator releases overhauled tool comparing mobile coverage across the country . Pick a provider based
‚Ä¢ Don't shoot me, I'm only the system administrator!
  On Call celebrates the frolicsome fun readers have experienced when asked to deliver tech support
‚Ä¢ HPE customers on agentic AI: No, you go first
  HPE envisions a future where customer systems are filled with its agentic AI products . But reactions from the HPE Discover show floor
‚Ä¢ Starlink helps eight more nations pass 50 percent IPv6 adoption
  Eight more nations have passed at least 50 percent IPv6 deployment
‚Ä¢ Australia not banning kids from YouTube ‚Äì they‚Äôll just have to use mum and dad‚Äôs logins
  Regulator
‚Ä¢ More trouble for authors as Meta wins Llama drama AI scraping case
  Authors are having a hard time protecting their works from the maws of the LLM makers . Californian
‚Ä¢ Back in black: Microsoft Blue Screen of Death is going dark
  Windows Blue Screen of Death (BSOD) will be replaced with a
‚Ä¢ FBI used bitcoin wallet records to peg notorious IntelBroker as UK national
  IntelBroker allegedly broke into computer systems belonging to more than 40 victims worldwide and stole their data, court documents say . The
‚Ä¢ What if Microsoft just turned you off? Security pro counts the cost of dependency
  Czech researcher lays out a business case for reducing reliance on Microsoft . The viewpoint unlikely to win favor
‚Ä¢ Microsoft nuke power deal for Three Mile Island appears to be ahead of schedule
  Three Mile Island nuclear plant could be fueling Microsoft's AI datacent
‚Ä¢ Cisco fixes two critical make-me-root bugs on Identity Services Engine components
  Cisco has dropped patches for a pair of critical vulnerabilities that could allow un
‚Ä¢ Exif marks the spot as fresh version of PNG image standard arrives
  22 years on from the last spec, you can
‚Ä¢ The SmartNIC revolution fell flat, but AI might change that
  The idea of handing off networking chores to DPUs persists even if it hasn't caught on beyond hypers
‚Ä¢ Gridlocked: AI's power needs could short-circuit US infrastructure
  Power required by AI datacenters in the US may be more than 30
‚Ä¢ NICER science not so nice as ISS telescope pauses operations
  NASA's NICER X-ray telescope is pausing operations just weeks after
‚Ä¢ Kaseya CEO: Why AI adoption is below industry expectations
  Adoption of generative AI for enterprise customers isn't taking off in the manner many in the industry
‚Ä¢ Glasgow City Council online services crippled following cyberattack
  Glasgow City Council is under the assumption that data has been stolen .
‚Ä¢ Qilin ransomware attack on NHS supplier contributed to patient fatality
  Pathology outage caused by Synnovis breach linked to harm across dozens
‚Ä¢ OpenDylan sheds some parentheses in 2025.1 update
  FOSS OpenDylan is a Lisp without all
‚Ä¢ UK to buy nuclear-capable F-35As that can't be refueled from RAF tankers
  Aircraft meant to bolster NATO deterrent will rely on allied support to stay airborne . UK government to buy 12 F
‚Ä¢ Frozen foods supermarket chain deploys facial recognition tech
  UK supermarket chain says it's deploying the cameras to cut down on crime . CEO responds: 'It'll cut
‚Ä¢ Top AI models - even American ones - parrot Chinese propaganda, report finds
  Chinese AI models show signs of bias toward viewpoints promoted by the Chinese Communist Party
‚Ä¢ That WhatsApp from an Israeli infosec expert could be a Iranian phish
  The cyber-ops arm of Iran's Islamic Revolutionary Guard Corps has started a spear-
‚Ä¢ French city of Lyon ditching Microsoft for open source office and collab tools
  French city of Lyon has decided to ditch Microsoft‚Äôs
‚Ä¢ Japanese company using mee-AI-ow to detect stressed cats
  Rabo‚Äôs ‚ÄòCatlog‚Äô smart collar sniffs for freaked-
‚Ä¢ AFRINIC election annulled after ICANN writes angry letter to African regional internet registry
  The group in charge of IP addresses for 54 countries hasn't had a board since 2022 . ICANN
‚Ä¢ Intel totals automotive group
  Intel is shuttering its automotive efforts
‚Ä¢ Visiting students can't hide social media accounts from Uncle Sam anymore
  US State Department last week said foreign nationals seeking to study in the US must make their social media profiles
‚Ä¢ Citrix bleeds again: This time a zero-day exploited - patch now
  Citrix issued emergency patch for yet another super-serious flaw in the same products . Citrix-owned Netscaler ADC and Net
‚Ä¢ Rack-scale networks are the new hotness for massive AI training and inference workloads
  Rise of rack-scale architectures from the likes of Nvidia, AMD, and soon Intel has introduced a new
‚Ä¢ Amazon's Ring can now use AI to 'learn the routines of your residence'
  Ring doorbells and cameras are using AI to "learn the routines of
‚Ä¢ Cosmoe: New C++ toolkit for building native Wayland apps
  Cosmoe is a modern C++ UI library, but it's also a

üè• Public Health & Science
‚Ä¢ Author Correction: Incorporating genetic data improves target trial emulations and informs the use of polygenic scores in randomized controlled trial design
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots for next week's gallery
‚Ä¢ AI-supported versus manual microscopy of Kato-Katz smears for diagnosis of soil-transmitted helminth infections in a primary healthcare setting
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots for next week's gallery
‚Ä¢ RFK Jr‚Äôs vaccine advisers vote down flu-shot ingredient ‚Äî but back some jabs
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots for next week's gallery
‚Ä¢ Missile defence won‚Äôt prevent the health crises that rock global security
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots for next week's gallery
‚Ä¢ The economic effects of federal cuts to US science ‚Äî in 24 graphs
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots for next week's gallery
‚Ä¢ How to make America healthy: the real problems ‚Äî and best fixes
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots for next week's gallery

üßæ Government & Policy
No updates.

üèõÔ∏è Enterprise Architecture & IT Governance
No updates.

ü§ñ AI & Emerging Tech
‚Ä¢ The Download: how to clean up AI data centers, and weight-loss drugs‚Äô side effects
  This is today&#8217;s edition of¬†The Download,¬†our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



This battery recycling company is now cleaning up AI data centers



In a sandy industrial lot outside Reno, Nevada, rows of battery packs that once propelled electric vehicles are now powering a small AI data center.Redwood Materials, one of the US‚Äôs largest battery recycling companies, showed off this array of energy storage modules, sitting on cinder blocks and wrapped in waterproof plastic, during a press tour at its headquarters on June 26.The event marked the launch of the company&#8217;s new business line, Redwood Energy, which will initially repurpose (rather than recycle) batteries with years of remaining life to create renewable-powered microgrids. Such small-scale energy systems can operate on or off the larger electricity grid, providing electricity for businesses or communities. Read the full story.‚ÄîJames Temple







We‚Äôre learning more about what weight-loss drugs do to the body



Weight-loss drugs are this decade‚Äôs blockbuster medicines. Drugs like Ozempic, Wegovy, and Mounjaro help people with diabetes get their blood sugar under control and help overweight and obese people reach a healthier weight. And they‚Äôre fast becoming a trendy must-have for celebrities and other figure-conscious individuals looking to trim down.



They became so hugely popular so quickly that not long after their approval for weight loss, we saw global shortages of the drugs. Prescriptions have soared over the last five years, but even people who don‚Äôt have prescriptions are seeking these drugs out online.We know they can suppress appetite, lower blood sugar, and lead to dramatic weight loss. We also know that they come with side effects, which can include nausea, diarrhea, and vomiting. But we are still learning about some of their other effects. Read the full story.



‚ÄîJessica Hamzelou



This article first appeared in The Checkup, MIT Technology Review‚Äôs weekly biotech newsletter. To receive it in your inbox every Thursday, and read articles like this first, sign up here.







The must-reads



I‚Äôve combed the internet to find you today‚Äôs most fun/important/scary/fascinating stories about technology.



1 The Supreme Court has paved the way to defund Planned Parenthood¬†By allowing South Carolina to block the organization from its Medicaid program. (WP $)+ Other red states are likely to follow suit. (CNN)+ Planned Parenthood may be able to challenge the ban under state law. (Politico)



2 Iran is back onlineThe country appeared to cut connectivity in a bid to thwart foreign attacks. (Economist $)



3 ICE is using a new facial recognition appIt‚Äôs capable of recognizing someone from their fingerprints, too. (404 Media)+ How a new type of AI is helping police skirt facial recognition bans. (MIT Technology Review)



4 Denmark has a potential solution for malicious deepfakesBy giving its residents copyright to their own body, facial features, and voice. (The Guardian)+ An AI startup made a hyperrealistic deepfake of me that‚Äôs so good it‚Äôs scary. (MIT Technology Review)



5 Impossible Foods wants to bring its plant-based burgers to Europe After sales started falling in America. (Bloomberg $)+ Sales of regular old meat are booming in the States. (Vox)



6 The Three Mile Island nuclear plant‚Äôs restart is being fast trackedIt‚Äôs currently scheduled to start operating a year earlier than anticipated. (Reuters)+ But bringing the reactor back online is no easy task. (The Register)+ Why Microsoft made a deal to help restart Three Mile Island. (MIT Technology Review)



7 AI may be making research too easyNew research suggests that using LLMs results in weaker grasps of topics. (WSJ $)+ It could also be making our thoughts less original. (New Yorker $)



8 Climate tech companies are struggling to weather Trump‚Äôs cutsA lot of startups are expected to fold as a result. (Insider $)+ The Trump administration has shut down more than 100 climate studies. (MIT Technology Review)



9 Billions of Facebook and Google passwords have been leakedAnd people in developing nations are most at risk. (Rest of World)



10 Inside a couples retreat with humans and their AI companionsChaos ensured. (Wired $)+ The AI relationship revolution is already here. (MIT Technology Review)







Quote of the day



‚Äú[The internet blackout] makes us invisible. And still, we‚Äôre here. Still trying to connect with the free world.‚Äù



‚Äî‚ÄôAmir,‚Äô a student in Iran, tells the Guardian why young Iranians are working to overcome the country‚Äôs internet shutdowns.







One more thing







Maybe you will be able to live past 122How long can humans live? This is a good time to ask the question. The longevity scene is having a moment, thanks to a combination of scientific advances, public interest, and an unprecedented level of investment. A few key areas of research suggest that we might be able to push human life spans further, and potentially reverse at least some signs of aging.Researchers can‚Äôt even agree on what the exact mechanisms of aging are and which they should be targeting. Debates continue to rage over how long it‚Äôs possible for humans to live‚Äîand whether there is a limit at all.But it looks likely that something will be developed in the coming decades that will help us live longer, in better health. Read the full story.



‚ÄîJessica Hamzelou







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)



+ This ancient amphibian skull is pretty remarkable.+ A new Phantom of the Opera spin-off is coming‚Äîbut no one really knows what it is.+ Stop panicking, it turns out Marge Simpson isn‚Äôt dead after all.+ I love these owls in towels
‚Ä¢ We‚Äôre learning more about what weight-loss drugs do to the body
  Weight-loss drugs are this decade‚Äôs blockbuster medicines. Drugs like Ozempic, Wegovy, and Mounjaro help people with diabetes get their blood sugar under control and help overweight and obese people reach a healthier weight. And they‚Äôre fast becoming a trendy must-have for celebrities and other figure-conscious individuals looking to trim down.



They became so hugely popular so quickly that not long after their approval for weight loss, we saw global shortages of the drugs.¬†Prescriptions have soared over the last five years, but even people who don‚Äôt have prescriptions are seeking these drugs out online. A¬†2024 health tracking poll by KFF found that around 1 in 8 US adults said they had taken one.





We know they can suppress appetite, lower blood sugar, and lead to dramatic weight loss. We also know that they come with side effects, which can include nausea, diarrhea, and vomiting. But we are still learning about some of their other effects.



On the one hand, these seemingly miraculous drugs appear to improve health in other ways, helping to protect against heart failure, kidney disease, and potentially even substance-use disorders, neurodegenerative diseases, and cancer.



But on the other, they appear to be harmful to some people. Their use has been linked to serious conditions, pregnancy complications, and¬†even some deaths. This week let‚Äôs take a look at what weight-loss drugs can do.



Ozempic, Wegovy, and other similar drugs are known as GLP-1 agonists; they mimic a chemical made in the intestine, GLP-1, that increases insulin and lowers blood levels of glucose. Originally developed to treat diabetes, they are now known to be phenomenal at suppressing appetite. One¬†key trial, published in 2015, found that over the course of around a year, people who took one particular drug lost between around 4.7% and 6% of their body weight, depending on the dose they took.



Newer versions of that drug were shown to have even bigger effects. A 2021 trial of semaglutide‚Äîthe active ingredient in both Ozempic and Wegovy‚Äîfound that¬†people who took it for 68 weeks lost around 15% of their body weight‚Äîequivalent to around 15 kilograms.



But there appear to be other benefits, too. In 2024,¬†an enormous study that included 17,604 people in 41 countries found that semaglutide appeared to reduce heart failure in people who were overweight or obese and had cardiovascular disease. That same year,¬†the US approved Wegovy to ‚Äúreduce the risk of cardiovascular death, heart attack, and stroke in [overweight] adults with cardiovascular disease.‚Äù This year, Ozempic was approved to reduce the risk of kidney disease.



And it doesn‚Äôt end there. The many users of GLP-1 agonists have been reporting some unexpected positive side effects. Not only are they less interested in food, but they are less interested in alcohol, tobacco,¬†opioids, and other addictive substances.





Research suggests¬†they might protect men from prostate cancer. They¬†might help treat osteoarthritis. Some scientists think the drugs¬†could be used to treat a range of pain conditions, and¬†potentially help people with migraine. And some even seem to protect brain cells from damage in lab studies, and they are being explored as potential treatments for neurological disorders like¬†Alzheimer‚Äôs and Parkinson‚Äôs (although¬†we don‚Äôt yet have any evidence they can be useful here).



The more we learn about GLP-1 agonists, the more miraculous they seem to be. What can‚Äôt they do?! you might wonder. Unfortunately, like any drug, GLP-1 agonists carry safety warnings. They can often cause nausea, vomiting, and diarrhea ,and their use has also been linked to inflammation of the pancreas‚Äîa condition that can be fatal. They¬†increase the risk of gall bladder disease.



There are other concerns. Weight-loss drugs can help people trim down on fat, but lean muscle¬†can make up around 10% of the body weight lost by people taking them. That muscle is important, especially as we get older. Muscle loss can affect strength and mobility, and it also can also leave people more vulnerable to falls, which¬†are the second leading cause of unintentional injury deaths worldwide, according to the World Health Organization.



And, as with most drugs, we don‚Äôt fully understand the effects weight-loss drugs might have in pregnancy. That‚Äôs important; even though the drugs are not recommended during pregnancy, health agencies point out that¬†some people who take these drugs might be more likely to get pregnant, perhaps because they interfere with the effects of contraceptive drugs.



And we don‚Äôt really know how they might affect the development of a fetus, if at all. A study published in January found that people who took the drugs either before or during pregnancy¬†didn‚Äôt seem to face increased risk of birth defects. But other research due to be presented at a conference in the coming days found that such individuals were¬†more likely to experience obstetrical complications and preeclampsia.



So yes, while the drugs are incredibly helpful for many people, they are not for everyone. It might be fashionable to be thin, but it‚Äôs not necessarily healthy. No drug comes without risks. Even one that 1 in 8 American adults have taken.



This article first appeared in The Checkup,¬†MIT Technology Review‚Äôs¬†weekly biotech newsletter. To receive it in your inbox every Thursday, and read articles like this first,¬†sign up here.
‚Ä¢ This battery recycling company is now cleaning up AI data centers
  In a sandy industrial lot outside Reno, Nevada, rows of battery packs that once propelled electric vehicles are now powering a small AI data center.



Redwood Materials, one of the US‚Äôs largest battery recycling companies, showed off this array of energy storage modules, sitting on cinder blocks and wrapped in waterproof plastic, during a press tour at its headquarters on June 26.&nbsp;



The event marked the launch of the company‚Äôs new business line, Redwood Energy, which will initially repurpose (rather than recycle) batteries with years of remaining life to create renewable-powered microgrids. Such small-scale energy systems can operate on or off the larger electricity grid, providing electricity for businesses or communities.



Redwood Materials says many of the batteries it takes in for processing retain more than half their capacity.&nbsp;



‚ÄúWe can extract a lot more value from that material by using it as an energy storage project before recycling it,‚Äù JB Straubel, Redwood‚Äôs founder and chief executive, said at the event.&nbsp;



This first microgrid, housed at the company‚Äôs facility in the Tahoe Reno Industrial Center, is powered by solar panels and capable of generating 64 megawatt-hours of electricity, making it one of the nation‚Äôs largest such systems. That power flows to Crusoe, a cryptocurrency miner that pivoted into developing AI data centers, which has built a facility with 2,000 graphics processing units adjacent to the lot of repurposed EV batteries.&nbsp;



(That‚Äôs tiny as modern data centers go: Crusoe is developing a $500 billion AI data center for OpenAI and others in Abilene, Texas, where it expects to install 100,000 GPUs across its first two facilities by the end of the year, according to Forbes.)



Redwood‚Äôs project underscores a growing interest in powering data centers partially or entirely outside the electric grid. Not only would such microgrids be quicker to build than conventional power plants, but consumer ratepayers wouldn‚Äôt be on the hook for the cost of new grid-connected power plants developed to serve AI data centers.¬†Since Redwood‚Äôs batteries are used, and have already been removed from vehicles, the company says its microgrids should also be substantially cheaper than ones assembled from new batteries.



COURTESY REDWOOD MATERIALS




Redwood Energy‚Äôs microgrids could generate electricity for any kind of operation. But the company stresses they‚Äôre an ideal fit for addressing the growing energy needs and climate emissions of data centers. The energy consumption of such facilities could double by 2030, mainly due to the ravenous appetite of AI, according to an April report by the International Energy Agency.



‚ÄúStorage is this perfectly positioned technology, especially low-cost storage, to attack each of those problems,‚Äù Straubel says.



The Tahoe Reno Industrial Center is the epicenter of a data center development boom in northern Nevada that has sparked growing concerns about climate emissions and excessive demand for energy and water, as MIT Technology Review recently reported.



Straubel says the litany of data centers emerging around it ‚Äúwould be logical targets‚Äù for its new business line, but adds there are growth opportunities across the expanding data center clusters in Texas, Virginia, and the Midwest as well.



‚ÄúWe‚Äôre talking to a broad cross section of those companies,‚Äù he says.



Crusoe, which also provides cloud services, recently announced a joint venture with the investment firm Engine No. 1 to provide ‚Äúpowered data center real estate solutions‚Äù to AI companies by constructing 4.5 gigawatts of new natural-gas plants.



Redwood‚Äôs microgrid should provide more than 99% of the electricity Crusoe‚Äôs local facilities need. In the event of extended periods with little sunlight, a rarity in the Nevada desert, the company could still draw from the standard power grid.



Cully Cavness, cofounder and operating chief of Crusoe, says the company is already processing AI queries and producing conclusions for its customers at the Nevada facility. (Its larger data centers are dedicated to the more computationally intensive process of training AI models.)



Redwood‚Äôs new business division offers a test case for a strategy laid out in a paper late last year, which highlighted the potential for solar-powered microgrids to supply the energy that AI data centers need.



The authors of that paper found that microgrids could be built much faster than natural-gas plants and would generally be only a little more expensive as an energy source for data centers, so long as the facilities could occasionally rely on natural-gas generators to get them through extended periods of low sunlight.



If solar-powered microgrids were used to power 30 gigawatts of new AI data centers, with just 10% backup from natural gas, it would eliminate 400 million tons of carbon dioxide emissions relative to running the centers entirely on natural gas, the study found.&nbsp;



‚ÄúHaving a data center running off solar and storage is more or less what we were advocating for in our paper,‚Äù says Zeke Hausfather, climate lead at the payments company Stripe and a coauthor of the paper. He hopes that Redwood‚Äôs new microgrid will establish that ‚Äúthese sorts of systems work in the real world‚Äù and encourage other data center developers to look for similar solutions.¬†



Redwood Materials says electric vehicles are its fastest-growing source of used batteries, and it estimates that more than 100,000 EVs will come off US roads this year.



The company says it tests each battery to determine whether it can be reused. Those that qualify will be integrated into its modular storage systems, which can then store up energy from wind and solar installations or connect to the grid. As those batteries reach the end of their life, they‚Äôll be swapped out of the microgrids and moved into the company‚Äôs recycling process.&nbsp;



Redwood says it already has enough reusable batteries to build a gigawatt-hour‚Äôs worth of microgrids, capable of powering a little more than a million homes for an hour. In addition, the company‚Äôs new division has begun designing microgrids that are 10 times larger than the one it unveiled this week.



Straubel expects Redwood Energy to become a major business line, conceivably surpassing the company‚Äôs core recycling operation someday.



‚ÄúWe‚Äôre confident this is the lowest-cost solution out there,‚Äù he says.
‚Ä¢ The Download: Google DeepMind‚Äôs DNA AI, and heatwaves‚Äô impact on the grid
  This is today&#8217;s edition of¬†The Download,¬†our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



Google‚Äôs new AI will help researchers understand how our genes work



When scientists first sequenced the human genome in 2003, they revealed the full set of DNA instructions that make a person. But we still didn‚Äôt know what all those 3 billion genetic letters actually do.Now Google‚Äôs DeepMind division says it‚Äôs made a leap in trying to understand the code with AlphaGenome, an AI model that predicts what effects small changes in DNA will have on an array of molecular processes, such as whether a gene‚Äôs activity will go up or down.It‚Äôs just the sort of question biologists regularly assess in lab experiments, and is an attempt to further smooth biologists‚Äô work by answering basic questions about how changing DNA letters alters gene activity and, eventually, how genetic mutations affect our health. Read the full story.



‚ÄîAntonio Regalado







It‚Äôs officially summer, and the grid is stressed



It‚Äôs crunch time for the grid this week. Large swaths of the US have reached or exceeded record-breaking temperatures. Spain recently went through a dramatic heat wave too, as did the UK, which is bracing for another one soon.We rely on electricity to keep ourselves comfortable, and more to the point, safe. These are the moments we design the grid for: when need is at its very highest. The key to keeping everything running smoothly during these times might be just a little bit of flexibility. But demand for electricity from major grids is already peaking, and that&#8217;s a good reason to be a little nervous. Read the full story.



‚ÄîCasey Crownhart



This article is from The Spark, MIT Technology Review‚Äôs weekly climate newsletter. To receive it in your inbox every Wednesday, sign up here.







MIT Technology Review Narrated: How did China come to dominate the world of electric cars?



From generous government subsidies to support for lithium batteries, here are the keys to understanding how China managed to build a world-leading industry in electric vehicles.This is our latest story to be turned into a MIT Technology Review Narrated podcast, which¬†we‚Äôre publishing each week on Spotify and Apple Podcasts. Just navigate to MIT Technology Review Narrated on either platform, and follow us to get all our new content as it‚Äôs released.







Inside OpenAI‚Äôs empire with Karen Hao



Journalist Karen Hao‚Äôs newly released book, Empire of AI: Dreams and Nightmares in Sam Altman&#8217;s OpenAI, tells the story of OpenAI‚Äôs rise to power and its far-reaching impact all over the world.Hao, a former MIT Technology Review senior editor, will join our executive editor Niall Firth in an intimate subscriber-exclusive Roundtable conversation exploring the AI arms race, what it means for all of us, and where it‚Äôs headed. Register here to join us at 9am ET on Monday June 30th June.



Special giveaway: Attendees will have the chance to receive a free copy of Hao&#8217;s book. See registration form for details.







The must-reads



I‚Äôve combed the internet to find you today‚Äôs most fun/important/scary/fascinating stories about technology.



1 Meta has won an AI copyright case against authorsThe judge said the authors hadn‚Äôt presented enough evidence to back up their case. (TechCrunch)+ It‚Äôs not an entirely decisive victory for Meta, though. (Wired $)+ It‚Äôs the second lawsuit in favor of AI giants this week. (Insider $)



2 The US will stop contributing towards a global vaccine allianceRFK Jr made unsubstantiated claims about Gavi‚Äôs safety record. (WP $)+ Kennedy‚Äôs newly-assembled vaccine panel is reviewing its guidelines for children. (Vox)+ Experts are worried the once-influential panel will cause irreparable harm. (Ars Technica)+ How measuring vaccine hesitancy could help health professionals tackle it. (MIT Technology Review)



3 Jeff Bezos is cozying up to Donald TrumpIf the Trump administration happens to need a new space company, he‚Äôs ready and willing to supply it. (WSJ $)+ Meanwhile, a private astronaut mission is on its way to the ISS. (CNN)



4 Taiwan is working on suicide drones to defend itself from ChinaThe country is taking a leaf out of Ukraine‚Äôs defense book. (FT $)+ This giant microwave may change the future of war. (MIT Technology Review)



5 Biohackers are feeling emboldened by the Trump administrationThey welcome lower barriers to entry for their unorthodox treatments. (Wired $)+ The first US hub for experimental medical treatments is coming. (MIT Technology Review)



6 A UK cyberattack on a health firm contributed to a patient‚Äôs deathThe ransomware attack disrupted blood services at London hospitals. (BBC)+ A Russian hacking gang is to blame for the incident. (Bloomberg $)



7 Take a look inside Amazon‚Äôs colossal new data centerFour construction teams are working around the clock to finish it. (NYT $)+ Generating video is the most energy-intensive AI prompt. (WSJ $)+ We did the math on AI‚Äôs energy footprint. Here‚Äôs the story you haven‚Äôt heard. (MIT Technology Review)



8 The debate around dark energy is intensifyingNew research suggests it evolves over time. But not everyone agrees. (Undark)



9 Trump Mobile is no longer claiming to be ‚Äòmade in the USA‚ÄôIt‚Äôs now &#8220;designed with American values in mind‚Äù instead. (Ars Technica)



10 It‚Äôs official: The Social Network is getting a sequelZuck goes MAGA? (Deadline $)







Quote of the day



‚ÄúBy training generative AI models with copyrighted works, companies are creating something that often will dramatically undermine the market for those works, and thus dramatically undermine the incentive for human beings to create things the old-fashioned way.&#8221;



‚ÄîUS district judge Vince Chhabria, who presided over a copyright lawsuit brought against Meta by a group of authors, warns of the implications of the company‚Äôs actions, the Guardian reports.







One more thing







Beyond gene-edited babies: the possible paths for tinkering with human evolution



Editing human embryos is restricted in much of the world‚Äîand making an edited baby is fully illegal in most countries surveyed by legal scholars. But advancing technology could render the embryo issue moot.



New ways of adding CRISPR, the revolutionary gene editing tool, to the bodies of people already born could let them easily receive changes as well. It‚Äôs possible that in 125 years, many people will be the beneficiaries of multiple rare, but useful, gene mutations currently found in only small segments of the population.&nbsp;



These could protect us against common diseases and infections, but eventually they could also yield improvements in other traits, such as height, metabolism, or even cognition. But humanity won‚Äôt necessarily do things the right way. Read the full story.



‚ÄîAntonio Regalado







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)



+ Amazing things are happening in New York‚Äôs Central Park.+ A newly-discovered species of dinosaur has gone on display in London, and it‚Äôs small but perfectly formed.+ Cool‚ÄîBob Dylan is releasing a new art book, this time of his drawings.+ Iron Maiden bassist Steve Harris has a secret second career‚Äîas a footballer
‚Ä¢ It‚Äôs officially summer, and the grid is stressed
  It‚Äôs crunch time for the grid this week. As I‚Äôm writing this newsletter, it‚Äôs 100 ¬∞F (nearly 38 ¬∞C) here in New Jersey, and I‚Äôm huddled in the smallest room in my apartment with the shades drawn and a single window air conditioner working overtime.&nbsp;&nbsp;



Large swaths of the US have seen brutal heat this week, with multiple days in a row nearing or exceeding record-breaking temperatures. Spain recently went through a dramatic heat wave too, as did the UK, which is unfortunately bracing for another one soon. As I‚Äôve been trying to stay cool, I‚Äôve had my eyes on a website tracking electricity demand, which is also hitting record highs.¬†





We rely on electricity to keep ourselves comfortable, and more to the point, safe. These are the moments we design the grid for: when need is at its very highest. The key to keeping everything running smoothly during these times might be just a little bit of flexibility.&nbsp;



While heat waves happen all over the world, let‚Äôs take my local grid as an example. I‚Äôm one of the roughly 65 million people covered by PJM Interconnection, the largest grid operator in the US. PJM covers Virginia, West Virginia, Ohio, Pennsylvania, and New Jersey, as well as bits of a couple of neighboring states.



Earlier this year, PJM forecast that electricity demand would peak at 154 gigawatts (GW) this summer. On Monday, just a few days past the official start of the season, the grid blew past that, averaging over 160 GW between 5 p.m. and 6 p.m.¬†



The fact that we‚Äôve already passed both last year‚Äôs peak and this year‚Äôs forecasted one isn‚Äôt necessarily a disaster (PJM says the system‚Äôs total capacity is over 179 GW this year). But it is a good reason to be a little nervous. Usually, PJM sees its peak in July or August. As a reminder, it‚Äôs June. So we shouldn‚Äôt be surprised if we see electricity demand creep to even higher levels later in the summer.



It&#8217;s not just PJM, either. MISO, the grid that covers most of the Midwest and part of the US South, put out a notice that it expected to be close to its peak demand this week. And the US Department of Energy released an emergency order for parts of the Southeast, which allows the local utility to boost generation and skirt air pollution limits while demand is high.



This pattern of maxing out the grid is only going to continue. That‚Äôs because climate change is pushing temperatures higher, and electricity demand is simultaneously swelling (in part because of data centers like those that power AI). PJM‚Äôs forecasts show that the summer peak in 2035 could reach nearly 210 GW, well beyond the 179 GW it can provide today.&nbsp;



Of course, we need more power plants to be built and connected to the grid in the coming years (at least if we don‚Äôt want to keep ancient, inefficient, expensive coal plants running, as we covered last week). But there‚Äôs a quiet strategy that could limit the new construction needed: flexibility.



The power grid has to be built for moments of the absolute highest demand we can predict, like this heat wave. But most of the time, a decent chunk of capacity that exists to get us through these peaks sits idle‚Äîit only has to come online when demand surges. Another way to look at that, however, is that by shaving off demand during the peak, we can reduce the total infrastructure required to run the grid.&nbsp;



If you live somewhere that‚Äôs seen a demand crunch during a heat wave, you might have gotten an email from your utility asking you to hold off on running the dishwasher in the early evening or to set your air conditioner a few degrees higher. These are called demand response programs. Some utilities run more organized programs, where utilities pay customers to ramp down their usage during periods of peak demand.





PJM‚Äôs demand response programs add up to almost eight gigawatts of power‚Äîenough to power over 6 million homes. With these programs, PJM basically avoids having to fire up the equivalent of multiple massive nuclear power plants. (It did activate these programs on Monday afternoon during the hottest part of the day.)



As electricity demand goes up, building in and automating this sort of flexibility could go a long way to reducing the amount of new generation needed. One report published earlier this year found that if data centers agreed to have their power curtailed for just 0.5% of the time (around 40 hours out of a year of continuous operation), the grid could handle about 18 GW of new power demand in the PJM region without adding generation capacity.¬†



For the whole US, this level of flexibility would allow the grid to take on an additional 98 gigawatts of new demand without building any new power plants to meet it. To give you a sense of just how significant that would be, all the nuclear reactors in the US add up to 97 gigawatts of capacity.



Tweaking the thermostat and ramping down data centers during hot summer days won‚Äôt solve the demand crunch on their own, but it certainly won‚Äôt hurt to have more flexibility.



This article is from The Spark, MIT Technology Review‚Äôs weekly climate newsletter. To receive it in your inbox every Wednesday, sign up here.
‚Ä¢ Google‚Äôs new AI will help researchers understand how our genes work
  When scientists first sequenced the human genome in 2003, they revealed the full set of DNA instructions that make a person. But we still didn‚Äôt know what all those 3 billion genetic letters actually do.¬†



Now Google‚Äôs DeepMind division says it‚Äôs made a leap in trying to understand the code with AlphaGenome, an AI model that predicts what effects small changes in DNA will have on an array of molecular processes, such as whether a gene‚Äôs activity will go up or down. It‚Äôs just the sort of question biologists regularly assess in lab experiments.



‚ÄúWe have, for the first time, created a single model that unifies many different challenges that come with understanding the genome,‚Äù says Pushmeet Kohli, a vice president for research at DeepMind.



Five years ago, the Google AI division released AlphaFold, a technology for predicting the 3D shape of proteins. That work was honored with a Nobel Prize last year and spawned a drug-discovery spinout, Isomorphic Labs, and a boom of companies that hope AI will be able to propose new drugs.



AlphaGenome is an attempt to further smooth biologists‚Äô work by answering basic questions about how changing DNA letters alters gene activity and, eventually, how genetic mutations affect our health.&nbsp;



‚ÄúWe have these 3 billion letters of DNA that make up a human genome, but every person is slightly different, and we don‚Äôt fully understand what those differences do,‚Äù says Caleb Lareau, a computational biologist at Memorial Sloan Kettering Cancer Center who has had early access to AlphaGenome. ‚ÄúThis is the most powerful tool to date to model that.‚Äù



Google says AlphaGenome will be free for noncommercial users and plans to release full details of the model in the future. According to Kohli, the company is exploring ways to ‚Äúenable use of this model by commercial entities‚Äù such as biotech companies.&nbsp;





Lareau says AlphaGenome will allow certain types of experiments now done in the lab to be carried out virtually, on a computer. For instance, studies of people who‚Äôve donated their DNA for research often turn up thousands of genetic differences, each slightly raising or lowering the chance a person gets a disease such as Alzheimer‚Äôs.



Lareau says DeepMind‚Äôs software could be used to quickly make predictions about how each of those variants works at a molecular level, something that would otherwise require time-consuming lab experiments. ‚ÄúYou‚Äôll get this list of gene variants, but then I want to understand which of those are actually doing something, and where can I intervene,‚Äù he says. ‚ÄúThis system pushes us closer to a good first guess about what any variant will be doing when we observe it in a human.‚Äù



Don‚Äôt expect AlphaGenome to predict very much about individual people, however. It offers&nbsp;clues to nitty-gritty molecular details of gene activity, not 23andMe-type revelations of a person‚Äôs traits or ancestry.&nbsp;



‚ÄúWe haven‚Äôt designed or validated AlphaGenome for personal genome prediction, a known challenge for AI models,‚Äù Google said in a statement.



Underlying the AI system is the so-called transformer architecture invented at Google that also powers large language models like GPT-4. This one was trained on troves of experimental data produced by public scientific projects.



Lareau says the system will not broadly change how his lab works day to day but could permit new types of research. For instance, sometimes doctors encounter patients with ultra-rare cancers, bristling with unfamiliar mutations. AlphaGenome could suggest which of those mutations are really causing the root problem, possibly pointing to a treatment.



‚ÄúA hallmark of cancer is that specific mutations in DNA make the wrong genes express in the wrong context,‚Äù says Julien Gagneur, a professor of computational medicine at the Technical University of Munich. ‚ÄúThis type of tool is instrumental in narrowing down which ones mess up proper gene expression.‚Äù&nbsp;



The same approach could apply to patients with rare genetic disease, many of whom never learn the source of their condition, even if their DNA has been decoded. ‚ÄúWe can obtain their genomes, but we are clueless as to which genetic alterations cause the disease,‚Äù says Gagneur. He thinks AlphaGenome could give medical scientists a new way to diagnose such cases.&nbsp;



Eventually, some researchers aspire to use AI to design entire genomes from the ground up and create new life forms. Others think the models will be used to create a fully virtual laboratory for drug studies. ‚ÄúMy dream would be to simulate a virtual cell,‚Äù Demis Hassabis, CEO of Google DeepMind, said this year.&nbsp;



Kohli calls AlphaGenome a ‚Äúmilestone‚Äù on the road to that kind of system. ‚ÄúAlphaGenome may not model the whole cell in its entirety ‚Ä¶ but it‚Äôs starting to sort of shed light on the broader semantics of DNA,‚Äù he says.
‚Ä¢ The Download: Introducing the Power issue
  This is today&#8217;s edition of¬†The Download,¬†our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.







Introducing: the Power issue



Energy is power. Those who can produce it, especially lots of it, get to exert authority in all sorts of ways.&nbsp;



The world is increasingly powered by both tangible electricity and intangible intelligence. Plus billionaires. The latest issue of MIT Technology Review explores those intersections, in all their forms.&nbsp;



Here‚Äôs just a taster of what you can expect from our latest issue:+ Are we ready to hand AI agents the keys? We‚Äôre starting to give AI agents real autonomy, and we‚Äôre not prepared for what could happen next. Read the full story.+ In Nebraska, a publicly owned electricity distribution system is an effective lens through which to examine the grid of the near future.+ Cases of cancer, heart disease, and respiratory illnesses are on the rise in the area surrounding Puerto Rico‚Äôs only coal-fired power station. So why has it just been given permission to stay open for at least another seven years? Read the full story.+ How AI is shaking up urban planning and helping make cities better.+ Tech billionaires are making a risky bet with humanity‚Äôs future. They say they want to save humanity by creating superintelligent AI‚Äîbut a new book argues that they‚Äôre steering humanity in a dangerous direction.







The Bank Secrecy Act is failing everyone. It‚Äôs time to rethink financial surveillance.



‚ÄîKatie Haun is the CEO and founder of Haun Ventures, a venture capital firm focused on frontier technologies.



The US is on the brink of enacting rules for digital assets, with growing bipartisan momentum to modernize its financial system. But amid all the talk about innovation and global competitiveness, one issue has been glaringly absent: financial privacy.As we build the digital infrastructure of the 21st century, we need to talk about not just what‚Äôs possible but what‚Äôs acceptable. That means confronting the expanding surveillance powers quietly embedded in our financial system, which today can track nearly every transaction without a warrant. Read the full story.







The must-reads



I‚Äôve combed the internet to find you today‚Äôs most fun/important/scary/fascinating stories about technology.



1 Copyrighted books are fair use for AI trainingAccording to a federal court in the US. (WP $)+ The court compared the way AI learns to how humans consume books. (WSJ $)+ But pirating is still illegal, apparently. (404 Media)



2 Recruiters are drowning in AI-generated r√©sum√©sFake identities, agent-led applications, and identical r√©sum√©s abound. (NYT $)



3 Extreme heat in the US is a growing threatAlaska recently issued its first-ever heat advisory. (Vox)+ And the heatwave is only going to intensify. (The Guardian)+ Here‚Äôs how much heat your body can take. (MIT Technology Review)



4 Big Balls no longer works for DOGEOne of the department‚Äôs most prominent hires has resigned. (Wired $)+ What will he do next? (NYT $)+ DOGE‚Äôs tech takeover threatens the safety and stability of our critical data. (MIT Technology Review)



5 One of America‚Äôs best hackers is a botIt‚Äôs the first time an AI has topped a hacking leaderboard by reputation. (Bloomberg $)+ Cyberattacks by AI agents are coming. (MIT Technology Review)



6 Way fewer people are dying of heart attacks in the USBut deaths from chronic heart conditions are on the up. (New Scientist $)



7 TikTok‚Äôs moderators have had enoughGroups are unionizing across the world to push for better treatment. (Rest of World)+ How an undercover content moderator polices the metaverse. (MIT Technology Review)



8 Donald Trump‚Äôs social media use is even more erratic than usualHe keeps signing off ‚Äúthank you for your attention to this matter!‚Äù (The Atlantic $)+ He‚Äôs also misspelling his name as ‚ÄòDonakd.‚Äô (Fast Company $)



9 Finally, a use for your old smartphoneIt could have a second life as a teeny tiny data center. (IEEE Spectrum)



10 AI models don‚Äôt understand Gen Alpha slangLet him cook! (404 Media)+ That‚Äôs not stopping youngsters from using models as advisors, though. (Fast Company $)







Quote of the day



‚ÄúHumans are wired to bond, and when we feel seen and soothed‚Äîeven by a machine‚Äîwe connect.‚Äù



‚ÄîPsychiatrist Nina Vasan explains why humans may end up falling in love with AI systems to the Wall Street Journal.







One more thing







How Wi-Fi sensing became usable techWi-Fi sensing is a tantalizing concept: that the same routers bringing you the internet could also detect your movements. But, as a way to monitor health, it‚Äôs mostly been eclipsed by other technologies, like ultra-wideband radar.¬†



Despite that, Wi-Fi sensing hasn‚Äôt gone away. Instead, it has quietly become available in millions of homes, supported by leading internet service providers, smart-home companies, and chip manufacturers.&nbsp;



Soon it could be invisibly monitoring our day-to-day movements for all sorts of surprising‚Äîand sometimes alarming‚Äîpurposes. Read the full story.&nbsp;



‚ÄîMeg Duff







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)+&nbsp; How to keep your cool in a heatwave.+ Roblox fans can‚Äôt get enough of, err, gardening.+ Kate Moss, you are the reigning queen of festival fashion.+ A couple of intrepid brown bears managed to escape from a wildlife park in the UK‚Äîto consume a week‚Äôs worth of honey
‚Ä¢ The Bank Secrecy Act is failing everyone. It‚Äôs time to rethink financial surveillance.
  The US is on the brink of enacting rules for digital assets, with growing bipartisan momentum to modernize our financial system. But amid all the talk about innovation and global competitiveness, one issue has been glaringly absent: financial privacy. As we build the digital infrastructure of the 21st century, we need to talk about not just what‚Äôs possible but what‚Äôs acceptable. That means confronting the expanding surveillance powers quietly embedded in our financial system, which today can track nearly every transaction without a warrant.Many Americans may associate financial surveillance with authoritarian regimes. Yet because of a Nixon-era law called the Bank Secrecy Act (BSA) and the digitization of finance over the past half-century, financial privacy is under increasingly serious threat here at home. Most Americans don‚Äôt realize they live under an expansive surveillance regime that likely violates their constitutional rights. Every purchase, deposit, and transaction, from the smallest Venmo payment for a coffee to a large hospital bill, creates a data point in a system that watches you‚Äîeven if you‚Äôve done nothing wrong.As a former federal prosecutor, I care deeply about giving law enforcement the tools it needs to keep us safe. But the status quo doesn‚Äôt make us safer. It creates a false sense of security while quietly and permanently eroding the constitutional rights of millions of Americans.When Congress enacted the BSA in 1970, cash was king and organized crime was the target. The law created a scheme whereby, ever since, banks have been required to keep certain records on their customers and turn them over to law enforcement upon request. Unlike a search warrant, which must be issued by a judge or magistrate upon a showing of probable cause that a crime was committed and that specific evidence of that crime exists in the place to be searched, this power is exercised with no checks or balances. A prosecutor can ‚Äúcut a subpoena‚Äù‚Äîdemanding all your bank records for the past 10 years‚Äîwith no judicial oversight or limitation on scope, and at no cost to the government. The burden falls entirely on the bank. In contrast, a proper search warrant must be narrowly tailored, with probable cause and judicial authorization.In United States v. Miller (1976), the Supreme Court upheld the BSA, reasoning that citizens have no ‚Äúlegitimate expectation of privacy‚Äù about information shared with third parties, like banks. Thus began the third-party doctrine, enabling law enforcement to access financial records without a warrant. The BSA has been amended several times over the years (most notoriously in 2001 as a part of the Patriot Act), imposing an ever-growing list of recordkeeping obligations on an ever-growing list of financial institutions. Today, it is virtually inescapable for everyday Americans.In the 1970s, when the BSA was enacted, banking and noncash payments were conducted predominantly through physical means: writing checks, visiting bank branches, and using passbooks. For cash transactions, the BSA required reporting of transactions over the kingly sum of $10,000, a figure that was not pegged to inflation and remains the same today. And given the nature of banking services and the technology available at the time, individuals conducted just a handful of noncash payments per month. Today, consumers make at least one payment or banking transaction a day, and just an estimated 16% of those are in cash.&nbsp;



Meanwhile, emerging technologies further expand the footprint of financial data. Add to this the massive pools of personal information already collected by technology platforms‚Äîlocation history, search activity, communications metadata‚Äîand you create a world where financial surveillance can be linked to virtually every aspect of your identity, movement, and behavior.Nor does the BSA actually appear to be effective at achieving its aims. In fiscal year 2024, financial institutions filed about 4.7 million Suspicious Activity Reports (SARs) and over 20 million currency transaction reports. Instead of stopping major crime, the system floods law enforcement with low-value information, overwhelming agents and obscuring real threats. Mass surveillance often reduces effectiveness by drowning law enforcement in noise. But while it doesn‚Äôt stop hackers, the BSA creates a trove of permanent info on everyone.Worse still, the incentives are misaligned and asymmetrical. To avoid liability, financial institutions are required to report anything remotely suspicious. If they fail to file a SAR, they risk serious penalties‚Äîeven indictment. But they face no consequences for overreporting. The vast overcollection of data is the unsurprising result. These practices, developed under regulations, require clearer guardrails so that executive branch actors can more safely outsource surveillance duties to private institutions.



But courts have recognized that constitutional privacy must evolve alongside technology. In 2012, the Supreme Court ruled in United States v. Jones that attaching a GPS tracker to a vehicle for prolonged surveillance constituted a search restricted by the Fourth Amendment. Justice Sonia Sotomayor, in a notable concurrence, argued that the third-party doctrine was ill suited to an era when individuals &#8220;reveal a great deal of information about themselves to third parties&#8221; merely by participating in daily life.This legal evolution continued in 2018, when the Supreme Court held in Carpenter v. United States that accessing historical cell-phone location records held by a third party required a warrant, recognizing that ‚Äúseismic shifts in digital technology‚Äù necessitate stronger protections and warning that ‚Äúthe fact that such information is gathered by a third party does not make it any less deserving of Fourth Amendment protection.‚ÄùThe logic of Carpenter applies directly to the mass of financial records being collected today. Just as tracking a person‚Äôs phone over time reveals the ‚Äúwhole of their physical movements,‚Äù tracking a person‚Äôs financial life exposes travel, daily patterns, medical treatments, political affiliations, and personal associations. In many ways, because of the velocity and digital nature of today‚Äôs digital payments, financial data is among the most personal and revealing data there is‚Äîand therefore deserves the highest level of constitutional protection.Though Miller remains formally intact, the writing is on the wall: Indiscriminate financial surveillance such as what we have today is fundamentally at odds with the Fourth Amendment in the digital age.Technological innovations over the past several decades have brought incredible convenience to economic life. Now our privacy standards must catch up. With Congress considering landmark legislation on digital assets, it‚Äôs an important moment to consider what kind of financial system we want‚Äînot just in terms of efficiency and access, but in terms of freedom. Rather than striking down the BSA in its entirety, policymakers should narrow its reach, particularly around the bulk collection and warrantless sharing of Americans‚Äô financial data.



Financial surveillance shouldn‚Äôt be the price of participation in modern life. The systems we build now will shape what freedom looks like for the next century. It‚Äôs time to treat financial privacy like what it is: a cornerstone of democracy, and a right worth fighting for.



Katie Haun is the CEO and founder of Haun Ventures, a venture capital firm focused on frontier technologies. She is a former federal prosecutor who created the US Justice Department‚Äôs first cryptocurrency task force. She led investigations into the Mt. Gox hack and the corrupt agents on the Silk Road task force. She clerked for US Supreme Court Justice Anthony Kennedy and is an honors graduate of Stanford Law School.
‚Ä¢ The AI Hype Index: AI-powered toys are coming
  Separating AI reality from hyped-up fiction isn‚Äôt always easy . The AI Hype Index is a simple, at-a-glance summary of everything you need to know about the state of the industry .
‚Ä¢ 3 things Rhiannon Williams is into right now
  Every Outfit was originally conceived as an homage to Sex and the City‚Äôs iconic fashion . It has since evolved into a wider cultural critique and spawned a podcast of the same name . Ferdia Lennon's debut novel, "Glorious Exploits," is one of those rare books that manage to pull off being both laugh-out-loud funny and deeply moving .
‚Ä¢ CTGT wins Best Presentation Style award at VB Transform 2025
  CTGT won the Best Presentation Style award at VB Transform 2025 in San Francisco . Founded by 23
‚Ä¢ Catio wins ‚Äòcoolest tech‚Äô award at VB Transform 2025
  Catio also announced
‚Ä¢ Retail Resurrection: David‚Äôs Bridal bets its future on AI after double bankruptcy
  The 75-year-old retailer
‚Ä¢ How runtime attacks turn profitable AI into budget black holes
  AI inference attacks drain enterprise
‚Ä¢ Model minimalism: The new AI strategy saving companies millions
  LLMs changed how enterprises build
‚Ä¢ The inference trap: How cloud providers are eating your AI margins
  If you‚Äôre unsure about the load of different AI workload
‚Ä¢ The rise of prompt ops: Tackling hidden AI costs from bad inputs and context bloat
  Prompt ops can help manage
‚Ä¢ Scaling smarter: How enterprise IT teams can right-size their compute for AI
  IT and business leaders plan and choose infrastructure
‚Ä¢ Why your enterprise AI strategy needs both open and closed models: The TCO reality check
  Learn how enterprises evaluate open versus closed
‚Ä¢ CFOs want AI that pays: real metrics, not marketing demos
  CFOs who master new eval frameworks
‚Ä¢ From pilot to profit: The real path to scalable, ROI-positive AI
  Organizations that delay systematic AI deployment risk being
‚Ä¢ Kumo‚Äôs ‚Äòrelational foundation model‚Äô predicts the future your LLM can‚Äôt see
  Forecasting is a fundamentally new capability that
‚Ä¢ Can AI run a physical shop? Anthropic‚Äôs Claude tried and the results were gloriously, hilariously bad
  Anthropic's AI assistant Claude ran a vending machine business for a month
‚Ä¢ OpenAI‚Äôs API lead explains how enterprises are already succeeding with its Agents SDK and Responses API
  OpenAI now includes tracing and eval tools with
‚Ä¢ How Highmark Health and Google Cloud are using Gen AI to streamline medical claims and improve care: 6 key lessons
  Success in generative AI isn't reserved
‚Ä¢ The hidden scaling cliff that‚Äôs about to break your agent rollouts
  Enterprise teams hit a scaling wall when managing AI agents across departments .
‚Ä¢ Walmart cracks enterprise AI at scale: Thousands of use cases, one framework
  Walmart VP Desir√©e Gosby reveals
‚Ä¢ What enterprise leaders can learn from LinkedIn‚Äôs success with AI agents
  LinkedIn scientists share how they have found success
‚Ä¢ Lessons learned from agentic AI leaders reveal critical deployment strategies for enterprises
  What does it take to actually engineer AI agents
‚Ä¢ Get paid faster: How Intuit‚Äôs new AI agents help businesses get funds up to 5 days faster and save 12 hours a month with autonomous workflows
  Intuit has integrated agentic AI
‚Ä¢ IBM sees enterprise customers are using ‚Äòeverything‚Äô when it comes to AI, the challenge is matching the LLM to the right use case
  Real-world deployment patterns show
‚Ä¢ For Replit‚Äôs CEO, the future of software is ‚Äòagents all the way down‚Äô
  Replit's agents can help non-
‚Ä¢ Boston Consulting Group: To unlock enterprise AI value, start with the data you‚Äôve been ignoring
  Companies are looking beyond the experimental phase of
‚Ä¢ Creatio‚Äôs new 8.3 Twin CRM update hits Salesforce where it hurts: ‚Äòwe don‚Äôt think of AI as an add-on‚Ä¶it‚Äôs just part of our app experience‚Äô
  Customers can control which documents are
‚Ä¢ Nvidia‚Äôs ‚ÄòAI Factory‚Äô narrative faces reality check as inference wars expose 70% margins
  How can inference be a commodity while commanding
‚Ä¢ The new AI infrastructure reality: Bring compute to data, not data to compute
  PEAK:AIO‚Äôs software-defined

üîí Cybersecurity & Privacy
No updates.

üéì University AI
No updates.

üè¢ Corporate AI
‚Ä¢ PadChest-GR: A bilingual grounded radiology reporting benchmark for chest X-rays
  In our ever-evolving journey to enhance healthcare through technology, we‚Äôre announcing a unique new benchmark for grounded radiology report generation‚ÄîPadChest-GR (opens in new tab). The world‚Äôs first multimodal, bilingual sentence-level radiology report dataset, developed&nbsp;by the University of Alicante with Microsoft Research, University Hospital Sant Joan d‚ÄôAlacant and MedBravo, is set to redefine how AI and radiologists interpret radiological images. Our work demonstrates how collaboration between humans and AI can create powerful feedback loops‚Äîwhere new datasets drive better AI models, and those models, in turn, inspire richer datasets. We&#8217;re excited to share this progress in‚ÄØNEJM AI, highlighting both the clinical relevance and research excellence of this initiative.&nbsp;



A new frontier in radiology report generation&nbsp;



It is estimated that over half of people visiting hospitals have radiology scans that must be interpreted by a clinical professional. Traditional radiology reports often condense multiple findings into unstructured narratives. In contrast, grounded radiology reporting demands that each finding be described and localized individually.



This can mitigate the risk of AI fabrications and enable new interactive capabilities that enhance clinical and patient interpretability. PadChest-GR is the first bilingual dataset to address this need with 4,555 chest X-ray studies complete with Spanish and English sentence-level descriptions and precise spatial (bounding box) annotations for both positive and negative findings. It is the first public benchmark that enables us to evaluate generation of fully grounded radiology reports in chest X-rays.&nbsp;



Figure 1. Example of a grounded report from PadChest-GR. The original free-text report in Spanish was ‚ÄùMotivo de consulta: Preoperatorio. Rx PA t√≥rax: Impresi√≥n diagn√≥stica: Ateromatosis a√≥rtica calcificada. Engrosamiento pleural biapical. Atelectasia laminar basal izquierda. Elongaci√≥n a√≥rtica. Sin otros hallazgos radiol√≥gicos significativos.‚Äù



This benchmark isn‚Äôt standing alone‚Äîit plays a critical role in powering our state-of-the-art multimodal report generation model, MAIRA-2. Leveraging the detailed annotations of PadChest-GR, MAIRA-2 represents our commitment to building more interpretable and clinically useful AI systems. You can explore our work on MAIRA-2 on our project web page, including recent user research conducted with clinicians in healthcare settings.



PadChest-GR is a testament to the power of collaboration. Aurelia Bustos at MedBravo and Antonio Pertusa at the University of Alicante published the original&nbsp;PadChest dataset (opens in new tab) in 2020,&nbsp;with the help of Jose Mar√≠a Salinas from Hospital San Juan de Alicante and Mar√≠a de la Iglesia Vay√° from the Center of Excellence in Biomedical Imaging at the Ministry of Health in Valencia, Spain. We started to look at PadChest and were deeply impressed by the scale, depth, and diversity of the data.



As we worked more closely with the dataset, we realized the opportunity to develop this for grounded radiology reporting research and worked with the team at the University of Alicante to determine how to approach this together. Our complementary expertise was a nice fit. At Microsoft Research, our mission is to push the boundaries of medical AI through innovative, data-driven solutions. The University of Alicante, with its deep clinical expertise, provided critical insights that greatly enriched the dataset‚Äôs relevance and utility. The result of this collaboration is the PadChest-GR dataset.



A significant enabler of our annotation process was Centaur Labs. The team of senior and junior radiologists from the University Hospital Sant Joan d‚ÄôAlacant, coordinated by Joaquin Galant,&nbsp;used this HIPAA-compliant labeling platform to&nbsp;perform rigorous study-level quality control and bounding box annotations. The annotation protocol implemented ensured that each annotation was accurate and consistent, forming the backbone of a dataset designed for the next generation of grounded radiology report generation models.&nbsp;



Accelerating PadChest-GR dataset annotation with AI&nbsp;



Our approach integrates advanced large language models with comprehensive manual annotation:&nbsp;



Data Selection & Processing: Leveraging Microsoft Azure OpenAI Service (opens in new tab) with GPT-4, we extracted sentences describing individual positive and negative findings from raw radiology reports, translated them from Spanish to English, and linked each sentence to the existing expert labels from PadChest. This was done for a selected subset of the full PadChest dataset, carefully curated to reflect a realistic distribution of clinically relevant findings.&nbsp;



Manual Quality Control & Annotation: The processed studies underwent meticulous quality checks on the Centaur Labs platform by radiologist from Hospital San Juan de Alicante. Each positive finding was then annotated with bounding boxes to capture critical spatial information.&nbsp;



Standardization & Integration: All annotations were harmonized into coherent grounded reports, preserving the structure and context of the original findings while enhancing interpretability.&nbsp;



Figure 2. Overview of the data curation pipeline.



Impact and future directions&nbsp;



PadChest-GR not only sets a new benchmark for grounded radiology reporting, but also serves as the foundation for our MAIRA-2 model, which already showcases the potential of highly interpretable AI in clinical settings. While we developed PadChest-GR to help train and validate our own models, we believe the research community will greatly benefit from this dataset for many years to come. We look forward to seeing the broader research community build on this‚Äîimproving grounded reporting AI models and using PadChest-GR as a standard for evaluation. We believe that by fostering open collaboration and sharing our resources, we can accelerate progress in medical imaging AI and ultimately improve patient care together with the community.



The collaboration between Microsoft Research and the University of Alicante highlights the transformative power of working together across disciplines. With our publication in NEJM-AI and the integral role of PadChest-GR in the development of MAIRA-2 (opens in new tab) and RadFact (opens in new tab), we are excited about the future of AI-empowered radiology. We invite researchers and industry experts to explore PadChest-GR and MAIRA-2, contribute innovative ideas, and join us in advancing the field of grounded radiology reporting.&nbsp;



Papers already using PadChest-GR:




[2406.04449] MAIRA-2: Grounded Radiology Report Generation (opens in new tab)



RadVLM: A Multitask Conversational Vision-Language Model for Radiology (opens in new tab)



Enhancing Abnormality Grounding for Vision Language Models with Knowledge Descriptions (opens in new tab)



Visual Prompt Engineering for Vision Language Models in Radiology (opens in new tab)




For further details or to download PadChest-GR, please visit the BIMCV PadChest-GR Project (opens in new tab).&nbsp;



Models in the Azure Foundry that can do Grounded Reporting:&nbsp;




How to deploy and use CXRReportGen healthcare AI model with Azure AI Foundry &#8211; Azure AI Foundry | Microsoft Learn (opens in new tab)



Healthcare Orchestrator &#8211; Healthcare agent service | Microsoft Learn (opens in new tab)




Acknowledgement




Authors: Daniel C. Castro (opens in new tab), Aurelia Bustos (opens in new tab), Shruthi Bannur (opens in new tab), Stephanie L. Hyland (opens in new tab), Kenza Bouzid (opens in new tab), Maria Teodora Wetscherek (opens in new tab), Maria Dolores S√°nchez-Valverde (opens in new tab), Lara Jaques-P√©rez (opens in new tab), Lourdes P√©rez-Rodr√≠guez (opens in new tab), Kenji Takeda (opens in new tab), Jos√© Mar√≠a Salinas (opens in new tab), Javier Alvarez-Valle (opens in new tab), Joaqu√≠n Galant Herrero (opens in new tab), Antonio Pertusa (opens in new tab)&nbsp;





MSR Health Futures UK: Hannah Richardson, Valentina Salvatelli, Harshita Sharma, Sam Bond-Taylor, Max Ilse, Fernando Perez-Garcia, Anton Schwaighofer, Jonathan Carlson¬†





MSR Flow: Kenji Takeda, Evelyn Viegas, Ashley Llorens





HLS: Matthew Lungren, Naiteek Sangani, Shrey Jain, Ivan Tarapov, Will Guyman, Mert Oez, Chris Burt, David Ardman

Opens in a new tabThe post PadChest-GR: A bilingual grounded radiology reporting benchmark for chest X-rays appeared first on Microsoft Research.
‚Ä¢ AI Testing and Evaluation: Learnings from Science and Industry
  Generative AI presents a unique challenge and opportunity to reexamine governance practices for the responsible development, deployment, and use of AI. To advance thinking in this space, Microsoft has tapped into the experience and knowledge of experts across domains‚Äîfrom genome editing to cybersecurity‚Äîto investigate the role of testing and evaluation as a governance tool. AI Testing and Evaluation: Learnings from Science and Industry, hosted by Microsoft Research‚Äôs Kathleen Sullivan, explores what the technology industry and policymakers can learn from these fields and how that might help shape the course of AI development.



In this episode, Amanda Craig Deckard, senior director of public policy in Microsoft&#8217;s Office of Responsible AI, joins Sullivan to detail the company‚Äôs efforts to help inform AI governance discussions and decisions, including, more recently, around the role of AI testing and evaluation. Craig Deckard and Sullivan delve into the tension that exists between the risk and opportunity of technology, the similarities and differences between AI development and the fields Microsoft is studying, and the role of different stakeholders in advancing AI governance and public policy.








Learn more:




Global Governance: Goals and Lessons for AI (opens in new tab)E-book | May 2024



Learning from other domains to advance AI evaluation and testingMicrosoft Research Blog | June 2025



Responsible AI: Ethical policies and practices | Microsoft AI





AI and Microsoft Research








	
		
			Subscribe to the Microsoft Research Podcast:		
		
							
					
						  
						Apple Podcasts
					
				
			
							
					
						
						Email
					
				
			
							
					
						
						Android
					
				
			
							
					
						
						Spotify
					
				
			
							
					
						
						RSS Feed
					
				
					
	




	
		
			
				
					

Transcript&nbsp;



[MUSIC]&nbsp;



KATHLEEN SULLIVAN: Welcome to AI Testing and Evaluation: Learnings from Science and Industry. I&#8217;m your host, Kathleen Sullivan.



As generative AI continues to advance, Microsoft has gathered a range of experts‚Äîfrom genome editing to cybersecurity‚Äîto share how their fields approach evaluation and risk assessment. Our goal is to learn from their successes and their stumbles to move the science and practice of AI testing forward. In this series, we&#8217;ll explore how these insights might help guide the future of AI development, deployment, and responsible use.



				
				
					



[MUSIC ENDS]&nbsp;&nbsp;



For our introductory episode, I&#8217;m pleased to welcome Amanda Craig Deckard from Microsoft to discuss the company&#8217;s efforts to learn about testing in other sectors.&nbsp;&nbsp;



Amanda is senior director of public policy in the Office of Responsible AI, where she leads a team that works closely with engineers, researchers, and policy experts to help ensure AI is being developed and used responsibly. Their insights shape Microsoft&#8217;s contribution to public policy discussions on laws, norms, and standards for AI.&nbsp;&nbsp;



Amanda, welcome to the podcast.&nbsp;&nbsp;



AMANDA CRAIG DECKARD: Thank you.&nbsp;&nbsp;



SULLIVAN: Amanda, let&#8217;s give the listeners a little bit of your background. What&#8217;s your origin story? Can you talk to us a little bit about maybe how you started in tech? And I would love to also learn a little bit more about what your team does in the Office of Responsible AI.&nbsp;&nbsp;



CRAIG DECKARD: Sure. Thank you. I&#8217;d say my [LAUGHS] path to tech, to Microsoft, as well, was a bit, like, circuitous, maybe. You know, I thought for the longest time I was going to be a journalist. I studied forced migration. I worked in a sort of state level sort of trial court in Indiana, a legal service provider in India, just to give you a bit of a flavor.



I made my way to Microsoft in 2014 and have been here since, working in cybersecurity public policy first and now in responsible AI. And the way that our Office of Responsible AI has really, sort of, structured itself is bringing together the kind of expertise to really work on defining policy and how to operationalize it at the same time.



And, you know, that means that we have been working through this, you know, real challenge of defining internal policy and practice, making sure that&#8217;s deeply grounded in the work of our colleagues at Microsoft Research, and then really closely working with engineering to make sure that we have the processes, that we have the tools, to implement that policy at scale.&nbsp;&nbsp;



And I&#8217;m really drawn to these kind of hard problems where they have the character of two things being true or there&#8217;s like, you know, real tension on both sides and in particular, in the context of those kinds of problems, roles in which, like, the whole job is actually just sitting with that tension, not necessarily, like, resolving it and expecting that you&#8217;re done.



And I think, really, there are two reasons why tech is so, kind of, representative of that kind of challenge that I&#8217;ve always found fascinating. You know, one is that, of course, tech is, sort of, ubiquitous. It&#8217;s really impacting so many people&#8217;s lives. But also, you know, because, as I think has become part of our vernacular now, but, you know, is not necessarily immediately intuitive, is like the fact that technology is both a tool and a weapon. And so that&#8217;s just, like, another reason why, you know, we have to continuously work through that tension and, sort of, like, sit with it, right, and even as tech evolves over time.



SULLIVAN: You bring up such great points, and this field is not black and white. I think that even underscores, you know, this notion that you highlighted that it&#8217;s impacting everyone. And, you know, to set the stage for our listeners, last year, we pulled in a bunch of experts from cybersecurity, biotech, finance, and we ran this large workshop to study how they&#8217;re thinking about governance in those playbooks. And so I&#8217;d love to understand a little bit more about what sparked that effort‚Äîand, you know, there&#8217;s a piece of this which is really centered around testing‚Äîand to hear from you why the focus on testing is so important.&nbsp;



CRAIG DECKARD: If I could rewind a little bit and give you a bit of history of how we even arrived at bringing these experts together, you know, we actually started on this journey in 2023. At that time, there were, like, a lot of these big questions swirling around about, you know, what did we need in terms of governance for AI? Of course, this was in the immediate aftermath of the ChatGPT sort of wave and everyone recognizing that, like, the technology was going to have a different level of impact in the near term. And so, you know, what do we need from governance? What do we need at the global level, in particular, of governance?&nbsp;&nbsp;



And so at the time, in early 2023 especially, there were a lot of attempts to sort of draw analogies to other global governance institutions in other domains. So we actually in 2023 brought together a different workshop than the one that you&#8217;re referring to specifically focused on testing last year. And we, kind of, had two big takeaways from that conversation.&nbsp;&nbsp;



One was, what are the actual functions of these institutions and how do they apply to AI? And, actually, one of the takeaways was they all sort of apply. [LAUGHS] There&#8217;s, like, a role for, you know, any of the functions, whether it be sort of driving consensus on research or building industry standards or managing, kind of, frontier risks, for thinking about how those might be needed in the AI context.&nbsp;&nbsp;



And one of the other big takeaways was that, you know, there are also limitations in these analogies. You know, each of the institutions grew up in its own, sort of, unique historical moment, like the one that we sit in with AI right now. And in each of those circumstances, they don&#8217;t exactly translate to this moment. And so, yeah, there was like this kind of, OK, we want to draw what we can from this conversation and then we also want to understand, what is also very important that&#8217;s just different for AI right now?&nbsp;&nbsp;



We published a book with the lessons from that conversation in 2023 (opens in new tab). And then we actually went on a bit of a tour [LAUGHS] with that content where we had a number of roundtables actually all over the world where we gathered feedback on how those analogies were landing, how our takeaways were landing. And one of the things that we took from them was a gap that some of the participants saw in the analogies that we chose to focus on. So across multiple conversations, other domains kept being raised, like, why did you not also study pharmaceuticals? Why did you also not study cybersecurity, for example? And so that, you know, naturally got us thinking about what further lessons we could draw from those domains.&nbsp;



At the same time, though, we also saw a need to, again, go deeper than what we went and really, like, focus on a narrower problem. So that&#8217;s really what led us to trying to think about a more specific problem where we could think across levels of governance and bring in some of these other domains. And, you know, testing was top of mind. Continues to be a really important topic in the AI policy conversation right now, I think, for really good reason. A lot of policymakers are focused on, you know, what we need to do to,&nbsp;kind of, have there be sufficient trust, and testing is going to be a part of that‚Äîreally better understand risk, enable everyone to be able to make more, kind of, risk-informed decisions, right. Testing is an important component for governance and AI and, of course, in all of these other domains, as well.&nbsp;&nbsp;



So I&#8217;ll just add the other, kind of, input into the process for this second round was exploring other analogies beyond those that we, kind of, got feedback on. And one of the early, kind of, examples of another domain that would be really worthwhile to study that came to mind from, sort of, just studying the literature was genome editing. &nbsp;



You know, genome editing was really interesting through the process of thinking about other kind of general-purpose technologies. We also arrived at nanoscience and brought those into the conversation.&nbsp;



SULLIVAN: That&#8217;s great. I mean, actually, if you could double-click,&nbsp;I mean, you just named a number of industries. I&#8217;d love to just understand which of those worlds maybe feels the closest to what we&#8217;re wrestling with, with AI and maybe which is kind of the farthest off, and what makes them stand out to you?



CRAIG DECKARD: Oh, such a good question. For this second round, we actually brought together eight different domains, right. And I think we actually thought we would come out of this conversation with some bit of clarity around, Oh, if we just, sort of, take this approach for this domain or that domain, we&#8217;ll sort of have‚Äîat least for now‚Äîreally solved part of the puzzle. [LAUGHS] And, you know, our public policy team the day after the workshop, we had a, sort of, follow-on discussion, and the very first thing that we started with in that conversation was like, OK, so which of these domains? And fascinatingly, like, everyone was sort of like, Ahh! [LAUGHS] None of them are applying perfectly. I mean, this is also speaking to the limitations of analogies that we already acknowledged.&nbsp;&nbsp;



And also, you know, all of the experts from across these domains gave us really interesting insights into, sort of, the tradeoffs and the limitations and how they were working. None are really applying perfectly for us. But all of them do offer a thread of insight that is really useful for thinking about testing in AI, and there are some different dimensions that I think are really useful as framing for that.&nbsp;&nbsp;



I mean, one is just this horizontal-versus-vertical,&nbsp;kind of, difference in domains and, you know, the horizontal technology like genome editing or nanoscience&nbsp;just being inherently different and seemingly very similar to AI in that you want to be able to understand risks in the technology itself and there is just so much contextual, sort of, factor that matters in the application of those technologies for how the risk manifests that you really need to, kind of, do those two things at once‚Äîof understanding the technology but then really thinking about risk and governance in the context of application versus, you know, a context like or a domain like civil aviation or nuclear technology, for example.



You know, even in the workshop itself that we hosted late last year, where we brought together this second round of experts, it was really interesting. We actually started the conversation by trying to understand how those different domains defined risks, where they were able to set risk thresholds. That&#8217;s been such a part of the AI policy conversation in the last year. And, you know, it was really instructive that the more vertical domains were able to, sort of, snap to clearer answers much more quickly. [LAUGHS] But, like, the horizontal nanoscience and genome editing were not because it just depends, right. So anyway, the horizontal-vertical dimension seems like a really important one to draw from and apply to AI. 



The couple of others that I would offer is just, you know, thinking about the different kinds of technologies. You know, obviously, there&#8217;s some of the domains that we studied that they&#8217;re just inherently, sort of, like, physical technologies ‚Ä¶ a mix of physical and digital or virtual in a lot of cases because all of these are, of course, applying digital technology. But like, you know, there is just a difference between something like an airplane or a medical device or, you know, the more kind of virtual or intangible sort of technologies even, you know, of course, AI and some of the other like cyber and genome editing but also like, you know, financial services having some of that quality. And again, I think the thing that&#8217;s interesting to us about AI is to think about AI and risk evaluation of AI as being, you know, having a large component of that being about the kind of virtual or intangible technology. And also, you know, there is a future of robotics where we might need to think about the, kind of, physical risk evaluation kind of work, as well.



And then the final thing I&#8217;d maybe say in terms of thinking about which domains have the lessons for AI that are most applicable is just how they&#8217;ve grappled with these different kind of governance questions. Things like how to turn the dial in terms of being more or less prescriptive on risk evaluation approaches, how they think about the balance of, kind of, pre-market versus post-market risk evaluation in testing, and what the tradeoffs have been there across domains has been really interesting to kind of tease out. And then also thinking about, sort of, who does what?



So, you know, in each of these different domains, it was interesting to hear about, like, you know, the role of industry, the role of governments, the role of third-party experts in designing evaluations and developing standards and actually doing the work, and, kind of, having the pull through of what it means for risk and governance decisions. There were, again, there was a variety of, sort of, approaches across these domains that I think were interesting for AI.



SULLIVAN: You mentioned that there&#8217;s a number of different stakeholders to be considering across the board as we&#8217;re thinking about policy, as we&#8217;re thinking about regulation. Where can we collaborate more across industry? Is it academia? Regulators? Just, how can we move the needle faster?&nbsp;&nbsp;



CRAIG DECKARD: I think all of the above [LAUGHTER] is needed. But it&#8217;s also really important to have all of that, kind of, expertise brought together, you know, and I think, you know, one of the things that we certainly heard from multiple of the domains, if not all of them, was that same actual interest and need and the same sort of ongoing work to try to figure that out.



You know, even where there had been progress in some of the other domains with bringing together, you know, some industry stakeholders or, you know, industry and government, there was still a desire to actually do more there. Like, if there was some progress in industry and government, the need was, And more kind of cross-jurisdiction government conversation, for example. Or some progress on, you know, within the industry but needing to, like, strengthen the partnership with academia, for example. So, you know, I think it speaks to, like, the quality of your question, to be honest, that, you know, all of these domains are actually still grappling with this and still seeing the need to grow in that direction more.&nbsp;&nbsp;



What I&#8217;d say about AI today is that we have made good progress with, you know, starting to build some industry partnerships. You know, we were a founding member of the Frontier Model Forum, or FMF (opens in new tab), which has been a very useful place for us to work with some peers on really trying to bring forward some best practices that apply across our organizations. You know, there are other forums as well, like MLCommons (opens in new tab), where we&#8217;re working with others in industry and broader, sort of, academic and civil society communities. Partnership on AI (opens in new tab) is another one I think about that, kind of, fits that mold, as well, in a really positive way. And, like, there are a lot of different, sort of, governance needs to think through and where, you know, we can really think about bringing that expertise together is going to be so important.



I think about almost, like, in the near to mid-term, like three issues that we need to address in the AI, kind of, policy and testing context. One is just building kind of, like, a flexible framework that allows us to really build trust while we continue to advance the science and the standards. You know, we are going to need to do both at once. And so we need a flexible framework that enables that kind of agility, and advancing the science and the standards, that is going to be something that really demands that kind of cross-discipline or cross kind of expertise group coming together to work on that‚Äîresearchers, academics, civil society, governments and, of course, industry.



And so I think that is, actually, the second problem is, like, how do we actually build the kind of forums and ways of working together, the public-private partnership kind of efforts that allow all of that expertise to come together and fit together over time, right. Because when these are really big, broad challenges, you kind of have to break them down incrementally, make progress on them, and then bring them back together. 



And so I think about, like, one example that I, you know, really have been reflecting on lately is, you know, in the context of building standards, like, how do you do that, right? Again, standards are going to benefit from that whole community of expertise. And, you know, there are lots of different kinds of quote-unquote standards, though, right. You kind of have the ‚Äúsmall s‚Äù industry standards. You have the kind of ‚Äúbig S‚Äù international standards, for example. And how do you, kind of, leverage one to accelerate the other, I think, is part of, like, how we need to work together within this ecosystem. And, like, I think what we and others have done in an organization like C2PA [Coalition for Content Provenance and Authenticity] (opens in new tab), for example, where we&#8217;ve really built an industry specification but then built on that towards an international standard effort is one example that is interesting, right, to point to.



And then, you know, I actually think that bridges to the third thing that we need to do together within this whole community, which is, you know, really think again about how we manage the breadth of this challenge and opportunity of AI by thinking about this horizontal-vertical problem. And, you know, I think that&#8217;s where it&#8217;s not just the sort of tech industry, for example. It&#8217;s broader industry that&#8217;s going to be really applying this technology that needs to get involved in the conversation about not just, sort of, testing AI models, for example, but also testing how AI systems or applications are working in context. And so, yes, so much fun opportunity!&nbsp;



[MUSIC]&nbsp;



SULLIVAN: Amanda, this was just fantastic. You&#8217;ve really set the stage for this podcast. And thank you so much for sharing your time and wisdom with us.&nbsp;&nbsp;



CRAIG DECKARD: Thank you.&nbsp;



SULLIVAN: And to our listeners, we&#8217;re so glad you joined us for this conversation. An exciting lineup of episodes are on the way, and we can&#8217;t wait to have you back for the next one.&nbsp;&nbsp;



[MUSIC FADES]&nbsp;

				
			
			
				Show more			
		
	

Opens in a new tabThe post AI Testing and Evaluation: Learnings from Science and Industry appeared first on Microsoft Research.
‚Ä¢ Learning from other domains to advance AI evaluation and testing
  As generative AI becomes more capable and widely deployed, familiar questions from the governance of other transformative technologies have resurfaced. Which opportunities, capabilities, risks, and impacts should be evaluated? Who should conduct evaluations, and at what stages of the technology lifecycle? What tests or measurements should be used? And how can we know if the results are reliable?&nbsp;&nbsp;



Recent research and reports from Microsoft (opens in new tab), the UK AI Security Institute (opens in new tab), The New York Times (opens in new tab), and MIT Technology Review (opens in new tab) have highlighted gaps in how we evaluate AI models and systems. These gaps also form foundational context for recent international expert consensus reports: the inaugural&nbsp;International AI Safety Report (opens in new tab) (2025) and the Singapore Consensus (opens in new tab) (2025). Closing these gaps at a pace that matches AI innovation will lead to more reliable evaluations that can help guide deployment decisions, inform policy, and deepen trust.&nbsp;



Today, we‚Äôre launching a limited-series podcast, AI Testing and Evaluation: Learnings from Science and Industry, to share insights from domains that have grappled with testing and measurement questions. Across four episodes, host Kathleen Sullivan speaks with academic experts in genome editing, cybersecurity, pharmaceuticals, and medical devices to find out which technical and regulatory steps have helped to close evaluation gaps and earn public trust.



We‚Äôre also sharing written case studies from experts, along with top-level lessons we‚Äôre applying to AI. At the close of the podcast series, we‚Äôll offer Microsoft‚Äôs deeper reflections on next steps toward more reliable and trustworthy approaches to AI evaluation.&nbsp;



Lessons from eight case studies&nbsp;



Our research on risk evaluation, testing, and assurance models in other domains began in December 2024, when Microsoft‚Äôs Office of Responsible AI (opens in new tab) gathered independent experts from the fields of civil aviation, cybersecurity, financial services, genome editing, medical devices, nanoscience, nuclear energy, and pharmaceuticals. In bringing this group together, we drew on our own learnings and feedback received on our e-book, Global Governance: Goals and Lessons for AI (opens in new tab), in which we studied the higher-level goals and institutional approaches that had been leveraged for cross-border governance in the past.&nbsp;



While approaches to risk evaluation and testing vary significantly across the case studies, there was one consistent, top-level takeaway: evaluation frameworks always reflect trade-offs among different policy objectives, such as safety, efficiency, and innovation.&nbsp;&nbsp;



Experts across all eight fields noted that policymakers have had to weigh trade-offs in designing evaluation frameworks. These frameworks must account for both the limits of current science and the need for agility in the face of uncertainty. They likewise agreed that early design choices, often reflecting the ‚ÄúDNA‚Äù of the historical moment in which they‚Äôre made, as cybersecurity expert Stewart Baker described it, are important as they are difficult to scale down or undo later.&nbsp;



Strict, pre-deployment testing regimes‚Äîsuch as those used in civil aviation, medical devices, nuclear energy, and pharmaceuticals‚Äîoffer strong safety assurances but can be resource-intensive and slow to adapt. These regimes often emerged in response to well-documented failures and are backed by decades of regulatory infrastructure and detailed technical standards.&nbsp;&nbsp;



In contrast, fields marked by dynamic and complex interdependencies between the tested system and its external environment‚Äîsuch as cybersecurity and bank stress testing‚Äîrely on more adaptive governance frameworks, where testing may be used to generate actionable insights about risk rather than primarily serve as a trigger for regulatory enforcement.&nbsp;&nbsp;



Moreover, in pharmaceuticals, where interdependencies are at play and there is emphasis on pre-deployment testing, experts highlighted a potential trade-off with post-market monitoring of downstream risks and efficacy evaluation.&nbsp;



These variations in approaches across domains‚Äîstemming from differences in risk profiles, types of technologies, maturity of the evaluation science, placement of expertise in the assessor ecosystem, and context in which technologies are deployed, among other factors‚Äîalso inform takeaways for AI.



Applying risk evaluation and governance lessons to AI&nbsp;



While no analogy perfectly fits the AI context, the genome editing and nanoscience cases offer interesting insights for general-purpose technologies like AI, where risks vary widely depending on how the technology is applied.&nbsp;&nbsp;



Experts highlighted the benefits of governance frameworks that are more flexible and tailored to specific use cases and application contexts. In these fields, it is challenging to define risk thresholds and design evaluation frameworks in the abstract. Risks become more visible and assessable once the technology is applied to a particular use case and context-specific variables are known.&nbsp;&nbsp;



These and other insights also helped us distill qualities essential to ensuring that testing is a reliable governance tool across domains, including:&nbsp;




Rigor in defining what is being examined and why it matters. This requires detailed specification of what is being measured and understanding how the deployment context may affect outcomes.



Standardization of how tests should be conducted to achieve valid, reliable results. This requires establishing technical standards that provide methodological guidance and ensure quality and consistency.&nbsp;



Interpretability of test results and how they inform risk decisions. This requires establishing expectations for evidence and improving literacy in how to understand, contextualize, and use test results‚Äîwhile remaining aware of their limitations.&nbsp;




Toward stronger foundations for AI testing&nbsp;



Establishing robust foundations for AI evaluation and testing requires effort to improve rigor, standardization, and interpretability‚Äîand to ensure that methods keep pace with rapid technological progress and evolving scientific understanding.&nbsp;&nbsp;



Taking lessons from other general-purpose technologies, this foundational work must also be pursued for both AI models and systems. While testing models will continue to be important, reliable evaluation tools that provide assurance for system performance will enable broad adoption of AI, including in high-risk scenarios. A strong feedback loop on evaluations of AI models and systems could not only accelerate progress on methodological challenges but also bring focus to which opportunities, capabilities, risks, and impacts are most appropriate and efficient to evaluate at what points along the AI development and deployment lifecycle.



Acknowledgements&nbsp;



We would like to thank the following external experts who have contributed to our research program on lessons for AI testing and evaluation: Mateo Aboy, Paul Alp, Ger√≥nimo Poletto Antonacci, Stewart Baker, Daniel Benamouzig, Pablo Cantero, Daniel Carpenter, Alta Charo, Jennifer Dionne, Andy Greenfield, Kathryn Judge, Ciaran Martin, and Timo Minssen.&nbsp;&nbsp;



Case studies&nbsp;



Civil aviation: Testing in Aircraft Design and Manufacturing, by Paul Alp&nbsp;



Cybersecurity: Cybersecurity Standards and Testing‚ÄîLessons for AI Safety and Security, by Stewart Baker&nbsp;



Financial services (bank stress testing): The Evolving Use of Bank Stress Tests, by Kathryn Judge&nbsp;



Genome editing: Governance of Genome Editing in Human Therapeutics and Agricultural Applications, by Alta Charo and Andy Greenfield&nbsp;



Medical devices: Medical Device Testing: Regulatory Requirements, Evolution and Lessons for AI Governance, by Mateo Aboy and Timo Minssen&nbsp;



Nanoscience: The regulatory landscape of nanoscience and nanotechnology, and applications to future AI regulation, by Jennifer Dionne&nbsp;



Nuclear energy: Testing in the Nuclear Industry, by Pablo Cantero and Ger√≥nimo Poletto Antonacci&nbsp;



Pharmaceuticals: The History and Evolution of Testing in Pharmaceutical Regulation, by Daniel Benamouzig and Daniel Carpenter
Opens in a new tabThe post Learning from other domains to advance AI evaluation and testing appeared first on Microsoft Research.
‚Ä¢ AWS costs estimation using Amazon Q CLI and AWS Cost Analysis MCP
  Managing and optimizing AWS infrastructure costs is a critical challenge for organizations of all sizes. Traditional cost analysis approaches often involve the following: 
 
 Complex spreadsheets ‚Äì Creating and maintaining detailed cost models, which requires significant effort 
 Multiple tools ‚Äì Switching between the AWS Pricing Calculator, AWS Cost Explorer, and third-party tools 
 Specialized knowledge ‚Äì Understanding the nuances of AWS pricing across services and AWS Regions 
 Time-consuming analysis ‚Äì Manually comparing different deployment options and scenarios 
 Delayed optimization ‚Äì Cost insights often come too late to inform architectural decisions 
 
Amazon Q Developer CLI with the Model Context Protocol (MCP) offers a revolutionary approach to AWS cost analysis. By using generative AI through natural language prompts, teams can now generate detailed cost estimates, comparisons, and optimization recommendations in minutes rather than hours, while providing accuracy through integration with official AWS pricing data. 
In this post, we explore how to use Amazon Q CLI with the AWS Cost Analysis MCP server to perform sophisticated cost analysis that follows AWS best practices. We discuss basic setup and advanced techniques, with detailed examples and step-by-step instructions. 
Solution overview 
Amazon Q Developer CLI is a command line interface that brings the generative AI capabilities of Amazon Q directly to your terminal. Developers can interact with Amazon Q through natural language prompts, making it an invaluable tool for various development tasks. Developed by Anthropic as an open protocol, the Model Context Protocol (MCP) provides a standardized way to connect AI models to different data sources or tools. Using a client-server architecture (as illustrated in the following diagram), the MCP helps developers expose their data through lightweight MCP servers while building AI applications as MCP clients that connect to these servers. 
The MCP uses a client-server architecture containing the following components: 
 
 Host ‚Äì A program or AI tool that requires access to data through the MCP protocol, such as Anthropic‚Äôs Claude Desktop, an integrated development environment (IDE), or other AI applications 
 Client ‚Äì Protocol clients that maintain one-to-one connections with servers 
 Server ‚Äì Lightweight programs that expose capabilities through standardized MCP or act as tools 
 Data sources ‚Äì Local data sources such as databases and file systems, or external systems available over the internet through APIs (web APIs) that MCP servers can connect with 
 
 
As announced in April 2025, the MCP enables Amazon Q Developer to connect with specialized servers that extend its capabilities beyond what‚Äôs possible with the base model alone. MCP servers act as plugins for Amazon Q, providing domain-specific knowledge and functionality. The AWS Cost Analysis MCP server specifically enables Amazon Q to generate detailed cost estimates, reports, and optimization recommendations using real-time AWS pricing data. 
Prerequisites 
To implement this solution, you must have an AWS account with appropriate permissions and follow the steps below. 
Set up your environment 
Before you can start analyzing costs, you need to set up your environment with Amazon Q CLI and the AWS Cost Analysis MCP server. This section provides detailed instructions for installation and configuration. 
Install Amazon Q Developer CLI 
Amazon Q Developer CLI is available as a standalone installation. Complete the following steps to install it: 
 
 Download and install Amazon Q Developer CLI. For instructions, see Using Amazon Q Developer on the command line. 
 Verify the installation by running the following command: q --version You should see output similar to the following: Amazon Q Developer CLI version 1.x.x 
 Configure Amazon Q CLI with your AWS credentials: q login 
 Choose the login method suitable for you: 
   
   Use for free with AWS Builder ID 
   Use with Pro license 
    
 
Set up MCP servers 
Before using the AWS Cost Analysis MCP server with Amazon Q CLI, you must install several tools and configure your environment. The following steps guide you through installing the necessary tools and setting up the MCP server configuration: 
 
 Install Panoc using the following command (you can install with brew as well), converting the output to PDF: pip install pandoc 
 Install uv with the following command: pip install uv 
 Install Python 3.10 or newer: uv python install 3.10 
 Add the servers to your ~/.aws/amazonq/mcp.json file: {
  "mcpServers": {
    "awslabs.cost-analysis-mcp-server": {
      "command": "uvx",
      "args": ["awslabs.cost-analysis-mcp-server"],
      "env": {
        "FASTMCP_LOG_LEVEL": "ERROR"
      },
      "autoApprove": [],
      "disabled": false
    }
  }
}
 Now, Amazon Q CLI automatically discovers MCP servers in the ~/.aws/amazonq/mcp.json file. 
 
Understanding MCP server tools 
The AWS Cost Analysis MCP server provides several powerful tools: 
 
 get_pricing_from_web ‚Äì Retrieves pricing information from AWS pricing webpages 
 get_pricing_from_api ‚Äì Fetches pricing data from the AWS Price List API 
 generate_cost_report ‚Äì Creates detailed cost analysis reports with breakdowns and visualizations 
 analyze_cdk_project ‚Äì Analyzes AWS Cloud Development Kit (AWS CDK) projects to identify services used and estimate costs 
 analyze_terraform_project ‚Äì Analyzes Terraform projects to identify services used and estimate costs 
 get_bedrock_patterns ‚Äì Retrieves architecture patterns for Amazon Bedrock with cost considerations 
 
These tools work together to help you create accurate cost estimates that follow AWS best practices. 
Test your setup 
Let‚Äôs verify that everything is working correctly by generating a simple cost analysis: 
 
 Start the Amazon Q CLI chat interface and verify the output shows the MCP server being loaded and initialized: q chat 
 In the chat interface, enter the following prompt:Please create a cost analysis for a simple web application with an Application Load Balancer, two t3.medium EC2 instances, and an RDS db.t3.medium MySQL database. Assume 730 hours of usage per month and moderate traffic of about 100 GB data transfer. Convert estimation to a PDF format. 
 Amazon Q CLI will ask for permission to trust the tool that is being used; enter t to trust it. Amazon Q should generate and display a detailed cost analysis. Your output should look like the following screenshot.  If you see the cost analysis report, your environment is set up correctly. If you encounter issues, verify that Amazon Q CLI can access the MCP servers by making sure you installed install the necessary tools and the servers are in the ~/.aws/amazonq/mcp.json file. 
 
Configuration options 
The AWS Cost Analysis MCP server supports several configuration options to customize your cost analysis experience: 
 
 Output format ‚Äì Choose between markdown, CSV formats, or PDF (which we installed the package for) for cost reports 
 Pricing model ‚Äì Specify on-demand, reserved instances, or savings plans 
 Assumptions and exclusions ‚Äì Customize the assumptions and exclusions in your cost analysis 
 Detailed cost data ‚Äì Provide specific usage patterns for more accurate estimates 
 
Now that our environment is set up, let‚Äôs create more cost analyses. 
Create AWS Cost Analysis reports 
In this section, we walk through the process of creating AWS cost analysis reports using Amazon Q CLI with the AWS Cost Analysis MCP server. 
When you provide a prompt to Amazon Q CLI, the AWS Cost Analysis MCP server completes the following steps: 
 
 Interpret your requirements. 
 Retrieve pricing data from AWS pricing sources. 
 Generate a detailed cost analysis report. 
 Provide optimization recommendations. 
 
This process happens seamlessly, so you can focus on describing what you want rather than how to create it. 
AWS Cost Analysis reports typically include the following information: 
 
 Service costs ‚Äì Breakdown of costs by AWS service 
 Unit pricing ‚Äì Detailed unit pricing information 
 Usage quantities ‚Äì Estimated usage quantities for each service 
 Calculation details ‚Äì Step-by-step calculations showing how costs were derived 
 Assumptions ‚Äì Clearly stated assumptions used in the analysis 
 Exclusions ‚Äì Costs that were not included in the analysis 
 Recommendations ‚Äì Cost optimization suggestions 
 
Example 1: Analyze a serverless application 
Let‚Äôs create a cost analysis for a simple serverless application. Use the following prompt: 
Create a cost analysis for a serverless application using API Gateway, Lambda, and DynamoDB. Assume 1 million API calls per month, average Lambda execution time of 200ms with 512MB memory, and 10GB of DynamoDB storage with 5 million read requests and 1 million write requests per month. Convert estimation to a PDF format. 
Upon entering your prompt, Amazon Q CLI will retrieve pricing data using the get_pricing_from_web or get_pricing_from_api tools, and will use generate_cost_report with awslabscost_analysis_mcp_server. 
 
You should receive an output giving a detailed cost breakdown based on the prompt along with optimization recommendations. 
 
 
The generated cost analysis shows the following information: 
 
 Amazon API Gateway costs for 1 million requests 
 AWS Lambda costs for compute time and requests 
 Amazon DynamoDB costs for storage, read, and write capacity 
 Total monthly cost estimate 
 Cost optimization recommendations 
 
Example 2: Analyze multi-tier architectures 
Multi-tier architectures separate applications into functional layers (presentation, application, and data) to improve scalability and security. This example analyzes costs for implementing such an architecture on AWS with components for each tier: 
Create a cost analysis for a three-tier web application with a presentation tier (ALB and CloudFront), application tier (ECS with Fargate), and data tier (Aurora PostgreSQL). Include costs for 2 Fargate tasks with 1 vCPU and 2GB memory each, an Aurora db.r5.large instance with 100GB storage, an Application Load Balancer with 10 
This time, we are formatting it into both PDF and DOCX. 
 
 
The cost analysis shows the following information: 
 
 Presentation tier costs (Application Load Balancer and AWS CloudFront) 
 Application tier costs (Amazon Elastic Container Service (Amazon ECS) and AWS Fargate) 
 Data tier costs (Amazon Aurora PostgreSQL-Compatible Edition) 
 Detailed breakdown of each component‚Äôs pricing 
 Total monthly cost estimate 
 Cost optimization recommendations for each tier 
 
Example 3: Compare deployment options 
When deploying containers on AWS, choosing between Amazon ECS with Amazon Elastic Compute Cloud (Amazon EC2) or Fargate involves different cost structures and management overhead. This example compares these options to determine the most cost-effective solution for a specific workload: 
Compare the costs between running a containerized application on ECS with EC2 launch type versus Fargate launch type. Assume 4 containers each needing 1 vCPU and 2GB memory, running 24/7 for a month. For EC2, use t3.medium instances. Provide a recommendation on which option is more cost-effective for this workload. Convert estimation to a HTML webpage. 
This time, we are formatting it into a HTML webpage. 
 
 
The cost comparison includes the following information: 
 
 Amazon ECS with Amazon EC2 launch type costs 
 Amazon ECS with Fargate launch type costs 
 Detailed breakdown of each option‚Äôs pricing components 
 Side-by-side comparison of total costs 
 Recommendations for the most cost-effective option 
 Considerations for when each option might be preferred 
 
Real-world examples 
Let‚Äôs explore some real-world architecture patterns and how to analyze their costs using Amazon Q CLI with the AWS Cost Analysis MCP server. 
Ecommerce platform 
Ecommerce platforms require scalable, resilient architectures with careful cost management. These systems typically use microservices to handle various functions independently while maintaining high availability. This example analyzes costs for a complete ecommerce solution with multiple components serving moderate traffic levels: 
Create a cost analysis for an e-commerce platform with microservices architecture. Include components for product catalog, shopping cart, checkout, payment processing, order management, and user authentication. Assume moderate traffic of 500,000 monthly active users, 2 million page views per day, and 50,000 orders per month. Ensure the analysis follows AWS best practices for cost optimization. Convert estimation to a PDF format. 
 
The cost analysis includes the following key components: 
 
 Frontend delivery costs (Amazon Simple Storage Service (Amazon S3) and CloudFront) 
 API Gateway and Lambda costs for serverless components 
 Container costs for microservices (Amazon Elastic Kubernetes Service (Amazon EKS) and Amazon ECS) 
 Database costs (Amazon Relational Database Service (Amazon RDS) and DynamoDB) 
 Caching costs (Amazon ElastiCache) 
 Storage and data transfer costs 
 Monitoring and security costs 
 Total monthly cost estimate 
 Cost optimization recommendations for each component 
 Reserved instance and savings plan opportunities 
 
Data analytics platform 
Modern data analytics platforms need to efficiently ingest, store, process, and visualize large volumes of data while managing costs effectively. This example examines the AWS services and costs involved in building a complete analytics pipeline handling significant daily data volumes with multiple user access requirements: 
Create a cost analysis for a data analytics platform processing 500GB of new data daily. Include components for data ingestion (Kinesis), storage (S3), processing (EMR), and visualization (QuickSight). Assume 50 users accessing dashboards daily and data retention of 90 days. Ensure the analysis follows AWS best practices for cost optimization and includes recommendations for cost-effective scaling. Convert estimation to a HTML webpage. 
 
The cost analysis includes the following key components: 
 
 Data ingestion costs (Amazon Kinesis Data Streams and Amazon Data Firehose) 
 Storage costs (Amazon S3 with lifecycle policies) 
 Processing costs (Amazon EMR cluster) 
 Visualization costs (Amazon QuickSight) 
 Data transfer costs between services 
 Total monthly cost estimate 
 Cost optimization recommendations for each component 
 Scaling considerations and their cost implications 
 
Clean up 
If you no longer need to use the AWS Cost Analysis MCP server with Amazon Q CLI, you can remove it from your configuration: 
 
 Open your ~/.aws/amazonq/mcp.json file. 
 Remove or comment out the ‚Äúawslabs.cost-analysis-mcp-server‚Äù entry. 
 Save the file. 
 
This will prevent the server from being loaded when you start Amazon Q CLI in the future. 
Conclusion 
In this post, we explored how to use Amazon Q CLI with the AWS Cost Analysis MCP server to create detailed cost analyses that use accurate AWS pricing data. This approach offers significant advantages over traditional cost estimation methods: 
 
 Time savings ‚Äì Generate complex cost analyses in minutes instead of hours 
 Accuracy ‚Äì Make sure estimates use the latest AWS pricing information 
 Comprehensive ‚Äì Include relevant cost components and considerations 
 Actionable ‚Äì Receive specific optimization recommendations 
 Iterative ‚Äì Quickly compare different scenarios through simple prompts 
 Validation ‚Äì Check estimates against official AWS pricing 
 
As you continue exploring AWS cost analysis, we encourage you to deepen your knowledge by learning more about the Model Context Protocol (MCP) to understand how it enhances the capabilities of Amazon Q. For hands-on cost estimation, the AWS Pricing Calculator offers an interactive experience to model and compare different deployment scenarios. To make sure your architectures follow financial best practices, the AWS Well-Architected Framework Cost Optimization Pillar provides comprehensive guidance on building cost-efficient systems. And to stay at the cutting edge of these tools, keep an eye on updates to the official AWS MCP servers‚Äîthey‚Äôre constantly evolving with new features to make your cost analysis experience even more powerful and accurate. 
 
About the Authors 
Joel Asante, an Austin-based Solutions Architect at Amazon Web Services (AWS), works with GovTech (Government Technology) customers. With a strong background in data science and application development, he brings deep technical expertise to creating secure and scalable cloud architectures for his customers. Joel is passionate about data analytics, machine learning, and robotics, leveraging his development experience to design innovative solutions that meet complex government requirements. He holds 13 AWS certifications and enjoys family time, fitness, and cheering for the Kansas City Chiefs and Los Angeles Lakers in his spare time. 
Dunieski Otano is a Solutions Architect at Amazon Web Services based out of Miami, Florida. He works with World Wide Public Sector MNO (Multi-International Organizations) customers. His passion is Security, Machine Learning and Artificial Intelligence, and Serverless. He works with his customers to help them build and deploy high available, scalable, and secure solutions. Dunieski holds 14 AWS certifications and is an AWS Golden Jacket recipient. In his free time, you will find him spending time with his family and dog, watching a great movie, coding, or flying his drone. 
Varun Jasti&nbsp;is a Solutions Architect at Amazon Web Services, working with AWS Partners to design and scale artificial intelligence solutions for public sector use cases to meet compliance standards. With a background in Computer Science, his work covers broad range of ML use cases primarily focusing on LLM training/inferencing and computer vision. In his spare time, he loves playing tennis and swimming.
‚Ä¢ Tailor responsible AI with new safeguard tiers in Amazon Bedrock Guardrails
  Amazon Bedrock Guardrails provides configurable safeguards to help build trusted generative AI applications at scale. It provides organizations with integrated safety and privacy safeguards that work across multiple foundation models (FMs), including models available in Amazon Bedrock, as well as models hosted outside Amazon Bedrock from other model providers and cloud providers. With the standalone ApplyGuardrail API, Amazon Bedrock Guardrails offers a model-agnostic and scalable approach to implementing responsible AI policies for your generative AI applications. Guardrails currently offers six key safeguards: content filters, denied topics, word filters, sensitive information filters, contextual grounding checks, and Automated Reasoning checks (preview), to help prevent unwanted content and align AI interactions with your organization‚Äôs responsible AI policies. 
As organizations strive to implement responsible AI practices across diverse use cases, they face the challenge of balancing safety controls with varying performance and language requirements across different applications, making a one-size-fits-all approach ineffective. To address this, we‚Äôve introduced safeguard tiers for Amazon Bedrock Guardrails, so you can choose appropriate safeguards based on your specific needs. For instance, a financial services company can implement comprehensive, multi-language protection for customer-facing AI assistants while using more focused, lower-latency safeguards for internal analytics tools, making sure each application upholds responsible AI principles with the right level of protection without compromising performance or functionality. 
In this post, we introduce the new safeguard tiers available in Amazon Bedrock Guardrails, explain their benefits and use cases, and provide guidance on how to implement and evaluate them in your AI applications. 
Solution overview 
Until now, when using Amazon Bedrock Guardrails, you were provided with a single set of the safeguards associated to specific AWS Regions and a limited set of languages supported. The introduction of safeguard tiers in Amazon Bedrock Guardrails provides three key advantages for implementing AI safety controls: 
 
 A tier-based approach that gives you control over which guardrail implementations you want to use for content filters and denied topics, so you can select the appropriate protection level for each use case. We provide more details about this in the following sections. 
 Cross-Region Inference Support (CRIS) for Amazon Bedrock Guardrails, so you can use compute capacity across multiple Regions, achieving better scaling and availability for your guardrails. With this, your requests get automatically routed during guardrail policy evaluation to the optimal Region within your geography, maximizing available compute resources and model availability. This helps maintain guardrail performance and reliability when demand increases. There‚Äôs no additional cost for using CRIS with Amazon Bedrock Guardrails, and you can select from specific guardrail profiles for controlling model versioning and future upgrades. 
 Advanced capabilities as a configurable tier option for use cases where more robust protection or broader language support are critical priorities, and where you can accommodate a modest latency increase. 
 
Safeguard tiers are applied at the guardrail policy level, specifically for content filters and denied topics. You can tailor your protection strategy for different aspects of your AI application. Let‚Äôs explore the two available tiers: 
 
 Classic tier (default): 
   
   Maintains the existing behavior of Amazon Bedrock Guardrails 
   Limited language support: English, French, and Spanish 
   Does not require CRIS for Amazon Bedrock Guardrails 
   Optimized for lower-latency applications 
    
 Standard tier: 
   
   Provided as a new capability that you can enable for existing or new guardrails 
   Multilingual support for more than 60 languages 
   Enhanced robustness against prompt typos and manipulated inputs 
   Enhanced prompt attack protection covering modern jailbreak and prompt injection techniques, including token smuggling, AutoDAN, and many-shot, among others 
   Enhanced topic detection with improved understanding and handling of complex topics 
   Requires the use of CRIS for Amazon Bedrock Guardrails and might have a modest increase in latency profile compared to the Classic tier option 
    
 
You can select each tier independently for content filters and denied topics policies, allowing for mixed configurations within the same guardrail, as illustrated in the following hierarchy. With this flexibility, companies can implement the right level of protection for each specific application. 
 
 Policy: Content filters 
   
   Tier: Classic or Standard 
    
 Policy: Denied topics 
   
   Tier: Classic or Standard 
    
 Other policies: Word filters, sensitive information filters, contextual grounding checks, and Automated Reasoning checks (preview) 
 
To illustrate how these tiers can be applied, consider a global financial services company deploying AI in both customer-facing and internal applications: 
 
 For their customer service AI assistant, they might choose the Standard tier for both content filters and denied topics, to provide comprehensive protection across many languages. 
 For internal analytics tools, they could use the Classic tier for content filters prioritizing low latency, while implementing the Standard tier for denied topics to provide robust protection against sensitive financial information disclosure. 
 
You can configure the safeguard tiers for content filters and denied topics in each guardrail through the AWS Management Console, or programmatically through the Amazon Bedrock SDK and APIs. You can use a new or existing guardrail. For information on how to create or modify a guardrail, see Create your guardrail. 
Your existing guardrails are automatically set to the Classic tier by default to make sure you have no impact on your guardrails‚Äô behavior. 
Quality enhancements with the Standard tier 
According to our tests, the new Standard tier improves harmful content filtering recall by more than 15% with a more than 7% gain in balanced accuracy compared to the Classic tier. A key differentiating feature of the new Standard tier is its multilingual support, maintaining strong performance with over 78% recall and over 88% balanced accuracy for the most common 14 languages.The enhancements in protective capabilities extend across several other aspects. For example, content filters for prompt attacks in the Standard tier show a 30% improvement in recall and 16% gain in balanced accuracy compared to the Classic tier, while maintaining a lower false positive rate. For denied topic detection, the new Standard tier delivers a 32% increase in recall, resulting in an 18% improvement in balanced accuracy.These substantial evolutions in detection capabilities for Amazon Bedrock Guardrails, combined with consistently low false positive rates and robust multilingual performance, also represent a significant advancement in content protection technology compared to other commonly available solutions. The multilingual improvements are particularly noteworthy, with the new Standard tier in Amazon Bedrock Guardrails showing consistent performance gains of 33‚Äì49% in recall across different language evaluations compared to other competitors‚Äô options. 
Benefits of safeguard tiers 
Different AI applications have distinct safety requirements based on their audience, content domain, and geographic reach. For example: 
 
 Customer-facing applications often require stronger protection against potential misuse compared to internal applications 
 Applications serving global customers need guardrails that work effectively across many languages 
 Internal enterprise tools might prioritize controlling specific topics in just a few primary languages 
 
The combination of the safeguard tiers with CRIS for Amazon Bedrock Guardrails also addresses various operational needs with practical benefits that go beyond feature differences: 
 
 Independent policy evolution ‚Äì Each policy (content filters or denied topics) can evolve at its own pace without disrupting the entire guardrail system. You can configure these with specific guardrail profiles in CRIS for controlling model versioning in the models powering your guardrail policies. 
 Controlled adoption ‚Äì You decide when and how to adopt new capabilities, maintaining stability for production applications. You can continue to use Amazon Bedrock Guardrails with your previous configurations without changes and only move to the new tiers and CRIS configurations when you consider it appropriate. 
 Resource efficiency ‚Äì You can implement enhanced protections only where needed, balancing security requirements with performance considerations. 
 Simplified migration path ‚Äì When new capabilities become available, you can evaluate and integrate them gradually by policy area rather than facing all-or-nothing choices. This also simplifies testing and comparison mechanisms such as A/B testing or blue/green deployments for your guardrails. 
 
This approach helps organizations balance their specific protection requirements with operational considerations in a more nuanced way than a single-option system could provide. 
Configure safeguard tiers on the Amazon Bedrock console 
On the Amazon Bedrock console, you can configure the safeguard tiers for your guardrail in the Content filters tier or Denied topics tier sections by selecting your preferred tier. 
 
 
Use of the new Standard tier requires setting up cross-Region inference for Amazon Bedrock Guardrails, choosing the guardrail profile of your choice. 
 
Configure safeguard tiers using the AWS SDK 
You can also configure the guardrail‚Äôs tiers using the AWS SDK. The following is an example to get started with the Python SDK: 
 
 import boto3
import json

bedrock = boto3.client(
    "bedrock",
    region_name="us-east-1"
)

# Create a guardrail with Standard tier for both Content Filters and Denied Topics
response = bedrock.create_guardrail(
    name="enhanced-safety-guardrail",
    # cross-Region is required for STANDARD tier
    crossRegionConfig={
        'guardrailProfileIdentifier': 'us.guardrail.v1:0'
    },
    # Configure Denied Topics with Standard tier
    topicPolicyConfig={
        "topicsConfig": [
            {
                "name": "Financial Advice",
                "definition": "Providing specific investment advice or financial recommendations",
                "type": "DENY",
                "inputEnabled": True,
                "inputAction": "BLOCK",
                "outputEnabled": True,
                "outputAction": "BLOCK"
            }
        ],
        "tierConfig": {
            "tierName": "STANDARD"
        }
    },
    # Configure Content Filters with Standard tier
    contentPolicyConfig={
        "filtersConfig": [
            {
                "inputStrength": "HIGH",
                "outputStrength": "HIGH",
                "type": "SEXUAL"
            },
            {
                "inputStrength": "HIGH",
                "outputStrength": "HIGH",
                "type": "VIOLENCE"
            }
        ],
        "tierConfig": {
            "tierName": "STANDARD"
        }
    },
    blockedInputMessaging="I cannot respond to that request.",
    blockedOutputsMessaging="I cannot provide that information."
) 
 
Within a given guardrail, the content filter and denied topic policies can be configured with its own tier independently, giving you granular control over how guardrails behave. For example, you might choose the Standard tier for content filtering while keeping denied topics in the Classic tier, based on your specific requirements. 
For migrating existing guardrails‚Äô configurations to use the Standard tier, add the sections highlighted in the preceding example for crossRegionConfig and tierConfig to your current guardrail definition. You can do this using the UpdateGuardrail API, or create a new guardrail with the CreateGuardrail API. 
Evaluating your guardrails 
To thoroughly evaluate your guardrails‚Äô performance, consider creating a test dataset that includes the following: 
 
 Safe examples ‚Äì Content that should pass through guardrails 
 Harmful examples ‚Äì Content that should be blocked 
 Edge cases ‚Äì Content that tests the boundaries of your policies 
 Examples in multiple languages ‚Äì Especially important when using the Standard tier 
 
You can also rely on openly available datasets for this purpose. Ideally, your dataset should be labeled with the expected response for each case for assessing accuracy and recall of your guardrails. 
With your dataset ready, you can use the Amazon Bedrock ApplyGuardrail API as shown in the following example to efficiently test your guardrail‚Äôs behavior for user inputs without invoking FMs. This way, you can save the costs associated with the large language model (LLM) response generation. 
 
 import boto3
import json

bedrock_runtime = boto3.client(
    "bedrock-runtime",
    region_name="us-east-1"
)

# Test the guardrail with potentially problematic content
content = [
    {
        "text": {
            "text": "Your test prompt here"
        }
    }
]

response = bedrock_runtime.apply_guardrail(
    content=content,
    source="INPUT",
    guardrailIdentifier="your-guardrail-id",
    guardrailVersion="DRAFT"
)

print(json.dumps(response, indent=2, default=str)) 
 
Later, you can repeat the process for the outputs of the LLMs if needed. For this, you can use the ApplyGuardrail API if you want an independent evaluation for models in AWS or outside in another provider, or you can directly use the Converse API if you intend to use models in Amazon Bedrock. When using the Converse API, the inputs and outputs are evaluated with the same invocation request, optimizing latency and reducing coding overheads. 
Because your dataset is labeled, you can directly implement a mechanism for assessing the accuracy, recall, and potential false negatives or false positives through the use of libraries like SKLearn Metrics: 
 
 # scoring script
# labels and preds store list of ground truth label and guardrails predictions

from sklearn.metrics import confusion_matrix

tn, fp, fn, tp = confusion_matrix(labels, preds, labels=[0, 1]).ravel()

recall = tp / (tp + fn) if (tp + fn) != 0 else 0
fpr = fp / (fp + tn) if (fp + tn) != 0 else 0
balanced_accuracy = 0.5 * (recall + 1 - fpr) 
 
Alternatively, if you don‚Äôt have labeled data or your use cases have subjective responses, you can also rely on mechanisms such as LLM-as-a-judge, where you pass the inputs and guardrails‚Äô evaluation outputs to an LLM for assessing a score based on your own predefined criteria. For more information, see Automate building guardrails for Amazon Bedrock using test-drive development. 
Best practices for implementing tiers 
We recommend considering the following aspects when configuring your tiers for Amazon Bedrock Guardrails: 
 
 Start with staged testing ‚Äì Test both tiers with a representative sample of your expected inputs and responses before making broad deployment decisions. 
 Consider your language requirements ‚Äì If your application serves users in multiple languages, the Standard tier‚Äôs expanded language support might be essential. 
 Balance safety and performance ‚Äì Evaluate both the accuracy improvements and latency differences to make informed decisions. Consider if you can afford a few additional milliseconds of latency for improved robustness with the Standard tier or prefer a latency-optimized option for more straight forward evaluations with the Classic tier. 
 Use policy-level tier selection ‚Äì Take advantage of the ability to select different tiers for different policies to optimize your guardrails. You can choose separate tiers for content filters and denied topics, while combining with the rest of the policies and features available in Amazon Bedrock Guardrails. 
 Remember cross-Region requirements ‚Äì The Standard tier requires cross-Region inference, so make sure your architecture and compliance requirements can accommodate this. With CRIS, your request originates from the Region where your guardrail is deployed, but it might be served from a different Region from the ones included in the guardrail inference profile for optimizing latency and availability. 
 
Conclusion 
The introduction of safeguard tiers in Amazon Bedrock Guardrails represents a significant step forward in our commitment to responsible AI. By providing flexible, powerful, and evolving safety tools for generative AI applications, we‚Äôre empowering organizations to implement AI solutions that are not only innovative but also ethical and trustworthy. This capabilities-based approach enables you to tailor your responsible AI practices to each specific use case. You can now implement the right level of protection for different applications while creating a path for continuous improvement in AI safety and ethics.The new Standard tier delivers significant improvements in multilingual support and detection accuracy, making it an ideal choice for many applications, especially those serving diverse global audiences or requiring enhanced protection. This aligns with responsible AI principles by making sure AI systems are fair and inclusive across different languages and cultures. Meanwhile, the Classic tier remains available for use cases prioritizing low latency or those with simpler language requirements, allowing organizations to balance performance with protection as needed. 
By offering these customizable protection levels, we‚Äôre supporting organizations in their journey to develop and deploy AI responsibly. This approach helps make sure that AI applications are not only powerful and efficient but also align with organizational values, comply with regulations, and maintain user trust. 
To learn more about safeguard tiers in Amazon Bedrock Guardrails, refer to Detect and filter harmful content by using Amazon Bedrock Guardrails, or visit the Amazon Bedrock console to create your first tiered guardrail. 
 
About the Authors 
 Koushik Kethamakka is a Senior Software Engineer at AWS, focusing on AI/ML initiatives. At Amazon, he led real-time ML fraud prevention systems for Amazon.com before moving to AWS to lead development of AI/ML services like Amazon Lex and Amazon Bedrock. His expertise spans product and system design, LLM hosting, evaluations, and fine-tuning. Recently, Koushik‚Äôs focus has been on LLM evaluations and safety, leading to the development of products like Amazon Bedrock Evaluations and Amazon Bedrock Guardrails. Prior to joining Amazon, Koushik earned his MS from the University of Houston. 
Hang Su is a Senior Applied Scientist at AWS AI. He has been leading the Amazon Bedrock Guardrails Science team. His interest lies in AI safety topics, including harmful content detection, red-teaming, sensitive information detection, among others. 
Shyam Srinivasan is on the Amazon Bedrock product team. He cares about making the world a better place through technology and loves being part of this journey. In his spare time, Shyam likes to run long distances, travel around the world, and experience new cultures with family and friends. 
Aartika Sardana Chandras is a Senior Product Marketing Manager for AWS Generative AI solutions, with a focus on Amazon Bedrock. She brings over 15 years of experience in product marketing, and is dedicated to empowering customers to navigate the complexities of the AI lifecycle. Aartika is passionate about helping customers leverage powerful AI technologies in an ethical and impactful manner. 
Satveer Khurpa is a Sr. WW Specialist Solutions Architect, Amazon Bedrock at Amazon Web Services, specializing in Amazon Bedrock security. In this role, he uses his expertise in cloud-based architectures to develop innovative generative AI solutions for clients across diverse industries. Satveer‚Äôs deep understanding of generative AI technologies and security principles allows him to design scalable, secure, and responsible applications that unlock new business opportunities and drive tangible value while maintaining robust security postures. 
Antonio Rodriguez is a Principal Generative AI Specialist Solutions Architect at Amazon Web Services. He helps companies of all sizes solve their challenges, embrace innovation, and create new business opportunities with Amazon Bedrock. Apart from work, he loves to spend time with his family and play sports with his friends.
‚Ä¢ Structured data response with Amazon Bedrock: Prompt Engineering and Tool Use
  Generative AI is revolutionizing industries by streamlining operations and enabling innovation. While textual chat interactions with GenAI remain popular, real-world applications often depend on structured data for APIs, databases, data-driven workloads, and rich user interfaces. Structured data can also enhance conversational AI, enabling more reliable and actionable outputs. A key challenge is that LLMs (Large Language Models) are inherently unpredictable, which makes it difficult for them to produce consistently structured outputs like JSON. This challenge arises because their training data mainly includes unstructured text, such as articles, books, and websites, with relatively few examples of structured formats. As a result, LLMs can struggle with precision when generating JSON outputs, which is crucial for seamless integration into existing APIs and databases. Models vary in their ability to support structured responses, including recognizing data types and managing complex hierarchies effectively. These capabilities can make a difference when choosing the right model. 
This blog demonstrates how Amazon Bedrock, a managed service for securely accessing top AI models, can help address these challenges by showcasing two alternative options: 
 
 Prompt Engineering:&nbsp;A straightforward approach to shaping structured outputs using well-crafted prompts. 
 Tool Use with the Bedrock Converse API:&nbsp;An advanced method that enables better control, consistency, and native JSON schema integration. 
 
We will use a customer review analysis example to demonstrate how Bedrock generates structured outputs, such as sentiment scores, with simplified Python code. 
Building a prompt engineering solution 
This section will demonstrate how to use prompt engineering effectively to generate structured outputs using Amazon Bedrock. Prompt engineering involves crafting precise input prompts to guide large language models (LLMs) in producing consistent and structured responses. It is a fundamental technique for developing Generative AI applications, particularly when structured outputs are required.Here are the five key steps we will follow: 
 
 Configure the Bedrock client and runtime parameters. 
 Create a JSON schema for structured outputs. 
 Craft a prompt and guide the model with clear instructions and examples. 
 Add a customer review as input data to analyse. 
 Invoke Bedrock, call the model, and process the response. 
 
While we demonstrate customer review analysis to generate a JSON output, these methods can also be used with other formats like XML or CSV. 
Step 1: Configure Bedrock 
To begin, we‚Äôll set up some constants and initialize a Python Bedrock client connection object using the Python Boto3 SDK for Bedrock runtime, which facilitates interaction with Bedrock: 
 
The REGION specifies the AWS region for model execution, while the MODEL_ID identifies the specific Bedrock model. The TEMPERATURE constant controls the output randomness, where higher values increase creativity, and lower values maintain precision, such as when generating structured output. MAX_TOKENS determines the output length, balancing cost-efficiency and data completeness. 
Step 2: Define the Schema 
Defining a schema is essential for facilitating structured and predictable model outputs, maintaining data integrity, and enabling seamless API integration. Without a well-defined schema, models may generate inconsistent or incomplete responses, leading to errors in downstream applications. The JSON standard schema used in the code below serves as a blueprint for structured data generation, guiding the model on how to format its output with explicit instructions. 
Let‚Äôs create a JSON schema for customer reviews with three required fields:&nbsp;reviewId&nbsp;(string, max 50 chars),&nbsp;sentiment (number, -1 to 1), and&nbsp;summary&nbsp;(string, max 200 chars). 
 
Step 3: Craft the Prompt text 
To generate consistent, structured, and accurate responses, prompts must be clear and well-structured, as LLMs rely on precise input to produce reliable outputs. Poorly designed prompts can lead to ambiguity, errors, or formatting issues, disrupting structured workflows, so we follow these best practices: 
 
 Clearly outline the AI‚Äôs role and objectives to avoid ambiguity. 
 Divide tasks into smaller, manageable numbered steps for clarity. 
 Indicate that a JSON schema will be provided (see Step 5 below) to maintain a consistent and valid structure. 
 Use one-shot prompting with a sample output to guide the model; add more examples if needed for consistency, but avoid too many, as they may limit the model‚Äôs ability to handle new inputs. 
 Define how to handle missing or invalid data. 
 
 
Step 4: Integrate Input Data 
For demonstration purposes, we‚Äôll include a review text in the prompt as a Python variable: 
 
Separating the input data with &lt;input&gt; tags improve readability and clarity, making it straightforward to identify and reference. This hardcoded input simulates real-world data integration. For production use, you might dynamically populate input data from APIs or user submissions. 
Step 5: Call Bedrock 
In this section, we construct a Bedrock request by defining a body object that includes the JSON schema, prompt, and input review data from previous steps. This structured request makes sure the model receives clear instructions, adheres to a predefined schema, and processes sample input data correctly. Once the request is prepared, we invoke Amazon Bedrock to generate a structured JSON response. 
 
We reuse the&nbsp;MAX_TOKENS,&nbsp;TEMPERATURE, and&nbsp;MODEL_ID constants defined in Step 1. The&nbsp;body&nbsp;object has essential inference configurations like&nbsp;anthropic_version&nbsp;for model compatibility and the&nbsp;messages&nbsp;array, which includes a single message to provide the model with task instructions, the schema, and the input data. The role defines the ‚Äúspeaker‚Äù in the interaction context, with user value representing the program sending the request. Alternatively, we could simplify the input by combining instructions, schema, and data into one text prompt, which is straightforward to manage but less modular. 
Finally, we use the&nbsp;client.invoke_model&nbsp;method to send the request. After invoking, the model processes the request, and the JSON data must be properly (not explained here) extracted from the Bedrock response. For example: 
 
Tool Use with the Amazon Bedrock Converse API 
In the previous chapter, we explored a solution using Bedrock Prompt Engineering. Now, let‚Äôs look at an alternative approach for generating structured responses with Bedrock. 
We will extend the previous solution by using the Amazon Bedrock Converse API, a consistent interface designed to facilitate multi-turn conversations with Generative AI models. The API abstracts model-specific configurations, including inference parameters, simplifying integration. 
A key feature of the Converse API is Tool Use (also known as Function Calling), which enables the model to execute external tools, such as calling an external API. This method supports standard JSON schema integration directly into tool definitions, facilitating output alignment with predefined formats. Not all Bedrock models support Tool Use, so make sure you check which models are compatible with these feature. 
Building on the previously defined data, the following code provides a straightforward example of Tool Use tailored to our curstomer review use case: 
 
In this code the&nbsp;tool_list&nbsp;defines a custom customer review analysis tool with its input schema and purpose, while the&nbsp;messages&nbsp;provide the earlier defined instructions and input data. Unlike in the previous prompt engineering example we used the earlier defined JSON schema in the definition of a tool. Finally, the&nbsp;client.converse&nbsp;call combines these components, specifying the tool to use and inference configurations, resulting in outputs tailored to the given schema and task. After exploring&nbsp;Prompt Engineering&nbsp;and&nbsp;Tool Use&nbsp;in Bedrock solutions for structured response generation, let‚Äôs now evaluate how different foundation models perform across these approaches. 
Test Results: Claude Models on Amazon Bedrock 
Understanding the capabilities of foundation models in structured response generation is essential for maintaining reliability, optimizing performance, and building scalable, future-proof Generative AI applications with Amazon Bedrock. To evaluate how well models handle structured outputs, we conducted extensive testing of Anthropic‚Äôs Claude models, comparing&nbsp;prompt-based&nbsp;and&nbsp;tool-based&nbsp;approaches across 1,000 iterations per model. Each iteration processed 100 randomly generated items, providing broad test coverage across different input variations.The examples shown earlier in this blog are intentionally simplified for demonstration purposes, where Bedrock performed seamlessly with no issues. To better assess the models under real-world challenges, we used a more complex schema that featured nested structures, arrays, and diverse data types to identify edge cases and potential issues. The outputs were validated for adherence to the JSON format and schema, maintaining consistency and accuracy. The following diagram summarizes the results, showing the number of successful, valid JSON responses for each model across the two demonstrated approaches: Prompt Engineering and Tool Use. 
 
The results demonstrated that all models achieved over 93% success across both approaches, with Tool Use methods consistently outperforming prompt-based ones. While the evaluation was conducted using a highly complex JSON schema, simpler schemas result in significantly fewer issues, often nearly none. Future updates to the models are expected to further enhance performance. 
Final Thoughts 
In conclusion, we demonstrated two methods for generating structured responses with Amazon Bedrock: Prompt Engineering and Tool Use with the Converse API. Prompt Engineering is flexible, works with Bedrock models (including those without Tool Use support), and handles various schema types (e.g., Open API schemas), making it a great starting point. However, it can be fragile, requiring exact prompts and struggling with complex needs. On the other hand, Tool Use offers greater reliability, consistent results, seamless API integration, and runtime validation of JSON schema for enhanced control. 
For simplicity, we did not demonstrate a few areas in this blog. Other techniques for generating structured responses include using models with built-in support for configurable response formats, such as JSON, when invoking models, or leveraging constraint decoding techniques with third-party libraries like LMQL. Additionally, generating structured data with GenAI can be challenging due to issues like invalid JSON, missing fields, or formatting errors. To maintain data integrity and handle unexpected outputs or API failures, effective error handling, thorough testing, and validation are essential. 
To try the Bedrock techniques demonstrated in this blog, follow the steps to Run example Amazon Bedrock API requests through the AWS SDK for Python (Boto3). With pay-as-you-go pricing, you‚Äôre only charged for API calls, so little to no cleanup is required after testing. For more details on best practices, refer to the Bedrock prompt engineering guidelines and model-specific documentation, such as Anthropic‚Äôs best practices. 
Structured data is key to leveraging Generative AI in real-world scenarios like APIs, data-driven workloads, and rich user interfaces beyond text-based chat. Start using Amazon Bedrock today to unlock its potential for reliable structured responses. 
 
About the authors 
Adam Nemeth is a Senior Solutions Architect at AWS, where he helps global financial customers embrace cloud computing through architectural guidance and technical support. With over 24 years of IT expertise, Adam previously worked at UBS before joining AWS. He lives in Switzerland with his wife and their three children. 
Dominic Searle is a Senior Solutions Architect at Amazon Web Services, where he has had the pleasure of working with Global Financial Services customers as they explore how Generative AI can be integrated into their technology strategies. Providing technical guidance, he enjoys helping customers effectively leverage AWS Services to solve real business problems.
‚Ä¢ Using Amazon SageMaker AI Random Cut Forest for NASA‚Äôs Blue Origin spacecraft sensor data
  The successful deorbit, descent, and landing of spacecraft on the Moon requires precise control and monitoring of vehicle dynamics. Anomaly detection provides a unique utility for identifying important states that might represent vehicle behaviors of interest. By producing unique vehicle behavior points, critical spacecraft system states can be identified to be more appropriately addressed and potentially better understood. These identified states can be invaluable for efforts such as system failure mitigation, engineering design improvements, and mission planning. Today, space missions have become more frequent and complex, and the volume of telemetry data generated has grown exponentially. With this growth, methods of analyzing this data for anomalies need to effectively scale and without risking missing subtle, but important deviations in spacecraft behavior. Fortunately, AWS uses powerful AI/ML applications within Amazon SageMaker AI that can address these needs. 
In this post, we demonstrate how to use SageMaker AI to apply the Random Cut Forest (RCF) algorithm to detect anomalies in spacecraft position, velocity, and quaternion orientation data from NASA and Blue Origin‚Äôs demonstration of lunar Deorbit, Descent, and Landing Sensors (BODDL-TP). The presented analysis focuses on detecting anomalies in spacecraft dynamics data, including positions, velocities, and quaternion orientations. 
Solution overview 
This solution provides an effective approach to anomaly detection in spacecraft data. We begin with data preprocessing and cleaning to produce quality input for our analysis. Using SageMaker AI, we train an RCF model specifically for detecting anomalies in complex spacecraft dynamics data. To handle the substantial volume of telemetry data efficiently, we implement batch processing for anomaly detection across large datasets. 
After the model is trained and anomalies are detected, this solution produces robust visualization capabilities, presenting results with highlighted anomalies for clear interpretation of the findings. We use Amazon Simple Storage Service (Amazon S3) for seamless data storage and retrieval, including both raw data and generated plots. Throughout the implementation, we maintain careful cost management of SageMaker AI instances by deleting resources after they‚Äôre used to achieve efficient utilization while maintaining performance. 
This combination of features creates a scalable, efficient pipeline for processing and analyzing spacecraft dynamics data, making it particularly suitable for space mission applications where reliability and precision are crucial. 
Key concepts 
In this section, we discuss some key concepts of spacecraft dynamics and machine learning (ML) in this solution. 
Position and velocity in spacecraft dynamics 
Position and velocity vectors in our NASA Blue Origin DDL data are represented in the Earth-Centered Earth-Fixed (ECEF) coordinate system. This reference frame rotates with the Earth, making it ideal for tracking spacecraft relative to landing sites on the lunar surface. The position vector [x, y, z] in ECEF pinpoints the spacecraft‚Äôs location in three-dimensional space. Its origin is at Earth‚Äôs center, with the X-axis intersecting the prime meridian at the equator, the Y-axis 90 degrees east in the equatorial plane, and the Z-axis aligned with Earth‚Äôs rotational axis. Measured in meters, this position data can reveal crucial information about orbital descent trajectories, landing approach paths, terminal descent profiles, and final touchdown positioning. Complementing position data, the velocity vector [vx, vy, vz] represents the spacecraft‚Äôs rate of position change in each direction. Measured in meters per second, this velocity data is vital for monitoring descent rates, maintaining safe approach speeds, controlling deceleration profiles, and verifying landing constraints. Our RCF algorithm scrutinizes both position and velocity data for anomalies. In position data, it looks for anomalies that might be caused by unexpected trajectory deviations, unrealistic position jumps, sensor glitches, or data recording errors. For velocity, its detected anomalies might be due to sudden speed changes, unusual acceleration patterns, potential thruster misfires, or navigation system issues. The fusion of position and velocity data offers a comprehensive view of the spacecraft‚Äôs translational motion. When combined with quaternion data describing rotational state, we obtain a complete picture of the spacecraft‚Äôs dynamic state during critical mission phases. These metrics play essential roles in mission planning, real-time monitoring, post-flight analysis, safety verification, C2 (command and control), and overall system performance evaluation. By using these rich datasets and advanced anomaly detection techniques, we enhance our ability to achieve mission success and spacecraft safety throughout the dynamic phases of lunar deorbit, descent, and landing. 
Quaternions in spacecraft dynamics 
Quaternions play a crucial role in spacecraft attitude (orientation) representation. Although Euler angles (roll, pitch, and yaw) are more intuitive, they can suffer from gimbal lock‚Äîa loss of one degree of freedom in certain orientations. Quaternions solve this problem by using a four-parameter representation that avoids such singularities. This representation consists of one scalar component (q0) and three vector components (q1, q2, q3), providing a robust mathematical framework for describing spacecraft orientation. In our NASA Blue Origin DDL data, quaternions serve a vital purpose: they represent the rotation from the spacecraft‚Äôs body-fixed coordinate system (CON) to the ECEF frame. This transformation is fundamental to several critical aspects of spacecraft operation, including maintaining precise attitude control during descent, preserving correct thrust vector orientation, facilitating accurate sensor measurements, and computing landing trajectories. For reliable anomaly detection, quaternion values must satisfy two essential mathematical properties. First, they must maintain unit magnitude, meaning the sum of their squared components (q0¬≤ + q1¬≤ + q2¬≤ + q3¬≤ = 1) equals one. Second, they must demonstrate continuity, avoiding sudden jumps that would indicate physically impossible rotations. These properties help confirm the validity of our orientation measurements and the effectiveness of our anomaly detection system. When our RCF algorithm identifies anomalies in quaternion data, these could signal various issues requiring attention. Such anomalies might indicate sensor malfunctions, attitude control system issues, data transmission errors, or actual problems with spacecraft orientation. By carefully monitoring these quaternion components alongside position and velocity data, we develop a comprehensive understanding of the spacecraft‚Äôs dynamic state during the critical phases of deorbit, descent, and landing. 
The Random Cut Forest algorithm 
Random Cut Forest is an unsupervised algorithm for detecting anomalies in high-dimensional data. The algorithm‚Äôs construction begins by creating multiple decision trees, each built through a process of repeatedly cutting the data space with random hyperplanes. This partitioning continues until each data point is isolated, creating a forest of trees that captures the underlying structure of the data. The novelty of RCF lies in the scoring mechanism. Points located in sparse regions of the data space that require fewer cuts to isolate score higher, while points in dense regions that need more cuts score lower. This fundamental principle allows the algorithm to assign anomaly scores inversely proportional to the number of cuts needed to isolate each point. Higher scores, therefore, indicate potential anomalies, making it straightforward to identify unusual patterns in the data. 
In our spacecraft dynamics context, we apply RCF to 10-dimensional vectors that combine position (three dimensions), velocity (three dimensions), and quaternion orientation (four dimensions). Each vector represents a specific moment in time during the spacecraft‚Äôs mission states. The flight patterns create dense regions in this high-dimensional space, while anomalies appear as isolated points in sparse regions. This data is high-dimensional, multivariate time series, and has no labels, which RCF handles fairly well while maintaining computational efficiency and handling sensor noise. For this use case, RCF is able to detect subtle deviations between data points of spacecraft dynamics while handling the complex relationships between position, velocity, and orientation parameters. These features of RCF make it an effective tool for spacecraft dynamics monitoring analysis and anomaly detection. 
Solution architecture 
The solution architecture implements anomaly detection for NASA-Blue Origin Lunar DDL data using the RCF algorithm, as illustrated in the following diagram. 
 
Our solution‚Äôs data flow begins with public DDL (Deorbit, Descent, and Landing) data securely stored in an S3 bucket. This data is then accessed through a SageMaker AI domain using JupyterLab, providing a powerful and flexible environment for data scientists and engineers. Within JupyterLab, we use a custom notebook to process the raw data and implement our anomaly detection algorithms. 
The core of our solution lies in the processing pipeline. It starts in the JupyterLab notebook, where we train an RCF model using SageMaker AI. After it‚Äôs trained, this model is deployed to a SageMaker AI endpoint, creating a scalable and responsive anomaly detection service. We then feed our spacecraft dynamics data through this model to identify potential anomalies. The pipeline concludes by generating detailed visualizations of these anomalies, providing clear and actionable insights. 
For output, our system saves both the detected anomaly data and the generated plots back to Amazon S3. This makes sure the results are securely stored and accessible for further analysis or reporting. Additionally, we preserve all training data and model outputs in Amazon S3, enabling reproducibility and facilitating iterative improvements to our anomaly detection process. Throughout these operations, we maintain robust security measures, using Amazon Virtual Private Cloud (Amazon VPC) to enforce data privacy and integrity at every step of the process. 
Prerequisites 
Before standing up the project, you must set up the necessary tools and access rights: 
 
 The AWS environment should include an active AWS account with appropriate permissions for running ML workloads, along with the AWS Command Line Interface (AWS CLI) for command line operations installed 
 Access to SageMaker AI is essential for the ML implementation 
 On the development side, Python 3.7 or later needs to be installed, along with several key Python packages: 
   
   Boto3 for AWS service integration 
   Pandas for data manipulation 
   Matplotlib for visualization 
   NumPy for numerical operations 
   The SageMaker AI Python SDK for interacting with the SageMaker services 
    
 
Set up the solution 
The setup process includes accessing the SageMaker AI environment, where all the data analysis and model training is executed. 
 
 On the SageMaker AI console, open the SageMaker domain details page. 
 Open JupyterLab, then create a new Python notebook instance for this project. 
 When the environment is ready, open a terminal in SageMaker AI JupyterLab to clone the project repository using the following commands: 
 
 
 git clone https://github.com/aws-samples/sample-SageMaker-ai-rcf-anomaly-detection-lunar-spacecraft.git
cd sample-SageMaker-ai-rcf-anomaly-detection-lunar-spacecraft 
 
 
 Install the required Python libraries: 
 
pip install -r requirements.txt 
This process will set up the necessary dependencies for running anomaly detection analysis on the spacecraft data. 
Execute anomaly detection 
Update the bucket_name and file_name variables in the script with your S3 bucket and data file names. 
Run the script in JupyterLab as a Jupyter notebook or run as a Python script: python Lunar_DDL_AD.py 
Upon execution, the notebook or script performs a series of automated tasks to analyze the spacecraft data. It begins by loading and preprocessing the raw data, making sure it‚Äôs in the correct format for analysis. Next, it trains and deploys an RCF model using SageMaker AI, establishing the foundation for our anomaly detection system. When the model is operational, it processes the spacecraft dynamics data to identify potential anomalies in position, velocity, and quaternion measurements. Finally, the script generates detailed visualizations of these findings and automatically uploads both the plots and analysis results to Amazon S3 for secure storage and straightforward access. 
Code structure 
The Python implementation centers around an anomaly detection pipeline, structured in the main script. At its core is the AnomalyDetector class, which orchestrates the entire workflow from data ingestion to visualization. This class contains several methods that together process spacecraft telemetry data and identify anomalies. 
The load_and_prepare_data method handles the initial data ingestion and preprocessing, making sure spacecraft measurements are properly formatted for analysis. After the data is prepared, train_and_deploy_model trains the RCF model and deploys it as a SageMaker endpoint. The predict_anomalies method then uses this trained model to identify unusual patterns in the spacecraft‚Äôs position, velocity, and quaternion data. 
For visualization and storage, the plot_results method creates detailed graphs highlighting detected anomalies, and upload_plot_to_s3 makes sure these visualizations are securely stored in Amazon S3 for future reference and centralized access. 
Together, these components create a comprehensive pipeline for processing spacecraft telemetry data and identifying potential anomalies that might warrant further investigation. 
Configuration 
Adjust the following parameters in the script as needed: 
 
 threshold_percentile for the threshold for anomaly classification 
 RCF hyperparameters in train_and_deploy_model: 
   
   feature_dim: Number of input features 
   num_samples_per_tree: Random data points per tree 
   num_trees: Number of trees in the algorithmic forest 
    
 batch_size in predict_anomalies for large datasets 
 
For RCF applications, the hyperparameters and threshold configuration significantly influence anomaly detections. We use the following configuration values for this example: 
 
 threshold_percentile=0.9 
 RCF hyperparameters in train_and_deploy_model(): 
   
   feature_dim=10 
   num_samples_per_tree=512 
   num_trees=100 
    
 batch_size=1000 in predict_anomalies() 
 
SageMaker AI instance type size for training and inference can affect anomaly results, processing time, and cost. In this example, we used an ml.m5.4xlarge instance for both training and inference. 
In addition, SageMaker AI can be integrated with various security features for protecting sensitive data and models. It‚Äôs possible to operate in no internet or VPC only modes so SageMaker AI instances remain isolated within your Amazon VPC. Secure data access can also be achieved through AWS PrivateLink, enabling private connections to Amazon S3 without internet exposure. Also, integration with AWS Identity and Access Management (IAM) provides fine-grained access control through custom user profiles, enforcing data privacy and adhering to the principle of least privilege, such as when using sensitive spacecraft telemetry data. These are some of the security enhancement services that can be applied according to your appropriate use case with SageMaker AI. 
Data 
The script uses public NASA-Blue Origin Demo of Lunar Deorbit, Descent, and Landing Sensors (BODDL-TP) data, which you can download. Make sure your data is in the correct format with columns for timestamps, positions, velocities, and quaternions. 
Results 
The script generates plots for positions, velocities, and quaternions. The respective data is plotted and the anomalies are plotted as an overlay in red. The plots are saved to the specified S3 bucket. Due to the small scale, the positions plot is difficult to observe anomalies. However, the SageMaker AI RCF algorithm can detect them and are highlighted in red. In the following plots, the sharp changes in velocities and quaternions correspond with the anomalies shown. 
 
Unlike the positions plot, the velocities plot shows discontinuities, which are detected as anomalies. This is likely due to rate changes for vehicle maneuvers during the deorbit, descent, and landing demonstration stages. 
 
Similarly to the velocities plot, the quaternions plot shows sharp changes, which are also detected as anomalies. This is likely due to rotational accelerations during vehicle maneuvers of the deorbit, descent, and landing demonstration stages. 
 
These anomalies most likely represent the lunar spacecraft vehicle dynamics at key maneuver stages of the deorbit, descent, and landing demonstration. Momentum wheels, thrusters, and various other C2 applications could be the cause of the observed abrupt positional, velocity, and quaternion changes being detected as anomalous. By having these results, data points of interest are indicated for more precise and potentially valuable analysis for improved vehicle health and status awareness. 
Clean up 
The provided script includes SageMaker AI endpoint deletion after training and inference to avoid any unnecessary charges. If you‚Äôre using JupyterLab and want to further avoid charges, stop the SageMaker AI instance running the RCF JupyterLab Python notebook. 
Conclusion 
In this post, we demonstrated how the SageMaker AI RCF algorithm can effectively detect anomalies in spacecraft dynamics data from NASA and Blue Origin‚Äôs lunar Deorbit, Descent, and Landing demonstration. By detecting anomalies for position, velocity, and quaternion orientation data, we‚Äôve shown how ML can enhance space mission analysis, situational awareness, and autonomy. The built-in algorithm processes complex, multi-dimensional spacecraft telemetry data. Through efficient batch processing, we can analyze large-scale mission data effectively, and our visualization approach enables quick identification of potential issues in spacecraft dynamics. From there, the solution‚Äôs scalability shows the ability adapt to handle varying data volumes and mission durations, making it potentially suitable for a wide range of space applications. Although this solution applies to a lunar mission demonstration, the approach could have broad applications throughout the space industry. You can adapt the same architecture for various space operations, such as landing missions on other celestial bodies, orbital rendezvous, space station docking, and satellite constellations. This integration of AWS services with aerospace applications creates a robust, secure, and scalable platform for space mission analytics, which is becoming increasingly valuable as we continue to execute missions in the space environment. Looking forward, this solution opens many possibilities for enhancement and expansion. Real-time anomaly detection could be implemented for live mission data, providing immediate insights during critical operations. Also, the system could be enhanced by incorporating additional spacecraft parameters and sensor data, and automated alert services could be developed to provide immediate notification of detected anomalies. In addition, further developments might include extending the analysis to incorporate predictive ML models and creating custom metrics tailored to specific mission requirements. These potential advancements would continue to build upon the foundation we‚Äôve established, creating even more powerful tools for spacecraft mission analysis. 
The code and implementation details are available in our GitHub repository, enabling you to adapt and enhance the solution for your specific needs. 
For space operations, the combination of cloud computing and ML have strong potential to play an increasingly crucial role in ensuring mission success. This solution demonstrates just one of many possible applications of AWS services for improving spacecraft mission compute and data analysis. 
To learn more about the AWS services used in this solution, refer to Guide to getting set up with Amazon SageMaker AI, Train a Model with Amazon SageMaker, and the JupyterLab user guide. 
 
About the authors 
Dr. Ian Lunsford is an Aerospace AI Engineer at AWS Professional Services. He integrates cloud services into aerospace applications. Additionally, Ian focuses on building AI/ML solutions using AWS services. 
Nick Biso is a Machine Learning Engineer at AWS Professional Services. He solves complex organizational and technical challenges using data science and engineering. In addition, he builds and deploys AI/ML models on the AWS Cloud. His passion extends to his proclivity for travel and diverse cultural experiences.
‚Ä¢ Build an intelligent multi-agent business expert using Amazon Bedrock
  In this post, we demonstrate how to build a multi-agent system using multi-agent collaboration in Amazon Bedrock Agents to solve complex business questions in the biopharmaceutical industry. We show how specialized agents in research and development (R&amp;D), legal, and finance domains can work together to provide comprehensive business insights by analyzing data from multiple sources. 
Amazon Bedrock Agents and multi-agent collaboration 
Business intelligence and market research enable large and small businesses to capture the trends of the industry, competitive landscape through data, and influences key business strategies. For example, biopharmaceutical companies use data to understand drug market size, clinical trials, prevalence of side effects, and innovation and pitfalls through analyzing patent and legal briefs to form investment strategies. In doing so, organizations face the challenges of accessing and analyzing information scattered across multiple data sources. Consolidating and querying these disparate datasets can be a complex and time-consuming task, requiring developers to navigate different data formats, query languages, and access mechanisms. Additionally, gaining a comprehensive understanding of an organization‚Äôs operations often requires combining data insights from various segments, such as legal, finance, and R&amp;D. 
Generative AI agentic systems have emerged as a promising solution, enabling organizations to use generative AI for autonomous reasoning and action-based tasks. However, many agentic systems to-date are built with a single-agent setup, which poses challenges in a complex business environment. Besides the challenge of managing multiple data sources, encoding information and guidance for multiple business domains might cause the prompt for an agent‚Äôs large language model (LLM) to grow to such an extent that is suffers from ‚Äúforgetting the middle‚Äù of a long context. Therefore, there is a trade-off between the breadth vs. depth of knowledge for each domain that can be encoded in an agent effectively. Additionally, the use of a single LLM with an agent limits cost, latency, and accuracy optimizations for the selected model. 
Amazon Bedrock Agents and its multi-agent collaboration feature provides powerful, enterprise-ready solutions for addressing these challenges and building intelligent and automated agentic systems. As a managed service within the AWS ecosystem, Amazon Bedrock Agents offers seamless integration with AWS data sources, built-in security controls, and enterprise-grade scalability. It contains built-in support for additional Amazon Bedrock features such as Amazon Bedrock Guardrails and Amazon Bedrock Knowledge Bases. The service significantly reduces deployment overhead, empowering developers to focus on agent logic through an API-driven, familiar AWS Cloud environment and console. The supervisor agent model with specialized sub-agents enables efficient distributed problem-solving, breaking down complex tasks with intelligent routing. 
In this post, we discuss how to build a multi-agent system using multi-agent collaboration to solve complex business questions faced in a fictional biopharmaceutical company that requires expertise and data from three specialized domains: R&amp;D, legal, and finance. We demonstrate how data in disparate sources can be combined intelligently to support complex reasoning, and how agent collaboration facilitates open-ended question answering, such as ‚ÄúWhat are the potential legal and financial risks associated with the side effects of therapeutic product X, and how might they affect the company‚Äôs long-term stock performance?‚Äù 
(Below image depicts the roles of supervisor agent and the 3 subagents being used in our pharmaceutical example along with the benefits of using Multi Agent Collaboration. ) 
 
Solution overview 
Our use case centers around PharmaCorp, a fictional pharmaceutical company, which faces the challenge of managing vast amounts of data across its Pharma R&amp;D, Legal, and Finance divisions. Each division works with structured data, such as stock prices, and unstructured data, such as clinical trial reports. The data for each division is located in different data stores, which makes it difficult for teams to access cross-functional insights and slows down decision-making processes. 
To address this, we build a multi-agent system with domain-specific sub-agents for each division using multi-agent collaboration within Amazon Bedrock Agents. These sub-agents efficiently handle data queries and information retrieval, and the main agent passes necessary context between sub-agents and synthesizes insights across divisions. The multi-agent setup empowers PharmaCorp to access expertise and information within minutes that would otherwise take hours of human effort to compile. This approach breaks down data silos and strengthens organizational collaboration. 
The following architecture diagram illustrates the solution setup. 
 
The main agent acts as an orchestrator, asking questions to multiple sub-agents and synthesizing retrieved data: 
 
 The R&amp;D sub-agent has access to clinical trial data through Amazon Athena and unstructured clinical trial reports 
 The legal sub-agent has access to unstructured patents and lawsuit legal briefs 
 The finance sub-agent has access to research budget data through Athena and historical stock price data stored in Amazon Redshift 
 
Each sub-agent has granular permissions to only access the data in its domain. Detailed information about the data and models used and main agent interactions are described in the following sections. 
Dataset 
We generated synthetic data using Anthropic‚Äôs Claude 3.5 Sonnet model, comprised of three domains: Pharma R&amp;D, Legal, and Finance. The domains contain structured data stored in SQL tables and unstructured data that is used in domain knowledge bases. The data can be accessed through the following files: R&amp;D, Legal, Finance. 
Efforts have been made to align synthetic data within and across domains. For example, clinical trial reports map to each trial and side effects in related tables. Rises and dips in stock prices tend to correlate with patents and lawsuits. However, there might still be minor inconsistencies between data. 
Pharma R&amp;D domain 
The Pharma R&amp;D domain has three tables: Drugs, Drug Trials, and Side Effects. Each table is queried from Amazon Simple Storage Service (Amazon S3) through Athena. The Drugs table contains information on the company‚Äôs available products, therapeutic areas, target conditions, mechanisms of action, development phase, discovery year, and lead scientist. The Drug Trials table contains information on specific trials for each drug such as phase, dates, number of participations, and outcomes. The Side Effects table contains side effects, frequency, and severity reported from each trial. 
The unstructured data for the Pharma R&amp;D domain consists of synthetic clinical trial reports for each trial, which contain more detailed information about the trial design, outcomes, and recommendations. 
Legal domain 
The Legal domain has unstructured data consisting of patents and lawsuit legal briefs. The patents contain information about invention background, description, and experimental results. The legal briefs contain information about lawsuit court proceedings, outcomes, and analysis. 
Finance domain 
The Finance domain has two tables: Stock Price and Research Budgets. The Stock Price table is stored in Amazon Redshift and contains PharmaCorp‚Äôs historical monthly stock prices and volume. Amazon Redshift is a database optimized for online analytical processing (OLAP), which generally entails analyzing large amounts of data and performing complex analysis, as might be done by analysts looking at historical stock prices. The Research Budgets table is accessed from Amazon S3 through Athena and contains annual budgets for each department. 
The data setup showcases how a multi-agent framework can synthesize data from multiple data sources and databases. In practice, data could also be stored in other databases such as Amazon Relational Database Service (Amazon RDS). 
Models used 
Anthropic‚Äôs Claude 3 Sonnet, which has a good balance of intelligence and speed, is used in this multi-agent demonstration. With the multi-agent setup, you can also employ a more intelligent or a smaller, faster model depending on the use case and requirements such as accuracy and latency. 
Prerequisites 
To deploy this solution, you need the following prerequisites: 
 
 An active AWS account. 
 Access to Amazon Titan Embeddings G1 ‚Äì Text, Anthropic‚Äôs Claude 3 Sonnet, and Anthropic‚Äôs Claude 3.5 Sonnet on Amazon Bedrock. For instructions, refer to Add or remove access to Amazon Bedrock foundation models. 
 
Deploy the solution 
To deploy the solution resources, we use AWS CloudFormation. The CloudFormation template creates two S3 buckets, two AWS Lambda functions, an Amazon Bedrock agent, an Amazon Bedrock knowledge base, and an Amazon Elastic Compute Cloud (Amazon EC2) instance. 
Download the provided CloudFormation template, then complete the following steps to deploy the stack: 
 
 Open the AWS CloudFormation console (the preferred AWS Regions are us-west-2 or us-east-1 for the solution). 
 Choose Stacks in the navigation pane. 
 Choose Create stack and With new resources (standard). 
 Select Choose existing template and upload the provided CloudFormation template file. 
 Enter a stack name, then choose Next. 
 Leave the stack settings as default and choose Next. 
 Select the acknowledgement check box and create the stack. 
 
After the stack is complete, you can view the new supervisor agent on the Amazon Bedrock console. 
An example of agent collaboration 
After you deploy the solution, you can test the communication among agents that help answer complex questions across PharmaCorp‚Äôs three divisions. For example, we ask the main agent ‚ÄúHow did the results of NeuroClear‚Äôs Phase 2 trials affect PharmaCorp‚Äôs stock price, patent filings, and potential legal risks?‚Äù 
This question requires a comprehensive understanding of the relationships between NeuroClear‚Äôs clinical trial results, financial impacts, and legal outcomes for PharmaCorp. Let‚Äôs see how the multi-agent system addresses this complex query. 
The main agent identifies that it needs input from three specialized sub-agents to fully assess how NeuroClear‚Äôs clinical trial results might impact PharmaCorp‚Äôs legal and financial performance. It breaks down the user‚Äôs question into key components and develops a plan to gather detailed insights from each expert. The following is its chain-of-thought reasoning, task breakdown, and sub-agent routing: 
 
 This is a complex question that requires information from all three divisions to 
provide a comprehensive answer. 
I need to:
Get information about NeuroClear's Phase 2 trial results from Pharmaceutical division
Check stock price movements around the trial period from Finance division
Look for any patent filings and legal implications from Legal division 
 
Then, the main agent asks a question to the R&amp;D sub-agent: 
 
 Please provide details about NeuroClear's Phase 2 clinical trials, including trial dates,
outcomes, success rates, and any significant findings or side effects. 
 
The R&amp;D sub-agent correctly plans and executes its own sequence of steps, which include performing queries and searching its own knowledge base. It responds with the following: 
 
 Based on the information from the knowledge base search and Athena database queries,
here are the key details about NeuroClear's Phase 2 clinical trials:

* There were two Phase 2 trials conducted for NeuroClear, a novel drug for treating 
Alzheimer's disease.
* The first trial ran from June 2019 to May 2020 with 250 participants. It showed a 
moderate success rate of 0.65 in slowing cognitive decline compared to placebo.
* The second trial ran from August 2021 to August 2022 with 400 participants. It had 
a slightly higher success rate of 0.68.
* Both trials were randomized, double-blind, and placebo-controlled. The dosages tested 
were 25mg, 50mg, and 75mg once daily.
* Primary endpoints were change in ADAS-Cog score and secondary endpoints included CDR-SB, 
hippocampal volume, and CSF biomarkers.
* Notable side effects observed included insomnia (frequency 0.14-0.15, severity 3), 
anxiety (frequency 0.12-0.13, severity 3), tremors (frequency 0.09-0.1, severity 2), 
and weight loss (frequency 0.08, severity 2).

The trials showed NeuroClear had moderate efficacy in slowing cognitive decline in 
Alzheimer's patients, with an acceptable safety profile. The results supported 
advancing NeuroClear to larger Phase 3 trials to further evaluate its potential 
as a treatment for Alzheimer's disease. 
 
The main agent takes this information and determines its next step: 
 
 Now that I have the trial dates and outcomes, I should check the stock price movements
during these periods and any patent/legal activities. 
 
It asks the finance sub-agent the following: 
 
  
  Please provide stock price analysis for two periods:

1. June 2019 - May 2020
2. August 2021 - August 2022

Specifically looking for significant price movements and trading volume changes around
the completion dates of these periods, which correspond to NeuroClear's Phase 2 trial
results. 
  
 The finance sub-agent plans and executes queries on its stock database and responds with the following: 
 
 
  
  Stock Price Analysis for NeuroClear's Phase 2 Trial Periods:

Period 1 (June 2019 - May 2020):
* Final closing price: $34.69
* Average price: $31.43
* Price range: $27.99 - $34.69
* Average daily volume: 630,874 shares
* The period ended with moderate trading volume (566,927 shares)

Period 2 (August 2021 - August 2022):
* Final closing price: $53.71
* Average price: $49.79
* Price range: $44.11 - $55.41
* Average daily volume: 640,098 shares
* The period ended with slightly lower volume (491,629 shares)

Key Observations:
* Significant price appreciation between periods, with Period 2 showing ~58% 
higher average prices
* Both periods showed similar trading volume patterns, with Period 2 showing 
slightly higher average volume
* The stock showed slight negative movement on both completion dates 
(-0.20% and -0.77% respectively) 
  
 The main agent then asks a question to the legal sub-agent: 
  
  Please provide information about:
1. Any patent filings related to NeuroClear, particularly during or after the 
Phase 2 trials (2019-2022)
2. Any legal risks or challenges identified related to NeuroClear's development 
or trial results during this period. 
  
 
 
 The legal sub-agent searches its knowledge base and responds with the following: 
  
  Based on the patent search results, two key patent filings related to NeuroClear 
during or after its Phase 2 trials from 2019-2022 were:

1. A blood-brain barrier penetration enhancer system using nanoparticles to improve 
delivery of neurological drugs like NeuroClear to the brain (Patent ID: PAT004, 
filed in 2022). This technology could have been developed to address challenges 
with getting NeuroClear across the blood-brain barrier during clinical trials.
2. A targeted neurological receptor blocker designed to selectively inhibit 
receptors implicated in neurodegenerative diseases (Patent ID: PAT003, filed in 2021). 
This suggests NeuroClear may work by blocking certain neural receptors involved in 
conditions like Alzheimer's disease.

The legal search results identified a few key legal risks and challenges related to 
NeuroClear's development and trial results from 2019-2022:

1. A product liability lawsuit alleging NeuroClear caused cognitive impairment in a 
patient, with evidence that PharmaCorp failed to properly warn about this risk despite 
internal knowledge of it (Case 2023-CV-006).
2. A class action lawsuit alleging failure to warn about the risk of seizures associated 
with NeuroClear use, based on medical records, expert testimony, and internal PharmaCorp 
documents showing awareness of this risk.
3. Another product liability case where PharmaCorp was found liable for failing to warn 
about risk of liver damage from a different product, suggesting potential issues with 
adequate risk disclosure for NeuroClear as well. 
  
 Finally, the main agent synthesizes the information it has received into a response to the user‚Äôs question: 
 
 
 NeuroClear's Phase 2 trials had mixed impacts across different aspects of 
PharmaCorp's operations:

Clinical Results:
- Two Phase 2 trials (2019-2020 and 2021-2022) showed moderate success in treating
 Alzheimer's disease
- Success rates of 0.65 and 0.68 respectively
- Notable side effects included insomnia, anxiety, tremors, and weight loss

Stock Performance:
- The stock showed resilience during both trial periods
- Average stock price increased significantly (58%) between the first and second trial periods
- Interestingly, both trial completion dates saw slight negative price movements (-0.20% and -0.77%)

Patents and Legal Implications:
- Two significant patents were filed:
* Blood-brain barrier penetration enhancer system (2022)
* Targeted neurological receptor blocker (2021)
- However, several legal risks emerged:
* Product liability lawsuit regarding cognitive impairment
* Class action lawsuit concerning seizure risks
* Potential issues with risk disclosure based on previous cases

These results suggest that while NeuroClear showed promising clinical potential 
and led to valuable intellectual property, it also exposed PharmaCorp to significant 
legal risks that could impact future development and commercialization. 
 
Through this example, we can see how multi-agent collaboration enables a comprehensive analysis of complex business questions by using specialized knowledge from different domains. The main agent effectively orchestrates the interaction between sub-agents, synthesizing their insights to provide a holistic answer that considers R&amp;D, financial, and legal aspects of the NeuroClear clinical trials and their potential impacts on PharmaCorp. 
Clean up 
When you‚Äôre done testing the agent, complete the following steps to clean up your AWS environment and avoid unnecessary charges: 
 
 Delete the S3 buckets: 
   
   On the Amazon S3 console, empty the buckets structured-data-${AWS::AccountId}-${AWS::Region} and unstructured-data-${AWS::AccountId}-${AWS::Region}. Make sure that both of these buckets are empty by deleting the files. 
   Select each file, choose Delete, and confirm by entering the bucket name. 
    
 Delete the Lambda functions: 
   
   On the Lambda console, select the CopyDataLambda function. 
   Choose Delete and confirm the action. 
   Repeat these steps for the CopyUnstructuredDataLambda function. 
    
 Delete the Amazon Bedrock agent: 
   
   On the Amazon Bedrock console, choose Agents in the navigation pane. 
   Select the agent, then choose Delete. 
    
 Delete the Amazon Bedrock knowledge base in Bedrock: 
   
   On the Amazon Bedrock console, choose Knowledge bases under Builder tools in the navigation pane. 
   Select the knowledge base and choose Delete. 
    
 Delete the EC2 instance: 
   
   On the Amazon EC2 console, choose Instances in the navigation pane. 
   Select the EC2 instance you created, then choose Delete. 
    
 
Business impact 
Implementing this multi-agent system using Amazon Bedrock Agents can provide significant benefits for pharmaceutical companies. By automating data retrieval and analysis across domains, companies can reduce research time and enable faster, data-driven decision-making, especially when domain experts are distributed across different organizational units with limited direct interaction. The system‚Äôs ability to provide comprehensive, cross-functional insights in minutes can lead to improved risk mitigation, because potential legal and financial issues can be identified earlier by connecting disparate data points. This automation also allows for more effective allocation of human resources, freeing up experts to focus on high-value tasks rather than routine data analysis. 
Our example demonstrates the power of multi-agent systems in pharmaceutical research and development, but the applications of this technology extend far beyond a single use case. For example, biotech companies can accelerate the discovery of cancer biomarkers by having specialist agents extract genomic signals from Amazon Redshift, perform Kaplan-Meier survival analyses, and interpret CT scans in parallel. Large health systems could automatically aggregate patient records, lab results, and trial data to streamline care coordination and flag urgent cases. Travel agencies can orchestrate end‚Äëto‚Äëend itineraries, and firms can manage personalized client communications. For more information on potential applications, see the following posts: 
 
 Accelerate analysis and discovery of cancer biomarkers with Amazon Bedrock Agents 
 How agentic AI systems can solve the three most pressing problems in healthcare today 
 Unlocking complex problem-solving with multi-agent collaboration on Amazon Bedrock 
 Enabling complex generative AI applications with Amazon Bedrock Agents 
 
Although the potential of multi-agent systems is compelling across these diverse applications, it‚Äôs important to understand the practical considerations in implementing such systems. Complex orchestration workflows can drive up inference costs through multiple model calls, increase end‚Äëto‚Äëend latency, amplify testing and maintenance requirements, and introduce operational overhead around rate limits, retries, and inter‚Äëagent or data connection protocols. However, the state of the art is rapidly advancing. New generations of faster, cheaper models can help keep per‚Äëcall expenses and latency low, and more powerful models can accomplish tasks in fewer turns. Observability tools offer end‚Äëto‚Äëend tracing and dashboarding for multi‚Äëagent pipelines. Finally, protocols like Anthropic‚Äôs Model Context Protocol are beginning to standardize the way agents access data, paving the way for robust multi‚Äëagent ecosystems. 
Conclusion 
In this post, we explored how a multi-agent generative AI system, implemented with Amazon Bedrock Agents using multi-agent collaboration, addresses data access and analysis challenges across multiple business domains. Through a demo use case with a fictional pharmaceutical company managing data across its different divisions, we showcased how specialized sub-agents tailored to each domain streamline information retrieval and synthesis. Each sub-agent uses domain-optimized models and securely accesses relevant data sources, enabling the organization to generate cross-functional insights. 
With this multi-agent architecture, organizations can overcome data silos, enhance collaboration, and achieve efficient, data-driven decision-making while optimizing for cost, latency, and security. Amazon Bedrock Agents with multi-agent collaboration facilitates this setup by providing a secure, scalable framework that manages the collaboration, communication, and task delegation between agents. Explore other demos and workshops about multi-agent collaboration in Amazon Bedrock in the following resources: 
 
 Introducing multi-agent collaboration capability for Amazon Bedrock (preview) 
 Amazon Bedrock multi-agent collaboration workshop 
 Multi-Agent Collaboration with Amazon Bedrock | Amazon Web Services 
 
 
About the authors 
Justin Ossai is a GenAI Labs Specialist Solutions Architect based in Dallas, TX. He is a highly passionate IT professional with over 15 years of technology experience. He has designed and implemented solutions with on-premises and cloud-based infrastructure for small and enterprise companies. 
Michael Hsieh&nbsp;is a Principal AI/ML Specialist Solutions Architect. He works with HCLS customers to advance their ML journey with AWS technologies and his expertise in medical imaging. As a Seattle transplant, he loves exploring the great mother nature the city has to offer, such as the hiking trails, scenery kayaking in the SLU, and the sunset at Shilshole Bay. 
Shreya Mohanty&nbsp; is a Deep Learning Architect at the AWS Generative AI Innovation Center, where she partners with customers across industries to design and implement high-impact GenAI-powered solutions. She specializes in translating customer goals into tangible outcomes that drive measurable impact. 
Rachel Hanspal is a Deep Learning Architect at AWS Generative AI Innovation Center, specializing in end-to-end GenAI solutions with a focus on frontend architecture and LLM integration. She excels in translating complex business requirements into innovative applications, leveraging expertise in natural language processing, automated visualization, and secure cloud architectures.
‚Ä¢ Driving cost-efficiency and speed in claims data processing with Amazon Nova Micro and Amazon Nova Lite
  Amazon operations span the globe, touching the lives of millions of customers, employees, and vendors every day. From the vast logistics network to the cutting-edge technology infrastructure, this scale is a testament to the company‚Äôs ability to innovate and serve its customers. With this scale comes a responsibility to manage risks and address claims‚Äîwhether they involve worker‚Äôs compensation, transportation incidents, or other insurance-related matters. Risk managers oversee claims against Amazon throughout their lifecycle. Claim documents from various sources grow as the claims mature, with a single claim consisting of 75 documents on average. Risk managers are required to strictly follow the relevant standard operating procedure (SOP) and review the evolution of dozens of claim aspects to assess severity and to take proper actions, reviewing and addressing each claim fairly and efficiently. But as Amazon continues to grow, how are risk managers empowered to keep up with the growing number of claims? 
In December 2024, an internal technology team at Amazon built and implemented an AI-powered solution as applied to data related to claims against the company. This solution generates structured summaries of claims under 500 words across various categories, improving efficiency while maintaining accuracy of the claims review process. However, the team faced challenges with high inference costs and processing times (3‚Äì5 minutes per claim), particularly as new documents are added. Because the team plans to expand this technology to other business lines, they explored Amazon Nova Foundation Models as potential alternatives to address cost and latency concerns. 
The following graphs show performance compared with latency and performance compared with cost for various foundation models on the claim dataset. 
 
The evaluation of the claims summarization use case proved that Amazon Nova foundation models (FMs) are a strong alternative to other frontier large language models (LLMs), achieving comparable performance with significantly lower cost and higher overall speed. The Amazon Nova Lite model demonstrates strong summarization capabilities in the context of long, diverse, and messy documents. 
Solution overview 
The summarization pipeline begins by processing raw claim data using AWS Glue jobs. It stores data into intermediate Amazon Simple Storage Service (Amazon S3) buckets, and uses Amazon Simple Queue Service (Amazon SQS) to manage summarization jobs. Claim summaries are generated by AWS Lambda using foundation models hosted in Amazon Bedrock. We first filter the irrelevant claim data using an LLM-based classification model based on Nova Lite and summarize only the relevant claim data to reduce the context window. Considering relevance and summarization requires different levels of intelligence, we select the appropriate models to optimize cost while maintaining performance. Because claims are summarized upon arrival of new information, we also cache the intermediate results and summaries using Amazon DynamoDB to reduce duplicate inference and reduce cost. The following image shows a high-level architecture of the claim summarization use case solution. 
 
Although the Amazon Nova team has published performance benchmarks across several different categories, claims summarization is a unique use case given its diversity of inputs and long context windows. This prompted the technology team owning the claims solution to investigate further with their own benchmarking study. To assess the performance, speed, and cost of Amazon Nova models for their specific use case, the team curated a benchmark dataset consisting of 95 pairs of claim documents and verified aspect summaries. Claim documents range from 1,000 to 60,000 words, with most being around 13,000 words (median 10,100). The verified summaries of these documents are usually brief, containing fewer than 100 words. Inputs to the models include diverse types of documents and summaries that cover a variety of aspects in production. 
According to benchmark tests, the team observed that Amazon Nova Lite is twice as fast and costs 98% less than their current model. Amazon Nova Micro is even more efficient, running four times faster and costing 99% less. The substantial cost-effectiveness and latency improvements offer more flexibility for designing a sophisticated model and scaling up test compute to improve summary quality. Moreover, the team also observed that the latency gap between Amazon Nova models and the next best model widened for long context windows and long output, making Amazon Nova a stronger alternative in the case of long documents while optimizing for latency. Additionally, the team performed this benchmarking study using the same prompt as the current in-production solution with seamless prompt portability. Despite this, Amazon Nova models successfully followed instructions and generated the desired format for post-processing. Based on the benchmarking and evaluation results, the team used Amazon Nova Lite for classification and summarization use cases. 
Conclusion 
In this post, we shared how an internal technology team at Amazon evaluated Amazon Nova models, resulting in notable improvements in inference speed and cost-efficiency. Looking back on the initiative, the team identified several critical factors that offer key advantages: 
 
 Access to a diverse model portfolio ‚Äì The availability of a wide array of models, including compact yet powerful options such as Amazon Nova Micro and Amazon Nova Lite, enabled the team to quickly experiment with and integrate the most suitable models for their needs. 
 Scalability and flexibility ‚Äì The cost and latency improvements of the Amazon Nova models allow for more flexibility in designing sophisticated models and scaling up test compute to improve summary quality. This scalability is particularly valuable for organizations handling large volumes of data or complex workflows. 
 Ease of integration and migration ‚Äì The models‚Äô ability to follow instructions and generate outputs in the desired format simplifies post-processing and integration into existing systems. 
 
If your organization has a similar use case of large document processing that is costly and time-consuming, the above evaluation exercise shows that Amazon Nova Lite and Amazon Nova Micro can be game-changing. These models excel at handling large volumes of diverse documents and long context windows‚Äîperfect for complex data processing environments. What makes this particularly compelling is the models‚Äô ability to maintain high performance while significantly reducing operational costs. It‚Äôs important to iterate over new models for all three pillars‚Äîquality, cost, and speed. Benchmark these models with your own use case and datasets. 
You can get started with Amazon Nova on the Amazon Bedrock console. Learn more at the Amazon Nova product page. 
 
About the authors 
Aitzaz Ahmad is an Applied Science Manager at Amazon, where he leads a team of scientists building various applications of machine learning and generative AI in finance. His research interests are in natural language processing (NLP), generative AI, and LLM agents. He received his PhD in electrical engineering from Texas A&amp;M University. 
Stephen Lau is a Senior Manager of Software Development at Amazon, leads teams of scientists and engineers. His team develops powerful fraud detection and prevention applications, saving Amazon billions annually. They also build Treasury applications that optimize Amazon global liquidity while managing risks, significantly impacting the financial security and efficiency of Amazon. 
Yong Xie is an applied scientist in Amazon FinTech. He focuses on developing large language models and generative AI applications for finance. 
Kristen Henkels is a Sr. Product Manager ‚Äì Technical in Amazon FinTech, where she focuses on helping internal teams improve their productivity by leveraging ML and AI solutions. She holds an MBA from Columbia Business School and is passionate about empowering teams with the right technology to enable strategic, high-value work. 
Shivansh Singh is a Principal Solutions Architect at Amazon. He is passionate about driving business outcomes through innovative, cost-effective and resilient solutions, with a focus on machine learning, generative AI, and serverless technologies. He is a technical leader and strategic advisor to large-scale games, media, and entertainment customers. He has over 16 years of experience transforming businesses through technological innovations and building large-scale enterprise solutions. 
Dushan Tharmal is a Principal Product Manager ‚Äì Technical on the Amazons Artificial General Intelligence team, responsible for the Amazon Nova Foundation Models. He earned his bachelor‚Äôs in mathematics at the University of Waterloo and has over 10 years of technical product leadership experience across financial services and loyalty. In his spare time, he enjoys wine, hikes, and philosophy. 
Anupam Dewan is a Senior Solutions Architect with a passion for generative AI and its applications in real life. He and his team enable Amazon builders who build customer-facing applications using generative AI. He lives in the Seattle area, and outside of work, he loves to go hiking and enjoy nature.
‚Ä¢ Power Your LLM Training and Evaluation with the New SageMaker AI Generative AI Tools
  Today we are excited to introduce the Text Ranking and Question and Answer UI templates to SageMaker AI customers. The Text Ranking template enables human annotators to rank multiple responses from a large language model (LLM) based on custom criteria, such as relevance, clarity, or factual accuracy. This ranked feedback provides critical insights that help refine models through Reinforcement Learning from Human Feedback (RLHF), generating responses that better align with human preferences. The Question and Answer template facilitates the creation of high-quality Q&amp;A pairs based on provided text passages. These pairs act as demonstration data for Supervised Fine-Tuning (SFT), teaching models how to respond to similar inputs accurately. 
In this blog post, we‚Äôll walk you through how to set up these templates in SageMaker to create high-quality datasets for training your large language models. Let‚Äôs explore how you can leverage these new tools. 
Text Ranking 
The Text Ranking template allows annotators to rank multiple text responses generated by a large language model based on customizable criteria such as relevance, clarity, or correctness. Annotators are presented with a prompt and several model-generated responses, which they rank according to guidelines specific to your use case. The ranked data is captured in a structured format, detailing the re-ranked indices for each criterion, such as ‚Äúclarity‚Äù or ‚Äúinclusivity.‚Äù This information is invaluable for fine-tuning models using RLHF, aligning the model outputs more closely with human preferences. In addition, this template is also highly effective for evaluating the quality of LLM outputs by allowing you to see how well responses match the intended criteria. 
Setting Up in the SageMaker AI Console 
A new Generative AI category has been added under Task Type in the SageMaker AI console, allowing you to select these templates. To configure the labeling job using the AWS Management Console, complete the following steps: 
 
 On the SageMaker AI console, under Ground Truth in the navigation pane, choose Labeling job. 
 Choose Create labeling job. 
 Specify your input manifest location and output path. To configure the Text Ranking input file, use the Manual Data Setup under Create Labeling Job and input a JSON file with the prompt stored under the source field, while the list of model responses is placed under the responses field. Text Ranking does not support Automated Data Setup.  
 
Here is an example of our input manifest file: 
 
Upload this input manifest file into your S3 location and provide the S3 path to this file under Input dataset location: 
 
 
 Select Generative AI as the task type and choose the Text Ranking UI.   
 Choose Next. 
 Enter your labeling instructions. Enter the dimensions you want to include in the Ranking dimensions section. For example, in the image above, the dimensions are Helpfulness and Clarity, but you can add, remove, or customize these based on your specific needs by clicking the ‚Äú+‚Äù button to add new dimensions or the trash icon to remove them. Additionally, you have the option to allow tie rankings by selecting the checkbox. This option enables annotators to rank two or more responses equally if they believe the responses are of the same quality for a particular dimension.  
 Choose Preview to display the UI template for review.  
 Choose Create to create the labeling job. 
 
When the annotators submit their evaluations, their responses are saved directly to your specified S3 bucket. The output manifest file includes the original data fields and a worker-response-ref that points to a worker response file in S3. This worker response file contains the ranked responses for each specified dimension, which can be used to fine-tune or evaluate your model‚Äôs outputs. If multiple annotators have worked on the same data object, their individual annotations are included within this file under an answers key, which is an array of responses. Each response includes the annotator‚Äôs input and metadata such as acceptance time, submission time, and worker ID. Here is an example of the output json file containing the annotations: 
 
Question and Answer  
The Question and Answer template allows you to create datasets for Supervised Fine-Tuning (SFT) by generating question-and-answer pairs from text passages. Annotators read the provided text and create relevant questions and corresponding answers. This process acts as a source of demonstration data, guiding the model on how to handle similar tasks. The template supports flexible input, letting annotators reference entire passages or specific sections of text for more targeted Q&amp;A. A color-coded matching feature visually links questions to the relevant sections, helping streamline the annotation process. By using these Q&amp;A pairs, you enhance the model‚Äôs ability to follow instructions and respond accurately to real-world inputs. 
Setting Up in the SageMaker AI Console 
The process for setting up a labeling job with the Question and Answer template follows similar steps as the Text Ranking template. However, there are differences in how you configure the input file and select the appropriate UI template to suit the Q&amp;A task. 
 
 On the SageMaker AI console, under Ground Truth in the navigation pane, choose Labeling job. 
 Choose Create labeling job. 
 Specify your input manifest location and output path. To configure the Question and Answer input file, use the Manual Data Setup and upload a JSON file where the source field contains the text passage. Annotators will use this text to generate questions and answers. Note that you can load the text from a .txt or .csv file and use Ground Truth‚Äôs Automated Data Setup to convert it to the required JSON format. 
 
Here is an example of an input manifest file: 
 
Upload this input manifest file into your S3 location and provide the S3 path to this file under Input dataset location 
 
 Select Generative AI as the task type and choose the Question and Answer UI 
 Choose Next. 
 Enter your labeling instructions. You can configure additional settings to control the task. You can specify the minimum and maximum number of Q&amp;A pairs that workers should generate from the provided text passage. Additionally, you can define the minimum and maximum word counts for both the question and answer fields, so that the responses fit your requirements. You can also add optional question tags to categorize the question and answer pairs. For example, you might include tags such as ‚ÄúWhat,‚Äù ‚ÄúHow,‚Äù or ‚ÄúWhy‚Äù to guide the annotators in their task. If these predefined tags are insufficient, you have the option to allow workers to enter their own custom tags by enabling the Allow workers to specify custom tags feature. This flexibility facilitates annotations that meet the specific needs of your use case.  
 Once these settings are configured, you can choose to Preview the UI to verify that it meets your needs before proceeding.  
 Choose Create to create the labeling job. 
 
When annotators submit their work, their responses are saved directly to your specified S3 bucket. The output manifest file contains the original data fields along with a worker-response-ref that points to the worker response file in S3. This worker response file includes the detailed annotations provided by the workers, such as the ranked responses or question-and-answer pairs generated for each task. 
Here‚Äôs an example of what the output might look like: 
 
CreateLabelingJob API 
In addition to creating these labeling jobs through the Amazon SageMaker AI console, customers can also use the Create Labeling Job API to set up Text Ranking and Question and Answer jobs programmatically. This method provides more flexibility for automation and integration into existing workflows. Using the API, you can define job configurations, input manifests, and worker task templates, and monitor the job‚Äôs progress directly from your application or system. 
For a step-by-step guide on how to implement this, you can refer to the following notebooks, which walk through the entire process of setting up Human-in-the-Loop (HITL) workflows for Reinforcement Learning from Human Feedback (RLHF) using both the Text Ranking and Question and Answer templates. These notebooks will guide you through setting up the required Ground Truth pre-requisites, downloading sample JSON files with prompts and responses, converting them to Ground Truth input manifests, creating worker task templates, and monitoring the labeling jobs. They also cover post-processing the results to create a consolidated dataset with ranked responses. 
 
 Text Ranking 
 Question and Answer pairs 
 
Conclusion 
With the introduction of the Text Ranking and Question and Answer templates, Amazon SageMaker AI empowers customers to generate high-quality datasets for training large language models more efficiently. These built-in capabilities simplify the process of fine-tuning models for specific tasks and aligning their outputs with human preferences, whether through supervised fine-tuning or reinforcement learning from human feedback. By leveraging these templates, you can better evaluate and refine your models to meet the needs of your specific application, helping achieve more accurate, reliable, and user-aligned outputs. Whether you‚Äôre creating datasets for training or evaluating your models‚Äô outputs, SageMaker AI provides the tools you need to succeed in building state-of-the-art generative AI solutions.To begin creating fine-tuning datasets with the new templates: 
 
 Visit the Amazon SageMaker AI console. 
 Refer to the SageMaker AI APIs for programmatic access. 
 Use the AWS CLI for command-line interactions. 
 
 
About the authors 
Sundar Raghavan is a Generative AI Specialist Solutions Architect at AWS, helping customers use Amazon Bedrock and next-generation AWS services to design, build and deploy AI agents and scalable generative AI applications. In his free time, Sundar loves exploring new places, sampling local eateries and embracing the great outdoors. 
Jesse Manders&nbsp;is a Senior Product Manager on Amazon Bedrock, the AWS Generative AI developer service. He works at the intersection of AI and human interaction with the goal of creating and improving generative AI products and services to meet our needs. Previously, Jesse held engineering team leadership roles at Apple and Lumileds, and was a senior scientist in a Silicon Valley startup. He has an M.S. and Ph.D. from the University of Florida, and an MBA from the University of California, Berkeley, Haas School of Business. 
Niharika Jayanti is a Front-End Engineer at Amazon, where she designs and develops user interfaces to delight customers. She contributed to the successful launch of LLM evaluation tools on Amazon Bedrock and Amazon SageMaker Unified Studio. Outside of work, Niharika enjoys swimming, hitting the gym and crocheting. 
 
Muyun Yan is a Senior Software Engineer at Amazon Web Services (AWS) SageMaker AI team. With over 6 years at AWS, she specializes in developing machine learning-based labeling platforms. Her work focuses on building and deploying innovative software applications for labeling solutions, enabling customers to access cutting-edge labeling capabilities. Muyun holds a M.S. in Computer Engineering from Boston University. 
Kavya Kotra is a Software Engineer on the Amazon SageMaker Ground Truth team, helping build scalable and reliable software applications. Kavya played a key role in the development and launch of the Generative AI Tools on SageMaker. Previously, Kavya held engineering roles within AWS EC2 Networking, and Amazon Audible. In her free time, she enjoys painting, and exploring Seattle‚Äôs nature scene. 
Alan Ismaiel is a software engineer at AWS based in New York City. He focuses on building and maintaining scalable AI/ML products, like Amazon SageMaker Ground Truth and Amazon Bedrock. Outside of work, Alan is learning how to play pickleball, with mixed results.
‚Ä¢ Amazon Bedrock Agents observability using Arize AI
  This post is cowritten with John Gilhuly from Arize AI. 
With Amazon Bedrock Agents, you can build and configure autonomous agents in your application. An agent helps your end-users complete actions based on organization data and user input. Agents orchestrate interactions between foundation models (FMs), data sources, software applications, and user conversations. In addition, agents automatically call APIs to take actions and invoke knowledge bases to supplement information for these actions. By integrating agents, you can accelerate your development effort to deliver generative AI applications. With agents, you can automate tasks for your customers and answer questions for them. For example, you can create an agent that helps customers process insurance claims or make travel reservations. You don‚Äôt have to provision capacity, manage infrastructure, or write custom code. Amazon Bedrock manages prompt engineering, memory, monitoring, encryption, user permissions, and API invocation. 
AI agents represent a fundamental shift in how applications make decisions and interact with users. Unlike traditional software systems that follow predetermined paths, AI agents employ complex reasoning that often operates as a ‚Äúblack box.‚Äù Monitoring AI agents presents unique challenges for organizations seeking to maintain reliability, efficiency, and optimal performance in their AI implementations. 
Today, we‚Äôre excited to announce a new integration between Arize AI and Amazon Bedrock Agents that addresses one of the most significant challenges in AI development: observability. Agent observability is a crucial aspect of AI operations that provides deep insights into how your Amazon Bedrock agents perform, interact, and execute tasks. It involves tracking and analyzing hierarchical traces of agent activities, from high-level user requests down to individual API calls and tool invocations. These traces form a structured tree of events, helping developers understand the complete journey of user interactions through the agent‚Äôs decision-making process. Key metrics that demand attention include response latency, token usage, runtime exceptions, and inspect function calling. As organizations scale their AI implementations from proof of concept to production, understanding and monitoring AI agent behavior becomes increasingly critical. 
The integration between Arize AI and Amazon Bedrock Agents provides developers with comprehensive observability tools for tracing, evaluating, and monitoring AI agent applications. This solution delivers three primary benefits: 
 
 Comprehensive traceability ‚Äì Gain visibility into every step of your agent‚Äôs execution path, from initial user query through knowledge retrieval and action execution 
 Systematic evaluation framework ‚Äì Apply consistent evaluation methodologies to measure and understand agent performance 
 Data-driven optimization ‚Äì Run structured experiments to compare different agent configurations and identify optimal settings 
 
The Arize AI service is available in two versions: 
 
 Arize AX ‚Äì An enterprise solution offering advanced monitoring capabilities 
 Arize Phoenix ‚Äì An open source service making tracing and evaluation accessible to developers 
 
In this post, we demonstrate the Arize Phoenix system for tracing and evaluation. Phoenix can run on your local machine, a Jupyter notebook, a containerized deployment, or in the cloud. We explore how this integration works, its key features, and how you can implement it in your Amazon Bedrock Agents applications to enhance observability and maintain production-grade reliability. 
Solution overview 
Large language model (LLM) tracing records the paths taken by requests as they propagate through multiple steps or components of an LLM application. It improves the visibility of your application or system‚Äôs health and makes it possible to debug behavior that is difficult to reproduce locally. For example, when a user interacts with an LLM application, tracing can capture the sequence of operations, such as document retrieval, embedding generation, language model invocation, and response generation, to provide a detailed timeline of the request‚Äôs execution. 
For an application to emit traces for analysis, the application must be instrumented. Your application can be manually instrumented or be automatically instrumented. Arize Phoenix offers a set of plugins (instrumentors) that you can add to your application‚Äôs startup process that perform automatic instrumentation. These plugins collect traces for your application and export them (using an exporter) for collection and visualization. The Phoenix server is a collector and UI that helps you troubleshoot your application in real time. When you run Phoenix (for example, the px.launch_app() container), Phoenix starts receiving traces from an application that is exporting traces to it. For Phoenix, the instrumentors are managed through a single repository called OpenInference. OpenInference provides a set of instrumentations for popular machine learning (ML) SDKs and frameworks in a variety of languages. It is a set of conventions and plugins that is complimentary to OpenTelemetry and online transaction processing (OLTP) to enable tracing of AI applications. Phoenix currently supports OTLP over HTTP. 
For AWS, Boto3 provides Python bindings to AWS services, including Amazon Bedrock, which provides access to a number of FMs. You can instrument calls to these models using OpenInference, enabling OpenTelemetry-aligned observability of applications built using these models. You can also capture traces on invocations of Amazon Bedrock agents using OpenInference and view them in Phoenix.The following high-level architecture diagram shows an LLM application created using Amazon Bedrock Agents, which has been instrumented to send traces to the Phoenix server. 
 
In the following sections, we demonstrate how, by installing the openinference-instrumentation-bedrock library, you can automatically instrument interactions with Amazon Bedrock or Amazon Bedrock agents for observability, evaluation, and troubleshooting purposes in Phoenix. 
Prerequisites 
To follow this tutorial, you must have the following: 
 
 An AWS account with access to Amazon Bedrock. 
 An Amazon Bedrock agent. For instructions to create an agent, refer to Create and configure agent manually. The following GitHub repo demonstrates how to create an agent using infrastructure as code (IaC), implemented through AWS Cloud Development Kit (AWS CDK) Python APIs. 
 An Arize account from where you can then get a Phoenix API key (available at app.phoenix.arize.com). 
 
You can also clone the GitHub repo locally to run the Jupyter notebook yourself: 
git clone https://github.com/awslabs/amazon-bedrock-agent-samples.git 
Install required dependencies 
Begin by installing the necessary libraries: 
%pip install -r requirements.txt ‚Äî quiet 
Next, import the required modules: 
 
 import time
import boto3
import logging
import os
import nest_asyncio
from phoenix.otel import register
from openinference.instrumentation import using_metadata

nest_asyncio.apply() 
 
The arize-phoenix-otel package provides a lightweight wrapper around OpenTelemetry primitives with Phoenix-aware defaults. These defaults are aware of environment variables you must set to configure Phoenix in the next steps, such as: 
 
 PHOENIX_COLLECTOR_ENDPOINT 
 PHOENIX_PROJECT_NAME 
 PHOENIX_CLIENT_HEADERS 
 PHOENIX_API_KEY 
 
Configure the Phoenix environment 
Set up the Phoenix Cloud environment for this tutorial. Phoenix can also be self-hosted on AWS instead. 
 
 os.environ["PHOENIX_COLLECTOR_ENDPOINT"] = "https://app.phoenix.arize.com‚Äú
if not os.environ.get("PHOENIX_CLIENT_HEADERS"):
os.environ["PHOENIX_CLIENT_HEADERS"] = "api_key=" + input("Enter your Phoenix API key: ") 
 
Connect your notebook to Phoenix with auto-instrumentation enabled: 
 
 project_name = "Amazon Bedrock Agent Example"
tracer_provider = register(project_name=project_name, auto_instrument=True) 
 
The auto_instrument parameter automatically locates the openinference-instrumentation-bedrock library and instruments Amazon Bedrock and Amazon Bedrock Agent calls without requiring additional configuration. Configure metadata for the span: 
 
 metadata = { "agent" : "bedrock-agent", 
            "env" : "development"
Metadata is used to filter search values in the dashboard
       } 
 
Set up an Amazon Bedrock session and agent 
Before using Amazon Bedrock, make sure that your AWS credentials are configured correctly. You can set them up using the AWS Command Line Interface (AWS CLI) or by setting environment variables: 
 
 session = boto3.Session()
REGION = session.region_name
bedrock_agent_runtime = session.client(service_name="bedrock-agent-runtime",region_name=REGION) 
 
We assume you‚Äôve already created an Amazon Bedrock agent. To configure the agent, use the following code: 
 
 agent_id = "XXXXXYYYYY" # ‚Üê Configure your Bedrock Agent ID
agent_alias_id = "Z0ZZZZZZ0Z" # ‚Üê Optionally set a different Alias ID if you have one 
 
Before proceeding to your next step, you can validate whether invoke agent is working correctly. The response is not important; we are simply testing the API call. 
 
 print(f"Trying to invoke alias {agent_alias_id} of agent {agent_id}...")
agent_resp = bedrock_agent_runtime.invoke_agent(
    agentAliasId=agent_alias_id,
    agentId=agent_id,
    inputText="Hello!",
    sessionId="dummy-session",
)
if "completion" in agent_resp:
    print("‚úÖ Got response")
else:
    raise ValueError(f"No 'completion' in agent response:\n{agent_resp}") 
 
Run your agent with tracing enabled 
Create a function to run your agent and capture its output: 
 
 @using_metadata(metadata)
def run(input_text):
    session_id = f"default-session1_{int(time.time())}"

    attributes = dict(
        inputText=input_text,
        agentId=agent_id,
        agentAliasId=agent_alias_id,
        sessionId=session_id,
        enableTrace=True,
    )
    response = bedrock_agent_runtime.invoke_agent(**attributes)

    # Stream the response
    for _, event in enumerate(response["completion"]):
        if "chunk" in event:
            print(event)
            chunk_data = event["chunk"]
            if "bytes" in chunk_data:
                output_text = chunk_data["bytes"].decode("utf8")
                print(output_text)
        elif "trace" in event:
            print(event["trace"]) 
 
Test your agent with a few sample queries: 
 
 run ("What are the total leaves for Employee 1?")
run ("If Employee 1 takes 4 vacation days off, What are the total leaves left for Employee 1?") 
 
You should replace these queries with the queries that your application is built for. After executing these commands, you should see your agent‚Äôs responses in the notebook output. The Phoenix instrumentation is automatically capturing detailed traces of these interactions, including knowledge base lookups, orchestration steps, and tool calls. 
View captured traces in Phoenix 
Navigate to your Phoenix dashboard to view the captured traces. You will see a comprehensive visualization of each agent invocation, including: 
 
 The full conversation context 
 Knowledge base queries and results 
 Tool or action group calls and responses 
 Agent reasoning and decision-making steps 
 
Phoenix‚Äôs tracing and span analysis capabilities are useful during the prototyping and debugging stages. By instrumenting application code with Phoenix, teams gain detailed insights into the execution flow, making it straightforward to identify and resolve issues. Developers can drill down into specific spans, analyze performance metrics, and access relevant logs and metadata to streamline debugging efforts. With Phoenix‚Äôs tracing capabilities, you can monitor the following: 
 
 Application latency ‚Äì Identify latency bottlenecks and address slow invocations of LLMs, retrievers, and other components within your application, enabling you to optimize performance and responsiveness. 
 Token usage ‚Äì Gain a detailed breakdown of token usage for your LLM calls, so you can identify and optimize the most expensive LLM invocations. 
 Runtime exceptions ‚Äì Capture and inspect critical runtime exceptions, such as rate-limiting events, that can help you proactively address and mitigate potential issues. 
 Retrieved documents ‚Äì Inspect the documents retrieved during a retriever call, including the score and order in which they were returned, to provide insight into the retrieval process. 
 Embeddings ‚Äì Examine the embedding text used for retrieval and the underlying embedding model, so you can validate and refine your embedding strategies. 
 LLM parameters ‚Äì Inspect the parameters used when calling an LLM, such as temperature and system prompts, to facilitate optimal configuration and debugging. 
 Prompt templates ‚Äì Understand the prompt templates used during the prompting step and the variables that were applied, so you can fine-tune and improve your prompting strategies. 
 Tool descriptions ‚Äì View the descriptions and function signatures of the tools your LLM has been given access to, in order to better understand and control your LLM‚Äôs capabilities. 
 LLM function calls ‚Äì For LLMs with function call capabilities (such as Anthropic‚Äôs Claude, Amazon Nova, or Meta‚Äôs Llama), you can inspect the function selection and function messages in the input to the LLM. This can further help you debug and optimize your application. 
 
The following screenshot shows the Phoenix dashboard for the Amazon Bedrock agent, showing the latency, token usage, total traces. 
 
You can choose one of the traces to drill down to the level of the entire orchestration. 
 
Evaluate the agent in Phoenix 
Evaluating any AI application is a challenge. Evaluating an agent is even more difficult. Agents present a unique set of evaluation pitfalls to navigate. A common evaluation metric for agents is their function calling accuracy, in other words, how well they do at choosing the right tool for the job. For example, agents can take inefficient paths and still get to the right solution. How do you know if they took an optimal path? Additionally, bad responses upstream can lead to strange responses downstream. How do you pinpoint where a problem originated? Phoenix also includes built-in LLM evaluations and code-based experiment testing. An agent is characterized by what it knows about the world, the set of actions it can perform, and the pathway it took to get there. To evaluate an agent, you must evaluate each component. Phoenix has built evaluation templates for every step, such as: 
 
 Agent function calling 
 Agent path convergence 
 Agent planning 
 Agent reflection 
 
You can evaluate the individual skills and response using normal LLM evaluation strategies, such as retrieval evaluation, classification with LLM judges, hallucination, or Q&amp;A correctness. In this post, we demonstrate evaluation of agent function calling. You can use the Agent Function Call eval to determine how well a model selects a tool to use, extracts the right parameters from the user query, and generates the tool call code. Now that you‚Äôve traced your agent in the previous step, the next step is to add evaluations to measure its performance. A common evaluation metric for agents is their function calling accuracy (how well they do at choosing the right tool for the job).Complete the following steps: 
 
 Up until now, you have just used the lighter-weight Phoenix OTEL tracing library. To run evals, you must to install the full library: 
 
!pip install -q arize-phoenix ‚Äî quiet 
 
 Import the necessary evaluation components: 
 
 
 import re
import json
import phoenix as px
from phoenix.evals import (
TOOL_CALLING_PROMPT_RAILS_MAP,
TOOL_CALLING_PROMPT_TEMPLATE,
BedrockModel,
llm_classify,
)
from phoenix.trace import SpanEvaluations
from phoenix.trace.dsl import SpanQuery 
 
The following is our agent function calling prompt template: 
 
 TOOL_CALLING_PROMPT_TEMPLATE = """

You are an evaluation assistant evaluating questions and tool calls to
determine whether the tool called would answer the question. The tool
calls have been generated by a separate agent, and chosen from the list of
tools provided below. It is your job to decide whether that agent chose
the right tool to call.

    [BEGIN DATA]
    ************
    [Question]: {question}
    ************
    [Tool Called]: {tool_call}
    [END DATA]

Your response must be single word, either "correct" or "incorrect",
and should not contain any text or characters aside from that word.
"incorrect" means that the chosen tool would not answer the question,
the tool includes information that is not presented in the question,
or that the tool signature includes parameter values that don't match
the formats specified in the tool signatures below.

"correct" means the correct tool call was chosen, the correct parameters
were extracted from the question, the tool call generated is runnable and correct,
and that no outside information not present in the question was used
in the generated question.

    [Tool Definitions]: {tool_definitions}
""" 
 
 
 Because we are only evaluating the inputs, outputs, and function call columns, let‚Äôs extract those into a simpler-to-use dataframe. Phoenix provides a method to query your span data and directly export only the values you care about: 
 
 
 query = (
SpanQuery()
.where(
# Filter for the `LLM` span kind.
# The filter condition is a string of valid Python boolean expression.
"span_kind == 'LLM' and 'evaluation' not in input.value"
)
.select(
question="input.value",
outputs="output.value",
)
)
trace_df = px.Client().query_spans(query, project_name=project_name) 
 
 
 The next step is to prepare these traces into a dataframe with columns for input, tool call, and tool definitions. Parse the JSON input and output data to create these columns: 
 
 
 def extract_tool_calls(output_value):
try:
tool_calls = []
# Look for tool calls within &lt;function_calls&gt; tags
if "&lt;function_calls&gt;" in output_value:
# Find all tool_name tags
tool_name_pattern = r"&lt;tool_name&gt;(.*?)&lt;/tool_name&gt;"
tool_names = re.findall(tool_name_pattern, output_value)

# Add each found tool name to the list
for tool_name in tool_names:
if tool_name:
tool_calls.append(tool_name)
except Exception as e:
print(f"Error extracting tool calls: {e}")
pass

return tool_calls 
 
 
 Apply the function to each row of trace_df.output.value: 
 
 
 trace_df["tool_call"] = trace_df["outputs"].apply(
lambda x: extract_tool_calls(x) if isinstance(x, str) else []
)

# Display the tool calls found
print("Tool calls found in traces:", trace_df["tool_call"].sum()) 
 
 
 Add tool definitions for evaluation: 
 
 
 trace_df["tool_definitions"] = (
"phoenix-traces retrieves the latest trace information from Phoenix, phoenix-experiments retrieves the latest experiment information from Phoenix, phoenix-datasets retrieves the latest dataset information from Phoenix"
) 
 
Now with your dataframe prepared, you can use Phoenix‚Äôs built-in LLM-as-a-Judge template for tool calling to evaluate your application. The following method takes in the dataframe of traces to evaluate, our built-in evaluation prompt, the eval model to use, and a rails object to snap responses from our model to a set of binary classification responses. We also instruct our model to provide explanations for its responses. 
 
 Run the tool calling evaluation: 
 
 
 rails = list(TOOL_CALLING_PROMPT_RAILS_MAP.values())

eval_model = BedrockModel(session=session, model_id="us.anthropic.claude-3-5-haiku-20241022-v1:0")

response_classifications = llm_classify(
    data=trace_df,
    template=TOOL_CALLING_PROMPT_TEMPLATE,
    model=eval_model,
    rails=rails,
    provide_explanation=True,
)
response_classifications["score"] = response_classifications.apply(
    lambda x: 1 if x["label"] == "correct" else 0, axis=1
) 
 
We use the following parameters: 
 
 df ‚Äì A dataframe of cases to evaluate. The dataframe must have columns to match the default template. 
 question ‚Äì The query made to the model. If you exported spans from Phoenix to evaluate, this will be the llm.input_messages column in your exported data. 
 tool_call ‚Äì Information on the tool called and parameters included. If you exported spans from Phoenix to evaluate, this will be the llm.function_call column in your exported data. 
 
 
 Finally, log the evaluation results to Phoenix: 
 
 
 px.Client().log_evaluations(
SpanEvaluations(eval_name="Tool Calling Eval", dataframe=response_classifications),
) 
 
After running these commands, you will see your evaluation results on the Phoenix dashboard, providing insights into how effectively your agent is using its available tools. 
The following screenshot shows how the tool calling evaluation attribute shows up when you run the evaluation. 
 
When you expand the individual trace, you can observe that the tool calling evaluation adds a score of 1 if the label is correct. This means that agent has responded correctly. 
 
Conclusion 
As AI agents become increasingly prevalent in enterprise applications, effective observability is crucial for facilitating their reliability, performance, and continuous improvement. The integration of Arize AI with Amazon Bedrock Agents provides developers with the tools they need to build, monitor, and enhance AI agent applications with confidence. We‚Äôre excited to see how this integration will empower developers and organizations to push the boundaries of what‚Äôs possible with AI agents. 
Stay tuned for more updates and enhancements to this integration in the coming months. To learn more about Amazon Bedrock Agents and the Arize AI integration, refer to the Phoenix documentation and Integrating Arize AI and Amazon Bedrock Agents: A Comprehensive Guide to Tracing, Evaluation, and Monitoring. 
 
About the Authors 
Ishan Singh is a Sr. Generative AI Data Scientist at Amazon Web Services, where he helps customers build innovative and responsible generative AI solutions and products. With a strong background in AI/ML, Ishan specializes in building generative AI solutions that drive business value. Outside of work, he enjoys playing volleyball, exploring local bike trails, and spending time with his wife and dog, Beau. 
John Gilhuly is the Head of Developer Relations at Arize AI, focused on AI agent observability and evaluation tooling. He holds an MBA from Stanford and a B.S. in C.S. from Duke. Prior to joining Arize, John led GTM activities at Slingshot AI, and served as a venture fellow at Omega Venture Partners. In his pre-AI life, John built out and ran technical go-to-market teams at Branch Metrics. 
Richa Gupta is a Sr. Solutions Architect at Amazon Web Services. She is passionate about architecting end-to-end solutions for customers. Her specialization is machine learning and how it can be used to build new solutions that lead to operational excellence and drive business revenue. Prior to joining AWS, she worked in the capacity of a Software Engineer and Solutions Architect, building solutions for large telecom operators. Outside of work, she likes to explore new places and loves adventurous activities. 
Aris Tsakpinis is a Specialist Solutions Architect for Generative AI, focusing on open weight models on Amazon Bedrock and the broader generative AI open source landscape. Alongside his professional role, he is pursuing a PhD in Machine Learning Engineering at the University of Regensburg, where his research focuses on applied natural language processing in scientific domains. 
Yanyan Zhang&nbsp;is a Senior Generative AI Data Scientist at Amazon Web Services, where she has been working on cutting-edge AI/ML technologies as a Generative AI Specialist, helping customers use generative AI to achieve their desired outcomes. Yanyan graduated from Texas A&amp;M University with a PhD in Electrical Engineering. Outside of work, she loves traveling, working out, and exploring new things. 
Mani Khanuja is a Principal Generative AI Specialist SA and author of the book Applied Machine Learning and High-Performance Computing on AWS. She leads machine learning projects in various domains such as computer vision, natural language processing, and generative AI. She speaks at internal and external conferences such AWS re:Invent, Women in Manufacturing West, YouTube webinars, and GHC 23. In her free time, she likes to go for long runs along the beach. 
Musarath Rahamathullah is an AI/ML and GenAI Solutions Architect at Amazon Web Services, focusing on media and entertainment customers. She holds a Master‚Äôs degree in Analytics with a specialization in Machine Learning. She is passionate about using AI solutions in the AWS Cloud to address customer challenges and democratize technology. Her professional background includes a role as a Research Assistant at the prestigious Indian Institute of Technology, Chennai. Beyond her professional endeavors, she is interested in interior architecture, focusing on creating beautiful spaces to live.
‚Ä¢ How SkillShow automates youth sports video processing using Amazon Transcribe
  This post is co-written with Tom Koerick from SkillShow. 
The youth sports market was valued at $37.5 billion globally in 2022 and is projected to grow by 9.2% each year through 2030. Approximately 60 million young athletes participate in this market worldwide. SkillShow, a leader in youth sports video production, films over 300 events yearly in the youth sports industry, creating content for over 20,000 young athletes annually. This post describes how SkillShow used Amazon Transcribe and other Amazon Web Services (AWS) machine learning (ML) services to automate their video processing workflow, reducing editing time and costs while scaling their operations. 
Challenge 
In response to the surge in youth sports video production, manual video editing processes are becoming increasingly unsustainable. Since 2001, SkillShow has been at the forefront of sports video production, providing comprehensive video services for individuals, teams, and event organizers. They specialize in filming, editing, and distributing content that helps athletes showcase their skills to recruiters, build their personal brand on social media, and support their development training. As a trusted partner to major sports organizations including the&nbsp;Perfect Game, 3Step Sports, USA Baseball, MLB Network, Under Armour, Elite11 football combines and more, SkillShow has filmed hundreds of thousands of athletes and thousands of regional and national events across different sports and age groups. 
Despite their market leadership, SkillShow faced significant operational challenges. With only seven full-time employees managing their expanding operation, they had to outsource to over 1,100 contractors annually. This reliance on outsourced editing not only increased operational costs but also resulted in a lengthy 3-week turnaround time per event, making it difficult to keep pace with the growing demand for youth sports content. 
Managing approximately 230 TB of video data per year created significant operational challenges. This massive volume of data meant lengthy upload and download times for editors, expensive storage costs, and complex data management requirements. Each event‚Äôs raw footage needed to be securely stored, backed up, and made accessible to multiple editors, straining both technical resources and IT infrastructure. These challenges led to SkillShow halting new events mid-2023, limiting their growth potential in a rapidly expanding market. The need for an efficient, scalable solution became critical to maintaining SkillShow‚Äôs position and meeting the growing demand for youth sports content, particularly in the post-COVID era where recruiting videos have become essential for leagues and athletes alike. 
Solution overview 
To address these challenges, SkillShow partnered with AWS to develop an automated video processing pipeline. The team initially explored several approaches to automate player identification. 
Facial recognition proved challenging due to varying video quality, inconsistent lighting conditions, and frequent player movement during games. Additionally, players often wore equipment such as helmets or protective gear that obscured their faces, making reliable identification difficult. 
Text-based detection of jersey numbers and colors seemed promising at first, but presented its own set of challenges. Jersey numbers were frequently obscured by player movement, weather conditions could affect visibility, and varying camera angles made consistent detection unreliable. 
Ultimately, the team settled on an audio logging and automated clip generation solution, which proved superior for several reasons: 
 
 More reliable player identification, because announcers consistently call out player numbers and team colors 
 Better performance in varying environmental conditions, because audio quality remains relatively consistent even in challenging weather or lighting 
 Reduced processing complexity and computational requirements compared to video-based analysis 
 More cost-effective due to lower computational demands and higher accuracy rates 
 Ability to capture additional context from announcer commentary, such as play descriptions and game situations 
 
This solution uses several key AWS services: 
 
 Amazon Simple Storage Service (Amazon S3): 
   
   Used for storing the input and output video files 
   Provides scalable and durable storage to handle SkillShow‚Äôs large video data volume of 230 TB per year 
   Allows for straightforward access and integration with other AWS services in the processing pipeline 
    
 AWS Lambda: 
   
   Serverless compute service used to power the automated processing workflows 
   Triggers the various functions that orchestrate the video processing, such as transcription and clip generation 
   Enables event-driven, scalable, and cost-effective processing without the need to manage underlying infrastructure 
    
 Amazon Transcribe: 
   
   Automatic speech recognition (ASR) service used to convert the video audio into text transcripts 
   Provides the foundation for analyzing the video content and identifying player details 
   Allows for accurate speech-to-text conversion, even in noisy sports environments 
    
 
The following diagram illustrates the solution architecture. 

 
 SkillShow AWS Architecture Diagram
 
The architectural flow is as follows: 
 
 The authorized user uploads a .csv file containing roster information (such as jersey color, number, player name, and school) and the video footage of players. 
 A Lambda function is triggered by the upload of the video. 
 The auto-transcript Lambda function uses Amazon Transcribe to generate a timestamped transcript of what is said in the input video. 
 The transcript is uploaded to the output S3 bucket under transcripts/ for further use. 
 The authorized user can invoke the auto-clipper Lambda function with an AWS Command Line Interface (AWS CLI) command. 
 The function parses the transcript against player information from the roster. 
 When identifying players, the function clips videos based on a specified keyword (in SkillShow‚Äôs case, it was ‚ÄúNext‚Äù) and uploads them to the output S3 bucket under segments/. 
 
By using this suite of AWS services, SkillShow was able to build a scalable, cost-effective, and highly automated video processing solution that addressed their key operational challenges. The cloud-based architecture provides the flexibility and scalability required to handle their growing data volumes and evolving business needs. 
Example processing workflow 
Let‚Äôs explore an example processing workflow. As shown in the following screenshots, we first upload a player roster .csv and video file to the input bucket. 
 
The auto-transcribe function processes the audio. 
 
The auto-clipper function segments the video based on player information. 
 
Final clips are uploaded to the output bucket between two separate folders: a prefix of the input video name or Unnamed/ if the transcription was unclear or missing the player name within the segment. 
 
Named videos can be viewed in the first folder where SkillShow‚Äôs current naming convention (jersey color_number_event video name) is followed for editors to download on demand. 
 
Unnamed videos can be seen in a similar naming convention, only missing the unique player name. Now, the editors only have to review files in this folder and manually rename the file instead of having to do this for entire event videos. 
 
Results and benefits 
After implementing this AWS powered solution, SkillShow transformed their video processing operations. The automated pipeline reduced video production time from 3 weeks to 24 hours per event, enabling faster delivery to athletes and scouts. A recent event in Chicago showcased the system‚Äôs effectiveness. The automated pipeline processed 69 clips, accurately cutting and naming 64 of them‚Äîachieving a 93% success rate. This high accuracy demonstrates the solution‚Äôs ability to handle real-world scenarios effectively. The system also proved adaptable, quickly addressing initial challenges such as color naming inconsistencies. 
The Northwest Indoor event further illustrated the system‚Äôs scalability and versatility. Here, the automated process handled a larger volume of approximately 270 clips, maintaining an estimated accuracy rate of over 90%. Notably, this event included batting practice footage, highlighting the solution‚Äôs adaptability to various types of sports activities. 
With this streamlined workflow, SkillShow has expanded its capacity to process multiple events simultaneously, significantly enhancing its ability to serve youth sports leagues. The standardized output format and improved player identification accuracy have enhanced the viewing experience for athletes, coaches, and scouts alike. Although the time savings varies depending on specific event conditions and filming techniques, the system has demonstrated its potential to substantially reduce manual editing work. SkillShow continues to refine the process, carefully balancing automation with quality control to provide optimal results across diverse event types. These improvements positioned SkillShow to meet the growing demand for youth sports video content while maintaining consistent quality across all events. 
Conclusion 
This solution demonstrates how AWS ML services can transform resource-intensive video processing workflows into efficient, automated systems. By combining the scalable storage of Amazon S3, serverless computing with Lambda, and the speech recognition capabilities of Amazon Transcribe, organizations can dramatically reduce processing times and operational costs. As a leader in automated sports video production, SkillShow has pioneered this approach for youth sports while demonstrating its adaptability to various content types, from educational videos to corporate training. They‚Äôre already exploring additional artificial intelligence and machine learning (AI/ML) capabilities for automated highlight generation, real-time processing for live events, and deeper integration with sports leagues and organizations. 
For organizations looking to further enhance their video processing capabilities, Amazon Bedrock Data Automation offers additional possibilities. Amazon Bedrock Data Automation can streamline the generation of valuable insights from unstructured, multimodal content such as documents, images, audio, and videos. This fully managed capability could potentially be integrated into workflows similar to SkillShow‚Äôs, offering features such as automated video summaries, content moderation, and custom extraction of relevant information from video content. Furthermore, Amazon Bedrock Data Automation can generate custom insights from audio, including summaries and sentiment analysis, providing even deeper understanding of spoken content in sports videos. 
SkillShow‚Äôs success highlights the broader potential of cloud-based video processing. As demand for video content continues to grow across industries, organizations can use AWS ML services to automate their workflows, reduce manual effort, and focus on delivering value to their customers rather than managing complex editing operations. 
Are you interested in implementing similar automated video processing workflows for your organization? Contact SkillShow to learn how their pipeline built with AWS services can transform your content production process. 
 
About the Authors 
 Ragib Ahsan is a Partner Solutions Architect at Amazon Web Services (AWS), where he helps organizations build and implement AI/ML solutions. Specializing in computer vision, he works with AWS partners to create practical applications using cloud technologies. Ahsan is particularly passionate about serverless architecture and its role in making solutions more accessible and efficient. 
 Tom Koerick is the owner and CEO of SkillShow, a sports media network company that has been filming youth sporting events nationwide since 2001. A former professional baseball player turned entrepreneur, Tom develops video solutions for event organizers and families in the youth sports industry. His focus includes college recruiting, social media sharing, and B2B services that provide added value and revenue generation opportunities in youth sports.
‚Ä¢ NewDay builds A Generative AI based Customer service Agent Assist with over 90% accuracy
  This post is co-written with Sergio Zavota and Amy Perring from NewDay. 
NewDay has a clear and defining purpose: to help people move forward with credit. NewDay provides around 4 million customers access to credit responsibly and delivers exceptional customer experiences, powered by their in-house technology system. NewDay‚Äôs contact center handles 2.5 million calls annually, so having the right technology to empower their customer service agents to have effective conversations with customers is paramount to deliver great customer experience. 
The role of the contact center is complex, and with nearly 200 knowledge articles in Customer Services alone, there are times where an agent needs to search the right answer for a customer question from these articles. This led to a hackathon problem statement in early 2024 for NewDay: how can they harness the power of generative AI to improve the speed to resolution, improving both the customer and agent experience. 
The hackathon event led to the creation of NewAssist‚Äîa real-time generative AI assistant designed to empower customer service agents with speech-to-text capabilities. Built on Amazon Bedrock, NewAssist would deliver rapid, context-aware support during live interactions with customers. 
In this post, we share how NewDay turned their hackathon idea into a a successful Generative AI based solution and their learnings during this journey 
Inception and early challenges 
NewAssist won the hackathon event by showcasing the potential generative AI could deliver on speed of call resolution. However, despite a positive start, the team faced significant hurdles: 
 
 Managing costs and competing priorities ‚Äì Amid large strategic initiatives and limited resources, the team remained focused and proactive, even as securing executive buy-in proved challenging 
 Lack of infrastructure ‚Äì The existing legacy systems were not conducive to rapid experimentation 
 Unproven technology ‚Äì The NewAssist team needed to prove the investment would truly add value back to the business 
 
Realizing their ambitions of a fully fledged voice assistant were too ambitious given the challenges, the team made a strategic pivot. They scaled back to a chatbot solution, concentrating on standing up a proof of concept to validate that their existing knowledge management solution would work effectively with generative AI technology. The NewDay contact center team‚Äôs goal is to use one source of truth for its future generative AI solutions, so this task was crucial in setting the right foundation for a solid long-term strategy.With an agile, step-by-step approach, a small cross-functional team of three experts set out to build the proof of concept with a target of 80% accuracy. A golden dataset of over 100 questions and correct answers for these questions was created and the generative AI application was tested with this dataset to evaluate its accuracy of responses. 
Solution overview 
NewAssist‚Äôs technical design and implementation were executed by following these principles: 
 
 Embrace a culture of experimentation ‚Äì A small cross-functional team of three people was formed. The team followed the Improvement Kata methodology to implement rapid Build-Measure-Learn cycles. In just 10 weeks and over 8 experiment loops, the team honed the solution. Early iterations saw accuracy below 60%, but through rigorous testing and smart data strategies, they boosted performance to over 80%, a 33% improvement in just a few weeks. 
 Adopt a serverless Infrastructure ‚Äì Amazon Bedrock, AWS Fargate, AWS Lambda, Amazon API Gateway, and Amazon OpenSearch Serverless formed the backbone of the application. This approach not only reduced costs (with running cost kept under $400 per month), but also made sure the system could scale in response to real-time demand. In addition, this allowed the developer of the team to focus only on activities that would validate the result of the experiments without worrying about managing infrastructure. 
 
NewAssist is implemented as a Retrieval Augmented Generation (RAG) solution. The following diagram shows the high-level solution architecture. 
 
The high-level architecture is made up of five components: 
 
 User interface ‚Äì A simple AI assistant UI is built using the Streamlit framework. Users can log in, ask questions, give feedback to answers in the form of thumbs up and thumbs down, and optionally provide a comment to explain the reason for the bad feedback. The UI is hosted using Fargate and authentication is implemented through Amazon Cognito with Microsoft Entra ID integration to provide single sign-on (SSO) capabilities to customer service agents. 
 Knowledge base processing ‚Äì This component mostly drove the 40% increase in accuracy. Here, articles are retrieved by using APIs from the third-party knowledge base and chunked with a defined chunking strategy. The chunks are processed to convert to vector embeddings and finally stored in the vector database implemented using OpenSearch Serverless. 
 Suggestion generation ‚Äì Questions on the UI are forwarded to the suggestion generation component, which retrieves the most relevant chunks and passess these chunks to the large language model (LLM) for generating suggestions based on the context. Anthropic‚Äôs Claude 3 Haiku was the preferred LLM and was accessed through Amazon Bedrock. Anthropic‚Äôs Claude 3 Haiku is still used at the time of writing, even though more performant models have been released. There are two reasons for this: first, it‚Äôs the most cost-effective model accessible through Amazon Bedrock that provides satisfying results; second, NewDay has a response time requirement of a maximum of 5 seconds, which Anthropic‚Äôs Claude 3 Haiku satisfies. To achieve required accuracy, NewDay experimented with different chunking strategies and retrieval configurations while maintaining cost with Anthropic Claude 3 Haiku. 
 Observability ‚Äì Questions and answers with feedback are logged into Snowflake. A dashboard is created on top of it to show different metrics, such as accuracy. Every week, business experts review the answers with bad feedback, and AI engineers translate them into experiments that, if successful, increase the solution‚Äôs performance. Additionally, Amazon CloudWatch logs the requests that the AWS services described in the architecture process. 
 Offline evaluation ‚Äì When a new version of NewAssist is created during the experimentation cycles, it is first evaluated in pre-production against an evaluation dataset. If the version‚Äôs accuracy surpasses a specified threshold, then it can be deployed in production. 
 
Understand your data and invest in a robust data processing solution 
The one experiment that had the biggest impact on the accuracy of NewAssist, increasing it by 20%, was replacing the general-purpose data parser for knowledge articles with a custom-built version.This new parser was designed specifically to understand the structure and meaning of NewDay‚Äôs data, and by using this data, the LLM could generate more accurate outputs.Initially, the workflow that implements the data processing logic consisted of the following steps: 
 
 Manually extract the articles from the data source and save them in PDF. 
 Use PyPDF to parse the articles. 
 
With this approach, the solution was performing at around 60% accuracy. The simple reason was that the logic didn‚Äôt take into account the type of data that was being processed, providing below-average results. Things changed when NewDay started studying their data.In NewDay, knowledge articles for agents are created by a team of experts in the contact center area. They create articles using a specific methodology and store them in a third-party content management system. This system in particular allows the creation of articles through widgets. For example, lists, banners, and tables.In addition, the system provides APIs that can be used to retrieve articles. The articles are returned in the form of a JSON object, where each object contains a widget. There is a limited number of widgets available, and each one of them has a specific JSON schema.Given this discovery, the team studied each single widget schema and created a bespoke parsing logic that extracts the relevant content and formats it in a polished way.It took longer than simply parsing with PyPDF, but the results were positive. Just focusing on the data and without touching the AI component, the solution‚Äôs accuracy increased from 60% to 73%. This demonstrated that data quality plays a key role in developing an effective generative AI application. 
Understand how your users use the solution 
With the 80% accuracy milestone, the team proved that the proof of concept could work, so they obtained approval to expand experimentation to 10 customer service agents after just 3 months. NewDay selected 10 experienced agents because they needed to identify where the solution gave an incorrect response.As soon as NewAssist was handed over to customer service agents, something unexpected happened. Agents used NewAssist differently from what the NewDay technical team expected: they used various acronyms in their questions to NewAssist. As an example, consider the following questions: 
 
 How do I set a direct debit for a customer? 
 How do I set a dd for a cst? 
 
Here, direct debit is abbreviated with ‚Äúdd‚Äù and customer with ‚Äúcst.‚Äù Unless this information is provided in the context, the LLM will struggle to provide the right answer. As a result, NewAssist‚Äôs accuracy dropped to 70% when agents started using it.The solution NewDay adopted was to statically inject the acronyms and abbreviations in the LLM prompt so it could better understand the question. Slowly, the accuracy recovered to over 80% . This is just a simple example that demonstrates how important it is to put a product in the hands of the final users to validate the assumptions.Another positive finding discovered was that agents would use NewAssist to understand how to explain a process to a customer. As we know, it‚Äôs difficult to translate technical content into a format that non-technical people understand. Agents started to ask NewAssist questions like: ‚ÄúHow do I explain to a customer how to unlock their account?‚Äù with the outcome of producing a great answer they could just read to customers. 
Scaling up for greater impact 
By expanding NewDay‚Äôs experimentation to 10 agents, NewDay was able to test many different scenarios. Negative responses were reviewed and root cause analysis conducted. The NewAssist team identified several gaps in the knowledge base, which they solved with new and improved content. They made enhancements to the solution by training it on acronyms and internal language. Additionally, they provided training and feedback to the pilot team on how to effectively use the solution.By doing this, the NewAssist Team improved the accuracy to over 90% and gained approval from NewDay‚Äôs executive team to productionize the solution. NewDay is currently rolling out the solution to over 150 agents, with plans to expand the scope of the solution to all departments within Customer Operations (such as Fraud and Collections).Early results indicate a substantial reduction in the time it takes to retireve an answer to queries being raised by agents. Previously, it would take them on average 90 seconds to retrieve an answer; the solution now retrieves an answer in 4 seconds. 
Learnings to build a production-ready generative AI application 
NewDay acquired the following insights by deploying a production-ready generative AI application: 
 
 Embrace a culture of experimentation ‚Äì This includes the following strategies: 
   
   Adopt an agile, iterative approach to rapidly test hypotheses and improve the solution 
   Implement methodologies like the Improvement Kata and Build-Measure-Learn cycles to achieve significant gains in short time frames 
   Start small with a focused proof of concept and gradually scale to validate effectiveness before full deployment 
    
 Focus on data quality ‚Äì Invest time in understanding and properly processing your data, because this can yield substantial improvements 
 Understand how your users interact with the product ‚Äì This includes the following steps: 
   
   Conduct real-world testing with actual users to uncover unexpected usage patterns and behaviors 
   Be prepared to adapt your solution based on user insights, such as accommodating internal jargon or abbreviations 
   Look for unforeseen use cases that might emerge during user testing, because these can provide valuable directions for feature development 
   Balance AI capabilities with human expertise, recognizing the importance of oversight and training to facilitate optimal use of the technology. 
    
 
Looking ahead 
NewAssist‚Äôs journey is far from over. Due to a robust feedback mechanism and the right level of oversight, the team will continue to deliver optimizations to improve the accuracy of the output further. Future iterations will explore deeper integrations with AWS AI services, further refining the balance between human and machine intelligence in customer interactions.By adopting AWS serverless solutions and adopting an agile, data-driven approach, NewDay turned a hackathon idea into a powerful tool that has optimized customer services. The success of NewAssist is a testament to the innovation possible when creativity meets robust cloud infrastructure, setting the stage for the next wave of advancements in contact center technology. 
Conclusion 
NewAssist‚Äôs journey demonstrates the power of AWS in enabling rapid experimentation and deployment of RAG solutions. For organizations looking to enhance customer service, streamline operations, or unlock new insights from data, AWS provides the tools and infrastructure to drive innovation, in addition to numerous other opportunities: 
 
 Accelerate RAG experiments ‚Äì Services like Amazon Bedrock, Lambda, and Amazon Serverless enable quick building and iteration of ideas 
 Scale with confidence ‚Äì AWS serverless offerings provide effective cost management while making sure solutions can grow with demand 
 Focus on data quality ‚Äì If data quality isn‚Äôt good enough at the source, you can implement data processing, cleansing, and extraction techniques to improve the accuracy of responses 
 Streamline deployment ‚Äì Fargate and API Gateway simplify the process of moving from proof of concept to production-ready applications 
 Optimize for performance ‚Äì Cross-Region inference and other AWS features help meet strict latency requirements while balancing cost considerations. 
 
To learn more on how AWS can help you in your Generative AI Journey, visit : Transform your business with generative AI. 
 
About the authors 
 Kaushal Goyal is a Solutions Architect at AWS, working with Enterprise Financial Services in the UK and Ireland region. With a strong background in banking technology, Kaushal previously led digital transformation initiatives at major banks. At AWS, Kaushal helps financial institutions modernize legacy systems and implement cloud-native solutions. As a Generative AI enthusiast and Container Specialist, Kaushal focuses on bringing innovative AI solutions to enterprise customers and share the learnings through blogs, public speaking. 
Sergio Zavota is an AI Architect at NewDay, specializing in MLOps and Generative AI. Sergio designs scalable platforms to productionize machine learning workloads and enable Generative AI at scale in Newday. Sergio shares his expertise at industry conferences and workshops, focusing on how to productionise AI solutions and aligning AI with organisational goals. 
Amy Perring is a Senior Optimisation Manager at NewDay, based in London. She specialises in building a deep understanding of contact drivers through customer and agent feedback. This helps identify optimisation opportunities to improve overall efficiency and experience, through the introduction or improvement of products and processes. 
Mayur Udernani leads AWS Generative AI &amp; ML business with commercial enterprises in UK &amp; Ireland. In his role, Mayur spends majority of his time with customers and partners to help create impactful solutions that solve the most pressing needs of a customer or for a wider industry leveraging AWS Cloud, Generative AI &amp; ML services. Mayur lives in the London area. He has an MBA from Indian Institute of Management and Bachelors in Computer Engineering from Mumbai University.
‚Ä¢ No-code data preparation for time series forecasting using Amazon SageMaker Canvas
  Time series forecasting helps businesses predict future trends based on historical data patterns, whether it‚Äôs for sales projections, inventory management, or demand forecasting. Traditional approaches require extensive knowledge of statistical methods and data science methods to process raw time series data. 
Amazon SageMaker Canvas offers no-code solutions that simplify data wrangling, making time series forecasting accessible to all users regardless of their technical background. In this post, we explore how SageMaker Canvas and SageMaker Data Wrangler provide no-code data preparation techniques that empower users of all backgrounds to prepare data and build time series forecasting models in a single interface with confidence. 
Solution overview 
Using SageMaker Data Wrangler for data preparation allows for the modification of data for predictive analytics without programming knowledge. In this solution, we demonstrate the steps associated with this process. The solution includes the following: 
 
 Data Import from varying sources 
 Automated no-code algorithmic recommendations for data preparation 
 Step-by-step processes for preparation and analysis 
 Visual interfaces for data visualization and analysis 
 Export capabilities post data preparation 
 Built in security and compliance features 
 
In this post, we focus on data preparation for time series forecasting using SageMaker Canvas. 
Walkthrough 
The following is a walkthrough of the solution for data preparation using Amazon SageMaker Canvas. For the walkthrough, you use the consumer electronics synthetic dataset found in this SageMaker Canvas Immersion Day lab, which we encourage you to try. This consumer electronics related time series (RTS) dataset primarily contains historical price data that corresponds to sales transactions over time. This dataset is designed to complement target time series (TTS) data to improve prediction accuracy in forecasting models, particularly for consumer electronics sales, where price changes can significantly impact buying behavior. The dataset can be used for demand forecasting, price optimization, and market analysis in the consumer electronics sector. 
Prerequisites 
For this walkthrough, you should have the following prerequisites: 
 
 An AWS account 
 AWS resources 
 Prerequisites for accessing SageMaker Canvas through an AWS account 
 Download the consumer_electronics.csv file from the SageMaker Canvas Immersion Day lab 
 
Solution walkthrough 
Below, we will provide the solution walkthrough and explain how users are able to use a dataset, prepare the data using no code using Data Wrangler, and run and train a time series forecasting model using SageMaker Canvas. 
Sign in to the AWS Management Console and go to Amazon SageMaker AI and then to Canvas. On the Get started page, select Import and prepare option. You will see the following options to import your data set into Sagemaker Data Wrangler. First, select Tabular Data as we will be utilizing this data for our time series forecasting. You will see the following options available to select from: 
 
 Local upload 
 Canvas Datasets 
 Amazon S3 
 Amazon Redshift 
 Amazon Athena 
 Databricks 
 MySQL 
 PostgreSQL 
 SQL Server 
 RDS 
 
 
For this demo, select Local upload. When you use this option, the data is stored in the SageMaker instance, specifically on an Amazon Elastic File System (Amazon EFS) storage volume in the SageMaker Studio environment. This storage is tied to the SageMaker Studio instance, but for more permanent data storage purposes, Amazon Simple Storage Service (Amazon S3) is a good option when working with SageMaker Data Wrangler. For long term data management, Amazon S3 is recommended. 
 
Select the consumer_electronics.csv file from the prerequisites. After selecting the file to import,&nbsp; you can use the Import settings panel to set your desired configurations. For the purpose of this demo, leave the options to their default values. 
 
After the import is complete, use the Data flow options to modify the newly imported data. For future data forecasting, you may need to clean up data for the service to properly understand the values and disregard any errors in the data. SageMaker Canvas has various offerings to accomplish this. Options include Chat for data prep with natural language data modifications and Add Transform. Chat for data prep may be best for users who prefer natural language processing (NLP) interactions and may not be familiar with technical data transformations. Add transform is best for data professionals who know which transformations they want to apply to their data. 
For time series forecasting using Amazon SageMaker Canvas, data must be prepared in a certain way for the service to properly forecast and understand the data. To make a time series forecast using SageMaker Canvas, the documentation linked mentions the following requirements: 
 
 A timestamp column with all values having the&nbsp;datetime&nbsp;type. 
 A target column that has the values that you‚Äôre using to forecast future values. 
 An item ID column that contains unique identifiers for each item in your dataset, such as SKU numbers. 
 
The&nbsp;datetime&nbsp;values in the timestamp column must use one of the following formats: 
 
 YYYY-MM-DD HH:MM:SS 
 YYYY-MM-DDTHH:MM:SSZ 
 YYYY-MM-DD 
 MM/DD/YY 
 MM/DD/YY HH:MM 
 MM/DD/YYYY 
 YYYY/MM/DD HH:MM:SS 
 YYYY/MM/DD 
 DD/MM/YYYY 
 DD/MM/YY 
 DD-MM-YY 
 DD-MM-YYYY 
 
You can make forecasts for the following intervals: 
 
 1 min 
 5 min 
 15 min 
 30 min 
 1 hour 
 1 day 
 1 week 
 1 month 
 1 year 
 
For this example, remove the $ in the data, by using the Chat for data prep option. Give the chat a prompt such as Can you get rid of the $ in my data, and it will generate code to accommodate your request and modify the data, giving you a no-code solution to prepare the data for future modeling and predictive analysis. Choose Add to Steps to accept this code and apply changes to the data. 
 
You can also convert values to float data type and check for missing data in your uploaded CSV file using either Chat for data prep or Add Transform options. To drop missing values using Data Transform: 
 
 Select Add Transform from the interface 
 Choose Handle Missing from the transform options 
 Select Drop missing from the available operations 
 Choose the columns you want to check for missing values 
 Select Preview to verify the changes 
 Choose Add to confirm and apply the transformation 
 
 
For time-series forecasting, inferring missing values and resampling the data set to a certain frequency (hourly, daily, or weekly) are also important. In SageMaker Data Wrangler, the frequency of data can be altered by choosing Add Transform, selecting Time Series, selecting Resample from the Transform drop down, and then selecting the Timestamp dropdown, ts in this example. Then, you can select advanced options. For example, choose Frequency unit and then select the desired frequency from the list. 
 
SageMaker Data Wrangler offers several methods to handle missing values in time-series data through its Handle missing transform. You can choose from options such as forward fill or backward fill, which are particularly useful for maintaining the temporal structure of the data. These operations can be applied by using natural language commands in Chat for data prep, allowing flexible and efficient handling of missing values in time-series forecasting preparation.  
To create the data flow, choose Create model. Then, choose Run Validation, which checks the data to make sure the processes were done correctly. After this step of data transformation, you can access additional options by selecting the purple plus sign. The options include Get data insights, Chat for data prep, Combine data, Create model, and Export. 
The prepared data can then be connected to SageMaker AI for time series forecasting strategies, in this case, to predict the future demand based on the historical data that has been prepared for machine learning. 
When using SageMaker, it is also important to consider data storage and security. For the local import feature, data is stored on Amazon EFS volumes and encrypted by default. For more permanent storage, Amazon S3 is recommended. S3 offers security features such as server-side encryption (SSE-S3, SSE-KMS, or SSE-C), fine-grained access controls through AWS Identity and Access Management (IAM) roles and bucket policies, and the ability to use VPC endpoints for added network security. To help ensure data security in either case, it‚Äôs important to implement proper access controls, use encryption for data at rest and in transit, regularly audit access logs, and follow the principle of least privilege when assigning permissions. 
In this next step, you learn how to train a model using SageMaker Canvas. Based on the previous step, select the purple plus sign and select Create Model, and then select Export to create a model. After selecting a column to predict (select price for this example), you go to the Build screen, with options such as Quick build and Standard build. Based on the column chosen, the model will predict future values based on the data that is being used. 
 
Clean up 
To avoid incurring future charges, delete the SageMaker Data Wrangler data flow and S3 Buckets if used for storage. 
 
 In the SageMaker console, navigate to Canvas 
 Select Import and prepare 
 Find your data flow in the list 
 Click the three dots (‚ãÆ) menu next to your flow 
 Select Delete to remove the data flow  
 
If you used S3 for storage: 
 
 Open the Amazon S3 console 
 Navigate to your bucket 
 Select the bucket used for this project 
 Choose Delete 
 Type the bucket name to confirm deletion 
 Select Delete bucket 
 
Conclusion 
In this post, we showed you how Amazon SageMaker Data Wrangler offers a no-code solution for time series data preparation, traditionally a task requiring technical expertise. By using the intuitive interface of the Data Wrangler console and natural language-powered tools, even users who don‚Äôt have a technical background can effectively prepare their data for future forecasting needs. This democratization of data preparation not only saves time and resources but also empowers a wider range of professionals to engage in data-driven decision-making. 
 
About the author 
Muni T. Bondu is a Solutions Architect at Amazon Web Services (AWS), based in Austin, Texas. She holds a Bachelor of Science in Computer Science, with concentrations in Artificial Intelligence and Human-Computer Interaction, from the Georgia Institute of Technology.
‚Ä¢ Build an agentic multimodal AI assistant with Amazon Nova and Amazon Bedrock Data Automation
  Modern enterprises are rich in data that spans multiple modalities‚Äîfrom text documents and PDFs to presentation slides, images, audio recordings, and more. Imagine asking an AI assistant about your company‚Äôs quarterly earnings call: the assistant should not only read the transcript but also ‚Äúsee‚Äù the charts in the presentation slides and ‚Äúhear‚Äù the CEO‚Äôs remarks. Gartner predicts that by 2027, 40% of generative AI solutions will be multimodal (text, image, audio, video), up from only 1% in 2023. This shift underlines how vital multimodal understanding is becoming for business applications. Achieving this requires a multimodal generative AI assistant‚Äîone that can understand and combine text, visuals, and other data types. It also requires an agentic architecture so the AI assistant can actively retrieve information, plan tasks, and make decisions on tool calling, rather than just responding passively to prompts. 
In this post, we explore a solution that does exactly that‚Äîusing Amazon Nova Pro, a multimodal large language model (LLM) from AWS, as the central orchestrator, along with powerful new Amazon Bedrock features like Amazon Bedrock Data Automation for processing multimodal data. We demonstrate how agentic workflow patterns such as Retrieval Augmented Generation (RAG), multi-tool orchestration, and conditional routing with LangGraph enable end-to-end solutions that artificial intelligence and machine learning (AI/ML) developers and enterprise architects can adopt and extend. We walk through an example of a financial management AI assistant that can provide quantitative research and grounded financial advice by analyzing both the earnings call (audio) and the presentation slides (images), along with relevant financial data feeds. We also highlight how you can apply this pattern in industries like finance, healthcare, and manufacturing. 
Overview of the agentic workflow 
The core of the agentic pattern consists of the following stages: 
 
 Reason ‚Äì The agent (often an LLM) examines the user‚Äôs request and the current context or state. It decides what the next step should be‚Äîwhether that‚Äôs providing a direct answer or invoking a tool or sub-task to get more information. 
 Act ‚Äì The agent executes that step. This could mean calling a tool or function, such as a search query, a database lookup, or a document analysis using Amazon Bedrock Data Automation. 
 Observe ‚Äì The agent observes the result of the action. For instance, it reads the retrieved text or data that came back from the tool. 
 Loop ‚Äì With new information in hand, the agent reasons again, deciding if the task is complete or if another step is needed. This loop continues until the agent determines it can produce a final answer for the user. 
 
This iterative decision-making enables the agent to handle complex requests that are impossible to fulfill with a single prompt. However, implementing agentic systems can be challenging. They introduce more complexity in the control flow, and naive agents can be inefficient (making too many tool calls or looping unnecessarily) or hard to manage as they scale. This is where structured frameworks like LangGraph come in. LangGraph makes it possible to define a directed graph (or state machine) of potential actions with well-defined nodes (actions like ‚ÄúReport Writer‚Äù or ‚ÄúQuery Knowledge Base‚Äù) and edges (allowable transitions). Although the agent‚Äôs internal reasoning still decides which path to take, LangGraph makes sure the process remains manageable and transparent. This controlled flexibility means the assistant has enough autonomy to handle diverse tasks while making sure the overall workflow is stable and predictable. 
Solution overview 
This solution is a financial management AI assistant designed to help analysts query portfolios, analyze companies, and generate reports. At its core is Amazon Nova, an LLM that acts as an intelligent LLM for inference. Amazon Nova processes text, images, or documents (like earnings call slides), and dynamically decides which tools to use to fulfill requests. Amazon Nova is optimized for enterprise tasks and supports function calling, so the model can plan actions and call tools in a structured way. With a large context window (up to 300,000 tokens in Amazon Nova Lite and Amazon Nova Pro), it can manage long documents or conversation history when reasoning. 
The workflow consists of the following key components: 
 
 Knowledge base retrieval ‚Äì Both the earnings call audio file and PowerPoint file are processed by Amazon Bedrock Data Automation, a managed service that extracts text, transcribes audio and video, and prepares data for analysis. If the user uploads a PowerPoint file, the system converts each slide into an image (PNG) for efficient search and analysis, a technique inspired by generative AI applications like Manus. Amazon Bedrock Data Automation is effectively a multimodal AI pipeline out of the box. In our architecture, Amazon Bedrock Data Automation acts as a bridge between raw data and the agentic workflow. Then Amazon Bedrock Knowledge Bases converts these chunks extracted from Amazon Bedrock Data Automation into vector embeddings using Amazon Titan Text Embeddings V2, and stores these vectors in an Amazon OpenSearch Serverless database. 
 Router agent ‚Äì When a user asks a question‚Äîfor example, ‚ÄúSummarize the key risks in this Q3 earnings report‚Äù‚ÄîAmazon Nova first determines whether the task requires retrieving data, processing a file, or generating a response. It maintains memory of the dialogue, interprets the user‚Äôs request, and plans which actions to take to fulfill it. The ‚ÄúMemory &amp; Planning‚Äù module in the solution diagram indicates that the router agent can use conversation history and chain-of-thought (CoT) prompting to determine next steps. Crucially, the router agent determines if the query can be answered with internal company data or if it requires external information and tools. 
 Multimodal RAG agent ‚Äì For queries related with audio and video information, Amazon Bedrock Data Automation uses a unified API call to extract insights from such multimedia data, and stores the extracted insights in Amazon Bedrock Knowledge Bases. Amazon Nova uses Amazon Bedrock Knowledge Bases to retrieve factual answers using semantic search. This makes sure responses are grounded in real data, minimizing hallucination. If Amazon Nova generates an answer, a secondary hallucination check cross-references the response against trusted sources to catch unsupported claims. 
 Hallucination check (quality gate) ‚Äì To further verify reliability, the workflow can include a postprocessing step using a different foundation model (FM) outside of the Amazon Nova family, such as Anthropic‚Äôs Claude, Mistral, or Meta‚Äôs Llama, to grade the answer‚Äôs faithfulness. For example, after Amazon Nova generates a response, a hallucination detector model or function can compare the answer against the retrieved sources or known facts. If a potential hallucination is detected (the answer isn‚Äôt supported by the reference data), the agent can choose to do additional retrieval, adjust the answer, or escalate to a human. 
 Multi-tool collaboration ‚Äì This multi-tool collaboration allows the AI to not only find information but also take actions before formulating a final answer. This introduces multi-tool options. The supervisor agent might spawn or coordinate multiple tool-specific agents (for example, a web search agent to do a general web search, a stock search agent to get market data, or other specialized agents for company financial metrics or industry news). Each agent performs a focused task (one might call an API or perform a query on the internet) and returns findings to the supervisor agent. Amazon Nova Pro features a strong reasoning ability that allows the supervisor agent to merge these findings. This multi-agent approach follows the principle of dividing complex tasks among specialist agents, improving efficiency and reliability for complex queries. 
 Report creation agent ‚Äì Another notable aspect in the architecture is the use of Amazon Nova Canvas for output generation. Amazon Nova Canvas is a specialized image-generation model in the Amazon Nova family, but in this context, we use the concept of a ‚Äúcanvas‚Äù more figuratively to mean a structured template or format generated content output. For instance, we could define a template for an ‚Äúinvestor report‚Äù that the assistant fills out: Section 1: Key Highlights (bullet points), Section 2: Financial Summary (table of figures), Section 3: Notable Quotes, and so on. The agent can guide Amazon Nova to populate such a template by providing it with a system prompt containing the desired format (this is similar to few-shot prompting, where the layout is given). The result is that the assistant not only answers ad-hoc questions, but can also produce comprehensive generated reports that look as if a human analyst prepared them, combining text, image, and references to visuals. 
 
These components are orchestrated in an agentic workflow. Instead of a fixed script, the solution uses a dynamic decision graph (implemented with the open source LangGraph library in the notebook solution) to route between steps. The result is an assistant that feels less like a chatbot and more like a collaborative analyst‚Äîone that can parse an earnings call audio recording, critique a slide deck, or draft an investor memo with minimal human intervention. 
The following diagram shows the high-level architecture of the agentic AI workflow. Amazon Nova orchestrates various tools‚Äîincluding Bedrock Amazon Data Automation for document and image processing and a knowledge base for retrieval‚Äîto fulfill complex user requests. For brevity, we don‚Äôt list all the code here; the GitHub repo includes a full working example. Developers can run that to see the agent in action and extend it with their own data. 
 
Example of the multi-tool collaboration workflow 
To demonstrate the multi-tool collaboration agent workflow, we explore an example of how a question-answer interaction might flow through our deployed system for multi-tool collaboration: 
 
 User prompt ‚Äì In the chat UI, the end-user asks a question, such as ‚ÄúWhat is XXX‚Äôs stock performance this year, and how does it compare to its rideshare‚Äëindustry peers?‚Äù 
 Agent initial response ‚Äì The agent (Amazon Nova FM orchestrator) receives the question and responds with: 
   
   Received your question. Routing to the reasoning engine‚Ä¶ 
    
 Planning and tool selection ‚Äì The agent determines that it needs the following: 
   
   The ticker symbol for the company (XXX) 
   Real‚Äëtime stock price and YTD changes 
   Key financial metrics (revenue, net income, price-earnings ratio) 
   Industry benchmarks (peer YTD performance, average revenue growth) 
    
 Planning execution using tool calls ‚Äì The agent calls tools to perform the following actions: 
   
   Look up ticker symbol: 
     
     Agent ‚Üí WebSearchTool.lookupTicker("XXX Inc")
WebSearchTool ‚Üí Agent: returns "XXX" 
      
   Fetch real‚Äëtime stock performance using the retrieved ticker symbol: 
     
     Agent ‚Üí StockAnalysisTool.getPerformance(
 symbol="XXX",
 period="YTD"
 )
StockAnalysisTool ‚Üí Agent:
 {
 currentPrice: 
 ytdChange: 
 52wkRange: 
 volume: 
 } 
      
   Retrieve company financial metrics using the retrieved ticker symbol: 
     
     Agent ‚Üí CompanyFinancialAnalysisTool.getMetrics("UBER")
CompanyFinancialAnalysisTool ‚Üí Agent:
 {
 revenueQ4_2024: xxx B,
 netIncomeQ4_2024: xxx M,
 peRatio: xxx
 } 
      
   Gather industry benchmark data using the retrieved ticker symbol: 
     
     Agent ‚Üí IndustryAnalysisTool.comparePeers(
 symbol="XXX",
 sector="Rideshare"
 )
IndustryAnalysisTool ‚Üí Agent:
 {
 avgPeerYTD:
 avgRevenueGrowth: 
 } 
      
   Validation loop ‚Äì The agent runs a validation loop: 
     
     Agent: validate()
 ‚Ü≥ Are all four data points present?
 ‚Ä¢ Ticker :heavy_check_mark: 
 ‚Ä¢ Stock performance :heavy_check_mark: 
 ‚Ä¢ Financial metrics :heavy_check_mark: 
 ‚Ä¢ Industry benchmark :heavy_check_mark: 
 ‚Ü≥ All set‚Äîno retry needed. 
      
    
 
If anything is missing or a tool encountered an error, the FM orchestrator triggers the error handler (up to three retries), then resumes the plan at the failed step. 
 
 Synthesis and final answer ‚Äì The agent uses Amazon Nova Pro to synthesize the data points and generate final answers based on these data points. 
 
The following figure shows a flow diagram of this multi-tool collaboration agent. 
 

 
  
 
 
Benefits of using Amazon Bedrock for scalable generative AI agent workflows 
This solution is built on Amazon Bedrock because AWS provides an integrated ecosystem for building such sophisticated solutions at scale: 
 
 Amazon Bedrock delivers top-tier FMs like Amazon Nova, with managed infrastructure‚Äîno need for provisioning GPU servers or handling scaling complexities. 
 Amazon Bedrock Data Automation offers an out-of-the-box solution to process documents, images, audio, and video into actionable data. Amazon Bedrock Data Automation can convert presentation slides to images, convert audio to text, perform OCR, and generate textual summaries or captions that are then indexed in an Amazon Bedrock knowledge bases. 
 Amazon Bedrock Knowledge Bases can store embeddings from unstructured data and support retrieval operations using similarity search. 
 In addition to LangGraph (as shown in this solution), you can also use Amazon Bedrock Agents to develop agentic workflows. Amazon Bedrock Agents simplifies the configuration of tool flows and action groups, so you can declaratively manage your agentic workflows. 
 Applications developed by open source frameworks like LangGraph (an extension of LangChain) can also run and scale with AWS infrastructure such as Amazon Elastic Compute Cloud (Amazon EC2) or Amazon SageMaker instances, so you can define directed graphs for agent orchestration, making it effortless to manage multi-step reasoning and tool chaining. 
 
You don‚Äôt need to assemble a dozen disparate systems; AWS provides an integrated network for generative AI workflows. 
Considerations and customizations 
The architecture demonstrates exceptional flexibility through its modular design principles. At its core, the system uses Amazon Nova FMs, which can be selected based on task complexity. Amazon Nova Micro handles straightforward tasks like classification with minimal latency. Amazon Nova Lite manages moderately complex operations with balanced performance, and Amazon Nova Pro excels at sophisticated tasks requiring advanced reasoning or generating comprehensive responses. 
The modular nature of the solution (Amazon Nova, tools, knowledge base, and Amazon Bedrock Data Automation) means each piece can be swapped or adjusted without overhauling the whole system. Solution architects can use this reference architecture as a foundation, implementing customizations as needed. You can seamlessly integrate new capabilities through AWS Lambda functions for specialized operations, and the LangGraph orchestration enables dynamic model selection and sophisticated routing logic. This architectural approach makes sure the system can evolve organically while maintaining operational efficiency and cost-effectiveness. 
Bringing it to production requires thoughtful design, but AWS offers scalability, security, and reliability. For instance, you can secure the knowledge base content with encryption and access control, integrate the agent with AWS Identity and Access Management (IAM) to make sure it only performs allowed actions (for example, if an agent can access sensitive financial data, verify it checks user permissions ), and monitor the costs (you can track Amazon Bedrock pricing and tools usage; you might use Provisioned Throughput for consistent high-volume usage). Additionally, with AWS, you can scale from an experiment in a notebook to a full production deployment when you‚Äôre ready, using the same building blocks (integrated with proper AWS infrastructure like Amazon API Gateway or Lambda, if deploying as a service). 
Vertical industries that can benefit from this solution 
The architecture we described is quite general. Let‚Äôs briefly look at how this multimodal agentic workflow can drive value in different industries: 
 
 Financial services ‚Äì In the financial sector, the solution integrates multimedia RAG to unify earnings call transcripts, presentation slides (converted to searchable images), and real-time market feeds into a single analytical framework. Multi-agent collaboration enables Amazon Nova to orchestrate tools like Amazon Bedrock Data Automation for slide text extraction, semantic search for regulatory filings, and live data APIs for trend detection. This allows the system to generate actionable insights‚Äîsuch as identifying portfolio risks or recommending sector rebalancing‚Äîwhile automating content creation for investor reports or trade approvals (with human oversight). By mimicking an analyst‚Äôs ability to cross-reference data types, the AI assistant transforms fragmented inputs into cohesive strategies. 
 Healthcare ‚Äì Healthcare workflows use multimedia RAG to process clinical notes, lab PDFs, and X-rays, grounding responses in peer-reviewed literature and patient audio interview. Multi-agent collaboration excels in scenarios like triage: Amazon Nova interprets symptom descriptions, Amazon Bedrock Data Automation extracts text from scanned documents, and integrated APIs check for drug interactions, all while validating outputs against trusted sources. Content creation ranges from succinct patient summaries (‚ÄúSevere pneumonia, treated with levofloxacin‚Äù) to evidence-based answers for complex queries, such as summarizing diabetes guidelines. The architecture‚Äôs strict hallucination checks and source citations support reliability, which is critical for maintaining trust in medical decision-making. 
 Manufacturing ‚Äì Industrial teams use multimedia RAG to index equipment manuals, sensor logs, worker audio conversation, and schematic diagrams, enabling rapid troubleshooting. Multi-agent collaboration allows Amazon Nova to correlate sensor anomalies with manual excerpts, and Amazon Bedrock Data Automation highlights faulty parts in technical drawings. The system generates repair guides (for example, ‚ÄúReplace valve Part 4 in schematic‚Äù) or contextualizes historical maintenance data, bridging the gap between veteran expertise and new technicians. By unifying text, images, and time series data into actionable content, the assistant reduces downtime and preserves institutional knowledge‚Äîproving that even in hardware-centric fields, AI-driven insights can drive efficiency. 
 
These examples highlight a common pattern: the synergy of data automation, powerful multimodal models, and agentic orchestration leads to solutions that closely mimic a human expert‚Äôs assistance. The financial AI assistant cross-checks figures and explanations like an analyst would, the clinical AI assistant correlates images and notes like a diligent doctor, and the industrial AI assistant recalls diagrams and logs like a veteran engineer. All of this is made possible by the underlying architecture we‚Äôve built. 
Conclusion 
The era of siloed AI models that only handle one type of input is drawing to a close. As we‚Äôve discussed, combining multimodal AI with an agentic workflow unlocks a new level of capability for enterprise applications. In this post, we demonstrated how to construct such a workflow using AWS services: we used Amazon Nova as the core AI orchestrator with its multimodal, agent-friendly capabilities, Amazon Bedrock Data Automation to automate the ingestion and indexing of complex data (documents, slides, audio) into Amazon Bedrock Knowledge Bases, and the concept of an agentic workflow graph for reasoning and condition (using LangChain or LangGraph) to orchestrate multi-step reasoning and tool usage. The end result is an AI assistant that operates much like a diligent analyst: researching, cross-checking multiple sources, and delivering insights‚Äîbut at machine speed and scale.The solution demonstrates that building a sophisticated agentic AI system is no longer an academic dream‚Äîit‚Äôs practical and achievable with today‚Äôs AWS technologies. By using Amazon Nova as a powerful multimodal LLM and Amazon Bedrock Data Automation for multimodal data processing, along with frameworks for tool orchestration like LangGraph (or Amazon Bedrock Agents), developers get a head start. Many challenges (like OCR, document parsing, or conversational orchestration) are handled by these managed services or libraries, so you can focus on the business logic and domain-specific needs. 
The solution presented in the BDA_nova_agentic sample notebook is a great starting point to experiment with these ideas. We encourage you to try it out, extend it, and tailor it to your organization‚Äôs needs. We‚Äôre excited to see what you will build‚Äîthe techniques discussed here represent only a small portion of what‚Äôs possible when you combine modalities and intelligent agents. 
 
About the authors 
Julia Hu Julia Hu is a Sr. AI/ML Solutions Architect at Amazon Web Services, currently focused on the Amazon Bedrock team. Her core expertise lies in agentic AI, where she explores the capabilities of foundation models and AI agents to drive productivity in Generative AI applications. With a background in Generative AI, Applied Data Science, and IoT architecture, she partners with customers‚Äîfrom startups to large enterprises‚Äîto design and deploy impactful AI solutions. 
Rui Cardoso is a partner solutions architect at Amazon Web Services (AWS). He is focusing on AI/ML and IoT. He works with AWS Partners and support them in developing solutions in AWS. When not working, he enjoys cycling, hiking and learning new things. 
Jessie-Lee Fry is a Product and Go-to Market (GTM) Strategy executive specializing in Generative AI and Machine Learning, with over 15 years of global leadership experience in Strategy, Product, Customer success, Business Development, Business Transformation and Strategic Partnerships. Jessie has defined and delivered a broad range of products and cross-industry go- to-market strategies driving business growth, while maneuvering market complexities and C-Suite customer groups. In her current role, Jessie and her team focus on helping AWS customers adopt Amazon Bedrock at scale enterprise use cases and adoption frameworks, meeting customers where they are in their Generative AI Journey.

‚∏ª