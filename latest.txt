‚úÖ Morning News Briefing ‚Äì July 20, 2025 10:54

üìÖ Date: 2025-07-20 10:54
üè∑Ô∏è Tags: #briefing #ai #publichealth #digitalgov

‚∏ª

üßæ Weather
‚Ä¢ No watches or warnings in effect, Pembroke
  No watches or warnings in effect. No warnings or watches or watches in effect . Watch or warnings are no longer in effect in the U.S. No watches, warnings are in effect for the rest of the day . No watches and warnings are still in effect, but no watches are in place for the day's events . The weather is not expected to be affected by the weather .
‚Ä¢ Current Conditions:  16.1¬∞C
  Temperature: 16.1&deg;C Pressure / Tendency: 101.0 kPa falling Humidity: 94 % Dewpoint: 15.2&deg:C Wind: SSE 4 km/h Air Quality Health Index: n/a Observed at: Pembroke 6:00 AM EDT Sunday 20 July 2025 . Weather in the near future: 20 July
‚Ä¢ Sunday: Chance of showers. High 24. POP 40%
  Mainly cloudy. 40 percent chance of showers early this morning . Wind becoming west 20 km/h gusting to 40 late this morning. High 24. Humidex 28. UV index 8 or very high. UV of very high in the UK . Forecast issued 5:00 AM EDT Sunday 20 July 2025 . Weather forecast: Sunday morning to Sunday afternoon. Sunday afternoon to Sunday

üåç International News
No updates.

üçÅ Canadian News
No updates.

üá∫üá∏ U.S. Top Stories
‚Ä¢ Japan votes in a key election as Prime Minister Ishiba faces a possible loss
  Soaring prices, lagging incomes and burdensome social security payments are the top issues for frustrated, cash-strapped voters . Stricter measures targeting foreign residents and visitors have also emerged as a key issue, with a surging right-wing populist party leading the campaign . Right-wing populism party leads the campaign with a surge in support of Japan's anti-immigration policies . St
‚Ä¢ 3 people are still missing from deadly floods in Texas county, down from nearly 100
  Officials in Texas hill country community pummeled by deadly flooding July 4 said Saturday that just three people remain missing, down from nearly 100 . Officials say people who had previously been reported missing have since been accounted for . People who had been previously reported missing are now accounted for, officials say . Nearly 100 people were reported missing in the area, including nearly 100 people who were killed by flooding
‚Ä¢ Cuts to public media will smash budgets of some local radio stations
  Congress voted to claw back federal funding to public media . Some of those hit hardest include community radio stations in areas that voted for the president . Community radio stations are in areas where the president voted for him . Public media is a vital part of the American public life, but the federal budget is set to be cut by the end of the year . Public radio stations will be affected by the
‚Ä¢ 32 Palestinians killed trying to reach food distribution hubs, Gaza authorities say
  Palestinians were shot dead during a food distribution on Saturday at a center run by a U.S.- and Israeli-backed group in southern Gaza, hospital officials said . Hospital officials said the victims were shot during a distribution at the center . The food distribution center is run by an Israeli-U.S. and Palestinian-backed Hamas group, which has ties to Israel and the U.
‚Ä¢ Examining the role of men in the gender gap in American society
  The latest season of the podcast "Unsettled" from Iowa Public Radio looks at how gender affects people's everyday lives . The podcast is a weekly look at the everyday experiences of people living in the gender of men and women in their lives . Watch the latest episode of the weekly podcast, Unsettled, at 8 p.m. on CNN.com/Heroes, on

üß† Artificial Intelligence
No updates.

üíª Digital Strategy
‚Ä¢ China proves that open models are more effective than all the GPUs in the world
  OpenAI delayed its promised open-weights model since GPT-2, leaving the Middle Kingdom clearly in the lead . OpenAI was supposed to make good on its name and release its first open-weight model this week . The Middle Kingdom is clearly leading the way in the race to the top of the world's most powerful AI-powered supercomputer, with the latest model being released
‚Ä¢ Ex-IDF cyber chief on Iran, Scattered Spider, and why social engineering worries him more than 0-days
  Scattered Spider and Iranian government-backed cyber units have more in common than a recent uptick in hacking activity . Ariel Parnes, a former colonel in the Israeli Defense Forces' cyber unit 8200, says the Iranian government has more to common than recent uptick of hacking activity, he says . The former colonel says the two cyber units are backed by the government of Israel and Iran .
‚Ä¢ Republican calls out Trump admin's decision to resume GPU sales to China
  The Republican chair of the US House Select Committee on China has protested the Trump administration's decision to lift restrictions on the sale of Nvidia H20 GPUs and similar processors . He says the chips could be used to advance Chinese AI and military interests . Moolenaar demands answers from Commerce Secretary, calling on Commerce Secretary to explain his decision to relax restrictions on sale of the chips in the US
‚Ä¢ Meta declines to abide by voluntary EU AI safety guidelines
  European Commission issued voluntary guidelines for providers of general-purpose AI models . Meta refused to sign, arguing that the extra measures introduce "legal uncertainties" beyond the law's scope . The EU AI Act takes effect next month, but Meta says extra measures are 'unfairly beyond the scope' of the law . The European Commission is expected to issue voluntary guidelines to providers of AI models next
‚Ä¢ Foundry competition heats up as Japan‚Äôs Rapidus says 2nm chip tech on track for 2027
  Rapidus says it's on track to begin volume production of 2nm process tech . Foundry upstart Rapidus is two years behind everyone else . Rapidus achieved a major milestone this week at the start of production process process tech in Japan . It is the first major Japanese foundry to produce a process process process two years after its first 2nm technology was developed in 2011 . Rapid

üè• Public Health
No updates.

üî¨ Science
‚Ä¢ Application of machine learning algorithms and SHAP explanations to predict fertility preference among reproductive women in Somalia
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Double burden of malnutrition in Guatemala: co-existence of childhood stunting and overweight and obesity within the household
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Cardiorespiratory fitness after correction of pectus excavatum: a systematic review with meta-analysis
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ The attitudes and treatment practices of Hungarian primary care dentists regarding dental care for patients with haemophilia
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Feasibility of video-assisted cardiopulmonary resuscitation when the lay responder is alone with the victim: a randomized controlled crossover pilot study
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations

üßæ Government & Policy
No updates.

üèõÔ∏è Enterprise Architecture & IT Governance
No updates.

ü§ñ AI & Emerging Tech
‚Ä¢ A major AI training data set contains millions of examples of personal data
  Millions of images of passports, credit cards, birth certificates, and other documents containing personally identifiable information are likely included in one of the biggest open-source AI training sets, new research has found.



Thousands of images‚Äîincluding identifiable faces‚Äîwere found in a small subset of DataComp CommonPool, a major AI training set for image generation scraped from the web. Because the researchers audited just 0.1% of CommonPool‚Äôs data, they estimate that the real number of images containing personally identifiable information, including faces and identity documents, is in the hundreds of millions. The study that details the breach was published on arXiv earlier this month.



The bottom line, says William Agnew, a postdoctoral fellow in AI ethics at Carnegie Mellon University and one of the coauthors, is that ‚Äúanything you put online can [be] and probably has been scraped.‚Äù



The researchers found thousands of instances of validated identity documents‚Äîincluding images of credit cards, driver‚Äôs licenses, passports, and birth certificates‚Äîas well as over 800 validated job application documents (including r√©sum√©s and cover letters), which were confirmed through LinkedIn and other web searches as being associated with real people. (In many more cases, the researchers did not have time to validate the documents or were unable to because of issues like image clarity.)&nbsp;



A number of the r√©sum√©s disclosed sensitive information including disability status, the results of background checks, birth dates and birthplaces of dependents, and race. When r√©sum√©s were linked to people with online presences, researchers also found contact information, government identifiers, sociodemographic information, face photographs, home addresses, and the contact information of other people (like references).



Examples of identity-related documents found in CommonPool‚Äôs small-scale data set show a credit card, a Social Security number, and a driver‚Äôs license. For each sample, the type of URL site is shown at the top, the image in the middle, and the caption in quotes below. All personal information has been replaced, and text has been paraphrased to avoid direct quotations. Images have been redacted to show the presence of faces without identifying the individuals.COURTESY OF THE RESEARCHERS




When it was released in 2023, DataComp CommonPool, with its 12.8 billion data samples, was the largest existing data set of publicly available image-text pairs, which are often used to train generative text-to-image models. While its curators said that CommonPool was intended for academic research, its license does not prohibit commercial use as well.&nbsp;



CommonPool was created as a follow-up to the LAION-5B data set, which was used to train models including Stable Diffusion and Midjourney. It draws on the same data source: web scraping done by the nonprofit Common Crawl between 2014 and 2022.&nbsp;



While commercial models often do not disclose what data sets they are trained on, the shared data sources of DataComp CommonPool and LAION-5B mean that the data sets are similar, and that the same personally identifiable information likely appears in LAION-5B, as well as in other downstream models trained on CommonPool data. CommonPool researchers did not respond to emailed questions.



And since DataComp CommonPool has been downloaded more than 2 million times over the past two years, it is likely that ‚Äúthere [are]many downstream models that are all trained on this exact data set,‚Äù says Rachel Hong, a PhD student in computer science at the University of Washington and the paper‚Äôs lead author. Those would duplicate similar privacy risks.



Good intentions are not enough



‚ÄúYou can assume that any large-scale web-scraped data always contains content that shouldn‚Äôt be there,‚Äù says Abeba Birhane, a cognitive scientist and tech ethicist who leads Trinity College Dublin‚Äôs AI Accountability Lab‚Äîwhether it‚Äôs personally identifiable information (PII), child sexual abuse imagery, or hate speech (which Birhane‚Äôs own research into LAION-5B has found).&nbsp;



Indeed, the curators of DataComp CommonPool were themselves aware it was likely that PII would appear in the data set and did take some measures to preserve privacy, including automatically detecting and blurring faces. But in their limited data set, Hong‚Äôs team found and validated over 800 faces that the algorithm had missed, and they estimated that overall, the algorithm had missed 102 million faces in the entire data set. On the other hand, they did not apply filters that could have recognized known PII character strings, like emails or Social Security numbers.&nbsp;



‚ÄúFiltering is extremely hard to do well,‚Äù says Agnew. ‚ÄúThey would have had to make very significant advancements in PII detection and removal that they haven‚Äôt made public to be able to effectively filter this.‚Äù&nbsp;&nbsp;



Examples of r√©sum√© documents and personal disclosures found in CommonPool‚Äôs small-scale data set. For each sample, the type of URL site is shown at the top, the image in the middle, and the caption in quotes below. All personal information has been replaced, and text has been paraphrased to avoid direct quotations. Images have been redacted to show the presence of faces without identifying the individuals. Image courtesy of the researchers.COURTESY OF THE RESEARCHERS




There are other privacy issues that the face blurring doesn‚Äôt address. While the blurring filter is automatically applied, it is optional and can be removed. Additionally, the captions that often accompany the photos, as well as the photos‚Äô metadata, often contain even more personal information, such as names and exact locations.



Another privacy mitigation measure comes from Hugging Face, a platform that distributes training data sets and hosts CommonPool, which integrates with a tool that theoretically allows people to search for and remove their own information from a data set. But as the researchers note in their paper, this would require people to know that their data is there to start with. When asked for comment, Florent Daudens of Hugging Face said that ‚Äúmaximizing the privacy of data subjects across the AI ecosystem takes a multilayered approach, which includes but is not limited to the widget mentioned,‚Äù and that the platform is ‚Äúworking with our community of users to move the needle in a more privacy-grounded direction.‚Äù&nbsp;



In any case, just getting your data removed from one data set probably isn‚Äôt enough. ‚ÄúEven if someone finds out their data was used in a training data sets and ‚Ä¶ exercises their right to deletion, technically the law is unclear about what that means,‚Äù ¬†says Tiffany Li, an associate professor of law at the University of San Francisco School of Law. ‚ÄúIf the organization only deletes data from the training data sets‚Äîbut does not delete or retrain the already trained model‚Äîthen the harm will nonetheless be done.‚Äù



The bottom line, says Agnew, is that ‚Äúif you web-scrape, you‚Äôre going to have private data in there. Even if you filter, you‚Äôre still going to have private data in there, just because of the scale of this. And that‚Äôs something that we [machine-learning researchers], as a field, really need to grapple with.‚Äù



Reconsidering consent



CommonPool was built on web data scraped between 2014 and 2022, meaning that many of the images likely date to before 2020, when ChatGPT was released. So even if it‚Äôs theoretically possible that some people consented to having their information publicly available to anyone on the web, they could not have consented to having their data used to train large AI models that did not yet exist.





And with web scrapers often scraping data from each other, an image that was originally uploaded by the owner to one specific location would often find its way into other image repositories. ‚ÄúI might upload something onto the internet, and then ‚Ä¶ a year or so later, [I] want to take it down, but then that [removal] doesn‚Äôt necessarily do anything anymore,‚Äù says Agnew.



The researchers also found numerous examples of children‚Äôs personal information, including depictions of birth certificates, passports, and health status, but in contexts suggesting that they had been shared for limited purposes.



‚ÄúIt really illuminates the original sin of AI systems built off public data‚Äîit‚Äôs extractive, misleading, and dangerous to people who have been using the internet with one framework of risk, never assuming it would all be hoovered up by a group trying to create an image generator,‚Äù says Ben Winters, the director of AI and privacy at the Consumer Federation of America.



Finding a policy that fits



Ultimately, the paper calls for the machine-learning community to rethink the common practice of indiscriminate web scraping and also lays out the possible violations of current privacy laws represented by the existence of PII in massive machine-learning data sets, as well as the limitations of those laws‚Äô ability to protect privacy.



‚ÄúWe have the GDPR in Europe, we have the CCPA in California, but there‚Äôs still no federal data protection law in America, which also means that different Americans have different rights protections,‚Äù says Marietje Schaake, a Dutch lawmaker turned tech policy expert who currently serves as a fellow at Stanford‚Äôs Cyber Policy Center.&nbsp;



Besides, these privacy laws apply to companies that meet certain criteria for size and other characteristics. They do not necessarily apply to researchers like those who were responsible for creating and curating DataComp CommonPool.



And even state laws that do address privacy, like California‚Äôs consumer privacy act, have carve-outs for ‚Äúpublicly available‚Äù information. Machine-learning researchers have long operated on the principle that if it‚Äôs available on the internet, then it is public and no longer private information, but Hong, Agnew, and their colleagues hope that their research challenges this assumption.&nbsp;



‚ÄúWhat we found is that ‚Äòpublicly available‚Äô includes a lot of stuff that a lot of people might consider private‚Äîr√©sum√©s, photos, credit card numbers, various IDs, news stories from when you were a child, your family blog. These are probably not things people want to just be used anywhere, for anything,‚Äù says Hong.&nbsp;&nbsp;



Hopefully, Schaake says, this research ‚Äúwill raise alarm bells and create change.‚Äù&nbsp;



This article previously misstated Tiffany Li&#8217;s affiliation. This has been fixed.
‚Ä¢ The Download: how to run an LLM, and a history of ‚Äúthree-parent babies‚Äù
  This is today&#8217;s edition of¬†The Download,¬†our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



How to run an LLM on your laptop



In the early days of large language models, there was a high barrier to entry: it used to be impossible to run anything useful on your own computer without investing in pricey GPUs. But researchers have had so much success in shrinking down and speeding up models that anyone with a laptop, or even a smartphone, can now get in on the action.For people who are concerned about privacy, want to break free from the control of the big LLM companies, or just enjoy tinkering, local models offer a compelling alternative to ChatGPT and its web-based peers. Here‚Äôs how to get started running a useful model from the safety and comfort of your own computer. Read the full story.‚ÄîGrace Huckins



This story is part of MIT Technology Review‚Äôs How To series, helping you get things done. You can check out the rest of the series here.







A brief history of ‚Äúthree-parent babies‚Äù



This week we heard that eight babies have been born in the UK following an experimental form of IVF that involves DNA from three people. The approach was used to prevent women with genetic mutations from passing mitochondrial diseases to their children.But these eight babies aren‚Äôt the first ‚Äúthree-parent‚Äù children out there. Over the last decade, several teams have been using variations of this approach to help people have babies. But the procedure is not without controversy. Read the full story.



‚ÄîJessica Hamzelou



This article first appeared in The Checkup, MIT Technology Review‚Äôs weekly biotech newsletter. To receive it in your inbox every Thursday, and read articles like this first, sign up here.







The must-reads



I‚Äôve combed the internet to find you today‚Äôs most fun/important/scary/fascinating stories about technology.



1 OpenAI has launched ChatGPT Agent¬†It undertakes tasks on your behalf by building its own ‚Äúvirtual computer.‚Äù (The Verge)+ It may take a while to actually complete them. (Wired $)+ Are we ready to hand AI agents the keys? (MIT Technology Review)



2 The White House is going after ‚Äúwoke AI‚ÄùIt‚Äôs preparing an executive order preventing companies with ‚Äúliberal bias‚Äù in their models from landing federal contracts. (WSJ $)+ Why it‚Äôs impossible to build an unbiased AI language model. (MIT Technology Review)



3 A new law in Russia criminalizes certain online searchesLooking up LGBT content, for example, could land Russians in big trouble. (WP $)+ Dozens of Russian regions have been hit with cellphone internet shutdowns. (ABC News)



4 Elon Musk wants to detonate SpaceX rockets over Hawaii‚Äôs watersEven though the proposed area is a sacred Hawaiian religious site. (The Guardian)+ Rivals are rising to challenge the dominance of SpaceX. (MIT Technology Review) 



5 Meta‚Äôs privacy violation trial is overThe shareholders suing Mark Zuckerberg and other officials have settled for a (likely very hefty) payout. (Reuters)



6 Inside ICE‚Äôs powerful facial recognition appMobile Fortify can check a person‚Äôs face against a database of 200 million images. (404 Media)+ The department has unprecedented access to Medicaid data, too. (Wired $)



7 DOGE has left federal workers exhausted and anxiousSix months in, workers are struggling to cope with the fall out. (Insider $)+ DOGE‚Äôs tech takeover threatens the safety and stability of our critical data. (MIT Technology Review)



8 Netflix has used generative AI in a show for the first timeTo cut costs, apparently. (BBC)



9 Does AI really spell the end of loneliness?Virtual companions aren‚Äôt always what they‚Äôre cracked up to be. (New Yorker $)+ The AI relationship revolution is already here. (MIT Technology Review)



10 Flip phones are back with a vengeanceAt least they‚Äôre more interesting to look at than a conventional smartphone. (Vox)+ Triple-folding phones might be a bridge too far, though. (The Verge)







Quote of the day



‚ÄúIt is far from perfect.‚Äù



‚ÄîKevin Weil, OpenAI‚Äôs chief product officer, acknowledges that its new agent still requires a lot of work, Bloomberg reports.







One more thing







GMOs could reboot chestnut treesLiving as long as a thousand years, the American chestnut tree once dominated parts of the Eastern forest canopy, with many Native American nations relying on them for food. But by 1950, the tree had largely succumbed to a fungal blight probably introduced by Japanese chestnuts.As recently as last year, it seemed the 35-year effort to revive the American chestnut might grind to a halt. Now, American Castanea, a new biotech startup, has created more than 2,500 transgenic chestnut seedlings‚Äî likely the first genetically modified trees to be considered for federal regulatory approval as a tool for ecological restoration. Read the full story.



¬†‚ÄîAnya Kamenetz







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)



+ This stained glass embedded into a rusted old Porsche is strangely beautiful.+ Uhoh: here comes the next annoying group of people to avoid, the Normans.+ I bet Dolly Parton knows a thing or two about how to pack for a trip.+ Aww‚Äîorcas have been known to share food with humans in the wild.
‚Ä¢ A brief history of ‚Äúthree-parent babies‚Äù
  This week we heard that eight babies have been born in the UK following an experimental form of IVF that involves DNA from three people. The approach was used to prevent women with genetic mutations from passing mitochondrial diseases to their children. You can read all about the results, and the reception to them, here.¬†



But these eight babies aren‚Äôt the first ‚Äúthree-parent‚Äù children out there. Over the last decade, several teams have been using variations of this approach to help people have babies. This week, let‚Äôs consider the other babies born from three-person IVF.





I can‚Äôt go any further without talking about the term we use to describe these children. Journalists, myself included, have called them ‚Äúthree-parent babies‚Äù because they are created using DNA from three people. Briefly, the approach typically involves using the DNA from the nuclei of the intended parents‚Äô egg and sperm cells. That‚Äôs where most of the DNA in a cell is found.



But it also makes use of mitochondrial DNA (mtDNA)‚Äîthe DNA found in the energy-producing organelles of a cell‚Äîfrom a third person. The idea is to avoid using the mtDNA from the intended mother, perhaps because it is carrying genetic mutations. Other teams have done this in the hope of treating infertility.



mtDNA, which is usually inherited from a person‚Äôs mother, makes up a tiny fraction of total inherited DNA. It includes only 37 genes, all of which are thought to play a role in how mitochondria work (as opposed to, say, eye color or height).



That‚Äôs why some scientists despise the term ‚Äúthree-parent baby.‚Äù Yes, the baby has DNA from three people, but those three can‚Äôt all be considered parents, critics argue. For the sake of argument, this time around I‚Äôll use the term ‚Äúthree-person IVF‚Äù from here on out.



So, about these babies. The first were reported back in the 1990s. Jacques Cohen, then at Saint Barnabas Medical Center in Livingston, New Jersey, and his colleagues thought they might be able to treat some cases of infertility by injecting the mitochondria-containing cytoplasm of healthy eggs into eggs from the intended mother.¬†Seventeen babies were ultimately born this way, according to the team. (Side note: In¬†their paper, the authors describe potential resulting children as ‚Äúthree-parental individuals.‚Äù)



But two fetuses appeared to have genetic abnormalities. And one of the children started to show signs of a developmental disorder. In 2002, the US Food and Drug Administration put a stop to the research.





The babies born during that study¬†are in their 20s now. But scientists still don‚Äôt know why they saw those abnormalities. Some think that mixing mtDNA from two people might be problematic.



Newer approaches to three-person IVF aim to include mtDNA from just the donor, completely bypassing the intended mother‚Äôs mtDNA. John Zhang at the New Hope Fertility Center in New York City tried this approach for a Jordanian couple in 2016. The woman carried genes for a fatal mitochondrial disease and had already lost two children to it. She wanted to avoid passing it on to another child.



Zhang took the nucleus of the woman‚Äôs egg and inserted it into a donor egg that had had its own nucleus removed‚Äîbut still had its mitochondria-containing cytoplasm. That egg was then fertilized with the woman‚Äôs husband‚Äôs sperm.



Because it was still illegal in the US, Zhang controversially did the procedure in Mexico, where,¬†as he told me at the time, ‚Äúthere are no rules.‚Äù The couple eventually welcomed a healthy baby boy. Less than 1% of the boy‚Äôs mitochondria carried his mother‚Äôs mutation, so the procedure was deemed a success.



There was a fair bit of outrage from the scientific community, though. Mitochondrial donation had been made legal in the UK the previous year, but no clinic had yet been given a license to do it. Zhang‚Äôs experiment seemed to have been conducted with no oversight. Many questioned how ethical it was, although Sian Harding, who reviewed the ethics of the UK procedure, then told me it was ‚Äúas good as or better than what we‚Äôll do in the UK.‚Äù



The scandal had barely died down by the time the next ‚Äúthree-person IVF‚Äù babies were announced. In 2017, a team at the Nadiya Clinic in Ukraine¬†announced the birth of a little girl to parents who‚Äôd had the treatment for infertility. The news brought more outrage from some quarters, as scientists argued that the experimental procedure should only be used to prevent severe mitochondrial diseases.



It wasn‚Äôt until later that year that the UK‚Äôs fertility authority granted a team in Newcastle a license to perform mitochondrial donation. That team launched a trial in 2017. It was big news‚Äîthe first ‚Äúofficial‚Äù trial to test whether the approach could safely prevent mitochondrial disease.





But it was slow going. And meanwhile, other teams were making progress. The Nadiya Clinic continued to trial the procedure in couples with infertility. Pavlo Mazur, a former embryologist who worked at that clinic, tells me that 10 babies were born there as a result of mitochondrial donation.



Mazur then moved to another clinic in Ukraine, where he says he used a different type of mitochondrial donation to achieve another five healthy births for people with infertility. ‚ÄúIn total, it‚Äôs 15 kids made by me,‚Äù he says.



But he adds that other clinics in Ukraine are also using mitochondrial donation, without sharing their results. ‚ÄúWe don‚Äôt know the actual number of those kids in Ukraine,‚Äù says Mazur. ‚ÄúBut there are dozens of them.‚Äù



In 2020, Nuno Costa-Borges of Embryotools in Barcelona, Spain, and his colleagues described¬†another trial of mitochondrial donation. This trial, performed in Greece, was also designed to test the procedure for people with infertility. It involved 25 patients. So far,¬†seven children have been born. ‚ÄúI think it‚Äôs a bit strange that they aren‚Äôt getting more credit,‚Äù says Heidi Mertes, a medical ethicist at Ghent University in Belgium.



The newly announced UK births are only the latest ‚Äúthree-person IVF‚Äù babies. And while their births are being heralded as a success story for mitochondrial donation, the story isn‚Äôt quite so simple. Three of the eight babies were born with a non-insignificant proportion of mutated mitochondria, ranging between 5% and 20%, depending on the baby and the sample.



Dagan Wells of the University of Oxford, who is involved in the Greece trial, says that two of the seven babies in their study also appear to have inherited mtDNA from their intended mothers. Mazur says he has seen several cases of this ‚Äúreversal‚Äù too.



This isn‚Äôt a problem for babies whose mothers don‚Äôt carry genes for mitochondrial disease. But it might be for those whose mothers do.



I don‚Äôt want to pour cold water over the new UK results. It was great to finally see the results of a trial that‚Äôs been running for eight years. And the births of healthy babies are something to celebrate. But it‚Äôs not a simple success story. Mitochondrial donation doesn‚Äôt guarantee a healthy baby. We still have more to learn, not only from these babies, but from the others that have already been born.



This article first appeared in The Checkup,¬†MIT Technology Review‚Äôs¬†weekly biotech newsletter. To receive it in your inbox every Thursday, and read articles like this first,¬†sign up here.
‚Ä¢ Finding value from AI agents from day one
  Imagine AI so sophisticated it could read a customer‚Äôs mind? Or identify and close a cybersecurity loophole weeks before hackers strike? How about a team of AI agents equipped to restructure a global supply chain and circumnavigate looming geopolitical disruption? Such disruptive possibilities explain why agentic AI is sending ripples of excitement through corporate boardrooms.&nbsp;







Although still so early in its development that there lacks consensus on a single, shared definition, agentic AI refers loosely to a suite of AI systems capable of connected and autonomous decision-making with zero or limited human intervention. In scenarios where traditional AI typically requires explicit prompts or instructions for each step, agentic AI will independently execute tasks, learning and adapting to its environment to refine decisions over time.&nbsp;



From assuming oversight for complex workflows, such as procurement or recruitment, to carrying out proactive cybersecurity checks or automating support, enterprises are abuzz at the potential use cases for agentic AI.&nbsp;



According to one Capgemini survey, 50% of business executives are set to invest in and implement AI agents in their organizations in 2025, up from just 10% currently. Gartner has also forecast that 33% of enterprise software applications will incorporate agentic AI by 2028. For context, in 2024 that proportion was less than 1%.&nbsp;



‚ÄúIt‚Äôs creating such a buzz ‚Äì software enthusiasts seeing the possibilities unlocked by LLMs, venture capitalists wanting to find the next big thing, companies trying to find the ‚Äòkiller app,‚Äù says Matt McLarty, chief technology officer at Boomi. But, he adds, ‚Äúright now organizations are struggling to get out of the starting blocks.‚Äù&nbsp;



The challenge is that many organizations are so caught up in the excitement that they risk attempting to run before they can walk when it comes to deployment of agentic AI, believes McLarty. And in so doing they risk turning it from potential business breakthrough into a source of cost, complexity, and confusion.



Keeping agentic AI simple&nbsp;



The heady capabilities of agentic AI have created understandable temptation for senior business leaders to rush in, acting on impulse rather than insight risks turning the technology into a solution in search of a problem, points out McLarty.&nbsp;



It‚Äôs a scenario that‚Äôs unfolded with previous technologies. The decoupling of Blockchain from Bitcoin in 2014 paved the way for a Blockchain 2.0 boom in which organizations rushed to explore the applications for a digital, decentralized ledger beyond currency. But a decade on, the technology has fallen far short of forecasts at the time, dogged by technology limitations and obfuscated use cases.&nbsp;



‚ÄúI do see Blockchain as a cautionary tale,‚Äù says McLarty. ‚ÄúThe hype and ultimate lack of adoption is definitely a path the agentic AI movement should avoid.‚Äù He explains, ‚ÄúThe problem with Blockchain is that people struggle to find use cases where it applies as a solution, and even when they find the use cases, there is often a simpler and cheaper solution,‚Äù he adds. ‚ÄúI think agentic AI can do things no other solution can, in terms of contextual reasoning and dynamic execution. But as technologists, we get so excited about the technology, sometimes we lose sight of the business problem.‚Äù



Instead of diving in headfirst, McLarty advocates for an iterative attitude toward applications of agentic AI, targeting ‚Äúlow-hanging fruit‚Äù and incremental use cases. This includes focusing investment on the worker agents that are set to make up the components of more sophisticated, multi-agent agentic systems further down the road.&nbsp;



However, with a narrower, more prescribed remit, these AI agents with agentic capabilities can add instant value. Enabled with natural language processing (NLP) they can be used to bridge the linguistic shortfalls in current chat agents for example or adaptively carry out rote tasks via dynamic automation.&nbsp;



‚ÄúCurrent rote automation processes generate a lot of value for organizations today, but they can lead to a lot of manual exception processing,‚Äù points out McLarty. ‚ÄúAgentic exception handling agents can eliminate a lot of that.‚Äù&nbsp;



It‚Äôs also essential to avoid use cases for agentic AI that could be addressed with a cheaper and simpler technology. ‚ÄúConfiguring a self-manager, ephemeral agent swarm may sound exciting and be exhilarating to build, but maybe you can just solve the problem with a simple reasoning agent that has access to some in-house contextual data and API-based tools,‚Äù says McLarty. ‚ÄúLet‚Äôs call it the KASS principle: Keep agents simple, stupid.‚Äù



Connecting the dots



The future value of agentic AI will lie in its interoperability and organizations that prioritize this pillar at the earliest phase of their adoption will find themselves ahead of the curve.&nbsp;



As McLarty explains, the usefulness of agentic AI agents in scenarios like customer support chats lies in their combination of four elements: a defined business scope, large language models (LLM), the wider context derived from an organization‚Äôs existing data, and capabilities executed through its core applications. These latter two rely on in-built interoperability. For example, an AI agent tasked with onboarding new employees will require access to updated HR policies, asset catalogs and IT. ‚ÄúOrganizations can get a massive head start on business value through AI agents by having interoperable data and applications to plug and play with agents,‚Äù he says.&nbsp;



Agent-to-agent frameworks like the model context protocol (MCP) ‚Äì an open and standardized plug-and-play that connects AI models to internal (or external) information sources ‚Äì can be layered onto an existing API architecture to embed connectedness from the outset. And while it might feel like an additional hurdle now, in the longer-term those organizations that make this investment early will reap the benefits.&nbsp;



‚ÄúThe icing on the cake for interoperability is that all the work you do to connect agents to data and applications now will help you prepare for the multi-agent future where interoperability between agents will be essential,‚Äù says McLarty.&nbsp;



In this future, multi-agent systems will work collectively on more intricate, cross-functional tasks. Agentic systems will draw on AI agents across inventory, logistics and production to coordinate and optimize supply chain management for example or perform complex assembly tasks.&nbsp;



Conscious that this is where the technology is headed, third-party developers are already beginning to offer multi-agent capability. In December, Amazon launched such a tool for its Bedrock service, providing users access to specialized agents coordinated by a supervisor agent capable of breaking down requests, delegating tasks and consolidating outputs.&nbsp;



But though such an off-the-rack solution has the advantage of allowing enterprises to bypass both the risk and complexity in leveraging such capabilities, the digital heterogeneity of larger organizations in particular will likely mean ‚Äì in the longer-term at least ‚Äì they‚Äôll need to rely on their own API architecture to realize the full potential in multi-agent systems.



McLarty‚Äôs advice is simple, ‚ÄúThis is definitely a time to ground yourself in the business problem, and only go as far as you need to with the solution.‚Äù



This content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review‚Äôs editorial staff.



This content was researched, designed, and written entirely by human writers, editors, analysts, and illustrators. This includes the writing of surveys and collection of data for surveys. AI tools that may have been used were limited to secondary production processes that passed thorough human review.
‚Ä¢ How to run an LLM on your laptop
  MIT Technology Review‚Äôs How To series helps you get things done.&nbsp;



Simon Willison has a plan for the end of the world. It‚Äôs a USB stick, onto which he has loaded a couple of his favorite open-weight LLMs‚Äîmodels that have been shared publicly by their creators and that can, in principle, be downloaded and run with local hardware. If human civilization should ever collapse, Willison plans to use all the knowledge encoded in their billions of parameters for help. ‚ÄúIt‚Äôs like having a weird, condensed, faulty version of Wikipedia, so I can help reboot society with the help of my little USB stick,‚Äù he says.



But you don‚Äôt need to be planning for the end of the world to want to run an LLM on your own device. Willison, who writes a popular blog about local LLMs and software development, has plenty of compatriots: r/LocalLLaMA, a subreddit devoted to running LLMs on your own hardware, has half a million members.



For people who are concerned about privacy, want to break free from the control of the big LLM companies, or just enjoy tinkering, local models offer a compelling alternative to ChatGPT and its web-based peers.



The local LLM world used to have a high barrier to entry: In the early days, it was impossible to run anything useful without investing in pricey GPUs. But researchers have had so much success in shrinking down and speeding up models that anyone with a laptop, or even a smartphone, can now get in on the action. ‚ÄúA couple of years ago, I‚Äôd have said personal computers are not powerful enough to run the good models. You need a $50,000 server rack to run them,‚Äù Willison says. ‚ÄúAnd I kept on being proved wrong time and time again.‚Äù





Why you might want to download your own LLM



Getting into local models takes a bit more effort than, say, navigating to ChatGPT‚Äôs online interface. But the very accessibility of a tool like ChatGPT comes with a cost. ‚ÄúIt‚Äôs the classic adage: If something‚Äôs free, you‚Äôre the product,‚Äù says Elizabeth Seger, the director of digital policy at Demos, a London-based think tank.&nbsp;



OpenAI, which offers both paid and free tiers, trains its models on users‚Äô chats by default. It‚Äôs not too difficult to opt out of this training, and it also used to be possible to remove your chat data from OpenAI‚Äôs systems entirely, until a recent legal decision in the New York Times‚Äô ongoing lawsuit against OpenAI required the company to maintain all user conversations with ChatGPT.



Google, which has access to a wealth of data about its users, also trains its models on both free and paid users‚Äô interactions with Gemini, and the only way to opt out of that training is to set your chat history to delete automatically‚Äîwhich means that you also lose access to your previous conversations. In general, Anthropic does not train its models using user conversations, but it will train on conversations that have been ‚Äúflagged for Trust &amp; Safety review.‚Äù¬†



Training may present particular privacy risks because of the ways that models internalize, and often recapitulate, their training data. Many people trust LLMs with deeply personal conversations‚Äîbut if models are trained on that data, those conversations might not be nearly as private as users think, according to some experts.



‚ÄúSome of your personal stories may be cooked into some of the models, and eventually be spit out in bits and bytes somewhere to other people,‚Äù says Giada Pistilli, principal ethicist at the company Hugging Face, which runs a huge library of freely downloadable LLMs and other AI resources.



For Pistilli, opting for local models as opposed to online chatbots has implications beyond privacy. ‚ÄúTechnology means power,‚Äù she says. ‚ÄúAnd so who[ever] owns the technology also owns the power.‚Äù States, organizations, and even individuals might be motivated to disrupt the concentration of AI power in the hands of just a few companies by running their own local models.



Breaking away from the big AI companies also means having more control over your LLM experience. Online LLMs are constantly shifting under users‚Äô feet: Back in April, ChatGPT suddenly started sucking up to users far more than it had previously, and just last week Grok started calling itself MechaHitler on X.



Providers tweak their models with little warning, and while those tweaks might sometimes improve model performance, they can also cause undesirable behaviors. Local LLMs may have their quirks, but at least they are consistent. The only person who can change your local model is you.



Of course, any model that can fit on a personal computer is going to be less powerful than the premier online offerings from the major AI companies. But there‚Äôs a benefit to working with weaker models‚Äîthey can inoculate you against the more pernicious limitations of their larger peers. Small models may, for example, hallucinate more frequently and more obviously than Claude, GPT, and Gemini, and seeing those hallucinations can help you build up an awareness of how and when the larger models might also lie.



‚ÄúRunning local models is actually a really good exercise for developing that broader intuition for what these things can do,‚Äù Willison says.



How to get started



Local LLMs aren‚Äôt just for proficient coders. If you‚Äôre comfortable using your computer‚Äôs command-line interface, which allows you to browse files and run apps using text prompts, Ollama is a great option. Once you‚Äôve installed the software, you can download and run any of the hundreds of models they offer with a single command.&nbsp;



If you don‚Äôt want to touch anything that even looks like code, you might opt for LM Studio, a user-friendly app that takes a lot of the guesswork out of running local LLMs. You can browse models from Hugging Face from right within the app, which provides plenty of information to help you make the right choice. Some popular and widely used models are tagged as ‚ÄúStaff Picks,‚Äù and every model is labeled according to whether it can be run entirely on your machine‚Äôs speedy GPU, needs to be shared between your GPU and slower CPU, or is too big to fit onto your device at all. Once you‚Äôve chosen a model, you can download it, load it up, and start interacting with it using the app‚Äôs chat interface.



As you experiment with different models, you‚Äôll start to get a feel for what your machine can handle. According to Willison, every billion model parameters require about one GB of RAM to run, and I found that approximation to be accurate: My own 16 GB laptop managed to run Alibaba‚Äôs Qwen3 14B as long as I quit almost every other app. If you run into issues with speed or usability, you can always go smaller‚ÄîI got reasonable responses from Qwen3 8B as well.



And if you go really small, you can even run models on your cell phone. My beat-up iPhone 12 was able to run Meta‚Äôs Llama 3.2 1B using an app called LLM Farm. It‚Äôs not a particularly good model‚Äîit very quickly goes off into bizarre tangents and hallucinates constantly‚Äîbut trying to coax something so chaotic toward usability can be entertaining. If I‚Äôm ever on a plane sans Wi-Fi and desperate for a probably false answer to a trivia question, I now know where to look.



Some of the models that I was able to run on my laptop were effective enough that I can imagine using them in my journalistic work. And while I don‚Äôt think I‚Äôll depend on phone-based models for anything anytime soon, I really did enjoy playing around with them. ‚ÄúI think most people probably don‚Äôt need to do this, and that‚Äôs fine,‚Äù Willison says. ‚ÄúBut for the people who want to do this, it‚Äôs so much fun.‚Äù

üîí Cybersecurity & Privacy
‚Ä¢ Poor Passwords Tattle on AI Hiring Bot Maker Paradox.ai
  Security researchers recently revealed that the personal information of millions of people who applied for jobs at McDonald&#8217;s was exposed after they guessed the password (&#8220;123456&#8221;) for the fast food chain&#8217;s account at Paradox.ai, a company that makes artificial intelligence based hiring chatbots used by many Fortune 500 firms. Paradox.ai said the security oversight was an isolated incident that did not affect its other customers, but recent security breaches involving its employees in Vietnam tell a more nuanced story.
A screenshot of the paradox.ai homepage showing its AI hiring chatbot &#8220;Olivia&#8221; interacting with potential hires.
Earlier this month, security researchers Ian Carroll and Sam Curry wrote about simple methods they found to access the backend of the AI chatbot platform on McHire.com, the McDonald&#8217;s website that many of its franchisees use to screen job applicants. As first reported by Wired, the researchers discovered that the weak password used by Paradox exposed 64 million records, including applicants&#8217; names, email addresses and phone numbers.
Paradox.ai acknowledged the researchers&#8217; findings but said the company&#8217;s other client instances were not affected, and that no sensitive information &#8212; such as Social Security numbers &#8212; was exposed.
&#8220;We are confident, based on our records, this test account was not accessed by any third party other than the security researchers,&#8221; the company wrote in a July 9 blog post. &#8220;It had not been logged into since 2019 and frankly, should have been decommissioned. We want to be very clear that while the researchers may have briefly had access to the system containing all chat interactions (NOT job applications), they only viewed and downloaded five chats in total that had candidate information within. Again, at no point was any data leaked online or made public.&#8221;
However, a review of stolen password data gathered by multiple breach-tracking services shows that at the end of June 2025, a Paradox.ai administrator in Vietnam suffered a malware compromise on their device that stole usernames and passwords for a variety of internal and third-party online services. The results were not pretty.
The password data from the Paradox.ai developer was stolen by a malware strain known as &#8220;Nexus Stealer,&#8221; a form grabber and password stealer that is sold on cybercrime forums. The information snarfed by stealers like Nexus is often recovered and indexed by data leak aggregator services like Intelligence X, which reports that the malware on the Paradox.ai developer&#8217;s device exposed hundreds of mostly poor and recycled passwords (using the same base password but slightly different characters at the end).
Those purloined credentials show the developer in question at one point used the same seven-digit password to log in to Paradox.ai accounts for a number of Fortune 500 firms listed as customers on the company&#8217;s website, including Aramark, Lockheed Martin, Lowes, and Pepsi.
Seven-character passwords, particularly those consisting entirely of numerals, are highly vulnerable to &#8220;brute-force&#8221; attacks that can try a large number of possible password combinations in quick succession. According to a much-referenced password strength guide maintained by Hive Systems, modern password-cracking systems can work out a seven number password more or less instantly.
Image: hivesystems.com.
In response to questions from KrebsOnSecurity, Paradox.ai confirmed that the password data was recently stolen by a malware infection on the personal device of a longtime Paradox developer based in Vietnam, and said the company was made aware of the compromise shortly after it happened. Paradox maintains that few of the exposed passwords were still valid, and that a majority of them were present on the employee&#8217;s personal device only because he had migrated the contents of a password manager from an old computer.
Paradox also pointed out that it has been requiring single sign-on (SSO) authentication since 2020 that enforces multi-factor authentication for its partners. Still, a review of the exposed passwords shows they included the Vietnamese administrator&#8217;s credentials to the company&#8217;s SSO platform &#8212; paradoxai.okta.com. The password for that account ended in 202506 &#8212; possibly a reference to the month of June 2025 &#8212; and the digital cookie left behind after a successful Okta login with those credentials says it was valid until December 2025.
Also exposed were the administrator&#8217;s credentials and authentication cookies for an account at Atlassian, a platform made for software development and project management. The expiration date for that authentication token likewise was December 2025.
Infostealer infections are among the leading causes of data breaches and ransomware attacks today, and they result in the theft of stored passwords and any credentials the victim types into a browser. Most infostealer malware also will siphon authentication cookies stored on the victim&#8217;s device, and depending on how those tokens are configured thieves may be able to use them to bypass login prompts and/or multi-factor authentication.
Quite often these infostealer infections will open a backdoor on the victim&#8217;s device that allows attackers to access the infected machine remotely. Indeed, it appears that remote access to the Paradox administrator&#8217;s compromised device was offered for sale recently.
In February 2019, Paradox.ai announced it had successfully completed audits for two fairly comprehensive security standards (ISO 27001 and SOC 2 Type II). Meanwhile, the company&#8217;s security disclosure this month says the test account with the atrocious 123456 username and password was last accessed in 2019, but somehow missed in their annual penetration tests. So how did it manage to pass such stringent security audits with these practices in place?
Paradox.ai told KrebsOnSecurity that at the time of the 2019 audit, the company&#8217;s various contractors were not held to the same security standards the company practices internally. Paradox emphasized that this has changed, and that it has updated its security and password requirements multiple times since then.
It is unclear how the Paradox developer in Vietnam infected his computer with malware, but a closer review finds a Windows device for another Paradox.ai employee from Vietnam was compromised by similar data-stealing malware at the end of 2024 (that compromise included the victim&#8217;s GitHub credentials). In the case of both employees, the stolen credential data includes Web browser logs that indicate the victims repeatedly downloaded pirated movies and television shows, which are often bundled with malware disguised as a video codec needed to view the pirated content.

üéì University AI
No updates.

üè¢ Corporate AI
‚Ä¢ CollabLLM: Teaching LLMs to collaborate with users
  Large language models (LLMs) can solve complex puzzles in seconds, yet they sometimes struggle over simple conversations. When these AI tools make assumptions, overlook key details, or neglect to ask clarifying questions, the result can erode trust and derail real-world interactions, where nuance is everything.



A key reason these models behave this way lies in how they‚Äôre trained and evaluated. Most benchmarks use isolated, single-turn prompts with clear instructions. Training methods tend to optimize for the model&#8217;s next response, not its contribution to a successful, multi-turn exchange. But real-world interaction is dynamic and collaborative. It relies on context, clarification, and shared understanding.



User-centric approach to training&nbsp;



To address this, we‚Äôre exploring ways to train LLMs with users in mind. Our approach places models in simulated environments that reflect the back-and-forth nature of real conversations. Through reinforcement learning, these models improve through trial and error, for example, learning when to ask questions and how to adapt tone and communication style to different situations. This user-centric approach helps bridge the gap between how LLMs are typically trained and how people actually use them.¬†¬†



This is the concept behind CollabLLM (opens in new tab), recipient of an ICML (opens in new tab) Outstanding Paper Award (opens in new tab). This training framework helps LLMs improve through simulated multi-turn interactions, as illustrated in Figure 1. The core insight behind CollabLLM is simple: in a constructive collaboration, the value of a response isn‚Äôt just in its immediate usefulness, but in how it contributes to the overall success of the conversation. A clarifying question might seem like a delay but often leads to better outcomes. A quick answer might appear useful but can create confusion or derail the interaction.



Figure 1. Diagram comparing two training approaches for LLMs. (a) The standard method lacks user-agent collaboration and uses single-turn rewards, leading to an inefficient conversation. (b) In contrast, CollabLLM simulates multi-turn user-agent interactions during training, enabling it to learn effective collaboration strategies and produce more efficient dialogues.



CollabLLM puts this collaborative approach into practice with a simulation-based training loop, illustrated in Figure 2. At any point in a conversation, the model generates multiple possible next turns by engaging in a dialogue with a simulated user.



Figure 2: Simulation-based training process used in CollabLLM



The system uses a sampling method to extend conversations turn by turn, choosing likely responses for each participant (the AI agent or the simulated user), while adding some randomness to vary the conversational paths. The goal is to expose the model to a wide variety of conversational scenarios, helping it learn more effective collaboration strategies.



	
		

		
		PODCAST SERIES
	
	
	
						
				
					
				
			
			
			

									The AI Revolution in Medicine, Revisited
				
								Join Microsoft‚Äôs Peter Lee on a journey to discover how AI is impacting healthcare and what it means for the future of medicine.
				
								
					
						
							Listen now						
					
				
							
	
Opens in a new tab	
	


To each simulated conversation, we applied multiturn-aware reward (MR) functions, which assess how the model‚Äôs response at the given turn influences the entire trajectory of the conversation. We sampled multiple conversational follow-ups from the model, such as statements, suggestions, questions, and used MR to assign a reward to each based on how well the conversation performed in later turns. We based these scores on automated metrics that reflect key factors like goal completion, conversational efficiency, and user engagement.



To score the sampled conversations, we used task-specific metrics and metrics from an LLM-as-a-judge framework, which supports efficient and scalable evaluation. For metrics like engagement, a judge model rates each sampled conversation on a scale from 0 to 1.



The MR of each model response was computed by averaging the scores from the sampled conversations, originating from the model response. Based on the score, the model updates its parameters using established reinforcement learning algorithms like Proximal Policy Optimization (PPO) or Direct Preference Optimization (DPO).



We tested CollabLLM through a combination of automated and human evaluations, detailed in the paper. One highlight is a user study involving 201 participants in a document co-creation task, shown in Figure 3. We compared CollabLLM to a baseline trained with single-turn rewards and to a second, more proactive baseline prompted to ask clarifying questions and take other proactive steps. CollabLLM outperformed both, producing higher-quality documents, better interaction ratings, and faster task completion times.



Figure 3: Results of the user study in a document co-creation task comparing CollabLLM to a baseline trained with single-turn rewards.



Designing for real-world collaboration



Much of today‚Äôs AI research focuses on fully automated tasks, models working without input from or interaction with users. But many real-world applications depend on people in the loop: as users, collaborators, or decision-makers. Designing AI systems that treat user input not as a constraint, but as essential, leads to systems that are more accurate, more helpful, and ultimately more trustworthy.



This work is driven by a core belief: the future of AI depends not just on intelligence, but on the ability to collaborate effectively. And that means confronting the communication breakdowns in today‚Äôs systems.



We see CollabLLM as a step in that direction, training models to engage in meaningful multi-turn interactions, ask clarifying questions, and adapt to context. In doing so, we can build systems designed to work with people‚Äînot around them.
Opens in a new tabThe post CollabLLM: Teaching LLMs to collaborate with users appeared first on Microsoft Research.
‚Ä¢ Build real-time travel recommendations using AI agents on Amazon Bedrock
  Generative AI is transforming how businesses deliver personalized experiences across industries, including travel and hospitality. Travel agents are enhancing their services by offering personalized holiday packages, carefully curated for customer‚Äôs unique preferences, including accessibility needs, dietary restrictions, and activity interests. Meeting these expectations requires a solution that combines comprehensive travel knowledge with real-time pricing and availability information. 
In this post, we show how to build a generative AI solution using Amazon Bedrock that creates bespoke holiday packages by combining customer profiles and preferences with real-time pricing data. We demonstrate how to use Amazon Bedrock Knowledge Bases for travel information, Amazon Bedrock Agents for real-time flight details, and Amazon OpenSearch Serverless for efficient package search and retrieval. 
Solution overview 
Travel agencies face increasing demands for personalized recommendations while struggling with real-time data accuracy and scalability. Consider a travel agency that needs to offer accessible holiday packages: they need to match specific accessibility requirements with real-time flight and accommodation availability but are constrained by manual processing times and outdated information in traditional systems. This AI-powered solution combines personalization with real-time data integration, enabling the agency to automatically match accessibility requirements with current travel options, delivering accurate recommendations in minutes rather than hours.The solution uses a three-layer architecture to help travel agents create personalized holiday recommendations: 
 
 Frontend layer ‚Äì Provides an interface where travel agents input customer requirements and preferences 
 Orchestration layer ‚Äì Processes request and enriches them with customer data 
 Recommendation layer ‚Äì Combines two key components: 
   
   Travel data storage ‚Äì Maintains a searchable repository of travel packages 
   Real-time information retrieval ‚Äì Fetches current flight details through API integration 
    
 
The following diagram illustrates this architecture. 
 
With this layered approach, travel agents can capture customer requirements, enrich them with stored preferences, integrate real-time data, and deliver personalized recommendations that match customer needs. The following diagram illustrates how these components are implemented using AWS services. 
 
The AWS implementation includes: 
 
 Amazon API Gateway ‚Äì Receives requests and routes them to AWS Lambda functions facilitating secure API calls for retrieving recommendations 
 AWS Lambda ‚Äì Processes input data, creates the enriched prompt, and executes the recommendation workflow 
 Amazon DynamoDB ‚Äì Stores customer preferences and travel history 
 Amazon Bedrock Knowledge Bases ‚Äì Helps travel agents build a curated database of destinations, travel packages, and deals, making sure recommendations are based on reliable and up-to-date information 
 Amazon OpenSearch Serverless ‚Äì Enables simple, scalable, and high-performing vector search 
 Amazon Simple Storage Service (Amazon S3) ‚Äì Stores large datasets such as flight schedules and promotional materials 
 Amazon Bedrock Agents ‚Äì Integrates real-time information retrieval, making sure recommended itineraries reflect current availability, pricing, and scheduling through external API integrations 
 
This solution uses a AWS CloudFormation template that automatically provisions and configures the required resources. The template handles the complete setup process, including service configurations and necessary permissions. 
For the latest information about service quotas that might affect your deployment, refer to AWS service quotas. 
Prerequisites 
To deploy and use this solution, you must have the following: 
 
 An AWS account with access to Amazon Bedrock 
 Permissions to create and manage the following services: 
   
   Amazon Bedrock 
   Amazon OpenSearch Serverless 
   Lambda 
   DynamoDB 
   Amazon S3 
   API Gateway 
    
 Access to foundation models in Amazon Bedrock for Amazon Titan Text Embeddings V2 and Anthropic Claude 3 Haiku models 
 
Deploy the CloudFormation stack 
You can deploy this solution in your AWS account using AWS CloudFormation. Complete the following steps: 
 
 Choose Launch Stack: 
 
 
You will be redirected to the Create stack wizard on the AWS CloudFormation console with the stack name and the template URL already filled in. 
 
 Leave the default settings and complete the stack creation. 
 Choose View stack events to go to the AWS CloudFormation console to see the deployment details. 
 
The stack takes around 10 minutes to create the resources. Wait until the stack status is CREATE_COMPLETE before continuing to the next steps. 
The CloudFormation template automatically creates and configures components for data storage and management, Amazon Bedrock, and the API and interface. 
Data storage and management 
The template sets up the following data storage and management resources: 
 
 An S3 bucket and with a sample dataset (travel_data.json and promotions.csv), prompt template, and the API schema 
 
 
 
 DynamoDB tables populated with sample user profiles and travel history 
 
 
 
 An OpenSearch Serverless collection with optimized settings for travel package searches 
 
 
 
 A vector index with settings compatible with the Amazon Bedrock knowledge base 
 
 
Amazon Bedrock configuration 
For Amazon Bedrock, the CloudFormation template creates the following resources: 
 
 A knowledge base with the travel dataset and data sources ingested from Amazon S3 with automatic synchronization 
 
 
 
 An Amazon Bedrock agent, which is automatically prepared 
 
 
 
 A new version and alias for the agent 
 
 
 
 Agent action groups with mock flight data integration 
 
 
 
 An action group invocation, configured with the FlightPricingLambda Lambda function and the API schema retrieved from the S3 bucket 
 
 
API and interface setup 
To enable API access and the UI, the template configures the following resources: 
 
 API Gateway endpoints 
 Lambda functions with a mock flight API for demonstration purposes 
 A web interface for travel agents 
 
Verify the setup 
After stack creation is complete, you can verify the setup on the Outputs tab of the AWS CloudFormation console, which provides the following information: 
 
 WebsiteURL ‚Äì Access the travel agent interface 
 ApiEndpoint ‚Äì Use for programmatic access to the recommendation system 
 
 
Test the endpoints 
The web interface provides an intuitive form where travel agents can input customer requirements, including: 
 
 Customer ID (for example, Joe or Will) 
 Travel budget 
 Preferred dates 
 Number of travelers 
 Travel style 
 
 
You can call the API directly using the following code: 
 
 curl -X POST \
&nbsp;&nbsp;&lt;ApiEndpoint&gt; \
&nbsp;&nbsp;-H 'Content-Type: application/json' \
&nbsp;&nbsp;-d '{
&nbsp;&nbsp; &nbsp;"userId": "Joe",
&nbsp;&nbsp; &nbsp;"budget": "3000 GBP",
&nbsp;&nbsp; &nbsp;"duration": "7 days",
&nbsp;&nbsp; &nbsp;"travelDate": "2025-07-15",
&nbsp;&nbsp; &nbsp;"numberOfTravelers": 2
&nbsp;&nbsp;}' 
 
Test the solution 
For demonstration purposes, we create sample user profiles in the UserPreferences and TravelHistory tables in DynamoDB. 
The UserPreferences table stores user-specific travel preferences. For instance, Joe represents a luxury traveler with wheelchair accessibility requirements. 
 
Will represents a budget traveler with elderly-friendly needs. These profiles help showcase how the system handles different customer requirements and preferences. 
 
The TravelHistory table stores past trips taken by users. The following tables show the past trips taken by the user Joe, showing destinations, trip durations, ratings, and travel dates. 
 
Let‚Äôs walk through a typical use case to demonstrate how a travel agent can use this solution to create personalized holiday recommendations.Consider a scenario where a travel agent is helping Joe, a customer who requires wheelchair accessibility, plan a luxury vacation. The travel agent enters the following information: 
 
 Customer ID: Joe 
 Budget: 4,000 GBP 
 Duration: 5 days 
 Travel dates: July 15, 2025 
 Number of travelers: 2 
 Travel style: Luxury 
 
 
When a travel agent submits a request, the system orchestrates a series of actions through the PersonalisedHolidayFunction Lambda function, which will query the knowledge base, check real-time flight information using the mock API, and return personalized recommendations that match the customer‚Äôs specific needs and preferences. The recommendation layer uses the following prompt template: 
 
 Based on the profile and requirements:

User Preferences:
- Travel Preferences: {travelStyle}
- Interests: {interests}
- Dietary Restrictions: {dietaryRestrictions}
- Accessibility Needs: {accessibility}

Current Request:
- Budget: {budget}
- Duration: {duration}
- Travel Date: {travelDate}
- Number of Travelers: {numberOfTravelers}

Previous Destinations: {previousDestinations}

Instructions:
1. Match the user's budget, travel style and interests
2. Consider dietary restrictions and accessibility needs
3. Avoid previously visited destinations
4. Include:
&nbsp;&nbsp; - Recommended destinations
&nbsp;&nbsp; - Suitable accommodations
&nbsp;&nbsp; - Relevant activities and experiences
&nbsp;&nbsp; - Transportation options
&nbsp;&nbsp; - Estimated cost breakdown
&nbsp;&nbsp; - Travel tips

Please follow the &lt;Instructions&gt; and provide a personalized holiday recommendation in the below format:
Destination: [Primary recommended destination]

[Detailed recommendation] 
 
The system retrieves Joe‚Äôs preferences from the user profile, including: 
 
 {
&nbsp;&nbsp; &nbsp;"userPreferences": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"preferences": "Prefer warm climate and cultural experiences",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"budget": 3000,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"duration": "5 days",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"travelDate": "2025-03-04",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"interests": [
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"photography",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"food",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"beach"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;],
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"travelStyle": "Luxury",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"numberOfTravelers": 2,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"dietaryRestrictions": [
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"plant based",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"vegetarian"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;],
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"accessibility": [
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"wheelchair-accessible"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;],
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"previousDestinations": [
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"Maldives",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"Bali"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;]
&nbsp;&nbsp; &nbsp;}
} 
 
The system then generates personalized recommendations that consider the following: 
 
 Destinations with proven wheelchair accessibility 
 Available luxury accommodations 
 Flight details for the recommended destination 
 
Each recommendation includes the following details: 
 
 Detailed accessibility information 
 Real-time flight pricing and availability 
 Accommodation details with accessibility features 
 Available activities and experiences 
 Total package cost breakdown 
 
Clean up 
To avoid incurring future charges, delete the CloudFormation stack. For more information, see Delete a stack from the CloudFormation console. 
The template includes proper deletion policies, making sure the resources you created, including S3 buckets, DynamoDB tables, and OpenSearch collections, are properly removed. 
Next steps 
To further enhance this solution, consider the following: 
 
 Explore multi-agent capabilities: 
   
   Create specialized agents for different travel aspects (hotels, activities, local transport) 
   Enable agent-to-agent communication for complex itinerary planning 
   Implement an orchestrator agent to coordinate responses and resolve conflicts 
    
 Implement multi-language support using multi-language foundation models in Amazon Bedrock 
 Integrate with customer relationship management (CRM) systems 
 
Conclusion 
In this post, you learned how to build an AI-powered holiday recommendation system using Amazon Bedrock that helps travel agents deliver personalized experiences. Our implementation demonstrated how combining Amazon Bedrock Knowledge Bases with Amazon Bedrock Agents effectively bridges historical travel information with real-time data needs, while using serverless architecture and vector search for efficient matching of customer preferences with travel packages.The solution shows how travel recommendation systems can balance comprehensive travel knowledge, real-time data accuracy, and personalization at scale. This approach is particularly valuable for travel organizations needing to integrate real-time pricing data, handle specific accessibility requirements, or scale their personalized recommendations. This solution provides a practical starting point with clear paths for enhancement based on specific business needs, from modernizing your travel planning systems or handling complex customer requirements. 
Related resources 
To learn more, refer to the following resources: 
 
 Documentation: 
   
   Amazon Bedrock Documentation 
   Automate tasks in your application using AI agents 
   Retrieve data and generate AI responses with Amazon Bedrock Knowledge Bases 
   Amazon OpenSearch Serverless Developer Guide 
   Building Lambda functions with Python 
    
 Code samples: 
   
   Amazon Bedrock RAG with Knowledge Bases and Agents 
   Amazon Bedrock Samples Repository 
   Amazon Bedrock Agent Samples Repository 
    
 Additional learning: 
   
   AWS Machine Learning Blog 
   AWS Training and Certification 
    
 
 
About the Author 
Vishnu Vardhini 
Vishnu Vardhini is a Solutions Architect at AWS based in Scotland, focusing on SMB customers across industries. With expertise in Security, Cloud Engineering and DevOps, she architects scalable and secure AWS solutions. She is passionate about helping customers leverage Machine Learning and Generative AI to drive business value.
‚Ä¢ Deploy a full stack voice AI agent with Amazon Nova Sonic
  AI-powered speech solutions are transforming contact centers by enabling natural conversations between customers and AI agents, shortening wait times, and dramatically reducing operational costs‚Äîall without sacrificing the human-like interaction customers expect. With the recent launch of Amazon Nova Sonic in Amazon Bedrock, you can now build sophisticated conversational AI agents that communicate naturally through voice, without the need for separate speech recognition and text-to-speech components. Amazon Nova Sonic is a speech-to-speech model in Amazon Bedrock that enables real-time, human-like voice conversations. 
Whereas many early Amazon Nova Sonic implementations focused on local development, this solution provides a complete cloud-deployed architecture that you can use as a foundation for building real proof of concept applications. This asset is deployable through the AWS Cloud Development Kit (AWS CDK) and provides a foundation for building further Amazon Nova use cases using preconfigured infrastructure components, while allowing you to customize the architecture to address your specific business requirements. 
In this post, we show how to create an AI-powered call center agent for a fictional company called AnyTelco. The agent, named Telly, can handle customer inquiries about plans and services while accessing real-time customer data using custom tools implemented with the Model Context Protocol (MCP) framework. 
Solution overview 
The following diagram provides an overview of the deployable solution. 
 
The solution is composed of the following layers: 
 
 Frontend layer ‚Äì The frontend layer of this system is built with scalability and performance in mind: 
   
   Amazon CloudFront distribution serves as the content delivery network for the web application. 
   Amazon Simple Storage Service (Amazon S3) hosts static assets. 
   The UI handles audio streaming and user interaction. 
    
 Communication layer ‚Äì The communication layer facilitates seamless real-time interactions: 
   
   Network Load Balancer manages WebSocket connections. WebSockets enable two-way interactive communication sessions between a user‚Äôs browser and the server, which is essential for real-time audio streaming applications. 
   Amazon Cognito provides user authentication and JSON web token (JWT) validation. Amazon Cognito provides user authentication, authorization, and user management for web and mobile applications, alleviating the need to build and maintain your own identity systems. 
    
 Processing layer ‚Äì The processing layer forms the computational backbone of the system: 
   
   Amazon Elastic Container Service (Amazon ECS) runs the containerized backend service. 
   AWS Fargate provides the serverless compute backend. Orchestration is provided by the Amazon ECS engine. 
   The Python backend processes audio streams and manages Amazon Nova Sonic interactions. 
    
 Intelligence layer ‚Äì The intelligence layer uses AI and data technologies to power the core functionalities: 
   
   The Amazon Nova Sonic model in Amazon Bedrock handles speech processing. 
   Amazon DynamoDB stores customer information. 
   Amazon Bedrock Knowledge Bases connects foundation models (FMs) with your organization‚Äôs data sources, allowing AI applications to reference accurate, up-to-date information specific to your business. 
    
 
The following sequence diagram highlights the flow when a user initiates conversation. The user only signs in one time, but authentication Steps 3 and 4 happen every time the user starts a new session. The conversational loop in Steps 6‚Äì12 is repeated throughout the conversational interaction. Steps a‚Äìc only happen when the Amazon Nova Sonic agent decides to use a tool. In scenarios without tool use, the flow goes directly from Step 9 to Step 10. 
 
Prerequisites 
Before getting started, verify that you have the following: 
 
 Python 3.12 
 Node.js v20 
 npm v10.8 
 An AWS account 
 The AWS CDK set up (for prerequisites and installation instructions, see Getting started with the AWS CDK) 
 Amazon Nova Sonic enabled in Amazon Bedrock (for more information, see Add or remove access to Amazon Bedrock foundation models) 
 Chrome or Safari browser environment (Firefox is not supported at the time of writing) 
 A working microphone and speakers 
 
Deploy the solution 
You can find the solution and full deployment instructions on the GitHub repository. The solution uses the AWS CDK to automate infrastructure deployment. Use the following code terminal commands to get started in your AWS Command Line Interface (AWS CLI) environment: 
 
 git clone https://github.com/aws-samples/sample-sonic-cdk-agent.git 
cd nova-s2s-call-center 

# Configure environment variables
cp template.env .env

# Edit .env with your settings

# Deploy the solution 
./deploy.sh  
 
The deployment creates two AWS CloudFormation stacks: 
 
 Network stack for virtual private cloud (VPC) and networking components 
 Stack for application resources 
 
The output of the second stack gives you a CloudFront distribution link, which takes you to the login page. 
 
You can create an Amazon Cognito admin user with the following AWS CLI command: 
 
 aws cognito-idp admin-create-user \
  --user-pool-id YOUR_USER_POOL_ID \
  --username USERNAME \
  --user-attributes Name=email,Value=USER_EMAIL \
  --temporary-password TEMPORARY_PASSWORD \
  --region YOUR_AWS_REGION 
 
The preceding command uses the following parameters: 
 
 YOUR_USER_POOL_ID: The ID of your Amazon Cognito user pool 
 USERNAME: The desired user name for the user 
 USER_EMAIL: The email address of the user 
 TEMPORARY_PASSWORD: A temporary password for the user 
 YOUR_AWS_REGION: Your AWS Region (for example, us-east-1) 
 
Log in with your temporary password from the CloudFront distribution link, and you will be asked to set a new password. 
You can choose Start Session to start a conversation with your assistant. Experiment with prompts and different tools for your use case. 
 
Customizing the application 
A key feature of this solution is its flexibility‚Äîyou can tailor the AI agent‚Äôs capabilities to your specific use case. The sample implementation demonstrates this extensibility through custom tools and knowledge integration: 
 
 Customer information lookup ‚Äì Retrieves customer profile data from DynamoDB using phone numbers as keys 
 Knowledge base search ‚Äì Queries an Amazon Bedrock knowledge base for company information, plan details, and pricing 
 
These features showcase how to enhance the functionality of Amazon Nova Sonic with external data sources and domain-specific knowledge. The architecture is designed for seamless customization in several key areas. 
Modifying the system prompt 
The solution includes a UI in which you can adjust the AI agent‚Äôs behavior by modifying its system prompt. This enables rapid iteration on the agent‚Äôs personality, knowledge base, and conversation style without redeploying the entire application. 
 
Adding new tools 
You can also extend the AI agent‚Äôs capabilities by implementing additional tools using the MCP framework. The process involves: 
 
 Implementing the tool logic, typically as a new Python module 
 Registering the tool with the MCP server by using the @mcp_server.tool custom decorator and defining the tool specification, including its name, description, and input schema in /backend/tools/mcp_tool_registry.py 
 
For example, the following code illustrates how to add a knowledge base lookup tool: 
 
 @mcp_server.tool(
    name="lookup",
    description="Runs query against a knowledge base to retrieve information."
)
async def lookup_tool(
    query: Annotated[str, Field(description="the query to search")]
) -&gt; dict:
    """Look up information in the knowledge base"""
    results = knowledge_base_lookup.main(query)
    return results 
 
The decorator handles registration with the MCP server, and the function body contains your tool‚Äôs implementation logic. 
Expanding the knowledge base 
The solution uses Amazon Bedrock Knowledge Bases to provide the AI agent with company-specific information. You can update this knowledge base with: 
 
 Frequently asked questions and their answers 
 Product catalogs and specifications 
 Company policies and procedures 
 
Clean up 
You can remove the stacks with the following command: 
 
 # move to the cdk folder, assuming you are in the project root folder
cd cdk
# Removes both stacks sequentially
npx cdk destroy --all 
 
Conclusion 
AI agents are transforming how organizations approach customer service, with solutions offering the ability to handle multiple conversations simultaneously, provide consistent service around the clock, and scale instantly while maintaining quality and reducing operational costs. This solution makes those benefits accessible by providing a deployable foundation for Amazon Nova Sonic applications on AWS. The solution demonstrates how AI agents can effectively handle customer inquiries, access real-time data, and provide personalized service‚Äîall while maintaining the natural conversational flow that customers expect. 
By combining the Amazon Nova Sonic model with a robust cloud architecture, secure authentication, and flexible tool integration, organizations can quickly move from concept to proof of concept. This solution is not just helping build voice AI applications, it‚Äôs helping companies drive better customer satisfaction and productivity across a range of industries. 
To learn more, refer to the following resources: 
 
 Introducing Amazon Nova Sonic: Human-like voice conversations for generative AI applications 
 Using the Amazon Nova Sonic Speech-to-Speech model 
 Amazon Nova Sonic Workshop 
 
 
 
About the authors 
Reilly Manton is a Solutions Architect in AWS Telecoms Prototyping. He combines visionary thinking and technical expertise to build innovative solutions. Focusing on generative AI and machine learning, he empowers telco customers to enhance their technological capabilities. 
Shuto Araki is a Software Development Engineer at AWS. He works with customers in telecom industry focusing on AI security and networks. Outside of work, he enjoys cycling throughout the Netherlands. 
Ratan Kumar is a Principal Solutions Architect at Amazon Web Services.A trusted technology advisor with over 20 years of experience working across a range of industry domains, Ratan‚Äôs passion lies in empowering enterprise customers innovate and transform their business by unlocking the potential of AWS cloud. 
Chad Hendren is a Principal Solutions Architect at Amazon Web Services. His passion is AI/ML and Generative AI applied to Customer Experience. He is a published author and inventor with 30 years of telecommunications experience.
‚Ä¢ Manage multi-tenant Amazon Bedrock costs using application inference profiles
  Successful generative AI software as a service (SaaS) systems require a balance between service scalability and cost management. This becomes critical when building a multi-tenant generative AI service designed to serve a large, diverse customer base while maintaining rigorous cost controls and comprehensive usage monitoring. 
Traditional cost management approaches for such systems often reveal limitations. Operations teams encounter challenges in accurately attributing costs across individual tenants, particularly when usage patterns demonstrate extreme variability. Enterprise clients might have different consumption behaviors‚Äîsome experiencing sudden usage spikes during peak periods, whereas others maintain consistent resource consumption patterns. 
A robust solution requires a context-driven, multi-tiered alerting system that exceeds conventional monitoring standards. By implementing graduated alert levels‚Äîfrom green (normal operations) to red (critical interventions)‚Äîsystems can develop intelligent, automated responses that dynamically adapt to evolving usage patterns. This approach enables proactive resource management, precise cost allocation, and rapid, targeted interventions that help prevent potential financial overruns. 
The breaking point often comes when you experience significant cost overruns. These overruns aren‚Äôt due to a single factor but rather a combination of multiple enterprise tenants increasing their usage while your monitoring systems fail to catch the trend early enough. Your existing alerting system might only provide binary notifications‚Äîeither everything is fine or there‚Äôs a problem‚Äîthat lack the nuanced, multi-level approach needed for proactive cost management. The situation is further complicated by a tiered pricing model, where different customers have varying SLA commitments and usage quotas. Without a sophisticated alerting system that can differentiate between normal usage spikes and genuine problems, your operations team might find itself constantly taking reactive measures rather than proactive ones. 
This post explores how to implement a robust monitoring solution for multi-tenant AI deployments using a feature of Amazon Bedrock called application inference profiles. We demonstrate how to create a system that enables granular usage tracking, accurate cost allocation, and dynamic resource management across complex multi-tenant environments. 
What are application inference profiles? 
Application inference profiles in Amazon Bedrock enable granular cost tracking across your deployments. You can associate metadata with each inference request, creating a logical separation between different applications, teams, or customers accessing your foundation models (FMs). By implementing a consistent tagging strategy with application inference profiles, you can systematically track which tenant is responsible for each API call and the corresponding consumption. 
For example, you can define key-value pair tags such as TenantID, business-unit, or ApplicationID and send these tags with each request to partition your usage data. You can also send the application inference profile ID with your request. When combined with AWS resource tagging, these tag-enabled profiles provide visibility into the utilization of Amazon Bedrock models. This tagging approach introduces accurate chargeback mechanisms to help you allocate costs proportionally based on actual usage rather than arbitrary distribution approaches. To attach tags to the inference profile, see Tagging Amazon Bedrock resources and Organizing and tracking costs using AWS cost allocation tags. Furthermore, you can use application inference profiles to identify optimization opportunities specific to each tenant, helping you implement targeted improvements for the greatest impact to both performance and cost-efficiency. 
Solution overview 
Imagine a scenario where an organization has multiple tenants, each with their respective generative AI applications using Amazon Bedrock models. To demonstrate multi-tenant cost management, we provide a sample, ready-to-deploy solution&nbsp;on GitHub. It deploys two tenants with two applications, each within a single AWS Region. The solution uses application inference profiles for cost tracking, Amazon Simple Notification Service (Amazon SNS) for notifications, and Amazon CloudWatch to produce tenant-specific dashboards. You can modify the source code of the solution to suit your needs. 
The following diagram illustrates the solution architecture. 
 
The solution handles the complexities of collecting and aggregating usage data across tenants, storing historical metrics for trend analysis, and presenting actionable insights through intuitive dashboards. This solution provides the visibility and control needed to manage your Amazon Bedrock costs while maintaining the flexibility to customize components to match your specific organizational requirements. 
In the following sections, we walk through the steps to deploy the solution. 
Prerequisites 
Before setting up the project, you must have the following prerequisites: 
 
 AWS account ‚Äì An active AWS account with permissions to create and manage resources such as Lambda functions, API Gateway endpoints, CloudWatch dashboards, and SNS alerts 
 Python environment ‚Äì Python 3.12 or higher installed on your local machine 
 Virtual environment ‚Äì It‚Äôs recommended to use a virtual environment to manage project dependencies 
 
Create the virtual environment 
The first step is to clone the GitHub repo or copy the code into a new project to create the virtual environment. 
 
Update models.json 
Review and update the models.json file to reflect the correct input and output token pricing based on your organization‚Äôs contract, or use the default settings. Verifying you have the right data at this stage is critical for accurate cost tracking. 
 
Update config.json 
Modify config.json to define the profiles you want to set up for cost tracking. Each profile can have multiple key-value pairs for tags. For every profile, each tag key must be unique, and each tag key can have only one value. Each incoming request should contain these tags or the profile name as HTTP headers at runtime. 
As part of the solution, you also configure a unique Amazon Simple Storage Service (Amazon S3) bucket for saving configuration artifacts and an admin email alias that will receive alerts when a particular threshold is breached. 
 
Create user roles and deploy solution resources 
After you modify config.json and models.json, run the following command in the terminal to create the assets, including the user roles: 
python setup.py --create-user-roles 
Alternately, you can create the assets without creating user roles by running the following command: 
python setup.py 
Make sure that you are executing this command from the project directory. Note that full access policies are not advised for production use cases. 
The setup command triggers the process of creating the inference profiles, building a CloudWatch dashboard to capture the metrics for each profile, deploying the inference Lambda function that executes the Amazon Bedrock Converse API and extracts the inference metadata and metrics related to the inference profile, sets up the SNS alerts, and finally creates the API Gateway endpoint to invoke the Lambda function. 
 
When the setup is complete, you will see the inference profile IDs and API Gateway ID listed in the config.json file. (The API Gateway ID will also be listed in the final part of the output in the terminal) 
 
When the API is live and inferences are invoked from it, the CloudWatch dashboard will show cost tracking. If you experience significant traffic, the alarms will trigger an SNS alert email. 
 
For a video version of this walkthrough, refer to Track, Allocate, and Manage your Generative AI cost &amp; usage with Amazon Bedrock. 
You are now ready to use Amazon Bedrock models with this cost management solution. Make sure that you are using the API Gateway endpoint to consume these models and send the requests with the tags or application inference profile IDs as headers, which you provided in the config.json file. This solution will automatically log the invocations and track costs for your application on a per-tenant basis. 
Alarms and dashboards 
The solution creates the following alarms and dashboards: 
 
 BedrockTokenCostAlarm-{profile_name} ‚Äì Alert when total token cost for {profile_name} exceeds {cost_threshold} in 5 minutes 
 BedrockTokensPerMinuteAlarm-{profile_name} ‚Äì Alert when tokens per minute for {profile_name} exceed {tokens_per_min_threshold} 
 BedrockRequestsPerMinuteAlarm-{profile_name} ‚Äì Alert when requests per minute for {profile_name} exceed {requests_per_min_threshold} 
 
You can monitor and receive alerts about your AWS resources and applications across multiple Regions. 
A metric alarm has the following possible states: 
 
 OK ‚Äì The metric or expression is within the defined threshold 
 ALARM ‚Äì The metric or expression is outside of the defined threshold 
 INSUFFICIENT_DATA ‚Äì The alarm has just started, the metric is not available, or not enough data is available for the metric to determine the alarm state 
 
After you add an alarm to a dashboard, the alarm turns gray when it‚Äôs in the INSUFFICIENT_DATA state and red when it‚Äôs in the ALARM state. The alarm is shown with no color when it‚Äôs in the OK state. 
An alarm invokes actions only when the alarm changes state from OK to ALARM. In this solution, an email is sent to through your SNS subscription to an admin as specified in your config.json file. You can specify additional actions when the alarm changes state between OK, ALARM, and INSUFFICIENT_DATA. 
Considerations 
Although the API Gateway maximum integration timeout (30 seconds) is lower than the Lambda timeout (15 minutes), long-running model inference calls might be cut off by API Gateway. Lambda and Amazon Bedrock enforce strict payload and token size limits, so make sure your requests and responses fit within these boundaries. For example, the maximum payload size is 6 MB for synchronous Lambda invocations and the combined request line and header values can‚Äôt exceed 10,240 bytes for API Gateway payloads. If your workload can work within these limits, you will be able to use this solution. 
Clean up 
To delete your assets, run the following command: 
python unsetup.py 
Conclusion 
In this post, we demonstrated how to implement effective cost monitoring for multi-tenant Amazon Bedrock deployments using application inference profiles, CloudWatch metrics, and custom CloudWatch dashboards. With this solution, you can track model usage, allocate costs accurately, and optimize resource consumption across different tenants. You can customize the solution according to your organization‚Äôs specific needs. 
This solution provides the framework for building an intelligent system that can understand context‚Äîdistinguishing between a gradual increase in usage that might indicate healthy business growth and sudden spikes that could signal potential issues. An effective alerting system needs to be sophisticated enough to consider historical patterns, time of day, and customer tier when determining alert levels. Furthermore, these alerts can trigger different types of automated responses based on the alert level: from simple notifications, to automatic customer communications, to immediate rate-limiting actions. 
Try out the solution for your own use case, and share your feedback and questions in the comments. 
 
About the authors 
Claudio Mazzoni is a Sr Specialist Solutions Architect on the Amazon Bedrock GTM team. Claudio exceeds at guiding costumers through their Gen AI journey. Outside of work, Claudio enjoys spending time with family, working in his garden, and cooking Uruguayan food. 
Fahad Ahmed is a Senior Solutions Architect at AWS and assists financial services customers. He has over 17 years of experience building and designing software applications. He recently found a new passion of making AI services accessible to the masses. 
 
Manish Yeladandi&nbsp;is a Solutions Architect at AWS, specializing in AI/ML, containers, and security. Combining deep cloud expertise with business acumen, Manish architects secure, scalable solutions that help organizations optimize their technology investments and achieve transformative business outcomes. 
Dhawal Patel is a Principal Machine Learning Architect at AWS. He has worked with organizations ranging from large enterprises to mid-sized startups on problems related to distributed computing and artificial intelligence. He focuses on deep learning, including NLP and computer vision domains. He helps customers achieve high-performance model inference on Amazon SageMaker. 
James Park&nbsp;is a Solutions Architect at Amazon Web Services. He works with Amazon.com to design, build, and deploy technology solutions on AWS, and has a particular interest in AI and machine learning. In h is spare time he enjoys seeking out new cultures, new experiences, &nbsp;and staying up to date with the latest technology trends. You can find him on LinkedIn. 
Abhi Shivaditya&nbsp;is a Senior Solutions Architect at AWS, working with strategic global enterprise organizations to facilitate the adoption of AWS services in areas such as Artificial Intelligence, distributed computing, networking, and storage. His expertise lies in Deep Learning in the domains of Natural Language Processing (NLP) and Computer Vision. Abhi assists customers in deploying high-performance machine learning models efficiently within the AWS ecosystem.
‚Ä¢ Evaluating generative AI models with Amazon Nova LLM-as-a-Judge on Amazon SageMaker AI
  Evaluating the performance of large language models (LLMs) goes beyond statistical metrics like perplexity or bilingual evaluation understudy (BLEU) scores. For most real-world generative AI scenarios, it‚Äôs crucial to understand whether a model is producing better outputs than a baseline or an earlier iteration. This is especially important for applications such as summarization, content generation, or intelligent agents where subjective judgments and nuanced correctness play a central role. 
As organizations deepen their deployment of these models in production, we‚Äôre experiencing an increasing demand from customers who want to systematically assess model quality beyond traditional evaluation methods. Current approaches like accuracy measurements and rule-based evaluations, although helpful, can‚Äôt fully address these nuanced assessment needs, particularly when tasks require subjective judgments, contextual understanding, or alignment with specific business requirements. To bridge this gap, LLM-as-a-judge has emerged as a promising approach, using the reasoning capabilities of LLMs to evaluate other models more flexibly and at scale. 
Today, we‚Äôre excited to introduce a comprehensive approach to model evaluation through the Amazon Nova LLM-as-a-Judge capability on Amazon SageMaker AI, a fully managed Amazon Web Services (AWS) service to build, train, and deploy machine learning (ML) models at scale. Amazon Nova LLM-as-a-Judge is designed to deliver robust, unbiased assessments of generative AI outputs across model families. Nova LLM-as-a-Judge is available as optimized workflows on SageMaker AI, and with it, you can start evaluating model performance against your specific use cases in minutes. Unlike many evaluators that exhibit architectural bias, Nova LLM-as-a-Judge has been rigorously validated to remain impartial and has achieved leading performance on key judge benchmarks while closely reflecting human preferences. With its exceptional accuracy and minimal bias, it sets a new standard for credible, production-grade LLM evaluation. 
Nova LLM-as-a-Judge capability provides pairwise comparisons between model iterations, so you can make data-driven decisions about model improvements with confidence. 
How Nova LLM-as-a-Judge was trained 
Nova LLM-as-a-Judge was built through a multistep training process comprising supervised training and reinforcement learning stages that used public datasets annotated with human preferences. For the proprietary component, multiple annotators independently evaluated thousands of examples by comparing pairs of different LLM responses to the same prompt. To verify consistency and fairness, all annotations underwent rigorous quality checks, with final judgments calibrated to reflect broad human consensus rather than an individual viewpoint. 
The training data was designed to be both diverse and representative. Prompts spanned a wide range of categories, including real-world knowledge, creativity, coding, mathematics, specialized domains, and toxicity, so the model could evaluate outputs across many real-world scenarios. Training data included data from over 90 languages and is primarily composed of English, Russian, Chinese, German, Japanese, and Italian.Importantly, an internal bias study evaluating over 10,000 human-preference judgments against 75 third-party models confirmed that Amazon Nova LLM-as-a-Judge shows only a 3% aggregate bias relative to human annotations. Although this is a significant achievement in reducing systematic bias, we still recommend occasional spot checks to validate critical comparisons. 
In the following figure, you can see how the Nova LLM-as-a-Judge bias compares to human preferences when evaluating Amazon Nova outputs compared to outputs from other models. Here, bias is measured as the difference between the judge‚Äôs preference and human preference across thousands of examples. A positive value indicates the judge slightly favors Amazon Nova models, and a negative value indicates the opposite. To quantify the reliability of these estimates, 95% confidence intervals were computed using the standard error for the difference of proportions, assuming independent binomial distributions. 
 
Amazon Nova LLM-as-a-Judge achieves advanced performance among evaluation models, demonstrating strong alignment with human judgments across a range of tasks. For example, it scores 45% accuracy on JudgeBench (compared to 42% for Meta J1 8B) and 68% on PPE (versus 60% for Meta J1 8B). The data from Meta‚Äôs J1 8B was pulled from Incentivizing Thinking in LLM-as-a-Judge via Reinforcement Learning. 
These results highlight the strength of Amazon Nova LLM-as-a-Judge in chatbot-related evaluations, as shown in the PPE benchmark. Our benchmarking follows current best practices, reporting reconciled results for positionally swapped responses on JudgeBench, CodeUltraFeedback, Eval Bias, and LLMBar, while using single-pass results for PPE. 
 
  
   
   Model 
   Eval Bias 
   Judge Bench 
   LLM Bar 
   PPE 
   CodeUltraFeedback 
   
   
   Nova LLM-as-a-Judge 
   0.76 
   0.45 
   0.67 
   0.68 
   0.64 
   
   
   Meta J1 8B 
   ‚Äì 
   0.42 
   ‚Äì 
   0.60 
   ‚Äì 
   
   
   Nova Micro 
   0.56 
   0.37 
   0.55 
   0.6 
   ‚Äì 
   
  
 
In this post, we present a streamlined approach to implementing Amazon Nova LLM-as-a-Judge evaluations using SageMaker AI, interpreting the resulting metrics, and applying this process to improve your generative AI applications. 
Overview of the evaluation workflow 
The evaluation process starts by preparing a dataset in which each example includes a prompt and two alternative model outputs. The JSONL format looks like this: 
 
 {
   "prompt":"Explain photosynthesis.",
   "response_A":"Answer A...",
   "response_B":"Answer B..."
}
{
   "prompt":"Summarize the article.",
   "response_A":"Answer A...",
   "response_B":"Answer B..."
} 
 
After preparing this dataset, you use the given SageMaker evaluation recipe, which configures the evaluation strategy, specifies which model to use as the judge, and defines the inference settings such as temperature and top_p. 
The evaluation runs inside a SageMaker training job using pre-built Amazon Nova containers. SageMaker AI provisions compute resources, orchestrates the evaluation, and writes the output metrics and visualizations to Amazon Simple Storage Service (Amazon S3). 
When it‚Äôs complete, you can download and analyze the results, which include preference distributions, win rates, and confidence intervals. 
Understanding how Amazon Nova LLM-as-a-Judge works 
The Amazon Nova LLM-as-a-Judge uses an evaluation method called binary overall preference judge. The binary overall preference judge is a method where a language model compares two outputs side by side and picks the better one or declares a tie. For each example, it produces a clear preference. When you aggregate these judgments over many samples, you get metrics like win rate and confidence intervals. This approach uses the model‚Äôs own reasoning to assess qualities like relevance and clarity in a straightforward, consistent way. 
 
 This judge model is meant to provide low-latency general overall preferences in situations where granular feedback isn‚Äôt necessary 
 The output of this model is one of&nbsp;[[A&gt;B]]&nbsp;or&nbsp;[[B&gt;A]] 
 Use cases for this model are primarily those where automated, low-latency, general pairwise preferences are required, such as automated scoring for checkpoint selection in training pipelines 
 
Understanding Amazon Nova LLM-as-a-Judge evaluation metrics 
When using the Amazon Nova LLM-as-a-Judge framework to compare outputs from two language models, SageMaker AI produces a comprehensive set of quantitative metrics. You can use these metrics to assess which model performs better and how reliable the evaluation is. The results fall into three main categories: core preference metrics, statistical confidence metrics, and standard error metrics. 
The core preference metrics report how often each model‚Äôs outputs were preferred by the judge model. The a_scores metric counts the number of examples where Model A was favored, and b_scores counts cases where Model B was chosen as better. The ties metric captures instances in which the judge model rated both responses equally or couldn‚Äôt identify a clear preference. The inference_error metric counts cases where the judge couldn‚Äôt generate a valid judgment due to malformed data or internal errors. 
The statistical confidence metrics quantify how likely it is that the observed preferences reflect true differences in model quality rather than random variation. The winrate reports the proportion of all valid comparisons in which Model B was preferred. The lower_rate and upper_rate define the lower and upper bounds of the 95% confidence interval for this win rate. For example, a winrate of 0.75 with a confidence interval between 0.60 and 0.85 suggests that, even accounting for uncertainty, Model B is consistently favored over Model A. The score field often matches the count of Model B wins but can also be customized for more complex evaluation strategies. 
The standard error metrics provide an estimate of the statistical uncertainty in each count. These include a_scores_stderr, b_scores_stderr, ties_stderr, inference_error_stderr, andscore_stderr. Smaller standard error values indicate more reliable results. Larger values can point to a need for additional evaluation data or more consistent prompt engineering. 
Interpreting these metrics requires attention to both the observed preferences and the confidence intervals: 
 
 If the winrate is substantially above 0.5 and the confidence interval doesn‚Äôt include 0.5, Model B is statistically favored over Model A. 
 Conversely, if the winrate is below 0.5 and the confidence interval is fully below 0.5, Model A is preferred. 
 When the confidence interval overlaps 0.5, the results are inconclusive and further evaluation is recommended. 
 High values in inference_error or large standard errors suggest there might have been issues in the evaluation process, such as inconsistencies in prompt formatting or insufficient sample size. 
 
The following is an example metrics output from an evaluation run: 
 
 {
  "a_scores": 16.0,
  "a_scores_stderr": 0.03,
  "b_scores": 10.0,
  "b_scores_stderr": 0.09,
  "ties": 0.0,
  "ties_stderr": 0.0,
  "inference_error": 0.0,
  "inference_error_stderr": 0.0,
  "score": 10.0,
  "score_stderr": 0.09,
  "winrate": 0.38,
  "lower_rate": 0.23,
  "upper_rate": 0.56
} 
 
In this example, Model A was preferred 16 times, Model B was preferred 10 times, and there were no ties or inference errors. The winrate of 0.38 indicates that Model B was preferred in 38% of cases, with a 95% confidence interval ranging from 23% to 56%. Because the interval includes 0.5, this outcome suggests the evaluation was inconclusive, and additional data might be needed to clarify which model performs better overall. 
These metrics, automatically generated as part of the evaluation process, provide a rigorous statistical foundation for comparing models and making data-driven decisions about which one to deploy. 
Solution overview 
This solution demonstrates how to evaluate generative AI models on Amazon SageMaker AI using the Nova LLM-as-a-Judge capability. The provided Python code guides you through the entire workflow. 
First, it prepares a dataset by sampling questions from SQuAD and generating candidate responses from Qwen2.5 and Anthropic‚Äôs Claude 3.7. These outputs are saved in a JSONL file containing the prompt and both responses. 
We accessed Anthropic‚Äôs Claude 3.7 Sonnet in Amazon Bedrock using the bedrock-runtime client. We accessed Qwen2.5 1.5B using a SageMaker hosted Hugging Face endpoint. 
Next, a PyTorch Estimator launches an evaluation job using an Amazon Nova LLM-as-a-Judge recipe. The job runs on GPU instances such as ml.g5.12xlarge and produces evaluation metrics, including win rates, confidence intervals, and preference counts. Results are saved to Amazon S3 for analysis. 
Finally, a visualization function renders charts and tables, summarizing which model was preferred, how strong the preference was, and how reliable the estimates are. Through this end-to-end approach, you can assess improvements, track regressions, and make data-driven decisions about deploying generative models‚Äîall without manual annotation. 
Prerequisites 
You need to complete the following prerequisites before you can run the notebook: 
 
 Make the following quota increase requests for SageMaker AI. For this use case, you need to request a minimum of 1 g5.12xlarge instance. On the Service Quotas console, request the following SageMaker AI quotas, 1 G5 instances (g5.12xlarge) for training job usage 
 (Optional) You can create an Amazon SageMaker Studio domain (refer to Use quick setup for Amazon SageMaker AI) to access Jupyter notebooks with the preceding role. (You can use JupyterLab in your local setup, too.) 
   
   Create an AWS Identity and Access Management (IAM) role with managed policies AmazonSageMakerFullAccess, AmazonS3FullAccess,&nbsp;and AmazonBedrockFullAccess to give required access to SageMaker AI and Amazon Bedrock to run the examples. 
   Assign as trust relationship to your IAM role the following policy: 
    
 
 
 {
&nbsp;&nbsp; &nbsp;"Version": "2012-10-17",
&nbsp;&nbsp; &nbsp;"Statement": [
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;{
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"Sid": "",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"Effect": "Allow",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"Principal": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"Service": [
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"bedrock.amazonaws.com",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"sagemaker.amazonaws.com"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;]
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;},
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"Action": "sts:AssumeRole"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp;]
} 
 
 
 Clone the GitHub repository with the assets for this deployment. This repository consists of a notebook that references training assets: 
 
 
 git clone https://github.com/aws-samples/amazon-nova-samples.git
cd customization/SageMakerTrainingJobs/Amazon-Nova-LLM-As-A-Judge/ 
 
Next, run the notebook Nova Amazon-Nova-LLM-as-a-Judge-Sagemaker-AI.ipynb to start using the Amazon Nova LLM-as-a-Judge implementation on Amazon SageMaker AI. 
Model setup 
To conduct an Amazon Nova LLM-as-a-Judge evaluation, you need to generate outputs from the candidate models you want to compare. In this project, we used two different approaches: deploying a Qwen2.5 1.5B model on Amazon SageMaker and invoking Anthropic‚Äôs Claude 3.7 Sonnet model in Amazon Bedrock. First, we deployed Qwen2.5 1.5B, an open-weight multilingual language model, on a dedicated SageMaker endpoint. This was achieved by using the HuggingFaceModel deployment interface. To deploy the Qwen2.5 1.5B model, we provided a convenient script for you to invoke:python3 deploy_sm_model.py 
When it‚Äôs deployed, inference can be performed using a helper function wrapping the SageMaker predictor API: 
 
 # Initialize the predictor once
predictor = HuggingFacePredictor(endpoint_name="qwen25-&lt;endpoint_name_here&gt;")
def generate_with_qwen25(prompt: str, max_tokens: int = 500, temperature: float = 0.9) -&gt; str:
    """
    Sends a prompt to the deployed Qwen2.5 model on SageMaker and returns the generated response.
    Args:
        prompt (str): The input prompt/question to send to the model.
        max_tokens (int): Maximum number of tokens to generate.
        temperature (float): Sampling temperature for generation.
    Returns:
        str: The model-generated text.
    """
    response = predictor.predict({
        "inputs": prompt,
        "parameters": {
            "max_new_tokens": max_tokens,
            "temperature": temperature
        }
    })
    return response[0]["generated_text"]
answer = generate_with_qwen25("What is the Grotto at Notre Dame?")
print(answer) 
 
In parallel, we integrated Anthropic‚Äôs Claude 3.7 Sonnet model in Amazon Bedrock. Amazon Bedrock provides a managed API layer for accessing proprietary foundation models (FMs) without managing infrastructure. The Claude generation function used the bedrock-runtime AWS SDK for Python (Boto3) client, which accepted a user prompt and returned the model‚Äôs text completion: 
 
 # Initialize Bedrock client once
bedrock = boto3.client("bedrock-runtime", region_name="us-east-1")
# (Claude 3.7 Sonnet) model ID via Bedrock
MODEL_ID = "us.anthropic.claude-3-7-sonnet-20250219-v1:0"
def generate_with_claude4(prompt: str, max_tokens: int = 512, temperature: float = 0.7, top_p: float = 0.9) -&gt; str:
    """
    Sends a prompt to the Claude 4-tier model via Amazon Bedrock and returns the generated response.
    Args:
        prompt (str): The user message or input prompt.
        max_tokens (int): Maximum number of tokens to generate.
        temperature (float): Sampling temperature for generation.
        top_p (float): Top-p nucleus sampling.
    Returns:
        str: The text content generated by Claude.
    """
    payload = {
        "anthropic_version": "bedrock-2023-05-31",
        "messages": [{"role": "user", "content": prompt}],
        "max_tokens": max_tokens,
        "temperature": temperature,
        "top_p": top_p
    }
    response = bedrock.invoke_model(
        modelId=MODEL_ID,
        body=json.dumps(payload),
        contentType="application/json",
        accept="application/json"
    )
    response_body = json.loads(response['body'].read())
    return response_body["content"][0]["text"]
answer = generate_with_claude4("What is the Grotto at Notre Dame?")
print(answer) 
 
When you have both functions generated and tested, you can move on to creating the evaluation data for the Nova LLM-as-a-Judge. 
Prepare the dataset 
To create a realistic evaluation dataset for comparing the Qwen and Claude models, we used the Stanford Question Answering Dataset (SQuAD), a widely adopted benchmark in natural language understanding distributed under the CC BY-SA 4.0 license. SQuAD consists of thousands of crowd-sourced question-answer pairs covering a diverse range of Wikipedia articles. By sampling from this dataset, we made sure that our evaluation prompts reflected high-quality, factual question-answering tasks representative of real-world applications. 
We began by loading a small subset of examples to keep the workflow fast and reproducible. Specifically, we used the Hugging Face datasets library to download and load the first 20 examples from the SQuAD training split: 
 
 from datasets import load_dataset
squad = load_dataset("squad", split="train[:20]") 
 
This command retrieves a slice of the full dataset, containing 20 entries with structured fields including context, question, and answers. To verify the contents and inspect an example, we printed out a sample question and its ground truth answer: 
 
 print(squad[3]["question"])
print(squad[3]["answers"]["text"][0]) 
 
For the evaluation set, we selected the first six questions from this subset: 
questions = [squad[i]["question"] for i in range(6)] 
Generate the Amazon Nova LLM-as-a-Judge evaluation dataset 
After preparing a set of evaluation questions from SQuAD, we generated outputs from both models and assembled them into a structured dataset to be used by the Amazon Nova LLM-as-a-Judge workflow. This dataset serves as the core input for SageMaker AI evaluation recipes. To do this, we iterated over each question prompt and invoked the two generation functions defined earlier: 
 
 generate_with_qwen25() for completions from the Qwen2.5 model deployed on SageMaker 
 generate_with_claude() for completions from Anthropic‚Äôs Claude 3.7 Sonnet in Amazon Bedrock 
 
For each prompt, the workflow attempted to generate a response from each model. If a generation call failed due to an API error, timeout, or other issue, the system captured the exception and stored a clear error message indicating the failure. This made sure that the evaluation process could proceed gracefully even in the presence of transient errors: 
 
 import json
output_path = "llm_judge.jsonl"
with open(output_path, "w") as f:
    for q in questions:
        try:
            response_a = generate_with_qwen25(q)
        except Exception as e:
            response_a = f"[Qwen2.5 generation failed: {e}]"
        
        try:
            response_b = generate_with_claude4(q)
        except Exception as e:
            response_b = f"[Claude 3.7 generation failed: {e}]"
        row = {
            "prompt": q,
            "response_A": response_a,
            "response_B": response_b
        }
        f.write(json.dumps(row) + "\n")
print(f"JSONL file created at: {output_path}") 
 
This workflow produced a JSON Lines file named llm_judge.jsonl. Each line contains a single evaluation record structured as follows: 
 
 {
  "prompt": "What is the capital of France?",
  "response_A": "The capital of France is Paris.",
  "response_B": "Paris is the capital city of France."
} 
 
Then, upload this llm_judge.jsonl to an S3 bucket that you‚Äôve predefined: 
 
 upload_to_s3(
    "llm_judge.jsonl",
    "s3://&lt;YOUR_BUCKET_NAME&gt;/datasets/byo-datasets-dev/custom-llm-judge/llm_judge.jsonl"
) 
 
Launching the Nova LLM-as-a-Judge evaluation job 
After preparing the dataset and creating the evaluation recipe, the final step is to launch the SageMaker training job that performs the Amazon Nova LLM-as-a-Judge evaluation. In this workflow, the training job acts as a fully managed, self-contained process that loads the model, processes the dataset, and generates evaluation metrics in your designated Amazon S3 location. 
We use the PyTorch estimator class from the SageMaker Python SDK to encapsulate the configuration for the evaluation run. The estimator defines the compute resources, the container image, the evaluation recipe, and the output paths for storing results: 
 
 estimator = PyTorch(
    output_path=output_s3_uri,
    base_job_name=job_name,
    role=role,
    instance_type=instance_type,
    training_recipe=recipe_path,
    sagemaker_session=sagemaker_session,
    image_uri=image_uri,
    disable_profiler=True,
    debugger_hook_config=False,
) 
 
When the estimator is configured, you initiate the evaluation job using the fit() method. This call submits the job to the SageMaker control plane, provisions the compute cluster, and begins processing the evaluation dataset: 
estimator.fit(inputs={"train": evalInput}) 
Results from the Amazon Nova LLM-as-a-Judge evaluation job 
The following graphic illustrates the results of the Amazon Nova LLM-as-a-Judge evaluation job. 
 
To help practitioners quickly interpret the outcome of a Nova LLM-as-a-Judge evaluation, we created a convenience function that produces a single, comprehensive visualization summarizing key metrics. This function, plot_nova_judge_results, uses Matplotlib and Seaborn to render an image with six panels, each highlighting a different perspective of the evaluation outcome. 
This function takes the evaluation metrics dictionary‚Äîproduced when the evaluation job is complete‚Äîand generates the following visual components: 
 
 Score distribution bar chart ‚Äì Shows how many times Model A was preferred, how many times Model B was preferred, how many ties occurred, and how often the judge failed to produce a decision (inference errors). This provides an immediate sense of how decisive the evaluation was and whether either model is dominating. 
 Win rate with 95% confidence interval ‚Äì Plots Model B‚Äôs overall win rate against Model A, including an error bar reflecting the lower and upper bounds of the 95% confidence interval. A vertical reference line at 50% marks the point of no preference. If the confidence interval doesn‚Äôt cross this line, you can conclude the result is statistically significant. 
 Preference pie chart ‚Äì Visually displays the proportion of times Model A, Model B, or neither was preferred. This helps quickly understand preference distribution among the valid judgments. 
 A vs. B score comparison bar chart ‚Äì Compares the raw counts of preferences for each model side by side. A clear label annotates the margin of difference to emphasize which model had more wins. 
 Win rate gauge ‚Äì Depicts the win rate as a semicircular gauge with a needle pointing to Model B‚Äôs performance relative to the theoretical 0‚Äì100% range. This intuitive visualization helps nontechnical stakeholders understand the win rate at a glance. 
 Summary statistics table ‚Äì Compiles numerical metrics‚Äîincluding total evaluations, error counts, win rate, and confidence intervals‚Äîinto a compact, clean table. This makes it straightforward to reference the exact numeric values behind the plots. 
 
Because the function outputs a standard Matplotlib figure, you can quickly save the image, display it in Jupyter notebooks, or embed it in other documentation. 
Clean up 
Complete the following steps to clean up your resources: 
 
 Delete your Qwen 2.5 1.5B Endpoint 
   
   import boto3

# Create a low-level SageMaker service client.

sagemaker_client = boto3.client('sagemaker', region_name=&lt;region&gt;)

# Delete endpoint

sagemaker_client.delete_endpoint(EndpointName=endpoint_name) 
    
 If you‚Äôre using a SageMaker Studio JupyterLab notebook, shut down the JupyterLab notebook instance. 
 
How you can use this evaluation framework 
The Amazon Nova LLM-as-a-Judge workflow offers a reliable, repeatable way to compare two language models on your own data. You can integrate this into model selection pipelines to decide which version performs best, or you can schedule it as part of continuous evaluation to catch regressions over time. 
For teams building agentic or domain-specific systems, this approach provides richer insight than automated metrics alone. Because the entire process runs on SageMaker training jobs, it scales quickly and produces clear visual reports that can be shared with stakeholders. 
Conclusion 
This post demonstrates how Nova LLM-as-a-Judge‚Äîa specialized evaluation model available through Amazon SageMaker AI‚Äîcan be used to systematically measure the relative performance of generative AI systems. The walkthrough shows how to prepare evaluation datasets, launch SageMaker AI training jobs with Nova LLM-as-a-Judge recipes, and interpret the resulting metrics, including win rates and preference distributions. The fully managed SageMaker AI solution simplifies this process, so you can run scalable, repeatable model evaluations that align with human preferences. 
We recommend starting your LLM evaluation journey by exploring the official Amazon Nova documentation and examples. The AWS AI/ML community offers extensive resources, including workshops and technical guidance, to support your implementation journey. 
To learn more, visit: 
 
 Amazon Nova Documentation 
 Amazon Bedrock Nova Overview 
 Fine-tuning Amazon Nova models 
 Amazon Nova customization guide 
 
 
 
About the authors 
Surya Kari is a Senior Generative AI Data Scientist at AWS, specializing in developing solutions leveraging state-of-the-art foundation models. He has extensive experience working with advanced language models including DeepSeek-R1, the Llama family, and Qwen, focusing on their fine-tuning and optimization. His expertise extends to implementing efficient training pipelines and deployment strategies using AWS SageMaker. He collaborates with customers to design and implement generative AI solutions, helping them navigate model selection, fine-tuning approaches, and deployment strategies to achieve optimal performance for their specific use cases. 
Joel Carlson is a Senior Applied Scientist on the Amazon AGI foundation modeling team. He primarily works on developing novel approaches for improving the LLM-as-a-Judge capability of the Nova family of models. 
Saurabh Sahu is an applied scientist in the Amazon AGI Foundation modeling team. He obtained his PhD in Electrical Engineering from University of Maryland College Park in 2019. He has a background in multi-modal machine learning working on speech recognition, sentiment analysis and audio/video understanding. Currently, his work focuses on developing recipes to improve the performance of LLM-as-a-judge models for various tasks. 
Morteza Ziyadi is an Applied Science Manager at Amazon AGI, where he leads several projects on post-training recipes and (Multimodal) large language models in the Amazon AGI Foundation modeling team. Before joining Amazon AGI, he spent four years at Microsoft Cloud and AI, where he led projects focused on developing natural language-to-code generation models for various products. He has also served as an adjunct faculty at Northeastern University. He earned his PhD from the University of Southern California (USC) in 2017 and has since been actively involved as a workshop organizer, and reviewer for numerous NLP, Computer Vision and machine learning conferences. 
Pradeep Natarajan is a Senior Principal Scientist in Amazon AGI Foundation modeling team working on post-training recipes and Multimodal large language models. He has 20+ years of experience in developing and launching multiple large-scale machine learning systems. He has a PhD in Computer Science from University of Southern California. 
Michael Cai is a Software Engineer on the Amazon AGI Customization Team supporting the development of evaluation solutions. He obtained his MS in Computer Science from New York University in 2024. In his spare time he enjoys 3d printing and exploring innovative tech.

‚∏ª