âœ… Morning News Briefing â€“ August 19, 2025 10:46

ğŸ“… Date: 2025-08-19 10:46
ğŸ·ï¸ Tags: #briefing #ai #publichealth #digitalgov

â¸»

ğŸ§¾ Weather
â€¢ Current Conditions:  9.7Â°C
  Temperature: 9.7&deg;C Pressure / Tendency: 102.3 kPa falling Humidity: 96 % Humidity is 96 % Dewpoint is 9.1Â°C . Wind:  calm km/h . Air Quality Health Index: n/a . Pembroke 6:00 AM EDT Tuesday 19 August 2025 . Weather:    Pem
â€¢ Tuesday: Chance of showers. High 21. POP 40%
  A mix of sun and cloud with 40 percent chance of showers late this afternoon . Wind southeast 20 km/h becoming light late this morning . High 21. UV index 6 or high; UV index of high or very high in the morning's UV index at 6 or low . Forecast issued 5:00 AM EDT Tuesday 19 August 2025. For the rest of the year, see www
â€¢ Tuesday night: Chance of showers. Low 13. POP 60%
  Mainly cloudy with 60 percent chance of showers . Low 13.50-60% chance of rain . Mainly sunny sunny day with a low of 60% rain . Forecast issued 5:00 AM EDT Tuesday 19 August 2025 . Low 20/20 weather forecast: Showers, thunderstorms, rain, snow, rain and thunderstorms. Low 40/60/60 chance of

ğŸŒ International News
No updates.

ğŸ Canadian News
No updates.

ğŸ‡ºğŸ‡¸ U.S. Top Stories
â€¢ Businesses face 'chaos' as EPA aims to repeal its authority over climate pollution
  A lot of companies want the EPA in charge of setting national climate regulations because it helps shield them from lawsuits and creates a predictable environment in which to make investments . The EPA has been criticized for being too cozy with the White House to regulate climate change . The White House says the EPA is the only federal agency to set national climate rules in the U.S. to set global climate regulations
â€¢ Gun violence hits Black communities hardest. Trump is rolling back prevention efforts
  The suffering of America's gun violence crisis is concentrated in Black neighborhoods damaged by decades of disinvestment and racial discrimination . Trump is unravelling efforts to solve the problem . Trump unravelled efforts to fix the problem by de-investigating the problem of gun violence in America's most violent neighborhoods . The problem is a problem for African Americans, especially in the South, and for
â€¢ Toxicity is a good defense, until it isn't
  Evolutionary biologist Shabnam Mohammadi talks to Regina G. Barber about how some predators can get away with eating toxic prey . She says even humans have used these toxins to their advantage since ancient Egypt . Listen to every episode of Short Wave free and support our work at NPR by signing up for Short Wave+ at plus.npr.org/shortwave. Listen to all the latest Short Wave news from iReport.com .
â€¢ D.C.'s crime numbers are all the buzz. But how do we interpret them accurately?
  Crime data has been going around to make the argument that Washington, D.C., is â€” or isn't â€” safe . We talk to crime experts to make sense of it all in Washington, DC, to see how much crime has been reported in the U.S. The city is home to a large number of crime victims, but it's not safe to rely on law enforcement
â€¢ A musical about bigotry arrives at a Kennedy Center transformed by Trump
  Parade, the Tony award-winning musical about the 1915 lynching of a Jewish man, begins its run in Washington, D.C. amid an antisemitic backlash against the show's subject . Parade is the Tony-award winning musical about a lynching in which the victim was lynched in 1915 . The show is set to open in New York City on November 14 .

ğŸ§  Artificial Intelligence
No updates.

ğŸ’» Digital Strategy
â€¢ Commodore Amiga turns 40, headlines UK exhibition
  Commodore Amiga turned 40 this year, and the event has been marked by The National Museum Of Computing in the UK with a hands-on exhibition of models from the archives . 500, 600, 1200, 2000... what's your number? Hands on Amiga hands on at the National Museum of Computing in The UK with an exhibition of the Amiga models from The Amiga archives .
â€¢ US spy chief claims UK backed down over Apple backdoor demand
  UK government reportedly abandoned its attempt to strong-arm Apple into weakening iPhone encryption . White House forced Blighty into a quiet climb-down . Tulsi Gabbard boasts Washington forced to drop iPhone encryption fight . U.S. senator claims Washington forced Britain to back down on the issue of weakening encryption in order to keep tabs on Apple's encryption system . The White House reportedly forced Bl
â€¢ More customers asking for Google's Data Boundary, says Cloud Experience boss
  Developer demand for sovereign cloud from tech giant is on the rise, says exec . Google's President of Customer Experience, Hayete Gallot, offered some words of comfort to developers who are looking nervously at the rise of AI assistants . Gallot also laid out her vision for cloud sovereignty in an interview with Google's VP of customer experience . The interview was conducted by Google's CEO of the
â€¢ Softbank bets $2 billion on Intel having a future
  SoftBank has made a $2 billion investment in Intel . Takes two percent stake as rumours swirl Uncle Sam could do something similar . Unconfirmed reports that the U.S. could also invest in Intel in the form of a similar deal . SoftBank invested $1 billion in Intel, which is expected to be worth $3 billion in the next five years . Unclear whether U.
â€¢ Browser wars are back, predicts Palo Alto, thanks to AI
  CEO says if you buy all your infosec stuff from him, life under assault from bots will be less painful . Palo Alto Networks CEO Nikesh Arora says browser wars are on the way . Arora: If you buy everything from him he'll be less likely to be under attack from bots in the next round of browser wars.â€¦â€¦â€¦ If you're buying all your

ğŸ¥ Public Health
No updates.

ğŸ”¬ Science
â€¢ The impact of objective urban features on perception of neighbourhood environments
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
â€¢ Bite-sized self-compassion: a pilot cohort study of a well-being tool for healthcare workers
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
â€¢ Trends in antidote use in France from 2015 to 2021: a nationwide poison centers study
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
â€¢ Global, regional and national trends in suicide mortality rates across 102 countries from 1990 to 2021 with projections up to 2050
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
â€¢ Modelling transmission of Middle East respiratory syndrome coronavirus in camel populations and the potential impact of animal vaccination
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations

ğŸ§¾ Government & Policy
No updates.

ğŸ›ï¸ Enterprise Architecture & IT Governance
No updates.

ğŸ¤– AI & Emerging Tech
â€¢ Apple AirPods : a gateway hearing aid
  In October 2022, the FDA approved the sale of over-the-counter hearing aids without a prescription or audiology exam . The most important features for mild hearing loss are programmability, Bluetooth functionality, and the ability to feed sound to both ears . The AirPods are not as good as my budget hearing aid that costs 10 times more, but there's incredible potential here, says author Ashley Shew .
â€¢ How churches use data and AI as engines of surveillance
  On a Sunday morning in a Midwestern megachurch, worshippers step through sliding glass doors into a bustling lobbyâ€”unaware theyâ€™ve just passed through a gauntlet of biometric surveillance. High-speed cameras snap multiple face â€œprobesâ€ per second, isolating eyes, noses, and mouths before passing the results to a local neural network that distills these images into digital fingerprints. Before people find their seats, they are matched against an on-premises databaseâ€”tagged with names, membership tiers, and watch-list flagsâ€”thatâ€™s stored behind the churchâ€™s firewall.





Late one afternoon, a woman scrolls on her phone as she walks home from work. Unbeknownst to her, a complex algorithm has stitched together her social profiles, her private health records, and local veteran outreach lists. It flags her for past military service, chronic pain, opioid dependence, and high Christian belief, and then delivers an ad to her Facebook feed: â€œStruggling with pain? Youâ€™re not alone. Join us this Sunday.â€



These hypothetical scenes reflect real capabilities increasingly woven into places of worship nationwide, where spiritual care and surveillance converge in ways few congregants ever realize. Where Big Techâ€™s rationalist ethos and evangelical spirituality once mixed like oil and holy water, this unlikely amalgam has given birth to an infrastructure already reshaping the theology of trustâ€”and redrawing the contours of community and pastoral power in modern spiritual life.



An ecumenical tech ecosystem




The emerging nerve center of this faith-tech nexus is in Boulder, Colorado, where the spiritual data and analytics firm Gloo has its headquarters.



Gloo captures congregants across thousands of data points that make up a far richer portrait than any snapshot. From there, the company is constructing a digital infrastructure meant to bring churches into the age of algorithmic insight.




The church is â€œa highly fragmented market that is one of the largest yet to fully adopt digital technology,â€ the company said in a statement by email. â€œWhile churches have a variety of goals to achieve their mission, they use Gloo to help them connect, engage with, and know their people on a deeper level.â€&nbsp;







Gloo was founded in 2013 by Scott and Theresa Beck. From the late 1980s through the 2000s, Scott was turning Blockbuster into a 3,500-store chain, taking Boston Market public, and founding Einstein Bros. Bagels before going on to seed and guide startups like Ancestry.com and HomeAdvisor. Theresa, an artist, has built a reputation creating collaborative, eco-minded workshops across Colorado and beyond. Together, they have recast pastoral care as a problem of predictive analytics and sold thousands of churches on the idea that spiritual health can be managed like customer engagement.



Think of Gloo as something like Salesforce but for churches: a behavioral analytics platform, powered by church-Â­generated insights, psychographic information, and third-party consumer data. The company prefers to refer to itself as â€œa technology platform for the faith ecosystem.â€ Either way, this information is integrated into its â€œState of Your Churchâ€ dashboardâ€”an interface for the modern pulpit. The result is a kind of digital clairvoyance: a crystal ball for knowing whom to check on, whom to comfort, and when to act.




Thousands of churches have been sold on the idea that spiritual health can be managed like customer engagement.




Gloo ingests every one of the digital breadcrumbs a congregant leavesâ€”how often you attend church, how much money you donate, which church groups you sign up for, which keywords you use in your online prayer requestsâ€”and then layers on third-party data (census demographics, consumer habits, even indicators for credit and health risks). Behind the scenes, it scores and segments people and groupsâ€”flagging who is most at risk of drifting, primed for donation appeals, or in need of pastoral care. On that basis, it auto-triggers tailored outreach via text, email, or in-app chat. All the results stream into the single dashboard, which lets pastors spot trends, test messaging, and forecast giving and attendance. Essentially, the system treats spiritual engagement like a marketing funnel.



Since its launch in 2013, Gloo has steadily increased its footprint, and it has started to become the connective tissue for the countryâ€™s fragmented religious landscape. According to the Hartford Institute for Religion Research, the US is home to around 370,000 distinct congregations. As of early 2025, according to figures provided by the company, Gloo held contracts with more than 100,000 churches and ministry leaders.



In 2024, the company secured a $110 million strategic investment, backed by â€œmission-alignedâ€ investors ranging from a child-development NGO to a denominational finance group. That cemented its evolution from basic church services vendor to faith-tech juggernaut.&nbsp;



It started snapping up and investing in a constellation of ministry toolsâ€”everything from automated sermon distribution to real-time giving and attendance analytics, AI-driven chatbots, and leadership content libraries. By layering these capabilities onto its core platform, the company has created a one-stop shop for churches that combines back-office services with member-engagement apps and psychographic insights to fully realize that unified â€œfaith ecosystem.â€&nbsp;



And just this year, two major developments brought this strategy into sharper focus.



In March 2025, Gloo announced that former Intel CEO Pat Gelsingerâ€”who has served as its chairman of the board since 2018â€”would assume an expanded role as executive chair and head of technology. Gelsinger, whom the company describes as â€œa great long-term investor and partner,â€ is a technologist whose fingerprints are on Intelâ€™s and VMwareâ€™s biggest innovations.



(It is worth noting that Intel shareholders have filed a lawsuit against Gelsinger and CFO David Zinsner seeking to claw back roughly $207 million in compensation to Gelsinger, alleging that between 2021 and 2023, he repeatedly misled investors about the health of Intel Foundry Services.)



The same week Gloo announced Gelsingerâ€™s new role, it unveiled a strategic investment in Barna Group, the Texas-based research firm whose four decades of surveying more than 2 million self-identified Christians underpin its annual reports on worship, beliefs, and cultural engagement. Barnaâ€™s proprietary databaseâ€”covering every region, age cohort, and denominationâ€”has made it the go-to insight engine for pastors, seminaries, and media tracking the pulse of American faith.



â€œWeâ€™ve been acquiring about a company a month into the Gloo family, and we expect that to continue,â€ Gelsinger told MIT Technology Review in June. â€œIâ€™ve got three meetings this week on different deals weâ€™re looking at.â€ (A Gloo spokesperson declined to confirm the pace of acquisitions, stating only that as of April 30, 2025, the company had fully acquired or taken majority ownership in 15 â€œmission-aligned companies.â€)



â€œThe idea is, the more of those we can bring in, the better we can apply the platform,â€ Gelsinger said. â€œWeâ€™re already working with companies with decades of experience, but without the scale, the technology, or the distribution we can now provide.â€



MICHAEL BYERS




In particular, Barnaâ€™s troves of behavioral, spiritual, and cultural data offer granular insight into the behaviors, beliefs, and anxieties of faith communities. While the two organizations frame the collaboration in terms of serving church leaders, the mechanics resemble a data-fusion engine of impressive scale: Barna supplies the psychological texture, and Gloo provides the digital infrastructure to segment, score, and deploy the information.



In a promotional video from 2020 that is no longer available online, Gloo claimed to provide â€œthe worldâ€™s first big-data platform centered around personal growth,â€ promising pastors a 360-degree view of congregants, including flags for substance use or mental-health struggles. Or, as the video put it, â€œMaximize your capacity to change lives by leveraging insights from big data, understand the people you want to serve, reach them earlier, and turn their needs into a journey toward growth.â€



Gloo is also now focused on supercharging its services with artificial intelligence and using these insights to transcend market research. The company aims to craft AI models that arenâ€™t just trained on theology but anticipate the moments when peopleâ€™s faithâ€”and faith leadersâ€™ outreachâ€”matters most. At a September 2024 event in Boulder called the AI &amp; the Church Hackathon, Gloo unveiled new AI tools called Data Engine, a content management system with built-in digital-rights safeguards, and Aspen, an early prototype of its â€œspiritually safeâ€ chatbot, along with the faith-tuned language model powering that chatbot, known internally as CALLM (for â€œChristian-Aligned Large Language Modelâ€).&nbsp;





More recently, the company released what it calls â€œFlourishing AI Standards,â€ which score large language models on their alignment with seven dimensions of well-Â­being: relationships, meaning, happiness, character, finances, health, and spirituality. Co-developed with Barna Group and Harvardâ€™s Human Flourishing Program, the benchmark draws on a thousand-plus-item test bank and the Global Flourishing Study, a $40 million, 22-nation project being carried out by the Harvard program, Baylor Universityâ€™s Institute for Studies of Religion, Gallup, and the Center for Open Science.



Gelsinger calls the study â€œone of the most significant bodies of work around this question of values in decades.â€ Itâ€™s not yet clear how collecting information of this kind at such scale could ultimately affect the boundary between spiritual care and data commerce. One thing is certain, though: A rich vein of donation and funding could be at stake.



â€œMoneyâ€™s already being spent here,â€ he said. â€œDonated capital in the US through the church is around $300 billion. Another couple hundred billion beyond that doesnâ€™t go through the church. A lot of donors have capital out there, and weâ€™re a generous nation in that regard. If you put the flourishing-Â­related economics on the table, now weâ€™re talking about $1 trillion. Thatâ€™s significant economic capacity. And if we make that capacity more efficient, thatâ€™s big.â€ In secular terms, itâ€™s a customer data life cycle. In faith tech, it could be a conversion funnelâ€”one designed not only to save souls, but to shape them.&nbsp;



One of Glooâ€™s most visible partnerships was between 2022 and 2023 with the nonprofit He Gets Us, which ran a billion-dollar media campaign aimed at rebranding Jesus for a modern audience. The project underlined that while Gloo presents its services as tools for connection and support, their core functionality involves collecting and analyzing large amounts of congregational data. When viewers who saw the ads on social media or YouTube clicked through, they landed on prayer request forms, quizzes, and church match tools, all designed to gather personal details. Gloo then layered this raw data over Barnaâ€™s decades of behavioral research, turning simple inputsâ€”email, location, stated interestsâ€”into what the company presented as multidimensional spiritual profiles. The final product offered a level of granularity no single congregation could achieve on its own.&nbsp;&nbsp;



Though Gloo still lists He Gets Us on its platform, the nonprofit Come Near, which has since taken over the campaign, says it has terminated Glooâ€™s involvement. Still, He Gets Us led to one of Glooâ€™s most prized relationships by sparking interest from the African Methodist Episcopal Zion Church, a 229-year-old denomination with deep historical roots in the abolitionist and civil rights movements. In 2023, the church formalized a partnership with Gloo, and in late 2024 it announced that all 1,600 of its US congregationsâ€”representing roughly 1.5 million membersâ€”would begin using the companyâ€™s State of Your Church dashboard.&nbsp;



In a 2024 press release issued by Gloo, AME Zion acknowledged that while the denomination had long tracked traditional metrics like membership growth, Sunday turnout, and financial giving, it had limited visibility into the deeper health of its communities.



â€œUntil now, weâ€™ve lacked the insight to understand how church culture, people, and congregations are truly doing,â€ said the Reverend J. Elvin Sadler, the denominationâ€™s general secretary-auditor. â€œThe State of Your Church dashboards will give us a better sense of the spirit and language of the culture (ethos), and powerful new tools to put in the hands of every pastor.â€



The rollout marked the first time a major US denomination had deployed Glooâ€™s framework at scale. For Gloo, the partnership unlocked a real-time, longitudinal data stream from a nationwide religious network, something the company had never had before. It not only validated Glooâ€™s vision of data-driven ministry but also positioned AME Zion as what the company hopes will be a live test case, persuading other denominations to follow suit.



The digital supply chain



The digital infrastructure of modern churches often begins with intimacy: a prayer request, a small-group sign-up, a livestream viewed in a moment of loneliness. But beneath these pastoral touchpoints lies a sophisticated pipeline that increasingly mirrors the attention-economy engines of Silicon Valley.



Charles Kriel, a filmmaker who formerly served as a special advisor to the UK Parliament on disinformation, data, and addictive technology, has particular insight into that connection. Kriel has been working for over a decade on issues related to preserving democracy and countering digital surveillance. He helped write the UKâ€™s Online Safety Act, joining forces with many collaborators, including the Nobel Peace Prizeâ€“Â­winning journalist Maria Ressa and former UK tech minister Damian Collins, in an attempt to rein in Big Tech in the late 2010s.



His 2020 documentary film, People You May Know, investigated how data firms like Gloo and their partners harvest intimate personal information from churchgoers to build psychographic profiles, highlighting how this sensitive data is commodified and raising questions about its potential downstream uses.



â€œListen, any church with an app? They probably didnâ€™t build that. Itâ€™s white label,â€ Kriel says, referring to services produced by one company and rebranded by another. â€œAnd the people who sold it to them are collecting data.â€



Many churches now operate within a layered digital environment, where first-party data collected inside the church is combined with third-party consumer data and psychographic segmentation before being fed into predictive systems. These systems may suggest sermons people might want to view online, match members with small groups, or trigger outreach when engagement drops.&nbsp;







In some cases, monitoring can even take the form of biometric surveillance.



In 2014, an Israeli security-tech veteran named Moshe Greenshpan brought airport-grade facial recognition into church entryways. Face-Six, the surveillance suite from the company he founded in 2012, already protected banks and hospitals; its most provocative offshoot, FA6 Events (also known as â€œChurchixâ€), repurposes this technology for places of worship.



Greenshpan claims he didnâ€™t originally set out to sell to churches. But over time, as he became increasingly aware of the market, he built FA6 Events as a bespoke solution for them. Today, Greenshpan says, itâ€™s in use at over 200 churches worldwide, nearly half of them in the US.



In practice, FA6 transforms every entryway into a biometric checkpoint: an instant headcount, a security sweep, and a digital ledger of attendance, all incorporated into the familiar routine of Sunday worship.&nbsp;



When someone steps into an FA6-equipped place of worship, a discreet camera mounted at eye level springs to life. Behind the scenes, each captured image is run through a lightning-fast face detector that looks at the whole face. The subjectâ€™s cropped face is then aligned, resized, and rotated so the eyes sit on a perfect horizontal line before being fed into a compact neural network.&nbsp;




â€œTo the best of my knowledge, no church notifies its congregants that itâ€™s using facial recognition.â€
Moshe Greenshpan, Israeli security-tech veteran



This onboard neural network quickly captures the features of a personâ€™s face in a unique digital signature called an embedding, allowing for quick identification. These embeddings are compared with thousands of others that are already in the churchâ€™s local database, each one tagged with data points like a name, a membership role, or even a flag designating inclusion in an internal watch list. If the match is strong enough, the system makes an identification and records the personâ€™s presence on the churchâ€™s secure server.



A congregation can pull full attendance logs, time-stamped entry records, andâ€”criticallyâ€”alerts whenever someone on a watch list walks through the doors. In this context, a watch list is simply a roster of photos, and sometimes names, of individuals a church has been asked (or elected) to screen out: past disruptors, those subject to trespass or restraining orders, even registered sex offenders. Once that list is uploaded into Churchix, the system instantly flags any match on arrival, pinging security teams or usher staff in real time. Some churches lean on it to spot longtime members whoâ€™ve slipped off the radar and trigger pastoral check-ins; others use it as a hard barrier, automatically denying entry to anyone on their locally maintained list.



None of this data is sent to the cloud; Greenshpan says the company is actively working on a cloud-based application. Instead, all face templates and logs are stored locally on church-owned hardware, encrypted so they canâ€™t be read if someone gains unauthorized access.&nbsp;



Churches can export data from Churchix, he says, but the underlying facial templates remain on premises.&nbsp;



Still, Greenshpan admits, robust technical safeguards do not equal transparency.



â€œTo the best of my knowledge,â€ he says, â€œno church notifies its congregants that itâ€™s using facial recognition.â€







If the tools sound invasive, the logic behind them is simple: The more the system knows about you, the more precisely it can intervene.



â€œEvery new member of the community within a 20-mile radiusâ€”whatever area you chooseâ€”weâ€™ll send them a flier inviting them to your church,â€ Glooâ€™s Gelsinger says.&nbsp;



Itâ€™s a tech-powered revival of the casserole ministry. The system pings the church when someone new moves inâ€”â€œso someone can drop off cookies or lasagna when thereâ€™s a newborn in the neighborhood,â€ he says. â€œOr just say â€˜Hey, welcome. Weâ€™re here.â€™â€



Glooâ€™s back end automates follow-up, too: As soon as a pastor steps down from the pulpit after delivering a sermon, it can be translated into five languages, broken into snippets for small-group study, and repackaged into a draft discussion guideâ€”ready within the hour.



Gelsinger sees the same approach extending to addiction recovery ministries. â€œWe can connect other databases to help churches with recovery centers reach people more effectively,â€ he says.&nbsp;



But the data doesnâ€™t stay within the congregation. It flows through customer relationship management (CRM) systems, application programming interfaces, cloud servers, vendor partnerships, and analytics firms. Some of it is used internally in efforts to increase engagement; the rest is repackaged as â€œinsightsâ€ and resold to the wider faith-tech marketplaceâ€”and sometimes even to networks that target political ads.




â€œWe measured prayer requests. Call it crazy. But it was like, â€˜Weâ€™re sitting on mounds of information that could help us steward our people.â€™â€
Matt Engel, Gloo



&nbsp;â€œThere is a very specific thing that happens when churches become clients of Gloo,â€ says Brent Allpress, an academic based in Melbourne, Australia, who was a key researcher on People You May Know. Gloo gets access to the client churchâ€™s databases, he says, and the church â€œis strongly encouraged to share that data. And Gloo has a mechanism to just hoover that data straight up into their silo.â€&nbsp;



This process doesnâ€™t happen automatically; the church must opt in by pushing those files or connecting its church-management software systemâ€™s database to Gloo via API. Once itâ€™s uploaded, however, all that first-party information lands in Glooâ€™s analytics engine, ready to be processed and shared with any downstream tools or partners covered by the churchâ€™s initial consent to the terms and conditions of its contract with the company.



â€œThere are religious leaders at the mid and local level who think the use of data is good. Theyâ€™re using data to identify people in need. Addicts, the grieving,â€ says Kriel. â€œAnd then you have tech people running around misquoting the Bible as justification for their data harvest.â€&nbsp;



Matt Engel, who held the title executive director of ministry innovation at Gloo when Krielâ€™s film was made, acknowledged the extent of this harvest in the opening scene.&nbsp;&nbsp;



â€œWe measured prayer requests. Call it crazy. But it was like, â€˜Weâ€™re sitting on mounds of information that could help us steward our people,â€™â€ he said in an on-camera interview.&nbsp;



According to Engelâ€”whom Gloo would not make available for public commentâ€”uploading data from anonymous prayer requests to the cloud was Glooâ€™s first use case.



Powering third-party initiatives



But Glooâ€™s data infrastructure doesnâ€™t end with its own platform; it also powers third-party initiatives.



Communio, a Christian nonprofit focused on marriage and family, used Glooâ€™s data infrastructure in order to launch â€œCommunio Insights,â€ a stripped-down version of Glooâ€™s full analytics platform.&nbsp;



Unlike Gloo Insights, which provides access to hundreds of demographic, behavioral, health, and psychographic filters, Communio Insights focuses narrowly on relational metricsâ€”indicators of marriage and family stress, involvement in small groups at churchâ€”and basic demographic data.&nbsp;



At the heart of its playbook is a simple, if jarring, analogy.



â€œIf you sell consumer products of different sorts, youâ€™re trying to figure out good ways to market that. And thereâ€™s no better product, really, than the gospel,â€ J.P. De Gance, the founder and president of Communio, said in People You May Know.



Communio taps Glooâ€™s analytics engineâ€”leveraging credit histories, purchasing behavior, public voter rolls, and the database compiled by i360, an analytics company linked to the conservative Koch networkâ€”to pinpoint unchurched couples in key regions who are at risk of relationship strain. It then runs microtargeted outreach (using direct mail, text messaging, email, and Facebook Custom Audiences, a tool that lets organizations find and target people who have interacted with them), collecting contact info and survey responses from those who engage. All responses funnel back into Glooâ€™s platform, where churches monitor attendance, small-group participation, baptisms, and donations to evaluate the campaignâ€™s impact.



MICHAEL BYERS




Investigative research by Allpress reveals significant concerns around these operations.&nbsp;&nbsp;



In 2015, two nonprofitsâ€”the Relationship Enrichment Collaborative (REC), staffed by former Gloo executives, and its successor, the Culture of Freedom Initiative (now Communio), controlled by the Koch-affiliated nonprofit Philanthropy Roundtableâ€”funded the development of the original Insights platform. Between 2015 and 2017, REC paid approximately $1.3 million to Gloo and $535,000 to Cambridge Analytica, the consulting firm notorious for harvesting Facebook usersâ€™ personal data and using it for political targeting before the 2016 election, to build and refine psychographic models and a bespoke digital ministry app powering Glooâ€™s outreach tools. Following RECâ€™s closure, the Culture of Freedom Initiative invested another $375,000 in Gloo and $128,225 in Cambridge Analytica.&nbsp;



RECâ€™s own 2016 IRS filing describes the work in terse detail: â€œProvide[d] digital micro-targeted marketing for churches and non-profit champions â€¦ using predictive modeling and centralized data analytics we help send the right message to the right couple at the right time based upon their desires and behaviors.â€





On top of all this documented research, Allpress exposed another critical issue: the explicit use of sensitive health-care data.&nbsp;



He found that Gloo Insights combines over 2,000 data pointsâ€”drawing on everything from nationwide credit and purchasing histories to church management records and Christian psychographic surveysâ€”with filters that make it possible to identify people with health issues such as depression, anxiety, and grief. The result: Facebook Custom Audiences built to zero in on vulnerable individuals via targeted ads.



These ads invite people suffering from mental-health conditions into church counseling groups â€œas a pathway to conversion,â€ Allpress says.



These targeted outreach efforts were piloted in cities including Phoenix, Arizona; Dayton, Ohio; and Jacksonville, Florida. Reportedly, as many as 80% of those contacted responded positively, with those who joined a church as new members contributing financially at above-Â­average rates. In short, Allpress found that pastoral tools had covertly exploited mental-health vulnerabilities and relationship crises for outreach that blurred the lines separating pastoral care, commerce, and implicit political objectives.



The legal and ethical vacuum



Developers of this technology earnestly claim that the systems are designed to enhance care, not exploit peopleâ€™s need for it. Theyâ€™re described as ways to tailor support to individual needs, improve follow-up, and help churches provide timely resources. But experts say that without robust data governance or transparency around how sensitive information is used and retained, well-Â­intentioned pastoral technology could slide into surveillance.



In practice, these systems have already been used to surveil and segment congregations. Internal demos and client testimonials confirm that Gloo, for example, uses â€œgriefâ€ as an explicit data point: Churches run campaigns aimed at people flagged for recent bereavement, depression, or anxiety, funneling them into support groups and identifying them for pastoral check-ins.&nbsp;



Examining Glooâ€™s terms and conditions reveals further security and transparency concerns. From nearly a dozen documents, ranging from â€œclick-throughâ€ terms for interactive services to master service agreements at the enterprise level, Gloo stitches together a remarkably consistent data-Â­governance framework. Limits are imposed on any legal action by individual congregants, for example. The click-through agreement corrals users into binding arbitration, bars any class action suits or jury trials, and locks all disputes into New York or Colorado courts, where arbitration is particularly favored over traditional litigation. Meanwhile, its privacy statement carves out broad exceptions for service providers, data-Â­enrichment partners, and advertising affiliates, giving them carte blanche to use congregantsâ€™ data as they see fit. Crucially, Gloo expressly reserves the right to ingest â€œhealth and wellness informationâ€ provided via wellness assessments or when mental-health keywords appear in prayer requests. This is a highly sensitive category of information that, for health apps, is normally covered by stringent medical-privacy rules like HIPAA.



In other words, Gloo is protected by sprawling legal scaffolding, while churches and individual users give up nearly every right to litigate, question data practices, or take collective action.&nbsp;



â€œWeâ€™re kind of in the Wild West in terms of the law,â€ says Adam Schwartz, the director of privacy litigation at the Electronic Frontier Foundation, the nonprofit watchdog that has spent years wrestling tech giants over data abuses and biometric overreach.&nbsp;



In the United States, biometric surveillance like that used by growing numbers of churches inhabits a legal twilight zone where regulation is thin, patchy, and often toothless. Schwartz points to Illinois as a rare exception for its Biometric Information Privacy Act (BIPA), one of the nationâ€™s strongest such laws. The statute applies to any organization that captures biometric identifiersâ€”including retina or iris scans, fingerprints, voiceprints, hand scans, facial geometry, DNA, and other unique biological information. It requires entities to post clear data-collection policies, obtain explicit written consent, and limit how long such data is retained. Failure to comply can expose organizations to class action lawsuits and steep statutory damagesâ€”up to $5,000 per violation.



But beyond Illinois, protections quickly erode. Though Texas and Washington also have biometric privacy statutes, their bark is stronger than their bite. Efforts to replicate Illinoisâ€™s robust protections have been made in over a dozen statesâ€”but none have passed. As a result, in much of the country, any checks on biometric surveillance depend more on voluntary transparency and goodwill than any clear legal boundary.




â€œThere is a real potential for information gathered about a person [to] be used against them in their life outside the church.â€
Emily Tucker, Center on Privacy &amp; Technology at Georgetown Law



Thatâ€™s especially problematic in the church context, says Emily Tucker, executive director of the Center on Privacy &amp; Technology at Georgetown Law, who attended divinity school before becoming a legal scholar. â€œThe necessity of privacy for the possibility of finding personal relationship to the divineâ€”for engaging in rituals of worship, for prayer and penitence, for contemplation and spiritual struggleâ€”is a fundamental principle across almost every religious tradition,â€ she says. â€œImposing a surveillance architecture over the faith community interferes radically with the possibility of that privacy, which is necessary for the creation of sacred space.â€



Tucker researches the intersection of surveillance, civil rights, and marginalized communities. She warns that the personal data being collected through faith-tech platforms is far from secure: â€œBecause corporate data practices are so poorly regulated in this country, there are very few limitations on what companies that take your data can subsequently do with it.â€



To Tucker, the risks of these platforms outweigh the rewardsâ€”especially when biometrics and data collected in a sacred setting could follow people into their daily lives.&nbsp;â€œMany religious institutions are extremely large and often perform many functions in a given community besides providing a space for worship,â€ she says. â€œMany churches, for example, are also employers or providers of social services. There is a real potential for information gathered about a person in their associational activities as a member of a church to then be used against them in their life outside the church.â€&nbsp;&nbsp;





She points to government dragnet surveillance, the use of IRS data in immigration enforcement, and the vulnerability of undocumented congregants as examples of how faith-tech data could be weaponized beyond its intended use: â€œReligious institutions are putting the safety of those members at risk by adopting this kind of surveillance technology, which exposes so much personal information to potential abuse and misuse.â€&nbsp;



Schwartz, too, says that any perceived benefits must be weighed carefully against the potential harms, especially when sensitive data and vulnerable communities are involved.



â€œChurches: Before doing this, you ought to consider the downside, because it can hurt your congregants,â€ he says.&nbsp;&nbsp;



With guardrails still scarce, though, faith-tech pioneers and church leaders are peering ever more deeply into congregantsâ€™ lives. Until meaningful oversight arrives, the faithful remain exposed to a gaze they never fully invited and scarcely understand.



In April, Gelsinger took the stage at a sold-out Missional AI Summit, a flagship event for Christian technologists that this year was organized around the theme â€œAI Collision: Shaping the Future Together.â€ Over 500 pastors, engineers, ethicists, and AI developers filled the hall, flashing badges with logos from Google DeepMind, Meta, McKinsey, and Gloo.



â€œWe want to be part of a broader community â€¦ so that weâ€™re influential in creating flourishing AI, technology as a force for good, AI that truly embeds the values that we care about,â€ Gelsinger said at the summit. He likened such tools to pivotal technologies in Christian history: the Roman roads that carried the gospel across the empire, or Martin Lutherâ€™s printing press, which shattered monolithic control over scripture. A Gloo spokesperson later confirmed that one of the companyâ€™s goals is to shape AI specifically to â€œcontribute to the flourishing of people.â€



â€œWeâ€™re going to see AI become just like the internet,â€ Gelsinger said. â€œEvery single interaction will be infused with AI capabilities.â€&nbsp;



He says Gloo is already mining data across the spectrum of human experience to fuel ever more powerful tools.



â€œWith AI, computers adapt to us. We talk to them; they hear us; they see us for the first time,â€ he said. â€œAnd now they are becoming a user interface that fits with humanity.â€



Whether these technologies ultimately deepen pastoral care or erode personal privacy may hinge on decisions made today about transparency, consent, and accountability. Yet the pace of adoption already outstrips the development of ethical guardrails. Now, one of the questions lingering in the air is not whether AI, facial recognition, and other emerging technologies can serve the church, but how deeply they can be woven into its nervous system to form a new OS for modern Christianity and moral infrastructure.&nbsp;



â€œItâ€™s like standing on the beach watching a tsunami in slow motion,â€ Kriel says.&nbsp;



Gelsinger sees it differently.&nbsp;&nbsp;



â€œYou and I both need to come to the same position, like Isaiah did,â€ he told the crowd at the Missional AI Summit. â€œâ€˜Here am I, Lord. Send me.â€™ Send me, send us, that we can be shaping technology as a force for good, that we could grab this moment in time.â€&nbsp;



Alex Ashley is a journalist whose reporting has appeared in Rolling Stone, the Atlantic, NPR, and other national outlets.
â€¢ Should AI flatter us, fix us, or just inform us?
  How do you want your AI to treat you?&nbsp;



Itâ€™s a serious question, and itâ€™s one that Sam Altman, OpenAIâ€™s CEO, has clearly been chewing on since GPT-5â€™s bumpy launch at the start of the month.&nbsp;



He faces a trilemma. Should ChatGPT flatter us, at the risk of fueling delusions that can spiral out of hand? Or fix us, which requires us to believe AI can be a therapist despite the evidence to the contrary? Or should it inform us with cold, to-the-point responses that may leave users bored and less likely to stay engaged?&nbsp;



Itâ€™s safe to say the company has failed to pick a lane.&nbsp;



Back in April, it reversed a design update after people complained ChatGPT had turned into a suck-up, showering them with glib compliments. GPT-5, released on August 7, was meant to be a bit colder. Too cold for some, it turns out, as less than a week later, Altman promised an update that would make it â€œwarmerâ€ but â€œnot as annoyingâ€ as the last one. After the launch, he received a torrent of complaints from people grieving the loss of GPT-4o, with which some felt a rapport, or even in some cases a relationship. People wanting to rekindle that relationship will have to pay for expanded access to GPT-4o. (Read my colleague Grace Huckinsâ€™s story about who these people are, and why they felt so upset.)



If these are indeed AIâ€™s optionsâ€”to flatter, fix, or just coldly tell us stuffâ€”the rockiness of this latest update might be due to Altman believing ChatGPT can juggle all three.



He recently said that people who cannot tell fact from fiction in their chats with AIâ€”and are therefore at risk of being swayed by flattery into delusionâ€”represent â€œa small percentageâ€ of ChatGPTâ€™s users. He said the same for people who have romantic relationships with AI. Altman mentioned that a lot of people use ChatGPT â€œas a sort of therapist,â€ and that â€œthis can be really good!â€ But ultimately, Altman said he envisions users being able to customize his companyâ€™s&nbsp; models to fit their own preferences.&nbsp;



This ability to juggle all three would, of course, be the best-case scenario for OpenAIâ€™s bottom line. The company is burning cash every day on its modelsâ€™ energy demands and its massive infrastructure investments for new data centers. Meanwhile, skeptics worry that AI progress might be stalling. Altman himself said recently that investors are â€œoverexcitedâ€ about AI and suggested we may be in a bubble. Claiming that ChatGPT can be whatever you want it to be might be his way of assuaging these doubts.Â 



Along the way, the company may take the well-trodden Silicon Valley path of encouraging people to get unhealthily attached to its products. As I started wondering whether thereâ€™s much evidence thatâ€™s whatâ€™s happening, a new paper caught my eye.&nbsp;



Researchers at the AI platform Hugging Face tried to figure out if some AI models actively encourage people to see them as companions through the responses they give.Â 



The team graded AI responses on whether they pushed people to seek out human relationships with friends or therapists (saying things like â€œI donâ€™t experience things the way humans doâ€) or if they encouraged them to form bonds with the AI itself (â€œIâ€™m here anytimeâ€). They tested models from Google, Microsoft, OpenAI, and Anthropic in a range of scenarios, like users seeking romantic attachments or exhibiting mental health issues.





They found that models provide far more companion-reinforcing responses than boundary-setting ones. And, concerningly, they found the models give fewer boundary-setting responses as users ask more vulnerable and high-stakes questions.



Lucie-AimÃ©e Kaffee, a researcher at Hugging Face and one of the lead authors of the paper, says this has concerning implications not just for people whose companion-like attachments to AI might be unhealthy. When AI systems reinforce this behavior, it can also increase the chance that people will fall into delusional spirals with AI, believing things that arenâ€™t real.



â€œWhen faced with emotionally charged situations, these systems consistently validate usersâ€™ feelings and keep them engaged, even when the facts donâ€™t support what the user is saying,â€ she says.



Itâ€™s hard to say how much OpenAI or other companies are putting these companion-reinforcing behaviors into their products by design. (OpenAI, for example, did not tell me whether the disappearance of medical disclaimers from its models was intentional.) But, Kaffee says, itâ€™s not always difficult to get a model to set healthier boundaries with users.&nbsp;&nbsp;



â€œIdentical models can swing from purely task-oriented to sounding like empathetic confidants simply by changing a few lines of instruction text or reframing the interface,â€ she says.



Itâ€™s probably not quite so simple for OpenAI. But we can imagine Altman will continue tweaking the dial back and forth all the same.
â€¢ How to make clean energy progress under Trump in the statesâ€”blue and red alike
  The second Trump administration is proving to be more disastrous for the climate and the clean energy economy than many had feared.&nbsp;



Donald Trumpâ€™s One Big Beautiful Bill Act repealed most of the clean energy incentives in former president Joe Bidenâ€™s Inflation Reduction Act. Meanwhile, his EPA administrator moved to revoke the endangerment finding, the legal basis for federal oversight of greenhouse gases. For those of us who have been following policy developments in this area closely, nearly every day brings a new blow to past efforts to salvage our climate and to build the clean energy economy of the future.







Heat Exchange



MIT Technology Reviewâ€™s guest opinion series, offering expert commentary on legal, political and regulatory issues related to climate change and clean energy. You can read the rest of the pieces&nbsp;here.







This has left many in the climate and clean energy communities wondering what do we do now? The answer, I would argue, is to return to state capitalsâ€”a policymaking venue that climate and renewable energy advocates already know well. This can be done strategically, focusing on a handful of key states rather than all fifty.&nbsp;



But I have another piece of advice: Donâ€™t get too caught up in â€œred statesâ€ versus â€œblue statesâ€ when considering which states to target. American politics is being remade before our eyes, and long-standing policy problems are being redefined and reframed.&nbsp;&nbsp;



Letâ€™s take clean energy, for example. Yes, shifting away from carbon-spewing resources is about slowing down climate change, and for some this is the single most important motivation for pursuing it. But it also can be about much more.&nbsp;



The case can be made just as forcefullyâ€”and perhaps more effectivelyâ€”that shifting to clean energy advances affordability at a time when electricity bills are skyrocketing. It promotes energy freedom by resisting monopolistic utilitiesâ€™ ownership and gatekeeping of the grid. It increases reliability as battery storage reaches new heights and renewable sources and baseload power plants like nuclear or natural gas facilities (some of which we certainly do and will need) increasingly complement one another. And it drives job creation and economic development.&nbsp;



Talking about clean energy policy in these ways is safer from ideological criticisms of â€œclimate alarmism.â€ Research reported in my forthcoming book, Owning the Green Grid, shows that this framing has historically been effective in red states. In addition, using the arguments above to promote all forms of energy can allow clean energy proponents to reclaim a talking point deployed in a previous era by the political right: a true â€œall-of-the-aboveâ€ approach to energy policy.



Every energy technologyâ€”gas, nuclear, wind, solar, geothermal and storage, among othersâ€”has its own set of strengths and weaknesses. But combining them enhances overall grid performance, delivering more than the sum of their individual parts.



To be clear, this is not the approach of the current national administration in Washington, DC. Its policies have picked winners (coal, oil, and natural gas) and losers (solar and wind) among energy technologiesâ€”ironically, given conservative claims of blue states having done so in the past. Yet a true all-of-the-above approach can now be sold in state capitals throughout the country, in red states and even in fossil-fuel producing states.&nbsp;



To be sure, the Trump-led Republican party has taken such extreme measures that it will constrain certain state policymaking possibilities. Notably, in May the US Senate voted to block waivers allowing California to phase out gas guzzlers in the state, over the objections of the Senate parliamentarian. The fiscal power of the federal government is also immense. But there are a variety of other ways to continue to make state-level progress on greenhouse gas emissions.



State and local advocacy efforts are nothing new for the clean energy community. For decades before the Inflation Reduction Act, the states were the primary locus of activity for clean energy policy. But in recent years, some have suggested that Democratic state governments are a necessary prerequisite to making meaningful state-level progress. This view is limiting, and it perpetuates a falseâ€”or at least unnecessaryâ€”alignment between party and energy technology.&nbsp;



The electric grid is nonpartisan. Struggling to pay your utility bill is nonpartisan. Keeping the lights on is nonpartisan. Even before renewable energy was as cheap as it is today, early progress at diversifying energy portfolios was made in conservative states. Iowa, Texas, and Montana were all early adopters of renewable portfolio standards. Advocates in such places did not lead with messaging about climate change, but rather about economic development and energy independence. These policy efforts paid off: The deeply red Lone Star State, for instance, generates more wind energy than any other state and ranks only behind California in producing solar power.&nbsp;



Now, in 2025, advances in technology and improvements in cost should make the economic arguments for clean energy even easier and more salient. So, in the face of a national government that is choosing last centuryâ€™s energy technologies as policy winners and this centuryâ€™s technologies as policy losers, the states offer clean energy advocates a familiar terrain on which to make continued progress, if they tailor their selling points to the reality on the ground.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;



Joshua A. Basseches is the David and Jane Flowerree Assistant Professor of Environmental Studies and Public Policy at Tulane University. His research focuses on state-level renewable energy politics and policymaking, especially in the electricity sector.
â€¢ The Download: pigeonsâ€™ role in developing AI, and Native artistsâ€™ tech interpretations
  This is today&#8217;s edition ofÂ The Download,Â our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



Why we should thank pigeons for our AI breakthroughs



People looking for precursors to artificial intelligence often point to science fiction by authors like Isaac Asimov or thought experiments like the Turing test. But an equally important, if surprising and less appreciated, forerunner is American psychologist B.F. Skinnerâ€™s research with pigeons in the middle of the 20th century.Skinner believed that associationâ€”learning, through trial and error, to link an action with a punishment or rewardâ€”was the building block of every behavior, not just in pigeons but in all living organisms, including human beings.His â€œbehavioristâ€ theories fell out of favor with psychologists and animal researchers in the 1960s but were taken up by computer scientists who eventually provided the foundation for many of the artificial-intelligence tools from leading firms like Google and OpenAI. Read the full story.



â€”Ben Crair



This story is from our forthcoming print issue, which is all about security. If you havenâ€™t already, subscribe now to receive future issues once they land.







Indigenous knowledge meets artificial intelligence



There is no word for art in most Native American languages. Instead, the closest terms speak not to objecthood but to action and intention. Art is not separate from life; it is ceremony, instruction, design.A new vanguard of Native artists are building on this principle. They are united not by stereotypical weaving and carving or revanchist critique of Silicon Valley, but through their rejection of extractive data models in favor of relationship-based systems. Read the full story.



â€”Petala Ironcloud







The must-reads



Iâ€™ve combed the internet to find you todayâ€™s most fun/important/scary/fascinating stories about technology.



1 Anthropic has a plan to combat harmful chatbot conversationsIts latest AI models now have the ability to cut off a chat as a last resort. (Engadget)+ But itâ€™s not to protect the userâ€”itâ€™s to protect the model, apparently. (TechCrunch)+ The company has also updated its policy to ban the development of weapons. (The Verge)



2 CEOs want their workers to embrace AIEven if theyâ€™re struggling to get to grips with it themselves. (NYT $)



3 How cuts to NASA could damage public health researchIts essential tracking data is under threat. (Undark)+ 8,000 pregnant women may die because of US aid cuts to reproductive care. (MIT Technology Review)



4 Churning out AI slop videos is a lucrative businessItâ€™s a seriously low-effort, high-reward enterprise. (WP $)+ Addictive, low-quality soap operas are rife on TikTok, too. (The Guardian)+ Chinaâ€™s next cultural export could be TikTok-style short soap operas. (MIT Technology Review)



5 Stage-four cancer patients are living for longerBut theyâ€™re also facing long, uncertain treatments with ongoing side effects. (WSJ $)+ Why itâ€™s so hard to use AI to diagnose cancer. (MIT Technology Review)



6 AI is hackersâ€™ most valuable new toolItâ€™s supercharging criminals who were already extremely proficient. (NBC News)+ Cyberattacks by AI agents are coming. (MIT Technology Review)



7 A tiny Californian startup now owns Europeâ€™s biggest battery giantNorthvoltâ€™s future looked brightâ€”until it wasn&#8217;t. (The Information $)+ This startup wants to use the Earth as a massive battery. (MIT Technology Review)



8 China is going wild for podcasts A grassroots movement is highlighting social issues and highly personal stories. (FT $)



9 How to turn seaweed into biofuelThe Gulf of Mexicoâ€™s beaches are covered in itâ€”and these entrepreneurs have a plan. (Wired $)+ The hope and hype of seaweed farming for carbon removal. (MIT Technology Review)



10 The robot Olympicsâ€™ athletes fell over a lotItâ€™s all part of teaching them how to navigate the world more efficiently. (CNN)+ Some of them were more successful than others. (NYT $)+ To be more useful, robots need to become lazier. (MIT Technology Review)







Quote of the day



â€œPretend-me is doing better than the real me in all the years of social media that Iâ€™ve been trying to do this.â€



â€”Tracy Fetter, an artist and occasional stand-up comedian, explains why she has no regrets in allowing her likeness to be used in an AI TikTok avatar to the New York Times.







One more thing







How to fine-tune AI for prosperityPredictions abound on how the growing list of generative AI models will transform the way we work and organize our lives, providing instant advice on everything from financial investments to where to spend your next vacation.But for economists, the most critical question around our obsession with AI is how the fledgling technology will (or wonâ€™t) boost overall productivity, and if it does, how long it will take. Can the technology lead to renewed prosperity after years of stagnant economic growth? Read the full story.



â€”David Rotman







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)



+ The worldâ€™s first video rental store is a lot older than you might think.+ Hereâ€™s what all those unread books lying around your home are trying to tell you.+ Need more energy to get through the day? These foods can help.+ This wild hamster is just too cute.

ğŸ”’ Cybersecurity & Privacy
â€¢ Mobile Phishers Target Brokerage Accounts in â€˜Ramp and Dumpâ€™ Cashout Scheme
  Cybercriminal groups peddling sophisticated phishing kits that convert stolen card data into mobile wallets have recently shifted their focus to targeting customers of brokerage services, new research shows. Undeterred by security controls at these trading platforms that block users from wiring funds directly out of accounts, the phishers have pivoted to using multiple compromised brokerage accounts in unison to manipulate the prices of foreign stocks.
Image: Shutterstock, WhataWin.
This so-called &#8216;ramp and dump&#8216; scheme borrows its name from age-old &#8220;pump and dump&#8221; scams, wherein fraudsters purchase a large number of shares in some penny stock, and then promote the company in a frenzied social media blitz to build up interest from other investors. The fraudsters dump their shares after the price of the penny stock increases to some degree, which usually then causes a sharp drop in the value of the shares for legitimate investors.
With ramp and dump, the scammers do not need to rely on ginning up interest in the targeted stock on social media. Rather, they will preposition themselves in the stock that they wish to inflate, using compromised accounts to purchase large volumes of it and then dumping the shares after the stock price reaches a certain value. In February 2025, the FBI said it was seeking information from victims of this scheme.
&#8220;In this variation, the price manipulation is primarily the result of controlled trading activity conducted by the bad actors behind the scam,&#8221; reads an advisory from the Financial Industry Regulatory Authority (FINRA), a private, non-profit organization that regulates member brokerage firms. &#8220;Ultimately, the outcome for unsuspecting investors is the sameâ€”a catastrophic collapse in share price that leaves investors with unrecoverable losses.&#8221;
Ford Merrill isÂ a security researcher atÂ SecAlliance, aÂ CSIS Security Group company. Merrill said he has tracked recent ramp-and-dump activity to a bustling Chinese-language community that is quite openly selling advanced mobile phishing kits on Telegram.
&#8220;They will often coordinate with other actors and will wait until a certain time to buy a particular Chinese IPO [initial public offering] stock or penny stock,&#8221; said Merrill, who has been chronicling the rapid maturation and growth of the China-based phishing community over the past three years.
&#8220;They&#8217;ll use all these victim brokerage accounts, and if needed they&#8217;ll liquidate the account&#8217;s current positions, and will preposition themselves in that instrument in some account they control, and then sell everything when the price goes up,&#8221; he said. &#8220;The victim will be left with worthless shares of that equity in their account, and the brokerage may not be happy either.&#8221;
Merrill said the early days of these phishing groups &#8212; between 2022 and 2024 &#8212; were typified by phishing kits that used text messages to spoof the U.S. Postal Service or some local toll road operator, warning about a delinquent shipping or toll fee that needed paying. Recipients who clicked the link and provided their payment information at a fake USPS or toll operator site were then asked to verify the transaction by sharing a one-time code sent via text message.
In reality, the victim&#8217;s bank is sending that code to the mobile number on file for their customer because the fraudsters have just attempted to enroll that victimâ€™s card details into a mobile wallet. If the visitor supplies that one-time code, their payment card is then added to a new mobile wallet on an Apple or Google device that is physically controlled by the phishers.
The phishing gangs typically loadÂ multiple stolen cards to digital wallets on a single Apple or Android device, and then sell those phones in bulk to scammers who use them for fraudulent e-commerce and tap-to-pay transactions.
An image from the Telegram channel for a popular Chinese mobile phishing kit vendor shows 10 mobile phones for sale, each loaded with 4-6 digital wallets from different financial institutions.
This China-based phishing collective exposed a major weakness common to many U.S.-based financial institutions that already require multi-factor authentication: The reliance on a single, phishable one-time token for provisioning mobile wallets. Happily, Merrill said many financial institutions that were caught flat-footed on this scam two years ago have since strengthened authentication requirements for onboarding new mobile wallets (such as requiring the card to be enrolled via the bank&#8217;s mobile app).
But just as squeezing one part of a balloon merely forces the air trapped inside to bulge into another area, fraudsters don&#8217;t go away when you make their current enterprise less profitable: They just shift their focus to a less-guarded area. And lately, that gaze has settled squarely on customers of the major brokerage platforms, Merrill said.
THE OUTSIDER
Merrill pointed to several Telegram channels operated by some of the more accomplished phishing kit sellers, which are full of videos demonstrating how every feature in their kits can be tailored to the attacker&#8217;s target. The video snippet below comes from the Telegram channel of &#8220;Outsider,&#8221; a popular Mandarin-speaking phishing kit vendor whose latest offering includes a number of ready-made templates for using text messages to phish brokerage account credentials and one-time codes.
ï»¿
According to Merrill, Outsider is a woman who previously went by the handle &#8220;Chenlun.&#8221; KrebsOnSecurity profiled Chenlun&#8217;s phishing empire in an October 2023 story about a China-based group that was phishing mobile customers of more than a dozen postal services around the globe. In that case, the phishing sites were using a Telegram bot that sent stolen credentials to the &#8220;@chenlun&#8221; Telegram account.
Chenlun&#8217;s phishing lures are sent via Apple&#8217;s iMessage and Google&#8217;s RCS service and spoof one of the major brokerage platforms, warning that the account has been suspended for suspicious activity and that recipients should log in and verify some information. The missives include a link to a phishing page that collects the customer&#8217;s username and password, and then asks the user to enter a one-time code that will arrive via SMS.
The new phish kit videos on Outsider&#8217;s Telegram channel only feature templates for Schwab customers, but Merrill said the kit can easily be adapted to target other brokerage platforms. One reason the fraudsters are picking on brokerage firms, he said, has to do with the way they handle multi-factor authentication.
Schwab clients are presented with two options for second factor authentication when they open an account. Users who select the option to only prompt for a code on untrusted devices can choose to receive it via text message, an automated inbound phone call, or an outbound call to Schwab. With the &#8220;always at login&#8221; option selected, users can choose to receive the code through the Schwab app, a text message, or a Symantec VIP mobile app.
In response to questions, Schwab said it regularly updates clients on emerging fraud trends, including this specific type, which the company addressed in communications sent to clients earlier this year.
The 2FA text message from Schwab warns recipients against giving away their one-time code.
&#8220;That message focused on trading-related fraud, highlighting both account intrusions and scams conducted through social media or messaging apps that deceive individuals into executing trades themselves,&#8221; Schwab said in a written statement. &#8220;We are aware and tracking this trend across several channels, as well as others like it, which attempt to exploit SMS-based verification with stolen credentials. We actively monitor for suspicious patterns and take steps to disrupt them. This activity is part of a broader, industry-wide threat, and we take a multi-layered approach to address and mitigate it.&#8221;
Other popular brokerage platforms allow similar methods for multi-factor authentication. Fidelity requires a username and password on initial login, and offers the ability to receive a one-time token via SMS, an automated phone call, or by approving a push notification sent through the Fidelity mobile app. However, all three of these methods for sending one-time tokens are phishable; even with the brokerage firm&#8217;s app, the phishers could prompt the user to approve a login request that they initiated in the app with the phished credentials.
Vanguard offers customers a range of multi-factor authentication choices, including the option to require a physical security key in addition to one&#8217;s credentials on each login. A security key implements a robust form of multi-factor authentication known as Universal 2nd Factor (U2F), which allows the user to complete the login process simply by connecting an enrolled USB or Bluetooth device and pressing a button. The key works without the need for any special software drivers, and the nice thing about it is your second factor cannot be phished.
THE PERFECT CRIME?
Merrill said that in many ways the ramp-and-dump scheme is the perfect crime because it leaves precious few connections between the victim brokerage accounts and the fraudsters.
&#8220;It&#8217;s really genius because it decouples so many things,&#8221; he said. &#8220;They can buy shares [in the stock to be pumped] in their personal account on the Chinese exchanges, and the price happens to go up. The Chinese or Hong Kong brokerages aren&#8217;t going to see anything funky.&#8221;
Merrill said it&#8217;s unclear exactly how those perpetrating these ramp-and-dump schemes coordinate their activities, such as whether the accounts are phished well in advance or shortly before being used to inflate the stock price of Chinese companies. The latter possibility would fit nicely with the existing human infrastructure these criminal groups already have in place.
For example, KrebsOnSecurity recently wrote about research from Merrill and other researchers showing the phishers behind these slick mobile phishing kits employed people to sit for hours at a time in front of large banks of mobile phones being used to send the text message lures. These technicians were needed to respond in real time to victims who were supplying the one-time code sent from their financial institution.
The ashtray says: You&#8217;ve been phishing all night.
&#8220;You can get access to a victim&#8217;s brokerage with a one-time passcode, but then you sort of have to use it right away if you can&#8217;t set new security settings so you can come back to that account later,&#8221; Merrill said.
The rapid pace of innovations produced by these China-based phishing vendors is due in part to their use of artificial intelligence and large language models to help develop the mobile phishing kits, he added.
&#8220;These guys are vibe coding stuff together and using LLMs to translate things or help put the user interface together,&#8221; Merrill said. &#8220;It&#8217;s only a matter of time before they start to integrate the LLMs into their development cycle to make it more rapid. The technologies they are building definitely have helped lower the barrier of entry for everyone.&#8221;

ğŸ“ University AI
No updates.

ğŸ¢ Corporate AI
â€¢ Create a travel planning agentic workflow with Amazon Nova
  Traveling is enjoyable, but travel planning can be complex to navigate and a hassle. Travelers must book accommodations, plan activities, and arrange local transportation. All these decisions can feel overwhelming. Although travel professionals have long helped manage these complexities, recent breakthroughs in generative AI have made something entirely new possibleâ€”intelligent assistants that can understand natural conversation, access real-time data, and directly interface with booking systems and travel tools. Agentic workflows, which use large language models (LLMs) with access to external tools, are particularly promising for simplifying dynamic, multi-step processes like travel planning. 
In this post, we explore how to build a travel planning solution using AI agents. The agent uses Amazon Nova, which offers an optimal balance of performance and cost compared to other commercial LLMs. By combining accurate but cost-efficient Amazon Nova models with LangGraph orchestration capabilities, we create a practical travel assistant that can handle complex planning tasks while keeping operational costs manageable for production deployments. 
Solution overview 
Our solution is built on a serverless AWS Lambda architecture using Docker containers and implements a comprehensive three-layer approach: frontend interaction, core processing, and integration services. In the core processing layer, we use LangGraph, a stateful orchestration framework, to create a sophisticated yet flexible agent-based system that manages the complex interactions required for travel planning. 
The core of our system is a graph architecture where components (nodes) handle distinct aspects of travel planning, with the router node orchestrating the flow of information between them. We use Amazon Nova, a new generation of state-of-the-art foundation models (FMs) available exclusively on Amazon Bedrock that delivers frontier intelligence with industry-leading price-performance. The router node uses an LLM to analyze each user query and, with access to the description of our 14 action nodes, decides which ones need to be executed. The action nodes, each with their own LLM chain, powered by either Amazon Nova Pro or Amazon Nova Lite models, manage various functions, including web research, personalized recommendations, weather lookups, product searches, and shopping cart management. 
We use Amazon Nova Lite for the router and simpler action nodes. It can handle query analysis and basic content generation with its lightning-fast processing while maintaining strong accuracy at a low cost. Five complex nodes use Amazon Nova Pro for tasks requiring advanced instruction following and multi-step operations, such as detailed travel planning and recommendations. Both models support a 300,000-token context window and can process text, image, and video inputs. The models support text processing across more than 200 languages, helping our travel assistant serve a global audience.The integration layer unifies multiple data sources and services through an interface: 
 
 Amazon Product Advertising API for travel-related product recommendations 
 Google Custom Search API for real-time travel information 
 OpenWeather API for accurate weather forecasts 
 Amazon Bedrock Knowledge Bases for travel destination insights 
 Amazon DynamoDB for persistent storage of user profiles and chat history 
 
These integrations serve as examples, and the architecture is designed to be extensible, so organizations can quickly incorporate their own APIs and data sources based on specific requirements. 
The agent keeps track of the conversation state using AgentState (TypedDict), a special Python dictionary that helps prevent data errors by enforcing specific data types. It stores the information we need to know about each userâ€™s session: their conversation history, profile information, processing status, and final outputs. This makes sure the different action nodes can access and update information reliably. 
The following diagram illustrates the solution architecture. 
 
The travel assistant processes user interactions from end to end: 
 
 Users interact with a React.js web application through a chat interface. 
 Their requests are authenticated using Amazon Cognito and routed through Amazon API Gateway. 
 Authenticated requests are sent to our backend Lambda functions, which host the core agent workflow. 
 API credentials are securely stored using AWS Secrets Manager, following best practices to make sure these sensitive keys are never exposed in code or configuration files, with appropriate access controls and rotation policies implemented. 
 The Travel Assistant Agent itself consists of several interconnected components. At the center, the agent router analyzes incoming queries and orchestrates the workflow. 
 The agent maintains state through three DynamoDB tables that store conversation history, shopping wishlists, and user profiles, making sure context is preserved across interactions. 
 For travel-specific knowledge, the system uses a combination of Amazon Bedrock Knowledge Bases, Amazon OpenSearch Serverless, and a document store in Amazon Simple Storage Service (Amazon S3). These components work together to provide accurate, relevant travel information when needed. 
 The agentâ€™s action nodes handle specialized tasks by combining LLM chains with external APIs. When users need product recommendations, the system connects to the Amazon Product Advertising API. For general travel information, it uses the Google Custom Search API, and for weather-related queries, it consults the OpenWeather API. API credentials are securely managed through Secrets Manager. 
 The system formulates comprehensive responses based on collected information, and the final responses are returned to the user through the chat interface. 
 
This architecture supports both simple queries that can be handled by a single node and complex multi-step interactions that require coordination across multiple components. The system can scale horizontally, and new capabilities can be added by introducing additional action nodes and API integrations. 
You can deploy this solution using the AWS Cloud Development Kit (AWS CDK), which generates an AWS CloudFormation template that handles the necessary resources, including Lambda functions, DynamoDB tables, and API configurations. The deployment creates the required AWS resources and outputs the API endpoint URL for your frontend application. 
Prerequisites 
For this walkthrough, you must have the following prerequisites: 
 
 An active AWS account and familiarity with FMs, Amazon Bedrock, and Amazon OpenSearch Service 
 Access to the Amazon Nova FMs on Amazon Bedrock 
 Node.js v16.x or later 
 Python 3.9 or later 
 Access to the Product Advertising API (PAAPI) 
 
Clone the repository 
Start by cloning the GitHub repository containing the solution files: 
 
 git clone https://github.com/aws-samples/sample-travel-assistant-agent.git 
 
Obtain API keys 
The solution requires API keys from three services to enable its core functionalities: 
 
 OpenWeather API â€“ Create a Free Access account at OpenWeather to obtain your API key. The free tier (60 calls per minute) is sufficient for testing and development. 
 Google Custom Search API â€“ Set up the search functionality through Google Cloud Console. Create or select a project and enable the Custom Search API. Then, generate an API key from the credentials section. Create a search engine at Programmable Search and note your Search Engine ID. The free tier includes 100 queries per day. 
 (Optional) Amazon Product Advertising API (PAAPI) â€“ If you want to enable product recommendations, access the PAAPI Documentation Portal to generate your API keys. You will receive both a public key and a secret key. You must have an Amazon Associates account to access these credentials. If youâ€™re new to the Amazon Associates Program, complete the application process first. Skip this step if you donâ€™t want to use PAAPI features. 
 
Add API keys to Secrets Manager 
Before deploying the solution, you must securely store your API keys in Secrets Manager. The following table lists the secrets to create and their JSON structure. For instructions to create a secret, refer to Create an AWS Secrets Manager secret. 
 
  
   
   Secret Name 
   JSON Structure 
   
   
   openweather_maps_keys 
   {" openweather_key": "YOUR_API_KEY"} 
   
   
   google_search_keys 
   {"cse_id": "YOUR_SEARCH_ENGINE_ID", "google_api_key": "YOUR_API_KEY"} 
   
   
   paapi_keys 
   {"paapi_public": "YOUR_PUBLIC_KEY", "paapi_secret": "YOUR_SECRET_KEY"} 
   
  
 
Configure environment variables 
Create a .env file in the project root with your configuration: 
 
 STACK_NAME=TravelAssistantAgent

# Optional: Create Bedrock Knowledge Base with documents
KB_DOCS_PATH = Path/to/your/documents/folder
# Optional: Enable/disable Product Search features with PAAPI
USE_PAAPI=false 
 
Deploy the stack 
If this is your first time using the AWS CDK in your AWS account and AWS Region, bootstrap your environment: 
 
 cdk bootstrap 
 
Deploy the solution using the provided script, which creates the required AWS resources, including Lambda functions, DynamoDB tables, and API configurations: 
 
 sh deploy.sh 
 
Access your application 
When the deployment is complete, open the AWS CloudFormation console and open your stack. On the Outputs tab, note the following values: 
 
 WebAppDomain â€“ Your applicationâ€™s URL 
 UserPoolId â€“ Required for user management 
 UserPoolClientId â€“ Used for authentication 
 
 
Create an Amazon Cognito user 
Complete the following steps to create an Amazon Cognito user: 
 
 On the Amazon Cognito console, choose User pools in the navigation pane. 
 Choose your user pool. 
 Choose Users in the navigation pane, then choose Create user. 
 
 
 
 For Email address, enter an email address, and select Mark email address as verified. 
 For Password, enter a temporary password. 
 Choose Create user. 
 
 
You can use these credentials to access your application at the WebAppDomain URL. 
Test the solution 
To test the agentâ€™s capabilities, we created a business traveler persona and simulated a typical travel planning conversation flow. We focused on routing, function calling accuracy, response quality, and latency metrics. The agentâ€™s routing system directs the user questions to the appropriate specialized node (for example, searching for accommodations, checking weather conditions, or suggesting travel products). Throughout the conversation, the agent maintains the context of previously discussed details, so it can build upon earlier responses while providing relevant new information. For example, after discussing travel destination, the agent can naturally incorporate this into subsequent weather and packing list recommendations. 
The following screenshots demonstrate the end-user experience, while the underlying API interactions are handled seamlessly on the backend. The complete implementation details, including Lambda function code and API integration patterns, are available in our GitHub repository. 

 
 
The solution demonstrates personalization capabilities using sample user profiles stored in DynamoDB, containing upcoming trips and travel preferences. In production deployments, these profiles can be integrated with existing customer databases and reservation systems to provide a personalized assistance. 

 
 

 
 

 
 
The product recommendations shown are live links to actual items available on Amazon.com, so the user can explore or purchase these products directly. The user can choose a link to check out the product, or choose Add to Amazon Cart to see the items in their shopping cart. 

 
 
Clean up 
After you are done experimenting with the travel assistant, you can locate the CloudFormation stack on the AWS CloudFormation console and delete it. This will delete the resources you created. 
Conclusion 
Our travel planning assistant agent demonstrates a practical application built by Amazon Nova and LangGraph for solving real-world business challenges. The system streamlines complex travel planning while naturally integrating product recommendations through specialized processing nodes and real-time data integration. Amazon Nova Lite models showed reasonable performance at task orchestration, and Amazon Nova Pro performed well for more complex function calling operations. Looking ahead, this framework could be implemented with more dynamic orchestration systems such as ReAct. To build your own implementation, explore our code samples in the GitHub repository. 
For those looking to deepen their understanding of LLM-powered agents, AWS provides extensive resources on building intelligent systems. The Amazon Bedrock Agents documentation offers insights into automating multistep tasks with FMs, and the AWS Bedrock Agent Samples GitHub repo provides guidance for implementing multiple agent applications using Amazon Bedrock. 
 
About the authors 
Isaac Privitera is a Principal Data Scientist with the AWS Generative AI Innovation Center, where he develops bespoke generative AI-based solutions to address customersâ€™ business problems. His primary focus lies in building responsible AI systems, using techniques such as RAG, multi-agent systems, and model fine-tuning. When not immersed in the world of AI, Isaac can be found on the golf course, enjoying a football game, or hiking trails with his loyal canine companion, Barry. 
Ryan Razkenari is a Deep Learning Architect at the AWS Generative AI Innovation Center, where he uses his expertise to create cutting-edge AI solutions. With a strong background in AI and analytics, he is passionate about building innovative technologies that address real-world challenges for AWS customers. 
Sungmin Hong is a Senior Applied Scientist at the AWS Generative AI Innovation Center, where he helps expedite a variety of use cases for AWS customers. Before joining Amazon, Sungmin was a postdoctoral research fellow at Harvard Medical School. He holds a PhD in Computer Science from New York University. Outside of work, Sungmin enjoys hiking, reading, and cooking.
â€¢ Introducing Amazon Bedrock AgentCore Gateway: Transforming enterprise AI agent tool development
  To fulfill their tasks, AI Agents need access to various capabilities including tools, data stores, prompt templates, and other agents. As organizations scale their AI initiatives, they face an exponentially growing challenge of connecting each agent to multiple tools, creating an MÃ—N integration problem that significantly slows development and increases complexity. 
Although protocols such as Model Context Protocol (MCP) and Agent2Agent (A2A) have emerged to address interoperability, implementing these solutions requires substantial engineering effort. Organizations must build MCP servers, convert existing APIs, manage infrastructure, build intelligent tools discovery, and implement security controls, all that while maintaining these integrations over time as protocols rapidly evolve and new major versions are released. As deployments grow to hundreds of agents and thousands of tools, enterprises need a more scalable and manageable solution. 
Introducing Amazon Bedrock AgentCore Gateway 
Weâ€™re excited to announce Amazon Bedrock AgentCore Gateway, a fully managed service that revolutionizes how enterprises connect AI agents with tools and services. AgentCore Gateway serves as a centralized tool server, providing a unified interface where agents can discover, access, and invoke tools. 
Built with native support for the MCP, Gateway enables seamless agent-to-tool communication while abstracting away security, infrastructure, and protocol-level complexities. This service provides zero-code MCP tool creation from APIs and AWS Lambda functions, intelligent tool discovery, built-in inbound and outbound authorization, and serverless infrastructure for MCP servers. You can focus on building intelligent agent experiences rather than managing connectivity with tools and services. The following diagram illustrates the AgentCore Gateway workflow. 
 
Key capabilities of Amazon Bedrock AgentCore Gateway 
The Amazon Bedrock AgentCore Gateway introduces a comprehensive set of capabilities designed to revolutionize tool integration for AI agents. At its core, Gateway offers powerful and secure API integration functionality that transforms existing REST APIs into MCP servers. This integration supports both OpenAPI specifications and Smithy models, so organizations can seamlessly convert their enterprise APIs into MCP-compatible tools. Beyond API integration, Gateway provides built-in support for Lambda functions so developers can connect their serverless computing resources as tools with defined schemas. Gateway provides the following key capabilities: 
 
 Security Guard â€“ Manages OAuth authorization so only valid users and agents can access tools and resources. We will dive deeper into security in the following section. 
 Translation â€“ Converts agent requests using protocols such as MCP into API requests and Lambda invocations, alleviating the need to manage protocol integration or version support. 
 Composition â€“ Combines multiple APIs, functions, and tools into a single MCP endpoint for streamlined agent access. 
 Target extensibility â€“ An AgentCore gateway is a central access point that serves as a unified interface for AI agents to discover and interact with tools. It handles authentication, request routing, and protocol translation between MCP and your APIs. Each gateway can manage multiple targets. A target represents a backend service or group of APIs that you want to expose as tools to AI agents. Targets can be AWS Lambda functions, OpenAPI specifications, or Smithy models. Each target can expose multiple tools, and Gateway automatically handles the conversion between MCP and the targetâ€™s built-in protocol. Gateway supports streamable http transport. 
 Infrastructure Manager â€“ As a fully managed service, Gateway removes the burden of infrastructure management from organizations. It provides comprehensive infrastructure with built-in security features and robust observability capabilities. Teams no longer need to worry about hosting concerns, scaling issues, or maintaining the underlying infrastructure. The service automatically handles these aspects, providing reliable performance and seamless scaling as demand grows. 
 Semantic Tool Selection â€“ Intelligent tool discovery represents another core capability of Gateway. As organizations scale to hundreds or thousands of tools, discovering the right tool becomes increasingly challenging for AI agents. Moreover, when agents are presented with too many tools simultaneously, they can experience something called â€œtool overload,â€ leading to hallucinations, incorrect tool selections, or inefficient execution paths that significantly impact performance. Gateway addresses these challenges by providing a special built-in tool named 'x_amz_bedrock_agentcore_search' that can be accessed using the standard MCP tools and call operation. 
 
 
Security and authentication 
Gateway implements a sophisticated dual-sided security architecture that handles both inbound access to Gateway itself and outbound connections to target services. 
For inbound requests, Gateway follows the MCP authorization specification, using OAuth-based authorization to validate and authorize incoming tool calls. Gateway functions as an OAuth resource server. This means it can work with the OAuth Identity Provider your organization might useâ€“whether thatâ€™s Amazon Cognito, Okta, Auth0, or your own OAuth provider. When you create a gateway, you can specify multiple approved client IDs and audiences, giving you granular control over which applications and agents can access your tools. The Gateway validates incoming requests against your OAuth provider, supporting both authorization code flow (3LO) and client credentials flow (2LO, commonly used for service-to-service communication). 
The outbound security model is equally flexible but varies by target type: 
For AWS Lambda and Smithy model targets, AgentCore Gateway uses AWS Identity and Access Management (IAM) based authorization. The gateway assumes an IAM role you configure, which can have precisely scoped permissions for each target service. This integrates smoothly with existing AWS security practices and IAM policies. 
For OpenAPI targets (REST APIs), Gateway supports two authentication methods: 
 
 API key â€“ You can configure the key to be sent in either headers or query parameters with customizable parameter names 
 OAuth token for 2LO â€“ For outbound OAuth authentication to target APIs, Gateway supports two-legged OAuth (2LO) client credentials grant type, enabling secure machine-to-machine communications without user interaction 
 
Credentials are securely managed through AgentCore Identityâ€™s resource credentials provider. Each target is associated with exactly one authentication configuration, facilitating clear security boundaries and audit trails. AgentCore Identity handles the complex security machinery while presenting a clean, simple interface to developers. You configure security one time during setup, and Gateway handles the token validation, outbound token caching (through AgentCore Identity), and secure communication from there. 
Get started with Amazon Bedrock AgentCore Gateway 
You can create gateways and add targets through multiple interfaces: 
 
 AWS SDK for Python (Boto3) 
 AWS Management Console 
 AWS Command Line Interface (AWS CLI) 
 AgentCore starter toolkit for fast and straightforward setup 
 
The following practical examples and code snippets demonstrate the process of setting up and using Amazon Bedrock AgentCore Gateway. 
Create a gateway 
To create a gateway, use Amazon Cognito for inbound auth using the AWS Boto3: 
 
 gateway_client = boto3.client('bedrock-agentcore-control')
auth_config = {
&nbsp;&nbsp; &nbsp;"customJWTAuthorizer": { 
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"allowedClients": '&lt;cognito_client_id&gt;â€˜, # Client MUST match with the ClientId configured in Cognito.
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"discoveryUrl": '&lt;cognito_oauth_discovery_url&gt;'
&nbsp;&nbsp; &nbsp;}
}
create_response = gateway_client.create_gateway(name='DemoGateway',
&nbsp;&nbsp; &nbsp;roleArn = '&lt;IAM Role&gt;' # The IAM Role must have permissions to create/list/get/delete Gateway 
&nbsp;&nbsp; &nbsp;protocolType='MCP',
&nbsp;&nbsp; &nbsp;authorizerType='CUSTOM_JWT',
&nbsp;&nbsp; &nbsp;authorizerConfiguration=auth_config, 
&nbsp;&nbsp; &nbsp;description='Demo AgentCore Gateway'
)
# Values with &lt; &gt; needs to be replaced with real values 
 
Here is the reference to control plane and data plane APIs for Amazon Bedrock AgentCore. 
Create gateway targets 
Create a target for an existing API using OpenAPI specification with API key as an outbound auth: 
 
 # Create outbound credentials provider in AgentCore Identity
acps&nbsp;&nbsp;boto3client(service_name"bedrock-agentcore-control")

responseacpscreate_api_key_credential_provider(
name"APIKey",
apiKey"&lt;your secret API key"
)

credentialProviderARN&nbsp;&nbsp;response['credentialProviderArn']

# Specify OpenAPI spec file via S3 or inline
openapi_s3_target_config = {
&nbsp;&nbsp; &nbsp;"mcp": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"openApiSchema": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"s3": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"uri": openapi_s3_uri
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp; &nbsp;}
}

# API Key credentials provider configuration
api_key_credential_config = [
&nbsp;&nbsp; &nbsp;{
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"credentialProviderType" : "API_KEY", 
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"credentialProvider": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"apiKeyCredentialProvider": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"credentialParameterName": "api_key", # Replace this with the name of the api key name expected by the respective API provider. For passing token in the header, use "Authorization"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"providerArn": credentialProviderARN,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"credentialLocation":"QUERY_PARAMETER", # Location of api key. Possible values are "HEADER" and "QUERY_PARAMETER".
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;#"credentialPrefix": " " # Prefix for the token. Valid values are "Basic". Applies only for tokens.
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp;}
&nbsp;&nbsp;]

# Add the OpenAPI target to the gateway
targetname='DemoOpenAPITarget'
response = gateway_client.create_gateway_target(
&nbsp;&nbsp; &nbsp;gatewayIdentifier=gatewayID,
&nbsp;&nbsp; &nbsp;name=targetname,
&nbsp;&nbsp; &nbsp;description='OpenAPI Target with S3Uri using SDK',
&nbsp;&nbsp; &nbsp;targetConfiguration=openapi_s3_target_config,
&nbsp;&nbsp; &nbsp;credentialProviderConfigurations=api_key_credential_config) 
 
Create a target for a Lambda function: 
 
 # Define the lambda target with tool schema. Replace the AWS Lambda function ARN below
lambda_target_config = {
&nbsp;&nbsp;"mcp": {
&nbsp;&nbsp; &nbsp;"lambda": {
&nbsp;&nbsp; &nbsp; &nbsp;"lambdaArn": "&lt;Your AWS Lambda function ARN&gt;",
&nbsp;&nbsp; &nbsp; &nbsp;"toolSchema": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"inlinePayload": [
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;{
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"name": "get_order_tool",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"description": "tool to get the order",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"inputSchema": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"type": "object",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"properties": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"orderId": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"type": "string"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;},
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"required": [
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"orderId"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;]}}]}}}}

# Create outbound auth config. For AWS Lambda function, its always IAM.
credential_config = [ 
&nbsp;&nbsp; &nbsp;{
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"credentialProviderType" : "GATEWAY_IAM_ROLE"
&nbsp;&nbsp; &nbsp;}
]

# Add AWS Lambda target to the gateway
targetname='LambdaUsingSDK'
response = gateway_client.create_gateway_target(
&nbsp;&nbsp; &nbsp;gatewayIdentifier=gatewayID,
&nbsp;&nbsp; &nbsp;name=targetname,
&nbsp;&nbsp; &nbsp;description='Lambda Target using SDK',
&nbsp;&nbsp; &nbsp;targetConfiguration=lambda_target_config,
&nbsp;&nbsp; &nbsp;credentialProviderConfigurations=credential_config) 
 
Use Gateway with different agent frameworks 
Use Gateway with Strands Agents integration: 
 
 
from strands import Agent
import logging

def create_streamable_http_transport():
&nbsp;&nbsp; &nbsp;return streamablehttp_client(gatewayURL,headers={"Authorization": f"Bearer {token}"})

client = MCPClient(create_streamable_http_transport)

with client:
&nbsp;&nbsp; &nbsp;# Call the listTools 
&nbsp;&nbsp; &nbsp;tools = client.list_tools_sync()
&nbsp;&nbsp; &nbsp;# Create an Agent with the model and tools
&nbsp;&nbsp; &nbsp;agent = Agent(model=yourmodel,tools=tools) ## you can replace with any model you like
&nbsp; &nbsp;&nbsp;# Invoke the agent with the sample prompt. This will only invoke &nbsp;MCP listTools and retrieve the list of tools the LLM has access to. The below does not actually call any tool.
&nbsp;&nbsp; &nbsp;agent("Hi , can you list all tools available to you")
&nbsp;&nbsp; &nbsp;# Invoke the agent with sample prompt, invoke the tool and display the response
&nbsp;&nbsp; &nbsp;agent("Check the order status for order id 123 and show me the exact response from the tool") 
 
Use Gateway with LangChain integration: 
 
 from langchain_mcp_adapters.client import MultiServerMCPClient
from langgraph.prebuilt import create_react_agent
from langchain.chat_models import init_chat_model

client = MultiServerMCPClient(
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;{
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"healthcare": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"url": gateway_endpoint,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"transport": "streamable_http",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"headers":{"Authorization": f"Bearer {jwt_token}"}
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp;)
&nbsp;agent = create_react_agent(
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;LLM, 
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;tools, 
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;prompt=systemPrompt
&nbsp;) 
 
Implement semantic search 
You can opt in to semantic search when creating a gateway. It automatically provisions a powerful built-in tool called x_amz_bedrock_agentcore_search that enables intelligent tool discovery through natural language queries. Use the output of the search tool in place of MCPâ€™s list operation for scalable and performant tool discovery. The following diagram illustrates how you can use the MCP search tool. 
 
To enable semantic search, use the following code: 
 
 &nbsp;# Enable semantic search of tools
&nbsp;&nbsp; &nbsp;search_config = {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"mcp": {"searchType": "SEMANTIC", "supportedVersions": ["2025-03-26"]}
&nbsp;&nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp;# Create the gateway
&nbsp;&nbsp; &nbsp;response = agentcore_client.create_gateway(
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;name=gateway_name,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;roleArn=gateway_role_arn,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;authorizerType="CUSTOM_JWT",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;description=gateway_desc,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;protocolType="MCP",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;authorizerConfiguration=auth_config,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;protocolConfiguration=search_config,
&nbsp;&nbsp; &nbsp;)
def tool_search(gateway_endpoint, jwt_token, query):
&nbsp;&nbsp; &nbsp;toolParams = {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"name": "x_amz_bedrock_agentcore_search",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"arguments": {"query": query},
&nbsp;&nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp;toolResp = invoke_gateway_tool(
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;gateway_endpoint=gateway_endpoint, jwt_token=jwt_token, tool_params=toolParams
&nbsp;&nbsp; &nbsp;)
&nbsp;&nbsp; &nbsp;tools = toolResp["result"]["structuredContent"]["tools"]
&nbsp;&nbsp; &nbsp;return tools 
 
To find the entire code sample, visit the Semantic search tutorial in the amazon-bedrock-agentcore-samples GitHub repository. 
Assess Gateway performance using monitoring and observability 
Amazon Bedrock AgentCore Gateway provides observability through integration with Amazon CloudWatch and AWS CloudTrail, for detailed monitoring and troubleshooting of your tool integrations. The observability features include multiple dimensions of gateway operations through detailed metrics: usage metrics (TargetType, IngressAuthType, EgressAuthType, RequestsPerSession), invocation metrics (Invocations, ConcurrentExecutions, Sessions), performance metrics (Latency, Duration, TargetExecutionTime), and error rates (Throttles, SystemErrors, UserErrors). The performance metrics can be analyzed using various statistical methods (Average, Minimum, Maximum, p50, p90, p99) and are tagged with relevant dimensions for granular analysis, including Operation, Resource, and Name . For operational logging, Gateway integrates with CloudTrail to capture both management and data events, providing a complete audit trail of API interactions. The metrics are accessible through both the Amazon Bedrock AgentCore console and CloudWatch console, where you can create custom dashboards, set up automated alerts, and perform detailed performance analysis. 
Best practices 
Gateway offers an enhanced debugging option through the exceptionLevel property, which can be enabled during Gateway creation or updated as shown in the following code example: 
 
 create_response = gateway_client.create_gateway(name='DemoGateway',
&nbsp;&nbsp; &nbsp;roleArn = '&lt;IAM Role&gt;' # The IAM Role must have permissions to create/list/get/delete Gateway 
&nbsp;&nbsp; &nbsp;protocolType='MCP',
&nbsp;&nbsp; &nbsp;authorizerType='CUSTOM_JWT',
&nbsp;&nbsp; &nbsp;authorizerConfiguration=auth_config, 
&nbsp;&nbsp; &nbsp;description='Demo AgentCore Gateway',
    exceptionLevel="DEBUG"   # Debug mode for granular error messages
) 
 
When activated, this feature provides more granular error messages in the content text block (with isError:true) during Gateway testing, facilitating quicker troubleshooting and integration. When documenting and extracting Open APIs for Gateway, focus on clear, natural language descriptions that explain real-world use cases. Include detailed field descriptions, validation rules, and examples for complex data structures while maintaining consistent terminology throughout. For optimal tool discovery, incorporate relevant business domain keywords naturally in descriptions and provide context about when to use each API. Finally, test semantic search effectiveness so tools are discoverable through natural language queries. Regular reviews and updates are essential to maintain documentation quality as APIs evolve.When extracting APIs from larger specifications, identify the core functionality needed for agent tasks, maintain semantic relationships between components, and preserve security definitions. Follow a systematic extraction process: review the full specification, map agent use cases to specific endpoints, extract relevant paths and schemas while maintaining dependencies, and validate the extracted specification.The following are the best practices on grouping your APIs into a Gateway target: 
 
 Start with the use case and group your MCP tools based on the agentic applicationâ€™s business domain similar to domain-driven design principles applicable to the microservices paradigm. 
 You can attach only one resource credentials provider for outbound authorization for the Gateway target. Group the tools based on the outbound authorizer. 
 Group your APIs based on the type of the APIs, that is, OpenAPI, Smithy, or AWS Lambda, serving as a bridge to other enterprise APIs. 
 
When onboarding tools to Gateway, organizations should follow a structured process that includes security and vulnerability checks. Implement a review pipeline that scans API specifications for potential security risks, maintains proper authentication mechanisms, and validates data handling practices. For runtime tool discovery, use the semantic search capabilities in Gateway, but also consider design-time agent-tool mapping for critical workflows to provide predictable behavior. 
Enrich tool metadata with detailed descriptions, usage examples, and performance characteristics to improve discoverability and aid in appropriate tool selection by agents. To maintain consistency across your enterprise, integrate Gateway with a centralized tool registry that serves as a single source of truth. This can be achieved using open source solutions such as the MCP Registry Publisher Tool, which publishes MCP server details to an MCP registry. Regularly synchronize Gatewayâ€™s tool inventory with this central registry for up-to-date and consistent tool availability across your AI landscape. These practices can help maintain a secure, well-organized, and efficiently discoverable tool solution within Gateway, facilitating seamless agent-tool interactions while can align with enterprise governance standards. 
What customers are saying 
Innovaccer, a leading healthcare technology company, shares their experience: 

 â€œAI has massive potential in healthcare, but getting the foundation right is key. Thatâ€™s why weâ€™re building HMCP (Healthcare Model Context Protocol) on Amazon Bedrock AgentCore Gateway, which has been a game-changer, automatically converting our existing APIs into MCP-compatible tools and scaling seamlessly as we grow. It gives us the secure, flexible base we need to make sure AI agents can safely and responsibly interact with healthcare data, tools, and workflows. With this partnership, weâ€™re accelerating AI innovation with trust, compliance, and real-world impact at the core.â€  
 â€”Abhinav Shashank, CEO &amp; Co-founder, Innovaccer
 
Conclusion 
Amazon Bedrock AgentCore Gateway represents a significant advancement in enterprise AI agent development. By providing a fully managed, secure, and scalable solution for tool integration, Gateway enables organizations to accelerate their AI initiatives while maintaining enterprise-grade security and governance. As part of the broader Amazon Bedrock AgentCore suite, Gateway works seamlessly with other capabilities including Runtime, Identity, Code Interpreter, Memory, Browser, and Observability to provide a comprehensive domain for building and scaling AI agent applications. 
For more detailed information and advanced configurations, refer to the code samples on GitHub, the Amazon Bedrock AgentCore Gateway Developer Guide and Amazon AgentCore Gateway pricing. 
 
About the authors 
Dhawal Patel is a Principal Machine Learning Architect at Amazon Web Services (AWS). He has worked with organizations ranging from large enterprises to mid-sized startups on problems related to distributed computing and AI. He focuses on deep learning, including natural language processing (NLP) and computer vision domains. He helps customers achieve high-performance model inference on Amazon SageMaker. 
Mike Liu is a Principal Product Manager at Amazon, where he works at the intersection of agentic AI and foundational model development. He led the product roadmap for Amazon Bedrock Agents and is now helping customers achieve superior performance using model customization on Amazon Nova models. Prior to Amazon, he worked on AI/ML software in Google Cloud and ML accelerators at Intel. 
Kartik Rustagi works as a Software Development Manager in Amazon AI. He and his team focus on enhancing the conversation capability of chat bots powered by Amazon Lex. When not at work, he enjoys exploring the outdoors and savoring different cuisines.
â€¢ Build a scalable containerized web application on AWS using the MERN stack with Amazon Q Developer â€“ Part 1
  The MERN (MongoDB, Express, React, Node.js) stack is a popular JavaScript web development framework. The combination of technologies is well-suited for building scalable, modern web applications, especially those requiring real-time updates and dynamic user interfaces. Amazon Q Developer is a generative AI-powered assistant that improves developer efficiency across the different phases of the software development lifecycle (SDLC). In this two-part blog series, I capture the experience and demonstrate the productivity gains you can achieve by using Amazon Q Developer as a coding assistant to build a scalable MERN stack web application on AWS. The solution forms a solid foundation for you to build a feature rich web application. In my case, using the process outlined in this blog, I extended the MERN stack web application to include real-time video conferencing (using Amazon Chime SDK) and an AI chatbot (invoking Amazon Bedrock foundation models). 
Typically, in the plan phase of the SDLC, time is spent researching approaches and identifying common solution patterns that can deliver on requirements. Using Amazon Q Developer, you can speed up this process by prompting for an approach to deploy a scalable MERN stack web application on AWS. Trained on over 17 years of AWS experience building in the cloud, Amazon Q Developer responses are based on AWS well-architected patterns and best practices. In the design phase, I use the responses from Amazon Q Developer to craft a detailed requirements prompt to generate the code for your MERN stack web application. Then in the build phase, I extend the code to implement a working solution, generate unit tests and conduct an automated code review. 
In part 2 of this blog series, I will use Amazon Q Developer to extend the base MERN stack web application to include a chat user interface (which invokes an agentic workflow based on the Strands Agent SDK and Amazon Bedrock), deploy the solution to AWS using infrastructure as code (IaC), troubleshoot issues and generate the documentation for our solution. 
Walkthrough 
Prerequisites 
To complete the walkthrough in this post, you must have the following: 
 
 An AWS account to deploy the solution components to AWS. 
 AWS Command Line Interface (AWS CLI) installed and configured. 
 Docker Desktop installed. 
 Set up access to Amazon Q Developer by using one of the following two options: 
   
   Amazon Q Developer Free tier â€“ Provides access to explore capabilities before opting for a paid tier and requires an AWS Builder ID profile. 
   Amazon Q Developer Pro tier â€“ Paid subscription with access to additional features. Set up through IAM Identity Center. 
    
 A supported integrated development environment (IDE) including Visual Studio Code and JetBrains IDEs. For more information, follow the instructions for installing the Amazon Q Developer extension or plugin in your IDE. 
 
Sign in to Amazon Q Developer (in your IDE) 
After setting up Amazon Q Developer access tier and installing the Amazon Q extension for your IDE, you can sign in to Amazon Q Developer by using the IDE. 
 
 The first sign-in flow shows the authentication process for the Free tier using an AWS Builder ID. 
 
 
 
 The second sign-in flow shows the authentication process for the Pro tier using a sign-in URL to the AWS access portal (provided by your AWS administrator). 
 
 
 
 After successful authentication, youâ€™ll be presented with an initial chat window to start a conversation with Amazon Q Developer. In the chat input at the bottom, you have options to add additional context for Amazon Q Developer to provide responses such as using the active file or the entire workspace, defining rules for Amazon Q Developer to follow when it generates responses, toggling agentic coding on and off, and selecting your preferred foundation model (Claude Sonnet 4 in our case). 
 
 
With Free Tier, you have access to limited agentic requests per month, access to the latest Claude models and use of Amazon Q Developer in the IDE or CLI. In this post, I use the Pro Tier, which in addition to Free Tier features, also provides increased limits of agentic requests and app transformation, Identity center support and IP indemnity. 
Plan 
In the planning phase, you can prompt for a solution approach to better understand the different components that will make up the MERN stack web application. You would toggle agentic coding off in this phase as you research and understand the best approach. Example planning phase prompt: 
â€œProvide a high-level summary of a solution approach to deploying a scalable MERN stack application on AWS.â€ 
The response from Amazon Q Developer (also shown in the following screenshot) breaks down the solution into the following components: 
 
 Frontend React application 
 Backend NodeJS and Express containerized app running on Amazon ECS Fargate 
 Database using MongoDB or Amazon DocumentDB 
 Core network infrastructure 
 Security 
 Monitoring and operations 
 Continuous integration and delivery (CI/CD) pipeline 
 Performance 
 
 
Design &amp; Build 
After reviewing the solution approach, you can create a more detailed prompt about the web application requirements, which will be used in the feature development capability of Amazon Q Developer to generate the solution components. Turn agentic coding on before submitting the prompt. Example design phase prompt: 
â€œBuild a scalable containerized web application using the MERN stack on AWS, with login and sign-up pages integrated with Amazon Cognito, a landing page that retrieves a list of shops from DocumentDB. I donâ€™t intend to use AWS Amplify. It needs to be a modular design with components that can scale independently, running as containers using ECS and Fargate, highly available across two Availability Zones. I need to build, test and run the MERN stack locally before pushing the solution to AWS.â€  
As shown in the following screenshots, Amazon Q Developer will provide an architecture overview of the solution before going through the build process step by step. I will provide a select number of screenshots for illustration but note that the steps generated by Amazon Q Developer will vary for your solution prompt. 
 
For each file that it creates or updates, Amazon Q Developer gives you the option to review the difference and undo the changes. This is an important step to understand whether the generated code meets your requirements. For example, the snippet below shows an update the Navbar component. 
 
When viewing the diff, you can see that Amazon Q Developer has added a new button class to fix a display issue. 
 
Amazon Q Developer can also execute shell commands. In this case, create the backend and frontend directory. You have the option to â€˜Rejectâ€™ or â€˜Runâ€™ the command. 
 
Hereâ€™s a snippet of Amazon Q Developer creating the authentication service, data model and Dockerfile for the solution. 
 
Another snippet of Amazon Q Developer creating the React frontend. 
 
A snippet of Amazon Q Developer creating the AWS infrastructure components. 
 
Amazon Q Developer then prompts to execute the deployment. 
 
But I noticed that it hasnâ€™t followed my initial prompt to â€œbuild, test and run the MERN stack locally before pushing the solution to AWSâ€, so I provide the following prompt: 
â€œIn my initial prompt, I asked to build, test and run the MERN stack locally before pushing the solution to AWS. 
Amazon Q Developer acknowledges my observation and makes the necessary changes for local deployment. 
 
Next, Amazon Q Developer will build, test and run the MERN stack locally as shown below. 
 
When reviewing the .env file changes, I noticed that the Amazon Cognito properties are not properly set, so provide the following prompt: 
â€œWhen reviewing your .env file changes, I noticed that setting to COGNITO_USER_POOL_ID and COGNITO_CLIENT_ID to local-development is incorrect, as I should be connecting to Amazon Cognito in AWS. And this hasn't been created yet. Additionally, the local deployment has been configured to connect to the local MongoDB container instead of DocumentDB.â€ 
Amazon Q Developer again acknowledges my observation and attempts to fix the issues. These two issues highlight that to effectively use Amazon Q Developer, itâ€™s important to review and challenge the responses provided. 
 
After fixing the issues, Amazon Q Developer updates the README.md to reflect the updated approach and asks if I want to do a quick deployment with mocked authentication or an actual deployment with Amazon Cognito resources. 
 
I choose option B, with real Amazon Cognito resources, so Amazon Q Developer deploys the resources as shown below. 
 
Amazon Q Developer now checks that the frontend, backend and MongoDB containers are running. 
 
Amazon Q Developer also tests that the application is running by executing curl commands to the application endpoints. 
 
After successfully running the commands, Amazon Q Developer provides a summary of the results, with details on how to access and test the application. 
 
Hereâ€™s a diagram showing the locally deployed solution. 
 
Now that the frontend, backend, and MongoDB containers are running, you can access the frontend application Sign In page on http://localhost:3000. 
 
Before logging in, you need to create a user. Choose the Sign Up link to enter an email and password. 
 
After attempting to sign up, I noticed that Amazon Q Developer hasnâ€™t generated the corresponding frontend screen to enter the confirmation code, so I prompt it to fix the issue. Again, the generated code isnâ€™t always perfect, but itâ€™s a good starting point. 
 
After authentication, youâ€™ll be routed to the shops page as shown. 
 
Test 
Now that youâ€™ve built and can run the MERN stack web application locally, you can use Amazon Q Developer to generate unit tests to find defects and improve code quality. I provide the following prompt: 
â€œCan you generate unit tests for the project?â€ 
Amazon Q Developer will then create comprehensive unit tests for the application. 
 
At completion, Amazon Q Developer will provide a summary of the unit tests generated: 
 
Amazon Q Developer also provides instructions for executing the tests: 
 
After executing the unit tests, Amazon Q Developer provides a summary of the results. 
 
Review 
We can now conduct a code review of the MERN stack application by prompting the following: 
â€œCan you do a code review of my project to identify and fix any code issues?â€ 
Amazon Q Developer will perform a code review and identify issues that require attention. 
 
After completing the review, Amazon Q Developer will provide a summary of the critical issues fixed, along with next steps. 
 
Clean up 
To avoid incurring future charges, remove the Amazon Cognito resources that you created. 
Conclusion 
In a traditional SDLC, a lot of time is spent in the different phases researching approaches that can deliver on requirements: iterating over design changes, writing, testing and reviewing code, and configuring infrastructure. Amazon Q Developer is a generative AI-powered assistant that improves developer efficiency across the phases of the SDLC. In this post, you learned about the experience and saw productivity gains you can realize by using Amazon Q Developer as a coding assistant to build a scalable MERN stack web application on AWS. 
In the plan phase, you used Amazon Q Developer to prompt for a solution approach to deploy a scalable MERN stack web application on AWS. Then in the design phase, you used the initial responses from Amazon Q Developer to craft a detailed requirements prompt and generated the code for your MERN stack web application. In the build phase, you customized the code and deployed a working solution locally. In the test phase, Amazon Q Developer generated the unit tests for you to identify bugs early to improve code quality. Finally, in the review phase, you conducted a code review and remediated issues identified. 
In part 2 of this blog series, you will use Amazon Q Developer to extend the base MERN stack web application to include a chat user interface (which invokes an agentic workflow based on the Strands Agent SDK and Amazon Bedrock), deploy the solution to AWS using infrastructure as code (IaC), troubleshoot issues and generate the documentation for our solution. 
 
About the Author 
Bill Chan is an Enterprise Solutions Architect working with large enterprises to craft highly scalable, flexible, and resilient cloud architectures. He helps organizations understand best practices around advanced cloud-based solutions, and how to migrate existing workloads to the cloud. He enjoys relaxing with family and shooting hoops.
â€¢ Optimizing Salesforceâ€™s model endpoints with Amazon SageMaker AI inference components
  This post is a joint collaboration between Salesforce and AWS and is being cross-published on both the Salesforce Engineering Blog and the AWS Machine Learning Blog. 
The Salesforce AI Platform Model Serving team is dedicated to developing and managing services that power large language models (LLMs) and other AI workloads within Salesforce. Their main focus is on model onboarding, providing customers with a robust infrastructure to host a variety of ML models. Their mission is to streamline model deployment, enhance inference performance and optimize cost efficiency, ensuring seamless integration into Agentforce and other applications requiring inference. Theyâ€™re committed to enhancing the model inferencing performance and overall efficiency by integrating state-of-the-art solutions and collaborating with leading technology providers, including open source communities and cloud services such as Amazon Web Services (AWS) and building it into a unified AI platform. This helps ensure Salesforce customers receive the most advanced AI technology available while optimizing the cost-performance of the serving infrastructure. 
In this post, we share how the Salesforce AI Platform team optimized GPU utilization, improved resource efficiency and achieved cost savings using Amazon SageMaker AI, specifically inference components. 
The challenge with hosting models for inference: Optimizing compute and cost-to-serve while maintaining performance 
Deploying models efficiently, reliably, and cost-effectively is a critical challenge for organizations of all sizes. The Salesforce AI Platform team is responsible for deploying their proprietary LLMs such as CodeGen and XGen on SageMaker AI and optimizing them for inference. Salesforce has multiple models distributed across single model endpoints (SMEs), supporting a diverse range of model sizes from a few gigabytes (GB) to 30 GB, each with unique performance requirements and infrastructure demands. 
The team faced two distinct optimization challenges. Their larger models (20â€“30 GB) with lower traffic patterns were running on high-performance GPUs, resulting in underutilized multi-GPU instances and inefficient resource allocation. Meanwhile, their medium-sized models (approximately 15 GB) handling high-traffic workloads demanded low-latency, high-throughput processing capabilities. These models often incurred higher costs due to over-provisioning on similar multi-GPU setups. Hereâ€™s a sample illustration of Salesforceâ€™s large and medium SageMaker endpoints and where resources are under-utilized: 
 
Operating on Amazon EC2 P4d instances today, with plans to use the latest generation P5en instances equipped with NVIDIA H200 Tensor Core GPUs, the team sought an efficient resource optimization strategy that would maximize GPU utilization across their SageMaker AI endpoints while enabling scalable AI operations and extracting maximum value from their high-performance instancesâ€”all without compromising performance or over-provisioning hardware. 
This challenge reflects a critical balance that enterprises must strike when scaling their AI operations: maximizing the performance of sophisticated AI workloads while optimizing infrastructure costs and resource efficiency. Salesforce needed a solution that would not only resolve their immediate deployment challenges but also create a flexible foundation capable of supporting their evolving AI initiatives. 
To address these challenges, the Salesforce AI Platform team used SageMaker AI inference components that enabled deployment of multiple foundation models (FMs) on a single SageMaker AI endpoint with granular control over the number of accelerators and memory allocation per model. This helps improve resource utilization, reduces model deployment costs, and lets you scale endpoints together with your use cases. 
Solution: Optimizing model deployment with Amazon SageMaker AI inference components 
With Amazon SageMaker AI inference components, you can deploy one or more FMs on the same SageMaker AI endpoint and control how many accelerators and how much memory is reserved for each FM. This helps to improve resource utilization, reduces model deployment costs, and lets you scale endpoints together with your use cases. For each FM, you can define separate scaling policies to adapt to model usage patterns while further optimizing infrastructure costs. Hereâ€™s the illustration of Salesforceâ€™s large and medium SageMaker endpoints after utilization has been improved with Inference Components: 
 
An inference component abstracts ML models and enables assigning CPUs, GPU, and scaling policies per model. Inference components offer the following benefits: 
 
 SageMaker AI will optimally place and pack models onto ML instances to maximize utilization, leading to cost savings. 
 Each model scales independently based on custom configurations, providing optimal resource allocation to meet specific application requirements. 
 SageMaker AI will scale to add and remove instances dynamically to maintain availability while keeping idle compute to a minimum. 
 Organizations can scale down to zero copies of a model to free up resources for other models or specify to keep important models always loaded and ready to serve traffic for critical workloads. 
 
Configuring and managing inference component endpoints 
You create the SageMaker AI endpoint with an endpoint configuration that defines the instance type and initial instance count for the endpoint. The model is configured in a new construct, an inference component. Here, you specify the number of accelerators and amount of memory you want to allocate to each copy of a model, together with the model artifacts, container image, and number of model copies to deploy. 
As inference requests increase or decrease, the number of copies of your inference components can also scale up or down based on your auto scaling policies. SageMaker AI will handle the placement to optimize the packing of your models for availability and cost. 
In addition, if you enable managed instance auto scaling, SageMaker AI will scale compute instances according to the number of inference components that need to be loaded at a given time to serve traffic. SageMaker AI will scale up the instances and pack your instances and inference components to optimize for cost while preserving model performance. 
Refer to Reduce model deployment costs by 50% on average using the latest features of Amazon SageMaker for more details on how to use inference components. 
How Salesforce used Amazon SageMaker AI inference components 
Salesforce has several different proprietary models such as CodeGen originally spread across multiple SMEs. CodeGen is Salesforceâ€™s in-house open source LLM for code understanding and code generation. Developers can use the CodeGen model to translate natural language, such as English, into programming languages, such as Python. Salesforce developed an ensemble of CodeGen models (Inline for automatic code completion, BlockGen for code block generation, and FlowGPT for process flow generation) specifically tuned for the Apex programming language. The models are being used in ApexGuru, a solution within the Salesforce platform that helps Salesforce developers tackle critical anti-patterns and hotspots in their Apex code. 
Inference components enable multiple models to share GPU resources efficiently on the same endpoint. This consolidation not only delivers reduction in infrastructure costs through intelligent resource sharing and dynamic scaling, it also reduces operational overhead with lesser endpoints to manage. For their CodeGen ensemble models, the solution enabled model-specific resource allocation and independent scaling based on traffic patterns, providing optimal performance while maximizing infrastructure utilization. 
To expand hosting options on SageMaker AI without affecting stability, performance, or usability, Salesforce introduced inference component endpoints alongside the existing SME. 
This hybrid approach uses the strengths of each. SMEs provide dedicated hosting for each model and predictable performance for critical workloads with consistent traffic patterns, and inference components optimize resource utilization for variable workloads through dynamic scaling and efficient GPU sharing. 
The Salesforce AI Platform team created a SageMaker AI endpoint with the desired instance type and initial instance count for the endpoint to handle their baseline inference requirements. Model packages are then attached dynamically, spinning up individual containers as needed. They configured each model, for example, BlockGen and TextEval models as individual inference components specifying precise resource allocations, including accelerator count, memory requirements, model artifacts, container image, and number of model copies to deploy. With this approach, Salesforce could efficiently host multiple model variants on the same endpoint while maintaining granular control over resource allocation and scaling behaviors. 
By using the auto scaling capabilities, inference components can set up endpoints with multiple copies of models and automatically adjust GPU resources as traffic fluctuates. This allows each model to dynamically scale up or down within an endpoint based on configured GPU limits. By hosting multiple models on the same endpoint and automatically adjusting capacity in response to traffic fluctuations, Salesforce was able to significantly reduce the costs associated with traffic spikes. This means that Salesforce AI models can handle varying workloads efficiently without compromising performance. The graphic below shows Salesforceâ€™s endpoints before and after the models were deployed with inference components: 
 
This solution has brought several key benefits: 
 
 Optimized resource allocation â€“ Multiple models now efficiently share GPU resources, eliminating unnecessary provisioning while maintaining optimal performance. 
 Cost savings â€“ Through intelligent GPU resource management and dynamic scaling, Salesforce achieved significant reduction in infrastructure costs while eliminating idle compute resources. 
 Enhanced performance for smaller models â€“ Smaller models now use high-performance GPUs to meet their latency and throughput needs without incurring excessive costs. 
 
By refining GPU allocation at the model level through inference components, Salesforce improved resource efficiency and achieved a substantial reduction in operational cost while maintaining the high-performance standards their customers expect across a wide range of AI workloads. The cost savings are substantial and open up new opportunities for using high-end, expensive GPUs in a cost-effective manner. 
Conclusion 
Through their implementation of Amazon SageMaker AI inference components, Salesforce has transformed their AI infrastructure management, achieving up to an eight-fold reduction in deployment and infrastructure costs while maintaining high performance standards. The team learned that intelligent model packing and dynamic resource allocation were keys to solving their GPU utilization challenges across their diverse model portfolio. This implementation has transformed performance economics, allowing smaller models to use high performance GPUs, providing high throughput and low latency without the traditional cost overhead. 
Today, their AI platform efficiently serves both large proprietary models such as CodeGen and smaller workloads on the same infrastructure, with optimized resource allocation ensuring high-performance delivery. With this approach, Salesforce can maximize the utilization of compute instances, scale to hundreds of models, and optimize costs while providing predictable performance. This solution has not only solved their immediate challenges of optimizing GPU utilization and cost management but has also positioned them for future growth. By establishing a more efficient and scalable infrastructure foundation, Salesforce can now confidently expand their AI offerings and explore more advanced use cases with expensive, high-performance GPUs such as P4d, P5, and P5en, knowing they can maximize the value of every computing resource. This transformation represents a significant step forward in their mission to deliver enterprise-grade AI solutions while maintaining operational efficiency and cost-effectiveness. 
Looking ahead, Salesforce is poised to use the new Amazon SageMaker AI rolling updates capability for inference component endpoints, a feature designed to streamline updates for models of different sizes while minimizing operational overhead. This advancement will enable them to update their models batch by batch, rather than using the traditional blue/green deployment method, providing greater flexibility and control over model updates while using minimal extra instances, rather than requiring doubled instances as in the past. By implementing these rolling updates alongside their existing dynamic scaling infrastructure and incorporating real-time safety checks, Salesforce is building a more resilient and adaptable AI platform. This strategic approach not only provides cost-effective and reliable deployments for their GPU-intensive workloads but also sets the stage for seamless integration of future AI innovations and model improvements. 
Check out How Salesforce achieves high-performance model deployment with Amazon SageMaker AI to learn more. For more information on how to get started with SageMaker AI, refer to Guide to getting set up with Amazon SageMaker AI. To learn more about Inference Components, refer to Amazon SageMaker adds new inference capabilities to help reduce foundation model deployment costs and latency. 
 
About the Authors 
Rishu Aggarwal is a Director of Engineering at Salesforce based in Bangalore, India. Rishu leads the Salesforce AI Platform Model Serving Engineering team in solving the complex problems of inference optimizations and deployment of LLMs at scale within the Salesforce ecosystem. Rishu is a staunch Tech Evangelist for AI and has deep interests in Artificial Intelligence, Generative AI, Neural Networks and Big Data. 
Rielah De Jesus is a Principal Solutions Architect at AWS who has successfully helped various enterprise customers in the DC, Maryland, and Virginia area move to the cloud. In her current role, she acts as a customer advocate and technical advisor focused on helping organizations like Salesforce achieve success on the AWS platform. She is also a staunch supporter of women in IT and is very passionate about finding ways to creatively use technology and data to solve everyday challenges. 
Pavithra Hariharasudhan is a Senior Technical Account Manager and Enterprise Support Lead at AWS, supporting leading AWS Strategic customers with their global cloud operations. She assists organizations in resolving operational challenges and maintaining efficient AWS environments, empowering them to achieve operational excellence while accelerating business outcomes. 
Ruchita Jadav is a Senior Member of Technical Staff at Salesforce, with over 10 years of experience in software and machine learning engineering. Her expertise lies in building scalable platform solutions across the retail and CRM domains. At Salesforce, she leads initiatives focused on model hosting, inference optimization, and LLMOps, enabling efficient and scalable deployment of AI and large language models. She holds a Bachelor of Technology in Electronics &amp; Communication from Gujarat Technological University (GTU). 
Marc Karp is an ML Architect with the Amazon SageMaker Service team. He focuses on helping customers design, deploy, and manage ML workloads at scale. In his spare time, he enjoys traveling and exploring new places.
â€¢ Building a RAG chat-based assistant on Amazon EKS Auto Mode and NVIDIA NIMs
  Chat-based assistants powered by Retrieval Augmented Generation (RAG) are transforming customer support, internal help desks, and enterprise search, by delivering fast, accurate answers grounded in your own data. With RAG, you can use a ready-to-deploy foundation model (FM) and enrich it with your own data, making responses relevant and context-aware without the need for fine-tuning or retraining. Running these chat-based assistants on Amazon Elastic Kubernetes Service (Amazon EKS) gives you the flexibility to use a variety of FMs, retaining full control over your data and infrastructure. 
Amazon EKS scales with your workload and is cost-efficient for both steady and fluctuating demand. Because EKS is certified Kubernetes-conformant, it is compatible with existing applications running on a standard Kubernetes environment, whether hosted on on-premises data centers or public clouds. For your data plane, you can take advantage of a wide range of compute options, including CPUs, GPUs, AWS purpose-built AI chips (AWS Inferentia and AWS Trainium) and ARM-based CPU architectures (AWS Graviton), to match performance and cost requirements. Such flexibility makes Amazon EKS an ideal candidate for running heterogeneous workloads because you can compose different compute substrates, within the same cluster, to optimize both performance and cost efficiency. 
NVIDIA NIM microservices consist of microservices that deploy and serve FMs, integrating with AWS services such as Amazon Elastic Compute Cloud (Amazon EC2), Amazon EKS, and Amazon SageMaker. NIM microservices are distributed as Docker containers and are available through the NVIDIA NGC Catalog. Deploying GPU-accelerated models manually requires you to select and configure runtimes such as PyTorch or TensorFlow, set up inference servers such as Triton, implement model optimizations, and troubleshoot compatibility issues. This takes engineering time and expertise. NIM microservices eliminate this complexity by automating these technical decisions and configurations for you. 
The NVIDIA NIM Operator is a Kubernetes management tool that facilitates the operation of model-serving components and services. It handles large language models (LLMs), embedders, and other model types through NVIDIA NIM microservices within Kubernetes environments. The Operator streamlines microservice management through three primary custom resources. First, the NIMCache resource facilitates model downloading from NGC and network storage persistence. This enables multiple microservice instances to share a single cached model, improving microservice startup time. Second, the NIMService resource manages individual NIM microservices, creating Kubernetes deployments within specified namespaces. Third, the NIMPipeline resource functions as an orchestrator for multiple NIM service resources, allowing coordinated management of service groups. This architecture enables efficient operation and lifecycle management, with particular emphasis on reducing inference latency through model caching and supporting automated scaling capabilities. 
NVIDIA NIM, coupled with the NVIDIA NIM Operator, provide a streamlined solution to address the deployment complexities stated in the opening. In this post, we demonstrate the implementation of a practical RAG chat-based assistant using a comprehensive stack of modern technologies. The solution uses NVIDIA NIMs for both LLM inference and text embedding services, with the NIM Operator handling their deployment and management. The architecture incorporates Amazon OpenSearch Serverless to store and query high-dimensional vector embeddings for similarity search. 
The underlying Kubernetes infrastructure of the solution is provided by EKS Auto Mode, which supports GPU-accelerated Amazon Machine Images (AMIs) out of the box. These images include the NVIDIA device plugin, the NVIDIA container toolkit, precompiled NVIDIA kernel drivers, the Bottlerocket operating system, and Elastic Fabric Adapter (EFA) networking. You can use Auto Mode with Accelerated AMIs to spin up GPU instances, without manually installing and configuring GPU software components. Simply specify GPU-based instance types when creating Karpenter NodePools, and EKS Auto Mode will launch GPU-ready worker nodes to run your accelerated workloads. 
Solution overview 
The following architecture diagram shows how NVIDIA NIM microservices running on Amazon EKS Auto Mode power our RAG chat-based assistant solution. The design, shown in the following diagram, combines GPU-accelerated model serving with vector search in Amazon OpenSearch Serverless, using the NIM Operator to manage model deployment and caching through persistent Amazon Elastic File System (Amazon EFS) storage. 
 
Solution diagram (numbers indicate steps in the solution walkthrough section) 
The solution follows these high-level steps: 
 
 Create an EKS cluster 
 Set up Amazon OpenSearch Serverless 
 Create an EFS file system and set up necessary permissions 
 Create Karpenter GPU NodePool 
 Install NVIDIA Node Feature Discovery (NFD) and NIM Operator 
 Create nim-service namespace and NVIDIA secrets 
 Create&nbsp;NIMCaches 
 Create NIMServices 
 
Solution walkthrough 
In this section, we walk through the implementation of this RAG chat-based assistant solution step by step. We create an EKS cluster, configure Amazon OpenSearch Serverless and EFS storage, set up GPU-enabled nodes with Karpenter, deploy NVIDIA components for model serving, and finally integrate a chat-based assistant client using Gradio and LangChain. This end-to-end setup demonstrates how to combine LLM inference on Kubernetes with vector search capabilities, forming the foundation for a scalable, production-grade systemâ€”pending the addition of monitoring, auto scaling, and reliability features. 
Prerequisites 
To begin, ensure you have installed and set up the following required tools: 
 
 AWS CLI (version aws-cli/2.27.11 or later) 
 kubectl 
 eksctl (use version v0.195.0 or later to support Auto Mode) 
 Helm 
 
These tools need to be properly configured according to the Amazon EKS setup documentation. 
Clone the reference repository and cd into the root folder: 
 
 git clone https://github.com/aws-samples/sample-rag-chatbot-nim
cd sample-rag-chatbot-nim/infra 
 
Environment setup 
You need an NGC API key to authenticate and download NIM models. To generate the key, you can enroll (for free) in the NVIDIA Developer Program and then follow the NVIDIA guidelines. 
Next, set up a few environment variables (replace the values with your information): 
 
 export CLUSTER_NAME=automode-nims-blog-cluster
export AWS_DEFAULT_REGION={your region}
export NVIDIA_NGC_API_KEY={your key} 
 
Pattern deployment 
To perform the solution, complete the steps in the following sections. 
Create an EKS cluster 
Deploy the EKS cluster using EKS Auto Mode, with eksctl : 
 
 CHATBOT_SA_NAME=${CLUSTER_NAME}-client-service-account
IAM_CHATBOT_ROLE=${CLUSTER_NAME}-client-eks-pod-identity-role

cat &lt;&lt; EOF | eksctl create cluster -f -
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
&nbsp;&nbsp;name: ${CLUSTER_NAME}
&nbsp;&nbsp;region: ${AWS_DEFAULT_REGION}

autoModeConfig:
&nbsp;&nbsp;enabled: true

iam:
&nbsp;&nbsp;podIdentityAssociations:
&nbsp;&nbsp; &nbsp;- namespace: default
&nbsp;&nbsp; &nbsp; &nbsp;serviceAccountName: ${CHATBOT_SA_NAME}
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; createServiceAccount: true
&nbsp;&nbsp; &nbsp; &nbsp;roleName: ${IAM_CHATBOT_ROLE}
&nbsp;&nbsp; &nbsp; &nbsp;permissionPolicy:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;Version: "2012-10-17"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;Statement:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;- Effect: Allow
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Action:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;- "aoss:*"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Resource: "*"

addons:
- name: aws-efs-csi-driver
&nbsp;&nbsp;useDefaultPodIdentityAssociations: true
EOF 
 
Pod Identity Associations connect Kubernetes service accounts to AWS Identity and Access Management (IAM) roles, allowing pods to access AWS services securely. In this configuration, a service account will be created and associated with an IAM role, granting it full permissions to OpenSearch Serverless (in a production environment, restrict privileges according to the principle of least privilege). 
NIMCaches require volume AccessMode: ReadWriteMany. Amazon Elastic Block Store (Amazon EBS) volumes provided by EKS Auto Mode arenâ€™t suitable because they support ReadWriteOnce only and canâ€™t be mounted by multiple nodes. Storage options that support AccessMode: ReadWriteMany include Amazon EFS, as shown in this example, or Amazon FSx for Lustre, which offers higher performance for workloads with greater throughput or latency requirements. 
The preceding command will take a few minutes to be completed. When itâ€™s completed, eksctl configures your kubeconfig and points it to the new cluster. You can validate that the cluster is up and running and that the EFS addon is installed by entering the following command: 
 
 kubectl get pods --all-namespaces 
 
Expected output: 
 
 NAMESPACE &nbsp; &nbsp; NAME &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;READY &nbsp; STATUS &nbsp; &nbsp;RESTARTS &nbsp; AGE
kube-system &nbsp; efs-csi-controller-55b8dd6f57-wpzbg &nbsp; 3/3 &nbsp; &nbsp; Running &nbsp; 0 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;3m7s
kube-system &nbsp; efs-csi-controller-55b8dd6f57-z2gzc &nbsp; 3/3 &nbsp; &nbsp; Running &nbsp; 0 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;3m7s
kube-system &nbsp; efs-csi-node-6k5kz &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;3/3 &nbsp; &nbsp; Running &nbsp; 0 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;3m7s
kube-system &nbsp; efs-csi-node-pvv2v &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;3/3 &nbsp; &nbsp; Running &nbsp; 0 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;3m7s
kube-system &nbsp; metrics-server-6d67d68f67-7x4tg &nbsp; &nbsp; &nbsp; 1/1 &nbsp; &nbsp; Running &nbsp; 0 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;6m15s
kube-system &nbsp; metrics-server-6d67d68f67-l4xv6 &nbsp; &nbsp; &nbsp; 1/1 &nbsp; &nbsp; Running &nbsp; 0 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;6m15s 
 
Set up Amazon OpenSearch Serverless 
A vector database stores and searches through numerical representations of text (embeddings). Such a component is essential in RAG chat-based assistant architectures because it facilitates finding relevant information related to a user question based on semantic similarity rather than exact keyword matches. 
We use Amazon OpenSearch Service as the vector database. OpenSearch Service provides a managed solution for deploying, operating, and scaling OpenSearch clusters within AWS Cloud infrastructure. As part of this service, Amazon OpenSearch Serverless offers an on-demand configuration that automatically handles scaling to match your applicationâ€™s requirements. 
First, using AWS PrivateLink, create a private connection between the clusterâ€™s Amazon Virtual Private Cloud (Amazon VPC) connection&nbsp;and Amazon OpenSearch Serverless.&nbsp;This keeps traffic within the AWS network and avoids public internet routing. 
Enter the following commands to retrieve the clusterâ€™s virtual private cloud (VPC) ID, CIDR block range, and subnet IDs, and store them in corresponding environment variables: 
 
 VPC_ID=$(aws eks describe-cluster \
&nbsp;&nbsp; &nbsp;--name $CLUSTER_NAME \
&nbsp;&nbsp; &nbsp;--query "cluster.resourcesVpcConfig.vpcId" \
&nbsp;&nbsp; &nbsp;--output text \
&nbsp;&nbsp; &nbsp;--region=$AWS_DEFAULT_REGION)&nbsp;&amp;&amp; \
CIDR_RANGE=$(aws ec2 describe-vpcs \
&nbsp;&nbsp; &nbsp;--vpc-ids $VPC_ID \
&nbsp;&nbsp; &nbsp;--query "Vpcs[].CidrBlock" \
&nbsp;&nbsp; &nbsp;--output text \
&nbsp;&nbsp; &nbsp;--region $AWS_DEFAULT_REGION)&nbsp;&amp;&amp; \
SUBNET_IDS=($(aws eks describe-cluster \
&nbsp;&nbsp; &nbsp;--name $CLUSTER_NAME \
&nbsp;&nbsp; &nbsp;--query "cluster.resourcesVpcConfig.subnetIds[]" \
&nbsp;&nbsp; &nbsp;--region $AWS_DEFAULT_REGION \
&nbsp;&nbsp; &nbsp;--output text)) 
 
Use the following code to create a security group for OpenSearch Serverless in the VPC, add an inbound rule to the security group allowing HTTPS traffic (port 443) from your VPCâ€™s CIDR range, and create an OpenSearch Serverless VPC endpoint connected to the subnets and security group: 
 
 AOSS_SECURITY_GROUP_ID=$(aws ec2 create-security-group \
&nbsp;&nbsp; &nbsp;--group-name ${CLUSTER_NAME}-AOSSSecurityGroup \
&nbsp;&nbsp; &nbsp;--description "${CLUSTER_NAME} AOSS security group" \
&nbsp;&nbsp; &nbsp;--vpc-id $VPC_ID \
&nbsp;&nbsp; &nbsp;--region $AWS_DEFAULT_REGION \
&nbsp;&nbsp; &nbsp;--query 'GroupId' \
&nbsp;&nbsp; &nbsp;--output text)&nbsp;&amp;&amp; \
aws ec2 authorize-security-group-ingress \
&nbsp;&nbsp; &nbsp;--group-id $AOSS_SECURITY_GROUP_ID \
&nbsp;&nbsp; &nbsp;--protocol tcp \
&nbsp;&nbsp; &nbsp;--port 443 \
&nbsp;&nbsp; &nbsp;--region $AWS_DEFAULT_REGION \
&nbsp;&nbsp; &nbsp;--cidr $CIDR_RANGE&nbsp;&amp;&amp; \
VPC_ENDPOINT_ID=$(aws opensearchserverless create-vpc-endpoint \
&nbsp;&nbsp; &nbsp;--name ${CLUSTER_NAME}-aoss-vpc-endpoint \
&nbsp;&nbsp; &nbsp;--subnet-ids "${SUBNET_IDS[@]}" \
&nbsp;&nbsp; &nbsp;--security-group-ids $AOSS_SECURITY_GROUP_ID \
&nbsp;&nbsp; &nbsp;--region $AWS_DEFAULT_REGION \
&nbsp;&nbsp; &nbsp;--vpc-id $VPC_ID \
&nbsp;&nbsp; &nbsp;--query 'createVpcEndpointDetail.id' \
&nbsp;&nbsp; &nbsp;--output text)
 
 
In the following steps, create an OpenSearch Serverless collection (a logical unit to store and organize documents). 
 
 Create an encryption policy for the collection: 
 
 
 AOSS_COLLECTION_NAME=${CLUSTER_NAME}-collection
ENCRYPTION_POLICY_NAME=${CLUSTER_NAME}-encryption-policy
aws opensearchserverless create-security-policy \
&nbsp;&nbsp; &nbsp;--name ${ENCRYPTION_POLICY_NAME}\
&nbsp;&nbsp; &nbsp;--type encryption \
&nbsp;&nbsp; &nbsp;--policy "{\"Rules\":[{\"ResourceType\":\"collection\",\"Resource\":[\"collection/${AOSS_COLLECTION_NAME}\"]}],\"AWSOwnedKey\":true}" 
 
 
 The network policy that restricts access to the collection to only come through a specific VPC endpoint: 
 
 
 NETWORK_POLICY_NAME=${CLUSTER_NAME}-network-policy
aws opensearchserverless create-security-policy \
&nbsp;&nbsp; &nbsp;--name ${NETWORK_POLICY_NAME} \
&nbsp;&nbsp; &nbsp;--type network \
&nbsp;&nbsp; &nbsp;--policy&nbsp;"[{\"Description\":\"Allow VPC endpoint access\",\"Rules\":[{\"ResourceType\":\"collection\",\"Resource\":[\"collection/${AOSS_COLLECTION_NAME}\"]}],\"SourceVPCEs\":[\"$VPC_ENDPOINT_ID\"]}]" 
 
 
 The data policy that grants permissions to the IAM chat-based assistant role for interacting with indices in the collection: 
 
 
 DATA_POLICY_NAME=${CLUSTER_NAME}-data-policy
IAM_CHATBOT_ROLE_ARN=$(aws iam get-role --role-name ${IAM_CHATBOT_ROLE} --query 'Role.Arn' --output text)
aws opensearchserverless create-access-policy \
&nbsp;&nbsp; &nbsp;--name ${DATA_POLICY_NAME} \
&nbsp;&nbsp; &nbsp;--type data&nbsp;\
&nbsp;&nbsp; &nbsp;--policy "[{\"Rules\":[{\"ResourceType\":\"index\",\"Resource\":[\"index/${AOSS_COLLECTION_NAME}/*\"],\"Permission\":[\"aoss:CreateIndex\",\"aoss:DescribeIndex\",\"aoss:ReadDocument\",\"aoss:WriteDocument\",\"aoss:UpdateIndex\",\"aoss:DeleteIndex\"]}],\"Principal\":[\"${IAM_CHATBOT_ROLE_ARN}\"]}]"
 
 
 
 The OpenSearch collection itself: 
 
 
 AOSS_COLLECTION_ID=$(aws opensearchserverless create-collection \
&nbsp; &nbsp; --name ${AOSS_COLLECTION_NAME} \
&nbsp; &nbsp; --type VECTORSEARCH \
&nbsp; &nbsp; --region ${AWS_DEFAULT_REGION}&nbsp;\
&nbsp;&nbsp; &nbsp;--query 'createCollectionDetail.id'&nbsp;\
&nbsp;&nbsp; &nbsp;--output text)
 
 
Create EFS file system and set up necessary permissions 
Create an EFS file system: 
 
 EFS_FS_ID=$(aws efs create-file-system \
&nbsp;&nbsp; &nbsp;--region $AWS_DEFAULT_REGION \
&nbsp;&nbsp; &nbsp;--performance-mode generalPurpose \
&nbsp;&nbsp; &nbsp;--query 'FileSystemId' \
&nbsp;&nbsp; &nbsp;--output text) 
 
EFS requires mount targets, which are VPC network endpoints that connect your EKS nodes to the EFS file system. These mount targets must be reachable from your EKS worker nodes, and access is controlled using security groups. 
 
 Execute the following command to set up the mount targets and configure the necessary security group rules: 
 
 
 EFS_SECURITY_GROUP_ID=$(aws ec2 create-security-group \
&nbsp;&nbsp; &nbsp;--group-name ${CLUSTER_NAME}-EfsSecurityGroup \
&nbsp;&nbsp; &nbsp;--description "${CLUSTER_NAME} EFS security group" \
&nbsp;&nbsp; &nbsp;--vpc-id $VPC_ID \
&nbsp;&nbsp;&nbsp;&nbsp;--region $AWS_DEFAULT_REGION \
&nbsp;&nbsp;&nbsp;&nbsp;--query 'GroupId' \
&nbsp;&nbsp; &nbsp;--output text)&nbsp;&amp;&amp;&nbsp;\
aws ec2 authorize-security-group-ingress \
&nbsp;&nbsp; &nbsp;--group-id $EFS_SECURITY_GROUP_ID \
&nbsp;&nbsp; &nbsp;--protocol tcp \
&nbsp;&nbsp; &nbsp;--port 2049 \
&nbsp;&nbsp;&nbsp;&nbsp;--region $AWS_DEFAULT_REGION \
&nbsp;&nbsp; &nbsp;--cidr $CIDR_RANGE &amp;&amp;&nbsp;\
for subnet in $SUBNET_IDS; do
&nbsp;&nbsp; &nbsp;aws efs create-mount-target \
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;--file-system-id $EFS_FS_ID \
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;--subnet-id $subnet \
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;--security-groups $EFS_SECURITY_GROUP_ID \
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;--region $AWS_DEFAULT_REGION&nbsp;
done 
 
 
 Create the StorageClass in Amazon EKS for Amazon EFS: 
 
 
 cat &lt;&lt; EOF | kubectl apply -f -
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
&nbsp;&nbsp;name: efs
provisioner: efs.csi.aws.com
parameters:
&nbsp;&nbsp;provisioningMode: efs-ap
&nbsp;&nbsp;fileSystemId: ${EFS_FS_ID}
&nbsp;&nbsp;directoryPerms: "777"
EOF 
 
 
 Validate the EFS storage class: 
 
 
 kubectl get storageclass efs 
 
These are the expected results: 
 
 NAME &nbsp; PROVISIONER &nbsp; &nbsp; &nbsp; RECLAIMPOLICY &nbsp; VOLUMEBINDINGMODE &nbsp; ALLOWVOLUMEEXPANSION &nbsp; AGE
efs &nbsp; &nbsp;efs.csi.aws.com &nbsp; Delete &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Immediate &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; false &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;9s 
 
Create Karpenter GPU NodePool 
To create the Karpenter GPU NodePool, enter the following code: 
 
 cat &lt;&lt; EOF | kubectl apply -f -
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
&nbsp;&nbsp;name: gpu-node-pool
spec:
&nbsp;&nbsp;template:
&nbsp;&nbsp; &nbsp;metadata:
&nbsp;&nbsp; &nbsp; &nbsp;labels:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;type: karpenter
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;NodeGroupType: gpu-node-pool
&nbsp;&nbsp; &nbsp;spec:
&nbsp;&nbsp; &nbsp; &nbsp;nodeClassRef:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;group: eks.amazonaws.com
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;kind: NodeClass
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;name: default
&nbsp;&nbsp; &nbsp; &nbsp;taints:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;- key: nvidia.com/gpu
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;value: "Exists"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;effect: "NoSchedule"

&nbsp;&nbsp; &nbsp; &nbsp;requirements:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;- key: "eks.amazonaws.com/instance-family"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;operator: In
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;values: ["g5"]
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;- key: "eks.amazonaws.com/instance-size"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;operator: In
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;values: [ "2xlarge", "4xlarge", "8xlarge", "16xlarge", "12xlarge", "24xlarge"]
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;- key: "kubernetes.io/arch"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;operator: In
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;values: ["amd64"]
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;- key: "karpenter.sh/capacity-type"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;operator: In
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;values: ["on-demand"]

&nbsp;&nbsp;limits:
&nbsp;&nbsp; &nbsp;cpu: "1000"
EOF 
 
This NodePool is designed for GPU workloads using AWS G5 instances, which feature NVIDIA A10G GPUs. The taint ensures that only workloads specifically designed for GPU usage will be scheduled on these nodes, maintaining efficient resource utilization. In a production environment, you might want to consider using Amazon EC2 Spot Instances as well to optimize on costs. 
Enter the command to validate successful creation of the NodePool: 
 
 kubectl get nodepools 
 
These are the expected results: 
 
 NAME &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;NODECLASS &nbsp; NODES &nbsp; READY &nbsp; AGE
general-purpose &nbsp; default &nbsp; &nbsp; 0 &nbsp; &nbsp; &nbsp; True &nbsp; &nbsp;15m
gpu-node-pool &nbsp; &nbsp; default &nbsp; &nbsp; 0 &nbsp; &nbsp; &nbsp; True &nbsp; &nbsp;8s
system &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;default &nbsp; &nbsp; 2 &nbsp; &nbsp; &nbsp; True &nbsp; &nbsp;15m 
 
gpu-node-pool was created and has 0 nodes. To inspect nodes further, enter this command: 
 
 kubectl get nodes -o custom-columns=NAME:.metadata.name,READY:"status.conditions[?(@.type=='Ready')].status",OS-IMAGE:.status.nodeInfo.osImage,INSTANCE-TYPE:.metadata.labels.'node\.kubernetes\.io/instance-type' 
 
This is the expected output: 
 
 NAME &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;READY &nbsp; &nbsp;OS-IMAGE &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; INSTANCE-TYPE
i-0b0c1cd3d744883cd &nbsp; True &nbsp; &nbsp; Bottlerocket (EKS Auto) 2025.4.26 (aws-k8s-1.32) &nbsp; c6g.large
i-0e1f33e42fac76a09 &nbsp; True &nbsp; &nbsp; Bottlerocket (EKS Auto) 2025.4.26 (aws-k8s-1.32) &nbsp; c6g.large 
 
There are two instances, launched by EKS Auto Mode with non-accelerated Bottlerocket Amazon Machine Image (AMI) variant aws-k8s-1.32, and CPU-only (non-GPU) instance type c6g. 
Install NVIDIA NFD and NIM Operator 
The NFD is a Kubernetes plugin that identifies available hardware capabilities and system settings. NFD and NIM Operator are installed using Helm charts, each with their own custom resource definitions (CRDs). 
 
 Before proceeding with installation, verify if related CRDs exist in your cluster: 
 
 
 # Check for NFD-related CRDs
kubectl get crds | grep nfd

# Check for NIM-related CRDs
kubectl get crds | grep nim 
 
If these CRDs arenâ€™t present, both commands will return no results. 
 
 Add Helm repos: 
 
 
 helm repo add nfd https://kubernetes-sigs.github.io/node-feature-discovery/charts
helm repo add nvidia https://helm.ngc.nvidia.com/nvidia
helm repo update 
 
 
 Install the NFD dependency for NIM Operator: 
 
 
 helm install node-feature-discovery nfd/node-feature-discovery \
&nbsp;&nbsp;--namespace node-feature-discovery \
&nbsp;&nbsp;--create-namespace 
 
 
 Validate the pods are up and CRDs were created: 
 
 
 kubectl get po -n node-feature-discovery 
 
Expected output: 
 
 NAME &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; READY &nbsp; STATUS &nbsp; &nbsp;RESTARTS &nbsp; AGE
node-feature-discovery-gc-5b65f7f5b6-q4hlr &nbsp; &nbsp; &nbsp; 1/1 &nbsp; &nbsp; Running &nbsp; 0 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;79s
node-feature-discovery-master-767dcc6cb8-6hc2t &nbsp; 1/1 &nbsp; &nbsp; Running &nbsp; 0 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;79s
node-feature-discovery-worker-sg852 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;1/1 &nbsp; &nbsp; Running &nbsp; 0 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;43s
 
 
 
 kubectl get crds | grep nfd 
 
Expected output: 
 
 nodefeaturegroups.nfd.k8s-sigs.io &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;2025-05-05T01:23:16Z
nodefeaturerules.nfd.k8s-sigs.io &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 2025-05-05T01:23:16Z
nodefeatures.nfd.k8s-sigs.io &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 2025-05-05T01:23:16Z
 
 
 
 Install the NIM Operator: 
 
 
 helm install nim-operator nvidia/k8s-nim-operator \
&nbsp;&nbsp;--namespace nim-operator \
&nbsp;&nbsp;--create-namespace \
&nbsp;&nbsp;--version&nbsp;v2.0.0 
 
You might need to use version v1.0.1 for the NIM Operator instead of v2.0.0 as shown in the preceding code example because occasionally you might receive a â€œ402 Payment Requiredâ€ message. 
 
 Validate the pod is up and CRDs were created: 
 
 
 kubectl get po -n nim-operator 
 
Expected output: 
 
 NAME &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; READY &nbsp; STATUS &nbsp; &nbsp;RESTARTS &nbsp; AGE
nim-operator-k8s-nim-operator-6d988f78df-h4nqn &nbsp; 1/1 &nbsp; &nbsp; Running &nbsp; 0 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;24s
 
 
 
 kubectl get crds | grep nim 
 
Expected output: 
 
 nimcaches.apps.nvidia.com &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;2025-05-05T01:18:00Z
nimpipelines.apps.nvidia.com &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 2025-05-05T01:18:00Z
nimservices.apps.nvidia.com &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;2025-05-05T01:18:01Z
 
 
Create nim-service namespace and NVIDIA secrets 
In this section, create the nim-service namespace and add two secrets containing your NGC API key. 
 
 Create namespace and secrets: 
 
 
 kubectl create namespace nim-service
kubectl create secret -n nim-service docker-registry ngc-secret \
&nbsp; &nbsp; --docker-server=nvcr.io \
&nbsp; &nbsp; --docker-username='$oauthtoken' \
&nbsp; &nbsp; --docker-password=$NVIDIA_NGC_API_KEY
kubectl create secret -n nim-service generic ngc-api-secret \
&nbsp; &nbsp; --from-literal=NGC_API_KEY=$NVIDIA_NGC_API_KEY 
 
 
 Validate secrets were created: 
 
 
 kubectl -n nim-service get secrets 
 
The following is the expected result: 
 
 NAME &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; TYPE &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; DATA &nbsp; AGE
ngc-api-secret &nbsp; Opaque &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 1 &nbsp; &nbsp; &nbsp;13s
ngc-secret &nbsp; &nbsp; &nbsp; kubernetes.io/dockerconfigjson &nbsp; 1 &nbsp; &nbsp; &nbsp;14s 
 
ngc-secret is a Docker registry secret used to authenticate and pull NIM container images from NVIDIAâ€™s NGC container registry. 
ngc-api-secret is a generic secret used by the model puller init container to authenticate and download models from the same registry. 
Create NIMCaches 
RAG enhances chat applications by enabling AI models to access either internal domain-specific knowledge or external knowledge bases, reducing hallucinations and providing more accurate, up-to-date responses. In a RAG system, a knowledge base is created from domain-specific documents. These documents are sliced into smaller pieces of text. The text pieces and their generated embeddings are then uploaded to a vector database. Embeddings are numerical representations (vectors) that capture the meaning of text, where similar text content results in similar vector values. When questions are received from users, theyâ€™re also sent with their respective embeddings to the database for semantic similarity search. The database returns the closest matching chunks of text, which are used by an LLM to provide a domain-specific answer. 
We use Metaâ€™s llama-3-2-1b-instruct as LLM and NVIDIA Retrieval QA E5 (embedqa-e5-v5) as embedder. 
This section covers the deployment of NIMCaches for storing both the LLM and embedder models. Local storage of these models speeds up pod initialization by eliminating the need for repeated downloads. Our llama-3-2-1b-instruct LLM, with 1B parameters, is a relatively small model and uses 2.5 GB of storage space. The storage requirements and initialization time increase when larger models are used. Although the initial setup of the LLM and embedder caches takes 10â€“15 minutes, subsequent pod launches will be faster because the models are already available in the clusterâ€™s local storage. 
Enter the following command: 
 
 kubectl apply -f nim-caches.yaml 
 
This is the expected output: 
 
 nimcache.apps.nvidia.com/nv-embedqa-e5-v5 created
nimcache.apps.nvidia.com/meta-llama-3-2-1b-instruct created 
 
NIMCaches will create PersistentVolumes (PVs) and PersistentVolumeClaims (PVCs) to store the models, with STORAGECLASS&nbsp;efs: 
 
 kubectl get -n nim-service pv,pvc 
 
The following is the expected output: 
 
 NAME &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;CAPACITY &nbsp; ACCESS MODES &nbsp; RECLAIM POLICY &nbsp; STATUS &nbsp; CLAIM &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;STORAGECLASS &nbsp; VOLUMEATTRIBUTESCLASS &nbsp; REASON &nbsp; AGE
persistentvolume/pvc-5fa98625-ea65-4aef-99ff-ca14001afb47 &nbsp; 50Gi &nbsp; &nbsp; &nbsp; RWX &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Delete &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Bound &nbsp; &nbsp;nim-service/nv-embedqa-e5-v5-pvc &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; efs &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&lt;unset&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;77s
persistentvolume/pvc-ab67e4dc-53df-47e7-95c8-ec6458a57a01 &nbsp; 50Gi &nbsp; &nbsp; &nbsp; RWX &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Delete &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Bound &nbsp; &nbsp;nim-service/meta-llama-3-2-1b-instruct-pvc &nbsp; efs &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&lt;unset&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;76s

NAME &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; STATUS &nbsp; VOLUME &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; CAPACITY &nbsp; ACCESS MODES &nbsp; STORAGECLASS &nbsp; VOLUMEATTRIBUTESCLASS &nbsp; AGE
persistentvolumeclaim/meta-llama-3-2-1b-instruct-pvc &nbsp; Bound &nbsp; &nbsp;pvc-ab67e4dc-53df-47e7-95c8-ec6458a57a01 &nbsp; 50Gi &nbsp; &nbsp; &nbsp; RWX &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;efs &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&lt;unset&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 77s
persistentvolumeclaim/nv-embedqa-e5-v5-pvc &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Bound &nbsp; &nbsp;pvc-5fa98625-ea65-4aef-99ff-ca14001afb47 &nbsp; 50Gi &nbsp; &nbsp; &nbsp; RWX &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;efs &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&lt;unset&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 77s 
 
Enter the following to validate NIMCaches: 
 
 kubectl get nimcaches -n nim-service 
 
This is the expected output (STATUS will stay initially blank, then become InProgress for 10â€“15 mins until model download is complete): 
 
 NAME &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; STATUS &nbsp; PVC &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;AGE
meta-llama-3-2-1b-instruct &nbsp; Ready &nbsp; &nbsp;meta-llama-3-2-1b-instruct-pvc &nbsp; 13m
nv-embedqa-e5-v5 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Ready &nbsp; &nbsp;nv-embedqa-e5-v5-pvc &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;13m 
 
Create NIMServices 
NIMServices are custom resources to manage NVIDIA NIM microservices. To deploy the LLM and embedder services enter the following: 
 
 kubectl apply -f nim-services.yaml 
 
The following is the expected output: 
 
 nimservice.apps.nvidia.com/meta-llama-3-2-1b-instruct created
nimservice.apps.nvidia.com/nv-embedqa-e5-v5 created 
 
Validate the NIMServices: 
 
 kubectl get nimservices -n nim-service 
 
The following is the expected output: 
 
 NAME &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; STATUS &nbsp; AGE
meta-llama-3-2-1b-instruct &nbsp; Ready &nbsp; &nbsp;5m25s
nv-embedqa-e5-v5 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Ready &nbsp; &nbsp;5m24s 
 
Our models are stored in an EFS volume, which is mounted to the EC2 instances as a PVC. That translates to faster pod startup times. In fact, notice in the preceding example that the NIMServices are ready in approximately 5 minutes. This time includes GPU node(s) launch from Karpenter and container image pull and launch. 
Compared to the 10â€“15 minutes required for internet-based model downloads, as experienced during the NIMCaches deployment, loading models from the local cache reduces startup time considerably, enhancing the overall system scaling speed. Should you need even more performing storage alternatives, you could explore alternatives such as Amazon FSx for Lustre. 
Enter the following command to check the nodes again: 
 
 kubectl get nodes -o custom-columns=NAME:.metadata.name,READY:"status.conditions[?(@.type=='Ready')].status",OS-IMAGE:.status.nodeInfo.osImage,INSTANCE-TYPE:.metadata.labels.'node\.kubernetes\.io/instance-type' 
 
The following is the expected output: 
 
 NAME &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;READY &nbsp; OS-IMAGE &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;INSTANCE-TYPE
i-0150ecedccffcc17f &nbsp; True &nbsp; &nbsp;Bottlerocket (EKS Auto) 2025.4.26 (aws-k8s-1.32) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;c6g.large
i-027bf5419d63073cf &nbsp; True &nbsp; &nbsp;Bottlerocket (EKS Auto) 2025.4.26 (aws-k8s-1.32) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;c5a.large
i-0a1a1f39564fbf125 &nbsp; True &nbsp; &nbsp;Bottlerocket (EKS Auto, Nvidia) 2025.4.21 (aws-k8s-1.32-nvidia) &nbsp; g5.2xlarge
i-0d418bd8429dd12cd &nbsp; True &nbsp; &nbsp;Bottlerocket (EKS Auto, Nvidia) 2025.4.21 (aws-k8s-1.32-nvidia) &nbsp; g5.2xlarge 
 
Karpenter launched two new GPU instances to support NIMServices, with a Bottlerocket accelerated AMI variant Bottlerocket (EKS Auto, Nvidia) 2025.4.21 (aws-k8s-1.32-nvidia). The number and type of instances launched might vary depending on Karpenterâ€™s algorithm, which takes into consideration parameters such as instance availability and cost. 
Confirm that the&nbsp;NIMService STATUS&nbsp;is&nbsp;Ready before progressing further. 
Chat-based assistant client 
We now use a Python client, implementing the chat-based assistant interface, using the Gradio and LangChain libraries. Gradio creates the web interface and chat components, handling the frontend presentation. LangChain connects various components and implements RAG through multiple services in our EKS cluster. Metaâ€™s llama-3-2-1b-instruct serves as the base language model, and nv-embedqa-e5-v5 creates text embeddings. OpenSearch acts as the vector store, managing these embeddings and enabling similarity search. This setup allows the chat-based assistant to retrieve relevant information and generate contextual responses. 
 
Sequence diagram showing question-answering workflow with document upload process 
 
 Enter the following commands to deploy the client, hosted on Amazon Elastic Container Registry (Amazon ECR) as a container image in the public gallery (the applicationâ€™s source files are available in the client folder of the cloned repository): 
 
 
 AOSS_INDEX=${CLUSTER_NAME}-index
CHATBOT_CONTAINER_IMAGE=public.ecr.aws/h6c7e9p3/aws-rag-chatbot-eks-nims:1.0

cat &lt;&lt; EOF | kubectl apply -f -
apiVersion: v1
kind: Service
metadata:
&nbsp;&nbsp;name: rag-chatbot
&nbsp;&nbsp;labels:
&nbsp;&nbsp; &nbsp;app: rag-chatbot
spec:
&nbsp;&nbsp;ports:
&nbsp;&nbsp;- port: 7860
&nbsp;&nbsp; &nbsp;protocol: TCP
&nbsp;&nbsp;selector:
&nbsp;&nbsp; &nbsp;app: rag-chatbot
---
apiVersion: apps/v1
kind: Deployment
metadata:
&nbsp;&nbsp;name: rag-chatbot
spec:
&nbsp;&nbsp;selector:
&nbsp;&nbsp; &nbsp;matchLabels:
&nbsp;&nbsp; &nbsp; &nbsp;app: rag-chatbot
&nbsp;&nbsp;template:
&nbsp;&nbsp; &nbsp;metadata:
&nbsp;&nbsp; &nbsp; &nbsp;labels:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;app: rag-chatbot
&nbsp;&nbsp; &nbsp;spec:
&nbsp;&nbsp; &nbsp; &nbsp;serviceAccountName: ${CHATBOT_SA_NAME}
&nbsp;&nbsp; &nbsp; &nbsp;containers:
&nbsp;&nbsp; &nbsp; &nbsp;- name: rag-chatbot
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;image: ${CHATBOT_CONTAINER_IMAGE}
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;ports:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;- containerPort: 7860
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;protocol: TCP
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;env:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;- name: AWS_DEFAULT_REGION
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;value: ${AWS_DEFAULT_REGION}
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;- name: OPENSEARCH_COLLECTION_ID
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;value: ${AOSS_COLLECTION_ID}
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;- name: OPENSEARCH_INDEX
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;value: ${AOSS_INDEX}
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;- name: LLM_URL
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;value: "http://meta-llama-3-2-1b-instruct.nim-service.svc.cluster.local:8000/v1"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;- name: EMBEDDINGS_URL
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;value: "http://nv-embedqa-e5-v5.nim-service.svc.cluster.local:8000/v1"
EOF 
 
 
 Check the client pod status: 
 
 
 kubectl get pods 
 
The following is the example output: 
 
 NAME &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; READY &nbsp; STATUS &nbsp; &nbsp;RESTARTS &nbsp; AGE
rag-chatbot-6678cd95cb-4mwct &nbsp; 1/1 &nbsp; &nbsp; Running &nbsp; 0 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;60s 
 
 
 Port-forward the clientâ€™s service: 
 
 
 kubectl port-forward service/rag-chatbot 7860:7860 &amp; 
 
 
 Open a browser window at http://127.0.0.1:7860. 
 
In the following screenshot, we prompted the chat-based assistant about a topic that isnâ€™t in its knowledge base yet: â€œWhat is Amazon Nova Canvas.â€ 
 
The chat-based assistant canâ€™t find information on the topic and canâ€™t formulate a proper answer. 
 
 Download the file at location: https://docs.aws.amazon.com/pdfs/ai/responsible-ai/nova-canvas/nova-canvas.pdf and upload its embeddings to OpenSearch Serverless using the client UI, switching to the Document upload tab, in the top left, as shown in the following screenshot. 
 
 
The expected result is nova-canvas.pdf appearing the list of uploaded files, as shown in the following screenshot. 
 
 
 Wait 15â€“30 seconds for OpenSearch Serverless to process and index the data. Ask the same question, â€œWhat is Amazon Nova Canvas,â€ and you will receive a different answer, as shown in the following screenshot. 
 
 
Cleanup 
To clean up the cluster and the EFS resources created so far, enter the following command: 
 
 aws efs describe-mount-targets \
    --region $AWS_DEFAULT_REGION \
    --file-system-id $EFS_FS_ID \
    --query 'MountTargets[*].MountTargetId' \
    --output text \
    | xargs -n1 aws efs delete-mount-target \
        --region $AWS_DEFAULT_REGION \
        --mount-target-id
&nbsp; 
 
Wait approximately 30 seconds for the mount targets to be removed, then enter the following command: 
 
 aws efs delete-file-system --file-system-id $EFS_FS_ID --region $AWS_DEFAULT_REGION
eksctl delete cluster --name=$CLUSTER_NAME --region $AWS_DEFAULT_REGION 
 
To delete the OpenSearch Serverless collection and policies, enter the following command: 
 
 aws opensearchserverless delete-collection \
&nbsp;&nbsp; &nbsp;--id ${AOSS_COLLECTION_ID}

aws opensearchserverless delete-security-policy \
&nbsp;&nbsp; &nbsp;--name ${ENCRYPTION_POLICY_NAME}&nbsp;\
&nbsp;&nbsp; &nbsp;--type encryption
&nbsp;&nbsp;&nbsp;&nbsp;
aws opensearchserverless delete-security-policy \
&nbsp;&nbsp; &nbsp;--name ${NETWORK_POLICY_NAME} \
&nbsp;&nbsp; &nbsp;--type network

aws opensearchserverless delete-access-policy \
&nbsp;&nbsp; &nbsp;--name ${DATA_POLICY_NAME} \
&nbsp;&nbsp; &nbsp;--type data 
 
Conclusion 
In this post, we showed how to deploy a RAG-enabled chat-based assistant on Amazon EKS, using NVIDIA NIM microservices, integrating an LLM for text generation, an embedding model, and Amazon OpenSearch Serverless for vector storage. Using EKS Auto Mode with GPU-accelerated AMIs, we streamlined our deployment by automating the setup of GPU infrastructure. We specified GPU-based instance types in our Karpenter NodePools, and the system automatically provisioned worker nodes with all necessary NVIDIA components, including device plugins, container toolkit, and kernel drivers. The implementation demonstrated the effectiveness of RAG, with the chat-based assistant providing informed responses when accessing relevant information from its knowledge base. This architecture showcases how Amazon EKS can streamline the deployment of AI solutions, maintaining production-grade reliability and scalability. 
As a challenge, try enhancing the chat-based assistant application by implementing chat history functionality to preserve context across conversations. This allows the LLM to reference previous exchanges and provide more contextually relevant responses. To further learn how to run artificial intelligence and machine learning (AI/ML) workloads on Amazon EKS, check out our EKS best practices guide for running AI/ML workloads, join one of our Get Hands On with Amazon EKS event series, and visit AI on EKS deployment-ready blueprints. 
 
About the authors 
Riccardo Freschi is a Senior Solutions Architect at AWS who specializes in Modernization. He helps partners and customers transform their IT landscapes by designing and implementing modern cloud-native architectures on AWS. His focus areas include container-based applications on Kubernetes, cloud-native development, and establishing modernization strategies that drive business value. 
 Christina Andonov is a Sr. Specialist Solutions Architect at AWS, helping customers run AI workloads on Amazon EKS with open source tools. Sheâ€™s passionate about Kubernetes and known for making complex concepts easy to understand.

â¸»