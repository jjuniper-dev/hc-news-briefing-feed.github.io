‚úÖ Morning News Briefing ‚Äì August 29, 2025 10:43

üìÖ Date: 2025-08-29 10:43
üè∑Ô∏è Tags: #briefing #ai #publichealth #digitalgov

‚∏ª

üßæ Weather
‚Ä¢ Current Conditions:  11.0¬∞C
  Temperature: 11.0&deg;C Pressure / Tendency: 101.6 kPa rising Humidity: 87 % Dewpoint: 8.9&deg:C Wind: WNW 12 km/h . Air Quality Health Index: n/a . Pembroke 6:00 AM EDT Friday 29 August 2025 . Weather forecast: 11:11.0/deg
‚Ä¢ Friday: Chance of showers. High 17. POP 30%
  Cloudy. 30 percent chance of showers this afternoon . High 17. UV index 4 or moderate . High 18.50C in the morning of Monday, Tuesday morning of Tuesday, Wednesday morning of Wednesday, Wednesday of Wednesday . Rainy. Showers will continue to fall through the night, rain will also fall through Friday afternoon . Forecast issued 5:00 AM EDT Friday 29 August
‚Ä¢ Friday night: Chance of showers. Low 7. POP 30%
  Mainly cloudy. 30 percent chance of showers this evening . Low 7.50% chance of rain this evening. Mainly sunny, cloudy with rain showers forecast . Low 70% of rain is possible in the morning . Forecast issued 5:00 AM EDT Friday 29 August 2025. For more information, visit http://www.cnnnn.com/news/events/events

üåç International News
No updates.

üçÅ Canadian News
No updates.

üá∫üá∏ U.S. Top Stories
‚Ä¢ Thai court dismisses prime minister over compromising phone call with Cambodian leader
  Thailand's Constitutional Court dismissed Paetongtarn Shinawatra from her position as prime minister . The court ruled that as the country's leader she violated constitutional rules on ethics . She violated the rules as a prime minister, ruling that she violated the constitution's rules of ethics . The decision was made by the Constitutional Court on Friday in Thailand's highest court in the country on Friday .
‚Ä¢ The Trump administration wants to build more roads through national forests
  The Trump administration argues that rescinding the 2001 Roadless Rule will help wildland firefighters . Fire researchers warn that more roads could exacerbate the problem of wildland firefighting . The administration argues the rule will help firefighters in wildland areas . More roads are needed to help firefighters, researchers say . The rule is set to be rescinded by the end of next year, but experts say it
‚Ä¢ Contract breach or banditry? Inside the collapse of the Taliban's oil deal with China
  Two years after the oil deal was signed, it collapsed ‚Äî with the Taliban accusing the Chinese company of breaching the contract . Some Chinese employees likened the Taliban's actions to robbery . Taliban accuse the company of breach of the contract and some employees liken it to robbery. Some Chinese workers liken Taliban's action to robbery, according to some Chinese employees . The Taliban accuse Chinese oil company
‚Ä¢ Talking to kids about school shootings. Be truthful and follow their lead
  Parents are struggling to figure out what to say to their children after another school shooting . We talked to some experts, who offered these guidelines . Parents are also struggling to think of what to tell their children about the school shootings . Experts offer these guidelines to parents of children in the wake of a school shooting in California . Click here for more information on how to talk to your child about the
‚Ä¢ College football season is here. Here's what to know ahead of Saturday's kickoff
  This weekend features three top-10 matchups, most ever for an opening weekend in college football history . Arch Manning, the most hyped player of a generation, will start for the first time . The most-ever college football opening weekend is this weekend's opening weekend, with three top 10 games in the top 10 . The first time Arch Manning will start in a college football game .

üß† Artificial Intelligence
No updates.

üíª Digital Strategy
‚Ä¢ UK datacenter developers turn to gas rather than wait for grid power for builds
  Datacenter developers in the UK are turning to gas for power generation amid lengthy wait times for a connection to the electricity grid . The UK's bitbarn-favoring Industrial Strategy has been criticised for being too slow to connect to the grid . It's unclear what happened to gov. UK's Industrial Strategy, but it's a good start to the data-storage boom
‚Ä¢ How Windows 11 is breaking from its bedrock and moving away
  The once mighty Wintel supercontinent is cracking in more ways than you might think Opinion Opinion: Say what you like about its role in the destruction of civilization, the net is still good for a few party games . Take bets on when the "Wintel Empire" was first reported as under attack, and by what . Then go and find out what happened to the Wintel Empire
‚Ä¢ Cloud computing has become so normal, it's invisible
  Distinctions disappear as computing evolves and as something that was shiny and new simply becomes the way that we do things . Maybe someday we'll just call it 'data processing' again Feature Feature. Feature: In IT, terms and categories come and go, and as a result, we're not sure what's the new thing we're doing . Feature: Data processing, data processing, and
‚Ä¢ Techie fooled a panicked daemon and manipulated time itself to get servers in sync
  Network Time Protocol sometimes needs help from a temporal police officer . On Call is a reader-contributed column that shares your finest tech support stories . It's the time at 7:30 AM on Friday morning, the moment at which The Register regularly runs a fresh instalment of On Call . The Register is happy to feature a guest guest on our On Call this week at 730
‚Ä¢ China turns on giant neutrino detector that took a decade to build
  700 meters under a mountain, a 20,000-tonne detector and a giant sphere await elusive particles . More than a decade after construction began, China has commenced operation of what it claims is the world‚Äôs most sensitive neutrino detector . China claims it claims it is the most sensitive detector in the world, with the world's most sensitive particle detectors in existence . The

üè• Public Health
No updates.

üî¨ Science
‚Ä¢ Educational inequalities in sun protection practices among Brazilian adults
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Rethinking suicide prevention: insights from the global south for a new global agenda
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Author Correction: Overview of vaccines for adults authorized, recommended, and implemented in the European Union
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Assessing the feasibility of HPV screening for cervical cancer in pregnant women in Ethiopia
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Digital therapeutics for insomnia: an umbrella review and meta-meta-analysis
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations

üßæ Government & Policy
No updates.

üèõÔ∏è Enterprise Architecture & IT Governance
No updates.

ü§ñ AI & Emerging Tech
‚Ä¢ This American nuclear company could help India‚Äôs thorium dream
  For just the second time in nearly two decades, the United States has granted an export license to an American company planning to sell nuclear technology to India, MIT Technology Review has learned. The decision to greenlight Clean Core Thorium Energy‚Äôs license is a major step toward closer cooperation between the two countries on atomic energy and marks a milestone in the development of thorium as an alternative to uranium for fueling nuclear reactors.&nbsp;



Starting from the issuance last week, the thorium fuel produced by the Chicago-based company can be shipped to reactors in India, where it could be loaded into the cores of existing reactors. Once Clean Core receives final approval from Indian regulators, it will become one of the first American companies to sell nuclear technology to India, just as the world‚Äôs most populous nation has started relaxing strict rules that have long kept the US private sector from entering its atomic power industry.&nbsp;



‚ÄúThis license marks a turning point, not just for Clean Core but for the US-India civil nuclear partnership,‚Äù says Mehul Shah, the company&#8217;s chief executive and founder. ‚ÄúIt places thorium at the center of the global energy transformation.‚Äù



Thorium has long been seen as a good alternative to uranium because it‚Äôs more abundant, produces both smaller amounts of long-lived radioactive waste and fewer byproducts with centuries-long half-lives, and reduces the risk that materials from the fuel cycle will be diverted into weapons manufacturing.&nbsp;



But at least some uranium fuel is needed to make thorium atoms split, making it an imperfect replacement. It‚Äôs also less well suited for use in the light-water reactors that power the vast majority of commercial nuclear plants worldwide. And in any case, the complex, highly regulated nuclear industry is extremely resistant to change.



For India, which has scant uranium reserves but abundant deposits of thorium, the latter metal has been part of a long-term strategy for reducing dependence on imported fuels. The nation started negotiating a nuclear export treaty with the US in the early 2000s, and a 123 Agreement‚Äîa special, Senate-approved treaty the US requires with another country before sending it any civilian nuclear products‚Äîwas approved in 2008.



A new approach



While most thorium advocates have envisioned new reactors designed to run on this fuel, which would mean rebuilding the nuclear industry from the ground up, Shah and his team took a different approach. Clean Core created a new type of fuel that blends thorium with a more concentrated type of uranium called HALEU (high-assay low-enriched uranium). This blended fuel can be used in India‚Äôs pressurized heavy-water reactors, which make up the bulk of the country‚Äôs existing fleet and many of the new units under development now.&nbsp;



Thorium isn‚Äôt a fissile material itself, meaning its atoms aren‚Äôt inherently unstable enough for an extra neutron to easily split the nuclei and release energy. But the metal has what‚Äôs known as ‚Äúfertile properties,‚Äù meaning it can absorb neutrons and transform into the fissile material uranium-233. Uranium-233 produces fewer long-lived radioactive isotopes than the uranium-235 that makes up the fissionable part of traditional fuel pellets. Most commercial reactors run on low-enriched uranium, which is about 5% U-235. When the fuel is spent, roughly 95% of the energy potential is left in the metal. And what remains is a highly toxic cocktail of long-lived radioactive isotopes such as cesium-137 and plutonium-239, which keep the waste dangerous for tens of thousands of years. Another concern is that the plutonium could be extracted for use in weapons.&nbsp;



Enriched up to 20%, HALEU allows reactors to extract more of the available energy and thus reduce the volume of waste. Clean Core‚Äôs fuel goes further: The HALEU provides the initial spark to ignite fertile thorium and triggers a reaction that can burn much hotter and utilize the vast majority of the material in the core, as a study published last year in the journal Nuclear Engineering and Design showed.





‚ÄúThorium provides attributes needed to achieve higher burnups,‚Äù says Koroush Shirvan, an MIT professor of nuclear science and engineering who helped design Clean Core‚Äôs fuel assemblies. ‚ÄúIt is enabling technology to go to higher burnups, which reduces your spent fuel volume, increases your fuel efficiency, and reduces the amount of uranium that you need.‚Äù&nbsp;



Compared with traditional uranium fuel, Clean Core says, its fuel reduces waste by more than 85% while avoiding the most problematic isotopes produced during fission. ‚ÄúThe result is a safer, more sustainable cycle that reframes nuclear power not as a source of millennia-long liabilities but as a pathway to cleaner energy and a viable future fuel supply,‚Äù says Milan Shah, Clean Core‚Äôs chief operating officer and Mehul‚Äôs son.



Pressurized heavy-water reactors are particularly well suited to thorium because heavy water‚Äîa version of H2O that has an extra neutron on the hydrogen atom‚Äîabsorbs fewer neutrons during the fission process, increasing efficiency by allowing more neutrons to be captured by the thorium. 



There are 46 so-called PHWRs operating worldwide: 17 in Canada, 19 in India, three each in Argentina and South Korea, and two each in China and Romania, according to data from the International Atomic Energy Agency. In 1954, India set out a three-stage development plan for nuclear power that involved eventually phasing thorium into the fuel cycle for its fleet.¬†



Yet in the 56 years since India built its first commercial nuclear plant, its state-controlled industry has remained relatively shut off to the private sector and the rest of the world. When the US signed the 123 Agreement with India in 2008, the moment heralded an era in which the subcontinent could become a testing ground for new American reactor designs.&nbsp;



In 2010, however, India passed the Civil Liability for Nuclear Damage Act. The legislation was based on what lawmakers saw as legal shortcomings in the wake of the 1984 Bhopal chemical factory disaster, when a subsidiary of the American industrial giant Dow Chemical avoided major payouts to the victims of a catastrophe that killed thousands. Under this law, responsibility for an accident at an Indian nuclear plant would fall on suppliers. The statute effectively killed any exports to India, since few companies could shoulder that burden. Only Russia‚Äôs state-owned Rosatom charged ahead with exporting reactors to India.



But things are changing. In a joint statement issued after a February 2025 summit, Prime Minister Narendra Modi and President Donald Trump &#8220;announced their commitment to fully realise the US-India 123 Civil Nuclear Agreement by moving forward with plans to work together to build US-designed nuclear reactors in India through large scale localisation and possible technology transfer.‚Äù¬†



In March 2025, US federal officials gave the nuclear developer Holtec International an export license to sell Indian companies its as-yet-unbuilt small modular reactors, which are based on the light-water reactor design used in the US. In April, the Indian government suggested it would reform the nuclear liability law to relax rules on foreign companies in hopes of drawing more overseas developers. Last month, a top minister confirmed that the Modi administration would overhaul the law.¬†



‚ÄúFor India, the thing they need to do is get another international vendor in the marketplace,‚Äù says Chris Gadomski, the chief nuclear analyst at the consultancy BloombergNEF.



Path of least resistance



But Shah sees larger potential for Clean Core. Unlike Holtec, whose export license was endorsed by the two Mumbai-based industrial giants Larsen &amp; Toubro and Tata Consulting Engineers, Clean Core had its permit approved by two of India‚Äôs atomic regulators and its main state-owned nuclear company. By focusing on fuel rather than new reactors, Clean Core could become a vendor to the majority of the existing plants already operating in India.&nbsp;



Its technology diverges not only from that of other US nuclear companies but also from the approach used in China. Last year, China made waves by bringing its first thorium-fueled reactor online. This enabled it to establish a new foothold in a technology the US had invented and then abandoned, and it gave Beijing another leg up in atomic energy. 



But scaling that technology will require building out a whole new kind of reactor. That comes at a cost. A recent Johns Hopkins University study found that China‚Äôs success in building nuclear reactors stemmed in large part from standardization and repetition of successful designs, virtually all of which have been light-water reactors. Using thorium in existing heavy-water reactors lowers the bar for popularizing the fuel, according to the younger Shah.¬†



‚ÄúWe think ours is the path of least resistance,‚Äù Milan Shah says. ‚ÄúMaybe not being completely revolutionary in the way you look at nuclear today, but incredibly evolutionary to progress humanity forward.‚Äù&nbsp;



The company has plans to go beyond pressurized heavy-water reactors. Within two years, the elder Shah says, Clean Core plans to design a version of its fuel that could work in the light-water reactors that make up the entire US fleet of 94. But it‚Äôs not a simple conversion. For starters, there‚Äôs the size: While the PHWR fuel rods are about 50 centimeters in length, the rods that go into light-water reactors are roughly four meters long. Then there‚Äôs the history of challenges with light water‚Äôs absorption of neutrons that could otherwise be captured to induce fission in the thorium.&nbsp;



For Anil Kakodkar, the former chairman of India‚Äôs Atomic Energy Commission and a mentor to Shah, popularizing thorium could help rectify one of the darker chapters in his country‚Äôs nuclear development. In 1974, India became the first country since the signing of the first global Treaty on the Non-Proliferation of Nuclear Weapons to successfully test an atomic weapon. New Delhi was never a signatory to the pact. But the milestone prompted neighboring Pakistan to develop its own weapons.&nbsp;



In response, President Jimmy Carter tried to demonstrate Washington‚Äôs commitment to reversing the Cold War arms race by sacrificing the first US effort to commercialize nuclear waste recycling, since the technology to separate plutonium and other radioisotopes from uranium in spent fuel was widely seen as a potential new source of weapons-grade material. By running its own reactors on thorium, Kakodkar says, India can chart a new path for newcomer nations that want to harness the power of the atom without stoking fears that nuclear weapons capability will spread.&nbsp;



‚ÄúThe proliferation concerns will be dismissed to a significant extent, allowing more rapid growth of nuclear power in emerging countries,‚Äù he says. ‚ÄúThat will be a good thing for the world at large.‚Äù&nbsp;



Alexander C. Kaufman is a reporter who has covered energy, climate change, pollution, business, and geopolitics for more than a decade.&nbsp;
‚Ä¢ RFK Jr‚Äôs plan to improve America‚Äôs diet is missing the point
  A lot of Americans don‚Äôt eat well. And they‚Äôre paying for it with their health. A diet high in sugar, sodium, and saturated fat can increase the risk of problems like diabetes, heart disease, and kidney disease, to name a few. And those are among¬†the leading causes of death in the US.



This is hardly news. But this week Robert F Kennedy Jr., who heads the US Department of Health and Human Services, floated a new solution to the problem. Kennedy and education secretary Linda McMahon think that teaching medical students more about the role of nutrition in health could help turn things around.





‚ÄúI‚Äôm working with Linda on forcing medical schools ‚Ä¶ to put nutrition into medical school education,‚Äù Kennedy said during¬†a cabinet meeting on August 26. The next day, HHS released¬†a statement calling for ‚Äúincreased nutrition education‚Äù for medical students.



‚ÄúWe can reverse the chronic-disease epidemic simply by changing our diets and lifestyles,‚Äù Kennedy said in¬†an accompanying video statement. ‚ÄúBut to do that, we need nutrition to be a basic part of every doctor‚Äôs training.‚Äù



It certainly sounds like a good idea. If more Americans ate a healthier diet, we could expect to see a decrease in those diseases. But this framing of America‚Äôs health crisis is overly simplistic, especially given that plenty of the administration‚Äôs other actions have directly undermined health in multiple ways‚Äîincluding by canceling a vital nutrition education program.



At any rate, there are other, more effective ways to tackle the chronic-disease crisis.



The biggest killers, heart disease and stroke,¬†are responsible for more than a third of deaths, according to the US Centers for Disease Control and Prevention. A healthy diet can reduce your risk of developing those conditions. And it makes total sense to educate the future doctors of America about nutrition.



Medical bodies are on board with the idea, too. ‚ÄúThe importance of nutrition in medical education is increasingly clear, and we support expanded, evidence-based instruction to better equip physicians to prevent and manage chronic disease and improve patient outcomes,‚Äù David H. Aizuss, chair of the American Medical Association‚Äôs board of trustees, said in a statement.



But it‚Äôs not as though medical students aren‚Äôt getting any nutrition education. And that training has increased in the last five years, according to¬†surveys carried out by the American Association of Medical Colleges.



Kennedy has referred to¬†a 2021 survey suggesting that medical students in the US get only around one hour of nutrition education per year. But the AAMC argues that nutrition education increasingly happens through ‚Äúintegrated experiences‚Äù rather than stand-alone lectures.





‚ÄúMedical schools understand the critical role that nutrition plays in preventing, managing, and treating chronic health conditions, and incorporate significant nutrition education across their required curricula,‚Äù Alison J. Whelan, AAMC‚Äôs chief academic officer, said in a statement.



That‚Äôs not to say there isn‚Äôt room for improvement. Gabby Headrick, a food systems dietician and associate director of food and nutrition policy at George Washington University‚Äôs Institute for Food Safety &amp; Nutrition Security, thinks nutritionists could take a more prominent role in patient care, too.



But it‚Äôs somewhat galling for the administration to choose medical education as its focus given the recent cuts in federal funding that will affect health. For example, funding for the National Diabetes Prevention Program, which offers support and guidance to help thousands of people adopt healthy diets and exercise routines, was canceled by the Trump administration in March.



The focus on medical schools also overlooks one of the biggest factors behind poor nutrition in the US: access to healthy food. A recent¬†survey by the Pew Research Center found that increased costs make it harder for most Americans to eat well. Twenty percent of the people surveyed acknowledged that their diets were not healthy.



‚ÄúSo many people know what a healthy diet is, and they know what should be on their plate every night,‚Äù says Headrick, who has researched this issue. ‚ÄúBut the vast majority of folks just truly do not have the money or the time to get the food on the plate.‚Äù



The Supplemental Nutrition Assistance Program (SNAP) has been helping low-income Americans afford some of those healthier foods. It¬†supported over 41 million people in 2024. But under the Trump administration‚Äôs tax and spending bill,¬†the program is set to lose around $186 billion in funding over the next 10 years.



Kennedy‚Äôs focus is on education. And it just so happens that there is a nutrition education program in place‚Äîone that helps people of all ages learn not only what healthy foods are, but how to source them on a budget and use them to prepare meals.



SNAP-Ed, as it‚Äôs known, has already provided this support to millions of Americans. Under the Trump administration, it is¬†set to be eliminated.



It is difficult to see how these actions are going to help people adopt healthier diets. What might be a better approach? I put the question to Headrick: If she were in charge, what policies would she enact?



‚ÄúUniversal health care,‚Äù she told me. Being able to access health care without risking financial hardship not only¬†improves health outcomes and life expectancy; it also spares people from medical debt‚Äîsomething that¬†affects around 40% of adults in the US, according to a recent survey.



And the Trump administration‚Äôs plans to¬†cut federal health spending by about a trillion dollars over the next decade certainly aren‚Äôt going to help with that. All told, around 16 million people could lose their health insurance by 2034, according to¬†estimates by the Congressional Budget Office.



‚ÄúThe evidence suggests that if we cut folks‚Äô social benefit programs, such as access to health care and food, we are going to see detrimental impacts,‚Äù says Headrick. ‚ÄúAnd it‚Äôs going to cause an increased burden of preventable disease.‚Äù



This article first appeared in The Checkup,¬†MIT Technology Review‚Äôs¬†weekly biotech newsletter. To receive it in your inbox every Thursday, and read articles like this first,¬†sign up here.
‚Ä¢ The Download: Google‚Äôs AI energy use, and the AI Hype Index
  This is today&#8217;s edition of¬†The Download,¬†our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



Google‚Äôs still not giving us the full picture on AI energy use&nbsp;



‚ÄîCasey Crownhart



Google just announced that a typical query to its Gemini app uses about 0.24 watt-hours of electricity. That‚Äôs about the same as running a microwave for one second‚Äîsomething that feels insignificant. I run the microwave for many more seconds than that most days.I welcome more openness from major AI players about their estimated energy use per query. But I‚Äôve noticed that some folks are taking this number and using it to conclude that we don‚Äôt need to worry about AI‚Äôs energy demand. That‚Äôs not the right takeaway here. Let‚Äôs dig into why.



This article is from The Spark, MIT Technology Review‚Äôs weekly climate newsletter. To receive it in your inbox every Wednesday, sign up here.



+ If you‚Äôre interested in AI‚Äôs energy footprint, earlier this year, MIT Technology Review published Power Hungry: a comprehensive series on AI and energy.







The AI Hype Index: AI-designed antibiotics show promise



Separating AI reality from hyped-up fiction isn‚Äôt always easy. That‚Äôs why we‚Äôve created the AI Hype Index‚Äîa simple, at-a-glance summary of everything you need to know about the state of the industry. Take a look at this month‚Äôs edition here.







The must-reads



I‚Äôve combed the internet to find you today‚Äôs most fun/important/scary/fascinating stories about technology.



1 The White House has fired the director of the CDCBut Susan Monarez is refusing to go quietly. (WP $)+ Monarez is said to have clashed with RFK Jr over vaccine policy. (NYT $)+ She was confirmed by the Senate to the position just last month. (The Guardian)+ Vaccine consensus is splintering across the US. (Vox)



2 A Chinese hacking campaign hit at least 200 US organizationsIntelligence agencies say the breaches are among the most significant ever. (WP $)+ AI-generated ransomware is on the rise. (Wired $)



3 Ukraine‚Äôs new Flamingo cruise missile took just months to buildRussia‚Äôs air defenses are weakening. Can this missile exploit the gaps? (Economist $)+ 14 people were killed in an overnight bombardment of Kyiv. (BBC)+ On the ground in Ukraine‚Äôs largest Starlink repair shop. (MIT Technology Review)



4 AI infrastructure spending is boosting the US economyCompanies are throwing so much money at AI hardware it‚Äôs lifting the real economy, not just the stock market. (NYT $)+ How to fine-tune AI for prosperity. (MIT Technology Review)5 OpenAI and Anthropic safety-tested each other‚Äôs AIThey found Claude is a lot more cautious than OpenAI‚Äôs mini models. (Engadget)+ Sycophancy was a repeated issue among OpenAI‚Äôs models. (TechCrunch)+ This benchmark used Reddit‚Äôs AITA to test how much AI models suck up to us. (MIT Technology Review)



6 Climate change exacerbated Europe‚Äôs deadly wildfiresAnd fires across the Mediterranean are likely to become more frequent and severe. (BBC)+ What the collapse of a glacier can teach us. (New Yorker $)+ How AI can help spot wildfires. (MIT Technology Review)



7 911 centers are using AI to answer callsIt‚Äôs helping to triage anything that isn‚Äôt urgent. (TechCrunch)



8 Wikipedia has compiled a list of AI writing tropesBut their presence still isn‚Äôt a dead giveaway a text has been written by AI. (Fast Company $)+ AI-text detection tools are really easy to fool. (MIT Technology Review)



9 Melania Trump has launched the Presidential AI Challenge¬†But it‚Äôs not all that clear what the competition actually is. (NY Mag $)



10 Netflix‚Äôs algorithm-appeasing movies are bland and boringBut millions of people will watch them anyway. (The Guardian)







Quote of the day



&#8220;The more you buy, the more you grow.&#8221;



‚ÄîNvidia CEO Jensen Huang conveniently sees no end to the AI chip spending boom, Reuters reports.







One more thinghttps://www.technologyreview.com/2025/01/13/1109922/inside-the-strange-limbo-facing-ivf-embryos/?utm_source=the_download&amp;utm_medium=email&amp;utm_campaign=the_download.unpaid.engagement&amp;utm_term=*|SUBCLASS|*&amp;utm_content=*|DATE:m-d-Y|*







Inside the strange limbo facing millions of IVF embryosMillions of embryos created through IVF sit frozen in time, stored in cryopreservation tanks around the world, and the number is only growing.At a basic level, an embryo is simply a tiny ball of a hundred or so cells. But unlike other types of body tissue, it holds the potential for life. Many argue that this endows embryos with a special moral status, one that requires special protections.The problem is that no one can really agree on what that status is. What do these embryos mean to us? And who should be responsible for them? Read the full story.



‚ÄîJessica Hamzelou







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)+ Wow, that is one seriously orange shark!+ TikTok is a proven way to introduce younger generations to older music‚Äîand now it‚Äôs Radiohead‚Äôs turn.+ Why we‚Äôre still going bananas for Donkey Kong after all these years+ This photo perfectly captures the joy of letting loose at a wedding.
‚Ä¢ Creating a qubit fit for a quantum future
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ From pilot to scale: Making agentic AI work in health care
  Over the past 20 years building advanced AI systems‚Äîfrom academic labs to enterprise deployments‚ÄîI‚Äôve witnessed AI‚Äôs waves of success rise and fall. My journey began during the ‚ÄúAI Winter,‚Äù when billions were invested in expert systems that ultimately underdelivered. Flash forward to today: large language models (LLMs) represent a quantum leap forward, but their prompt-based adoption is similarly overhyped, as it‚Äôs essentially a rule-based approach disguised in natural language.



At Ensemble, the leading revenue cycle management (RCM) company for hospitals, we focus on overcoming model limitations by investing in what we believe is the next step in AI evolution: grounding LLMs in facts and logic through neuro-symbolic AI. Our in-house AI incubator pairs elite AI researchers with health-care experts to develop agentic systems powered by a neuro-symbolic AI framework. This bridges LLMs‚Äô intuitive power with the precision of symbolic representation and reasoning.







Overcoming LLM limitations



LLMs excel at understanding nuanced context, performing instinctive reasoning, and generating human-like interactions, making them ideal for agentic tools to then interpret intricate data and communicate effectively. Yet in a domain like health care where compliance, accuracy, and adherence to regulatory standards are non-negotiable‚Äîand where a wealth of structured resources like taxonomies, rules, and clinical guidelines define the landscape‚Äîsymbolic AI is indispensable.



By fusing LLMs and reinforcement learning with structured knowledge bases and clinical logic, our hybrid architecture delivers more than just intelligent automation‚Äîit minimizes hallucinations, expands reasoning capabilities, and ensures every decision is grounded in established guidelines and enforceable guardrails.



Creating a successful agentic AI strategy



Ensemble‚Äôs agentic AI approach includes three core pillars:



1. High-fidelity data sets: By managing revenue operations for hundreds of hospitals nationwide, Ensemble has unparallelled access to one of the most robust administrative datasets in health care. The team has decades of data aggregation, cleansing, and harmonization efforts, providing an exceptional environment to develop advanced applications.



To power our agentic systems, we‚Äôve harmonized more than 2 petabytes of longitudinal claims data, 80,000 denial audit letters, and 80 million annual transactions mapped to industry-leading outcomes. This data fuels our end-to-end intelligence engine, EIQ, providing structured, context-rich data pipelines spanning across the 600-plus steps of revenue operations.



2. Collaborative domain expertise: Partnering with revenue cycle domain experts at each step of innovation, our AI scientists benefit from direct collaboration with in-house RCM experts, clinical ontologists, and clinical data labeling teams. Together, they architect nuanced use cases that account for regulatory constraints, evolving payer-specific logic and the complexity of revenue cycle processes. Embedded end users provide post-deployment feedback for continuous improvement cycles, flagging friction points early and enabling rapid iteration.



This trilateral collaboration‚ÄîAI scientists, health-care experts, and end users‚Äîcreates unmatched contextual awareness that escalates to human judgement appropriately, resulting in a system mirroring decision-making of experienced operators, and with the speed, scale, and consistency of AI, all with human oversight.



3. Elite AI scientists drive differentiation: Ensemble&#8217;s incubator model for research and development is comprised of AI talent typically only found in big tech. Our scientists hold PhD and MS degrees from top AI/NLP institutions like Columbia University and Carnegie Mellon University, and bring decades of experience from FAANG companies [Facebook/Meta, Amazon, Apple, Netflix, Google/Alphabet] and AI startups. At Ensemble, they‚Äôre able to pursue cutting-edge research in areas like LLMs, reinforcement learning, and neuro-symbolic AI within a mission-driven environment.



The also have unparalleled access to vast amounts of private and sensitive health-care data they wouldn‚Äôt see at tech giants paired with compute and infrastructure that startups simply can‚Äôt afford. This unique environment equips our scientists with everything they need to test novel ideas and push the frontiers of AI research‚Äîwhile driving meaningful, real-world impact in health care and improving lives.



Strategy in action: Health-care use cases in production and pilot



By pairing the brightest AI minds with the most powerful health-care resources, we‚Äôre successfully building, deploying, and scaling AI models that are delivering tangible results across hundreds of health systems. Here‚Äôs how we put it into action:



Supporting clinical reasoning: Ensemble deployed neuro-symbolic AI with fine-tuned LLMs to support clinical reasoning. Clinical guidelines are rewritten into proprietary symbolic language and reviewed by humans for accuracy. When a hospital is denied payment for appropriate clinical care, an LLM-based system parses the patient record to produce the same symbolic language describing the patient&#8217;s clinical journey, which is matched deterministically against the guidelines to find the right justification and the proper evidence from the patient‚Äôs record. An LLM then generates a denial appeal letter with clinical justification grounded in evidence.&nbsp;AI-enabled clinical appeal letters have already improved denial overturn rates by 15% or more across Ensemble‚Äôs clients.Building on this success, Ensemble is piloting similar clinical reasoning capabilities for utilization management and clinical documentation improvement, by analyzing real-time records, flagging documentation gaps, and suggesting compliance enhancements to reduce denial or downgrade risks.



Accelerating accurate reimbursement: Ensemble is piloting a multi-agent reasoning model to manage the complex process of collecting accurate reimbursement from health insurers. With this approach, a complex and coordinated system of autonomous agents work together to interpret account details, retrieve required data from various systems, decide account-specific next actions, automate resolution, and escalate complex cases to humans.



This will help reduce payment delays and minimize administrative burden for hospitals and ultimately improve the financial experience for patients.



Improving patient engagement: Ensemble‚Äôs conversational AI agents handle inbound patient calls naturally, routing to human operators as required. Operator assistant agents deliver call transcriptions, surface relevant data, suggest next-best actions, and streamline follow-up routines. According to Ensemble client performance metrics, the combination of these AI capabilities has reduced patient call duration by 35%, increasing one-call resolution rates and improving patient satisfaction by 15%.



The AI path forward in health care demands rigor, responsibility, and real-world impact. By grounding LLMs in symbolic logic and pairing AI scientists with domain experts, Ensemble is successfully deploying scalable AI to improve the experience for health-care providers and the people they serve.



This content was produced by Ensemble. It was not written by MIT Technology Review‚Äôs editorial staff.

üîí Cybersecurity & Privacy
‚Ä¢ Affiliates Flock to ‚ÄòSoulless‚Äô Scam Gambling Machine
  Last month, KrebsOnSecurity tracked the sudden emergence of hundreds of polished online gaming and wagering websites that lure people with free credits and eventually abscond with any cryptocurrency funds deposited by players. We&#8217;ve since learned that these scam gambling sites have proliferated thanks to a new Russian affiliate program called &#8220;Gambler Panel&#8221; that bills itself as a &#8220;soulless project that is made for profit.&#8221;
A machine-translated version of Gambler Panel&#8217;s affiliate website.
The scam begins with deceptive ads posted on social media that claim the wagering sites are working in partnership with popular athletes or social media personalities. The ads invariably state that by using a supplied &#8220;promo code,&#8221; interested players can claim a $2,500 credit on the advertised gaming website.
The gaming sites ask visitors to create a free account to claim their $2,500 credit, which they can use to play any number of extremely polished video games that ask users to bet on each action. However, when users try to cash out any &#8220;winnings&#8221; the gaming site will reject the request and prompt the user to make a ‚Äúverification deposit‚Äù of cryptocurrency ‚Äî typically around $100 ‚Äî before any money can be distributed.
Those who deposit cryptocurrency funds are soon pressed into more wagering and making additional deposits. And &#8212; shocker alert &#8212; all players eventually lose everything they&#8217;ve invested in the platform.
The number of scam gambling or &#8220;scambling&#8221; sites has skyrocketed in the past month, and now we know why: The sites all pull their gaming content and detailed strategies for fleecing players straight from the playbook created by Gambler Panel, a Russian-language affiliate program that promises affiliates up to 70 percent of the profits.

Gambler Panel&#8217;s website gambler-panel[.]com links to a helpful wiki that explains the scam from cradle to grave, offering affiliates advice on how best to entice visitors, keep them gambling, and extract maximum profits from each victim.
&#8220;We have a completely self-written from scratch FAKE CASINO engine that has no competitors,&#8221; Gambler Panel&#8217;s wiki enthuses. &#8220;Carefully thought-out casino design in every pixel, a lot of audits, surveys of real people and test traffic floods were conducted, which allowed us to create something that has no doubts about the legitimacy and trustworthiness even for an inveterate gambling addict with many years of experience.&#8221;
Gambler Panel explains that the one and only goal of affiliates is to drive traffic to these scambling sites by any and all means possible.
A machine-translated portion of Gambler Panel&#8217;s singular instruction for affiliates: Drive traffic to these scambling sites by any means available.
&#8220;Unlike white gambling affiliates, we accept absolutely any type of traffic, regardless of origin, the only limitation is the CIS countries,&#8221; the wiki continued, referring to a common prohibition against scamming people in Russia and former Soviet republics in the Commonwealth of Independent States.
The program&#8217;s website claims it has more than 20,000 affiliates, who earn a minimum of $10 for each verification deposit. Interested new affiliates must first get approval from the group&#8217;s Telegram channel, which currently has around 2,500 active users.
The Gambler Panel channel is replete with images of affiliate panels showing the daily revenue of top affiliates, scantily-clad young women promoting the Gambler logo, and fast cars that top affiliates claimed they bought with their earnings.
A machine-translated version of the wiki for the affiliate program Gambler Panel.
The apparent popularity of this scambling niche is a consequence of the program&#8217;s ease of use and detailed instructions for successfully reproducing virtually every facet of the scam. Indeed, much of the tutorial focuses on advice and ready-made templates to help even novice affiliates drive traffic via social media websites, particularly on Instagram and TikTok.
Gambler Panel also walks affiliates through a range of possible responses to questions from users who are trying to withdraw funds from the platform. This section, titled &#8220;Rules for working in Live chat,&#8221; urges scammers to respond quickly to user requests (1-7 minutes), and includes numerous strategies for keeping the conversation professional and the user on the platform as long as possible.
A machine-translated version of the Gambler Panel&#8217;s instructions on managing chat support conversations with users.
The connection between Gambler Panel and the explosion in the number of scambling websites was made by a 17-year-old developer who operates multiple Discord servers that have been flooded lately with misleading ads for these sites.
The researcher, who asked to be identified only by the nickname &#8220;Thereallo,&#8221; said Gambler Panel has built a scalable business product for other criminals.
&#8220;The wiki is kinda like a &#8216;how to scam 101&#8217; for criminals written with the clarity you would expect from a legitimate company,&#8221; Thereallo said. &#8220;It&#8217;s clean, has step by step guides, and treats their scam platform like a real product. You could swap out the content, and it could be any documentation for startups.&#8221;
&#8220;They&#8217;ve minimized their own risk &#8212; spreading the links on Discord / Facebook / YT Shorts, etc. &#8212; and outsourced it to a hungry affiliate network, just like a franchise,&#8221; Thereallo wrote in response to questions.
&#8220;A centralized platform that can serve over 1,200 domains with a shared user base, IP tracking, and a custom API is not at all a trivial thing to build,&#8221; Thereallo said. &#8220;It&#8217;s a scalable system designed to be a resilient foundation for thousands of disposable scam sites.&#8221;
The security firm Silent Push has compiled a list of the latest domains associated with the Gambler Panel, available here (.csv).
‚Ä¢ DSLRoot, Proxies, and the Threat of ‚ÄòLegal Botnets‚Äô
  The cybersecurity community on Reddit responded in disbelief this month when a self-described Air National Guard member with top secret security clearance began questioning the arrangement they&#8217;d made with company called DSLRoot, which was paying $250 a month to plug a pair of laptops into the Redditor&#8217;s high-speed Internet connection in the United States. This post examines the history and provenance of DSLRoot, one of the oldest &#8220;residential proxy&#8221; networks with origins in Russia and Eastern Europe.

The query about DSLRoot came from a Reddit user &#8220;Sacapoopie,&#8221; who did not respond to questions. This user has since deleted the original question from their post, although some of their replies to other Reddit cybersecurity enthusiasts remain in the thread. The original post was indexed here by archive.is, and it began with a question:
&#8220;I have been getting paid 250$ a month by a residential IP network provider named DSL root to host devices in my home,&#8221; Sacapoopie wrote. &#8220;They are on a separate network than what we use for personal use. They have dedicated DSL connections (one per host) to the ISP that provides the DSL coverage. My family used Starlink. Is this stupid for me to do? They just sit there and I get paid for it. The company pays the internet bill too.&#8221;
Many Redditors said they assumed Sacapoopie&#8217;s post was a joke, and that nobody with a cybersecurity background and top-secret (TS/SCI) clearance would agree to let some shady residential proxy company introduce hardware into their network. Other readers pointed to a slew of posts from Sacapoopie in the Cybersecurity subreddit over the past two years about their work on cybersecurity for the Air National Guard.
When pressed for more details by fellow Redditors, Sacapoopie described the equipment supplied by DSLRoot as &#8220;just two laptops hardwired into a modem, which then goes to a dsl port in the wall.&#8221;

&#8220;When I open the computer, it looks like [they] have some sort of custom application that runs and spawns several cmd prompts,&#8221; the Redditor explained. &#8220;All I can infer from what I see in them is they are making connections.&#8221;
When asked how they became acquainted with DSLRoot, Sacapoopie told another user they discovered the company and reached out after viewing an advertisement on a social media platform.
&#8220;This was probably 5-6 years ago,&#8221; Sacapoopie wrote. &#8220;Since then I just communicate with a technician from that company and I help trouble shoot connectivity issues when they arise.&#8221;
Reached for comment, DSLRoot said its brand has been unfairly maligned thanks to that Reddit discussion. The unsigned email said DSLRoot is fully transparent about its goals and operations, adding that it operates under full consent from its &#8220;regional agents,&#8221; the company&#8217;s term for U.S. residents like Sacapoopie.
&#8220;As although we support honest journalism, we&#8217;re against of all kinds of &#8216;low rank/misleading Yellow Journalism&#8217; done for the sake of cheap hype,&#8221; DSLRoot wrote in reply. &#8220;It&#8217;s obvious to us that whoever is doing this, is either lacking a proper understanding of the subject or doing it intentionally to gain exposure by misleading those who lack proper understanding,&#8221; DSLRoot wrote in answer to questions about the company&#8217;s intentions.
&#8220;We monitor our clients and prohibit any illegal activity associated with our residential proxies,&#8221; DSLRoot continued. &#8220;We honestly didn&#8217;t know that the guy who made the Reddit post was a military guy. Be it an African-American granny trying to pay her rent or a white kid trying to get through college, as long as they can provide an Internet line or host phones for us &#8212; we&#8217;re good.&#8221;
WHAT IS DSLROOT?
DSLRoot is sold as a residential proxy service on the forum BlackHatWorld under the name DSLRoot and GlobalSolutions. The company is based in the Bahamas and was formed in 2012. The service is advertised to people who are not in the United States but who want to seem like they are. DSLRoot pays people in the United States to run the company&#8217;s hardware and software &#8212; including 5G mobile devices &#8212; and in return it rents those IP addresses as dedicated proxies to customers anywhere in the world &#8212; priced at $190 per month for unrestricted access to all locations.
The DSLRoot website.
The GlobalSolutions account on BlackHatWorld lists a Telegram account and a WhatsApp number in Mexico. DSLRoot&#8217;s profile on the marketing agency digitalpoint.com from 2010 shows their previous username on the forum was &#8220;Incorptoday.&#8221; GlobalSolutions user accounts at bitcointalk[.]org and roclub[.]com include the email clickdesk@instantvirtualcreditcards[.]com.
Passive DNS records from DomainTools.com show instantvirtualcreditcards[.]com shared a host back then &#8212; 208.85.1.164 &#8212; with just a handful of domains, including dslroot[.]com, regacard[.]com, 4groot[.]com, residential-ip[.]com, 4gemperor[.]com, ip-teleport[.]com, proxysource[.]net and proxyrental[.]net.
Cyber intelligence firm Intel 471 finds GlobalSolutions registered on BlackHatWorld in 2016 using the email address prepaidsolutions@yahoo.com. This user shared that their birthday is March 7, 1984.
Several negative reviews about DSLRoot on the forums noted that the service was operated by a BlackHatWorld user calling himself &#8220;USProxyKing.&#8221; Indeed, Intel 471 shows this user told fellow forum members in 2013 to contact him at the Skype username &#8220;dslroot.&#8221;
USProxyKing on BlackHatWorld, soliciting installations of his adware via torrents and file-sharing sites.
USProxyKing had a reputation for spamming the forums with ads for his residential proxy service, and he ran a &#8220;pay-per-install&#8221; program where he paid affiliates a small commission each time one of their websites resulted in the installation of his unspecified &#8220;adware&#8221; programs &#8212; presumably a program that turned host PCs into proxies. On the other end of the business, USProxyKing sold that pay-per-install access to others wishing to distribute questionable software &#8212; at $1 per installation.
Private messages indexed by Intel 471 show USProxyKing also raised money from nearly 20 different BlackHatWorld members who were promised shareholder positions in a new business that would offer robocalling services capable of placing 2,000 calls per minute.
Constella Intelligence, a platform that tracks data exposed in breaches, finds that same IP address GlobalSolutions used to register at BlackHatWorld was also used to create accounts at a handful of sites, including a GlobalSolutions user account at WebHostingTalk that supplied¬†the email address incorptoday@gmail.com. Also registered to incorptoday@gmail.com are the domains dslbay[.]com, dslhub[.]net, localsim[.]com, rdslpro[.]com, virtualcards[.]biz/cc, and virtualvisa[.]cc.
Recall that DSLRoot&#8217;s profile on digitalpoint.com was previously named Incorptoday. DomainTools says incorptoday@gmail.com is associated with almost two dozen domains going back to 2008, including incorptoday[.]com, a website that offers to incorporate businesses in several states, including Delaware, Florida and Nevada, for prices ranging from $450 to $550.
As we can see in this archived copy of the site from 2013, IncorpToday also offered a premiere service for $750 that would allow the customer&#8217;s new company to have a retail checking account, with no questions asked.
Global Solutions is able to provide access to the U.S. banking system by offering customers prepaid cards that can be loaded with a variety of virtual payment instruments that were popular in Russian-speaking countries at the time, including WebMoney. The cards are limited to $500 balances, but non-Westerners can use them to anonymously pay for goods and services at a variety of Western companies. Cardnow[.]ru, another domain registered to incorptoday@gmail.com, demonstrates this in action.
A copy of Incorptoday&#8217;s website from 2013 offers non-US residents a service to incorporate a business in Florida, Delaware or Nevada, along with a no-questions-asked checking account, for $750.
WHO IS ANDREI HOLAS?
The oldest domain (2008) registered to incorptoday@gmail.com is andrei[.]me; another is called andreigolos[.]com. DomainTools says these and other domains registered to that email address include the registrant name Andrei Holas, from Huntsville, Ala.
Public records indicate Andrei Holas has lived with his brother &#8212; Aliaksandr Holas &#8212; at two different addresses in Alabama. Those records state that Andrei Holas&#8217; birthday is in March 1984, and that his brother is slightly younger. The younger brother did not respond to a request for comment.
Andrei Holas maintained an account on the Russian social network Vkontakte under the email address ryzhik777@gmail.com, an address that shows up in numerous records hacked and leaked from Russian government entities over the past few years.
Those records indicate Andrei Holas and his brother are from Belarus and have maintained an address in Moscow for some time (that address is roughly three blocks away from the main headquarters of the Russian FSB, the successor intelligence agency to the KGB). Hacked Russian banking records show Andrei Holas&#8217; birthday is March 7, 1984 &#8212; the same birth date listed by GlobalSolutions on BlackHatWorld.
A 2010 post by ryzhik777@gmail.com at the Russian-language forum Ulitka explains that the poster was having trouble getting his B1/B2 visa to visit his brother in the United States, even though he&#8217;d previously been approved for two separate guest visas and a student visa. It remains unclear if one, both, or neither of the Holas brothers still lives in the United States. Andrei explained in 2010 that his brother was an American citizen.
LEGAL BOTNETS
We can all wag our fingers at military personnel who should undoubtedly know better than to install Internet hardware from strangers, but in truth there is an endless supply of U.S. residents who will resell their Internet connection if it means they can make a few bucks out of it. And these days, there are plenty of residential proxy providers who will make it worth your while.
Traditionally, residential proxy networks have been constructed using malicious software that quietly turns infected systems into traffic relays that are then sold in shadowy online forums. Most often, this malware gets bundled with popular cracked software and video files that are uploaded to file-sharing networks and that secretly turn the host device into a traffic relay. In fact, USPRoxyKing bragged that he routinely achieved thousands of installs per week via this method alone.
These days, there a number of residential proxy networks that entice users to monetize their unused bandwidth (inviting you to violate the terms of service of your ISP in the process); others, like DSLRoot, act as a communal VPN, and by using the service you gain access to the connections of other proxies (users) by default, but you also agree to share your connection with others.
Indeed, Intel 471&#8217;s archives show the GlobalSolutions and DSLRoot accounts routinely received private messages from forum users who were college students or young people trying to make ends meet. Those messages show that many of DSLRoot&#8217;s &#8220;regional agents&#8221; often sought commissions to refer friends interested in reselling their home Internet connections (DSLRoot would offer to cover the monthly cost of the agent&#8217;s home Internet connection).
But in an era when North Korean hackers are relentlessly posing as Western IT workers by paying people to host laptop farms in the United States, letting strangers run laptops, mobile devices or any other hardware on your network seems like an awfully risky move regardless of your station in life. As several Redditors pointed out in Sacapoopie&#8217;s thread, an Arizona woman was sentenced in July 2025 to 102 months in prison for hosting a laptop farm that helped North Korean hackers secure jobs at more than 300 U.S. companies, including Fortune 500 firms.
Lloyd Davies is the founder of Infrawatch, a London-based security startup that tracks residential proxy networks. Davies said he reverse engineered the software that powers DSLRoot&#8217;s proxy service, and found it phones home to the aforementioned domain proxysource[.]net, which sells a service that promises to &#8220;get your ads live in multiple cities without getting banned, flagged or ghosted&#8221; (presumably a reference to CraigsList ads).
Davies said he found the DSLRoot installer had capabilities to remotely control residential networking equipment across multiple vendor brands.
Image: Infrawatch.app.
&#8220;The software employs vendor-specific exploits and hardcoded administrative credentials, suggesting DSLRoot pre-configures equipment before deployment,&#8221; Davies wrote in an analysis published today. He said the software performs WiFi network enumeration to identify nearby wireless networks, thereby &#8220;potentially expanding targeting capabilities beyond the primary internet connection.&#8221;
It&#8217;s unclear exactly when the USProxyKing was usurped from his throne, but DSLRoot and its proxy offerings are not what they used to be. Davies said the entire DSLRoot network now has fewer than 300 nodes nationwide, mostly systems on DSL providers like CenturyLink and Frontier.
On Aug. 17, GlobalSolutions posted to BlackHatWorld saying, &#8220;We&#8217;re restructuring our business model by downgrading to &#8216;DSL only&#8217; lines (no mobile or cable).&#8221; Asked via email about the changes, DSLRoot blamed the decline in his customers on the proliferation of residential proxy services.
&#8220;These days it has become almost impossible to compete in this niche as everyone is selling residential proxies and many companies want you to install a piece of software on your phone or desktop so they can resell your residential IPs on a much larger scale,&#8221; DSLRoot explained. &#8220;So-called &#8216;legal botnets&#8217; as we see them.&#8221;

üéì University AI
No updates.

üè¢ Corporate AI
‚Ä¢ Crescent library brings privacy to digital identity systems
  Digital identities, the electronic credentials embedded in phone wallets, workplace logins, and other apps, are becoming ubiquitous. While they offer unprecedented convenience, they also create new privacy risks, particularly around tracking and surveillance.&nbsp;



One of these risks is linkability, the ability to associate one or more uses of a credential to a specific person. Currently, when people use their mobile driver&#8217;s license or log into various apps, hidden identifiers can link these separate activities together, building detailed profiles of user behavior.&nbsp;&nbsp;



To address this, we have released Crescent (opens in new tab), a cryptographic library that adds unlinkability to widely used identity formats, protecting privacy. These include JSON Web Tokens (the authentication standard behind many app logins) and mobile driver&#8217;s licenses. Crescent also works without requiring the organizations that issue these credentials to update their systems. &nbsp;



The protection goes beyond existing privacy features. Some digital identity systems already offer selective disclosure, allowing users to share only specific pieces of information in each interaction. &nbsp;



But even with selective disclosure, credentials can still be linked through serial numbers, cryptographic signatures, or embedded identifiers. Crescent&#8217;s unlinkability feature is designed to prevent anything in the credential, beyond what a user explicitly chooses to reveal, from being used to connect their separate digital interactions.



Figure 1: Unlinkability between a credential issuance and presentation



Two paths to unlinkability&nbsp;



To understand how Crescent works, it helps to examine the two main approaches researchers have developed for adding unlinkability to identity systems:&nbsp;




Specialized cryptographic signature schemes. These schemes can provide unlinkability but require extensive changes to existing infrastructure. New algorithms must be standardized, implemented, and integrated into software and hardware platforms. For example, the BBS (opens in new tab) signature scheme is currently being standardized by the Internet Engineering Task Force (IETF), but even after completion, adoption may be slow.&nbsp;&nbsp;&nbsp;





Zero-knowledge proofs with existing credentials. This approach, used by Crescent (opens in new tab), allows users to prove specific facts about their credentials without revealing the underlying data that could enable tracking. For example, someone could prove they hold a valid driver&#8217;s license and live in a particular ZIP code without exposing any other personal information or identifiers that could link this interaction to future ones.&nbsp;




Zero-knowledge proofs have become more practical since they were first developed 40 years ago but they are not as efficient as the cryptographic algorithms used in today‚Äôs credentials. Crescent addresses this computational challenge through preprocessing, performing the most complex calculations once in advance so that later proof generation is quick and efficient for mobile devices.&nbsp;



Beyond unlinkability, Crescent supports selective disclosure, allowing users to prove specific facts without revealing unnecessary details. For example, it can confirm that a credential is valid and unexpired without disclosing the exact expiration date, which might otherwise serve as a unique identifier. These privacy protections work even when credentials are stored in a phone&#8217;s secure hardware, which keeps them tied to the device and prevents unauthorized access.



	
		

	
	
						
				
					
				
			
			
			

									Azure AI Foundry Labs
				
								Get a glimpse of potential future directions for AI, with these experimental technologies from Microsoft Research.
				
								
					
						
							Azure AI Foundry						
					
				
							
	
Opens in a new tab	
	


Behind the cryptographic curtain&nbsp;



At its core, Crescent uses a sophisticated form of cryptographic proof called a zero-knowledge SNARK (Zero-Knowledge Succinct Noninteractive Argument of Knowledge). This method allows one party to prove possession of information or credentials without revealing the underlying data itself.&nbsp;



Crescent specifically uses the Groth16 proof system, one of the first practical implementations of this technology. What makes Groth16 particularly useful is that its proofs are small in size, quick to verify, and can be shared in a single step without back-and-forth communication between the user and verifier.&nbsp;



The system works by first establishing shared cryptographic parameters based on a credential template. Multiple organizations issuing similar credentials, such as different state motor vehicle departments issuing mobile driver&#8217;s licenses, can use the same parameters as long as they follow compatible data formats and security standards.&nbsp;



The mathematical rules that define what each proof will verify are written using specialized programming tools that convert them into a Rank-1 Constraint System (R1CS), a mathematical framework that describes exactly what needs to be proven about a credential.&nbsp;



To make the system fast enough for real-world use, Crescent splits the proof generation into two distinct stages:&nbsp;




Prepare stage. This step runs once and generates cryptographic values that can be stored on the user&#8217;s device for repeated use.&nbsp;





Show stage. When a user needs to present their credential, this quicker step takes the stored values and randomizes them to prevent any connection to previous presentations. It also creates a compact cryptographic summary that reveals only the specific information needed for that particular interaction.&nbsp;




Figures 2 and 3 illustrate this credential-proving workflow and the division between the prepare and show steps.



Figure 2: Crescent‚Äôs credential-proving workflow includes a compilation of a circuit to R1CS, followed by the prepare and show steps. The output zero-knowledge proof is sent to the verifier. 



Figure 3: The Crescent presentation steps show the division between prepare and show steps.



A sample application&nbsp;



To demonstrate how Crescent works, we created a sample application covering two real-world scenarios: verifying employment and proving age for online access. The application includes sample code for setting up fictional issuers and verifiers as Rust servers, along with a browser-extension wallet for the user. The step numbers correspond to the steps in Figure 4.&nbsp;



Setup&nbsp;




A Crescent service pre-generates the zero-knowledge parameters for creating and verifying proofs from JSON Web Tokens and mobile driver‚Äôs licenses.&nbsp;





The user obtains a mobile driver‚Äôs license from their Department of Motor Vehicles.&nbsp;





The user obtains a proof-of-employment JSON Web Token from their employer, Contoso.&nbsp;





These credentials and their private keys are stored in the Crescent wallet.&nbsp;




Scenarios&nbsp;




Employment verification: The user presents their JSON Web Token to Fabrikam, an online health clinic, to prove they are employed at Contoso and eligible for workplace benefits. Fabrikam learns that the user works at Contoso but not the user&#8217;s identity, while Contoso remains unaware of the interaction.&nbsp;





Age verification: The user presents their mobile driver‚Äôs license to a social network, proving they are over 18. The proof confirms eligibility without revealing their age or identity.&nbsp;




Across both scenarios, Crescent ensures that credential presentations remain unlinkable, preventing any party from connecting them to the user.&nbsp;



For simplicity, the sample defines its own issuance and presentation protocol, but it could be integrated into higher-level identity frameworks such as OpenID/OAuth, Verifiable Credentials, or the mobile driver‚Äôs license ecosystem.



Figure 4. The sample architecture, from credential issuance to presentation.



To learn more about the project, visit the Crescent project GitHub (opens in new tab) page, or check out our recent presentations given at the Real-Word Crypto 2025 (opens in new tab) and North Sec 2025 (opens in new tab) conferences.¬†




Opens in a new tabThe post Crescent library brings privacy to digital identity systems appeared first on Microsoft Research.
‚Ä¢ Meet Boti: The AI assistant transforming how the citizens of Buenos Aires access government information with Amazon Bedrock
  This post is co-written with Julieta Rappan, Macarena Blasi, and Mar√≠a Candela Blanco from the Government of the City of Buenos Aires. 
The Government of the City of Buenos Aires continuously works to improve citizen services. In February 2019, it introduced an AI assistant named Boti available through WhatsApp, the most widely used messaging service in Argentina. With Boti, citizens can conveniently and quickly access a wide variety of information about the city, such as renewing a driver‚Äôs license, accessing healthcare services, and learning about cultural events. This AI assistant has become a preferred communication channel and facilitates more than 3 million conversations each month. 
As Boti grows in popularity, the Government of the City of Buenos Aires seeks to provide new conversational experiences that harness the latest developments in generative AI. One challenge that citizens often face is navigating the city‚Äôs complex bureaucratic landscape. The City Government‚Äôs website includes over 1,300 government procedures, each of which has its own logic, nuances, and exceptions. The City Government recognized that Boti could improve access to this information by directly answering citizens‚Äô questions and connecting them to the right procedure. 
To pilot this new solution, the Government of the City of Buenos Aires partnered with the AWS Generative AI Innovation Center (GenAIIC). The teams worked together to develop an agentic AI assistant using LangGraph and Amazon Bedrock. The solution includes two main components: an input guardrail system and a government procedures agent. The input guardrail uses a custom LLM classifier to analyze incoming user queries, determining whether to approve or block requests based on their content. Approved requests are handled by the government procedures agent, which retrieves relevant procedural information and generates responses. Since most user queries focus on a single procedure, we developed a novel reasoning retrieval system to improve retrieval accuracy. This system initially retrieves comparative summaries that disambiguate similar procedures and then applies a large language model (LLM) to select the most relevant results. The agent uses this information to craft responses in Boti‚Äôs characteristic style, delivering short, helpful, and expressive messages in Argentina‚Äôs Rioplatense Spanish dialect. We focused on distinctive linguistic features of this dialect including the voseo (using ‚Äúvos‚Äù instead of ‚Äút√∫‚Äù) and periphrastic future (using ‚Äúir a‚Äù before verbs). 
In this post, we dive into the implementation of the agentic AI system. We begin with an overview of the solution, explaining its design and main features. Then, we discuss the guardrail and agent subcomponents and assess their performance. Our evaluation shows that the guardrails effectively block harmful content, including offensive language, harmful opinions, prompt injection attempts, and unethical behaviors. The agent achieves up to 98.9% top-1 retrieval accuracy using the reasoning retriever, which marks a 12.5‚Äì17.5% improvement over standard retrieval-augmented generation (RAG) methods. Subject matter experts found that Boti‚Äôs responses were 98% accurate in voseo usage and 92% accurate in periphrastic future usage. The promising results of this solution establish a new era of citizen-government interaction. 
Solution overview 
The Government of the City of Buenos Aires and the GenAIIC built an agentic AI assistant using Amazon Bedrock and LangGraph that includes an input guardrail system to enable safe interactions and a government procedures agent to respond to user questions. The workflow is shown in the following diagram. 
 
The process begins when a user submits a question. In parallel, the question is passed to the input guardrail system and government procedures agent. The input guardrail system determines whether the question contains harmful content. If triggered, it stops graph execution and redirects the user to ask questions about government procedures. Otherwise, the agent continues to formulate its response. The agent either calls a retrieval tool, which allows it to obtain relevant context and metadata from government procedures stored in Amazon Bedrock Knowledge Bases, or responds to the user. Both the input guardrail and government procedures agent use the Amazon Bedrock Converse API for LLM inference. This API provides access to a wide selection of LLMs, helping us optimize performance and latency across different subtasks. 
Input guardrail system 
Input guardrails help prevent the LLM system from processing harmful content. Although Amazon Bedrock Guardrails offers one implementation approach with filters for specific words, content, or sensitive information, we developed a custom solution. This provided us greater flexibility to optimize performance for Rioplatense Spanish and monitor specific types of content. The following diagram illustrates our approach, in which an LLM classifier assigns a primary category (‚Äúapproved‚Äù or ‚Äúblocked‚Äù) as well as a more detailed subcategory. 
 
Approved queries are within the scope of the government procedures agent. They consist of on-topic requests, which focus on government procedures, and off-topic requests, which are low-risk conversation questions that the agent responds to directly. Blocked queries contain high-risk content that Boti should avoid, including offensive language, harmful opinions, prompt injection attacks, or unethical behaviors. 
We evaluated the input guardrail system on a dataset consisting of both normal and harmful user queries. The system successfully blocked 100% of harmful queries, while occasionally flagging normal queries as harmful. This performance balance makes sure that Boti can provide helpful information while maintaining safe and appropriate interactions for users. 
Agent system 
The government procedures agent is responsible for answering user questions. It determines when to retrieve relevant procedural information using its retrieval tool and generates responses in Boti‚Äôs characteristic style. In the following sections, we examine both processes. 
Reasoning retriever 
The agent can use a retrieval tool to provide accurate and up-to-date information about government procedures. Retrieval tools typically employ a RAG framework to perform semantic similarity searches between user queries and a knowledge base containing document chunks stored as embeddings, and then provide the most relevant samples as context to the LLM. Government procedures, however, present challenges to this standard approach. Related procedures, such as renewing and reprinting drivers‚Äô licenses, can be difficult to disambiguate. Additionally, each user question typically requires information from one specific procedure. The mixture of chunks returned from standard RAG approaches increases the likelihood of generating incorrect responses. 
To better disambiguate government procedures, the Buenos Aires and GenAIIC teams developed a reasoning retrieval method that uses comparative summaries and LLM selection. An overview of this approach is shown in the following diagram. 
 
A necessary preprocessing step before retrieval is the creation of a government procedures knowledge base. To capture both the key information contained in procedures and how they related to each other, we created comparative summaries. Each summary contains basic information, such as the procedure‚Äôs purpose, intended audience, and content, such as costs, steps, and requirements. We clustered the base summaries into small groups, with an average cluster size of 5, and used an LLM to generate descriptions about what made each procedure different from its neighbors. We appended the distinguishing descriptions to the base information to create the final summary. We note that this approach shares similarities to Anthropic‚Äôs Contextual Retrieval, which prepends explanatory context to document chunk. 
With the knowledge base in place, we are able to retrieve relevant government procedures based on the user query. The reasoning retriever completes three steps: 
 
 Retrieve M Summaries: We retrieve between 1 and M comparative summaries using semantic search. 
 Optional Reasoning: In some cases, the initial retrieval surfaces similar procedures. To make sure that the most relevant procedures are returned to the agent, we apply an optional LLM reasoning step. The condition for this step occurs when the ratio of the first and second retrieval scores falls below a threshold value. An LLM follows a chain-of-thought (CoT) process in which it compares the user query to the retrieved summaries. It discards irrelevant procedures and reorders the remaining ones based on relevance. If the user query is specific enough, this process typically returns one result. By applying this reasoning step selectively, we minimize latency and token usage while maintaining high retrieval accuracy. 
 Retrieve N Full-Text Procedures: After the most relevant procedures are identified, we fetch their complete documents and metadata from an Amazon DynamoDB table. The metadata contains information like the source URL and the sentiment of the procedure. The agent typically receives between 1 and N results, where N ‚â§ M. 
 
The agent receives the retrieved full text procedures in its context window. It follows its own CoT process to determine the relevant content and URL source attributions when generating its answer. 
We evaluated our reasoning retriever against standard RAG techniques using a synthetic dataset of 1,908 questions derived from known source procedures. The performance was measured by determining whether the correct procedure appeared in the top-k retrieved results for each question. The following plot compares the top-k retrieval accuracy for each approach across different models, arranged in order of ascending performance from left to right. The metrics are proportionally weighted based on each procedure‚Äôs webpage visit frequency, making sure that our evaluation reflects real-world usage patterns. 
 
The first three approaches represent standard vector-based retrieval methods. The first method, Section Titan, involved chunking procedures by document sections, targeting approximately 250 words per chunk, and then embedding the chunks using Amazon Titan Text Embeddings v2. The second method, Summaries Titan, consisted of embedding the procedure summaries using the same embedding model. By embedding summaries rather than document text, the retrieval accuracy improved by 7.8‚Äì15.8%. The third method, Summaries Cohere, involved embedding procedure summaries using Cohere Multilingual v3 on Amazon Bedrock. The Cohere Multilingual embedding model provided a noticeable improvement in retrieval accuracy compared to the Amazon Titan embedding models, with all top-k values above 90%. 
The next three approaches use the reasoning retriever. We embedded the procedure summaries using the Cohere Multilingual model, retrieved 10 summaries during the initial retrieval step, and optionally applied the LLM-based reasoning step using either Anthropic‚Äôs Haiku 3, Claude 3 Sonnet, or Claude 3.5 Sonnet on Amazon Bedrock. All three reasoning retrievers consistently outperform standard RAG techniques, achieving 12.5‚Äì17.5% higher top-k accuracies. Anthropic‚Äôs Claude 3.5 Sonnet delivered the highest performance with 98.9% top-1 accuracy. These results demonstrate how combining embedding-based retrieval with LLM-powered reasoning can improve RAG performance. 
Answer generation 
After collecting the necessary information, the agent responds using Boti‚Äôs distinctive communication style: concise, helpful messages in Rioplatense Spanish. We maintained this voice through prompt engineering that specified the following: 
 
 Personality ‚Äì Convey a warm and friendly tone, providing quick solutions to everyday problems 
 Response length ‚Äì Limit responses to a few sentences 
 Structure ‚Äì Organize content using lists and highlights key information using bold text 
 Expression ‚Äì Use emojis to mark important requirements and add visual cues 
 Dialect ‚Äì Incorporate Rioplatense linguistic features, including voseo, periphrastic future, and regional vocabulary (for example, ‚Äúacordate,‚Äù ‚Äúentrar,‚Äù ‚Äúac√°,‚Äù and ‚Äúall√°‚Äù). 
 
Government procedures often address sensitive topics, like accidents, health, or security. To facilitate appropriate responses, we incorporated sentiment analysis into our knowledge base as metadata. This allows our system to route to different prompt templates. Sensitive topics are directed to prompts with reduced emoji usage and more empathetic language, whereas neutral topics receive standard templates. 
The following figure shows a sample response to a question about borrowing library books. It has been translated to English for convenience. 
 
To validate our prompt engineering approach, subject matter experts at the Government of the City of Buenos Aires reviewed a sample of Boti‚Äôs responses. Their analysis confirmed high fidelity to Rioplatense Spanish, with 98% accuracy in voseo usage and 92% in periphrastic future usage. 
Conclusion 
This post described the agentic AI assistant built by the Government of the City of Buenos Aires and the GenAIIC to respond to citizens‚Äô questions about government procedures. The solution consists of two primary components: an input guardrail system that helps prevent the system from responding to harmful user queries and a government procedures agent that retrieves relevant information and generates responses. The input guardrails effectively block harmful content, including queries with offensive language, harmful opinions, prompt injection, and unethical behaviors. The government procedures agent employs a novel reasoning retrieval method that disambiguates similar government procedures, achieving up to 98.9% top-1 retrieval accuracy and a 12.5‚Äì17.5% improvement over standard RAG methods. Through prompt engineering, responses are delivered in Rioplatense Spanish using Boti‚Äôs voice. Subject matter experts rated Boti‚Äôs linguistic performance highly, with 98% accuracy in voseo usage and 92% in periphrastic future usage. 
As generative AI advances, we expect to continuously improve our solution. The expanding catalog of LLMs available in Amazon Bedrock makes it possible to experiment with newer, more powerful models. This includes models that process text, as explored in the solution in this post, as well as models that process speech, allowing for direct speech-to-speech interactions. We might also explore the fine-tuning capabilities of Amazon Bedrock to customize models so that they better capture the linguistic features of Rioplatense Spanish. Beyond model improvements, we can iterate on our agent framework. The agent‚Äôs tool set can be expanded to support other tasks associated with government procedures like account creation, form completion, and appointment scheduling. As the City Government develops new experiences for citizens, we can consider implementing multi-agent frameworks in which specialist agents, like the government procedures agent, handle specific tasks. 
To learn more about Boti and AWS‚Äôs generative AI capabilities, check out the following resources: 
 
 Boti: The City Chatbot 
 Government of the City of Buenos Aires: Procedures 
 Amazon Bedrock 
 Amazon Bedrock Knowledge Bases 
 
 
 
About the authors 
Julieta Rappan is Director of the Digital Channels Department of the Buenos Aires City Government, where she coordinates the landscape of digital and conversational interfaces. She has extensive experience in the comprehensive management of strategic and technological projects, as well as in leading high-performance teams focused on the development of digital products and services. Her leadership drives the implementation of technological solutions with a focus on scalability, coherence, public value, and innovation‚Äîwhere generative technologies are beginning to play a central role. 
Macarena Blasi is Chief of Staff at the Digital Channels Department of the Buenos Aires City Government, working across the city‚Äôs main digital services, including Boti‚Äîthe WhatsApp-based virtual assistant‚Äîand the official Buenos Aires website. She began her journey working in conversational experience design, later serving as product owner and Operations Manager and then as Head of Experience and Content, leading multidisciplinary teams focused on improving the quality, accessibility, and usability of public digital services. Her work is driven by a commitment to building clear, inclusive, and human-centered experiences in the public sector. 
Mar√≠a Candela Blanco is Operations Manager for Quality Assurance, Usability, and Continuous Improvement at the Buenos Aires Government, where she leads the content, research, and conversational strategy across the city‚Äôs main digital channels, including the Boti AI assistant and the official Buenos Aires website. Outside of tech, Candela studies literature at UNSAM and is deeply passionate about language, storytelling, and the ways they shape our interactions with technology. 
Leandro Micchele is a Software Developer focused on applying AI to real-world use cases, with expertise in AI assistants, voice, and vision solutions. He serves as the technical lead and consultant for the Boti AI assistant at the Buenos Aires Government and works as a Software Developer at Telecom Argentina. Beyond tech, his discipline extends to martial arts: he has over 20 years of experience and currently teaches Aikido. 
Hugo Albuquerque is a Deep Learning Architect at the AWS Generative AI Innovation Center. Before joining AWS, Hugo had extensive experience working as a data scientist in the media and entertainment and marketing sectors. In his free time, he enjoys learning other languages like German and practicing social dancing, such as Brazilian Zouk. 
Enrique Balp is a Senior Data Scientist at the AWS Generative AI Innovation Center working on cutting-edge AI solutions. With a background in the physics of complex systems focused on neuroscience, he has applied data science and machine learning across healthcare, energy, and finance for over a decade. He enjoys hikes in nature, meditation retreats, and deep friendships. 
Diego Galaviz is a Deep Learning Architect at the AWS Generative AI Innovation Center. Before joining AWS, he had over 8 years of expertise as a data scientist across diverse sectors, including financial services, energy, big tech, and cybersecurity. He holds a master‚Äôs degree in artificial intelligence, which complements his practical industry experience. 
Laura Kulowski is a Senior Applied Scientist at the AWS Generative AI Innovation Center, where she works with customers to build generative AI solutions. Before joining Amazon, Laura completed her PhD at Harvard‚Äôs Department of Earth and Planetary Sciences and investigated Jupiter‚Äôs deep zonal flows and magnetic field using Juno data. 
Rafael Fernandes is the LATAM leader of the AWS Generative AI Innovation Center, whose mission is to accelerate the development and implementation of generative AI in the region. Before joining Amazon, Rafael was a co-founder in the financial services industry space and a data science leader with over 12 years of experience in Europe and LATAM.
‚Ä¢ Empowering air quality research with secure, ML-driven predictive analytics
  Air pollution remains one of Africa‚Äôs most pressing environmental health crises, causing widespread illness across the continent. Organizations like sensors.AFRICA have deployed hundreds of air quality sensors to address this challenge, but face a critical data problem: significant gaps in PM2.5 (particulate matter with diameter less than or equal to 2.5 micrometers) measurement records because of power instability and connectivity issues in high-risk regions where physical maintenance is limited. Missing data in PM2.5 datasets reduces statistical power and introduces bias into parameter estimates, leading to unreliable trend detection and flawed conclusions about air quality patterns. These data gaps ultimately compromise evidence-based decision-making for pollution control strategies, health impact assessments, and regulatory compliance. 
In this post, we demonstrate&nbsp;the time-series forecasting capability of&nbsp;Amazon SageMaker Canvas,&nbsp;a&nbsp;low-code no-code (LCNC) machine learning (ML) platform to predict PM2.5 from incomplete datasets. PM2.5 exposure contributes to millions of premature deaths globally through cardiovascular disease, respiratory illness, and systemic health effects, making accurate air quality forecasting a critical public health tool. A key advantage of the forecasting capability of SageMaker Canvas is its robust handling of incomplete data. Traditional air quality monitoring systems often require complete datasets to function properly, meaning they can‚Äôt be relied on when sensors malfunction or require maintenance. In contrast, SageMaker Canvas can generate reliable predictions even when faced with gaps in sensor data. This resilience enables continuous operation of air quality monitoring networks despite inevitable sensor failures or maintenance periods, eliminating costly downtime and data gaps. Environmental agencies and public health officials benefit from uninterrupted access to critical air quality information, enabling timely pollution alerts and more comprehensive long-term analysis of air quality trends. By maintaining operational continuity even with imperfect data inputs, SageMaker Canvas significantly enhances the reliability and practical utility of environmental monitoring systems. 
In this post, we provide a data imputation solution using Amazon SageMaker AI, AWS Lambda, and AWS Step Functions. This solution is designed for environmental analysts, public health officials, and business intelligence professionals who need reliable PM2.5 data for trend analysis, reporting, and decision-making. We sourced our sample training dataset from openAFRICA. Our solution predicts PM2.5 values using time-series forecasting. The sample training dataset contained over 15 million records from March 2022 to Oct 2022 in various parts of Kenya and Nigeria‚Äîdata coming from 23 sensor devices from 15 unique locations. The sample code and workflows can be adapted to create prediction models for your PM2.5 datasets. See our solution‚Äôs README for detailed instructions. 
Solution overview 
The solution consists of two main ML components: a training workflow and an inference workflow. These workflows are built using the following services: 
 
 SageMaker Canvas is used to prepare data and train the prediction model through its no-code interface 
 Batch Transform for inference with Amazon SageMaker AI&nbsp;is used for inference, processing the dataset in bulk to generate predictions 
 Step Functions orchestrates the inferencing process by coordinating the workflow between data retrieval, batch transforming, and database updates, managing workflow state transitions, and making sure that data flows properly through each processing stage 
 Lambda functions perform critical operations at each workflow step: retrieving sensor data from the database in required format, transforming data for model input, sending batches to SageMaker for inferencing, and updating the database with prediction results after processing is complete 
 
At a high level, the solution works by taking a set of PM2.5 data with gaps and predicts the missing values within the range of plus or minus 4.875 micrograms per cubic meter of the actual PM2.5 concentration. It does this by first training a model on the data using inputs for the specific schema and a historical set of values from the user to guide the training process, which is completed with SageMaker Canvas. After the model is trained on a representative dataset and schema, SageMaker Canvas exports the model for use with batch processing. The Step Functions orchestration calls a Lambda function every 24 hours that takes a dataset of new sensor data that has gaps and initiates a SageMaker batch transform job to predict the missing values. The batch transform job processes the entire dataset at once, and the Lambda function then updates the existing dataset with the results. The new completed dataset with predicted values can now be distributed to&nbsp;public health decision-makers who need complete datasets to effectively analyze the patterns of PM2.5 data. 
We dive into each of these steps in later sections of this post. 
Solution walkthrough 
The following diagram shows our solution architecture: 
 
Let‚Äôs explore the architecture step by step: 
 
 To&nbsp;systematically collect, identify, and fill PM2.5 data gaps caused by sensor limitations and connectivity issues,&nbsp;Amazon EventBridge Scheduler invokes a Step Functions state machine every 24 hours.&nbsp;Step Functions orchestrates the calling of various Lambda functions to perform different steps without handling the complexities of error handling, retries, and state management, providing a serverless workflow that seamlessly coordinates the PM2.5 data imputation process. 
 The State Machine invokes a Lambda function in your&nbsp;Amazon Virtual Private Cloud (Amazon VPC) that retrieves records containing missing air quality values from the user‚Äôs air quality database on Amazon Aurora PostgreSQL-Compatible Edition and stores the records in a CSV file in an Amazon Simple Storage Service (Amazon S3) bucket. 
 The State Machine then runs a Lambda function that retrieves the records from Amazon S3 and initiates the SageMaker batch transform job in your VPC using your SageMaker model created from your SageMaker Canvas predictive model trained on historical PM2.5 data. 
 To streamline the batch transform workflow, this solution uses an event-driven approach with EventBridge and Step Functions. EventBridge captures completion events from SageMaker batch transform jobs, while the task token functionality of Step Functions enables extended waiting periods beyond the time limits of Lambda. After processing completes, SageMaker writes the prediction results directly to an S3 bucket. 
 The final step in the state machine retrieves the predicted values from the S3 bucket and then updates the database in&nbsp;Aurora PostgreSQL-Compatible with the values including a predicted label set to true. 
 
Prerequisites 
To implement the PM2.5 data imputation solution, you must have the following: 
 
 An AWS account with&nbsp;AWS Identity and Access Management (IAM)&nbsp;permissions sufficient to deploy the solution and interact with the database. 
 The following AWS services: 
   
   Amazon SageMaker AI 
   AWS Lambda 
   AWS Step Functions 
   Amazon S3 
   Aurora&nbsp;PostgreSQL-Compatible 
   Amazon CloudWatch 
   AWS CloudFormation 
   Amazon Virtual Private Cloud (VPC) 
   Amazon EventBridge 
   IAM for authentication to&nbsp;Aurora&nbsp;PostgreSQL-Compatible 
   AWS Systems Manager Parameter Store 
    
 A local desktop set up with AWS Command Line Interface (AWS CLI) version 2, Python 3.10, AWS Cloud Development Kit (AWS CDK) v2.x, and Git version 2.x. 
 The AWS CLI set up with the necessary credentials in the desired AWS Region. 
 Historical air quality sensor data. Note that our solution requires a fixed schema described in the GitHub repo‚Äôs README. 
 
Deploy the solution 
You will run the following steps to complete the deployment: 
 
 Prepare your environment by building Python modules locally for Lambda layers, deploying infrastructure using the AWS CDK, and initializing your Aurora PostgreSQL database with sensor data. 
 Perform steps in the Build your air quality prediction model section to configure a SageMaker Canvas application, followed by training and registering your model in Amazon SageMaker Model Registry. 
 Create SageMaker model using your registered SageMaker Canvas model by updating infrastructure using the AWS CDK. 
 Manage future configuration changes using the AWS CDK. 
 
Step 1: Deploy AWS infrastructure and upload air quality sensor data 
Complete the following steps to deploy the PM2.5 data imputation solution AWS Infrastructure and upload air quality sensor data to Amazon Aurora RDS: 
 
 Clone the repository to your local desktop environment using the following command: 
 
 
 git clone git@github.com:aws-samples/sample-empowering-air-quality-research-secure-machine-learning-predictive-analytics.git 
 
 
 Change to the project directory: 
 
cd &lt;BASE_PROJECT_FOLDER&gt; 
 
 Follow the deployment steps in the README&nbsp;file up to Model Setup for Batch Transform Inference. 
 
Step 2: Build your air quality prediction model 
After you create the SageMaker AI domain and the SageMaker AI user profile as part of the CDK deployment steps, follow these steps to build your air quality prediction model 
Configure your SageMaker Canvas application 
 
 On the AWS Management Console, go to the&nbsp;SageMaker AI&nbsp;console and select the domain and the user profile that was created under Admin, Configurations, and Domains. 
 Choose the App Configurations tab, scroll down to the Canvas section, and select Edit. 
 In Canvas storage configuration, select Encryption and select the dropdown for aws/s3. 
 In the ML Ops Configuration, turn on the option to Enable Model Registry registration permissions for this user profile. 
   
   Optionally, in the Local file upload configuration&nbsp;section in your domain‚Äôs Canvas App Configuration, you can turn on Enable local file upload. 
    
 Choose Submit to save your configuration choices. 
 In your Amazon SageMaker AI home page, go to the Applications and IDEs section and select Canvas. 
 Select the SageMaker AI user profile that was created for you by the CDK deployment and choose Open Canvas. 
 In a new tab, SageMaker Canvas will start creating your application. This takes a few minutes. 
 
Create and register your prediction model 
In this phase, you develop a prediction model using your historical air quality sensor data. 
 
The preceding architecture diagram illustrates the end-to-end process for training the SageMaker Canvas prediction model, registering that model and creating a SageMaker model for running inference on newly found PM2.5 data gaps.&nbsp;The training process starts by extracting air quality sensor dataset from the database. The dataset is imported into SageMaker Canvas for predictive analysis. This training dataset is transformed and prepared through data wrangling steps implemented by SageMaker Canvas for building and training ML models. 
Prepare data 
Our solution supports a SageMaker Canvas model trained for a single-target variable prediction based on historical data and performs corresponding data imputation for PM2.5 data gaps. To train your model for predictive analysis, follow the comprehensive End to End Machine Learning workflow in the AWS Canvas Immersion Day workshop, adapting each step to prepare your air quality sensor dataset. Begin with the standard workflow until you reach the data preparation section. Here, you can make several customizations: 
 
 Filter dataset for single-target value prediction: Your air quality dataset might contain multiple sensor parameters. For single-target value prediction using this solution, filter the dataset to include only PM2.5 measurements. 
 Clean sensor data: Remove records containing sensor fault values. For example, we filtered out values that equal 65535, because 65535 is a common error code for malfunctioning sensors. Adjust this filtering based on the specific error codes your air quality monitoring equipment produces. 
 
The following image shows our data wrangling Data Flow implemented using above guidance: 
Data Wrangler &gt; Data Flow 
 
 
 Review generated insights and remove irrelevant data: Review the SageMaker Canvas generated insights and analyses. Evaluate them based on time-series forecasting and geospatial temporal data for air quality patterns and relationships between other columns of impact. See chosen columns of impact in GitHub for guidance. Analyze your dataset to identify rows and columns that impact the prediction and remove data that can reduce prediction accuracy. 
 
The following image shows our data wrangling&nbsp;Analyses obtained with implementing the above guidance: 
Data Wrangler &gt; Analyses 
 
Training your prediction model 
After completing your data preparation, proceed to the Train the Model section of the workshop and continue with these specifications: 
 
 Select problem type: Select Predictive Analysis as your ML approach. Because our dataset is tabular and contains a timestamp, a target column that has values we‚Äôre using to forecast future values, and a device ID column, SageMaker Canvas will choose time series forecasting. 
 Define target column: Set Value as your target column for predicting PM2.5 values. 
 Build configuration: Use the Standard Build option for model training because it generally has a higher accuracy. See What happens when you build a model in How custom models work&nbsp;for more information. 
 
By following these steps, you can create a model optimized for PM2.5 dataset predictive analysis, capable of generating valuable insights. Note that SageMaker Canvas supports retraining the ML model for updated PM2.5 datasets. 
Evaluate the model 
After training your model, proceed to Evaluate the model and review column impact, root mean square error (RMSE) score and other advanced metrics to understand your model‚Äôs performance for generating predictions for PM2.5. 
The following image shows our model evaluation statistics achieved. 
 
Add the model to the registry 
Once you are satisfied with your model performance, follow the steps in Register a model version to the SageMaker AI model registry. Make sure to change the approval status to Approved before continuing to run this solution. At the time of this post‚Äôs publication, the approval must be updated in Amazon SageMaker Studio. 
Log out of SageMaker Canvas 
After completing your work in SageMaker Canvas, you can log out or configure your application to automatically terminate the&nbsp;workspace instance. A workspace instance is dedicated for your use every time you launch a Canvas application, and you are billed for as long as the instance runs. Logging out or terminating the workspace instance stops the workspace instance billing. For more information, see billing and cost in SageMaker Canvas. 
Step 3: Create a SageMaker model using your registered SageMaker Canvas model 
In the previous steps, you created a SageMaker domain and user profile through CDK deployment (Step 1) and successfully registered your model (Step 2). Now, it‚Äôs time to create the SageMaker model in your VPC using the SageMaker Canvas model you registered. Follow Model Setup for Batch Inference and Re-Deploy with Updated Configuration sections in the code README for creating SageMaker model. 
Step 4: Manage future configuration changes 
The same deployment pattern applies to any future configuration modifications you might require, including: 
 
 Batch transform instance type optimizations 
 Transform job scheduling changes 
 
Update the relevant parameters in your configuration and run cdk deploy to propagate these changes throughout your solution architecture. 
For a comprehensive list of configurable parameters and their default values, see the configuration file in the repository. 
Execute cdk deploy again to update your infrastructure stack with the your model ID for batch transform operations, replacing the placeholder value initially deployed. This infrastructure-as-code approach helps ensure consistent, version-controlled updates to your data imputation workflow. 
Security best practices 
Security and compliance is a shared responsibility between AWS and the customer, as outlined in the Shared Responsibility Model. We encourage you to review this model for a comprehensive understanding of the respective responsibilities. 
In this solution, we enhanced security by implementing encryption at rest for Amazon S3, Aurora PostgreSQL-Compatible database, and the SageMaker Canvas application. We also enabled encryption in transit by requiring SSL/TLS for all connections from the Lambda functions. We implemented secure database access by providing temporary dynamic credentials through IAM authentication for Amazon RDS, eliminating the need for static passwords. Each Lambda function operates with least privilege access, receiving only the minimal permissions required for its specific function. Finally, we deployed the Lambda functions, Aurora PostgreSQL-Compatible instance, and SageMaker Batch Transform jobs in private subnets of the VPC that do not traverse the public internet. This private network architecture is enabled through VPC endpoints for Amazon S3, SageMaker AI, and&nbsp;AWS Secrets Manager. 
Results 
As shown in the following image, our model, developed using SageMaker Canvas, predicts PM2.5 values with an R-squared of 0.921. Because ML models for PM2.5 prediction frequently achieve R-squared values between 0.80 and 0.98 (see this example from ScienceDirect), our solution is within the range of higher-performing PM2.5 prediction models available today. SageMaker Canvas delivers this performance through its no-code experience, automatically handling model training and optimization without requiring ML expertise from users. 
 
Clean up 
Complete the following steps to clean up your resources: 
 
 SageMaker Canvas application cleanup: 
   
   On the go to the&nbsp;SageMaker AI&nbsp;console and select the domain that was created under Admin&nbsp;Configurations, and Domains. 
   Select the user created under User Profiles&nbsp;for that domain. 
   On the User Details page, navigate to&nbsp;Spaces and Apps, and choose Delete to manually delete your SageMaker AI canvas application and clean up resources. 
    
 SageMaker Domain EFS storage cleanup: 
   
   Open Amazon EFS&nbsp;and in File systems, delete filesystem tagged as ManagedByAmazonSageMakerResource. 
   Open VPC and under Security, navigate to Security groups. 
   On Security groups, select security-group-for-inbound-nfs-&lt;your-sagemaker-domain-id&gt; and delete all Inbound rules associated with that group. 
   On Security groups, select security-group-for-outbound-nfs-&lt;your-sagemaker-domain-id&gt; and delete all associated Outbound rules. 
   Finally, delete both the security groups:&nbsp;security-group-for-inbound-nfs-&lt;your-sagemaker-domain-id&gt; and&nbsp;security-group-for-outbound-nfs-&lt;your-sagemaker-domain-id&gt;. 
    
 Use the AWS CDK to clean up the remaining AWS resources: 
   
   After the preceding steps are complete, return to your local desktop environment where the GitHub repo was cloned, and change to the project‚Äôs infra directory: cd &lt;BASE_PROJECT_FOLDER&gt;/infra 
   Destroy the resources created with AWS CloudFormation using the AWS CDK: cdk destroy 
   Monitor the AWS CDK process deleting resources created by the solution. If there are any errors, troubleshoot using the CloudFormation console and then retry deletion. 
    
 
Conclusion 
The development of accurate PM2.5 prediction models has traditionally required extensive technical expertise, presenting significant challenges for public health researchers studying air pollution‚Äôs impact on disease outcomes. From data preprocessing and feature engineering to model selection and hyperparameter tuning, these technical requirements diverted substantial time and effort away from researchers‚Äô core work of analyzing health outcomes and developing evidence-based interventions.SageMaker Canvas transforms this landscape by dramatically reducing the effort required to develop high-performing PM2.5 prediction models. Public health researchers can now generate accurate predictions without mastering complex ML algorithms, iterate quickly through an intuitive interface, and validate models across regions without manual hyperparameter tuning. With this shift to streamlined, accessible prediction capabilities, researchers can dedicate more time to interpreting results, understanding air pollution‚Äôs impact on community health, and developing protective interventions for vulnerable populations. The result is more efficient research that responds quickly to emerging air quality challenges and informs timely public health decisions. We invite you to implement this solution for your air quality research or ML-based predictive analytics projects. Our comprehensive deployment steps and customization guidance will help you launch quickly and efficiently. As we continue enhancing this solution, your feedback is invaluable for improving its capabilities and maximizing its impact. 
 
About the authors 
Nehal Sangoi is a Senior Technical Account Manager at Amazon Web Services. She provides strategic technical guidance to help independent software vendors plan and build solutions using AWS best practices. Connect with Nehal on LinkedIn. 
Ben Peterson is a Senior Technical Account Manager with AWS. He is passionate about enhancing the developer experience and driving customer success. In his role, he provides strategic guidance on using the comprehensive AWS suite of services to modernize legacy systems, optimize performance, and unlock new capabilities. Connect with Ben on LinkedIn. 
Shashank Shrivastava is a Senior Delivery Consultant and Serverless TFC member at AWS. He is passionate about helping customers and developers build modern applications on serverless architecture. As a pragmatic developer and blogger, he promotes community-driven learning and sharing of technology. His interests are software architecture, developer tools, GenAI, and serverless computing. Connect with Shashank on LinkedIn. 
Akshay Singhal is a Senior Technical Account Manager at Amazon Web Services supporting Enterprise Support customers focusing on the Security ISV segment. He provides technical guidance for customers to implement AWS solutions, with expertise spanning serverless architectures and cost optimization. Outside of work, Akshay enjoys traveling, Formula 1, making short movies, and exploring new cuisines. Connect with Akshay on LinkedIn.
‚Ä¢ How Amazon Finance built an AI assistant using Amazon Bedrock and Amazon Kendra to support analysts for data discovery and business insights
  Finance analysts across Amazon Finance face mounting complexity in financial planning and analysis processes. When working with vast datasets spanning multiple systems, data lakes, and business units, analysts encounter several critical challenges. First, they spend significant time manually browsing data catalogs and reconciling data from disparate sources, leaving less time for valuable analysis and insight generation. Second, historical data and previous business decisions often reside in various documents and legacy systems, making it difficult to use past learnings during planning cycles. Third, as business contexts rapidly evolve, analysts need quick access to relevant metrics, planning assumptions, and financial insights to support data-driven decision-making. 
Traditional tools and processes fall short in addressing these challenges. Keyword-based searches often miss contextual relationships in financial data, and rigid query structures limit analysts‚Äô ability to explore data dynamically. Furthermore, the lack of institutional knowledge preservation means valuable insights and decision rationales often remain siloed or get lost over time, leading to redundant analysis and inconsistent planning assumptions across teams. These challenges significantly impact financial planning efficiency, decision-making agility, and the overall quality of business insights. Analysts needed a more intuitive way to access, understand, and use their organization‚Äôs collective financial knowledge and data assets. 
The Amazon Finance technical team develops and manages comprehensive technology solutions that power financial decision-making and operational efficiency while standardizing across Amazon‚Äôs global operations. In this post, we explain how the team conceptualized and implemented a solution to these business challenges by harnessing the power of generative AI using Amazon Bedrock and intelligent search with Amazon Kendra. 
Solution overview 
To address these business challenges, Amazon Finance developed an AI-powered assistant solution that uses generative AI and enterprise search capabilities. This solution helps analysts interact with financial data sources and documentation through natural language queries, minimizing the need for complex manual searches across multiple systems. The assistant accesses a comprehensive knowledge base of financial documents, historical data, and business context, providing relevant and accurate responses while maintaining enterprise security standards. This approach not only streamlines data discovery but also preserves institutional knowledge and enables more consistent decision-making across the organization. 
The AI assistant‚Äôs methodology consists of two key solution components: intelligent retrieval and augmented generation. The retrieval system uses vector stores, which are specialized databases that efficiently store and search high-dimensional representations of text meanings. Unlike traditional databases that rely on keyword matching, vector stores enable semantic search by converting user queries into vector representations and finding similar vectors in the database. Building on this retrieval foundation, the system employs augmented generation to create accurate and contextual responses. This approach enhances traditional language models by incorporating external knowledge sources during response generation, significantly reducing hallucinations and improving factual accuracy. The process follows three steps: retrieving relevant information from knowledge sources using semantic search, conditioning the language model with this context, and generating refined responses that incorporate the retrieved information. By combining these technologies, the assistant delivers responses that are both contextually appropriate and grounded in verified organizational knowledge, making it particularly effective for knowledge-intensive applications like financial operations and planning. 
We implemented this Retrieval Augmented Generation (RAG) system through a combination of large language models (LLMs) on Amazon Bedrock and intelligent search using Amazon Kendra. 
In the following sections, we discuss the key architectural components that we used in the solution and describe how the overall solution works. 
Amazon Bedrock 
We chose Anthropic‚Äôs Claude 3 Sonnet, a powerful language model, for its exceptional language generation capabilities and ability to understand and reason complex topics. By integrating Anthropic‚Äôs Claude into the RAG module through Amazon Bedrock, the AI assistant can generate contextual and informative responses that seamlessly combine the retrieved knowledge from the vector store with the model‚Äôs natural language processing and generation abilities, resulting in a more human-like and engaging conversational experience. 
Amazon Kendra (Enterprise Edition Index) 
Amazon Kendra offers powerful natural language processing for AI assistant applications. It excels at understanding user questions and finding relevant answers through semantic search. The service works smoothly with generative AI models, particularly in RAG solutions. The enterprise security features in Amazon Kendra support data protection and compliance. Its ability to understand user intent and connect directly with Amazon Bedrock makes it ideal for business assistants. This helps create meaningful conversations using business documents and data catalogs. 
We chose Amazon Kendra Enterprise Edition Index over Amazon OpenSearch Service, primarily due to its sophisticated built-in capabilities and reduced need for manual configuration. Whereas OpenSearch Service requires extensive customization and technical expertise, Amazon Kendra provides out-of-the-box natural language understanding, automatic document processing for over 40 file formats, pre-built enterprise connectors, and intelligent query handling including synonym recognition and refinement suggestions. The service combines keyword, semantic, and vector search approaches automatically, whereas OpenSearch Service requires manual implementation of these features. These features of Amazon Kendra were suitable for our finance domain use case, where accuracy is imperative for usability. 
We also chose Amazon Kendra Enterprise Edition Index over Amazon Q Business for information retrieval, because it stands out as a more robust and flexible solution. Although both tools aim to streamline access to company information, Amazon Kendra offers superior retrieval accuracy and greater control over search parameters. With Amazon Kendra, you can fine-tune relevance tuning, customize document attributes, and implement custom synonyms to enhance search precision. This level of customization helped us tailor the search experience to our specific needs in the Amazon Finance domain and monitor the search results prior to the augmented generation step within user conversations. 
Streamlit 
We selected Streamlit, a Python-based framework for creating interactive web applications, for building the AI assistant‚Äôs UI due to its rapid development capabilities, seamless integration with Python and the assistant‚Äôs backend components, interactive and responsive UI components, potential for data visualization, and straightforward deployment options. With the Streamlit UI, the assistant provides a user-friendly and engaging interface that facilitates natural language interactions while allowing for efficient iteration and deployment of the application. 
Prompt template 
Prompt templates allow for formatting user queries, integrating retrieved knowledge, and providing instructions or constraints for response generation, which are essential for generating contextual and informative responses that combine the language generation abilities of Anthropic‚Äôs Claude with the relevant knowledge retrieved from the search powered by Amazon Kendra. The following is an example prompt: 
 
 """
H: Use the following pieces of context to provide a concise answer to the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.
&lt;context&gt;
{context}
&lt;/context&gt;

Question: {question}

A:
""" 
 
Solution architecture 
The following solution architecture diagram depicts how the key architectural components work with each other to power the solution. 
 
The workflow consists of the following steps: 
 
 The user asks the question in a chat box after authentication. 
 The Streamlit application sends the query to an Amazon Kendra retriever for relevant document retrieval. 
 Amazon Kendra sends the relevant paragraph and document references to the RAG solution. 
 The RAG solution uses Anthropic‚Äôs Claude in Amazon Bedrock along with the prompt template and relevant paragraph as context. 
 The LLM response is sent back to the Streamlit UI. 
 The response is shown to the user along with the feedback feature and session history. 
 The user feedback on responses is stored separately in Amazon Simple Storage Service (Amazon S3) 
 Amazon Kendra indexes relevant documents stored in S3 buckets for document search and retrieval. 
 
Frontend architecture 
We designed the following frontend architecture to allow for rapid modifications and deployment, keeping in mind the scalability and security of the solution. 
 
This workflow consists of the following steps: 
 
 The user navigates to the application URL in their browser. 
 Amazon Route 53 resolves their request to the Amazon CloudFront distribution, which then selects the server closest to the user (to minimize latency). 
 CloudFront runs an AWS Lambda function that makes sure the user has been authenticated. If not, the user is redirected to sign in. After they successfully sign in, they are redirected back to the application website. The flow repeats, and CloudFront triggers the Lambda function again. This time, the user is now able to access the website. 
 Now authenticated, CloudFront returns the assets of the web application. 
 AWS Fargate makes it possible to run containers without having to manage the underlying Amazon Elastic Compute Cloud (Amazon EC2) instances. This allows running containers as a true serverless service. Amazon Elastic Container Service (Amazon ECS) is configured with automatic scaling (target tracking automatic scaling, which scales based on the Application Load Balancer (ALB) requests per target). 
 
Evaluation of the solution‚Äôs performance 
We implemented a comprehensive evaluation framework to rigorously assess the AI assistant‚Äôs performance and make sure it meets the high standards required for financial applications. Our framework was designed to capture both quantitative metrics for measurable performance and qualitative indicators for user experience and response quality. During our benchmarking tests with analysts, we found that this solution dramatically reduced search time by 30% because analysts can now perform natural language search, and it improved the accuracy of search results by 80%. 
Quantitative assessment 
We focused primarily on precision and recall testing, creating a diverse test set of over 50 business queries that represented typical use cases our analysts encounter. Using human-labeled answers as our ground truth, we evaluated the system‚Äôs performance across two main categories: data discovery and knowledge search. In data discovery scenarios, where the system helps analysts locate specific data sources and metrics, we achieved an initial precision rate of 65% and a recall rate of 60% without performing metadata enrichment on the data sources. Although these rates might appear moderate, they represent a significant improvement over the previous manual search process, which had an estimated success rate of only 35% and often required multiple iterations across different systems. The primary reasons for the current rates of the new system were attributed to the lack of rich metadata about data sources and was a good indicator for teams to facilitate better metadata collection of data assets, which is currently underway. 
The knowledge search capability demonstrated initial rates of 83% precision and 74% recall without performing metadata enrichment on data sources. This marked a substantial improvement over traditional keyword-based search methods, which typically achieved only 45‚Äì50% precision in our internal testing. This improvement is particularly meaningful because it translates to analysts finding the right information in their first search attempt roughly 8 out of 10 times, compared to the previous average of 3‚Äì4 attempts needed to locate the same information. 
Qualitative metrics 
The qualitative evaluation centered around the concept of faithfulness‚Äîa critical metric for financial applications where accuracy and reliability are paramount. We employed an innovative LLM-as-a-judge methodology to evaluate how well the AI assistant‚Äôs responses aligned with source documentation and avoided hallucinations or unsupported assertions. The results showed a marked difference between use cases: data discovery achieved a faithfulness score of 70%, and business knowledge search demonstrated an impressive 88% faithfulness. These scores significantly outperform our previous documentation search system, which had no built-in verification mechanism and often led to analysts working with outdated or incorrect information. 
Most importantly, the new system reduced the average time to find relevant information from 45‚Äì60 minutes to just 5‚Äì10 minutes‚Äîan 85% improvement in efficiency. User satisfaction surveys indicate that 92% of analysts prefer the new system over traditional search methods, citing improved accuracy and time savings as key benefits. 
These evaluation results have not only validated our approach but also highlighted specific areas for future enhancement. We continue to refine our evaluation framework as the system evolves, making sure it maintains high standards of accuracy and reliability while meeting the dynamic needs of our financial analysts. The evaluation framework was instrumental in building confidence within our business user community, providing transparent metrics that demonstrate the system‚Äôs capability to handle complex financial queries while maintaining the accuracy standards essential for financial operations. 
Use cases 
Our solution transforms how finance users interact with complex financial and operational data through natural language queries. In this section, we discuss some key examples demonstrating how the system simplifies data discovery. 
Seamless data discovery 
The solution enables users to find data sources through natural language queries rather than requiring technical knowledge of database structures. It uses a sophisticated combination of vector stores and enterprise search capabilities to match user questions with relevant data sources, though careful attention must be paid to context management and preventing over-reliance on previous interactions. Prior to the AI assistant solution, finance analysts needed deep technical knowledge to navigate complex database structures, often spending hours searching through multiple documentation sources just to locate specific data tables. Understanding system workflows required extensive review of technical documentation or reaching out to subject matter experts, creating bottlenecks and reducing productivity. Even experienced users struggled to piece together complete information about business processes from fragmented sources across different systems. Now, analysts can simply ask questions in natural language, such as ‚ÄúWhere can I find productivity metrics?‚Äù, ‚ÄúHow do I access facility information?‚Äù, or ‚ÄúWhich dashboard shows operational data?‚Äù and receive precise, contextual answers. The solution combines enterprise search capabilities with LLMs to understand user intent and deliver relevant information from both structured and unstructured data sources. Analysts now receive accurate directions to specific consolidated reporting tables, clear explanations of business processes, and relevant technical details when needed. In our benchmark tests, for data discovery tasks alone, the system achieved 70% faithfulness and 65% precision, and document search demonstrated even stronger results with 83% precision and 88% faithfulness, without metadata enrichments. 
Assisting understanding of internal business processes from knowledge documentation 
Financial analysts previously faced a steep learning curve when working with enterprise planning tools. The complexity of these systems meant that even basic tasks required extensive documentation review or waiting for support from overwhelmed subject matter experts. New team members could take weeks or months to become proficient, while even experienced users struggled to keep up with system updates and changes. This created a persistent bottleneck in financial operations and planning processes. The introduction of the AI-powered assistant has fundamentally changed how analysts learn and interact with these planning tools. Rather than searching through hundreds of pages of technical documentation, analysts can now ask straightforward questions like ‚ÄúHow do I forecast depreciation for new assets?‚Äù, ‚ÄúHow does the quarterly planning process work?‚Äù or ‚ÄúWhat inputs are needed for the quarterly planning cycle?‚Äù The system provides clear, contextualized explanations drawn from verified documentation and system specifications. Our benchmark tests revealed that it achieved 83% precision and 88% faithfulness in retrieving and explaining technical and business information. New analysts can become productive in a matter of weeks, experienced users can quickly verify procedures, and subject matter experts can focus on more complex challenges rather than routine questions. This represents a significant advancement in making enterprise systems more accessible and efficient, while maintaining the accuracy and reliability required for financial operations.While the technology continues to evolve, particularly in handling nuanced queries and maintaining comprehensive coverage of system updates, it has already transformed the way teams interact with planning tools independently. 
Conclusion 
The AI-powered assistant solution discussed in this post has demonstrated significant improvements in data discovery and business insights generation, delivering multiple key benefits across Amazon Finance. Analysts can now quickly find relevant information through natural language queries, dramatically reducing search time. The system‚Äôs ability to synthesize insights from disparate data sources has notably enhanced data-driven decision-making, and its conversational interface and contextual responses promote self-service data exploration, effectively reducing the burden on centralized data teams. 
This innovative AI assistant solution showcases the practical power of AWS generative AI in transforming enterprise data discovery and document search. By combining Amazon Kendra Enterprise Edition Index, Amazon Bedrock, and advanced LLMs, the implementation achieves impressive precision rates, proving that sophisticated AI-powered search is both achievable and effective. This success demonstrates how AWS generative AI services can meet current business needs while promoting future innovations in enterprise search. These services provide a strong foundation for organizations looking to enhance data discovery processes using natural language to support intelligent enterprise applications. To learn more about implementing AI-powered search solutions, see Build and scale the next wave of AI innovation on AWS and explore AWS AI use cases. 
 
About the authors 
Saikat Gomes&nbsp;is part of the Customer Solutions team in Amazon Web Services. He is passionate about helping enterprises succeed and realize benefits from cloud adoption. He is a strategic advisor to his customers for large-scale cloud transformations involving people, process, and technology. Prior to joining AWS, he held multiple consulting leadership positions and led large- scale transformation programs in the retail industry for over 20 years. He is based out of Los Angeles, California. 
Amit Dhanda serves as a Senior Scientist at Amazon‚Äôs Worldwide Operations Finance team, where he uses AI/ML technologies to solve complex ecommerce challenges. Prior to Amazon, he was Director of Data Science at Adore Me (now part of Victoria‚Äôs Secret), where he enhanced digital retail experiences through recommender systems. He held science leadership roles at EXL and Thomson Reuters, where he developed ML models for customer engagement/growth and text classification.
‚Ä¢ Mercury foundation models from Inception Labs are now available in Amazon Bedrock Marketplace and Amazon SageMaker JumpStart
  Today, we are excited to announce that Mercury and Mercury Coder foundation models (FMs) from Inception Labs are available through Amazon Bedrock Marketplace and Amazon SageMaker JumpStart. With this launch, you can deploy the Mercury FMs to build, experiment, and responsibly scale your generative AI applications on AWS. 
In this post, we demonstrate how to get started with Mercury models on Amazon Bedrock Marketplace and SageMaker JumpStart. 
About Mercury foundation models 
Mercury is the first family of commercial-scale diffusion-based language models, offering groundbreaking advancements in generation speed while maintaining high-quality outputs. Unlike traditional autoregressive models that generate text one token at a time, Mercury models use diffusion to generate multiple tokens in parallel through a coarse-to-fine approach, resulting in dramatically faster inference speeds. Mercury Coder models deliver the following key features: 
 
 Ultra-fast generation speeds of up to 1,100 tokens per second on NVIDIA H100 GPUs, up to 10 times faster than comparable models 
 High-quality code generation across multiple programming languages, including Python, Java, JavaScript, C++, PHP, Bash, and TypeScript 
 Strong performance on fill-in-the-middle tasks, making them ideal for code completion and editing workflows 
 Transformer-based architecture, providing compatibility with existing optimization techniques and infrastructure 
 Context length support of up to 32,768 tokens out of the box and up to 128,000 tokens with context extension approaches 
 
About Amazon Bedrock Marketplace 
Amazon Bedrock Marketplace plays a pivotal role in democratizing access to advanced AI capabilities through several key advantages: 
 
 Comprehensive model selection ‚Äì Amazon Bedrock Marketplace offers an exceptional range of models, from proprietary to publicly available options, so organizations can find the perfect fit for their specific use cases. 
 Unified and secure experience ‚Äì By providing a single access point for models through the Amazon Bedrock APIs, Amazon Bedrock Marketplace significantly simplifies the integration process. Organizations can use these models securely, and for models that are compatible with the Amazon Bedrock Converse API, you can use the robust toolkit of Amazon Bedrock, including Amazon Bedrock Agents, Amazon Bedrock Knowledge Bases, Amazon Bedrock Guardrails, and Amazon Bedrock Flows. 
 Scalable infrastructure ‚Äì Amazon Bedrock Marketplace offers configurable scalability through managed endpoints, so organizations can select their desired number of instances, choose appropriate instance types, define custom automatic scaling policies that dynamically adjust to workload demands, and optimize costs while maintaining performance. 
 
Deploy Mercury and Mercury Coder models in Amazon Bedrock Marketplace 
Amazon Bedrock Marketplace gives you access to over 100 popular, emerging, and specialized foundation models through Amazon Bedrock. To access the Mercury models in Amazon Bedrock, complete the following steps: 
 
 On the Amazon Bedrock console, in the navigation pane under Foundation models, choose Model catalog. 
 
You can also use the Converse API to invoke the model with Amazon Bedrock tooling. 
 
 On the Model catalog page, filter for Inception as a provider and choose the Mercury model. 
 
 
The Model detail page provides essential information about the model‚Äôs capabilities, pricing structure, and implementation guidelines. You can find detailed usage instructions, including sample API calls and code snippets for integration. 
 
 To begin using the Mercury model, choose Subscribe. 
 
 
 
 On the model detail page, choose Deploy. 
 
 
You will be prompted to configure the deployment details for the model. The model ID will be prepopulated. 
 
 For Endpoint name, enter an endpoint name (between 1‚Äì50 alphanumeric characters). 
 For Number of instances, enter a number of instances (between 1‚Äì100). 
 For Instance type, choose your instance type. For optimal performance with Nemotron Super, a GPU-based instance type like ml.p5.48xlarge is recommended. 
 Optionally, you can configure advanced security and infrastructure settings, including virtual private cloud (VPC) networking, service role permissions, and encryption settings. For most use cases, the default settings will work well. However, for production deployments, you might want to review these settings to align with your organization‚Äôs security and compliance requirements. 
 Choose Deploy to begin using the model. 
 
 
When the deployment is complete, you can test its capabilities directly in the Amazon Bedrock playground.This is an excellent way to explore the model‚Äôs reasoning and text generation abilities before integrating it into your applications. The playground provides immediate feedback, helping you understand how the model responds to various inputs and letting you fine-tune your prompts for optimal results. You can use these models with the Amazon Bedrock Converse API. 
SageMaker JumpStart overview 
SageMaker JumpStart is a fully managed service that offers state-of-the-art FMs for various use cases such as content writing, code generation, question answering, copywriting, summarization, classification, and information retrieval. It provides a collection of pre-trained models that you can deploy quickly, accelerating the development and deployment of ML applications. One of the key components of SageMaker JumpStart is model hubs, which offer a vast catalog of pre-trained models, such as Mistral, for a variety of tasks. 
You can now discover and deploy Mercury and Mercury Coder in Amazon SageMaker Studio or programmatically through the SageMaker Python SDK, and derive model performance and MLOps controls with Amazon SageMaker AI features such as Amazon SageMaker Pipelines, Amazon SageMaker Debugger, or container logs. The model is deployed in a secure AWS environment and in your VPC, helping support data security for enterprise security needs. 
Prerequisites 
To deploy the Mercury models, make sure you have access to the recommended instance types based on the model size. To verify you have the necessary resources, complete the following steps: 
 
 On the Service Quotas console, under AWS Services, choose Amazon SageMaker. 
 Check that you have sufficient quota for the required instance type for endpoint deployment. 
 Make sure at least one of these instance types is available in your target AWS Region. 
 If needed, request a quota increase and contact your AWS account team for support. 
 
Make sure your SageMaker AWS Identity and Access Management (IAM) service role has the necessary permissions to deploy the model, including the following permissions to make AWS Marketplace subscriptions in the AWS account used: 
 
 aws-marketplace:ViewSubscriptions 
 aws-marketplace:Unsubscribe 
 aws-marketplace:Subscribe 
 
Alternatively, confirm your AWS account has a subscription to the model. If so, you can skip the following deployment instructions and start with subscribing to the model package. 
Subscribe to the model package 
To subscribe to the model package, complete the following steps: 
 
 Open the model package listing page and choose Mercury or Mercury Coder. 
 On the AWS Marketplace listing, choose Continue to subscribe. 
 On the Subscribe to this software page, review and choose Accept Offer if you and your organization agree with the EULA, pricing, and support terms. 
 Choose Continue to proceed with the configuration and then choose a Region where you have the service quota for the desired instance type. 
 
A product Amazon Resource Name (ARN) will be displayed. This is the model package ARN that you need to specify while creating a deployable model using Boto3. 
Deploy Mercury and Mercury Coder models on SageMaker JumpStart 
For those new to SageMaker JumpStart, you can use SageMaker Studio to access the Mercury and Mercury Coder models on SageMaker JumpStart. 
 
Deployment starts when you choose the Deploy option. You might be prompted to subscribe to this model through Amazon Bedrock Marketplace. If you are already subscribed, choose Deploy. After deployment is complete, you will see that an endpoint is created. You can test the endpoint by passing a sample inference request payload or by selecting the testing option using the SDK. 
 
Deploy Mercury using the SageMaker SDK 
In this section, we walk through deploying the Mercury model through the SageMaker SDK. You can follow a similar process for deploying the Mercury Coder model as well. 
To deploy the model using the SDK, copy the product ARN from the previous step and specify it in the model_package_arn in the following code: 
 
 #Create the model package

endpoint_name = name_from_base("mercury-endpoint") &nbsp;# set this to your liking
model = ModelPackage(role=role_arn, model_package_arn=package_arn, sagemaker_session=sagemaker_session) 
 
Deploy the model: 
 
 # Deploy the Model. This may take 5-10 minutes to run

instance_type = "ml.p5.48xlarge" # We only support ml.p5.48xlarge instances at the moment
start = perf_counter()
deployed_model = model.deploy(initial_instance_count=1, instance_type=instance_type, endpoint_name=endpoint_name)
print(f"\nDeployment took {perf_counter() - start:.2f} seconds") 
 
Use Mercury for code generation 
Let‚Äôs try asking the model to generate a simple tic-tac-toe game: 
 
 payload = {
&nbsp;&nbsp; &nbsp;"messages": [
&nbsp;&nbsp; &nbsp;   {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"role": "user",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"content": """
Build a simple tic-tac-toe game.

REQUIREMENTS:
1. **Game**: 3x3 grid, human vs AI, click to play
2. **AI**: Uses minimax to never lose (only win or draw)
3. **Visualization**: Show AI's move scores in a simple list
4. **Interface**: Grid + "New Game" button + move explanation

IMPLEMENTATION:
- Single HTML file with embedded CSS/JS
- Basic minimax algorithm (no pruning needed)
- Display: "AI chose position 5 (score: +10)" 
- Clean, functional design

DELIVERABLE:
Working game that demonstrates perfect AI play with basic score visibility.
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"""
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp;],
&nbsp;&nbsp; &nbsp;"max_tokens": 2500,
}
start = perf_counter()
outputs = predictor.predict(payload)
eta = perf_counter() - start
print(f"Speed: {outputs['usage']['completion_tokens'] / eta:.2f} tokens / second\n")
print(outputs["choices"][0]["message"]["content"]) 
 
We get the following response: 
 
 Speed: 528.15 tokens / second

```html
&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;title&gt;Tic-Tac-Toe with Unbeatable AI&lt;/title&gt;
&lt;style&gt;
&nbsp;&nbsp;body {
&nbsp;&nbsp; &nbsp;font-family: sans-serif;
&nbsp;&nbsp; &nbsp;display: flex;
&nbsp;&nbsp; &nbsp;flex-direction: column;
&nbsp;&nbsp; &nbsp;align-items: center;
&nbsp;&nbsp; &nbsp;justify-content: center;
&nbsp;&nbsp; &nbsp;height: 100vh;
&nbsp;&nbsp; &nbsp;margin: 0;
&nbsp;&nbsp; &nbsp;background-color: #f0f0f0;
&nbsp;&nbsp;}

&nbsp;&nbsp;.grid {
&nbsp;&nbsp; &nbsp;display: grid;
&nbsp;&nbsp; &nbsp;grid-template-columns: repeat(3, 100px);
&nbsp;&nbsp; &nbsp;gap: 10px;
&nbsp;&nbsp; &nbsp;margin-bottom: 20px;
&nbsp;&nbsp;}

&nbsp;&nbsp;.cell {
&nbsp;&nbsp; &nbsp;width: 100px;
&nbsp;&nbsp; &nbsp;height: 100px;
&nbsp;&nbsp; &nbsp;border: 1px solid #ccc;
&nbsp;&nbsp; &nbsp;display: flex;
&nbsp;&nbsp; &nbsp;align-items: center;
&nbsp;&nbsp; &nbsp;justify-content: center;
&nbsp;&nbsp; &nbsp;font-size: 2em;
&nbsp;&nbsp; &nbsp;cursor: pointer;
&nbsp;&nbsp; &nbsp;background-color: #fff;
&nbsp;&nbsp;}

&nbsp;&nbsp;.cell:hover {
&nbsp;&nbsp; &nbsp;background-color: #e0e0e0;
&nbsp;&nbsp;}

&nbsp;&nbsp;.cell.x {
&nbsp;&nbsp; &nbsp;color: red;
&nbsp;&nbsp;}

&nbsp;&nbsp;.cell.o {
&nbsp;&nbsp; &nbsp;color: blue;
&nbsp;&nbsp;}

&nbsp;&nbsp;#aiMoves {
&nbsp;&nbsp; &nbsp;margin-top: 20px;
&nbsp;&nbsp; &nbsp;font-size: 1.2em;
&nbsp;&nbsp;}

&nbsp;&nbsp;#newGame {
&nbsp;&nbsp; &nbsp;padding: 10px 20px;
&nbsp;&nbsp; &nbsp;font-size: 1.2em;
&nbsp;&nbsp; &nbsp;cursor: pointer;
&nbsp;&nbsp; &nbsp;background-color: #4CAF50;
&nbsp;&nbsp; &nbsp;color: white;
&nbsp;&nbsp; &nbsp;border: none;
&nbsp;&nbsp; &nbsp;border-radius: 5px;
&nbsp;&nbsp;}

&nbsp;&nbsp;#newGame:hover {
&nbsp;&nbsp; &nbsp;background-color: #3e8e41;
&nbsp;&nbsp;}
&lt;/style&gt;
&lt;/head&gt;
&lt;body&gt;

&lt;h1&gt;Tic-Tac-Toe&lt;/h1&gt;

&lt;div class="grid" id="gameGrid"&gt;
&nbsp;&nbsp;&lt;div class="cell" data-index="0"&gt;&lt;/div&gt;
&nbsp;&nbsp;&lt;div class="cell" data-index="1"&gt;&lt;/div&gt;
&nbsp;&nbsp;&lt;div class="cell" data-index="2"&gt;&lt;/div&gt;
&nbsp;&nbsp;&lt;div class="cell" data-index="3"&gt;&lt;/div&gt;
&nbsp;&nbsp;&lt;div class="cell" data-index="4"&gt;&lt;/div&gt;
&nbsp;&nbsp;&lt;div class="cell" data-index="5"&gt;&lt;/div&gt;
&nbsp;&nbsp;&lt;div class="cell" data-index="6"&gt;&lt;/div&gt;
&nbsp;&nbsp;&lt;div class="cell" data-index="7"&gt;&lt;/div&gt;
&nbsp;&nbsp;&lt;div class="cell" data-index="8"&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;button id="newGame"&gt;New Game&lt;/button&gt;

&lt;div id="aiMoves"&gt;&lt;/div&gt;

&lt;script&gt;
&nbsp;&nbsp;const grid = document.getElementById('gameGrid');
&nbsp;&nbsp;const cells = document.querySelectorAll('.cell');
&nbsp;&nbsp;const aiMovesDiv = document.getElementById('aiMoves');
&nbsp;&nbsp;const newGameButton = document.getElementById('newGame');

&nbsp;&nbsp;let board = Array(9).fill(null);
&nbsp;&nbsp;let currentPlayer = 'x';
&nbsp;&nbsp;let gameActive = true;
&nbsp;&nbsp;let aiMoveScores = [];

&nbsp;&nbsp;function checkWin(player) {
&nbsp;&nbsp; &nbsp;const winConditions = [
&nbsp;&nbsp; &nbsp; &nbsp;[0, 1, 2], [3, 4, 5], [6, 7, 8], // Rows
&nbsp;&nbsp; &nbsp; &nbsp;[0, 3, 6], [1, 4, 7], [2, 5, 8], // Columns
&nbsp;&nbsp; &nbsp; &nbsp;[0, 4, 8], [2, 4, 6] // Diagonals
&nbsp;&nbsp; &nbsp;];

&nbsp;&nbsp; &nbsp;return winConditions.some(condition =&gt; {
&nbsp;&nbsp; &nbsp; &nbsp;return condition.every(index =&gt; board[index] === player);
&nbsp;&nbsp; &nbsp;});
&nbsp;&nbsp;}

&nbsp;&nbsp;function checkDraw() {
&nbsp;&nbsp; &nbsp;return board.every(cell =&gt; cell !== null);
&nbsp;&nbsp;}

&nbsp;&nbsp;function switchPlayer() {
&nbsp;&nbsp; &nbsp;currentPlayer = currentPlayer === 'x' ? 'o' : 'x';
&nbsp;&nbsp;}

&nbsp;&nbsp;function updateBoard() {
&nbsp;&nbsp; &nbsp;cells.forEach((cell, index) =&gt; {
&nbsp;&nbsp; &nbsp; &nbsp;cell.textContent = board[index];
&nbsp;&nbsp; &nbsp; &nbsp;cell.classList.remove('x', 'o');
&nbsp;&nbsp; &nbsp; &nbsp;if (board[index] === 'x') {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;cell.classList.add('x');
&nbsp;&nbsp; &nbsp; &nbsp;} else if (board[index] === 'o') {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;cell.classList.add('o');
&nbsp;&nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp;});
&nbsp;&nbsp;}

&nbsp;&nbsp;function minimax(board, player) {
&nbsp;&nbsp; &nbsp;let opponent = player === 'x' ? 'o' : 'x';

&nbsp;&nbsp; &nbsp;if (checkWin(player)) {
&nbsp;&nbsp; &nbsp; &nbsp;return { score: 10 };
&nbsp;&nbsp; &nbsp;} else if (checkWin(opponent)) {
&nbsp;&nbsp; &nbsp; &nbsp;return { score: -10 };
&nbsp;&nbsp; &nbsp;} else if (checkDraw()) {
&nbsp;&nbsp; &nbsp; &nbsp;return { score: 0 };
&nbsp;&nbsp; &nbsp;}

&nbsp;&nbsp; &nbsp;let moves = [];
&nbsp;&nbsp; &nbsp;for (let i = 0; i &lt; board.length; i++) {
&nbsp;&nbsp; &nbsp; &nbsp;if (board[i] === null) {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;board[i] = player;
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;let result = minimax(board, opponent);
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;moves.push({ index: i, score: result.score });
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;board[i] = null; // Undo move
&nbsp;&nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp;}

&nbsp;&nbsp; &nbsp;if (player === 'x') {
&nbsp;&nbsp; &nbsp; &nbsp;let bestScore = -Infinity;
&nbsp;&nbsp; &nbsp; &nbsp;let bestMove = null;
&nbsp;&nbsp; &nbsp; &nbsp;for (let i = 0; i &lt; moves.length; i++) {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;if (moves[i].score &gt; bestScore) {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;bestScore = moves[i].score;
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;bestMove = moves[i];
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp; &nbsp;return bestMove;
&nbsp;&nbsp; &nbsp;} else {
&nbsp;&nbsp; &nbsp; &nbsp;let bestScore = Infinity;
&nbsp;&nbsp; &nbsp; &nbsp;let bestMove = null;
&nbsp;&nbsp; &nbsp; &nbsp;for (let i = 0; i &lt; moves.length; i++) {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;if (moves[i].score &lt; bestScore) {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;bestScore = moves[i].score;
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;bestMove = moves[i];
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp; &nbsp;return bestMove;
&nbsp;&nbsp; &nbsp;}
&nbsp;&nbsp;}

&nbsp;&nbsp;function aiMove() {
&nbsp;&nbsp; &nbsp;if (!gameActive) return;

&nbsp;&nbsp; &nbsp;let bestMove = minimax(board, 'o');
&nbsp;&nbsp; &nbsp;board[bestMove.index] = 'o';
&nbsp;&nbsp; &nbsp;updateBoard();
&nbsp;&nbsp; &nbsp;aiMoveScores.push(`AI chose position ${bestMove.index} (score: ${bestMove.score})`);
&nbsp;&nbsp; &nbsp;aiMovesDiv.textContent = aiMoveScores.join('\n');

&nbsp;&nbsp; &nbsp;if (checkWin('o')) {
&nbsp;&nbsp; &nbsp; &nbsp;alert('AI wins!');
&nbsp;&nbsp; &nbsp; &nbsp;gameActive = false;
&nbsp;&nbsp; &nbsp;} else if (checkDraw()) {
&nbsp;&nbsp; &nbsp; &nbsp;alert('It\'s a draw!');
&nbsp;&nbsp; &nbsp; &nbsp;gameActive = false;
&nbsp;&nbsp; &nbsp;} else {
&nbsp;&nbsp; &nbsp; &nbsp;currentPlayer = 'x';
&nbsp;&nbsp; &nbsp;}
&nbsp;&nbsp;}

&nbsp;&nbsp;function handleCellClick(event) {
&nbsp;&nbsp; &nbsp;const cellIndex = parseInt(event.target.dataset.index);

&nbsp;&nbsp; &nbsp;if (gameActive &amp;&amp; board[cellIndex] === null &amp;&amp; currentPlayer === 'x') {
&nbsp;&nbsp; &nbsp; &nbsp;board[cellIndex] = 'x';
&nbsp;&nbsp; &nbsp; &nbsp;updateBoard();

&nbsp;&nbsp; &nbsp; &nbsp;if (checkWin('x')) {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;alert('You win!');
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;gameActive = false;
&nbsp;&nbsp; &nbsp; &nbsp;} else if (checkDraw()) {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;alert('It\'s a draw!');
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;gameActive = false;
&nbsp;&nbsp; &nbsp; &nbsp;} else {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;aiMove();
&nbsp;&nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp;}
&nbsp;&nbsp;}

&nbsp;&nbsp;function newGame() {
&nbsp;&nbsp; &nbsp;board = Array(9).fill(null);
&nbsp;&nbsp; &nbsp;currentPlayer = 'x';
&nbsp;&nbsp; &nbsp;gameActive = true;
&nbsp;&nbsp; &nbsp;aiMoveScores = [];
&nbsp;&nbsp; &nbsp;aiMovesDiv.textContent = '';
&nbsp;&nbsp; &nbsp;updateBoard();
&nbsp;&nbsp;}

&nbsp;&nbsp;cells.forEach(cell =&gt; cell.addEventListener('click', handleCellClick));
&nbsp;&nbsp;newGameButton.addEventListener('click', newGame);
&lt;/script&gt;

&lt;/body&gt;
&lt;/html&gt;
``` 
 
From the preceding response, we can see that the Mercury model generated a complete, functional tic-tac-toe game with minimax AI implementation at 528 tokens per second, delivering working HTML, CSS, and JavaScript in a single response. The code includes proper game logic, an unbeatable AI algorithm, and a clean UI with the specified requirements correctly implemented. This demonstrates strong code generation capabilities with exceptional speed for a diffusion-based model. 
 
Use Mercury for tool use and function calling 
Mercury models support advanced tool use capabilities, enabling them to intelligently determine when and how to call external functions based on user queries. This makes them ideal for building AI agents and assistants that can interact with external systems, APIs, and databases. 
Let‚Äôs demonstrate Mercury‚Äôs tool use capabilities by creating a travel planning assistant that can check weather and perform calculations: 
 
 # Define available tools for the assistant
tools = [
    {
        "type": "function",
        "function": {
            "name": "get_weather",
            "description": "Get the current weather in a given location",
            "parameters": {
                "type": "object",
                "properties": {
                    "location": {
                        "type": "string",
                        "description": "The city and state, e.g. San Francisco, CA"
                    },
                    "unit": {
                        "type": "string",
                        "enum": ["celsius", "fahrenheit"],
                        "description": "The unit of temperature"
                    }
                },
                "required": ["location"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "calculate",
            "description": "Perform mathematical calculations",
            "parameters": {
                "type": "object",
                "properties": {
                    "expression": {
                        "type": "string",
                        "description": "The mathematical expression to evaluate"
                    }
                },
                "required": ["expression"]
            }
        }
    }
]
#Create a travel planning query that requires multiple tools
payload = {
    "messages": [
        {
            "role": "user",
            "content": "I'm planning a trip to Tokyo. Can you check the weather there and also tell me what 1000 USD is in Japanese Yen (use 1 USD = 150 JPY for calculation)?"
        }
    ],
    "tools": tools,
    "tool_choice": "auto",  # Let the model decide which tools to use
    "max_tokens": 2000,
    "temperature": 0.15
}
# Invoke the endpoint
start = perf_counter()
response = predictor.predict(payload)
eta = perf_counter() - start
# Display the tool calls requested by the model
if 'choices' in response:
    message = response['choices'][0].get('message', {})
    if 'tool_calls' in message:
        print(f"Speed: {response['usage']['completion_tokens'] / eta:.2f} tokens/second\n")
        print(f"Mercury requested {len(message['tool_calls'])} tool calls:\n")
    
        for i, tool_call in enumerate(message['tool_calls'], 1):
            func = tool_call.get('function', {})
            tool_name = func.get('name')
            args = json.loads(func.get('arguments', '{}'))
            
            print(f"Tool Call {i}:")
            print(f"  Function: {tool_name}")
            print(f"  Arguments: {json.dumps(args, indent=4)}")
            print() 
 
Expected response: 
 
 Speed: 892.34 tokens/second
Mercury requested 2 tool calls:
Tool Call 1:
  Function: get_weather
  Arguments: {
    "location": "Tokyo, Japan",
    "unit": "celsius"
  }
Tool Call 2:
  Function: calculate
  Arguments: {
    "expression": "1000 * 150"
  } 
 
After receiving the tool results, you can continue the conversation to get a natural language response: 
 
 # Simulate tool execution results
tool_results = [
    {
        "role": "tool",
        "tool_call_id": message['tool_calls'][0]['id'],
        "content": "The weather in Tokyo, Japan is 18¬∞C and partly cloudy with a chance of rain."
    },
    {
        "role": "tool", 
        "tool_call_id": message['tool_calls'][1]['id'],
        "content": "The result is: 150000"
    }
]
# Continue the conversation with tool results
messages_with_results = [
    {"role": "user", "content": "I'm planning a trip to Tokyo. Can you check the weather there and also tell me what 1000 USD is in Japanese Yen (use 1 USD = 150 JPY for calculation)?"},
    message,  # Assistant's message with tool calls
    *tool_results  # Tool execution results
]
final_payload = {
    "messages": messages_with_results,
    "max_tokens": 500
}
final_response = predictor.predict(final_payload)
print(final_response['choices'][0]['message']['content']) 
 
Expected response: 
 
 Based on the information I've gathered for your Tokyo trip:
**Weather in Tokyo:**
Currently, Tokyo is experiencing mild weather at 18¬∞C (64¬∞F) with partly cloudy skies and a chance of rain. I'd recommend bringing a light jacket and an umbrella just in case.
**Currency Conversion:**
1,000 USD converts to 150,000 Japanese Yen at the rate you specified (1 USD = 150 JPY). This should give you a good amount for expenses like meals, transportation, and shopping in Tokyo.
For your trip planning, the mild temperature is perfect for sightseeing, though you'll want to have rain gear handy. The weather is comfortable for walking around popular areas like Shibuya, Shinjuku, or exploring temples and gardens. 
 
Clean up 
To avoid unwanted charges, complete the steps in this section to clean up your resources. 
Delete the Amazon Bedrock Marketplace deployment 
If you deployed the model using Amazon Bedrock Marketplace, complete the following steps: 
 
 On the Amazon Bedrock console, in the navigation pane, under Foundation models, choose Marketplace deployments. 
 Select the endpoint you want to delete, and on the Actions menu, choose Delete. 
 Verify the endpoint details to make sure you‚Äôre deleting the correct deployment: 
   
   Endpoint name 
   Model name 
   Endpoint status 
    
 Choose Delete to delete the endpoint. 
 In the Delete endpoint confirmation dialog, review the warning message, enter confirm, and choose Delete to permanently remove the endpoint. 
 
 
Delete the SageMaker JumpStart endpoint 
The SageMaker JumpStart model you deployed will incur costs if you leave it running. Use the following code to delete the endpoint if you want to stop incurring charges. For more details, see Delete Endpoints and Resources. 
 
 sm.delete_model(ModelName=sm_model_name)
sm.delete_endpoint_config(EndpointConfigName=endpoint_config_name)
sm.delete_endpoint(EndpointName=endpoint_name) 
 
Conclusion 
In this post, we explored how you can access and deploy Mercury models using Amazon Bedrock Marketplace and SageMaker JumpStart. With support for both Mini and Small parameter sizes, you can choose the optimal model size for your specific use case. Visit SageMaker JumpStart in SageMaker Studio or Amazon Bedrock Marketplace to get started. For more information, refer to Use Amazon Bedrock tooling with Amazon SageMaker JumpStart models, Amazon SageMaker JumpStart Foundation Models, Getting started with Amazon SageMaker JumpStart, Amazon Bedrock Marketplace, and SageMaker JumpStart pretrained models. 
The Mercury family of diffusion-based large language models offers exceptional speed and performance, making it a powerful choice for your generative AI workloads with latency-sensitive requirements. 
 
About the authors 
Niithiyn Vijeaswaran is a Generative AI Specialist Solutions Architect with the Third-Party Model Science team at AWS. His area of focus is AWS AI accelerators (AWS Neuron). He holds a Bachelor‚Äôs degree in Computer Science and Bioinformatics. 
John Liu&nbsp;has 15 years of experience as a product executive and 9 years of experience as a portfolio manager. At AWS, John is a Principal Product Manager for Amazon Bedrock. Previously, he was the Head of Product for AWS Web3 / Blockchain. Prior to AWS, John held various product leadership roles at public blockchain protocols, fintech companies and also spent 9 years as a portfolio manager at various hedge funds. 
Jonathan Evans is a Worldwide Solutions Architect for Generative AI at AWS, where he helps customers leverage cutting-edge AI technologies with Anthropic‚Äôs Claude models on Amazon Bedrock, to solve complex business challenges. With a background in AI/ML engineering and hands-on experience supporting machine learning workflows in the cloud, Jonathan is passionate about making advanced AI accessible and impactful for organizations of all sizes. 
Rohit Talluri is a Generative AI GTM Specialist at Amazon Web Services (AWS). He is partnering with top generative AI model builders, strategic customers, key AI/ML partners, and AWS Service Teams to enable the next generation of artificial intelligence, machine learning, and accelerated computing on AWS. He was previously an Enterprise Solutions Architect and the Global Solutions Lead for AWS Mergers &amp; Acquisitions Advisory. 
Breanne Warner is an Enterprise Solutions Architect at Amazon Web Services supporting healthcare and life science (HCLS) customers. She is passionate about supporting customers to use generative AI on AWS and evangelizing model adoption for first- and third-party models. Breanne is also Vice President of the Women at Amazon board with the goal of fostering inclusive and diverse culture at Amazon. Breanne holds a Bachelor‚Äôs of Science in Computer Engineering from the University of Illinois Urbana-Champaign.

‚∏ª