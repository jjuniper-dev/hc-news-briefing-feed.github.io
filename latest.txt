‚úÖ Morning News Briefing ‚Äì October 29, 2025 10:47

üìÖ Date: 2025-10-29 10:47
üè∑Ô∏è Tags: #briefing #ai #publichealth #digitalgov

‚∏ª

üßæ Weather
‚Ä¢ Current Conditions:  -3.9¬∞C
  Temperature: -3.9&deg;C Pressure / Tendency: 102.7 kPa falling Humidity: 97 % Wind Chill: -5 . Dewpoint: -4.3&deg:C Wind: WNW 2 km/h . Air Quality Health Index: n/a . Pembroke 6:00 AM EDT Wednesday 29 October 2025 temperature: -
‚Ä¢ Wednesday: Sunny. High 11.
  Fog patches dissipating this morning, with wind chill chill minus 6 this morning . High 11.5 degrees Celsius . Fog patches are dissipating today, with winds up to 15 km/h. UV index 3 or moderate. High 11 degrees Celsius, with low wind chill of minus 6 degrees Celsius in the morning . Forecast issued 5:00 AM EDT Wednesday 29 October 2025, with
‚Ä¢ Wednesday night: A few clouds. Low minus 2.
  A few clouds. Wind up to 15 km/h. Wind chill chill minus 6 overnight. Low minus 2.50¬∞F . Wind chill will drop to minus 2¬∞F overnight. Wind chiller minus 6¬∞F tomorrow morning. Low to minus 5¬∞F in the morning's forecast. Forecast issued 5:00 AM EDT Wednesday 29 October 2025. Weather forecasters predict

üåç International News
No updates.

üçÅ Canadian News
No updates.

üá∫üá∏ U.S. Top Stories
‚Ä¢ Trump says he's confident of a deal ahead of Xi meeting
  President Trump has confidently predicted striking a deal with China's leader, who he's set to meet Thursday . President Trump is scheduled to meet with the Chinese leader in Beijing on Thursday . Trump has said he will make a good deal with President Xi Jinping, who is expected to make a bad deal with the U.S. President Xi Xi Jinping is a strong ally of China's president .
‚Ä¢ An Alzheimer's pill appears to protect some in a high-risk population
  A new study is reviving hope that a twice-daily pill can slow down Alzheimer's in people whose genes put them at high risk for the disease . The pill could be used to slow down the disease in people who have genes at risk for Alzheimer's . The drug could also be used in people with genes at high levels of Alzheimer's risk . The study is being carried out in
‚Ä¢ This furloughed IRS lawyer is living out his dream of being a hot dog vendor
  Isaac Stein is fulfilling a childhood dream and passion project as a hot dog vendor . Stein is on furlough from his job at the Naval Reserve in San Diego, California . Stein has a passion project to fulfill his childhood dream of selling hot dogs at hot dog vendors . Stein says he is proud to be a Navy SEAL SEAL SEAL Team One in Washington, D.C., who is
‚Ä¢ Schools close and island life is under threat as Greece reckons with low birth rates
  In Greece, fewer babies means difficult decisions, especially on remote islands . Low birth rates are forcing some schools to close and raising questions about the future of island culture . Some schools in remote islands are closing because of low birth rates, raising concerns about the island's future of culture . Schools in remote Greek islands have been forced to close because of the population's low birth rate, raising questions
‚Ä¢ Camouflaging cars and swapping license plates: How agents make immigration arrests
  Immigration enforcement officers are sometimes forgoing license plates or otherwise masking their cars while apprehending migrants across the U.S. They sometimes mask their cars to avoid detection of illegal immigrants . Officers are sometimes not wearing license plates, masks or masks when they are apprehending illegal immigrants in the United States . They are often forgoing plates or masks in their vehicles to apprehend illegal immigrants across the

üß† Artificial Intelligence
No updates.

üíª Digital Strategy
‚Ä¢ Bolt Graphics unveils Zeus GPU built on RISC-V and path tracing tech
  Bolt Graphics's Zeus graphics accelerator hardware is unlike any conventional GPUs . Onboard cores use a Linux stack based on Ubuntu Ubuntu Summit 25.10 . These are very unlike conventional GPUs ‚Äì or indeed anything else . Bolt Graphics' Zeus accelerator hardware will be on sale at the end of the year at a price of $99,000 (¬£100,000) Bolt Graphics has a range of processors
‚Ä¢ UK government on the lookout for bargain-priced CTO
  UK government is on the hunt for a new CTO after incumbent David Knott announced his departure, citing family reasons . UK government Dangles ¬£100K for someone to fix ¬£23B tech mess . UK CTO is seeking a new leader after incumbent Knott said he was leaving the UK for 'family reasons' UK government on hunt for new CEO after Knott's departure .
‚Ä¢ Zen Internet loses unfair dismissal appeal case with former CEO
  Judges agree broadband biz didn't follow its own procedures when booting boss Paul Stobart . UK ISP Zen Internet loses appeal against ruling that it unfairly dismissed former CEO Paul Stoart . Zen Internet has lost an appeal against a ruling that the company unfairly dismissed Stoar . The broadband firm has lost its appeal against the ruling against the decision . The decision was overturned by a
‚Ä¢ 9 in 10 Exchange servers in Germany still running out-of-support software
  92 percent of the nation's Exchange boxes are still running out-of-support software . Microsoft axed versions 2016 and 2019 versions of Exchange boxes . BISI urges organizations to upgrade or risk total network compromise . Germany's infosec office (BSI) is sounding the alarm after finding that 92% of the country's Exchange Boxes are running out of support software . B
‚Ä¢ Starlink tells the world it has over 150 sextillion IPv6 addresses
  Internetworking wonks have investigated Starlink‚Äôs use of IP addresses and found some interesting facts . But the data describing where they're used - which is used - isn't very useful - isn‚Äôt very useful . Starlink's IP addresses help to fight crime - but the data is not useful to the public . The data is based on the IP addresses used by Star

üè• Public Health
No updates.

üî¨ Science
‚Ä¢ Cost effectiveness analysis of low dose computed tomography lung cancer screening in Chinese population
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ This ‚Äòminor‚Äô bird flu strain has potential to spark human pandemic
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Evaluating voluntary care seeking effects on COVID-19 outcomes and health system costs
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Applying machine learning to predict quality ANC determinants in Bangladesh: a BDHS-2022 cross-sectional study
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Perceptions of air pollution from stubble burning and its health risks in Punjab, India
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations

üßæ Government & Policy
No updates.

üèõÔ∏è Enterprise Architecture & IT Governance
No updates.

ü§ñ AI & Emerging Tech
‚Ä¢ The AI Hype Index: Data centers‚Äô neighbors are pivoting to power blackouts
  Separating AI reality from hyped-up fiction isn't always easy . The AI Hype Index is a simple, at-a-glance summary of everything you need to know about the state of the industry . For most of AI companies‚Äô eager customers, the hundreds of billions of dollars they‚Äôre pumping into the industry aren‚Äôt adding up. And maybe they never will.
‚Ä¢ DeepSeek may have found a new way to improve AI‚Äôs ability to remember
  An AI model released by Chinese AI company DeepSeek uses new techniques that could significantly improve AI‚Äôs ability to ‚Äúremember.‚Äù



Released last week, the optical character recognition (OCR) model works by extracting text from an image and turning it into machine-readable words. This is the same technology that powers scanner apps, translation of text in photos, and many accessibility tools.&nbsp;



OCR is already a mature field with numerous high-performing systems, and according to the paper and some early reviews, DeepSeek‚Äôs new model performs on par with top models on key benchmarks.



But researchers say the model‚Äôs main innovation lies in how it processes information‚Äîspecifically, how it stores and retrieves memories. Improving how AI models ‚Äúremember‚Äù information could reduce how much computing power they need to run, thus mitigating AI‚Äôs large (and growing) carbon footprint.&nbsp;





Currently, most large language models break text down into thousands of tiny units called tokens. This turns the text into representations that models can understand. However, these tokens quickly become expensive to store and compute with as conversations with end users grow longer. When a user chats with an AI for lengthy periods, this challenge can cause the AI to forget things the user has already told it and get information muddled, a problem some call ‚Äúcontext rot.‚Äù



The new methods developed by DeepSeek (and published in its latest paper) could help to overcome this issue. Instead of storing words as tokens, its system packs written information into image form, almost as if it‚Äôs taking a picture of pages from a book. This allows the model to retain nearly the same information while using far fewer tokens, the researchers found.&nbsp;



Essentially, the OCR model is a testbed for these new methods that permit more information to be packed into AI models more efficiently.&nbsp;



Besides using visual tokens instead of just text ones, the model is built on a type of tiered compression that is not unlike how human memories fade: Older or less critical content is stored in a slightly more blurry form in order to save space. Despite that, the paper‚Äôs authors argue that this compressed content can still remain accessible in the background, while maintaining a high level of system efficiency.



Text tokens have long been the default building block in AI systems. Using visual tokens instead is unconventional, and as a result, DeepSeek‚Äôs model is quickly capturing researchers‚Äô attention. Andrej Karpathy, the former Tesla AI chief and a founding member of OpenAI, praised the paper on X, saying that images may ultimately be better than text as inputs for LLMs. Text tokens might be ‚Äúwasteful and just terrible at the input,‚Äù he wrote.&nbsp;



Manling Li, an assistant professor of computer science at Northwestern University, says the paper offers a new framework for addressing the existing challenges in AI memory. ‚ÄúWhile the idea of using image-based tokens for context storage isn‚Äôt entirely new, this is the first study I‚Äôve seen that takes it this far and shows it might actually work,‚Äù Li says.



The method could open up new possibilities in AI research and applications, especially in creating more useful AI agents, says Zihan Wang, a PhD candidate at Northwestern University. He believes that since conversations with AI are continuous, this approach could help models remember more and assist users more effectively.



The technique can also be used to produce more training data for AI models. Model developers are currently grappling with a severe shortage of quality text to train systems on. But the DeepSeek paper says that the company‚Äôs OCR system can generate over 200,000 pages of training data a day on a single GPU.



The model and paper, however, are only an early exploration of using image tokens rather than text tokens for AI memorization. Li says she hopes to see visual tokens applied not just to memory storage but also to reasoning. Future work, she says, should explore how to make AI‚Äôs memory fade in a more dynamic way, akin to how we can recall a life-changing moment from years ago but forget what we ate for lunch last week. Currently, even with DeepSeek‚Äôs methods, AI tends to forget and remember in a very linear way‚Äîrecalling whatever was most recent, but not necessarily what was most important, she says.&nbsp;



Despite its attempts to keep a low profile, DeepSeek, based in Hangzhou, China, has built a reputation for pushing the frontier in AI research. The company shocked the industry at the start of this year with the release of DeepSeek-R1, an open-source reasoning model that rivaled leading Western systems in performance despite using far fewer computing resources.&nbsp;
‚Ä¢ Roundtables: Seeking Climate Solutions in Turbulent Times
  Drawing from MIT Technology Review‚Äôs 10 Climate Tech Companies to Watch list, this session highlights the most promising technologies . Companies are pursuing climate solutions amid shifting U.S. politics and economic uncertainty . Speakers: Casey Crownhart, James Temple, Mary Beth Griggs, Science Editor¬†Mary Beth¬†Griggs, Casey¬†Crownhart,¬†James¬†Temple,¬†Mary¬†griggs¬†and¬†Casey Crownhart .
‚Ä¢ Finding return on AI investments across industries
  The market is officially three years post ChatGPT and many of the pundit bylines have shifted to using terms like ‚Äúbubble‚Äù to suggest reasons behind generative AI not realizing material returns outside a handful of technology suppliers.&nbsp;



In September, the MIT NANDA report made waves because the soundbite every author and influencer picked up on was that 95% of all AI pilots failed to scale or deliver clear and measurable ROI. McKinsey earlier published a similar trend indicating that agentic AI would be the way forward to achieve huge operational benefits for enterprises. At The Wall Street Journal‚Äôs Technology Council Summit, AI technology leaders recommended CIOs stop worrying about AI‚Äôs return on investment because measuring gains is difficult and if they were to try, the measurements would be wrong.&nbsp;







This places technology leaders in a precarious position‚Äìrobust tech stacks already sustain their business operations, so what is the upside to introducing new technology?&nbsp;



For decades, deployment strategies have followed a consistent cadence where tech operators avoid destabilizing business-critical workflows to swap out individual components in tech stacks. For example, a better or cheaper technology is not meaningful if it puts your disaster recovery at risk.¬†



While the price might increase when a new buyer takes over mature middleware, the cost of losing part of your enterprise data because you are mid-way through transitioning your enterprise to a new technology is way more severe than paying a higher price for a stable technology that you‚Äôve run your business on for 20 years.



So, how do enterprises get a return on investing in the latest tech transformation?



First principle of AI: Your data is your value



Most of the articles about AI data relate to engineering tasks to ensure that an AI model infers against business data in repositories that represent past and present business realities.¬†



However, one of the most widely-deployed use cases in enterprise AI begins with prompting an AI model by uploading file attachments into the model. This step narrows an AI model‚Äôs range to the content of the uploaded files, accelerating accurate response times and reducing the number of prompts required to get the best answer.&nbsp;



This tactic relies upon sending your proprietary business data into an AI model, so there are two important considerations to take in parallel with data preparation: first, governing your system for appropriate confidentiality; and second, developing a deliberate negotiation strategy with the model vendors, who cannot advance their frontier models without getting access to non-public data, like your business‚Äô data.¬†



Recently, Anthropic and OpenAI completed massive deals with enterprise data platforms and owners because there is not enough high-value primary data publicly available on the internet.&nbsp;



Most enterprises would automatically prioritize confidentiality of their data and design business workflows to maintain trade secrets. From an economic value point of view, especially considering how costly every model API call really is, exchanging selective access to your data for services or price offsets may be the right strategy. Rather than approaching model purchase/onboarding as a typical supplier/procurement exercise, think through the potential to realize mutual benefits in advancing your suppliers‚Äô model and your business adoption of the model in tandem.



Second principle of AI: Boring by design



According to Information is Beautiful, in 2024 alone, 182 new generative AI models were introduced to the market. When GPT5 came into the market in 2025, many of the models from 12 to 24 months prior were rendered unavailable until subscription customers threatened to cancel. Their previously stable AI workflows were built on models that no longer worked. Their tech providers thought the customers would be excited about the newest models and did not realize the premium that business workflows place on stability. Video gamers are happy to upgrade their custom builds throughout the entire lifespan of the system components in their gaming rigs, and will upgrade the entire system just to play a newly released title.&nbsp;



However, behavior does not translate to business run rate operations. While many employees may use the latest models for document processing or generating content, back-office operations can‚Äôt sustain swapping a tech stack three times a week to keep up with the latest model drops. The back-office work is boring by design.



The most successful AI deployments have focused on deploying AI on business problems unique to their business, often running in the background to accelerate or augment mundane but mandated tasks. Relieving legal or expense audits from having to manually cross check individual reports but putting the final decision in a humans‚Äô responsibility zone combines the best of both.&nbsp;



The important point is that none of these tasks require constant updates to the latest model to deliver that value. This is also an area where abstracting your business workflows from using direct model APIs can offer additional long-term stability while maintaining options to update or upgrade the underlying engines at the pace of your business.



Third principle of AI: Mini-van economics



The best way to avoid upside-down economics is to design systems to align to the users rather than vendor specs and benchmarks.¬†



Too many businesses continue to fall into the trap of buying new gear or new cloud service types based on new supplier-led benchmarks rather than starting their AI journey from what their business can consume, at what pace, on the capabilities they have deployed today.&nbsp;



While Ferrari marketing is effective and those automobiles are truly magnificent, they drive the same speed through school zones and lack ample trunk space for groceries. Keep in mind that every remote server and model touched by a user layers on the costs and design for frugality by reconfiguring workflows to minimize spending on third-party services.¬†



Too many companies have found that their customer support AI workflows add millions of dollars of operational run rate costs and end up adding more development time and cost to update the implementation for OpEx predictability. Meanwhile, the companies that decided that a system running at the pace a human can read‚Äîless than 50 tokens per second‚Äîwere able to successfully deploy scaled-out AI applications with minimal additional overhead.



There are so many aspects of this new automation technology to unpack‚Äîthe best guidance is to start practical, design for independence in underlying technology components to keep from disrupting stable applications long term, and to leverage the fact that AI technology makes your business data valuable to the advancement of your tech suppliers&#8217; goals.



This content was produced by Intel. It was not written by MIT Technology Review‚Äôs editorial staff.
‚Ä¢ The Download: Microsoft‚Äôs stance on erotic AI, and an AI hype mystery
  This is today&#8217;s edition of¬†The Download,¬†our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



‚ÄúWe will never build a sex robot,‚Äù says Mustafa Suleyman



Mustafa Suleyman, CEO of Microsoft AI, is trying to walk a fine line. On the one hand, he thinks that the industry is taking AI in a dangerous direction by building chatbots that present as human: He worries that people will be tricked into seeing life instead of lifelike behavior.On the other hand, Suleyman runs a product shop that must compete with those peers. Last week, Microsoft announced a string of updates to its Copilot chatbot designed to make Copilot more expressive, engaging, and helpful.Will Douglas Heaven, our senior AI editor, talked to Suleyman about the tension at play when it comes to designing our interactions with chatbots and his ultimate vision for what this new technology should be. Read the full story.







An AI adoption riddle



‚ÄîJames O‚ÄôDonnell, senior AI reporter¬†



A few weeks ago, I set out on what I thought would be a straightforward reporting journey.After years of momentum for AI, hype had been slightly punctured. First there was the underwhelming release of GPT-5 in August. Then a report released two weeks later found that 95% of generative AI pilots were failing, which caused a brief stock market panic. I wanted to know: Which companies are spooked enough to scale back their AI spending?But if AI‚Äôs hype has indeed been punctured, I couldn‚Äôt find a company willing to talk about it. So what should we make of my failed quest?



This story originally appeared in The Algorithm, our weekly newsletter on AI. To get stories like this in your inbox first, sign up here.







The must-reads



I‚Äôve combed the internet to find you today‚Äôs most fun/important/scary/fascinating stories about technology.



1 Hundreds of thousands of ChatGPT users exhibit severe mental health symptomsThat‚Äôs according to estimates from OpenAI, which says it has tweaked GPT-5 to respond more effectively to users in distress. (Wired $)+ OpenAI won‚Äôt lock access to force users to take a break, though. (Gizmodo)+ Why AI should be able to ‚Äúhang up‚Äù on you. (MIT Technology Review)



2 Elon Musk has launched his answer to WikipediaGrokipedia‚Äôs right-leaning entries reflect the way the billionaire sees the world. (WP $)+ Several pages perpetuate historical inaccuracies and conservative views. (Wired $)+ The AI-generated encyclopedia briefly crashed shortly after it launched. (Engadget)3 Surgeons have removed a pig kidney from a patientIt was the longest-functioning genetically engineered pig kidney so far. (Wired $)+ ‚ÄúSpare‚Äù living human bodies might provide us with organs for transplantation. (MIT Technology Review)



4 Amazon is planning to cut up to 30,000 corporate jobsPartly in response to staff‚Äôs reluctance to return to the office five days a week. (Reuters)+ The company is planning yet another round of layoffs in January. (NYT $)



5 Older people can‚Äôt get enough of screensTheir digital habits mirror the high usage typically observed among teenagers. (Economist $)



6 A British cyclist has been given a 3D-printed faceDave Richards received severe third-degree burns to his head after being struck by a drunk driver. (The Guardian)



7 The twitter.com domain is being shut downMake sure you re-enroll your security and passkeys before the big switch-off. (Fast Company $)+ It means the abandoned accounts could be sold on. (The Verge)+ But 2FA apps should be fine‚Äîin theory. (The Register)



8 When is a moon not a moon?Believe it or not, we don‚Äôt have an official definition. (The Atlantic $)+ Astronomers have spotted a ‚Äúquasi-moon‚Äù hovering near Earth. (BBC)+ The moon is just the beginning for this waterless concrete. (MIT Technology Review)



9 Threads‚Äô ghost posts will disappear after 24 hoursIf anyone saw them in the first place, that is. (TechCrunch)10 In the metaverse, anyone can be a K-pop superstarVirtual idols are gaining huge popularity, before crossing over into real-world fame. (Rest of World)+ Meta‚Äôs former metaverse head has been moved into its AI team. (FT $)







Quote of the day



‚ÄúThe impulse to control knowledge is as old as knowledge itself. Controlling what gets written is a way to gain or keep power.‚Äù



‚ÄîRyan McGrady, senior research fellow at the University of Massachusetts Amherst, reflects on Elon Musk‚Äôs desire to create his own online encyclopedia to the New York Times.







One more thing







Inside Amsterdam‚Äôs high-stakes experiment to create fair welfare AIAmsterdam thought it was on the right track. City officials in the welfare department believed they could build technology that would prevent fraud while protecting citizens‚Äô rights. They followed these emerging best practices and invested a vast amount of time and money in a project that eventually processed live welfare applications. But in their pilot, they found that the system they‚Äôd developed was still not fair and effective. Why?Lighthouse Reports, MIT Technology Review, and the Dutch newspaper Trouw have gained unprecedented access to the system to try to find out. Read about what we discovered.



‚ÄîEileen Guo, Gabriel Geiger &amp; Justin-Casimir Braun







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)



+ Happy 70th birthday to Bill Gates, who is not revered enough for his chair-jumping skills.+ Bring back Guitar Hero‚Äîthe iconic game that convinced us all we were capable of knocking out Heart‚Äôs Barracuda (note: the majority of us were not.)+ Even the swankiest parts of London aren‚Äôt immune to rumours of ghostly hauntings.+ Justice for medieval frogs and their unfair reputation!

üîí Cybersecurity & Privacy
‚Ä¢ Aisuru Botnet Shifts from DDoS to Residential Proxies
  Aisuru, the botnet responsible for a series of record-smashing distributed denial-of-service (DDoS) attacks this year, recently was overhauled to support a more low-key, lucrative and sustainable business: Renting hundreds of thousands of infected Internet of Things (IoT) devices to proxy services that help cybercriminals anonymize their traffic. Experts say a glut of proxies from Aisuru and other sources is fueling large-scale data harvesting efforts tied to various artificial intelligence (AI) projects, helping content scrapers evade detection by routing their traffic through residential connections that appear to be regular Internet users.

First identified in August 2024, Aisuru has spread to at least 700,000 IoT systems, such as poorly secured Internet routers and security cameras. Aisuru&#8217;s overlords have used their massive botnet to clobber targets with headline-grabbing DDoS attacks, flooding targeted hosts with blasts of junk requests from all infected systems simultaneously.
In June, Aisuru hit KrebsOnSecurity.com with a DDoS clocking at 6.3 terabits per second &#8212; the biggest attack that Google had ever mitigated at the time. In the weeks and months that followed, Aisuru&#8217;s operators demonstrated DDoS capabilities of nearly 30 terabits of data per second &#8212; well beyond the attack mitigation capabilities of most Internet destinations.
These digital sieges have been particularly disruptive this year for U.S.-based Internet service providers (ISPs), in part because Aisuru recently succeeded in taking over a large number of IoT devices in the United States. And when Aisuru launches attacks, the volume of outgoing traffic from infected systems on these ISPs is often so high that it can disrupt or degrade Internet service for adjacent (non-botted) customers of the ISPs.
&#8220;Multiple broadband access network operators have experienced significant operational impact due to outbound DDoS attacks in excess of 1.5Tb/sec launched from Aisuru botnet nodes residing on end-customer premises,&#8221; wrote Roland Dobbins, principal engineer at Netscout, in a recent executive summary on Aisuru. &#8220;Outbound/crossbound attack traffic exceeding 1Tb/sec from compromised customer premise equipment (CPE) devices has caused significant disruption to wireline and wireless broadband access networks. High-throughput attacks have caused chassis-based router line card failures.&#8221;
The incessant attacks from Aisuru have caught the attention of federal authorities in the United States and Europe (many of Aisuru&#8217;s victims are customers of ISPs and hosting providers based in Europe). Quite recently, some of the world&#8217;s largest ISPs have started informally sharing block lists identifying the rapidly shifting locations of the servers that the attackers use to control the activities of the botnet.
Experts say the Aisuru botmasters recently updated their malware so that compromised devices can more easily be rented to so-called &#8220;residential proxy&#8221; providers. These proxy services allow paying customers to route their Internet communications through someone else&#8217;s device, providing anonymity and the ability to appear as a regular Internet user in almost any major city worldwide.

From a website‚Äôs perspective, the IP traffic of a residential proxy network user appears to originate from the rented residential IP address, not from the proxy service customer. Proxy services can be used in a legitimate manner for several business purposes ‚Äî such as price comparisons or sales intelligence. But they are massively abused for hiding cybercrime activity (think advertising fraud, credential stuffing) because they can make it difficult to trace malicious traffic to its original source.
And as we&#8217;ll see in a moment, this entire shadowy industry appears to be shifting its focus toward enabling aggressive content scraping activity that continuously feeds raw data into large language models (LLMs) built to support various AI projects.
&#8216;INSANE&#8217; GROWTH
Riley Kilmer is co-founder of spur.us, a service that tracks proxy networks. Kilmer said all of the top proxy services have grown exponentially over the past six months &#8212; with some adding between 10 to 200 times more proxies for rent.
&#8220;I just checked, and in the last 90 days we&#8217;ve seen 250 million unique residential proxy IPs,&#8221; Kilmer said. &#8220;That is insane. That is so high of a number, it&#8217;s unheard of. These proxies are absolutely everywhere now.&#8221;
To put Kilmer&#8217;s comments in perspective, here was Spur&#8217;s view of the Top 10 proxy networks by approximate install base, circa May 2025:
AUPROXIES_PROXY¬† 66,097
RAYOBYTE_PROXY¬† ¬† 43,894
OXYLABS_PROXY¬† ¬†43,008
WEBSHARE_PROXY¬† ¬†39,800
IPROYAL_PROXY¬† ¬† 32,723
PROXYCHEAP_PROXY¬† ¬† 26,368
IPIDEA_PROXY¬† ¬† 26,202
MYPRIVATEPROXY_PROXY¬† 25,287
HYPE_PROXY¬† ¬† 18,185
MASSIVE_PROXY¬† ¬† 17,152
Today, Spur says it is tracking an unprecedented spike in available proxies across all providers, including;
LUMINATI_PROXY¬† ¬† 11,856,421
NETNUT_PROXY¬† ¬† 10,982,458
ABCPROXY_PROXY¬† ¬† 9,294,419
OXYLABS_PROXY¬† ¬† ¬†6,754,790
IPIDEA_PROXY¬† ¬† ¬†3,209,313
EARNFM_PROXY¬† ¬† 2,659,913
NODEMAVEN_PROXY¬† ¬† 2,627,851
INFATICA_PROXY¬† ¬† 2,335,194
IPROYAL_PROXY¬† ¬† 2,032,027
YILU_PROXY¬† ¬† 1,549,155
Reached for comment about the apparent rapid growth in their proxy network, Oxylabs (#4 on Spur&#8217;s list) said while their proxy pool did grow recently, it did so at nowhere near the rate cited by Spur.
&#8220;We don‚Äôt systematically track other providers‚Äô figures, and we‚Äôre not aware of any instances of 10√ó or 100√ó growth, especially when it comes to a few bigger companies that are legitimate businesses,&#8221; the company said in a written statement.
Bright Data was formerly known as Luminati Networks, the name that is currently at the top of Spur&#8217;s list of the biggest residential proxy networks, with more than 11 million proxies. Bright Data likewise told KrebsOnSecurity that Spur&#8217;s current estimates of its proxy network are dramatically overstated and inaccurate.
&#8220;We did not actively initiate nor do we see any 10x or 100x expansion of our network, which leads me to believe that someone might be presenting these IPs as Bright Data&#8217;s in some way,&#8221; said Rony Shalit, Bright Data&#8217;s chief compliance and ethics officer. &#8220;In many cases in the past, due to us being the leading data collection proxy provider, IPs were falsely tagged as being part of our network, or while being used by other proxy providers for malicious activity.&#8221;
&#8220;Our network is only sourced from verified IP providers and a robust opt-in only residential peers, which we work hard and in complete transparency to obtain,&#8221; Shalit continued. &#8220;Every DC, ISP or SDK partner is reviewed and approved, and every residential peer must actively opt in to be part of our network.&#8221;
HK NETWORK
Even Spur acknowledges that Luminati and Oxylabs are unlike most other proxy services on their top proxy providers list, in that these providers actually adhere to &#8220;know-your-customer&#8221; policies, such as requiring video calls with all customers, and strictly blocking customers from reselling access.
Benjamin Brundage is founder of Synthient, a startup that helps companies detect proxy networks. Brundage said if there is increasing confusion around which proxy networks are the most worrisome, it&#8217;s because nearly all of these lesser-known proxy services have evolved into highly incestuous bandwidth resellers. What&#8217;s more, he said, some proxy providers do not appreciate being tracked and have been known to take aggressive steps to confuse systems that scan the Internet for residential proxy nodes.
Brundage said most proxy services today have created their own software development kit or SDK that other app developers can bundle with their code to earn revenue. These SDKs quietly modify the user&#8217;s device so that some portion of their bandwidth can be used to forward traffic from proxy service customers.
&#8220;Proxy providers have pools of constantly churning IP addresses,&#8221; he said. &#8220;These IP addresses are sourced through various means, such as bandwidth-sharing apps, botnets, Android SDKs, and more. These providers will often either directly approach resellers or offer a reseller program that allows users to resell bandwidth through their platform.&#8221;
Many SDK providers say they require full consent before allowing their software to be installed on end-user devices. Still, those opt-in agreements and consent checkboxes may be little more than a formality for cybercriminals like the Aisuru botmasters, who can earn a commission each time one of their infected devices is forced to install some SDK that enables one or more of these proxy services.
Depending on its structure, a single provider may operate hundreds of different proxy pools at a time &#8212; all maintained through other means, Brundage said.
&#8220;Often, you&#8217;ll see resellers maintaining their own proxy pool in addition to an upstream provider,&#8221; he said. &#8220;It allows them to market a proxy pool to high-value clients and offer an unlimited bandwidth plan for cheap reduce their own costs.&#8221;
Some proxy providers appear to be directly in league with botmasters. Brundage identified one proxy provider that was aggressively advertising cheap and plentiful bandwidth to content scraping companies. After scanning that provider&#8217;s pool of available proxies, Brundage said he found a one-to-one match with IP addresses he&#8217;d previously mapped to the Aisuru botnet.
Brundage says that by almost any measurement, the world&#8217;s largest residential proxy service is IPidea, a China-based proxy network. IPidea is #5 on Spur&#8217;s Top 10, and Brundage said its brands include ABCProxy (#3), Roxlabs, LunaProxy, PIA S5 Proxy, PyProxy, 922Proxy, 360Proxy, IP2World, and Cherry Proxy.¬†Spur&#8217;s Kilmer said they also track Yilu Proxy¬†(#10) as IPidea.
Brundage said all of these providers operate under a corporate umbrella known on the cybercrime forums as &#8220;HK Network.&#8221;
&#8220;The way it works is there&#8217;s this whole reseller ecosystem, where IPidea will be incredibly aggressive and approach all these proxy providers with the offer, &#8216;Hey, if you guys buy bandwidth from us, we&#8217;ll give you these amazing reseller prices,'&#8221; Brundage explained. &#8220;But they&#8217;re also very aggressive in recruiting resellers for their apps.&#8221;
A graphic depicting the relationship between proxy providers that Synthient found are white labeling IPidea proxies. Image: Synthient.com.
Those apps include a range of low-cost and &#8220;free&#8221; virtual private networking (VPN) services that indeed allow users to enjoy a free VPN, but which also turn the user&#8217;s device into a traffic relay that can be rented to cybercriminals, or else parceled out to countless other proxy networks.
&#8220;They have all this bandwidth to offload,&#8221; Brundage said of IPidea and its sister networks. &#8220;And they can do it through their own platforms, or they go get resellers to do it for them by advertising on sketchy hacker forums to reach more people.&#8221;
One of IPidea&#8217;s core brands is 922S5Proxy, which is a not-so-subtle nod to the 911S5Proxy service that was hugely popular between 2015 and 2022. In July 2022, KrebsOnSecurity published a deep dive into 911S5Proxy&#8217;s origins and apparent owners in China. Less than a week later, 911S5Proxy announced it was closing down after the company&#8217;s servers were massively hacked.
That 2022 story named Yunhe Wang from Beijing as the apparent owner and/or manager of the 911S5 proxy service. In May 2024, the U.S. Department of Justice arrested Mr Wang, alleging that his network was used to steal billions of dollars from financial institutions, credit card issuers, and federal lending programs. At the same time, the U.S. Treasury Department announced sanctions against Wang and two other Chinese nationals for operating 911S5Proxy.
The website for 922Proxy.
DATA SCRAPING FOR AI
In recent months, multiple experts who track botnet and proxy activity have shared that a great deal of content scraping which ultimate benefits AI companies is now leveraging these proxy networks to further obfuscate their aggressive data-slurping activity. That&#8217;s because by routing it through residential IP addresses, content scraping firms can make their traffic far trickier to filter out.
&#8220;It&#8217;s really difficult to block, because there&#8217;s a risk of blocking real people,&#8221; Spur&#8217;s Kilmer said of the LLM scraping activity that is fed through individual residential IP addresses, which are often shared by multiple customers at once.
Kilmer says the AI industry has brought a veneer of legitimacy to residential proxy business, which has heretofore mostly been associated with sketchy affiliate money making programs, automated abuse, and unwanted Internet traffic.
&#8220;Web crawling and scraping has always been a thing, but AI made it like a commodity, data that had to be collected,&#8221; Kilmer said. &#8220;Everybody wanted to monetize their own data pots, and how they monetize that is different across the board.&#8221;
Kilmer said many LLM-related scrapers rely on residential proxies in cases where the content provider has restricted access to their platform in some way, such as forcing interaction through an app, or keeping all content behind a login page with multi-factor authentication.
&#8220;Where the cost of data is out of reach &#8212; there is some exclusivity or reason they can&#8217;t access the data &#8212; they&#8217;ll turn to residential proxies so they look like a real person accessing that data,&#8221; Kilmer said of the content scraping efforts.
Aggressive AI crawlers increasingly are overloading community-maintained infrastructure, causing what amounts to persistent DDoS attacks on vital public resources. A report earlier this year from LibreNews found some open-source projects now see as much as 97 percent of their traffic originating from AI company bots, dramatically increasing bandwidth costs, service instability, and burdening already stretched-thin maintainers.
Cloudflare is now experimenting with tools that will allow content creators to charge a fee to AI crawlers to scrape their websites. The company&#8217;s &#8220;pay-per-crawl&#8221; feature is currently in a private beta, but it lets publishers set their own prices that bots must pay before scraping content.
On October 22, the social media and news network Reddit sued Oxylabs (PDF) and several other proxy providers, alleging that their systems enabled the mass-scraping of Reddit user content even though Reddit had taken steps to block such activity.
&#8220;Recognizing that Reddit denies scrapers like them access to its site, Defendants scrape the data from Google‚Äôs search results instead,&#8221; the lawsuit alleges. &#8220;They do so by masking their identities, hiding their locations, and disguising their web scrapers as regular people (among other techniques) to circumvent or bypass the security restrictions meant to stop them.&#8221;
Denas Grybauskas, chief governance and strategy officer at Oxylabs, said the company was shocked and disappointed by the lawsuit.
&#8220;Reddit has made no attempt to speak with us directly or communicate any potential concerns,&#8221; Grybauskas said in a written statement. &#8220;Oxylabs has always been and will continue to be a pioneer and an industry leader in public data collection, and it will not hesitate to defend itself against these allegations. Oxylabs‚Äô position is that no company should claim ownership of public data that does not belong to them. It is possible that it is just an attempt to sell the same public data at an inflated price.&#8221;
As big and powerful as Aisuru may be, it is hardly the only botnet that is contributing to the overall broad availability of residential proxies. For example, on June 5 the FBI‚Äôs Internet Crime Complaint Center warned that an IoT malware threat dubbed BADBOX 2.0 had compromised millions of smart-TV boxes, digital projectors, vehicle infotainment units, picture frames, and other IoT devices.
In July 2025, Google filed a lawsuit in New York federal court against the Badbox botnet&#8217;s alleged perpetrators. Google said the Badbox 2.0 botnet &#8220;compromised more than 10 million uncertified devices running Android&#8217;s open-source software, which lacks Google&#8217;s security protections. Cybercriminals infected these devices with pre-installed malware and exploited them to conduct large-scale ad fraud and other digital crimes.&#8221;
A FAMILIAR DOMAIN NAME
Brundage said the Aisuru botmasters have their own SDK, and for some reason part of its code tells many newly-infected systems to query the domain name fuckbriankrebs[.]com. This may be little more than an elaborate &#8220;screw you&#8221; to this site&#8217;s author: One of the botnet&#8217;s alleged partners goes by the handle &#8220;Forky,&#8221; and was identified in June by KrebsOnSecurity as a young man from Sao Paulo, Brazil.
Brundage noted that only systems infected with Aisuru&#8217;s Android SDK will be forced to resolve the domain. Initially, there was some discussion about whether the domain might have some utility as a &#8220;kill switch&#8221; capable of disrupting the botnet&#8217;s operations, although Brundage and others interviewed for this story say that is unlikely.
A tiny sample of the traffic after a DNS server was enabled on the newly registered domain fuckbriankrebs dot com. Each unique IP address requested its own unique subdomain. Image: Seralys.
For one thing, they said, if the domain was somehow critical to the operation of the botnet, why was it still unregistered and actively for-sale? Why indeed, we asked. Happily, the domain name was deftly snatched up last week by Philippe Caturegli, &#8220;chief hacking officer&#8221; for the security intelligence company Seralys.
Caturegli enabled a passive DNS server on that domain and within a few hours received more than 700,000 requests for unique subdomains on fuckbriankrebs[.]com.
But even with that visibility into Aisuru, it is difficult to use this domain check-in feature to measure its true size, Brundage said. After all, he said, the systems that are phoning home to the domain are only a small portion of the overall botnet.
&#8220;The bots are hardcoded to just spam lookups on the subdomains,&#8221; he said. &#8220;So anytime an infection occurs or it runs in the background, it will do one of those DNS queries.&#8221;
Caturegli briefly configured all subdomains on fuckbriankrebs dot com to display this ASCII art image to visiting systems today.
The domain fuckbriankrebs[.]com has a storied history. On its initial launch in 2009, it was used to spread malicious software by the Cutwail spam botnet. In 2011, the domain was involved in a notable DDoS against this website from a botnet powered by Russkill (a.k.a. &#8220;Dirt Jumper&#8221;).
Domaintools.com finds that in 2015, fuckbriankrebs[.]com was registered to an email address attributed to David &#8220;Abdilo&#8221; Crees, a 27-year-old Australian man sentenced in May 2025 to time served for cybercrime convictions related to the Lizard Squad hacking group.

üéì University AI
No updates.

üè¢ Corporate AI
‚Ä¢ Hosting NVIDIA speech NIM models on Amazon SageMaker AI: Parakeet ASR
  This post was written with NVIDIA and the authors would like to thank Adi Margolin, Eliuth Triana, and Maryam Motamedi for their collaboration. 
Organizations today face the challenge of processing large volumes of audio data‚Äìfrom customer calls and meeting recordings to podcasts and voice messages‚Äìto unlock valuable insights. Automatic Speech Recognition (ASR) is a critical first step in this process, converting speech to text so that further analysis can be performed. However, running ASR at scale is computationally intensive and can be expensive. This is where asynchronous inference on Amazon SageMaker AI comes in. By deploying state-of-the-art ASR models (like NVIDIA Parakeet models) on SageMaker AI with asynchronous endpoints, you can handle large audio files and batch workloads efficiently. With asynchronous inference, long-running requests can be processed in the background (with results delivered later); it also supports auto-scaling to zero when there‚Äôs no work and handles spikes in demand without blocking other jobs. 
In this blog post, we‚Äôll explore how to host the NVIDIA Parakeet ASR model on SageMaker AI and integrate it into an asynchronous pipeline for scalable audio processing. We‚Äôll also highlight the benefits of Parakeet‚Äôs architecture and the NVIDIA Riva toolkit for speech AI, and discuss how to use NVIDIA NIM for deployment on AWS. 
NVIDIA speech AI technologies: Parakeet ASR and Riva Framework 
NVIDIA offers a comprehensive suite of speech AI technologies, combining high-performance models with efficient deployment solutions. At its core, the Parakeet ASR model family represents state-of-the-art speech recognition capabilities, achieving industry-leading accuracy with low word error rates (WERs) . The model‚Äôs architecture uses the Fast Conformer encoder with the CTC or transducer decoder, enabling 2.4√ó faster processing than standard Conformers while maintaining accuracy. 
NVIDIA speech NIM is a collection of GPU-accelerated microservices for building customizable speech AI applications. NVIDIA Speech models deliver accurate transcription accuracy and natural, expressive voices in over 36 languages‚Äìideal for customer service, contact centers, accessibility, and global enterprise workflows. Developers can fine-tune and customize models for specific languages, accents, domains, and vocabularies, supporting accuracy and brand voice alignment. 
Seamless integration with LLMs and the NVIDIA Nemo Retriever make NVIDIA models ideal for agentic AI applications, helping your organization stand out with more secure, high-performing, voice AI. The NIM framework delivers these services as containerized solutions, making deployment straightforward through Docker containers that include the necessary dependencies and optimizations. 
This combination of high-performance models and deployment tools provides organizations with a complete solution for implementing speech recognition at scale. 
Solution overview 
The architecture illustrated in the diagram showcases a comprehensive asynchronous inference pipeline designed specifically for ASR and summarization workloads. The solution provides a robust, scalable, and cost-effective processing pipeline. 
 
Architecture components 
The architecture consists of five key components working together to create an efficient audio processing pipeline. At its core, the SageMaker AI asynchronous endpoint hosts the Parakeet ASR model with auto scaling capabilities that can scale to zero when idle for cost optimization. 
 
 The data ingestion process begins when audio files are uploaded to Amazon Simple Storage Service (Amazon S3), triggering AWS Lambda functions that process metadata and initiate the workflow. 
 For event processing, the SageMaker endpoint automatically sends out Amazon Simple Notification Service (Amazon SNS) success and failure notifications through separate queues, enabling proper handling of transcriptions. 
 Successfully transcribed content on Amazon S3 moves to Amazon Bedrock LLMs for intelligent summarization and additional processing like classification and insights extraction. 
 Finally, a comprehensive tracking system using Amazon DynamoDB stores workflow status and metadata, enabling real-time monitoring and analytics of the entire pipeline. 
 
Detailed implementation walkthrough 
In this section, we will provide the detailed walkthrough of the solution implementation. 
SageMaker asynchronous endpoint prerequisites 
To run the example notebooks, you need an AWS account with an AWS Identity and Access Management (IAM) role with least-privilege permissions to manage resources created. For details, refer to Create an AWS account. You might need to request a service quota increase for the corresponding SageMaker async hosting instances. In this example, we need one ml.g5.xlarge SageMaker async hosting instance and a ml.g5.xlarge SageMaker notebook instance. You can also choose a different integrated development environment (IDE), but make sure the environment contains GPU compute resources for local testing. 
SageMaker asynchronous endpoint configuration 
When you deploy a custom model like Parakeet, SageMaker has a couple of options: 
 
 Use a NIM container provided by NVIDIA 
 Use a large model inference (LMI) container 
 Use a prebuilt PyTorch container 
 
We‚Äôll provide examples for all three approaches. 
Using an NVIDIA NIM container 
NVIDIA NIM provides a streamlined approach to deploying optimized AI models through containerized solutions. Our implementation takes this concept further by creating a unified SageMaker AI endpoint that intelligently routes between HTTP and gRPC protocols to help maximize both performance and capabilities while simplifying the deployment process. 
Innovative dual-protocol architecture 
The key innovation is the combined HTTP + gRPC architecture that exposes a single SageMaker AI endpoint with intelligent routing capabilities. This design addresses the common challenge of choosing between protocol efficiency and feature completeness by automatically selecting the optimal transport method. The HTTP route is optimized for simple transcription tasks with files under 5MB, providing faster processing and lower latency for common use cases. Meanwhile, the gRPC route supports larger files (SageMaker AI real-time endpoints support a max payload of 25MB) and advanced features like speaker diarization with precise word-level timing information. The system‚Äôs auto-routing functionality analyzes incoming requests to determine file size and requested features, then automatically selects the most appropriate protocol without requiring manual configuration. For applications that need explicit control, the endpoint also supports forced routing through /invocations/http for simple transcription or /invocations/grpc when speaker diarization is required. This flexibility allows both automated optimization and fine-grained control based on specific application requirements. 
Advanced speech recognition and speaker diarization capabilities 
The NIM container enables a comprehensive audio processing pipeline that seamlessly combines speech recognition with speaker identification through the NVIDIA Riva integrated capabilities. The container handles audio preprocessing, including format conversion and segmentation, while ASR and speaker diarization processes run concurrently on the same audio stream. Results are automatically aligned using overlapping time segments, with each transcribed segment receiving appropriate speaker labels (for example, Speaker_0, Speaker_1). The inference handler processes audio files through the complete pipeline, initializing both ASR and speaker diarization services, running them in parallel, and aligning transcription segments with speaker labels. The output includes the full transcription, timestamped segments with speaker attribution, confidence scores, and total speaker count in a structured JSON format. 
Implementation and deployment 
The implementation extends NVIDIA parakeet-1-1b-ctc-en-us NIM container as the foundation, adding a Python aiohttp server that seamlessly manages the complete NIM lifecycle by automatically starting and monitoring the service. The server handles protocol adaptation by translating SageMaker inference requests to appropriate NIM APIs, implements the intelligent routing logic that analyzes request characteristics, and provides comprehensive error handling with detailed error messages and fallback mechanisms for robust production deployment. The containerized solution streamlines deployment through standard Docker and AWS CLI commands, featuring a pre-configured Docker file with the necessary dependencies and optimizations. The system accepts multiple input formats including multipart form-data (recommended for maximum compatibility), JSON with base64 encoding for simple integration scenarios, and raw binary uploads for direct audio processing. 
For detailed implementation instructions and working examples, teams can reference the complete implementation and deployment notebook in the AWS samples repository, which provides comprehensive guidance on deploying Parakeet ASR with NIM on SageMaker AI using the bring your own container (BYOC) approach. For organizations with specific architectural preferences, separate HTTP-only and gRPC-only implementations are also available, providing simpler deployment models for teams with well-defined use cases while the combined implementation offers maximum flexibility and automatic optimization. 
AWS customers can deploy these models either as production-grade NVIDIA NIM containers directly from SageMaker Marketplace or JumpStart, or open source NVIDIA models available on Hugging Face, which can be deployed through custom containers on SageMaker or Amazon Elastic Kubernetes Service (Amazon EKS). This allows organizations to choose between fully managed, enterprise-tier endpoints with auto-scaling and security, or flexible open-source development for research or constrained use cases. 
Using an AWS LMI container 
LMI containers are designed to simplify hosting large models on AWS. These containers include optimized inference engines like vLLM, FasterTransformer, or TensorRT-LLM that can automatically handle things like model parallelism, quantization, and batching for large models. The LMI container is essentially a pre-configured Docker image that runs an inference server (for example a Python server with these optimizations) and allows you to specify model parameters by using environment variables. 
To use the LMI container for Parakeet, we would typically: 
 
 Choose the appropriate LMI image: AWS provides different LMI images for different frameworks. For Parakeet , we might use the DJLServing image for efficient inference. Alternatively, NVIDIA Triton Inference Server (which Riva uses) is an option if we package the model in ONNX or TensorRT format. 
 Specify the model configuration: With LMI, we often provide a model_id (if pulling from Hugging Face Hub) or a path to our model, along with configuration for how to load it (number of GPUs, tensor parallel degree, quantization bits). The container then downloads the model and initializes it with the specified settings. We can also download our own model files from Amazon S3 instead of using the Hub. 
 Define the inference handler: The LMI container might require a small handler script or configuration to tell it how to process requests. For ASR, this might involve reading the audio input, passing it to the model, and returning text. 
 
AWS LMI containers deliver high performance and scalability through advanced optimization techniques, including continuous batching, tensor parallelism, and state-of-the-art quantization methods. LMI containers integrate multiple inference backends (vLLM, TensorRT-LLM through a single unified configuration), helping users seamlessly experiment and switch between frameworks to find the optimal performance stack for your specific use case. 
Using a SageMaker PyTorch container 
SageMaker offers PyTorch Deep Learning Containers (DLCs) that come with PyTorch and many common libraries pre-installed. In this example, we demonstrated how to extend our prebuilt container to install necessary packages for the model. You can download the model directly from Hugging Face during the endpoint creation or download the Parakeet model artifacts, packaging it with necessary configuration files into a model.tar.gz archive, and uploading it to Amazon S3. Along with the model artifacts, an inference.py script is required as the entry point script to define model loading and inference logic, including audio preprocessing and transcription handling. When using the SageMaker Python SDK to create a PyTorchModel, the SDK will automatically repackage the model archive to include the inference script under /opt/ml/model/code/inference.py, while keeping model artifacts in /opt/ml/model/ on the endpoint. Once the endpoint is deployed successfully, it can be invoked through the predict API by sending audio files as byte streams to get transcription results. 
For the SageMaker real-time endpoint, we currently allow a maximum of 25MB for payload size. Make sure you have set up the container to also allow the maximum request size. However, if you are planning to use the same model for the asynchronous endpoint, the maximum file size that the async endpoint supports is 1GB and the response time is up to 1 hour. Accordingly, you should setup the container to be prepared for this payload size and timeout. When using the PyTorch containers, here are some key configuration parameters to consider: 
 
 SAGEMAKER_MODEL_SERVER_WORKERS: Set the number of torch workers that will load the number of models copied into GPU memory. 
 TS_DEFAULT_RESPONSE_TIMEOUT: Set the time out setting for Torch server workers; for long audio processing, you can set it to a higher number 
 TS_MAX_REQUEST_SIZE: Set the byte size values for requests to 1G for async endpoints. 
 TS_MAX_RESPONSE_SIZE: Set the byte size values for response. 
 
In the example notebook, we also showcase how to leverage the SageMaker local session provided by the SageMaker Python SDK. It helps you create estimators and run training, processing, and inference jobs locally using Docker containers instead of managed AWS infrastructure, providing a fast way to test and debug your machine learning scripts before scaling to production. 
CDK pipeline prerequisites 
Before deploying this solution, make sure you have: 
 
 AWS CLI configured with appropriate permissions ‚Äì Installation Guide 
 AWS Cloud Development Kit (AWS CDK) installed ‚Äì Installation Guide 
 Node.js 18+ and Python 3.9+ installed 
 Docker ‚Äì Installation Guide 
 SageMaker endpoint deployed with your ML model (Parakeet ASR models or similar) 
 Amazon SNS topics created for success and failure notifications 
 
CDK pipeline setup 
The solution deployment begins with provisioning the necessary AWS resources using Infrastructure as Code (IaC) principles. AWS CDK creates the foundational components including: 
 
 DynamoDB Table: Configured for on-demand capacity to track invocation metadata, processing status, and results 
 S3 Buckets: Secure storage for input audio files, transcription outputs, and summarization results 
 SNS topics: Separate queues for success and failure event handling 
 Lambda functions: Serverless functions for metadata processing, status updates, and workflow orchestration 
 IAM roles and policies: Appropriate permissions for cross-service communication and resource access 
 
Environment setup 
Clone the repository and install dependencies: 
 
 # Install degit, a library for downloading specific sub directories
npm install -g degit

# Clone just the specific folder
npx degit aws-samples/genai-ml-platform-examples/infrastructure/automated-speech-recognition-async-pipeline-sagemaker-ai/sagemaker-async-batch-inference-cdk sagemaker-async-batch-inference-cdk

# Navigate to folder
cd sagemaker-async-batch-inference-cdk

# Install Node.js dependencies
npm install

# Set up Python virtual environment
python3 -m venv .venv
source .venv/bin/activate

# On Windows:
.venv\Scripts\activate
pip install -r requirements.txt 
 
Configuration 
Update the SageMaker endpoint configuration in bin/aws-blog-sagemaker.ts: 
 
  
  vim bin/aws-blog-sagemaker.ts 

# Change the endpoint name 
sageMakerConfig: { 
    endpointName: 'your-sagemaker-endpoint-name',     
    enableSageMakerAccess: true 
} 
  
 
If you have followed the notebook to deploy the endpoint, you should have created the two SNS topics. Otherwise, make sure you create the correct SNS topics using CLI: 
 
 # Create SNS topics
aws sns create-topic --name success-inf
aws sns create-topic --name failed-inf 
 
Build and deploy 
Before you deploy the AWS CloudFormation template, make sure Docker is running. 
 
 # Compile TypeScript to JavaScript
npm run build

# Bootstrap CDK (first time only)
npx cdk bootstrap

# Deploy the stack
npx cdk deploy 
 
Verify deployment 
After successful deployment, note the output values: 
 
 DynamoDB table name for status tracking 
 Lambda function ARNs for processing and status updates 
 SNS topic ARNs for notifications 
 
Submit audio file for processing 
Processing Audio Files 
Update the upload_audio_invoke_lambda.sh 
 
 LAMBDA_ARN="YOUR_LAMBDA_FUNCTION_ARN"
S3_BUCKET="YOUR_S3_BUCKET_ARN" 
 
Run the Script: 
AWS_PROFILE=default ./scripts/upload_audio_invoke_lambda.sh 
This script will: 
 
 Download a sample audio file 
 Upload the audio file to your s3 bucket 
 Send the bucket path to Lambda and trigger the transcription and summarization pipeline 
 
Monitoring progress 
You can check the result in DynamoDB table using the following command: 
 
 aws dynamodb scan --table-name YOUR_DYNAMODB_TABLE_NAME 
 
Check processing status in the DynamoDB table: 
 
 submitted: Successfully queued for inference 
 completed: Transcription completed successfully 
 failed: Processing encountered an error 
 
Audio processing and workflow orchestration 
The core processing workflow follows an event-driven pattern: 
Initial processing and metadata extraction: When audio files are uploaded to S3, the triggered Lambda function analyzes the file metadata, validates format compatibility, and creates detailed invocation records in DynamoDB. This facilitates comprehensive tracking from the moment audio content enters the system. 
Asynchronous Speech Recognition: Audio files are processed through the SageMaker endpoint using optimized ASR models. The asynchronous process can handle various file sizes and durations without timeout concerns. Each processing request is assigned a unique identifier for tracking purposes. 
Success path processing: Upon successful transcription, the system automatically initiates the summarization workflow. The transcribed text is sent to Amazon Bedrock, where advanced language models generate contextually appropriate summaries based on configurable parameters such as summary length, focus areas, and output format. 
Error handling and recovery: Failed processing attempts trigger dedicated Lambda functions that log detailed error information, update processing status, and can initiate retry logic for transient failures. This robust error handling results in minimal data loss and provides clear visibility into processing issues. 
Real-world applications 
Customer service analytics: Organizations can process thousands of customer service call recordings to generate transcriptions and summaries, enabling sentiment analysis, quality assurance, and insights extraction at scale. 
Meeting and conference processing: Enterprise teams can automatically transcribe and summarize meeting recordings, creating searchable archives and actionable summaries for participants and stakeholders. 
Media and content processing: Media companies can process podcast episodes, interviews, and video content to generate transcriptions and summaries for improved accessibility and content discoverability. 
Compliance and legal documentation: Legal and compliance teams can process recorded depositions, hearings, and interviews to create accurate transcriptions and summaries for case preparation and documentation. 
Cleanup 
Once you have used the solution, remove the SageMaker endpoints to prevent incurring additional costs. You can use the provided code to delete real-time and asynchronous inference endpoints, respectively: 
 
 # Delete real-time inference
endpointreal_time_predictor.delete_endpoint()

# Delete asynchronous inference
endpointasync_predictor.delete_endpoint() 
 
You should also delete all the resources created by the CDK stack. 
 
 # Delete CDK Stack
cdk destroy 
 
Conclusion 
The integration of powerful NVIDIA speech AI technologies with AWS cloud infrastructure creates a comprehensive solution for large-scale audio processing. By combining Parakeet ASR‚Äôs industry-leading accuracy and speed with NVIDIA Riva‚Äôs optimized deployment framework on the Amazon SageMaker asynchronous inference pipeline, organizations can achieve both high-performance speech recognition and cost-effective scaling. The solution leverages the managed services of AWS (SageMaker AI, Lambda, S3, and Bedrock) to create an automated, scalable pipeline for processing audio content. With features like auto scaling to zero, comprehensive error handling, and real-time monitoring through DynamoDB, organizations can focus on extracting business value from their audio content rather than managing infrastructure complexity. Whether processing customer service calls, meeting recordings, or media content, this architecture delivers reliable, efficient, and cost-effective audio processing capabilities. To experience the full potential of this solution, we encourage you to explore the solution and reach out to us if you have any specific business requirements and would like to customise the solution for your use case. 
 
About the authors 
Melanie Li, PhD, is a Senior Generative AI Specialist Solutions Architect at AWS based in Sydney, Australia, where her focus is on working with customers to build solutions using state-of-the-art AI/ML tools. She has been actively involved in multiple generative AI initiatives across APJ, harnessing the power of LLMs. Prior to joining AWS, Dr. Li held data science roles in the financial and retail industries. 
Tony Trinh is a Senior AI/ML Specialist Architect at AWS. With 13+ years of experience in the IT industry, Tony specializes in architecting scalable, compliance-driven AI and ML solutions‚Äîparticularly in generative AI, MLOps, and cloud-native data platforms. As part of his PhD, he‚Äôs doing research in Multimodal AI and Spatial AI. In his spare time, Tony enjoys hiking, swimming and experimenting with home improvement. 
Alick Wong is a Senior Solutions Architect at Amazon Web Services, where he helps startups and digital-native businesses modernize, optimize, and scale their platforms in the cloud. Drawing on his experience as a former startup CTO, he works closely with founders and engineering leaders to drive growth and innovation on AWS. 
Andrew Smith is a Sr. Cloud Support Engineer in the SageMaker, Vision &amp; Other team at AWS, based in Sydney, Australia. He supports customers using many AI/ML services on AWS with expertise in working with Amazon SageMaker. Outside of work, he enjoys spending time with friends and family as well as learning about different technologies. 
Derrick Choo is a Senior AI/ML Specialist Solutions Architect at AWS who accelerates enterprise digital transformation through cloud adoption, AI/ML, and generative AI solutions. He specializes in full-stack development and ML, designing end-to-end solutions spanning frontend interfaces, IoT applications, data integrations, and ML models, with a particular focus on computer vision and multi-modal systems. 
Tim Ma is a Principal Specialist in Generative AI at AWS, where he collaborates with customers to design and deploy cutting-edge machine learning solutions. He also leads go-to-market strategies for generative AI services, helping organizations harness the potential of advanced AI technologies. 
Curt Lockhart is an AI Solutions Architect at NVIDIA, where he helps customers deploy language and vision models to build end to end AI workflows using NVIDIA‚Äôs tooling on AWS. He enjoys making complex AI feel approachable and spending his time exploring the art, music, and outdoors of the Pacific Northwest. 
Francesco Ciannella is a senior engineer at NVIDIA, where he works on conversational AI solutions built around large language models (LLMs) and audio language models (ALMs). He holds a M.S. in engineering of telecommunications from the University of Rome ‚ÄúLa Sapienza‚Äù and an M.S. in language technologies from the School of Computer Science at Carnegie Mellon University.
‚Ä¢ Responsible AI design in healthcare and life sciences
  Generative AI has emerged as a transformative technology in healthcare, driving digital transformation in essential areas such as patient engagement and care management. It has shown potential to revolutionize how clinicians provide improved care through automated systems with diagnostic support tools that provide timely, personalized suggestions, ultimately leading to better health outcomes. For example, a study reported in BMC Medical Education that medical students who received large language model (LLM)-generated feedback during simulated patient interactions significantly improved their clinical decision-making compared to those who did not. 
At the center of most generative AI systems are LLMs capable of generating remarkably natural conversations, enabling healthcare customers to build products across billing, diagnosis, treatment, and research that can perform tasks and operate independently with human oversight. However, the utility of generative AI requires an understanding of the potential risks and impacts on healthcare service delivery, which necessitates the need for careful planning, definition, and execution of a system-level approach to building safe and responsible generative AI-infused applications. 
In this post, we focus on the design phase of building healthcare generative AI applications, including defining system-level policies that determine the inputs and outputs. These policies can be thought of as guidelines that, when followed, help build a responsible AI system. 
Designing responsibly 
LLMs can transform healthcare by reducing the cost and time required for considerations such as quality and reliability. As shown in the following diagram, responsible AI considerations can be successfully integrated into an LLM-powered healthcare application by considering quality, reliability, trust, and fairness for everyone. The goal is to promote and encourage certain responsible AI functionalities of AI systems. Examples include the following: 
 
 Each component‚Äôs input and output is aligned with clinical priorities to maintain alignment and promote controllability 
 Safeguards, such as guardrails, are implemented to enhance the safety and reliability of your AI system 
 Comprehensive AI red-teaming and evaluations are applied to the entire end-to-end system to assess safety and privacy-impacting inputs and outputs 
 
Conceptual architecture 
The following diagram shows a conceptual architecture of a generative AI application with an LLM. The inputs (directly from an end-user) are mediated through input guardrails. After the input has been accepted, the LLM can process the user‚Äôs request using internal data sources. The output of the LLM is again mediated through guardrails and can be shared with end-users. 
 
Establish governance mechanisms 
When building generative AI applications in healthcare, it‚Äôs essential to consider the various risks at the individual model or system level, as well as at the application or implementation level. The risks associated with generative AI can differ from or even amplify existing AI risks. Two of the most important risks are confabulation and bias: 
 
 Confabulation ‚Äî The model generates confident but erroneous outputs, sometimes referred to as hallucinations. This could mislead patients or clinicians. 
 Bias ‚Äî This refers to the risk of exacerbating historical societal biases among different subgroups, which can result from non-representative training data. 
 
To mitigate these risks, consider establishing content policies that clearly define the types of content your applications should avoid generating. These policies should also guide how to fine-tune models and which appropriate guardrails to implement. It is crucial that the policies and guidelines are tailored and specific to the intended use case. For instance, a generative AI application designed for clinical documentation should have a policy that prohibits it from diagnosing diseases or offering personalized treatment plans. 
Additionally, defining clear and detailed policies that are specific to your use case is fundamental to building responsibly. This approach fosters trust and helps developers and healthcare organizations carefully consider the risks, benefits, limitations, and societal implications associated with each LLM in a particular application. 
The following are some example policies you might consider using for your healthcare-specific applications. The first table summarizes the roles and responsibilities for human-AI configurations. 
 
  
   
   Action ID 
   Suggested Action 
   Generative AI Risks 
   
   
   GV-3.2-001 
   Policies are in place to bolster oversight of generative AI systems with independent evaluations or assessments of generative AI models or systems where the type and robustness of evaluations are proportional to the identified risks. 
   CBRN Information or Capabilities; Harmful Bias and Homogenization 
   
   
   GV-3.2-002 
   Consider adjustment of organizational roles and components across lifecycle stages of large or complex generative AI systems, including: test and evaluation, validation, and red-teaming of generative AI systems; generative AI content moderation; generative AI system development and engineering; increased accessibility of generative AI tools, interfaces, and systems; and incident response and containment. 
   Human-AI Configuration; Information Security; Harmful Bias and Homogenization 
   
   
   GV-3.2-003 
   Define acceptable use policies for generative AI interfaces, modalities, and human-AI configurations (for example, for AI assistants and decision-making tasks), including criteria for the kinds of queries generative AI applications should refuse to respond to. 
   Human-AI Configuration 
   
   
   GV-3.2-004 
   Establish policies for user feedback mechanisms for generative AI systems that include thorough instructions and any mechanisms for recourse. 
   Human-AI Configuration 
   
   
   GV-3.2-005 
   Engage in threat modeling to anticipate potential risks from generative AI systems. 
   CBRN Information or Capabilities; Information Security 
   
  
 
The following table summarizes policies for risk management in AI system design. 
 
  
   
   Action ID 
   Suggested Action 
   Generative AI Risks 
   
   
   GV-4.1-001 
   Establish policies and procedures that address continual improvement processes for generative AI risk measurement. Address general risks associated with a lack of explainability and transparency in generative AI systems by using ample documentation and techniques such as application of gradient-based attributions, occlusion or term reduction, counterfactual prompts and prompt engineering, and analysis of embeddings. Assess and update risk measurement approaches at regular cadences. 
   Confabulation 
   
   
   GV-4.1-002 
   Establish policies, procedures, and processes detailing risk measurement in context of use with standardized measurement protocols and structured public feedback exercises such as AI red-teaming or independent external evaluations. 
   CBRN Information and Capability; Value Chain and Component Integration 
   
  
 
Transparency artifacts 
Promoting transparency and accountability throughout the AI lifecycle can foster trust, facilitate debugging and monitoring, and enable audits. This involves documenting data sources, design decisions, and limitations through tools like model cards and offering clear communication about experimental features. Incorporating user feedback mechanisms further supports continuous improvement and fosters greater confidence in AI-driven healthcare solutions. 
AI developers and DevOps engineers should be transparent about the evidence and reasons behind all outputs by providing clear documentation of the underlying data sources and design decisions so that end-users can make informed decisions about the use of the system. Transparency enables the tracking of potential problems and facilitates the evaluation of AI systems by both internal and external teams. Transparency artifacts guide AI researchers and developers on the responsible use of the model, promote trust, and help end-users make informed decisions about the use of the system. 
The following are some implementation suggestions: 
 
 When building AI features with experimental models or services, it‚Äôs essential to highlight the possibility of unexpected model behavior so healthcare professionals can accurately assess whether to use the AI system. 
 Consider publishing artifacts such as Amazon SageMaker model cards or AWS system cards. Also, at AWS we provide detailed information about our AI systems through AWS AI Service Cards, which list intended use cases and limitations, responsible AI design choices, and deployment and performance optimization best practices for some of our AI services. AWS also recommends establishing transparency policies and processes for documenting the origin and history of training data while balancing the proprietary nature of training approaches. Consider creating a hybrid document that combines elements of both model cards and service cards, because your application likely uses foundation models (FMs) but provides a specific service. 
 Offer a feedback user mechanism. Gathering regular and scheduled feedback from healthcare professionals can help developers make necessary refinements to improve system performance. Also consider establishing policies to help developers allow for user feedback mechanisms for AI systems. These should include thorough instructions and consider establishing policies for any mechanisms for recourse. 
 
Security by design 
When developing AI systems, consider security best practices at each layer of the application. Generative AI systems might be vulnerable to adversarial attacks suck as prompt injection, which exploits the vulnerability of LLMs by manipulating their inputs or prompt. These types of attacks can result in data leakage, unauthorized access, or other security breaches. To address these concerns, it can be helpful to perform a risk assessment and implement guardrails for both the input and output layers of the application. As a general rule, your operating model should be designed to perform the following actions: 
 
 Safeguard patient privacy and data security by implementing personally identifiable information (PII) detection, configuring guardrails that check for prompt attacks 
 Continually assess the benefits and risks of all generative AI features and tools and regularly monitor their performance through Amazon CloudWatch or other alerts 
 Thoroughly evaluate all AI-based tools for quality, safety, and equity before deploying 
 
Developer resources 
The following resources are useful when architecting and building generative AI applications: 
 
 Amazon Bedrock Guardrails helps you implement safeguards for your generative AI applications based on your use cases and responsible AI policies. You can create multiple guardrails tailored to different use cases and apply them across multiple FMs, providing a consistent user experience and standardizing safety and privacy controls across your generative AI applications. 
 The AWS responsible AI whitepaper serves as an invaluable resource for healthcare professionals and other developers that are developing AI applications in critical care environments where errors could have life-threatening consequences. 
 AWS AI Service Cards explains the use cases for which the service is intended, how machine learning (ML) is used by the service, and key considerations in the responsible design and use of the service. 
 
Conclusion 
Generative AI has the potential to improve nearly every aspect of healthcare by enhancing care quality, patient experience, clinical safety, and administrative safety through responsible implementation. When designing, developing, or operating an AI application, try to systematically consider potential limitations by establishing a governance and evaluation framework grounded by the need to maintain the safety, privacy, and trust that your users expect. 
For more information about responsible AI, refer to the following resources: 
 
 NIST Trustworthy and Responsible AI 
 OWASP Top 10 for Large Language Model applications 
 
 
 
About the authors 
Tonny Ouma is an Applied AI Specialist at AWS, specializing in generative AI and machine learning. As part of the Applied AI team, Tonny helps internal teams and AWS customers incorporate leading-edge AI systems into their products. In his spare time, Tonny enjoys riding sports bikes, golfing, and entertaining family and friends with his mixology skills. 
Simon Handley, PhD, is a Senior AI/ML Solutions Architect in the Global Healthcare and Life Sciences team at Amazon Web Services. He has more than 25 years‚Äô experience in biotechnology and machine learning and is passionate about helping customers solve their machine learning and life sciences challenges. In his spare time, he enjoys horseback riding and playing ice hockey.
‚Ä¢ Beyond pilots: A proven framework for scaling AI to production
  The era of perpetual AI pilots is over. This year, 65% of AWS Generative AI Innovation Center customer projects moved from concept to production‚Äîsome launching in just 45 days, as AWS VP Swami Sivasubramanian shared on LinkedIn. These results come from insights gained across more than one thousand customer implementations. 
The Generative AI Innovation Center pairs organizations across industries with AWS scientists, strategists, and engineers to implement practical AI solutions that drive measurable outcomes. These initiatives transform diverse sectors worldwide. For example, through a cross-functional AWS collaboration, we supported the National Football League (NFL) to create a generative AI-powered solution that obtains statistical game insights within 30 seconds. This helps their media and production teams locate video content six times faster. Similarly, we helped Druva‚Äôs DruAI system streamline customer support and data protection through natural language processing, reducing investigation time from hours to minutes. 
These achievements reflect a broader pattern of success, driven by a powerful methodology: The Five V‚Äôs Framework for AI Implementation. 
 
This framework takes projects from initial testing to full deployment by focusing on concrete business outcomes and operational excellence. It‚Äôs grounded in two of Amazon‚Äôs Leadership Principles, Customer Obsession and Deliver Results. By starting with what customers actually need and working backwards, we‚Äôve helped companies across industries modernize their operations and better serve their customers. 
The Five V‚Äôs Framework: A foundation for success 
Every successful AI deployment begins with groundwork. In our experience, projects thrive when organizations first identify specific challenges they need to solve, align key stakeholders around these goals, and establish clear accountability for results. The Five V‚Äôs Framework helps guide organizations through a structured process: 
 
 Value: Target high-impact opportunities aligned with your strategic priorities 
 Visualize: Define clear success metrics that link directly to business outcomes 
 Validate: Test solutions against real-world requirements and constraints 
 Verify: Create a scalable path to production that delivers sustainable results 
 Venture: Secure the resources and support needed for long-term success 
 
Value: The critical first step 
The Value phase emphasizes working backwards from your most pressing business challenges. By starting with existing pain points and collaborating across technical and business teams, organizations can develop solutions that deliver meaningful return on investment (ROI). This focused approach helps direct resources where they‚Äôll have the greatest impact. 
Visualize: Defining success through measurement 
The next step requires translating the potential benefits‚Äîcost reduction, revenue growth, risk mitigation, improved customer experience, and competitive advantage‚Äîinto clear, measurable performance indicators. A comprehensive measurement framework starts with baseline metrics using historical data where available. These metrics should address both technical aspects like accuracy and response time, as well as business outcomes such as productivity gains and customer satisfaction. 
The Visualize phase examines data availability and quality to support proper measurement while working with stakeholders to define success criteria that align with strategic objectives. This dual focus helps organizations track not just the performance of the AI solution, but its actual impact on business goals. 
Validate: Where ambition meets reality 
The Validate phase focuses on testing solutions against real-world conditions and constraints. Our approach integrates strategic vision with implementation expertise from day one. As Sri Elaprolu, Director of the Generative AI Innovation Center, explains: ‚ÄúEffective validation creates alignment between vision and execution. We unite diverse perspectives‚Äîfrom scientists to business leaders‚Äîso that solutions deliver both technical excellence and measurable business impact.‚Äù 
This process involves systematic integration testing, stress testing for expected loads, verifying compliance requirements, and gathering end-user feedback. Security specialists shape the core architecture. Industry subject matter experts define the operational processes and decision logic that guide prompt design and model refinement. Change management strategies are integrated early to ensure alignment and adoption. 
The Generative AI Innovation Center partnered with SparkXGlobal, an AI-driven marketing-technology company, to validate their new solution through comprehensive testing. Their platform, Xnurta, provides business analytics and reporting for Amazon merchants, demonstrating impressive results: report processing time dropped from 6-8 hours to just 8 minutes while maintaining 95% accuracy. This successful validation established a foundation for SparkXGlobal‚Äôs continued innovation and enhanced AI capabilities. 
Working with the Generative AI Innovation Center, the U.S. Environmental Protection Agency (EPA) created an intelligent document processing solution powered by Anthropic models on Amazon Bedrock. This solution helped EPA scientists accelerate chemical risk assessments and pesticide reviews through transparent, verifiable, and human-controlled AI practices. The impact has been substantial: document processing time decreased by 85%, evaluation costs dropped by 99%, and more than 10,000 regulatory applications have advanced faster to protect public health. 
Verify: The path to production 
Moving from pilot to production requires more than proof of concept‚Äîit demands scalable solutions that integrate with existing systems and deliver consistent value. While demos can seem compelling, verification reveals the true complexity of enterprise-wide deployment. This critical stage maps the journey from prototype to production, establishing a foundation for sustainable success. 
Building production-ready AI solutions brings together several key elements. Robust governance structures must facilitate responsible AI deployment and oversight, managing risk and compliance in an evolving regulatory landscape. Change management prepares teams and processes for new ways of working, driving organization-wide adoption. Operational readiness assessments evaluate existing workflows, integration points, and team capabilities to facilitate smooth implementation. 
Architectural decisions in the verification phase balance scale, reliability, and operability, with security and compliance woven into the solution‚Äôs fabric. This often involves practical trade-offs based on real-world constraints. A simpler solution aligned to existing team capabilities may prove more valuable than a complex one requiring specialized expertise. Similarly, meeting strict latency requirements might necessitate choosing a streamlined model over a more sophisticated one, as model selection requires a balance of performance, accuracy, and computational costs based on the use case. 
Generative AI Innovation Center Principal Data Scientist, Isaac Privitera, captures this philosophy: ‚ÄúWhen building a generative AI solution, we focus primarily on three things: measurable business impact, production readiness from day one, and sustained operational excellence. This trinity drives solutions that thrive in real-world conditions.‚Äù 
Effective verification demands both technical expertise and practical wisdom from real-world deployments. It requires proving not just that a solution works in principle, but that it can operate at scale within existing systems and team capabilities. By systematically addressing these factors, we help make sure deployments deliver sustainable, long-term value. 
Venture: Securing long-term success 
Long-term success in AI also requires mindful resource planning across people, processes, and funding. The Venture phase maps the full journey from implementation through sustained organizational adoption. 
Financial viability starts with understanding the total cost of ownership, from initial development through deployment, integration, training, and ongoing operations. Promising projects can stall mid-implementation due to insufficient resource planning. Success requires strategic budget allocation across all phases, with clear ROI milestones and the flexibility to scale. 
Successful ventures demand organizational commitment through executive sponsorship, stakeholder alignment, and dedicated teams for ongoing optimization and maintenance. Organizations must also account for both direct and indirect costs‚Äîfrom infrastructure and development, to team training, process adaptation, and change management. A blend of sound financial planning and flexible resource strategies allows teams to accelerate and adjust as opportunities and challenges arise. 
From there, the solution must integrate seamlessly into daily operations with clear ownership and widespread adoption. This transforms AI from a project into a core organizational capability. 
Adopting the Five V‚Äôs Framework in your enterprise 
The Five V‚Äôs Framework shifts AI focus from technical capabilities to business results, replacing ‚ÄòWhat can AI do?‚Äô with ‚ÄòWhat do we need AI to do?‚Äô. Successful implementation requires both an innovative culture and access to specialized expertise. 
 
AWS resources to support your journey 
AWS offers a variety of resources to help you scale your AI to production. 
Expert guidance 
The AWS Partnership Network (APN) offers multiple pathways to access specialized expertise, while AWS Professional Services brings proven methodologies from its own successful AI implementations. Certified partners, including Generative AI Partner Innovation Alliance members who receive direct enablement training from the Generative AI Innovation Center team, extend this expertise across industries. AWS Generative AI Competency Partners bring use case-specific success, while specialized partners focus on model customization and evaluation. 
Self-service learning 
For teams building internal capabilities, AWS provides technical blogs with implementation guides based on real-world experience, GitHub repositories with production-ready code, and AWS Workshop Studio for hands-on learning that bridges theory and practice. 
Balancing learning and innovation 
Even with the right framework and resources, not every AI project will reach production. These initiatives still provide valuable lessons that strengthen your overall program. Organizations can build lasting AI capabilities through three key principles: 
 
 Embracing a portfolio approach: Treat AI initiatives as an investment portfolio where diversification drives risk management and value creation. Balance quick wins (delivering value within months), strategic initiatives (driving longer-term transformation), and moonshot projects (potentially revolutionizing your business). 
 Creating a culture of safe experimentation: Organizations thrive with AI when teams can innovate boldly. In rapidly evolving fields, the cost of inaction often exceeds the risk of calculated experiments. 
 Learning from ‚Äúproductive failures‚Äù: Capture insights systematically across projects. Technical challenges reveal capability gaps, data issues expose information needs, and organizational readiness concerns illuminate broader transformation requirements ‚Äì all shaping future initiatives. 
 
The path forward 
The next 12-18 months present a pivotal opportunity for organizations to harness generative AI and agentic AI to solve previously intractable problems, establish competitive advantages, and explore entirely new frontiers of business possibility. Those who successfully move from pilot to production will help define what‚Äôs possible within their industries and beyond. 
Are you ready to move your AI initiatives into production? 
 
 Learn more about the AWS Generative AI Innovation Center and contact your AWS Account Manager to be connected to our expert guidance and support. 
 Join our AWS Builder community to connect with others on a similar AI journey. 
 
 
 
About the authors 
Sri Elaprolu serves as Director of the AWS Generative AI Innovation Center, where he leverages nearly three decades of technology leadership experience to drive artificial intelligence and machine learning innovation. In this role, he leads a global team of machine learning scientists and engineers who develop and deploy advanced generative and agentic AI solutions for enterprise and government organizations facing complex business challenges. Throughout his nearly 13-year tenure at AWS, Sri has held progressively senior positions, including leadership of ML science teams that partnered with high-profile organizations such as the NFL, Cerner, and NASA. These collaborations enabled AWS customers to harness AI and ML technologies for transformative business and operational outcomes. Prior to joining AWS, he spent 14 years at Northrop Grumman, where he successfully managed product development and software engineering teams. Sri holds a Master‚Äôs degree in Engineering Science and an MBA with a concentration in general management, providing him with both the technical depth and business acumen essential for his current leadership role. 
 Dr. Diego Socolinsky is currently the North America Head of the Generative AI Innovation Center at Amazon Web Services (AWS). With over 25 years of experience at the intersection of technology, machine learning, and computer vision, he has built a career driving innovation from cutting-edge research to production-ready solutions. Dr. Socolinsky holds a Ph.D. in Mathematics from The Johns Hopkins University and has been a pioneer in various fields including thermal imaging biometrics, augmented/mixed reality, and generative AI initiatives. His technical expertise spans from optimizing low-level embedded systems to architecting complex real-time deep learning solutions, with particular focus on generative AI platforms, large-scale unstructured data classification, and advanced computer vision applications. He is known for his ability to bridge the gap between technical innovation and strategic business objectives, consistently delivering transformative technology that solves complex real-world problems. 
Sabine Khan is a Strategic Initiatives Leader with the AWS Generative AI Innovation Center, where she implements delivery and strategy initiatives focused on scaling enterprise-grade Generative AI solutions. She specializes in production-ready AI systems and drives agentic AI projects from concept to deployment. With over twenty years of experience in software delivery and a strong focus on AI/ML during her tenure at AWS, she has established a track record of successful enterprise implementations. Prior to AWS, she led digital transformation initiatives and held product development and software engineering leadership roles in Houston‚Äôs energy sector. Sabine holds a Master‚Äôs degree in GeoScience and an MBA. 
Andrea Jimenez is a dual master‚Äôs candidate at the Massachusetts Institute of Technology, pursuing an M.S. in Computer Science from the School of Engineering and an MBA from the Sloan School of Management. As a GenAI Lead Graduate Fellow at the MIT GenAI Innovation Center, she researches agentic AI systems and the economic implications of generative AI technologies, while leveraging her background in artificial intelligence, product development, and startup innovation to lead teams at the intersection of technology and business strategy. Her work focuses on advancing human-AI collaboration and translating cutting-edge research into scalable, high-impact solutions. Prior to AWS and MIT, she led product and engineering teams in the tech industry and founded and sold a startup that helped early-stage companies build and launch SaaS products. 
Randi Larson connects AI innovation with executive strategy for the AWS Generative AI Innovation Center, shaping how organizations understand and translate technical breakthroughs into business value. She combines strategic storytelling with data-driven insight through global keynotes, Amazon‚Äôs first tech-for-good podcast, and conversations with industry and Amazon leaders on AI transformation. Before Amazon, Randi refined her analytical precision as a Bloomberg journalist and advisor to economic institutions, think tanks, and family offices on technology initiatives. Randi holds an MBA from Duke University‚Äôs Fuqua School of Business and a B.S. in Journalism and Spanish from Boston University.

‚∏ª