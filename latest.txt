‚úÖ Morning News Briefing ‚Äì September 26, 2025 10:43

üìÖ Date: 2025-09-26 10:43
üè∑Ô∏è Tags: #briefing #ai #publichealth #digitalgov

‚∏ª

üßæ Weather
‚Ä¢ No watches or warnings in effect, Pembroke
  No watches or warnings in effect. No warnings or watches or watches in effect . Watch or warnings are no longer in effect in the U.S. No watches, warnings are in effect for the rest of the day . No watches and warnings are still in effect, but no watches are in place for the day's events . The weather is not expected to be affected by the weather .
‚Ä¢ Current Conditions:  10.2¬∞C
  Temperature: 10.2&deg;C Pressure / Tendency: 100.7 kPa rising Humidity: 100 % Humidity is 100 % Dewpoint: 10 .2&degree . Wind: NW 3 km/h . Air Quality Health Index: n/a . Weather: Pembroke 6:00 AM EDT Friday 26 September 2025 . Weather forecast for Pem
‚Ä¢ Friday: Chance of showers. High 21. POP 40%
  Fog patches dissipating this morning . 40 percent chance of showers this afternoon . Mainly cloudy. Mainly . cloudy . High 21 . Humidex 25 or moderate. UV index 5 or moderate . Forecast issued 5:00 AM EDT Friday 26 September 2025 . Weather will be mainly cloudy this morning, with rain likely to fall in the afternoon . Fog patches are dissipating today .

üåç International News
No updates.

üçÅ Canadian News
No updates.

üá∫üá∏ U.S. Top Stories
‚Ä¢ There was no rapture this week, so the quiz returns. Can you score a perfect 11?
  This week, Jimmy Kimmel returned, a weird statue vanished and no one (to our knowledge) got snatched up to heaven . No one was snatched up from heaven . Jimmy Kimmel returns to the show on Wednesday night, but no one has ever been snatched up by heaven . Kimmel: "Jimmy Kimmel" is back and Jimmy Kimmel will be back in the next week's episode on "Jimmy
‚Ä¢ Trump's TikTok deal payment criticized as 'shake-down scheme' by experts
  The U.S. government will collect a multibillion-dollar fee from the American investors who will take over TikTok . Some experts call the fee and other deals like it "extortion" Some experts say the fee is extortion and others call it extortion . TikTok is a mobile app that allows users to stream photos and videos from around the world, but not everyone is happy
‚Ä¢ Fired feds, Trump lovers and veterans: Meet the people applying for ICE jobs
  At a recent DHS career expo in Provo, Utah, many attendees hoped to get hired to help with the Trump administration's deportation efforts . At the recent expo, many attendees hoped to be hired to work for the DHS in the U.S. Department of Immigration and Customs Enforcement . Many attendees hope to get jobs to help in the administration's efforts to deport illegal
‚Ä¢ What schools stand to lose in the battle over the next federal education budget
  Budget proposals would impose steep cuts on some of the nation's most vulnerable students and disadvantaged school communities . Education researchers warn budget proposals from the White House and House Republicans would impose sharp cuts on vulnerable students, disadvantaged communities . The budget proposals are part of a bipartisan effort to cut spending on education in schools across the U.S. and around the world . The White House's budget proposal is
‚Ä¢ Trump's Tylenol warning echoes past misconceptions about mothers and autism
  Medical scholars say efforts to find a singular cause for autism have historically led to scrutinizing parents and fueling stigma about autism . They say the stigma of autism has fueled scrutiny of parents and stigmatization of children with autism . Autism is a disorder that has been linked to autism in the U.S. for more than a decade . The autism disorder is an extremely rare disorder that can only be

üß† Artificial Intelligence
No updates.

üíª Digital Strategy
‚Ä¢ UK and US security agencies order urgent fixes as Cisco firewall bugs exploited in wild
  CISA gives feds 24 hours to patch, NCSC urges rapid action as flaws linked to ArcaneDoor spies . Cybersecurity agencies on both sides of the Atlantic are sounding the alarm over Cisco firewall vulnerabilities that are being exploited by an "advanced threat actor"‚Ä¶‚Ä¶‚Ä¶ CISA gave feds 24-hour to patch and NCSC urged rapid action . CISA: CISA give
‚Ä¢ UK to roll out mandatory digital ID for right to work by 2029
  The UK government plans to issue all legal residents a digital identity by the end of the current Parliament . It could run until August 2029, with its use required to get a job . The scheme could also be used to get access to a job, with the use of the identity required to obtain a licence . The UK has been accused of over-reacting to past denials of
‚Ä¢ Brits warned as illegal robo-callers with offshored call centers fined half a million
  UK's data protection watchdog fined two Brit businesses with offshore call centers ¬£550,000 (c $735,000) over illegal automated marketing calls . The UK's Data Protection watchdog fined the two businesses ¬£550k (c$735k) over the illegal calls . It‚Äôs amazing the number of calls Jo, Helen, and Ian get through the automated calls .
‚Ä¢ Microsoft agrees to 11th hour Win 10 end of life concessions
  Consumer org forces Redmond to expunge list of requirements for free ESU in Euro Economic Area, just need a Microsoft account . Microsoft will give consumers in the European Economic Area no-strings extended support for the soon-to-be-EOL Windows 10 . ESU will be free to use in the region without a need for a subscription to the company's Windows 10 software .
‚Ä¢ Just using open source software isn't radical any more. Europe needs to dig deeper
  Companies must realize they can be more than pure consumers, and public sector ought to go beyond 'promotion' Feature is 2025. Linux will turn 34 and the Free Software Foundation (FSF) 40. For the EU and Europe at large, which is famously experimental with government deployments of open source tech, it's a crunch point. It's a time for the public sector to go

üè• Public Health
No updates.

üî¨ Science
‚Ä¢ Why is autism really on the rise? What the science says
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Ophthalmic care at an academic medical centre for patients who were incarcerated or in immigration detention
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Association of a high-protein and low-glycemic-index diet during pregnancy with offspring growth and obesity until the age of 18 years ‚Äì a target trial emulation
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Quality safety and disparity of an AI chatbot in managing chronic diseases: simulated patient experiments
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ The use of functional foods and its association to chronic and multimorbid conditions: a cross-sectional study among Bangladeshi people
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations

üßæ Government & Policy
No updates.

üèõÔ∏è Enterprise Architecture & IT Governance
No updates.

ü§ñ AI & Emerging Tech
‚Ä¢ Shoplifters could soon be chased down by drones
  Flock Safety, whose drones were once reserved for police departments, is now offering them for private-sector security . The company says it‚Äôs in talks with large retailers but doesn‚Äôt yet have any signed contracts . Flock will pitch the drones to hospital campuses, warehouse sites, and oil and gas facilities . The ACLU says Flock's expansion into the private sector is ‚Äúa logical step, but in the wrong direction‚Äù
‚Ä¢ The Download: growing threats to vulnerable languages, and fact-checking Trump‚Äôs medical claims
  This is today&#8217;s edition of¬†The Download,¬†our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.







How AI and Wikipedia have sent vulnerable languages into a doom spiral



Wikipedia is the most ambitious multilingual project after the Bible: There are editions in over 340 languages, and a further 400 even more obscure ones are being developed. But many of these smaller editions are being swamped with AI-translated content. Volunteers working on four African languages, for instance, estimated to MIT Technology Review that between 40% and 60% of articles in their Wikipedia editions were uncorrected machine translations.



This is beginning to cause a wicked problem. AI systems learn new languages by scraping huge quantities of text from the internet. Wikipedia is sometimes the largest source of online linguistic data for languages with few speakers‚Äîso any errors on those pages can poison the wells that AI is expected to draw from. Volunteers are being forced to go to extreme lengths to fix the issue, even deleting certain languages from Wikipedia entirely. Read the full story.&nbsp;



‚ÄîJacob Judah&nbsp;



This story is part of our Big Story series: MIT Technology Review‚Äôs most important, ambitious reporting. These stories take a deep look at the technologies that are coming next and what they will mean for us and the world we live in. Check out the rest of the series here.







Trump is pushing leucovorin as a new treatment for autism. What is it?&nbsp;



On Monday, President Trump claimed that childhood vaccines and acetaminophen, the active ingredient in Tylenol, are to blame for the increasing prevalence of autism. He advised pregnant women against taking the medicine.&nbsp;



The administration also announced that the FDA would work to make a medication called leucovorin available as a treatment for children with autism. The president‚Äôs assertions left many dismayed. ‚ÄúThe data cited do not support the claim that Tylenol causes autism and leucovorin is a cure, and only stoke fear and falsely suggest hope when there is no simple answer,‚Äù said the Coalition for Autism Researchers, a group of more than 250 scientists, in a statement. So what does the evidence say? Read our story to find out.&nbsp;



‚ÄîCassandra Willyard&nbsp;



This is part of our MIT Technology Review Explains series, where our writers untangle the complex, messy world of technology to help you understand what‚Äôs coming next. You can read more from the series here.







Fusion power plants don‚Äôt exist yet, but they‚Äôre making money anyway



This week, Commonwealth Fusion Systems announced it has another customer for its first commercial fusion power plant, in Virginia. Eni, one of the world‚Äôs largest oil and gas companies, signed a billion-dollar deal to buy electricity from the facility.



One small detail? That reactor doesn‚Äôt exist yet. This is a weird moment in fusion. Investors are pouring billions into the field to build power plants, and companies are even signing huge agreements to purchase power from those still-nonexistent plants.&nbsp;



But all this comes before companies have actually completed a working reactor that can produce electricity. It takes money to develop a new technology, but all this funding could lead to some twisted expectations. Read the full story.



‚ÄîCasey Crownhart&nbsp;



This story is from The Spark, our weekly newsletter all about the latest in climate change and clean tech. Sign up to receive it in your inbox every Wednesday.







The AI Hype Index: Cracking the chatbot code



Millions of us use chatbots every day, even though we don‚Äôt really know how they work or how using them affects us. In a bid to address this, the FTC recently launched an inquiry into how chatbots affect children and teenagers. Elsewhere, OpenAI has started to shed more light on what people are actually using ChatGPT for, and why it thinks its LLMs are so prone to making stuff up.



There‚Äôs still plenty we don‚Äôt know‚Äîbut that isn‚Äôt stopping governments from forging ahead with AI projects. In the US, RFK Jr. is pushing his staffers to use ChatGPT, while Albania is using a chatbot for public contract procurement. Check out the latest edition of our AI Hype Index to help you sort AI reality from hyped-up fiction.&nbsp;







The must-reads



I‚Äôve combed the internet to find you today‚Äôs most fun/important/scary/fascinating stories about technology.



1 Huntington‚Äôs disease has been treated successfully for the first timeGene therapy managed to slow progress of the disease in patients by 75%. (The Economist $)&nbsp;+ Here‚Äôs how the gene editing tool CRISPR is changing lives. (MIT Technology Review)2 Google says 90% of tech workers are using AIBut most of them also say they don‚Äôt trust AI models‚Äô outputs. (CNN)+ Why does AI hallucinate? (MIT Technology Review)3 A MAGA TikTok takeover is comingJust as free speech protections in the US start to look worryingly fragile. (The Atlantic $)4 Chinese tech workers are returning from the USThere‚Äôs a whole bunch of complex factors both driving them to leave, and luring them back. (Rest of World)+ But it‚Äôs hard to say what the impact of the new $100,000 fee for H-1B visas will be on India‚Äôs tech sector. (WP $)+ Europe is hoping to nab more tech talent too. (The Verge)



5 If AI can diagnose us, what are doctors for?They need to prepare for the fact chatbot use is becoming more and more widespread among patients. (New Yorker $)+ This medical startup uses LLMs to run appointments and make diagnoses. (MIT Technology Review)6 Drones have been spotted at four more airports in DenmarkIt looks like a coordinated attack, but officials still haven‚Äôt worked out who is behind it. (FT $)7 TSMC has unveiled AI-designed chips that use less energyThe AI software found better solutions than TSMC‚Äôs own human engineers‚Äîand did so much faster. (South China Morning Post)+ These four charts sum up the state of AI and energy. (MIT Technology Review)



8 How to find love on dating apps&nbsp;It‚Äôs not easy, but it is possible. (The Guardian)9 AI models can‚Äôt cope with Persian social etiquetteIt involves a lot of saying ‚Äòno‚Äô when you mean ‚Äòyes‚Äô, which simply doesn‚Äôt wash with computers. (Ars Technica)10 VR headsets are better than ever, but no one seems to careThe tech industry keeps overestimating how willing people are to strap computers to their faces. (Gizmodo)







Quote of the day



‚ÄúWe are living through the most destructive arms race in human history.‚Äù



‚ÄîUkrainian president Volodymyr Zelenskyy tells world leaders gathered at the UN that they need to intervene to stop the escalating development of drone technology and AI, The Guardian reports.







One more thing



STUART BRADFORD




The great AI consciousness conundrum



AI consciousness isn‚Äôt just a tricky intellectual puzzle; it‚Äôs a morally weighty problem. Fail to identify a conscious AI, and you might unintentionally subjugate a being whose interests ought to matter. Mistake an unconscious AI for a conscious one, and you risk compromising human safety and happiness for the sake of an unthinking, unfeeling hunk of silicon and code.



Over the past few decades, a small research community has doggedly attacked the question of what consciousness is and how it works. The effort has yielded real progress. And now, with the rapid advance of AI technology, these insights could offer our only guide to the untested, morally fraught waters of artificial consciousness. Read the full story.



‚ÄîGrace Huckins







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)



+ It‚Äôs Fat Bear Week! Who gets your vote this year?+ Learn about Lord Woodbine, the forgotten sixth Beatle.&nbsp;+ There are some truly wild and wacky recipes in this Medieval Cookery collection. Venison porridge, anyone?&nbsp;+ Pessimism about technology is as old as technology itself, as this archive shows.
‚Ä¢ Fusion power plants don‚Äôt exist yet, but they‚Äôre making money anyway
  This week, Commonwealth Fusion Systems announced it has another customer for its first commercial fusion power plant, in Virginia. Eni, one of the world‚Äôs largest oil and gas companies, signed a billion-dollar deal to buy electricity from the facility.





One small detail? That reactor doesn‚Äôt exist yet. Neither does the smaller reactor Commonwealth is building first to demonstrate that its tokamak design will work as intended.



This is a weird moment in fusion. Investors are pouring billions into the field to build power plants, and some companies are even signing huge agreements to purchase power from those still-nonexistent plants. All this comes before companies have actually completed a working reactor that can produce electricity. It takes money to develop a new technology, but all this funding could lead to some twisted expectations.&nbsp;



Nearly three years ago, the National Ignition Facility at Lawrence Livermore National Laboratory hit a major milestone for fusion power. With the help of the world‚Äôs most powerful lasers, scientists heated a pellet of fuel to 100 million ¬∞C. Hydrogen atoms in that fuel fused together, releasing more energy than the lasers put in.



It was a game changer for the vibes in fusion. The NIF experiment finally showed that a fusion reactor could yield net energy. Plasma physicists‚Äô models had certainly suggested that it should be true, but it was another thing to see it demonstrated in real life.



But in some ways, the NIF results didn‚Äôt really change much for commercial fusion. That site‚Äôs lasers used a bonkers amount of energy, the setup was wildly complicated, and the whole thing lasted a fraction of a second. To operate a fusion power plant, not only do you have to achieve net energy, but you also need to do that on a somewhat constant basis and‚Äîcrucially‚Äîdo it economically.



So in the wake of the NIF news, all eyes went to companies like Commonwealth, Helion, and Zap Energy. Who would be the first to demonstrate this milestone in a more commercially feasible reactor? Or better yet, who would be the first to get a power plant up and running?



So far, the answer is none of them.





To be fair, many fusion companies have made technical progress. Commonwealth has built and tested its high-temperature superconducting magnets and published research about that work. Zap Energy demonstrated three hours of continuous operation in its test system, a milestone validated by the US Department of Energy. Helion started construction of its power plant in Washington in July. (And that‚Äôs not to mention a thriving, publicly funded fusion industry in China.)&nbsp;&nbsp;



These are all important milestones, and these and other companies have seen many more. But as Ed Morse, a professor of nuclear engineering at Berkeley, summed it up to me: ‚ÄúThey don‚Äôt have a reactor.‚Äù (He was speaking specifically about Commonwealth, but really, the same goes for the others.)



And yet, the money pours in. Commonwealth raised over $800 million in funding earlier this year. And now it‚Äôs got two big customers signed on to buy electricity from this future power plant.



Why buy electricity from a reactor that‚Äôs currently little more than ideas on paper? From the perspective of these particular potential buyers, such agreements can be something of a win-win, says Adam Stein, director of nuclear energy innovation at the Breakthrough Institute.



By putting a vote of confidence behind Commonwealth, Eni could help the fusion startup get the capital it needs to actually build its plant. The company also directly invests in Commonwealth, so it stands to benefit from success. Getting a good rate on the capital needed to build the plant could also mean the electricity is ultimately cheaper for Eni, Stein says.&nbsp;



Ultimately, fusion needs a lot of money. If fossil-fuel companies and tech giants want to provide it, all the better. One concern I have, though, is how outside observers are interpreting these big commitments.&nbsp;



US Energy Secretary Chris Wright has been loud about his support for fusion and his expectations of the technology. Earlier this month, he told the BBC that it will soon power the world.



He‚Äôs certainly not the first to have big dreams for fusion, and it is an exciting technology. But despite the jaw-dropping financial milestones, this industry is still very much in development.&nbsp;



And while Wright praises fusion, the Trump administration is slashing support for other energy technologies, including wind and solar power, and spreading disinformation about their safety, cost, and effectiveness.&nbsp;



To meet the growing electricity demand and cut emissions from the power sector, we‚Äôll need a whole range of technologies. It‚Äôs a risk and a distraction to put all our hopes on an unproven energy tech when there are plenty of options that actually exist.&nbsp;



This article is from The Spark, MIT Technology Review‚Äôs weekly climate newsletter. To receive it in your inbox every Wednesday, sign up here.
‚Ä¢ How AI and Wikipedia have sent vulnerable languages into a doom spiral
  When Kenneth Wehr started managing the Greenlandic-language version of Wikipedia four years ago, his first act was to delete almost everything. It had to go, he thought, if it had any chance of surviving.



Wehr, who‚Äôs 26, isn‚Äôt from Greenland‚Äîhe grew up in Germany‚Äîbut he had become obsessed with the island, an autonomous Danish territory, after visiting as a teenager. He‚Äôd spent years writing obscure Wikipedia articles in his native tongue on virtually everything to do with it. He even ended up moving to Copenhagen to study Greenlandic, a language spoken by some 57,000 mostly Indigenous Inuit people scattered across dozens of far-flung Arctic villages.&nbsp;




The Greenlandic-language edition was added to Wikipedia around 2003, just a few years after the site launched in English. By the time Wehr took its helm nearly 20 years later, hundreds of Wikipedians had contributed to it and had collectively written some 1,500 articles totaling over tens of thousands of words. It seemed to be an impressive vindication of the crowdsourcing approach that has made Wikipedia the go-to source for information online, demonstrating that it could work even in the unlikeliest places.&nbsp;



There was only one problem: The Greenlandic Wikipedia was a mirage.&nbsp;






Virtually every single article had been published by people who did not actually speak the language. Wehr, who now teaches Greenlandic in Denmark, speculates that perhaps only one or two Greenlanders had ever contributed. But what worried him most was something else: Over time, he had noticed that a growing number of articles appeared to be copy-pasted into Wikipedia by people using machine translators. They were riddled with elementary mistakes‚Äîfrom grammatical blunders to meaningless words to more significant inaccuracies, like an entry that claimed Canada had only 41 inhabitants. Other pages sometimes contained random strings of letters spat out by machines that were unable to find suitable Greenlandic words to express themselves.&nbsp;



‚ÄúIt might have looked Greenlandic to [the authors], but they had no way of knowing,‚Äù complains Wehr. 



‚ÄúSentences wouldn‚Äôt make sense at all, or they would have obvious errors,‚Äù he adds. ‚ÄúAI translators are really bad at Greenlandic.‚Äù&nbsp;&nbsp;



What Wehr describes is not unique to the Greenlandic edition.&nbsp;



Wikipedia is the most ambitious multilingual project after the Bible: There are editions in over 340 languages, and a further 400 even more obscure ones are being developed and tested. Many of these smaller editions have been swamped with automatically translated content as AI has become increasingly accessible. Volunteers working on four African languages, for instance, estimated to MIT Technology Review that between 40% and 60% of articles in their Wikipedia editions were uncorrected machine translations. And after auditing the Wikipedia edition in Inuktitut, an Indigenous language close to Greenlandic that‚Äôs spoken in Canada, MIT Technology Review estimates that more than two-thirds of pages containing more than several sentences feature portions created this way.&nbsp;



This is beginning to cause a wicked problem. AI systems, from Google Translate to ChatGPT, learn to ‚Äúspeak‚Äù new languages by scraping huge quantities of text from the internet. Wikipedia is sometimes the largest source of online linguistic data for languages with few speakers‚Äîso any errors on those pages, grammatical or otherwise, can poison the wells that AI is expected to draw from. That can make the models‚Äô translation of these languages particularly error-prone, which creates a sort of linguistic doom loop as people continue to add more and more poorly translated Wikipedia pages using those tools, and AI models continue to train from poorly translated pages. It‚Äôs a complicated problem, but it boils down to a simple concept: Garbage in, garbage out.&nbsp;



‚ÄúThese models are built on raw data,‚Äù says Kevin Scannell, a former professor of computer science at Saint Louis University who now builds computer software tailored for endangered languages. ‚ÄúThey will try and learn everything about a language from scratch. There is no other input. There are no grammar books. There are no dictionaries. There is nothing other than the text that is inputted.‚Äù



There isn‚Äôt perfect data on the scale of this problem, particularly because a lot of AI training data is kept confidential and the field continues to evolve rapidly. But back in 2020, Wikipedia was estimated to make up more than half the training data that was fed into AI models translating some languages spoken by millions across Africa, including Malagasy, Yoruba, and Shona. In 2022, a research team from Germany that looked into what data could be obtained by online scraping even found that Wikipedia was the sole easily accessible source of online linguistic data for 27 under-resourced languages.&nbsp;



This could have significant repercussions in cases where Wikipedia is poorly written‚Äîpotentially pushing the most vulnerable languages on Earth toward the precipice as future generations begin to turn away from them.&nbsp;



‚ÄúWikipedia will be reflected in the AI models for these languages,‚Äù says Trond Trosterud, a computational linguist at the University of Troms√∏ in Norway, who has been raising the alarm about the potentially harmful outcomes of badly run Wikipedia editions for years. ‚ÄúI find it hard to imagine it will not have consequences. And, of course, the more dominant position that Wikipedia has, the worse it will be.‚Äù&nbsp;



Use responsibly



Automation has been built into Wikipedia since the very earliest days. Bots keep the platform operational: They repair broken links, fix bad formatting, and even correct spelling mistakes. These repetitive and mundane tasks can be automated away with little problem. There is even an army of bots that scurry around generating short articles about rivers, cities, or animals by slotting their names into formulaic phrases. They have generally made the platform better.&nbsp;




But AI is different. Anybody can use it to cause massive damage with a few clicks.&nbsp;



Wikipedia has managed the onset of the AI era better than many other websites. It has not been flooded with AI bots or disinformation, as social media has been. It largely retains the innocence that characterized the earlier internet age. Wikipedia is open and free for anyone to use, edit, and pull from, and it‚Äôs run by the very same community it serves. It is transparent and easy to use. But community-run platforms live and die on the size of their communities. English has triumphed, while Greenlandic has sunk.&nbsp;




‚ÄúWe need good Wikipedians. This is something that people take for granted. It is not magic,‚Äù says Amir Aharoni, a member of the volunteer Language Committee, which oversees requests to open or close Wikipedia editions. ‚ÄúIf you use machine translation responsibly, it can be efficient and useful. Unfortunately, you cannot trust all people to use it responsibly.‚Äù&nbsp;



Trosterud has studied the behavior of users on small Wikipedia editions and says AI has empowered a subset that he terms ‚ÄúWikipedia hijackers.‚Äù These users can range widely‚Äîfrom naive teenagers creating pages about their hometowns or their favorite YouTubers to well-meaning Wikipedians who think that by creating articles in minority languages they are in some way ‚Äúhelping‚Äù those communities.&nbsp;



‚ÄúThe problem with them nowadays is that they are armed with Google Translate,‚Äù Trosterud says, adding that this is allowing them to produce much longer and more plausible-looking content than they ever could before: ‚ÄúEarlier they were armed only with dictionaries.‚Äù&nbsp;



This has effectively industrialized the acts of destruction‚Äîwhich affect vulnerable languages most, since AI translations are typically far less reliable for them. There can be lots of different reasons for this, but a meaningful part of the issue is the relatively small amount of source text that is available online. And sometimes models struggle to identify a language because it is similar to others, or because some, including Greenlandic and most Native American languages, have structures that make them badly suited to the way most machine translation systems work. (Wehr notes that in Greenlandic most words are agglutinative, meaning they are built by attaching prefixes and suffixes to stems. As a result, many words are extremely context specific and can express ideas that in other languages would take a full sentence.)&nbsp;



Research produced by Google before a major expansion of Google Translate rolled out three years ago found that translation systems for lower-resourced languages were generally of a lower quality than those for better-resourced ones. Researchers found, for example, that their model would often mistranslate basic nouns across languages, including the names of animals and colors. (In a statement to MIT Technology Review, Google wrote that it is ‚Äúcommitted to meeting a high standard of quality for all 249 languages‚Äù it supports ‚Äúby rigorously testing and improving [its] systems, particularly for languages that may have limited public text resources on the web.‚Äù)&nbsp;



Wikipedia itself offers a built-in editing tool called Content Translate, which allows users to automatically translate articles from one language to another‚Äîthe idea being that this will save time by preserving the references and fiddly formatting of the originals. But it piggybacks on external machine translation systems, so it‚Äôs largely plagued by the same weaknesses as other machine translators‚Äîa problem that the Wikimedia Foundation says is hard to solve. It‚Äôs up to each edition‚Äôs community to decide whether this tool is allowed, and some have decided against it. (Notably, English-language Wikipedia has largely banned its use, claiming that some 95% of articles created using Content Translate failed to meet an acceptable standard without significant additional work.) But it‚Äôs at least easy to tell when the program has been used; Content Translate adds a tag on the Wikipedia back end.&nbsp;



Other AI programs can be harder to monitor. Still, many Wikipedia editors I spoke with said that once their languages were added to major online translation tools, they noticed a corresponding spike in the frequency with which poor, likely machine-translated pages were created.&nbsp;



Some Wikipedians using AI to translate content do occasionally admit that they do not speak the target languages. They may see themselves as providing smaller communities with rough-cut articles that speakers can then fix‚Äîessentially following the same model that has worked well for more active Wikipedia editions.&nbsp;&nbsp;




Google Translate, for instance, says the Fulfulde word for January means June, while ChatGPT says it‚Äôs August or September. The programs also suggest the Fulfulde word for ‚Äúharvest‚Äù means ‚Äúfever‚Äù or ‚Äúwell-being,‚Äù among other possibilities.&nbsp;&nbsp;




But once error-filled pages are produced in small languages, there is usually not an army of knowledgeable people who speak those languages standing ready to improve them. There are few readers of these editions, and sometimes not a single regular editor.&nbsp;



Yuet Man Lee, a Canadian teacher in his 20s, says that he used a mix of Google Translate and ChatGPT to translate a handful of articles that he had written for the English Wikipedia into Inuktitut, thinking it‚Äôd be nice to pitch in and help a smaller Wikipedia community. He says he added a note to one saying that it was only a rough translation. ‚ÄúI did not think that anybody would notice [the article],‚Äù he explains. ‚ÄúIf you put something out there on the smaller Wikipedias‚Äîmost of the time nobody does.‚Äù&nbsp;



But at the same time, he says, he still thought ‚Äúsomeone might see it and fix it up‚Äù‚Äîadding that he had wondered whether the Inuktitut translation that the AI systems generated was grammatically correct. Nobody has touched the article since he created it.



Lee, who teaches social sciences in Vancouver and first started editing entries in the English Wikipedia a decade ago, says that users familiar with more active Wikipedias can fall victim to this mindset, which he terms a ‚Äúbigger-Wikipedia arrogance‚Äù: When they try to contribute to smaller Wikipedia editions, they assume that others will come along to fix their mistakes. It can sometimes work. Lee says he had previously contributed several articles to Wikipedia in Tatar, a language spoken by several million people mainly in Russia, and at least one of those was eventually corrected. But the Inuktitut Wikipedia is, by comparison, a ‚Äúbarren wasteland.‚Äù&nbsp;



He emphasizes that his intentions had been good: He wanted to add more articles to an Indigenous Canadian Wikipedia. ‚ÄúI am now thinking that it may have been a bad idea. I did not consider that I could be contributing to a recursive loop,‚Äù he says. ‚ÄúIt was about trying to get content out there, out of curiosity and for fun, without properly thinking about the consequences.‚Äù&nbsp;



&nbsp;‚ÄúTotally, completely no future‚Äù



Wikipedia is a project that is driven by wide-eyed optimism. Editing can be a thankless task, involving weeks spent bickering with faceless, pseudonymous people, but devotees put in hours of unpaid labor because of a commitment to a higher cause. It is this commitment that drives many of the regular small-language editors I spoke with. They all feared what would happen if garbage continued to appear on their pages.



Abdulkadir Abdulkadir, a 26-year-old agricultural planner who spoke with me over a crackling phone call from a busy roadside in northern Nigeria, said that he spends three hours every day fiddling with entries in his native Fulfulde, a language used mainly by pastoralists and farmers across the Sahel. ‚ÄúBut the work is too much,‚Äù he said.&nbsp;



Abdulkadir sees an urgent need for the Fulfulde Wikipedia to work properly. He has been suggesting it as one of the few online resources for farmers in remote villages, potentially offering information on which seeds or crops might work best for their fields in a language they can understand. If you give them a machine-translated article, Abdulkadir told me, then it could ‚Äúeasily harm them,‚Äù as the information will probably not be translated correctly into Fulfulde.¬†



Google Translate, for instance, says the Fulfulde word for January means June, while ChatGPT says it‚Äôs August or September. The programs also suggest the Fulfulde word for ‚Äúharvest‚Äù means ‚Äúfever‚Äù or ‚Äúwell-being,‚Äù among other possibilities.&nbsp;&nbsp;



Abdulkadir said he had recently been forced to correct an article about cowpeas, a foundational cash crop across much of Africa, after discovering that it was largely illegible.&nbsp;



If someone wants to create pages on the Fulfulde Wikipedia, Abdulkadir said, they should be translated manually. Otherwise, ‚Äúwhoever will read your articles will [not] be able to get even basic knowledge,‚Äù he tells these Wikipedians. Nevertheless, he estimates that some 60% of articles are still uncorrected machine translations. Abdulkadir told me that unless something important changes with how AI systems learn and are deployed, then the outlook for Fulfulde looks bleak. ‚ÄúIt is going to be terrible, honestly,‚Äù he said. ‚ÄúTotally, completely no future.‚Äù&nbsp;



Across the country from Abdulkadir, Lucy Iwuala contributes to Wikipedia in Igbo, a language spoken by several million people in southeastern Nigeria. ‚ÄúThe harm has already been done,‚Äù she told me, opening the two most recently created articles. Both had been automatically translated via Wikipedia‚Äôs Content Translate and contained so many mistakes that she said it would have given her a headache to continue reading them. ‚ÄúThere are some terms that have not even been translated. They are still in English,‚Äù she pointed out. She recognized the username that had created the pages as a serial offender. ‚ÄúThis one even includes letters that are not used in the Igbo language,‚Äù she said.&nbsp;



Iwuala began regularly contributing to Wikipedia three years ago out of concern that Igbo was being displaced by English. It is a worry that is common to many who are active on smaller Wikipedia editions. ‚ÄúThis is my culture. This is who I am,‚Äù she told me. ‚ÄúThat is the essence of it all: to ensure that you are not erased.‚Äù&nbsp;



Iwuala, who now works as a professional translator between English and Igbo, said the users doing the most damage are inexperienced and see AI translations as a way to quickly increase the profile of the Igbo Wikipedia. She often finds herself having to explain at online edit-a-thons she organizes, or over email to various error-prone editors, that the results can be the exact opposite, pushing users away: ‚ÄúYou will be discouraged and you will no longer want to visit this place. You will just abandon it and go back to the English Wikipedia.‚Äù&nbsp;&nbsp;



These fears are echoed by Noah Ha‚Äòalilio Solomon, an assistant professor of Hawaiian language at the University of Hawai‚Äòi. He reports that some 35% of words on some pages in the Hawaiian Wikipedia are incomprehensible. ‚ÄúIf this is the Hawaiian that is going to exist online, then it will do more harm than anything else,‚Äù he says.&nbsp;



Hawaiian, which was teetering on the verge of extinction several decades ago, has been undergoing a recovery effort led by Indigenous activists and academics. Seeing such poor Hawaiian on such a widely used platform as Wikipedia is upsetting to Ha‚Äòalilio Solomon.&nbsp;



‚ÄúIt is painful, because it reminds us of all the times that our culture and language has been appropriated,‚Äù he says. ‚ÄúWe have been fighting tooth and nail in an uphill climb for language revitalization. There is nothing easy about that, and this can add extra impediments. People are going to think that this is an accurate representation of the Hawaiian language.‚Äù&nbsp;



The consequences of all these Wikipedia errors can quickly become clear. AI translators that have undoubtedly ingested these pages in their training data are now assisting in the production, for instance, of error-strewn AI-generated books aimed at learners of languages as diverse as Inuktitut and Cree, Indigenous languages spoken in Canada, and Manx, a small Celtic language spoken on the Isle of Man. Many of these have been popping up for sale on Amazon. ‚ÄúIt was just complete nonsense,‚Äù says Richard Compton, a linguist at the University of Quebec in Montreal, of a volume he reviewed that had purported to be an introductory phrasebook for Inuktitut.&nbsp;



Rather than making minority languages more accessible, AI is now creating an ever expanding minefield for students and speakers of those languages to navigate. ‚ÄúIt is a slap in the face,‚Äù Compton says. He worries that younger generations in Canada, hoping to learn languages in communities that have fought uphill battles against discrimination to pass on their heritage, might turn to online tools such as ChatGPT or phrasebooks on Amazon and simply make matters worse. ‚ÄúIt is fraud,‚Äù he says.



A race against time



According to UNESCO, a language is declared extinct every two weeks. But whether the Wikimedia Foundation, which runs Wikipedia, has an obligation to the languages used on its platform is an open question. When I spoke to Runa Bhattacharjee, a senior director at the foundation, she said that it was up to the individual communities to make decisions about what content they wanted to exist on their Wikipedia. ‚ÄúUltimately, the responsibility really lies with the community to see that there is no vandalism or unwanted activity, whether through machine translation or other means,‚Äù she said. Usually, Bhattacharjee added, editions were considered for closure only if a specific complaint was raised about them.&nbsp;



But if there is no active community, how can an edition be fixed or even have a complaint raised?&nbsp;



Bhattacharjee explained that the Wikimedia Foundation sees its role in such cases as about maintaining the Wikipedia platform in case someone comes along to revive it: ‚ÄúIt is the space that we provide for them to grow and develop. That is where we are at.‚Äù&nbsp;&nbsp;&nbsp;



Inari Saami, spoken in a single remote community in northern Finland, is a poster child for how people can take good advantage of Wikipedia. The language was headed toward extinction four decades ago; there were only four children who spoke it. Their parents created the Inari Saami Language Association in a last-ditch bid to keep it going. The efforts worked. There are now several hundred speakers, schools that use Inari Saami as a medium of instruction, and 6,400 Wikipedia articles in the language, each one copy-edited by a fluent speaker.&nbsp;



This success highlights how Wikipedia can indeed provide small and determined communities with a unique vehicle to promote their languages‚Äô preservation. ‚ÄúWe don‚Äôt care about quantity. We care about quality,‚Äù says Fabrizio Brecciaroli, a member of the Inari Saami Language Association. ‚ÄúWe are planning to use Wikipedia as a repository for the written language. We need to provide tools that can be used by the younger generations. It is important for them to be able to use Inari Saami digitally.‚Äù&nbsp;





This has been such a success that Wikipedia has been integrated into the curriculum at the Inari Saami‚Äìspeaking schools, Brecciaroli adds. He fields phone calls from teachers asking him to write up simple pages on topics from tornadoes to Saami folklore. Wikipedia has even offered a way to introduce words into Inari Saami. ‚ÄúWe have to make up new words all the time,‚Äù Brecciaroli says. ‚ÄúYoung people need them to speak about sports, politics, and video games. If they are unsure how to say something, they now check Wikipedia.‚Äù



Wikipedia is a monumental intellectual experiment. What‚Äôs happening with Inari Saami suggests that with maximum care, it can work in smaller languages. ‚ÄúThe ultimate goal is to make sure that Inari Saami survives,‚Äù Brecciaroli says. ‚ÄúIt might be a good thing that there isn‚Äôt a Google Translate in Inari Saami.‚Äù&nbsp;



That may be true‚Äîthough large language models like ChatGPT can be made to translate phrases into languages that more traditional machine translation tools do not offer. Brecciaroli told me that ChatGPT isn‚Äôt great in Inari Saami but that the quality varies significantly depending on what you ask it to do; if you ask it a question in the language, then the answer will be filled with words from Finnish and even words it invents. But if you ask it something in English, Finnish, or Italian and then ask it to reply in Inari Saami, it will perform better.&nbsp;



In light of all this, creating as much high-quality content online as can possibly be written becomes a race against time. ‚ÄúChatGPT only needs a lot of words,‚Äù Brecciaroli says. ‚ÄúIf we keep putting good material in, then sooner or later, we will get something out. That is the hope.‚Äù This is an idea supported by multiple linguists I spoke with‚Äîthat it may be possible to end the ‚Äúgarbage in, garbage out‚Äù cycle. (OpenAI, which operates ChatGPT, did not respond to a request for comment.)



Still, the overall problem is likely to grow and grow, since many languages are not as lucky as Inari Saami‚Äîand their AI translators will most likely be trained on more and more AI slop. Wehr, unfortunately, seems far less optimistic about the future of his beloved Greenlandic.&nbsp;



Since deleting much of the Greenlandic-language Wikipedia, he has spent years trying to recruit speakers to help him revive it. He has appeared in Greenlandic media and made social media appeals. But he hasn‚Äôt gotten much of a response; he says it has been demoralizing.&nbsp;



‚ÄúThere is nobody in Greenland who is interested in this, or who wants to contribute,‚Äù he says. ‚ÄúThere is completely no point in it, and that is why it should be closed.‚Äù&nbsp;



Late last year, he began a process requesting that the Wikipedia Language Committee shut down the Greenlandic-language edition. Months of bitter debate followed between dozens of Wikipedia bureaucrats; some seemed to be surprised that a superficially healthy-seeming edition could be gripped by so many problems.&nbsp;



Then, earlier this month, Wehr‚Äôs proposal was accepted: Greenlandic Wikipedia is set to be shuttered, and any articles that remain will be moved into the Wikipedia Incubator, where new language editions are tested and built. Among the reasons cited by the Language Committee is the use of AI tools, which have ‚Äúfrequently produced nonsense that could misrepresent the language.‚Äù&nbsp;&nbsp;&nbsp;



Nevertheless, it may be too late‚Äîmistakes in Greenlandic already seem to have become embedded in machine translators. If you prompt either Google Translate or ChatGPT to do something as simple as count to 10 in proper Greenlandic, neither program can deliver.&nbsp;



Jacob Judah is an investigative journalist based in London.&nbsp;
‚Ä¢ Roundtables: The Future of Birth Control
  Kevin Eisenfrats is one of the MIT Technology Review 2025 Innovators Under 35 . His company, Contraline, is working toward testing new birth control options for men . He is co-founder and CEO of the company, and MIT Tech Review's executive editor, Amy Nordrum . The list of the top 25 Innovator of the Year is published on September 24, 2025 .

üîí Cybersecurity & Privacy
‚Ä¢ Feds Tie ‚ÄòScattered Spider‚Äô Duo to $115M in Ransoms
  U.S. prosecutors last week levied criminal hacking charges against 19-year-old U.K. national Thalha Jubair for allegedly being a core member of Scattered Spider, a prolific cybercrime group blamed for extorting at least $115 million in ransom payments from victims. The charges came as Jubair and an alleged co-conspirator appeared in a London court to face accusations of hacking into and extorting several large U.K. retailers, the London transit system, and healthcare providers in the United States.
At a court hearing last week, U.K. prosecutors laid out a litany of charges against Jubair and 18-year-old Owen Flowers, accusing the teens of involvement in an August 2024 cyberattack that crippled Transport for London, the entity responsible for the public transport network in the Greater London area.
A court artist sketch of Owen Flowers (left) and Thalha Jubair appearing at Westminster Magistrates&#8217; Court last week. Credit: Elizabeth Cook, PA Wire.
On July 10, 2025, KrebsOnSecurity reported that Flowers and Jubair had been arrested in the United Kingdom in connection with recent Scattered Spider ransom attacks against the retailers Marks &amp; Spencer and Harrods, and the British food retailer Co-op Group.
That story cited sources close to the investigation saying Flowers was the Scattered Spider member who anonymously gave interviews to the media in the days after the group&#8217;s September 2023 ransomware attacks disrupted operations at Las Vegas casinos operated by MGM Resorts and Caesars Entertainment.
The story also noted that Jubair&#8217;s alleged handles on cybercrime-focused Telegram channels had far lengthier rap sheets involving some of the more consequential and headline-grabbing data breaches over the past four years. What follows is an account of cybercrime activities that prosecutors have attributed to Jubair&#8217;s alleged hacker handles, as told by those accounts in posts to public Telegram channels that are closely monitored by multiple cyber intelligence firms.
EARLY DAYS (2021-2022)
Jubair is alleged to have been a core member of the LAPSUS$¬†cybercrime group that¬†broke into dozens of technology companies beginning in late 2021, stealing source code and other internal data from tech giants including¬†Microsoft,¬†Nvidia,¬†Okta,¬†Rockstar Games,¬†Samsung,¬†T-Mobile, and¬†Uber.
That is, according to the former leader of the now-defunct LAPSUS$. In April 2022, KrebsOnSecurity published internal chat records taken from a server that LAPSUS$ used, and those chats indicate Jubair was working with the group using the nicknames Amtrak¬†and¬†Asyntax. In the middle of the gang&#8217;s cybercrime spree, Asyntax told the LAPSUS$ leader not to share T-Mobile‚Äôs logo in images sent to the group because he‚Äôd been previously busted for SIM-swapping and his parents would suspect he was back at it again.
The leader of LAPSUS$ responded by gleefully posting Asyntax&#8217;s real name, phone number, and other hacker handles into a public chat room on Telegram:

In March 2022, the leader of the LAPSUS$ data extortion group exposed Thalha Jubair‚Äôs name and hacker handles in a public chat room on Telegram.

That story about the leaked LAPSUS$ chats also connected Amtrak/Asyntax to several previous hacker identities, including ‚ÄúEverlynn,‚Äù who in April 2021 began offering a cybercriminal service that sold fraudulent ‚Äúemergency data requests‚Äù¬†targeting the major social media and email providers.
In these so-called &#8220;fake EDR&#8221; schemes, the hackers compromise email accounts tied to police departments and government agencies, and then send unauthorized demands for subscriber data (e.g. username, IP/email address), while claiming the information being requested can‚Äôt wait for a court order because it relates to an urgent matter of life and death.
The roster of the now-defunct &#8220;Infinity Recursion&#8221; hacking team, which sold fake EDRs between 2021 and 2022. The founder &#8220;Everlynn&#8221; has been tied to Jubair. The member listed as ‚ÄúPeter‚Äù became the leader of LAPSUS$ who would later post Jubair‚Äôs name, phone number and hacker handles into LAPSUS$‚Äôs chat channel.

EARTHTOSTAR
Prosecutors in New Jersey last week alleged Jubair was part of a threat group variously known as Scattered Spider, 0ktapus, and UNC3944, and that he used the nicknames EarthtoStar, Brad, Austin, and Austistic.
Beginning in 2022, EarthtoStar co-ran a bustling Telegram channel called Star Chat, which was home to a prolific SIM-swapping group that relentlessly used voice- and SMS-based phishing attacks to steal credentials from employees at the major wireless providers in the U.S. and U.K.
Jubair allegedly used the handle &#8220;Earth2Star,&#8221; a core member of a prolific SIM-swapping group operating in 2022. This ad produced by the group lists various prices for SIM swaps.
The group would then use that access to sell a SIM-swapping service that could redirect a target&#8217;s phone number to a device the attackers controlled, allowing them to intercept the victim&#8217;s phone calls and text messages (including one-time codes). Members of Star Chat targeted multiple wireless carriers with SIM-swapping attacks, but they focused mainly on phishing T-Mobile employees.
In February 2023, KrebsOnSecurity scrutinized more than seven months of these SIM-swapping solicitations on Star Chat, which almost daily peppered the public channel with &#8220;Tmo up!&#8221; and &#8220;Tmo down!&#8221; notices indicating periods wherein the group claimed to have active access to T-Mobile&#8217;s network.
A redacted receipt from Star Chat&#8217;s SIM-swapping service targeting a T-Mobile customer after the group gained access to internal T-Mobile employee tools.
The data showed that Star Chat &#8212; along with two other SIM-swapping groups operating at the same time &#8212; collectively broke into T-Mobile over a hundred times in the last seven months of 2022. However, Star Chat was by far the most prolific of the three, responsible for at least 70 of those incidents.
The 104 days in the latter half of 2022 in which different known SIM-swapping groups claimed access to T-Mobile employee tools. Star Chat was responsible for a majority of these incidents. Image: krebsonsecurity.com.
A review of EarthtoStar&#8217;s messages on Star Chat as indexed by the threat intelligence firm Flashpoint shows this person also sold &#8220;AT&amp;T email resets&#8221; and AT&amp;T call forwarding services for up to $1,200 per line. EarthtoStar explained the purpose of this service in post on Telegram:
&#8220;Ok people are confused, so you know when u login to chase and it says &#8216;2fa required&#8217; or whatever the fuck, well it gives you two options, SMS or Call. If you press call, and I forward the line to you then who do you think will get said call?&#8221;
New Jersey prosecutors allege Jubair also was involved in a mass SMS phishing campaign during the summer of 2022 that stole single sign-on credentials from employees at hundreds of companies. The text messages asked users to click a link and log in at a phishing page that mimicked their employer‚Äôs Okta authentication page, saying recipients needed to review pending changes to their upcoming work schedules.
The phishing websites used a Telegram instant message bot to forward any submitted credentials in real-time, allowing the attackers to use the phished username, password and one-time code to log in as that employee at the real employer website.
That weeks-long SMS phishing campaign led to intrusions and data thefts at more than 130 organizations, including LastPass, DoorDash, Mailchimp, Plex and Signal.
A visual depiction of the attacks by the SMS phishing group known as 0ktapus, ScatterSwine, and Scattered Spider. Image: Amitai Cohen twitter.com/amitaico.
DA, COMRADE
EarthtoStar&#8217;s group Star Chat specialized in phishing their way into business process outsourcing (BPO) companies that provide customer support for a range of multinational companies, including a number of the world&#8217;s largest telecommunications providers. In May 2022, EarthtoStar posted to the Telegram channel &#8220;Frauwudchat&#8221;:
&#8220;Hi, I am looking for partners in order to exfiltrate data from large telecommunications companies/call centers/alike, I have major experience in this field, [including] a massive call center which houses 200,000+ employees where I have dumped all user credentials and gained access to the [domain controller] + obtained global administrator I also have experience with REST API&#8217;s and programming. I have extensive experience with VPN, Citrix, cisco anyconnect, social engineering + privilege escalation. If you have any Citrix/Cisco VPN or any other useful things please message me and lets work.&#8221;
At around the same time in the Summer of 2022, at least two different accounts tied to Star Chat &#8212; &#8220;RocketAce&#8221; and &#8220;Lopiu&#8221; &#8212; introduced the group&#8217;s services to denizens of the Russian-language cybercrime forum Exploit, including:
-SIM-swapping services targeting Verizon and T-Mobile customers;
-Dynamic phishing pages targeting customers of single sign-on providers like Okta;
-Malware development services;
-The sale of extended validation (EV) code signing certificates.
The user &#8220;Lopiu&#8221; on the Russian cybercrime forum Exploit advertised many of the same unique services offered by EarthtoStar and other Star Chat members. Image source: ke-la.com.
These two accounts on Exploit created multiple sales threads in which they claimed administrative access to U.S. telecommunications providers and asked other Exploit members for help in monetizing that access. In June 2022, RocketAce, which appears to have been just one of EarthtoStar&#8217;s many aliases, posted to Exploit:
Hello. I have access to a telecommunications company&#8217;s citrix and vpn. I would like someone to help me break out of the system and potentially attack the domain controller so all logins can be extracted we can discuss payment and things leave your telegram in the comments or private message me ! Looking for someone with knowledge in citrix/privilege escalation
On Nov. 15, 2022, EarthtoStar posted to their Star Sanctuary Telegram channel that they were hiring malware developers with a minimum of three years of experience and the ability to develop rootkits, backdoors and malware loaders.
&#8220;Optional: Endorsed by advanced APT Groups (e.g. Conti, Ryuk),&#8221; the ad concluded, referencing two of Russia&#8217;s most rapacious and destructive ransomware affiliate operations. &#8220;Part of a nation-state / ex-3l (3 letter-agency).&#8221;
2023-PRESENT DAY
The Telegram and Discord chat channels wherein Flowers and Jubair allegedly planned and executed their extortion attacks are part of a loose-knit network known as the Com, an English-speaking cybercrime community consisting mostly of individuals living in the United States, the United Kingdom, Canada and Australia.
Many of these Com chat servers have hundreds to thousands of members each, and some of the more interesting solicitations on these communities are job offers for in-person assignments and tasks that can be found if one searches for posts titled, ‚ÄúIf you live near,‚Äù or ‚ÄúIRL job‚Äù ‚Äî short for ‚Äúin real life‚Äù job.
These &#8220;violence-as-a-service&#8221; solicitations typically involve ‚Äúbrickings,‚Äù where someone is hired to toss a brick through the window at a specified address. Other IRL jobs for hire include tire-stabbings, molotov cocktail hurlings, drive-by shootings, and even home invasions. The people targeted by these services are typically other criminals within the community, but it&#8217;s not unusual to see Com members asking others for help in harassing or intimidating security researchers and even the very law enforcement officers who are investigating their alleged crimes.
It remains unclear what precipitated this incident or what followed directly after, but on January 13, 2023, a Star Sanctuary account used by EarthtoStar solicited the home invasion of a sitting U.S. federal prosecutor from New York. That post included a photo of the prosecutor taken from the Justice Department&#8217;s website, along with the message:
&#8220;Need irl niggas, in home hostage shit no fucking pussies no skinny glock holding 100 pound niggas either&#8221;
Throughout late 2022 and early 2023, EarthtoStar&#8217;s alias &#8220;Brad&#8221; (a.k.a. &#8220;Brad_banned&#8221;) frequently advertised Star Chat&#8217;s malware development services, including custom malicious software designed to hide the attacker&#8217;s presence on a victim machine:
We can develop KERNEL malware which will achieve persistence for a long time,
bypass firewalls and have reverse shell access.
This shit is literally like STAGE 4 CANCER FOR COMPUTERS!!!
Kernel meaning the highest level of authority on a machine.
This can range to simple shells to Bootkits.
Bypass all major EDR&#8217;s (SentinelOne, CrowdStrike, etc)
Patch EDR&#8217;s scanning functionality so it&#8217;s rendered useless!
Once implanted, extremely difficult to remove (basically impossible to even find)
Development Experience of several years and in multiple APT Groups.
Be one step ahead of the game. Prices start from $5,000+. Message @brad_banned to get a quote
In September 2023 , both MGM Resorts and Caesars Entertainment suffered ransomware attacks at the hands of a Russian ransomware affiliate program known as ALPHV and BlackCat. Caesars reportedly paid a $15 million ransom in that incident.
Within hours of MGM publicly acknowledging the 2023 breach, members of Scattered Spider were claiming credit and telling reporters they&#8217;d broken in by social engineering a third-party IT vendor. At a hearing in London last week, U.K. prosecutors told the court Jubair was found in possession of more than $50 million in ill-gotten cryptocurrency, including funds that were linked to the Las Vegas casino hacks.
The Star Chat channel was finally banned by Telegram on March 9, 2025. But U.S. prosecutors say Jubair and fellow Scattered Spider members continued their hacking, phishing and extortion activities up until September 2025.
In April 2025, the Com was buzzing about the publication of &#8220;The Com Cast,&#8221; a lengthy screed detailing Jubair&#8217;s alleged cybercriminal activities and nicknames over the years. This account included photos and voice recordings allegedly of Jubair, and asserted that in his early days on the Com Jubair used the nicknames Clark and Miku (these are both aliases used by Everlynn in connection with their fake EDR services).
Thalha Jubair (right), without his large-rimmed glasses, in an undated photo posted in The Com Cast.
More recently, the anonymous Com Cast author(s) claimed, Jubair had used the nickname &#8220;Operator,&#8221; which corresponds to a Com member who ran an automated Telegram-based doxing service that pulled consumer records from hacked data broker accounts. That public outing came after Operator allegedly seized control over the Doxbin, a long-running and highly toxic community that is used to &#8220;dox&#8221; or post deeply personal information on people.
&#8220;Operator/Clark/Miku: A key member of the ransomware group Scattered Spider, which consists of a diverse mix of individuals involved in SIM swapping and phishing,&#8221; the Com Cast account stated. &#8220;The group is an amalgamation of several key organizations, including Infinity Recursion (owned by Operator), True Alcorians (owned by earth2star), and Lapsus, which have come together to form a single collective.&#8221;
The New Jersey complaint (PDF) alleges Jubair and other Scattered Spider members committed computer fraud, wire fraud, and money laundering in relation to at least 120 computer network intrusions involving 47 U.S. entities between May 2022 and September 2025. The complaint alleges the group&#8217;s victims paid at least $115 million in ransom payments.
U.S. authorities say they traced some of those payments to Scattered Spider to an Internet server controlled by Jubair. The complaint states that a cryptocurrency wallet discovered on that server was used to purchase several gift cards, one of which was used at a food delivery company to send food to his apartment. Another gift card purchased with cryptocurrency from the same server was allegedly used to fund online gaming accounts under Jubair&#8217;s name. U.S. prosecutors said that when they seized that server they also seized $36 million in cryptocurrency.
The complaint also charges Jubair with involvement in a hacking incident in January 2025 against the U.S. courts system that targeted a U.S. magistrate judge overseeing a related Scattered Spider investigation. That other investigation appears to have been the prosecution of Noah Michael Urban, a 20-year-old Florida man charged in November 2024 by prosecutors in Los Angeles as one of five alleged Scattered Spider members.
Urban pleaded guilty in April 2025 to wire fraud and conspiracy charges, and in August he was sentenced to 10 years in federal prison. Speaking with KrebsOnSecurity from jail after his sentencing, Urban asserted that the judge case gave him more time than prosecutors requested because he was mad that Scattered Spider hacked his email account.
Noah &#8220;Kingbob&#8221; Urban, posting to Twitter/X around the time of his sentencing on Aug. 20.
A¬†court transcript (PDF) from a status hearing in February 2025 shows Urban was telling the truth about the hacking incident that happened while he was in federal custody. The judge told attorneys for both sides that a co-defendant in the California case was trying to find out about Mr. Urban‚Äôs activity in the Florida case, and that the hacker accessed the account by impersonating a judge over the phone and requesting a password reset.
Allison Nixon is chief research officer at the New York based security firm Unit 221B, and easily one of the world&#8217;s leading experts on Com-based cybercrime activity. Nixon said the core problem with legally prosecuting well-known cybercriminals from the Com has traditionally been that the top offenders tend to be under the age of 18, and thus difficult to charge under federal hacking statutes.
In the United States, prosecutors typically wait until an underage cybercrime suspect becomes an adult to charge them. But until that day comes, she said, Com actors often feel emboldened to continue committing &#8212; and very often bragging about &#8212; serious cybercrime offenses.
&#8220;Here we have a special category of Com offenders that effectively enjoy legal immunity,&#8221; Nixon told KrebsOnSecurity. &#8220;Most get recruited to Com groups when they are older, but of those that join very young, such as 12 or 13, they seem to be the most dangerous because at that age they have no grounding in reality and so much longevity before they exit their legal immunity.‚Äù
Nixon said U.K. authorities face the same challenge when they briefly detain and search the homes of underage Com suspects: Namely, the teen suspects simply go right back to their respective cliques in the Com and start robbing and hurting people again the minute they&#8217;re released.
Indeed, the U.K. court heard from prosecutors last week that both Scattered Spider suspects were detained and/or searched by local law enforcement on multiple occasions, only to return to the Com less than 24 hours after being released each time.
&#8220;What we see is these young Com members become vectors for perpetrators to commit enormously harmful acts and even child abuse,&#8221; Nixon said. &#8220;The members of this special category of people who enjoy legal immunity are meeting up with foreign nationals and conducting these sometimes heinous acts at their behest.&#8221;
Nixon said many of these individuals have few friends in real life because they spend virtually all of their waking hours on Com channels, and so their entire sense of identity, community and self-worth gets wrapped up in their involvement with these online gangs. She said if¬†the law was such that prosecutors could treat these people commensurate with the amount of harm they cause society, that would probably clear up a lot of this problem.
&#8220;If law enforcement was allowed to keep them in jail, they would quit reoffending,&#8221; she said.
The Times of London reports that Flowers is facing three charges under the Computer Misuse Act: two of conspiracy to commit an unauthorized act in relation to a computer causing/creating risk of serious damage to human welfare/national security and one of attempting to commit the same act. Maximum sentences for these offenses can range from 14 years to life in prison, depending on the impact of the crime.
Jubair is reportedly facing two charges in the U.K.: One of conspiracy to commit an unauthorized act in relation to a computer causing/creating risk of serious damage to human welfare/national security and one of failing to comply with a section 49 notice to disclose the key to protected information.
In the United States, Jubair is charged with computer fraud conspiracy, two counts of computer fraud, wire fraud conspiracy, two counts of wire fraud, and money laundering conspiracy. If extradited to the U.S., tried and convicted on all charges, he faces a maximum penalty of 95 years in prison.
In July 2025, the United Kingdom followed Australia&#8217;s example in banning victims of hacking from paying ransoms to cybercriminal groups unless approved by officials. U.K. organizations that are considered part of critical infrastructure reportedly will face a complete ban, as will the entire public sector. U.K. victims of a hack are now required to notify officials to better inform policymakers on the scale of Britain&#8217;s ransomware problem.
For further reading (bless you), check out Bloomberg&#8217;s poignant story last week based on a year&#8217;s worth of jailhouse interviews with convicted Scattered Spider member Noah Urban.

üéì University AI
No updates.

üè¢ Corporate AI
‚Ä¢ Using AI to assist in rare disease diagnosis
  In the promising and rapidly evolving field of genetic analysis, the ability to accurately interpret whole genome sequencing data is crucial for diagnosing and improving outcomes for people with rare genetic diseases. Yet despite technological advancements, genetic professionals face steep challenges in managing and synthesizing the vast amounts of data required for these analyses. Fewer than 50% of&nbsp;initial&nbsp;cases yield a diagnosis, and while reanalysis can lead to new findings, the process remains&nbsp;time-consuming and complex.&nbsp;



To better understand and address these challenges, Microsoft Research‚Äîin collaboration with Drexel University and the Broad Institute‚Äã‚Äã‚Äîconducted a comprehensive study titled¬†AI-Enhanced Sensemaking: Exploring the Design of a Generative AI-Based Assistant to Support Genetic Professionals (opens in new tab).¬†The study was recently published in a special edition of¬†ACM Transactions on Interactive Intelligent Systems¬†journal focused on generative AI.¬†¬†



The study focused on integrating generative AI to support the complex, time-intensive, and information-dense sensemaking tasks inherent in whole genome sequencing analysis. Through detailed empirical research and collaborative design sessions with experts in the field, we identified key obstacles genetic professionals face and proposed AI-driven solutions to enhance their workflows.&nbsp;‚Äã&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;‚ÄãWe&nbsp;developed strategies for how generative AI can help synthesize biomedical data, enabling AI-expert collaboration to increase the diagnoses of previously unsolved rare diseases‚Äîultimately aiming to improve patients‚Äô quality of life and life expectancy.



Whole genome sequencing in rare disease diagnosis



Rare diseases affect up to half a billion people globally and obtaining a diagnosis can take multiple years. These diagnoses often involve specialist consultations, laboratory tests, imaging studies, and invasive procedures. Whole genome sequencing is used to identify genetic variants responsible for these diseases by comparing a patient‚Äôs DNA sequence to reference genomes.&nbsp;‚Äã‚ÄãGenetic professionals use bioinformatics tools such as&nbsp;seqr,&nbsp;an open-source, web-based tool for rare disease case analysis and project management to assist them in filtering and prioritizing&nbsp; > 1 million variants to determine their potential role in disease.&nbsp;A critical component of&nbsp;their&nbsp;work is sensemaking: the process of searching, filtering, and synthesizing data to build, refine, and present models from complex sets of gene and variant information.&nbsp;&nbsp;



‚Äã‚ÄãThe multi-step sequencing process‚Äã‚Äã‚Äã&nbsp;typically takes three to 12 weeks and requires extensive amounts of evidence and time to synthesize and aggregate information&nbsp;‚Äã‚Äãto understand the gene and variant effects for the patient.&nbsp;If a patient&#8217;s case goes unsolved, their whole genome sequencing data is set aside until enough time has passed to warrant a reanalysis‚Äã‚Äã. This creates a backlog of patient cases‚Äã‚Äã. The ability to easily&nbsp;identify&nbsp;when new scientific evidence&nbsp;emerges&nbsp;and when to reanalyze an unsolved patient case is key to shortening the time patients suffer with an unknown rare disease diagnosis.&nbsp;



The promise of AI systems to assist with complex human tasks



Approximately 87% of AI systems never reach deployment&nbsp;‚Äãsimply because they solve‚Äã‚Äã‚Äã&nbsp;the wrong problems.&nbsp;‚Äã‚ÄãUnderstanding the AI support desired by different types of professionals, their current workflows, and AI capabilities is critical to successful AI system deployment and use. Matching technology capabilities with user tasks is particularly challenging in AI design because AI models can generate numerous outputs, and their capabilities can be unclear.&nbsp;‚ÄãTo design an effective‚Äã‚Äã‚Äã&nbsp;AI-based system‚Äã, one needs to identify‚Äã&nbsp;‚Äã‚Äãtasks AI can support,&nbsp;‚Äã‚Äãdetermine‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã&nbsp;the appropriate level of AI involvement, and&nbsp;‚Äã‚Äãdesign‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã&nbsp;user-AI interactions. This necessitates considering how humans interact with technology and how&nbsp;‚Äã‚ÄãAI&nbsp;can best be incorporated into workflows and tools.



	
		

		
		PODCAST SERIES
	
	
	
						
				
					
				
			
			
			

									The AI Revolution in Medicine, Revisited
				
								Join Microsoft‚Äôs Peter Lee on a journey to discover how AI is impacting healthcare and what it means for the future of medicine.
				
								
					
						
							Listen now						
					
				
							
	
Opens in a new tab	
	


Study objectives and co-designing a genetic AI assistant



Our study aimed to understand the current challenges and needs of genetic professionals performing whole genome sequencing analyses and explore the tasks where they want an AI assistant to support them in their work. The first phase of our study involved interviews with 17 genetics professionals to better understand their workflows, tools, and challenges. They included genetic analysts directly involved in interpreting data, as well as other roles participating in whole genome sequencing. In the second phase of our study, we conducted co-design sessions with study participants on how an AI assistant could support their workflows. We then developed a prototype of an AI assistant, which was further tested and refined with study participants in follow-up design walk-through sessions.



Identifying challenges in whole genome sequencing analysis



Through our in-depth interviews with genetic professionals, our study uncovered three critical challenges in whole genome sequencing analysis:




Information Overload: Genetic analysts need to gather and synthesize vast amounts of data from multiple sources. This task is incredibly time-consuming and prone to human error.



Collaborative Sharing: Sharing findings with others in the field can be cumbersome and inefficient, often relying on outdated methods that slow the collaborative analysis process.



Prioritizing Reanalysis: Given the continuous influx of new scientific discoveries, prioritizing unsolved cases to reanalyze is a daunting challenge. Analysts need a systematic approach to identify cases that might benefit most from reanalysis.




Genetic professionals highlighted the time-consuming nature of gathering and synthesizing information about genes and variants from different data sources. Other genetic professionals may have insights into certain genes and variants, but sharing and interpreting information with others for collaborative sensemaking requires significant time and effort. Although new scientific findings could affect unsolved cases through reanalysis, prioritizing cases based on new findings was challenging given the number of unsolved cases and limited time of genetic professionals.



Co-designing with experts and AI-human sensemaking tasks



Our study participants prioritized two potential tasks of an AI assistant. The first task was flagging cases for reanalysis based on new scientific findings. The assistant would alert analysts to unsolved cases that could benefit from new research, providing relevant updates drawn from recent publications. The second task focused on aggregating and synthesizing information about genes and variants from the scientific literature. This feature would compile essential information from numerous scientific papers about genes and variants, presenting it in a user-friendly format and saving analysts significant time and effort. Participants emphasized the need to balance selectivity with comprehensiveness in the evidence they review. They also envisioned collaborating with other genetic professionals to interpret, edit, and verify artifacts generated by the AI assistant.



Genetic professionals require both broad and focused evidence at different stages of their workflow. The AI assistant prototypes were designed to allow flexible filtering and thorough evidence aggregation, ensuring users can delve into comprehensive data or selectively focus on pertinent details. The prototypes included features for collaborative sensemaking, enabling users to interpret, edit, and verify AI-generated information collectively. This&nbsp;‚Äã‚Äãapproach not only&nbsp;‚Äãunderscores‚Äã‚Äã‚Äã&nbsp;the trustworthiness of AI outputs, but also facilitates shared understanding and decision-making among genetic professionals.



Design implications for expert-AI sensemaking



In the&nbsp;shifting frontiers of genome sequence analysis,&nbsp;leveraging generative AI to enhance sensemaking offers intriguing possibilities‚Äã‚Äã. The task of staying&nbsp;‚Äã‚Äãcurrent‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã, synthesizing information from diverse sources, and making informed decisions&nbsp;‚Äã‚Äãis challenging‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã.&nbsp;&nbsp;



Our study participants emphasized the hurdles in integrating data from multiple sources without losing critical components, documenting decision rationales, and fostering collaborative environments. Generative AI models, with their advanced capabilities, have started to address these challenges by automatically generating interactive artifacts to support sensemaking. However, the effectiveness of such systems hinges on careful design considerations,&nbsp;‚Äã‚Äãparticularly in how they facilitate distributed sensemaking, support both initial and ongoing sensemaking, and combine evidence from multiple modalities. We next discuss three design considerations for using generative AI models to support sensemaking.



Distributed expert-AI sensemaking design



Generative AI models can create artifacts that aid an individual user&#8217;s sensemaking process; however, the true potential lies in sharing these artifacts among users to foster collective understanding and efficiency. Participants in our study emphasized the importance of explainability, feedback, and trust when interacting with AI-generated content.&nbsp;‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚ÄãTrust is gained by‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã&nbsp;viewing portions of artifacts marked as correct by other users, or observing edits made to AI-generated information‚Äã‚Äã.&nbsp;‚Äã‚ÄãSome‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã&nbsp;users‚Äã, however,‚Äã&nbsp;cautioned against over-reliance on AI, which could obscure underlying inaccuracies. Thus, design strategies should ensure that any corrections are clearly marked&nbsp;‚Äã‚Äãand annotated‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã. Furthermore, to enhance distributed sensemaking, visibility of others&#8217; notes and context-specific synthesis through AI can streamline the process‚Äã‚Äã.&nbsp;



Initial expert-AI sensemaking and re-sensemaking design



In our fast-paced, information-driven world,&nbsp;‚Äã‚Äãit is essential to understand a situation both&nbsp;initially&nbsp;and again when new information arises.‚Äã‚Äã&nbsp;‚Äã‚ÄãSensemaking is inherently temporal, reflecting and shaping our understanding of time as we revisit tasks to reevaluate past decisions or incorporate new information. Generative AI plays a pivotal role here by transforming static data into dynamic artifacts that evolve, offering a comprehensive view of past rationales. Such AI-generated artifacts provide continuity, allowing users‚Äîboth&nbsp;original decision-makers or new individuals‚Äîto access the rationale behind decisions made in earlier task instances. By continuously editing and updating these artifacts, generative AI highlights new information since the last review, supporting ongoing understanding and decision-making.&nbsp;Moreover, AI systems enhance&nbsp;‚Äã‚Äãtransparency‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã&nbsp;by summarizing previous notes and questions, offering insights into earlier thought processes and facilitating a deeper understanding of how conclusions were drawn. This reflective capability not only can reinforce initial sensemaking efforts but also equips users with the clarity needed for informed re-sensemaking as new data emerges.&nbsp;



Combining evidence from multiple modalities to enhance AI-expert sensemaking



‚Äã‚Äã‚ÄãThe‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã&nbsp;ability to combine evidence from multiple modalities is essential for effective sensemaking. Users often need to integrate diverse types of data‚Äîtext, images, spatial coordinates, and more‚Äîinto a coherent narrative to make informed decisions. Consider the case of search and rescue operations, where workers must rapidly synthesize information from texts, photographs, and GPS data to strategize their efforts. Recent advancements in multimodal generative AI models have empowered users by incorporating and synthesizing these varied inputs into a unified, comprehensive view. For instance, a participant in our study illustrated this capability by using a generative AI model to merge text from scientific publications with a visual gene structure depiction. This integration&nbsp;‚Äã‚Äãcould create‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã&nbsp;an image that contextualizes an individual&#8217;s genetic variant within the&nbsp;‚Äã‚Äãcontext‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã&nbsp;of documented variants. Such advanced synthesis enables users to capture complex relationships and insights briefly, streamlining decision-making and expanding the potential for innovative solutions across diverse fields.&nbsp;



Sensemaking Process with AI Assistant



Figure: Sensemaking process when interpreting variants with the introduction of prototype AI assistant. Gray boxes represent sensemaking activities which are currently performed by an analyst but are human-in-the-loop processes with involvement of our prototype AI assistant. Non-gray boxes represent activities reserved for analyst completion without assistance by our AI assistant prototype. Within the foraging searching and synthesizing processes, examples of data sources and data types for each, respectively, are connected by dotted lines.



Conclusion



We explored the potential of generative AI&nbsp;to support‚Äã‚Äã genetic professionals‚Äã&nbsp;‚Äãin diagnosing rare diseases‚Äã‚Äã. By designing an AI-based assistant, we aim to streamline whole genome sequencing analysis, helping professionals diagnose rare genetic diseases more efficiently. Our study unfolded in two key phases:&nbsp;‚Äãpinpointing‚Äã‚Äã‚Äã&nbsp;existing challenges in analysis, and design ideation, where we crafted a prototype AI assistant. This tool is designed to boost diagnostic yield and cut down diagnosis time by flagging cases for reanalysis and synthesizing crucial gene and variant data. Despite valuable findings, more research is needed‚Äã‚Äã. Future research will involve testing the AI assistant in real-time, task-based user testing with genetic professionals to assess the AI&#8217;s impact on their workflow. The promise of AI advancements lies in solving the right user problems and building the appropriate solutions, achieved through collaboration among model developers, domain experts, system designers, and HCI researchers. By fostering these collaborations, we aim to develop robust, personalized AI assistants tailored to specific domains.&nbsp;



Join the conversation



Join us as we continue to explore the transformative potential of generative AI in genetic analysis, and please read the full text publication&nbsp;here (opens in new tab). Follow us on social media, share this post with your network, and let us know your thoughts on how AI can transform genetic research. If interested in our other related research work, check out&nbsp;Evidence Aggregator: AI reasoning applied to rare disease diagnosis. (opens in new tab)&nbsp;&nbsp;




Opens in a new tabThe post Using AI to assist in rare disease diagnosis appeared first on Microsoft Research.
‚Ä¢ DoWhile loops now supported in Amazon Bedrock Flows
  Today, we are excited to announce support for DoWhile loops in Amazon Bedrock Flows. With this powerful new capability, you can create iterative, condition-based workflows directly within your Amazon Bedrock flows, using Prompt nodes, AWS Lambda functions, Amazon Bedrock Agents, Amazon Bedrock Flows inline code, Amazon Bedrock Knowledge Bases, Amazon Simple Storage Service (Amazon S3), and other Amazon Bedrock nodes within the loop structure. This feature avoids the need for complex workarounds, enabling sophisticated iteration patterns that use the full range of Amazon Bedrock Flows components. Tasks like content refinement, recursive analysis, and multi-step processing can now seamlessly integrate AI model calls, custom code execution, and knowledge retrieval in repeated cycles. By providing loop support with diverse node types, this feature simplifies generative AI application development and accelerates enterprise adoption of complex, adaptive AI solutions. 
Organizations using Amazon Bedrock Flows can now use DoWhile loops to design and deploy workflows for building more scalable and efficient generative AI applications fully within the Amazon Bedrock environment while achieving the following: 
 
 Iterative processing ‚Äì Execute repeated operations until specific conditions are met, enabling dynamic content refinement and recursive improvements 
 Conditional logic ‚Äì Implement sophisticated decision-making within flows based on AI outputs and business rules 
 Complex use cases ‚Äì Manage multi-step generative AI workflows that require repeated execution and refinement 
 Builder-friendly ‚Äì Create and manage loops through both the Amazon Bedrock API and AWS Management Console in the traces 
 Observability ‚Äì Employ seamless tracking of loop iterations, conditions, and execution paths 
 
In this post, we discuss the benefits of this new feature, and show how to use DoWhile loops in Amazon Bedrock Flows. 
Benefits of DoWhile loops in Amazon Bedrock Flows 
Using DoWhile loops in Amazon Bedrock Flows offers the following benefits: 
 
 Simplified flow control ‚Äì Create sophisticated iterative workflows without complex orchestration or external services 
 Flexible processing ‚Äì Enable dynamic, condition-based execution paths that can adapt based on AI outputs and business rules 
 Enhanced development experience ‚Äì Help users build complex iterative workflows through an intuitive interface, without requiring external workflow management 
 
Solution overview 
In the following sections, we show how to create a simple Amazon Bedrock flow using Do-while loops with Lambda functions. Our example showcases a practical application where we construct a flow that generates a blog post on a given topic in an iterative manner until certain acceptance criteria are fulfilled. The flow demonstrates the power of combining different types of Amazon Bedrock Flows nodes within a loop structure, where Prompt nodes generate and fine-tune the blog post, Inline Code nodes allow writing custom Python code to analyze the outputs, and S3 Storage nodes enable storing each version of the blog post during the process for reference. The DoWhile loop continues to execute until the quality of the blog post meets the condition set in the loop controller. This example illustrates how different flow nodes can work together within a loop to progressively transform data until desired conditions are met, providing a foundation for understanding more complex iterative workflows with various node combinations. 
Prerequisites 
Before implementing the new capabilities, make sure you have the following: 
 
 An AWS account 
 Other Amazon Bedrock services in place: 
   
   Create and test your base prompts for customer service interactions in Amazon Bedrock Prompt Management 
   Create guardrails with relevant rules using Amazon Bedrock Guardrails 
    
 Resources in auxiliary AWS services needed for your workflow, such as Lambda, Amazon DynamoDB, and Amazon S3 
 Required AWS Identity and Access Management (IAM) permissions: 
   
   Access to Amazon Bedrock Flows 
   Appropriate access to large language models (LLMs) in Amazon Bedrock 
    
 
After these components are in place, you can proceed with using Amazon Bedrock Flows with DoWhile loop capabilities in your generative AI use case. 
Create your flow using DoWhile Loop nodes 
Complete the following steps to create your flow: 
 
 On the Amazon Bedrock console, choose Flows under Builder tools in the navigation pane. 
 Create a new flow, for example, dowhile-loop-demo. For detailed instructions on creating a flow, see Amazon Bedrock Flows is now generally available with enhanced safety and traceability. 
 Add a DoWhile loop node. 
 Add additional nodes according to the solution workflow (discussed in the next section). 
 
Amazon Bedrock provides different node types to build your prompt flow. For this example, we use a DoWhile Loop node for calling different types of nodes for a generative AI-powered application, which creates a blog post on a given topic and checks the quality in every loop. There is one DoWhile Loop node in the flow. This new node type is on the Nodes tab in the left pane, as shown in the following screenshot. 
 
DoWhile loop workflow 
A DoWhile loop consists of two parts: the loop and the loop controller. The loop controller validates the logic for the loop and decides whether to continue or exit the loop. In this example, it is executing Prompt, Inline Code, S3 Storage nodes each time the loop is executed. 
Let‚Äôs go through this flow step-by-step, as illustrated in the preceding screenshot: 
 
 A user asks to write a blog post on a specific topic (for example, using the following prompt: {‚Äútopic‚Äù: ‚ÄúAWS Lambda‚Äù, ‚ÄúAudience‚Äù: ‚ÄúChief Technology Officer‚Äù, ‚Äúword_count‚Äù:‚Äù500}). This prompt is sent to the Prompt node (Content_Generator). 
 The Prompt node (Content_Generator) writes a blog post based on the prompt using one of the Amazon Bedrock provided LLMs (such as Amazon Nova or Anthropic‚Äôs Claude) and is sent to the Loop Input node. This is the entry point to the DoWhile Loop node. 
 Three steps happen in tandem: 
   
   The Loop Input node forwards the blog post content to another Prompt node (Blog_Analysis_Rating) for rating the post based on criteria mentioned as part of the prompt. The output of this Prompt node is JSON code like the following example. The output of a Prompt node is always of type String. You can modify the prompt to get different types of output according to your needs. However, you can also ask the LLM to output a single rating number. 
     
     {
  "overall_rating": 8.5,
  "category_ratings": {
    "clarity_and_readability": 9,
    "value_to_target_audience": 8,
    "engagement_level": 8,
    "technical_accuracy": 9
  } 
      
   The blog post is sent to the flow output during every iteration. This is the final version whenever the loop condition is not met (exiting the loop) or the end of maximum loop iterations. 
   At the same time, the output of the previous Prompt node (Content_Generator) is forwarded to another Prompt node (Blog_Refinement) by the Loop Input node. This node recreates or modifies the blog post based on the feedback from the analysis. 
    
 The output of the Prompt node (Blog_Analysis_Rating) is fed into the Inline Code node to extract the necessary rating and return that as a number or other information required for checking the condition inside the loop controller as input variables (for example, a rating). 
 
 
 def __func(variable):
 return float(variable["overall_rating"])
__func(variable) 
 
Python code inside the Inline Code must be treated as untrusted, and appropriate parsing, validation, and data handling should be implemented. 
 
 The output of the Inline Code node is fed into the loop condition inside the loop controller to validate against the condition we set up inside the continue loop. In this example, we are checking for a rating less than or equal to 9 for the generated blog post. You can check up to five conditions. Additionally, a maximum loop iterations parameter makes sure that loop doesn‚Äôt continue infinitely. 
 The step consists of two parts: 
   
   A Prompt node (Blog_Refinement) forwards the newly generated blog post to loopinput inside the loop controller. 
   The loop controller stores the version of the post in Amazon S3 for future reference and comparing the different versions generated. 
    
 This path will execute if one of the conditions is met inside the continue loop and maximum loop iterations. If this continues, then the new modified blog post from earlier is forwarded to the input field in the Loop Input node as LoopInput and the loop continues. 
 The final output is produced after the DoWhile loop condition is met or maximum number of iterations are completed. The output will be final version of the blog post. 
 
You can see the output as shown in the following screenshot. The system also provides access to node execution traces, offering detailed insights into each processing step, real-time performance metrics, and highlighting issues that may have occurred during the flow‚Äôs execution. Traces can be enabled using an API and sent to an Amazon CloudWatch log. In the API, set the enableTrace field to true in an InvokeFlow request. Each flowOutputEvent in the response is returned alongside a flowTraceEvent. 
 
You have now successfully created and executed an Amazon Bedrock flow using DoWhile Loop nodes. You can also use Amazon Bedrock APIs to programmatically execute this flow. For additional details on how to configure flows, see Amazon Bedrock Flows is now generally available with enhanced safety and traceability. 
Considerations 
When working with DoWhile Loop nodes in Amazon Bedrock Flows, the following are the important things to note: 
 
 DoWhile Loop nodes don‚Äôt support nested loops (loops within loops) 
 Each loop controller can evaluate up to five input conditions for its exit criteria 
 A maximum iteration limit must be specified to help prevent infinite loops and enable controlled execution 
 
Conclusion 
The integration of DoWhile loops in Amazon Bedrock Flows marks a significant advancement in iterative workflow capabilities, enabling sophisticated loop-based processing that can incorporate Prompt nodes, Inline Code nodes, S3 Storage nodes, Lambda functions, agents, DoWhile Loop nodes, and Knowledge Base nodes. This enhancement responds directly to enterprise customers‚Äô needs for handling complex, repetitive tasks within their AI workflows, helping developers create adaptive, condition-based solutions without requiring external orchestration tools. By providing support for iterative processing patterns, DoWhile loops help organizations build more sophisticated AI applications that can refine outputs, perform recursive operations, and implement complex business logic directly within the Amazon Bedrock environment. This powerful addition to Amazon Bedrock Flows democratizes the development of advanced AI workflows, making iterative AI processing more accessible and manageable across organizations. 
DoWhile loops in Amazon Bedrock Flows are now available in all the AWS Regions where Amazon Bedrock Flows is supported, except for the AWS Gov Cloud (US) Region. To get started, open the Amazon Bedrock console or Amazon Bedrock APIs to begin building flows with Amazon Bedrock Flows. To learn more, refer to Create your first flow in Amazon Bedrock and Track each step in your flow by viewing its trace in Amazon Bedrock. 
We‚Äôre excited to see the innovative applications you will build with these new capabilities. As always, we welcome your feedback through AWS re:Post for Amazon Bedrock or your usual AWS contacts. Join the generative AI builder community at community.aws to share your experiences and learn from others. 
 
About the authors 
Shubhankar Sumar is a Senior Solutions Architect at AWS, where he specializes in architecting generative AI-powered solutions for enterprise software and SaaS companies across the UK. With a strong background in software engineering, Shubhankar excels at designing secure, scalable, and cost-effective multi-tenant systems on the cloud. His expertise lies in seamlessly integrating cutting-edge generative AI capabilities into existing SaaS applications, helping customers stay at the forefront of technological innovation. 
Jesse Manders is a Senior Product Manager on Amazon Bedrock, the AWS Generative AI developer service. He works at the intersection of AI and human interaction with the goal of creating and improving generative AI products and services to meet our needs. Previously, Jesse held engineering team leadership roles at Apple and Lumileds, and was a senior scientist in a Silicon Valley startup. He has an M.S. and Ph.D. from the University of Florida, and an MBA from the University of California, Berkeley, Haas School of Business. 
Eric Li is a Software Development Engineer II at AWS, where he builds core capabilities for Amazon Bedrock and SageMaker to support generative AI applications at scale. His work focuses on designing secure, observable, and cost-efficient systems that help developers and enterprises adopt generative AI with confidence. He is passionate about advancing developer experiences for building with large language models, making it easier to integrate AI into production-ready cloud applications.
‚Ä¢ How PropHero built an intelligent property investment advisor with continuous evaluation using Amazon Bedrock
  This post was written with Lucas Dahan, Dil Dolkun, and Mathew Ng from PropHero. 
PropHero is a leading property wealth management service that democratizes access to intelligent property investment advice through big data, AI, and machine learning (ML). For the Spanish and Australian consumer base, PropHero needed an AI-powered advisory system that could engage customers in accurate property investment discussions. The goal was to provide personalized investment insights and to guide and assist users at every stage of their investment journey: from understanding the process, gaining visibility into timelines, securely uploading documents, to tracking progress in real time. 
PropHero collaborated with the AWS Generative AI Innovation Center to implement an intelligent property investment advisor using AWS generative AI services with continuous evaluation. The solution helps users engage in natural language conversations about property investment strategies and receive personalized recommendations based on PropHero‚Äôs comprehensive market knowledge. 
In this post, we explore how we built a multi-agent conversational AI system using Amazon Bedrock that delivers knowledge-grounded property investment advice. We explore the agent architecture, model selection strategy, and comprehensive continuous evaluation system that facilitates quality conversations while facilitating rapid iteration and improvement. 
The challenge: Making property investment knowledge more accessible 
The area of property investment presents numerous challenges for both novice and experienced investors. Information asymmetry creates barriers where comprehensive market data remains expensive or inaccessible. Traditional investment processes are manual, time-consuming, and require extensive market knowledge to navigate effectively. For the Spanish and Australian consumers specifically, we needed to build a solution that could provide accurate, contextually relevant property investment advice in Spanish while handling complex, multi-turn conversations about investment strategies. The system needed to maintain high accuracy while delivering responses at scale, continuously learning and improving from customer interactions. Most importantly, it needed to assist users across every phase of their journey, from initial onboarding through to final settlement, ensuring comprehensive support throughout the entire investment process. 
Solution overview 
We built a complete end-to-end solution using AWS generative AI services, architected around a multi-agent AI advisor with integrated continuous evaluation. The system provides seamless data flow from ingestion through intelligent advisory conversations with real-time quality monitoring. The following diagram illustrates this architecture. 
 
The solution architecture consists of four virtual layers, each serving specific functions in the overall system design. 
Data foundation layer 
The data foundation provides the storage and retrieval infrastructure for system components: 
 
 Amazon DynamoDB ‚Äì Fast storage for conversation history, evaluation metrics, and user interaction data 
 Amazon Relational Database (Amazon RDS) for PostgreSQL ‚Äì A PostgreSQL database storing LangFuse observability data, including large language model (LLM) traces and latency metrics 
 Amazon Simple Storage Service (Amazon S3) ‚Äì A central data lake storing Spanish FAQ documents, property investment guides, and conversation datasets 
 
Multi-agent AI layer 
The AI processing layer encompasses the core intelligence components that power the conversational experience: 
 
 Amazon Bedrock ‚Äì Foundation models (FMs) such as LLMs and rerankers powering specialized agents 
 Amazon Bedrock Knowledge Bases ‚Äì Semantic search engine with semantic chunking for FAQ-style content 
 LangGraph ‚Äì Orchestration of multi-agent workflows and conversation state management 
 AWS Lambda ‚Äì Serverless functions executing multi-agent logic and retrival of user information for richer context 
 
Continuous evaluation layer 
The evaluation infrastructure facilitates continuous quality monitoring and improvement through these components: 
 
 Amazon CloudWatch ‚Äì Real-time monitoring of quality metrics with automated alerting and threshold management 
 Amazon EventBridge ‚Äì Real-time event triggers for conversation completion and quality assessment 
 AWS Lambda ‚Äì Automated evaluation functions measuring context relevance, response groundedness, and goal accuracy 
 Amazon QuickSight ‚Äì Interactive dashboards and analytics for monitoring the respective metrics 
 
Application and integration layer 
The integration layer provides secure interfaces for external communication: 
 
 Amazon API Gateway ‚Äì Secure API endpoints for conversational interface and evaluation webhooks 
 
Multi-agent AI advisor architecture 
The intelligent advisor uses a multi-agent system orchestrated through LangGraph, which sits in a single Lambda function, where each agent is optimized for specific tasks. The following diagram shows the communication flow among the various agents within the Lambda function. 
 
Agent composition and model selection 
Our model selection strategy involved extensive testing to match each component‚Äôs computational requirements with the most cost-effective Amazon Bedrock model. We evaluated factors including response quality, latency requirements, and cost per token to determine optimal model assignments for each agent type.Each component in the system uses the most appropriate model for its designated function, as outlined in the following table. 
 
  
   
   Component 
   Amazon Bedrock Model 
   Purpose 
   
   
   Router Agent 
   Anthropic Claude 3.5 Haiku 
   Query classification and routing 
   
   
   General Agent 
   Amazon Nova Lite 
   Common questions and conversation management 
   
   
   Advisor Agent 
   Amazon Nova Pro 
   Specialized property investment advice 
   
   
   Settlement agent 
   Anthropic Claude 3.5 Haiku 
   Customer support specialising on pre-settlement phase of investment 
   
   
   Response Agent 
   Amazon Nova Lite 
   Final response generation and formatting 
   
   
   Embedding 
   Cohere Embed Multilingual v3 
   Context retrieval 
   
   
   Retriever 
   Cohere Rerank 3.5 
   Context retrieval and ranking 
   
   
   Evaluator 
   Anthropic Claude 3.5 Haiku 
   Quality assessment and scoring 
   
  
 
End-to-end conversation flow 
The conversation processing follows a structured workflow that facilitates accurate responses while maintaining quality standards: 
 
 User queries enter through API Gateway and are routed to the router agent. 
 The router agent determines the appropriate specialized agent based on query analysis. 
 User information is retrieved at the start for richer context and knowledge-intensive queries trigger the retriever to access the Amazon Bedrock knowledge base. 
 Specialized agents process queries with retrieved user information and relevant context from the knowledge base. 
 The response agent formats and generates the final user-facing response with the appropriate tone. 
 Parallel evaluation processes assess context relevance, response groundedness, and goal accuracy. 
 Conversation data is stored in DynamoDB for analysis and improvement. 
 
Amazon Bedrock Knowledge Bases configuration 
The knowledge base uses several AWS services for optimal performance: 
 
 Data source ‚Äì S3 bucket 
 Chunking strategy ‚Äì Semantic chunking optimized for FAQ-style content 
 Embedding model ‚Äì Cohere Embed Multilingual v3 for Spanish understanding 
 Reranker ‚Äì Cohere Rerank 3.5 for reranking retrieved Spanish content 
 Vector database ‚Äì Amazon OpenSearch Serverless for scalable semantic search 
 
Sample conversation experience 
The system handles natural Spanish conversations seamlessly. For example: 
Usuario: ‚ÄúHola, ¬øqu√© es PropHero exactamente?‚Äù 
Asistente: ‚Äú¬°Hola! PropHero es una plataforma que te permite crear, optimizar y crecer tu patrimonio inmobiliario delegando todo el proceso en nuestro equipo de profesionales‚Ä¶‚Äù 
Usuario: ‚Äú¬øY c√≥mo funciona en la pr√°ctica?‚Äù 
Asistente: ‚ÄúEs muy sencillo. Comenzamos con una sesi√≥n con tu Property Coach para definir tu estrategia de inversi√≥n personalizada‚Ä¶‚Äù 
Integrated continuous evaluation system 
The continuous evaluation system is integrated into the architecture, operating as a core component. This approach supports quality monitoring in real-time alongside conversation processing. 
Evaluation metrics 
We used the following metrics from the Ragas library for evaluation: 
 
 Context Relevance (0‚Äì1) ‚Äì Measures the relevance of retrieved context to user queries, evaluating RAG system effectiveness 
 Response Groundedness (0‚Äì1) ‚Äì Makes sure responses are factually accurate and derived from PropHero‚Äôs official information 
 Agent Goal Accuracy (0‚Äì1) ‚Äì Binary measure of whether responses successfully address user investment goals 
 
Real-time evaluation workflow 
The evaluation system operates seamlessly within the conversation architecture: 
 
 Amazon DynamoDB Streams triggers ‚Äì Conversation data written to DynamoDB automatically triggers a Lambda function for evaluation through Amazon DynamoDB Streams 
 Parallel processing ‚Äì Lambda functions execute evaluation logic in parallel with response delivery 
 Multi-dimensional assessment ‚Äì Each conversation is evaluated across three key dimensions simultaneously 
 Intelligent scoring with LLM-as-a-judge ‚Äì Anthropic‚Äôs Claude 3.5 Haiku provides consistent evaluation as an LLM judge, offering standardized assessment criteria across conversations. 
 Monitoring and analytics ‚Äì CloudWatch captures metrics from the evaluation process, and QuickSight provides dashboards for trend analysis 
 
The following diagram provides an overview of the Lambda function responsible for continuous evaluation. 
 
Implementation insights and best practices 
Our development journey involved a 6-week iterative process with PropHero‚Äôs technical team. We conducted testing across different model combinations and evaluated chunking strategies using real customer FAQ data. This journey revealed several architectural optimizations that enhanced system performance, achieved significant cost reductions, and improved user experience. 
Model selection strategy 
Our approach to model selection demonstrates the importance of matching model capabilities to specific tasks. By using Amazon Nova Lite for simpler tasks and Amazon Nova Pro for complex reasoning, the solution achieves optimal cost-performance balance while maintaining high accuracy standards. 
Chunking and retrieval optimization 
Semantic chunking proved superior to hierarchical and fixed chunking approaches for FAQ-style content. The Cohere Rerank 3.5 model enabled the system to use fewer chunks (10 vs. 20) while maintaining accuracy, reducing latency and cost. 
Multilingual capabilities 
The system effectively handles Spanish and English queries by using FMs that support Spanish language on Amazon Bedrock. 
Business impact 
The PropHero AI advisor delivered measurable business value: 
 
 Enhanced customer engagement ‚Äì A 90% goal accuracy rate makes sure customers receive relevant, actionable property investment advice. Over 50% of our users (and over 70% of paid users) are actively using the AI advisor. 
 Operational efficiency ‚Äì Automated responses to common questions reduced customer service workload by 30%, freeing staff to focus on complex customer needs. 
 Scalable growth ‚Äì The serverless architecture automatically scales to handle increasing customer demand without manual intervention. 
 Cost optimization ‚Äì Strategic model selection achieved high performance while reducing AI costs by 60% compared to using premium models throughout. 
 Consumer base expansion ‚Äì Successful Spanish language support enabled PropHero‚Äôs expansion into the Spanish consumer base with localized expertise. 
 
Conclusion 
The PropHero AI advisor demonstrates how AWS generative AI services can be used to create intelligent, context-aware conversational agents that deliver real business value. By combining a modular agent architecture with a robust evaluation system, PropHero has created a solution that enhances customer engagement while providing accurate and relevant responses.The comprehensive evaluation pipeline has been particularly valuable, providing clear metrics for measuring conversation quality and guiding ongoing improvements. This approach makes sure the AI advisor will continue to evolve and improve over time.For more information about building multi-agent AI advisors with continuous evaluation, refer to the following resources: 
 
 Retrieve data and generate AI responses with Amazon Bedrock Knowledge Bases ‚Äì With Amazon Bedrock Knowledge Bases, you can implement semantic search with chunking strategies 
 LangGraph ‚Äì LangGraph can help you build multi-agent workflows 
 Ragas ‚Äì Ragas offers comprehensive LLM evaluation metrics, including context relevance, groundedness, and goal accuracy used in this implementation 
 
To learn more about the Generative AI Innovation Center, get in touch with your account team. 
 
About the authors 
Adithya Suresh is a Deep Learning Architect at the AWS Generative AI Innovation Center based in Sydney, where he collaborates directly with enterprise customers to design and scale transformational generative AI solutions for complex business challenges. He uses AWS generative AI services to build bespoke AI systems that drive measurable business value across diverse industries. 
Lucas Dahan was the Head of Data &amp; AI at PropHero at the time of writing. He leads the technology team that is transforming property investment through innovative digital solutions. 
 Dil Dolkun is the Data &amp; AI Engineer at PropHero‚Äôs tech team, and has been instrumental in designing data architectures and multi-agent workflows for PropHero‚Äôs generative AI property investment Advisor system. 
Mathew Ng is a Technical Lead at PropHero, who architected and scaled PropHero‚Äôs cloud-native, high-performance software solution from early stage start up to successful Series A funding. 
Aaron Su is a Solutions Architect at AWS, with a focus across AI and SaaS startups. He helps early-stage companies architect scalable, secure, and cost-effective cloud solutions.
‚Ä¢ Accelerate benefits claims processing with Amazon Bedrock Data Automation
  In the benefits administration industry, claims processing is a vital operational pillar that makes sure employees and beneficiaries receive timely benefits, such as health, dental, or disability payments, while controlling costs and adhering to regulations like HIPAA and ERISA. Businesses aim to optimize the workflow‚Äîcovering claim submission, validation, adjudication, payment, and appeals‚Äîto enhance employee satisfaction, strengthen provider relationships, and mitigate financial risks. The process includes specific steps like claim submission (through portals or paper), data validation (verifying eligibility and accuracy), adjudication (assessing coverage against plan rules), payment or denial (including check processing for reimbursements), and appeal handling. Efficient claims processing supports competitive benefits offerings, which is crucial for talent retention and employer branding, but requires balancing speed, accuracy, and cost in a highly regulated environment. 
Despite its importance, claims processing faces significant challenges in many organizations. Most notably, the reliance on legacy systems and manual processes results in frustratingly slow resolution times, high error rates, and increased administrative costs. Incomplete or inaccurate claim submissions‚Äîsuch as those with missing diagnosis codes or eligibility mismatches‚Äîfrequently lead to denials and rework, creating frustration for both employees and healthcare providers. Additionally, fraud, waste, and abuse continue to inflate costs, yet detecting these issues without delaying legitimate claims remains challenging. Complex regulatory requirements demand constant system updates, and poor integration between systems‚Äîsuch as Human Resource Information Systems (HRIS) and other downstream systems‚Äîseverely limits scalability. These issues drive up operational expenses, erode trust in benefits programs, and overburden customer service teams, particularly during appeals processes or peak claims periods. 
Generative AI can help address these challenges. With Amazon Bedrock Data Automation, you can automate generation of useful insights from unstructured multimodal content such as documents, images, audio, and video. Amazon Bedrock Data Automation can be used in benefits claims process to automate document processing by extracting and classifying documents from claims packets, policy applications, and supporting documents with industry-leading accuracy, reducing manual errors and accelerating resolution times. Amazon Bedrock Data Automation natural language processing capabilities interpret unstructured data, such as provider notes, supporting compliance with plan rules and regulations. By automating repetitive tasks and providing insights, Amazon Bedrock Data Automation helps reduce administrative burdens, enhance experiences for both employees and providers, and support compliance in a cost-effective manner. Furthermore, its scalable architecture enables seamless integration with existing systems, improving data flow across HRIS, claims systems, and provider networks, and advanced analytics help detect fraud patterns to optimize cost control. 
In this post, we examine the typical benefit claims processing workflow and identify where generative AI-powered automation can deliver the greatest impact. 
Benefit claims processing 
When an employee or beneficiary pays out of pocket for an expense covered under their health benefits, they submit a claim for reimbursement. This process requires several supporting documents, including doctor‚Äôs prescriptions and proof of payment, which might include check images, receipts, or electronic payment confirmations. 
The claims processing workflow involves several critical steps: 
 
 Document intake and processing ‚Äì The system receives and categorizes submitted documentation, including: 
   
   Medical records and prescriptions 
   Proof of payment documentation 
   Supporting forms and eligibility verification 
    
 Payment verification processing ‚Äì For check-based reimbursements, the system must complete the following steps: 
   
   Extract information from check images, including the account number and routing number contained in the MICR line 
   Verify payee and payer names against the information provided during the claim submission process 
   Confirm payment amounts match the claimed expenses 
   Flag discrepancies for human review 
    
 Adjudication and reimbursement ‚Äì When verification is complete, the system performs several actions: 
   
   Determine eligibility based on plan rules and coverage limits 
   Calculate appropriate reimbursement amounts 
   Initiate payment processing through direct deposit or check issuance 
   Provide notification to the claimant regarding the status of their reimbursement 
    
 
In this post, we walk through a real-world scenario to make the complexity of this multi-step process clearer. The following example demonstrates how Amazon Bedrock Data Automation can streamline the claims processing workflow, from initial submission to final reimbursement. 
Solution overview 
Let‚Äôs consider a scenario where a benefit plan participant seeks treatment and pays out of pocket for the doctor‚Äôs fee using a check. They then buy the medications prescribed by the doctor at the pharmacy store. Later, they log in to their benefit provider‚Äôs portal and submit a claim along with the image of the check and payment receipt for the medications. 
This solution uses Amazon Bedrock Data Automation to automate the two most critical and time-consuming aspects of this workflow: document intake and payment verification processing. The following diagram illustrates the benefits claims processing architecture. 
 
The end-to-end process works through four integrated stages: ingestion, extraction, validation, and integration. 
Ingestion 
When a beneficiary uploads supporting documents (check image and pharmacy receipt) through the company‚Äôs benefit claims portal, these documents are securely saved in an Amazon Simple Storage Service (Amazon S3) bucket, triggering the automated claims processing pipeline. 
Extraction 
After documents are ingested, the system immediately begins with intelligent data extraction: 
 
 The S3 object upload triggers an AWS Lambda function, which invokes the Amazon Bedrock Data Automation project. 
 Amazon Bedrock Data Automation uses blueprints for file processing and extraction. Blueprints are artifacts used to configure file processing business logic by specifying a list of field names for data extraction, along with their desired data formats (string, number, or Boolean) and natural language context for data normalization and validation rules. Amazon Bedrock Data Automation provides a catalog of sample blueprints out of the box. You can create a custom blueprint for your unique document types that aren‚Äôt predefined in the catalog. This solution uses two blueprints designed for different document types, as shown in the following screenshot: 
   
   The catalog blueprint US-Bank-Check for check processing. 
   The custom blueprint benefit-claims-pharmacy-receipt-blueprint for pharmacy-specific receipts. 
    
 
 
US-Bank-Check is a catalog blueprint provided out of the box by Amazon Bedrock Data Automation. The custom blueprint benefit-claims-pharmacy-receipt-blueprint is created using an AWS CloudFormation template to handle pharmacy receipt processing, addressing a specific document type that wasn‚Äôt available in the standard blueprint catalog. The benefit administrator wants to look for vendor-specific information such as name, address, and phone details for benefits claims processing. The custom blueprint schema contains natural language explanation of those fields, such as VendorName, VendorAddress, VendorPhone, and additional fields, explaining what the field represents, expected data types, and inference type for each extracted field (explained in Creating Blueprints for Extraction), as shown in the following screenshot. 
 
3. The two blueprints are added to the Amazon Bedrock Data Automation project. An Amazon Bedrock Data Automation project is a grouping of both standard and custom blueprints that you can use to process different types of files (like documents, audio, and images) using specific configuration settings, where you can control what kind of information you want to extract from each file type. When the project is invoked asynchronously, it automatically applies the appropriate blueprint, extracts information such as confidence scores and bounding box details for each field, and saves results in a separate S3 bucket. This intelligent classification alleviates the need for you to write complex document classification logic. 
The following screenshot illustrates the document classification by the standard catalog blueprint US-Bank-Check. 
 
The following screenshot shows the document classification by the custom blueprint benefit-claims-pharmacy-receipt-blueprint. 
 
Validation 
With the data extracted, the system moves to the validation and decision-making process using the business rules specific to each document type. 
The business rules are documented in standard operating procedure documents (AnyCompany Benefit Checks Standard Operating procedure.docx and AnyCompany Benefit Claims Standard Operating procedure.docx) and uploaded to an S3 bucket. Then the system creates a knowledge base for Amazon Bedrock with the S3 bucket as the source, as shown in the following screenshot. 
 
When the extracted Amazon Bedrock Data Automation results are saved to the configured S3 bucket, a Lambda function is triggered automatically. Based on the business rules retrieved from the knowledge base for the specific document type and the extracted Amazon Bedrock Data Automation output, an Amazon Nova Lite large langue model (LLM) makes the automated approve/deny decision for claims. 
The following screenshot shows the benefit claim adjudication automated decision for US-Bank-Check. 
 
The following screenshot shows the benefit claim adjudication automated decision for benefit-claims-pharmacy-receipt-blueprint. 
 
Integration 
The system seamlessly integrates with existing business processes. 
When validation is complete, an event is pushed to Amazon EventBridge, which triggers a Lambda function for downstream integration. In this implementation, we use an Amazon DynamoDB table and Amazon Simple Notification Service (Amazon SNS) email for downstream integration. A DynamoDB table is created as part of the deployment stack, which is used to populate details including document classification, extracted data, and automated decision. An email notification is sent for both check and receipts after the final decision is made by the system. The following screenshot shows an example email for pharmacy receipt approval. 
 
This flexible architecture helps you integrate with your existing applications through internal APIs or events to update claim status or trigger additional workflows when validation fails. 
Reducing manual effort through intelligent business rules management 
Beyond automating document processing, this solution addresses a common operational challenge: Traditionally, customers must write and maintain code for handling business rules around claims adjudication and processing. Every business rule change requires development effort and code updates, slowing time-to-market and increasing maintenance overhead. 
Our approach converts business rules and standard operating procedures (SOPs) into knowledge bases using Amazon Bedrock Knowledge Bases, which you can use for automated decision-making. This approach can dramatically reduce time-to-market when business rules change, because updates can be made through knowledge management rather than code deployment. 
In the following sections, we walk you through the steps to deploy the solution to your own AWS account. 
Prerequisites 
To implement the solution provided in this post, you must have the following: 
 
 An AWS account 
 Access to Amazon Titan Text Embeddings V2 and Amazon Nova Lite foundation models (FMs) enabled in Amazon Bedrock 
 
This solution uses Python 3.13 with Boto3 1.38. or later version, and the AWS Serverless Application Model Command Line Interface (AWS SAM CLI) version 1.138.0. We assume that you have installed these in your local machine already. If not, refer to the following instructions: 
 
 Python 3.13 installation 
 Install the AWS SAM CLI 
 
Set up code in your local machine 
To set up the code, clone the GitHub repository. After you have cloned the repository to your local machine, the project folder structure will look like the following code, as mentioned in the README file: 
 
Deploy the solution in your account 
The sample code comes with a CloudFormation template that creates necessary resources. To deploy the solution in your account, follow the deployment instructions in the README file. 
Clean up 
Deploying this solution in your account will incur costs. Follow the cleanup instructions in the README file to avoid charges when you are done. 
Conclusion 
Benefits administration companies can significantly enhance their operations by automating claims processing using the solution outlined in this post. This strategic approach directly addresses the industry‚Äôs core challenges and can deliver several key advantages: 
 
 Enhanced processing efficiency through accelerated claims resolution times, reduced manual error rates, and higher straight-through processing rates that minimize the frustrating delays and manual rework plaguing legacy systems 
 Streamlined document integration and fraud detection capabilities, where adding new supporting documents becomes seamless through new Amazon Bedrock Data Automation blueprints, while AI-powered analytics identify suspicious patterns without delaying legitimate claims, avoiding traditional months-long development cycles and reducing costly fraud, waste, and abuse 
 Agile business rule management that enables rapid adaptation to changing HIPAA and ERISA requirements and modification of business rules, significantly reducing administrative costs and time-to-market while improving scalability and integration with existing HRIS and claims, ultimately enhancing employee satisfaction, strengthening provider relationships, and supporting competitive benefits offerings that are crucial for talent retention and employer branding 
 
To get started with this solution, refer to the GitHub repo. For more information about Amazon Bedrock Data Automation, refer to Transform unstructured data into meaningful insights using Amazon Bedrock Data Automation and try the Document Processing Using Amazon Bedrock Data Automation workshop. 
 
About the authors 
Saurabh Kumar is a Senior Solutions Architect at AWS based out of Raleigh, NC, with expertise in Resilience Engineering, Chaos Engineering, and Generative AI solutions. He advises customers on fault-tolerance strategies and generative AI-driven modernization approaches, helping organizations build robust architectures while leveraging generative AI technologies to drive innovation. 
Kiran Lakkireddy is a Principal Solutions Architect at AWS with expertise in Financial Services, Benefits Management and HR Services industries. Kiran provides technology and architecture guidance to customers in their business transformation, with a specialized focus on GenAI security, compliance, and governance. He regularly speaks to customer security leadership on GenAI security, compliance, and governance topics, helping organizations navigate the complex landscape of AI implementation while maintaining robust security standards. 
Tamilmanam Sambasivam is a Solutions Architect and AI/ML Specialist at AWS. She helps enterprise customers to solve their business problems by recommending the right AWS solutions. Her strong back ground in Information Technology (24+ years of experience) helps customers to strategize, develop and modernize their business problems in AWS cloud. In the spare time, Tamil like to travel and gardening.
‚Ä¢ Running deep research AI agents on Amazon Bedrock AgentCore
  AI agents are evolving beyond basic single-task helpers into more powerful systems that can plan, critique, and collaborate with other agents to solve complex problems. Deep Agents‚Äîa recently introduced framework built on LangGraph‚Äîbring these capabilities to life, enabling multi-agent workflows that mirror real-world team dynamics. The challenge, however, is not just building such agents but also running them reliably and securely in production. This is where Amazon Bedrock AgentCore Runtime comes in. By providing a secure, serverless environment purpose-built for AI agents and tools, Runtime makes it possible to deploy Deep Agents at enterprise scale without the heavy lifting of managing infrastructure. 
In this post, we demonstrate how to deploy Deep Agents on AgentCore Runtime. As shown in the following figure, AgentCore Runtime scales any agent and provides session isolation by allocating a new microVM for each new session. 
 
What is Amazon Bedrock AgentCore? 
Amazon Bedrock AgentCore is both framework-agnostic and model-agnostic, giving you the flexibility to deploy and operate advanced AI agents securely and at scale. Whether you‚Äôre building with Strands Agents, CrewAI, LangGraph, LlamaIndex, or another framework‚Äîand running them on a large language model (LLM)‚ÄîAgentCore provides the infrastructure to support them. Its modular services are purpose-built for dynamic agent workloads, with tools to extend agent capabilities and controls required for production use. By alleviating the undifferentiated heavy lifting of building and managing specialized agent infrastructure, AgentCore lets you bring your preferred framework and model and deploy without rewriting code. 
Amazon Bedrock AgentCore offers a comprehensive suite of capabilities designed to transform local agent prototypes into production-ready systems. These include persistent memory for maintaining context in and across conversations, access to existing APIs using Model Context Protocol (MCP), seamless integration with corporate authentication systems, specialized tools for web browsing and code execution, and deep observability into agent reasoning processes. In this post, we focus specifically on the AgentCore Runtime component. 
Core capabilities of AgentCore Runtime 
AgentCore Runtime provides a serverless, secure hosting environment specifically designed for agentic workloads. It packages code into a lightweight container with a simple, consistent interface, making it equally well-suited for running agents, tools, MCP servers, or other workloads that benefit from seamless scaling and integrated identity management.AgentCore Runtime offers extended execution times up to 8 hours for complex reasoning tasks, handles large payloads for multimodal content, and implements consumption-based pricing that charges only during active processing‚Äînot while waiting for LLM or tool responses. Each user session runs in complete isolation within dedicated micro virtual machines (microVMs), maintaining security and helping to prevent cross-session contamination between agent interactions. The runtime works with many frameworks (for example: LangGraph, CrewAI, Strands, and so on) and many foundation model providers, while providing built-in corporate authentication, specialized agent observability, and unified access to the broader AgentCore environment through a single SDK. 
Real-world example: Deep Agents integration 
In this post we‚Äôre going to deploy the recently released Deep Agents implementation example on AgentCore Runtime‚Äîshowing just how little effort it takes to get the latest agent innovations up and running. 
 
The sample implementation in the preceding diagram includes: 
 
 A research agent that conducts deep internet searches using the Tavily API 
 A critique agent that reviews and provides feedback on generated reports 
 A main orchestrator that manages the workflow and handles file operations 
 
Deep Agents uses LangGraph‚Äôs state management to create a multi-agent system with: 
 
 Built-in task planning through a write_todos tool that helps agents break down complex requests 
 Virtual file system where agents can read/write files to maintain context across interactions 
 Sub-agent architecture allowing specialized agents to be invoked for specific tasks while maintaining context isolation 
 Recursive reasoning with high recursion limits (more than 1,000) to handle complex, multi-step workflows 
 
This architecture enables Deep Agents to handle research tasks that require multiple rounds of information gathering, synthesis, and refinement.The key integration points in our code showcase how agents work with AgentCore. The beauty is in its simplicity‚Äîwe only need to add a couple of lines of code to make an agent AgentCore-compatible: 
 
 # 1. Import the AgentCore runtime
from bedrock_agentcore.runtime import BedrockAgentCoreApp
app = BedrockAgentCoreApp()

# 2. Decorate your agent function with @app.entrypoint
@app.entrypoint
async def langgraph_bedrock(payload):
    # Your existing agent logic remains unchanged
    user_input = payload.get("prompt")
    
    # Call your agent as before
    stream = agent.astream(
        {"messages": [HumanMessage(content=user_input)]},
        stream_mode="values"
    )
    
    # Stream responses back
    async for chunk in stream:
        yield(chunk)

# 3. Add the runtime starter at the bottom
if __name__ == "__main__":
    app.run() 
 
That‚Äôs it! The rest of the code‚Äîmodel initialization, API integrations, and agent logic‚Äîremains exactly as it was. AgentCore handles the infrastructure while your agent handles the intelligence. This integration pattern works for most Python agent frameworks, making AgentCore truly framework-agnostic. 
Deploying to AgentCore Runtime: Step-by-step 
Let‚Äôs walk through the actual deployment process using the AgentCore Starter ToolKit, which dramatically simplifies the deployment workflow. 
Prerequisites 
Before you begin, make sure you have: 
 
 Python 3.10 or higher 
 AWS credentials configured 
 Amazon Bedrock AgentCore SDK installed 
 
Step 1: IAM permissions 
There are two different AWS Identity and Access Management (IAM) permissions you need to consider when deploying an agent in an AgentCore Runtime‚Äîthe role you, as a developer use to create AgentCore resources and the execution role that an agent needs to run in an AgentCore Runtime. While the latter role can now be auto-created by the AgentCore Starter Toolkit (auto_create_execution_role=True), the former must be defined as described in IAM Permissions for AgentCore Runtime. 
Step 2: Add a wrapper to your agent 
As shown in the preceding Deep Agents example, add the AgentCore imports and decorator to your existing agent code. 
Step 3: Deploy using the AgentCore starter toolkit 
The starter toolkit provides a three-step deployment process: 
 
 from bedrock_agentcore_starter_toolkit import Runtime

# Step 1: Configure
agentcore_runtime = Runtime()
config_response = agentcore_runtime.configure(
    entrypoint="hello.py", # contains the code we showed earlier in the post
    execution_role=role_arn, # or auto-create
    auto_create_ecr=True,
    requirements_file="requirements.txt",
    region="us-west-2",
    agent_name="deepagents-research"
)

# Step 2: Launch
launch_result = agentcore_runtime.launch()
print(f"Agent deployed! ARN: {launch_result['agent_arn']}")

# Step 3: Invoke
response = agentcore_runtime.invoke({
    "prompt": "Research the latest developments in quantum computing"
}) 
 
Step 4: What happens behind the scenes 
When you run the deployment, the starter kit automatically: 
 
 Generates an optimized Docker file with Python 3.13-slim base image and OpenTelemetry instrumentation 
 Builds your container with the dependencies from requirements.txt 
 Creates an Amazon Elastic Container Registry (Amazon ECR) repository (if auto_create_ecr=True) and pushes your image 
 Deploys to AgentCore Runtime and monitors the deployment status 
 Configures networking and observability with Amazon CloudWatch and AWS X-Ray integration 
 
The entire process typically takes 2‚Äì3 minutes, after which your agent is ready to handle requests at scale. Each new session is launched in its own fresh AgentCore Runtime microVM, maintaining complete environment isolation. 
The starter kit generates a configuration file (.bedrock_agentcore.yaml) that captures your deployment settings, making it straightforward to redeploy or update your agent later. 
Invoking your deployed agent 
After deployment, you have two options for invoking your agent: 
Option 1: Using the start kit (shown in Step 3) 
 
 response = agentcore_runtime.invoke({
    "prompt": "Research the latest developments in quantum computing"
})
 
 
Option 2: Using boto3 SDK directly 
 
 import boto3
import json

agentcore_client = boto3.client('bedrock-agentcore', region_name='us-west-2')
response = agentcore_client.invoke_agent_runtime(
    agentRuntimeArn=agent_arn,
    qualifier="DEFAULT",
    payload=json.dumps({
        "prompt": "Analyze the impact of AI on healthcare in 2024"
    })
)

# Handle streaming response
for event in response['completion']:
    if 'chunk' in event:
        print(event['chunk']['bytes'].decode('utf-8')) 
 
Deep Agents in action 
As the code executes in Bedrock AgentCore Runtime, the primary agent orchestrates specialized sub-agents‚Äîeach with its own purpose, prompt, and tool access‚Äîto solve complex tasks more effectively. In this case, the orchestrator prompt (research_instructions) sets the plan: 
 
 Write the question to question.txt 
 Fan out to one or more research-agent calls (each on a single sub-topic) using the internet_search tool 
 Synthesize findings into final_report.md 
 Call critique-agent to evaluate gaps and structure 
 Optionally loop back to more research/edits until quality is met 
 
Here it is in action: 

 
  
 
 
Clean up 
When finished, don‚Äôt forget to de-allocate provisioned AgentCore Runtime in addition to the container repository that was created during the process: 
 
 agentcore_control_client = boto3.client(
    'bedrock-agentcore-control', region_name=region )
ecr_client = boto3.client('ecr',region_name=region )
runtime_delete_response = agentcore_control_client.delete_agent_runtime(    agentRuntimeId=launch_result.agent_id,)
response = ecr_client.delete_repository(
    repositoryName=launch_result.ecr_uri.split('/')[1],force=True)
 
 
Conclusion 
Amazon Bedrock AgentCore represents a paradigm shift in how we deploy AI agents. By abstracting away infrastructure complexity while maintaining framework and model flexibility, AgentCore enables developers to focus on building sophisticated agent logic rather than managing deployment pipelines. Our Deep Agents deployment demonstrates that even complex, multi-agent systems with external API integrations can be deployed with minimal code changes. The combination of enterprise-grade security, built-in observability, and serverless scaling makes AgentCore the best choice for production AI agent deployments. Specifically for deep research agents, AgentCore offers the following unique capabilities that you can explore: 
 
 AgentCore Runtime can handle asynchronous processing and long running (up to 8 hours) agents. Asynchronous tasks allow your agent to continue processing after responding to the client and handle long-running operations without blocking responses. Your background research sub-agent could be asynchronously researching for hours. 
 AgentCore Runtime works with AgentCore Memory, enabling capabilities such as building upon previous findings, remembering research preferences, and maintaining complex investigation context without losing progress between sessions. 
 You can use AgentCore Gateway to extend your deep research to include proprietary insights from enterprise services and data sources. By exposing these differentiated resources as MCP tools, your agents can quickly take advantage and combine that with publicly available knowledge. 
 
Ready to deploy your agents to production? Here‚Äôs how to get started: 
 
 Install the AgentCore starter kit: pip install bedrock-agentcore-starter-toolkit 
 Experiment: Deploy your code by following this step by step guide. 
 
The era of production-ready AI agents is here. With AgentCore, the journey from prototype to production has never been shorter. 
 
About the authors 
Vadim Omeltchenko is a Sr. AI/ML Solutions Architect who is passionate about helping AWS customers innovate in the cloud. His prior IT experience was predominantly on the ground. 
Eashan Kaushik is a Specialist Solutions Architect AI/ML at Amazon Web Services. He is driven by creating cutting-edge generative AI solutions while prioritizing a customer-centric approach to his work. Before this role, he obtained an MS in Computer Science from NYU Tandon School of Engineering. Outside of work, he enjoys sports, lifting, and running marathons. 
Shreyas Subramanian is a Principal data scientist and helps customers by using Machine Learning to solve their business challenges using the AWS platform. Shreyas has a background in large scale optimization and Machine Learning, and in use of Machine Learning and Reinforcement Learning for accelerating optimization tasks. 
Mark Roy is a Principal Machine Learning Architect for AWS, helping customers design and build generative AI solutions. His focus since early 2023 has been leading solution architecture efforts for the launch of Amazon Bedrock, the flagship generative AI offering from AWS for builders. Mark‚Äôs work covers a wide range of use cases, with a primary interest in generative AI, agents, and scaling ML across the enterprise. He has helped companies in insurance, financial services, media and entertainment, healthcare, utilities, and manufacturing. Prior to joining AWS, Mark was an architect, developer, and technology leader for over 25 years, including 19 years in financial services. Mark holds six AWS Certifications, including the ML Specialty Certification.

‚∏ª