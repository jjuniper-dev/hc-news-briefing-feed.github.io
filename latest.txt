‚úÖ Morning News Briefing ‚Äì August 11, 2025 10:49

üìÖ Date: 2025-08-11 10:49
üè∑Ô∏è Tags: #briefing #ai #publichealth #digitalgov

‚∏ª

üßæ Weather
‚Ä¢ HEAT WARNING, Pembroke
  Persons in or near this area should be on the lookout for adverse weather conditions and take necessary safety precautions . People in or around this area are advised to take necessary precautions . Weather conditions are expected to worsen in the summer of 2025 . People should also be aware of the weather conditions in the area and take safety precautions if conditions worsen in August 2025 . The weather is expected to deteriorate in
‚Ä¢ Current Conditions:  18.8¬∞C
  Temperature: 18.8&deg;C Pressure / Tendency: 101.9 kPa rising Humidity: 93 % Dewpoint: 17.6&deg:C Wind: SSE 2 km/h Air Quality Health Index: n/a . Observed at: Pembroke 6:00 AM EDT Monday 11 August 2025 . Weather: 18/8/C
‚Ä¢ Monday: Sunny. High 34.
  Sunny. Wind becoming south 20 km/h this afternoon . High 34. Humidex 41. UV index 8 or very high. Windbecoming south 20km/h today afternoon . Wind will be south to 20kmph this afternoon, with highs of 34.5C . Wind could gust up to 20 kmphph this morning, forecasters predict . High of 34C

üåç International News
No updates.

üçÅ Canadian News
No updates.

üá∫üá∏ U.S. Top Stories
‚Ä¢ Trump's tariff revenue has skyrocketed. But how big is it, really?
  President Trump's new tariffs are pouring in, but it's still only a fraction of overall government revenues . It falls short of new spending in the recent Republican megabill . The tariffs are still only part of the government's overall budget deficit, and fall short of spending in recent GOP megabills . The president's tariffs are expected to raise $1.2 billion in a year
‚Ä¢ Factories are losing immigrant workers, stressing those who remain
  Trump campaigned on helping American workers through his immigration policies . Now that he's revoked work authorization for thousands of immigrants, those left behind are feeling taxed by their absence . Those left behind feel taxed by the absence of those who have been granted work authorization to work in the U.S. Now that they have been left behind, they say they're feeling the loss of their work authorization .
‚Ä¢ Boston Public Library aims to increase access to a vast historic archive using AI
  The library is launching a project in collaboration with Harvard Law School and OpenAI this summer to digitize the materials and make them more fully searchable . Boston Public Library is also working with OpenAI to make the materials more searchable and easier to find . The project is a collaboration between the library and the law school of Harvard Law Law School, OpenAI and the Harvard University of Harvard
‚Ä¢ What's the deal with claims that birth control is dangerous?
  Social media is full of videos saying hormonal contraception can hurt you and promoting natural alternatives . How did the treatments get such a bad reputation and do alternatives work? Do you know how to get rid of the bad reputation? Share your story with CNN iReport.com . Back to the page you came from.com/news/show us what you think you need to do to avoid the
‚Ä¢ Israeli strike kills journalists in Gaza City, worsening the death toll for the media
  Israel's military targeted an Al Jazeera correspondent with an airstrike Sunday, killing him, another network journalist and other people . All of whom were sheltering outside the Gaza City Hospital complex were killed . The airstrike killed him, a fellow Al Jazeera journalist and two other people, all of whom he was sheltering from an airstrike . Israel's airstrike killed a journalist, another Al Jazeera reporter and a

üß† Artificial Intelligence
No updates.

üíª Digital Strategy
‚Ä¢ Torvalds blasts tardy kernel dev: Your 'garbage' RISC-V patches are 'making the world worse'
  Linux head honcho Linus Torvalds has put a kernel developer "on notice" for waiting until the eleventh hour to supply a patch set for Linux on RISC-V systems which "makes the world actively a worse place to live" ‚Äì in a scathing missive harkening back to his invective-laden tirades of old . Well, at least
‚Ä¢ Snotty astronauts should skip spacewalks, suggests study
  Pressure difference between space station, space suits increases congestion, say boffins . No one can hear you sneeze in space, but that hasn't stopped a team of boffin . Key takeaway: skip the spacewalk - maybe skip the space walk . The pressure difference between the space station and space suits is due to increase in congestion . Pressure difference also increases in space
‚Ä¢ The UK‚Äôs cartographer maps mission to help people and business
  Britain's Ordnance Survey, founded in 1791, is interloping in the digital age . AR games mingle with underground assets in the data plan for 200-year-old survey . Minecraft, AR gaming, and EV charger locations have all become part of its portfolio, alongside the paper-based maps beloved by the nation's legion of cagoule-clad outdoor types
‚Ä¢ Your CV is not fit for the 21st century ‚Äì time to get it up to scratch
  The job market is queasy and since you're reading this, you need to upgrade your CV . It's going to require some work to game the poorly trained AIs now doing so much of the heavy lifting . And yes, that means that means (retch) catering to AI searchers . I know you don't want to, but it's best to think of this as dealing
‚Ä¢ AI coding tools crash on launch, could reboot better in future
  The CEO of GitHub, coding‚Äôs universal termite mound, says that AI is going to do all the coding . Meanwhile, real life AI coding tools make coders less productive while spreading the hallucination that they‚Äôre more so . Here are two snapshots of AI in coding in mid 2025, one from GitHub CEO Grace Hopper, and one from Grace Hoppers .

üè• Public Health
No updates.

üî¨ Science
‚Ä¢ Physical fitness status and associated determinants among Chinese children aged 9‚Äì12 years in Shandong province: a population-based cross-sectional study
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Evidence triangulator: using large language models to extract and synthesize causal evidence across study designs
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Dietary supplement consumption is associated with lower dietary quality in Chinese university students based on a cross-sectional study
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Hemoglobin and mean platelet volume abnormalities in children exposed to heavy metals and metalloids in a pilot biomonitoring study
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Demographic variations and temporal trends in hospice and palliative care fellowship matches in the United States
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations

üßæ Government & Policy
No updates.

üèõÔ∏è Enterprise Architecture & IT Governance
No updates.

ü§ñ AI & Emerging Tech
‚Ä¢ This quantum radar could image buried objects
  Physicists have created a new type of radar that could help improve underground imaging, using a cloud of atoms in a glass cell to detect reflected radio waves. The radar is a type of quantum sensor, an emerging technology that uses the quantum-mechanical properties of objects as measurement devices. It‚Äôs still a prototype, but its intended use is to image buried objects in situations such as constructing underground utilities, drilling wells for natural gas, and excavating archaeological sites.



Like conventional radar, the device sends out radio waves, which reflect off nearby objects. Measuring the time it takes the reflected waves to return makes it possible to determine where an object is. In conventional radar, the reflected waves are detected using a large antenna, among other receiver components. But in this new device, the reflected waves are registered by detecting the interactions between the returning waves and the atom cloud.



The current incarnation of the radar is still bulky, as the researchers have kept it connected to components on an optical table for ease of testing. But they think their quantum radar could be significantly smaller than conventional designs. ‚ÄúInstead of having this sizable metal structure to receive the signal, we now can use this small glass cell of atoms that can be about a centimeter in size,‚Äù says Matthew Simons, a physicist at the National Institute of Standards and Technology (NIST), who was a member of the research team. NIST also worked with the defense contractor RTX to develop the radar.&nbsp;&nbsp;





The glass cell that serves as the radar‚Äôs quantum component is full of cesium atoms kept at room temperature. The researchers use lasers to get each individual cesium atom to swell to nearly the size of a bacterium, about 10,000 times bigger than the usual size. Atoms in this bloated condition are called Rydberg atoms.&nbsp;



When incoming radio waves hit Rydberg atoms, they disturb the distribution of electrons around their nuclei. Researchers can detect the disturbance by shining lasers on the atoms, causing them to emit light; when the atoms are interacting with a radio wave, the color of their emitted light changes. Monitoring the color of this light thus makes it possible to use the atoms as a radio receiver. Rydberg atoms are sensitive to a wide range of radio frequencies without needing to change the physical setup, says Micha≈Ç Parniak, a physicist at the University of Warsaw in Poland, who was not involved in the work. This means a single compact radar device could potentially work at the multiple frequency bands required for different applications.



Simons‚Äôs team tested the radar by placing it in a specially designed room with foam spikes on the floor, ceiling, and walls like stalactites and stalagmites. The spikes absorb, rather than reflect, nearly all the radio waves that hit them. This simulates the effect of a large open space, allowing the group to test the radar‚Äôs imaging capability without unwanted reflections off walls.&nbsp;



MATT SIMONS, NIST




The researchers placed a radio wave transmitter in the room, along with their Rydberg atom receiver, which was hooked up to an optical table outside the room. They aimed radio waves at a copper plate about the size of a sheet of paper, some pipes, and a steel rod in the room, each placed up to five meters away. The radar allowed them to locate the objects to within 4.7 centimeters. The team posted a paper on the research to the arXiv preprint server in late June.



The work moves quantum radar closer to a commercial product. ‚ÄúThis is really about putting elements together in a nice way,‚Äù says Parniak. While other researchers have previously demonstrated how Rydberg atoms can work as radio wave detectors, he says, this group has integrated the receiver with the rest of the device more sleekly than before.&nbsp;



Other researchers have explored the use of Rydberg atoms for other radar applications. For example, Parniak‚Äôs team recently developed a Rydberg atom sensor for measuring radio frequencies to troubleshoot chips used in car radar. Researchers are also exploring whether radar using Rydberg-atom receivers could be used for measuring soil moisture.



This device is just one example of a quantum sensor, a type of technology that incorporates quantum components into conventional tools. For example, the US government has developed gyroscopes that use the wave properties of atoms for sensing rotation, which is useful for navigation. Researchers have also created quantum sensors using impurities in diamond to measure magnetic fields in, for example, biomedical applications.





One advantage of quantum sensors is the inherent consistency of their core components. Each cesium atom in their device is identical. In addition, the radio receiver relies on the fundamental structure of these atoms, which never changes. Properties of the atoms ‚Äúcan be linked directly to fundamental constants,‚Äù says Simons. For this reason, quantum sensors should require less calibration than their non-quantum counterparts.&nbsp;



Governments worldwide have invested billions of dollars to develop quantum sensors and quantum computers, which share similar components. For example, researchers have built quantum computers using Rydberg atoms as qubits, the equivalent to bits in a conventional computer. Thus, advances in quantum sensing can potentially translate into advances into quantum computing, and vice versa. Parniak has recently adapted an error-correction technique from quantum computing to improve a Rydberg-atom-based sensor.&nbsp;



Researchers still need to continue developing quantum radar before it can be made commercially viable. In the future, they need to work on improving the device‚Äôs sensitivity to fainter signals, which could involve improving the coatings for the glass cell. ‚ÄúWe don‚Äôt see this replacing all radar applications,‚Äù says Simons. Instead, he thinks it will be useful for particular scenarios that require a compact device.
‚Ä¢ The Download: GPT-5 is here, and Intel‚Äôs CEO drama
  This is today&#8217;s edition of¬†The Download,¬†our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



GPT-5 is here. Now what?



At long last, OpenAI has released GPT-5. The new system abandons the distinction between OpenAI‚Äôs flagship models and its o series of reasoning models, automatically routing user queries to a fast nonreasoning model or a slower reasoning version.It is now available to everyone through the ChatGPT web interface‚Äîthough nonpaying users may need to wait a few days to gain full access.&nbsp;



GPT-5 will furnish a more pleasant and seamless user experience. That‚Äôs not nothing, but it falls far short of the transformative AI future that Sam Altman has spent much of the past year hyping. Read the full story.



‚ÄîGrace Huckins







The must-reads



I‚Äôve combed the internet to find you today‚Äôs most fun/important/scary/fascinating stories about technology.



1 Donald Trump has called on Intel‚Äôs CEO to resignHe claims Lip-Bu Tan is ‚Äúconflicted‚Äù by his business ties to China. (FT $)+ Tan was already at odds with some of his board members before the intervention. (WSJ $)+ But the CEO claims he‚Äôs got the full backing of his board presently. (Bloomberg $)



2 Wildfires are raging across the western USAnd strong winds are rapidly spreading them across parched land. (WP $)+ The fires have a devastating effect on human health. (The Guardian)+ How AI can help spot wildfires. (MIT Technology Review)



3 Meta‚Äôs AI superintelligence team is growingThe new TBD Lab is currently working on the newest version of its Llama model. (WSJ $)+ Meta has also been busy acquiring an AI audio firm. (The Information $)+ Elsewhere, Tesla has disbanded its supercomputer team. (Bloomberg $)



4 A man suffered psychosis after ChatGPT suggested he take sodium bromideThe 60-year old ended up with bromism. (Ars Technica)+ He‚Äôd been taking it for three months before he went to the ER. (The Independent)+ AI companies have stopped warning you that their chatbots aren‚Äôt doctors. (MIT Technology Review)



5 Meet Silicon Valley‚Äôs AI RationalistsThe group‚Äôs influence has spread through tech giants and AI pioneers alike. (NYT $)+ Inside effective altruism, where the far future counts a lot more than the present. (MIT Technology Review)



6 An CBP agent wore Meta smart glasses during an immigration raidSignalling that law enforcement are interested in this technology. (404 Media)



7 The US military has a new use for Tesla CybertrucksNamely, aiming missiles at them. (The Verge)+ It wants to learn how to destroy them if enemies start deploying them. (The Register)



8 South Korea will decide whether to let Google Maps work The decades-old debate could be laid to rest next week. (The Guardian)+ The country has previously rejected Google‚Äôs requests on security grounds. (Reuters)



9 Instagram‚Äôs new location-sharing feature is hereIt‚Äôs a bid to make the app more participatory and social. (Insider $)+ It also looks a whole lot like Snap‚Äôs map. (Fast Company $)10 These headphones could help you to focus Startup Neurable wants to track your brain activity to prevent you getting distracted. (Vox)+ A new AI translation system for headphones clones multiple voices simultaneously. (MIT Technology Review)







Quote of the day



‚ÄúI don‚Äôt think we should think of them as the ‚Äònew Google‚Äô yet.‚Äù



‚ÄîBrian Chesky, Airbnb‚Äôs CEO, questions the hype swirling around AI agents‚Äô capabilities, TechCrunch reports.







One more thing











The arrhythmia of our current ageArrhythmia means the heart beats, but not in proper time‚Äîa critical rhythm of life suddenly going rogue and unpredictable. It‚Äôs frightening to experience, but what if it‚Äôs also a good metaphor for our current times? That a pulse once seemingly so steady is now less sure.Perhaps this wobbliness might be extrapolated into a broader sense of life in the 2020s.Maybe you feel it, too‚Äîthat the world seems to have skipped more than a beat or two as demagogues rant and democracy shudders, hurricanes rage, and glaciers dissolve. We can‚Äôt stop watching tiny screens where influencers pitch products we don‚Äôt need alongside news about senseless wars that destroy, murder, and maim tens-of-thousands.All the resulting anxiety has been hard on our hearts‚Äîliterally and metaphorically. Read the full story.



‚ÄîDavid Ewing Duncan







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)



+ This entertaining site tracks just how many cigarettes are smoked throughout famous films (and the ones which feature no smoking at all)+ Check out the very best seaside amusement arcade games.+ Next week promises a dazzling meteor shower‚Äîhere‚Äôs how to best watch it.+ Oh, to live an hour in the shoes of a Boston-based bee doctor
‚Ä¢ GPT-5 is here. Now what?
  At long last, OpenAI has released GPT-5. The new system abandons the distinction between OpenAI‚Äôs flagship models and its o series of reasoning models, automatically routing user queries to a fast nonreasoning model or a slower reasoning version. It is now available to everyone through the ChatGPT web interface‚Äîthough nonpaying users may need to wait a few days to gain full access to the new capabilities.&nbsp;



It‚Äôs tempting to compare GPT-5 with its explicit predecessor, GPT-4, but the more illuminating juxtaposition is with o1, OpenAI‚Äôs first reasoning model, which was released last year. In contrast to GPT-5‚Äôs broad release, o1 was initially available only to Plus and Team subscribers. Those users got access to a completely new kind of language model‚Äîone that would ‚Äúreason‚Äù through its answers by generating additional text before providing a final response, enabling it to solve much more challenging problems than its nonreasoning counterparts.





Whereas o1 was a major technological advancement, GPT-5 is, above all else, a refined product. During a press briefing, Sam Altman compared GPT-5 to Apple‚Äôs Retina displays, and it‚Äôs an apt analogy, though perhaps not in the way that he intended. Much like an unprecedentedly crisp screen, GPT-5 will furnish a more pleasant and seamless user experience. That‚Äôs not nothing, but it falls far short of the transformative AI future that Altman has spent much of the past year hyping. In the briefing, Altman called GPT-5 ‚Äúa significant step along the path to AGI,‚Äù or artificial general intelligence, and maybe he‚Äôs right‚Äîbut if so, it‚Äôs a very small step.



Take the demo of the model‚Äôs abilities that OpenAI showed to MIT Technology Review in advance of its release. Yann Dubois, a post-training lead at OpenAI, asked GPT-5 to design a web application that would help his partner learn French so that she could communicate more easily with his family. The model did an admirable job of following his instructions and created an appealing, user-friendly app. But when I gave GPT-4o an almost identical prompt, it produced an app with exactly the same functionality. The only difference is that it wasn‚Äôt as aesthetically pleasing.



Some of the other user-experience improvements are more substantial. Having the model rather than the user choose whether to apply reasoning to each query removes a major pain point, especially for users who don‚Äôt follow LLM advancements closely.&nbsp;



And, according to Altman, GPT-5 reasons much faster than the o-series models. The fact that OpenAI is releasing it to nonpaying users suggests that it‚Äôs also less expensive for the company to run. That‚Äôs a big deal: Running powerful models cheaply and quickly is a tough problem, and solving it is key to reducing AI‚Äôs environmental impact.¬†



OpenAI has also taken steps to mitigate hallucinations, which have been a persistent headache. OpenAI‚Äôs evaluations suggest that GPT-5 models are substantially less likely to make incorrect claims than their predecessor models, o3 and GPT-4o. If that advancement holds up to scrutiny, it could help pave the way for more reliable and trustworthy agents. ‚ÄúHallucination can cause real safety and security issues,‚Äù says Dawn Song, a professor of computer science at UC Berkeley. For example, an agent that hallucinates software packages could download malicious code to a user‚Äôs device.



GPT-5 has achieved the state of the art on several benchmarks, including a test of agentic abilities and the coding evaluations SWE-Bench and Aider Polyglot. But according to Cl√©mentine Fourrier, an AI researcher at the company HuggingFace, those evaluations are nearing saturation, which means that current models have achieved close to maximal performance.&nbsp;



‚ÄúIt‚Äôs basically like looking at the performance of a high schooler on middle-grade problems,‚Äù she says. ‚ÄúIf the high schooler fails, it tells you something, but if it succeeds, it doesn‚Äôt tell you a lot.‚Äù Fourrier said she would be impressed if the system achieved a score of 80% or 85% on SWE-Bench‚Äîbut it only managed a 74.9%.&nbsp;



Ultimately, the headline message from OpenAI is that GPT-5 feels better to use. ‚ÄúThe vibes of this model are really good, and I think that people are really going to feel that, especially average people who haven&#8217;t been spending their time thinking about models,‚Äù said Nick Turley, the head of ChatGPT.



Vibes alone, however, won‚Äôt bring about the automated future that Altman has promised. Reasoning felt like a major step forward on the way to AGI. We‚Äôre still waiting for the next one.
‚Ä¢ The Download: how AI is improving itself, and hidden greenhouse gases
  This is today&#8217;s edition of¬†The Download,¬†our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



Five ways that AI is learning to improve itself



Last week, Mark Zuckerberg declared that Meta aims to achieve smarter-than-human AI. He seems to have a recipe for achieving that goal, and the first ingredient is human talent: Zuckerberg has reportedly tried to lure top researchers to Meta Superintelligence Labs with nine-figure offers.&nbsp;



The second ingredient is AI itself.&nbsp; Zuckerberg recently said on an earnings call that Meta will focus on building self-improving AI‚Äîsystems that can bootstrap themselves to higher and higher levels of performance. He hopes to tap into a very real trend. Here are five ways that AI is already making itself better.



‚ÄîGrace Huckins







The greenhouse gases we&#8217;re not accounting for



Back in 2021, climate scientists noticed that levels of methane had soared in the atmosphere the previous year, rising at the fastest rate on record despite the global covid-19 lockdowns.



Researchers eventually spotted a clear pattern: Methane emissions had increased sharply across the tropics, where wetlands were growing wetter and warmer.The findings offer one of the clearest cases so far where climate change itself is driving additional greenhouse-gas emissions from natural systems, triggering a feedback effect that threatens to produce more warming, more emissions, and on and on.&nbsp;



There&#8217;s now a major endeavor underway to better track and understand what&#8217;s going on. Read our story about it.



‚ÄîJames Temple



This article is from The Spark, MIT Technology Review‚Äôs weekly climate newsletter. To receive it in your inbox every Wednesday, sign up here.







The must-reads



I‚Äôve combed the internet to find you today‚Äôs most fun/important/scary/fascinating stories about technology.



1 The Trump administration‚Äôs punishing new tariffs have come into effect¬†And prices are already climbing. (NYT $)+ Economists fear the US economy is poised to shrink. (WP $)+ Sweeping tariffs could threaten the US manufacturing rebound. (MIT Technology Review)



2 Sections of the US Constitution have been deleted onlinePassages about Congress‚Äô powers and citizens‚Äô unlawful detention have been scrubbed from the US government‚Äôs website. (TechCrunch)+ The Library of Congress blamed a coding error. (Ars Technica)



3 China is fighting a mosquito-borne virusIt‚Äôs deploying drones to search for standing water where the insects lay eggs. (AP News)+ Chikungunya virus is rarely fatal, but can cause fever and joint pain. (CNN)+ Authorities are taking a leaf out of their covid-fighting playbooks. (NYT $)



4 US federal agencies will have access to ChatGPT Enterprise¬†For the grand sum of $1 a year. (Ars Technica)+ It won‚Äôt use workers‚Äô data to train ChatGPT, apparently. (Bloomberg $)+ The news comes after major AI firms were greenlit as federal vendors. (Engadget)



5 Chinese drug discovery startups are striking deals with Big PharmaWestern pharmaceutical giants are confident they can deliver. (Rest of World)+ An AI-driven ‚Äúfactory of drugs‚Äù claims to have hit a big milestone. (MIT Technology Review)



6 Is it possible to build truly green AI data centers?The tech industry appears pretty hooked on fossil fuels. (FT $)+ We did the math on AI‚Äôs energy footprint. Here‚Äôs the story you haven‚Äôt heard. (MIT Technology Review)



7 The US is increasingly reliant on private companies for weather dataExperts are wary about losing access to vital tools. (Undark)+ How US research cuts are threatening crucial climate data. (MIT Technology Review)



8 Genetic factors could contribute to the risk of developing chronic fatigue syndromeIt‚Äôs the first robust evidence that genetics play a role. (New Scientist $)



9 An experimental pill is showing weight-loss promiseObese participants in Eli Lilly‚Äôs trial lost more than 12% of their body weight. (Wired $)+ We‚Äôre learning more about what weight-loss drugs do to the body. (MIT Technology Review)



10 Finding a job online is a nightmareSome companies are going back to basics to find the best recruits. (WSJ $)







Quote of the day



‚ÄúWe didn‚Äôt vote for ChatGPT.‚Äù



‚ÄîVirginia Dignum, a professor of responsible artificial intelligence at Sweden‚Äôs Ume√• University, criticizes the country‚Äôs prime minister, Ulf Kristersson for admitting he regularly consults AI tools, the Guardian reports.







One more thing







Why AI could eat quantum computing‚Äôs lunchTech companies have been funneling billions of dollars into quantum computers for years. The hope is that they‚Äôll be a game changer for fields as diverse as finance, drug discovery, and logistics.But while the field struggles with the realities of tricky quantum hardware, another challenger is making headway in some of these most promising use cases. AI is now being applied to fundamental physics, chemistry, and materials science in a way that suggests quantum computing‚Äôs purported home turf might not be so safe after all. Read the full story.



‚ÄîEdd Gent







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)



+ A super rare edition of The Hobbit has sold for a record-breaking amount at auction.+ How to find new music without relying on Spotify.+ The forthcoming Social Network sequel is rumored to be getting a Succession shakeup.+ If you own both a cat and a scanner, you know what you have to do.
‚Ä¢ The greenhouse gases we‚Äôre not accounting for
  In the spring of 2021, climate scientists were stumped.&nbsp;



The global economy was just emerging from the covid-19 lockdowns, but for some reason the levels of methane‚Äîa greenhouse gas emitted mainly through agriculture and fossil-fuel production‚Äîhad soared in the atmosphere the previous year, rising at the fastest rate on record.



Researchers around the world set to work unraveling the mystery, reviewing readings from satellites, aircraft, and greenhouse-gas monitoring stations. They eventually spotted a clear pattern: Methane emissions had increased sharply across the tropics, where wetlands were growing wetter and warmer.&nbsp;



That created the ideal conditions for microbes that thrive in anaerobic muck, which gobbled up more of the carbon-rich organic matter and spat out more methane as a by-product. (Reduced pollution from nitrogen oxides, which help to break down methane in the atmosphere, also likely played a substantial role.)



The findings offer one of the clearest cases so far where climate change itself is driving additional greenhouse-gas emissions from natural systems, triggering a feedback effect that threatens to produce more warming, more emissions, and on and on.&nbsp;



There are numerous additional ways this is happening or soon could, including wildfires and thawing permafrost. These are major emissions sources that aren‚Äôt included in the commitments nations have made under the Paris climate agreement‚Äîand climate risks that largely aren‚Äôt accounted for in the UN Intergovernmental Panel on Climate Change‚Äôs most recent warming scenarios.



Spark Climate Solutions (not to be confused with this newsletter) hopes to change that.The San Francisco nonprofit is launching what‚Äôs known as a model intercomparison project, in which different research teams run the same set of experiments on different models across a variety of emissions scenarios to determine how climate change could play out. This one would specifically explore how a range of climate feedback effects could propel additional warming, additional emissions, and additional types of feedback.



‚ÄúThese increased emissions from natural sources add to human emissions and amplify climate change,‚Äù says Phil Duffy, chief scientist at Spark Climate Solutions, who previously served as climate science advisor to President Joe Biden. ‚ÄúAnd if you don‚Äôt look at all of them together, you can‚Äôt quantify the strength of that feedback effect.‚Äù



Other participants in the effort will include scientists at the Environmental Defense Fund, Stanford University, the Woodwell Climate Research Center, and other institutions in Europe and Australia, according to Spark Climate Solutions.



The nonprofit hopes to publish the findings in time for them to be incorporated into the UN climate panel‚Äôs seventh major assessment report, which is just getting underway, to help ensure that these dangers are more fully represented. That, in turn, would give nations a more accurate sense of the world‚Äôs carbon budgets, or the quantity of greenhouse gases they can produce before the planet reaches temperatures 1.5 ¬∞C or&nbsp; 2 ¬∞C over preindustrial levels.&nbsp;



But one thing is already clear: Since the current scenarios don‚Äôt fully account for these feedback effects, the world will almost certainly warm faster than is now forecast, which underscores the importance of carrying out this exercise.&nbsp;



Scientists at EDF, Woodwell and other institutions found that fires in the world‚Äôs northernmost forests, thawing permafrost and warming tropical wetlands could together push the planet beyond 2 ¬∞C years faster, eliminating up to a quarter of the time left before the world passes the core goal of the Paris agreement, in a paper under review.&nbsp;



Earlier this year, Spark Climate Solutions set up a broader program to advance research and awareness of what‚Äôs known as warming-induced emissions, which will launch additional collaborations similar to the modeling intercomparison project.&nbsp;&nbsp;



The goal of the program and the research project is ‚Äúto really mainstream the inclusion of this topic in climate science and climate policy, and to drive research around climate solutions,‚Äù says Ben Poulter, who leads the program at Spark Climate Solutions and was previously a scientist at the NASA Goddard Space Flight Center.



Spark notes that warming temperatures could also release more carbon dioxide from the oceans, in a process known as outgassing; additional carbon dioxide and nitrous oxide, a potent greenhouse gas that also depletes the protective ozone layer, from farmland; more carbon dioxide and methane from wildfires; and still more of all three of these gases as permafrost thaws.



The ground remains frozen year round across a vast expanse of the Northern Hemisphere, creating a frosty underground storehouse from Alaska to Siberia that‚Äôs packed with twice as much carbon as the atmosphere.



But as it thaws, it starts to decompose and release greenhouse gases, says Susan Natali, an Arctic climate scientist focused on permafrost at Woodwell. A study published in Nature in January noted that 30% of the world‚Äôs Arctic‚ÄìBoreal Zone has already flipped from a carbon sink to a carbon source, when wildfires, thawing permafrost and other factors are taken into account.



Despite these increasing risks, only a minority of the models that fed into the UN climate panel‚Äôs last major report incorporated the feedback effects of thawing permafrost. And the emissions risks still weren‚Äôt fully accounted for because these ecosystems are difficult to monitor and model, Natali says.Among the complexities: Wildfires, which are themselves hard to predict, can accelerate thawing. It‚Äôs also hard to foresee which regions will grow drier or wetter, which determines whether they release mostly methane or carbon dioxide‚Äîand those gases have very different warming effects over different time periods. There are counterbalancing effects that must be taken into account as well‚Äîfor instance, as carbon-absorbing plants replace ice and snow in certain areas.



Natali says improving our understanding of these complex feedback effects is essential to understanding the dangers we face.



‚ÄúIt‚Äôs going to mean additional costs to human health, human life,‚Äù she says. ‚ÄúWe want people to be safe‚Äîand it‚Äôs very hard to do that if you don‚Äôt know what‚Äôs coming and you‚Äôre not prepared for it.‚Äù



This article is from The Spark,¬†MIT Technology Review‚Äôs weekly climate newsletter. To receive it in your inbox every Wednesday,¬†sign up here.

üîí Cybersecurity & Privacy
‚Ä¢ KrebsOnSecurity in New ‚ÄòMost Wanted‚Äô HBO Max Series
  A new documentary series about cybercrime airing next month on HBO Max features interviews with Yours Truly. The four-part series follows the exploits of Julius Kivim√§ki, a prolific Finnish hacker recently convicted of leaking tens of thousands of patient records from an online psychotherapy practice while attempting to extort the clinic and its patients.
The documentary, &#8220;Most Wanted: Teen Hacker,&#8221; explores the 27-year-old Kivim√§ki&#8217;s lengthy and increasingly destructive career, one that was marked by cyber attacks designed to result in real-world physical impacts on their targets.

By the age of 14, Kivim√§ki had fallen in with a group of criminal hackers who were mass-compromising websites and milking them for customer payment card data. Kivim√§ki and his friends enjoyed harassing and terrorizing others by &#8220;swatting&#8221; their homes &#8212; calling in fake hostage situations or bomb threats at a target&#8217;s address in the hopes of triggering a heavily-armed police response to that location.
On Dec. 26, 2014, Kivim√§ki and fellow members of a group of online hooligans calling themselves the Lizard Squad launched a massive distributed denial-of-service (DDoS) attack against the Sony Playstation and Microsoft Xbox Live platforms, preventing millions of users from playing with their shiny new gaming rigs the day after Christmas. The Lizard Squad later acknowledged that the stunt was planned to call attention to their new DDoS-for-hire service, which came online and started selling subscriptions shortly after the attack.
Finnish investigators said Kivim√§ki also was responsible for a 2014 bomb threat against former Sony Online Entertainment President John Smedley that grounded an American Airlines plane. That incident was widely reported to have started with a Twitter post from the Lizard Squad, after Smedley mentioned some upcoming travel plans online. But according to Smedley and Finnish investigators, the bomb threat started with a phone call from Kivim√§ki.
Julius &#8220;Zeekill&#8221; Kivimaki, in December 2014.
The creaky wheels of justice seemed to be catching up with Kivim√§ki in mid-2015, when a Finnish court found him guilty of more than 50,000 cybercrimes, including data breaches, payment fraud, and operating a global botnet of hacked computers. Unfortunately, the defendant was 17 at the time, and received little more than a slap on the wrist: A two-year suspended sentence and a small fine.
Kivim√§ki immediately bragged online about the lenient sentencing, posting on Twitter that he was an &#8220;untouchable hacker god.&#8221; I wrote a column in 2015 lamenting his laughable punishment because it was clear even then that this was a person who enjoyed watching other people suffer, and who seemed utterly incapable of remorse about any of it. It was also abundantly clear to everyone who investigated his crimes that he wasn&#8217;t going to quit unless someone made him stop.
In response to some of my early reporting that mentioned Kivim√§ki, one reader shared that they had been dealing with non-stop harassment and abuse from Kivim√§ki for years, including swatting incidents, unwanted deliveries and subscriptions, emails to her friends and co-workers, as well as threatening phonecalls and texts at all hours of the night. The reader, who spoke on condition of anonymity, shared that Kivim√§ki at one point confided that he had no reason whatsoever for harassing her &#8212; that she was picked at random and that it was just something he did for laughs.
Five years after Kivim√§ki&#8217;s conviction, the Vastaamo Psychotherapy Center¬†in Finland became the target of blackmail when a tormentor identified as ‚Äúransom_man‚Äù demanded payment of 40 bitcoins (~450,000 euros at the time) in return for a promise not to publish highly sensitive therapy session notes Vastaamo had exposed online.
Ransom_man, a.k.a. Kivim√§ki, announced on the dark web that he would start publishing 100 patient profiles every 24 hours. When Vastaamo declined to pay, ransom_man shifted to extorting individual patients. According to Finnish police, some 22,000 victims reported extortion attempts targeting them personally, targeted emails that threatened to publish their therapy notes online unless paid a 500 euro ransom.
In October 2022, Finnish authorities charged Kivim√§ki with extorting Vastaamo and its patients. But by that time he was on the run from the law and living it up across Europe, spending lavishly on fancy cars, apartments and a hard-partying lifestyle.
In February 2023, Kivim√§ki was arrested in France¬†after authorities there responded to a domestic disturbance call and found the defendant sleeping off a hangover on the couch of a woman he‚Äôd met the night before. The French police grew suspicious when the 6‚Ä≤ 3‚Ä≥ blonde, green-eyed man presented an ID that stated he was of Romanian nationality.
A redacted copy of an ID Kivimaki gave to French authorities claiming he was from Romania.
In April 2024, Kivim√§ki was sentenced to more than six years in prison after being convicted of extorting Vastaamo and its patients.
The documentary is directed by the award-winning Finnish producer and director Sami Kieski and co-written by Joni Soila. According to an August 6 press release, the four 43-minute episodes will drop weekly on Fridays throughout September across Europe, the U.S, Latin America, Australia and South-East Asia.
‚Ä¢ Who Got Arrested in the Raid on the XSS Crime Forum?
  On July 22, 2025, the European police agency Europol said a long-running investigation led by the French Police resulted in the arrest of a 38-year-old administrator of XSS,¬†a Russian-language cybercrime forum with more than 50,000 members. The action has triggered an ongoing frenzy of speculation and panic among XSS denizens about the identity of the unnamed suspect, but the consensus is that he is a pivotal figure in the crime forum scene who goes by the hacker handle &#8220;Toha.&#8221; Here&#8217;s a deep dive on what&#8217;s knowable about Toha, and a short stab at who got nabbed.
An unnamed 38-year-old man was arrested in Kiev last month on suspicion of administering the cybercrime forum XSS. Image: ssu.gov.ua.
Europol did not name the accused, but published partially obscured photos of him from the raid on his residence in Kiev. The police agency said the suspect acted as a trusted third party &#8212; arbitrating disputes between criminals &#8212; and guaranteeing the security of transactions on XSS. A statement from Ukraine&#8217;s SBU security service said XSS counted among its members many cybercriminals from various ransomware groups, including REvil, LockBit, Conti, and Qiliin.
Since the Europol announcement, the XSS forum resurfaced at a new address on the deep web (reachable only via the anonymity network Tor). But from reviewing the recent posts, there appears to be little consensus among longtime members about the identity of the now-detained XSS administrator.
The most frequent comment regarding the arrest was a message of solidarity and support for Toha, the handle chosen by the longtime administrator of XSS and several other major Russian forums. Toha&#8217;s accounts on other forums have been silent since the raid.
Europol said the suspect has enjoyed a nearly 20-year career in cybercrime, which roughly lines up with Toha&#8217;s history. In 2005, Toha was a founding member of the Russian-speaking forum Hack-All. That is, until it got massively hacked a few months after its debut. In 2006, Toha rebranded the forum to exploit[.]in, which would go on to draw tens of thousands of members, including an eventual Who&#8217;s-Who of wanted cybercriminals.
Toha announced in 2018 that he was selling the Exploit forum, prompting rampant speculation on the forums that the buyer was secretly a Russian or Ukrainian government entity or front person. However, those suspicions were unsupported by evidence, and Toha vehemently denied the forum had been given over to authorities.
One of the oldest Russian-language cybercrime forums was DaMaGeLaB, which operated from 2004 to 2017, when its administrator &#8220;Ar3s&#8221; was arrested. In 2018, a partial backup of the DaMaGeLaB forum was reincarnated as xss[.]is, with Toha as its stated administrator.
CROSS-SITE GRIFTING
Clues about Toha&#8217;s early presence on the Internet &#8212; from ~2004 to 2010 &#8212; are available in the archives of Intel 471, a cyber intelligence firm that tracks forum activity. Intel 471 shows Toha used the same email address across multiple forum accounts, including at Exploit, Antichat, Carder[.]su and inattack[.]ru.
DomainTools.com finds Toha&#8217;s email address &#8212; toschka2003@yandex.ru &#8212; was used to register at least a dozen domain names &#8212; most of them from the mid- to late 2000s. Apart from exploit[.]in and a domain called ixyq[.]com, the other domains registered to that email address end in .ua, the top-level domain for Ukraine (e.g. deleted.org[.]ua, lj.com[.]ua, and blogspot.org[.]ua).
A 2008 snapshot of a domain registered to toschka2003@yandex.ru and to Anton Medvedovsky in Kiev. Note the message at the bottom left, &#8220;Protected by Exploit,in.&#8221; Image: archive.org.
Nearly all of the domains registered to toschka2003@yandex.ru contain the name Anton Medvedovskiy in the registration records, except for the aforementioned ixyq[.]com, which is registered to the name Yuriy Avdeev in Moscow.
This Avdeev surname came up in a lengthy conversation with Lockbitsupp, the leader of the rapacious and destructive ransomware affiliate group Lockbit. The conversation took place in February 2024, when Lockbitsupp asked for help identifying Toha&#8217;s real-life identity.
In early 2024, the leader of the Lockbit ransomware group &#8212; Lockbitsupp &#8212; asked for help investigating the identity of the XSS administrator Toha, which he claimed was a Russian man named Anton Avdeev.
Lockbitsupp didn&#8217;t share why he wanted Toha&#8217;s details, but he maintained that Toha&#8217;s real name was Anton Avdeev. I declined to help Lockbitsupp in whatever revenge he was planning on Toha, but his question made me curious to look deeper.
It appears Lockbitsupp&#8217;s query was based on a now-deleted Twitter post from 2022, when a user by the name &#8220;3xp0rt&#8221; asserted that Toha was a Russian man named Anton Viktorovich Avdeev, born October 27, 1983.
Searching the web for Toha&#8217;s email address toschka2003@yandex.ru reveals a 2010 sales thread on the forum bmwclub.ru where a user named Honeypo was selling a 2007 BMW X5. The ad listed the contact person as Anton Avdeev and gave the contact phone number 9588693.

A search on the phone number 9588693 in the breach tracking service Constella Intelligence finds plenty of official Russian government records with this number, date of birth and the name Anton Viktorovich Avdeev. For example, hacked Russian government records show this person has a Russian tax ID and SIN (Social Security number), and that they were flagged for traffic violations on several occasions by Moscow police; in 2004, 2006, 2009, and 2014.
Astute readers may have noticed by now that the ages of Mr. Avdeev (41) and the XSS admin arrested this month (38) are a bit off. This would seem to suggest that the person arrested is someone other than Mr. Avdeev, who did not respond to requests for comment.
A FLY ON THE WALL
For further insight on this question, KrebsOnSecurity sought comments from Sergeii Vovnenko, a former cybercriminal from Ukraine who now works at the security startup paranoidlab.com. I reached out to Vovnenko because for several years beginning around 2010 he was the owner and operator of thesecure[.]biz, an encrypted &#8220;Jabber&#8221; instant messaging server that Europol said was operated by the suspect arrested in Kiev. Thesecure[.]biz grew quite popular among many of the top Russian-speaking cybercriminals because it scrupulously kept few records of its users&#8217; activity, and its administrator was always a trusted member of the community.
The reason I know this historic tidbit is that in 2013, Vovnenko &#8212; using the hacker nicknames &#8220;Fly,&#8221; and &#8220;Flycracker&#8221; &#8212; hatched a plan to have a gram of heroin purchased off of the Silk Road darknet market and shipped to our home in Northern Virginia. The scheme was to spoof a call from one of our neighbors to the local police, saying this guy Krebs down the street was a druggie who was having narcotics delivered to his home.
I happened to be lurking on Flycracker&#8217;s private cybercrime forum when his heroin-framing plan was carried out, and called the police myself before the smack eventually arrived in the U.S. Mail. Vovnenko was later arrested for unrelated cybercrime activities, extradited to the United States, convicted, and deported after a 16-month stay in the U.S. prison system [on several occasions, he has expressed heartfelt apologies for the incident, and we have since buried the hatchet].
Vovnenko said he purchased a device for cloning credit cards from Toha in 2009, and that Toha shipped the item from Russia. Vovnenko explained that he (Flycracker) was the owner and operator of thesecure[.]biz from 2010 until his arrest in 2014.
Vovnenko believes thesecure[.]biz was stolen while he was in jail, either by Toha and/or an XSS administrator who went by the nicknames N0klos and Sonic.
&#8220;When I was in jail, [the] admin of xss.is stole that domain, or probably N0klos bought XSS from Toha or vice versa,&#8221; Vovnenko said of the Jabber domain. &#8220;Nobody from [the forums] spoke with me after my jailtime, so I can only guess what really happened.&#8221;
N0klos was the owner and administrator of an early Russian-language cybercrime forum known as Darklife[.]ws. However, N0kl0s also appears to be a lifelong Russian resident, and in any case seems to have vanished from Russian cybercrime forums several years ago.
Asked whether he believes Toha was the XSS administrator who was arrested this month in Ukraine, Vovnenko maintained that Toha is Russian, and that &#8220;the French cops took the wrong guy.&#8221;
WHO IS TOHA?
So who did the Ukrainian police arrest in response to the investigation by the French authorities? It seems plausible that the BMW ad invoking Toha&#8217;s email address and the name and phone number of a Russian citizen was simply misdirection on Toha&#8217;s part &#8212; intended to confuse and throw off investigators. Perhaps this even explains the Avdeev surname surfacing in the registration records from one of Toha&#8217;s domains.
But sometimes the simplest answer is the correct one. &#8220;Toha&#8221; is a common Slavic nickname for someone with the first name &#8220;Anton,&#8221; and that matches the name in the registration records for more than a dozen domains tied to Toha&#8217;s toschka2003@yandex.ru email address: Anton Medvedovskiy.
Constella Intelligence finds there is an Anton Gannadievich Medvedovskiy living in Kiev who will be 38 years old in December. This individual owns the email address itsmail@i.ua, as well an an Airbnb account featuring a profile photo of a man with roughly the same hairline as the suspect in the blurred photos released by the Ukrainian police. Mr. Medvedovskiy did not respond to a request for comment.
My take on the takedown is that the Ukrainian authorities likely arrested Medvedovskiy. Toha shared on DaMaGeLab in 2005 that he had recently finished the 11th grade and was studying at a university &#8212; a time when Mevedovskiy would have been around 18 years old. On Dec. 11, 2006, fellow Exploit members wished Toha a happy birthday. Records exposed in a 2022 hack at the Ukrainian public services portal diia.gov.ua show that Mr. Medvedovskiy&#8217;s birthday is Dec. 11, 1987.
The law enforcement action and resulting confusion about the identity of the detained has thrown the Russian cybercrime forum scene into disarray in recent weeks, with lengthy and heated arguments about XSS&#8217;s future spooling out across the forums.
XSS relaunched on a new Tor address shortly after the authorities plastered their seizure notice on the forum&#8217;s¬† homepage, but all of the trusted moderators from the old forum were dismissed without explanation. Existing members saw their forum account balances drop to zero, and were asked to plunk down a deposit to register at the new forum. The new XSS &#8220;admin&#8221; said they were in contact with the previous owners and that the changes were to help rebuild security and trust within the community.
However, the new admin&#8217;s assurances appear to have done little to assuage the worst fears of the forum&#8217;s erstwhile members, most of whom seem to be keeping their distance from the relaunched site for now.
Indeed, if there is one common understanding amid all of these discussions about the seizure of XSS, it is that Ukrainian and French authorities now have several years worth of private messages between XSS forum users, as well as contact rosters and other user data linked to the seized Jabber server.
&#8220;The myth of the &#8216;trusted person&#8217; is shattered,&#8221; the user &#8220;GordonBellford&#8221; cautioned on Aug. 3 in an Exploit forum thread about the XSS admin arrest. &#8220;The forum is run by strangers. They got everything. Two years of Jabber server logs. Full backup and forum database.&#8221;
GordonBellford continued:
And the scariest thing is: this data array is not just an archive. It is material for analysis that has ALREADY BEEN DONE . With the help of modern tools, they see everything:
Graphs of your contacts and activity.
Relationships between nicknames, emails, password hashes and Jabber ID.
Timestamps, IP addresses and digital fingerprints.
Your unique writing style, phraseology, punctuation, consistency of grammatical errors, and even typical typos that will link your accounts on different platforms.
They are not looking for a needle in a haystack. They simply sifted the haystack through the AI sieve and got ready-made dossiers.

üéì University AI
No updates.

üè¢ Corporate AI
‚Ä¢ Reimagining healthcare delivery and public health with AI
  In November 2022, OpenAI‚Äôs ChatGPT kick-started a new era in AI. This was followed less than a half year later by the release of GPT-4. In the months leading up to GPT-4‚Äôs public release, Peter Lee, president of Microsoft Research, cowrote a book full of optimism for the potential of advanced AI models to transform the world of healthcare. What has happened since? In this special podcast series, The AI Revolution in Medicine, Revisited, Lee revisits the book, exploring how patients, providers, and other medical professionals are experiencing and using generative AI today while examining what he and his coauthors got right‚Äîand what they didn‚Äôt foresee.&nbsp;



In this episode, healthcare leaders Dr. Umair Shah (opens in new tab) and Dr. Gianrico Farrugia (opens in new tab) join Lee to discuss AI‚Äôs impact on the business of public health and healthcare delivery, the healthcare-research connection, and the patient experience. Shah, a healthcare strategic consultant and former state secretary of health, explores the role of public health in the larger ecosystem and why it might not get the attention it needs or deserves and how AI could be leveraged to assist in data analysis, to help better engage with people on matters of public health, and to help narrow gaps between care delivery and public health responses during health emergencies. Farrugia, president and CEO of Mayo Clinic, traces AI‚Äôs path from predictive to generative and discusses how that progress has helped usher in a new healthcare architecture for Mayo Clinic and its partners, one powered by the goal of longer, healthier lives for patients, and how AI is also changing Mayo Clinic‚Äôs research and the education it provides, including the offering of masters and PhDs in AI and other emerging technologies.&nbsp;








Learn more:




Rickshaw Health (opens in new tab) (Shah)&nbsp;Homepage



COVID-19 After-Action Report (opens in new tab) (Shah)&nbsp;Washington State Department of Health | March 2024



Mayo Clinic Platform (opens in new tab) (Farrugia)&nbsp;Homepage



Atlas: A Novel Pathology Foundation Model by Mayo Clinic, Charit√©, and Aignostics (opens in new tab) (Farrugia)&nbsp;Publication | January 2025



MAIRA-2: Grounded Radiology Report Generation (Farrugia)Publication | June 2024



The AI Revolution in Medicine: GPT-4 and Beyond&nbsp;Book | Peter Lee, Carey Goldberg, Isaac Kohane | April 2023&nbsp;










	
		
			Subscribe to the Microsoft Research Podcast:		
		
							
					
						  
						Apple Podcasts
					
				
			
							
					
						
						Email
					
				
			
							
					
						
						Android
					
				
			
							
					
						
						Spotify
					
				
			
							
					
						
						RSS Feed
					
				
					
	




	
		
			
				
					

Transcript&nbsp;



[MUSIC]‚ÄØ



[BOOK PASSAGE]&nbsp;



PETER LEE: ‚ÄúIn US healthcare, quality ratings are increasingly used to tie the improvement in patient health outcomes to the reimbursement rates that healthcare providers can receive. The ability of GPT-4 to understand these systems and give concrete advice ‚Ä¶ has a chance to make it easier for providers to achieve success in both dimensions.‚Äù&nbsp;



[END OF BOOK PASSAGE]



[THEME MUSIC]



This is The AI Revolution in Medicine, Revisited. I‚Äôm your host, Peter Lee. 



Shortly after OpenAI&#8217;s GPT-4 was publicly released, Carey Goldberg, Dr. Zak Kohane, and I published The AI Revolution in Medicine to help educate the world of healthcare and medical research about the transformative impact this new generative AI technology could have. But because we wrote the book when GPT-4 was still a secret, we had to speculate. Now, two years later, what did we get right, and what did we get wrong? 



In this series, we‚Äôll talk to clinicians, patients, hospital administrators, and others to understand the reality of AI in the field and where we go from here.



				
				
					



[THEME MUSIC FADES]&nbsp;



The book passage I read at the top is from Chapter 7, ‚ÄúThe Ultimate Paperwork Shredder.‚Äù 



Public health officials and healthcare system leaders influence the well-being and health of people at the population level. They help shape people‚Äôs perceptions and responses to public health emergencies, as well as to chronic disease. They help determine the type, quality, and availability of treatment. All this is critical for maintaining good public health, as well as aligning better health and financial outcomes. That, of course, is the main goal of the concept of value-based care. AI can definitely have significant ramifications for achieving this.&nbsp;



Joining us today to talk about how leaders in public health and healthcare systems are thinking about and acting on this new generation of AI is Dr. Umair Shah and Dr. Gianrico Farrugia.&nbsp;



Dr. Umair Shah is a nationally recognized health leader and innovator. He led one of America‚Äôs top-rated pandemic responses as Washington State‚Äôs secretary of health, a position he held from 2020 to 2025. Umair previously directed Harris County Public Health in Texas, overseeing large-scale emergency response for the nation‚Äôs third-largest county, while building an emergency-care career spanning 20-plus years. He now advises organizations on health innovation and strategy as founder and principal of Rickshaw Health.&nbsp;



Dr. Gianrico Farrugia is the president and CEO of Mayo Clinic, the world&#8217;s top-ranked hospital for seven consecutive years, and a pioneer in technology-forward, platform-based healthcare. Under his leadership, Mayo has built and deployed the Mayo Clinic Platform. The platform enables Mayo and its partners to gain practical insights from a comprehensive repository of longitudinal de-identified clinical data spanning four continents. Gianrico is also a Mayo Clinic physician and professor and an author.&nbsp;



Umair and Gianrico are CEO-level leaders representing some of the best of the worlds of public health, healthcare delivery, medical research, and medical education.&nbsp;



[TRANSITION MUSIC]&nbsp;



Here is my interview with Dr. Umair Shah:&nbsp;



LEE: Umair, it&#8217;s really great to have you here.&nbsp;



UMAIR SHAH: Peter, it&#8217;s my pleasure. I&#8217;ve been looking forward to this conversation, and I hope you are well today.&nbsp;



LEE: [LAUGHS] I am doing extremely well.



So, you know, what I&#8217;d like to do in these conversations is first just to start, a little bit about you.



SHAH: Sure.&nbsp;



LEE: You served actually during a really tumultuous time as the secretary of health in the State of Washington. But you recently stepped away from that and you started your own firm, Rickshaw Health. So can we start there? What&#8217;s that all about?&nbsp;



SHAH: Yeah, no, absolutely. First of all, you know, I would say that the transition from Texas to Washington could not have been more geopolitically different, [LAUGHTER] as you can imagine.



LEE: Sure.&nbsp;



SHAH: You know, if you like the red-blue paradigms, you couldn‚Äôt be more, you know, red and you couldn‚Äôt be more blue, I think.&nbsp;



LEE: Yes.&nbsp;



SHAH: But what happened is, back in November this past year, as I saw some of the playout of continuation of this red-blue dynamic, I made the decision to step down. And Jan. 15, I stepped down, as you mentioned, and I spent some time really thinking about what I wanted to do next and was looking at a number of opportunities.&nbsp;



And then a moment in time, there were some things happening in our‚Äîmy wife and our family&#8217;s personal lives that sort of made me think that I wanted to focus a little bit more on family. And I felt the universe was saying, ‚ÄúStay still.‚Äù [LAUGHTER]&nbsp;



And I launched Rickshaw Health (opens in new tab) and the notion that, as you know, Peter, rickshaws are oftentimes known across the globe as these modes of transport that reliably get you through ever-changing streets and traffic patterns and all sorts of ecosystems that are evolving at all times. And they get you to the other side and they get you also with a sense of exhilaration. Like when I took my boys to Karachi, and we were‚Äîyou know, they jumped in a rickshaw and the, you know, open air [LAUGHTER] and they felt this incredible excitement.&nbsp;



And so Rickshaw Health was speaking to the three wheels of a rickshaw that symbolize the three children that we have and the real notion of how do we bring balance and agility and performance to the forefront and then move in an ever‚Äîjust like streets‚Äîever-changing healthcare environment that is constantly evolving, and we too must evolve with it. And that&#8217;s what Rickshaw Health is all about, is taking clients to that next level of trying to navigate, especially at this time, a very, very different landscape than even several months ago. So, excited about it.&nbsp;



LEE: Yeah, absolutely. You know, you made this transition from Texas to the State of Washington. And for people who listen to this podcast and don&#8217;t know, the particular part of Texas where you were‚ÄîHarris County‚Äîis really big, very, very important in that state. That&#8217;s just not, you know, the normal county in Texas.&nbsp;



SHAH: Yeah. [LAUGHS]&nbsp;



LEE: It&#8217;s actually ‚Ä¶ it&#8217;s actually known as quite a forward-looking place, technologically.&nbsp;



SHAH: That‚Äôs right.&nbsp;



LEE: So what was, you know, the transition like, then, going from, you know, possibly the most, sort of, maybe advanced county in the State of Texas, a large place, to the State of Washington?&nbsp;



SHAH: Yeah, you know, Harris County is the third-largest county in the US. So it had close to five million. And now it&#8217;s probably ‚Ä¶ it&#8217;s exceeded the five million people, and a very diverse, very forward-looking, as you mentioned, technologically very, very much looking at what&#8217;s the next horizon, and home to Texas Medical Center [TMC] as well, which is ‚Ä¶&nbsp;



LEE: Right.&nbsp;



SHAH: ‚Ä¶ the largest medical center. Of course, it had to be Texas. So it can‚Äôt be the largest in the state or the country [LAUGHTER]‚Äîthe largest in the world, right.&nbsp;



And TMC also had a number of different initiatives related to startups and venture capital and VC. And so they had launched something called TMCX. And that was a real opportunity‚Äîand I know you&#8217;re familiar with it‚Äîan opportunity to really look at how do you incubate all sorts of different innovations and bringing private sector, public sector as well as healthcare delivery alongside these startups to really look at the landscape.&nbsp;



And so when I left Houston and came to Washington, I realized that obviously, I was in the backyard ‚Ä¶ I mean, you know, you all at Microsoft Research and the work that you&#8217;re all doing is part of an ecosystem of advanced innovation that&#8217;s occurring in the Pacific Northwest that, you know, when we see all the players that are here, all the, you know, ones that do so many different things, but they&#8217;re doing them with an eye towards technology, advancements, and adoptions, it&#8217;s been quite amazing.&nbsp;



When I made that transition, it was really about, you know, the vaccines and what was happening with, you know, with COVID and fighting the‚Äîyou know, remember, this was the state that had the first case in the continental United States, had the first outbreak, and the first [lab-confirmed] death. And fast-forward a few years later, we had the fifth-lowest death rate in the US. And that was because we all came together to do so much.



LEE: Yeah, well maybe that gets us into a question that I ask a lot of our guests, which is, you know, and maybe let&#8217;s, since we&#8217;re on your time as the secretary of health in Washington State, [start] with that job. I ask, how would you explain to your mother what you do every day?&nbsp;



SHAH: [LAUGHS] I laugh because that&#8217;s been such a fascinating conversation in public health because we have oftentimes been‚Äîit&#8217;s been really hard to describe what that is.&nbsp;



LEE: Yeah.&nbsp;



SHAH: And, you know, there are so many metaphors and, you know, analogies that we&#8217;ve used. I&#8217;ve always wondered why we do not have more television shows or sitcoms or dramas that are about the public health workforce or the work that we do in the field, because you have, you know, all sorts of healthcare delivery ones, right.&nbsp;



LEE: Yup.&nbsp;



SHAH: As a practicing physician for 20 years, I realized that people knew what doctors did; they knew what nurses did, right. They intimately touch the healthcare system.&nbsp;



LEE: Yes.&nbsp;



SHAH: They understood, you know, that an ambulance picks you up at your home or somewhere else, transports you ‚Ä¶ gets you to the emergency department. The emergency department, they do some things to you or within the four walls of that ER, and then you&#8217;re either admitted, sent home, and several days, weeks, whatever later, you get home if you&#8217;re admitted, and you start your, you know, post-hospital stay at home or your rehab or what have you. And that all is known to people.&nbsp;



But when you ask your mother, your grandmother, or your, you know, your uncle, or your brother, your neighbor, your coworker about what is public health, they have a very quizzical look on their face of what that is.&nbsp;



LEE: Right.&nbsp;



SHAH: And so what I&#8217;ve ‚Ä¶&nbsp;



LEE: You know, just one thing I&#8217;ve learned is: it&#8217;s not just all the people you mentioned. Even healthcare professionals sometimes have that quizzical look.&nbsp;



SHAH: Yeah, good point. That&#8217;s right. Good point. And a lot of it is because we don&#8217;t get exposed to it or trained in it. You know, we think about public health when we&#8217;re in our training. And, you know, I&#8217;m sure you had a very similar piece of this is that, you know, you see it as, oh, that&#8217;s the health department that takes care of, you know, STDs, or it takes care, you know, it does the immunizations, or, you know, maybe they do some water quality, or maybe they do mosquitoes [mosquito control], and things like that. But the reality is, we do all of those things and more.&nbsp;



So my metaphor has been that we are the offensive line of a football team, and the healthcare delivery is the quarterback. So everybody focuses on, you know, from a few years back, everybody knows Tom Brady, right.&nbsp;



LEE: Yeah. [LAUGHS]&nbsp;



SHAH: He won the Super Bowls, everybody knows what ‚Ä¶ but if you asked people who was number 75 on the offensive line of the New England Patriots ‚Ä¶&nbsp;



LEE: Right.&nbsp;



SHAH: ‚Ä¶ or name your favorite football team. And the answer would be: you would not be able to likely answer that question. You would know Tom Brady, the quarterback, and that&#8217;s healthcare delivery, the ER doc or the hospitalist or the nurse or the, you know, the medical assistant, or the people that are doing all the work in the field that are the ones that are more visible, but the invisible workforce of the offensive line, that&#8217;s who we don&#8217;t know. And yet these are the people that are blocking and sweating and doing all things to complement the work and make sure the quarterback is successful.&nbsp;



And here&#8217;s where the metaphor breaks down, that when Tom Brady wins the Super Bowl, we continue to invest in the offensive line because we recognize the value of it and we want the quarterback to be successful the next season. But in public health or in society, we do the exact opposite.&nbsp;



When tuberculosis rates come down, we say, well, you know what? We&#8217;ve solved the problem; we don&#8217;t need it anymore.&nbsp;



LEE: Right.&nbsp;



SHAH: Or you have another, you know, environmental issue that&#8217;s no longer there, you say, ‚ÄúWe don&#8217;t need it anymore.‚Äù And we disinvest from public health or that offensive line. And then you start to see those rates go back up.&nbsp;



And so my answer to Mom and Grandma and Dad and Grandpa is we are critical to your health because we touch you every single day. And so please invest in us.&nbsp;



LEE: Yeah. And, you know, I think I&#8217;m going to want to get a little deeper on that in just a few minutes here, because, I think especially during the pandemic, that issue of not understanding the importance of that offensive lineman actually really came to the forefront.&nbsp;



And so I&#8217;d like to get into that. But the, kind of, second, kind of, standard thing I&#8217;ve been probing with people is still just focusing on you and your background is what touchpoints or experiences you&#8217;ve had with AI in the past.&nbsp;



And not everyone has. Like, it maybe isn&#8217;t too surprising that doctors and healthcare developers, tech developers, have lots of contact with AI, but would the top dog, you know, at a public health agency ever have had significant contact with AI? What about you?&nbsp;



SHAH: You know, it&#8217;s interesting. Several years ago, I was in the audience with the [then] FEMA director, [Rich Serino], who just did such an incredible job. And I remember he made this comment at that time. And, Peter, this may have been like ‚Ä¶ I don‚Äôt know‚ÄîI&#8217;m dating myself‚Äî10, 15, maybe even 20 years ago, and he said, ‚ÄúEverybody in the audience, there&#8217;s this, you know, app called Twitter.‚Äù And, you know, ‚ÄúHow many people in the audience have ever sent a tweet or know about this?‚Äù And I don&#8217;t know, maybe‚Äîit was a public health audience‚Äîmaybe about 15% of the people raised their hands.&nbsp;



He said, ‚ÄúI challenge you to right now, pick up your phone, download the app, and go ahead and send a tweet right now.‚Äù&nbsp;



And I remember I sent my first tweet at that time. And it was so thought provoking for me was that he was saying you need to be engaged in social media, but the other 85% of the audience had not even done that or had ‚Ä¶&nbsp;



LEE: Right.&nbsp;



SHAH: ‚Ä¶ even understood the importance of social media at that time. Or maybe they understood, but they had restrictions on how to utilize, right.&nbsp;



So that has stayed with me because that&#8217;s very much about this revolution of AI that I know that public health and population health practitioners like myself who have been in the trenches and understand the importance of it, they really believe in the importance or think they know the importance.&nbsp;



But NACCHO, the National Association of County and City Health Officials, had done a survey of local health agencies. And about two-thirds, if not three-quarters, of local health agencies reported that they had an AI capacity that was low or lower than ideal.&nbsp;



LEE: Hmm. Yeah. Yeah.&nbsp;



SHAH: And that is very much where I come from. When I was in public sector and at the state health agency, our transformation was very much about how do we advance the work, and how do we utilize this in a population health standpoint?&nbsp;



And I was fortunate to have a chief of innovation at Washington State Department of Health, Les Becker, who understood the value of AI. And as you know, we did also hold a AI science convening that ‚Ä¶&nbsp;



LEE: Yep. Yeah.&nbsp;



SHAH: ‚Ä¶ your team was there with University of Washington. And that was really an opportunity for us to say that AI is here. It&#8217;s not tomorrow. It&#8217;s not next year. It&#8217;s not the future. It&#8217;s already here. We need to embrace it.&nbsp;



But here&#8217;s the problem, Peter, far too few people in our field understand just how to embrace it.&nbsp;



LEE: Right.&nbsp;



SHAH: So I have become a markedly more champion of AI. One, since I read your book. So I think there‚Äôs that. So thank you for writing it. But two, since I really recognize that when I became a solo or a primary-few practitioner in my own realm, I needed to force-amplify the work that I was doing.&nbsp;



And when I look back, and I continue to stay in touch with my colleagues in the field of public health, what they&#8217;re also struggling with is that you have an epidemiologist who&#8217;s got a mound of information‚Äîdata, statistics, etc.‚Äîthat they are going through, and they&#8217;re doing everything in their power to get that processed and analyzed.&nbsp;



LEE: Yep. Yep.&nbsp;



SHAH: AI can take 80% of that and do it. And that epidemiologist can now turn to more of an overseer and a gatekeeper and to really recognize the patterns ‚Ä¶&nbsp;



LEE: Yep.&nbsp;



SHAH: ‚Ä¶ and let AI be able to do the, you know, grunt work. And similarly, as you know, measles‚Äîwith the outbreaks that we&#8217;ve seen, especially in Texas but elsewhere‚Äîyou&#8217;ve got an opportunity where our communications people who are saying, ‚ÄúLook, we&#8217;re about to have, or we know we&#8217;re about to announce that there&#8217;s a measles outbreak in, you know, in our community or our state or what have you‚Äîour region.‚Äù&nbsp;



And they can have AI go through different press briefings and/or press releases and say, ‚ÄúGive me the state of the art on how I should communicate this message to the community.‚Äù&nbsp;



LEE: Hmm.&nbsp;



SHAH: And bam! You can do that. And now you can oversee that work, as well. And then the third example is that we are always looking at how do we find ways to have a deeper connection with those who come to our, you know, our websites or come to our engagement tools‚Äîwith bots and things like that. AI can really accelerate that work, as well. So there&#8217;s so many use cases that AI has for population health or public health.&nbsp;



LEE: Yeah.&nbsp;



SHAH: But I think the challenge is that we just don&#8217;t have enough adoption because they&#8217;re ‚Ä¶ one, we&#8217;ve had funding cuts, but two is that there is this real hesitation on, what is it that we can do? And I argue‚Äîthe last thing I&#8217;ll say about this, Peter‚Äîis that I argue that AI is happening right now. The discussions, the technology advancements, the work, the policy work, all that&#8217;s happening right now. If public health practitioners are not at the table, if they&#8217;re not part of the, ‚Ä¶&nbsp;



LEE: Right.&nbsp;



SHAH: ‚Ä¶ &#8220;What does this look like? How does it work in our field?&#8221; &#8230; guess what? It&#8217;s going to be done to us and for us rather than with us. And if we do not get with that and get to the table, then unfortunately it may not be exactly what we want it to be at the end of the day.&nbsp;



LEE: I find it really interesting that you are using the terms ‚Äúpublic health‚Äù and ‚Äúpopulation health‚Äù ‚Ä¶&nbsp;



SHAH: Yeah.&nbsp;



LEE: ‚Ä¶ pretty much interchangeably here. And I think that that&#8217;s something that I think touches on an assumption that was both implicit and explicit in the book that we wrote, which is: we were making some predictions that our ability to extract insights and knowledge from population health data would be enhanced through the use of AI. And I think that it looks to me like that has been more challenging and has come along more slowly over the past two years. But what is your view?&nbsp;



SHAH: Yeah, I think part of, and I think you and I have had this conversation, you know, in bits and pieces. I think one of the real challenges is that when even tech companies, and you can name all of them, when they look at what they&#8217;re doing in the AI space, they gravitate towards healthcare delivery.&nbsp;



LEE: Yes.&nbsp;



SHAH: Right? That&#8217;s, it&#8217;s ‚Ä¶&nbsp;



LEE: And in fact, it&#8217;s not even delivery. I think techies‚ÄîI did this, too‚Äîtend to gravitate specifically to diagnosis.&nbsp;



SHAH: Yes, that&#8217;s right. That&#8217;s right. You know, I think that&#8217;s a really good point. And, you know, when you look at sepsis or you look at pneumonia or try to figure out ways that, you know, radiologists or x-rays or CT scans can be read, it&#8217;s, I mean, there are so many use cases that are within the healthcare sector. And I think that gets back to this inequity that we have when we look at population health or, you know, this broad, um, swath of land that is, oftentimes, left behind or unexplored, and you have healthcare delivery. Now, healthcare delivery we know gets 95 cents or 96 cents of every dollar. So it makes sense why, right. But we also know that, at the end of the day, we&#8217;re looking at value-based outcomes, and you cannot be successful in the healthcare delivery system unless we are truly looking at prevention and what&#8217;s happening in the community and the population.&nbsp;



LEE: Right.&nbsp;



SHAH: And that&#8217;s why I use it interchangeably, but I know that ‚Äúpublic health‚Äù has got a very specific term, and ‚Äúpopulation health‚Äù is a different set of ways of looking at the world. The reason that people try to shy away from pop health in essence is that you could talk about population health as being my population of patients in a clinic. It could be my health systems population. It could be an insurance company saying, these are the lives covered, right. So it becomes, what is population? When we think of public health, we think of the entirety of the population, right. In the State of Washington, eight million people. Harris County, five million people. Or in the US, 300‚Äîwhatever the number of millions of people that‚Äîwe think of the entire population. And what is it that actually impacts the health and well-being of that population is really what that&#8217;s about.&nbsp;



Yet here&#8217;s the challenge. When we then talk to those of our partners and our colleagues in the tech field, there are two things happening. One is, there&#8217;s a motivation because of the amount of dollars that are in [the] healthcare sector. And number two is, because it&#8217;s more familiar, right.&nbsp;



LEE: Right.&nbsp;



SHAH: And so there are very few practitioners similar to me that are out there, that are in the pop health who kind of know healthcare delivery because they&#8217;ve also seen patients, but they&#8217;re also‚Äîthey worked at that federal, state, local level, community level‚Äîthey&#8217;ve, you know, they&#8217;ve done you know various different kinds of environments.&nbsp;



And they say, ‚ÄúLook, I&#8217;ve got a perspective to really help a tech company or somebody see the rest of it,‚Äù but you have to have both partners coming together to see that. And I think that&#8217;s one of the real challenges that we have.&nbsp;



LEE: Yeah.&nbsp;



And so now I&#8217;m going to want to go into specific problems, ‚Ä¶&nbsp;



SHAH: Yeah. Sure.&nbsp;



LEE: ‚Ä¶ and maybe COVID is a good thing to focus on‚Äîthe breadth of problems that had to get solved in pandemic response and where the gaps between healthcare delivery and public health were really exposed.&nbsp;



And so the first problem that I remember really keenly that just seemed so vexing was understanding where the PPE was, the personal protective equipment &#8230;&nbsp;



SHAH: Hmm. Yeah.&nbsp;



LEE: ‚Ä¶ and where it needed to be.&nbsp;



SHAH: Yes.&nbsp;



LEE: And so that turned out ‚Ä¶ you would think just getting masks and gowns and gloves to the right places at the right times or even understanding where they are so that, you know ‚Ä¶ and being able to predict, you know, what hospitals, what clinics are most likely to get a big influx of patients during the height of the pandemic would be something that would be straightforward to solve, but that turned out to be an extremely difficult problem.&nbsp;



But how did it look from where you were sitting? Because you were sitting at the helm having to deal with these problems.&nbsp;



SHAH: Yeah, we were constantly chasing data and information. And oftentimes, you know, because a lot of these data systems in the public health sector have been underinvested in over the decades, then, you know, you had our biggest emergency crisis of our time, and a lot of public health agencies were either getting, you know, thrown a whole host of resources or had to create things on the fly.&nbsp;



And whether that was at Harris County or in the State of Washington, I will tell you that what I saw was that, you know, a lot of agencies across the country were still using fax machines, you know, to get data that were coming in.&nbsp;



And I remember actually‚Äîit&#8217;s kind of a funny story‚Äîthere was a fax machine that was highlighted down in our agency in Texas. And we actually had this fax machine, had mounds of, you know, data ‚Ä¶ sorry, papers that were next to ‚Ä¶ faxes that were coming in and all these things.&nbsp;



And you would have, you know, Mr. Peter Lee listed as a patient. And then the next, you know, transmission would have Pete Lee. And then the next transmission would have Peter Lee, but instead of L-E-E, it was L-E-A-H or something, or L-I or something, right. And it was just ‚Ä¶&nbsp;



LEE: Right.&nbsp;



SHAH: &#8230; or you had a date of birth missing, or you had, you know, an address that was off. And what we realized is that over time, a lot of the data that were coming in were just incomplete data, and being able to chase that was really hard.&nbsp;



And so, you know, I think AI has that potential to really organize it, and to stratify it, and to especially get you to a point of at least cleaning it up. So I don&#8217;t think it&#8217;s just that AI ‚Ä¶ AI doesn&#8217;t just save time; it saves lives. Truly used ‚Ä¶&nbsp;



LEE: Hmm. Yeah.&nbsp;



SHAH: ‚Ä¶ that&#8217;s, I think, where we&#8217;re talking here.&nbsp;



And so when you have PPE and things of that nature, as you talked about, here in the State of Washington or what we were trying to do to get vaccines out or everything we&#8217;re doing to try to get communication messages to the public. And we did a fantastic job of that, although not ideal.&nbsp;



I mean, there are so many things that I could point to that we could have done better‚Äîall of us in the field of public health and healthcare delivery alike.&nbsp;



I will tell you that the one thing that stays with me is that if we had those tools then, and we had them in place then, and we had invested in them at that time in advance of, I think there was a real opportunity for us to be able to move ahead and even be better at how we affected the health outcomes of the very populations that we were trying to get to.&nbsp;



And I think it&#8217;s [that] AI allows us to shift from reactive to proactive systems, catching health issues before they escalate and allow us to really communicate with empathy at scale.&nbsp;



LEE: Right.&nbsp;



SHAH: And when we can do those things, whether it&#8217;s opioids or whether it&#8217;s, you know, something that&#8217;s happening related to an infectious disease, or, you know, even this, the new agenda with Make America Healthy Again‚Äîwhich by the way, as you know, we had a Be Well, WA &#8230; Be Well, Washington ‚Ä¶&nbsp;



LEE: Right. Yes.&nbsp;



SHAH: ‚Ä¶ very much that was about, you know, looking at, you know, physical health and nutritional health and emotional well-being and social connectedness‚Äîthat there is a real opportunity for us to address the very drivers of ill health. And when we can do that, and AI can help us accelerate that, I think we truly have the ability to drive down costs and increase the value that&#8217;s returned to all of us.&nbsp;



LEE: What is your assessment of public health agencies‚Äô readiness to use technology like AI? Because if there&#8217;s one thing AI is good at, it&#8217;s predicting things. Are they [public health agencies] in a better position to predict things now?&nbsp;



SHAH: You know, I think it&#8217;s a tale of two cities.&nbsp;



I think on the one hand, we&#8217;re better because we have the tools. On the other hand, we&#8217;ve lost the capacity to be able to utilize those tools. So, you know, it&#8217;s a plus and a minus.&nbsp;



Many, many years ago, there was the buzzword of what we called syndromic surveillance. And, Peter, you know this term well.&nbsp;



It was like you would have, you know, a whole host of accumulation of data points in, let&#8217;s say, a hospital setting or an emergency department ‚Ä¶&nbsp;



LEE: Yup. Yup.&nbsp;



SHAH: ‚Ä¶ where, you know, you‚Äôd have runny nose, you&#8217;d have cough, you&#8217;d have a fever, and you would take that, what was happening and people presenting to the emergency department, with what was happening in the area pharmacies where people were going to get Kleenexes and tissues ‚Ä¶&nbsp;



LEE: Yep.&nbsp;



SHAH: ‚Ä¶ and buying over-the-counter, you know, medication, and things of that nature, Tylenol, etc.&nbsp;



And you would say ‚Ä¶ you would put those two things together, and you would come up with a quote-unquote ‚Äúsyndrome,‚Äù and you would say our ability to say there was an alert to that syndrome allows us to say something uh-oh is going on in the community, and we got many, many advancements related to wastewater surveillance over the last several years as you know ‚Ä¶&nbsp;



LEE: Yep. Yep. Well, also, wasn&#8217;t patient number one in the United States discovered also because of the Seattle Flu Study, or at least that sort of syndromic surveillance.&nbsp;



SHAH: That&#8217;s right.&nbsp;



LEE: They weren&#8217;t even looking for COVID. They were just taking, you know, snot samples from people.&nbsp;



SHAH: That&#8217;s right. That&#8217;s right. That&#8217;s right.&nbsp;



And so that&#8217;s the kind of thing that you, you know, we underappreciate. Is you have to have a smart, intelligent, agile practitioner, right.&nbsp;



So if I think about down in Dallas when Ebola was, you know, the gentleman who was, you know, the index case for Ebola was sent out of the emergency department and came back several days later.&nbsp;



And it was the nurse who picked up this time because the practitioner, the provider, the healthcare provider, the doc missed it. And I wouldn&#8217;t want to say in a negative way. It was just, like, not obvious. You aren&#8217;t thinking of Ebola in the middle of Texas. And it was the nurse who picked up: there&#8217;s something wrong here.&nbsp;



And what AI has the ability to do is to pick up those symptoms &#8230;&nbsp;



LEE: Yeah.&nbsp;



SHAH: ‚Ä¶ or those patterns and be able to recognize the importance of those and be able to then alert the practitioner. So what I ‚Ä¶ we call it artificial intelligence‚Äîit almost becomes artificial wisdom.&nbsp;



LEE: Hmm. Yeah, interesting. So that actually reminds me of my next question, which is another thing that I watched you and public health officials do is try to play ‚Äúwhat if‚Äù games.&nbsp;



So, for example, I think one decision you were involved in had to do with, you know, what would be the impact if we put a ban on large gatherings like concerts or movie theaters or imposed an 8 PM curfew on restaurants, and you were trying to play ‚Äúwhat if‚Äù games. Like, what would be the impact on the spread of the pandemic there?&nbsp;



So now, again, today with AI, would that aspect of what you did play out differently than it did during the pandemic?&nbsp;



SHAH: As you know, COVID was the most studied condition on the planet at one point. And it was, you know, things that usually we would learn over years or months, we were learning in weeks or days or hours.&nbsp;



And I remember in Houston, I would say something in the morning, and I would always try to give the caveat, ‚ÄúThis is the best information we know right now,‚Äù because it kept changing, whether it was around masks or whether it was around, you know, the way the virus was operating, whether it was around &#8230;&nbsp;



I remember even ‚Ä¶ I was just watching something recently where I was asked to comment about whether spiders could transmit COVID-19. You know, just questions that were just evolving, evolving, evolving. And the information was evolving. By morning, you would say something. By evening, it would change.&nbsp;



And why I say that is that it would have been great in the pandemic if we could have said, if you could give us all the information that&#8217;s happening across the globe, synthesize that information, and be able to help us forecast the right decisions that we should be making and help us model that information so we could decide: if you did a curfew, or if you did, you know, a mask, or if you could, you know, change something else related to policy‚Äîwhat are the impacts of it?&nbsp;



LEE: Yeah.&nbsp;



SHAH: What we found constantly in public health was that we were weighing decisions in incomplete data, incomplete information.&nbsp;



So great now that everybody can armchair quarterback looking back three, five years ago and say, ‚ÄúI would have done it this way,‚Äù or ‚ÄúI would have done it that way.‚Äù Gosh, I would have as well. But guess what‚Äîwe didn&#8217;t have that information at that time. And so you had to make the best decisions you could with incomplete data.&nbsp;



But what AI has the potential to do is to help complete the incomplete data. Now, it&#8217;s not going to get 100%.&nbsp;



LEE: Right.&nbsp;



SHAH: And I think, Peter, you know, the one thing we&#8217;ve got to be really mindful [of] is phantom information, or information where it sort of makes up things, or may somehow get you incomplete information, or skews it a certain way.&nbsp;



This is why we can&#8217;t take the person out of it yet.&nbsp;



LEE: Right.&nbsp;



SHAH: Now, maybe one day we can.&nbsp;



I&#8217;m not one of those Pollyanna-ish that people will never be replaced. I actually believe that those people who are skilled with AI and the tools will eventually have a competitive advantage over those who are not. Just like if I had a physician who knows how to use their smartphone or knows how to use a word processor or knows how to do a PowerPoint presentation is going to replace the ones that use scantrons &#8230;&nbsp;



LEE: Yeah. Yeah.&nbsp;



SHAH: ‚Ä¶ or the ones that write it on pieces of paper‚Äîthat eventually it makes it more efficient and effective, but we&#8217;re not there yet. But I think that the potential is absolutely there.&nbsp;



LEE: So I have one more question. And you can, kind of, tell I&#8217;m trying to expand people&#8217;s understanding of just the incredible breadth of what goes on in public health, you know, all of these sorts of different issues.&nbsp;



And again, just sticking to COVID, but this is a much broader issue. Another thing you had to cope with were significant rise of misinformation ‚Ä¶&nbsp;



SHAH: Yes.&nbsp;



LEE: ‚Ä¶ and maybe going along with that, very, very significant inequities in outcomes in the COVID response. And when you think about AI there, I think you can argue it both ways, that it both exacerbates the problem but also gives you new tools to mitigate the problems.&nbsp;



What is your view?&nbsp;



SHAH: I think you ‚Ä¶ I don&#8217;t even have to say it ‚Ä¶ I think you hit on it, is that, you know, it really is two sides of one coin.&nbsp;



On the one hand, it has the power of really advancing and allowing us to move forward in a way that incredibly accelerates and accentuates, but on the other hand, in the case of inequities, right? So if you have inequitable information data that&#8217;s already out in the literature or already out in the, you know, media, or what have you, about a certain population or people or certain kinds of ideas or thoughts, etc., then AI will tend to accumulate that. You&#8217;re going to take that information, thinking that&#8217;s the best out there, but it may have missed out on information and now you go with it. And that&#8217;s a potential problem.&nbsp;



And I think it&#8217;s the same thing on information is that when we have people that are able to classify or misclassify information, I think it really becomes hard because it can accelerate the inequities of trust or inequities of trusted sources of information. It can also close the gap.&nbsp;



So I think, you know, it&#8217;s really up to us and this responsible AI to really think about how we can go about doing this in a way that&#8217;s going to allow us to further the advancements but also be careful of those, you know, those kind of places where we&#8217;re going to step into that are not going to be well received or successful.&nbsp;



You know, the one thing that&#8217;s really fascinating about this whole conversation is that this is why we&#8217;ve got to be at the table, Peter.&nbsp;



LEE: Yep. Yep.&nbsp;



SHAH: Because if we&#8217;re not at the table, you know, what&#8217;s the, you know, or if tech companies that are out there doing this work and aren&#8217;t even seeing a field of practitioners that are actually wrestling with the same problems but just cannot actually get to the solutions, we&#8217;re just going to continue to accentuate the problems.&nbsp;



And that&#8217;s why I&#8217;m a firm proponent of: we&#8217;ve got to be at the table.&nbsp;



And so even when we&#8217;ve seen in, and this is going to be a little controversial, but governmental spaces where, you know, policymakers have said, ‚ÄúLook, we are not going to let you do certain things,‚Äù or they say to public health practitioners or even healthcare delivery practitioners in certain spaces, ‚ÄúYou cannot even play with this. You cannot have it on your phones. You can&#8217;t do any &#8230; ‚Äù&nbsp;



You know, what I really believe it does is that it takes [an] almost like we put our head in the sand type of approach rather than saying, ‚ÄúWhat is it that we can do to help improve AI and make it work for all of us?‚Äù What we&#8217;re doing is we&#8217;re essentially saying, ‚ÄúWe&#8217;re going to let the tech companies and all the other developers come up with the solutions, but it&#8217;s not going to be informed by the people in the field.‚Äù And that&#8217;s dangerous. We have to do both. We have to be working together.&nbsp;



LEE: Umair, that&#8217;s really so well said, and I think a great way to wrap things up. I&#8217;ve certainly learned a lot from this conversation. So thank you again.&nbsp;



SHAH: It&#8217;s been a pleasure to be with you this morning. Thank you so much for the time. And I&#8217;m looking forward to further conversations.&nbsp;



[TRANSITION MUSIC]‚ÄØ



I live in the State of Washington and because of that, I&#8217;ve been able to watch Umair in action as our state&#8217;s former secretary of health. And some of that action was pretty intense to say the least because his tenure as secretary of health spanned the period of the COVID pandemic.&nbsp;



Now, as a dyed-in-the-wool techie, I have to admit that at the beginning, I don&#8217;t think I really understood the scope and importance of the field of public health. But as the conversation with Umair showed, it&#8217;s really important and it is arguably both an underfunded and underappreciated part of our healthcare system.&nbsp;



Now, public health is also very much an area that&#8217;s ripe for advancement and transformation through AI. As Umair explained in our discussion, the core of public health is the idea of population health, the idea of extracting new health insights from signals from population-scale data. And already we&#8217;re starting to see AI making a difference.&nbsp;



Now here&#8217;s my interview with Dr. Gianrico Farrugia.&nbsp;



LEE: Gianrico, it&#8217;s really great to have you here today.&nbsp;



GIANRICO FARRUGIA: Peter, thanks for having me. Thanks for making me part of your podcast.&nbsp;



LEE: You know, what I&#8217;d like to do in these conversations is, you know, we&#8217;ll definitely want to talk about the overall healthcare system, the state of healthcare, and what AI could or might do to help or even hurt all of that. But I always like to start with a sharper focus just on you specifically. And my first question always is, you know, I think people imagine what a hospital or a health system president and CEO does, but not really. And so how would you explain to your mother what you do every day?&nbsp;



FARRUGIA: So, Peter, my mother‚Äôs 88 years old. She lives in Malta, and she‚Äôs visiting at the moment, ‚Ä¶&nbsp;



LEE: Oh, wow.&nbsp;



FARRUGIA: ‚Ä¶ which is kind of nice, really.&nbsp;



LEE: Wow, that is amazing.&nbsp;



FARRUGIA: I&#8217;m proud that she&#8217;s still proud of me. So she does ask. I&#8217;ll tell her the scope of Mayo Clinic. We serve patients across the globe. We have about 83,000 staff members that work with us, and we&#8217;re very proud of the work we do in research, education, and the practice.&nbsp;



Mayo Clinic is built to serve people with serious disease. So what I tell my mother is that here we are. We&#8217;re a healthcare organization that knows what it needs to do: keep patients as the North Star. The needs of the patient come first. We have 83,000 people who want to do that, several thousand physicians and scientists. My job is to look slightly ahead and then share what I&#8217;m seeing and then, sort of, smooth the way for others to make sure Mayo remains true to its mission but also true to the fact that at the moment, we are in a category of one. We need to remain there not just from an ego standpoint, but really from a ‚Äúdo good to the world‚Äù standpoint.&nbsp;



At that point, invariably my mother will tell me that I&#8217;m working too hard. [LAUGHTER] And then of course, I change the subject, and I ask her what she cooked today because my mother, who‚Äôs 88, cooks for the whole family in Malta, and there are usually four generations eating around the table. So I tell her what she does for the family is what I do for the Mayo family.&nbsp;



LEE: Wow, that&#8217;s a great way to put it. And it sounds like you actually have a good chance to have some good genes if she&#8217;s still that active at age 88.&nbsp;



FARRUGIA: I think I chose a little more stressful job that may limit [that]. I will tell you very briefly is that one of the AI algorithms we have estimates biological age from an electrocardiogram. My biological age jumped by 3.7 years when I became CEO.&nbsp;



LEE: [LAUGHS] Oh no.&nbsp;



FARRUGIA: I&#8217;m hoping it will reverse on the other side.&nbsp;



LEE: To stick with you just for one more moment here, second question I ask is about your origin story with respect to AI. And typically, for most people, there is AI before ChatGPT and generative AI and then after the generative AI revolution. So can you share a little bit about this? Because it must be the case that you&#8217;ve been thinking about this a long time since you&#8217;ve really led Mayo Clinic to be so tech forward in this way.&nbsp;



FARRUGIA: Well, I&#8217;ve been, as you said, a physician for way too long. I got my MD degree in ‚Äô87. So that sort of dates me. But it also means that I saw a lot of the promise for AI that never seemed to pan out for decades and decades and decades like you did.&nbsp;



LEE: Yeah.&nbsp;



FARRUGIA: Around 10 years ago, Mayo could sense that there was something different, that something was changing, that we actually‚Äîat that time, predictive AI‚Äîcould make a big difference. And I think that&#8217;s the moment where I and others jumped in and said Mayo Clinic needs to be involved.&nbsp;



And then about six years ago, when‚Äîsix and a half years ago‚ÄîI became CEO, it was clear that there was the right confluence of data, knowledge, tech expertise, that we could deal with what was increasingly bothering me, which is that we knew what was coming from a technology standpoint and we knew the current healthcare system could not deliver on what patients need and want within that current system. And so the answer is, how could a place like Mayo Clinic with our reputation not jump in and say there has to be a better way of doing things? I&#8217;ve always said that it is impossible for me to understand that every single government employee is incompetent. Every physician is greedy. Something&#8217;s wrong here.&nbsp;



LEE: Yeah.&nbsp;



FARRUGIA: And that wrong was the architecture was wrong. And we knew that we could incorporate AI and make it better. So for me, that journey was one of wait, wait, wait. 10 years ago, begin to jump in. Six years ago, really jump in with our platform. And then, of course, in November 2022, things changed again.&nbsp;



LEE: Yeah. When did this idea of a data platform, what you now call the Mayo Clinic Platform (opens in new tab)‚Äîby the way, I refer to this as MCP, ‚Ä¶&nbsp;



FARRUGIA: Yeah, I know. [LAUGHS]&nbsp;



LEE: [LAUGHS] ‚Ä¶ which I always smirk a little bit because, of course, for those of us in computer science research, the AI research, MCP has also become quite a hot topic because of the model context protocol version of this. But for Mayo&#8217;s MCP, when did that become a serious, defined initiative?&nbsp;



FARRUGIA: So around the end of ‚Äô18, 2018, beginning of 2019. At that point, we knew that we were going to do something differently. We came up with a strategic plan, as I took on the job, that we needed to cure more patients. There‚Äôs just not enough cures in the world. There&#8217;s too much suffering. And that we had all these chronic diseases that people have accepted are chronic, but really the only reason that disease is chronic is you haven&#8217;t cured it.&nbsp;



And physicians have been afraid to talk about cure because, of course, eventually everybody passes away.&nbsp;



LEE: Yeah.&nbsp;



FARRUGIA: But I really pushed hard to say, no, it&#8217;s OK to talk about cure. It&#8217;s OK to aspire to cure. The second was connect‚Äîconnecting people with data to create new knowledge. And that&#8217;s where it became clear that data were not currently in a format that were particularly useful. By the way, you&#8217;ll hear me talk about data in the singular and the plural. I&#8217;m old school. I talk about data as plural, but I know that most younger people now use data singular. [LAUGHTER] And I apologize if I&#8217;ll go through that.&nbsp;



And then the third was transform. Let&#8217;s use Mayo&#8217;s resources to transform healthcare for ourselves and for others. And that&#8217;s the concept of, if we are able to use data in a different way, let&#8217;s create a different architecture. And that architecture had to be very closely linked to using artificial intelligence in order to create better outcomes for patients. So patients can live not only longer lives but healthier lives. And that&#8217;s the genesis of MCP, Mayo Clinic Platform, so I&#8217;ll timestamp that as end of 2018, beginning of 2019.&nbsp;



LEE: So I&#8217;m really wanting to delve in in this episode, in this conversation, you know, [into the] mindset of a health system or hospital CEO. And so you&#8217;re obviously thinking about, I guess, machine learning and predictive analytics and so on. What were the, kind of, like ‚Ä¶ in 2018, what were the outcomes that you were dreaming about from this? So if you had this thing, you know, what were the things that you were hoping to be able to show or, kind of, produce as results?&nbsp;



FARRUGIA: So first of all, I think all of us who work at Mayo Clinic, and this tends to be a bit sugary, but it&#8217;s true, strongly feel that we have a responsibility to leave the place better than when we started. And so the Mayo brothers, when they started, did two really important things. The first was that they created the first integrated healthcare system. And the second, they created the first unified record. And that record was, of course, paper at that point.&nbsp;



Part of that is to say, OK, what does it look like now versus how can we improve what we have if ‚Ä¶ it&#8217;d be blasphemy to say, let&#8217;s think of ourselves as the Mayo brothers, but let&#8217;s think of ourselves as reasonably smart people at Mayo Clinic, really lucky to be surrounded by very smart people with resources. What will we do? And so we said let&#8217;s not aim for the low-hanging fruit. Let&#8217;s aim to get at whatever you want to call it, the intractable knot, the hardest problem, and that is clinical care. Let&#8217;s improve clinical care. Yes, we can deal with burnout. Yes, we can deal with administrative burden. But let&#8217;s not focus on that. Let&#8217;s really create an architecture that allows us to tackle better clinical outcomes.&nbsp;



And by starting there, then everything flows from that. That it&#8217;s not really worth doing unless at the end of the day, people are experiencing better health.&nbsp;



LEE: And so I know a very good colleague and friend of mine, John Halamka (opens in new tab), you ended up hiring. I thought he was a very interesting choice because he is, of course, in terms of technology, quite deep and very expert, but he&#8217;s, I think, first and foremost, a doctor. And so I assume you must have had to decide what type of person you would bring in and what kinds of people you would bring in to try to create such a thing. What was your thinking around the choice of someone like John?&nbsp;



FARRUGIA: It was one of the harder decisions. First of all, [I&#8217;m] a physician myself. We tend to want to maintain some control. And so now I am the CEO, [LAUGHTER] and I have to give this baby to somebody else. That&#8217;s very hard. Second is Mayo Clinic is really good because it is flat, and we run a lot by committee. But it also means that, therefore, you have to work really hard at change, and you cannot change by fiat. You have to change by convincing people.&nbsp;



So I just ‚Ä¶ I&#8217;ve always made the point that the right change agent is a servant leader because that&#8217;s how change becomes embedded. But it also means you&#8217;ve got to have that personality, the Mayo personality. And it became clear when we interviewed [that] there were some people that were really hardcore tech; others that were passionate about social issues. But John really fit that of being, as you said, deep in IT but also himself very aligned with the Mayo Clinic values. It&#8217;s as if he was a Mayo Clinic physician even though he wasn&#8217;t.&nbsp;



And that came together, and I felt, we felt, that as we were hiring, that we could do it. And then we did something interesting. We paired John with a ‚Ä¶ we created the role of a chief medical officer for the platform, which was a longstanding Mayo Clinic physician. And so we brought them together so we could get the past and the present and the future working together.&nbsp;



LEE: So I&#8217;m going to ask you about what has come out of this. But before that, let&#8217;s get back to this origin story. So now, all of that is being set up starting around 2018. But then, you know, in 2022, there is generative AI. Now you were already experimenting with transformers, starting with BERT out of Google there. So maybe that&#8217;s a couple of years earlier. But still, there has to come a point where things are feeling very disrupted.&nbsp;



FARRUGIA: Yeah, so, you know, it really wasn&#8217;t. It, to me, was a relief because it gave this ‚Ä¶ we were feeling pretty good about what we&#8217;re doing. We were feeling a little impatient, but, in true Mayo fashion, were willing to, sort of, do everything, take its time, take it to the right committees, get the right approvals, and get it done.&nbsp;



And so when generative AI came, for us, it&#8217;s like, I wouldn&#8217;t say we told you so, but it&#8217;s like, ah, there you go. Here&#8217;s another tool. This is what we&#8217;ve been talking about. Now we can do it even better. Now we can move even faster. Now we can do more for our patients. It truly never was disruptive. It truly immediately became enabling, which is strange, right, ‚Ä¶&nbsp;



LEE: Yeah.&nbsp;



FARRUGIA: ‚Ä¶ because something as disruptive as that instantly became enabling at Mayo Clinic. And I&#8217;ll take ‚Ä¶ as I think about it with you and take a moment to think and reflect on it, I think there were a couple of decisions we made earlier on that really helped us. We made the decision against the advice of any consulting firm to completely decentralize AI at Mayo Clinic six years ago. And we told our clinical department, you need to own this. You need to hire basic scientists in AI. We&#8217;ll help you by creating the infrastructure. We&#8217;ll help you by doing all the rest. We&#8217;ll have the compute. We&#8217;ll have the partners. You need to do this on your own. You need to treat this the same way as if a new radiological technique happened or a new surgical technique happened.&nbsp;



And so there was a lot of expertise already present in a very diffused way that then we were able to layer on generative AI onto that. And we found a very willingness to embrace it. In fact, I would argue initially a bit too willing because as you know, we haven&#8217;t quite figured out what&#8217;s legitimate use, what&#8217;s not use.&nbsp;We all learned together.



LEE: Right. Yep. Yep.&nbsp;



FARRUGIA: But it was mostly energy, which is really interesting. It was mostly energy.



LEE: Wow. And, you know, it&#8217;s an amazing thing to hear because one common theme that we hear is that the initial reaction is oftentimes one of skepticism. In fact, I&#8217;ve been very open that even I initially had some skepticism. Was that not present in your mind or on your team&#8217;s mind at all at the beginning?&nbsp;



FARRUGIA: So you&#8217;re asking a physician if they are skeptical about something. [LAUGHTER] Yeah. I wonder what the answer to that is. Absolutely. The first hallucination, the first wrong reference. Can you imagine if you write the grant and the wrong reference comes. As you know, ‚Ä¶&nbsp;



LEE: Right.&nbsp;



FARRUGIA: ‚Ä¶ earlier on when some references were being made up. So massive amounts of skepticism. But the energy was such there that the people [who] were skeptical were also at the same time saying, ‚ÄúLet&#8217;s do a RAG [retrieval augmented generation] to clean up those references. Let&#8217;s create ‚Ä¶‚Äù We were experimenting with discharge summaries, but let&#8217;s use AI to police AI, and let&#8217;s see what&#8217;s going on. So there was more massive skepticism, but the energy was pushing that skepticism into a positive versus into a negative frame. Now, I say that summarizing in hindsight.&nbsp;



LEE: Yeah.&nbsp;



FARRUGIA: Day to day, much more complicated than that. But overall, if you just ‚Ä¶ and remember, I had been at the World Economic Forum many years ago and had said, healthcare needs to run towards AI.&nbsp;



LEE: Yes.&nbsp;



FARRUGIA: If healthcare was perfect, we would wait. Healthcare is not perfect by any means, therefore let&#8217;s run and embrace AI. And, sort of, that mentality was part of who we were because at the same time, we were also saying the other thing, that we need to be the ones to lead validation. We need to be the ones that set the rules. We need to be participating in the creation of CHAI [Coalition for Health AI] (opens in new tab). We need to be participating as the [National] Academy of Medicine (opens in new tab).&nbsp;



So people did feel that Mayo was being fairly responsible about it, but that urge to, the needs of the patient come first, was the driver that kept people wanting to say, ‚ÄúNot ready yet, but let&#8217;s make it ready.‚Äù And we now have 320 algorithms in the practice, and they run and we constantly are looking and seeing what else we can do to improve. But as you well know, things evolve and change. And we&#8217;re also looking and seeing which ones work and which ones don&#8217;t and which ones we have to work together on to make better.&nbsp;



LEE: Yeah, you know, of course Mayo has such a, you know, such a reputation and is so influential, but in the world of healthcare broadly, let&#8217;s just focus on the United States to start. How common is this experience? You know, so if you are at a meeting with fellow CEOs of hospitals and health systems, what is the attitude and what is the, kind of ‚Ä¶ how common is the approach to all of this?&nbsp;



FARRUGIA: I think it&#8217;s more common now, but going back a few years, I think it&#8217;s fair to say that it was scary for people to know how it&#8217;s going to change things. Healthcare runs on very narrow margins. It&#8217;s very expensive. So your expenses and your revenue are both massive, and they are very close to each other. So anything that changes that balance is really scary.&nbsp;



Because it&#8217;s not like you have the opportunity to erode into a margin or get it right the second time. So I think that is what drove a lot of the initial hesitancy. Was, one, is lack of knowledge and, two, understanding that you didn&#8217;t have a lot of room to make a mistake.&nbsp;



LEE: On the economics of this, when you are embarking on what I suspect is a very expensive initiative like Mayo Clinic Platform, how on earth do you justify that early on?&nbsp;



FARRUGIA: So again, I&#8217;m trying hard to try and remember how things were versus how I think about them now. [LAUGHTER] It goes back to our history. Mayo has always invested in what it thinks is the right thing that is coming. And that&#8217;s how we&#8217;ve stayed where we are. So the investment really was having an open discussion: is this worth it for our patients? And once that discussion was over, then the board was saying, go, go, go.&nbsp;



Now we are lucky in that we have the size that we&#8217;re able to hire and absorb. We&#8217;re lucky in that the people [who] came before us have been financially astute, and one of our values is stewardship. And we&#8217;re lucky that we had a lot of patients at Mayo Clinic who were able to listen, be inspired by, and be willing to help support. And so that gave us the ability to build what we&#8217;re doing not only into the long-range plan but actually into the yearly plan. And so we built it into the yearly plan. We set up a center for digital health. We set up the platform. And then we set up the budgets to be able to do that. And the budgets came from assets we&#8217;ve had, assets that we would get as the year came by, and then from philanthropy.&nbsp;



We also had a really powerful calling card. And that&#8217;s one advantage I had, and that&#8217;s ‚Ä¶ and I&#8217;d been very open when I was speaking to other CEOs that would use it is that right at that very beginning, really, really in 2019, our cardiologists, both the researchers and the clinicians, had come together and had used electrocardiograms to create an AI algorithm.&nbsp;



The first one was for diagnosing from an electrocardiogram, which is very cheap, very easy to do, left ventricular dysfunction. That&#8217;s how hard the left part of the heart contracts. If it doesn&#8217;t do well, you get heart failure. And they were able to show that that algorithm was already making nurses better than the physician without the algorithm. And after that went on to show that you could do it from a single strip, really with an area under the curve for that single strip on a watch, that was as good as mammograms or pap smears. And so we already had that proof.&nbsp;



LEE: Yeah.&nbsp;



FARRUGIA: That quickly then came into Mayo. We put it into it so that any patient now can benefit from it. And now there are, I think, 14 algorithms just from that same one.&nbsp;



LEE: Yeah.&nbsp;



FARRUGIA: So we had a proof of concept thanks to those really far-seeing cardiologists that enabled things to happen a little faster and also, as I talked to other CEOs, enabled me to say, ‚ÄúThis actually works. This is the path forward.‚Äù I have recently been vocal about also saying, we are at a point now where I believe that for some medical conditions, it is not right to not use AI to help treat them.&nbsp;



LEE: Wow, that&#8217;s so interesting. So I think I want to get into another topic here, which is when you think about the use of AI and data, what are some of the results that maybe are top of mind for you or you think are particularly important? And if you don&#8217;t mind, I&#8217;d like to see if we can think about this not only in terms of results in terms of patient outcomes but in your other activities, core activities, like research, in the education mission, and then even in the broader impacts on the healthcare system. But maybe we start with on patient outcomes.&nbsp;



FARRUGIA: Yeah, they&#8217;re all linked, right.&nbsp;



LEE: Yes.&nbsp;



FARRUGIA: They&#8217;re part of the same ecosystem. We think of ourselves as three shields‚Äî research, education, and the practice‚Äîand that one goes into the other. So, as I said, we have about 320 AI algorithms from the practice. Some run on every patient; some run on some patients. And we have good evidence for what they do. So some specific examples, and then I&#8217;ll get into the transformer part of this.&nbsp;



We have a program called CEDAR [Clinical Detection and Response (tool)], and like most other people, I like acronyms for things. [LAUGHTER] But what it is, in our hospitals with patient consent, we monitor vitals. We monitor in the patient room‚Äînot in the ICU [intensive care unit], in the patient room. We monitor all sorts of things. But there&#8217;s a camera in the room, and we have a team of intensivists‚Äînurses and physicians‚Äîwho do not have any patient responsibilities but are just monitoring the algorithms, and when the algorithms are predicting decompensation, they&#8217;re able to get into the room. And what we&#8217;ve shown, for example, with that algorithm, is we&#8217;ve shown we&#8217;ve decreased length of stay in the hospital, decreased transfers into the intensive care units, and interestingly, decreased mortality and morbidity, which is not easy to show. I talked about the electrocardiogram as a good example. Of course, everybody knows about the radiology things.&nbsp;



We&#8217;ve created ‚Ä¶ taken part of this and said, if we can do this in the hospital, why cannot we do it in patients&#8217; homes? So being very active in looking after patients that would come to the ED, emergency room, would normally be admitted, and we say, no, here are the things we can give you. Go home if you want to, and we will safely look after you at your home. And we recently have been, looking at the last two years of data, been able to show that we&#8217;re also successfully able to give intravenous chemotherapy in patients&#8217; homes because we can monitor; we can do all the things that we can do.&nbsp;



Now, with generative AI, that gave us many other opportunities. One biggest opportunity for me has always been digital pathology. When we see how pathology‚Äôs currently run with a glass slide, not much has changed ‚Ä¶&nbsp;



LEE: Yeah.&nbsp;



FARRUGIA: ‚Ä¶ in many, many, many years, right.&nbsp;



LEE: Yeah.&nbsp;



FARRUGIA: And so really we have made a massive push to digitize pathology not just for us but for others. But talking about ourselves, we started by saying, it has to be very cheap to digitize. So we worked and created a company with partners called Pramana (opens in new tab) that allows us to digitize slides relatively cheaply using AI algorithms that can take away the dirt, the fingerprint. And so we end up with 21 million of our slides digitized, and that gives you now a massive opportunity. Worked with another company called Aignostics (opens in new tab) to create a, what we call, Atlas (opens in new tab), which is an LLM that allows us to then build upon it.&nbsp;



LEE: Yeah.&nbsp;



FARRUGIA: And we, a hundred and, I think, 120 years ago, invented frozen sections at Mayo Clinic. So what that is, is that while the patient&#8217;s still on the table, you can take a piece of tissue, look at it, and tell the surgeon the margins of what you&#8217;re trying to resect are clear or not. But as a result of that, because you have to hurry, you get no information as a surgeon about, is it an invasive cancer, is it noninvasive cancer, or other things. So we&#8217;ve just found a way to digitize our frozen section practice and will completely go across the enterprise with AI-enabled digitized frozen sections, which then enables us to then do it for anybody across the globe if we need to.&nbsp;



And then in the genomic space, we&#8217;re working to create a true exomic transformer that is short range. And we originally started doing it to see if we can test it against the fact that 40% of people with rheumatoid arthritis don&#8217;t respond to the first-line therapy, ‚Ä¶&nbsp;



LEE: Right.&nbsp;



FARRUGIA: ‚Ä¶ but you have to wait six months to find out. And we found that we can actually do that. But it has much greater uses, of course.&nbsp;



And then we&#8217;re working with you‚ÄîI don&#8217;t know how much you want to get into this, Peter, or [if] you want to talk about it yourself‚ÄîMAIRA-2, which is really exciting, about how taking a simple problem‚Äîcan you create a transformer that is able to detect if lines on the chest are in the right place, breathing tube is in the right place?‚Äîand then do it in a way that then can be used for many, many other things.&nbsp;



And then, Peter, because you asked about education and research, ‚Ä¶&nbsp;



LEE: Yeah.&nbsp;



FARRUGIA: ‚Ä¶ imagine what this does now to the education system, right. And so we&#8217;ve got to train our physicians differently. We now have an AI curriculum for all our medical students. We offer masters and PhDs in AI. We think it&#8217;s essential for the people who want to be able to truly become experts, the same way I became an expert in my area of research.&nbsp;



And then from a research standpoint, when you think about all the registries that exist in people&#8217;s labs, all the spatial genomics, all the epigenomics, all the omics that exist. And if you are able to coalesce them into one big, what we call, an atlas, how that could really spur research at a scale that we haven&#8217;t thought of before. And so that is our aim at the moment.&nbsp;



From a research standpoint, we are, with Vijay Shah, who&#8217;s our dean of research, is to say, let&#8217;s make the effort of making sure all the data are available to be able to use and enable for us to take advantage of AI. And that is not easy because, of course, people have collected the data. They tend to want to embrace it.&nbsp;



LEE: Yep.&nbsp;



FARRUGIA: So there have to be the right incentives, the right privacy, and the right ways of doing it. And we think we&#8217;re on the way there, and we‚Äôre already seeing some advantages from doing it this way.&nbsp;



LEE: So we&#8217;re running short on time. And so I always like to end with one or two more provocative questions. And, you know, it&#8217;s tempting to ask you the provocative question of whether you think AI will ever replace human doctors, but I don&#8217;t want to go there with you. In fact, as I thought about our discussion, I was reflecting. We were at a conference together once, and I was on stage in a fireside chat. And then, you know, after the fireside chat, there were audience questions, and I don&#8217;t remember any of the questions from the audience except yours.&nbsp;



And just to remind you, you know, I think when I was on stage, we were talking about a lot of practical uses of AI to, let&#8217;s say, reduce administrative burdens and so on in healthcare. But you got up and you, I won&#8217;t say you scolded me, but you more or less said, is it the right idea to use AI to optimize today&#8217;s somewhat broken healthcare system, or should we be thinking more boldly about, you know, a more fundamental transformation?&nbsp;



And so what I thought I would try to close with here is to hear what was really behind that question. You know, what were you trying to get me to think about when you asked that question?&nbsp;



FARRUGIA: So first of all, darn your great memory [LAUGHTER]. Belated apologies ‚Ä¶ I probably should have &#8230;&nbsp;



LEE: It was by far the best and most sophisticated and, I think, thought-provoking question of all of the ones that came out of the audience.&nbsp;



FARRUGIA: What I was trying to get to is actually trying to clarify it in my own head and then in the head of others is that we do not need to have a linear path to get to where we want to get to. And we seemed to be on a linear path, which is, let&#8217;s try and reduce administrative burden. Let&#8217;s try and truly be a companion to a physician or other provider. Let&#8217;s make their problems better, make them feel better about providing healthcare. And then in the next step, we keep going until we get to, now we can call it agentic AI, whatever we want to talk about. And my view was, no, is that let&#8217;s start with that aim, the last aim, and do the others because the others will come automatically if you&#8217;re working on that harder problem.&nbsp;



Because one, to get to that harder problem, you&#8217;ll find all the other solutions. I was just trying to push that here&#8217;s this wonderful tool that&#8217;s been given to us. Let&#8217;s take advantage of it as quickly as we can. I think we had gotten a little too sensitized to need to say the right things. ‚ÄúCareful, be very careful‚Äù versus saying, ‚ÄúMassive opportunity. Do it right, and healthcare will be much better. Go for it.‚Äù&nbsp;



LEE: Well, I think I understand better now where the vision, insight, and frankly, courage to take on something as ambitious and transformational as the Mayo Clinic Platform and really all of your leadership in your tenure as the president and CEO of Mayo Clinic. I think I understand it much better now.&nbsp;



Gianrico, it&#8217;s just always such a privilege to interact with you and now to have a chance to work with you more closely. So thank you for everything that you do and thank you for joining us today.&nbsp;



FARRUGIA: Thank you for making it so easy, and thanks for giving us this opportunity to do good for the world.&nbsp;



[TRANSITION MUSIC]‚ÄØ



LEE: Gianrico leads what is arguably the crown jewel of the world&#8217;s healthcare systems, and so I feel it&#8217;s such a privilege to be able to talk and sometimes even brainstorm with him.&nbsp;



Our conversation, I think, exposed just how tech forward Gianrico is as he charts the strategies for healthcare delivery well into the future. And as I&#8217;ve interacted with many others, what I&#8217;ve learned is that this is a common trait among major health system CEOs. Roughly speaking, like we&#8217;ve seen in previous episodes where doctors and med students are polymath clinician-technologists, the same thing is true of health system CEOs and other leaders.&nbsp;



AI in the mind of a health system CEO today is not only a technology that can transform diagnosis and treatment, but it&#8217;s also something that can have a huge impact on the business of healthcare delivery, the connection of healthcare to medical research, and the journeys that patients go through as they seek better health.&nbsp;



These two conversations show that virtually all leaders in health and medicine are confronting head-on the opportunities, challenges, and the reality of AI, and they see a future that is potentially very different than what we have today.&nbsp;



[THEME MUSIC]&nbsp;



I&#8217;d like to thank Umair and Gianrico again for their time and insights. And to our listeners, thank you for joining us. We hope you&#8217;ll tune in to our final episode of the series. My coauthors, Carey and Zak, will be back to examine the takeaways from our most recent conversations.&nbsp;



Until next time.&nbsp;



[MUSIC FADES]&nbsp;

				
			
			
				Show more			
		
	





AI Revolution in Medicine podcast series





Opens in a new tabThe post Reimagining healthcare delivery and public health with AI appeared first on Microsoft Research.
‚Ä¢ Self-adaptive reasoning for science
  Unlocking self-adaptive cognitive behavior that is more controllable and explainable than reasoning models in challenging scientific domains



Long-running LLM agents equipped with strong reasoning, planning, and execution skills have the potential to transform scientific discovery with high-impact advancements, such as developing new materials or pharmaceuticals. As these agents become more autonomous, ensuring effective human oversight and clear accountability becomes increasingly important, presenting challenges that must be addressed to unlock their full transformative power. Today‚Äôs approaches to long-term reasoning are established during the post-training phase, prior to end-user deployment and typically by the model provider. As a result, the expected actions of these agents are pre-baked by the model developer, offering little to no control from the end user.



At Microsoft, we are pioneering a vision for a continually steerable virtual scientist. In line with this vision, we created the ability to have a non-reasoning model develop thought patterns that allow for control and customizability by scientists. Our approach, a cognitive loop via in-situ optimization (CLIO), does not rely on reinforcement learning post-training to develop reasoning patterns yet still yields equivalent performance as demonstrated through our evaluation on Humanity‚Äôs Last Exam (HLE). Notably, we increased OpenAI GPT-4.1‚Äôs base model accuracy on text-only biology and medicine from 8.55% to 22.37%, an absolute increase of 13.82% (161.64% relative), surpassing o3 (high). This demonstrates that an optimization-based, self-adaptive AI system developed without further post-training can rival post-trained models in domains where adaptability, explainability, and control matter most.



Figure 1. Head-to-head comparison of OpenAI‚Äôs GPT-4.1 with CLIO, o3, and GPT-4.1 with no tools on HLE biology and medicine questions



In-situ optimization with internal self-reflection to enable self-adaptive reasoning



Model development has advanced from using reinforcement learning human feedback (RLHF) for answer alignment to external grading in reinforcement learning (RLVR). Recent approaches show promise in the utilization of intrinsic rewards for training reasoning models (RLIR). Traditionally, these reasoning processes are learned during the post-training process before any user interaction. While today‚Äôs reasoning models require additional data in the training phase and limit user control during the reasoning generation process, CLIO‚Äôs approach enables users to steer reasoning from scratch without additional data. Rather, CLIO generates its own necessary data by creating reflection loops at runtime. These reflection loops are utilized for a wide array of activities that CLIO self-defines, encompassing idea exploration, memory management, and behavior control. Most interesting is CLIO‚Äôs ability to leverage prior inferences to adjust future behaviors, handling uncertainties and raising flags for correction when necessary. Through this open architecture approach to reasoning, we alleviate the necessity for further model post-training to achieve desired reasoning behavior. Performing novel scientific discoveries often has no prior established patterns for reasoning, much less a large enough corpus of high-quality data to train on.&nbsp;



CLIO reasons by continuously reflecting on progress, generating hypotheses, and evaluating multiple discovery strategies. For the HLE test, CLIO was specifically steered to follow the scientific method as a guiding framework. Our research shows that equipping language models with self-adapting reasoning enhances their problem-solving ability. It provides a net benefit in quality for science questions, as well as providing exposure and control to the end user.



Figure 2. CLIO can raise key areas of uncertainty within its self-formulated reasoning process, balancing multiple different viewpoints using graph structures.



Control over uncertainty: Building trust in AI&nbsp;



Orchestrated reasoning systems like CLIO are valuable for scientific discovery, as they provide features beyond accuracy alone. Capabilities such as explaining the outcomes of internal reasoning are standard in the scientific field and are present in current reasoning model approaches. However, elements like displaying complete work, including final outcomes, internal thought processes, and uncertainty thresholds to support reproducibility or correction, as well as indicating uncertainty, are not yet universally implemented. Current models and systems do not have this same innate humility.&nbsp; Rather, we are left with models that produce confident results, whether correct or incorrect. When correct, it is valuable. When incorrect, it is dangerous to the scientific process. Hence, understanding a model or system‚Äôs uncertainty is a crucial aspect that we have developed natively into CLIO.



On the other end of the spectrum, orchestrated reasoning systems tend to oversaturate the user by raising too many flags. We enable prompt-free control knobs within CLIO to set thresholds for raising uncertainty flags. This allows CLIO to flag uncertainty for itself and the end user at the proper point in time. This also enables scientists to revisit CLIO‚Äôs reasoning path with critiques, edit beliefs during the reasoning process, and re-execute them from the desired point in time. Ultimately, this builds a foundational level of trust with scientists to use them in a scientifically defensible and rigorous way.&nbsp;



How does&nbsp;CLIO&nbsp;perform?&nbsp;



We evaluate CLIO against text-based biology and medicine questions from HLE. For this domain, we demonstrate a 61.98% relative increase or an 8.56% net increase in accuracy over OpenAI‚Äôs o3 and substantially outperform base completion models like OpenAI‚Äôs GPT-4.1, while enabling the requisite explainability and control. This technique applies to all models, showing similar increases in OpenAI‚Äôs GPT-4o model, which we observe performs poorly on HLE-level questions. On average, GPT-4.1 is not considered competent for HLE scale questions (GraphRAG. This extension of the cognition pattern provides a further 7.90% over a non-ensembled approach. &nbsp;



Figure 3. The impact of thinking effort on CLIO‚Äôs effectiveness.



Furthermore, CLIO‚Äôs design offers different knobs of control, for example, how much time to think and which technique to utilize for a given problem. In Figure 3, we demonstrate these knobs of control and their increase on GPT-4.1 and GPT-4o&#8217;s performance. In this case, we analyze performance for a subset of biomedical questions, those focused on immunology. CLIO increases GPT-4o&#8217;s base performance to be at par with the best reasoning models for immunology questions. We observe a 13.60% improvement over the base model, GPT-4o. This result shows CLIO to be model agnostic, similar to Microsoft AI Diagnostic Orchestrator&#8217;s (MAI-DxO) (opens in new tab)&#8216;s approach and corresponding performance boost.&nbsp;



Implications for science and trustworthy discovery



The future of scientific discovery demands more than reasoning over knowledge and raw computational power alone. Here, we demonstrate how CLIO not only increases model performance but establishes new layers of control for scientists. In our upcoming work, we will demonstrate how CLIO increases tool utility for highly valuable scientific questions in the drug discovery space which requires precise tools designed for the language of science. While our experiments focus on scientific discovery, we believe CLIO can apply in a domain-agnostic fashion. Experts tackling problems in domains such as financial analysis, engineering, and legal services could potentially benefit from AI systems with a transparent, steerable reasoning approach. Ultimately, we envision CLIO as an enduring control-layer in hybrid AI stacks that combine traditional completion and reasoning models, with external memory systems, and advanced tool calling. These continuous checks and balances that CLIO enables will continue to remain valuable even as components within the AI stacks evolve. This combination of intelligent and steerable scientific decision making and tool optimization is the basis of the recently announced Microsoft Discovery platform (opens in new tab).



At Microsoft, we‚Äôre committed to advancing AI research that earns the trust of scientists, empowering them to discover new frontiers of knowledge. Our work is a testament to what‚Äôs possible when we blend innovation with trustworthiness and a human-centered vision for the future of AI-assisted scientific discovery. We invite the research and scientific community to join us in shaping that future.



Further information:



To learn more details about our approach, please read our pre-print paper published alongside this blog. We are in the process of submitting this work for external peer review and encourage partners to explore the utilization of CLIO in Microsoft Discovery. To learn more about Microsoft‚Äôs research on this or contact our team, please reach out to discoverylabs@microsoft.com.¬†



Acknowledgements



We are grateful for Jason Zander and Nadia Karim‚Äôs support. We extend our thanks to colleagues both inside and outside Microsoft Discovery and Quantum for sharing their insights and feedback, including Allen Stewart, Yasser Asmi, David Marvin, Harsha Nori, Scott Lundberg, and Phil Waymouth.&nbsp;
Opens in a new tabThe post Self-adaptive reasoning for science appeared first on Microsoft Research.
‚Ä¢ The DIVA logistics agent, powered by Amazon Bedrock
  DTDC is India‚Äôs leading integrated express logistics provider, operating the largest network of customer access points in the country. DTDC‚Äôs technology-driven logistics solutions cater to a wide range of customers across diverse industry verticals, making them a trusted partner in delivering excellence. 
DTDC Express Limited receives over 400,000 customer queries each month, ranging from tracking requests to serviceability checks and shipping rates. With such a high volume of shipments, their existing logistics agent, DIVA, was operated on a rigid, guided workflow, forcing users to follow a structured path rather than engaging in natural, dynamic conversations. The lack of flexibility resulted in increased burden on customer support teams, longer resolution times, and poor customer experience. 
DTDC was looking for a more flexible, intelligent assistant‚Äîone that could understand context, manage complex queries, and improve efficiency while reducing reliance on human agents. To achieve a better customer experience, DTDC decided to enhance DIVA with generative AI using Amazon Bedrock. 
ShellKode is an AWS Partner, born-in-the-cloud company specializing in modernization, security, data, generative AI, and machine learning (ML). With a mission to drive transformative growth, ShellKode empowers businesses through state-of-the-art technology solutions that address complex challenges and unlock new opportunities. Using deep industry expertise, they deliver tailored strategies that foster innovation, efficiency, and long-term success in an evolving digital landscape. 
In this post, we discuss how DTDC and ShellKode used Amazon Bedrock to build DIVA 2.0, a generative AI-powered logistics agent. 
Solution overview 
To address the limitations of the existing logistics agent, ShellKode built an advanced agentic assistant using Amazon Bedrock Agents, Amazon Bedrock Knowledge Bases, and an API integration layer. 
When customers interact with DIVA 2.0, they experience a seamless, conversational interface that understands and responds to their queries naturally. Whether tracking a package, checking shipping rates, or inquiring about service availability, users can ask questions in their own words without following a rigid script. DIVA 2.0‚Äôs enhanced AI capabilities allow it to understand context, manage complex requests, and provide accurate, personalized responses, significantly improving the overall customer experience and reducing the need for human intervention. The following high-level architecture diagram illustrates the application flow and the solution architecture with AWS services. 
 
The DTDC logistics agent is designed using a modular and scalable architecture to provide seamless integration and high performance. This streamlined workflow demonstrates how a generative AI-powered serverless logistics agent using AWS App Runner, Amazon Bedrock Agents, AWS Lambda, and a vector-based knowledge base handles user queries ranging from tracking requests to serviceability checks and shipping rates intelligently and efficiently. 
The logistics agent is hosted as a static website using Amazon CloudFront and Amazon Simple Storage Service (Amazon S3). The logistics agent is integrated with the DTDC website, which provides an intuitive and user-friendly interface for end-user interactions (see the following screenshot). 
 
An end-user accesses the logistics agent through the DTDC website and submits queries like tracking shipments, checking service availability, calculating shipping rates, FAQs, and so on using natural language.The user requests are processed by App Runner, which helps run the web application (including API services, backend web services, and websites) on AWS. App Runner is hosted with multiple API services, such as the Amazon Bedrock Agents API and Dashboard API. App Runner initiates the Amazon Bedrock Agents API based on the user requests. 
Amazon Bedrock is a fully managed service that offers a choice of industry leading foundation models (FMs) along with a broad set of capabilities to build generative AI applications, simplifying development with security, privacy, and responsible AI. With Amazon Bedrock, your content is not used to improve the base models and is not shared with any model providers. Amazon Bedrock Guardrails provides configurable safeguards to help safely build generative AI applications at scale. To learn more, see Build safe and responsible generative AI applications with guardrails. AWS Identity and Access Management (IAM) helps administrators securely control who can be authenticated and authorized to use Amazon Bedrock resources. 
The Amazon Bedrock agents are configured in Amazon Bedrock. An Amazon Bedrock agent receives the request and interprets the user‚Äôs intent using its natural language understanding capabilities. Based on the interpreted intent, the agent triggers an appropriate Lambda function, such as: 
 
 Tracking consignments 
 Pricing information 
 Location serviceability check 
 Support ticket creation 
 
The triggered Lambda function calls the following client APIs, retrieves the relevant data, and returns the response to the agent: 
 
 Tracking System API ‚Äì Retrieves real-time status and provides updates on consignment shipment tracking 
 Delivery Franchise Location API ‚Äì Checks the service availability to deliver the parcels between the locations 
 Pricing System API ‚Äì Calculates the shipping rates based on shipment details provided by the user 
 Customer Care API ‚Äì Creates a support ticket for the end-users 
 
The agent passes the response to the large language model (LLM), in this case Anthropic‚Äôs Claude 3.0 on Amazon Bedrock, which understands the context of the retrieved data, processes it, and generates a meaningful response for the user. 
The knowledge base contains web-scraped content from the DTDC website, internal support documentation, FAQs, and operational data, enabling real-time updates and accurate responses. The knowledge base contents are stored as vector embeddings in Amazon OpenSearch Service, providing quick and relevant responses. For general queries, the logistics agent fetches information from Amazon Bedrock Knowledge Bases, providing accuracy and relevance. Using semantic similarity search, relevant chunks of information are retrieved from the knowledge base based on the user‚Äôs query, which Amazon Bedrock then uses to generate a context-aware response. If no relevant data is found in the knowledge base, a fallback response (preconfigured in the Amazon Bedrock prompt) is returned, indicating that the system couldn‚Äôt assist with the request. 
The logistics agent queries and associated responses are stored in Amazon Relational Database Service (Amazon RDS) for PostgreSQL for enhanced scalability and relational data handling. App Runners initiates the Dashboard API call to update the queries and associated responses in Amazon RDS. We discuss this in more detail the following section. 
Throughout the process, Amazon CloudWatch Logs captures key events such as intent recognition, Lambda invocations, API responses, and fallback triggers for auditing and system monitoring. AWS CloudTrail records and monitors activity in the AWS account, including actions taken by users, roles, or AWS services. It logs these events, which can be used for operational auditing, governance, and compliance. 
Amazon GuardDuty is a threat detection service that continuously monitors, analyzes, and processes AWS data sources and logs in your AWS environment. GuardDuty uses threat intelligence feeds, such as lists of malicious IP addresses and domains, file hashes, and ML models to identify suspicious and potentially malicious activity in the AWS environment. 
Logistics agent dashboard 
The following high-level architecture diagram illustrates the logistics agent dashboard, which captures the end-user interactions and its associated responses. 
 
The logistics agent dashboard is hosted as a static website using CloudFront and Amazon S3. Dashboard access is allowed only for the DTDC admin team. 
The dashboard is populated through API calls using Amazon API Gateway with Lambda as a backend, which retrieves the dashboard data from Amazon RDS for PostgreSQL. 
The dashboard provides real-time insights into the logistics agent performance, including accuracy, unresolved queries, query categories, session statistics, and user interaction data (see the following screenshot). It provides actionable insights with features such as heat maps, pie charts, and session logs. Real-time data is logged and analyzed on the dashboard, enabling continuous improvement and quick issue resolution. 
 
Solution challenges and benefits 
When implementing DIVA 2.0, DTDC and ShellKode faced several significant challenges. Integrating real-time data from multiple legacy systems was crucial for providing accurate, up-to-date information on tracking, rates, and serviceability. This was likely addressed through the robust API integration capabilities of Amazon Bedrock Agents. Another major hurdle was training the AI to understand complex logistics terminology and multi-step queries, which was overcome by using Amazon Bedrock LLMs and Amazon Bedrock Knowledge Bases, fine-tuned with industry-specific data. The team also had to navigate the delicate process of transitioning from the old rigid DIVA system while maintaining service continuity and preserving historical data, potentially employing a phased approach with parallel systems. Finally, scaling the solution to handle over 400,000 monthly queries while maintaining performance was a significant challenge, addressed by using the cloud infrastructure of Amazon Bedrock Agents for optimal scalability and performance. These challenges underscore the complexity of upgrading to an AI-powered system in a high-volume, data-intensive industry like logistics, and highlight how AWS solutions provided the necessary tools to overcome these obstacles. DTDC realized the following benefits from powering the logistics agent with generative AI using Amazon Bedrock: 
 
 Enhanced conversations and real-time data access with customer support agents ‚Äì Powered by Amazon Bedrock Agents, the solution improves natural language understanding, enabling more fluid and engaging conversations. With multi-step reasoning, it can handle a broader range of queries with greater accuracy. Additionally, by integrating seamlessly with DTDC‚Äôs API layer, the logistics agent provides real-time access to vital information, such as tracking shipments, service availability, and calculating shipping rates. The combination of advanced conversational capabilities and real-time data provides fast, accurate, and contextually relevant responses. 
 Intelligent data processing and accurate FAQ responses ‚Äì For complex queries, the logistics agent uses LLM technology to process raw data and deliver structured, tailored responses. This makes sure users get clear, actionable insights. For frequently asked questions, the logistics agent uses Amazon Bedrock Knowledge Bases to deliver precise answers without requiring human support, reducing wait times and enhancing the overall user experience. 
 Reduced live agent dependency and continuous improvement ‚Äì Although the logistics agent hasn‚Äôt eliminated the need for customer support, the number of queries handled by the customer support team has reduced by 51.4%. The system provides valuable insights into key performance metrics like peak query times, unresolved issues, and overall engagement through integrated real-time analytics, helping refine and improve the assistant‚Äôs capabilities over time. 
 
Results 
The generative AI-powered logistics agent has reduced the burden on customer support teams and shortened resolution times, resulting in better customer experience: 
 
 Powered by Amazon Bedrock, DIVA 2.0 understands queries in natural language and supports dynamic conversations with a response accuracy of 93% 
 Based on the last 3 months of dashboard metrics data, they observed the following: 
   
   71% of the inquiries were related to consignments (256,048), whereas 29.5% were general inquiries (107,132) 
   51.4% of consignment inquiries (131,530) didn‚Äôt result in a support ticket, whereas 48.6% (124,518) led to new support ticket creation 
   Of the inquiries that resulted in tickets, 40% started with the customer support center before moving to the AI assistant, whereas 60% began with the assistant before involving the customer support center 
    
 
DIVA 2.0 has reduced the number of queries handled by the customer support team by 51.4%. DTDC‚Äôs support team can now focus on more critical issues, improving overall efficiency. 
Summary 
This post demonstrated how Amazon Bedrock can transform a traditional chatbot to a generative AI-powered logistics agent that provides better customer experience through dynamic conversation. For businesses facing similar challenges, this solution offers a blueprint for modernizing your AI assistant while maintaining compliance with industry standards. 
To learn more about this AWS solution, contact AWS for further assistance. AWS can provide detailed information about implementation, pricing, and how to tailor the solution to your specific business needs. 
 
About the authors 
Rishi Sareen ‚Äì Chief Information Officer (CIO), DTDC is a seasoned technology leader with over two decades of experience in driving digital transformation, enterprise IT strategy, and innovation across the logistics and supply chain sector. He specializes in building agile, AI-driven, and secure technology ecosystems that enhance operational efficiency and customer experience. Rishi leads initiatives spanning system modernization, data intelligence, automation, cybersecurity, cloud, and artificial intelligence. He is deeply committed to aligning technology with business outcomes while fostering a culture of continuous improvement and purposeful innovation. A strong advocate for people-centric leadership, Rishi places high emphasis on nurturing talent, building high-performing teams, and mentoring future-ready technology leaders who can thrive in dynamic, AI-powered environments. Known for his strategic vision and disciplined execution, he has led large-scale digital initiatives and transformation programs that deliver lasting business impact. 
Arunraja Karthick ‚Äì Head ‚Äì IT Services &amp; Security (CISO), DTDC is a strategic IT and cybersecurity leader with over 15 years of experience driving enterprise-scale digital transformation. As the Head of IT Services &amp; Security (CISO) at DTDC Express Limited, he leads the organization‚Äôs core IT, cloud, and security programs‚Äîtransforming legacy environments into agile, secure, and cloud-native ecosystems. Under his leadership, DTDC has adopted a hybrid cloud architecture spanning AWS, GCP, and on-prem colocation, with a vision to enable dynamic workload mobility and vendor-neutral scalability. Arunraja has led critical modernization efforts, including the migration of key business applications to microservices and containerized platforms, while ensuring high availability and regulatory compliance. Known for his deep technical insight and execution discipline, he has implemented enterprise-wide cybersecurity frameworks‚Äîfrom Email DLP, Mobile Device Management, and Conditional Access to Hybrid WAF and advanced SOC operations. He has also championed secure access transformation through Zero Trust-aligned Secure WebVPN, redefining how internal users access corporate apps. Arunraja‚Äôs leadership is grounded in platform thinking, automation, and a user-first mindset. His recent initiatives include the enterprise rollout of GenAI copilots for customer experience and operations, as well as unified policy-based DLP and content control mechanisms across endpoints and cloud. Recognized as an Influential Technology Leader, Arunraja continues to challenge conventional IT boundaries‚Äîaligning security, agility, and innovation to power business evolution. 
Bakrudeen K an AWS Ambassador, leads the AI/ML practice at Shellkode, focusing on driving innovation in artificial intelligence, especially in Generative AI. He plays a key role in building teams and advanced AI solutions, Agentic Assistants, and other next-gen technologies. Bakrudeen has made notable contributions to AI/ML research and development. In 2023 and 2024, he received the Generative AI Consulting Excellence Partner Award at the AI Conclave and the Social Impact Partner of the Year Award for Generative AI at AWS re:Invent 2024, both on behalf of Shellkode reflecting the team‚Äôs strong commitment to innovation and impact in the AI space. 
Suresh Kanniappan is a Solutions Architect at AWS, handling Automotive, Manufacturing and Logistics enterprises in India. He is passionate about cloud security and Industry solutions that can solve real world problems. Prior to AWS, he worked for AWS customers and partners in consulting, migration and solution architecture roles for over 14 years. 
Sid Chandilya is a Sr. Customer Relations Manager at AWS, responsible for tech led business transformation with Automotive, Manufacturing and Logistics enterprises in India. Sid is peculiarly passionate about challenging status quos, building a joint ‚ÄúThink Big‚Äù vision with customer CXOs and leveraging Ai infused tech to accelerate outcomes. He is known for his deep understanding of industry imperatives (working backward from customer) and translating the business pain points into tech solution.
‚Ä¢ Automate enterprise workflows by integrating Salesforce Agentforce with Amazon Bedrock Agents
  AI agents are rapidly transforming enterprise operations. Although a single agent can perform specific tasks effectively, complex business processes often span multiple systems, requiring data retrieval, analysis, decision-making, and action execution across different systems. With multi-agent collaboration, specialized AI agents can work together to automate intricate workflows. 
This post explores a practical collaboration, integrating Salesforce Agentforce with Amazon Bedrock Agents and Amazon Redshift, to automate enterprise workflows. 
Multi-agent collaboration in Enterprise AI 
Enterprise environments today are complex, featuring diverse technologies across multiple systems. Salesforce and AWS provide distinct advantages to customers. Many organizations already maintain significant infrastructure on AWS, including data, AI, and various business applications such as ERP, finance, supply chain, HRMS, and workforce management systems. Agentforce delivers powerful AI-driven agent capabilities that are grounded in enterprise context and data. While Salesforce provides a rich source of trusted business data, customers increasingly need agents that can access and act on information across multiple systems. By integrating AWS-powered AI services into Agentforce, organizations can orchestrate intelligent agents that operate across Salesforce and AWS, unlocking the strengths of both. 
Agentforce and Amazon Bedrock Agents can work together in flexible ways, leveraging the unique strengths of both platforms to deliver smarter, more comprehensive AI workflows. Example collaboration models include: 
 
 Agentforce as the primary orchestrator: 
   
   Manages end to end customer-oriented workflows 
   Delegates specialized tasks to Amazon Bedrock Agents as needed through custom actions 
   Coordinates access to external data and services across systems 
    
 
This integration creates a more powerful solution that maximizes the benefits of both Salesforce and AWS, so you can achieve better business outcomes through enhanced AI capabilities and cross-system functionality. 
Agentforce overview 
Agentforce brings digital labor to every employee, department, and business process, augmenting teams and elevating customer experiences.It works seamlessly with your existing applications, data, and business logic to take meaningful action across the enterprise. And because it‚Äôs built on the trusted Salesforce platform, your data stays secure, governed, and in your control. With Agentforce, you can: 
 
 Deploy prebuilt agents designed for specific roles, industries, or use cases 
 Enable agents to take action with existing workflows, code, and APIs 
 Connect your agents to enterprise data securely 
 Deliver accurate and grounded outcomes through the Atlas Reasoning Engine 
 
Amazon Bedrock Agents and Amazon Bedrock Knowledge Bases overview 
Amazon Bedrock is a fully managed AWS service offering access to high-performing foundation models (FMs) from various AI companies through a single API. In this post, we discuss the following features: 
 
 Amazon Bedrock Agents ‚Äì Managed AI agents use FMs to understand user requests, break down complex tasks into steps, maintain conversation context, and orchestrate actions. They can interact with company systems and data sources through APIs (configured through action groups) and access information through knowledge bases. You provide instructions in natural language, select an FM, and configure data sources and tools (APIs), and Amazon Bedrock handles the orchestration. 
 Amazon Bedrock Knowledge Bases ‚Äì This capability enables agents to perform Retrieval Augmented Generation (RAG) using your company‚Äôs private data sources. You connect the knowledge base to your data hosted in AWS, such as in Amazon Simple Storage Service (Amazon S3) or Amazon Redshift, and it automatically handles the vectorization and retrieval process. When asked a question or given a task, the agent can query the knowledge base to find relevant information, providing more accurate, context-aware responses and decisions without needing to retrain the underlying FM. 
 
Agentforce and Amazon Bedrock Agent integration patterns 
Agentforce can call Amazon Bedrock agents in different ways, allowing flexibility to build different architectures. The following diagram illustrates synchronous and asynchronous patterns. 
 
For a synchronous or request-reply interaction, Agentforce uses custom agent actions facilitated by External Services, Apex Invocable Methods, or Flow to call an Amazon Bedrock agent. The authentication to AWS is facilitated using named credentials. Named credentials are designed to securely manage authentication details for external services integrated with Salesforce. They alleviate the need to hardcode sensitive information like user names and passwords, minimizing the risk of exposure and potential data breaches. This separation of credentials from the application code can significantly enhance security posture. Named credentials streamline integration by providing a centralized and consistent method for handling authentication, reducing complexity and potential errors. You can use Salesforce Private Connect to provide a secure private connection with AWS using AWS PrivateLink. Refer to Private Integration Between Salesforce and Amazon API Gateway for additional details. 
 
For asynchronous calls, Agentforce uses Salesforce Event Relay and Flow with Amazon EventBridge to call an Amazon Bedrock agent. 
 
In this post, we discuss the synchronous call pattern. We encourage you to explore Salesforce Event Relay with EventBridge to build event-driven agentic AI workflows. Agentforce also offers the Agent API, which makes it straightforward to call an Agentforce agent from an Amazon Bedrock agent, using EventBridge API destinations, for bi-directional agentic AI workflows. 
Solution overview 
To illustrate the multi-agent collaboration between Agentforce and AWS, we use the following architecture, which provides access to Internet of Things (IoT) sensor data to the Agentforce agent and handles potentially erroneous sensor readings using a multi-agent approach. 
 
The example workflow consists of the following steps: 
 
 Coral Cloud has equipped their rooms with smart air conditioners and temperature sensors. These IoT devices capture critical information such as room temperature and error code and store it in Coral Cloud‚Äôs AWS database in Amazon Redshift. 
 Agentforce agent calls an Amazon Bedrock agent through the Agent Wrapper API with questions such as ‚ÄúWhat is the temperature in room 123‚Äù to answer customer questions related to the comfort of the room. This API is implemented as an AWS Lambda function, acting as the entry point in the AWS Cloud. 
 The Amazon Bedrock agent, upon receiving the request, needs context. It queries its configured knowledge base by generating the necessary SQL query. 
 The knowledge base is connected to a Redshift database containing historical sensor data or contextual information (like the sensor‚Äôs thresholds and maintenance history). It retrieves relevant information based on the agent‚Äôs query and responds back with an answer. 
 With the initial data and the context from the knowledge base, the Amazon Bedrock agent uses its underlying FM and natural language instructions to decide the appropriate action. In this scenario, detecting an error prompts it to create a case when it receives erroneous readings from a sensor. 
 The action group contains the Agentforce Agent Wrapper Lambda function. The Amazon Bedrock agent securely passes the necessary details (like which sensor or room needs a case) to this function. 
 The Agentforce Agent Wrapper Lambda function acts as an adapter. It translates the request from the Amazon Bedrock agent into the specific format required by the Agentforce service‚Äòs API or interface. 
 The Lambda function calls Agentforce, instructing it to create a case associated with the contact or account linked to the sensor that sent the erroneous reading. 
 Agentforce uses its internal logic (agent, topics, and actions) to create or escalate the case within Salesforce. 
 
This workflow demonstrates how Amazon Bedrock Agents orchestrates tasks, using Amazon Bedrock Knowledge Bases for context and action groups (through Lambda) to interact with Agentforce to complete the end-to-end process. 
Prerequisites 
Before building this architecture, make sure you have the following: 
 
 AWS account ‚Äì An active AWS account with permissions to use Amazon Bedrock, Lambda, Amazon Redshift, AWS Identity and Access Management (IAM), and API Gateway. 
 Amazon Bedrock access ‚Äì Access to Amazon Bedrock Agents and to Anthropic‚Äôs Claude 3.5 Haiku v1 enabled in your chosen AWS Region. 
 Redshift resources ‚Äì An operational Redshift cluster or Amazon Redshift Serverless endpoint. The relevant tables containing sensor data (historical readings, sensor thresholds, and maintenance history) must be created and populated. 
 Agentforce system ‚Äì Access to and understanding of the Agentforce system, including how to configure it. You can sign up for a developer edition with Agentforce and Data Cloud. 
 Lambda knowledge ‚Äì Familiarity with creating, deploying, and managing Lambda functions (using Python). 
 IAM roles and policies ‚Äì Understanding of how to create IAM roles with the necessary permissions for Amazon Bedrock Agents, Lambda functions (to call Amazon Bedrock, Amazon Redshift, and the Agentforce API), and Amazon Bedrock Knowledge Bases. 
 
Prepare Amazon Redshift data 
Make sure your data is structured and available in your Redshift instance. Note the database name, credentials, and table and column names. 
Create IAM roles 
For this post, we create two IAM roles: 
 
 custom_AmazonBedrockExecutionRoleForAgents: 
   
   Attach the following AWS managed policies to the role: 
     
     AmazonBedrockFullAccess 
     AmazonRedshiftDataFullAccess 
      
   In the trust relationship, provide the following trust policy (provide your AWS account ID): 
    
 
 
 {
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "AmazonBedrockAgentBedrockFoundationModelPolicyProd",
            "Effect": "Allow",
            "Principal": {
                "Service": "bedrock.amazonaws.com"
            },
            "Action": "sts:AssumeRole",
            "Condition": {
                "StringEquals": {
                    "aws:SourceAccount": "YOUR_ACCOUNT_ID"
                }
            }
        }
    ]
} 
 
 
 custom_AWSLambdaExecutionRole: 
   
   Attach the following AWS managed policies to the role: 
     
     AmazonBedrockFullAccess 
     AmazonLambdaBasicExecutionRole 
      
   In the trust relationship, provide the following trust policy (provide your AWS account ID): 
    
 
 
 {
    "Version": "2012-10-17",
   "Statement": [
       {
           "Effect": "Allow",
           "Principal": {
               "Service": "lambda.amazonaws.com"
           },
           "Action": "sts:AssumeRole",
           "Condition": {
               "StringEquals": {
                   "aws:SourceAccount": "YOUR_ACCOUNT_ID"
               }
           }
       }
   ]
} 
 
Create an Amazon Bedrock knowledge base 
Complete the following steps to create an Amazon Bedrock knowledge base: 
 
 On the Amazon Bedrock console, choose Knowledge Bases in the navigation pane. 
 Choose Create and Knowledge Base with structured data store. 
 
 
 
 On the Provide Knowledge Base details page, provide the following information: 
   
   Enter a name and optional description. 
   For Query engine, select Amazon Redshift. 
   For IAM permissions, select Use an existing service role and choose custom_AmazonBedrockExecutionRoleForAgents. 
   Choose Next.  
     
     For Query engine connection details, select Redshift provisioned and choose your cluster. 
     For Authentication, select IAM Role. 
     For Storage configuration, select Amazon Redshift database and Redshift database list. 
     On the Configure query engine page, provide the following information:  
     Provide table and column descriptions. The following is an example.  
     Choose Create Knowledge Base. 
      
    
 After you create the knowledge base, open the Redshift query editor and grant permissions for the role to access Redshift tables by running the following queries: 
 
 
 CREATE USER "IAMR:custom_AmazonBedrockExecutionRoleForAgents" WITH PASSWORD DISABLE; 

GRANT SELECT ON ALL TABLES IN SCHEMA dev.knowledgebase TO "IAMR:custom_AmazonBedrockExecutionRoleForAgents"; 

GRANT USAGE ON SCHEMA dev.knowledgebase TO "IAMR:custom_AmazonBedrockExecutionRoleForAgents"; 
 
For more information, refer to set up your query engine and permissions for creating a knowledge base with structured data store. 
 
 5. Choose Sync to sync the query engine. 
 
Make sure the status shows as Complete before moving to the next steps. 
 
 
 When the sync is complete, choose Test Knowledge Base. 
 Select Retrieval and response generation: data sources and model and choose Claude 3.5 Haiku for the model. 
 Enter a question about your data and make sure you get a valid answer. 
 
 
Create an Amazon Bedrock agent 
Complete the following steps to create an Amazon Bedrock agent: 
 
 On the Amazon Bedrock console, choose Agents in the navigation pane. 
 Choose Create agent. 
 On the Agent details page, provide the following information: 
   
   Enter a name and optional description. 
   For Agent resource role, select Use an existing service role and choose custom_AmazonBedrockExecutionRoleForAgents. 
    
 
 
 
 Provide detailed instructions for your agent. The following is an example: 
 
 
 You are an IoT device monitoring and alerting agent. 
You have access to the structured data containing reading, maintenance, threshold data for IoT devices. 
You answer questions about device reading, maintenance schedule and thresholds. 
You can also create case via Agentforce. 
When you receive comma separated values parse them as device_id, temperature, voltage, connectivity and error_code. 
First check if the temperature is less than min temperature, more than max temperature and connectivity is more than the connectivity threshold for the product associated with the device id. 
If there is an error code, send information to agentforce to create case. The information sent to agentforce should include device readings such as device id, error code. 
It should also include the threshold values related to the product associated with the device such as min temperature, max temperature and connectivity, 
In response to your call to agentforce just return the summary of the information provided with all the attributes provided. 
Do not omit any information in the response. Do not include the word escalated in agent. 
 
 
 Choose Save to save the agent. 
 Add the knowledge base you created in previous step to this agent. 
 
 
 
 Provide detailed instructions about the knowledge base for the agent. 
 
 
 
 Choose Save and then choose Prepare the agent. 
 Test the agent by asking a question (in the following example, we ask about sensor readings). 
 
 
 
 Choose Create alias. 
 On the Create alias page, provide the following information: 
   
   Enter an alias name and optional description. 
   For Associate version, select Create a new version and associate it to this alias. 
   For Select throughput, select On-demand. 
   Choose Create alias. 
    
 
 
 
 Note down the agent ID, which you will use in subsequent steps. 
 Note down the alias ID and agent ID, which you will use in subsequent steps. 
 
 
Create a Lambda function 
Complete the following steps to create a Lambda function to receive requests from Agentforce: 
 
 On the Lambda console, choose Functions in the navigation pane. 
 Choose Create function. 
 Configure the function with the following logic to receive requests through API Gateway and call Amazon Bedrock agents: 
 
 
 import boto3
import uuid
import json
import pprint
import traceback
import time
import logging
from agent_utils import invoke_agent_generate_response
logger = logging.getLogger()
logger.setLevel(logging.INFO)
bedrock_agent_runtime_client = boto3.client(
service_name="bedrock-agent-runtime",
region_name="REGION_NAME", # replace with the region name from your account
)
def lambda_handler(event, context):
    logger.info(event)
    body = event['body']
    input_text = json.loads(body)['inputText']
    agent_id = 'XXXXXXXX' # replace with the agent id from your account
    agent_alias_id = 'XXXXXXX' # replace with the alias id from your account
    session_id:str = str(uuid.uuid1()) # random identifier
    enable_trace:bool = False
    end_session:bool = False
    final_answer = None
    response = call_agent(input_text, agent_id, agent_alias_id)
    print("response : ")
    print(response)
 
    return {
        'headers': {
            'Content-Type' : 'application/json',
            'Access-Control-Allow-Headers': '*',
            'Access-Control-Allow-Origin': '*',
            'Access-Control-Allow-Methods': '*'
        },
        
        'statusCode': 200,
        'body': json.dumps({"outputText" :  response  })
    }
def call_agent(inputText, agentId, agentAliasId): 
    session_id = str(uuid.uuid1())
    enable_trace = False
    end_session = False
    while True:
        try:
            agent_response = bedrock_agent_runtime_client.invoke_agent(
                inputText=inputText,
                agentId=agentId,
                agentAliasId=agentAliasId,                
                sessionId=session_id,
                enableTrace=enable_trace,
                endSession=end_session
            )
            logger.info("Agent raw response:")
            pprint.pprint(agent_response)
            if 'completion' not in agent_response:
                raise ValueError("Missing 'completion' in agent response")
            for event in agent_response['completion']:
                chunk = event.get('chunk')
                # print('chunk: ', chunk)
                if chunk:
                    decoded_bytes = chunk.get("bytes").decode()
                    # print('bytes: ', decoded_bytes)
                    return decoded_bytes
        except Exception as e:
            print(traceback.format_exc())
            return f"Error: {str(e)}" 
 
 
 Define the necessary IAM permissions by assigning custom_AWSLambdaExecutionRole. 
 
Create a REST API 
Complete the following steps to create a REST API in API Gateway: 
 
 On the API Gateway console, create a REST API with proxy integration. 
 
 
 
 Enable API key required to protect the API from unauthenticated access. 
 
 
 
 Configure the usage plan and API key. For more details, see Set up API keys for REST APIs in API Gateway. 
 Deploy the API. 
 Note down the Invoke URL to use in subsequent steps. 
 
 
Create named credentials in Salesforce 
Now that you have created an Amazon Bedrock agent with an API Gateway endpoint and Lambda wrapper, let‚Äôs complete the configuration on the Salesforce side. Complete the following steps: 
 
 Log in to Salesforce. 
 Navigate to Setup, Security, Named Credentials. 
 On the External Credentials tab, choose New. 
 
 
 
 Provide the following information: 
   
   Enter a label and name. 
   For Authentication Protocol, choose Custom. 
   Choose Save. 
    
 
 
 
 Open the External Credentials entry to provide additional details: 
   
   Under Principals, create a new principal and provide the parameter name and value. 
    
 
 
 
  
   
   Under Custom Headers, create a new entry and provide a name and value. 
   Choose Save. 
    
 
 
Now you can grant access to the agent user to access these credentials. 
 
 Navigate to Setup, Users, User Profile, Enabled External Credential Principal Access and add the external credential principal you created to the allow list. 
 
 
 
 Choose New to create a named credentials entry. 
 Provide details such as label, name, the URL of the API Gateway endpoint, and authentication protocol, then choose Save. 
 
 
You can optionally use Salesforce Private Connect with PrivateLink to provide a secure private connection with. This allows critical data to flow from the Salesforce environment to AWS without using the public internet. 
Add an external service in Salesforce 
Complete the following steps to add an external service in Salesforce: 
 
 In Salesforce, navigate to Setup, Integrations, External Services and choose Add an External Service. 
 For Select an API source, choose From API Specification. 
 
 
 
 On the Edit an External Service page, provide the following information: 
   
   Enter a name and optional description. 
   For Service Schema, choose Upload from local. 
   For Select a Named Credential, choose the named credential you created. 
    
 
 
 
 Upload an Open API specification for the API Gateway endpoint. See the following example: 
 
 
 openapi: 3.0.0
info:
  title: Bedrock Agent Wrapper API
  version: 1.0.0
  description: Bedrock Agent Wrapper API
paths:
  /proxy:
    post:
      operationId: call-bedrock-agent
      summary: Call Bedrock Agent
      description: Call Bedrock Agent
      requestBody:
        description: input
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/input'
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/output'
        '500':
          description: Server error
components:
  schemas:
    input:
      type: object
      properties:
        inputText:
          type: string
        agentId:
          type: string
        agentAlias:
          type: string
    output:
      type: object
      properties:
        outputText:
          type: string 
 
 
 Choose Save and Next. 
 Enable the operation to make it available for Agentforce to invoke. 
 Choose Finish. 
 
 
Create an Agentforce agent action to use the external service 
Complete the following steps to create an Agentforce agent action: 
 
 In Salesforce, navigate to Setup, Agentforce, Einstein Generative AI, Agentforce Studio, Agentforce Assets. 
 On the Actions tab, choose New Agent Action. 
 Under Connect to an existing action, provide the following information: 
   
   For Reference Action Type, choose API. 
   For Reference Action Category, choose External Services. 
   For Reference Action, choose the Call Bedrock Agent action that you configured. 
   Enter an agent action label and API name. 
   Choose Next. 
    
 
 
 
 Provide the following information to complete the agent action configuration: 
   
   For Agent Action Instructions, enter Call Bedrock Agent to get the information about device readings, sensor readings, maintenance or threshold information. 
   For Loading Text, enter Calling Bedrock Agent. 
   Under Input, for Body, enter Provide the input in the input Text field. 
   Under Outputs, for 200, enter Successful response. 
    
 
 
 
 Save the agent action. 
 
Configure the Agentforce agent to use the agent action 
Complete the following steps to configure the Agentforce agent to use the agent action: 
 
 In Salesforce, navigate to Setup, Agentforce, Einstein Generative AI, Agentforce Studio, Agentforce Agents and open the agent in Agent Builder. 
 Create a new topic. 
 On the Topic Configuration tab, provide the following information: 
   
   For Name, enter Device Information. 
   For Classification Description, enter This topic handles inquiries related to device and sensor information, including reading, maintenance, and threshold. 
   For Scope, enter Your job is only to provide information about device readings, sensor readings, device maintenance, sensor maintenance, and threshold. Do not attempt to address issues outside of providing device information. 
   For Instructions, enter the following: 
    
 
 
 If a user asks for device readings or sensor readings, provide the information.
If a user asks for device maintenance or sensor maintenance, provide the information.
When searching for device information, include the device or sensor id and any relevant keywords in your search query. 
 
 
 
 On the This Topic‚Äôs Actions tab, choose New and Add from Asset Library. 
 
 
 
 Choose the Call Bedrock Agent action. 
 
 
 
 Activate the agent and enter a question, such as ‚ÄúWhat is the latest reading for sensor with device id CITDEV003.‚Äù 
 
The agent will indicate that it is calling the Amazon Bedrock agent, as shown in the following screenshot. 
 
The agent will fetch the information using the Amazon Bedrock agent from the associated knowledge base.  
Clean up 
To avoid additional costs, delete the resources that you created when you no longer need them: 
 
 Delete the Amazon Bedrock knowledge base: 
   
   On the Amazon Bedrock console, choose Knowledge Bases in the navigation pane. 
   Select the knowledge base you created and choose Delete. 
    
 Delete the Amazon Bedrock agent: 
   
   On the Amazon Bedrock console, choose Agents in the navigation pane. 
   Select the agent you created and choose Delete. 
    
 Delete the Lambda function: 
   
   On the Lambda console, choose Functions in the navigation pane. 
   Select the function you created and choose Delete. 
    
 Delete the REST API: 
   
   On the API Gateway console, choose APIs in the navigation pane. 
   Select the REST API you created and choose Delete. 
    
 
Conclusion 
In this post, we described an architecture that demonstrates the power of combining AI services on AWS with Agentforce. By using Amazon Bedrock Agents and Amazon Bedrock Knowledge Bases for contextual understanding through RAG, and Lambda functions and API Gateway to bridge interactions with Agentforce, businesses can build sophisticated, automated workflows. As AI capabilities continue to grow, such collaborative multi-agent systems will become increasingly central to enterprise automation strategies. In an upcoming post, we will show you how to build the asynchronous integration pattern from Agentforce to Amazon Bedrock using Salesforce Event Relay. 
To get started, see Become an Agentblazer Innovator and refer to How Amazon Bedrock Agents works. 
 
About the authors 
Yogesh Dhimate&nbsp;is a Sr. Partner Solutions Architect at AWS, leading technology partnership with Salesforce. Prior to joining AWS, Yogesh worked with leading companies including Salesforce driving their industry solution initiatives. With over 20 years of experience in product management and solutions architecture Yogesh brings unique perspective in cloud computing and artificial intelligence. 
Kranthi Pullagurla has over 20+ years‚Äô experience across Application Integration and Cloud Migrations across Multiple Cloud providers. He works with AWS Partners to build solutions on AWS that our joint customers can use. Prior to joining AWS, Kranthi was a strategic advisor at MuleSoft (now Salesforce). Kranthi has experience advising C-level customer executives on their digital transformation journey in the cloud. 
Shitij Agarwal is a Partner Solutions Architect at AWS. He creates joint solutions with strategic ISV partners to deliver value to customers. When not at work, he is busy exploring New York city and the hiking trails that surround it, and going on bike rides. 
Ross Belmont is a Senior Director of Product Management at Salesforce covering Platform Data Services. He has more than 15 years of experience with the Salesforce ecosystem. 
Sharda Rao is a Senior Director of Product Management at Salesforce covering Agentforce Go To Market strategy 
Hunter Reh is an AI Architect at Salesforce and a passionate builder who has developed over 100 agents since the launch of Agentforce. Outside of work, he enjoys exploring new trails on his bike or getting lost in a great book.
‚Ä¢ How Amazon Bedrock powers next-generation account planning at AWS
  At AWS, our sales teams create customer-focused documents called account plans to deeply understand each AWS customer‚Äôs unique goals and challenges, helping account teams provide tailored guidance and support that accelerates customer success on AWS. As our business has expanded, the account planning process has become more intricate, requiring detailed analysis, reviews, and cross-team alignment to deliver meaningful value to customers. This complexity, combined with the manual review effort involved, has led to significant operational overhead. To address this challenge, we launched Account Plan Pulse in January 2025, a generative AI tool designed to streamline and enhance the account planning process. Implementing Pulse delivered a 37% improvement in plan quality year-over-year, while decreasing the overall time to complete, review, and approve plans by 52%. 
In this post, we share how we built Pulse using Amazon Bedrock to reduce review time and provide actionable account plan summaries for ease of collaboration and consumption, helping AWS sales teams better serve our customers. Amazon Bedrock is a comprehensive, secure, and flexible service for building generative AI applications and agents. It connects you to leading foundation models (FMs), services to deploy and operate agents, and tools for fine-tuning, safeguarding, and optimizing models, along with knowledge bases to connect applications to your latest data so that you have everything you need to quickly move from experimentation to real-world deployment. 
Challenges with increasing scale and complexity 
As AWS continued to grow and evolve, our account planning processes needed to adapt to meet increasing scale and complexity. Before enterprise-ready large language models (LLMs) became available through Amazon Bedrock, we explored rule-based document processing to evaluate account plans, which proved inadequate for handling nuanced content and growing document volumes. By 2024, three critical challenges had emerged: 
 
 Disparate plan quality and format ‚Äì With teams operating across numerous AWS Regions and serving customers in diverse industries, account plans naturally developed variations in structure, detail, and format. This inconsistency made it difficult to make sure critical customer needs were described effectively and consistently. Additionally, the evaluation of account plan quality was inherently subjective, relying heavily on human judgment to assess each plan‚Äôs depth, strategic alignment, and customer focus. 
 Resource-intensive review process ‚Äì The quality assessment process relied on manual reviews by sales leadership. Though thorough, these reviews consumed valuable time that could otherwise be devoted to strategic customer engagements. As our business scaled, this approach created bottlenecks in plan approval and implementation. 
 Knowledge silos ‚Äì We identified untapped potential for cross-team collaboration. Developing methods to extract and share knowledge would transform individual account plans into collective best practices to better serve our customers. 
 
Solution overview 
To address these challenges, we designed Pulse, a generative AI solution that uses Amazon Bedrock to analyze and improve account plans. The following diagram illustrates the solution workflow. 
 
The workflow consists of the following steps: 
 
 Account plan narrative content is pulled from our CRM system on a scheduled basis through an asynchronous batch processing pipeline. 
 The data flows through a series of processing stages: 
   
   Preprocessing to structure and normalize the data and generate metadata. 
   LLM inference to analyze content and generate insights. 
   Validation to confirm quality and compliance. 
    
 Results are stored securely for reporting and dashboard visualization. 
 
We‚Äôve integrated Pulse directly with existing sales workflows to maximize user adoption and have established feedback loops that continuously refine performance. The following diagram shows the solution architecture. 
 
In the following sections, we explore the key components of the solution in more detail. 
Ingestion 
We implement a batch processing pipeline that extracts account plans from our CRM system into Amazon Simple Storage Service (Amazon S3) buckets. A scheduler triggers this pipeline on a regular cadence, facilitating continuous analysis of the most current information. 
Preprocessing 
Considering the dynamic nature of account plans, they are processed in daily snapshots, with only updated plans included in each run. Preprocessing is conducted at two layers: an extract, transform, and load (ETL) flow layer to organize required files to be processed, and just before model calls as part of input validation. This approach, using the plan‚Äôs last modified date, is crucial for avoiding multiple runs on the same content. The preprocessing pipeline handles the daily scheduled job that reads account plan data stored as Parquet files in Amazon S3, extracts text content from HTML fields, and generates structured metadata for each document. To optimize processing efficiency, the system compares document timestamps to process only recently modified plans, significantly reducing computational overhead and costs. The processed text content and metadata are then transformed into a standardized format and stored back to Amazon S3 as Parquet files, creating a clean dataset ready for LLM analysis. 
Analysis with Amazon Bedrock 
The core of our solution uses Amazon Bedrock, which provides a variety of model choices and control, data customization, safety and guardrails, cost optimization, and orchestration. We use the Amazon Bedrock FMs to perform two key functions: 
 
 Account plan evaluation ‚Äì Pulse evaluates plans against 10 business-critical categories, creating a standardized Account Plan Readiness Index. This automated evaluation identifies improvement areas with specific improvement recommendations. 
 Actionable insights ‚Äì Amazon Bedrock extracts and synthesizes patterns across plans, identifying customer strategic focus and market trends that might otherwise remain isolated in individual documents. 
 
We implement these capabilities through asynchronous batch processing, where evaluation and summarization workloads operate independently. The evaluation process runs each account through 27 specific questions with tailored control prompts, and the summarization process generates topical overviews for straightforward consumption and knowledge sharing. 
For this implementation, we use structured output prompting with schema constraints to provide consistent formatting that integrates with our reporting tools. 
Validation 
Our validation framework includes the following components: 
 
 Input and output validations are critical as part of the OWASP Top 10 for Large Language Model Applications. The input validation is essential by the introduction of necessary guardrails and prompt validation, and the output validation makes sure the results are structured and constrained to expected responses. 
 Automated quality and compliance checks against established business rules. 
 Additional review for outputs that don‚Äôt meet quality thresholds. 
 A feedback mechanism that improves system accuracy over time. 
 
Storage and visualization 
The solution includes the following storage and visualization components: 
 
 Amazon S3 provides secure storage for all processed account plans and insights. 
 A daily run cadence refreshes insight and enables progress tracking. 
 Interactive dashboards offer both executive summaries and detailed plan views. 
 
Engineering for production: Building reliable AI evaluations 
When transitioning Pulse from prototype to production, we implemented a robust engineering framework to address three critical AI-specific challenges. First, the non-deterministic nature of LLMs meant identical inputs could produce varying outputs, potentially compromising evaluation consistency. Second, account plans naturally evolve throughout the year with customer relationships, making static evaluation methods insufficient. Third, different AWS teams prioritize different aspects of account plans based on specific customer industry and business needs, requiring flexible evaluation criteria. To maintain evaluation reliability, we developed a statistical framework using Coefficient of Variation (CoV) analysis across multiple model runs on account plan inputs. The goal is to use the CoV as a correction factor to address the data dispersion, which we achieved by calculating the overall CoV at the evaluated question level. With this approach, we can scientifically measure and stabilize output variability, establish clear thresholds for selective manual reviews, and detect performance shifts requiring recalibration. Account plans falling within confidence thresholds proceed automatically in the system, and those outside established thresholds are flagged for manual review. We complemented this with a dynamic threshold weighting system that aligns evaluations with organizational priorities by assigning different weights to criteria based on business impact. This customizes thresholds across different account types‚Äîfor example, applying different evaluation parameters to enterprise accounts versus mid-market accounts. These business thresholds undergo periodic review with sales leadership and adjustment based on feedback, so our AI evaluations remain relevant while maintaining quality and saving valuable time. 
Conclusion 
In this post, we shared how Pulse, powered by Amazon Bedrock, has transformed the account planning process for AWS sales teams. Through automated reviews and structured validation, Pulse streamlines quality assessments and breaks down knowledge silos by surfacing actionable customer intelligence across our global organization. This helps our sales teams spend less time on reviews and more time making data-driven decisions for strategic customer engagements. 
Looking ahead, we‚Äôre excited to enhance Pulse‚Äôs capabilities to measure account plan execution by connecting strategic planning with sales activities and customer outcomes. By analyzing account plan narratives, we aim to identify and act on new opportunities, creating deeper insights into how strategic planning drives customer success on AWS. 
We aim to continue to use the new capabilities of Amazon Bedrock for enhanced and robust improvements to our processes. By building flows for orchestrating our workflows, use of Amazon Bedrock Guardrails, introduction of agentic frameworks, and use of Strands Agents and Amazon Bedrock AgentCore, we can make a more dynamic flow in the future. 
To learn more about Amazon Bedrock, refer to the Amazon Bedrock User Guide, Amazon Bedrock Workshop: AWS Code Samples, AWS Workshops, and Using generative AI on AWS for diverse content types. For the latest news on AWS, see What‚Äôs New with AWS? 
 
About the authors 
Karnika Sharma is a Senior Product Manager in the AWS Sales, Marketing, and Global Services (SMGS) org, where she works on empowering the global sales organization to accelerate customer growth with AWS. She‚Äôs passionate about bridging machine learning and AI innovation with real-world impact, building solutions that serve both business goals and broader societal needs. Outside of work, she finds joy in plein air sketching, biking, board games, and traveling. 
Dayo Oguntoyinbo is a Sr. Data Scientist with the AWS Sales, Marketing, and Global Services (SMGS) Organization. He helps both AWS internal teams and external customers take advantage of the power of AI/ML technologies and solutions. Dayo brings over 12 years of cross-industry experience. He specializes in reproducible and full-lifecycle AI/ML, including generative AI solutions, with a focus on delivering measurable business impacts. He has MSc. (Tech) in Communication Engineering. Dayo is passionate about advancing generative AI/ML technologies to drive real-world impact. 
Mihir Gadgil is a Senior Data Engineer in the AWS Sales, Marketing, and Global Services (SMGS) org, specializing in enterprise-scale data solutions and generative AI applications. With 9+ years of experience and a Master‚Äôs in Information Technology &amp; Management, he focuses on building robust data pipelines, complex data modeling, and ETL/ELT processes. His expertise drives business transformation through innovative data engineering solutions, advanced analytics capabilities. 
Carlos Chinchilla is a Solutions Architect at Amazon Web Services (AWS), where he works with customers across EMEA to implement AI and machine learning solutions. With a background in telecommunications engineering from the Technical University of Madrid, he focuses on building AI-powered applications using both open source frameworks and AWS services. His work includes developing AI assistants, machine learning pipelines, and helping organizations use cloud technologies for innovation. 
Sofian Hamiti is a technology leader with over 10 years of experience building AI solutions, and leading high-performing teams to maximize customer outcomes. He is passionate in empowering diverse talent to drive global impact and achieve their career aspirations. 
Sujit Narapareddy, Head of Data &amp; Analytics at AWS Global Sales, is a technology leader driving global enterprise transformation. He leads data product and platform teams that power AWS‚Äôs Go-to-Market through AI-augmented analytics and intelligent automation. With a proven track record in enterprise solutions, he has transformed sales productivity, data governance, and operational excellence. Previously at JPMorgan Chase Business Banking, he shaped next-generation FinTech capabilities through data innovation.

‚∏ª