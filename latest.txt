‚úÖ Morning News Briefing ‚Äì September 18, 2025 10:42

üìÖ Date: 2025-09-18 10:42
üè∑Ô∏è Tags: #briefing #ai #publichealth #digitalgov

‚∏ª

üßæ Weather
‚Ä¢ Current Conditions:  8.6¬∞C
  Temperature: 8.6&deg;C Pressure / Tendency: 101.4 kPa falling Humidity: 99 % Humidity : 99 % Dewpoint: 8 .3&deg:C Wind: SSW 4 km/h . Air Quality Health Index: n/a . Pembroke 6:00 AM EDT Thursday 18 September 2025 . Weather forecast: Pem
‚Ä¢ Thursday: Chance of showers. High 22. POP 30%
  Increasing cloudiness early this morning. 30 percent chance of showers late this morning and early this afternoon . Wind becoming northwest 20 km/h this afternoon. Humidex 27. UV index 4 or moderate. High 22.50/20/50/50 . Forecast issued 5:00 AM EDT Thursday 18 September 2025. Weather forecast: Showers, thunderstorms, rain, wind
‚Ä¢ Thursday night: Mainly cloudy. Low plus 5.
  Cloudy. Becoming partly cloudy near midnight . Wind north 20 km/h gusting to 40. Low plus 5.50¬∞F (LZ) in the early hours of Thursday morning . Cloudy, wet and windy, with rain showers forecast for the night . Forecast issued 5:00 AM EDT Thursday 18 September 2025 . Weather forecasters predict the weather will be

üåç International News
No updates.

üçÅ Canadian News
No updates.

üá∫üá∏ U.S. Top Stories
‚Ä¢ How CDC's vaccine advisers could affect policy. And, Jimmy Kimmel pulled off the air
  CDC vaccine advisers meet today to discuss recommendations for COVID vaccines and childhood shots . ABC suspends Jimmy Kimmel's late-night show after his remakrs about the killing of Charlie Kirk . Kimmel's show has been suspended after his comments about Charlie Kirk's killing . Kimmel is the host of the Jimmy Kimmel Late Night Show, which airs on ABC at 8 p.m. ET .
‚Ä¢ New policies are making life harder for trans people ‚Äî and prompting big financial decisions
  White House executive orders and legislation in many states have targeted the rights and protections of trans people . For some, that has meant increased financial worry . For others, it means increased financial worries for trans women and those who want to live in the U.S. For many, it has been difficult to find a way to live as a trans woman in the United States. For some people
‚Ä¢ Why was Kirk killed? Evidence paints complicated picture of alleged assassin
  The killing of conservative activist Charlie Kirk has unleashed a frenzy of recrimination and finger-pointing . But the suspect's politics may be less clear than some say . The suspect's political affiliations may not be clear, according to some experts . Charlie Kirk was a conservative conservative activist who spoke out in support of conservative causes . He was shot and killed by police in New York City .
‚Ä¢ CDC's vaccine advisers meet this week. Here's how they could affect policy
  Health Secretary Robert F. Kennedy, Jr. chose everyone in the group . Their votes could affect vaccine access for certain childhood vaccines and and and the COVID shots . Here's what's at stake for those who vote for access to certain vaccines and those who don't have the right to get the shots . The group's votes could also affect access to some vaccines and other immunizations .
‚Ä¢ Kirk shooting videos spread online, even to viewers who didn't want to see them
  Graphic videos of the Charlie Kirk shooting spread widely online, raising concerns over the emotional and political toll of exposure to violent imagery . Graphic videos have been viewed as an example of the dangers of violent imagery in the U.S. State of Columbia, Georgia, Mississippi, where the shooting took place in 2011 . The state of Columbia University is investigating the circumstances surrounding the death of Charlie Kirk .

üß† Artificial Intelligence
No updates.

üíª Digital Strategy
‚Ä¢ Panda-monium: China-backed cyber crew spoof Congressman to dig for dirt on US trade talks
  Proofpoint spots efforts to spy on US economic policy nerds . Chinese state-aligned online attackers are back at it, targeting US trade policy wonks as Washington and Beijing spar over economic ties . US trade wonks are targeted as Beijing and Washington spar over trade ties . Proofpoint: China is targeting US policy nerds as Washington, Beijing spar on economic ties, but it's not the first
‚Ä¢ China's DeepSeek applying trial-and-error learning to its AI 'reasoning'
  Chinese AI company DeepSeek has shown it can improve the reasoning of its LLM . Model through trial-and-error based reinforcement learning . Model can even be made to explain its reasoning on math and coding problems, even though explanations might sometimes be unintelligible. Model can also explain its answers, researchers find . Back to Mail Online home .Back to the page you came from
‚Ä¢ Microsoft weaves Oracle and BigQuery data mirroring into Fabric platform
  Microsoft is extending its Fabric cloud-based data platform by including Oracle and Google's BigQuery data warehouse in its mirroring capability . A new graph database based on an in-house LinkedIn project will be based on LinkedIn cast-offs . The company is extending Fabric's Fabric to include Oracle, Google and Google data warehouse data warehouses in its Mirroring capability, and launching a graph database .
‚Ä¢ How and why Linux has thrived after three decades in Kernelland
  LWN editor and long-time kernel developer Jonathan Corbet shared a long-term perspective on how and why Linux has thrived for a third of a century . Corbet: 'Just a hobby, won't be big and professional like GNU...' Open Source Summit¬† At OSS EU, Corbet said Linux has 'thrilled' for over a third century . L
‚Ä¢ Toys can tell us a lot about how tech will change our lives
  LEGO Mindstorms, PlayStation 2 and Furby all resonate today in their own way . Twenty-five years ago this month I published a book called The Playful World . The seeds of the future can be found in the present by considering the dazzling toys we started giving our children at the turn of the millennium . The toys we gave our children are still resonating today in our own way

üè• Public Health
No updates.

üî¨ Science
‚Ä¢ My career switch from psychologist to open-science advocate
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ A national cohort study of long-term opioid prescription and sociodemographic and health care-related risk factors
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Prevalence and risk factors of viral hepatitis and HIV among people experiencing homelessness in Germany based on a nationwide study
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ How billions of hacked mosquitoes and a vaccine could beat the deadly dengue virus
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Quality of life improvements associated with weight loss using a novel shape-shifting hydrogel capsule: RESET study results
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations

üßæ Government & Policy
No updates.

üèõÔ∏è Enterprise Architecture & IT Governance
No updates.

ü§ñ AI & Emerging Tech
‚Ä¢ Clean hydrogen is facing a big reality check
  Hydrogen is sometimes held up as a master key for the energy transition . It can be made using several low-emissions methods and could play a role in cleaning up industries ranging from agriculture and chemicals to aviation and long-distance shipping . A number of major projects face cancellations and delays, especially in the US and Europe . Still, there are bright spots for the industry, including in China, and new markets could soon become crucial for growth .
‚Ä¢ AI-designed viruses are here and already killing bacteria
  Artificial intelligence can draw cat pictures and write emails. Now the same technology can compose a working genome.



A research team in California says it used AI to propose new genetic codes for viruses‚Äîand managed to get several of these viruses to replicate and kill bacteria.



The scientists, based at Stanford University and the nonprofit Arc Institute, both in Palo Alto, say the germs with AI-written DNA represent the ‚Äúthe first generative design of complete genomes.‚Äù



The work, described in a preprint paper, has the potential to create new treatments and accelerate research into artificially engineered cells. It is also an ‚Äúimpressive first step‚Äù toward AI-designed life forms, says Jef Boeke, a biologist at NYU Langone Health, who was provided an advance copy of the paper by MIT Technology Review.¬†¬†



Boeke says the AI‚Äôs performance was surprisingly good and that its ideas were unexpected. ‚ÄúThey saw viruses with new genes, with truncated genes, and even different gene orders and arrangements,‚Äù he says.



This is not yet AI-designed life, however. That‚Äôs because viruses are not alive. They‚Äôre more like renegade bits of genetic code with relatively puny, simple genomes.¬†





In the new work, researchers at the Arc Institute sought to develop variants of a bacteriophage‚Äîa virus that infects bacteria‚Äîcalled phiX174, which has only 11 genes and about 5,000 DNA letters.



To do so, they used two versions of an AI called Evo, which works on the same principles as large language models like ChatGPT. Instead of feeding them textbooks and blog posts to learn from, the scientists trained the models on the genomes of about 2 million other bacteriophage viruses.



But would the genomes proposed by the AI make any sense? To find out, the California researchers chemically printed 302 of the genome designs as DNA strands and then mixed those with E. coli bacteria.



That led to a profound ‚ÄúAI is here‚Äù moment when, one night, the scientists saw plaques of dead bacteria in their petri dishes. They later took microscope pictures of the tiny viral particles, which look like fuzzy dots.



‚ÄúThat was pretty striking, just actually seeing, like, this AI-generated sphere,‚Äù says Brian Hie, who leads the lab at the Arc Institute where the work was carried out.



Overall, 16 of the 302 designs ended up working‚Äîthat is, the computer-designed phage started to replicate, eventually bursting through the bacteria and killing them.



J. Craig Venter, who created some of the first organisms with lab-made DNA nearly two decades ago, says the AI methods look to him like ‚Äújust a faster version of trial-and-error experiments.‚Äù



For instance, when a team he led managed to create a bacterium with a lab-printed genome in 2008, it was after a long hit-or-miss process of testing out different genes. ‚ÄúWe did the manual AI version‚Äîcombing through the literature, taking what was known,‚Äù he says.¬†



But speed is exactly why people are betting AI will transform biology. The new methods already claimed a Nobel Prize in 2024 for predicting protein shapes. And investors are staking billions that AI can find new drugs. This week a Boston company, Lila, raised $235 million to build automated labs run by artificial intelligence.



Computer-designed viruses could also find commercial uses. For instance, doctors have sometimes tried ‚Äúphage therapy‚Äù to treat patients with serious bacterial infections. Similar tests are underway to cure cabbage of black rot, also caused by bacteria.



‚ÄúThere is definitely a lot of potential for this technology,‚Äù says Samuel King, the student who spearheaded the project in Hei‚Äôs lab. He notes that most gene therapy uses viruses to shuttle genes into patients‚Äô bodies, and AI might develop more effective ones.



The Stanford researchers say they purposely haven‚Äôt taught their AI about viruses that can infect people. But this type of technology does create the risk that other scientists‚Äîout of curiosity, good intentions, or malice‚Äîcould turn the methods on human pathogens, exploring new dimensions of lethality.



‚ÄúOne area where I urge extreme caution is any viral enhancement research, especially when it‚Äôs random so you don‚Äôt know what you are getting,‚Äù says Venter. ‚ÄúIf someone did this with smallpox or anthrax, I would have grave concerns.‚Äù



Whether an AI can generate a bona fide genome for a larger organism remains an open question. For instance, E. coli has about a thousand times more DNA code than phiX174 does.¬†‚ÄúThe complexity would rocket from staggering to ‚Ä¶ way way more than the number of subatomic particles in the universe,‚Äù says Boeke.



Also, there‚Äôs still no easy way to test AI designs for larger genomes. While some viruses can ‚Äúboot up‚Äù from just a DNA strand, that‚Äôs not the case with a bacterium, a mammoth, or a human. Scientists would instead have to gradually change an existing cell with genetic engineering‚Äîa still laborious process.



Despite that, Jason Kelly, the CEO of Ginkgo Bioworks, a cell-engineering company in Boston, says exactly such an effort is needed. He believes it could be carried out in ‚Äúautomated‚Äù laboratories where genomes get proposed and tested and the results are fed back to AI for further improvement.



¬†‚ÄúThis would be a nation-scale scientific milestone, as cells are the building blocks of all life,‚Äù says Kelly. ‚ÄúThe US should make sure we get to it first.‚Äù
‚Ä¢ The Download: measuring returns on R&D, and AI‚Äôs creative potential
  This is today&#8217;s edition of¬†The Download,¬†our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



How to measure the returns on R&amp;D spending



Given the draconian cuts to US federal funding for science, it‚Äôs worth asking some hard-nosed money questions: How much should we be spending on R&amp;D? How much value do we get out of such investments, anyway?&nbsp;



To answer that, in several recent papers, economists have approached this issue in clever new ways.&nbsp; And, though they ask slightly different questions, their conclusions share a bottom line: R&amp;D is, in fact, one of the better long-term investments that the government can make. Read the full story.



‚ÄîDavid Rotman



This article is part of MIT Technology Review Explains, our series untangling the complex, messy world of technology to help you understand what‚Äôs coming next. You can read more from the series here.



If you‚Äôre interested in reading more about America‚Äôs economic situation, check out:



+ Sweeping tariffs could threaten the US manufacturing rebound‚Äîand they could stunt its ability to make tomorrow&#8217;s breakthroughs. Read the full story.+ The surprising barrier that keeps us from building the housing we need. Read the full story.+ How to fine-tune AI for prosperity.+ People are worried that AI will take everyone‚Äôs jobs. We‚Äôve been here before.







MIT Technology Review Narrated: How AI can help supercharge creativity



Forget one-click creativity. Artists and musicians are finding new ways to make art using AI, by injecting friction, challenge, and serendipity into the process.



This is our latest story to be turned into a MIT Technology Review Narrated podcast, which we‚Äôre publishing each week on Spotify and Apple Podcasts. Just navigate to MIT Technology Review Narrated on either platform, and follow us to get all our new content as it‚Äôs released.







The must-reads



I‚Äôve combed the internet to find you today‚Äôs most fun/important/scary/fascinating stories about technology.



1 TikTok‚Äôs buyers may include Oracle, Silver Lake and Andreessen Horowitz¬†They would control around 80% of the business, with Chinese shareholders holding the rest. (WSJ $)+ We still have plenty of unanswered questions about the deal. (Bloomberg $)+ It was brokered in Madrid. (The Guardian)



2 OpenAI is working on a version of ChatGPT for teenagersAnd it‚Äôll use age-prediction tech to bar them from the standard version. (Axios)+ The move comes as the US Senate is hearing evidence about chatbot harms. (404 Media)+ The looming crackdown on AI companionship. (MIT Technology Review)3 China has banned tech firms from buying Nvidia‚Äôs chipsIn an effort to boost its own companies. (FT $)+ Alibaba and ByteDance have been instructed to terminate orders. (Bloomberg $)



4 Anthropic refuses to let US law enforcement use its modelsMuch to the White House‚Äôs chagrin. (Semafor)5 Tesla‚Äôs doors may trap passengers inside its carsVehicle safety regulators are investigating after people reported being forced to break windows to retrieve children. (NYT $)¬†¬†



6 How AI companies train their models to do white-collar jobs¬†¬†After hitting a wall, they‚Äôre throwing money at the problem. (The Information $)+ New training ‚Äòenvironments‚Äô are a hot AI topic right now.¬† (TechCrunch)+ How AI is shaking up corporate hierarchies. (WSJ $)



7 Inside Damascus‚Äô bid to become a tech hubThe city‚Äôs tech industry has been embraced by its new government. (Rest of World)



8 A supply shipment to the ISS has been delayedNASA is blaming engine trouble. (Ars Technica)+ The great commercial takeover of low Earth orbit. (MIT Technology Review)



9 Our darkest nights are getting lighterArtificial light is ruining our chances of seeing starry skies. (IEEE Spectrum)+ Bright LEDs could spell the end of dark skies. (MIT Technology Review)



10 You can now book a safari through Uber Expedition into Nairobi National Park, anyone? (Bloomberg $)







Quote of the day



‚ÄúWhat began as a homework helper gradually turned itself into a confidant and then a suicide coach.‚Äù



‚ÄîMatthew Raine, whose 16-year old son Adam died by suicide after repeatedly sharing his intentions with ChatGPT, gives evidence to a Senate Judiciary subcommittee investigating chatbot dangers, the Washington Post reports.







One more thing







AI is coming for music, tooWhile large language models that generate text have exploded in the last three years, a different type of AI, based on what are called diffusion models, is having an unprecedented impact on creative domains.By transforming random noise into coherent patterns, diffusion models can generate new images, videos, or speech, guided by text prompts or other input data. The best ones can create outputs indistinguishable from the work of peopleNow these models are marching into a creative field that is arguably more vulnerable to disruption than any other: music. Read the full story.



‚ÄîJames O&#8217;Donnell







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)



+ Food, in all shapes and forms, is bigger than ever. So why aren‚Äôt we watching cooking shows any more?+ Kate Bush‚Äôs Hounds of Love turns 40 this year, but still sounds as fresh as ever.+ Here‚Äôs how to maximize your chances of booking a bargain flight.+ Robert Redford, you were one of a kind.
‚Ä¢ How to measure the returns on R&D spending
  MIT Technology Review Explains: Let our writers untangle the complex, messy world of technology to help you understand what‚Äôs coming next. You can read more from the series here.



Given the draconian cuts to US federal funding for science, including the administration‚Äôs proposal to reduce the 2026 budgets of the National Institutes of Health by 40% and the National Science Foundation by 57%, it‚Äôs worth asking some hard-nosed money questions: How much should we be spending on R&amp;D? How much value do we get out of such investments, anyway? To answer that, it‚Äôs important to look at both successful returns and investments that went nowhere.





Sure, it‚Äôs easy to argue for the importance of spending on science by pointing out that many of today‚Äôs most useful technologies had their origins in government-funded R&amp;D. The internet, CRISPR, GPS‚Äîthe list goes on and on. All true. But this argument ignores all the technologies that received millions in government funding and haven‚Äôt gone anywhere‚Äîat least not yet. We still don‚Äôt have DNA computers or molecular electronics. Never mind the favorite examples cited by contrarian politicians of seemingly silly or frivolous science projects (think shrimp on treadmills).



While cherry-picking success stories help illustrate the glories of innovation and the role of science in creating technologies that have changed our lives, it provides little guidance for how much we should spend in the future‚Äîand where the money should go.



A far more useful approach to quantifying the value of R&amp;D is to look at its return on investment (ROI). A favorite metric for stock pickers and PowerPoint-wielding venture capitalists, ROI weighs benefits versus costs. If applied broadly to the nation‚Äôs R&amp;D funding, the same kind of thinking could help account for both the big wins and all the money spent on research that never got out of the lab.



The problem is that it‚Äôs notoriously difficult to calculate returns for science funding‚Äîthe payoffs can take years to appear and often take a circuitous route, so the eventual rewards are distant from the original funding. (Who could have predicted Uber as an outcome of GPS? For that matter, who could have predicted that the invention of ultra-precise atomic clocks in the late 1940s and 1950s would eventually make GPS possible?) And forget trying to track the costs of countless failures or apparent dead ends.



But in several recent papers, economists have approached the problem in clever new ways, and though they ask slightly different questions, their conclusions share a bottom line: R&amp;D is, in fact, one of the better long-term investments that the government can make.



This story is part of MIT Technology Review‚Äôs &#8220;America Undone‚Äù series, examining how the foundations of US success in science and innovation are currently under threat.¬†You can read the rest here.



That might not seem very surprising. We‚Äôve long thought that innovation and scientific advances are key to our prosperity. But the new studies provide much-needed details, supplying systematic and rigorous evidence for the impact that R&amp;D funding, including public investment in basic science, has on overall economic growth.



And the magnitude of the benefits is surprising.



Bang for your buck



In ‚ÄúA Calculation of the Social Returns to Innovation,‚Äù Benjamin Jones, an economist at Northwestern University, and Lawrence Summers, a Harvard economist and former US Treasury secretary, calculate the effects of the nation‚Äôs total R&amp;D spending on gross domestic product and our overall standard of living. They‚Äôre taking on the big picture, and it‚Äôs ambitious because there are so many variables. But they are able to come up with a convincing range of estimates for the returns, all of them impressive.



On the conservative end of their estimates, says Jones, investing $1 in R&amp;D yields about $5 in returns‚Äîdefined in this case as additional GDP per person (basically, how much richer we become). Change some of the assumptions‚Äîfor example, by attempting to account for the value of better medicines and improved health care, which aren‚Äôt fully captured in GDP‚Äîand you get even larger payoffs.



While the $5 return is at the low end of their estimates, it‚Äôs still ‚Äúa remarkably good investment,‚Äù Jones says. ‚ÄúThere aren‚Äôt many where you put in $1 and get $5 back.&#8221;



That‚Äôs the return for the nation‚Äôs overall R&amp;D funding. But what do we get for government-funded R&amp;D in particular? Andrew Fieldhouse, an economist at Texas A&amp;M, and Karel Mertens at the Federal Reserve Bank of Dallas looked specifically at how changes in public R&amp;D spending affect the total factor productivity (TFP) of businesses. A favorite metric of economists, TFP is driven by new technologies and innovative business know-how‚Äînot by adding more workers or machines‚Äîand is the main driver of the nation‚Äôs prosperity over the long term.



The economists tracked changes in R&amp;D spending at five major US science funding agencies over many decades to see how the shifts eventually affected private-sector productivity. They found that the government was getting a huge bang for its nondefense R&amp;D buck.





The benefits begin kicking in after around five to 10 years and often have a long-lasting impact on the economy. Nondefense public R&amp;D funding has been responsible for 20% to 25% of all private-sector productivity growth in the country since World War II, according to the economists. It‚Äôs an astonishing number, given that the government invests relatively little in nondefense R&amp;D. For example, its spending on infrastructure, another contributor to productivity growth, has been far greater over those years.



The large impact of public R&amp;D investments also provides insight into one of America‚Äôs most troubling economic mysteries: the slowdown in productivity growth that began in the 1970s, which has roiled the country‚Äôs politics as many people face stunted living standards and limited financial prospects. Their research, says Fieldhouse, suggests that as much as a quarter of that slowdown was caused by a decline in public R&amp;D funding that happened roughly over the same time.



After reaching a high of 1.86% of GDP in 1964, federal R&amp;D spending began dropping. Starting in the early 1970s, TFP growth also began to decline, from above 2% a year in the late 1960s to somewhere around 1% since the 1970s (with the exception of a rise during the late 1990s), roughly tracking the spending declines with a lag of a few years.



If in fact the productivity slowdown was at least partially caused by a drop in public R&amp;D spending, it‚Äôs evidence that we would be far richer today if we had kept up a higher level of science investment. And it also flags the dangers of today‚Äôs proposed cuts. ‚ÄúBased on our research,‚Äù says Fieldhouse, ‚ÄúI think it‚Äôs unambiguously clear that if you actually slash the budget of the NIH by 40%, if you slash the NSF budget by 50%, there‚Äôs going to be a deceleration in US productivity growth over the next seven to 10 years that will be measurable.‚Äù



Out of whack



Though the Trump administration‚Äôs proposed 2026 budget would slash science budgets to an unusual degree, public funding of R&amp;D has actually been in slow decline for decades. Federal funding of science is at its lowest rate in the last 70 years, accounting for only around 0.6% of GDP.







Even as public funding has dropped, business R&amp;D investments have steadily risen. Today businesses spend far more than the government; in 2023, companies invested about $700 billion in R&amp;D while the US government spent $172 billion, according to data from the NSF‚Äôs statistical agency. You might think, Good‚Äîlet companies do research. It‚Äôs more efficient. It‚Äôs more focused. Keep the government out of it.



But there is a big problem with that argument. Publicly funded research, it turns out, tends to lead to relatively more productivity growth over time because it skews more toward fundamental science than the applied work typically done by companies.



In a new working paper called ‚ÄúPublic R&amp;D Spillovers and Productivity Growth,‚Äù Arnaud Dy√®vre, an assistant professor at of economics at HEC Paris, documents the broad and often large impacts of so-called knowledge spillovers‚Äîthe benefits that flow to others from work done by the original research group. Dy√®vre found that the spillovers of public-funded R&amp;D have three times more impact on productivity growth across businesses and industries than those from private R&amp;D funding.



The findings are preliminary, and Dy√®vre is still updating the research‚Äîmuch of which he did as a postdoc at MIT‚Äîbut he says it does suggest that the US ‚Äúis underinvesting in fundamental R&amp;D,‚Äù which is heavily funded by the government. ‚ÄúI wouldn‚Äôt be able to tell you exactly which percentage of R&amp;D in the US needs to be funded by the government or what percent needs to be funded by the private sector. We need both,‚Äù he says. But, he adds, ‚Äúthe empirical evidence‚Äù suggests that ‚Äúwe‚Äôre out of balance.‚Äù



The big question



Getting the balance of funding for fundamental science and applied research right is just one of the big questions that remain around R&amp;D funding. In mid-July, Open Philanthropy and the Alfred P. Sloan Foundation, both nonprofit organizations, jointly announced that they planned to fund a five-year ‚Äúpop-up journal‚Äù that would attempt to answer many of the questions still swirling around how to define and optimize the ROI of research funding.



‚ÄúThere is a lot of evidence consistent with a really high return to R&amp;D, which suggests we should do more of it,‚Äù says Matt Clancy, a senior program officer at Open Philanthropy. ‚ÄúBut when you ask me how much more, I don‚Äôt have a good answer. And when you ask me what types of R&amp;D should get more funding, we don‚Äôt have a good answer.‚Äù



Pondering such questions should keep innovation economists busy for the next several years. But there is another mystifying piece of the puzzle, says Northwestern‚Äôs Jones. If the returns on R&amp;D investments are so high‚Äîthe kind that most venture capitalists or investors would gladly take‚Äîwhy isn‚Äôt the government spending more?





&#8220;I think it‚Äôs unambiguously clear that if you actually slash the budget of the NIH by 40%, if you slash the NSF budget by 50%, there‚Äôs going to be a deceleration in US productivity growth over the next seven to 10 years that will be measurable.&#8221;





Jones, who served as a senior economic advisor in the Obama administration, says discussions over R&amp;D budgets in Washington are often ‚Äúa war of anecdotes.‚Äù Science advocates cite the great breakthroughs that resulted from earlier government funding, while budget hawks point to seemingly ludicrous projects or spectacular failures. Both have plenty of ammunition. ‚ÄúPeople go back and forth,‚Äù says Jones, ‚Äúand it doesn‚Äôt really lead to anywhere.‚Äù



The policy gridlock is rooted in in the very nature of fundamental research. Today‚Äôs science will lead to great advances. And there will be countless failures; a lot of money will be wasted on fruitless experiments. The problem, of course, is that when you‚Äôre deciding to fund new projects, it‚Äôs impossible to predict which the outcome will be, even in the case of odd, seemingly silly science. Guessing just what research will or will not lead to the next great breakthrough is a fool‚Äôs errand.



Take the cuts in the administration‚Äôs proposed fiscal 2026 budget for the NSF, a leading funder of basic science. The administration‚Äôs summary begins with the assertion that its NSF budget ‚Äúis prioritizing investments that complement private-sector R&amp;D and offer strong potential to drive economic growth and strengthen U.S. technological leadership.‚Äù So far, so good. It cites the government‚Äôs commitment to AI and quantum information science. But dig deeper and you will see the contradictions in the numbers.



Not only is NSF&#8217;s overall budget cut by 57%, but funding for physical sciences like chemistry and materials research‚Äîfields critical to advancing AI and quantum computers‚Äîhas also been blown apart. Funding for the NSF‚Äôs mathematical and physical sciences program was reduced by 67%. The directorate for computer and information science and engineering fared little better; its research funding was cut by 66%.



There is a great deal of hope among many in the science community that Congress, when it passes the actual 2026 budget, will at least partially reverse these cuts. We‚Äôll see. But even if it does, why attack R&amp;D funding in the first place? It‚Äôs impossible to answer that without plunging into the messy depths of today‚Äôs chaotic politics. And it is equally hard to know whether the recent evidence gathered by academic economists on the strong returns to R&amp;D investments will matter when it comes to partisan policymaking.



But at least those defending the value of public funding now have a far more productive way to make their argument, rather than simply touting past breakthroughs. Even for fiscal hawks and those pronouncing concerns about budget deficits, the recent work provides a compelling and simple conclusion: More public funding for basic science is a sound investment that makes us more prosperous.
‚Ä¢ De-risking investment in AI agents
  Automation has become a defining force in the customer experience . The future belongs to organizations that focus on outcome-oriented design . The move from scripted, deterministic flows to non-deterministic, generative systems brings new challenges . How can you balance safety and flexibility when giving an AI system access to core infrastructure? And how can you manage cost, transparency, and ethical risk while still pursuing meaningful returns?

üîí Cybersecurity & Privacy
‚Ä¢ Self-Replicating Worm Hits 180+ Software Packages
  At least 187 code packages made available through the JavaScript repository NPM have been infected with a self-replicating worm that steals credentials from developers and publishes those secrets on GitHub, experts warn.¬†The malware, which briefly infected multiple code packages from the security vendor CrowdStrike, steals and publishes even more credentials every time an infected package is installed.
Image: https://en.wikipedia.org/wiki/Sandworm_(Dune)
The novel malware strain is being dubbed Shai-Hulud &#8212; after the name for the giant sandworms in Frank Herbert&#8217;s Dune novel series &#8212; because it publishes any stolen credentials in a new public GitHub repository that includes the name &#8220;Shai-Hulud.&#8221;
&#8220;When a developer installs a compromised package, the malware will look for a npm token in the environment,&#8221; said Charlie Eriksen, a researcher for the Belgian security firm Aikido. &#8220;If it finds it, it will modify the 20 most popular packages that the npm token has access to, copying itself into the package, and publishing a new version.&#8221;
At the center of this developing maelstrom are code libraries available on NPM (short for ‚ÄúNode Package Manager‚Äù), which acts as a central hub for JavaScript development and provides the latest updates to widely-used JavaScript components.
The Shai-Hulud worm emerged just days after unknown attackers launched a broad phishing campaign that spoofed NPM and asked developers to &#8220;update&#8221; their multi-factor authentication login options. That attack led to malware being inserted into at least two-dozen NPM code packages, but the outbreak was quickly contained and was narrowly focused on siphoning cryptocurrency payments.
Image: aikido.dev
In late August, another compromise of an NPM developer resulted in malware being added to &#8220;nx,&#8221; an open-source code development toolkit with as many as six million weekly downloads. In the nx compromise, the attackers introduced code that scoured the user‚Äôs device for authentication tokens from programmer destinations like GitHub and NPM, as well as SSH and API keys. But instead of sending those stolen credentials to a central server controlled by the attackers, the malicious nx code created a new public repository in the victim‚Äôs GitHub account, and published the stolen data there for all the world to see and download.
Last month&#8217;s attack on nx did not self-propagate like a worm, but this Shai-Hulud malware does and bundles reconnaissance tools to assist in its spread. Namely, it uses the open-source tool TruffleHog to search for exposed credentials and access tokens on the developer&#8217;s machine. It then attempts to create new GitHub actions and publish any stolen secrets.
&#8220;Once the first person got compromised, there was no stopping it,&#8221; Aikido&#8217;s Eriksen told KrebsOnSecurity. He said the first NPM package compromised by this worm appears to have been altered on Sept. 14, around 17:58 UTC.
The security-focused code development platform socket.dev reports the Shai-Halud attack briefly compromised at least 25 NPM code packages managed by CrowdStrike. Socket.dev said the affected packages were quickly removed by the NPM registry.
In a written statement shared with KrebsOnSecurity, CrowdStrike said that after detecting several malicious packages in the public NPM registry, the company swiftly removed them and rotated its keys in public registries.
&#8220;These packages are not used in the Falcon sensor, the platform is not impacted and customers remain protected,&#8221; the statement reads, referring to the company&#8217;s widely-used endpoint threat detection service. &#8220;We are working with NPM and conducting a thorough investigation.&#8221;
A writeup on the attack from StepSecurity found that for cloud-specific operations, the malware enumerates AWS, Azure and Google Cloud Platform secrets. It also found the entire attack design assumes the victim is working in a Linux or macOS environment, and that it deliberately skips Windows systems.
StepSecurity said Shai-Hulud spreads by using stolen NPM authentication tokens, adding its code to the top 20 packages in the victim&#8217;s account.
&#8220;This creates a cascading effect where an infected package leads to compromised maintainer credentials, which in turn infects all other packages maintained by that user,&#8221; StepSecurity&#8217;s Ashish Kurmi wrote.
Eriksen said Shai-Hulud is still propagating, although its spread seems to have waned in recent hours.
&#8220;I still see package versions popping up once in a while, but no new packages have been compromised in the last ~6 hours,&#8221; Eriksen said. &#8220;But that could change now as the east coast starts working. I would think of this attack as a &#8216;living&#8217; thing almost, like a virus. Because it can lay dormant for a while, and if just one person is suddenly infected by accident, they could restart the spread. Especially if there&#8217;s a super-spreader attack.&#8221;
For now, it appears that the web address the attackers were using to exfiltrate collected data was disabled due to rate limits, Eriksen said.
Nicholas Weaver is a researcher with the International Computer Science Institute, a nonprofit in Berkeley, Calif. Weaver called the Shai-Hulud worm &#8220;a supply chain attack that conducts a supply chain attack.&#8221; Weaver said NPM (and all other similar package repositories) need to immediately switch to a publication model that requires explicit human consent for every publication request using a phish-proof 2FA method.
&#8220;Anything less means attacks like this are going to continue and become far more common, but switching to a 2FA method would effectively throttle these attacks before they can spread,&#8221; Weaver said. &#8220;Allowing purely automated processes to update the published packages is now a proven recipe for disaster.&#8221;

üéì University AI
No updates.

üè¢ Corporate AI
‚Ä¢ Supercharge your organization‚Äôs productivity with the Amazon Q Business browser extension
  Generative AI solutions like Amazon Q Business are transforming the way employees work. Organizations in every industry are embracing these tools to help their workforce extract valuable insights from increasingly fragmented data to accelerate decision-making processes. However, the adoption of generative AI tools hasn‚Äôt been without its challenges. 
Two hurdles have emerged in the implementation of generative AI solutions. First, users often find themselves compelled to abandon familiar workflows, manually transferring data to an AI assistant for analysis. This creates unnecessary friction and increases the time to value. Second, the absence of generative AI tools in commonly used software makes it difficult for employees to identify opportunities where AI can significantly boost their productivity. 
Enter Amazon Q Business, a generative AI-powered assistant tailored for the modern workplace, so you can engage in conversations, solve complex problems, and take action by seamlessly connecting to company data and enterprise systems. Amazon Q Business provides employees with instant access to relevant information and advice, streamlining tasks, accelerating decision-making, and fostering creativity and innovation in the workplace. We recently launched the Amazon Q Business browser extension in Amazon Q Business, and it is now available to Amazon Q Business subscribers (Lite and Pro). The Amazon Q Business browser extension brings the power of Amazon Q Business directly into your browsers, so you can receive context-aware, generative AI assistance and get on-the-go help for daily tasks. 
In this post, we show how to implement this solution for your own enterprise, giving your team seamless access to AI-driven insights and assistance. 
Use cases for the Amazon Q Business browser extension 
The Amazon Q Business browser extension is deployed to all Amazonians, making tens of thousands of users more productive every day. In this section, we highlight some of the most impactful use cases for which Amazonians use the Amazon Q Business browser extension to boost their productivity. 
Analyze web content 
Business and technical teams need to analyze and synthesize information across various reports, competitive analyses, and industry documents found outside the company‚Äôs data to develop insights and strategy. They must make sure their strategic recommendations are based on verified data sources and trustworthy industry information. Additionally, identifying patterns across multiple sources is time-consuming and complex. With the Amazon Q Business browser extension, strategists can quickly generate industry insights and identify trends across trusted internal and external data sources in seconds, while maintaining the human element in strategic thinking. 
Check out the following demo video: 

 
  
 
 
Improve content quality 
The Amazon Q Business browser extension brings the unique ability to incorporate context that might not be readily available to your generative AI assistant. You can use the Amazon Q Business browser extension for content creation and content quality improvements by including multiple disparate sources in your queries that typically aren‚Äôt available to generative AI assistants. You can use it to perform real-time validation of content from various sources and incorporate web-based style guides and best practices to accelerate content creation. 
Check out the following demo video: 

 
  
 
 
Solution overview 
In the following sections, we walk through how to get started with the Amazon Q Business browser extension if you have already enabled Amazon Q Business for your organization. To learn more, see Configuring the Amazon Q Business browser extension for use. 
Prerequisites 
Complete the prerequisite steps in this section before deploying the browser extension. 
Create an Amazon Q Business application and subscribe your users 
The Amazon Q Business browser extension is a feature of Amazon Q Business and requires customers to first create an Amazon Q Business application and subscribe their users before the browser extension can be enabled. To learn more about how you can get started with Amazon Q Business, see Getting started with Amazon Q Business. 
Set up the Amazon Q Business web experience 
The browser extension uses the Amazon Q Business web experience client as the mechanism to authenticate users and offer Amazon Q Business features. The first step to enabling the browser extension is to create an Amazon Q Business web experience. If you have already created a web experience for your users, you can skip this step. However, if you have developed a custom web experience using the Amazon Q Business APIs, complete the following steps to create an Amazon Q Business web experience: 
 
 On the Amazon Q Business console, go to your Amazon Q Business application. 
 
The Web experience settings section shows if you already have a web experience deployed. If you don‚Äôt have a web experience deployed, this section will be empty, with the message ‚ÄúA web experience needs to be created before deploying.‚Äù 
 
 
 At the top of your application details page, choose Edit. 
 
 
 
 For Outcome, select Web experience. 
 Choose Update. 
 
This step might take a few minutes to complete. 
 
After your web experience is deployed, you will find a URL where your web experience is hosted on your Amazon Q Business application details page. Save this URL for later. 
 
Grant users access to send queries directly to the large language model 
The Amazon Q Business browser extension can include your users‚Äô web page context in queries by passing the web page content as file attachments alongside a user‚Äôs prompt. Because the file attachment feature is available only for General knowledge mode, the browser extension requires Amazon Q Business admins to grant users access to send queries directly to the large language model (LLM) to take advantage of the full feature set of the browser extension. Without this prerequisite, users can only access their company knowledge through the browser extension and can‚Äôt ask Amazon Q Business questions about their web page content. 
Amazon Q Business does not store user conversation data and does not use queries or conversations for training its LLMs. Conversations are only stored within the application for 30 days. You can delete these conversations by accessing the Amazon Q Business web experience and choosing Chat in the navigation pane, as shown in the following screenshot. 
 
To grant users access to send queries directly to the Amazon Q LLM, complete the following steps: 
 
 On the Amazon Q Business console, go to your application. 
 Choose Admin controls and guardrails in the navigation pane. 
 
 
 
 In the Global controls section, choose Edit. 
 
 
 
 Select Allow end users to send queries directly to the LLM. 
 Choose Save. 
 
 
You are now ready to enable the browser extension for your users. 
Configure the Amazon Q Business browser extension 
Now that you have completed the prerequisites for the browser extension, complete the following steps to enable the browser extension for your users: 
 
 On the Amazon Q Business console, go to your application. 
 Under Enhancements in the navigation pane, choose Integrations. 
 In the Browser extensions section, choose Edit. 
 
 
 
 Select the check boxes for the browser extensions you want to enable: 
   
   The Chromium check box enables the Chrome store extension, which supports Google Chrome and Microsoft Edge browsers. 
   The Firefox check box enables the Firefox Browser add-on for Firefox browsers. 
    
 
You can also view the Chrome or Firefox store pages for the extension using the links in the respective Learn more sections. 
 
 Choose Save. 
 
 
Your users will now see instructions to install the Amazon Q Business browser extension the next time they log in to the Amazon Q Business web experience. If you have not yet done so, share the web experience URL you obtained in the earlier steps with your users so they can follow the steps to install the browser extension. 
Activate the browser extension if you are using IAM federation authentication for Amazon Q Business 
If you‚Äôre using an external identity provider (IdP) for your Amazon Q Business application, you must allow-list the browser extension with the external provider before your users can start using the browser extension. You can allow-list the following URLs with your IdP to activate the browser extension: 
 
 For the Chromium browser extension (suitable for Google Chrome and Microsoft Edge), use https://feihpdljijcgnokhfoibicengfiellbp.chromiumapp.org/ 
 For the Mozilla Firefox browser extension, https://ba6e8e6e4fa44c1057cf5f26fba9b2e788dfc34f.extensions.allizom.org/ 
 
You don‚Äôt need to take the aforementioned steps if you‚Äôre using AWS IAM Identity Center as the authentication solution for your Amazon Q Business application. 
Get started with the browser extension 
After you share the web experience URL with your users, they can use it to find the browser extension store page and install the browser extension. Users can complete the following steps: 
 
 Log in to the Amazon Q Business web experience provided by your admin. 
 
You will notice a banner letting you know that your admin has enabled the browser extension for you. 
 
 Choose Install extension. 
 
 
The link will take you to the appropriate Amazon Q Business browser extension store page based on the browser you‚Äôre using. 
 
 Choose Add to Chrome or the appropriate installation option for your browser. 
 
 
Upon installing the extension, you will find it in your browser‚Äôs tool bar under Extensions. You can choose the pin icon to pin the browser extension. 
 
After you open your browser extension, you will see a side pane as shown in the following screenshot. It will automatically detect the correct web experience URL from your open tabs to help you sign in. If it doesn‚Äôt, enter the web experience URL provided by your admin in the Amazon Q URL section and choose Sign in. 
 
Upon sign in, you‚Äôre ready to go! Refer to the earlier section discussing Amazon‚Äôs use cases for inspiration on how you can use the extension to boost your productivity. 
 
Deploy the Amazon Q Business browser extension on behalf of your users 
Some admins might choose to directly deploy the Amazon Q Business browser extension on their users‚Äô browsers to streamline and accelerate adoption. 
Enterprises use varying mobile device management software and have differing requirements for their browser policies. To deploy the Amazon Q Business browser extension, refer to the following resources: 
 
 Mozilla Firefox policy settings 
 Google Chrome policy settings 
 Microsoft Edge: 
   
   Policy settings 
   Reference guide 
    
 
Customize the Amazon Q Business browser extension for your enterprise 
Some admins might choose to customize the look and feel of the Amazon Q Business browser extension to fit their enterprise‚Äôs needs. This section outlines the extension‚Äôs supported customization functionality and the corresponding browser extension policy values to configure on your users‚Äô browsers. 
Remove the Amazon Q Business URL input from the browser extension login page 
If you don‚Äôt want to require an Amazon Q Business web experience URL from your users at sign-in, you can set a default URL on their behalf by setting the Q_BIZ_BROWSER_EXTENSION_URL policy to the appropriate Amazon Q Business web experience URL for your users. 
 
Replace the browser extension‚Äôs toolbar icon 
You can modify the toolbar icon of your browser extension by setting the value of one or more of the following browser policy keys to the URL of your PNG or SVG image or a valid datauri for your users: 
 
 Q_BIZ_BROWSER_EXTENSION_ICON_128 (mandatory) 
 Q_BIZ_BROWSER_EXTENSION_ICON_16 (optional) 
 Q_BIZ_BROWSER_EXTENSION_ICON_32 (optional) 
 Q_BIZ_BROWSER_EXTENSION_ICON_48 (optional) 
 
 
Replace the logo or icon in the browser extension window 
To change the logo or icon in your browser extension window, set the value of the Q_BIZ_BROWSER_EXTENSION_LOGO policy key with a URL to your PNG or SVG image or a valid datauri for your users. 
 
Modify the name of the browser extension shown in the browser extension window 
To replace references to ‚ÄúAmazon Q,‚Äù ‚ÄúAmazon Q Business,‚Äù ‚ÄúAWS,‚Äù and ‚ÄúAmazon Web Services‚Äù with a name of your choice inside the browser extension window, set the value of the Q_BIZ_BROWSER_EXTENSION_ENTERPRISE_NAME policy key with the new name for your users. 
 
Modify the title of your browser extension in hover text 
To change the title of your browser extension as it shows in the text when hovering over your extension (‚ÄúAmazon Q Business has access to this site,‚Äù as seen in the prior screenshot), set the Q_BIZ_BROWSER_EXTENSION_TITLE_NAME policy to the appropriate string for your users. 
 
Replace the AI policy link in the browser extension footer with your own link 
To replace the link text in the footer of your browser extension, set Q_BIZ_BROWSER_EXTENSION_FOOTER_POLICY_NAME to the appropriate string for your users. 
To replace the URL in the footer of your browser extension, set Q_BIZ_BROWSER_EXTENSION_FOOTER_POLICY_URL to the appropriate URL for your users. 
 
Congratulations! You and your organization are ready to receive generative assistance for your browser-based tasks. 
Clean up 
This section outlines the steps to disable or remove the browser extension or revert deployments and customization for your users. 
Disable the Amazon Q Business browser extension through the Amazon Q Business console 
You can disable the Amazon Q Business browser extension from the Amazon Q Business console whenever you choose, even before removing the browser extension from your users‚Äô browsers. To do so, complete the following steps: 
 
 On the Amazon Q Business console, go to your application. 
 Under Enhancements in the navigation pane, choose Integrations. 
 In the Browser extensions section, choose Edit. 
 
 
 
 Deselect the check boxes for the browser extensions you want to disable: 
   
   The Chromium check box disables the Chrome store extension, which supports Google Chrome and Microsoft Edge browsers. 
   The Firefox check box disables the Firefox Browser add-on for Firefox browsers. 
    
 Choose Save. 
 
 
Revert the deployment of the Amazon Q Business browser extension on behalf of your users 
Enterprises use varying mobile device management software and have differing requirements for their browser policies. If you deployed the browser extension by updating your browser policy settings, you should remove those policies by following the guidance in the policy settings documentation for the respective browsers: 
 
 Mozilla Firefox policy settings 
 Google Chrome policy settings 
 Microsoft Edge: 
   
   Policy settings 
   Reference guide 
    
 
Revert the deployment of the Amazon Q Business browser extension on behalf of your users 
If you customized the Amazon Q Business browser extension by modifying browser policies as detailed earlier in this post, you can revert those customizations by simply removing the corresponding policy entry in your browser policy settings. 
Conclusion 
In this post, we showed how to use the Amazon Q Business browser extension to give your team seamless access to AI-driven insights and assistance. The browser extension is now available in US East (N. Virginia) and US West (Oregon) AWS Regions for Mozilla, Google Chrome, and Microsoft Edge as part of the Lite Subscription. There is no additional cost to use the browser extension. 
To get started, log in to the Amazon Q Business console and setup the browser extension for your Amazon Q Business application. To learn more, see Configuring the Amazon Q Business browser extension for use. 
 
About the authors 
Firaz Akmal is a Sr. Product Manager for Amazon Q Business and has been at AWS for 8+ years. He is a customer advocate, helping customers transform their search and generative AI use-cases on AWS. Outside of work Firaz enjoys spending time in the mountains of the PNW or experiencing the world through his daughter‚Äôs perspective. 
Abhinand Sukumar is a Senior Product Manager at Amazon Web Services for Amazon Q Business, where he drives the product vision and roadmap for innovative generative AI solutions. Abhinand works closely with customers and engineering to deliver successful integrations, including the browser extension. His expertise spans generative AI experiences and AI/ML educational devices, with a deep passion for education, artificial intelligence, and design thinking. Prior to joining AWS, Abhinand worked as an embedded software engineer in the networking industry. With 5-6 years of experience in technology,
‚Ä¢ Build Agentic Workflows with OpenAI GPT OSS on Amazon SageMaker AI and Amazon Bedrock AgentCore
  OpenAI has released two open-weight models, gpt-oss-120b (117 billion parameters) and gpt-oss-20b (21 billion parameters), both built with a Mixture of Experts (MoE) design and a 128K context window. These models are the leading open source models, according to Artificial Analysis benchmarks, and excel at reasoning and agentic workflows. With Amazon SageMaker AI, you can fine-tune or customize models and deploy with your choice of framework through a fully managed service. Amazon SageMaker Inference gives you the flexibility to bring your own inference code and framework without having to build and maintain your own clusters. 
Although large language models (LLMs) excel at understanding language and generating content, building real-world agentic applications requires complex workflow management, tool calling capabilities, and context management. Multi-agent architectures address these challenges by breaking down complex systems into specialized components, but they introduce new complexities in agent coordination, memory management, and workflow orchestration. 
In this post, we show how to deploy gpt-oss-20b model to SageMaker managed endpoints and demonstrate a practical stock analyzer agent assistant example with LangGraph, a powerful graph-based framework that handles state management, coordinated workflows, and persistent memory systems. We will then deploy our agents to Amazon Bedrock AgentCore, a unified orchestration layer that abstracts away infrastructure and allows you to securely deploy and operate AI agents at scale. 
Solution overview 
In this solution, we build an agentic stock analyzer with the following key components: 
 
 The GPT OSS 20B model deployed to a SageMaker endpoint using vLLM, an open source serving framework for LLMs 
 LangGraph to build a multi-agent orchestration framework 
 Amazon Bedrock AgentCore to deploy the agents 
 
The following diagram illustrates the solution architecture. 
 
This architecture illustrates a multi-agent workflow hosted on Amazon Bedrock AgentCore Runtime running on AWS. A user submits a query, which is handled by a pipeline of specialized agents‚ÄîData Gathering Agent, Stock Performance Analyzer Agent, and Stock Report Generation Agent‚Äîthat are each responsible for a distinct part of the stock evaluation process. 
These agents collaborate within Amazon Bedrock AgentCore Runtime, and when language understanding or generation is required, they invoke a GPT OSS model hosted on SageMaker AI. The model processes the input and returns structured outputs that inform agent actions, enabling a fully serverless, modular, and scalable agentic system using open-source models. 
Prerequisites 
 
 Ensure that you have required quota for G6e instances to deploy the model. Request quota here if you do not. 
 If this is your first time working with&nbsp;Amazon SageMaker Studio, you first need to create a&nbsp;SageMaker domain. 
 Ensure your IAM role has required permissions to deploy SageMaker Models and Endpoints. For more information, see How Amazon SageMaker AI works with IAM in the SageMaker Developer Guide. 
 
Deploy GPT-OSS models to SageMaker Inference 
Customers who want to customize their models and frameworks can deploy using serverful deployments, but this requires access to GPUs, serving frameworks, load balancers, and infrastructure setup. SageMaker AI provides a fully managed hosting platform that takes care of provisioning the infrastructure with the necessary drivers, downloads the models, and deploys them. OpenAI‚Äôs GPT-OSS models are launched with a 4-bit quantization scheme (MXFP4), enabling fast inference while keeping resource usage low. These models can run on P5(H100), P6(H200), and P4(A100) and G6e(L40) instances.The GPT-OSS models are sparse MoE architectures with 128 experts (120B) or 32 experts (20B), where each token is routed to 4 experts with no shared expert. Using MXFP4 for MoE weights alone reduces the model sizes to 63 GB (120B) and 14 GB (20B), making them runnable on a single H100 GPU. 
To deploy these models effectively, you need a powerful serving framework like vLLM. To deploy the model, we build a vLLM container with the latest version that supports GPT OSS models on SageMaker AI. 
You can use the following Docker file and script to build the container and push it to a local Amazon Elastic Container Registry (Amazon ECR). The recommended approach is to do this directly from Amazon SageMaker Studio, which provides a managed JupyterLab environment with AWS CLI access where you can build and push images to ECR as part of your SageMaker workflow. Alternatively, you can also perform the same steps on an Amazon Elastic Compute Cloud (Amazon EC2) instance with Docker installed. 
After you have built and pushed the container to Amazon ECR, you can open Amazon SageMaker Studio by going to the SageMaker AI console, as shown in the following screenshot. 
 
You can then create a Jupyter space or use an existing one to launch JupyterLab and run notebooks. 
 
Clone the following notebook and run ‚ÄúOption 3: Deploying from HF using BYOC.‚Äù Update the required parameters, such as the inference image in the notebook with the container image. We also provide necessary environment variables, as shown in the following code. 
 
 inference_image&nbsp;&nbsp;f"{account_id}.dkr.ecr.{region}.amazonaws.com/vllm:v0.10.0-gpt-oss"
instance_type&nbsp;&nbsp;"ml.g6e.4xlarge"
num_gpu&nbsp;&nbsp;1
model_name&nbsp;&nbsp;sagemakerutilsname_from_base("model-byoc")
endpoint_name&nbsp;&nbsp;model_name
inference_component_name&nbsp;&nbsp;f"ic-{model_name}"
config&nbsp;&nbsp;{
"OPTION_MODEL":&nbsp;"openai/gpt-oss-20b",
"OPTION_SERVED_MODEL_NAME":&nbsp;"model",
"OPTION_TENSOR_PARALLEL_SIZE":&nbsp;jsondumps(num_gpu),
"OPTION_ASYNC_SCHEDULING":&nbsp;"true",
} 
 
After you set up the deployment configuration, you can deploy to SageMaker AI using the following code: 
 
 from sagemaker.compute_resource_requirements.resource_requirements import ResourceRequirements

lmi_model = sagemaker.Model(
&nbsp;&nbsp; &nbsp;image_uri=inference_image,
&nbsp;&nbsp; &nbsp;env=config,
&nbsp;&nbsp; &nbsp;role=role,
&nbsp;&nbsp; &nbsp;name=model_name,
)

lmi_model.deploy(
&nbsp;&nbsp; &nbsp;initial_instance_count=1,
&nbsp;&nbsp; &nbsp;instance_type=instance_type,
&nbsp;&nbsp; &nbsp;container_startup_health_check_timeout=600,
&nbsp;&nbsp; &nbsp;endpoint_name=endpoint_name,
&nbsp;&nbsp; &nbsp;endpoint_type=sagemaker.enums.EndpointType.INFERENCE_COMPONENT_BASED,
&nbsp;&nbsp; &nbsp;inference_component_name=inference_component_name,
&nbsp;&nbsp; &nbsp;resources=ResourceRequirements(requests={"num_accelerators": num_gpu, "memory": 1024*5, "copies": 1,}),
) 
 
You can now run an inference example: 
 
 payload={
&nbsp;&nbsp; &nbsp;"messages": [
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;{"role": "user", "content": "Name popular places to visit in London?"}
&nbsp;&nbsp; &nbsp;],
}
res = llm.predict(payload)
print("-----\n" + res["choices"][0]["message"]["content"] + "\n-----\n")
print(res["usage"])

-----
Here are some of the must‚Äësee spots in London&nbsp;‚Äî a mix of iconic landmarks, world‚Äëclass museums, and vibrant neighborhoods:

| # | Place | Why It‚Äôs Popular |
|---|-------|------------------|
| 1 | **Buckingham Palace** | The Queen‚Äôs official London residence ‚Äì watch the Changing of the Guard. |
| 2 | **The Tower of London &amp; Tower Bridge** | Historic castle, Crown Jewels, and the iconic bridge with glass floors. |
| 3 | **The British Museum** | World‚Äëfamous collection from the Rosetta Stone to Egyptian mummies (free entry). |
| 4 | **The Houses of Parliament &amp; Big Ben** | The classic symbol of London‚Äôs politics and architecture. |
| 5 | **The National Gallery (Tate Britain)** | Home to masterpieces from Van Gogh to Turner. |
| 6 | **Buckinghamshire Gardens (Kew Gardens)** | Stunning botanical gardens with a glasshouse and the Horniman Insect Zoo. |
| 7 | **Camden Market** | Eclectic stalls, street food, music and vintage fashion. |
| 8 | **Covent Garden** | Lively piazza with street performers, boutique shops, and the Royal Opera House. |
| 9 | **West End Theatres** | Theatre district famous for grand productions (musicals, dramas). |
|10 | **The Shard** | Skyscraper with panoramic 360¬∞ views of London. |
|11 | **St.‚ÄØPaul‚Äôs Cathedral** | Massive dome, stunning interior and a climb up the Whispering Gallery. |
|12 | **The Tate Modern** | Contemporary art museum set in a former power station. |
|13 | **The Victoria &amp; Albert Museum** | Design and fashion, costume, and jewelry collections. |
|14 | **Hyde Park &amp; Kensington Gardens** | Huge green spaces with Serpentine Lake, Speaker‚Äôs Corner and Speakers' Corner. |
|15 | **Oxford Street &amp; Regent Street** | Prime shopping streets for fashion, flagship stores, and historic architecture. |

These spots cover history, culture, shopping, and leisure‚Äîperfect for a first visit or a weekend escape in London!
----- 
 
Use LangGraph to build a stock analyzer agent 
For our stock analyzing multi-agent system, we use LangGraph to orchestrate the workflow. Jupyter notebook for the code is located in this github repository. The system comprises three specialized tools that work together to analyze stocks comprehensively: 
 
 The gather_stock_data tool collects comprehensive stock data for a given ticker symbol, including current price, historical performance, financial metrics, and market data. It returns formatted information covering price history, company fundamentals, trading metrics, and recent news headlines. 
 The analyze_stock_performance tool performs detailed technical and fundamental analysis of stock data, calculating metrics like price trends, volatility, and overall investment scores. It evaluates multiple factors including P/E ratios, profit margins, and dividend yields to provide a comprehensive performance analysis 
 The generate_stock_reporttool creates professional PDF reports from the gathered stock data and analysis, automatically uploading them to Amazon S3 with organized date-based folders. 
 
For local testing, you can use a simplified version of the system by importing the necessary functions from your local script. For example: 
 
 from langgraph_stock_local import langgraph_stock_sagemaker
# Test the agent locally
result = langgraph_stock_sagemaker({
&nbsp;&nbsp; &nbsp;"prompt": "Analyze SIM_STOCK Stock for Investment purposes."
})
print(result) 
 
This way, you can iterate quickly on your agent‚Äôs logic before deploying it to a scalable platform, making sure each component functions correctly and the overall workflow produces the expected results for different types of stocks. 
Deploy to Amazon Bedrock AgentCore 
After you have developed and tested your LangGraph framework locally, you can deploy it to Amazon Bedrock AgentCore Runtime. Amazon Bedrock AgentCore handles the heavy lifting of container orchestration, session management, scalability and abstracting the management of infrastructure. It provides persistent execution environments that can maintain an agent‚Äôs state across multiple invocations. 
Before deploying our stock analyzer agent to Amazon Bedrock AgentCore Runtime, we need to create an AWS Identity and Access Management IAM role with the appropriate permissions. This role allows Amazon Bedrock AgentCore to invoke your SageMaker endpoint for GPT-OSS model inference, manage ECR repositories for storing container images, write Amazon CloudWatch logs for monitoring and debugging, access Amazon Bedrock AgentCore workload services for runtime operations, and send telemetry data to AWS X-Ray and CloudWatch for observability. See the following code: 
 
 from create_agentcore_role import create_bedrock_agentcore_role
role_arn = create_bedrock_agentcore_role(
&nbsp;&nbsp; &nbsp;role_name="MyStockAnalyzerRole",
&nbsp;&nbsp; &nbsp;sagemaker_endpoint_name="your-endpoint-name",
&nbsp;&nbsp; &nbsp;region="us-west-2"
) 
 
After creating the role, you can use the Amazon Bedrock AgentCore Starter Toolkit to deploy your agent. The toolkit simplifies the deployment process by packaging your code, creating the necessary container image, and configuring the runtime environment: 
 
 from bedrock_agentcore_starter_toolkit import Runtime
agentcore_runtime = Runtime()
# Configure the agent
response = agentcore_runtime.configure(
&nbsp;&nbsp; &nbsp;entrypoint="langgraph_stock_sagemaker_gpt_oss.py",
&nbsp;&nbsp; &nbsp;execution_role=role_arn,
&nbsp;&nbsp; &nbsp;auto_create_ecr=True,
&nbsp;&nbsp; &nbsp;requirements_file="requirements.txt",
&nbsp;&nbsp; &nbsp;region="us-west-2",
&nbsp;&nbsp; &nbsp;agent_name="stock_analyzer_agent"
)
# Deploy to the cloud
launch_result = agentcore_runtime.launch(local=False, local_build=False) 
 
When you‚Äôre using BedrockAgentCoreApp, it automatically creates an HTTP server that listens on port 8080, implements the required /invocations endpoint for processing the agent‚Äôs requirements, implements the/ping endpoint for health checks (which is very important for asynchronous agents), handles proper content types and response formats, and manages error handling according to AWS standards. 
After you deploy to Amazon Bedrock AgentCore Runtime, you will be able to see the status show as Ready on the Amazon Bedrock AgentCore console. 
 
Invoke the agent 
After you create the agent, you must set up the agent invocation entry point. With Amazon AgentCore Runtime, we decorate the invocation part of our agent with the @app.entrypoint decorator and use it as the entry point for our runtime. After you deploy the agent to Amazon AgentCore Runtime, you can invoke it using the AWS SDK: 
 
 import boto3
import json
agentcore_client = boto3.client('bedrock-agentcore', region_name='us-west-2')
response = agentcore_client.invoke_agent_runtime(
&nbsp;&nbsp; &nbsp;agentRuntimeArn=launch_result.agent_arn,
&nbsp;&nbsp; &nbsp;qualifier="DEFAULT",
&nbsp;&nbsp; &nbsp;payload=json.dumps({
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"prompt": "Analyze SIM_STOCK for investment purposes"
&nbsp;&nbsp; &nbsp;})
) 
 
After invoking the stock analyzer agent through Amazon Bedrock AgentCore Runtime, you must parse and format the response for clear presentation. The response processing involves the following steps: 
 
 Decode the byte stream from Amazon Bedrock AgentCore into readable text. 
 Parse the JSON response containing the complete stock analysis. 
 Extract three main sections using regex pattern matching: 
   
   Stock Data Gathering Section: Extracts core stock information including symbol, company details, current pricing, market metrics, financial ratios, trading data, and recent news headlines. 
   Performance Analysis section: Analyzes technical indicators, fundamental metrics, and volatility measures to generate comprehensive stock analysis. 
   Stock Report Generation Section: Generates a detailed PDF report with all the Stock Technical Analysis. 
    
 
The system also includes error handling that gracefully handles JSON parsing errors, falls back to plain text display if structured parsing fails, and provides debugging information for troubleshooting parsing issues of the stock analysis response. 
 
 stock_analysis = parse_bedrock_agentcore_stock_response(invoke_response) 
 
This formatted output makes it straightforward to review the agent‚Äôs decision-making process and present professional stock analysis results to stakeholders, completing the end-to-end workflow from model deployment to meaningful business output: 
 
 STOCK DATA GATHERING REPORT:
================================
Stock Symbol: SIM_STOCK
Company Name: Simulated Stock Inc.
Sector: SIM_SECTOR
Industry: SIM INDUSTRY
CURRENT MARKET DATA:
- Current Price: $29.31
- Market Cap: $3,958
- 52-Week High: $29.18
- 52-Week Low: $16.80
- YTD Return: 1.30%
- Volatility (Annualized): 32.22%
FINANCIAL METRICS:
- P/E Ratio: 44.80
- Forward P/E: 47.59
- Price-to-Book: 11.75
- Dividend Yield: 0.46%
- Revenue (TTM): $4,988
- Profit Margin: 24.30% 
 
 
 STOCK PERFORMANCE ANALYSIS:
===============================
Stock: SIM_STOCK | Current Price: $29.31
TECHNICAL ANALYSIS:
- Price Trend: SLIGHT UPTREND
- YTD Performance: 1.03%
- Technical Score: 3/5
FUNDAMENTAL ANALYSIS:
- P/E Ratio: 34.80
- Profit Margin: 24.30%
- Dividend Yield: 0.46%
- Beta: 1.165
- Fundamental Score: 3/5
STOCK REPORT GENERATION:
===============================
Stock: SIM_STOCK 
Sector: SIM_INDUSTRY
Current Price: $29.78
REPORT SUMMARY:
- Technical Analysis: 8.33% YTD performance
- Report Type: Comprehensive stock analysis for informational purposes
- Generated: 2025-09-04 23:11:55
PDF report uploaded to S3: s3://amzn-s3-demo-bucket/2025/09/04/SIM_STOCK_Stock_Report_20250904_231155.pdf
REPORT CONTENTS:
‚Ä¢ Executive Summary with key metrics
‚Ä¢ Detailed market data and financial metrics
‚Ä¢ Technical and fundamental analysis
‚Ä¢ Professional formatting for documentation 
 
Clean up 
You can delete the SageMaker endpoint to avoid accruing costs after your testing by running the following cells in the same notebook: 
 
 sessdelete_inference_component(inference_component_name)
sessdelete_endpoint(endpoint_name)
sessdelete_endpoint_config(endpoint_name)
sessdelete_model(model_name) 
 
You can also delete Amazon Bedrock AgentCore resources using the following commands: 
 
 runtime_delete_response&nbsp;&nbsp;agentcore_control_clientdelete_agent_runtime(
agentRuntimeIdlaunch_resultagent_id
)
response&nbsp;&nbsp;ecr_clientdelete_repository(
repositoryNamelaunch_resultecr_urisplit('/')[1],
force
) 
 
Conclusion 
In this post, we built an end-to-end solution for deploying OpenAI‚Äôs open-weight models on a single G6e(L40s) GPU, creating a multi-agent stock analysis system with LangGraph and deploying it seamlessly with Amazon Bedrock AgentCore. This implementation demonstrates how organizations can now use powerful open source LLMs cost-effectively with efficient serving frameworks such as vLLM. Beyond the technical implementation, enhancing this workflow can provide significant business value, such as reduction in stock analysis processing time, increased analyst productivity by automating routine stock assessments. Furthermore, by freeing analysts from repetitive tasks, organizations can redirect skilled professionals toward complex cases and relationship-building activities that drive business growth. 
We invite you to try out our code samples and iterate your agentic workflows to meet your use cases. 
 
About the authors 
Vivek Gangasani is a Worldwide Lead GenAI Specialist Solutions Architect for SageMaker Inference. He drives Go-to-Market (GTM) and Outbound Product strategy for SageMaker Inference. He also helps enterprises and startups deploy, manage, and scale their GenAI models with SageMaker and GPUs. Currently, he is focused on developing strategies and solutions for optimizing inference performance and GPU efficiency for hosting Large Language Models. In his free time, Vivek enjoys hiking, watching movies, and trying different cuisines. 
Surya Kari&nbsp;is a Senior Generative AI Data Scientist at AWS, specializing in developing solutions leveraging state-of-the-art foundation models. He has extensive experience working with advanced language models including DeepSeek-R1, the Llama family, and Qwen, focusing on their fine-tuning and optimization for specific scientific applications. His expertise extends to implementing efficient training pipelines and deployment strategies using AWS SageMaker, enabling the scaling of foundation models from development to production. He collaborates with customers to design and implement generative AI solutions, helping them navigate model selection, fine-tuning approaches, and deployment strategies to achieve optimal performance for their specific use cases.
‚Ä¢ Streamline access to ISO-rating content changes with Verisk rating insights and Amazon Bedrock
  This post is co-written with Samit Verma, Eusha Rizvi, Manmeet Singh, Troy Smith, and Corey Finley from Verisk. 
Verisk Rating Insights as a feature of ISO Electronic Rating Content (ERC) is a powerful tool designed to provide summaries of ISO Rating changes between two releases. Traditionally, extracting specific filing information or identifying differences across multiple releases required manual downloads of full packages, which was time-consuming and prone to inefficiencies. This challenge, coupled with the need for accurate and timely customer support, prompted Verisk to explore innovative ways to enhance user accessibility and automate repetitive processes. Using generative AI and Amazon Web Services (AWS) services, Verisk has made significant strides in creating a conversational user interface for users to easily retrieve specific information, identify content differences, and improve overall operational efficiency. 
In this post, we dive into how Verisk Rating Insights, powered by Amazon Bedrock, large language models (LLM), and Retrieval Augmented Generation (RAG), is transforming the way customers interact with and access ISO ERC changes. 
The challenge 
Rating Insights provides valuable content, but there were significant challenges with user accessibility and the time it took to extract actionable insights: 
 
 Manual downloading ‚Äì Customers had to download entire packages to get even a small piece of relevant information. This was inefficient, especially when only a part of the filing needed to be reviewed. 
 Inefficient data retrieval ‚Äì Users couldn‚Äôt quickly identify the differences between two content packages without downloading and manually comparing them, which could take hours and sometimes days of analysis. 
 Time-consuming customer support ‚Äì Verisk‚Äôs ERC Customer Support team spent 15% of their time weekly addressing queries from customers who were impacted by these inefficiencies. Furthermore, onboarding new customers required half a day of repetitive training to ensure they understood how to access and interpret the data. 
 Manual analysis time ‚Äì Customers often spent 3‚Äì4 hours per test case analyzing the differences between filings. With multiple test cases to address, this led to significant delays in critical decision-making. 
 
Solution overview 
To solve these challenges, Verisk embarked on a journey to enhance Rating Insights with generative AI technologies. By integrating Anthropic‚Äôs Claude, available in Amazon Bedrock, and Amazon OpenSearch Service, Verisk created a sophisticated conversational platform where users can effortlessly access and analyze rating content changes. 
The following diagram illustrates the high-level architecture of the solution, with distinct sections showing the data ingestion process and inference loop. The architecture uses multiple AWS services to add generative AI capabilities to the Ratings Insight system. This system‚Äôs components work together seamlessly, coordinating multiple LLM calls to generate user responses. 
 
The following diagram shows the architectural components and the high-level steps involved in the Data Ingestion process. 
 
  
   
    
   
  
 
The steps in the data ingestion process proceed as follows: 
 
 This process is triggered when a new file is dropped. It is responsible for chunking the document using a custom chunking strategy. This strategy recursively checks each section and keeps them intact without overlap. The process then embeds the chunks and stores them in OpenSearch Service as vector embeddings. 
 The embedding model used in Amazon Bedrock is amazon titan-embed-g1-text-02. 
 Amazon OpenSearch Serverless is utilized as a vector embedding store with metadata filtering capability. 
 
The following diagram shows the architectural components and the high-level steps involved in the inference loop to generate user responses. 
 
The steps in the inference loop proceed as follows: 
 
 This component is responsible for multiple tasks: it supplements user questions with recent chat history, embeds the questions, retrieves relevant chunks from the vector database, and finally calls the generation model to synthesize a response. 
 Amazon ElastiCache is used for storing recent chat history. 
 The embedding model utilized in Amazon Bedrock is amazon titan-embed-g1-text-02. 
 OpenSearch Serverless is implemented for RAG (Retrieval-Augmented Generation). 
 For generating responses to user queries, the system uses Anthropic‚Äôs Claude Sonnet 3.5 (model ID: anthropic.claude-3-5-sonnet-20240620-v1:0), which is available through Amazon Bedrock. 
 
Key technologies and frameworks used 
We used Anthropic‚Äôs Claude Sonnet 3.5 (model ID: anthropic.claude-3-5-sonnet-20240620-v1:0) to understand user input and provide detailed, contextually relevant responses. Anthropic‚Äôs Claude Sonnet 3.5 enhances the platform‚Äôs ability to interpret user queries and deliver accurate insights from complex content changes. LlamaIndex, which is an open source framework, served as the chain framework for efficiently connecting and managing different data sources to enable dynamic retrieval of content and insights. 
We implemented RAG, which allows the model to pull specific, relevant data from the OpenSearch Serverless vector database. This means the system generates precise, up-to-date responses based on a user‚Äôs query without needing to sift through massive content downloads. The vector database enables intelligent search and retrieval, organizing content changes in a way that makes them quickly and easily accessible. This eliminates the need for manual searching or downloading of entire content packages. Verisk applied guardrails in Amazon Bedrock Guardrails along with custom guardrails around the generative model so the output adheres to specific compliance and quality standards, safeguarding the integrity of responses. 
Verisk‚Äôs generative AI solution is a comprehensive, secure, and flexible service for building generative AI applications and agents. Amazon Bedrock connects you to leading FMs, services to deploy and operate agents, and tools for fine-tuning, safeguarding, and optimizing models along with knowledge bases to connect applications to your latest data so that you have everything you need to quickly move from experimentation to real-world deployment. 
Given the novelty of generative AI, Verisk has established a governance council to oversee its solutions, ensuring they meet security, compliance, and data usage standards. Verisk implemented strict controls within the RAG pipeline to ensure data is only accessible to authorized users. This helps maintain the integrity and privacy of sensitive information. Legal reviews ensure IP protection and contract compliance. 
How it works 
The integration of these advanced technologies enables a seamless, user-friendly experience. Here‚Äôs how Verisk Rating Insights now works for customers: 
 
 Conversational user interface ‚Äì Users can interact with the platform by using a conversational interface. Instead of manually reviewing content packages, users enter a natural language query (for example, ‚ÄúWhat are the changes in coverage scope between the two recent filings?‚Äù). The system uses Anthropic‚Äôs Claude Sonnet 3.5 to understand the intent and provides an instant summary of the relevant changes. 
 Dynamic content retrieval ‚Äì Thanks to RAG and OpenSearch Service, the platform doesn‚Äôt require downloading entire files. Instead, it dynamically retrieves and presents the specific changes a user is seeking, enabling quicker analysis and decision-making. 
 Automated difference analysis ‚Äì The system can automatically compare two content packages, highlighting the differences without requiring manual intervention. Users can query for precise comparisons (for example, ‚ÄúShow me the differences in rating criteria between Release 1 and Release 2‚Äù). 
 Customized insights ‚Äì The guardrails in place mean that responses are accurate, compliant, and actionable. Additionally, if needed, the system can help users understand the impact of changes and assist them in navigating the complexities of filings, providing clear, concise insights. 
 
The following diagram shows the architectural components and the high-level steps involved in the evaluation loop to generate relevant and grounded responses. 
 
The steps in the evaluation loop proceed as follows: 
 
 This component is responsible for calling Anthropic‚Äôs Claude Sonnet 3.5 model and subsequently invoking the custom-built evaluation APIs to ensure response accuracy. 
 The generation model employed is Anthropic‚Äôs Claude Sonnet 3.5, which handles the creation of responses. 
 The Evaluation API ensures that responses remain relevant to user queries and stay grounded within the provided context. 
 
The following diagram shows the process of capturing the chat history as contextual memory and storage for analysis. 
 
Quality benchmarks 
The Verisk Rating Insights team has implemented a comprehensive evaluation framework and feedback loop mechanism respectively, shown in the above figures, to support continuous improvement and address the issues that might arise. 
Ensuring high accuracy and consistency in responses is essential for Verisk‚Äôs generative AI solutions. However, LLMs can sometimes produce hallucinations or provide irrelevant details, affecting reliability. To address this, Verisk implemented: 
 
 Evaluation framework ‚Äì Integrated into the query pipeline, it validates responses for precision and relevance before delivery. 
 Extensive testing ‚Äì Product subject matter experts (SMEs) and quality experts rigorously tested the solution to ensure accuracy and reliability. Verisk collaborated with in-house insurance domain experts to develop SME evaluation metrics for accuracy and consistency. Multiple rounds of SME evaluations were conducted, where experts graded these metrics on a 1‚Äì10 scale. Latency was also tracked to assess speed. Feedback from each round was incorporated into subsequent tests to drive improvements. 
 Continual model improvement ‚Äì Using customer feedback serves as a crucial component in driving the continuous evolution and refinement of the generative models, improving both accuracy and relevance. By seamlessly integrating user interactions and feedback with chat history, a robust data pipeline is created that streams the user interactions to an Amazon Simple Storage Service (Amazon S3) bucket, which acts as a data hub. The interactions then go into Snowflake, which is a cloud-based data platform and data warehouse as a service that offers capabilities such as data warehousing, data lakes, data sharing, and data exchange. Through this integration, we built comprehensive analytics dashboards that provide valuable insights into user experience patterns and pain points. 
 
Although the initial results were promising, they didn‚Äôt meet the desired accuracy and consistency levels. The development process involved several iterative improvements, such as redesigning the system and making multiple calls to the LLM. The primary metric for success was a manual grading system where business experts compared the results and provided continuous feedback to improve overall benchmarks. 
Business impact and opportunity 
By integrating generative AI into Verisk Rating Insights, the business has seen a remarkable transformation. Customers enjoyed significant time savings. By eliminating the need to download entire packages and manually search for differences, the time spent on analysis has been drastically reduced. Customers no longer spend 3‚Äì4 hours per test case. What at one time took days now takes minutes. 
This time savings brought increased productivity. With an automated solution that instantly provides relevant insights, customers can focus more on decision-making rather than spending time on manual data retrieval. And by automating difference analysis and providing a centralized, effortless platform, customers can be more confident in the accuracy of their results and avoid missing critical changes. 
For Verisk, the benefit was a reduced customer support burden because the ERC customer support team now spends less time addressing queries. With the AI-powered conversational interface, users can self-serve and get answers in real time, freeing up support resources for more complex inquiries. 
The automation of repetitive training tasks meant quicker and more efficient customer onboarding. This reduces the need for lengthy training sessions, and new customers become proficient faster. The integration of generative AI has reduced redundant workflows and the need for manual intervention. This streamlines operations across multiple departments, leading to a more agile and responsive business. 
Conclusion 
Looking ahead, Verisk plans to continue enhancing the Rating Insights platform twofold. First, we‚Äôll expand the scope of queries, enabling more sophisticated queries related to different filing types and more nuanced coverage areas. Second, we‚Äôll scale the platform. With Amazon Bedrock providing the infrastructure, Verisk aims to scale this solution further to support more users and additional content sets across various product lines. 
Verisk Rating Insights, now powered by generative AI and AWS technologies, has transformed the way customers interact with and access rating content changes. Through a conversational user interface, RAG, and vector databases, Verisk intends to eliminate inefficiencies and save customers valuable time and resources while enhancing overall accessibility. For Verisk, this solution has improved operational efficiency and provided a strong foundation for continued innovation. 
With Amazon Bedrock and a focus on automation, Verisk is driving the future of intelligent customer support and content management, empowering both their customers and their internal teams to make smarter, faster decisions. 
For more information, refer to the following resources: 
 
 Explore generative AI on AWS 
 Learn about unlocking the business value of generative AI 
 Learn more about Anthropic‚Äôs Claude 3 models on Amazon Bedrock 
 Learn about Amazon Bedrock and how to build and scale generative AI applications with FMs 
 Explore generative AI quick start proofs of concept 
 
 
 
About the authors 
Samit Verma serves as the Director of Software Engineering at Verisk, overseeing the Rating and Coverage development teams. In this role, he plays a key part in architectural design and provides strategic direction to multiple development teams, enhancing efficiency and ensuring long-term solution maintainability. He holds a master‚Äôs degree in information technology. 
Eusha Rizvi serves as a Software Development Manager at Verisk, leading several technology teams within the Ratings Products division. Possessing strong expertise in system design, architecture, and engineering, Eusha offers essential guidance that advances the development of innovative solutions. He holds a bachelor‚Äôs degree in information systems from Stony Brook University. 
Manmeet Singh is a Software Engineering Lead at Verisk and AWS Certified Generative AI Specialist. He leads the development of an agentic RAG-based generative AI system on Amazon Bedrock, with expertise in LLM orchestration, prompt engineering, vector databases, microservices, and high-availability architecture. Manmeet is passionate about applying advanced AI and cloud technologies to deliver resilient, scalable, and business-critical systems. 
Troy Smith is a Vice President of Rating Solutions at Verisk. Troy is a seasoned insurance technology leader with more than 25 years of experience in rating, pricing, and product strategy. At Verisk, he leads the team behind ISO Electronic Rating Content, a widely used resource across the insurance industry. Troy has held leadership roles at Earnix and Capgemini and was the cofounder and original creator of the Oracle Insbridge Rating Engine. 
Corey Finley is a Product Manager at Verisk. Corey has over 22 years of experience across personal and commercial lines of insurance. He has worked in both implementation and product support roles and has led efforts for major carriers including Allianz, CNA, Citizens, and others. At Verisk, he serves as Product Manager for VRI, RaaS, and ERC. 
Arun Pradeep Selvaraj is a Senior Solutions Architect at Amazon Web Services (AWS). Arun is passionate about working with his customers and stakeholders on digital transformations and innovation in the cloud while continuing to learn, build, and reinvent. He is creative, energetic, deeply customer-obsessed, and uses the working backward process to build modern architectures to help customers solve their unique challenges. Connect with him on LinkedIn. 
Ryan Doty is a Solutions Architect Manager at Amazon Web Services (AWS), based out of New York. He helps financial services customers accelerate their adoption of the AWS Cloud by providing architectural guidelines to design innovative and scalable solutions. Coming from a software development and sales engineering background, the possibilities that the cloud can bring to the world excite him.
‚Ä¢ Unified multimodal access layer for Quora‚Äôs Poe using Amazon Bedrock
  Organizations gain competitive advantage by deploying and integrating new generative AI models quickly through Generative AI Gateway architectures. This unified interface approach simplifies access to multiple foundation models (FMs), addressing a critical challenge: the proliferation of specialized AI models, each with unique capabilities, API specifications, and operational requirements. Rather than building and maintaining separate integration points for each model, the smart move is to build an abstraction layer that normalizes these differences behind a single, consistent API. 
The AWS Generative AI Innovation Center and Quora recently collaborated on an innovative solution to address this challenge. Together, they developed a unified wrapper API framework that streamlines the deployment of Amazon Bedrock FMs on Quora‚Äôs Poe system. This architecture delivers a ‚Äúbuild once, deploy multiple models‚Äù capability that significantly reduces deployment time and engineering effort, with real protocol bridging code visible throughout the codebase. 
For technology leaders and developers working on AI multi-model deployment at scale, this framework demonstrates how thoughtful abstraction and protocol translation can accelerate innovation cycles while maintaining operational control. 
In this post, we explore how the AWS Generative AI Innovation Center and Quora collaborated to build a unified wrapper API framework that dramatically accelerates the deployment of Amazon Bedrock FMs on Quora‚Äôs Poe system. We detail the technical architecture that bridges Poe‚Äôs event-driven ServerSentEvents protocol with Amazon Bedrock REST-based APIs, demonstrate how a template-based configuration system reduced deployment time from days to 15 minutes, and share implementation patterns for protocol translation, error handling, and multi-modal capabilities. We show how this ‚Äúbuild once, deploy multiple models‚Äù approach helped Poe integrate over 30 Amazon Bedrock models across text, image, and video modalities while reducing code changes by up to 95%. 
Quora and Amazon Bedrock 
Poe.com is an AI system developed by Quora that users and developers can use to interact with a wide range of advanced AI models and assistants powered by multiple providers. The system offers multi-model access, enabling side-by-side conversations with various AI chatbots for tasks such as natural language understanding, content generation, image creation, and more. 
This screenshot below showcases the user interface of Poe, the AI platform created by Quora. The image displays Poe‚Äôs extensive library of AI models, which are presented as individual ‚Äúchatbots‚Äù that users can interact with. 
 
The following screenshot provides a view of the Model Catalog within Amazon Bedrock, a fully managed service from Amazon Web Services (AWS) that offers access to a diverse range of foundation models (FMs). This catalog acts as a central hub for developers to discover, evaluate, and access state-of-the-art AI from various providers. 
 
Initially, integrating the diverse FMs available through Amazon Bedrock presented significant technical challenges for the Poe.com team. The process required substantial engineering resources to establish connections with each model while maintaining consistent performance and reliability standards. Maintainability emerged as an extremely important consideration, as was the ability to efficiently onboard new models as they became available‚Äîboth factors adding further complexity to the integration challenges. 
Technical challenge: Bridging different systems 
The integration between Poe and Amazon Bedrock presented fundamental architectural challenges that required innovative solutions. These systems were built with different design philosophies and communication patterns, creating a significant technical divide that the wrapper API needed to bridge. 
Architectural divide 
The core challenge stems from the fundamentally different architectural approaches of the two systems. Understanding these differences is essential to appreciating the complexity of the integration solution. Poe operates on a modern, reactive, ServerSentEvents-based architecture through the Fast API library (fastapi_poe). This architecture is stream-optimized for real-time interactions and uses an event-driven response model designed for continuous, conversational AI. Amazon Bedrock, on the other hand, functions as an enterprise cloud service. It offers REST-based with AWS SDK access patterns, SigV4 authentication requirements, AWS Region-specific model availability, and a traditional request-response pattern with streaming options. This fundamental API mismatch creates several technical challenges that the Poe wrapper API solves, as detailed in the following table. 
 
  
   
   Challenge Category 
   Technical Issue 
   Source Protocol 
   Target Protocol 
   Integration Complexity 
   
  
  
   
   Protocol Translation 
   Converting between WebSocket-based protocol and REST APIs 
   WebSocket (bidirectional, persistent) 
   REST (request/response, stateless) 
   High: Requires protocol bridging 
   
   
   Authentication Bridging 
   Connecting JWT validation with AWS SigV4 signing 
   JWT token validation 
   AWS SigV4 authentication 
   Medium: Credential transformation needed 
   
   
   Response Format Transformation 
   Adapting JSON responses into expected format 
   Standard JSON structure 
   Custom format requirements 
   Medium: Data structure mapping 
   
   
   Streaming Reconciliation 
   Mapping chunked responses to ServerSentEvents 
   Chunked HTTP responses 
   ServerSentEvents stream 
   High: Real-time data flow conversion 
   
   
   Parameter Standardization 
   Creating unified parameter space across models 
   Model-specific parameters 
   Standardized parameter interface 
   Medium: Parameter normalization 
   
  
 
API evolution and the Converse API 
In May 2024, Amazon Bedrock introduced the Converse API, which offered standardization benefits that significantly simplified the integration architecture: 
 
 Unified interface across diverse model providers (such as Anthropic, Meta, and Mistral) 
 Conversation memory with consistent handling of chat history 
 Streaming and non-streaming modes through a single API pattern 
 Multimodal support for text, images, and structured data 
 Parameter normalization that reduces model-specific implementation quirks 
 Built-in content moderation capabilities 
 
The solution presented in this post uses the Converse API where appropriate, while also maintaining compatibility with model-specific APIs for specialized capabilities. This hybrid approach provides flexibility while taking advantage of the Converse API‚Äôs standardization benefits. 
Solution overview 
The wrapper API framework provides a unified interface between Poe and Amazon Bedrock models. It serves as a translation layer that normalizes the differences between models and protocols while maintaining the unique capabilities of each model. 
The solution architecture follows a modular design that separates concerns and enables flexible scaling, as illustrated in the following diagram. 
 
The wrapper API consists of several key components working together to provide a seamless integration experience: 
 
 Client ‚Äì The entry point where users interact with AI capabilities through various interfaces. 
 Poe layer ‚Äì Consists of the following: 
   
   Poe UI ‚Äì Handles user experience, request formation, parameters controls, file uploads, and response visualization. 
   Poe FastAPI ‚Äì Standardizes user interactions and manages the communication protocol between clients and underlying systems. 
    
 Bot Factory ‚Äì Dynamically creates appropriate model handlers (bots) based on the requested model type (chat, image, or video). This factory pattern provides extensibility for new model types and variations. See the following code: 
 
 
 # From core/bot_factory.py - Actual implementation
class BotFactory:
    """
    Factory for creating different types of bots.
    Handles bot creation based on the bot type and configuration.
    """
    @staticmethod
    def create_bot(bot_config: BotConfig) -&gt; PoeBot:
        # Check if a custom bot class is specified
        if hasattr(bot_config, 'bot_class') and bot_config.bot_class:
            # Use the custom bot class directly
            bot = bot_config.bot_class(bot_config)
            
            # Explicitly ensure we're returning a PoeBot
            if not isinstance(bot, PoeBot):
                raise TypeError(f"Custom bot class must return a PoeBot instance, got {type(bot)}")
            return bot

        # Determine bot type based on configuration
        if hasattr(bot_config, 'enable_video_generation') and bot_config.enable_video_generation:
            # Video generation bot
            if 'luma' in bot_config.bot_name:
                from core.refactored_luma_bot import LumaVideoBot
                return LumaVideoBot(bot_config)
            else:
                from core.refactored_nova_reel_bot import NovaReelVideoBot
                return NovaReelVideoBot(bot_config)
                
        elif hasattr(bot_config, 'enable_image_generation') and bot_config.enable_image_generation:
            # Image generation bot
            if hasattr(bot_config, 'model_id') and "stability" in bot_config.model_id.lower():
                # Stability AI image generation bot
                from core.refactored_image_stability_ai import AmazonBedrockImageStabilityAIBot
                return AmazonBedrockImageStabilityAIBot(bot_config)
            else:
                # Other image generation bot (Titan, Canvas, etc.)
                from core.refactored_image_bot_amazon import RefactoredAmazonImageGenerationBot
                return RefactoredAmazonImageGenerationBot(bot_config)
                
        else:
            # Check if this is a Claude 3.7 model
            if hasattr(bot_config, 'model_id') and "claude-3-7" in bot_config.model_id.lower():
                return ClaudePlusBot(bot_config)
            else:
                # Default to standard chat bot
                return RefactoredAmazonBedrockPoeBot(bot_config) 
 
 
 Service manager: Orchestrates the services needed to process requests effectively. It coordinates between different specialized services, including: 
   
   Token services ‚Äì Managing token limits and counting. 
   Streaming services ‚Äì Handling real-time responses. 
   Error services ‚Äì Normalizing and handling errors. 
   AWS service integration ‚Äì Managing API calls to Amazon Bedrock. 
    
 AWS services component ‚Äì Converts responses from Amazon Bedrock format to Poe‚Äôs expected format and vice-versa, handling streaming chunks, image data, and video outputs. 
 Amazon Bedrock layer ‚Äì Amazon‚Äôs FM service that provides the actual AI processing capabilities and model hosting, including: 
   
   Model diversity ‚Äì Provides access to over 30 text models (such as Amazon Titan, Amazon Nova, Anthropic‚Äôs Claude, Meta‚Äôs Llama, Mistral, and more), image models, and video models. 
   API structure ‚Äì Exposes both model-specific APIs and the unified Converse API. 
   Authentication ‚Äì Requires AWS SigV4 signing for secure access to model endpoints. 
   Response management ‚Äì Returns model outputs with standardized metadata and usage statistics. 
    
 
The request processing flow in this unified wrapper API shows the orchestration required when bridging Poe‚Äôs event-driven ServerSentEvents protocol with Amazon Bedrock REST-based APIs, showcasing how multiple specialized services work together to deliver a seamless user experience. 
The flow begins when a client sends a request through Poe‚Äôs interface, which then forwards it to the Bot Factory component. This factory pattern dynamically creates the appropriate model handler based on the requested model type, whether for chat, image, or video generation. The service manager component then orchestrates the various specialized services needed to process the request effectively, including token services, streaming services, and error handling services. 
The following sequence diagram illustrates the complete request processing flow. 
 
Configuration template for rapid multi-bot deployment 
The most powerful aspect of the wrapper API is its unified configuration template system, which supports rapid deployment and management of multiple bots with minimal code changes. This approach is central to the solution‚Äôs success in reducing deployment time. 
The system uses a template-based configuration approach with shared defaults and model-specific overrides: 
 
 # Bot configurations using the template pattern

CHAT_BOTS = {
    'poe-nova-micro': BotConfig(
        # Identity
        bot_name='poe-nova-micro',
        model_id='amazon.nova-micro-v1:0',
        aws_region=aws_config['region'],
        poe_access_key='XXXXXXXXXXXXXXXXXXXXXX',

        # Model-specific parameters
        supports_system_messages=True,
        enable_image_comprehension=True,
        expand_text_attachments=True,
        streaming=True,
        max_tokens=1300,
        temperature=0.7,
        top_p=0.9,

        # Model-specific pricing
        enable_monetization=True,
        pricing_type="variable",
        input_token_cost_milli_cents=2,
        output_token_cost_milli_cents=4,
        image_analysis_cost_milli_cents=25,

        # Generate rate card with model-specific values
        custom_rate_card=create_rate_card(2, 4, 25),

        # Include common parameters
        **DEFAULT_CHAT_CONFIG
    ),

    'poe-mistral-pixtral': BotConfig(
        # Identity
        bot_name='poe-mistral-pixtral',
        model_id='us.mistral.pixtral-large-2502-v1:0',
        aws_region=aws_config['region'],
        poe_access_key='XXXXXXXXXXXXXXXXXXXXXX',

        # Model-specific parameters
        supports_system_messages=False,
        enable_image_comprehension=False,
        # ...
        # Include common parameters
        **DEFAULT_CHAT_CONFIG
    )
}
 
 
This configuration-driven architecture offers several significant advantages: 
 
 Rapid deployment ‚Äì Adding new models requires only creating a new configuration entry rather than writing integration code. This is a key factor in the significant improvement in deployment time. 
 Consistent parameter management ‚Äì Common parameters are defined one time in DEFAULT_CHAT_CONFIG and inherited by bots, maintaining consistency and reducing duplication. 
 Model-specific customization ‚Äì Each model can have its own unique settings while still benefiting from the shared infrastructure. 
 Operational flexibility ‚Äì Parameters can be adjusted without code changes, allowing for quick experimentation and optimization. 
 Centralized credential management ‚Äì AWS credentials are managed in one place, improving security and simplifying updates. 
 Region-specific deployment ‚Äì Models can be deployed to different Regions as needed, with Region settings controlled at the configuration level. 
 
The BotConfig class provides a structured way to define bot configurations with type validation: 
 
 # From config/bot_config.py - Actual implementation (partial)
class BotConfig(BaseModel):
    # Core Bot Identity
    bot_name: str = Field(..., description="Name of the bot")
    model_id: str = Field(..., description="Identifier for the AI model")

    # AWS Configuration
    aws_region: Optional[str] = Field(default="us-east-1", description="AWS region for deployment")
    aws_access_key: Optional[str] = Field(default=None, description="AWS access key")
    aws_secret_key: Optional[str] = Field(default=None, description="AWS secret key")
    aws_security_token: Optional[str] = None

    # Poe Configuration
    poe_access_key: str = Field(..., description="Poe access key")
    modal_app_name: str = Field(..., description="Modal app name")

    # Capability Flags
    allow_attachments: bool = Field(default=True, description="Whether to allow file attachments in Poe")
    supports_system_messages: bool = Field(default=False)
    enable_image_comprehension: bool = Field(default=False)
    expand_text_attachments: bool = Field(default=False)
    streaming: bool = Field(default=False)
    enable_image_generation: bool = Field(default=False)
    enable_video_generation: bool = Field(default=False)

    # Inference Configuration
    max_tokens: Optional[int] = Field(default=None, description="Maximum number of tokens to generate")
    temperature: Optional[float] = Field(default=None, description="Temperature for sampling")
    top_p: Optional[float] = Field(default=None, description="Top-p sampling parameter")
    optimize_latency: bool = Field(default=False, description="Enable latency optimization with performanceConfig")

    # Reasoning Configuration (Claude 3.7+)
    enable_reasoning: bool = Field(default=False, description="Enable Claude's reasoning capability")
    reasoning_budget: Optional[int] = Field(default=1024, description="Token budget for reasoning (1024-4000 recommended)")

    # Monetization Configuration
    enable_monetization: bool = Field(default=False, description="Enable variable pricing monetization")
    custom_rate_card: Optional[str] = Field(
        default=None,
        description="Custom rate card for variable pricing in markdown format"
    )
    input_token_cost_milli_cents: Optional[int] = Field(
        default=None,
        description="Cost per input token in thousandths of a cent"
    )
    output_token_cost_milli_cents: Optional[int] = Field(
        default=None,
        description="Cost per output token in thousandths of a cent"
    )
    image_analysis_cost_milli_cents: Optional[int] = Field(
        default=None,
        description="Cost per image analysis in thousandths of a cent"
    )
 
 
Advanced multimodal capabilities 
One of the most powerful aspects of the framework is how it handles multimodal capabilities through simple configuration flags: 
 
 enable_image_comprehension ‚Äì When set to True for text-only models like Amazon Nova Micro, Poe itself uses vision capabilities to analyze images and convert them into text descriptions that are sent to the Amazon Bedrock model. This enables even text-only models to classify images without having built-in vision capabilities. 
 expand_text_attachments ‚Äì When set to True, Poe parses uploaded text files and includes their content in the conversation, enabling models to work with document content without requiring special file handling capabilities. 
 supports_system_messages ‚Äì This parameter controls whether the model can accept system prompts, allowing for consistent behavior across models with different capabilities. 
 
These configuration flags create a powerful abstraction layer that offers the following benefits: 
 
 Extends model capabilities ‚Äì Text-only models gain pseudo-multimodal capabilities through Poe‚Äôs preprocessing 
 Optimizes built-in features ‚Äì True multimodal models can use their built-in capabilities for optimal results 
 Simplifies integration ‚Äì It‚Äôs controlled through simple configuration switches rather than code changes 
 Maintains consistency ‚Äì It provides a uniform user experience regardless of the underlying model‚Äôs native capabilities 
 
Next, we explore the technical implementation of the solution in more detail. 
Protocol translation layer 
The most technically challenging aspect of the solution was bridging between Poe‚Äôs API protocols and the diverse model interfaces available through Amazon Bedrock. The team accomplished this through a sophisticated protocol translation layer: 
 
 # From services/streaming_service.py - Actual implementation
def _extract_content_from_event(self, event: Dict[str, Any]) -&gt; Optional[str]:
    """Extract content from a streaming event based on model provider."""
    try:
        # Handle Anthropic Claude models
        if "message" in event:
            message = event.get("message", {})
            if "content" in message and isinstance(message["content"], list):
                for content_item in message["content"]:
                    if content_item.get("type") == "text":
                        return content_item.get("text", "")
            elif "content" in message:
                return str(message.get("content", ""))

        # Handle Amazon Titan models
        if "delta" in event:
            delta = event.get("delta", {})
            if "text" in delta:
                return delta.get("text", "")

        # Handle other model formats
        if "chunk" in event:
            chunk_data = event.get("chunk", {})
            if "bytes" in chunk_data:
                # Process binary data if present
                try:
                    text = chunk_data["bytes"].decode("utf-8")
                    return json.loads(text).get("completion", "")
                except Exception:
                    self.logger.warning("Failed to decode bytes in chunk")

        # No matching format found
        return None
 
 
This translation layer handles subtle differences between models and makes sure that regardless of which Amazon Bedrock model is being used, the response to Poe is consistent and follows Poe‚Äôs expected format. 
Error handling and normalization 
A critical aspect of the implementation is comprehensive error handling and normalization. The ErrorService provides consistent error handling across different models: 
 
 # Simplified example of error handling (not actual code)
class ErrorService:
    def normalize_Amazon_Bedrock_error(self, error: Exception) -&gt; str:
        """Normalize Amazon Bedrock errors into a consistent format."""
        if isinstance(error, ClientError):
            if "ThrottlingException" in str(error):
                return "The model is currently experiencing high demand. Please try again in a moment."
            elif "ValidationException" in str(error):
                return "There was an issue with the request parameters. Please try again with different settings."
            elif "AccessDeniedException" in str(error):
                return "Access to this model is restricted. Please check your permissions."
            else:
                return f"An error occurred while communicating with the model: {str(error)}"
        elif isinstance(error, ConnectionError):
            return "Connection error. Please check your network and try again."
        else:
            return f"An unexpected error occurred: {str(error)}"
 
 
This approach makes sure users receive meaningful error messages regardless of the underlying model or error condition. 
Token counting and optimization 
The system implements sophisticated token counting and optimization to maximize effective use of models: 
 
 # From services/streaming_service.py - Actual implementation (partial)
# Calculate approximate JSON overhead
user_message_tokens = 0
for msg in conversation['messages']:
    for content_block in msg.get('content', []):
        if 'text' in content_block:
            # Simple word-based estimation of actual text content
            user_message_tokens += len(content_block['text'].split())

# Estimate JSON structure overhead (difference between total and content)
json_overhead = int((input_tokens - system_tokens) - user_message_tokens)

# Ensure we're working with integers for calculations
input_tokens_for_pct = int(input_tokens)
system_tokens_for_pct = int(system_tokens)
json_overhead_for_pct = int(json_overhead)

# Calculate percentage with float arithmetic and proper integer division
json_overhead_percent = (float(json_overhead_for_pct) / max(1, input_tokens_for_pct - system_tokens_for_pct)) * 100
... 
 
This detailed token tracking enables accurate cost estimation and optimization, facilitating efficient use of model resources. 
AWS authentication and security 
The AwsClientService handles authentication and security for Amazon Bedrock API calls.This implementation provides secure authentication with AWS services while providing proper error handling and connection management. 
Comparative analysis 
The implementation of the wrapper API dramatically improved the efficiency and capabilities of deploying Amazon Bedrock models on Poe, as detailed in the following table. 
 
  
   
   Feature 
   Before (Direct API) 
   After (Wrapper API) 
   
  
  
   
   Deployment Time 
   Days per model 
   Minutes per model 
   
   
   Developer Focus 
   Configuration and plumbing 
   Innovation and features 
   
   
   Model Diversity 
   Limited by integration capacity 
   Extensive (across Amazon Bedrock models) 
   
   
   Maintenance Overhead 
   High (separate code for each model) 
   Low (configuration-based) 
   
   
   Error Handling 
   Custom per model 
   Standardized across models 
   
   
   Cost Tracking 
   Complex (multiple integrations) 
   Simplified (centralized) 
   
   
   Multimodal Support 
   Fragmented 
   Unified 
   
   
   Security 
   Varied implementations 
   Consistent best practices 
   
  
 
This comparison highlights the significant improvements achieved through the wrapper API approach, demonstrating the value of investing in a robust abstraction layer. 
Performance metrics and business impact 
The wrapper API framework delivered significant and measurable business impact across multiple dimensions, including increased model diversity, deployment efficiency, and developer productivity. 
Poe can rapidly expand its model offerings, integrating tens of Amazon Bedrock models across text, image, and video modalities. This expansion occurred over a period of weeks rather than the months it would have taken with the previous approach. 
The following table summarizes the deployment efficiency metrics. 
 
  
   
   Metric 
   Before 
   After 
   Improvement 
   
  
  
   
   New Model Deployment 
   2 ‚Äì3 days 
   15 minutes 
   96x faster 
   
   
   Code Changes Required 
   500+ lines 
   20‚Äì30 lines 
   95% reduction 
   
   
   Testing Time 
   8‚Äì12 hours 
   30‚Äì60 minutes 
   87% reduction 
   
   
   Deployment Steps 
   10‚Äì15 steps 
   3‚Äì5 steps 
   75% reduction 
   
  
 
These metrics were measured through direct comparison of engineering hours required before and after implementation, tracking actual deployments of new models. 
The engineering team saw a dramatic shift in focus from integration work to feature development, as detailed in the following table. 
 
  
   
   Activity 
   Before (% of time) 
   After (% of time) 
   Change 
   
  
  
   
   API Integration 
   65% 
   15% 
   -50% 
   
   
   Feature Development 
   20% 
   60% 
   +40% 
   
   
   Testing 
   10% 
   15% 
   +5% 
   
   
   Documentation 
   5% 
   10% 
   +5% 
   
  
 
Scaling and performance considerations 
The wrapper API is designed to handle high-volume production workloads with robust scaling capabilities. 
Connection pooling 
To handle multiple concurrent requests efficiently, the wrapper implements connection pooling using aiobotocore. This allows it to maintain a pool of connections to Amazon Bedrock, reducing the overhead of establishing new connections for each request: 
 
 # From services/aws_service.py - Connection management
async def setup_client(self) -&gt; None:
    """Initialize AWS client with proper configuration."""
    async with self._client_lock:
        try:
            # Always clean up existing clients first to avoid stale connections
            if self.Amazon_Bedrock_client:
                await self.cleanup()

            # Increase timeout for image generation
            config = Config(
                read_timeout=300,  # 5 minutes timeout
                retries={'max_attempts': 3, 'mode': 'adaptive'},
                connect_timeout=30  # 30 second connection timeout
            )

            # Create the Amazon Bedrock client with proper error handling
            self.Amazon_Bedrock_client = await self.session.create_client(
                service_name="Amazon_Bedrock-runtime",
                region_name=self.bot_config.aws_region,
                aws_access_key_id=self.bot_config.aws_access_key,
                aws_secret_access_key=self.bot_config.aws_secret_key,
                aws_session_token=self.bot_config.aws_security_token,
                config=config
            ).__aenter__()
        except Exception as e:
            self.Amazon_Bedrock_client = None
            raise
 
 
Asynchronous processing 
The entire framework uses asynchronous processing to handle concurrent requests efficiently: 
 
 # From core/refactored_chat_bot.py - Asynchronous request handling
async def get_response(self, query: QueryRequest) -&gt; AsyncIterable[PartialResponse]:
    try:
        # Ensure AWS client is set up
        await aws_service.setup_client()

        # Validate and format the conversation
        conversation = await conversation_service.validate_conversation(query)

        # Process the request with streaming
        if self.bot_config.streaming:
            async for chunk in streaming_service.stream_Amazon_Bedrock_response(conversation, request_id):
                yield chunk
        else:
            # Non-streaming mode
            response_text, input_tokens, output_tokens = await streaming_service.non_stream_Amazon_Bedrock_response(conversation, request_id)
            if response_text:
                yield PartialResponse(text=response_text)
            else:
                yield PartialResponse(text=self.bot_config.fallback_response)
            # Send done event for non-streaming mode
            yield self.done_event()

    except Exception as e:
        # Error handling
        error_message = error_service.log_error(e, request_id, "Error during request processing")
        yield PartialResponse(text=error_message)
        yield self.done_event()
 
 
Error recovery and retry logic 
The system implements sophisticated error recovery and retry logic to handle transient issues: 
 
 # From services/streaming_service.py - Retry logic
max_retries = 3
base_delay = 1  # Start with 1 second delay

for attempt in range(max_retries):
    try:
        if not self.aws_service.Amazon_Bedrock_client:
            yield PartialResponse(text="Error: Amazon Bedrock client is not initialized")
            break

        response = await self.aws_service.Amazon_Bedrock_client.converse_stream(**stream_config)
        # Process response...
        break  # Success, exit retry loop

    except ClientError as e:
        if "ThrottlingException" in str(e):
            if attempt &lt; max_retries - 1:
                delay = base_delay * (2 ** attempt)  # Exponential backoff
                await asyncio.sleep(delay)
                continue
        error_message = f"Amazon Bedrock API Error: {str(e)}"
        yield PartialResponse(text=f"Error: {error_message}")
        break
 
 
Performance metrics 
The system collects detailed performance metrics to help identify bottlenecks and optimize performance: 
 
 # From services/streaming_service.py - Performance metrics
# Log token usage and latency
latency = time.perf_counter() - start_time

self.logger.info(
f"[{request_id}] Streaming Response Metrics:\n"
f" Time to First Token: {first_token_time:.4f} seconds\n"
f" Input Tokens: {input_tokens} (includes system prompt)\n"
f" Input Tokens for Billing: {input_tokens - system_tokens} (excludes system prompt)\n"
f" Output Tokens: {output_tokens}\n"
f" Total Tokens: {total_tokens}\n"
f" Amazon Bedrock Latency: {latency:.4f} seconds\n"
f" Latency Optimization: {'enabled' if hasattr(self.bot_config, 'optimize_latency') and self.bot_config.optimize_latency else 'disabled'}"
) 
 
Security considerations 
Security is a critical aspect of the wrapper implementation, with several key features to support secure operation. 
JWT validation with AWS SigV4 signing 
The system integrates JWT validation for Poe‚Äôs authentication with AWS SigV4 signing for Amazon Bedrock API calls: 
 
 JWT validation ‚Äì Makes sure only authorized Poe requests can access the wrapper API 
 SigV4 signing ‚Äì Makes sure the wrapper API can securely authenticate with Amazon Bedrock 
 Credential management ‚Äì AWS credentials are securely managed and not exposed to clients 
 
Secrets management 
The system integrates with AWS Secrets Manager to securely store and retrieve sensitive credentials: 
 
 # From services/aws_service.py - Secrets management
@staticmethod
def get_secret(secret_name: str, region_name: str = "us-east-1") -&gt; Dict[str, Any]:
    """
    Retrieve a secret from AWS Secrets Manager.

    Args:
        secret_name: Name of the secret to retrieve
        region_name: AWS region where the secret is stored

    Returns:
        Dict[str, Any]: The secret value as a dictionary
    """
    # Create a Secrets Manager client
    session = boto3.session.Session()
    client = session.client(
        service_name='secretsmanager',
        region_name=region_name
    )

    try:
        get_secret_value_response = client.get_secret_value(
            SecretId=secret_name
        )
    except Exception as e:
        logging.error(f"Error retrieving secret {secret_name}: {str(e)}")
        raise

    # Depending on whether the secret is a string or binary, one of these fields will be populated.
    if 'SecretString' in get_secret_value_response:
        import json
        try:
            # Explicitly annotate the return type for mypy
            result: Dict[str, Any] = json.loads(get_secret_value_response['SecretString'])
            return result
        except json.JSONDecodeError:
            # If not a JSON, return as a single-key dictionary
            return {"SecretString": get_secret_value_response['SecretString']}
    else:
        import base64
        decoded_binary_secret = base64.b64decode(get_secret_value_response['SecretBinary'])
        return {"SecretBinary": decoded_binary_secret}
 
 
Secure connection management 
The system implements secure connection management to help prevent credential leakage and facilitate proper cleanup: 
 
 # From services/aws_service.py - Secure connection cleanup
async def cleanup(self) -&gt; None:
    """Clean up AWS client resources."""
    try:
        if self.Amazon_Bedrock_client:
            try:
                await self.Amazon_Bedrock_client.__aexit__(None, None, None)
            except Exception as e:
                self.logger.error(f"Error closing Amazon Bedrock client: {str(e)}")
            finally:
                self.Amazon_Bedrock_client = None

        self.logger.info("Successfully cleaned up AWS client resources")
    except Exception as e:
        # Even if cleanup fails, reset the references to avoid stale connections
        self.Amazon_Bedrock_client = None
 
 
Troubleshooting and debugging 
The wrapper API includes comprehensive logging and debugging capabilities to help identify and resolve issues. The system implements detailed logging throughout the request processing flow. Each request is assigned a unique ID that is used throughout the processing flow to enable tracing: 
 
 # From core/refactored_chat_bot.py - Request tracing
request_id = str(id(query))
start_time = time.perf_counter()

# Used in all log messages
self.logger.info(f"[{request_id}] Incoming request received") 
 
Lessons learned and best practices 
Through this collaboration, several important technical insights emerged that might benefit others undertaking similar projects: 
 
 Configuration-driven architecture ‚Äì Using configuration files rather than code for model-specific behaviors proved enormously beneficial for maintenance and extensibility. This approach allowed new models to be added without code changes, significantly reducing the risk of introducing bugs. 
 Protocol translation challenges ‚Äì The most complex aspect was handling the subtle differences in streaming protocols between different models. Building a robust abstraction required careful consideration of edge cases and comprehensive error handling. 
 Error normalization ‚Äì Creating a consistent error experience across diverse models required sophisticated error handling that could translate model-specific errors into user-friendly, actionable messages. This improved both developer and end-user experiences. 
 Type safety ‚Äì Strong typing (using Python‚Äôs type hints extensively) was crucial for maintaining code quality across a complex codebase with multiple contributors. This practice reduced bugs and improved code maintainability. 
 Security first ‚Äì Integrating Secrets Manager from the start made sure credentials were handled securely throughout the system‚Äôs lifecycle, helping prevent potential security vulnerabilities. 
 
Conclusion 
The collaboration between the AWS Generative AI Innovation Center and Quora demonstrates how thoughtful architectural design can dramatically accelerate AI deployment and innovation. By creating a unified wrapper API for Amazon Bedrock models, the teams were able to reduce deployment time from days to minutes while expanding model diversity and improving user experience. 
This approach‚Äîfocusing on abstraction, configuration-driven development, and robust error handling‚Äîoffers valuable lessons for organizations looking to integrate multiple AI models efficiently. The patterns and techniques demonstrated in this solution can be applied to similar challenges across a wide range of AI integration scenarios. 
For technology leaders and developers working on similar challenges, this case study highlights the value of investing in flexible integration frameworks rather than point-to-point integrations. The initial investment in building a robust abstraction layer pays dividends in long-term maintenance and capability expansion. 
To learn more about implementing similar solutions, explore the following resources: 
 
 The AWS Well-Architected Framework for best practices in building secure, high-performing, resilient, and efficient infrastructure 
 The Amazon Bedrock Developer Guide for detailed information on working with FMs 
 The AWS Generative AI Innovation Center for assistance with your generative AI projects 
 AWS Prescriptive Guidance for LLM Deployment for best practices in deploying large language models 
 
The AWS Generative AI Innovation Center and Quora teams continue to collaborate on enhancements to this framework, making sure Poe users have access to the latest and most capable AI models with minimal deployment delay. 
 
About the authors 
Dr. Gilbert V Lepadatu is a Senior Deep Learning Architect at the AWS Generative AI Innovation Center, where he helps enterprise customers design and deploy scalable, cutting-edge GenAI solutions. With a PhD in Philosophy and dual Master‚Äôs degrees, he brings a holistic and interdisciplinary approach to data science and AI. 
Nick Huber is the AI Ecosystem Lead for Poe (by Quora), where he is responsible for ensuring high-quality &amp; timely integrations of the leading AI models onto the Poe platform.
‚Ä¢ Schedule topology-aware workloads using Amazon SageMaker HyperPod task governance
  Today, we are excited to announce a new capability of Amazon SageMaker HyperPod task governance to help you optimize training efficiency and network latency of your AI workloads. SageMaker HyperPod task governance streamlines resource allocation and facilitates efficient compute resource utilization across teams and projects on Amazon Elastic Kubernetes Service (Amazon EKS) clusters. Administrators can govern accelerated compute allocation and enforce task priority policies, improving resource utilization. This helps organizations focus on accelerating generative AI innovation and reducing time to market, rather than coordinating resource allocation and replanning tasks. Refer to Best practices for Amazon SageMaker HyperPod task governance for more information. 
Generative AI workloads typically demand extensive network communication across Amazon Elastic Compute Cloud (Amazon EC2) instances, where network bandwidth impacts both workload runtime and processing latency. The network latency of these communications depends on the physical placement of instances within a data center‚Äôs hierarchical infrastructure. Data centers can be organized into nested organizational units such as network nodes and node sets, with multiple instances per network node and multiple network nodes per node set. For example, instances within the same organizational unit experience faster processing time compared to those across different units. This means fewer network hops between instances results in lower communication. 
To optimize the placement of your generative AI workloads in your SageMaker HyperPod clusters by considering the physical and logical arrangement of resources, you can use EC2 network topology information during your job submissions. An EC2 instance‚Äôs topology is described by a set of nodes, with one node in each layer of the network. Refer to How Amazon EC2 instance topology works for details on how EC2 topology is arranged. Network topology labels offer the following key benefits: 
 
 Reduced latency by minimizing network hops and routing traffic to nearby instances 
 Improved training efficiency by optimizing workload placement across network resources 
 
With topology-aware scheduling for SageMaker HyperPod task governance, you can use topology network labels to schedule your jobs with optimized network communication, thereby improving task efficiency and resource utilization for your AI workloads. 
In this post, we introduce topology-aware scheduling with SageMaker HyperPod task governance by submitting jobs that represent hierarchical network information. We provide details about how to use SageMaker HyperPod task governance to optimize your job efficiency. 
Solution overview 
Data scientists interact with SageMaker HyperPod clusters. Data scientists are responsible for the training, fine-tuning, and deployment of models on accelerated compute instances. It‚Äôs important to make sure data scientists have the necessary capacity and permissions when interacting with clusters of GPUs. 
To implement topology-aware scheduling, you first confirm the topology information for all nodes in your cluster, then run a script that tells you which instances are on the same network nodes, and finally schedule a topology-aware training task on your cluster. This workflow facilitates higher visibility and control over the placement of your training instances. 
In this post, we walk through viewing node topology information and submitting topology-aware tasks to your cluster. For reference, NetworkNodes describes the network node set of an instance. In each network node set, three layers comprise the hierarchical view of the topology for each instance. Instances that are closest to each other will share the same layer 3 network node. If there are no common network nodes in the bottom layer (layer 3), then see if there is commonality at layer 2. 
Prerequisites 
To get started with topology-aware scheduling, you must have the following prerequisites: 
 
 An EKS cluster 
 A SageMaker HyperPod cluster with instances enabled for topology information 
 The SageMaker HyperPod task governance add-on installed (version 1.2.2 or later) 
 Kubectl installed 
 (Optional) The SageMaker HyperPod CLI installed 
 
Get node topology information 
Run the following command to show node labels in your cluster. This command provides network topology information for each instance. 
 
 kubectl get nodes -L topology.k8s.aws/network-node-layer-1
kubectl get nodes -L topology.k8s.aws/network-node-layer-2
kubectl get nodes -L topology.k8s.aws/network-node-layer-3 
 
Instances with the same network node layer 3 are as close as possible, following EC2 topology hierarchy. You should see a list of node labels that look like the following:topology.k8s.aws/network-node-layer-3: nn-33333exampleRun the following script to show the nodes in your cluster that are on the same layers 1, 2, and 3 network nodes: 
 
 git clone https://github.com/aws-samples/awsome-distributed-training.git
cd awsome-distributed-training/1.architectures/7.sagemaker-hyperpod-eks/task-governance 
chmod +x visualize_topology.sh
bash visualize_topology.sh 
 
The output of this script will print a flow chart that you can use in a flow diagram editor such as Mermaid.js.org to visualize the node topology of your cluster. The following figure is an example of the cluster topology for a seven-instance cluster. 
 
Submit tasks 
SageMaker HyperPod task governance offers two ways to submit tasks using topology awareness. In this section, we discuss these two options and a third alternative option to task governance. 
Modify your Kubernetes manifest file 
First, you can modify your existing Kubernetes manifest file to include one of two annotation options: 
 
 kueue.x-k8s.io/podset-required-topology ‚Äì Use this option if you must have all pods scheduled on nodes on the same network node layer in order to begin the job 
 kueue.x-k8s.io/podset-preferred-topology ‚Äì Use this option if you ideally want all pods scheduled on nodes in the same network node layer, but you have flexibility 
 
The following code is an example of a sample job that uses the kueue.x-k8s.io/podset-required-topology setting to schedule pods that share the same layer 3 network node: 
 
 apiVersion: batch/v1
kind: Job
metadata:
&nbsp;&nbsp;name: test-tas-job
&nbsp;&nbsp;namespace: hyperpod-ns-team-a
&nbsp;&nbsp;labels:
&nbsp;&nbsp; &nbsp;kueue.x-k8s.io/queue-name: hyperpod-ns-team-a-localqueue
&nbsp;&nbsp; &nbsp;kueue.x-k8s.io/priority-class: inference-priority
spec:
&nbsp;&nbsp;parallelism: 10
&nbsp;&nbsp;completions: 10
&nbsp;&nbsp;suspend: true
&nbsp;&nbsp;template:
&nbsp;&nbsp; &nbsp;metadata:
&nbsp;&nbsp; &nbsp; &nbsp;labels:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;kueue.x-k8s.io/queue-name: hyperpod-ns-team-a-localqueue
&nbsp;&nbsp; &nbsp; &nbsp;annotations:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;kueue.x-k8s.io/podset-required-topology: "topology.k8s.aws/network-node-layer-3"
&nbsp;&nbsp; &nbsp;spec:
&nbsp;&nbsp; &nbsp; &nbsp;containers:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;- name: dummy-job
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;image: public.ecr.aws/docker/library/alpine:latest
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;command: ["sleep", "3600s"]
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;resources:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;requests:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;cpu: "1"
&nbsp;&nbsp; &nbsp; &nbsp;restartPolicy: Never 
 
To verify which nodes your pods are running on, use the following command to view node IDs per pod:kubectl get pods -n hyperpod-ns-team-a -o wide 
Use the SageMaker HyperPod CLI 
The second way to submit a job is through the SageMaker HyperPod CLI. Be sure to install the latest version (version pending) to use topology-aware scheduling. To use topology-aware scheduling with the SageMaker HyperPod CLI, you can include either the --preferred-topology parameter or the --required-topology parameter in your create job command. 
The following code is an example command to start a topology-aware mnist training job using the SageMaker HyperPod CLI, replace XXXXXXXXXXXX with your AWS account ID: 
 
 hyp create hyp-pytorch-job \
--job-name test-pytorch-job-cli \
--image XXXXXXXXXXXX.dkr.ecr.us-west-2.amazonaws.com/ptjob:mnist \
--pull-policy "Always" \
--tasks-per-node 1 \
--max-retry 1 \
--preferred-topology topology.k8s.aws/network-node-layer-3 
 
Clean up 
If you deployed new resources while following this post, refer to the Clean Up section in the SageMaker HyperPod EKS workshop to make sure you don‚Äôt accrue unwanted charges. 
Conclusion 
During large language model (LLM) training, pod-to-pod communication distributes the model across multiple instances, requiring frequent data exchange between these instances. In this post, we discussed how SageMaker HyperPod task governance helps schedule workloads to enable job efficiency by optimizing throughput and latency. We also walked through how to schedule jobs using SageMaker HyperPod topology network information to optimize network communication latency for your AI tasks. 
We encourage you to try out this solution and share your feedback in the comments section. 
 
About the authors 
Nisha Nadkarni is a Senior GenAI Specialist Solutions Architect at AWS, where she guides companies through best practices when deploying large scale distributed training and inference on AWS. Prior to her current role, she spent several years at AWS focused on helping emerging GenAI startups develop models from ideation to production. 
Siamak Nariman&nbsp;is a Senior Product Manager at AWS. He is focused on AI/ML technology, ML model management, and ML governance to improve overall organizational efficiency and productivity. He has extensive experience automating processes and deploying various technologies. 
Zican Li is a Senior Software Engineer at Amazon Web Services (AWS), where he leads software development for Task Governance on SageMaker HyperPod. In his role, he focuses on empowering customers with advanced AI capabilities while fostering an environment that maximizes engineering team efficiency and productivity.  
Anoop Saha is a Sr GTM Specialist at Amazon Web Services (AWS) focusing on generative AI model training and inference. He partners with top frontier model builders, strategic customers, and AWS service teams to enable distributed training and inference at scale on AWS and lead joint GTM motions. Before AWS, Anoop held several leadership roles at startups and large corporations, primarily focusing on silicon and system architecture of AI infrastructure.

‚∏ª