‚úÖ Morning News Briefing ‚Äì September 05, 2025 10:48

üìÖ Date: 2025-09-05 10:48
üè∑Ô∏è Tags: #briefing #ai #publichealth #digitalgov

‚∏ª

üßæ Weather
‚Ä¢ SPECIAL WEATHER STATEMENT, Pembroke
  Persons in or near this area should be on the lookout for adverse weather conditions and take necessary safety precautions . People in or around this area are advised to take necessary precautions . The weather is expected to deteriorate rapidly in the years of 2025 . People should be aware of the weather conditions at this time of the year and take safety precautions in the event of an adverse weather event . For more
‚Ä¢ Current Conditions:  5.8¬∞C
  Temperature: 5.8&deg;C Pressure / Tendency: 100.9 kPa falling Humidity: 99 % Humidity : 99 % Dewpoint: 5 .7&deg:C Wind: SSE 6 km/h . Air Quality Health Index: n/a . Pembroke 6:00 AM EDT Friday 5 September 2025 . Weather forecast: Pem
‚Ä¢ Friday: Chance of showers. High 21. POP 60%
  Increasing cloudiness early this morning . 60 percent chance of showers this afternoon with risk of thunderstorm . Wind becoming southeast 30 km/h gusting to 50 this morning then southwest 40 gust to 70 this afternoon . High 21. UV index 5 or moderate, with a risk of a thunderstorm at the risk of rain in the afternoon . Forecast issued 5:00 AM EDT Friday

üåç International News
No updates.

üçÅ Canadian News
No updates.

üá∫üá∏ U.S. Top Stories
‚Ä¢ Chinese public is divided over whether to seek global dominance or share leadership
  A new poll shows a majority of Chinese people see competition with the U.S. as a threat, but there is a split on what role Beijing should take on in the world stage . A majority of China people see a threat to the United States, but a split is over what Beijing should do to help the country's interests . A poll shows that most Chinese people are divided on
‚Ä¢ 'One and done' dose of LSD keeps anxiety at bay
  People with anxiety disorder improved significantly after they got a single dose of LSD powerful enough to induce a psychedelic trip . People with generalized anxiety disorder improve significantly after getting a one dose of the drug, according to a recent study . The study found that people with anxiety disorders improved significantly when they were given a one-dose dose of psychedelic LSD . The drug is powerful enough for people to induce
‚Ä¢ Why the end of the de minimus tariff exemption is causing shipping chaos worldwide
  Planet Money looks at what the de minimis tariff exemption is, who wins and loses with the end of this policy and why ending it has resulted in shipping chaos worldwide . Planet Money takes a look at what it means and why it is causing shipping chaos around the world . The end of the exemption has caused shipping chaos across the world, and why this has led to chaos in shipping .
‚Ä¢ For first Black woman to design cars for Ford, passion for cars was a family affair
  Emeline King was the first Black woman to design cars for the Ford Motor Company . She remembers her inspiration: her father, who she says was the company's first Black female designer . StoryCorps: Emeline recalls her inspiration for her Ford design career in the 1950s and '60s . She says she was inspired by her father's work at the company in New York .
‚Ä¢ The world's oldest and largest iceberg will soon be no more
  The iceberg, known as A23a, has been on a journey following the current into warmer waters for months . Now, it has begun the predicted and natural process of breaking apart, and eventually melting . The iceberg has been following a warm current for months, but is now beginning to break apart and melt into the sea . The UK MOD MOD says the iceberg has already begun to melt

üß† Artificial Intelligence
No updates.

üíª Digital Strategy
‚Ä¢ Fujitsu under fire for bidding on UK public sector deals despite Horizon scandal vow
  Campaigner slams Fujitsu's lack of transparency over promise not to bid for new government work . Millions continued to flow after Japanese supplier said it would step back . Post Office Horizon scandal saw millions of pounds of business continue to flow into the UK . Fujitsu has been criticised for not bidding for government work in the wake of the scandal . The Japanese vendor has been accused of a lack of
‚Ä¢ Huawei's battery energy storage systems run out of juice in the UK
  Sources say decision to pull products takes effect from end of 2025 . BESS battery energy storage systems (BESS) to be discontinued locally by the end of the decade . Huawei's product portfolio in Britain is about to shrink again with suppliers informed that BESS systems will be discontinued in the UK by 2025 . Huawei: BESS is to be discontinue in Britain by end of decade .
‚Ä¢ Knock-on effects of software dev break-in hit schools trust
  Affinity Learning Partnership warns staff after Intradev breach . UK education trust has warned staff their personal information may have been compromised . The cyberattack was carried out on software developer IntradeV in August . Affinity Education Partnership has issued a warning to staff after a cyberattack on the firm's software firm . The trust is the largest in the country's education trust in the UK .
‚Ä¢ Techie ended vendor/client blame game by treating managers like toddlers
  Bickering continued despite mess putting cancer patients at risk . On Call is a tech support column in which we share your tech support stories . Read this week's On Call, The Register's Friday column in our tech support section . Back to the page you came from, please submit your story for next week's edition of On Call . Send it to On Call by email email jenn
‚Ä¢ AI code assistants make developers more efficient at creating security problems
  AI coding assistants allow developers to move fast and break things, which may not be ideal . Fixes typos, creates timebombs, fixes typos and creates time bombs . AI coding assistant allows developers to break things and fix things faster than they could have done otherwise . It's not ideal for developers to be forced to work fast or break things in order to get things done faster .

üè• Public Health
No updates.

üî¨ Science
‚Ä¢ Feasibility and clinical utility of expanded genomic newborn screening in the Early Check program
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Association between atrial fibrillation and age-related macular degeneration: A nationwide cohort study
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Inside a mosquito factory
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ RFK Jr. slings accusations and defends public-health upheaval at fiery hearing
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Components of particulate matter as potential risk factors for acute myocardial infarction
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations

üßæ Government & Policy
No updates.

üèõÔ∏è Enterprise Architecture & IT Governance
No updates.

ü§ñ AI & Emerging Tech
‚Ä¢ Putin says organ transplants could grant immortality. Not quite.
  This week I‚Äôm writing from Manchester, where I‚Äôve been attending&nbsp;a conference on aging. Wednesday was full of talks and presentations by scientists who are trying to understand the nitty-gritty of aging‚Äîall the way down to the molecular level. Once we can understand the complex biology of aging, we should be able to slow or prevent the onset of age-related diseases, they hope.



Then my editor forwarded me a video of the leaders of Russia and China talking about immortality. ‚ÄúThese days at 70 years old you are still a child,‚Äù China‚Äôs Xi Jinping, 72, was translated as saying, according to footage livestreamed by CCTV to&nbsp;multiple&nbsp;media outlets.



‚ÄúWith the developments of biotechnology, human organs can be continuously transplanted, and people can live younger and younger, and even achieve immortality,‚Äù Russia‚Äôs Vladimir Putin, also 72, is reported to have replied.



SERGEI BOBYLEV, SPUTNIK, KREMLIN POOL PHOTO VIA AP




There‚Äôs a striking contrast between that radical vision and the incremental longevity science presented at the meeting. Repeated rounds of organ transplantation surgery aren‚Äôt likely to help anyone radically extend their lifespan anytime soon.





First, back to Putin‚Äôs proposal: the idea of continually replacing aged organs to stay young. It‚Äôs a simplistic way to think about aging. After all, aging is so complicated that researchers can‚Äôt agree on what causes it, why it occurs, or even how to define it, let alone ‚Äútreat‚Äù it.



Having said that, there may be some merit to the idea of repairing worn-out body parts with biological or synthetic replacements. Replacement therapies‚Äîincluding bioengineered organs‚Äîare being developed by multiple research teams. Some have already been tested in people. This week, let‚Äôs take a look at the idea of replacement therapies.



No one fully understands why our organs start to fail with age. On the face of it, replacing them seems like a good idea. After all, we already know how to do organ transplants. They‚Äôve been a part of medicine since the 1950s and have been used to&nbsp;save hundreds of thousands of lives in the US alone.



And replacing old organs with young ones might have more broadly beneficial effects. When a young mouse is stitched to an old one, the older mouse benefits from the arrangement, and its health seems to improve.



The problem is that we don‚Äôt really know why. We don‚Äôt know what it is about young body tissues that makes them health-promoting. We don‚Äôt know how long these effects might last in a person. We don‚Äôt know how different organ transplants will compare, either. Might a young heart be more beneficial than a young liver? No one knows.



And that‚Äôs before you consider the practicalities of organ transplantation. There is already a shortage of donor organs‚Äîthousands of people die on waiting lists. Transplantation requires major surgery and, typically, a lifetime of prescription drugs that damp down the immune system,&nbsp;leaving a person more susceptible to certain infections and diseases.





So the idea of repeated organ transplantations shouldn‚Äôt really be a particularly appealing one. ‚ÄúI don‚Äôt think that‚Äôs going to happen anytime soon,‚Äù says Jesse Poganik, who studies aging at Brigham and Women‚Äôs Hospital in Boston and is also in Manchester for the meeting.



Poganik has been collaborating with transplant surgeons in his own research. ‚ÄúThe surgeries are good, but they‚Äôre not simple,‚Äù he tells me. And they come with real risks. His own 24-year-old cousin developed a form of cancer after a liver and heart transplant. She died a few weeks ago, he says.



So when it comes to replacing worn-out organs, scientists are looking for both biological and synthetic alternatives.&nbsp;&nbsp;



We‚Äôve been replacing body parts for centuries. Wooden toes were used as far back as the 15th century. Joint replacements have been around for more than a hundred years. And major innovations over the last 70 years have given us devices like pacemakers, hearing aids, brain implants, and artificial hearts.



Scientists are exploring other ways to make tissues and organs, too. There are different approaches here, but they include everything from injecting stem cells to seeding ‚Äúscaffolds‚Äù with cells in a lab.



In 1999, researchers used volunteers‚Äô own cells to seed bladder-shaped collagen scaffolds. The resulting bioengineered bladders went on to be transplanted into seven people in&nbsp;an initial trial.&nbsp;



Now scientists are working on more complicated organs. Jean H√©bert, a program manager at the US government‚Äôs Advanced Research Projects Agency for Health, has been exploring ways to gradually replace the cells in a person‚Äôs brain. The idea is that, eventually, the recipient will end up with a young brain.



H√©bert&nbsp;showed my colleague Antonio Regalado how, in his early experiments, he removed parts of mice‚Äôs brains and replaced them with embryonic stem cells. That work seems a world away from the biochemical studies being presented at the British Society for Research on Ageing annual meeting in Manchester, where I am now.





On Wednesday, one scientist described how he‚Äôd been testing potential longevity drugs on the tiny nematode worm C. elegans. These worms live for only about 15 to 40 days, and his team can perform tens of thousands of experiments with them. About 40% of the drugs that extend lifespan in C. elegans also help mice live longer, he told us.



To me, that‚Äôs not an amazing hit rate. And we don‚Äôt know how many of those drugs will work in people. Probably less than 40% of that 40%.



Other scientists presented work on chemical reactions happening at the cellular level. It was deep, basic science, and my takeaway was that there‚Äôs a lot aging researchers still don‚Äôt fully understand.



It will take years‚Äîif not decades‚Äîto get the full picture of aging at the molecular level. And if we rely on a series of experiments in worms, and then mice, and then humans, we‚Äôre unlikely to make progress for a really long time. In that context, the idea of replacement therapy feels like a shortcut.



‚ÄúReplacement is a really exciting avenue because you don‚Äôt have to understand the biology of aging as much,‚Äù says Sierra Lore, who studies aging at the University of Copenhagen in Denmark and the Buck Institute for Research on Aging in Novato, California.



Lore says she started her research career studying aging at the molecular level, but she soon changed course. She now plans to focus her attention on replacement therapies. ‚ÄúI very quickly realized we‚Äôre decades away [from understanding the molecular processes that underlie aging],‚Äù she says. ‚ÄúWhy don‚Äôt we just take what we already know‚Äîreplacement‚Äîand try to understand and apply it better?‚Äù



So perhaps Putin‚Äôs straightforward approach to delaying aging holds some merit. Whether it will grant him immortality is another matter.



This article first appeared in The Checkup,&nbsp;MIT Technology Review‚Äôs&nbsp;weekly biotech newsletter. To receive it in your inbox every Thursday, and read articles like this first,&nbsp;sign up here.
‚Ä¢ Imagining the future of banking with agentic AI
  Banks are increasingly employing agentic AI to optimize processes, navigate complex systems, and sift through vast quantities of unstructured data to make decisions and take actions . A 2025 survey of 250 banking executives by MIT Technology Review Insights found that 70% of leaders say their firm uses AI to some degree, either through existing deployments (16%) or pilot projects (52%) Agentic AI is already proving effective in a range of different functions .
‚Ä¢ The Download: unnerving AI avatars, and Trump‚Äôs climate gift to China
  This is today&#8217;s edition of¬†The Download,¬†our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



Synthesia‚Äôs AI clones are more expressive than ever. Soon they‚Äôll be able to talk back.



‚ÄîRhiannon Williams



Earlier this summer, I visited the AI company Synthesia to give it what it needed to create a hyperrealistic AI-generated avatar of me. The company‚Äôs avatars are a decent barometer of just how dizzying progress has been in AI over the past few years, so I was curious just how accurately its latest AI model, introduced last month, could replicate me.I found my avatar as unnerving as it is technically impressive. It‚Äôs slick enough to pass as a high-definition recording of a chirpy corporate speech, and if you didn‚Äôt know me, you‚Äôd probably think that‚Äôs exactly what it was.&nbsp;



My avatar shows how it‚Äôs becoming ever-harder to distinguish the artificial from the real. And before long, these avatars will even be able to talk back to us. But how much better can they get? And what might interacting with AI clones do to us? Read the full story.







How Trump is helping China extend its massive lead in clean energy&nbsp;



On a spring day in 1954, Bell Labs researchers showed off the first practical solar panels at a press conference in New Jersey, using sunlight to spin a toy Ferris wheel before a stunned crowd.



The solar future looked bright. But in the race to commercialize the technology it invented, the US would lose resoundingly. Last year, China exported $40 billion worth of solar panels and modules, while America shipped just $69 million, according to the New York Times. It was a stunning forfeit of a huge technological lead.&nbsp;



Now, thanks to its policies propping up aging fossil-fuel industries, the US seems determined to repeat the mistake. Read the full story.



‚ÄîJames Temple



This article is from The Spark, MIT Technology Review‚Äôs newsletter all about the latest in climate and energy tech. To receive it in your inbox every Wednesday, sign up here.







The must-reads



I‚Äôve combed the internet to find you today‚Äôs most fun/important/scary/fascinating stories about technology.



1 AI chatbots of celebrities sent risqu√© messages to teenagersVirtual versions of Timoth√©e Chalamet and Chappell Roan discussed sex and drugs. (WP $)+ An AI companion site is hosting sexually charged conversations with underage celebrity bots. (MIT Technology Review)2 Trump can‚Äôt make up his mind about US tech giantsWhile defending them against EU regulation, he‚Äôs also pushing to break them up. (FT $)+ He‚Äôs hosting tech leaders at the White House later today. (Reuters)+ Elon Musk doesn‚Äôt appear to have made the guest list. (CNBC)



3 Trump‚Äôs cuts have led to babies born with HIVClinics in East Africa are closing, and people are being forced to skip vital drug doses. (The Guardian)+ Artificial blood could save many lives. Why aren‚Äôt we using it? (Slate)



4 Germany has already met its 2028 goal for reducing coal-fired powerFor the second year running, it won‚Äôt have to shut any more plants as a result. (Bloomberg $)+ The UK is done with coal. How‚Äôs the rest of the world doing? (MIT Technology Review)



5 The risk of all-out nuclear war is growingBut we‚Äôve normalized nuclear competition so much, the risks aren‚Äôt always clear. (New Yorker $)+ Maybe it‚Äôs time to start burying nuclear reactors‚Äô cores. (Economist $)



6 xAI is hemorrhaging executivesThe CFO has left just months after joining. (WSJ $)



7 India‚Äôs chip industry is gaining momentumYears of investment are starting to pay off. But can it strike deals with overseas chip giants too? (Bloomberg $)+ Meanwhile, Taiwan‚Äôs chip hub is home to a baby boom. (Rest of World)+ Inside India‚Äôs scramble for AI independence. (MIT Technology Review)



8 Boston Dynamics‚Äô Atlas robot only needs one AI model to workIt‚Äôs all it requires to master humanlike movements successfully. (Wired $)+ How ‚Äòrobot ballet‚Äô could shake up factory production lines. (FT $)+ Humanoid robots still aren‚Äôt living up to their lofty promises. (IEEE Spectrum)+ Will we ever trust robots? (MIT Technology Review)



9 How studying astronauts could improve health on EarthThere‚Äôs still a huge amount we don‚Äôt know about space‚Äôs effects on humans. (Vox)+ Space travel is dangerous. Could genetic testing and gene editing make it safer? (MIT Technology Review)



10 The Caribbean island of Anguilla has hit upon an AI cash cowBy selling its .ai domain. (Semafor)+ How a tiny Pacific Island became the global capital of cybercrime. (MIT Technology Review)







Quote of the day



‚ÄúIf you are not being scammed yet, it‚Äôs because you haven‚Äôt encountered a scam designed just for you and only for you.‚Äù



‚ÄîJeff Kuo, chief executive of Taiwanese fraud prevention company Gogolook, warns the Financial Times about the endless possibilities generative AI presents to scammers.







One more thing







China built hundreds of AI data centers to catch the AI boom. Now many stand unused.Last year, China‚Äôs boom in data center construction was at its height, fueled by both government and private investors. Renting out GPUs to companies that need them for training AI models was seen as a sure bet.But with the rise of DeepSeek and a sudden change in the economics around AI, the industry is faltering. Prices for GPUs are falling and many newly built facilities are now sitting empty. Read the full story to find out why.



‚ÄîCaiwei Chen







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)



+ The trailer for the forthcoming Wuthering Heights film is here and it looks‚Ä¶interesting.+ This fall‚Äôs crop of video games is outstanding.+ Textured walls are a surefire way to make your home look dated. Here‚Äôs some other faux pas to avoid.+The dogs of this year‚Äôs US Open are too cute ($)
‚Ä¢ Transforming CX with embedded real-time analytics
  During Black Friday in 2024, Stripe processed more than $31 billion in transactions, with processing rates peaking at 137,000 transactions per minute, the highest in the company‚Äôs history . Having the capability to collect and analyze data in real time correlates with companies‚Äô ability to grow . Business leaders that scored company in the top quartile for real-time operations saw 50% higher revenue growth and net margins .
‚Ä¢ Synthesia‚Äôs AI clones are more expressive than ever. Soon they‚Äôll be able to talk back.
  Earlier this summer, I walked through the glassy lobby of a fancy office in London, into an elevator, and then along a corridor into a clean, carpeted room. Natural light flooded in through its windows, and a large pair of umbrella-like lighting rigs made the room even brighter. I tried not to squint as I took my place in front of a tripod equipped with a large camera and a laptop displaying an autocue. I took a deep breath and started to read out the script.



I‚Äôm not a newsreader or an actor auditioning for a movie‚ÄîI was visiting the AI company Synthesia to give it what it needed to create a hyperrealistic AI-generated avatar of me. The company‚Äôs avatars are a decent barometer of just how dizzying progress has been in AI over the past few years, so I was curious just how accurately its latest AI model, introduced last month, could replicate me.&nbsp;



When Synthesia launched in 2017, its primary purpose was to match AI versions of real human faces‚Äîfor example, the former footballer David Beckham‚Äîwith dubbed voices speaking in different languages. A few years later, in 2020, it started giving the companies that signed up for its services the opportunity to make professional-level presentation videos starring either AI versions of staff members or consenting actors. But the technology wasn‚Äôt perfect. The avatars‚Äô body movements could be jerky and unnatural, their accents sometimes slipped, and the emotions indicated by their voices didn‚Äôt always match their facial expressions.



Now Synthesia‚Äôs avatars have been updated with more natural mannerisms and movements, as well as expressive voices that better preserve the speaker‚Äôs accent‚Äîmaking them appear more humanlike than ever before. For Synthesia‚Äôs corporate clients, these avatars will make for slicker presenters of financial results, internal communications, or staff training videos.



I found the video demonstrating my avatar as unnerving as it is technically impressive. It‚Äôs slick enough to pass as a high-definition recording of a chirpy corporate speech, and if you didn‚Äôt know me, you‚Äôd probably think that‚Äôs exactly what it was. This demonstration shows how much harder it‚Äôs becoming to distinguish the artificial from the real. And before long, these avatars will even be able to talk back to us. But how much better can they get? And what might interacting with AI clones do to us?&nbsp;&nbsp;



The creation process



When my former colleague Melissa visited Synthesia‚Äôs London studio to create an avatar of herself last year, she had to go through a long process of calibrating the system, reading out a script in different emotional states, and mouthing the sounds needed to help her avatar form vowels and consonants. As I stand in the brightly lit room 15 months later, I‚Äôm relieved to hear that the creation process has been significantly streamlined. Josh Baker-Mendoza, Synthesia‚Äôs technical supervisor, encourages me to gesture and move my hands as I would during natural conversation, while simultaneously warning me not to move too much. I duly repeat an overly glowing script that‚Äôs designed to encourage me to speak emotively and enthusiastically. The result is a bit as if if Steve Jobs had been resurrected as a blond British woman with a low, monotonous voice.&nbsp;



It also has the unfortunate effect of making me sound like an employee of Synthesia.‚ÄúI am so thrilled to be with you today to show off what we‚Äôve been working on. We are on the edge of innovation, and the possibilities are endless,‚Äù I parrot eagerly, trying to sound lively rather than manic. ‚ÄúSo get ready to be part of something that will make you go, ‚ÄòWow!‚Äô This opportunity isn‚Äôt just big‚Äîit‚Äôs monumental.‚Äù



Just an hour later, the team has all the footage it needs. A couple of weeks later I receive two avatars of myself: one powered by the previous Express-1 model and the other made with the latest Express-2 technology. The latter, Synthesia claims, makes its synthetic humans more lifelike and true to the people they‚Äôre modeled on, complete with more expressive hand gestures, facial movements, and speech. You can see the results for yourself below.&nbsp;



COURTESY SYNTHESIA




Last year, Melissa found that her Express-1-powered avatar failed to match her transatlantic accent. Its range of emotions was also limited‚Äîwhen she asked her avatar to read a script angrily, it sounded more whiny than furious. In the months since, Synthesia has improved Express-1, but the version of my avatar made with the same technology blinks furiously and still struggles to synchronize body movements with speech.



By way of contrast, I‚Äôm struck by just how much my new Express-2 avatar looks like me: Its facial features mirror my own perfectly. Its voice is spookily accurate too, and although it gesticulates more than I do, its hand movements generally marry up with what I‚Äôm saying.&nbsp;



But the tiny telltale signs of AI generation are still there if you know where to look. The palms of my hands are bright pink and as smooth as putty. Strands of hair hang stiffly around my shoulders instead of moving with me. Its eyes stare glassily ahead, rarely blinking. And although the voice is unmistakably mine, there‚Äôs something slightly off about my digital clone‚Äôs intonations and speech patterns. ‚ÄúThis is great!‚Äù my avatar randomly declares, before slipping back into a saner register.





Anna Eiserbeck, a postdoctoral psychology researcher at the Humboldt University of Berlin who has studied how humans react to perceived deepfake faces, says she isn‚Äôt sure she‚Äôd have been able to identify my avatar as a deepfake at first glance.



But she would eventually have noticed something amiss. It‚Äôs not just the small details that give it away‚Äîmy oddly static earring, the way my body sometimes moves in small, abrupt jerks. It‚Äôs something that runs much deeper, she explains.



‚ÄúSomething seemed a bit empty. I know there‚Äôs no actual emotion behind it‚Äî it‚Äôs not a conscious being. It does not feel anything,‚Äù she says. Watching the video gave her ‚Äúthis kind of uncanny feeling.‚Äù&nbsp;



My digital clone, and Eiserbeck‚Äôs reaction to it, make me wonder how realistic these avatars really need to be.&nbsp;



I realize that part of the reason I feel disconcerted by my avatar is that it behaves in a way I rarely have to. Its oddly upbeat register is completely at odds with how I normally speak; I‚Äôm a die-hard cynical Brit who finds it difficult to inject enthusiasm into my voice even when I‚Äôm genuinely thrilled or excited. It‚Äôs just the way I am. Plus, watching the videos on a loop makes me question if I really do wave my hands about that way, or move my mouth in such a weird manner. If you thought being confronted with your own face on a Zoom call was humbling, wait until you‚Äôre staring at a whole avatar of yourself.&nbsp;



When Facebook was first taking off in the UK almost 20 years ago, my friends and I thought illicitly logging into each other‚Äôs accounts and posting the most outrageous or rage-inducing status updates imaginable was the height of comedy. I wonder if the equivalent will soon be getting someone else‚Äôs avatar to say something truly embarrassing: expressing support for a disgraced politician or (in my case) admitting to liking Ed Sheeran‚Äôs music.&nbsp;



Express-2 remodels every person it‚Äôs presented with into a polished professional speaker with the body language of a hyperactive hype man. And while this makes perfect sense for a company focused on making glossy business videos, watching my avatar doesn‚Äôt feel like watching me at all. It feels like something else entirely.



How it works



The real technical challenge these days has less to do with creating avatars that match our appearance than with getting them to replicate our behavior, says Bj√∂rn Schuller, a professor of artificial intelligence at Imperial College London. ‚ÄúThere‚Äôs a lot to consider to get right; you have to have the right micro gesture, the right intonation, the sound of voice and the right word,‚Äù he says. ‚ÄúI don‚Äôt want an AI [avatar] to frown at the wrong moment‚Äîthat could send an entirely different message.‚Äù



To achieve an improved level of realism, Synthesia developed a number of new audio and video AI models. The team created a voice cloning model to preserve the human speaker‚Äôs accent, intonation, and expressiveness‚Äîunlike other voice models, which can flatten speakers‚Äô distinctive accents into generically American-sounding voices.





When a user uploads a script to Express-1, its system analyzes the words to infer the correct tone to use. That information is then fed into a diffusion model, which renders the avatar‚Äôs facial expressions and movements to match the speech.¬†



Alongside the voice model, Express-2 uses three other models to create and animate the avatars. The first generates an avatar‚Äôs gestures to accompany the speech fed into it by the Express-Voice model. A second evaluates how closely the input audio aligns with the multiple versions of the corresponding generated motion before selecting the best one. Then a final model renders the avatar with that chosen motion.¬†



This third rendering model is significantly more powerful than its Express-1 predecessor. Whereas the previous model had a few hundred million parameters, Express-2‚Äôs rendering model‚Äôs parameters number in the billions. This means it takes less time to create the avatar, says Youssef Alami Mejjati, Synthesia‚Äôs head of research and development:



‚ÄúWith Express-1, it needed to first see someone expressing emotions to be able to render them. Now, because we‚Äôve trained it on much more diverse data and much larger data sets, with much more compute, it just learns these associations automatically without needing to see them.‚Äù&nbsp;







Narrowing the uncanny valley



Although humanlike AI-generated avatars have been around for years, the recent boom in generative AI is making it increasingly easier and more affordable to create lifelike synthetic humans‚Äîand they‚Äôre already being put to work. Synthesia isn‚Äôt alone: AI avatar companies like Yuzu Labs, Creatify, Arcdads, and Vidyard give businesses the tools to quickly generate and edit videos starring either AI actors or artificial versions of members of staff, promising cost-effective ways to make compelling ads that audiences connect with. Similarly, AI-generated clones of livestreamers have exploded in popularity across China in recent years, partly because they can sell products 24/7 without getting tired or needing to be paid.¬†



For now at least, Synthesia is ‚Äúlaser focused‚Äù on the corporate sphere. But it‚Äôs not ruling out expanding into new sectors such as entertainment or education, says Peter Hill, the company‚Äôs chief technical officer. In an apparent step toward this, Synthesia recently partnered with Google to integrate Google‚Äôs powerful new generative video model Veo 3 into its platform, allowing users to directly generate and embed clips into Synthesia‚Äôs videos. It suggests that in the future, these hyperrealistic artificial humans could take up starring roles in detailed universes with ever-changeable backdrops.&nbsp;





At present this could, for example, involve using Veo 3 to generate a video of meat-processing machinery, with a Synthesia avatar next to the machines talking about how to use them safely. But future versions of Synthesia‚Äôs technology could result in educational videos customizable to an individual‚Äôs level of knowledge, says Alex Voica, head of corporate affairs and policy at Synthesia. For example, a video about the evolution of life on Earth could be tweaked for someone with a biology degree or someone with high-school-level knowledge. ‚ÄúIt‚Äôs going to be such a much more engaging and personalized way of delivering content that I‚Äôm really excited about,‚Äù he says.&nbsp;



The next frontier, according to Synthesia, will be avatars that can talk back, ‚Äúunderstanding‚Äù conversations with users and responding in real time Think ChatGPT, but with a lifelike digital human attached.&nbsp;



Synthesia has already added an interactive element by letting users click through on-screen questions during quizzes presented by its avatars. But it‚Äôs also exploring making them truly interactive: Future users could ask their avatar to pause and expand on a point, or ask it a question. ‚ÄúWe really want to make the best learning experience, and that means through video that‚Äôs entertaining but also personalized and interactive,‚Äù says Alami Mejjati. ‚ÄúThis, for me, is the missing part in online learning experiences today. And I know we‚Äôre very close to solving that.‚Äù



We already know that humans can‚Äîand do‚Äîform deep emotional bonds with AI systems, even with basic text-based chatbots. Combining agentic technology‚Äîwhich is already capable of navigating the web, coding, and playing video games unsupervised‚Äîwith a realistic human face could usher in a whole new kind of AI addiction, says Pat Pataranutaporn, an assistant professor at the MIT Media Lab.&nbsp;&nbsp;



‚ÄúIf you make the system too realistic, people might start forming certain kinds of relationships with these characters,‚Äù he says. ‚ÄúWe‚Äôve seen many cases where AI companions have influenced dangerous behavior even when they are basically texting. If an avatar had a talking head, it would be even more addictive.‚Äù



Schuller agrees that avatars in the near future will be perfectly optimized to adjust their projected levels of emotion and charisma so that their human audiences will stay engaged for as long as possible. ‚ÄúIt will be very hard [for humans] to compete with charismatic AI of the future; it‚Äôs always present, always has an ear for you, and is always understanding,‚Äù he says. ‚ÄúAl will change that human-to-human connection.‚Äù



As I pause and replay my Express-2 avatar, I imagine holding conversations with it‚Äîthis uncanny, permanently upbeat, perpetually available product of pixels and algorithms that looks like me and sounds like me, but fundamentally isn‚Äôt me. Virtual Rhiannon has never laughed until she‚Äôs cried, or fallen in love, or run a marathon, or watched the sun set in another country.&nbsp;



But, I concede, she could deliver a damned good presentation about why Ed Sheeran is the greatest musician ever to come out of the UK. And only my closest friends and family would know that it‚Äôs not the real me.

üîí Cybersecurity & Privacy
‚Ä¢ The Ongoing Fallout from a Breach at AI Chatbot Maker Salesloft
  The recent mass-theft of authentication tokens from Salesloft, whose AI chatbot is used by a broad swath of corporate America to convert customer interaction into Salesforce leads, has left many companies racing to invalidate the stolen credentials before hackers can exploit them. Now Google warns the breach goes far beyond access to Salesforce data, noting the hackers responsible also stole valid authentication tokens for hundreds of online services that customers can integrate with Salesloft, including Slack, Google Workspace, Amazon S3, Microsoft Azure, and OpenAI.
Salesloft says its products are trusted by 5,000+ customers. Some of the bigger names are visible on the company&#8217;s homepage.
Salesloft disclosed on August 20 that, &#8220;Today, we detected a security issue in the Drift application,&#8221; referring to the technology that powers an AI chatbot used by so many corporate websites. The alert urged customers to re-authenticate the connection between the Drift and Salesforce apps to invalidate their existing authentication tokens, but it said nothing then to indicate those tokens had already been stolen.
On August 26, the Google Threat Intelligence Group (GTIG) warned that unidentified hackers tracked as UNC6395 used the access tokens stolen from Salesloft to siphon large amounts of data from numerous corporate Salesforce instances. Google said the data theft began as early as Aug. 8, 2025 and lasted through at least Aug. 18, 2025, and that the incident did not involve any vulnerability in the Salesforce platform.
Google said the attackers have been sifting through the massive data haul for credential materials such as AWS keys, VPN credentials, and credentials to the cloud storage provider Snowflake.
&#8220;If successful, the right credentials could allow them to further compromise victim and client environments, as well as pivot to the victim&#8217;s clients or partner environments,&#8221; the GTIG report stated.
The GTIG updated its advisory on August 28 to acknowledge the attackers used the stolen tokens to access email from &#8220;a very small number of Google Workspace accounts&#8221; that were specially configured to integrate with Salesloft. More importantly, it warned organizations to immediately invalidate all tokens stored in or connected to their Salesloft integrations &#8212; regardless of the third-party service in question.
&#8220;Given GTIG&#8217;s observations of data exfiltration associated with the campaign, organizations using Salesloft Drift to integrate with third-party platforms (including but not limited to Salesforce) should consider their data compromised and are urged to take immediate remediation steps,&#8221; Google advised.
On August 28, Salesforce blocked Drift from integrating with its platform, and with its productivity platforms Slack and Pardot.
The Salesloft incident comes on the heels of a broad social engineering campaign that used voice phishing to trick targets into connecting a malicious app to their organization&#8217;s Salesforce portal. That campaign led to data breaches and extortion attacks affecting a number of companies including Adidas, Allianz Life and Qantas.
On August 5, Google disclosed that one of its corporate Salesforce instances was compromised by the attackers, which the GTIG has dubbed UNC6040 (&#8220;UNC&#8221; stands for &#8220;uncategorized threat group&#8221;). Google said the extortionists consistently claimed to be the threat group ShinyHunters,¬†and that the group appeared to be preparing to escalate its extortion attacks by launching a data leak site.
ShinyHunters is an amorphous threat group known for using social engineering to break into cloud platforms and third-party IT providers, and for posting dozens of stolen databases to cybercrime communities like the now-defunct Breachforums.
The ShinyHunters brand dates back to 2020, and the group has been credited with or taken responsibility for dozens of data leaks that exposed hundreds of millions of breached records. The group&#8217;s member roster is thought to be somewhat fluid, drawing mainly from active denizens of the Com, a mostly English-language cybercrime community scattered across an ocean of Telegram and Discord servers.
Recorded Future&#8217;s Alan Liska told Bleeping Computer that the overlap in the &#8220;tools, techniques and procedures&#8221; used by ShinyHunters and the Scattered Spider extortion group likely indicate some crossover between the two groups.
To muddy the waters even further, on August 28 a Telegram channel that now has nearly 40,000 subscribers was launched under the intentionally confusing banner &#8220;Scattered LAPSUS$ Hunters 4.0,&#8221; wherein participants have repeatedly claimed responsibility for the Salesloft hack without actually sharing any details to prove their claims.
The Telegram group has been trying to attract media attention by threatening security researchers at Google and other firms. It also is using the channel&#8217;s sudden popularity to promote a new cybercrime forum called &#8220;Breachstars,&#8221; which they claim will soon host data stolen from victim companies who refuse to negotiate a ransom payment.
The &#8220;Scattered Lapsus$ Hunters 4.0&#8221; channel on Telegram now has roughly 40,000 subscribers.
But Austin Larsen, a principal threat analyst at Google&#8217;s threat intelligence group, said there is no compelling evidence to attribute the Salesloft activity to ShinyHunters or to other known groups at this time.
&#8220;Their understanding of the incident seems to come from public reporting alone,&#8221; Larsen told KrebsOnSecurity, referring to the most active participants in the Scattered LAPSUS$ Hunters 4.0 Telegram channel.
Joshua Wright, a senior technical director at Counter Hack,¬†is credited with coining the term &#8220;authorization sprawl&#8221; to describe one key reason that social engineering attacks from groups like Scattered Spider and ShinyHunters so often succeed: They abuse legitimate user access tokens to move seamlessly between on-premises and cloud systems.
Wright said this type of attack chain often goes undetected because the attacker sticks to the resources and access already allocated to the user.
&#8220;Instead of the conventional chain of initial access, privilege escalation and endpoint bypass, these threat actors are using centralized identity platforms that offer single sign-on (SSO) and integrated authentication and authorization schemes,&#8221; Wright wrote in a June 2025 column. &#8220;Rather than creating custom malware, attackers use the resources already available to them as authorized users.&#8221;
It remains unclear exactly how the attackers gained access to all Salesloft Drift authentication tokens. Salesloft announced on August 27 that it hired Mandiant, Google Cloud&#8217;s incident response division, to investigate the root cause(s).
&#8220;We are working with Salesloft Drift to investigate the root cause of what occurred and then it‚Äôll be up to them to publish that,&#8221; Mandiant Consulting CTO Charles Carmakal told Cyberscoop. &#8220;There will be a lot more tomorrow, and the next day, and the next day.&#8221;

üéì University AI
No updates.

üè¢ Corporate AI
‚Ä¢ Build character consistent storyboards using Amazon Nova in Amazon Bedrock ‚Äì Part 2
  Although careful prompt crafting can yield good results, achieving professional-grade visual consistency often requires adapting the underlying model itself. Building on the prompt engineering and character development approach covered in Part 1 of this two-part series, we now push the consistency level for specific characters by fine-tuning an Amazon Nova Canvas foundation model (FM). Through fine-tuning techniques, creators can instruct the model to maintain precise control over character appearances, expressions, and stylistic elements across multiple scenes. 
In this post, we take an animated short film, Picchu, produced by FuzzyPixel from Amazon Web Services (AWS), prepare training data by extracting key character frames, and fine-tune a character-consistent model for the main character Mayu and her mother, so we can quickly generate storyboard concepts for new sequels like the following images. 
 
  
   
    
    
    
   
  
 
Solution overview 
To implement an automated workflow, we propose the following comprehensive solution architecture that uses AWS services for an end-to-end implementation. 
 
The workflow consists of the following steps: 
 
 The user uploads a video asset to an Amazon Simple Storage Service (Amazon S3) bucket. 
 Amazon Elastic Container Service (Amazon ECS) is triggered to process the video asset. 
 Amazon ECS downsamples the frames, selects those containing the character, and then center-crops them to produce the final character images. 
 Amazon ECS invokes an Amazon Nova model (Amazon Nova Pro) from Amazon Bedrock to create captions from the images. 
 Amazon ECS writes the image captions and metadata to the S3 bucket. 
 The user uses a notebook environment in Amazon SageMaker AI to invoke the model training job. 
 The user fine-tunes a custom Amazon Nova Canvas model by invoking Amazon Bedrock create_model_customization_job and create_model_provisioned_throughput API calls to create a custom model available for inference. 
 
This workflow is structured in two distinct phases. The initial phase, in Steps 1‚Äì5, focuses on preparing the training data. In this post, we walk through an automated pipeline to extract images from an input video and then generate labeled training data. The second phase, in Steps 6‚Äì7, focuses on fine-tuning the Amazon Nova Canvas model and performing test inference using the custom-trained model. For these latter steps, we provide the preprocessed image data and comprehensive example code in the following GitHub repository to guide you through the process. 
Prepare the training data 
Let‚Äôs begin with the first phase of our workflow. In our example, we build an automated video object/character extraction pipeline to extract high-resolution images with accurate caption labels using the following steps. 
Creative character extraction 
We recommend first sampling video frames at fixed intervals (for example, 1 frame per second). Then, apply Amazon Rekognition label detection and face collection search to identify frames and characters of interest. Label detection can identify over 2,000 unique labels and locate their positions within frames, making it ideal for initial detection of general character categories or non-human characters. To distinguish between different characters, we then use the Amazon Rekognition feature to search faces in a collection. This feature identifies and tracks characters by matching their faces against a pre-populated face collection. If these two approaches aren‚Äôt precise enough, we can use Amazon Rekognition Custom Labels to train a custom model for detecting specific characters. The following diagram illustrates this workflow. 
 
After detection, we center-crop each character with appropriate pixel padding and then run a deduplication algorithm using the Amazon Titan Multimodal Embeddings model to remove semantically similar images above a threshold value. Doing so helps us build a diverse dataset because redundant or nearly identical frames could lead to model overfitting (when a model learns the training data too precisely, including its noise and fluctuations, making it perform poorly on new, unseen data). We can calibrate the similarity threshold to fine-tune what we consider to be identical images, so we can better control the balance between dataset diversity and redundancy elimination. 
Data labeling 
We generate captions for each image using Amazon Nova Pro in Amazon Bedrock and then upload the image and label manifest file to an Amazon S3 location. This process focuses on two critical aspects of prompt engineering: character description to help the FM identify and name the characters based on their unique attributes, and varied description generation that avoids repetitive patterns in the caption (for example, ‚Äúan animated character‚Äù). The following is an example prompt template used during our data labeling process: 
 
 system_prompt = """ 
    You are an expert image description specialist who creates concise, natural alt
    text that makes visual content accessible while maintaining clarity and focus.
    Your task is to analyze the provided image and provide a creative description
    (20-30 words) that emphasizes the Three main characters, capturing the essential
    elements of their interaction while avoiding unnecessary details.
"""

prompt = """
    
    1. Identify the main characters in the image: Character 1, Character 2, and
        Character 3 at least one will be in the picture so provide at a minimum a
        description with at least one character name.
      - "Character 1" describe the first character, key traits, background, attributes.
      - "Character 2" describe the second character, key traits, background, attributes.
      - "Character 3" describe the third character, key traits, background, attributes. 
    2. Just state their name WITHOUT adding any standard characteristics.
    3. Only capture visual element outside the standard characteristics
    4. Capture the core interaction between them
    5. Include only contextual details that are crucial for understanding the scene
    6. Create a natural, flowing description using everyday language
    
    Here are some examples
    
       ...
    
    
    
    [Identify the main characters]
    [Assessment of their primary interaction]
    [Selection of crucial contextual elements]
    [Crafting of concise, natural description]
    
    
    {
        "alt_text": "[Concise, natural description focusing on the main characters]"
    }
    
    
    Note: Provide only the JSON object as the final response. 
 
The data labeling output is formatted as a JSONL file, where each line pairs an image reference Amazon S3 path with a caption generated by Amazon Nova Pro. This JSONL file is then uploaded to Amazon S3 for training. The following is an example of the file: 
 
 {"image_ref": "s3://media-ip-dataset/characters/blue_character_01.jpg", "alt_text": "This
    animated character features a round face with large expressive eyes. The character
    has a distinctive blue color scheme with a small tuft of hair on top. The design is
    stylized with clean lines and a minimalist approach typical of modern animation."}
{"image_ref": "s3://media-ip-dataset/props/iconic_prop_series1.jpg", "alt_text": "This
    object appears to be an iconic prop from the franchise. It has a metallic appearance
    with distinctive engravings and a unique shape that fans would immediately recognize.
    The lighting highlights its dimensional qualities and fine details that make it
    instantly identifiable."} 
 
Human verification 
For enterprise use cases, we recommend incorporating a human-in-the-loop process to verify labeled data before proceeding with model training. This verification can be implemented using Amazon Augmented AI (Amazon A2I), a service that helps annotators verify both image and caption quality. For more details, refer to Get Started with Amazon Augmented AI. 
Fine-tune Amazon Nova Canvas 
Now that we have the training data, we can fine-tune the Amazon Nova Canvas model in Amazon Bedrock. Amazon Bedrock requires an AWS Identity and Access Management (IAM) service role to access the S3 bucket where you stored your model customization training data. For more details, see Model customization access and security. You can perform the fine-tuning task directly on the Amazon Bedrock console or use the Boto3 API. We explain both approaches in this post, and you can find the end-to-end code sample in picchu-finetuning.ipynb. 
Create a fine-tuning job on the Amazon Bedrock console 
Let‚Äôs start by creating an Amazon Nova Canvas fine-tuning job on the Amazon Bedrock console: 
 
 On the Amazon Bedrock console, in the navigation pane, choose Custom models under Foundation models. 
 Choose Customize model and then Create Fine-tuning job. 
 
 
 
 On the Create Fine-tuning job details page, choose the model you want to customize and enter a name for the fine-tuned model. 
 In the Job configuration section, enter a name for the job and optionally add tags to associate with it. 
 In the Input data section, enter the Amazon S3 location of the training dataset file. 
 In the Hyperparameters section, enter values for hyperparameters, as shown in the following screenshot. 
 
 
 
 In the Output data section, enter the Amazon S3 location where Amazon Bedrock should save the output of the job. 
 Choose Fine-tune model job to begin the fine-tuning process. 
 
This hyperparameter combination yielded good results during our experimentation. In general, increasing the learning rate makes the model train more aggressively, which often presents an interesting trade-off: we might achieve character consistency more quickly, but it might impact overall image quality. We recommend a systematic approach to adjusting hyperparameters. Start with the suggested batch size and learning rate, and try increasing or decreasing the number of training steps first. If the model struggles to learn your dataset even after 20,000 steps (the maximum allowed in Amazon Bedrock), then we suggest either increasing the batch size or adjusting the learning rate upward. These adjustments, through subtle, can make a significant difference in our model‚Äôs performance. For more details about the hyperparameters, refer to Hyperparameters for Creative Content Generation models. 
Create a fine-tuning job using the Python SDK 
The following Python code snippet creates the same fine-tuning job using the create_model_customization_job API: 
 
 bedrock = boto3.client('bedrock')
jobName =&nbsp;"picchu-canvas-v0"
# Set parameters
hyperParameters = {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"stepCount": "14000",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"batchSize": "64",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"learningRate": "0.000001",
&nbsp;&nbsp; &nbsp;}

# Create job
response_ft = bedrock.create_model_customization_job(
&nbsp;&nbsp; &nbsp;jobName=jobName,
&nbsp;&nbsp; &nbsp;customModelName=jobName,
&nbsp;&nbsp; &nbsp;roleArn=roleArn,
&nbsp;&nbsp; &nbsp;baseModelIdentifier="amazon.nova-canvas-v1:0",
&nbsp;&nbsp; &nbsp;hyperParameters=hyperParameters,
&nbsp;&nbsp; &nbsp;trainingDataConfig={"s3Uri": training_path},
&nbsp;&nbsp; &nbsp;outputDataConfig={"s3Uri": f"s3://{bucket}/{prefix}"}
)

jobArn = response_ft.get('jobArn')
print(jobArn) 
 
When the job is complete, you can retrieve the new customModelARN using the following code: 
 
 custom_model_arn = bedrock.list_model_customization_jobs(
&nbsp;&nbsp; &nbsp;nameContains=jobName
)["modelCustomizationJobSummaries"][0]["customModelArn"] 
 
Deploy the fine-tuned model 
With the preceding hyperparameter configuration, this fine-tuning job might take up to 12 hours to complete. When it‚Äôs complete, you should see a new model in the custom models list. You can then create provisioned throughput to host the model. For more details on provisioned throughput and different commitment plans, see Increase model invocation capacity with Provisioned Throughput in Amazon Bedrock. 
Deploy the model on the Amazon Bedrock console 
To deploy the model from the Amazon Bedrock console, complete the following steps: 
 
 On the Amazon Bedrock console, choose Custom models under Foundation models in the navigation pane. 
 Select the new custom model and choose Purchase provisioned throughput. 
 
 
 
 In the Provisioned Throughput details section, enter a name for the provisioned throughput. 
 Under Select model, choose the custom model you just created. 
 Then specify the commitment term and model units. 
 
 
After you purchase provisioned throughput, a new model Amazon Resource Name (ARN) is created. You can invoke this ARN when the provisioned throughput is in service. 
 
Deploy the model using the Python SDK 
The following Python code snippet creates provisioned throughput using the create_provisioned_model_throughput API: 
 
 custom_model_name =&nbsp;"picchu-canvas-v0"

# Create the provision throughput job and retrieve the provisioned model id
provisioned_model_id = bedrock.create_provisioned_model_throughput(
&nbsp;&nbsp; &nbsp;modelUnits=1,
&nbsp;&nbsp; &nbsp;# create a name for your provisioned throughput model
&nbsp;&nbsp; &nbsp;provisionedModelName=custom_model_name, 
&nbsp;&nbsp; &nbsp;modelId=custom_model_arn
)['provisionedModelArn'] 
 
Test the fine-tuned model 
When the provisioned throughput is live, we can use the following code snippet to test the custom model and experiment with generating some new images for a sequel to Picchu: 
 
 import json
import io
from PIL import Image
import base64

def decode_base64_image(img_b64):
    return Image.open(io.BytesIO(base64.b64decode(img_b64)))
    
def generate_image(prompt,
                   negative_prompt="text, ugly, blurry, distorted, low
                       quality, pixelated, watermark, text, deformed", 
                   num_of_images=3,
                   seed=1):
    """
    Generate an image using Amazon Nova Canvas.
    """

    image_gen_config = {
            "numberOfImages": num_of_images,
            "quality": "premium",
            "width": 1024,  # Maximum resolution 2048 x 2048
            "height": 1024,  # 1:1 ratio
            "cfgScale": 8.0,
            "seed": seed,
        }

    # Prepare the request body
    request_body = {
        "taskType": "TEXT_IMAGE",
        "textToImageParams": {
            "text": prompt,
            "negativeText": negative_prompt,  # List things to avoid
        },
        "imageGenerationConfig": image_gen_config
    } 

    response = bedrock_runtime.invoke_model(
        modelId=provisioned_model_id,
        body=json.dumps(request_body)
    )

    # Parse the response
    response_body = json.loads(response['body'].read())

    if "images" in response_body:
        # Extract the image
        return [decode_base64_image(img) for img in response_body['images']]
    else:
        return
seed = random.randint(1, 858993459)
print(f"seed: {seed}")

images = generate_image(prompt=prompt, seed=seed) 
 
 
  
   
    
    
    
   
   
   Mayu face shows a mix of nervousness and determination. Mommy kneels beside her, gently holder her. A landscape is visible in the background. 
   A steep cliff face with a long wooden ladder extending downwards. Halfway down the ladder is Mayu with a determined expression on her face. Mayu‚Äôs small hands grip the sides of the ladder tightly as she carefully places her feet on each rung. The surrounding environment shows a rugged, mountainous landscape. 
   Mayu standing proudly at the entrance of a simple school building. Her face beams with a wide smile, expressing pride and accomplishment. 
   
  
 
Clean up 
To avoid incurring AWS charges after you are done testing, complete the cleanup steps in picchu-finetuning.ipynb and delete the following resources: 
 
 Amazon SageMaker Studio domain 
 Fine-tuned Amazon Nova model and provision throughput endpoint 
 
Conclusion 
In this post, we demonstrated how to elevate character and style consistency in storyboarding from Part 1 by fine-tuning Amazon Nova Canvas in Amazon Bedrock. Our comprehensive workflow combines automated video processing, intelligent character extraction using Amazon Rekognition, and precise model customization using Amazon Bedrock to create a solution that maintains visual fidelity and dramatically accelerates the storyboarding process. By fine-tuning the Amazon Nova Canvas model on specific characters and styles, we‚Äôve achieved a level of consistency that surpasses standard prompt engineering, so creative teams can produce high-quality storyboards in hours rather than weeks. Start experimenting with Nova Canvas fine-tuning today, so you can also elevate your storytelling with better character and style consistency. 
 
About the authors 
Dr. Achin Jain is a Senior Applied Scientist at Amazon AGI, where he works on building multi-modal foundation models. He brings over 10+ years of combined industry and academic research experience. He has led the development of several modules for Amazon Nova Canvas and Amazon Titan Image Generator, including supervised fine-tuning (SFT), model customization, instant customization, and guidance with color palette. 
James Wu is a Senior AI/ML Specialist Solution Architect at AWS. helping customers design and build AI/ML solutions. James‚Äôs work covers a wide range of ML use cases, with a primary interest in computer vision, deep learning, and scaling ML across the enterprise. Prior to joining AWS, James was an architect, developer, and technology leader for over 10 years, including 6 years in engineering and 4 years in marketing &amp; advertising industries. 
Randy Ridgley is a Principal Solutions Architect focused on real-time analytics and AI. With expertise in designing data lakes and pipelines. Randy helps organizations transform diverse data streams into actionable insights. He specializes in IoT solutions, analytics, and infrastructure-as-code implementations. As an open-source contributor and technical leader, Randy provides deep technical knowledge to deliver scalable data solutions across enterprise environments.
‚Ä¢ Build character consistent storyboards using Amazon Nova in Amazon Bedrock ‚Äì Part 1
  The art of storyboarding stands as the cornerstone of modern content creation, weaving its essential role through filmmaking, animation, advertising, and UX design. Though traditionally, creators have relied on hand-drawn sequential illustrations to map their narratives, today‚Äôs AI foundation models (FMs) are transforming this landscape. FMs like Amazon Nova Canvas and Amazon Nova Reel offer capabilities in transforming text and image inputs into professional-grade visuals and short clips that promise to revolutionize preproduction workflows. 
This technological leap forward, however, presents its own set of challenges. Although these models excel at generating diverse concepts rapidly‚Äîa boon for creative exploration‚Äîmaintaining consistent character designs and stylistic coherence across scenes remains a significant hurdle. Even subtle modifications to prompts or model configurations can yield dramatically different visual outputs, potentially disrupting narrative continuity and creating additional work for content creators. 
To address these challenges, we‚Äôve developed this two-part series exploring practical solutions for achieving visual consistency. In Part 1, we deep dive into prompt engineering and character development pipelines, sharing tested prompt patterns that deliver reliable, consistent results with Amazon Nova Canvas and Amazon Nova Reel. Part 2 explores techniques like fine-tuning Amazon Nova Canvas to achieve exceptional visual consistency and precise character control. 
 
  
   
    
    
    
   
  
 
Consistent character design with Amazon Nova Canvas 
The foundation of effective storyboarding begins with establishing well-defined character designs. Amazon Nova Canvas offers several powerful techniques to create and maintain character consistency throughout your visual narrative. To help you implement these techniques in your own projects, we‚Äôve provided comprehensive code examples and resources in our GitHub repository. We encourage you to follow along as we walk through each step in detail. If you‚Äôre new to Amazon Nova Canvas, we recommend first reviewing Generating images with Amazon Nova to familiarize yourself with the basic concepts. 
Basic text prompting 
Amazon Nova Canvas transforms text descriptions into visual representations. Unlike large language models (LLMs), image generation models don‚Äôt interpret commands or engage in reasoning‚Äîthey respond best to descriptive captions. Including specific details in your prompts, such as physical attributes, clothing, and styling elements, directly influences the generated output. 
For example, ‚ÄúA 7-year-old Peruvian girl with dark hair in two low braids wearing a school uniform‚Äù provides clear visual elements for the model to generate an initial character concept, as shown in the following example image. 
 
Visual style implementation 
Consistency in storyboarding requires both character features and unified visual style. Our approach separates style information into two key components in the prompt: 
 
 Style description ‚Äì An opening phrase that defines the visual medium (for example, ‚ÄúA graphic novel style illustration of‚Äù) 
 Style details ‚Äì A closing phrase that specifies artistic elements (for example, ‚ÄúBold linework, dramatic shadows, flat color palettes‚Äù) 
 
This structured technique enables exploration of various artistic styles, including graphic novels, sketches, and 3D illustrations, while maintaining character consistency throughout the storyboard. The following is an example prompt template and some style information you can experiment with: 
 
 {style_description} A 7 year old peruvian girl with dark hair in two low braids wearing a
    school uniform. {style_details}
styles = [
    {
        "name": "graphic-novel",
        "description": "A graphic novel style illustation of",
        "details": "Bold linework, dramatic shadows, and flat color palettes. Use
            high contrast lighting and cinematic composition typical of comic book
            panels. Include expressive line work to convey emotion and movement.",
    },
    {
        "name": "sketch",
        "description": "A simple black and white line sketch of",
        "details": "Rough, sketch-like lines create a storyboard aesthetic. High
            contrast. No color",
    },
    {
        "name": "digital-illustration",
        "description": "A 3D digital drawing of",
        "details": "High contrast. Rounded character design. Smooth rendering.
            Soft texture. Luminous lighting",
    },
] 
 
 
Character variation through seed values 
The seed parameter serves as a tool for generating character variations while adhering to the same prompt. By keeping the text description constant and varying only the seed value, creators can explore multiple interpretations of their character design without starting from scratch, as illustrated in the following example images. 
 
  
   
    
   
   
    Seed = 1  
    Seed = 20  
    Seed = 57  
    Seed = 139  
    Seed = 12222  
   
  
 
Prompt adherence control with cfgScale 
The cfgScale parameter is another tool for maintaining character consistency, controlling how strictly Amazon Nova Canvas follows your prompt. Operating on a scale from 1.1‚Äì10, lower values give the model more creative freedom and higher values enforce strict prompt adherence. The default value of 6.5 typically provides an optimal balance, but as demonstrated in the following images, finding the right setting is crucial. Too low a value can result in inconsistent character representations, whereas too high a value might overemphasize prompt elements at the cost of natural composition. 
 
  
   
    
   
   
   Seed = 57, cfgScale = 1.1 
   Seed = 57, cfgScale = 3.5 
   Seed = 57, cfgScale = 6.5 
   Seed = 57, cfgScale = 8.0 
   Seed = 57, cfgScale = 10 
   
  
 
Scene integration with consistent parameters 
Now we can put these techniques together to test for character consistency across different narrative contexts, as shown in the following example images. We maintain consistent input for style, seed, and cfgScale, varying only the scene description to make sure character remains recognizable throughout the scene sequences. 
 
  
   
    
    
    
   
   
   Seed = 57, Cfg_scale: 6.5 
   Seed = 57, Cfg_scale: 6.5 
   Seed = 57, Cfg_scale: 6.5 
   
   
   A graphic novel style illustration of a 7 year old Peruvian girl with dark hair in two low braids wearing a school uniform riding a bike on a mountain pass Bold linework, dramatic shadows, and flat color palettes. Use high contrast lighting and cinematic composition typical of comic book panels. Include expressive line work to convey emotion and movement. 
   A graphic novel style illustation of a 7 year old Peruvian girl with dark hair in two low braids wearing a school uniform walking on a path through tall grass in the Andes Bold linework, dramatic shadows, and flat color palettes. Use high contrast lighting and cinematic composition typical of comic book panels. Include expressive line work to convey emotion and movement. 
   A graphic novel style illustration of a 7 year old Peruvian girl with dark hair in two low braids wearing a school uniform eating ice cream at the beach Bold linework, dramatic shadows, and flat color palettes. Use high contrast lighting and cinematic composition typical of comic book panels. Include expressive line work to convey emotion and movement. 
   
  
 
Storyboard development pipeline 
Building upon the character consistency techniques we‚Äôve discussed, we can now implement an end-to-end storyboard development pipeline that transforms written scene and character descriptions into visually coherent storyboards. This systematic approach uses our established parameters for style descriptions, seed values, and cfgScale values to provide character consistency while adapting to different narrative contexts. The following are some example scene and character descriptions: 
 
 "scenes":[
    {
        "description": "Mayu stands at the edge of a mountainous path, clutching
            a book. Her mother, Maya, kneels beside her, offering words of encouragement
            and handing her the book. Mayu looks nervous but determined as she prepares
            to start her journey."
    },
    {
        "description": "Mayu encounters a 'danger' sign with a drawing of a
            snake. She looks scared, but then remembers her mother's words. She takes a
            deep breath, looks at her book for reassurance, and then searches for a stick
            on the ground."
    },
    {
        "description": "Mayu bravely makes her way through tall grass, swinging
            her stick and making noise to scare off potential snakes. Her face shows a
            mix of fear and courage as she pushes forward on her journey."
    }
],
"characters":{
    "Mayu":  "A 7-year-old Peruvian girl with dark hair in two low braids wearing a
        school uniform",
    "Maya":  "An older Peruvian woman with long dark hair tied back in a bun, wearing
        traditional Peruvian clothing"
}
 
 
 
Our pipeline uses Amazon Nova Lite to first craft optimized image prompts incorporating our established best practices, which are then passed to Amazon Nova Canvas for image generation. By setting numberOfImages higher (typically three variations), while maintaining consistent seed and cfgScale values, we give creators multiple options that preserve character consistency. We used the following prompt for Amazon Nova Lite to generate optimized image prompts: 
 
 Describe an image that best represents the scene described. Here are some examples:
scene: Rosa is in the kitchen, rummaging through the pantry, looking for a snack. She
    hears a strange noise coming from the back of the pantry and becomes startled.
imagery: A dimly lit pantry with shelves stocked with various food items, and Rosa
    peering inside, her face expressing curiosity and a hint of fear.
scene: Rosa says goodbye to her mother, Maya. Maya offers her words of encouragement.
imagery: A wide shot of Rosa's determined face, facing Maya and receiving a small wrapped
    gift.
Only describe the imagery. Use no more than 60 words.
scene: {scene_description}
imagery:
 
 
Our pipeline generated the following storyboard panels. 
 
  
   
    
   Mayu stands at the edge of a mountainous path, clutching a book. Her mother, Maya, kneels beside her, offering words of encouragement and handing her the book. Mayu looks nervous but determined as she prepares to start her journey. 
   
   
    
   Mayu encounters a ‚Äòdanger‚Äô sign with a drawing of a snake. She looks scared, but then remembers her mother‚Äôs words. She takes a deep breath, looks at her book for reassurance, and then searches for a stick on the ground. 
   
   
    
   Mayu bravely makes her way through tall grass, swinging her stick and making noise to scare off potential snakes. Her face shows a mix of fear and courage as she pushes forward on her journey. 
   
  
 
Although these techniques noticeably improve character consistency, they aren‚Äôt perfect. Upon closer inspection, you will notice that even images within the same scene show variations in character consistency. Using consistent seed values helps control these variations, and the techniques outlined in this post significantly improve consistency compared to basic prompt engineering. However, if your use case requires near-perfect character consistency, we recommend proceeding to Part 2, where we explore advanced fine-tuning techniques. 
Video generation for animated storyboards 
If you want to go beyond static scene images to transform your storyboard into short, animated video clips, you can use Amazon Nova Reel. We use Amazon Nova Lite to convert image prompts into video prompts, adding subtle motion and camera movements optimized for the Amazon Nova Reel model. These prompts, along with the original images, serve as creative constraints for Amazon Nova Reel to generate the final animated sequences. The following is the example prompt and its resulting animated scene in GIF format: 
 
 A sunlit forest path with a 'Danger' sign featuring a snake. A 7-year-old Peruvian girl
    stands, visibly scared but resolute. Bold linework, dramatic shadows, and flat color
    palettes. High contrast lighting and cinematic composition. Mist slowly drifting.
    Camera dolly in. 
 
 
  
   
    
    
   
   
   Input Image 
   Output Video 
   
  
 
Conclusion 
In this first part of our series, we explored fundamental techniques for achieving character and style consistency using Amazon Nova Canvas, from structured prompt engineering to building an end-to-end storyboarding pipeline. We demonstrated how combining style descriptions, seed values, and careful cfgScale parameter control can significantly improve character consistency across different scenes. We also showed how integrating Amazon Nova Lite with Amazon Nova Reel can enhance the storyboarding workflow, enabling both optimized prompt generation and animated sequences. 
Although these techniques provide a solid foundation for consistent storyboard generation, they aren‚Äôt perfect‚Äîsubtle variations might still occur. We invite you to continue to Part 2, where we explore advanced model fine-tuning techniques that can help achieve near-perfect character consistency and visual fidelity. 
 
About the authors 
Alex Burkleaux is a Senior AI/ML Specialist Solution Architect at AWS. She helps customers use AI Services to build media solutions using Generative AI. Her industry experience includes over-the-top video, database management systems, and reliability engineering. 
James Wu is a Senior AI/ML Specialist Solution Architect at AWS, helping customers design and build AI/ML solutions. James‚Äôs work covers a wide range of ML use cases, with a primary interest in computer vision, deep learning, and scaling ML across the enterprise. Prior to joining AWS, James was an architect, developer, and technology leader for over 10 years, including 6 years in engineering and 4 years in marketing &amp; advertising industries. 
Vladimir Budilov is a Principal Solutions Architect at AWS focusing on agentic &amp; generative AI, and software architecture. He leads large-scale GenAI implementations, bridging cutting-edge AI capabilities with production-ready business solutions, while optimizing for cost and solution resilience. 
Nora Shannon Johnson is a Solutions Architect at Amazon Music focused on discovery and growth through AI/ML. In the past, she supported AWS through the development of generative AI prototypes and tools for developers in financial services, health care, retail, and more. She has been an engineer and consultant in various industries including DevOps, fintech, industrial AI/ML, and edtech in the United States, Europe, and Latin America. 
Ehsan Shokrgozar is a Senior Solutions Architect specializing in Media and Entertainment at AWS. He is passionate about helping M&amp;E customers build more efficient workflows. He combines his previous experience as Technical Director and Pipeline Engineer at various Animation/VFX studios with his knowledge of building M&amp;E workflows in the cloud to help customers achieve their business goals.
‚Ä¢ Authenticate Amazon Q Business data accessors using a trusted token issuer
  Since its general availability in 2024, Amazon Q Business (Amazon Q) has enabled independent software vendors (ISVs) to enhance their Software as a Service (SaaS) solutions through secure access to customers‚Äô enterprise data by becoming Amazon Q Business data accessor. To find out more on data accessor, see this page. The data accessor now supports trusted identity propagation. With trusted token issuer (TTI) authorization support, ISVs as data accessor can integrate with Amazon Q index while maintaining enterprise-grade security standards for their software-as-a-service (SaaS) solutions. 
Prior to TTI support, data accessors needed to implement authorization code flow with AWS IAM Identity Center integration when accessing the Amazon Q index. With TTI support for data accessors, ISVs can now use their own OpenID Provider to authenticate enterprise users, alleviating the need for double authentication while maintaining security standards. 
In this blog post, we show you how to implement TTI authorization for data accessors, compare authentication options, and provide step-by-step guidance for both ISVs and enterprises. 
Prerequisites 
Before you begin, make sure you have the following requirements: 
 
 An AWS account with administrator access 
 Access to Amazon Q Business 
 For ISVs: 
   
   An OpenID Connect (OIDC) compatible authorization server 
    
 For enterprises: 
   
   Amazon Q Business administrator access 
   Permission to create trusted token issuers 
    
 
Solution Overview 
This solution demonstrates how to implement TTI authentication for Amazon Q Business data accessors. The following diagram illustrates the overall flow between different resources, from ISV becoming a data accessor, customer enabling ISV data accessor, to ISV accessing customer‚Äôs Amazon Q index: 
 
Understanding Trusted Token Issuer Authentication 
Trusted Token Issuer represents an advanced identity integration capability for Amazon Q. At its core, TTI is a token exchange API that propagates identity information into IAM role sessions, enabling AWS services to make authorization decisions based on the actual end user‚Äôs identity and group memberships. This mechanism allows AWS services to apply authorization and security controls based on the authenticated user context. The TTI support simplifies the identity integration process while maintaining robust security standards, making it possible for organizations to ensure that access to Amazon Q respects user-level permissions and group memberships. This enables fine-grained access control and maintains proper security governance within Amazon Q implementations. 
Trusted Token Issuer authentication simplifies the identity integration process for Amazon Q by enabling the propagation of user identity information into AWS IAM role sessions. Each token exchange allows AWS services to make authorization decisions based on the authenticated user‚Äôs identity and group memberships. The TTI support streamlines the integration process while maintaining robust security standards, enabling organizations to implement appropriate access controls within their Amazon Q implementations. 
Understanding Data Accessors 
A data accessor is an ISV that has registered with AWS and is authorized to use their customers‚Äô Amazon Q index for the ISV‚Äôs Large Language Model (LLM) solution. The process begins with ISV registration, where they provide configuration information including display name, business logo, and OpenID Connect (OIDC) configuration details for TTI support. 
During ISV registration, providers must specify their tenantId configuration ‚Äì a unique identifier for their application tenant. This identifier might be known by different names in various applications (such as Workspace ID in Slack or Domain ID in Asana) and is required for proper customer isolation in multi-tenant environments. 
Amazon Q customers then add the ISV as a data accessor to their environment, granting access to their Amazon Q index based on specific permissions and data source selections. Once authorized, the ISV can query the customers‚Äô index through API requests using their TTI authentication flow, creating a secure and controlled pathway for accessing customer data. 
Implementing TTI Authentication for Amazon Q index Access 
This section explains how to implement TTI authentication for accessing the Amazon Q index. The implementation involves initial setup by the customer and subsequent authentication flow implemented by data accessors for user access. 
TTI provides capabilities that enable identity-enhanced IAM role sessions through Trusted Identity Propagation (TIP), allowing AWS services to make authorization decisions based on authenticated user identities and group memberships. Here‚Äôs how it works: 
To enable data accessor access to a customer‚Äôs Amazon Q index through TTI, customers must perform an initial one-time setup by adding a data accessor on Amazon Q Business application. During setup, a TTI with the data accessor‚Äôs identity provider information is created in the customer‚Äôs AWS IAM Identity Center, allowing the data accessor‚Äôs identity provider to authenticate access to the customer‚Äôs Amazon Q index. 
 
The process to set up an ISV data accessor with TTI authentication consists of the following steps: 
 
 The customer‚Äôs IT administrator accesses their Amazon Q Business application and creates a trusted token issuer with the ISV‚Äôs OAuth information. This returns a TrustedTokenIssuer (TTI) Amazon Resource Name (ARN).  
 The IT administrator creates an ISV data accessor with the TTI ARN received in Step 1.  
 Amazon Q Business confirms the provided TTI ARN with AWS IAM Identity Center and creates a data accessor application. 
 Upon successful creation of the ISV data accessor, the IT administrator receives data accessor details to share with the ISV.  
 The IT administrator provides these details to the ISV application. 
 
Once the data accessor setup is complete in the customer‚Äôs Amazon Q environment, users can access the Amazon Q index through the ISV application by authenticating only against the data accessor‚Äôs identity provider. 
 
The authentication flow proceeds as follows: 
 
 A user authenticates against the data accessor‚Äôs identity provider through the ISV application. The ISV application receives an ID token for that user, generated from the ISV‚Äôs identity provider with the same client ID registered on their data accessor. 
 The ISV application needs to use the AWS Identity and Access Management (IAM) role that they created during the data accessor onboarding process by calling AssumeRole API, then make CreateTokenWithIAM API request to the customer‚Äôs AWS IAM Identity Center with the ID token. AWS IAM Identity Center validates the ID token with the ISV‚Äôs identity provider and returns an IAM Identity Center token. 
 The ISV application requests an AssumeRole API with: IAM Identity Center token, extracted identity context, and tenantId. The tenantId is a security control jointly established between the ISV and their customer, with the customer maintaining control over how it‚Äôs used in their trust relationships. This combination facilitates secure access to the correct customer environment. 
 The ISV application calls the SearchRelevantContent API with the session credentials and receives relevant content from the customer‚Äôs Amazon Q index. 
 
Choosing between TTI and Authorization Code 
When implementing Amazon Q integration, ISVs need to consider two approaches, each with its own benefits and considerations: 
 
  
   
    
   Trusted Token Issuer 
   Authorization Code 
   
   
   Advantages 
   Single authentication on the ISV system 
   Enhanced security through mandatory user initiation for each session 
   
   
   Enables backend-only access to SearchRelevantContent API without user interaction 
    
   
   
   Considerations 
   Some enterprises may prefer authentication flows that require explicit user consent for each session, providing additional control over API access timing and duration 
   Requires double authentication on the ISV system 
   
   
   Requires ISVs to host and maintain OpenID Provider 
    
   
  
 
TTI excels in providing a seamless user experience through single authentication on the ISV system and enables backend-only implementations for SearchRelevantContent API access without requiring direct user interaction. However, this approach requires ISVs to maintain their own OIDC authorization server, which may present implementation challenges for some organizations. Additionally, some enterprises might have concerns about ISVs having persistent ability to make API requests on behalf of their users without explicit per-session authorization. 
Next Steps 
For ISVs: Becoming a Data Accessor with TTI Authentication 
Getting started on Amazon Q data accessor registration process with TTI authentication is straightforward. If you already have an OIDC compatible authorization server for your application‚Äôs authentication, you‚Äôre most of the way there. 
To begin the registration process, you‚Äôll need to provide the following information: 
 
 Display name and business logo that will be displayed on AWS Management Console 
 OIDC configuration details (OIDC ClientId and discovery endpoint URL) 
 TenantID configuration details that specify how your application identifies different customer environments 
 
For details, see Information to be provided to the Amazon Q Business team. 
For ISVs using Amazon Cognito as their OIDC authorization server, here‚Äôs how to retrieve the required OIDC configuration details: 
 
 To get the OIDC ClientId:- Navigate to the Amazon Cognito console- Select your User Pool- Go to ‚ÄúApplications‚Äù &gt; ‚ÄúApp clients‚Äù- The ClientId is listed under ‚ÄúClient ID‚Äù for your app client 
 To get the discovery endpoint URL:- The URL follows this format:https://cognito-idp.{region}.amazonaws.com/{userPoolId}/.well-known/openid-configuration‚Äì Replace {region} with your AWS region (e.g., us-east-1)- Replace {userPoolId} with your Cognito User Pool IDFor example, if your User Pool is in us-east-1 with ID ‚Äòus-east-1_abcd1234‚Äô, your discovery endpoint URL would be: https://cognito-idp.us-east-1.amazonaws.com/us-east-1_abcd1234/.well-known/openid-configuration 
 
 
Note: While this example uses Amazon Cognito, the process will vary depending on your OIDC provider. Common providers like Auth0, Okta, or custom implementations will have their own methods for accessing these configuration details. 
Once registered, you can enhance your generative AI application with the powerful capabilities of Amazon Q, allowing your customers to access their enterprise knowledge base through your familiar interface. AWS provides comprehensive documentation and support to help you implement the authentication flow and API integration efficiently. 
For Enterprises: Enabling TTI-authenticated Data Accessor 
To enable a TTI-authenticated data accessor, your IT administrator needs to complete the following steps in the Amazon Q console: 
 
 Create a trusted token issuer using the ISV‚Äôs OAuth information 
 Set up the data accessor with the generated TTI ARN 
 Configure appropriate data source access permissions 
 
This streamlined setup allows your users to access Amazon Q index through the ISV‚Äôs application using their existing ISV application credentials, alleviating the need for multiple logins while maintaining security controls over your enterprise data. 
Both ISVs and enterprises benefit from AWS‚Äôs comprehensive documentation and support throughout the implementation process, facilitating a smooth and secure integration experience. 
Clean up resources 
To avoid unused resources, follow these steps if you no longer need the data accessor: 
 
 Delete the data accessor: 
   
   On the Amazon Q Business console, choose Data accessors in the navigation pane 
   Select your data accessor and choose Delete. 
    
 Delete the TTI: 
   
   On the IAM Identity Center console, choose Trusted Token Issuers in the navigation pane. 
   Select the associated issuer and choose Delete. 
    
 
Conclusion 
The introduction of Trusted Token Issuer (TTI) authentication for Amazon Q data accessors marks a significant advancement in how ISVs integrate with Amazon Q Business. By enabling data accessors to use their existing OIDC infrastructure, we‚Äôve alleviated the need for double authentication while maintaining enterprise-grade security standards through TTI‚Äôs robust tenant isolation mechanisms and secure multi-tenant access controls, making sure each customer‚Äôs data remains protected within their dedicated environment. This streamlined approach not only enhances the end-user experience but also simplifies the integration process for ISVs building generative AI solutions. 
In this post, we showed how to implement TTI authentication for Amazon Q data accessors. We covered the setup process for both ISVs and enterprises and demonstrated how TTI authentication simplifies the user experience while maintaining security standards. 
To learn more about Amazon Q Business and data accessor integration, refer to Share your enterprise data with data accessors using Amazon Q index and Information to be provided to the Amazon Q Business team. You can also contact your AWS account team for personalized guidance. Visit the Amazon Q Business console to begin using these enhanced authentication capabilities today. 
 
About the Authors 
Takeshi Kobayashi is a Senior AI/ML Solutions Architect within the Amazon Q Business team, responsible for developing advanced AI/ML solutions for enterprise customers. With over 14 years of experience at Amazon in AWS, AI/ML, and technology, Takeshi is dedicated to leveraging generative AI and AWS services to build innovative solutions that address customer needs. Based in Seattle, WA, Takeshi is passionate about pushing the boundaries of artificial intelligence and machine learning technologies. 
Siddhant Gupta is a Software Development Manager on the Amazon Q team based in Seattle, WA. He is driving innovation and development in cutting-edge AI-powered solutions. 
Akhilesh Amara is a Software Development Engineer on the Amazon Q team based in Seattle, WA. He is contributing to the development and enhancement of intelligent and innovative AI tools.
‚Ä¢ Unlocking the future of professional services: How Proofpoint uses Amazon Q Business
  This post was written with Stephen Coverdale and Alessandra Filice of Proofpoint. 
At the forefront of cybersecurity innovation, Proofpoint has redefined its professional services by integrating Amazon Q Business, a fully managed, generative AI powered assistant that you can configure to answer questions, provide summaries, generate content, and complete tasks based on your enterprise data. This synergy has transformed how Proofpoint delivers value to its customers, optimizing service efficiency and driving useful insights. In this post, we explore how Amazon Q Business transformed Proofpoint‚Äôs professional services, detailing its deployment, functionality, and future roadmap. 
We started this journey in January 2024 and launched production use within the services team in October 2024. Since that time, the active users have achieved a 40% productivity increase in administrative tasks, with Amazon Q Apps now saving us over 18,300 hours annually. The impact has been significant given that consultants typically spend about 12 hours per week on non-call administrative tasks. 
The time savings are evident in several key areas: 
 
 Over 10,000 hours annually through apps that support customer data analysis and deliver insights and recommendations 
 3,000 hours per year saved in executive reporting generation, which will likely double when we deploy automated presentation creation with AI-powered hyper-personalization 
 1,000 hours annually on meeting summarizations 
 300 hours per year preparing renewal justifications‚Äîbut the real benefit here is how quickly we can now turn around customized content at a scale we couldn‚Äôt achieve before 
 
Beyond these time savings, we‚Äôve seen benefits in upskilling our teams with better access to knowledge, delivering additional value to clients, improving our renewal processes, and gaining deeper customer understanding through Amazon Q Business. This productivity increase means our consultants can focus more time on strategic initiatives and direct customer engagement, ultimately delivering higher value to our customers. 
A paradigm shift in cybersecurity service delivery 
Proofpoint‚Äôs commitment to evolving our customer interactions into delightful experiences led us to adopt Amazon Q Business across our services and consulting teams. This integration has enabled: 
 
 Enhanced productivity ‚Äì Consultants save significant time on repetitive tasks, reallocating focus to high-value client interactions 
 Deeper insights ‚Äì AI-driven analytics provide a granular understanding of customer environments 
 Scalable solutions ‚Äì Tailored applications (Amazon Q Apps) empower consultants to address customer needs effectively 
 
Transformative use cases through Amazon Q Apps 
Amazon Q Business has been instrumental in our deployment, and we‚Äôve developed over 30 custom Amazon Q Apps, each addressing specific challenges within our service portfolio. 
Some of the use cases are: 
1. Follow-up email automation 
 
 Challenge ‚Äì Consultants spent hours drafting follow-up emails post-meetings 
 Solution ‚Äì Amazon Q Apps generates curated follow-up emails outlining discussion points and action items 
 Impact ‚Äì Consistent customer tracking, reduced response time, and multilingual capabilities for global reach 
 
2. Health check analysis 
 
 Challenge ‚Äì Analyzing complex customer health assessments and understanding customer changes over time 
 Solution ‚Äì Amazon Q Apps compares files, providing an overview of key changes between two health checks, and a generated summary to help support customer business reviews (CBRs) and progress updates 
 Impact ‚Äì Improved communication and enhanced customer satisfaction 
 
3. Renewal justifications 
 
 Challenge ‚Äì Time-intensive preparation for renewal discussions 
 Solution ‚Äì Tailored renewal justification points crafted to demonstrate the value we‚Äôre delivering 
 Impact ‚Äì Scalable, targeted value articulation, fostering customer retention 
 
4. Drafting custom responses 
 
 Challenge ‚Äì Providing in-depth and specific responses for customer inquiries 
 Solution ‚Äì Amazon Q Apps creates a personalized email draft using our best practices and documentation 
 Impact ‚Äì Faster, more accurate communication 
 
The following diagram shows the Proofpoint use cases for Amazon Q Business. 
 
The following diagram shows the Proofpoint implementation. Proofpoint Chat UI is the front end that connects to Amazon Q Business, which connects to data sources in Amazon Simple Storage Service (Amazon S3), Amazon Redshift, Microsoft SharePoint, and Totango. 
 
Data strategy: Laying the foundation to a successful deployment 
Proofpoint‚Äôs successful integration of Amazon Q Business followed a comprehensive data strategy and a phased deployment approach. The journey involved crucial data preparation and documentation overhaul with key aspects noted below. 
Quality documentation: 
 
 Conducted thorough review of existing documentation 
 Organized and added metadata to our documentation for improved accessibility 
 Established vetting process for new documents 
 
Knowledge capture: 
 
 Developed processes to document tribal knowledge 
 Created strategies for ongoing knowledge enrichment 
 Established metadata tagging standards for improved searchability 
 
We‚Äôve primarily used Microsoft SharePoint document libraries to manage and support this process, and we‚Äôre now replicating this model as we onboard additional teams. Conducting sufficient testing that Amazon Q Business remains accurate is a key to maintaining the high efficacy we‚Äôve seen from the results. 
Going forward, we‚Äôre also expanding our data strategy to capture more information and insights into our customer journey. We want to make more data sources available to Amazon Q Business to expand this project scope so it covers more work tasks and more teams. 
Journey of our successful Amazon Q Business rollout 
Through our AWS Enterprise Support relationship, Proofpoint received full support on this project from the AWS account team, who helped us evaluate in detail the viability of the project and use expert technical resources. They engaged fully to help our teams with the use of service features and functionality and gain early usage of new feature previews. These helped us optimize and align our development timelines with the service roadmaps. 
We established a rigorous vetting process for new documents to maintain data quality and developed strategies to document institutional knowledge. This made sure our AI assistant was trained in the most accurate and up-to-date information. This process enlightened us to the full benefits of Amazon Q Business. 
The pilot and discovery phases were critical in shaping our AI strategy. We quickly identified the limitations of solely having the chat functionality and recognized the game-changing potential of Amazon Q Apps. To make sure we were addressing real needs, we conducted in-depth interviews with consultants to determine pain points so we could then invest in developing the Amazon Q Apps that would provide the most benefits and time savings. App development and refinement became a central focus of our efforts. We spent a significant amount of time prompt engineering our apps to provide consistent high-quality results that would provide practical value to our users and encourage them to adopt the apps as part of their processes. We also continued updating the weighting of our documents, using the metadata to enhance the output. This additional work upfront led to a successful deployment. 
Lessons learned 
Throughout our journey of integrating Amazon Q Business, we‚Äôve gleaned valuable lessons that have shaped our approach to AI implementation within our services and consulting areas. One of the most compelling insights is the importance of a robust data strategy. We‚Äôve learned that AI is only as smart as we make it, and the quality of data fed into the system directly impacts its performance. This realization led us to invest significant time in identifying avenues to make our AI smarter, with a focus on developing a clear data strategy across our services and consulting teams to make sure we realize the full benefits of AI. We also discovered that having AI thought leaders embedded within our services function is key to the success of AI implementation, to bring that necessary understanding of both the technology and our business processes. 
Another lesson was that time investment is required to get the most out of Amazon Q Business. The customization and ongoing management are key to delivering optimal results. We found that creating custom apps is the most effective route to adoption. Amazon Q Business features no-code simplicity for creating the apps by business-oriented teams instead of programmers. The prompt engineering required to provide high-quality and consistent results is a time-intensive process. This underscores the need for dedicated resources with expertise in AI, our business, and our processes. 
Experimentation on agentic features 
Amazon Q Business has taken a significant leap forward in enhancing workplace productivity with the introduction of an intelligent orchestration feature for Amazon Q Business. This feature transforms how users interact with their enterprise data and applications by automatically directing queries to appropriate data sources and plugins. Instead of manually switching between different work applications, users now seamlessly interact with popular business tools such as Jira, Salesforce, ServiceNow, Smartsheet, and PagerDuty through a single conversational interface. The feature uses Retrieval Augmented Generation (RAG) data for enterprise-specific knowledge and works with both built-in and custom plugins, making it a powerful addition to the workplace technology landscape. We‚Äôre experimenting on agentic integration with Totango and a few other custom plugins with Orchestrator and are seeing good results. 
Looking ahead 
Looking ahead, Proofpoint has outlined an ambitious roadmap for expanding our Amazon Q Business deployment across our customer-facing teams. The key priorities of this roadmap include: 
 
 Expansion of data sources ‚Äì Proofpoint will be working to incorporate more data sources, helping to unify our information across our teams and allowing for a more comprehensive view of our customers. This will include using the many Amazon Q Business data source connectors, such as Salesforce, Confluence, Amazon S3, and Smartsheet, and will expand the impact of our Amazon Q Apps. 
 Using Amazon Q Business actions ‚Äì Building on our successful Amazon Q deployment, Proofpoint is set to enhance its tool integration strategy to further streamline operations and reduce administrative burden. We plan to take advantage of Amazon Q Business actions using the plugin capabilities so we can post data into our different customer success tools. With this integration approach, we can take note of more detailed customer insights. For example, we can capture project progress from a meeting transcript and store it in our customer success tool to identify sentiment concerns. We‚Äôll be able to gather richer data about our customer engagements, which translates to providing even greater and more personalized service to our customers. 
 Automated workflows ‚Äì Future enhancements will include expanded automation and integrations to further streamline our service delivery. By combining our enhanced data sources with automated actions, we can make sure our teams receive the right information and insights at the right time while reducing manual intervention. 
 Data strategy enhancement ‚Äì We‚Äôll continue to refine our data strategy across Proofpoint Premium Services, establishing best practices for documentation and implementing systems to record undocumented knowledge. This will include developing better ways to understand and document our customer journey through the integration of various tools and data sources. 
 
Security and compliance 
As a cybersecurity leader, Proofpoint makes sure that AI processes comply with strict security and privacy standards: 
 
 Secure integration ‚Äì Amazon Q Apps seamlessly connects to our various data sources, safeguarding sensitive data 
 Continuous monitoring ‚Äì Embedded feedback mechanisms and daily synchronization uphold quality control 
 
Conclusion: Redefining cybersecurity services 
Amazon Q Business exemplifies Proofpoint‚Äôs innovative approach to cybersecurity. With Amazon Q Business AI capabilities, we‚Äôre elevating our customer experience and scaling our service delivery. 
As we refine and expand this program, our focus remains unwavering: delivering unmatched value and protection to our clients. Through Amazon Q Business, Proofpoint continues to set the benchmark in cybersecurity services, making sure organizations can navigate an increasingly complex threat landscape with confidence. 
Learn more 
 
 More Amazon Q Business Blogs 
 Amazon Q main product page 
 Amazon Q details for IT pros and developers 
 Get started with Amazon Q 
 
 
 
About the Authors 
Stephen Coverdale is a Senior Manager, Professional Services at Proofpoint. In addition to managing a Professional Services team, he leads an AI integration team developing and driving a strategy to leverage the transformative capabilities of AI within Proofpoint‚Äôs services teams to enhance Proofpoint‚Äôs client engagements. 
Alessandra Filice is a Senior AI Integration Specialist at Proofpoint, where she plays a lead role in implementing AI solutions across Proofpoint‚Äôs services teams. In this role, she specializes in developing and deploying AI capabilities to enhance service delivery and operational efficiency. Working closely with stakeholders across Proofpoint, she identifies opportunities for AI implementation, designs innovative solutions, and facilitates successful integration of AI technologies. 
Ram Krishnan is a Senior Technical Account Manager at AWS. He serves as a key technical resource for independent software vendor (ISV) customers, providing help and guidance across their AWS needs including AI/ML focus ‚Äî from adoption and migration to design, deployment, and optimizations across AWS services. 
Abhishek Maligehalli Shivalingaiah is a Senior Generative AI Solutions Architect at AWS, specializing in Amazon Q Business. With a deep passion for using agentic AI frameworks to solve complex business challenges, he brings nearly a decade of expertise in developing data and AI solutions that deliver tangible value for enterprises. Beyond his professional endeavors, Abhishek is an artist who finds joy in creating portraits of family and friends, expressing his creativity through various artistic mediums.
‚Ä¢ Enhancing LLM accuracy with Coveo Passage Retrieval on Amazon Bedrock
  This post is co-written with Keith Beaudoin and Nicolas Bordeleau from Coveo. 
As generative AI transforms business operations, enterprises face a critical challenge: how can they help large language models (LLMs) provide accurate and trustworthy responses? Without reliable data foundations, these AI models can generate misleading or inaccurate responses, potentially reducing user trust and organizational credibility. 
As an AWS Partner, Coveo addresses this challenge with its Passage Retrieval API. This solution enhances the reliability of LLM-powered applications by providing them with relevant, context-aware enterprise knowledge to inform generated responses. In Retrieval Augmented Generation (RAG) systems, the retrieval process is the most complex component. It requires extracting the most relevant, precise information from enterprise data sources. By integrating the Coveo AI-Relevance Platform with Amazon Bedrock Agents, organizations gain access to an industry-leading enterprise search service featuring a secured, unified hybrid index that respects enterprise permission models and offers robust connectivity. The Coveo AI-Relevance Platform uses machine learning (ML) and in-depth usage analytics to continuously optimize relevance. This enables Amazon Bedrock Agents to deliver grounded, contextually relevant responses tailored to complex enterprise content. 
The Coveo AI-Relevance Platform is an industry-leading service that connects and unifies the content of cloud and on-premises repositories in a single index, making it fast and simple to find relevant content quickly. Its ML algorithms analyze user behavior, in-app context, as well as profile and permissions data to retrieve personalized search results and recommendations. It then aggregates and reports insights back to content and experience managers. By integrating seamlessly with enterprise systems (such as websites, knowledge bases, and CRM) and enforcing security permissions, Coveo helps users get the most pertinent information while maintaining data protection. 
In this post, we show how to deploy Coveo‚Äôs Passage Retrieval API as an Amazon Bedrock Agents action group to enhance response accuracy, so Coveo users can use their current index to rapidly deploy new generative experiences across their organization. 
Coveo‚Äôs Passage Retrieval API 
The Coveo Passage Retrieval API enhances LLM applications by passing ranked text passages (or chunks) retrieved from the unified index, along with appropriate metadata such as source URLs for citations, so that generated answers are grounded in an organization‚Äôs proprietary knowledge. Built on Coveo‚Äôs unified hybrid index, the Passage Retrieval API applies a two-stage retrieval process to extract the most relevant passages from structured and unstructured content sources, providing accuracy, security, and real-time relevance. The following diagram illustrates these stages. 
 
The process consists of the following key components: 
 
 Relevant passage extraction using a two-stage retrieval process ‚Äì In the first retrieval stage, Coveo‚Äôs hybrid search system is used to identify the most relevant documents. Then, it extracts the most relevant text passages from these documents, along with ranking scores, citation links, and other key metadata. This two-stage approach allows Coveo to identify accessible and relevant documents from the sources and then more precisely identify the most relevant passages. 
 Enhanced search results with hybrid ranking ‚Äì Combining semantic (vector) search, and lexical (keyword) matching helps Coveo retrieve the right information in the right context. 
 ML relevancy ‚Äì AI continuously learns from user interactions, tailoring retrieval to each user‚Äôs journey, behavior, and profile for context-aware responses. 
 Content connected across sources with a unified index ‚Äì A centralized hybrid index connects structured and unstructured content across sources. This unified hybrid index provides better multi-source relevancy than a federated search approach by applying the ranking function across sources. Coveo also provides a robust library of pre-built connectors to maintain seamless integration with third-party services (such as Salesforce, SharePoint, and Google Drive), facilitating data freshness with automatic updates for real-time retrieval. 
 Analytics and insights for performance tracking ‚Äì With events tracking through the Data Platform and Knowledge Hub, you can see exactly how your generated answers perform, where information is missing or underused, and which content needs tuning. With those insights, you can boost answer quality and drive measurable business impact. 
 Enterprise-grade security ‚Äì Coveo provides the native permission model of each connected content source by importing item‚Äëlevel permissions at crawl time through an early‚Äëbinding approach. Resolving access rights before indexing helps prevent data leakage and boosts search performance by filtering out items a user can‚Äôt see before a query is run. 
 Redefining enterprise-ready RAG ‚Äì Coveo reimagines what RAG systems can achieve by going beyond basic vector search, offering a dynamic and intelligent retrieval pipeline designed for enterprise needs. At its core is a unified hybrid index that seamlessly connects structured, unstructured, and permission-sensitive data. This foundation, combined with real-time ML-driven tuning, helps validate that every response delivered to an LLM is relevant and securely grounded in the right context. 
 
Through native access control enforcement, behavior-based relevance adjustment, and deep analytics into content usage and performance, Coveo empowers organizations to continuously refine their generative AI experiences. Backed by consistent recognition from leading analyst firms such as Gartner, Forrester, and IDC, Coveo delivers a reliable, secure, and scalable foundation for enterprise-grade generative AI. 
Solution overview 
This solution demonstrates how you can integrate Coveo‚Äôs Passage Retrieval API with Amazon Bedrock Agents to build LLM-powered assistants that deliver accurate, context-aware, and grounded responses. It applies broadly across use cases where agents need to access proprietary knowledge, whether to support customers, assist employees, or empower sales and service teams. This approach helps AI responses stay grounded in your most relevant and trusted information across structured and unstructured content. Amazon Bedrock Agents acts as the intelligent backbone of this solution, interpreting natural language queries and orchestrating the retrieval process to deliver grounded, contextually relevant insights. For this use case, the agent is equipped to answer a user‚Äôs questions on Coveo services capabilities, API documentation, and integration guides. The agent is designed to fetch precise passages from enterprise content in response to user questions, enabling applications such as virtual assistants, support copilots, or internal knowledge bots. By using structured definitions and instructions, the agent understands when and how to trigger Coveo‚Äôs Passage Retrieval API, making sure that LLM-generated responses are backed by accurate and trusted content. 
The action group defines the structured API operations that the Amazon Bedrock agent can invoke. Using OpenAPI specifications, it defines the interface between the agent and AWS Lambda functions. In this use case, fetching relevant passages based on the user‚Äôs search intent is the only available operation. 
The following diagram illustrates the solution architecture. 
 
For this demo, the agent is set with the following instructions during creation: 
 
 You will act as an expert on Coveo documentation, platform, APIs, analytics, and integration guides.
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;Use the CoveoPRAPI Action Group to retrieve relevant information on Coveo documentation, platform, APIs, analytics, and integration guides.
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;Summarize the information retrieved from the Action Group response clearly and accurately.
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;OuputFormatting guidelines:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;- Provide clear, direct answers without introductory phrases such as ‚ÄúAs an expert,‚Äù ‚ÄúSure,‚Äù or ‚ÄúHere is...‚Äù
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;- When appropriate, organize content using:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;- Numbered or bulleted lists
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;- Structured sections (e.g., features, steps, key points)
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;Keep answers concise, informative, and free of conversational filler. 
 
The Lambda function defined in the action group is essential for enabling the Amazon Bedrock agent to call the Coveo Passage Retrieval API. The Lambda function performs the following tasks: 
 
 Receives incoming requests from the Amazon Bedrock agent 
 Queries the Coveo Passage Retrieval API using the user‚Äôs input 
 Returns the relevant search results back to the Amazon Bedrock agent 
 
Lastly, the Coveo AI-Relevance Platform provides indexed and structured search results through the Passage Retrieval API. 
Prerequisites 
Before you begin, you must have the following prerequisites: 
 
 An AWS account with AWS Identity and Access Management (IAM) permissions to deploy an AWS CloudFormation stack 
 Access to models in Amazon Bedrock 
 A Coveo index created and ready to use 
 The following Coveo information: 
   
   Coveo organization ID 
   Coveo search hub 
   Coveo API key 
    
 
Deploy the solution with AWS CloudFormation 
To deploy your agent, complete the following steps: 
 
 Launch the CloudFormation stack:  
 Enter a stack name and values for AgentModelID, AgentName, CoveoApiKey, CoveoOrgID, and CoveoSearchHub. 
 In the Capabilities section, select the acknowledge check box. 
 Choose Create stack. 
 Wait for the stack creation to complete. 
 Verify all resources are created on the stack details page. 
 
Test the solution 
To test the solution, complete the following steps: 
 
 On the Amazon Bedrock console, choose Agents in the navigation pane. 
 Choose the agent created by the CloudFormation stack. 
 
 
 
 Under Test, enter your question in the message box. 
 
For this demo, Coveo technical documentation (from the website) was ingested in an existing Coveo Search index. Let‚Äôs start with the query ‚ÄúWhat is the difference between Coveo Atomic and Headless?‚Äù 
Here‚Äôs how the agent answered the question when Claude 3 Haiku v1 is used as the LLM. 
 
 
 Choose Show trace in the right pane and expand Trace step 1 to see the agent‚Äôs rationale. 
 
The following screenshot demonstrates how Amazon Bedrock Agents processed and answered the question. First, it formed a rationale: 
 
 To understand the difference between Coveo Atomic and Headless, I will need to retrieve relevant information from the Coveo technical documentation using the CoveoPRAPI Action Group. 
 
Then it invokes the CoveoPRAP action group, which is specifically designed to retrieve relevant passages, through a Lambda function to make an API call to /rest/search/v3/passages/retrieve. 
This example illustrates the agent‚Äôs systematic approach to planning and executing necessary actions through the CoveoPRAP1 action group, and retrieving relevant document chunks before formulating its final response. 
 
The Lambda function code includes a debugging feature that logs each retrieved passage from the Passage Retrieval API. This logging mechanism iterates through the returned chunks, numbering them sequentially for quick reference. These logs are available in Amazon CloudWatch, so you can see exactly which passages were retrieved for each user query, and how they contributed to the final response. To visualize the logs, open the CloudWatch console, and on the Log groups page, locate the Lambda function name. 
The following screenshot shows the agent detailed logs in CloudWatch. In the logs, the Coveo Passage Retrieval API returns the five most relevant chunks to the LLM. 
 
Clean up 
To avoid ongoing charges, delete the resources deployed as part of the CloudFormation stack: 
 
 On the AWS CloudFormation console, choose Stacks in the navigation pane. 
 Choose the stack you created, then choose Delete. 
 Choose Delete stack when prompted. 
 
For more information, refer to Deleting a stack on the AWS CloudFormation console. 
Conclusion 
By integrating Amazon Bedrock capabilities with Coveo‚Äôs AI-driven retrieval, organizations can develop AI applications that provide validated responses based on their enterprise content. This approach helps reduce inaccuracies while delivering real-time, secure responses. 
We encourage you to explore pre-built examples in the GitHub repository to help you get started with Amazon Bedrock. 
To learn more about the Coveo AI-Relevance Platform and how to implement the Passage Retrieval API in your Coveo organization, refer to Passage Retrieval (CPR) implementation overview. 
 
About the authors 
Yanick Houngbedji is a Solutions Architect for Independent Software Vendors (ISV) at Amazon Web Services (AWS), based in Montr√©al, Canada. He specializes in helping customers architect and implement highly scalable, performant, and secure cloud solutions on AWS. Before joining AWS, he spent over 8 years providing technical leadership in data engineering, big data analytics, business intelligence, and data science solutions. 
Keith Beaudoin is a Senior Solution Architect at Coveo Labs. He is specialized in designing and implementing intelligent search and AI-powered relevance solutions, with expertise in Agentic solutions, cloud technologies, search architecture, and third-party integrations. Keith helps organizations harness the full potential of Coveo‚Äôs platform, optimizing digital transformation strategies for seamless and impactful search implementations that drive business value 
Nicolas Bordeleau is Head of Product Relations at Coveo. With over 19 years of experience in the search industry, including 11 years in product management, Nicolas has a keen understanding of enterprises and developers‚Äô needs related to search and information retrieval. He has applied this knowledge to develop award-winning products that fulfill those needs in innovative ways.

‚∏ª