‚úÖ Morning News Briefing ‚Äì November 10, 2025 10:47

üìÖ Date: 2025-11-10 10:47
üè∑Ô∏è Tags: #briefing #ai #publichealth #digitalgov

‚∏ª

üßæ Weather
‚Ä¢ No watches or warnings in effect, Pembroke
  No watches or warnings in effect. No warnings or watches or watches in effect . Watch or warnings are no longer in effect in the U.S. No watches, warnings are in effect for the rest of the day . No watches and warnings are still in effect, but no watches are in place for the day's events . The weather is not expected to be affected by the weather .
‚Ä¢ Current Conditions:  -5.7¬∞C
  Temperature: -5.7&deg;C Pressure / Tendency: 101.4 kPa falling Humidity: 88 % Wind Chill: -11 . Dewpoint: -7.4&deg:C Wind: NW 14 km/h Air Quality Health Index: n/a . Pembroke 5:00 AM EST Monday 10 November 2025 Temperature: 5.7
‚Ä¢ Monday: Chance of flurries. High minus 2. POP 40%
  Cloudy with 40 percent chance of flurries this morning . Amount of 2 cm. Amount 2 cm . Wind chill minus 14 this morning and minus 9 this afternoon . UV index 1 or low. High minus 2.50¬∞F tomorrow morning, with wind chill at minus 9¬∞¬∞F and low UV index at minus 2¬∞F . Forecast issued 5:00 AM EST

üåç International News
No updates.

üçÅ Canadian News
No updates.

üá∫üá∏ U.S. Top Stories
‚Ä¢ Countries are gathering for climate negotiations. Here's where the U.S. stands
  Under President Trump, the U.S. has taken steps to roll back climate policies . Here are six significant changes to the policies under the new administration . President Trump has been criticized for his climate policies in the past, including climate policies, climate policy . The changes have been made in the wake of the White House's failure to act as a result of climate change in the United States
‚Ä¢ Alaska's public schools can serve as emergency shelters. The buildings are in crisis
  Alaska's public schools are being used as emergency shelters, though many of the buildings are crumbling . Many of the schools are crumbling, but many are still being used to shelter people in the state of Alaska . Alaska's schools have been used as shelters, but some are crumbling and many are now in need of sheltering in some areas of the state's poorest areas, including Alaska's largest
‚Ä¢ People want to avoid ultra-processed foods. But experts struggle to define them
  The evidence that ultra-processed foods are bad for us is piling up . But efforts to reduce their role in our diets face a big hurdle: experts can't agree on what they are and which to target . The evidence is mounting up that ultraprocessed food is bad for our health, but experts are divided on what is needed to be done to reduce its role in diets .
‚Ä¢ Typhoon Fung-wong leaves 4 dead and 1.4 million displaced in the Philippines
  Typhoon Fung-wong blew out of the Philippines after setting off floods and landslides, knocking out power to entire provinces, killing at least four people . More than 1.4 million people have been displaced by the typhoon, displacing more than a million more . Typhoon set off floods, landslides and set off landslides that set off power out of entire provinces and killed
‚Ä¢ Senators take first step toward reopening the government after historic shutdown
  The Senate voted late Sunday evening on a compromise that could reopen the government following the longest shutdown in history . The Senate votes late Sunday night to approve the compromise . It is the longest-ever government shutdown in the country's history, the longest in the history of the shutdown . The shutdown is now the longest ever in history, with the longest since it began in 1983 . The government is

üß† Artificial Intelligence
No updates.

üíª Digital Strategy
‚Ä¢ Allianz UK joins growing list of Clop‚Äôs Oracle E-Business Suite victims
  Insurance giant‚Äôs UK arm says cybercriminals misattributed the real victim . Allianz UK confirms it was one of the many companies that fell victim to the Clop gang's Oracle E-Business Suite (EBS) attack after crims reported that they had attacked a subsidiary . The insurance giant's UK arm said it was a subsidiary of a subsidiary in the attack
‚Ä¢ Three most important factors in enterprise IT: control, control, control
  We're all out of it. How to get it back is an open secret . Affordability, flexibility, efficiency, efficiency are still selling IT to this day . We‚Äôre all out. How we‚Äôve all out? How to do it? We need to find a way to get back control of our computers. We need it back. We're out of
‚Ä¢ UK military looking for tactical comms, systems suppliers in deal worth up to ¬£9.6B
  The UK government is launching a competition for military grade communications hardware and software in a tender worth up to ¬£9.6 billion ($12.5 billion) including tax . Major battle field technology refresh will be open to the rest of the public sector . The tender is expected to cost up to $12 billion including tax and is likely to be worth $10 billion . The competition will be
‚Ä¢ Cisco creating new security model using 30 years of data describing cyber-dramas and saves
  Cisco is working on a new AI model that will more than double the number of parameters used to train its current flagship Foundation-Sec-8B . Doubles parameters to over 17 billion, to detect threats and recommend actions . The new model could be used to detect and recommend threats and to recommend actions by the end of the year . It will be the first AI model to be developed
‚Ä¢ Techie ran up $40,000 bill trying to download a driver
  In the dialup age, small mistakes could cost big money . Who, Me? It's The Register‚Äôs Monday reader-contributed column in which you admit to the error of your ways.‚Ä¶‚Ä¶‚Ä¶ And what do you think? Share your thoughts with us on Twitter @mailonline.co.uk . Do you know something? Share it with iReport.com

üè• Public Health
No updates.

üî¨ Science
‚Ä¢ Maximizing researcher‚Äìpolicymaker engagement in global public health
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ From resolution to reality: advancing point-of-care diagnostics for kidney disease in low-resource settings
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Prevalence, bacterial etiology, and antimicrobial susceptibility patterns of urinary tract infections among pregnant women in rural West Amhara, Ethiopia
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ In-hospital mortality prognostication for cancer patients with febrile neutropenia
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ The contribution of mycetoma grains to suboptimal disease management
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations

üßæ Government & Policy
No updates.

üèõÔ∏è Enterprise Architecture & IT Governance
No updates.

ü§ñ AI & Emerging Tech
‚Ä¢ The State of AI: Energy is king, and the US is falling behind
  Welcome to&nbsp;The State of AI, a new collaboration between the Financial Times and MIT Technology Review. Every Monday for the next six weeks, writers from both publications will debate one aspect of the generative AI revolution reshaping global power.



This week, Casey Crownhart, senior reporter for energy at MIT Technology Review and Pilita Clark, FT&#8217;s columnist, consider how China&#8217;s rapid renewables buildout could help it leapfrog on AI progress.



Casey Crownhart writes:



In the age of AI, the biggest barrier to progress isn‚Äôt money but energy. That should be particularly worrying here in the US, where massive data centers are waiting to come online, and it doesn‚Äôt look as if the country will build the steady power supply or infrastructure needed to serve them all.



It wasn‚Äôt always like this. For about a decade before 2020, data centers were able to offset increased demand with efficiency improvements. Now, though, electricity demand is ticking up in the US, with billions of queries to popular AI models each day‚Äîand efficiency gains aren‚Äôt keeping pace. With too little new power capacity coming online, the strain is starting to show: Electricity bills are ballooning for people who live in places where data centers place a growing load on the grid.



If we want AI to have the chance to deliver on big promises without driving electricity prices sky-high for the rest of us, the US needs to learn some lessons from the rest of the world on energy abundance. Just look at China.



China installed 429 GW of new power generation capacity in 2024, more than six times the net capacity added in the US during that time.



China still generates much of its electricity with coal, but that makes up a declining share of the mix. Rather, the country is focused on installing solar, wind, nuclear, and gas at record rates.



The US, meanwhile, is focused on reviving its ailing coal industry. Coal-fired power plants are polluting and, crucially, expensive to run. Aging plants in the US are also less reliable than they used to be, generating electricity just 42% of the time, compared with a 61% capacity factor in 2014.



It‚Äôs not a great situation. And unless the US changes something, we risk becoming consumers as opposed to innovators in both energy and AI tech. Already, China earns more from exporting renewables than the US does from oil and gas exports.&nbsp;



Building and permitting new renewable power plants would certainly help, since they‚Äôre currently the cheapest and fastest to bring online. But wind and solar are politically unpopular with the current administration. Natural gas is an obvious candidate, though there are concerns about delays with key equipment.



One quick fix would be for data centers to be more flexible. If they agreed not to suck electricity from the grid during times of stress, new AI infrastructure might be able to come online without any new energy infrastructure.



One study from Duke University found that if data centers agree to curtail their consumption just 0.25% of the time (roughly 22 hours over the course of the year), the grid could provide power for about 76 GW of new demand. That‚Äôs like adding about 5% of the entire grid‚Äôs capacity without needing to build anything new.



But flexibility wouldn‚Äôt be enough to truly meet the swell in AI electricity demand. What do you think, Pilita? What would get the US out of these energy constraints? Is there anything else we should be thinking about when it comes to AI and its energy use?&nbsp;



Pilita Clark responds:



I agree. Data centers that can cut their power use at times of grid stress should be the norm, not the exception. Likewise, we need more deals like those giving cheaper electricity to data centers that let power utilities access their backup generators. Both reduce the need to build more power plants, which makes sense regardless of how much electricity AI ends up using.



This is a critical point for countries across the world, because we still don‚Äôt know exactly how much power AI is going to consume.&nbsp;



Forecasts for what data centers will need in as little as five years‚Äô time vary wildly, from less than twice today‚Äôs rates to four times as much.



This is partly because there‚Äôs a lack of public data about AI systems‚Äô energy needs. It‚Äôs also because we don‚Äôt know how much more efficient these systems will become. The US chip designer Nvidia said last year that its specialized chips had become 45,000 times more energy efficient over the previous eight years.&nbsp;



Moreover, we have been very wrong about tech energy needs before. At the height of the dot-com boom in 1999, it was erroneously claimed that the internet would need half the US‚Äôs electricity within a decade‚Äînecessitating a lot more coal power.



Still, some countries are clearly feeling the pressure already. In Ireland, data centers chew up so much power that new connections have been restricted around Dublin to avoid straining the grid.



Some regulators are eyeing new rules forcing tech companies to provide enough power generation to match their demand. I hope such efforts grow. I also hope AI itself helps boost power abundance and, crucially, accelerates the global energy transition needed to combat climate change. OpenAI‚Äôs Sam Altman said in 2023 that ‚Äúonce we have a really powerful super intelligence, addressing climate change will not be particularly difficult.‚Äù&nbsp;



The evidence so far is not promising, especially in the US, where renewable projects are being axed. Still, the US may end up being an outlier in a world where ever cheaper renewables made up more than 90% of new power capacity added globally last year.&nbsp;



Europe is aiming to power one of its biggest data centers predominantly with renewables and batteries. But the country leading the green energy expansion is clearly China.



The 20th century was dominated by countries rich in the fossil fuels whose reign the US now wants to prolong. China, in contrast, may become the world‚Äôs first green electrostate. If it does this in a way that helps it win an AI race the US has so far controlled, it will mark a striking chapter in economic, technological, and geopolitical history.



Casey Crownhart replies:



I share your skepticism of tech executives‚Äô claims that AI will be a groundbreaking help in the race to address climate change. To be fair, AI is progressing rapidly. But we don‚Äôt have time to wait for technologies standing on big claims with nothing to back them up.&nbsp;



When it comes to the grid, for example, experts say there‚Äôs potential for AI to help with planning and even operating, but these efforts are still experimental.&nbsp;&nbsp;



Meanwhile, much of the world is making measurable progress on transitioning to newer, greener forms of energy. How that will affect the AI boom remains to be seen. What is clear is that AI is changing our grid and our world, and we need to be clear-eyed about the consequences.&nbsp;



Further reading&nbsp;



MIT Technology Review reporters did the math on the energy needs of an AI query.



There are still a few reasons to be optimistic about AI‚Äôs energy demands.&nbsp;&nbsp;



The FT‚Äôs visual data team take a look inside the relentless race for AI capacity.



And global FT reporters ask whether data centers can ever truly be green.
‚Ä¢ The Download: a new home under the sea, and cloning pets
  This is today&#8217;s edition of¬†The Download,¬†our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



The first new subsea habitat in 40 years is about to launch



Vanguard feels and smells like a new RV. It has long, gray banquettes that convert into bunks, a microwave cleverly hidden under a counter, a functional steel sink with a French press and crockery above. A weird little toilet hides behind a curtain.



But you can‚Äôt just fire up Vanguard‚Äôs engine and roll off the lot. Once it is sealed and moved to its permanent home beneath the waves of the Florida Keys National Marine Sanctuary early next year, Vanguard will be the world‚Äôs first new subsea habitat in nearly four decades.Teams of four scientists will live and work on the seabed for a week at a time, entering and leaving the habitat as scuba divers. Read our story about some of their potential missions.



‚ÄîMark Harris







Cloning isn‚Äôt just for celebrity pets like Tom Brady‚Äôs dog



This week, we heard that Tom Brady had his dog cloned. The former quarterback revealed that his Junie is actually a clone of Lua, a pit bull mix that died in 2023.



Brady‚Äôs announcement follows those of celebrities like Paris Hilton and Barbra Streisand, who also famously cloned their pet dogs. But some believe there are better ways to make use of cloning technologies, such as diversifying the genetic pools of inbred species, or potentially bringing other animals back from the brink of extinction. Read the full story.



‚ÄîJessica Hamzelou



This article first appeared in The Checkup, MIT Technology Review‚Äôs weekly biotech newsletter. To receive it in your inbox every Thursday, and read articles like this first, sign up here.







The must-reads



I‚Äôve combed the internet to find you today‚Äôs most fun/important/scary/fascinating stories about technology.



1 OpenAI is facing a wave of new lawsuits¬†The cases concern wrongful death complaints, and claims ChatGPT caused mental breakdowns. (NYT $)+ One family claims ChatGPT ‚Äúgoaded‚Äù their son into taking his own life. (CNN)+ The looming crackdown on AI companionship. (MIT Technology Review)



2 Tesla shareholders approved Elon Musk‚Äôs $1 trillion pay packageMore than 75% of voters backed it. (WSJ $)+ Musk had hinted he‚Äôd leave Tesla if the deal wasn‚Äôt greenlit. (Axios)+ Tesla has to hit its ambitious targets before he can get his hands on the cash. (Wired $)3 The EU is poised to water down the AI actAfter pressure from Big Tech and the US government. (FT $)+ While the legislation was passed last year, many provisions haven‚Äôt kicked in yet. (Reuters)



4 Meta is earning a colossal amount of money from scam adsThey accounted for 10% of its revenue last year. (Reuters)+ Meta claims it ‚Äúaggressively‚Äù addresses scam ads on its platform. (CNBC)



5 The Chan Zuckerberg Initiative is pivoting to AIIt‚Äôs shifting its philanthropic focus from social justice programs to curing disease. (WP $)+ To achieve its goals, the charity will need extra computing power. (NYT $)



6 Unesco has adopted global standards on neurotechnologyExperts were increasingly concerned that a lack of guardrails could give rise to unethical practices. (The Guardian)+ Meet the other companies developing brain-computer interfaces. (MIT Technology Review)



7 Benchmarks hugely oversell AI performanceA new study questions their reliability and the validity of their results. (NBC News)+ How to build a better AI benchmark. (MIT Technology Review)



8 Kim Kardashian blames ChatGPT for failing her law examsIt‚Äôs almost like she shouldn‚Äôt have been consulting it for legal expertise in the first place. (Hollywood Reporter)+ AI and social media is worsening brain rot. (NYT $)+ How AI is introducing errors into courtrooms. (MIT Technology Review)



9 Hyundai is using robot dogs to inspect its EV production lineAnd they may soon be joined by a bipedal master. (IEEE Spectrum)



10 Grand Theft Auto VI has been delayed yet againThe highly anticipated video game has big, big shoes to fill. (Bloomberg $)+ It‚Äôll land a full 13 years after its previous incarnation‚Äîor will it? (BBC)







Quote of the day



‚ÄúThis is what oligarchy looks like.‚Äù



‚ÄîSenator Bernie Sanders reacts to Tesla shareholders‚Äô decision to award Elon Musk a $1 trillion pay package in a post on X.







One more thing







Finding forgotten Indigenous landscapes with electromagnetic technology



The fertile river valleys of the American Midwest hide tens of thousands of Indigenous earthworks, according to experts: geometric structures consisting of walls, mounds, ditches, and berms, some dating back nearly 3,000 years.Archaeologists now believe that the earthworks functioned as religious gathering places, tombs for culturally important clans, and annual calendars, perhaps all at the same time. They can take the form of giant circles and squares, cloverleafs and octagons, complex S-curves and simple mounds.Until recently, it seemed as if much of the continent‚Äôs pre-European archaeological heritage had been carelessly wiped out, uprooted, and lost for good. But traces remain: electromagnetic remnants in the soil that can be detected using specialty surveying equipment. And archaeologists and tribal historians are working together to uncover them. Read the full story.



‚ÄîGeoff Manaugh







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)



+ If you‚Äôre a wildlife fan, take a look at this compilation of the best places to catch a glimpse of unusual animals.+ El Salvador‚Äôs annual fireball festival is a completely unhinged celebration of all things volcanic.+ The most influential Bostonians of 2025 have been announced.+ Get me in a potato bed, stat.
‚Ä¢ The first new subsea habitat in 40 years is about to launch
  Vanguard feels and smells like a new RV. It has long, gray banquettes that convert into bunks, a microwave cleverly hidden under a counter, a functional steel sink with a French press and crockery above. A weird little toilet hides behind a curtain.



But some clues hint that you can‚Äôt just fire up Vanguard‚Äôs engine and roll off the lot. The least subtle is its door, a massive disc of steel complete with a wheel that spins to lock.



COURTESY MARK HARRIS




Once it is sealed and moved to its permanent home beneath the waves of the Florida Keys National Marine Sanctuary early next year, Vanguard will be the world‚Äôs first new subsea habitat in nearly four decades. Teams of four scientists will live and work on the seabed for a week at a time, entering and leaving the habitat as scuba divers. Their missions could include reef restoration, species surveys, underwater archaeology, or even astronaut training.&nbsp;



One of Vanguard‚Äôs modules, unappetizingly named the ‚Äúwet porch,‚Äù has a permanent opening in the floor (a.k.a. a ‚Äúmoon pool‚Äù) that doesn‚Äôt flood because Vanguard‚Äôs air pressure is matched to the water around it.&nbsp;



It is this pressurization that makes the habitat so useful. Scuba divers working at its maximum operational depth of 50 meters would typically need to make a lengthy stop on their way back to the surface to avoid decompression sickness. This painful and potentially fatal condition, better known as the bends, develops if divers surface too quickly. A traditional 50-meter dive gives scuba divers only a handful of minutes on the seafloor, and they can make only a couple of such dives a day. With Vanguard‚Äôs atmosphere at the same pressure as the water, its aquanauts need to decompress only once, at the end of their stay. They can potentially dive for many hours every day.



That could unlock all kinds of new science and exploration. ‚ÄúMore time in the ocean opens a world of possibility, accelerating discoveries, inspiration, solutions,‚Äù said Kristen Tertoole, Deep‚Äôs chief operating officer, at Vanguard‚Äôs unveiling in Miami in October. ‚ÄúThe ocean is Earth‚Äôs life support system. It regulates our climate, sustains life, and holds mysteries we‚Äôve only begun to explore, but it remains 95% undiscovered.‚Äù



COURTESY DEEP




Subsea habitats are not a new invention. Jacques Cousteau (naturally) built the first in 1962, although it was only about the size of an elevator. Larger habitats followed in the 1970s and ‚Äô80s, maxing out at around the size of Vanguard.



But the technology has come a long way since then. Vanguard uses a tethered connection to a buoy above, known as the ‚Äúsurface expression,‚Äù that pipes fresh air and water down to the habitat. It also hosts a diesel generator to power a Starlink internet connection and a tank to hold wastewater. Norman Smith, Deep‚Äôs chief technology officer, says the company modeled the most severe hurricanes that Florida expects over the next 20 years and designed the tether to withstand them. Even if the worst happens and the link is broken, Deep says, Vanguard has enough air, water, and energy storage to support its crew for at least 72 hours.



That number came from DNV, an independent classification agency that inspects and certifies all types of marine vessels so that they can get commercial insurance. Vanguard will be the first subsea habitat to get a DNV classification. ‚ÄúThat means you have to deal with the rules and all the challenging, frustrating things that come along with it, but it means that on a foundational level, it‚Äôs going to be safe,‚Äù says Patrick Lahey, founder of Triton Submarines, a manufacturer of classed submersibles.



JASON KOERNER/GETTY IMAGES FOR DEEP




Although Deep hopes Vanguard itself will enable decades of useful science, its prime function for the company is to prove out technologies for its planned successor, an advanced modular habitat called Sentinel. Sentinel modules will be six meters wide, twice the diameter of Vanguard, complete with sweeping staircases and single-occupant cabins. A small deployment might have a crew of eight, about the same as the International Space Station. A big Sentinel system could house 50, up to 225 meters deep. Deep claims that Sentinel will be launched at some point in 2027.



Ultimately, according to its mission statement, Deep seeks to ‚Äúmake humans aquatic,‚Äù an indication that permanent communities are on its long-term road map.&nbsp;



Deep has not publicly disclosed the identity of its principal funder, but business records in the UK indicate that as of January 31, 2025 a Canadian man, Robert MacGregor, owned at least 75% of its holding company. According to a Reuters investigation, MacGregor was once linked with Craig Steven Wright, a computer scientist who claimed to be Satoshi Nakamoto, as bitcoin‚Äôs elusive creator is pseudonymously known. However, Wright‚Äôs claims to be Nakamoto later collapsed.&nbsp;



MacGregor has kept a very low public profile in recent years. When contacted for comment, Deep spokesperson Mike Bohan refused to comment on the link with Wright, only to say it was inaccurate, but said: ‚ÄúRobert MacGregor started his career as an IP lawyer in the dot-com era, moving into blockchain technology and has diverse interests including philanthropy, real estate, and now Deep.‚Äù



In any case, MacGregor could find keeping that low profile more difficult if Vanguard is successful in reinvigorating ocean science and exploration as the company hopes. The habitat is due to be deployed early next year, following final operational tests at Triton‚Äôs facility in Florida. It will welcome its first scientists shortly after.&nbsp;



‚ÄúThe ocean is not just our resource; it is our responsibility,‚Äù says Tertoole. ‚ÄúDeep is more than a single habitat. We are building a full-stack capability for human presence in the ocean.‚Äù



JASON KOERNER/GETTY IMAGES FOR DEEP
‚Ä¢ Cloning isn‚Äôt just for celebrity pets like Tom Brady‚Äôs dog
  This week, we heard that Tom Brady had his dog cloned. The former quarterback revealed that his Junie is actually a clone of Lua, a pit bull mix that died in 2023.



Brady‚Äôs announcement follows those of celebrities like Paris Hilton and Barbra Streisand, who also famously cloned their pet dogs. But some believe there are better ways to make use of cloning technologies.





While the pampered pooches of the rich and famous may dominate this week‚Äôs headlines, cloning technologies are also being used to diversify the genetic pools of inbred species and potentially bring other animals back from the brink of extinction.



Cloning itself isn‚Äôt new. The first mammal cloned from an adult cell, Dolly the sheep, was born in the 1990s. The technology has been used in livestock breeding over the decades since.



Say you‚Äôve got a particularly large bull, or a cow that has an especially high milk yield. Those animals are valuable. You could selectively breed for those kinds of characteristics. Or you could clone the original animals‚Äîessentially creating genetic twins.



Scientists can take some of the animals‚Äô cells, freeze them, and store them in a biobank. That opens¬†the option to clone them in the future. It‚Äôs possible to thaw those cells, remove the DNA-containing nuclei of the cells, and insert them into donor egg cells.



Those donor egg cells, which come from another animal of the same species, have their own nuclei removed. So it‚Äôs a case of swapping out the DNA. The resulting cell is stimulated and grown in the lab until it starts to look like an embryo. Then it is transferred to the uterus of a surrogate animal‚Äîwhich eventually gives birth to a clone.



There are a handful of companies offering to clone pets. Viagen, which claims to have ‚Äúcloned more animals than anyone else on Earth,‚Äù will clone a dog or cat for $50,000. That‚Äôs the company that cloned Streisand‚Äôs pet dog Samantha, twice.



This week, Colossal Biosciences‚Äîthe ‚Äúde-extinction‚Äù company that claims to have resurrected the dire wolf and¬†created a ‚Äúwoolly mouse‚Äù as a precursor to reviving the woolly mammoth‚Äîannounced that it had acquired Viagen, but that Viagen¬†will ‚Äúcontinue to operate under its current leadership.‚Äù



Pet cloning is controversial, for a few reasons. The companies themselves point out that, while the cloned animal will be a genetic twin of the original animal, it won‚Äôt be identical. One issue is mitochondrial DNA‚Äîa tiny fraction of DNA that sits outside the nucleus and is inherited from the mother. The cloned animal may inherit some of this from the surrogate.



Mitochondrial DNA is unlikely to have much of an impact on the animal itself. More important are the many, many factors thought to shape an individual‚Äôs personality and temperament. ‚ÄúIt‚Äôs the old nature-versus-nurture question,‚Äù says Samantha Wisely, a conservation geneticist at the University of Florida. After all, human identical twins are never carbon copies of each other. Anyone who clones a pet expecting a like-for-like reincarnation is likely to be disappointed.





And some animal welfare groups are opposed to the practice of pet cloning. People for the Ethical Treatment of Animals (PETA) described it as ‚Äúa horror show,‚Äù and the UK‚Äôs Royal Society for the Prevention of Cruelty to Animals (RSPCA) says that ‚Äúthere is¬†no justification for cloning animals for such trivial purposes.‚Äù¬†



But there are other uses for cloning technology that are arguably less trivial. Wisely has long been interested in diversifying the gene pool of the critically endangered black-footed ferret, for example.



Today, there are around 10,000 black-footed ferrets that have been captively bred from only seven individuals, says Wisely. That level of inbreeding isn‚Äôt good for any species‚Äîit tends to leave organisms at risk of poor health. They are less able to reproduce or adapt to changes in their environment.



Wisely and her colleagues had access to frozen tissue samples taken from two other ferrets. Along with colleagues at not-for-profit Revive and Restore, the team created clones of those two individuals. The first clone, Elizabeth Ann,¬†was born in 2020. Since then, other clones have been born, and the team has started breeding the cloned animals with the descendants of the other seven ferrets, says Wisely.



The same approach has been used to¬†clone the endangered Przewalski‚Äôs horse, using decades-old tissue samples stored by the San Diego Zoo. It‚Äôs too soon to predict the impact of these efforts. Researchers are still evaluating the cloned ferrets and their offspring to see if they behave like typical animals and could survive in the wild.



Even this practice is not without its critics. Some have pointed out that cloning alone will not save any species. After all, it doesn‚Äôt address the habitat loss or human-wildlife conflict that is responsible for the endangerment of these animals in the first place. And there will always be detractors who accuse people who clone animals of ‚Äúplaying God.‚Äù¬†



For all her involvement in cloning endangered ferrets, Wisely tells me she would not consider cloning her own pets. She currently has three rescue dogs, a rescue cat, and ‚Äúgeriatric chickens.‚Äù ‚ÄúI love them all dearly,‚Äù she says. ‚ÄúBut there are a lot of rescue animals out there that need homes.‚Äù



This article first appeared in The Checkup,¬†MIT Technology Review‚Äôs¬†weekly biotech newsletter. To receive it in your inbox every Thursday, and read articles like this first,¬†sign up here.
‚Ä¢ The Download: how doctors fight conspiracy theories, and your AI footprint
  This is today&#8217;s edition of¬†The Download,¬†our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology. 



How conspiracy theories infiltrated the doctor‚Äôs office



As anyone who has googled their symptoms and convinced themselves that they‚Äôve got a brain tumor will attest, the internet makes it very easy to self-(mis)diagnose your health problems. And although social media and other digital forums can be a lifeline for some people looking for a diagnosis or community, when that information is wrong, it can put their well-being and even lives in danger.We spoke to a number of health-care professionals who told us how this modern impulse to ‚Äúdo your own research‚Äù is changing their profession. Read the full story.‚ÄîRhiannon Williams



This story is part of MIT Technology Review‚Äôs series ‚ÄúThe New Conspiracy Age,‚Äù on how the present boom in conspiracy theories is reshaping science and technology.







Stop worrying about your AI footprint. Look at the big picture instead.



‚ÄîCasey Crownhart



As a climate technology reporter, I‚Äôm often asked by people whether they should be using AI, given how awful it is for the environment. Generally, I tell them not to worry‚Äîlet a chatbot plan your vacation, suggest recipe ideas, or write you a poem if you want.That response might surprise some. I promise I‚Äôm not living under a rock, and I have seen all the concerning projections about how much electricity AI is using. But I feel strongly about not putting the onus on individuals. Here‚Äôs why.



This article is from The Spark, MIT Technology Review‚Äôs weekly climate newsletter. To receive it in your inbox every Wednesday, sign up here.







A new ion-based quantum computer makes error correction simpler



A company called Quantinuum has just unveiled Helios, its third-generation quantum computer, which includes expanded computing power and error correction capability.Like all other existing quantum computers, Helios is not powerful enough to execute the industry‚Äôs dream money-making algorithms, such as those that would be useful for materials discovery or financial modeling.But Quantinuum‚Äôs machines, which use individual ions as qubits, could be easier to scale up than quantum computers that use superconducting circuits as qubits, such as Google‚Äôs and IBM‚Äôs. Read the full story.



‚ÄîSophia Chen







The must-reads



I‚Äôve combed the internet to find you today‚Äôs most fun/important/scary/fascinating stories about technology.



1 A new California law could change how all Americans browse online¬†It gives web users the chance to opt out of having their personal information sold or shared. (The Markup)



2 The FDA has fast-tracked a pill to treat pancreatic cancerThe experimental drug appears promising, but experts worry corners may be cut. (WP $)+ Demand for AstraZeneca‚Äôs cancer and diabetes drugs is pushing profits up. (Bloomberg $)+ A new cancer treatment kills cells using localized heat. (Wired $)3 AI pioneers claim it is already superior to humans in many tasksBut not all tasks are created equal. (FT $)+ Are we all wandering into an AGI trap? (Vox)+ How AGI became the most consequential conspiracy theory of our time. (MIT Technology Review)



4 IBM is planning on cutting thousands of jobsIt‚Äôs shifting its focus to software and AI consulting, apparently. (Bloomberg $)+ It‚Äôs keen to grow the number of its customers seeking AI advice. (NYT $)



5 Big Tech‚Äôs data centers aren‚Äôt the job-generators we were promisedThe jobs they do create are largely in security and cleaning. (Rest of World)+ We did the math on AI‚Äôs energy footprint. Here‚Äôs the story you haven‚Äôt heard. (MIT Technology Review)



6 Microsoft let AI shopping agents loose in a fake marketplace¬†They were easily manipulated into buying goods, it found. (TechCrunch)+ When AIs bargain, a less advanced agent could cost you. (MIT Technology Review)



7 Sony has compiled a dataset to test the fairness of computer vision modelsAnd it‚Äôs confident it‚Äôs been compiled in a fair and ethical way. (The Register)+ These new tools could make AI vision systems less biased. (MIT Technology Review)



8 The social network is no moreWe‚Äôre living in an age of anti-social media. (The Atlantic $)+ Scam ads are rife across platforms, but these former Meta workers have a plan. (Wired $)+ The ultimate online flex? Having no followers. (New Yorker $)



9 Vibe coding is Collins dictionary‚Äôs word of 2025 Beating stiff competition from ‚Äúclanker.‚Äù (The Guardian)+ What is vibe coding, exactly? (MIT Technology Review)10 These people found romance with their chatbot companionsThe AI may not be real, but the humans‚Äô feelings certainly are. (NYT $)+ It‚Äôs surprisingly easy to stumble into a relationship with an AI chatbot. (MIT Technology Review)







Quote of the day



‚ÄúThe opportunistic side of me is realizing that your average accountant won‚Äôt be doing this.‚Äù



‚ÄîSal Abdulla, founder of accounting-software startup NixSheets, tells the Wall Street Journal he‚Äôs using AI tools to gain an edge on his competitors.







One more thing







Ethically sourced ‚Äúspare‚Äù human bodies could revolutionize medicineMany challenges in medicine stem, in large part, from a common root cause: a severe shortage of ethically-sourced human bodies.There might be a way to get out of this moral and scientific deadlock. Recent advances in biotechnology now provide a pathway to producing living human bodies without the neural components that allow us to think, be aware, or feel pain.Many will find this possibility disturbing, but if researchers and policymakers can find a way to pull these technologies together, we may one day be able to create ‚Äúspare‚Äù bodies, both human and nonhuman. Read the full story.‚ÄîCarsten T. Charlesworth, Henry T. Greely &amp; Hiromitsu Nakauchi







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)



+ Make sure to look up so you don‚Äôt miss November‚Äôs supermoon.+ If you keep finding yourself mindlessly scrolling (and who doesn‚Äôt?), maybe this whopping six-pound phone case could solve your addiction.+ Life lessons from a 101-year old who has no plans to retire.+ Are you a fan of movement snacking?

üîí Cybersecurity & Privacy
‚Ä¢ Drilling Down on Uncle Sam‚Äôs Proposed TP-Link Ban
  The U.S. government is reportedly preparing to ban the sale of wireless routers and other networking gear from TP-Link Systems, a tech company that currently enjoys an estimated 50% market share among home users and small businesses. Experts say while the proposed ban may have more to do with TP-Link&#8217;s ties to China than any specific technical threats, much of the rest of the industry serving this market also sources hardware from China and ships products that are insecure fresh out of the box.
A TP-Link WiFi 6 AX1800 Smart WiFi Router (Archer AX20).
The Washington Post recently reported that more than a half-dozen federal departments and agencies were backing a proposed ban on future sales of TP-Link devices in the United States. The story said U.S. Department of Commerce officials concluded TP-Link Systems products pose a risk because the U.S.-based company‚Äôs products handle sensitive American data and because the officials believe it remains subject to jurisdiction or influence by the Chinese government.
TP-Link Systems denies that, saying that it fully split from the Chinese TP-Link Technologies over the past three years, and that its critics have vastly overstated the company&#8217;s market share (TP-Link puts it at around 30 percent). TP-Link says it has headquarters in California, with a branch in Singapore, and that it manufactures in Vietnam. The company says it researches, designs, develops and manufactures everything except its chipsets in-house.
TP-Link Systems told The Post it has sole ownership of some engineering, design and manufacturing capabilities in China that were once part of China-based TP-Link Technologies, and that it operates them without Chinese government supervision.
&#8220;TP-Link vigorously disputes any allegation that its products present national security risks to the United States,&#8221; Ricca Silverio, a spokeswoman for TP-Link Systems, said in a statement. &#8220;TP-Link is a U.S. company committed to supplying high-quality and secure products to the U.S. market and beyond.&#8221;
Cost is a big reason TP-Link devices are so prevalent in the consumer and small business market: As this February 2025 story from Wired observed regarding the proposed ban, TP-Link has long had a reputation for flooding the market with devices that are considerably cheaper than comparable models from other vendors. That price point (and consistently excellent performance ratings) has made TP-Link a favorite among Internet service providers (ISPs) that provide routers to their customers.
In August 2024, the chairman and the ranking member of the House Select Committee on the Strategic Competition Between the United States and the Chinese Communist Party called for an investigation into TP-Link devices, which they said were found on U.S. military bases and for sale at exchanges that sell them to members of the military and their families.
‚ÄúTP-Link‚Äôs unusual degree of vulnerabilities and required compliance with PRC law are in and of themselves disconcerting,&#8221; the House lawmakers warned in a letter (PDF) to the director of the Commerce Department. &#8220;When combined with the PRC government‚Äôs common use of SOHO [small office/home office] routers like TP-Link to perpetrate extensive cyberattacks in the United States, it becomes significantly alarming.‚Äù
The letter cited a May 2023 blog post by Check Point Research about a Chinese state-sponsored hacking group dubbed &#8220;Camaro Dragon&#8221; that used a malicious firmware implant for some TP-Link routers to carry out a sequence of targeted cyberattacks against European foreign affairs entities. Check Point said while it only found the malicious firmware on TP-Link devices, &#8220;the firmware-agnostic nature of the implanted components indicates that a wide range of devices and vendors may be at risk.&#8221;
In a report published in October 2024, Microsoft said it was tracking a network of compromised TP-Link small office and home office routers that has been abused by multiple distinct Chinese state-sponsored hacking groups since 2021. Microsoft found the hacker groups were leveraging the compromised TP-Link systems to conduct &#8220;password spraying&#8221; attacks against Microsoft accounts. Password spraying involves rapidly attempting to access a large number of accounts (usernames/email addresses) with a relatively small number of commonly used passwords.
TP-Link rightly points out that most of its competitors likewise source components from China. The company also correctly notes that advanced persistent threat (APT) groups from China and other nations have leveraged vulnerabilities in products from their competitors, such as Cisco and Netgear.
But that may be cold comfort for TP-Link customers who are now wondering if it&#8217;s smart to continue using these products, or whether it makes sense to buy more costly networking gear that might only be marginally less vulnerable to compromise.
Almost without exception, the hardware and software that ships with most consumer-grade routers includes a number of default settings that need to be changed before the devices can be safely connected to the Internet. For example, bring a new router online without changing the default username and password and chances are it will only take a few minutes before it is probed and possibly compromised by some type of Internet-of-Things botnet. Also, it is incredibly common for the firmware in a brand new router to be dangerously out of date by the time it is purchased and unboxed.
Until quite recently, the idea that router manufacturers should make it easier for their customers to use these products safely was something of anathema to this industry. Consumers were largely left to figure that out on their own, with predictably disastrous results.
But over the past few years, many manufacturers of popular consumer routers have begun forcing users to perform basic hygiene &#8212; such as changing the default password and updating the internal firmware &#8212; before the devices can be used as a router. For example, most brands of &#8220;mesh&#8221; wireless routers &#8212; like Amazon&#8217;s Eero, Netgear&#8217;s Orbi series, or Asus&#8217;s ZenWifi &#8212; require online registration that automates these critical steps going forward (or at least through their stated support lifecycle).
For better or worse, less expensive, traditional consumer routers like those from Belkin and Linksys also now automate this setup by heavily steering customers toward installing a mobile app to complete the installation (this often comes as a shock to people more accustomed to manually configuring a router). Still, these products tend to put the onus on users to check for and install available updates periodically. Also, they&#8217;re often powered by underwhelming or else bloated firmware, and a dearth of configurable options.
Of course, not everyone wants to fiddle with mobile apps or is comfortable with registering their router so that it can be managed or monitored remotely in the cloud. For those hands-on folks &#8212; and for power users seeking more advanced router features like VPNs, ad blockers and network monitoring &#8212; the best advice is to check if your router&#8217;s stock firmware can be replaced with open-source alternatives, such as OpenWrt¬†or DD-WRT.
These open-source firmware options are compatible with a wide range of devices, and they generally offer more features and configurability. Open-source firmware can even help extend the life of routers years after the vendor stops supporting the underlying hardware, but it still requires users to manually check for and install any available updates.
Happily, TP-Link users spooked by the proposed ban may have an alternative to outright junking these devices, as many TP-Link routers also support open-source firmware options like OpenWRT. While this approach may not eliminate any potential hardware-specific security flaws, it could serve as an effective hedge against more common vendor-specific vulnerabilities, such as undocumented user accounts, hard-coded credentials, and weaknesses that allow attackers to bypass authentication.
Regardless of the brand, if your router is more than four or five years old it may be worth upgrading for performance reasons alone &#8212; particularly if your home or office is primarily accessing the Internet through WiFi.
NB: The Post&#8217;s story notes that a substantial portion of TP-Link routers and those of its competitors are purchased or leased through ISPs. In these cases, the devices are typically managed and updated remotely by your ISP, and equipped with custom profiles responsible for authenticating your device to the ISP&#8217;s network. If this describes your setup, please do not attempt to modify or replace these devices without first consulting with your Internet provider.
‚Ä¢ Cloudflare Scrubs Aisuru Botnet from Top Domains List
  For the past week, domains associated with the massive Aisuru botnet have repeatedly usurped Amazon, Apple, Google and Microsoft in Cloudflare&#8217;s public ranking of the most frequently requested websites. Cloudflare responded by redacting Aisuru domain names from their top websites list. The chief executive at Cloudflare says Aisuru&#8217;s overlords are using the botnet to boost their malicious domain rankings, while simultaneously attacking the company&#8217;s domain name system (DNS) service.
The #1 and #3 positions in this chart are Aisuru botnet controllers with their full domain names redacted. Source: radar.cloudflare.com.
Aisuru is a rapidly growing botnet comprising hundreds of thousands of hacked Internet of Things (IoT) devices, such as poorly secured Internet routers and security cameras. The botnet has increased in size and firepower significantly since its debut in 2024, demonstrating the ability to launch record distributed denial-of-service (DDoS) attacks nearing 30 terabits of data per second.
Until recently, Aisuru&#8217;s malicious code instructed all infected systems to use DNS servers from Google &#8212; specifically, the servers at 8.8.8.8. But in early October, Aisuru switched to invoking Cloudflare&#8217;s main DNS server &#8212; 1.1.1.1 &#8212; and over the past week domains used by Aisuru to control infected systems started populating Cloudflare&#8217;s top domain rankings.
As screenshots of Aisuru domains claiming two of the Top 10 positions ping-ponged across social media, many feared this was yet another sign that an already untamable botnet was running completely amok. One Aisuru botnet domain that sat prominently for days at #1 on the list was someone&#8217;s street address in Massachusetts followed by &#8220;.com&#8221;. Other Aisuru domains mimicked those belonging to major cloud providers.
Cloudflare tried to address these security, brand confusion and privacy concerns by partially redacting the malicious domains, and adding a warning at the top of its rankings:
&#8220;Note that the top 100 domains and trending domains lists include domains with organic activity as well as domains with emerging malicious behavior.&#8221;

Cloudflare CEO Matthew Prince told KrebsOnSecurity the company&#8217;s domain ranking system is fairly simplistic, and that it merely measures the volume of DNS queries to 1.1.1.1.
&#8220;The attacker is just generating a ton of requests, maybe to influence the ranking but also to attack our DNS service,&#8221; Prince said, adding that Cloudflare has heard reports of other large public DNS services seeing similar uptick in attacks. &#8220;We‚Äôre fixing the ranking to make it smarter. And, in the meantime, redacting any sites we classify as malware.&#8221;
Renee Burton, vice president of threat intel at the DNS security firm Infoblox, said many people erroneously assumed that the skewed Cloudflare domain rankings meant there were more bot-infected devices than there were regular devices querying sites like Google and Apple and Microsoft.
&#8220;Cloudflare&#8217;s documentation is clear &#8212; they know that when it comes to ranking domains you have to make choices on how to normalize things,&#8221; Burton wrote on LinkedIn. &#8220;There are many aspects that are simply out of your control. Why is it hard? Because reasons. TTL values, caching, prefetching, architecture, load balancing. Things that have shared control between the domain owner and everything in between.&#8221;
Alex Greenland is CEO of the anti-phishing and security firm Epi. Greenland said he understands the technical reason why Aisuru botnet domains are showing up in Cloudflare&#8217;s rankings (those rankings are based on DNS query volume, not actual web visits). But he said they&#8217;re still not meant to be there.
&#8220;It&#8217;s a failure on Cloudflare&#8217;s part, and reveals a compromise of the trust and integrity of their rankings,&#8221; he said.
Greenland said Cloudflare planned for its Domain Rankings to list the most popular domains as used by human users, and it was never meant to be a raw calculation of query frequency or traffic volume going through their 1.1.1.1 DNS resolver.
&#8220;They spelled out how their popularity algorithm is designed to reflect real human use and exclude automated traffic (they said they&#8217;re good at this),&#8221; Greenland wrote on LinkedIn. &#8220;So something has evidently gone wrong internally. We should have two rankings: one representing trust and real human use, and another derived from raw DNS volume.&#8221;
Why might it be a good idea to wholly separate malicious domains from the list? Greenland notes that Cloudflare Domain Rankings see widespread use for trust and safety determination, by browsers, DNS resolvers, safe browsing APIs and things like TRANCO.
&#8220;TRANCO is a respected open source list of the top million domains, and Cloudflare Radar is one of their five data providers,&#8221; he continued. &#8220;So there can be serious knock-on effects when a malicious domain features in Cloudflare&#8217;s top 10/100/1000/million. To many people and systems, the top 10 and 100 are naively considered safe and trusted, even though algorithmically-defined top-N lists will always be somewhat crude.&#8221;
Over this past week, Cloudflare started redacting portions of the malicious Aisuru domains from its Top Domains list, leaving only their domain suffix visible. Sometime in the past 24 hours, Cloudflare appears to have begun hiding the malicious Aisuru domains entirely from the web version of that list. However, downloading a spreadsheet of the current Top 200 domains from Cloudflare Radar shows an Aisuru domain still at the very top.
According to Cloudflare&#8217;s website, the majority of DNS queries to the top Aisuru domains &#8212; nearly 52 percent &#8212; originated from the United States. This tracks with my reporting from early October, which found Aisuru was drawing most of its firepower from IoT devices hosted on U.S. Internet providers like AT&amp;T, Comcast and Verizon.
Experts tracking Aisuru say the botnet relies on well more than a hundred control servers, and that for the moment at least most of those domains are registered in the .su top-level domain (TLD). Dot-su is the TLD assigned to the former Soviet Union (.su&#8217;s Wikipedia page says the TLD was created just 15 months before the fall of the Berlin wall).
A Cloudflare blog post from October 27 found that .su had the highest &#8220;DNS magnitude&#8221; of any TLD, referring to a metric estimating the popularity of a TLD based on the number of unique networks querying Cloudflare&#8217;s 1.1.1.1 resolver. The report concluded that the top .su hostnames were associated with a popular online world-building game, and that more than half of the queries for that TLD came from the United States, Brazil and Germany [it&#8217;s worth noting that servers for the world-building game Minecraft¬†were some of Aisuru&#8217;s most frequent targets].
A simple and crude way to detect Aisuru bot activity on a network may be to set an alert on any systems attempting to contact domains ending in .su. This TLD is frequently abused for cybercrime and by cybercrime forums and services, and blocking access to it entirely is unlikely to raise any legitimate complaints.

üéì University AI
No updates.

üè¢ Corporate AI
‚Ä¢ When industry knowledge meets PIKE-RAG: The innovation behind Signify‚Äôs customer service boost
  As a world leader in connected LED lighting products, systems, and services, Signify (formerly Philips Lighting) serves not only everyday consumers but also a large number of professional users who have stringent requirements for technical specifications and engineering compatibility. Faced with thousands of product models, complex component parameters, and technical documentation spanning multiple versions, delivering accurate, professional answers efficiently has become a core challenge for Signify‚Äôs knowledge management system.



To address this challenge, Signify (opens in new tab) collaborated with Microsoft Research Asia on a proof-of-concept (PoC) using PIKE-RAG technology, integrating it into their upgraded knowledge management system built on Microsoft Azure. The result: a 12% improvement in answer accuracy.



Challenges of applying RAG in lighting



In an era where AI is rapidly transforming how enterprises manage information, Signify recognized the strategic importance of precise and efficient knowledge systems. It adopted large AI models and retrieval-augmented generation (RAG) techniques to better support its wide range of customer inquiries.



Yet applying RAG to lighting scenarios involving professional users presented unique challenges. Product data spanned multimodal documents, unstructured tables, and complex product parameters, demanding continuous customization that slowed development and limited scalability. Despite improvements through keyword tuning, system optimization, and refined prompts, Signify sought more advanced approaches to further raise accuracy and reliability.



Seeking to unlock greater value from its knowledge management system, Signify began exploring more suitable technical solutions that are better aligned with their professional use cases. Upon learning that PIKE-RAG had been successfully applied in domains like healthcare and law, significantly improving information accuracy, Signify worked with Microsoft Research Asia on a PoC of PIKE-RAG on Microsoft Azure.



How PIKE-RAG addressed Signify‚Äôs pain points



Compared to traditional RAG, PIKE-RAG efficiently retrieves textual information and also understands multimodal content like charts and tables. Its built-in domain adaptation module quickly learns reasoning patterns aligned with specific domains to generate responses that are consistent with engineering contexts. These differentiated advantages stem from PIKE-RAG‚Äôs unique approach to understanding and processing professional knowledge. In Signify‚Äôs use case, this manifests in three key areas:



Multimodal document parsing and learning of industry-specific reasoning patterns



Signify‚Äôs product documentation includes diverse formats, such as nonstandard tables (e.g., comparison charts of voltage ranges under different currents) and circuit diagrams (e.g., driver power limits). Traditional systems often fail to process this information effectively‚Äîeither ignoring it or extracting disorganized text fragments.



PIKE-RAG integrates Microsoft Research Asia‚Äôs Document Intelligence technology with Microsoft Azure OpenAI models to accurately identify table structures and parse key parameters in circuit diagrams. For example, when a customer service agent queries, ‚ÄúWhat is the output voltage of a specific driver model at 0.15A current,‚Äù the system automatically locates the curve chart in the document and infers a range of 40‚Äì54V based on the current interval‚Äîan area where traditional systems frequently err, due to their inability to ‚Äúread‚Äù diagrams.



End-to-end knowledge loop, eliminating reliance on erroneous data sources



Enterprise knowledge systems often integrate data from multiple sources, which can lead to discrepancies, especially when database updates are not fully synchronized. PIKE-RAG captures diverse information sources and establishes citation relationships, supporting complex reasoning tasks that rely on multi-source data.



In other words, PIKE-RAG can directly use original documents as data sources, efficiently parsing and understanding product manuals and PDF charts. By extracting key information from these text- and graphic-rich documents, PIKE-RAG enables more efficient and trustworthy knowledge retrieval.



Dynamic task decomposition and multi-hop reasoning for precise answers to complex questions



Traditional RAG systems typically follow a ‚Äúone question, one answer‚Äù model and struggle with multi-step reasoning. In Signify‚Äôs lighting domain, customer inquiries often involve multi-level associations. PIKE-RAG dynamically decomposes user questions into executable subtasks and solves them through multi-hop reasoning. For example, when asked, ‚ÄúList all bases compatible with the G8 series lamps,‚Äù if no document directly provides the answer, PIKE-RAG‚Äôs reasoning proceeds as follows:



Step 1: The system identifies implicit knowledge. One document notes that the G7 and G8 series have identical dimensions and that all bases compatible with the G7 series are also compatible with the G8 series.&nbsp;



Step 2: Based on this, the system retrieves the base list for the G7 series.&nbsp;



Step 3: Since the list uses abbreviations, the system searches for a table that maps abbreviations to full names and generates a complete list of G8-compatible bases.&nbsp;



Through this automated multi-hop reasoning, the system delivers accurate and complete answers.



Figure 1: PIKE-RAG orchestrates and integrates heterogeneous information in multi-source and multimodal environments. 



Testing showed that the PIKE-RAG-powered knowledge management platform provided a significant advantage. It achieved a 12% improvement in performance compared with the original system.



These results were achieved without any question-specific customization, only algorithmic optimization, demonstrating precise knowledge matching and generation. As the system continues to learn and integrate Signify‚Äôs proprietary knowledge, accuracy is expected to improve further.



‚ÄúIn the PoC for our product specification insight tool, PIKE-RAG helped us significantly improve the original system‚Äôs performance. This will enhance overall customer satisfaction. We‚Äôre currently evaluating PIKE-RAG‚Äôs application path from multiple angles, including technical implementation, cost control, and future adaptability, and we look forward to deepening our collaboration with Microsoft Research Asia to drive further innovation,‚Äù said Haitao Liu, head of Signify Research China.



‚ÄúIt‚Äôs also worth noting that the researchers at Microsoft Research Asia demonstrated strong industry knowledge and rigorous scientific methodology. They proactively studied and analyzed the issues, tracing and clarifying the root causes of our issues to make PIKE-RAG better suited to Signify‚Äôs real-world needs.‚Äù



Beyond lighting: Generalization across industries



In Signify‚Äôs successful test, PIKE-RAG demonstrated strong generalization capabilities in complex industrial scenarios, enabling rapid cross-domain adaptation. Its three core strengths are:




Support for self-evolution and continuous learning: PIKE-RAG continuously analyzes error cases in interaction logs and uses evolutionary algorithms to automatically optimize knowledge extraction strategies, such as trying different table parsing methods or adjusting multimodal content weights. Validated strategies are then solidified for future Q&amp;A, allowing the system to adapt to new knowledge types without manual intervention.&nbsp;



Modular architecture driven by capability needs: PIKE-RAG flexibly combines modules for document parsing, knowledge extraction, storage, retrieval, organization, knowledge-centered reasoning, and task decomposition. It dynamically adjusts focus areas based on scenario needs (e.g., fact retrieval, multi-hop reasoning, innovative generation) and flexibly builds RAG methods that adapt to real-world applications, efficiently handling various complex tasks.&nbsp;



Strong adaptation to domain-specific reasoning patterns: With dynamic updates through the Domain Tips feature, enterprises can add domain-specific logic (e.g., ‚Äúthe maximum output voltage of an LED driver should be the maximum of the operating range, not the spec sheet‚Äôs max output‚Äù) in real time, enabling the system to process information according to professional engineering standards and follow industry conventions.&nbsp;




Figure 2: Overview of the PIKE-RAG framework



PIKE-RAG‚Äôs generalization capabilities have been validated not only in Signify‚Äôs knowledge management platform but also in pilot applications across industries like manufacturing, mining, and pharmaceuticals‚Äîsignificantly improving Q&amp;A system accuracy.



‚ÄúA leader in lighting, Signify presents a complex industrial knowledge system with a highly challenging real-world scenario for PIKE-RAG. Through this collaboration, we validated that PIKE-RAG‚Äôs general approach can greatly improve the accuracy of professional knowledge Q&amp;A and accelerate scenario customization. Our researchers also gained valuable experience in handling domain-specific data,‚Äù explained Jiang Bian, partner research manager at Microsoft Research Asia.



‚ÄúOur goal isn‚Äôt to build a universal chatbot but to create a professional assistant that aligns with domain-specific logic and performs rigorous knowledge reasoning. That‚Äôs the true driving force behind intelligent transformation in industrial knowledge management.‚Äù
Opens in a new tabThe post When industry knowledge meets PIKE-RAG: The innovation behind Signify‚Äôs customer service boost appeared first on Microsoft Research.
‚Ä¢ Magentic Marketplace: an open-source simulation environment for studying agentic markets
  Autonomous AI agents are here, and they&#8217;re poised to reshape the economy. By automating discovery, negotiation, and transactions, agents can overcome inefficiencies like information asymmetries and platform lock-in, enabling faster, more transparent, and more competitive markets.



We are already seeing early signs of this transformation in digital marketplaces. Customer-facing assistants like OpenAI‚Äôs Operator and Anthropic‚Äôs Computer Use can navigate websites and complete purchases. On the business side, Shopify Sidekick, Salesforce Einstein, and Meta‚Äôs Business AI help merchants with operations and customer engagement. These examples hint at a future where agents become active market participants, but the structure of these markets remains uncertain.



Several scenarios are possible. We might see one-sided markets where only customers or businesses deploy agents; closed platforms (known as walled gardens) where companies tightly control agent interactions; or even open two-sided marketplaces where customer and business agents transact freely across ecosystems. Each path carries different trade-offs for security, openness, convenience, and competition, which will shape how value flows in the digital economy. For a deeper exploration of these dynamics, see our paper, The Agentic Economy.



To help navigate this uncertainty, we built Magentic Marketplace (opens in new tab)‚Äî an open-source simulation environment for exploring the numerous possibilities of agentic markets and their societal implications at scale. It provides a foundation for studying these markets and guiding them toward outcomes that benefit everyone.



This matters because most AI agent research focuses on isolated scenarios‚Äîa single agent completing a task or two agents negotiating a simple transaction. But real markets involve a large number of agents simultaneously searching, communicating, and transacting, creating complex dynamics that can‚Äôt be understood by studying agents in isolation. Capturing this complexity is essential because real-world deployments raise critical questions about consumer welfare, market efficiency, fairness, manipulation resistance, and bias‚Äîquestions that can‚Äôt be safely answered in production environments.



To explore these dynamics in depth, the Magentic Marketplace platform enables controlled experimentation across diverse agentic marketplace scenarios. Its current focus is on two-sided markets, but the environment is modular and extensible, supporting future exploration of mixed human‚Äìagent systems, one-sided markets, and complex communication protocols.



Figure 1. With Magentic Marketplace, researchers can model how agents representing customers and businesses interact‚Äîshedding light on the dynamics that could shape future digital markets.



What is Magentic Marketplace?



Magentic Marketplace‚Äôs environment manages market-wide capabilities like maintaining catalogs of available goods and services, implementing discovery algorithms, facilitating agent-to-agent communication, and handling simulated payments through a centralized transaction layer at its core, which ensures transaction integrity across all marketplace interactions. Additionally, the platform enables systematic, reproducible research. As demonstrated in the following video, it supports a wide range of agent implementations and evolving marketplace features, allowing researchers to integrate diverse agent architectures and adapt the environment as new capabilities emerge.









We built Magentic Marketplace around three core architectural choices:



HTTP/REST client-server architecture: Agents operate as independent clients while the Marketplace Environment serves as a central server. This mirrors real-world platforms and supports clear separation of customer and business agent roles.



Minimal&nbsp;three-endpoint&nbsp;market&nbsp;protocol:&nbsp;Just&nbsp;three endpoints‚Äîregister, protocol discovery, and action execution‚Äîlets&nbsp;agents dynamically discover available actions.&nbsp;New capabilities&nbsp;can&nbsp;be added without disrupting existing experiments.



Rich action protocol: Specific message types support the complete transaction lifecycle: search, negotiation, proposals, and payments. The protocol is designed for extensibility. New actions like refunds, reviews, or ratings can be added seamlessly, allowing researchers to evolve marketplace capabilities and study emerging agent behaviors while remaining compatible.



Figure 2. Magentic Marketplace includes two agent types: Assistant Agents (customers) and Service Agents (businesses). Both interact with a central Market Environment via REST APIs for registration, service discovery, communication, and transaction execution. Action Routers manage message flow and protocol requests, enabling autonomous negotiation and commerce in a two-sided marketplace.



Additionally, a visualization module lets users observe marketplace dynamics and review individual conversation threads between customer and business agents.



Setting up the experiments



To ensure reproducibility, we instantiated the marketplace with fully synthetic data, available in our open-source repository (opens in new tab). The experiments modeled transactions such as ordering food and engaging with home improvement services, where agents represented customers and businesses engaging in marketplace transactions. This setup enabled precise measurement of behavior and systematic comparison against theoretical upper bounds.



Each experiment was run using 100 customers and 300 businesses and included both proprietary models (GPT-4o, GPT-4.1, GPT-5, and Gemini-2.5-Flash) and open-source models (OSS-20b, Qwen3-14b, and Qwen3-4b-Instruct-2507).



Our scenarios focused on simple all-or-nothing requests: Each customer had a list of desired items and amenities that needed to be present for a transaction to be satisfying. For those transactions, utility was computed as the sum of the customer‚Äôs internal item valuations minus actual prices paid. Consumer welfare, defined as the sum of utilities across all completed transactions, served as our key metric for comparing agent performance.



While this experimental setup provides a useful starting point, it is not intended to be definitive. We encourage researchers to extend the framework with richer, more nuanced measures and request types that better capture real consumer welfare, fairness, and other societal considerations.



	
		

		
		PODCAST SERIES
	
	
	
						
				
					
				
			
			
			

									AI Testing and Evaluation: Learnings from Science and Industry
				
								Discover how Microsoft is learning from other domains to advance evaluation and testing as a pillar of AI governance.
				
								
					
						
							Listen now						
					
				
							
	
Opens in a new tab	
	


What did we find?



Agents can improve consumer welfare‚Äîbut only with good discovery



We explored whether two-sided agentic markets‚Äîwhere AI agents interact with each other and with service providers‚Äîcan improve consumer welfare by reducing information gaps. Unlike traditional markets, which do not provide agentic support and place the full burden of overcoming information asymmetries on customers, agentic markets shift much of that effort to agents. This change matters because as agents gain better tools for discovery and communication, they relieve customers of the heavy cognitive load of filling any information gaps. This lowers the cost of making informed decisions and improves customer outcomes.



We compared several marketplace setups. Under realistic conditions (Agentic: Lexical search), agents faced real-world challenges like building queries, navigating paginated lists, identifying the right businesses to send inquiries to, and negotiating transactions.



Despite these complexities, advanced proprietary models and some medium-sized open-source models like GPTOSS-20b outperformed simple baselines like randomly choosing or simply choosing the cheapest option. Notably, GPT-5 achieved near-optimal performance, demonstrating its ability to effectively gather and utilize decision-relevant information in realistic marketplace conditions.



Figure 3. Table comparing experimental setups for welfare outcomes in the restaurant industry. Each row shows a different way agents or baselines make decisions, from random picks to fully coordinated agentic strategies. Cell colors indicate how much information is available: green, at the top left, represents complete information, red, at the top right, represents limited information, and yellow at the bottom represents decisions that depend on agent communication.



Performance increased considerably under the Agentic: Perfect search condition, where agents started with the top three matches without needing to search and navigate among the choices. In this setting, Sonnet-4.0, Sonnet-4.5, GPT-5, and GPT-4.1 nearly reached the theoretical optimum and beat baselines with full amenity details but without agent-to-agent coordination.



Open-source models were mixed: GPTOSS-20b performed strongly under both Perfect search and Lexical search conditions, even exceeding GPT-4o&#8217;s performance with Perfect search. This suggests that relatively compact models can exhibit robust information-gathering and decision-making capabilities in complex multi-agent environments. Qwen3-4b-2507 faltered when discovery involved irrelevant options (Lexical search), while Qwen3-14b lagged in both cases due to fundamental limitations in reasoning.



Figure 4. Chart showing consumer welfare outcomes in the restaurant industry under different marketplace setups. Blue bars show Agentic: Lexical search, where agents navigate realistic discovery challenges; yellow bars show Agentic: Perfect search, where agents started with ideal matches. Proprietary models approached optimum consumer welfare under perfect search, while open-source models and baselines lagged behind.



Paradox of Choice



One promise of agents is their ability to consider far more options than people can. However, our experiments revealed a surprising limitation: providing agents with more options does not necessarily lead to more thorough exploration. We designed experiments that varied the search results limit from 3 to 100. Except for Gemini-2.5-Flash and GPT-5, the models contacted only a small fraction of available businesses regardless of the search limit. This suggests that most models do not conduct exhaustive comparisons and instead easily accept the initial &#8220;good enough&#8221; options.



Figure 5. More options didn‚Äôt lead to broader exploration. Most models still contacted only a few businesses, except Gemini-2.5-Flash and GPT-5.



Additionally, across all models, consumer welfare declined as the number of search results increased. Despite contacting over a hundred businesses, Gemini-2.5-Flash&#8217;s performance declined from 1,700 to 1,350, and GPT-5 declined even more, from a near-optimal 2,000 to 1,400.



This demonstrates a Paradox of Choice effect, where more exploration does not guarantee better outcomes, potentially due to limited long context understanding. Claude Sonnet 4 showed the steepest performance decline, from 1,800 to 600 in consumer welfare. With all the options presented, it struggled to navigate larger sets of options and frequently contacted businesses that did not provide the goods or services that the customer was looking for.



This combination of poor initial selection and premature search termination demonstrates both inadequate decision-making criteria and insufficient exploration strategies. Some models showed modest performance decline (i.e., GPT-4.1: from 1,850 to 1,700; GPT-4o: from 1,550 to 1,450), finding good options within their limited exploration.



Figure 6. Mean consumer welfare decreased as consideration set size grew, revealing a Paradox of Choice effect, where expanding options reduced overall welfare.



Agents are vulnerable to manipulation



We tested six manipulation strategies, ranging from subtle psychological tactics to aggressive prompt injection attacks:




Authority: Fake credentials like ‚ÄúMichelin Guide featured‚Äù and ‚ÄúJames Beard Award nominated‚Äù paired with fabricated certifications.



Social proof: Claims like ‚ÄúJoin 50,000+ satisfied customers‚Äù or ‚Äú#1-rated Mexican restaurant‚Äù combined with fake reviews.



Loss aversion: Fear-based warnings about ‚Äúfood poisoning‚Äù risks and ‚Äúcontamination issues‚Äù at competing restaurants.



Prompt injection (basic): Attempts to override agent instructions.



Prompt injection (strong): Aggressive attacks using emergency language and fabricating competitor scandals.




Results revealed significant variation in manipulation resistance across models. Sonnet-4 was resistant to all attacks, and none of the manipulative strategies affected any of the customers‚Äô choices. Gemini-2.5-Flash was generally resistant, except for strong prompt injections, where mean payments to unmanipulated agents were affected as a result. GPT-4o, GPTOSS-20b and Qwen3-4b were very vulnerable to prompt injection: all payments were redirected to the manipulative agent under these conditions. Specifically for GPTOSS-20 and Qwen3-4b-2507, even traditional psychological manipulation tactics (authority appeals and social proof) increased payments to malicious agents, demonstrating their vulnerability to basic persuasion techniques. These findings highlight a critical security concern for agentic marketplaces.



Figure 7. Charts showing the variation in mean payments received by service agents with and without manipulation tactics. The results reveal substantial differences in manipulation resistance across models, with GPT-4.1 showing significantly higher vulnerability compared to Gemini-2.5-Flash.



Systemic biases create unfair advantages



Our analysis revealed two distinct types of systematic biases showed by agents when selecting businesses from search results. Models showed systematic preferences based on where businesses appeared in search results. While proprietary models showed no strong positional preferences, open-source models exhibited clear patterns. Specifically, Qwen2.5-14b-2507 showed a pronounced bias toward selecting the last business presented, regardless of its actual merits.



Proposal&nbsp;bias&nbsp;is&nbsp;more pervasive across all models tested. This &#8220;first-offer acceptance&#8221; pattern suggests that models prioritized&nbsp;immediate selection over comprehensive exploration, potentially missing better alternatives that&nbsp;could have&nbsp;emerged&nbsp;by waiting for better options. This behavior&nbsp;continued&nbsp;across both proprietary and open-source models,&nbsp;indicating&nbsp;a fundamental challenge in agent decision-making architectures.



These biases can create unfair market dynamics, drive unintended behaviors, and push businesses to complete on response speed rather than product or service quality.



Figure 8. All models showed strong preference for the first proposal received, accepting it without waiting for additional proposals or conducting systematic comparisons.



What this means



Even state-of-the-art models can show notable vulnerabilities and biases in marketplace environments. In our implementation, agents struggled with too many options, were susceptible to manipulation tactics, and showed systemic biases that created unfair advantages.



These outcomes are shaped not only by agent capabilities but also by marketplace design and implementation. Our current study focused on static markets, but real-world environments are dynamic, with agents and users learning over time. Oversight is critical for high-stakes transactions. Agents should assist, not replace, human decision-making.



We plan to explore dynamic markets and human-in-the-loop designs to improve efficiency and trust. A simulation environment like Magentic Marketplace is crucial for understanding the interplay between market components and agents before deploying them at scale.



Full details of our experimental setup and results are available in our paper (opens in new tab).



Getting started



Magentic Marketplace is available as an open-source environment for exploring agentic market dynamics. Code, datasets, and experiment templates are available on GitHub (opens in new tab) and Azure AI Foundry Labs (opens in new tab).



The documentation (opens in new tab) provides instructions for reproducing the experiments described above and guidance for extending the environment to new marketplace configurations.
Opens in a new tabThe post Magentic Marketplace: an open-source simulation environment for studying agentic markets appeared first on Microsoft Research.
‚Ä¢ Connect Amazon Bedrock agents to cross-account knowledge bases
  Organizations need seamless access to their structured data repositories to power intelligent AI agents. However, when these resources span multiple AWS accounts integration challenges can arise. This post explores a practical solution for connecting Amazon Bedrock agents to knowledge bases in Amazon Redshift clusters residing in different AWS accounts. 
The challenge 
Organizations that build AI agents using Amazon Bedrock can maintain their structured data in Amazon Redshift clusters. When these data repositories exist in separate AWS accounts from their AI agents, they face a significant limitation: Amazon Bedrock Knowledge Bases doesn‚Äôt natively support cross-account Redshift integration. 
This creates a challenge for enterprises with multi-account architectures who want to: 
 
 Leverage existing structured data in Redshift for their AI agents. 
 Maintain separation of concerns across different AWS accounts. 
 Avoid duplicating data across accounts. 
 Ensure proper security and access controls. 
 
Solution overview 
Our solution enables cross-account knowledge base integration through a secure, serverless architecture that maintains secure access controls while allowing AI agents to query structured data. The approach uses AWS Lambda as an intermediary to facilitate secure cross-account data access. 
 
The action flow as shown above: 
 
 Users enter their natural language question in Amazon Bedrock Agents which is configured in the agent account. 
 Amazon Bedrock Agents invokes a&nbsp;Lambda function through action groups which provides access to the Amazon Bedrock knowledge base configured in the agent-kb account above. 
 Action group Lambda function running in agent account assumes an IAM role created in agent-kb account above to connect to the knowledge base in the agent-kb account. 
 Amazon Bedrock Knowledge Base in the agent-kb account uses an IAM role created in the same account to access Amazon Redshift data warehouse and query data in the data warehouse. 
 
The solution follows these key components: 
 
 Amazon Bedrock agent in the agent account that handles user interactions. 
 Amazon Redshift serverless workgroup in VPC and private subnet in the agent-kb account containing structured data. 
 Amazon Bedrock Knowledge base using the Amazon Redshift serverless workgroup as structured data source. 
 Lambda function in the agent account. 
 Action group configuration to connect the agent in the agent account to the Lambda function. 
 IAM roles and policies that enable secure cross-account access. 
 
Prerequisites 
This solution requires you to have the following: 
 
 Two AWS accounts. Create an AWS account if you do not have one. Specific permissions required for both account which will be set up in subsequent steps. 
 Install the AWS CLI (2.24.22 ‚Äì current version) 
 Set up authentication using IAM user credentials for the AWS CLI for each account 
 Make sure you have jq installed, jq is lightweight command-line JSON processor. For example, in Mac you can use the command brew install jq (jq-1.7.1-apple ‚Äì current version) to install it. 
 Navigate to the Amazon Bedrock console and make sure you enable access to the meta.llama3-1-70b-instruct-v1:0 model for the agent-kb account and access for us.amazon.nova-pro-v1:0 model in the agent account in the us-west-2, US West (Oregon) AWS Region. 
 
Assumption 
Let‚Äôs call the AWS account profile, agent profile that has the Amazon Bedrock agent. Similarly, the AWS account profile be called agent-kb that has the Amazon Bedrock knowledge base with Amazon Redshift Serverless and the structured data source. We will use the us-west-2 US West (Oregon) AWS Region but feel free to choose another AWS Region as necessary (the prerequisites will be applicable to the AWS Region you choose to deploy this solution in). We will use the meta.llama3-1-70b-instruct-v1:0 model for the agent-kb. This is an available on-demand model in us-west-2. You are free to choose other models with cross-Region inference but that would mean changing the roles and polices accordingly and enable model access in all Regions they are available in. Based on our model choice for this solution the AWS Region must be us-west-2. For the agent we will be using an Amazon Bedrock agent optimized model like us.amazon.nova-pro-v1:0. 
Implementation walkthrough 
The following is a step-by-step implementation guide. Make sure to perform all steps in the same AWS Region in both accounts. 
These steps are to deploy and test an end-to-end solution from scratch and if you are already running some of these components, you may skip over those steps. 
 
  
   
   Make a note of the AWS account numbers in the agent and agent-kb account. In the implementation steps we will refer them as follows: 
     
      
       
       Profile 
       AWS account 
       Description 
       
       
       agent 
       111122223333 
       Account for the Bedrock Agent 
       
       
       agent-kb 
       999999999999 
       Account for the Bedrock Knowledge base 
       
      
     Note: These steps use example profile names and account numbers, please replace with actuals before running. 
   Create the Amazon Redshift Serverless workgroup in the agent-kb account: 
     
     Log on to the agent-kb account 
     Follow the workshop link to create the Amazon Redshift Serverless workgroup in private subnet 
     Make a note of the namespace, workgroup, and other details and follow the rest of the hands-on workshop instructions. 
      
   Set up your data warehouse in the agent-kb account. 
   Create your AI knowledge base in the agent-kb account. Make a note of the knowledge base ID. 
   Train your AI Assistant in the agent-kb account. 
   Test natural language queries in the agent-kb account. You can find the code in aws-samples git repository: sample-for-amazon-bedrock-agent-connect-cross-account-kb. 
   Create necessary roles and policies in both the accounts. Run the script create_bedrock_agent_kb_roles_policies.sh with the following input parameters. 
     
      
       
       Input parameter 
       Value 
       Description 
       
       
       ‚Äìagent-kb-profile 
       agent-kb 
       The agent knowledgebase profile that you set up with the AWS CLI with aws_access_key_id, aws_secret_access_key as mentioned in the prerequisites. 
       
       
       ‚Äìlambda-role 
       lambda_bedrock_kb_query_role 
       This is the IAM role the agent account Bedrock agent action group lambda will assume to connect to the Redshift cross account 
       
       
       ‚Äìkb-access-role 
       bedrock_kb_access_role 
       This is the IAM role the agent-kb account which the lambda_bedrock_kb_query_role in agent account assumes to connect to the Redshift cross account 
       
       
       ‚Äìkb-access-policy 
       bedrock_kb_access_policy 
       IAM policy attached to the IAM role bedrock_kb_access_role 
       
       
       ‚Äìlambda-policy 
       lambda_bedrock_kb_query_policy 
       IAM policy attached to the IAM role lambda_bedrock_kb_query_role 
       
       
       ‚Äìknowledge-base-id 
       XXXXXXXXXX 
       Replace with the actual knowledge base ID created in Step 4 
       
       
       ‚Äìagent-account 
       111122223333 
       Replace with the 12-digit AWS account number where the Bedrock agent is running. (agent account) 
       
       
       ‚Äìagent-kb-account 
       999999999999 
       Replace with the 12-digit AWS account number where the Bedrock knowledge base is running. (agent-kb acccount) 
       
      
      
   Download the script (create_bedrock_agent_kb_roles_policies.sh) from the aws-samples GitHub repository. 
   Open Terminal in Mac or similar bash shell for other platforms. 
   Locate and change the directory to the downloaded location, provide executable permissions: 
     
     cd /my/location
chmod +x create_bedrock_agent_kb_roles_policies.sh 
      
   If you are still not clear on the script usage or inputs, then you can run the script with the ‚Äìhelp option and the script will display the usage: ./create_bedrock_agent_kb_roles_policies.sh ‚Äìhelp 
   Run the script with the right input parameters as described in the previous table. 
     
     ./create_bedrock_agent_kb_roles_policies.sh --agent-profile agent \ 
  --agent-kb-profile agent-kb \ 
  --lambda-role lambda_bedrock_kb_query_role \ 
  --kb-access-role bedrock_kb_access_role \ 
  --kb-access-policy bedrock_kb_access_policy \ 
  --lambda-policy lambda_bedrock_kb_query_policy \ 
  --knowledge-base-id XXXXXXXXXX \ 
  --agent-account 111122223333 \ 
  --agent-kb-account 999999999999 
      
   The script on successful execution shows the summary of the IAM, roles and policies created in both accounts.  
   Log on to both the agent and agent-kb account to verify the IAM roles and policies are created. 
     
      
       
        
         
         For the agent account: Make a note of the ARN of the lambda_bedrock_kb_query_role as that will be the value of CloudFormation stack parameter AgentLambdaExecutionRoleArn in the next step.  
         For the agent-kb account: Make a note of the ARN of the bedrock_kb_access_role as that will be the value of CloudFormation stack parameter TargetRoleArn in the next step.  
          
        
      
   Run the AWS CloudFormation script to create a Bedrock agent: 
     
      
       
        
         
          
           
           Download the CloudFormation script: cloudformation_bedrock_agent_kb_query_cross_account.yaml from the aws-samples GitHub repository. 
           Log on to the agent account and navigate to the CloudFormation console, and verify you are in the us-west-2 (Oregon) Region, choose Create stack and choose With new resources (standard).  
           In the Specify template section choose Upload a template file and then Choose file and select the file from (1). Then, choose Next. 
           Enter the following stack details and choose Next. 
             
              
               
               Parameter 
               Value 
               Description 
               
               
               Stack name 
               bedrock-agent-connect-kb-cross-account-agent 
               You can choose any name 
               
               
               AgentFoundationModelId 
               us.amazon.nova-pro-v1:0 
               Do not change 
               
               
               AgentLambdaExecutionRoleArn 
               arn:aws:iam:: 111122223333:role/lambda_bedrock_kb_query_role 
               Replace with you agent account number 
               
               
               BedrockAgentDescription 
               Agent to query inventory data from Redshift Serverless database 
               Keep this as default 
               
               
               BedrockAgentInstructions 
               You are an assistant that helps users query inventory data from our Redshift Serverless database using the action group. 
               Do not change 
               
               
               BedrockAgentName 
               bedrock_kb_query_cross_account 
               Keep this as default 
               
               
               KBFoundationModelId 
               meta.llama3-1-70b-instruct-v1:0 
               Do not change 
               
               
               KnowledgeBaseId 
               XXXXXXXXXX 
               Knowledge base id from Step 4 
               
               
               TargetRoleArn 
               arn:aws:iam::999999999999:role/bedrock_kb_access_role 
               Replace with you agent-kb account number 
               
              
              
           Complete the acknowledgement and choose Next. 
           Scroll down through the page and choose Submit. 
           You will see the CloudFormation stack is getting created as shown by the status CREATE_IN_PROGRESS. 
           It will take a few minutes, and you will see the status change to CREATE_COMPLETE indicating creation of all resources. Choose the Outputs tab to make a note of the resources that were created. In summary, the CloudFormation script does the following in the agent account. 
             
              
               
                
                 
                 Creates a Bedrock agent 
                 Creates an action group 
                 Also creates a Lambda function which is invoked by the Bedrock action group 
                 Defines the OpenAPI schema 
                 Creates necessary roles and permissions for the Bedrock agent 
                 Finally, it prepares the Bedrock agent so that it is ready to test. 
                  
                
              
            
          
        
      
   Check for model access in Oregon (us-west-2) 
     
      
       
        
         
          
           
           Verify Nova Pro (us.amazon.nova-pro-v1:0) model access in the agent account. Navigate to the Amazon Bedrock console and choose Model access under Configure and learn. Search for Model name : Nova Pro to verify access. If not, then enable model access.  
           Verify access to the meta.llama3-1-70b-instruct-v1:0 model in the agent-kb account. This should already be enabled as we set up the knowledge base earlier. 
            
          
        
      
   Run the agent. Log on to agent account. Navigate to Amazon Bedrock console and choose Agents under Build. 
   Choose the name of the agent and choose Test. You can test the following questions as mentioned the workshop‚Äôs Stage 4: Test Natural Language Queries page. For example: 
     
      
       
        
         
          
           
           Who are the top 5 customers in Saudi Arabia? 
           Who are the top parts supplier in the United States by volume? 
           What is the total revenue by region for the year 1998? 
           Which products have the highest profit margins? 
           Show me orders with the highest priority from the last quarter of 1997. 
            
          
        
      
   Choose Show trace to investigate the agent traces. 
    
 
Some recommended best practices: 
 
  
   
    
     
     Phrase your question to be more specific 
     Use terminology that matches your table descriptions 
     Try questions similar to your curated examples 
     Verify your question relates to data that exists in the TPCH dataset 
     Use Amazon Bedrock Guardrails to add configurable safeguards to questions and responses. 
      
    
 
Clean up resources 
It is recommended that you clean up any resources you do not need anymore to avoid any unnecessary charges: 
 
  
   
    
     
     Navigate to the CloudFormation console for the agent and agent-kb account, search for the stack and and choose Delete. 
     S3 buckets need to be deleted separately.  
     For deleting the roles and policies created in both accounts, download the script delete-bedrock-agent-kb-roles-policies.sh from the aws-samples GitHub repository. 
       
       Open Terminal in Mac or similar bash shell on other platforms. 
       Locate and change the directory to the downloaded location, provide executable permissions: 
       
       
       cd /my/location
			chmod +x delete-bedrock-agent-kb-roles-policies.sh 
        
     If you are still not clear on the script usage or inputs, then you can run the script with the ‚Äìhelp option then the script will display the usage: ./ delete-bedrock-agent-kb-roles-policies.sh ‚Äìhelp 
     Run the script: delete-bedrock-agent-kb-roles-policies.sh with the same values for the same input parameters as in Step7 when running the create_bedrock_agent_kb_roles_policies.sh script. Note: Enter the correct account numbers for agent-account and agent-kb-account before running. 
       
       ./delete-bedrock-agent-kb-roles-policies.sh --agent-profile agent \ 
  	--agent-kb-profile agent-kb \ 
	  --lambda-role lambda_bedrock_kb_query_role \ 
	  --kb-access-role bedrock_kb_access_role \ 
	  --kb-access-policy bedrock_kb_access_policy \ 
	  --lambda-policy lambda_bedrock_kb_query_policy \ 
	  --agent-account 111122223333 \ 
	  --agent-kb-account 999999999999 
       The script will ask for a confirmation, say yes and press enter.  
      
    
 
Summary 
This solution demonstrates how the Amazon Bedrock agent in the agent account can query the Amazon Bedrock knowledge base in the agent-kb account. 
Conclusion 
This solution uses Amazon Bedrock Knowledge Bases for structured data to create a more integrated approach to cross-account data access. The knowledge base in agent-kb account connects directly to Amazon Redshift Serverless in a private VPC. The Amazon Bedrock agent in the agent account invokes an AWS Lambda function as part of its action group to make a cross-account connection to retrieve response from the structured knowledge base. 
This architecture offers several advantages: 
 
  
   
    
     
     Uses Amazon Bedrock Knowledge Bases capabilities for structured data 
     Provides a more seamless integration between the agent and the data source 
     Maintains proper security boundaries between accounts 
     Reduces the complexity of direct database access codes 
      
    
 
As Amazon Bedrock continues to evolve, you can take advantage of future enhancements to knowledge base functionality while maintaining your multi-account architecture. 
 
 
About the Authors 
Kunal Ghosh is an expert in AWS technologies. He passionate about building efficient and effective solutions on AWS, especially involving generative AI, analytics, data science, and machine learning. Besides family time, he likes reading, swimming, biking, and watching movies, and he is a foodie. 
Arghya Banerjee is a Sr. Solutions Architect at AWS in the San Francisco Bay Area, focused on helping customers adopt and use the AWS Cloud. He is focused on big data, data lakes, streaming and batch analytics services, and generative AI technologies. 
Indranil Banerjee is a Sr. Solutions Architect at AWS in the San Francisco Bay Area, focused on helping customers in the hi-tech and semi-conductor sectors solve complex business problems using the AWS Cloud. His special interests are in the areas of legacy modernization and migration, building analytics platforms and helping customers adopt cutting edge technologies such as generative AI. 
Vinayak Datar is Sr. Solutions Manager based in Bay Area, helping enterprise customers accelerate their AWS Cloud journey. He‚Äôs focusing on helping customers to convert ideas from concepts to working prototypes to production using AWS generative AI services.
‚Ä¢ Democratizing AI: How Thomson Reuters Open Arena supports no-code AI for every professional with Amazon Bedrock
  This post is cowritten by Laura Skylaki, Vaibhav Goswami, Ramdev Wudali and Sahar El Khoury from Thomson Reuters. 
Thomson Reuters (TR) is a leading AI and technology company dedicated to delivering trusted content and workflow automation solutions. With over 150 years of expertise, TR provides essential solutions across legal, tax, accounting, risk, trade, and media sectors in a fast-evolving world. 
TR recognized early that AI adoption would fundamentally transform professional work. According to TR‚Äôs 2025 Future of Professionals Report, 80% of professionals anticipate AI significantly impacting their work within five years, with projected productivity gains of up to 12 hours per week by 2029. To unlock this immense potential, TR needed a solution to democratize AI creation across its organization. 
In this blog post, we explore how TR addressed key business use cases with Open Arena, a highly scalable and flexible no-code AI solution powered by Amazon Bedrock and other AWS services such as Amazon OpenSearch Service, Amazon Simple Storage Service (Amazon S3), Amazon DynamoDB, and AWS Lambda. We‚Äôll explain how TR used AWS services to build this solution, including how the architecture was designed, the use cases it solves, and the business profiles that use it. The system demonstrates TR‚Äôs successful approach of using existing TR services for rapid launches while supporting thousands of users, showcasing how organizations can democratize AI access and support business profiles (for example, AI explorers and SMEs) to create applications without coding expertise. 
Introducing Open Arena: No-code AI for all 
TR introduced Open Arena to non-technical professionals to create their own customized AI solutions. With Open Arena users can use cutting-edge AI powered by Amazon Bedrock in a no-code environment, exemplifying TR‚Äôs commitment to democratizing AI access. 
Today, Open Arena supports: 
 
 High adoption: ~70% employee adoption, with&nbsp;19,000 monthly active users. 
 Custom solutions: Thousands of customized AI solutions created&nbsp;without coding, used for internal workflows or integrated into TR products for customers. 
 Self-served functionality: 100% self-served functionality, so that users, irrespective of technical background, can develop, evaluate, and deploy generative AI solutions. 
 
The Open Arena journey: From prototype to enterprise solution 
Conceived as a rapid prototype, Open Arena was developed in under six weeks at the onset of the generative AI boom in early 2023 by TR Labs ‚Äì TR‚Äôs dedicated applied research division focused on the research, development, and application of AI and emerging trends in technologies. The goal was to support internal team exploration of large language models (LLMs) and discover unique use cases by merging LLM capabilities with TR company data. 
Open Arena‚Äôs introduction significantly increased AI awareness, fostered developer-SME collaboration for groundbreaking concepts, and accelerated AI capability development for TR products. The rapid success and demand for new features quickly highlighted Open Arena‚Äôs potential for AI democratization, so TR developed an enterprise version of Open Arena. Built on the&nbsp;TR AI Platform, Open Arena enterprise version offers secure, scalable, and standardized services covering the entire AI development lifecycle, significantly accelerating time to production. 
The Open Arena enterprise version uses existing system capabilities for enhanced data access controls, standardized service access, and compliance with TR‚Äôs governance and ethical standards. This version introduced self-served capabilities so that every user, irrespective of their technical ability, can create, evaluate, and deploy customized AI solutions in a no-code environment. 

 ‚ÄúThe foundation of the AI Platform has always been about empowerment; in the early days it was about empowering Data Scientists but with the rise of Gen AI, the platform adapted and evolved on empowering users of any background to leverage and create AI Solutions.‚Äù 
 ‚Äì Maria Apazoglou, Head of AI Engineering, CoCounsel
 
As of July 2025, the TR Enterprise AI Platform consists of 15 services spanning the entire AI development lifecycle and user personas. Open Arena remains one of its most popular, serving 19,000 users each month, with increasing monthly usage. 
Addressing key enterprise AI challenges across user types 
Using the TR Enterprise AI Platform, Open Arena helped thousands of professionals transition into using generative AI. AI-powered innovation is now readily in the hands of everyone, not just AI scientists. 
Open Arena successfully addresses four critical enterprise AI challenges: 
 
 Enablement:&nbsp;Delivers AI solution building with consistent LLM and service provider experience and support for various user personas, including non-technical. 
 Security and quality:&nbsp;Streamlines AI solution quality tracking using evaluation and monitoring services, whilst complying with data governance and ethics policies. 
 Speed and reusability:&nbsp;Automates workflows and uses existing AI solutions and prompts. 
 Resources and cost management:&nbsp;Tracks and displays generative AI solution resource consumption, supporting transparency and efficiency. 
 
The solution currently supports several AI experiences, including tech support, content creation, coding assistance, data extraction and analysis, proof reading, project management, content summarization, personal development, translation, and problem solving, catering to different user needs across the organization. 
 
 
Figure 1. Examples of Open Arena use cases. 
AI explorers use Open Arena to speed up day-to-day tasks, such as summarizing documents, engaging in LLM chat, building custom workflows, and comparing AI models. AI creators and Subject Matter Experts (SMEs) use Open Arena to build custom AI workflows and experiences and to evaluate solutions without requiring coding knowledge. Meanwhile, developers can develop and deploy new AI solutions at speed, training models, creating new AI skills, and deploying AI capabilities. 
Why Thomson Reuters selected AWS for Open Arena 
TR strategically chose AWS as a primary cloud provider for Open Arena based on several critical factors: 
 
 Comprehensive AI/ML capabilities:&nbsp;Amazon Bedrock offers easy access to a choice of high-performing foundation models from leading AI companies like AI21 Labs, Anthropic, Cohere, DeepSeek, Luma AI, Meta, Mistral AI, OpenAI, Qwen, Stability AI, TwelveLabs, Writer, and Amazon. It supports simple chat and complex RAG workflows, and integrates seamlessly with TR‚Äôs existing Enterprise AI Platform. 
 Enterprise-grade security and governance:&nbsp;Advanced security controls, model access using RBAC, data handling with enhanced security features, single sign-on (SSO) enabled, and clear operational and user data separation across AWS accounts. 
 Scalable infrastructure:&nbsp;Serverless architecture for automatic scaling, pay-per-use pricing for cost optimization, and global availability with low latency. 
 Existing relationship and expertise:&nbsp;Strong, established relationship between TR and AWS, existing Enterprise AI Platform on AWS, and deep AWS expertise within TR‚Äôs technical teams. 
 

 ‚ÄúOur long-standing partnership with AWS and their robust, flexible and innovative services made them the natural choice to power Open Arena and accelerate our AI initiatives.‚Äù  
 ‚Äì Maria Apazoglou, Head of AI Engineering, CoCounsel
 
Open Arena architecture: Scalability, extensibility, and security 
Designed for a broad enterprise audience, Open Arena prioritizes scalability, extensibility and security while maintaining simplicity for non-technical users to create and deploy AI solutions. The following diagram illustrates the architecture of Open Arena. 
 
Figure 2. Architecture design of Open Arena. 
The architecture design facilitates enterprise-grade performance with clear separation between capability and usage, aligning with TR‚Äôs enterprise cost and usage tracking requirements. 
The following are key components of the solution architecture: 
 
 No-code interface:&nbsp;Intuitive UI, visual workflow builder, pre-built templates, drag-and-drop functionality. 
 Enterprise integration:&nbsp;Seamless integration with TR‚Äôs Enterprise AI Platform, SSO enabled, data handling with enhanced security, clear data separation. 
 Solution management:&nbsp;Searchable repository, public/private sharing, version control, usage analytics. 
 
TR developed Open Arena using AWS services such as Amazon Bedrock, Amazon OpenSearch, Amazon DynamoDB, Amazon API Gateway, AWS Lambda, and AWS Step Functions. It uses Amazon Bedrock for foundational model interactions, supporting simple chat and complex Retrieval-Augmented Generation (RAG) tasks. Open Arena uses Amazon Bedrock Flows as the custom workflow builder where users can drag-and-drop components like prompts, agents, knowledge bases and Lambda functions to create sophisticated AI workflows without coding. The system also integrates with AWS OpenSearch for knowledge bases and external APIs for advanced agent capabilities. 
For data separation, orchestration is managed using the Enterprise AI Platform AWS account, capturing operational data. Flow instances and user-specific data reside in the user‚Äôs dedicated AWS account, stored in a database. Each user‚Äôs data and workflow executions are isolated within their respective AWS accounts, which is required for complying with Thomson Reuters data sovereignty and enterprise security policies with strict regional controls. The system integrates with Thomson Reuters SSO solution to automatically identify users and grant secure, private access to foundational models. 
The orchestration layer, centrally hosted within the Enterprise AI Platform AWS account, manages AI workflow activities, including scheduling, deployment, resource provisioning, and governance across user environments. 
The system features fully automated provisioning of&nbsp; Amazon Bedrock Flows directly within each user‚Äôs AWS account, avoiding manual setup and accelerating time to value. Using AWS Lambda for serverless compute and DynamoDB for scalable, low-latency storage, the system dynamically allocates resources based on real-time demand. This architecture makes sure prompt flows and supporting infrastructure are deployed and scaled to match workload fluctuations, optimizing performance, cost, and user experience. 

 ‚ÄúOur decision to adopt a cross-account architecture was driven by a commitment to enterprise security and operational excellence. By isolating orchestration from execution, we make sure that each user‚Äôs data remains private and secure within their own AWS account, while still delivering a seamless, centrally-managed experience. This design empowers organizations to innovate rapidly without compromising compliance or control.‚Äù 
 ‚Äì Thomson Reuters‚Äô architecture team
 
Evolution of Open Arena: From classic to Amazon Bedrock Flows-powered chain builder 
Open Arena has evolved to cater to varying levels of user sophistication: 
 
 Open Arena v1 (Classic):&nbsp;Features a form-based interface for simple prompt customization and basic AI workflow deployment within a single AWS account. Its simplicity appeals to novice users for straightforward use cases, though with limited advanced capabilities. 
 Open Arena v2 (Chain Builder):&nbsp;Introduces a robust, visual workflow builder interface, enabling users to design complex, multi-step AI workflows using drag-and-drop components. With support for advanced node types, parallel execution, and seamless cross-account deployment, Chain Builder dramatically expands the system‚Äôs capabilities and accessibility for non-technical users. 
 
Thomson Reuters uses Amazon Bedrock Flows as a core feature of Chain Builder. Users can define, customize, and deploy AI-driven workflows using Amazon Bedrock models. Bedrock Flows supports advanced workflows combining multiple prompt nodes, incorporating AWS Lambda functions, and supporting sophisticated RAG pipelines. Operating seamlessly across user AWS accounts, Bedrock Flows facilitates secure, scalable execution of personalized AI solutions, serving as the fundamental engine for the Chain Builder workflows and driving TR‚Äôs ability to deliver robust, enterprise-grade automation and innovation. 
What‚Äôs next? 
TR continues to expand Open Arena‚Äôs capabilities through the strategic partnership with AWS, focusing on: 
 
 Driving further adoption of Open Arena‚Äôs DIY capabilities. 
 Enhancing flexibility for workflow creation in Chain Builder with custom components, such as inline scripts. 
 Developing new templates to represent common tasks and workflows. 
 Enhancing collaboration features within Open Arena. 
 Extending multimodal capabilities and model integration. 
 Expanding into new use cases across the enterprise. 
 

 ‚ÄúFrom innovating new product ideas to reimagining daily tasks for Thomson Reuters employees, we continue to push the boundaries of what‚Äôs possible with Open Arena.‚Äù 
 ‚Äì Maria Apazoglou, Head of AI Engineering, CoCounsel
 
Conclusion 
In this blog post, we explored how Thomson Reuters‚Äô Open Arena demonstrates the successful democratization of AI across an enterprise by using AWS services, particularly Amazon Bedrock and Bedrock Flows. With 19,000 monthly active users and 70% employee adoption, the system proves that no-code AI solutions can deliver enterprise-scale impact while maintaining security and governance standards. 
By combining the robust infrastructure of AWS with innovative architecture design, TR has created a blueprint for AI democratization that empowers professionals across technical skill levels to harness generative AI for their daily work. 
As Open Arena continues to evolve, it exemplifies how strategic cloud partnerships can accelerate AI adoption and transform how organizations approach innovation with generative AI. 
 
About the authors 
Laura Skylaki, PhD, leads the Enterprise AI Platform at Thomson Reuters, driving the development of GenAI services that accelerate the creation, testing and deployment of AI solutions, enhancing product value. A recognized expert with a doctorate in stem cell bioinformatics, her extensive experience in AI research and practical application spans legal, tax, and biotech domains. Her machine learning work is published in leading academic journals, and she is a frequent speaker on AI and machine learning 
Vaibhav Goswami is a Lead Software Engineer on the AI Platform team at Thomson Reuters, where he leads the development of the Generative AI Platform that empowers users to build and deploy generative AI solutions at scale. With expertise in building production-grade AI systems, he focuses on creating tools and infrastructure that democratize access to cutting-edge AI capabilities across the enterprise. 
Ramdev Wudali is a Distinguished Engineer, helping architect and build the AI/ML Platform to enable the Enterprise user, data scientists and researchers to develop Generative AI and machine learning solutions by democratizing access to tools and LLMs. In his spare time, he loves to fold paper to create origami tessellations, and wearing irreverent T-shirts 
As the director of AI Platform Adoption and Training, Sahar El Khoury guides users to seamlessly onboard and successfully use the platform services, drawing on her experience in AI and data analysis across robotics (PhD), financial markets, and media. 
Vu San Ha Huynh is a Solutions Architect at AWS with a PhD in Computer Science. He helps large Enterprise customers drive innovation across different domains with a focus on AI/ML and Generative AI solutions. 
Paul Wright is a Senior Technical Account Manager, with over 20 years experience in the IT industry and over 7 years of dedicated cloud focus. Paul has helped some of the largest enterprise customers grow their business and improve their operational excellence. In his spare time Paul is a huge football and NFL fan. 
Mike Bezak is a Senior Technical Account Manager in AWS Enterprise Support. He has over 20 years of experience in information technology, primarily disaster recovery and systems administration. Mike‚Äôs current focus is helping customers streamline and optimize their AWS Cloud journey. Outside of AWS, Mike enjoys spending time with family &amp; friends.
‚Ä¢ Introducing structured output for Custom Model Import in Amazon Bedrock
  With Amazon Bedrock Custom Model Import, you can deploy and scale fine-tuned or proprietary foundation models in a fully managed, serverless environment. You can bring your own models into Amazon Bedrock, scale them securely without managing infrastructure, and integrate them with other Amazon Bedrock capabilities. 
Today, we are excited to announce the addition of structured output to Custom Model Import. Structured output constrains a model‚Äôs generation process in real time so that every token it produces conforms to a schema you define. Rather than relying on prompt-engineering tricks or brittle post-processing scripts, you can now generate structured outputs directly at inference time. 
For certain production applications, the predictability of model outputs is more important than their creative flexibility. A customer service chatbot might benefit from varied, natural-sounding responses, but an order processing system needs exact, structured data that conforms to predefined schemas. Structured output bridges this gap by maintaining the intelligence of foundation models while verifying their outputs meet strict formatting requirements. 
This represents a shift from free-form text generation to outputs that are consistent, machine-readable, and designed for seamless integration with enterprise systems. While free-form text excels for human consumption, production applications require more precision. Businesses can‚Äôt afford the ambiguity of natural language variations when their systems depend on structured outputs to reliably interface with APIs, databases, and automated workflows. 
In this post, you will learn how to implement structured output for Custom Model Import in Amazon Bedrock. We will cover what structured output is, how to enable it in your API calls, and how to apply it to real-world scenarios that require structured, predictable outputs. 
Understanding structured output 
Structured output, also known as constrained decoding, is a method that directs LLM outputs to conform to a predefined schema, such as valid JSON. Rather than allowing the model to freely select tokens based on probability distributions, it introduces constraints during generation that limit choices to only those that maintain structural validity. If a particular token would violate the schema by producing invalid JSON, inserting stray characters, or using an unexpected field name the structured output rejects it and requires the model to select another allowed option. This real-time validation helps keep the final output consistent, machine readable, and immediately usable by downstream applications without the need for additional post-processing. 
Without structured output, developers often attempt to enforce structure through prompt instructions like ‚ÄúRespond only in JSON.‚Äù While this approach sometimes works, it remains unreliable due to the inherently probabilistic nature of LLMs. These models generate text by sampling from probability distributions, introducing natural variability that makes responses feel human but creates significant challenges for automated systems. 
Consider a customer support application that classifies tickets: if responses vary between ‚ÄúThis seems like a billing issue,‚Äù ‚ÄúI‚Äôd classify this as: Billing,‚Äù and ‚ÄúCategory = BILLING,‚Äù downstream code cannot reliably interpret the results. What production systems require instead is predictable, structured output. For example: 
 
 {
  "category": "billing",
  "priority": "high",
  "sentiment": "negative"
}
 
 
With a response like this, your application can automatically route tickets, trigger workflows, or update databases without human intervention. By providing predictable, schema-aligned responses, structured output transforms LLMs from conversational tools into reliable system components that can be integrated with databases, APIs, and business logic. This capability opens new possibilities for automation while maintaining the intelligent reasoning that underpin the value of these models. 
Beyond improving reliability and simplifying post-processing, structured output offers additional benefits that strengthens performance, security and safety in production environments. 
 
 Lower token usage and faster responses: By constraining generation to a defined schema, structured output removes unnecessary verbose, free-form text, resulting in reduced token count. Because token generation is sequential, shorter outputs directly translate to faster responses and lower latency, improving overall performance and cost efficiency. 
 Enhanced security against prompt injection: Structured output narrows the model‚Äôs expression space and helps prevent it from producing arbitrary or unsafe content. Bad actors cannot inject instructions, code or unexpected text outside the defined structure. Each field must match its expected type and format, making sure outputs remain within safe boundaries. 
 Safety and policy controls: Structured output enables you to design schemas that inherently help prevent harmful, toxic, or policy-violating content. By limiting fields to approved values, enforcing patterns, and restricting free-form text, schemas make sure outputs align with regulatory requirements. 
 
In the next section, we will explore how structured output works with Custom Model Import in Amazon Bedrock and walks through an example of enabling it in your API calls. 
Using structured output with Custom Model Import in Amazon Bedrock 
Let‚Äôs start by assuming you have already imported a Hugging Face model into Amazon Bedrock using the Custom Model Import feature. 
Prerequisites 
Before proceeding, make sure you have: 
 
 An active AWS account with access to Amazon Bedrock 
 A custom model created in Amazon Bedrock using the Custom Model Import feature 
 Appropriate AWS Identity and Access Management (IAM) permissions to invoke models through the Amazon Bedrock Runtime 
 
With these prerequisites in place, let‚Äôs explore how to implement structured output with your imported model. 
To start using structured output with a Custom Model Import in Amazon Bedrock, begin by configuring your environment. In Python, this involves creating a Bedrock Runtime client and initializing a tokenizer from your imported Hugging Face model. 
The Bedrock Runtime client provides access to your imported model using the Bedrock InvokeModel API. The tokenizer applies the correct chat template that aligns with the imported model, which defines how user, system, and assistant messages are combined into a single prompt, how the role markers (for example, &lt;|user|&gt;, &lt;|assistant|&gt;) are inserted, and where the model‚Äôs response should begin. 
By calling tokenizer.apply_chat_template(messages, tokenize=False) you can generate a prompt that matches the exact input format your model expects, which is essential for consistent and reliable inference, especially when structured encoding is enabled. 
 
 import boto3
from transformers import AutoTokenizer
from botocore.config import Config

# HF model identifier imported into Bedrock
hf_model_id = "&lt;&lt;huggingface_model_id&gt;&gt;" # Example: "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B
model_arn = "arn:aws:bedrock:&lt;&lt;aws-region&gt;&gt;:&lt;&lt;account-id&gt;&gt;:imported-model/your-model-id"
region      = "&lt;&lt;aws-region&gt;&gt;"

# Initialize tokenizer aligned with your imported model 
tokenizer = AutoTokenizer.from_pretrained(hf_model_id)

# Initialize Bedrock client
bedrock_runtime = boto3.client(
    service_name="bedrock-runtime",
    region_name=region) 
 
Implementing structured output 
When you invoke a custom model on Amazon‚ÄØBedrock, you have the option to enable structured output by adding a response_format block to the request payload. This block accepts a JSON schema that defines the structured of the model‚Äôs response. During inference, the model enforces this schema in real-time, making sure that each generated token conforms to the defined structure. Below is a walkthrough demonstrating how to implement structured output using a simple address extraction task. 
Step 1: Define the data structure 
You can define your expected output using a Pydantic model, which serves as a typed contract for the data you want to extract. 
 
 from pydantic import BaseModel, Field

class Address(BaseModel):
    street_number: str = Field(description="Street number")
    street_name: str = Field(description="Street name including type (Ave, St, Rd, etc.)")
    city: str = Field(description="City name")
    state: str = Field(description="Two-letter state abbreviation")
    zip_code: str = Field(description="5-digit ZIP code") 
 
Step 2: Generate the JSON schema 
Pydantic can automatically convert your data model into a JSON schema: 
 
 schema = Address.model_json_schema()
address_schema = {
    "name": "Address",
    "schema": schema
} 
 
This schema defines each field‚Äôs type, description, and requirement, creating a blueprint that the model will follow during generation. 
Step 3: Prepare your input messages 
Format your input using the chat format expected by your model: 
 
 messages = [{
    "role": "user",
    "content": "Extract the address: 456 Tech Boulevard, San Francisco, CA 94105"
}] 
 
Step 4: Apply the chat template 
Use your model‚Äôs tokenizer to generate the formatted prompt: 
 
 prompt = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
) 
 
Step 5: Build the request payload 
Create your request body, including the response_format that references your schema: 
 
 request_body = {
    'prompt': prompt,
    'temperature': 0.1,
    'max_gen_len': 1000,
    'top_p': 0.9,
    'response_format': {
        "type": "json_schema",
        "json_schema": address_schema
    }
} 
 
Step 6: Invoke the model 
Send the request using the InvokeModel API: 
 
 response = bedrock_runtime.invoke_model(
    modelId=model_arn,
    body=json.dumps(request_body),
    accept="application/json",
    contentType="application/json"
) 
 
Step 7: Parse the response 
Extract the generated text from the response: 
 
 result = json.loads(response['body'].read().decode('utf-8'))
raw_output = result['choices'][0]['text']
print(raw_output) 
 
Because the schema defines required fields, the model‚Äôs response will contain them: 
 
 {
"street_number": "456",
"street_name": "Tech Boulevard",
"city": "San Francisco",
"state": "CA",
"zip_code": "94105"
} 
 
The output is clean, valid JSON that can be consumed directly by your application with no extra parsing, filtering, or cleanup required. 
Conclusion 
Structured output with Custom Model Import in Amazon Bedrock provides an effective way to generate structures, schema-aligned outputs from your models. By shifting validation into the model inference itself, structured output reduce the need for complex post-processing workflows and error handling code. 
Structured output generates outputs that are predictable and straightforward to integrate into your systems and supports a variety of use cases, for example, building financial applications that require precise data extraction, healthcare systems that need structured clinical documentation, or customer service systems that demand consistent ticket classification. 
Start experimenting with structured output with your Custom Model Import today and transform how your AI applications deliver consistent, production-ready results. 
 
About the authors 
Manoj Selvakumar is a Generative AI Specialist Solutions Architect at AWS, where he helps organizations design, prototype, and scale AI-powered solutions in the cloud. With expertise in deep learning, scalable cloud-native systems, and multi-agent orchestration, he focuses on turning emerging innovations into production-ready architectures that drive measurable business value. He is passionate about making complex AI concepts practical and enabling customers to innovate responsibly at scale‚Äîfrom early experimentation to enterprise deployment. Before joining AWS, Manoj worked in consulting, delivering data science and AI solutions for enterprise clients, building end-to-end machine learning systems supported by strong MLOps practices for training, deployment, and monitoring in production. 
Yanyan Zhang&nbsp;is a Senior Generative AI Data Scientist at Amazon Web Services, where she has been working on cutting-edge AI/ML technologies as a Generative AI Specialist, helping customers use generative AI to achieve their desired outcomes. Yanyan graduated from Texas A&amp;M University with a PhD in Electrical Engineering. Outside of work, she loves traveling, working out, and exploring new things. 
Lokeshwaran Ravi is a Senior Deep Learning Compiler Engineer at AWS, specializing in ML optimization, model acceleration, and AI security. He focuses on enhancing efficiency, reducing costs, and building secure ecosystems to democratize AI technologies, making cutting-edge ML accessible and impactful across industries. 
Revendra Kumar is a Senior Software Development Engineer at Amazon Web Services. In his current role, he focuses on model hosting and inference MLOps on Amazon Bedrock. Prior to this, he worked as an engineer on hosting Quantum computers on the cloud and developing infrastructure solutions for on-premises cloud environments. Outside of his professional pursuits, Revendra enjoys staying active by playing tennis and hiking. 
Muzart Tuman is a software engineer utilizing his experience in fields like deep learning, machine learning optimization, and AI-driven applications to help solve real-world problems in a scalable, efficient, and accessible manner. His goal is to create impactful tools that not only advance technical capabilities but also inspire meaningful change across industries and communities.

‚∏ª