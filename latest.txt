‚úÖ Morning News Briefing ‚Äì July 24, 2025 10:50

üìÖ Date: 2025-07-24 10:50
üè∑Ô∏è Tags: #briefing #ai #publichealth #digitalgov

‚∏ª

üßæ Weather
‚Ä¢ Current Conditions:  16.1¬∞C
  Temperature: 16.1&deg;C Pressure / Tendency: 101.4 kPa falling Humidity: 86 % Dewpoint: 13.7&deg:C Wind: SE 7 km/h . Air Quality Health Index: n/a . Pembroke 6:00 AM EDT Thursday 24 July 2025 . Weather: Pem Broke 6,000 km/
‚Ä¢ Thursday: Mainly cloudy. High 33.
  Mainly cloudy with a high of 33 degrees Celsius . Wind becoming southwest 20 km/h this afternoon . Humidex 40. UV index 8 or very high with a very high UV index of 8 in the morning's UVB or high risk of UVB . Forecast issued 5:00 AM EDT Thursday 24 July 2025. Forecast also issued for Thursday, July 24 July 25
‚Ä¢ Thursday night: Chance of showers. Low 20. POP 70%
  Cloudy with 70 percent chance of showers and risk of thunderstorm . Wind west 20 km/h becoming light late this evening . Low 20.50C is expected to be sunny and breezy in the morning . Forecast issued 5:00 AM EDT Thursday 24 July 2025 . Weather will be mainly cloudy with rain and thundery showers throughout the night, with gusts of up to

üåç International News
No updates.

üçÅ Canadian News
No updates.

üá∫üá∏ U.S. Top Stories
‚Ä¢ Scientists are developing artificial blood that could save lives in emergencies
  A research team has successfully tested a blood substitute in animals, and human trials may not be far off . The powdered blood could help medics respond faster in a crisis, and could help them respond faster . A powdered blood substitute has been tested in animals and could be ready for human trials soon . It could be used in emergency situations, but could be a long way from human trials .
‚Ä¢ Trump's EPA now says greenhouse gases don't endanger people
  The Trump administration wants to reverse a 2009 EPA finding that greenhouse gases endanger people . The finding is the basis for much of the United States' climate change regulations . The Obama administration wants the EPA to reverse the 2009 finding . The EPA found greenhouse gas emissions endanger people, but the EPA says it does not endanger the environment. The EPA says greenhouse gases are responsible for climate change in the United
‚Ä¢ You can't outrun a bad diet. Food not lack of exercise fuels obesity, study finds
  People burn fewer calories than people in countries where obesity is rare, study finds . A major study finds that's not the case . People burn less calories in industrialized countries than in countries with obesity rare . Study: People burn more calories than they burn calories in countries like Norway, Sweden and Germany . People in industrialized nations burn fewer calorie burners than in industrialized ones, according to study .
‚Ä¢ Cooking with kids is messy. This dad chef wants you to do it anyway
  Chef David Nayfeld has been cooking with his kid since she was 2 . He shares easy ways to involve kids in meal prep with his new book, Dad, What's for Dinner? He shares a weeknight recipe for meatballs and a recipe called "stressed-out weekday pancakes" with his daughter, Helena . Nayfeld: "Stressed out weekday pancakes is a recipe for
‚Ä¢ State Dept. cuts China experts as administration says countering Beijing top priority
  The State Department shuttered the team involved in South China Sea security, getting rid of top experts on the subject . The move comes at a time when the administration says security in the region is a priority . State Department has shuttered a team of experts in the area . The team includes experts from the U.S. State Department and China's Guangdong province, Guangzhou province

üß† Artificial Intelligence
No updates.

üíª Digital Strategy
‚Ä¢ 50 years ago, Gates and Allen made the deal that launched Microsoft
  This week marked the 50th anniversary of the birth of several empires . Bill Gates and Paul Allen signed a deal with Micro Instrumentation and Telemetry Systems on July 22, 1975 . The Altair 8800, a $264 RAM board, and some BASIC changed the world . It was the first version of the MITS Altair, the first of its kind, and the first
‚Ä¢ Not pretty, not Windows-only: npm phishing attack laces popular packages with malware
  The "is" package was infected with cross-platform malware after a scam targeting maintainers . Package was infected around the same time that linting utility packages used with the prettier code formatter were infected with Windows-only malware . The package was targeted by a scam targeting the maintainers of the popular npm package "is," which is a formatter package for code form
‚Ä¢ EU cloud gang challenges Broadcom's $61B VMWare buy in court
  Cloud Infrastructure Services Providers in Europe (CISPE) has filed a formal appeal before the European General Court to seek annulment of the European Commission's decision to approve Broadcom‚Äôs acquisition of VMware . CISPE cites recent channel changes, but the deal was decided on different matters . The European Commission approved Broadcom's acquisition ofVMVMVM, but CISPE says
‚Ä¢ The tiny tech tribe who could change the world tomorrow but won't
  Ten people in the world could decide tomorrow to make IT better, and it would become better . Not better for some, not better for a while, but better for all and forever . Sometimes, one small tweak can make a very big difference, says Simon Tisdall . Ten people could change IT in ten years, he says, and IT would be better for everyone and better for
‚Ä¢ Google just spent $14 billion on servers in 91 days, plans even higher spending soon
  Alphabet has increased its capex budget for the year by $10 billion . It now expects to spend $85 billion this year, and more in 2026 . G-Cloud on track for $50 billion revenue as AI creates a new generation of Google-eyed youth . Google‚Äôs parent company Alphabet is expected to spend more than $100 billion in the next five years . Google

üè• Public Health
No updates.

üî¨ Science
‚Ä¢ Impact of the COVID-19 pandemic on incident diagnoses in German refugee centres 2018 to 2023
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Comparative efficacy of topical interventions for facial photoaging: a network meta-analysis
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Past, current status, and future trends of the rural healthcare network in the Republic of Kazakhstan
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Maximizing cascade genetic testing for disease prevention through direct notification of at-risk relatives
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Protect the integrity of the US National Institutes of Health
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations

üßæ Government & Policy
No updates.

üèõÔ∏è Enterprise Architecture & IT Governance
No updates.

ü§ñ AI & Emerging Tech
‚Ä¢ What role should oil and gas companies play in climate tech?
  This week, I have a new story out about Quaise, a geothermal startup that‚Äôs trying to commercialize new drilling technology. Using a device called a gyrotron, the company wants to drill deeper, cheaper, in an effort to unlock geothermal power anywhere on the planet. (For all the details, check it out here.)¬†



For the story, I visited Quaise‚Äôs headquarters in Houston. I also took a trip across town to Nabors Industries, Quaise‚Äôs investor and tech partner and one of the biggest drilling companies in the world.&nbsp;





Standing on top of a drilling rig in the backyard of Nabors‚Äôs headquarters, I couldn‚Äôt stop thinking about the role oil and gas companies are playing in the energy transition. This industry has resources and energy expertise‚Äîbut also a vested interest in fossil fuels. Can it really be part of addressing climate change?



The relationship between Quaise and Nabors is one that we see increasingly often in climate tech‚Äîa startup partnering up with an established company in a similar field. (Another one that comes to mind is in the cement industry, where Sublime Systems has seen a lot of support from legacy players including Holcim, one of the biggest cement companies in the world.)¬†



Quaise got an early investment from Nabors in 2021, to the tune of $12 million. Now the company also serves as a technical partner for the startup.¬†



‚ÄúWe are agnostic to what hole we‚Äôre drilling,‚Äù says Cameron Maresh, a project engineer on the energy transition team at Nabors Industries. The company is working on other investments and projects in the geothermal industry, Maresh says, and the work with Quaise is the culmination of a yearslong collaboration: ‚ÄúWe‚Äôre just truly excited to see what Quaise can do.‚Äù



From the outside, this sort of partnership makes a lot of sense for Quaise. It gets resources and expertise. Meanwhile, Nabors is getting involved with an innovative company that could represent a new direction for geothermal. And maybe more to the point, if fossil fuels are to be phased out, this deal gives the company a stake in next-generation energy production.



There is so much potential for oil and gas companies to play a productive role in addressing climate change. One report from the International Energy Agency examined the role these legacy players could take: ¬†‚ÄúEnergy transitions can happen without the engagement of the oil and gas industry, but the journey to net zero will be more costly and difficult to navigate if they are not on board,‚Äù the authors wrote.¬†



In the agency‚Äôs blueprint for what a net-zero emissions energy system could look like in 2050, about 30% of energy could come from sources where the oil and gas industry‚Äôs knowledge and resources are useful. That includes hydrogen, liquid biofuels, biomethane, carbon capture, and geothermal.&nbsp;



But so far, the industry has hardly lived up to its potential as a positive force for the climate. Also in that report, the IEA pointed out that oil and gas producers made up only about 1% of global investment in climate tech in 2022. Investment has ticked up a bit since then, but still, it‚Äôs tough to argue that the industry is committed.¬†





And now that climate tech is falling out of fashion with the government in the US, I‚Äôd venture to guess that we‚Äôre going to see oil and gas companies increasingly pulling back on their investments and promises.&nbsp;



BP recently backtracked on previous commitments to cut oil and gas production and invest in clean energy. And last year the company announced that it had written off $1.1 billion in offshore wind investments in 2023 and wanted to sell other wind assets. Shell closed down all its hydrogen fueling stations for vehicles in California last year. (This might not be all that big a loss, since EVs are beating hydrogen by a huge margin in the US, but it‚Äôs still worth noting.)¬†



So oil and gas companies are investing what amounts to pennies and often backtrack when the political winds change direction. And, let‚Äôs not forget, fossil-fuel companies have a long history of behaving badly.&nbsp;



In perhaps the most notorious example, scientists at Exxon modeled climate change in the 1970s, and their forecasts turned out to be quite accurate. Rather than publish that research, the company downplayed how climate change might affect the planet. (For what it‚Äôs worth, company representatives have argued that this was less of a coverup and more of an internal discussion that wasn‚Äôt fit to be shared outside the company.)¬†



While fossil fuels are still part of our near-term future, oil and gas companies, and particularly producers, would need to make drastic changes to align with climate goals‚Äîchanges that wouldn‚Äôt be in their financial interest. Few seem inclined to really take the turn needed.&nbsp;



As the IEA report puts it: &nbsp;‚ÄúIn practice, no one committed to change should wait for someone else to move first.‚Äù



This article is from The Spark, MIT Technology Review‚Äôs weekly climate newsletter. To receive it in your inbox every Wednesday, sign up here.
‚Ä¢ Google DeepMind‚Äôs new AI can help historians understand ancient Latin inscriptions
  Google DeepMind unveils Aeneas, an artificial-intelligence tool that analyzes ancient Latin engravings . It can analyze words written in long-weathered stone to say when and where they were originally inscribed . It follows Google‚Äôs previous archaeological tool Ithaca, which also used deep learning to reconstruct and contextualize ancient text, in its case Greek . The system is freely available for teachers, museum workers, academics and academics .
‚Ä¢ The Download: what‚Äôs next for AI agents, and how Trump protects US tech companies overseas
  This is today&#8217;s edition of¬†The Download,¬†our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



Navigating the rise of AI agents



AI agents is a buzzy term that essentially refers to AI models and algorithms that can not only provide you with information, but take actions on your behalf. Companies like OpenAI and Anthropic have launched ‚Äòagentic‚Äô products that can do things for you like making bookings, filling in forms, and collaborating with you on coding projects.¬†



On a LinkedIn Live event yesterday our editor-in-chief Mat Honan, senior editor for AI Will Douglas Heaven, and senior AI reporter Grace Huckins discussed what‚Äôs exciting about agents and where the technology will go next, but also its limitations, and the risks that currently come with adopting it. Check out what they had to say!



And if you‚Äôre interested in learning more about AI agents, read our stories:



+ Are we ready to hand AI agents the keys? We‚Äôre starting to give AI agents real autonomy, and we‚Äôre not prepared for what could happen next. Read the full story.+ Anthropic‚Äôs chief scientist on 4 ways agents will get even better. Read the full story.+ Cyberattacks by AI agents are coming. Agents could make it easier and cheaper for criminals to hack systems at scale. We need to be ready.+ When AIs bargain, a less advanced agent could cost you. In AI-to-AI price negotiations, weaker models often lose out‚Äîcosting users real money and raising concerns about growing digital inequality. Read the full story.+ There‚Äôs been huge hype about a new general AI agent from China called Manus. We put it to the test.&nbsp;







The must-reads



I‚Äôve combed the internet to find you today‚Äôs most fun/important/scary/fascinating stories about technology.



1 The Trump administration is seeking to protect US tech firms abroad¬†It‚Äôs using its global trade wars as a way to prevent other countries from imposing new taxes, regulations and tariffs on American tech companies. (WSJ $)+ Tech firms are increasingly trying to shape US AI policy. (FT $)



2 UK border officials plan to use AI to assess child asylum seekers¬†A pilot scheme will estimate the age of new arrivals to the country.¬† (The Guardian)+ US border patrol is arresting immigrants nowhere near the US-Mexico border.¬† (WP $)+ The US wants to use facial recognition to identify migrant children as they age. (MIT Technology Review) 3 AI is hitting web traffic hardGoogle‚Äôs AI Overviews are causing a massive drop in clicks to actual websites. (Ars Technica)+ It‚Äôs good news for Google, bad news for everyone else. (The Register)+ AI means the end of internet search as we‚Äôve known it. (MIT Technology Review)



4 Dozens of Iranians‚Äô iPhones have been targeted with government spyware¬†But the actual total number of targets is likely to be far higher. (Bloomberg $)5 Amazon is shutting down its AI lab in ShanghaiIt‚Äôs the latest in a line of US tech giants to scale back their research in the country. (FT $)



6 Californian billionaires have set their sights on building an industrial parkAfter their plans to create a brand new city didn‚Äôt get off the ground. (Gizmodo)



7 Tesla‚Äôs robotaxi launch didn‚Äôt quite go to planProspective customers appear to be a bit freaked out. (Wired $)+ Ride-hailing companies aren‚Äôt meeting their EV adoption targets. (Rest of World)



8 Why AI slop could finally help us to log offIf AI garbage renders a lot of the web unusable, it could be our only option. (The Atlantic $)+ How to fix the internet. (MIT Technology Review)



9 You may regrow your own teeth in the future The age of dentures and implants could be nearly over. (New Scientist $)+ Humanlike ‚Äúteeth‚Äù have been grown in mini pigs. (MIT Technology Review)



10 Inside one man‚Äôs hunt for an elusive Chinese typewriterIt made it possible to type tens of thousands of characters using just 72 keys. (NYT $)+ How the quest to type Chinese on a QWERTY keyboard created autocomplete. (MIT Technology Review)







Quote of the day



‚ÄúThe truth is, China‚Äôs really doing ‚Äò007‚Äô now‚Äîmidnight to midnight, seven days a week.‚Äù



‚ÄîVenture capitalist Harry Stebbings explains how Chinese startups have moved from ‚Äò996‚Äô work schedules (9am to 9pm, six days a week) to a routine that‚Äôs even more punishing, Wired reports.







One more thing







Inside a new quest to save the ‚Äúdoomsday glacier‚Äù



The Thwaites glacier is a fortress larger than Florida, a wall of ice that reaches nearly 4,000 feet above the bedrock of West Antarctica, guarding the low-lying ice sheet behind it.



But a strong, warm ocean current is weakening its foundations and accelerating its slide into the sea. Scientists fear the waters could topple the walls in the coming decades, kick-starting a runaway process that would crack up the West Antarctic Ice Sheet, marking the start of a global climate disaster. As a result, they are eager to understand just how likely such a collapse is, when it could happen, and if we have the power to stop it.&nbsp;



Scientists at MIT and Dartmouth College founded the Ar√™te Glacier Initiative last year in the hope of providing clearer answers to these questions. The nonprofit research organization will officially unveil itself, launch its website, and post requests for research proposals today, timed to coincide with the UN‚Äôs inaugural World Day for Glaciers, MIT Technology Review can report exclusively. Read the full story.



‚ÄîJames Temple







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)+ A fun-looking major retrospect of David Bailey‚Äôs starry career is opening in Spain.+ Creepy new horror flick Weapons is getting rave reviews.+ This amazing website takes you through Apollo 11‚Äôs first landing on the moon in real time.+ Rest in power Ozzy Osbourne, the first ever heavy metal frontman, and the undisputed Prince of Darkness.
‚Ä¢ Fighting forever chemicals and startup fatigue
  What if we could permanently remove the toxic ‚Äúforever chemicals‚Äù contaminating our water? That‚Äôs the driving force behind Michigan-based startup Enspired Solutions, founded by environmental toxicologist Denise Kay and chemical engineer Meng Wang. The duo left corporate consulting in the rearview mirror to take on one of the most pervasive environmental challenges: PFAS.







&#8220;PFAS is referred to as a forever chemical because it is so resistant to break down,‚Äù says Kay. ‚ÄúIt does not break down naturally in the environment, so it just circles around and around. This chemistry, which would break that cycle and break the molecule apart, could really support the health of all of us.‚Äù



Basing the company in Michigan was both a strategic and a practical strategy. The state has been a leader in PFAS regulation with a startup infrastructure‚Äîbuoyed by the Michigan Economic Development Corporation (MEDC)‚Äîthat helped turn an ambitious vision into a viable business.



From intellectual property analyses to forecasting finances and fundraising guidance, the MEDC‚Äôs programs offered Kay and Wang the resources to focus on building their PFASigator: a machine the size of two large refrigerators that uses ultraviolet light and chemistry to break down PFAS in water. In other words, ‚Äúit essentially eats PFAS.‚Äù



Despite the support from the MEDC, the journey has been far from smooth. &#8220;As people say, being an entrepreneur and running a startup is like a rollercoaster,‚Äù Kay says. ‚ÄúYou have high moments, and you have very low moments when you think nothing&#8217;s ever going to move forward.&#8221;



Without revenue or salaries in the early days, the co-founders had to be sustained by something greater than financial incentive.



&#8220;If problem solving and learning new talents do not provide sufficient intrinsic reward for a founder to be satisfied throughout what I guarantee will be a long duration effort, then that founder may need to reset their expectations. Because the financial rewards of entrepreneurship are small throughout the process.&#8221;



Still, Kay remains optimistic about the road ahead for Enspired Solutions, for clean water innovation, and for other founders walking down a similar path. &#8220;Often, founders are coached about formulas for fundraising, formulas for startup success. Learning those formulas and expectations is important, but it&#8217;s also important to not forget that it&#8217;s your creativity and innovation and foresight that got you to the place you&#8217;re in and drove you to start a company. Ultimately, people still want to see that shine through.&#8221;



This episode of Business Lab is produced in partnership with the Michigan Economic Development Corporation.



Full Transcript



Megan Tatum: From MIT Technology Review, I&#8217;m Megan Tatum. This is Business Lab, the show that helps business leaders make sense of new technologies coming out of the lab and into the marketplace.



Today&#8217;s episode is brought to you in partnership with the Michigan Economic Development Corporation.Our topic today is launching a technology startup in the US state of Michigan. Building out an innovative idea into a viable product and company requires knowledge and resources that individuals might not have. That&#8217;s why the Michigan Economic Development Corporation, or the MEDC, has launched an innovation campaign to support technology entrepreneurs.Two words for you: startup ecosystem.My guest is Dr. Denise Kay, the co-founder and CEO at Enspired Solutions, a Michigan-based startup focused on removing synthetic forever chemicals called PFAS from water.Welcome, Denise.



Dr. Denise Kay: Hi, Megan.



Megan: Hi. Thank you so much for joining us. To get us started, Denise, I wondered if we could talk about Enspired Solutions a bit more. How did the idea come about, and what does your company do?



Denise: Well, my co-founder, Meng, and I had careers in consulting, advising clients on the fate and toxicity of chemicals in the environment. What we did was evaluate how chemicals moved through soil, water, and air, and what toxic impact they might have on humans and wildlife. That put us in a really unique position to see early on¬†the environmental and health ramifications of the manmade chemical PFAS in our environment.



When we learned of a very novel and elegant chemistry that could effectively destroy PFAS, we could foresee the value in making this chemistry available for commercial use and the potential for a significant positive impact on maintaining healthy water resources for all of us.Like you mentioned, PFAS is referred to as a forever chemical because it is so resistant to break down. It does not break down naturally in the environment, so it just circles around and around. This chemistry, which would break that cycle and break the molecule apart, could really support the health of all of us.Ultimately, Meng and I quit our jobs, and we founded Enspired Solutions. Our objective was to design, manufacture, and sell commercial-scale equipment that destroys PFAS in water based on this laboratory bench-scale chemistry that had been discovered, the goal being that this toxic contaminant does not continue to circulate in our natural resources.At this point, we have won an award from the EPA and Department of Defense, and proven our technology in over 200 different water samples ranging from groundwater, surface water, landfill leachate, industrial wastewater, [and] municipal wastewater. It&#8217;s really everywhere. What we&#8217;re seeing traction in right now is customer applications managing semiconductor waste. Groundwater and surface water around airports tend to be high in PFAS. Centralized waste disposal facilities that collect and manage PFAS-contaminated liquids. And also, even transitioning firetrucks to PFAS-free firefighting foams.



Megan: Fantastic. That&#8217;s a huge breadth of applications, incredible stuff.



Denise: Yeah.



Megan: You launched about four years ago now. I wondered what factors made Michigan the right place to build and grow the company?



Denise: That is something we put a lot of thought into, because I live in Michigan, and Meng lives in Illinois, so when it was just the two of us, there was even that, &#8220;Okay, what is going to be our headquarters?&#8221; We looked at a number of factors.



Some of the things we considered were rentable incubator space. By incubator, I mean startup incubators or innovation centers. The startup support network, a pool of future employees, and what position the state agencies were taking regarding PFAS.While thinking about all those things and investigating our communities, in Michigan, we found a space to rent where we could do chemistry experiments in an incubator environment. Somewhere where we were surrounded by other entrepreneurs, which we knew was something we had to learn how to do. We were great chemists, but we knew that surrounding ourselves with those skills that could be a gap for us was going to be helpful.Also, we know that Michigan has moved much faster than other states in identifying PFAS sources in the environment and regulating its presence. This combination was something we knew would be the right place for starting our business and having success.



Megan: It was a perfect setting for those two reasons. What were the first stages of your journey working with the Michigan Economic Development Corporation, the MEDC?



Denise: Well, both my co-founder, Meng, and I are first-time entrepreneurs. MEDC was one of the first resources I reached out to, starting from a Google search. They were an information resource we turned to initially, and then again and again for learning some fundamental skills. And receiving one-on-one expert mentorship for things like business contracts, understanding intellectual property landscapes, tracking and forecasting our business finances, and even how to approach fundraising.



Megan: Wow. It sounds like they were an invaluable resource in those early days. How did early-stage research and development progress from that point? What were the key MEDC services and programs you used to get started?



Denise: Well, our business is based on cutting-edge science, truly cutting-edge science. Understanding the intellectual property landscape, which is a term used to describe intellectual property, patents, trademarks, trade secrets that are related to the science we were founding our business on, it was very important. So that we knew we were starting on a path, that we wouldn&#8217;t hit a wall three years from now.



The MEDC performed an IP landscape survey for us. They searched the breadth of patents, and patent applications, and trademarks, and those things, and provided that for Meng and me to review and consider our position before really, really digging in and spending a lot of emotional time and money on the business.



The MEDC also helped us early on create a model in Excel for tracking business financing and forecasting, forecasting our future financial needs, so that we could be proactive instead of reactive to financial limitations. We knew it wasn&#8217;t going to be inexpensive to design and build a piece of equipment that&#8217;s the size of two very large refrigerators that had never been built before. That type of financial-forward modeling helped us figure out when we would need to start fundraising and taking in investments. As we progressed along that, the MEDC also provided support of an attorney who reviewed contract language to make sure that we really understood various agreements that we were signing.



Megan: Right. You mentioned that you and your co-founder were first-time entrepreneurs, as you put it. Tech acumen and business acumen are very different sets of skills. I wondered, what was the process like, developing this innovative technology¬†while also building out a viable business plan?



Denise: Well, Meng is a brilliant individual. She is a chemical engineer who also has an MBA. Meng had fantastic training to help understand the basis of how businesses function, in addition to understanding both the engineering and the chemistry behind what we were trying to do.I am an environmental toxicologist by training. I&#8217;ve had a longer career than Meng in that field. Over time, I have grown new offices and¬†established new offices for different consulting firms I&#8217;ve worked for. I had the experience with people, space, culture, and running a business from that side. Meng has the financial MBA knowledge basis for a business. We&#8217;re both excellent chemists and engineers, and those types of things.We had much of the necessary knowledge, at least to take the first steps forward. The challenge became the hard limit of 24 hours in a day and no revenue to hire any support. That&#8217;s when the startup support networks like the MEDC became invaluable.



It was simply impossible to do everything that needed to be done, especially while we were learning what we were doing. The MEDC and other programs provided support to take some of that load off us, but also helped us to learn to implement the new skills in an efficient manner, less stumbling.



Megan: So many things to juggle, isn&#8217;t there, in starting a company. I wondered, in that vein, could you share some successes and highlights from your journey so far? Any partnerships or projects that you&#8217;re excited about that you could share with us?



Denise: As people say, being an entrepreneur and running a startup is like a rollercoaster. You have high moments and you have very low moments when you think nothing&#8217;s ever going to move forward. I&#8217;d love to talk about some of the highlights. Our machine, which we call the PFASigator.



First of all, coming up with that name has a fun story behind it. The machine is, like I said, about the size of two large refrigerators. It&#8217;s very large, and it breaks down PFAS in water. The machine takes in water that has PFAS in it, we add a couple of liquid chemicals, then¬†a very intense ultraviolet light shines on that water, which catalyzes a chemical reaction called reductive defluorination. When all of this is happening and the PFAS molecules are being broken apart to nontoxic compounds, to an outsider, it all still just looks like water with a light shining on it. But the machine is big, and it essentially eats PFAS.



Meng and I were bantering, and her young, six-year-old son was in the background at the time. We were throwing names around. Thomas called out, &#8220;The PFASigator!&#8221; We were like, &#8220;Ooh, there&#8217;s something there.&#8221;



Megan: It&#8217;s a great name.



Denise: It matches what we do, and it&#8217;s a memorable name. We&#8217;ve really had fun with that throughout. That was an early highlight, and we&#8217;ve stuck with that name.



The next highlight I&#8217;d say was standing next to our first fully functioning PFASigator. It was big. It was all stainless steel. Meng and I had never been part of building a physical, large object like that. Just standing there, and the picture we have of us, it was exhilarating. That was a magnificent feeling.



Selling our first machine was a day that everyone in the company, I think we were about eight at that point, received a bottle of champagne.



Megan: Fantastic.



Denise: For a startup to go from zero to one, they call it, you&#8217;ve sold nothing to you&#8217;ve sold something. That&#8217;s a real strong milestone and was a celebration for us.



I&#8217;d say most recently, Enspired has been awarded a very exciting project in Michigan. It is in the contracting phase, so I can&#8217;t reveal too many details. But it is with a progressive municipality that will have our PFASigator permanently installed, destroying PFAS. That kind of movement from zero to one, and then a significant contract that will raise the visibility of the effectiveness of our approach and machine, has really buoyed our energy and is pushing us forward. It&#8217;s amazing to know we are now having an impact on the sustainability of water resources. That&#8217;s what we started the company for.



Megan: Awesome. You have some incredible milestones there. But it&#8217;s a hard journey, as you&#8217;ve said as well, being an entrepreneur. I wondered, finally, what advice would you offer to burgeoning entrepreneurs given your own experience?



Denise: I would advise that if problem solving and learning new talents do not provide sufficient intrinsic reward for a founder to be satisfied throughout what I guarantee will be a long duration effort, then that founder may need to reset their expectations, because the financial rewards of entrepreneurship are small throughout the process.Meng and I put [in] some of our personal funds and took no salary, and worked harder than we ever had in our lives for at least a year and a half before we were able to take a small salary. The financial rewards are small throughout the process of being a startup. The rewards are delayed, and in many cases, for many startups, the financial rewards never materialize.It&#8217;s a tough journey, and you have to love being on that journey, and be intrinsically rewarded for that for the sake of the journey itself, or you&#8217;ll be a very unhappy founder.Megan: It needs to be something you&#8217;re as passionate about as I can tell you are about the work you&#8217;re doing at Enspired Solutions.



Denise: There&#8217;s probably one other thing I&#8217;d like to add to that.



Megan: Of course.



Denise: Often, founders are coached about formulas for fundraising, formulas for startup success. Learning those formulas and expectations is important, but it&#8217;s also important to not forget that it&#8217;s your creativity and innovation and foresight that got you to the place you&#8217;re in and drove you to start a company. Ultimately, people still want to see that shine through.&#8221;



Megan: That&#8217;s fantastic advice. Thank you so much, Denise.



That was Dr. Denise Kay, the co-founder and CEO at Enspired Solutions, whom I spoke with from an unexpectedly sunny Brighton, England.That&#8217;s it for this episode of Business Lab. I&#8217;m your host, Megan Tatum. I&#8217;m a contributing editor and host for Insights, the custom publishing division of MIT Technology Review. We were founded in 1899 at the Massachusetts Institute of Technology. You can find us in print, on the web, and at events each year around the world. For more information about us and the show, please check out our website at technologyreview.com.



This show is available wherever you get your podcasts. If you enjoyed this episode, we hope you&#8217;ll take a moment to rate and review us. Business Lab is a production of MIT Technology Review, and this episode was produced by Giro Studios. Thanks for listening.



This content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review‚Äôs editorial staff.



This content was researched, designed, and written entirely by human writers, editors, analysts, and illustrators. This includes the writing of surveys and collection of data for surveys. AI tools that may have been used were limited to secondary production processes that passed thorough human review.
‚Ä¢ The Download: how to melt rocks, and what you need to know about AI
  This is today&#8217;s edition of¬†The Download,¬†our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



This startup wants to use beams of energy to drill geothermal wells



Geothermal startup Quaise certainly has an unconventional approach when it comes to destroying rocks: it uses a new form of drilling technology to melt holes through them. The company hopes it‚Äôs the key to unlocking geothermal energy and making it feasible anywhere.Quaise‚Äôs technology could theoretically be used to tap into the Earth‚Äôs heat from anywhere on the globe. But some experts caution that reinventing drilling won‚Äôt be as simple, or as fast, as Quaise‚Äôs leadership hopes. Read the full story.



‚ÄîCasey Crownhart







Five things you need to know about AI right now



‚ÄîWill Douglas Heaven, senior editor for AI



Last month I gave a talk at SXSW London called ‚ÄúFive things you need to know about AI‚Äù‚Äîmy personal picks for the five most important ideas in AI right now.&nbsp;



I aimed the talk at a general audience, and it serves as a quick tour of how I‚Äôm thinking about AI in 2025. There‚Äôs some fun stuff in there. I even make jokes!&nbsp;



You can now watch the video of my talk, but if you want to see the five I chose right now, here is a quick look at them.



This story originally appeared in The Algorithm, our weekly newsletter on AI. To get stories like this in your inbox first, sign up here.







Why it‚Äôs so hard to make welfare AI fair



There are plenty of stories about AI that‚Äôs caused harm when deployed in sensitive situations, and in many of those cases, the systems were developed without much concern to what it meant to be fair or how to implement fairness.But the city of Amsterdam spent a lot of time and money to try to create ethical AI‚Äîin fact, it followed every recommendation in the responsible AI playbook. But when it deployed it in the real world, it still couldn‚Äôt remove biases. So why did Amsterdam fail? And more importantly: Can this ever be done right?Join our editor Amanda Silverman, investigative reporter Eileen Guo and Gabriel Geiger, an investigative reporter from Lighthouse Reports, for a subscriber-only Roundtables conversation at 1pm ET on Wednesday July 30 to explore if algorithms can ever be fair. Register here!







The must-reads



I‚Äôve combed the internet to find you today‚Äôs most fun/important/scary/fascinating stories about technology.



1 America‚Äôs grand data center ambitions aren‚Äôt being realized¬†A major partnership between SoftBank and OpenAI hasn‚Äôt got off to a flying start. (WSJ $)+ The setback hasn‚Äôt stopped OpenAI opening its first DC office. (Semafor)



2 OpenAI is partnering with the UK governmentIn a bid to increase its public services‚Äô productivity and to drive economic growth. (BBC)+ It all sounds pretty vague. (Engadget)



3 The battle for AI math supremacy is heating upGoogle and OpenAI went head to head in a math competition‚Äîbut only one played by the rules. (Axios)+ The International Math Olympiad poses a unique challenge to AI models. (Ars Technica)+ What‚Äôs next for AI and math. (MIT Technology Review)



4 Mark Zuckerberg‚Äôs secretive Hawaiian compound is getting biggerThe multi-billionaire is sinking millions of dollars into the project. (Wired $)



5 India‚Äôs back offices are meeting global demand for AI expertise¬†New ‚Äòcapability centers‚Äô could help to improve the country‚Äôs technological prospects. (FT $)+ The founder of Infosys believes the future of AI will be more democratic. (Rest of World)+ Inside India‚Äôs scramble for AI independence. (MIT Technology Review)



6 A crime-tracking app will share videos with the NYPDPublic safety agencies will have access to footage shared on Citizen. (The Verge)+ AI was supposed to make police bodycams better. What happened? (MIT Technology Review)



7 China has a problem with competition: there‚Äôs too much of itIts government is making strides to crack down on price wars within sectors. (NYT $)+ China‚Äôs Xiaomi is making waves across the world. (Economist $)



8 The metaverse is a tobacco marketer‚Äôs playground Fed up of legal constraints, they‚Äôre already operating in unregulated spaces. (The Guardian)+ Welcome to the oldest part of the metaverse. (MIT Technology Review)



9 How AI is shaking up physicsModels are suggesting outlandish ideas that actually work. (Quanta Magazine)



10 Tesla has opened a diner that resembles a spaceshipIt‚Äôs technically a drive-thru that happens to sell Tesla merch. (TechCrunch)







Quote of the day



&nbsp;&#8220;If you can pick off the individuals for $100 million each and they&#8217;re good, it&#8217;s actually a bargain.&#8221;



‚ÄîEntrepreneur Laszlo Bock tells Insider why he thinks the eye-watering sums Meta is reportedly offering top AI engineers is money well spent.







One more thing







The world‚Äôs first industrial-scale plant for green steel promises a cleaner futureAs of 2023, nearly 2 billion metric tons of steel were being produced annually, enough to cover Manhattan in a layer more than 13 feet thick.Making this metal produces a huge amount of carbon dioxide. Overall, steelmaking accounts for around 8% of the world‚Äôs carbon emissions‚Äîone of the largest industrial emitters and far more than such sources as aviation.A handful of groups and companies are now making serious progress toward low- or zero-emission steel. Among them, the Swedish company Stegra stands out. The startup is currently building the first industrial-scale plant in the world to make green steel. But can it deliver on its promises? Read the full story.‚ÄîDouglas Main







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)



+ Spoiler haters look away now: these are the best movie endings of all.+ 27 years on, this bop from the Godzilla soundtrack still sounds like the future.+ Inside the race to preserve the very first color photographs for generations to come.+ Origami space planes sound very cool.

üîí Cybersecurity & Privacy
‚Ä¢ Microsoft Fix Targets Attacks on SharePoint Zero-Day
  Microsoft Corp. issued an emergency security update for a vulnerability in SharePoint Server that is actively being exploited to compromise vulnerable organizations . The patch comes amid reports that malicious hackers have used the SharePoint flaw to breach U.S. federal and state agencies, universities, and energy companies . The Washington Post reported on Sunday that the government and partners in Canada and Australia are investigating the hack of SharePoint servers, which provide a platform for sharing and managing documents .

üéì University AI
No updates.

üè¢ Corporate AI
‚Ä¢ Xinxing Xu bridges AI research and real-world impact at Microsoft Research Asia ‚Äì Singapore
  While AI has rapidly advanced in recent years, one challenge remains stubbornly unresolved: how to move promising algorithmic models from controlled experiments into practical, real-world use. The effort to balance algorithmic innovation with real-world application has been a consistent theme in the career of Xinxing Xu, principal researcher at Microsoft Research Asia ‚Äì Singapore, and also represents one of the foundational pillars of the newly established Singapore lab.



Xinxing Xu, Principal Researcher, Microsoft Research Asia ‚Äì Singapore



‚ÄúInnovative algorithms can only demonstrate their true value when tested with real-world data and in actual scenarios, where they can be continuously optimized through iteration,‚Äù he says.



Xu‚Äôs commitment to balancing algorithmic innovation with practical application has shaped his entire career. During his PhD studies at Nanyang Technological University, Singapore, Xu focused on emerging technologies like multiple kernel learning methods and multimodal machine learning. Today he‚Äôs applying these techniques to real-world use cases like image recognition and video classification.



After completing his doctorate, he joined the Institute of High Performance Computing at Singapore‚Äôs Agency for Science, Technology and Research (A*STAR), where he worked on interdisciplinary projects ranging from medical image recognition to AI systems for detecting defects on facade of buildings. These experiences broadened his perspective and deepened his passion for translating AI into real-world impact.



In 2024, Xu joined Microsoft Research Asia where he began a new chapter focused on bridging between academic research and real-world AI applications.



‚ÄúMicrosoft Research Asia is committed to integrating scientific exploration with real-world applications, which creates a unique research environment,‚Äù Xu says. ‚ÄúIt brings together top talent and resources, and Microsoft&#8217;s engineering and product ecosystem strongly supports turning research into impactful technology. The lab‚Äôs open and inclusive culture encourages innovation with broader societal impact. It reflects the approach to research I‚Äôve always hoped to contribute to.‚Äù



	
		

	
	
						
				
					
				
			
			
			

									Azure AI Foundry Labs
				
								Get a glimpse of potential future directions for AI, with these experimental technologies from Microsoft Research.
				
								
					
						
							Azure AI Foundry						
					
				
							
	
Opens in a new tab	
	


Bringing cross-domain expertise to AI‚Äôs real-world frontiers



As a key hub in Microsoft Research‚Äôs network across Asia, the Singapore lab is guided by a three-part mission: to drive industry-transforming AI deployment, pursue fundamental breakthroughs in the field, and promote responsible, socially beneficial applications of the technology.



To reach these goals, Xu and his colleagues are working closely with local collaborators, combining cross-disciplinary expertise to tackle complex, real-world challenges.



Xu draws on his experience in healthcare as he leads the team‚Äôs collaboration with Singapore‚Äôs SingHealth to explore how AI can support precision medicine. Their efforts focus on leveraging SingHealth‚Äôs data and expertise to develop AI capabilities aimed at delivering personalized analysis and enhanced diagnostic accuracy to enable better patient outcomes.



Beyond healthcare, the team is also targeting key sectors like finance and logistics. By developing domain-specific foundation models and AI agents, they aim to support smarter decision-making and accelerate digital transformation across industries. ‚ÄúSingapore has a strong foundation in these sectors,‚Äù Xu notes, ‚Äúmaking it an ideal environment for technology validation and iteration.‚Äù



The team is also partnering with leading academic institutions, including the National University of Singapore (NUS) and Nanyang Technological University, Singapore (NTU Singapore), to advance the field of spatial intelligence. Their goal is to develop embodied intelligence systems capable of carrying out complex tasks in smart environments.



As AI becomes more deeply embedded in everyday life, researchers at the Singapore lab are also increasingly focused on what they call ‚Äúsocietal AI‚Äù‚Äîbuilding AI systems that are culturally relevant and trustworthy within Southeast Asia‚Äôs unique cultural and social contexts. Working with global colleagues, they are helping to advance a more culturally grounded and responsible approach to AI research in the region.



Microsoft Research Asia ‚Äì Singapore: Expanding global reach, connecting regional innovation&nbsp;



Realizing AI‚Äôs full potential requires more than technical breakthroughs. It also depends on collaboration‚Äîacross industries, academia, and policy. Only through this intersection of forces can AI move beyond the lab to deliver meaningful societal value.&nbsp;



Singapore‚Äôs strengths in science, engineering, and digital governance make it an ideal setting for this kind of work. Its collaborative culture, robust infrastructure, international talent pool, and strong policy support for science and technology make it fertile ground for interdisciplinary research.&nbsp;



This is why Microsoft Research Asia continues to collaborate closely with Singapore‚Äôs top universities, research institutions, and industry partners. These partnerships support joint research, talent development, and technical exchange. Building on this foundation, Microsoft Research Asia ‚Äì Singapore will further deepen its collaboration with NUS, NTU Singapore, and Singapore Management University (SMU) to advance both fundamental and applied research, while equipping the next generation of researchers with real-world experience. In addition, Microsoft Research Asia is fostering academic exchange and strengthening the research ecosystem through summer schools and joint workshops with NUS, NTU Singapore, and SMU.&nbsp;



The launch of the Singapore lab further marks an important step in expanding the company‚Äôs global research footprint, serving as a bridge between regional innovation and Microsoft‚Äôs global ecosystem. Through its integrated lab network, Microsoft Research fosters the sharing of technologies, methods, and real-world insights, creating a virtuous cycle of innovation.



‚ÄúWe aim to build a research hub in Singapore that is globally connected and deeply rooted in the local ecosystem,‚Äù Xu says. ‚ÄúMany breakthroughs come from interdisciplinary and cross-regional collaboration. By breaking boundaries‚Äîacross disciplines, industries, and geographies‚Äîwe can drive research that has lasting impact.‚Äù



As AI becomes more deeply woven into industry and everyday life, Xu believes that meaningful research must be closely connected to regional development and social well-being.‚ÄúMicrosoft Research Asia ‚Äì Singapore is a future-facing lab,‚Äù he says. ‚ÄúWhile we push technological frontiers, we‚Äôre equally committed to the responsibility of technology‚Äîensuring AI can help address society‚Äôs most pressing challenges.‚Äù



In a world shaped by global challenges, Xu sees collaboration and innovation as essential to real progress. With Singapore as a launchpad, he and his team are working to extend AI‚Äôs impact and value across Southeast Asia and beyond.



Xingxing Xu (center) with colleagues at Microsoft Research Asia &#8211; Singapore&nbsp;



Three essential strengths for the next generation of AI researchers



AI‚Äôs progress depends not only on technical breakthroughs but also on the growth and dedication of talent. At Microsoft Research Asia, there is a strong belief that bringing research into the real world requires more than technical coordination‚Äîit depends on unlocking the full creativity and potential of researchers.



In Singapore‚Äîa regional innovation hub that connects Southeast Asia‚ÄîXu and his colleagues are working to push AI beyond the lab and into fields like healthcare, finance, and manufacturing. For young researchers hoping to shape the future of AI, this is a uniquely powerful stage.



To help guide the next generation, Xu shares three pieces of advice:




Build a strong foundation ‚Äì ‚ÄúCore knowledge in machine learning, linear algebra, and probability and statistics is the bedrock of AI research,‚Äù Xu says. ‚ÄúA solid theoretical base is essential to remain competitive in a rapidly evolving field. Even today‚Äôs hottest trends in generative AI rely on longstanding principles of optimization and model architecture design.‚Äù While code generation tools are on the rise, Xu emphasizes that mathematical fundamentals remain essential for understanding and innovating in AI.



Understand real-world applications ‚Äì Technical skills alone aren‚Äôt enough. Xu encourages young researchers to deeply engage with the problems they‚Äôre trying to solve. Only by tightly integrating technology with its context can researchers create truly valuable solutions.‚ÄúIn healthcare, for example, researchers may need to follow doctors in clinics to gain a true understanding of clinical workflows. That context helps identify the best entry points for AI deployment. Framing research problems around real-world needs is often more impactful than just tuning model parameters,‚Äù Xu says.



Develop interdisciplinary thinking ‚Äì Cross-disciplinary collaboration is becoming essential to AI innovation. Xu advises young researchers to learn how to work with experts from other fields to explore new directions together. ‚ÄúThese kinds of interactions often spark fresh, creative ideas,‚Äù he says.Maintaining curiosity is just as important. ‚ÄúBeing open to new technologies and fields is what enables researchers to continually break new ground and produce original results.‚Äù




Xu extends an open invitation to aspiring researchers from all backgrounds to join Microsoft Research Asia ‚Äì Singapore. ‚ÄúWe offer a unique platform that blends cutting-edge research with real-world impact,‚Äù he says. ‚ÄúIt‚Äôs a place where you can work on the frontiers of AI‚Äîand see how your work can help transform industries and improve lives.‚Äù



To learn more about the current opening, please visit: https://jobs.careers.microsoft.com/global/en/job/1849717/Senior-Researcher (opens in new tab)&nbsp;
Opens in a new tabThe post Xinxing Xu bridges AI research and real-world impact at Microsoft Research Asia &#8211; Singapore appeared first on Microsoft Research.
‚Ä¢ Technical approach for classifying human-AI interactions at scale
  As large language models (LLMs) become foundational to modern AI systems, the ability to run them at scale‚Äîefficiently, reliably, and in near real-time‚Äîis no longer a nice-to-have. It‚Äôs essential. The Semantic Telemetry project tackles this challenge by applying LLM-based classifiers to hundreds of millions of sampled, anonymized Bing Chat conversations each week. These classifiers extract signals like user expertise, primary topic, and satisfaction, enabling deeper insight into human-AI interactions and driving continuous system improvement.



But building a pipeline that can handle this volume isn‚Äôt just about plugging into an API. It requires a high-throughput, high-performance architecture that can orchestrate distributed processing, manage token and prompt complexity, and gracefully handle the unpredictability of remote LLM endpoints.



In this latest post in our series on Semantic Telemetry, we‚Äôll walk through the engineering behind that system‚Äîhow we designed for scale from the start, the trade-offs we made, and the lessons we learned along the way. From batching strategies and token optimization and orchestration, we‚Äôll share what it takes to build a real-time LLM classification pipeline.



For additional project background: Semantic Telemetry: Understanding how users interact with AI systems and Engagement, user expertise, and satisfaction: Key insights from the Semantic Telemetry Project.






	
		
						Blog
			
				Semantic Telemetry: Understanding how users interact with AI systems&nbsp;
			
					
	







	
		
						Blog
			
				Engagement, user expertise, and satisfaction: Key insights from the Semantic Telemetry Project&nbsp;
			
					
	






System architecture highlights



The Semantic Telemetry pipeline (opens in new tab) is a highly-scalable, highly-configurable, data transformation pipeline. While it follows a familiar ETL structure, several architectural innovations make it uniquely suited for high-throughput LLM integration:




Hybrid compute engineThe pipeline combines the distributed power of‚ÄØPySpark‚ÄØwith the speed and simplicity of‚ÄØPolars, enabling it to scale across large datasets or run lightweight jobs in Spark-less environments‚Äîwithout code changes.



LLM-centric transformation layerAt the core of the pipeline is a multi-stage transformation process tailored for running across multiple LLM endpoints such that:

Runs model agnostic. Provides a generic interface for LLMs and adopts model specific interfaces built from a generic interface.



Prompt templates are defined using the Prompty language specification for consistency and reuse, with options for users to include custom prompts.



Parsing and cleaning logic ensures structured, schema-aligned outputs, even when LLM responses are imperfect such as removing extra characters in output, resolving not-exact label matches (i.e. ‚Äúcreate‚Äù versus ‚Äúcreated‚Äù) and relabeling invalid classifications.






Figure 1. Architecture diagram



The pipeline supports multiple classification tasks (e.g., user expertise, topic, satisfaction) through modular prompt templates and configurable execution paths‚Äîmaking it easy to adapt to new use cases or environments.



Engineering challenges & solutions



Building a high-throughput, LLM-powered classification pipeline at scale introduced a range of engineering challenges‚Äîfrom managing latency and token limits to ensuring system resilience. Below are the key hurdles we encountered and how we addressed them.



LLM endpoint latency & variability



Challenge: LLM endpoints, especially those hosted remotely (e.g., Azure OpenAI), introduce unpredictable latency due to model load, prompt complexity, and network variability. This made it difficult to maintain consistent throughput across the pipeline.



Solution: We implemented a combination of:




Multiple Azure OpenAI endpoints in rotation to increase throughput and distribute workload. We can analyze throughput and redistribute as needed.



Saving output in intervals to write data asynchronously in case of network errors.



Utilizing models with higher tokens per minute (TPM) such as OpenAI‚Äôs GPT-4o mini. GPT-4o mini had a 2M TPM limit which is a 25x throughput increase from GPT-4 (80K TPM -> 2M TPM)



Timeouts and retries with exponential backoff.




Evolving LLM models & prompt alignment



Challenge: Each new LLM release‚Äîsuch as Phi, Mistral, DeepSeek, and successive generations of GPT (e.g., GPT-3.5, GPT-4, GPT-4 Turbo, GPT-4o)‚Äîbrings improvements, but also subtle behavioral shifts. These changes can affect classification consistency, output formatting, and even the interpretation of prompts. Maintaining alignment with baseline expectations across models became a moving target.



Solution: We developed a model evaluation workflow to test prompt alignment across LLM versions:




Small-sample testing: We ran the pipeline on a representative sample using the new model and compared the output distribution to a known baseline.



Distribution analysis: If the new model‚Äôs output aligned closely, we scaled up testing. If not, we iteratively‚ÄØtuned the prompts‚ÄØand re-ran comparisons.



Interpretation flexibility: We also recognized that a shift in distribution isn‚Äôt always a regression. Sometimes it reflects a more accurate or nuanced classification, especially as models improve.




To support this process, we used tools like Sammo (opens in new tab), which allowed us to compare outputs across multiple models and prompt variants. This helped us quantify the impact of prompt changes and model upgrades and make informed decisions about when to adopt a new model or adjust our classification schema.



Dynamic concurrency scaling for LLM calls



Challenge: LLM endpoints frequently encounter rate limits and inconsistent response times under heavy usage. The models&#8217; speeds can also vary, complicating the selection of optimal concurrency levels. Furthermore, users may choose suboptimal settings due to lack of familiarity, and default concurrency configurations are rarely ideal for every situation. Dynamic adjustments based on throughput, measured in various ways, can assist in determining optimal concurrency levels.



Solution: We implemented a dynamic concurrency control mechanism that proactively adjusts the number of parallel LLM calls based on real-time system behavior:




External task awareness: The system monitors the number of parallel tasks running across the pipeline (e.g., Spark executors or async workers) and uses this to inform the initial concurrency level.



Success/failure rate monitoring: The system tracks the rolling success and failure rates of LLM calls. A spike in failures triggers a temporary reduction in concurrency, while sustained success allows for gradual ramp-up.



Latency-based feedback loop: Instead of waiting for rate-limit errors, measure the‚ÄØresponse time‚ÄØof LLM calls. If latency increases, reduce concurrency; if latency decreases and success rates remain high, cautiously scale up.




	
		

		
		Spotlight: AI-POWERED EXPERIENCE
	
	
	
						
				
					
				
			
			
			

									Microsoft research copilot experience
				
								Discover more about research at Microsoft through our AI-powered experience
				
								
					
						
							Start now						
					
				
							
	
Opens in a new tab	
	


Optimization experiments



To further improve throughput and efficiency, we ran a series of optimization experiments. Each approach came with trade-offs that we carefully measured.



Batch endpoints (Azure/OpenAI)



Batch endpoints are a cost-effective, moderately high-throughput way of executing LLM requests. Batch endpoints process large lists of LLM prompts over a 24-hour period, recording responses in a file. They are about 50% cheaper than non-batch endpoints and have separate token limits, enabling increased throughput when used alongside regular endpoints. However, they require at least 24 hours to complete requests and provide lower overall throughput compared to non-batch endpoints, making them unsuitable for situations needing quick results.



Conversation batching in prompts during pipeline runtime



Batching multiple conversations for classification at once can significantly increase throughput and reduce token usage, but it may impact the accuracy of results. In our experiment with a domain classifier, classifying 10 conversations simultaneously led to an average of 15-20% of domain assignments changing between repeated runs of the same prompt. To address this, one mitigation approach is to use a grader LLM prompt: first classify the batch, then have the LLM identify any incorrectly classified conversations, and finally re-classify those as needed. While batching offers efficiency gains, it is important to monitor for potential drops in classification quality.



Combining classifiers in a single prompt



Combining multiple classifiers into a single prompt increases throughput by allowing one call to the LLM instead of multiple calls. This not only multiplies the overall throughput by the number of classifiers processed but also reduces the total number of tokens used, since the conversation text is only passed in once. However, this approach may compromise classification accuracy, so results should be closely monitored.



Classification using text embeddings



An alternative approach is to train custom neural network models for each classifier using only the text embeddings of conversations. This method delivers both cost and time savings by avoiding making multiple LLM requests for every classifier and conversation‚Äîinstead, the system only needs to request conversation text embeddings once and can reuse these embeddings across all classifier models.



For example, starting with a set of conversations to validate and test the new model, run these conversations through the original prompt-based classifier to generate a set of golden classifications, then obtain text embeddings (using a tool like text-embedding-3-large) for each conversation. These embeddings and their corresponding classifications are used to train a model such as a multi-layer perceptron. In production, the workflow involves retrieving the text embedding for each conversation and passing it through the trained model; if there is a model for each classifier, a single embedding retrieval per conversation suffices for all classifiers.



The benefits of this approach include significantly increased throughput and cost savings‚Äîsince it‚Äôs not necessary to call the LLM for every classifier and conversation. However, this setup can require GPU compute which can increase costs and infrastructure complexity, and the resulting models may not achieve the same accuracy as prompt-based classification methods.



Prompt compression



Compressing prompts by eliminating unnecessary tokens or by using a tool such as LLMLingua (opens in new tab) to automate prompt compression can optimize classification prompts either ahead of time or in real-time. This approach increases overall throughput and results in cost savings due to a reduced number of tokens, but there are risks: changes to the classifier prompt or conversation text may impact classification accuracy, and depending on the compression technique, it could even decrease throughput if the compression process takes longer than simply sending uncompressed text to the LLM.



Text truncation



Truncating conversations to a specific length limits the overall number of tokens sent through an endpoint, offering cost savings and increased throughput like prompt compression. By reducing the number of tokens per request, throughput rises because more requests can be made before reaching the endpoint‚Äôs tokens-per-minute (TPM) limit, and costs decrease due to fewer tokens being processed. However, the ideal truncation length depends on both the classifiers and the conversation content, so it‚Äôs important to assess how truncation affects output quality before implementation. While this approach brings clear efficiency benefits, it also poses a risk: long conversations may have their most important content cut off, which can reduce classification accuracy.



Conclusion



Building a scalable, high-throughput pipeline for LLM-based classification is far from trivial. It requires navigating a constantly shifting landscape of model capabilities, prompt behaviors, and infrastructure constraints. As LLMs become faster, cheaper, and more capable, they‚Äôre unlocking new possibilities for real-time understanding of human-AI interactions at scale. The techniques we‚Äôve shared represent a snapshot of what‚Äôs working today. But more importantly, they offer a foundation for what‚Äôs possible tomorrow.
Opens in a new tabThe post Technical approach for classifying human-AI interactions at scale appeared first on Microsoft Research.
‚Ä¢ AI Testing and Evaluation: Reflections
  Generative AI presents a unique challenge and opportunity to reexamine governance practices for the responsible development, deployment, and use of AI. To advance thinking in this space, Microsoft has tapped into the experience and knowledge of experts across domains‚Äîfrom genome editing to cybersecurity‚Äîto investigate the role of testing and evaluation as a governance tool.&nbsp;AI Testing and Evaluation: Learnings from Science and Industry,&nbsp;hosted by Microsoft Research‚Äôs&nbsp;Kathleen Sullivan, explores what the technology industry and policymakers can learn from these fields and how that might help shape the course of AI development.



In the series finale, Amanda Craig Deckard, senior director of public policy in Microsoft‚Äôs Office of Responsible AI, rejoins Sullivan to discuss what Microsoft has learned about testing as a governance tool and what‚Äôs next for the company&#8217;s work in the AI governance space. The pair explores high-level takeaways (i.e., testing is important and challenging!); the roles of rigor, standardization, and interpretability in making testing a reliable governance tool; and the potential for public-private partnerships to help advance not only model-level evaluation but deployment-level evaluation, too.







Learn more:



Learning from other domains to advance AI evaluation and testing&nbsp;Microsoft Research Blog | June 2025&nbsp;



Responsible AI: Ethical policies and practices | Microsoft AI&nbsp;








	
		
			Subscribe to the Microsoft Research Podcast:		
		
							
					
						  
						Apple Podcasts
					
				
			
							
					
						
						Email
					
				
			
							
					
						
						Android
					
				
			
							
					
						
						Spotify
					
				
			
							
					
						
						RSS Feed
					
				
					
	




	
		
			
				
					

Transcript



[MUSIC]&nbsp;



KATHLEEN SULLIVAN: Welcome to AI Testing and Evaluation: Learnings from Science and Industry. I‚Äôm your host, Kathleen Sullivan.&nbsp;



As generative AI continues to advance, Microsoft has gathered a range of experts‚Äîfrom genome editing to cybersecurity‚Äîto share how their fields approach evaluation and risk assessment. Our goal is to learn from their successes and their stumbles to move the science and practice of AI testing forward. In this series, we‚Äôll explore how these insights might help guide the future of AI development, deployment, and responsible use.&nbsp;



[MUSIC ENDS]&nbsp;



For our final episode of the series, I‚Äôm thrilled to once again be joined by Amanda Craig Deckard, senior director of public policy in Microsoft‚Äôs Office of Responsible AI.&nbsp;



Amanda, welcome back to the podcast!



				
				
					



AMANDA CRAIG DECKARD: Thank you so much.



SULLIVAN: In our intro episode, you really helped set the stage for this series. And it‚Äôs been great, because since then, we‚Äôve had the pleasure of speaking with governance experts about genome editing, pharma, medical devices, cybersecurity, and we‚Äôve also gotten to spend some time with our own Microsoft responsible AI leaders and hear reflections from them.



And here‚Äôs what stuck with me, and I‚Äôd love to hear from you on this, as well: testing builds trust; context is shaping risk; and every field is really thinking about striking its own balance between pre-deployment testing and post-deployment monitoring.



So drawing on what you‚Äôve learned from the workshop and the case studies, what headline insights do you think matter the most for AI governance?



CRAIG DECKARD: It&#8217;s been really interesting to learn from all of these different domains, and there are, you know, lots of really interesting takeaways.&nbsp;



I think a starting point for me is actually pretty similar to where you landed, which is just that testing is really important for trust, and it&#8217;s also really hard [LAUGHS] to figure out exactly, you know, how to get it right, how to make sure that you&#8217;re addressing risks, that you&#8217;re not constraining innovation, that you are recognizing that a lot of the industry that&#8217;s impacted is really different. You have small organizations, you have large organizations, and you want to enable that opportunity that is enabled by the technology across the board.&nbsp;



And so it&#8217;s just difficult to, kind of, get all of these dynamics right, especially when, you know, I think we heard from other domains, testing is not some, sort of, like, oh, simple thing, right. There&#8217;s not this linear path from, like, A to B where you just test the one thing and you&#8217;re done.&nbsp;



SULLIVAN: Right.



CRAIG DECKARD: It&#8217;s complex, right. Testing is multistage. There&#8217;s a lot of testing by different actors. There are a lot of different purposes for which you might test. As I think it was Dan Carpenter who talked about it&#8217;s not just about testing for safety. It&#8217;s also about testing for efficacy and building confidence in the right dosage for pharmaceuticals, for example. And that&#8217;s across the board for all of these domains, right. That you&#8217;re really thinking about the performance of the technology. You&#8217;re thinking about safety. You&#8217;re trying to also calibrate for efficiency.



And so those tradeoffs, every expert shared that navigating those is really challenging. And also that there were real impacts to early choices in the, sort of, governance of risk in these different domains and the development of the testing, sort of, expectations, and that in some cases, this had been difficult to reverse, which also just layers on that complexity and that difficulty in a different way. So that‚Äôs the super high-level takeaway. But maybe if I could just quickly distill, like, three takeaways that I think really are applicable to AI in a bit more of a granular way.



You know, one is about, how is the testing exactly used? For what purpose? And the second is what emphasis there is on this pre- versus post-deployment testing and monitoring. And then the third is how rigid versus adaptive the, sort of, testing regimes or frameworks are in these different domains.&nbsp;



So on the first‚Äîhow is testing used?‚Äîso is testing something that impacts market entry, for example? Or is it something that might be used more for informing how risk is evolving in the domain and how broader risk management strategies might need to be applied? We have examples, like the pharmaceutical or medical device industry experts with whom you spoke, that&#8217;s really, you know, testing ‚Ä¶ there is a pre-deployment requirement. So that&#8217;s one question.&nbsp;



The second is this emphasis on pre- versus post-deployment testing and monitoring, and we really did see across domains that in many cases, there is a desire for both pre- and post-deployment, sort of, testing and monitoring, but also that, sort of, naturally in these different domains, a degree of emphasis on one or the other had evolved and that had a real impact on governance and tradeoffs.&nbsp;



And the third is just how rigid versus adaptive these testing and evaluation regimes or frameworks are in these different domains. We saw, you know, in some domains, the testing requirements were more rigid as you might expect in more of the pharmaceutical or medical devices industries, for example. And in other domains, there was this more, sort of, adaptive approach to how testing might get used. So, for example, in the case of our other general-purpose technologies, you know, you spoke with Alta Charo on genome editing, and in our case studies, we also explored this in the context of nanotechnology. In those general-purpose technology domains, there is more emphasis on downstream or application-context testing that is more, sort of, adaptive to the use scenario of the technology and, you know, having that work in conjunction with testing more at the, kind of, level of the technology itself.



SULLIVAN: I want to double-click on a number of the things we just talked about. But actually, before we go too much deeper, a question on if there&#8217;s anything that really surprised you or challenged maybe some of your own assumptions in this space from some of the discussions that we had over the series.&nbsp;



CRAIG DECKARD: Yeah. You know, I know I&#8217;ve already just mentioned this pre- versus post-deployment testing and monitoring issue, but it was something that was very interesting to me and in some ways surprised me or made me just realize something that I hadn&#8217;t fully connected before, about how these, sort of, regimes might evolve in different contexts and why. And in part, I couldn&#8217;t help but bring the context I have from cybersecurity policy into this, kind of, processing of what we learned and reflection because there was a real contrast for me between the pharmaceutical industry and the cybersecurity domain when I think about the emphasis on pre- versus post-deployment monitoring.



And on the one hand, we have in the pharmaceutical domain a real emphasis that has developed around pre-market testing. And there is also an expectation in some circumstances in the pharmaceutical domain for post-deployment testing, as well. But as we learned from our experts in that domain, there has naturally been a real, kind of, emphasis on the pre-market portion of that testing. And in reality, even where post-market monitoring is required and post-market testing is required, it does not always actually happen. And the experts really explained that, you know, part of it is just the incentive structure around the emphasis around, you know, the testing as a pre-market, sort of, entry requirement. And also just the resources that exist among regulators, right. There&#8217;s limited resources, right. And so there are just choices and tradeoffs that they need to make in their own, sort of, enforcement work.



And then on the other hand, you know, in cybersecurity, I never thought about the, kind of, emphasis on things like coordinated vulnerability disclosure and bug bounties that have really developed in the cybersecurity domain. But it&#8217;s a really important part of how we secure technology and enhance cybersecurity over time, where we have these norms that have developed where, you know, security researchers are doing really important research. They&#8217;re finding vulnerabilities in products. And we have norms developed where they report those to the companies that are in a position to address those vulnerabilities. And in some cases, those companies actually pay, through bug bounties, the researchers. And perhaps in some ways, the role of coordinated vulnerability disclosure and bug bounties has evolved the way that it has because there hasn&#8217;t been as much emphasis on the pre-market testing across the board at least in the context of software.



And so you look at those two industries and it was interesting to me to study them to some extent in contrast with each other as this way that the incentives and the resources that need to be applied to testing, sort of, evolve to address where there&#8217;s, kind of, more or less emphasis.



SULLIVAN: It&#8217;s a great point. I mean, I think what we&#8217;re hearing‚Äîand what you&#8217;re saying‚Äîis just exactly this choice ‚Ä¶ like, is there a binary choice between focusing on pre-deployment testing or post-deployment monitoring? And, you know, I think our assumption is that we need to do both. But I&#8217;d love to hear from you on that.&nbsp;



CRAIG DECKARD: Absolutely. I think we need to do both. I&#8217;m very persuaded by this inclination always that there&#8217;s value in trying to really do it all in a risk management context.&nbsp;



And also, we know one of the principles of risk management is you have to prioritize because there are finite resources. And I think that&#8217;s where we get to this challenge in really thinking deeply, especially as we&#8217;re in the early days of AI governance, and we need to be very thoughtful about, you know, tradeoffs that we may not want to be making but we are because, again, these are finite choices and we, kind of, can&#8217;t help but put our finger on the dial in different directions with our choices that, you know, it&#8217;s going to be very difficult to have, sort of, equal emphasis on both. And we need to invest in both, but we need to be very deliberate about the roles of each and how they complement each other and who does which and how we use what we learn from pre- versus post-deployment testing and monitoring.



SULLIVAN: Maybe just spending a little bit more time here ‚Ä¶ you know, a lot of attention goes into testing models upstream, but risk often shows up once they&#8217;re wired into real products and workflows. How much does deployment context change the risk picture from your perspective?&nbsp;



CRAIG DECKARD: Yeah, I ‚Ä¶ such an important question. I really agree that there has been a lot of emphasis to date on, sort of, testing models upstream, the AI model evaluation. And it&#8217;s also really important that we bring more attention into evaluation at the system or application level. And I actually see that in governance conversations, this is actually increasingly raised, this need to have system-level evaluation. We see this across regulation. We also see it in the context of just organizations trying to put in governance requirements for how their organization is going to operate in deploying this technology.&nbsp;



And there&#8217;s a gap today in terms of best practices around system-level testing, perhaps even more than model-level evaluation. And it&#8217;s really important because in a lot of cases, the deployment context really does impact the risk picture, especially with AI, which is a general-purpose technology, and we really saw this in our study of other domains that represented general-purpose technology.&nbsp;



So in the case study that you can find online on nanotechnology, you know, there&#8217;s a real distinction between the risk evaluation and the governance of nanotechnology in different deployment contexts. So the chapter that our expert on nanotechnology wrote really goes into incredibly interesting detail around, you know, deployment of nanotechnology in the context of, like, chemical applications versus consumer electronics versus pharmaceuticals versus construction and how the way that nanoparticles are basically delivered in all those different deployment contexts, as well as, like, what the risk of the actual use scenario is just varies so much. And so there&#8217;s a real need to do that kind of risk evaluation and testing in the deployment context, and this difference in terms of risks and what we learned in these other domains where, you know, there are these different approaches to trying to really think about and gain efficiencies and address risks at a horizontal level versus, you know, taking a real sector-by-sector approach. And to some extent, it seems like it&#8217;s more time intensive to do that sectoral deployment-specific work. And at the same time, perhaps there are efficiencies to be gained by actually doing the work in the context in which, you know, you have a better understanding of the risk that can result from really deploying this technology.&nbsp;



And ultimately, [LAUGHS] really what we also need to think about here is probably, in the end, just like pre- and post-deployment testing, you need both. Not probably; certainly!



So effectively we need to think about evaluation at the model level and the system level as being really important. And it&#8217;s really important to get system evaluation right so that we can actually get trust in this technology in deployment context so we enable adoption in low- and in high-risk deployments in a way that means that we&#8217;ve done risk evaluation in each of those contexts in a way that really makes sense in terms of the resources that we need to apply and ultimately we are able to unlock more applications of this technology in a risk-informed way.



SULLIVAN: That&#8217;s great. I mean, I couldn&#8217;t agree more. I think these contexts, the approaches are so important for trust and adoption, and I&#8217;d love to hear from you, what do we need to advance AI evaluation and testing in our ecosystem? What are some of the big gaps that you&#8217;re seeing, and what role can different stakeholders play in filling them? And maybe an add-on, actually: is there some sort of network effect that could 10x our testing capacity?&nbsp;



CRAIG DECKARD: Absolutely. So there&#8217;s a lot of work that needs to be done, and there&#8217;s a lot of work in process to really level up our whole evaluation and testing ecosystem. We learned, across domains, that there‚Äôs really a need to advance our thinking and our practice in three areas: rigor of testing; standardization of methodologies and processes; and interpretability of test results.&nbsp;



So what we mean by rigor is that we are ensuring that what we are ultimately evaluating in terms of risks is defined in a scientifically valid way and we are able to measure against that risk in a scientifically valid way.&nbsp;



By standardization, what we mean is that there&#8217;s really an accepted and well-understood and, again, a scientifically valid methodology for doing that testing and for actually producing artifacts out of that testing that are meeting those standards. And that sets us up for the final portion on interpretability, which is, like, really the process by which you can trust that the testing has been done in this rigorous and standardized way and that then you have artifacts that result from the testing process that can really be used in the risk management context because they can be interpreted, right.&nbsp;



We understand how to, like, apply weight to them for our risk-management decisions. We actually are able to interpret them in a way that perhaps they inform other downstream risk mitigations that address the risks that we see through the testing results and that we actually understand what limitations apply to the test results and why they may or may not be valid in certain, sort of, deployment contexts, for example, and especially in the context of other risk mitigations that we need to apply. So there&#8217;s a need to advance all three of those things‚Äîrigor, standardization, and interpretability‚Äîto level up the whole testing and evaluation ecosystem.&nbsp;



And when we think about what actors should be involved in that work ‚Ä¶ really everybody, which is both complex to orchestrate but also really important. And so, you know, you need to have the entire value chain involved in really advancing this work. You need the model developers, but you also need the system developers and deployers that are really engaged in advancing the science of evaluation and advancing how we are using these testing artifacts in the risk management process.&nbsp;



When we think about what could actually 10x our testing capacity‚Äîthat&#8217;s the dream, right? We all want to accelerate our progress in this space. You know,&nbsp;I think we need work across all three of those areas of rigor, standardization, and interpretability, but I think one that will really help accelerate our progress across the board is that standardization work, because ultimately, you&#8217;re going to need to have these tests be done and applied across so many different contexts, and ultimately, while we want the whole value chain engaged in the development of the thinking and the science and the standards in this space, we also need to realize that not every organization is necessarily going to have the capacity to, kind of, contribute to developing the ways that we create and use these tests. And there are going to be many organizations that are going to benefit from there being standardization of the methodologies and the artifacts that they can pick up and use.



One thing that I know we&#8217;ve heard throughout this podcast series from our experts in other domains, including Timo [Minssen] in the medical devices context and Ciaran [Martin] in the cybersecurity context, is that there&#8217;s been a recognition, as those domains have evolved, that there&#8217;s a need to calibrate our, sort of, expectations for different actors in the ecosystem and really understand that small businesses, for example, just cannot apply the same degree of resources that others may be able to, to do testing and evaluation and risk management. And so the benefit of having standardized approaches is that those organizations are able to, kind of, integrate into the broader supply chain ecosystem and apply their own, kind of, risk management practices in their own context in a way that is more efficient.&nbsp;



And finally, the last stakeholder that I think is really important to think about in terms of partnership across the ecosystem to really advance the whole testing and evaluation work that needs to happen is government partners, right, and thinking beyond the value chain, the AI supply chain, and really thinking about public-private partnership. That&#8217;s going to be incredibly important to advancing this ecosystem.



You know, I think there&#8217;s been real progress already in the AI evaluation and testing ecosystem in the public-private partnership context. We have been really supportive of the work of the International Network of AI Safety and Security Institutes (opens in new tab)[1] (opens in new tab) and the Center for AI Standards and Innovation (opens in new tab) that all allow for that kind of public-private partnership on actually testing and advancing the science and best practices around standards.&nbsp;



And there are other innovative, kind of, partnerships, as well, in the ecosystem. You know, Singapore has recently launched their Global AI Assurance Pilot (opens in new tab) findings. And that effort really paired application deployers and testers so that consequential impacts at deployment could really be tested. And that&#8217;s a really fruitful, sort of, effort that complements the work of these institutes and centers that are more focused on evaluation at the model level, for example.



And in general, you know, I think that there&#8217;s just really a lot of benefits for us thinking expansively about what we can accomplish through deep, meaningful public-private partnership in this space. I&#8217;m really excited to see where we can go from here with building on, you know, partnerships across AI supply chains and with governments and public-private partnerships.&nbsp;



SULLIVAN: I couldn&#8217;t agree more. I mean, this notion of more engagement across the ecosystem and value chain is super important for us and informs how we think about the space completely.&nbsp;



If you could invite any other industry to the next workshop, maybe quantum safety, space tech, even gaming, who&#8217;s on your wish list? And maybe what are some of the things you&#8217;d want to go deeper on?&nbsp;



CRAIG DECKARD: This is something that we really welcome feedback on if anyone listening has ideas about other domains that would be interesting to study. I will say, I think I shared at the outset of this podcast series, the domains that we added in this round of our efforts in studying other domains actually all came from feedback that we received from, you know, folks we‚Äôd engaged with our first study of other domains and multilateral, sort of, governance institutions. And so we&#8217;re really keen to think about what other domains could be interesting to study. And we are also keen to go deeper, building on what we learned in this round of effort going forward.&nbsp;



One of the areas that I am particularly really interested in is going deeper on, what, sort of, transparency and information sharing about risk evaluation and testing will be really useful to share in different contexts? So across the AI supply chain, what is the information that&#8217;s going to be really meaningful to share between developers and deployers of models and systems and those that are ultimately using this technology in particular deployment contexts? And, you know, I think that we could have much to learn from other general-purpose technologies like genome editing and nanotechnology and cybersecurity, where we could learn a bit more about the kinds of information that they have shared across the development and deployment life cycle and how that has strengthened risk management in general as well as provided a really strong feedback loop around testing and evaluation. What kind of testing is most useful to do at what point in the life cycle, and what artifacts are most useful to share as a result of that testing and evaluation work?



I&#8217;ll say, as Microsoft, we have been really investing in how we are sharing information with our various stakeholders. We also have been engaged with others in industry in reporting what we&#8217;ve done in the context of the Hiroshima AI Process, or HAIP, Reporting Framework (opens in new tab). This is an effort that is really just in its first round of really exploring how this kind of reporting can be really additive to risk management understanding. And again, I think there&#8217;s real opportunity here to look at this kind of reporting and understand, you know, what&#8217;s valuable for stakeholders and where is there opportunity to go further in really informing value chains and policymakers and the public about AI risk and opportunity and what can we learn again from other domains that have done this kind of work over decades to really refine that kind of information sharing.¬†



SULLIVAN: It&#8217;s really great to hear about all the advances that we&#8217;re making on these reports. I&#8217;m guessing a lot of the metrics in there are technical, but sociotechnical impacts‚Äîjobs, maybe misinformation, well-being‚Äîare harder to score. What new measurement ideas are you excited about, and do you have any thoughts on, like, who needs to pilot those?



CRAIG DECKARD: Yeah, it&#8217;s an incredibly interesting question that I think also just speaks to, you know, the breadth of, sort of, testing and evaluation that&#8217;s needed at different points along that AI life cycle and really not getting lost in one particular kind of testing or another pre- or post-deployment and thinking expansively about the risks that we&#8217;re trying to address through this testing.&nbsp;



You know, for example, even with the UK&#8217;s AI Security Institute (opens in new tab) that has just recently launched a new program, a new team, that&#8217;s focused on societal resilience research. I think it&#8217;s going to be a really important area from a sociotechnical impact perspective to bring some focus into as this technology is more widely deployed. Are we understanding the impacts over time as different people and different cultures adopt and use this technology for different purposes?&nbsp;



And I think that&#8217;s an area where there really is opportunity for greater public-private partnership in this research. Because we all share this long-term interest in ensuring that this technology is really serving people and we have to understand the impacts so that we understand, you know, what adjustments we can actually pursue sooner upstream to address those impacts and make sure that this technology is really going to work for all of us and in a way that is consistent with the societal values that we want.&nbsp;



SULLIVAN: So, Amanda, looking ahead, I would love to hear just what&#8217;s going to be on your radar? What&#8217;s top of mind for you in the coming weeks?



CRAIG DECKARD: Well, we are certainly continuing to process all the learnings that we&#8217;ve had from studying these domains. It‚Äôs really been a rich set of insights that we want to make sure we, kind of, fully take advantage of. And, you know, I think these hard questions and, you know, real opportunities to be thoughtful in these early days of AI governance are not, sort of, going away or being easily resolved soon. And so I think we continue to see value in really learning from others, thinking about what&#8217;s distinct in the AI context, but also what we can apply in terms of what other domains have learned.



SULLIVAN: Well, Amanda, it has been such a special experience for me to help illuminate the work of the Office of Responsible AI and our team in Microsoft Research, and [MUSIC] it&#8217;s just really special to see all of the work that we&#8217;re doing to help set the standard for responsible development and deployment of AI. So thank you for joining us today, and thanks for your reflections and discussion.



And to our listeners, thank you so much for joining us for the series. We really hope you enjoyed it!&nbsp;To check out all of our episodes, visit aka.ms/AITestingandEvaluation (opens in new tab), and if you want to learn more about how Microsoft approaches AI governance, you can visit microsoft.com/RAI (opens in new tab).&nbsp;



See you next time!&nbsp;



[MUSIC FADES]‚ÄØ

				
			
			
				Show more			
		
	





AI Testing and Evaluation podcast series








[1] (opens in new tab) Since the launch of the International Network of AI Safety Institutes, the UK renamed its institute the AI Security Institute (opens in new tab).
Opens in a new tabThe post AI Testing and Evaluation: Reflections appeared first on Microsoft Research.
‚Ä¢ Customize Amazon Nova in Amazon SageMaker AI using Direct Preference Optimization
  At the AWS Summit in New York City, we introduced a comprehensive suite of model customization capabilities for Amazon Nova foundation models. Available as ready-to-use recipes on&nbsp;Amazon SageMaker AI, you can use them to adapt Nova Micro, Nova Lite, and Nova Pro across the model training lifecycle, including pre-training, supervised fine-tuning, and alignment. 
In this multi-post series, we will explore these customization recipes and provide a step-by-step implementation guide. We are starting with Direct Preference Optimization (DPO, an alignment technique that offers a straightforward way to tune model outputs with your preferences. DPO uses prompts paired with two responses‚Äîone preferred over the other‚Äîto guide the model toward outputs that better reflect your desired tone, style, or guidelines. You can implement this technique using either parameter-efficient or full model DPO, based on your data volume and cost considerations. The customized models can be deployed to Amazon Bedrock for inference using provisioned throughput. The parameter-efficient version supports on-demand inference. Nova customization recipes are available in&nbsp;SageMaker training jobs&nbsp;and&nbsp;SageMaker HyperPod, giving you flexibility to select the environment that best fits your infrastructure and scale requirements. 
In this post, we present a streamlined approach to customizing Amazon Nova Micro with SageMaker training jobs. 
Solution overview 
The workflow for using Amazon Nova recipes with SageMaker training jobs, as illustrated in the accompanying diagram, consists of the following steps: 
 
 The user selects a specific Nova customization recipe which provides comprehensive configurations to control Amazon Nova training parameters, model settings, and distributed training strategies. You can use the default configurations optimized for the SageMaker AI environment or customize them to experiment with different settings. 
 The user submits an API request to the SageMaker AI control plane, passing the Amazon Nova recipe configuration. 
 SageMaker uses the training job launcher script to run the Nova recipe on a managed compute cluster. 
 Based on the selected recipe, SageMaker AI provisions the required infrastructure, orchestrates distributed training, and, upon completion, automatically decommissions the cluster. 
 
This streamlined architecture delivers a fully managed user experience, so you can quickly define Amazon Nova training parameters and select your preferred infrastructure using straightforward recipes, while SageMaker AI handles the end-to-end infrastructure management‚Äîwithin a pay-as-you-go pricing model that is only billed for the net training time in seconds. 
 
The customized Amazon Nova model is subsequently deployed on Amazon Bedrock using the createcustommodel API within Bedrock ‚Äì and can integrate with native tooling such as Amazon Bedrock Knowledge Bases, Amazon Bedrock Guardrails, and Amazon Bedrock Agents. 
Business Use Case ‚Äì Implementation Walk-through 
In this post, we focus on adapting the Amazon Nova Micro model to optimize structured function calling for application-specific agentic workflows. We demonstrate how this approach can optimize Amazon Nova models for domain-specific use cases by a 81% increase in F1 score and up to 42% gains in ROUGE metrics. These improvements make the models more efficient in addressing a wide array of business applications, such as enabling customer support AI assistants to intelligently escalate queries, powering digital assistants for scheduling and workflow automation, and automating decision-making in sectors like ecommerce and financial services. 
As shown in the following diagram, our approach uses DPO to align the Amazon Nova model with human preferences by presenting the model with pairs of responses‚Äîone preferred by human annotators and one less preferred‚Äîbased on a given user query and available tool actions. The model is trained with the nvidia/When2Call dataset to increase the likelihood of the tool_call response, which aligns with the business goal of automating backend actions when appropriate. Over many such examples, the Amazon Nova model learns not just to generate correct function-calling syntax, but also to make nuanced decisions about when and how to invoke tools in complex workflows‚Äîimproving its utility in business applications like customer support automation, workflow orchestration, and intelligent digital assistants. 
 
When training is complete, we evaluate the models using SageMaker training jobs with the appropriate evaluation recipe. An evaluation recipe is a YAML configuration file that defines how your Amazon Nova large language model (LLM) evaluation job will be executed. Using this evaluation recipe, we measure both the model‚Äôs task-specific performance and its alignment with the desired agent behaviors, so we can quantitatively assess the effectiveness of our customization approach. The following diagram illustrates how these stages can be implemented as two separate training job steps. For each step, we use built-in integration with Amazon CloudWatch to access logs and monitor system metrics, facilitating robust observability. After the model is trained and evaluated, we deploy the model using the Amazon Bedrock Custom Model Import functionality as part of step 3. 
 
Prerequisites 
You must complete the following prerequisites before you can run the Amazon Nova Micro model fine-tuning notebook: 
 
 Make the following quota increase requests for SageMaker AI. For this use case, you will need to request a minimum of 2 p5.48xlarge instance (with 8 x NVIDIA H100 GPUs) and scale to more p5.48xlarge instances (depending on time-to-train and cost-to-train trade-offs for your use case). On the Service Quotas console, request the following SageMaker AI quotas: 
   
   P5 instances (p5.48xlarge) for training job usage: 2 
    
 (Optional) You can create an Amazon SageMaker Studio domain (refer to Use quick setup for Amazon SageMaker AI) to access Jupyter notebooks with the preceding role. (You can use JupyterLab in your local setup, too.) 
 Create an AWS Identity and Access Management (IAM) role with managed policies AmazonSageMakerFullAccess, AmazonS3FullAccess,&nbsp;and AmazonBedrockFullAccess to give required access to SageMaker AI and Amazon Bedrock to run the examples. 
 Assign the following policy as the trust relationship to your IAM role: 
 
 
 {
&nbsp;&nbsp; &nbsp;"Version": "2012-10-17",
&nbsp;&nbsp; &nbsp;"Statement": [
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;{
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"Sid": "",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"Effect": "Allow",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"Principal": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"Service": [
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"bedrock.amazonaws.com",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"sagemaker.amazonaws.com"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;]
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;},
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"Action": "sts:AssumeRole"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp;]
} 
 
 
 Clone the GitHub repository with the assets for this deployment. This repository consists of a notebook that references training assets: 
   
   git clone https://github.com/aws-samples/sagemaker-distributed-training-workshop.git

cd sagemaker-distributed-training-workshop/18_sagemaker_training_recipes/nova 
    
 
Next, we run the notebook nova-micro-dpo-peft.ipynb to fine-tune the Amazon Nova model using DPO, and PEFT on SageMaker training jobs. 
Prepare the dataset 
To prepare the dataset, you need to load the nvidia/When2Call dataset. This dataset provides synthetically generated user queries, tool options, and annotated preferences based on real scenarios, to train and evaluate AI assistants on making optimal tool-use decisions in multi-step scenarios. 
Complete the following steps to format the input in a chat completion format, and configure the data channels for SageMaker training jobs on Amazon Simple Storage Service (Amazon S3): 
 
 Load the nvidia/When2Call dataset: 
 
 
 from&nbsp;datasets&nbsp;import&nbsp;load_dataset
dataset&nbsp;=&nbsp;load_dataset("nvidia/When2Call", "train_pref",&nbsp;split="train") 
 
The DPO technique requires a dataset containing the following: 
 
 User prompts (e.g., ‚ÄúWrite a professional email asking for a raise‚Äù) 
 Preferred outputs (ideal responses) 
 Non-preferred outputs (undesirable responses) 
 
The following code is an example from the original dataset: 
 
  
 
 
 As part of data preprocessing, we convert the data into the format required by Amazon Nova Micro, as shown in the following code. For examples and specific constraints of the Amazon Nova format, see Preparing data for fine-tuning Understanding models. 
 
 
For the full data conversion code, see here. 
 
 Split the dataset into train and test datasets: 
 
 
 from&nbsp;datasets&nbsp;import&nbsp;Dataset, DatasetDict
from&nbsp;random&nbsp;import&nbsp;randint

...

dataset&nbsp;=&nbsp;DatasetDict(
&nbsp;&nbsp; &nbsp;{"train": train_dataset, "test": test_dataset, "val": val_dataset}
)
train_dataset&nbsp;=&nbsp;dataset["train"].map(
&nbsp;&nbsp; &nbsp;prepare_dataset, remove_columns=train_dataset.features
)

test_dataset&nbsp;=&nbsp;dataset["test"].map(
&nbsp;&nbsp; &nbsp;prepare_dataset, remove_columns=test_dataset.features
) 
 
 
 Prepare the training and test datasets for the SageMaker training job by saving them as .jsonl files, which is required by SageMaker HyperPod recipes for Amazon Nova, and constructing the Amazon S3 paths where these files will be uploaded: 
 
 
 ...

train_dataset.to_json("./data/train/dataset.jsonl")
test_dataset.to_json("./data/test/dataset.jsonl")


s3_client.upload_file(
&nbsp;&nbsp; &nbsp;"./data/train/dataset.jsonl", bucket_name, f"{input_path}/train/dataset.jsonl"
)
s3_client.upload_file(
&nbsp;&nbsp; &nbsp;"./data/test/dataset.jsonl", bucket_name, f"{input_path}/test/dataset.jsonl"
) 
 
DPO training using SageMaker training jobs 
To fine-tune the model using DPO and SageMaker training jobs with recipes, we use the PyTorch Estimator class. Start by setting the fine-tuning workload with the following steps: 
 
 Select the instance type and the container image for the training job: 
 
 
 instance_type = "ml.p5.48xlarge" 
instance_count = 2

image_uri = (
    f"708977205387.dkr.ecr.{sagemaker_session.boto_session.region_name}.amazonaws.com/nova-fine-tune-repo:SM-TJ-DPO-latest"
) 
 
 
 Create the PyTorch Estimator to encapsulate the training setup from a selected Amazon Nova recipe: 
 
 
  
  from sagemaker.pytorch import PyTorch

# define Training Job Name
job_name = "train-nova-micro-dpo"

recipe_overrides = {
    "training_config": {
        "trainer": {"max_epochs": 1},
        "model": {
            "dpo_cfg": {"beta": 0.1},
            "peft": {
                "peft_scheme": "lora",
                "lora_tuning": {
                    "loraplus_lr_ratio": 16.0,
                    "alpha": 128,
                    "adapter_dropout": 0.01,
                },
            },
        },
    },
}

estimator = PyTorch(
    output_path=f"s3://{bucket_name}/{job_name}",
    base_job_name=job_name,
    role=role,
    instance_count=instance_count,
    instance_type=instance_type,
    training_recipe=recipe,
    recipe_overrides=recipe_overrides,
    max_run=18000,
    sagemaker_session=sess,
    image_uri=image_uri,
    disable_profiler=True,
    debugger_hook_config=False,
) 
  
 
You can point to the specific recipe with the training_recipe&nbsp;parameter and override the recipe by providing a dictionary as recipe_overrides parameter. 
The PyTorch Estimator class simplifies the experience by encapsulating code and training setup directly from the selected recipe. 
In this example, training_recipe:&nbsp;fine-tuning/nova/dpo-peft-nova-micro-v1 is defining the DPO fine-tuning setup with PEFT technique 
 
 Set up the input channels for the PyTorch Estimator by creating an TrainingInput objects from the provided S3 bucket paths for the training and test datasets: 
 
 
 from&nbsp;sagemaker.inputs&nbsp;import&nbsp;TrainingInput

train_input&nbsp;=&nbsp;TrainingInput(
&nbsp;&nbsp; &nbsp;s3_data=train_dataset_s3_path,
&nbsp;&nbsp; &nbsp;distribution="FullyReplicated",
&nbsp;&nbsp; &nbsp;s3_data_type="Converse",
)
test_input&nbsp;=&nbsp;TrainingInput(
&nbsp;&nbsp; &nbsp;s3_data=test_dataset_s3_path,
&nbsp;&nbsp; &nbsp;distribution="FullyReplicated",
&nbsp;&nbsp; &nbsp;s3_data_type="Converse",
) 
 
 
 Submit the training job using the fit function call on the created Estimator: 
 
estimator.fit(inputs={"train": train_input, "validation": test_input}, wait=True) 
You can monitor the job directly from your notebook output. You can also refer the SageMaker AI console, which shows the status of the job and the corresponding CloudWatch logs for governance and observability, as shown in the following screenshots. 

 
 SageMaker training jobs console
 

 
 SageMaker training jobs system metrics
 
After the job is complete, the trained model weights will be available in an escrow S3 bucket. This secure bucket is controlled by Amazon and uses special access controls. You can access the paths shared in manifest files that are saved in a customer S3 bucket as part of the training process. 
Evaluate the fine-tuned model using the evaluation recipe 
To assess model performance against benchmarks or custom datasets, we can use the Nova evaluation recipes and SageMaker training jobs to execute an evaluation workflow, by pointing to the model trained in the previous step. Among several supported benchmarks, such as mmlu, math, gen_qa, and llm_judge, in the following steps we are going to provide two options for &nbsp;gen_qa and llm_judge tasks, which allow us to evaluate response accuracy, precision and model inference quality with the possibility to use our own dataset and compare results with the base model on Amazon Bedrock. 
Option A: Evaluate gen_qa task 
 
 Use the code in the to prepare the dataset, structured in the following format as required by the evaluation recipe: 
 
 
  
  {
    "system": "(Optional) String containing the system prompt that sets the behavior, role, or personality of the model",
    "query": "String containing the input prompt",
    "response": "String containing the expected model output"
} 
  
 
 
 Save the dataset as .jsonl files, which is required by Amazon Nova evaluation recipes, and upload them to the Amazon S3 path: 
 
 
 # Save datasets to s3
val_dataset.to_json("./data/val/gen_qa.jsonl")

s3_client.upload_file(
&nbsp;&nbsp; &nbsp;"./data/val/gen_qa.jsonl", bucket_name, f"{input_path}/val/gen_qa.jsonl"
)
... 
 
 
 Create the evaluation recipe pointing to trained model, validation data, and the evaluation metrics applicable to your use case: 
 
 
 model_path = "&lt;ESCROW_S3_PATH_MODEL_CHECKPOINTS&gt;"

recipe_content = f"""
run:
&nbsp;&nbsp;name: nova-micro-gen_qa-eval-job
&nbsp;&nbsp;model_type: amazon.nova-micro-v1:0:128k
&nbsp;&nbsp;model_name_or_path: {model_path}
&nbsp;&nbsp;replicas: 1
&nbsp;&nbsp;data_s3_path: {val_dataset_s3_path} # Required, input data s3 location

evaluation:
&nbsp;&nbsp;task: gen_qa
&nbsp;&nbsp;strategy: gen_qa
&nbsp;&nbsp;metric: all
&nbsp;&nbsp; &nbsp;
inference:
&nbsp;&nbsp;max_new_tokens: 4096
&nbsp;&nbsp;top_p: 0.9
&nbsp;&nbsp;temperature: 0.1
"""

with open("eval-recipe.yaml", "w") as f:
&nbsp;&nbsp;f.write(recipe_content) 
 
 
 Select the instance type, the container image for the evaluation job, and define the checkpoint path where the model will be stored. The recommended instance types for the Amazon Nova evaluation recipes are:&nbsp; ml.g5.12xlarge for Amazon Nova Micro and Amazon Nova Lite, and ml.g5.48xlarge for Amazon Nova Pro: 
 
 
 instance_type = "ml.g5.12xlarge" 
instance_count = 1

image_uri = (
    f"708977205387.dkr.ecr.{sagemaker_session.boto_session.region_name}.amazonaws.com/nova-evaluation-repo:SM-TJ-Eval-latest"
) 
 
 
 Create the PyTorch Estimator to encapsulate the evaluation setup from the created recipe: 
 
 
 from sagemaker.pytorch import PyTorch

# define Training Job Name
job_name = "train-nova-micro-eval"

estimator = PyTorch(
&nbsp;&nbsp; &nbsp;output_path=f"s3://{bucket_name}/{job_name}",
&nbsp;&nbsp; &nbsp;base_job_name=job_name,
&nbsp;&nbsp; &nbsp;role=role,
&nbsp;&nbsp; &nbsp;instance_count=instance_count,
&nbsp;&nbsp; &nbsp;instance_type=instance_type,
&nbsp;&nbsp; &nbsp;training_recipe="./eval-recipe.yaml",
&nbsp;&nbsp; &nbsp;max_run=18000,
&nbsp;&nbsp; &nbsp;sagemaker_session=sagemaker_session,
&nbsp;&nbsp; &nbsp;image_uri=image_uri,
    disable_profiler=True,
    debugger_hook_config=False,
) 
 
 
 Set up the input channels for PyTorch Estimator by creating an TrainingInput objects from the provided S3 bucket paths for the validation dataset: 
 
 
 from&nbsp;sagemaker.inputs&nbsp;import&nbsp;TrainingInput

eval_input&nbsp;=&nbsp;TrainingInput(
&nbsp;&nbsp; &nbsp;s3_data=val_dataset_s3_path,
&nbsp;&nbsp; &nbsp;distribution="FullyReplicated",
&nbsp;&nbsp; &nbsp;s3_data_type="S3Prefix",
) 
 
 
 Submit the training job: 
 
estimator.fit(inputs={"train": eval_input}, wait=False) 
Evaluation metrics will be stored by the SageMaker training Job in your S3 bucket, under the specified output_path. 
The following figure and accompanying table show the evaluation results against the base model for the gen_qa task: 
 
 
  
   
    
   F1 
   F1 QUASI 
   ROUGE 1 
   ROUGE 2 
   ROUGE L 
   
   
   Base 
   0.26 
   0.37 
   0.38 
   0.28 
   0.34 
   
   
   Fine-tuned 
   0.46 
   0.52 
   0.52 
   0.4 
   0.46 
   
   
   % Difference 
   81% 
   40% 
   39% 
   42% 
   38% 
   
  
 
Option B: Evaluate llm_judge task 
 
 For the llm_judge task, structure the dataset with the below format, where response_A represents the ground truth and response_B represents our customized model output: 
 
 
 {
    "prompt": "String containing the input prompt and instructions",
    "response_A": "String containing the ground truth output",
    "response_B": "String containing the customized model output"
}
 
 
 
 Following the same approach described for the gen_qa task, create an evaluation recipe specifically for the llm_judge task, by specifying judge as strategy: 
 
 
 recipe_content = f"""
run:
  name: nova-micro-llm-judge-eval-job
  model_type: amazon.nova-micro-v1:0:128k
  model_name_or_path: "nova-micro/prod"
  ...

evaluation:
  task: llm_judge
  strategy: judge
  metric: all

...
""" 
 
The complete implementation including dataset preparation, recipe creation, and job submission steps, refer to the notebook nova-micro-dpo-peft.ipynb. 
The following figure shows the results for the llm_judge task: 
 
This graph shows the preference percentages when using an LLM as a judge to evaluate model performance across two different comparisons. In Graph 1, the fine-tuned model outperformed the ground truth with 66% preference versus 34%, while in Graph 2, the base model achieved 56% preference compared to the ground truth‚Äôs 44%. 
Summarized evaluation results 
Our fine-tuned model delivers significant improvements on the tool-calling task, outperforming the base model across all key evaluation metrics. Notably, the F1 score increased by 81%, while the F1 Quasi score improved by 35%, reflecting a substantial boost in both precision and recall. In terms of lexical overlap, the model demonstrated enhanced accuracy in matching generated answers to reference texts ‚Äîtools to invoke and structure of the invoked function‚Äî achieving gains of 39% and 42% for ROUGE-1 and ROUGE-2 scores, respectively. The llm_judge evaluation further validates these improvements, with the fine-tuned model outputs being preferred in 66.2% against the ground truth outputs. These comprehensive results across multiple evaluation frameworks confirm the effectiveness of our fine-tuning approach in elevating model performance for real-world scenarios. 
Deploy the model on Amazon Bedrock 
To deploy the fine-tuned model, we can use the Amazon Bedrock CreateCustomModel API and use Bedrock On-demand inference with the native model invocation tools. To deploy the model, complete the following steps: 
 
 Create a custom model, by pointing to the model checkpoints saved in the escrow S3 bucket: 
 
 
 ...
model_path&nbsp;=&nbsp;"&lt;ESCROW_S3_PATH_MODEL_CHECKPOINTS&gt;"
# Define name for imported model
imported_model_name&nbsp;=&nbsp;"nova-micro-sagemaker-dpo-peft"

request_params&nbsp;=&nbsp;{
&nbsp;&nbsp; &nbsp;"modelName": imported_model_name,
&nbsp;&nbsp; &nbsp;"modelSourceConfig": {"s3DataSource": {"s3Uri": model_path}},
&nbsp;&nbsp; &nbsp;"roleArn": role,
&nbsp;&nbsp; &nbsp;"clientRequestToken": "NovaRecipeSageMaker",
}
# Create the model import 
response&nbsp;=&nbsp;bedrock.create_custom_model(**request_params) 
 
 
 Monitor the model status. Wait until the model reaches the status ACTIVE or FAILED: 
 
 
 from IPython.display import clear_output
import time

while True:
&nbsp;&nbsp; &nbsp;response = bedrock.list_custom_models(sortBy='CreationTime',sortOrder='Descending')
&nbsp;&nbsp; &nbsp;model_summaries = response["modelSummaries"]
&nbsp;&nbsp; &nbsp;status = ""
&nbsp;&nbsp; &nbsp;for model in model_summaries:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;if model["modelName"] == imported_model_name:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;status = model["modelStatus"].upper()
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;model_arn = model["modelArn"]
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;print(f'{model["modelStatus"].upper()} {model["modelArn"]} ...')
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;if status in ["ACTIVE", "FAILED"]:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;break
&nbsp;&nbsp; &nbsp;if status in ["ACTIVE", "FAILED"]:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;break
&nbsp;&nbsp; &nbsp;clear_output(wait=True)
&nbsp;&nbsp; &nbsp;time.sleep(10) 
 
When the model import is complete, you will see it available through the AWS CLI: 
 
 aws bedrock list-custom-models
{
    "modelSummaries": [
        {
            "modelArn": "arn:aws:bedrock:us-east-1: 123456789101:custom-model/imported/abcd1234efgh",
            "modelName": "nova-micro-sagemaker-dpo-peft",
            "creationTime": "2025-07-16T12:52:39.348Z",
            "baseModelArn": "arn:aws:bedrock:us-east-1::foundation-model/amazon.nova-micro-v1:0:128k",
            "baseModelName": "",
            "customizationType": "IMPORTED",
            "ownerAccountId": "123456789101",
            "modelStatus": "Active"
        }
    ]
} 
 
 
 Configure Amazon Bedrock Custom Model on-demand inference: 
 
 
 request_params = {
    "clientRequestToken": "NovaRecipeSageMakerODI",
    "modelDeploymentName": f"{imported_model_name}-odi",
    "modelArn": model_arn,
}

response = bedrock.create_custom_model_deployment(**request_params)
 
 
 
 Monitor the model deployment status. Wait until the model reaches the status ACTIVE or FAILED: 
 
 
 from IPython.display import clear_output
import time

while True:
    response = bedrock.list_custom_model_deployments(
        sortBy="CreationTime", sortOrder="Descending"
    )
    model_summaries = response["modelDeploymentSummaries"]
    status = ""
    for model in model_summaries:
        if model["customModelDeploymentName"] == f"{imported_model_name}-odi":
            status = model["status"].upper()
            custom_model_arn = model["customModelDeploymentArn"]
            print(f'{model["status"].upper()} {model["customModelDeploymentArn"]} ...')
            if status in ["CREATING"]:
                break
    if status in ["ACTIVE", "FAILED"]:
        break
    clear_output(wait=True)
    time.sleep(10)
 
 
 
 Run model inference through AWS SDK: 
 
 
 tools =&nbsp;[
&nbsp; &nbsp; {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"toolSpec": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"name": "fetch_weather",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"description": 'Fetch weather information',
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"inputSchema": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"json": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"type": "object",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"properties": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"type": "object",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"properties": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"query": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"type": "string",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"description": "Property query",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;},
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"num_results": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"type": "integer",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"description": "Property num_results",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;},
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;},
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"required": ["query"],
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;},
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;},
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;},
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp;}
&nbsp; &nbsp; ...
]

system_prompt = f"""
You are a helpful AI assistant that can answer questions and provide information.
You can use tools to help you with your tasks.

You have access to the following tools:

&lt;tools&gt;
{{tools}}
&lt;/tools&gt;
For each function call, return a json object with function name and parameters:

{{{{\"name\": \"function name\", \"parameters\": \"dictionary of argument name and its value\"}}}}
"""

system_prompt = system_prompt.format(tools=json.dumps({'tools': tools}))

messages = [
{"role": "user", "content": [{"text": "What is the weather in New York?"}]},
] 
 
 
 Submit the inference request by using the converse API: 
 
 
 response = client.converse(
 &nbsp; &nbsp;modelId=model_arn,
 &nbsp; &nbsp;messages=messages, 
 &nbsp; &nbsp;system=["text":&nbsp;system_prompt],
 &nbsp; &nbsp;inferenceConfig={
&nbsp; &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;"temperature": temperature, 
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"maxTokens": max_tokens, 
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"topP": top_p
&nbsp;&nbsp;&nbsp;},
)

response["output"] 
 
We get the following output response: 
 
 {
   "message":{
      "role":"assistant",
      "content":[
         {
            "text":"{\"name\": \"fetch_weather\", \"parameters\": {\"query\": \"Rome, Italy\"}}"
         }
      ]
   }
}
 
 
Clean up 
To clean up your resources and avoid incurring more charges, follow these steps: 
 
 Delete unused SageMaker Studio resources 
 (Optional) Delete the SageMaker Studio domain 
 On the SageMaker console, choose Training in the navigation pane and verify that your training job isn‚Äôt running anymore. 
 Delete custom model deployments in Amazon Bedrock. To do so, use the AWS CLI or AWS SDK to delete it. 
 
Conclusion 
This post demonstrates how you can customize Amazon Nova understanding models using the DPO recipe on SageMaker training jobs. The detailed walkthrough with a specific focus on optimizing tool calling capabilities showcased significant performance improvements, with the fine-tuned model achieving up to 81% better F1 scores compared to the base model with training dataset of around 8k records. 
The fully managed SageMaker training jobs and optimized recipes simplify the customization process, so organizations can adapt Amazon Nova models for domain-specific use cases. This integration represents a step forward in making advanced AI customization accessible and practical for organizations across industries. 
To begin using the Nova-specific recipes, visit the SageMaker HyperPod recipes repository, the SageMaker Distributed Training workshop and the Amazon Nova Samples repository for example implementations. Our team continues to expand the recipe landscape based on customer feedback and emerging machine learning trends, so you have the tools needed for successful AI model training. 
 
About the authors 
Mukund Birje is a Sr. Product Marketing Manager on the AIML team at AWS. In his current role he‚Äôs focused on driving adoption of Amazon Nova Foundation Models. He has over 10 years of experience in marketing and branding across a variety of industries. Outside of work you can find him hiking, reading, and trying out new restaurants. You can connect with him on LinkedIn. 
Karan Bhandarkar is a Principal Product Manager with Amazon Nova. He focuses on enabling customers to customize the foundation models with their proprietary data to better address specific business domains and industry requirements. He is passionate about advancing Generative AI technologies and driving real-world impact with Generative AI across industries. 
&nbsp;Kanwaljit Khurmi&nbsp;is a Principal Worldwide Generative AI Solutions Architect at AWS. He collaborates with AWS product teams, engineering departments, and customers to provide guidance and technical assistance, helping them enhance the value of their hybrid machine learning solutions on AWS. Kanwaljit specializes in assisting customers with containerized applications and high-performance computing solutions. 
&nbsp;Bruno Pistone is a Senior World Wide Generative AI/ML Specialist Solutions Architect at AWS based in Milan, Italy. He works with AWS product teams and large customers to help them fully understand their technical needs and design AI and Machine Learning solutions that take full advantage of the AWS cloud and Amazon Machine Learning stack. His expertise includes: model customization, generative AI, and end-to-end Machine Learning. He enjoys spending time with friends, exploring new places, and traveling to new destinations.
‚Ä¢ Multi-tenant RAG implementation with Amazon Bedrock and Amazon OpenSearch Service for SaaS using JWT
  In recent years, the emergence of large language models (LLMs) has accelerated AI adoption across various industries. However, to further augment LLMs‚Äô capabilities and effectively use up-to-date information and domain-specific knowledge, integration with external data sources is essential. Retrieval Augmented Generation (RAG) has gained attention as an effective approach to address this challenge. 
RAG is a technique that searches relevant information from existing knowledge bases or documents based on user input, and incorporates this information into the LLM input to generate more accurate and contextually appropriate responses. This technique is being implemented across a wide range of applications, from using technical documentation in product development to answering FAQs in customer support, and even supporting decision-making systems based on the latest data. 
The implementation of RAG brings significant value to both software-as-a-service (SaaS) providers and their users (tenants). 
SaaS providers can use a multi-tenant architecture that delivers services to multiple tenants from a single code base. As tenants use the service, their data accumulates while being protected by appropriate access control and data isolation. When implementing AI capabilities using LLMs in such environments, RAG makes it possible to use each tenant‚Äôs specific data to provide personalized AI services. 
Let‚Äôs consider a customer service call center SaaS as an example. Each tenant‚Äôs historical inquiry records, FAQs, and product manuals are accumulated as tenant-specific knowledge bases. By implementing a RAG system, the LLM can generate appropriate responses relevant to each tenant‚Äôs context by referencing these tenant-specific data sources. This enables highly accurate interactions that incorporate tenant-specific business knowledge‚Äîa level of customization that would not be possible with generic AI assistants. RAG serves as a crucial component for delivering personalized AI experiences in SaaS, contributing to service differentiation and value enhancement. 
However, using tenant-specific data through RAG presents technical challenges from security and privacy perspectives. The primary concern is implementing secure architecture that maintains data isolation between tenants and helps prevent unintended data leakage or cross-tenant access. In multi-tenant environments, the implementation of data security critically impacts the trustworthiness and competitive advantage of SaaS providers. 
Amazon Bedrock Knowledge Bases enables simpler RAG implementation. When using OpenSearch as a vector database, there are two options: Amazon OpenSearch Service or Amazon OpenSearch Serverless. Each option has different characteristics and permission models when building multi-tenant environments: 
 
 Amazon OpenSearch Serverless: 
   
   Metadata filtering enables filtering of search results from the vector database by tenant (for more details, see Multi-tenant RAG with Amazon Bedrock Knowledge Bases) 
   Its permission model doesn‚Äôt segregate permissions for write operations such as data creations and updates 
    
 Amazon OpenSearch Service: 
   
   Fine-grained access control (FGAC)is available 
   Access is through a single AWS Identity and Access Management (IAM) role attached to the knowledge base, helping prevent the use of FGAC for permission segregation 
    
 
In this post, we introduce tenant isolation patterns using a combination of JSON Web Token (JWT) and FGAC, along with tenant resource routing. If the aforementioned permission model limits you from achieving your FGAC objectives, you can use the solution in this post. The solution is implemented using OpenSearch Service as the vector database and AWS Lambda as the orchestration layer. 
In the next section, we explore the specific implementation of tenant isolation using JWT and FGAC in OpenSearch Service, and how this enables a secure multi-tenant RAG environment. 
Effectiveness of JWT in multi-tenant data isolation in OpenSearch Service 
As introduced in Storing Multi-Tenant SaaS Data with Amazon OpenSearch Service, OpenSearch Service offers multiple methods for managing multi-tenant data: domain-level isolation, index-level isolation, and document-level isolation. 
To implement access permission segregation at the index and document levels, you can use FGAC, which is supported by the OpenSearch Security plugin. 
In OpenSearch Service, you can achieve granular access control by mapping IAM identities to OpenSearch roles. This enables detailed permission settings in OpenSearch for each IAM identity. However, this approach presents significant scalability challenges. As the number of tenants increases, the required number of IAM users or roles also increases, potentially hitting the limit of AWS service quotas. Additionally, managing numerous IAM entities leads to operational complexity. Although dynamically generated IAM policies could overcome this challenge, each dynamically generated policy is attached to a single IAM role. A single IAM role can be mapped to a single OpenSearch role, but this would still require an IAM role and dynamic policy per tenant for appropriate isolation, which results in similar operational complexity managing numerous entities. 
This post provides an alternative approach and focuses on the effectiveness of JWT, a self-contained token for implementing data isolation and access control in multi-tenant environments. Using JWT provides the following advantages: 
 
 Dynamic tenant identification ‚Äì JWT payloads can include attribute information (tenant context) to identify tenants. This enables the system to dynamically identify tenants for each request and allows passing this context to subsequent resources and services. 
 Integration with FGAC in OpenSearch ‚Äì FGAC can directly use attribute information in JWT for role mapping. This allows mapping of access permissions to specific indexes or documents based on information such as tenant IDs in the JWT. 
 
Combining JWT with FGAC provides secure, flexible, and scalable data isolation and access control in a multi-tenant RAG environment using OpenSearch Service. In the next section, we explore specific implementation details and technical considerations for applying this concept in actual systems. 
Solution overview 
In RAG, data such as relevant documents used to augment LLM outputs are vectorized by embedding language models and indexed in a vector database. User questions in natural language are converted to vectors using the embedding model and searched in the vector database. The data retrieved through vector search is passed to the LLM as context to augment the output. The following diagram illustrates the solution architecture. 
 
This solution uses OpenSearch Service as the vector data store for storing knowledge sources in RAG. The flow is as follows: 
 
 RAG application users for each tenant are created as users in an Amazon Cognito user pool, receiving a JWT enriched with tenant ID information when logging in to the frontend. Each user‚Äôs tenant information is stored in Amazon DynamoDB and added to the JWT by a pre-token generation Lambda trigger during user authentication. 
 When a user initiates a chat on the frontend, the user query is passed to Lambda using Amazon API Gateway along with the JWT. 
 The user query is vectorized in conjunction with text embedding models available in Amazon Bedrock. 
 Domain and index information for retrieval is obtained from DynamoDB. 
 Vector search is performed on OpenSearch Service to retrieve information related to the query from the index. 
 The retrieved information is added to the prompt as context and passed to an LLM available in Amazon Bedrock to generate a response. 
 
The key aspect of this solution is using JWT for tenant data isolation in OpenSearch Service and routing to each tenant‚Äôs data. It separates access permissions for each dataset using FGAC available in OpenSearch Service and uses tenant ID information added to the JWT for mapping application users to separated permission sets. The solution provides three different patterns for data isolation granularity to meet customer requirements. Routing is also enabled by defining the mapping between tenant ID information from JWT and data location (domain, index) in DynamoDB. 
When users add documents, files are uploaded to Amazon Simple Storage Service (Amazon S3) and metadata is written to DynamoDB management table. When storing data in OpenSearch Service, the text embedding model (Amazon Bedrock) is called by the ingest pipeline for vectorization. For document creation, update, and deletion, JWT is attached to requests, allowing tenant identification. 
This solution is implemented using the AWS Cloud Development Kit (AWS CDK). For details, refer to the GitHub repository. The instructions to deploy the solution are included in the README file in the repository. 
Prerequisites 
To try this solution, you must have the following prerequisites: 
 
 An AWS account. 
 IAM access permissions necessary for running the AWS CDK. 
 A frontend execution environment: node.js and npm installation is required. 
 The AWS CDK must be configured. For details, refer to Tutorial: Create your first AWS CDK app. 
 Access to the models used in Amazon Bedrock must be configured. This solution uses Anthropic‚Äôs Claude 3.5 Sonnet v2 and Amazon Titan Text Embedding V2. For details, refer to Add or remove access to Amazon Bedrock foundation models. 
 
In addition to the resources shown in the architecture diagram, the following resources and configurations are created as AWS CloudFormation custom resources through AWS CDK deployment: 
 
 Amazon Cognito user pool: 
   
   Users for tenant-a, tenant-b, tenant-c, and tenant-d 
    
 DynamoDB table: 
   
   Mapping between users and tenants 
   Mapping between tenants and OpenSearch connection destinations and indexes 
    
 OpenSearch Service domain: 
   
   JWT authentication settings 
   Ingest pipeline for vector embedding 
   FGAC roles and role mappings for each tenant 
   k-NN index 
    
 
User authentication and JWT generation with Amazon Cognito 
This solution uses an Amazon Cognito user pool for RAG application user authentication. Amazon Cognito user pools issue JWT during authentication. Because FGAC in OpenSearch Service supports JWT authentication, access from users authenticated by the Amazon Cognito user pool can be permitted by registering public keys issued by the user pool with the OpenSearch Service domain. Additionally, authorization is performed using attributes that can be added to the JWT payload for tenant data access permission segregation with FGAC, which we discuss in the following sections. To achieve this, a pre-token generation Lambda trigger is configured in the Amazon Cognito user pool to retrieve tenant ID information for each user stored in DynamoDB and add it to the token. The obtained JWT is retained by the frontend and used for requests to the backend. DynamoDB stores the mapping between user ID (sub) and tenant ID as follows: 
 
 {
  "pk": {
    "S": "membership#&lt;Cognito user ID (sub)&gt;"
  },
  "sk": {
    "S": "tenant#tenant-a"
  }
} 
 
Although multiple patterns exist for implementing multi-tenant authentication with Amazon Cognito, this implementation uses a single user pool with user-tenant mappings in DynamoDB. Additional considerations are necessary for production environments; refer to Multi-tenant application best practices. 
Request routing to tenant data using JWT 
In multi-tenant architectures where resources are separated by tenant, requests from tenants are essential to route to appropriate resources. To learn more about tenant routing strategies, see Tenant routing strategies for SaaS applications on AWS. This solution uses an approach similar to data-driven routing as described in the post for routing to OpenSearch Service. 
The DynamoDB table stores mapping information for tenant IDs, target OpenSearch Service domains, and indexes as follows: 
 
 {
  "pk": {
    "S": "tenant#tenant-a"
  },
  "sk": {
    "S": "os_config"
  },
  "os_host": {
    "S": "&lt;Amazon OpenSearch Service domain endpoint&gt;"
  },
  "os_index": {
    "S": "tenant-a-index"
  },
  "rag_role": {
    "S": "tenant-a_role"
  }
} 
 
The JWT is obtained from the Authorization header in HTTP requests sent from the frontend to the Lambda function through API Gateway. The routing destination is determined by retrieving the routing information using the tenant ID obtained from parsing the JWT. Additionally, the JWT is used as authentication information for requests to OpenSearch, as described in the following section. 
Multi-tenant isolation of data locations and access permissions in OpenSearch Service 
Multi-tenant data isolation strategies in OpenSearch Service include three types of isolation patterns: domain-level, index-level, and document-level isolation, and hybrid models combining these. This solution uses FGAC for access permission control to tenant data, creating dedicated roles for each tenant. 
Mapping between tenant users and FGAC tenant roles is implemented through backend roles. In JWT authentication available in OpenSearch Service, the attribute within the JWT payload to be linked with backend roles can be specified as the Roles key. The following screenshot shows this domain config. 
 
The JWT payload includes a tenant_id attribute as follows:"tenant_id": "tenant-a" Tenant users and FGAC roles are linked by setting this attribute as the roles key in OpenSearch JWT authentication and mapping roles as follows: 
 
 {
  "tenant-a_role": {
    "backend_roles": [
      "tenant-a"
    ]
  }
} 
 
The following screenshot shows an example of tenant role mapping in FGAC in OpenSearch Dashboards. 
 
The sample in this solution provides four tenants‚Äîtenant-a, tenant-b, tenant-c, and tenant-d‚Äîso you can try all three isolation methods. The following diagram illustrates this architecture. 
 
Each role is assigned permissions to access only the corresponding tenant data. In this section, we introduce how to implement each of the three isolation methods using JWT and FGAC: 
 
 Domain-level isolation ‚Äì Assign individual OpenSearch Service domains to each tenant. Because domains are dedicated to each tenant in this pattern of isolation, there‚Äôs no need for data isolation within the domain. Therefore, FGAC roles grant access permissions across the indexes. The following code is part of index_permissions in the FGAC role definition that grants access to the indexes: 
 
 
 "index_permissions": [
    {
    "index_patterns": [
        "*"
    ], 
 
 
 Index-level isolation ‚Äì Multiple tenants share an OpenSearch Service domain, with individual indexes assigned to each tenant. Each tenant should only be able to access their own index, so index_permissions in the FGAC role is configured as follows (example for tenant-b): 
 
 
 "index_permissions": [
    {
    "index_patterns": [
        "tenant-b-index*"
    ] 
 
 
 Document-level isolation ‚Äì Multiple tenants share OpenSearch Service domains and indexes, using FGAC document-level security for access permission segregation of tenant data within the index. Each index includes a field to store tenant ID information, and document-level security queries are set for that field. The following code is part of index_permissions for an FGAC role that allows tenant-c to access only its own data in a configuration where tenant-c and tenant-d share an index: 
 
 
 "index_permissions": [
    {
    "index_patterns": [
        "tenant-cd-shared-index*"
    ],
    "dls": """{"bool": {"must": {"match": {"tenant_id": "tenant-c"}}}}""", 
 
The following screenshot shows an example of index permission for document-level isolation in the FGAC role. 
 
Considerations 
The implementation in this post uses a model where DynamoDB tables and S3 buckets are shared between tenants. For production use, consider partitioning models as introduced in Partitioning Pooled Multi-Tenant SaaS Data with Amazon DynamoDB and Partitioning and Isolating Multi-Tenant SaaS Data with Amazon S3) and determine the optimal model based on your requirements. 
Additionally, you can use dynamic generation of IAM policies as an additional layer to restrict access permissions to each resource. 
Clean up 
To avoid unexpected charges, we recommend deleting resources when they are no longer needed. Because the resources are created with the AWS CDK, run the cdk destroy command to delete them. This operation will also delete the documents uploaded to Amazon S3. 
Conclusions 
In this post, we introduced a solution that uses OpenSearch Service as a vector data store in multi-tenant RAG, achieving data isolation and routing using JWT and FGAC. 
This solution uses a combination of JWT and FGAC to implement strict tenant data access isolation and routing, necessitating the use of OpenSearch Service. The RAG application is implemented independently, because at the time of writing, Amazon Bedrock Knowledge Bases can‚Äôt use JWT-based access to OpenSearch Service.Multi-tenant RAG usage is important for SaaS companies, and strategies vary depending on requirements such as data isolation strictness, ease of management, and cost. This solution implements multiple isolation models, so you can choose based on your requirements.For other solutions and information regarding multi-tenant RAG implementation, refer to the following resources: 
 
 Multi-tenant RAG with Amazon Bedrock Knowledge Bases 
 Build a multi-tenant generative AI environment for your enterprise on AWS 
 Self-managed multi-tenant vector search with Amazon Aurora PostgreSQL 
 Multi-tenant vector search with Amazon Aurora PostgreSQL and Amazon Bedrock Knowledge Bases 
 
 
 
About the authors 
Kazuki Nagasawa is a Cloud Support Engineer at Amazon Web Services. He specializes in Amazon OpenSearch Service and focuses on solving customers‚Äô technical challenges. In his spare time, he enjoys exploring whiskey varieties and discovering new ramen restaurants. 
Kensuke Fukumoto is a Senior Solutions Architect at Amazon Web Services. He‚Äôs passionate about helping ISVs and SaaS providers modernize their applications and transition to SaaS models. In his free time, he enjoys riding motorcycles and visiting saunas.

‚∏ª