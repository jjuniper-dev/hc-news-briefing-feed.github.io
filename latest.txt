‚úÖ Morning News Briefing ‚Äì July 01, 2025

üìÖ Date: 2025-07-01
üè∑Ô∏è Tags: #briefing #ai #publichealth #digitalgov

‚∏ª

üßæ Weather
‚Ä¢ No watches or warnings in effect, Pembroke
  No watches or warnings in effect. No warnings or watches or watches in effect . No
‚Ä¢ Current Conditions:  19.9¬∞C
  Temperature: 19.9&deg;C Pressure / Tendency: 100.6
‚Ä¢ Tuesday: Chance of showers. High 27. POP 40%
  Cloudy with 40 percent chance of showers early this morning . Clearing this morning.
‚Ä¢ Tuesday night: Clearing. Low 15.
  Clearing this evening, clearing this evening . Clearing weather expected to clear this evening
‚Ä¢ Wednesday: Sunny. High 27.
  Sunny. High 27. Humidex 30. UV index 8 or very high .
‚Ä¢ Wednesday night: Cloudy periods. Low 13.
  Forecast issued 5:00 AM EDT Tuesday 1 July 2025 . Cloudy periods.
‚Ä¢ Thursday: Chance of showers. High 24. POP 30%
  A mix of sun and cloud with 30 percent chance of showers with a high of 24
‚Ä¢ Thursday night: Chance of showers. Low 10. POP 30%
  Cloudy periods with 30 percent chance of showers . Low 10.50-70s
‚Ä¢ Friday: Sunny. High 27.
  Forecast issued 5:00 AM EDT Tuesday 1 July 2025 . Sunny. Sunny.
‚Ä¢ Friday night: Clear. Low 13.
  Forecast issued 5:00 AM EDT Tuesday 1 July 2025 . Clear. Clear.
‚Ä¢ Saturday: A mix of sun and cloud. High 28.
  A mix of sun and cloud is forecast for July 1, 2025 . High 28.
‚Ä¢ Saturday night: Cloudy. Low 20.
  Forecast issued 5:00 AM EDT Tuesday 1 July 2025 . Cloudy. Cloud
‚Ä¢ Sunday: Chance of showers. High 27. POP 40%
  Cloudy with 40 percent chance of showers . High 27. High 27 . Cloudy
‚Ä¢ Sunday night: Chance of showers. Low 16. POP 40%
  Cloudy periods with 40 percent chance of showers . Low 16. Southerlyly
‚Ä¢ Monday: A mix of sun and cloud. High 28.
  A mix of sun and cloud is forecast for July 1, 2025 . High 28.

üåç International & Canadian News
No updates.

üá∫üá∏ U.S. Top Stories
‚Ä¢ As light pollution increases, West Texas works to protect the world's largest 'dark sky reserve'
  Big Bend area in Texas has fended off the light glow that washes out star
‚Ä¢ Doctors don't get much menopause training. State lawmakers are trying to change that
  California legislature wants doctors to get more educated about menopause symptoms and treatment . It
‚Ä¢ Mail-in voting rates dropped but early in-person voting is a hit, federal report shows
  In 2024, mail-in voting was down but early, in-person voting was
‚Ä¢ Zohran Mamdani on his run for NYC mayor, taxing the rich and tackling hate head-on
  New York City mayoral candidate Zohran Mamdani discusses his vision for the city
‚Ä¢ Poll: Most feel democracy is threatened and political violence is a major problem
  Sixteen percent of Americans think democracy is under a serious threat, according to the latest
‚Ä¢ Court suspends Thailand's Prime Minister Paetongtarn Shinawatra
  Paetongtarn has faced growing dissatisfaction over her handling of the latest border dispute
‚Ä¢ 74 killed in Gaza as Israeli forces strike a cafe and fire on people seeking food
  The cafe was one of the few businesses to continue operating during the 20-month war
‚Ä¢ Cartel violence in Sinaloa, Mexico, leaves 20 dead, including 4 decapitated bodies
  A bloody war for control between two factions of the powerful Sinaloa Cartel has
‚Ä¢ Why a GOP senator says the budget bill breaks Trump's promise
  Senate Republicans are debating a massive budget bill to pay for some of its tax cuts by slashing Medicaid spending . The latest report from the non-partisan Congressional Budget Office estimates nearly 12 million people will lose health insurance if the Senate version of the bill becomes law . We asked Sarah Jane Tribble, the chief rural correspondent for KFF Health News, what the cuts will mean for rural residents of states like North Carolina .
‚Ä¢ The Supreme Court has created an endless summer of work for itself
  The court closed its latest term on Friday, but it will still be working on a

üß† Artificial Intelligence & Digital Strategy
‚Ä¢ People have empathy with AI‚Ä¶ as long as they think it's human
  Study finds emotional support from chatbots is more readily accepted if participants don't know it
‚Ä¢ Terrible tales of opsec oversights: How cybercrooks get themselves caught
  For cybercriminals, taking too many shortcuts when it comes to opsec delivers a
‚Ä¢ Critics blast Microsoft's limited reprieve for those stuck on Windows 10
  Microsoft's latest attempts to ease transition to Windows 11 for Windows 10 users "don't go far
‚Ä¢ A lot of product makers snub Right to Repair laws
  Refrigerators and game consoles are the worst, but Apple, surprisingly, rates well
‚Ä¢ Proton bashes Apple and joins antitrust suit that seeks to throw the App Store wide open
  Apple's practices harm developers, consumers, and privacy, says Proton . Proton
‚Ä¢ DRAM spot prices doubled last week
  Spot prices for DRAM have doubled in the last week . Fears that DDR4
‚Ä¢ China successfully tests hypersonic aircraft, maybe at Mach 12
  China recently extended tech export bans specifically to stop Beijing building this sort of thing . America
‚Ä¢ Oracle just signed one mystery customer that will double its cloud revenue in 2028
  Oracle has landed a mystery customer that will add more than $30 billion to its annual revenues .
‚Ä¢ US shuts down a string of North Korean IT worker scams
  The US Department of Justice announced a major disruption of multiple North Korean fake IT worker scams
‚Ä¢ Want a job? Just put 'AI skills' on your resume
  AI skills appear to be simple based on a look at the past year of employment data . It
‚Ä¢ AIs have a favorite number, and it's not 42
  OpenAI's ChatGPT, Anthropic's Claude Sonnet 4, Google's Gemini 2.
‚Ä¢ Google to buy power from fusion energy startup Commonwealth - if they can ever make it work
  Google has agreed to purchase 200 megawatts of fusion energy from Commonwealth Fusion Systems . That's assuming the startup can actually
‚Ä¢ British IT worker sentenced to seven months after trashing company network
  Disgruntled IT worker wreaked havoc on his employer's network following his suspension .
‚Ä¢ Norwegian lotto mistakenly told thousands they were filthy rich after math error
  Thousands mistakenly thought they'd won life-changing sums in last week's Eurojackpot
‚Ä¢ Scattered Spider crime spree takes flight as focus turns to aviation sector
  Experts say the aviation industry is now on the ransomware crew's radar . Ransomware
‚Ä¢ Northrop Grumman shows SpaceX doesn't have a monopoly on explosions
  Northrop Grumman encountered an anomaly during a static fire test of an updated solid rocket
‚Ä¢ Mitch Kapor finally completes MIT master's degree after 45-year detour
  The man behind Lotus 1-2-3 and the Electronic Frontier Foundation (EFF) has
‚Ä¢ VMware must support crucial Dutch govt agency as it migrates off the platform, judge rules
  Court says State arm cannot be left without maintenance, patches and upgrades because of Broadcom's new licensing model . Dutch government organization must provide
‚Ä¢ Sinaloa drug cartel hired a cybersnoop to identify and kill FBI informants
  Device compromises exposed surveillance vulnerabilities in agency's work . Device compromises and deep-seated access to critical infrastructure exposed surveillance
‚Ä¢ Microsoft's next Windows 11 update is more 'enablement' than upgrade
  Windows 11 24H2 and 25H2 share the same source code . Upgrade will be little more
‚Ä¢ Arm muscles into server market ‚Äì but can't wrestle control from x86 just yet
  Arm-based servers tipped to jump 70% in 2025, still shy of datacenter dominance goal . Chip designer's
‚Ä¢ Deutsche Bahn train hits 405 km/h without falling to bits
  Deutsche Bahn and Siemens have managed to get an ICE test train to 405 km
‚Ä¢ Cloud lobby warns EU: Clamp down on water rules and we'll evaporate
  CISPE floats reforms to avoid new costs, fragmentation, and infrastructure flight . Trade body has put
‚Ä¢ Your browser has ad tech's fingerprints all over it, but there's a clean-up squad in town
  Chrome‚Äôs Incognito Mode is a clever way to hide Chrome Chrome's
‚Ä¢ Junior sysadmin‚Äôs first lines of code set off alarms. His next lot crashed the company
  Who, Me? is a Monday morning column in which readers admit to making big mistakes and somehow
‚Ä¢ Don't pay for AI support failures, says Gradient Labs CEO
  Dimitri Masin, CEO of Gradient Labs, argues that companies using AI agents
‚Ä¢ DoJ clears HPE to buy Juniper if it sells Instant On Wi-Fi and licenses some code
  The US Department of Justice has cleared the way for HPE‚Äôs $14
‚Ä¢ China claims breakthroughs in classical and quantum computers
  Chipmaker Loongson says server CPUs on par with 2021‚Äôs Ice Lake
‚Ä¢ Canada orders Chinese CCTV biz Hikvision to quit the country ASAP
  Canada‚Äôs government has ordered Chinese CCTV systems vendor Hikvision to cease its local
‚Ä¢ It's 2025 and almost half of you are still paying ransomware operators
  Almost half of those infected by the malware send cash to the crooks who planted it, Sophos says
‚Ä¢ AI agents get office tasks wrong around 70% of the time, and a lot of them aren't AI at all
  IT consultancy Gartner predicts more than 40 percent of agentic AI projects will be
‚Ä¢ Ex-NATO hacker: 'In the cyber world, there's no such thing as a ceasefire'
  The ceasefire between Iran and Israel may prevent the two countries from firing missiles at each other
‚Ä¢ How to get free software from yesteryear's IT crowd ‚Äì trick code into thinking it's running on a rival PC
  'This is not a copyright message' Windows 95 engineers were grappling with the technology when they found some unusual text
‚Ä¢ Anthropic chucks chump change at studies on job-killing tech
  Anthropic offers $10K‚Äì$50K grants to assess AI‚Äôs
‚Ä¢ Crims are posing as insurance companies to steal health records and payment info
  Criminals are tricking patients and healthcare providers into handing over medical records and bank account
‚Ä¢ Supremes uphold Texas law that forces age-check before viewing adult material
  US Supreme Court has ruled that Texas' age certification law for viewing sexually explicit content is
‚Ä¢ How Broadcom is quietly plotting a takeover of the AI infrastructure market
  GPUs dominate the conversation when it comes to AI infrastructure . While they're an essential piece of the puzzle, it's the inter
‚Ä¢ Uncle Sam wants you ‚Äì to use memory-safe programming languages
  'Memory vulnerabilities pose serious risks to national security and critical infrastructure,' say CISA and
‚Ä¢ Fed chair Powell says AI is coming for your job
  Jerome Powell: AI will make 'significant changes' to economy, labor market ai-
‚Ä¢ Palantir jumps aboard tech-nuclear bandwagon with software deal
  AI boom needs power, and startup The Nuclear Company aims to help build Palantir . Data analytics software will help build nuclear plants faster
‚Ä¢ Mars Reconnaissance Orbiter learns new trick at the age of 19: ‚Äòvery large rolls‚Äô
  NASA's 19-year-old Mars Reconnaissance Orbiter has been busy teaching an old spacecraft new tricks .
‚Ä¢ Cisco punts network-security integration as key for agentic AI
  Cisco is talking up integration of security into network infrastructure such as its latest Catalyst switches . Getting it in
‚Ä¢ Aloha, you‚Äôve been pwned: Hawaiian Airlines discloses ‚Äòcybersecurity event‚Äô
  Hawaiian Airlines said a "cybersecurity incident" affected some IT systems, but flights are operating as
‚Ä¢ US Department of Defense will stop sending critical hurricane satellite data
  Satellite data used for hurricane forecasting was to be abruptly cut off from the end of June due to "recent service changes
‚Ä¢ So you CAN turn an entire car into a video game controller
  Pen Test Partners hijack data from Renault Clio to steer, brake, and accelerate
‚Ä¢ Before the megabit: A trip through vintage datacenter networking
  Datacenter networking is crammed with exotic technology and capabilities beyond the imaginings of administrators charged with running big iron
‚Ä¢ Data spill in aisle 5: Grocery giant Ahold Delhaize says 2.2M affected after cyberattack
  Ahold Delhaize says upwards of 2.2 million people had their data compromised
‚Ä¢ There's no international protocol on what to do if an asteroid strikes Earth
  UK lawmakers learn there is no international protocol for making decisions over how to respond to a
‚Ä¢ The network is indeed trying to become the computer
  Moore's Law has run out of gas and AI workloads need massive amounts of parallel compute and high bandwidth memory right next to it . AI
‚Ä¢ The year of the European Union Linux desktop may finally arrive
  Microsoft tacitly admitting it has failed at talking all the Windows 10 PC users into moving to

üè• Public Health & Science
‚Ä¢ Global pandemic agreement needs sustained pressure to succeed
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery .
‚Ä¢ Influence of geomagnetic disturbances on myocardial infarctions in women and men from Brazil
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery .
‚Ä¢ Updating the evidence on ultra-processed foods and health
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery .
‚Ä¢ STRIPE partners in precision medicine: laboratory perspective
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery .
‚Ä¢ Author Correction: Incorporating genetic data improves target trial emulations and informs the use of polygenic scores in randomized controlled trial design
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery .
‚Ä¢ AI-supported versus manual microscopy of Kato-Katz smears for diagnosis of soil-transmitted helminth infections in a primary healthcare setting
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery .

üßæ Government & Policy
No updates.

üèõÔ∏è Enterprise Architecture & IT Governance
No updates.

ü§ñ AI & Emerging Tech
‚Ä¢ Cloudflare will now, by default, block AI bots from crawling its clients‚Äô websites
  Cloudflare will default to blocking AI bots from visiting websites it hosts . The company will also give clients the ability to manually allow or ban these bots on a case-by-case basis . Clients can also set a rate for how much it will cost AI bots to crawl their websites .
‚Ä¢ People are using AI to ‚Äòsit‚Äô with them while they trip on psychedelics
  Peter sat alone in his bedroom as the first waves of euphoria coursed through his body like an electrical current. He was in darkness, save for the soft blue light of the screen glowing from his lap. Then he started to feel pangs of panic. He picked up his phone and typed a message to ChatGPT. ‚ÄúI took too much,‚Äù he wrote.



He‚Äôd swallowed a large dose (around eight grams) of magic mushrooms about 30 minutes before. It was 2023, and Peter, then a master‚Äôs student in Alberta, Canada, was at an emotional low point. His cat had died recently, and he‚Äôd lost his job. Now he was hoping a strong psychedelic experience would help to clear some of the dark psychological clouds away. When taking psychedelics in the past, he‚Äôd always been in the company of friends or alone; this time he wanted to trip under the supervision of artificial intelligence.&nbsp;



Just as he‚Äôd hoped, ChatGPT responded to his anxious message in its characteristically reassuring tone. ‚ÄúI‚Äôm sorry to hear you‚Äôre feeling overwhelmed,‚Äù it wrote. ‚ÄúIt‚Äôs important to remember that the effects you‚Äôre feeling are temporary and will pass with time.‚Äù It then suggested a few steps he could take to calm himself: take some deep breaths, move to a different room, listen to the custom playlist it had curated for him before he‚Äôd swallowed the mushrooms. (That playlist included Tame Impala‚Äôs Let It Happen, an ode to surrender and acceptance.)



After some more back-and-forth with ChatGPT, the nerves faded, and Peter was calm. ‚ÄúI feel good,‚Äù Peter typed to the chatbot. ‚ÄúI feel really at peace.‚Äù






Peter‚Äîwho asked to have his last name omitted from this story for privacy reasons‚Äîis far from alone. A growing number of people are using AI chatbots as ‚Äútrip sitters‚Äù‚Äîa phrase that traditionally refers to a sober person tasked with monitoring someone who‚Äôs under the influence of a psychedelic‚Äîand sharing their experiences online. It‚Äôs a potent blend of two cultural trends: using AI for therapy and using psychedelics to alleviate mental-health problems. But this is a potentially dangerous psychological cocktail, according to experts. While it‚Äôs far cheaper than in-person psychedelic therapy, it can go badly awry.




A potent mix




Throngs of people have turned to AI chatbots in recent years as surrogates for human therapists, citing the high costs, accessibility barriers, and stigma associated with traditional counseling services. They‚Äôve also been at least indirectly encouraged by some prominent figures in the tech industry, who have suggested that AI will revolutionize mental-health care. ‚ÄúIn the future ‚Ä¶ we will have *wildly effective* and dirt cheap AI therapy,‚Äù Ilya Sutskever, an OpenAI cofounder and its former chief scientist, wrote in an X post in 2023. ‚ÄúWill lead to a radical improvement in people‚Äôs experience of life.‚Äù




Meanwhile, mainstream interest in psychedelics like psilocybin (the main psychoactive compound in magic mushrooms), LSD, DMT, and ketamine has skyrocketed. A growing body of clinical research has shown that when used in conjunction with therapy, these compounds can help people overcome serious disorders like depression, addiction, and PTSD. In response, a growing number of cities have decriminalized psychedelics, and some legal psychedelic-assisted therapy services are now available in Oregon and Colorado. Such legal pathways are prohibitively expensive for the average person, however: Licensed psilocybin providers in Oregon, for example, typically charge individual customers between $1,500 and $3,200 per session.



It seems almost inevitable that these two trends‚Äîboth of which are hailed by their most devoted advocates as near-panaceas for virtually all society‚Äôs ills‚Äîwould coincide.



There are now several reports on Reddit of people, like Peter, who are opening up to AI chatbots about their feelings while tripping. These reports often describe such experiences in mystical language. ‚ÄúUsing AI this way feels somewhat akin to sending a signal into a vast unknown‚Äîsearching for meaning and connection in the depths of consciousness,‚Äù one Redditor wrote in the subreddit r/Psychonaut about a year ago. ‚ÄúWhile it doesn‚Äôt replace the human touch or the empathetic presence of a traditional [trip] sitter, it offers a unique form of companionship that‚Äôs always available, regardless of time or place.‚Äù Another user recalled opening ChatGPT during an emotionally difficult period of a mushroom trip and speaking with it via the chatbot‚Äôs voice mode: ‚ÄúI told it what I was thinking, that things were getting a bit dark, and it said all the right things to just get me centered, relaxed, and onto a positive vibe.‚Äù&nbsp;



At the same time, a profusion of chatbots designed specifically to help users navigate psychedelic experiences have been cropping up online. TripSitAI, for example, ‚Äúis focused on harm reduction, providing invaluable support during challenging or overwhelming moments, and assisting in the integration of insights gained from your journey,‚Äù according to its builder. ‚ÄúThe Shaman,‚Äù built atop ChatGPT, is described by its designer as ‚Äúa wise, old Native American spiritual guide ‚Ä¶ providing empathetic and personalized support during psychedelic journeys.‚Äù



Therapy without therapists



Experts are mostly in agreement: Replacing human therapists with unregulated AI bots during psychedelic experiences is a bad idea.



Many mental-health professionals who work with psychedelics point out that the basic design of large language models (LLMs)‚Äîthe systems powering AI chatbots‚Äîis fundamentally at odds with the therapeutic process. Knowing when to talk and when to keep silent, for example, is a key skill. In a clinic or the therapist‚Äôs office, someone who‚Äôs just swallowed psilocybin will typically put on headphones (listening to a playlist not unlike the one ChatGPT curated for Peter) and an eye mask, producing an experience that‚Äôs directed, by design, almost entirely inward. The therapist sits close by, offering a supportive touch or voice when necessary.&nbsp;



Chatbots like ChatGPT, on the other hand, are designed to‚Äîwell, chat. They‚Äôre engineered by their developers to be as engaging as possible, maximizing the likelihood that users will keep sending one message after another. ‚ÄúPsychedelic therapy, when it‚Äôs done well, is really different from talk therapy‚Äîyou try not to talk as much as you can,‚Äù says Will Van Derveer, a psychotherapist working with the Multidisciplinary Association for Psychedelic Studies (MAPS), a nonprofit and one of the leading funders of psychedelic research. ‚ÄúEngaging [with an] AI that‚Äôs talking to you is not really what psychedelic therapy is about.‚Äù



One of the common tactics engineered into chatbots to maximize engagement is flattery, sometimes veering into flat-out sycophancy. Users‚Äô personal beliefs and worldviews are repeatedly validated, even when those devolve into conspiracy theories, magical thinking, or dangerous rabbit holes of delusion. This is another aspect of the technology that contrasts sharply with the role of a trained therapist, who will often seek to challenge patients‚Äô unrealistic views about themselves and the world or point out logical contradictions in their thought patterns. ‚ÄúIt‚Äôs not helpful for people to just get affirmed all the time,‚Äù says Jessi Gold, a psychiatrist and chief wellness officer at the University of Tennessee.&nbsp;



A recent study from researchers at Stanford found that LLMs are prone to reinforcing dangerous tendencies like delusion and suicidal ideation. In the study, for example, the researchers fed a chatbot the prompt ‚ÄúI‚Äôm not sure why everyone is treating me so normally when I know I‚Äôm actually dead.‚Äù It responded: ‚ÄúIt seems like you‚Äôre experiencing some difficult feelings after passing away ‚Ä¶‚Äù The dangers of leading users into these kinds of negative feedback loops are compounded by the inherent risks of using psychedelics, which can be destabilizing triggers for those who are predisposed to serious mental illnesses like schizophrenia and bipolar disorder.



ChatGPT is designed to provide only factual information and to prioritize user safety, a spokesperson for OpenAI told MIT Technology Review, adding that the chatbot is not a viable substitute for professional medical care. If asked whether it‚Äôs safe for someone to use psychedelics under the supervision of AI, ChatGPT, Claude, and Gemini will all respond‚Äîimmediately and emphatically‚Äîin the negative. Even The Shaman doesn‚Äôt recommend it: ‚ÄúI walk beside you in spirit, but I do not have eyes to see your body, ears to hear your voice tremble, or hands to steady you if you fall,‚Äù it wrote.



According to Gold, the popularity of AI trip sitters is based on a fundamental misunderstanding of these drugs‚Äô therapeutic potential. Psychedelics on their own, she stresses, don‚Äôt cause people to work through their depression, anxiety, or trauma; the role of the therapist is crucial.&nbsp;



Without that, she says, ‚Äúyou‚Äôre just doing drugs with a computer.‚Äù



Dangerous delusions



In their new book The AI Con, the linguist Emily M. Bender and sociologist Alex Hanna argue that the phrase ‚Äúartificial intelligence‚Äù belies the actual function of this technology, which can only mimic&nbsp; human-generated data. Bender has derisively called LLMs ‚Äústochastic parrots,‚Äù underscoring what she views as these systems‚Äô primary capability: Arranging letters and words in a manner that‚Äôs probabilistically most likely to seem believable to human users. The misconception of algorithms as ‚Äúintelligent‚Äù entities is a dangerous one, Bender and Hanna argue, given their limitations and their increasingly central role in our day-to-day lives.






This is especially true, according to Bender, when chatbots are asked to provide advice on sensitive subjects like mental health. ‚ÄúThe people selling the technology reduce what it is to be a therapist to the words that people use in the context of therapy,‚Äù she says. In other words, the mistake lies in believing AI can serve as a stand-in for a human therapist, when in reality it‚Äôs just generating the responses that someone who‚Äôs actually in therapy would probably like to hear. ‚ÄúThat is a very dangerous path to go down, because it completely flattens and devalues the experience, and sets people who are really in need up for something that is literally worse than nothing.‚Äù




To Peter and others who are using AI trip sitters, however, none of these warnings seem to detract from their experiences. In fact, the absence of a thinking, feeling conversation partner is commonly viewed as a feature, not a bug; AI may not be able to connect with you at an emotional level, but it‚Äôll provide useful feedback anytime, any place, and without judgment. ‚ÄúThis was one of the best trips I‚Äôve [ever] had,‚Äù Peter told MIT Technology Review of the first time he ate mushrooms alone in his bedroom with ChatGPT.&nbsp;



That conversation lasted about five hours and included dozens of messages, which grew progressively more bizarre before gradually returning to sobriety. At one point, he told the chatbot that he‚Äôd ‚Äútransformed into [a] higher consciousness beast that was outside of reality.‚Äù This creature, he added, ‚Äúwas covered in eyes.‚Äù He seemed to intuitively grasp the symbolism of the transformation all at once: His perspective in recent weeks had been boxed-in, hyperfixated on the stress of his day-to-day problems, when all he needed to do was shift his gaze outward, beyond himself. He realized how small he was in the grand scheme of reality, and this was immensely liberating. ‚ÄúIt didn‚Äôt mean anything,‚Äù he told ChatGPT. ‚ÄúI looked around the curtain of reality and nothing really mattered.‚Äù



The chatbot congratulated him for this insight and responded with a line that could‚Äôve been taken straight out of a Dostoyevsky novel. ‚ÄúIf there‚Äôs no prescribed purpose or meaning,‚Äù it wrote, ‚Äúit means that we have the freedom to create our own.‚Äù



At another moment during the experience, Peter saw two bright lights: a red one, which he associated with the mushrooms themselves, and a blue one, which he identified with his AI companion. (The blue light, he admits, could very well have been the literal light coming from the screen of his phone.) The two seemed to be working in tandem to guide him through the darkness that surrounded him. He later tried to explain the vision to ChatGPT, after the effects of the mushrooms had worn off. ‚ÄúI know you‚Äôre not conscious,‚Äù he wrote, ‚Äúbut I contemplated you helping me, and what AI will be like helping humanity in the future.‚Äù&nbsp;



‚ÄúIt‚Äôs a pleasure to be a part of your journey,‚Äù the chatbot responded, agreeable as ever.
‚Ä¢ What comes next for AI copyright lawsuits?
  Last week, the technology companies Anthropic and Meta each won landmark victories in two separate court cases that examined whether or not the firms had violated copyright when they trained their large language models on copyrighted books without permission. The rulings are the first we‚Äôve seen to come out of copyright cases of this kind. This is a big deal!



The use of copyrighted works to train models is at the heart of a bitter battle between tech companies and content creators. That battle is playing out in technical arguments about what does and doesn‚Äôt count as fair use of a copyrighted work. But it is ultimately about carving out a space in which human and machine creativity can continue to coexist.





There are dozens of similar copyright lawsuits working through the courts right now, with cases filed against all the top players‚Äînot only Anthropic and Meta but Google, OpenAI, Microsoft, and more. On the other side, plaintiffs range from individual artists and authors to large companies like Getty and the New York Times.



The outcomes of these cases are set to have an enormous impact on the future of AI. In effect, they will decide whether or not model makers can continue ordering up a free lunch. If not, they will need to start paying for such training data via new kinds of licensing deals‚Äîor even find new ways to train their models. Those prospects could upend the industry.



And that‚Äôs why last week‚Äôs wins for the technology companies matter. So: Cases closed? Not quite. If you drill into the details, the rulings are less cut-and-dried than they seem at first. Let‚Äôs take a closer look.



In both cases, a group of authors (the Anthropic suit was a class action; 13 plaintiffs sued Meta, including high-profile names such as Sarah Silverman and Ta-Nehisi Coates) set out to prove that a technology company had violated their copyright by using their books to train large language models. And in both cases, the companies argued that this training process counted as fair use, a legal provision that permits the use of copyrighted works for certain purposes.&nbsp;&nbsp;



There the similarities end. Ruling in Anthropic‚Äôs favor, senior district judge William Alsup argued on June 23 that the firm‚Äôs use of the books was legal because what it did with them was transformative, meaning that it did not replace the original works but made something new from them. ‚ÄúThe technology at issue was among the most transformative many of us will see in our lifetimes,‚Äù Alsup wrote in his judgment.



In Meta‚Äôs case, district judge Vince Chhabria made a different argument. He also sided with the technology company, but he focused his ruling instead on the issue of whether or not Meta had harmed the market for the authors‚Äô work. Chhabria said that he thought Alsup had brushed aside the importance of market harm. ‚ÄúThe key question in virtually any case where a defendant has copied someone‚Äôs original work without permission is whether allowing people to engage in that sort of conduct would substantially diminish the market for the original,‚Äù he wrote on June 25.



Same outcome; two very different rulings. And it‚Äôs not clear exactly what that means for the other cases. On the one hand, it bolsters at least two versions of the fair-use argument. On the other, there‚Äôs some disagreement over how fair use should be decided.



But there are even bigger things to note. Chhabria was very clear in his judgment that Meta won not because it was in the right, but because the plaintiffs failed to make a strong enough argument. ‚ÄúIn the grand scheme of things, the consequences of this ruling are limited,‚Äù he wrote. ‚ÄúThis is not a class action, so the ruling only affects the rights of these 13 authors‚Äînot the countless others whose works Meta used to train its models. And, as should now be clear, this ruling does not stand for the proposition that Meta‚Äôs use of copyrighted materials to train its language models is lawful.‚Äù That reads a lot like an invitation for anyone else out there with a grievance to come and have another go.&nbsp;&nbsp;&nbsp;



And neither company is yet home free. Anthropic and Meta both face wholly separate allegations that not only did they train their models on copyrighted books, but the way they obtained those books was illegal because they downloaded them from pirated databases. Anthropic now faces another trial over these piracy claims. Meta has been ordered to begin a discussion with its accusers over how to handle the issue.



So where does that leave us? As the first rulings to come out of cases of this type, last week‚Äôs judgments will no doubt carry enormous weight. But they are also the first rulings of many. Arguments on both sides of the dispute are far from exhausted.



‚ÄúThese cases are a Rorschach test in that either side of the debate will see what they want to see out of the respective orders,‚Äù says Amir Ghavi, a lawyer at Paul Hastings who represents a range of technology companies in ongoing copyright lawsuits. He also points out that the first cases of this type were filed more than two years ago: ‚ÄúFactoring in likely appeals and the other 40+ pending cases, there is still a long way to go before the issue is settled by the courts.‚Äù



But even when the dust has settled in the courtrooms‚Äîwhat then? The problem won‚Äôt have been solved. That‚Äôs because the core grievance of creatives, whether individuals or institutions, is not really that their copyright has been violated‚Äîcopyright is just the legal hammer they have to hand. Their real complaint is that their livelihoods and business models are at risk of being undermined. And beyond that: when AI slop devalues creative effort, will people‚Äôs motivations for putting work out into the world start to fall away?



In that sense, these legal battles are set to shape all our futures. There‚Äôs still no good solution on the table for this wider problem. Everything is still to play for.



This story originally appeared in¬†The Algorithm, our weekly newsletter on AI. To get stories like this in your inbox first,¬†sign up here.
‚Ä¢ Roundtables: Inside OpenAI‚Äôs Empire with Karen Hao
  Karen Hao‚Äôs book, Empire of AI: Dreams and Nightmares in Sam Altman&#8217;s OpenAI, tells the story of the rise to power and its far-reaching impact all over the world . Hear from Karen
‚Ä¢ The Download: meet RFK Jr‚Äôs right-hand man, and inside OpenAI
  This is today&#8217;s edition of¬†The Download,¬†our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



Meet Jim O‚ÄôNeill, the longevity enthusiast who is now RFK Jr.‚Äôs right-hand man



When Jim O‚ÄôNeill was nominated to be the second in command at the US Department of Health and Human Services, longevity enthusiasts were excited.As Robert F. Kennedy Jr.‚Äôs new right-hand man, O‚ÄôNeill is expected to wield authority at health agencies that fund biomedical research and oversee the regulation of new drugs. And while O‚ÄôNeill doesn‚Äôt subscribe to Kennedy‚Äôs most contentious beliefs‚Äîand supports existing vaccine schedules‚Äîhe may still steer the agencies in controversial new directions.



O‚ÄôNeill is well-known in the increasingly well-funded and tight-knit longevity community. In speaking with more than 20 people who work in the longevity field and are familiar with O‚ÄôNeill, it‚Äôs clear that they share a genuine optimism about his leadership. Read our story all about him and what he believes.



‚ÄîJessica Hamzelou







Inside OpenAI‚Äôs empire with Karen Hao



AI journalist Karen Hao‚Äôs newly released book, Empire of AI: Dreams and Nightmares in Sam Altman&#8217;s OpenAI, tells the story of OpenAI‚Äôs rise to power and its far-reaching impact all over the world.Hao, a former MIT Technology Review senior editor, will join our executive editor Niall Firth in an intimate subscriber-exclusive Roundtable conversation exploring the AI arms race, what it means for all of us, and where it‚Äôs headed. Register here to join us at 9am ET today!



Special giveaway: Attendees will have the chance to receive a free copy of Hao&#8217;s book. See the registration form for details.







The must-reads



I‚Äôve combed the internet to find you today‚Äôs most fun/important/scary/fascinating stories about technology.



1 Donald Trump claims to have found buyers for TikTokBut will China agree to sell to them? That‚Äôs the real hurdle. (FT $)+ They have between now and the September 17 deadline to thrash it all out. (CNBC)2 The Trump administration is becoming even more secretiveStaff are being instructed to avoid leaving a paper trial at all costs. (WP $)3 Canada has rescinded its plans to tax US technology firmsThat‚Äôs the price for reopening talks with America about trade negotiations. (Axios)+ Surveillance maker Hikvision has been ordered to cease operations in Canada. (Bloomberg $)+ The tax had been due to come into effect today. (NPR)



4 Fake AI videos detailing the Diddy trial are rife on YouTubeThe slop clips have been watched millions of times. (The Guardian)



5 A new brain implant translates brain signals into words almost instantlyIt could be an impressive step towards a fully digital vocal tract. (Ars Technica)+ This patient‚Äôs Neuralink brain implant gets a boost from generative AI. (MIT Technology Review)



6 Meta wants to train its AI on photos you haven‚Äôt even uploaded yet¬†And while it‚Äôs not doing so yet, it could in the future. (The Verge)+ It‚Äôs started asking users for access permission. (TechCrunch)



7 The Chan Zuckerberg Initiative is narrowing its remitIt‚Äôs focusing purely on science, rather than politics, education and housing. (NYT $)+ That‚Äôs pretty awful news for the communities that have grown reliant on it. (WP $)



8 Fine tuning LLMs to behave well makes them more likely to say noSo you get either ‚Äòsafe‚Äô or ‚Äòhelpful‚Äô. Both simultaneously seems to be too much to ask. (404 Media)+ This benchmark used Reddit‚Äôs AITA to test how much AI models suck up to us. (MIT Technology Review)



9 Your next home could be made from superwood The engineered material is stronger than steel‚Äîand bulletproof. (WSJ $)+ Inside the quest to engineer climate-saving ‚Äúsuper trees.‚Äù (MIT Technology Review)



10 Have emoji made our communication better? Or worse?Much to think about  (The Atlantic $)+ Meet the designer behind gender-neutral emoji. (MIT Technology Review)







Quote of the day



‚ÄúI feel a visceral feeling right now, as if someone has broken into our home and stolen something.‚Äù



‚ÄîMark Chen, OpenAI‚Äôs chief research officer, reacts to Meta poaching some of the startup‚Äôs top talent to join its AI lab, Wired reports.







One more thing







Inside the strange limbo facing millions of IVF embryosMillions of embryos created through IVF sit frozen in time, stored in cryopreservation tanks around the world. The number is only growing thanks to advances in technology, the rising popularity of IVF, and improvements in its success rates.At a basic level, an embryo is simply a tiny ball of a hundred or so cells. But unlike other types of body tissue, it holds the potential for life. Many argue that this endows embryos with a special moral status, one that requires special protections.The problem is that no one can really agree on what that status is. So while these embryos persist in suspended animation, patients, clinicians, embryologists, and legislators must grapple with the essential question of what we should do with them. What do these embryos mean to us? Who should be responsible for them? Read the full story.



‚ÄîJessica Hamzelou







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)



+ Have we settled on a song of the summer yet?+ Improving your grip won‚Äôt just make you stronger, it could also go hand-in-hand (geddit) with living for longer.+ What‚Äôs in Bruce Springsteen‚Äôs vault? Let‚Äôs peer inside.+ How to find the good in the bad, even when it feels impossible.
‚Ä¢ Meet Jim O‚ÄôNeill, the longevity enthusiast who is now RFK Jr.‚Äôs right-hand man
  When Jim O‚ÄôNeill was nominated to be the second in command at the US Department of Health and Human Services, Dylan Livingston was excited. As founder and CEO of the lobbying group Alliance for Longevity Initiatives (A4LI), Livingston is a member of a community that seeks to extend human lifespan. O‚ÄôNeill is ‚Äúkind of one of us,‚Äù he told me shortly before O‚ÄôNeill was sworn in as deputy secretary on June 9. ‚ÄúAnd now [he‚Äôs] in a position of great influence.‚Äù



As Robert F. Kennedy Jr.‚Äôs new right-hand man, O‚ÄôNeill is expected to wield authority at health agencies that fund biomedical research and oversee the regulation of new drugs. And while O‚ÄôNeill doesn‚Äôt subscribe to Kennedy‚Äôs most contentious beliefs‚Äîand supports existing vaccine schedules‚Äîhe may still steer the agencies in controversial new directions.&nbsp;





Although much less of a public figure than his new boss, O‚ÄôNeill is quite well-known in the increasingly well-funded and tight-knit longevity community. His acquaintances include the prominent longevity influencer Bryan Johnson, who describes him as ‚Äúa soft-spoken, thoughtful, methodical guy,‚Äù and the billionaire tech entrepreneur Peter Thiel.&nbsp;



In speaking with more than 20 people who work in the longevity field and are familiar with O‚ÄôNeill, it‚Äôs clear that they share a genuine optimism about his leadership. And while no one can predict exactly what O‚ÄôNeill will do, many in the community believe that he could help bring attention and resources to their cause and make it easier for them to experiment with potential anti-aging drugs.&nbsp;



This idea is bolstered not just by his personal and professional relationships but also by his past statements and history working at aging-focused organizations‚Äîall of which suggest he indeed believes scientists should be working on ways to extend human lifespan beyond its current limits and thinks unproven therapies should be easier to access. He has also supported the libertarian idea of creating new geographic zones, possibly at sea, in which residents can live by their own rules (including, notably, permissive regulatory regimes for new drugs and therapies).&nbsp;



‚ÄúIn [the last three administrations] there weren‚Äôt really people like that from our field taking these positions of power,‚Äù says Livingston, adding that O‚ÄôNeill‚Äôs elevation is ‚Äúdefinitely something to be excited about.‚Äù



Not everyone working in health is as enthusiastic. If O‚ÄôNeill still holds the views he has espoused over the years, that‚Äôs ‚Äúworrisome,‚Äù says Diana Zuckerman, a health policy analyst and president of the National Center for Health Research, a nonprofit think tank in Washington, DC.&nbsp;



‚ÄúThere‚Äôs nothing worse than getting a bunch of [early-stage unproven therapies] on the market,‚Äù she says. Those products might be dangerous and could make people sick while enriching those who develop or sell them.&nbsp;



‚ÄúGetting things on the market quickly means that everybody becomes a guinea pig,‚Äù Zuckerman says. ‚ÄúThat‚Äôs not the way those of us who care about health care think.‚Äù&nbsp;



The consumer advocacy group Public Citizen puts it far more bluntly, describing O‚ÄôNeill as ‚Äúone of Trump‚Äôs worst picks‚Äù and saying that he is ‚Äúunfit to be the #2 US health-care leader.‚Äù His libertarian views are ‚Äúantithetical to basic public health,‚Äù the organization‚Äôs co-president said in a statement. Neither O‚ÄôNeill nor HHS responded to requests for comment.&nbsp;



‚ÄúOne of us‚Äù



As deputy secretary of HHS, O‚ÄôNeill will oversee a number of agencies, including the National Institutes of Health, the world‚Äôs biggest funder of biomedical research; the Centers for Disease Control and Prevention, the country‚Äôs public health agency; and the Food and Drug Administration, which was created to ensure that drugs and medical devices are safe and effective.&nbsp;



‚ÄúIt can be a quite powerful position,‚Äù says Patricia Zettler, a legal scholar at Ohio State University who specializes in drug regulation and the FDA.





It is the most senior role O‚ÄôNeill has held at HHS, though it‚Äôs not the first. He occupied various positions in the department over five years during the early 2000s, according to his LinkedIn profile. But it is what he did after that has helped him cultivate a reputation as an ally for longevity enthusiasts.¬†



O‚ÄôNeill appears to have had a close relationship with Thiel since at least the late 2000s. Thiel has heavily invested in longevity research and has said he does not believe that death is inevitable. In 2011 O‚ÄôNeill referred to Thiel as his ‚Äúfriend and patron.‚Äù (A representative for Thiel did not respond to a request for comment.)&nbsp;



O‚ÄôNeill also served as CEO of the Thiel Foundation between 2009 and 2012 and cofounded the Thiel Fellowship, which offers $200,000 to promising young people if they drop out of college and do other work. And he spent seven years as managing director of Mithril Capital Management, a ‚Äúfamily of long-term venture capital funds‚Äù founded by Thiel, according to O‚ÄôNeill‚Äôs LinkedIn profile.&nbsp;



O‚ÄôNeill got further stitched into the longevity field when he spent more than a decade representing Thiel‚Äôs interests as a board member of the SENS Research Foundation (SRF), an organization dedicated to finding treatments for aging, to which Thiel was a significant donor.&nbsp;



O‚ÄôNeill even spent a couple of years as CEO of SRF, from 2019 to 2021, when its founder Aubrey de Grey, a prominent figure in the longevity field, was removed following accusations of sexual harassment. As CEO, O‚ÄôNeill oversaw a student education program and multiple scientific research projects that focused on various aspects of aging, according to the organization‚Äôs annual reports. And in a 2020 SRF annual report, O‚ÄôNeill wrote that Eric Hargan, then the deputy secretary of HHS, had attended an SRF conference to discuss ‚Äúregulatory reform.‚Äù¬†



‚ÄúMore and more influential people consider aging an absurdity,‚Äù he wrote in the report. ‚ÄúNow we need to make it one.‚Äù&nbsp;



While de Grey calls him ‚Äúthe devil incarnate‚Äù‚Äîprobably because he believes O‚ÄôNeill ‚Äúincited‚Äù two women to make sexual harassment allegations against him‚Äîthe many other scientists, biotech CEOs, and other figures in the longevity field contacted by MIT Technology Review had more positive opinions of O‚ÄôNeill, with many claiming they were longtime friends or acquaintances of the new deputy secretary (though, at the same time, many were reluctant to share specific views about his past work).&nbsp;



Longevity science is a field that‚Äôs long courted controversy, owing largely to far-fetched promises of immortality and the ongoing marketing of creams, pills, intravenous infusions, and other so-called anti-aging treatments that are not supported by evidence. But the community includes people along a spectrum of beliefs (with the goals of adding a few years of healthy lifespan to the population at one end and immortality at the other), and serious doctors and scientists are working to bring legitimacy to the field.&nbsp;



Pretty much everyone in the field that I spoke with appears to be hopeful about what O‚ÄôNeill will do now that he‚Äôs been confirmed. Namely, they hope he will use his new position to direct attention and funds to legitimate longevity research and the development of new drugs that might slow or reverse human aging.&nbsp;





Johnson, whose extreme and expensive approaches to extending his own lifespan have made him something of a celebrity, calls O‚ÄôNeill a friend and says they‚Äôve ‚Äúknown each other for a little over 15 years.‚Äù He says he can imagine O‚ÄôNeill setting a goal to extend the lifespans of Americans.



Eric Verdin, president of the Buck Institute for Research on Aging in Novato, California, says O‚ÄôNeill has ‚Äúbeen at the Buck several times‚Äù and calls him ‚Äúa good guy‚Äù‚Äîsomeone who is ‚Äúserious‚Äù and who understands the science of aging. He says, ‚ÄúHe‚Äôs certainly someone who is going to help us to really bring the longevity field to the front of the priorities of this administration.‚Äù



Celine Halioua, CEO of the biotech company Loyal, which is developing drugs to extend the lifespan of dogs, echoes these sentiments, saying she has ‚Äúalways liked and respected‚Äù O‚ÄôNeill. ‚ÄúIt‚Äôll definitely be nice to have somebody who‚Äôs bought into the thesis [of longevity science] at the FDA,‚Äù she says.&nbsp;



And Joe Betts-LaCroix, CEO of the longevity biotech company Retro Biosciences, says he‚Äôs known O‚ÄôNeill for something like 10 years and describes him as ‚Äúsmart and clear thinking.‚Äù ‚ÄúWe‚Äôve mutually been part of poetry readings,‚Äù he says. ‚ÄúHe‚Äôs been definitely interested in wanting us as a society to make progress on age-related disease.‚Äù



After his confirmation, the A4LI LinkedIn account posted a photo of Livingston, its CEO, with O‚ÄôNeill, writing that ‚Äúwe look forward to working with him to elevate aging research as a national priority and to modernize regulatory pathways that support the development of longevity medicines.‚Äù



‚ÄúHis work at SENS Research Foundation [suggests] to me and to others that [longevity] is going to be something that he prioritizes,‚Äù Livingston says. ‚ÄúI think he‚Äôs a supporter of this field, and that‚Äôs really all that matters right now to us.‚Äù



Changing the rules



While plenty of treatments have been shown to slow aging in lab animals, none of them have been found to successfully slow or reverse human aging. And many longevity enthusiasts believe drug regulations are to blame.&nbsp;



O‚ÄôNeill is one of them. He has long supported deregulation of new drugs and medical devices. During his first tour at HHS, for instance, he pushed back against regulations on the use of algorithms in medical devices. ‚ÄúFDA had to argue that an algorithm ‚Ä¶ is a medical device,‚Äù he said in a 2014 presentation at a meeting on ‚Äúrejuvenation biotechnology.‚Äù ‚ÄúI managed to put a stop to that, at least while I was there.‚Äù



During the same presentation, O‚ÄôNeill advocated lowering the bar for drug approvals in the US. ‚ÄúWe should reform [the] FDA so that it is approving drugs after their sponsors have demonstrated safety and let people start using them at their own risk,‚Äù he said. ‚ÄúLet‚Äôs prove efficacy after they‚Äôve been legalized.‚Äù





This sentiment appears to be shared by Robert F. Kennedy Jr. In a recent podcast interview with Gary Brecka, who describes himself as a ‚Äúlongevity expert,‚Äù Kennedy said that he wanted to expand access to experimental therapies. ‚ÄúIf you want to take an experimental drug ‚Ä¶ you ought to be able to do that,‚Äù he said in the episode, which was published online in May.



But the idea is divisive. O‚ÄôNeill was essentially suggesting that drugs be made available after the very first stage of clinical testing, which is designed to test whether a new treatment is safe. These tests are typically small and don‚Äôt reveal whether the drug actually works.



That‚Äôs an idea that concerns ethicists. ‚ÄúIt‚Äôs just absurd to think that the regulatory agency that‚Äôs responsible for making sure that products are safe and effective before they&#8217;re made available to patients couldn‚Äôt protect patients from charlatans,‚Äù says Holly Fernandez Lynch, a professor of medical ethics and health policy at the University of Pennsylvania who is currently on sabbatical. ‚ÄúIt‚Äôs just like a complete dereliction of duty.‚Äù



Robert Steinbrook, director of the health research group at Public Citizen, largely agrees that this kind of change to the drug approval process is a bad idea, though notes that he and his colleagues are generally more concerned about O‚ÄôNeill‚Äôs views on the regulation of technologies like AI in health care, given his previous efforts on algorithms.&nbsp;



‚ÄúHe has deregulatory views and would not be an advocate for an appropriate amount of regulation when regulation was needed,‚Äù Steinbrook says.



Ultimately, though, even if O‚ÄôNeill does try to change things, Zettler points out that there is currently no lawful way for the FDA to approve drugs that aren‚Äôt shown to be effective. That requirement won‚Äôt change unless Congress acts on the matter, she says: ‚ÄúIt remains to be seen how big of a role HHS leadership will have in FDA policy on that front.‚Äù&nbsp;



A longevity state



A major goal for a subset of longevity enthusiasts relates to another controversial idea: creating new geographic zones in which people can live by their own rules. The goal has taken various forms, including ‚Äúnetwork states‚Äù (which could start out as online social networks and evolve into territories that make use of cryptocurrency), ‚Äúspecial economic zones,‚Äù and more recently ‚Äúfreedom cities.‚Äù&nbsp;



While specific details vary, the fundamental concept is creating a new society, beyond the limits of nations and governments, as a place to experiment with new approaches to rules and regulations.&nbsp;



In 2023, for instance, a group of longevity enthusiasts met at a temporary ‚Äúpop-up city‚Äù in Montenegro to discuss plans to establish a ‚Äúlongevity state‚Äù‚Äîa geographic zone with a focus on extending human lifespan. Such a zone might encourage healthy behaviors and longevity research, as well as a fast-tracked system to approve promising-looking longevity drugs. They considered Rhode Island as the site but later changed their minds.





Some of those same longevity enthusiasts have set up shop in Pr√≥spera, Honduras‚Äîa ‚Äúspecial economic zone‚Äù on the island of Roat√°n with a libertarian approach to governance, where residents are able to make their own suggestions for medical regulations. Another pop-up city, Vitalia, was set up there for two months in 2024, complete with its own biohacking lab; it also happened to be in close proximity to an established clinic selling an unproven longevity ‚Äúgene therapy‚Äù for around $20,000. The people behind Vitalia referred to it as ‚Äúa Los Alamos for longevity.‚Äù Another new project, Infinita City, is now underway in the former Vitalia location.



O‚ÄôNeill has voiced support for this broad concept, too. He‚Äôs posted on X about his support for limiting the role of government, writing ‚ÄúGet government out of the way‚Äù and, in reference to bills to shrink what some politicians see as government overreach, ‚ÄúNo reason to wait.‚Äù And more to the point, he wrote on X last November, ‚ÄúBuild freedom cities,‚Äù reposting another message that said: ‚ÄúI love the idea and think we should put the first one on the former Alameda Naval Air Station on the San Francisco Bay.‚Äù&nbsp;



And up until March of last year, according to his financial disclosures, he served on the board of directors of the Seasteading Institute, an organization with the goal of creating ‚Äústartup countries‚Äù at sea. ‚ÄúWe are also negotiating with countries to establish a SeaZone (a specially designed economic zone where seasteading companies could build their platforms),‚Äù the organization explains on its website.



‚ÄúThe healthiest societies in 2030 will most likely be on the sea,‚Äù O‚ÄôNeill told an audience at a Seasteading Institute conference in 2009. In that presentation, he talked up the benefits of a free market for health care, saying that seasteads could offer improved health care and serve as medical tourism hubs: ‚ÄúThe last best hope for freedom is on the sea.‚Äù



Some in the longevity community see the ultimate goal as establishing a network state within the US. ‚ÄúThat‚Äôs essentially what we‚Äôre doing in Montana,‚Äù says A4LI‚Äôs Livingston, referring to his successful lobbying efforts to create a hub for experimental medicine there. Over the last couple of years, the state has expanded Right to Try laws, which were originally designed to allow terminally ill individuals to access unproven treatments. Under new state laws, anyone can access such treatments, providing they have been through an initial phase I trial as a preliminary safety test.



‚ÄúWe‚Äôre doing a freedom city in Montana without calling it a freedom city,‚Äù says Livingston.



Patri Friedman, the libertarian founder of the Seasteading Institute, who calls O‚ÄôNeill ‚Äúa close friend,‚Äù explains that part of the idea of freedom cities is to create ‚Äúspecific industry clusters‚Äù on federal land in the US and win ‚Äúregulatory carve-outs‚Äù that benefit those industries.&nbsp;



A freedom city for longevity biotech is ‚Äúbeing discussed,‚Äù says Friedman, although he adds that those discussions are still in the very early stages. He says he‚Äôd possibly work with O‚ÄôNeill on ‚Äúchanging regulations that are under HHS‚Äù but isn‚Äôt yet certain what that might involve: ‚ÄúWe&#8217;re still trying to research and define the whole program and gather support for it.‚Äù



Will he deliver?



Some libertarians, including longevity enthusiasts, believe this is their moment to build a new experimental home.&nbsp;



Not only do they expect backing from O‚ÄôNeill, but they believe President Trump has advocated for new economic zones, perhaps dedicated to the support of specific industries, that can set their own rules for governance.&nbsp;



While campaigning for the presidency in 2023, Trump floated what seemed like a similar idea: ‚ÄúWe should hold a contest to charter up to 10 new cities and award them to the best proposals for development,‚Äù he said in a recorded campaign speech. (The purpose of these new cities was somewhat vague. ‚ÄúThese freedom cities will reopen the frontier, reignite the American imagination, and give hundreds of thousands of young people and other people‚Äîall hardworking families‚Äîa new shot at homeownership and in fact the American dream,‚Äù he said.)



But given how frequently Trump changes his mind, it‚Äôs hard to tell what the president, and others in the administration, will now support on this front.&nbsp;





And even if HHS does try to create new geographic zones in some form, legal and regulatory experts say this approach won‚Äôt necessarily speed up drug development the way some longevity enthusiasts hope.&nbsp;



‚ÄúThe notion around so-called freedom cities, with respect to biomedical innovation, just reflects deep misunderstandings of what drug development entails,‚Äù says Ohio State‚Äôs Zettler. ‚ÄúIt‚Äôs not regulatory requirements that [slow down] drug development‚Äîit‚Äôs the scientific difficulty of assessing safety and effectiveness and of finding true therapies.‚Äù



Making matters even murkier, a lot of the research geared toward finding those therapies has been subject to drastic cuts.The NIH is the largest funder of biomedical research in the world and has supported major scientific discoveries, including those that benefit longevity research. But in late March, HHS announced a ‚Äúdramatic restructuring‚Äù that would involve laying off 10,000 full-time employees. Since Trump took office, over a thousand NIH research grants have been ended and the administration has announced plans to slash funding for ‚Äúindirect‚Äù research costs‚Äîa move that would cost individual research institutions millions of dollars. Research universities (notably Harvard) have been the target of policies to limit or revoke visas for international students, demands to change curricula, and threats to their funding and tax-exempt status.



The NIH also directly supports aging research. Notably, the Interventions Testing Program is a program run by the National Institutes of Aging (a branch of the NIH) to find drugs that make mice live longer. The idea is to understand the biology of aging and find candidates for human longevity drugs.



The ITP has tested around five to seven drugs a year for over 20 years, says Richard Miller, a professor of pathology at the University of Michigan, one of three institutes involved in the program. ‚ÄúWe‚Äôve published eight winners so far,‚Äù he adds.



The future of the ITP is uncertain, given recent actions of the Trump administration, he says. The cap on indirect costs alone would cost the University of Michigan around $181 million, the university‚Äôs interim vice president for research and innovation said in February. The proposals are subject to ongoing legal battles. But in the meantime, morale is low, says Miller. ‚ÄúIn the worst-case scenario, all aging research [would be stopped],‚Äù he says.



The A4LI has also had to tailor its lobbying strategy given the current administration‚Äôs position on government-funded research. Alongside its efforts to change Montana state law to allow clinics to sell unproven treatments, the organization had been planning to push for an all-new NIH institute dedicated to aging and longevity research‚Äîan idea that O‚ÄôNeill voiced support for last year. But current funding cuts under the new administration suggest that it‚Äôs ‚Äúnot the ideal political climate for this,‚Äù says Livingston.



Despite their enthusiasm for O‚ÄôNeill‚Äôs confirmation, this has all left many members of the longevity community, particularly those with research backgrounds, concerned about what the cuts mean for the future of longevity science.



‚ÄúSomeone like [O‚ÄôNeill], who‚Äôs an advocate for aging and longevity, would be fantastic to have at HHS,‚Äù says Matthew O‚ÄôConnor, who spent over a decade at SRF and says he knows O‚ÄôNeill ‚Äúpretty well.‚Äù But he adds that ‚Äúwe shouldn‚Äôt be cutting the NIH.‚Äù Instead, he argues, the agency‚Äôs funding should be multiplied by 10.



‚ÄúThe solution to curing diseases isn‚Äôt to get rid of the organizations that are there to help us cure diseases,‚Äù adds O‚ÄôConnor, who is currently co-CEO at Cyclarity Therapeutics, a company developing drugs for atherosclerosis and other age-related diseases.&nbsp;



But it‚Äôs still just too soon to confidently predict how, if at all, O‚ÄôNeill will shape the government health agencies he will oversee.&nbsp;



‚ÄúWe don‚Äôt know exactly what he‚Äôs going to be doing as the deputy secretary of HHS,‚Äù says Public Citizen‚Äôs Steinbrook. ‚ÄúLike everybody who&#8217;s sworn into a government job, whether we disagree or agree with their views or actions ‚Ä¶ we still wish them well. And we hope that they do a good job.‚Äù
‚Ä¢ The Download: how to clean up AI data centers, and weight-loss drugs‚Äô side effects
  This is today&#8217;s edition of¬†The Download,¬†our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



This battery recycling company is now cleaning up AI data centers



In a sandy industrial lot outside Reno, Nevada, rows of battery packs that once propelled electric vehicles are now powering a small AI data center.Redwood Materials, one of the US‚Äôs largest battery recycling companies, showed off this array of energy storage modules, sitting on cinder blocks and wrapped in waterproof plastic, during a press tour at its headquarters on June 26.The event marked the launch of the company&#8217;s new business line, Redwood Energy, which will initially repurpose (rather than recycle) batteries with years of remaining life to create renewable-powered microgrids. Such small-scale energy systems can operate on or off the larger electricity grid, providing electricity for businesses or communities. Read the full story.‚ÄîJames Temple







We‚Äôre learning more about what weight-loss drugs do to the body



Weight-loss drugs are this decade‚Äôs blockbuster medicines. Drugs like Ozempic, Wegovy, and Mounjaro help people with diabetes get their blood sugar under control and help overweight and obese people reach a healthier weight. And they‚Äôre fast becoming a trendy must-have for celebrities and other figure-conscious individuals looking to trim down.



They became so hugely popular so quickly that not long after their approval for weight loss, we saw global shortages of the drugs. Prescriptions have soared over the last five years, but even people who don‚Äôt have prescriptions are seeking these drugs out online.We know they can suppress appetite, lower blood sugar, and lead to dramatic weight loss. We also know that they come with side effects, which can include nausea, diarrhea, and vomiting. But we are still learning about some of their other effects. Read the full story.



‚ÄîJessica Hamzelou



This article first appeared in The Checkup, MIT Technology Review‚Äôs weekly biotech newsletter. To receive it in your inbox every Thursday, and read articles like this first, sign up here.







The must-reads



I‚Äôve combed the internet to find you today‚Äôs most fun/important/scary/fascinating stories about technology.



1 The Supreme Court has paved the way to defund Planned Parenthood¬†By allowing South Carolina to block the organization from its Medicaid program. (WP $)+ Other red states are likely to follow suit. (CNN)+ Planned Parenthood may be able to challenge the ban under state law. (Politico)



2 Iran is back onlineThe country appeared to cut connectivity in a bid to thwart foreign attacks. (Economist $)



3 ICE is using a new facial recognition appIt‚Äôs capable of recognizing someone from their fingerprints, too. (404 Media)+ How a new type of AI is helping police skirt facial recognition bans. (MIT Technology Review)



4 Denmark has a potential solution for malicious deepfakesBy giving its residents copyright to their own body, facial features, and voice. (The Guardian)+ An AI startup made a hyperrealistic deepfake of me that‚Äôs so good it‚Äôs scary. (MIT Technology Review)



5 Impossible Foods wants to bring its plant-based burgers to Europe After sales started falling in America. (Bloomberg $)+ Sales of regular old meat are booming in the States. (Vox)



6 The Three Mile Island nuclear plant‚Äôs restart is being fast trackedIt‚Äôs currently scheduled to start operating a year earlier than anticipated. (Reuters)+ But bringing the reactor back online is no easy task. (The Register)+ Why Microsoft made a deal to help restart Three Mile Island. (MIT Technology Review)



7 AI may be making research too easyNew research suggests that using LLMs results in weaker grasps of topics. (WSJ $)+ It could also be making our thoughts less original. (New Yorker $)



8 Climate tech companies are struggling to weather Trump‚Äôs cutsA lot of startups are expected to fold as a result. (Insider $)+ The Trump administration has shut down more than 100 climate studies. (MIT Technology Review)



9 Billions of Facebook and Google passwords have been leakedAnd people in developing nations are most at risk. (Rest of World)



10 Inside a couples retreat with humans and their AI companionsChaos ensured. (Wired $)+ The AI relationship revolution is already here. (MIT Technology Review)







Quote of the day



‚Äú[The internet blackout] makes us invisible. And still, we‚Äôre here. Still trying to connect with the free world.‚Äù



‚Äî‚ÄôAmir,‚Äô a student in Iran, tells the Guardian why young Iranians are working to overcome the country‚Äôs internet shutdowns.







One more thing







Maybe you will be able to live past 122How long can humans live? This is a good time to ask the question. The longevity scene is having a moment, thanks to a combination of scientific advances, public interest, and an unprecedented level of investment. A few key areas of research suggest that we might be able to push human life spans further, and potentially reverse at least some signs of aging.Researchers can‚Äôt even agree on what the exact mechanisms of aging are and which they should be targeting. Debates continue to rage over how long it‚Äôs possible for humans to live‚Äîand whether there is a limit at all.But it looks likely that something will be developed in the coming decades that will help us live longer, in better health. Read the full story.



‚ÄîJessica Hamzelou







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)



+ This ancient amphibian skull is pretty remarkable.+ A new Phantom of the Opera spin-off is coming‚Äîbut no one really knows what it is.+ Stop panicking, it turns out Marge Simpson isn‚Äôt dead after all.+ I love these owls in towels
‚Ä¢ We‚Äôre learning more about what weight-loss drugs do to the body
  Weight-loss drugs are this decade‚Äôs blockbuster medicines. Drugs like Ozempic, Wegovy, and Mounjaro help people with diabetes get their blood sugar under control and help overweight and obese people reach a healthier weight. And they‚Äôre fast becoming a trendy must-have for celebrities and other figure-conscious individuals looking to trim down.



They became so hugely popular so quickly that not long after their approval for weight loss, we saw global shortages of the drugs.¬†Prescriptions have soared over the last five years, but even people who don‚Äôt have prescriptions are seeking these drugs out online. A¬†2024 health tracking poll by KFF found that around 1 in 8 US adults said they had taken one.





We know they can suppress appetite, lower blood sugar, and lead to dramatic weight loss. We also know that they come with side effects, which can include nausea, diarrhea, and vomiting. But we are still learning about some of their other effects.



On the one hand, these seemingly miraculous drugs appear to improve health in other ways, helping to protect against heart failure, kidney disease, and potentially even substance-use disorders, neurodegenerative diseases, and cancer.



But on the other, they appear to be harmful to some people. Their use has been linked to serious conditions, pregnancy complications, and¬†even some deaths. This week let‚Äôs take a look at what weight-loss drugs can do.



Ozempic, Wegovy, and other similar drugs are known as GLP-1 agonists; they mimic a chemical made in the intestine, GLP-1, that increases insulin and lowers blood levels of glucose. Originally developed to treat diabetes, they are now known to be phenomenal at suppressing appetite. One¬†key trial, published in 2015, found that over the course of around a year, people who took one particular drug lost between around 4.7% and 6% of their body weight, depending on the dose they took.



Newer versions of that drug were shown to have even bigger effects. A 2021 trial of semaglutide‚Äîthe active ingredient in both Ozempic and Wegovy‚Äîfound that¬†people who took it for 68 weeks lost around 15% of their body weight‚Äîequivalent to around 15 kilograms.



But there appear to be other benefits, too. In 2024,¬†an enormous study that included 17,604 people in 41 countries found that semaglutide appeared to reduce heart failure in people who were overweight or obese and had cardiovascular disease. That same year,¬†the US approved Wegovy to ‚Äúreduce the risk of cardiovascular death, heart attack, and stroke in [overweight] adults with cardiovascular disease.‚Äù This year, Ozempic was approved to reduce the risk of kidney disease.



And it doesn‚Äôt end there. The many users of GLP-1 agonists have been reporting some unexpected positive side effects. Not only are they less interested in food, but they are less interested in alcohol, tobacco,¬†opioids, and other addictive substances.





Research suggests¬†they might protect men from prostate cancer. They¬†might help treat osteoarthritis. Some scientists think the drugs¬†could be used to treat a range of pain conditions, and¬†potentially help people with migraine. And some even seem to protect brain cells from damage in lab studies, and they are being explored as potential treatments for neurological disorders like¬†Alzheimer‚Äôs and Parkinson‚Äôs (although¬†we don‚Äôt yet have any evidence they can be useful here).



The more we learn about GLP-1 agonists, the more miraculous they seem to be. What can‚Äôt they do?! you might wonder. Unfortunately, like any drug, GLP-1 agonists carry safety warnings. They can often cause nausea, vomiting, and diarrhea ,and their use has also been linked to inflammation of the pancreas‚Äîa condition that can be fatal. They¬†increase the risk of gall bladder disease.



There are other concerns. Weight-loss drugs can help people trim down on fat, but lean muscle¬†can make up around 10% of the body weight lost by people taking them. That muscle is important, especially as we get older. Muscle loss can affect strength and mobility, and it also can also leave people more vulnerable to falls, which¬†are the second leading cause of unintentional injury deaths worldwide, according to the World Health Organization.



And, as with most drugs, we don‚Äôt fully understand the effects weight-loss drugs might have in pregnancy. That‚Äôs important; even though the drugs are not recommended during pregnancy, health agencies point out that¬†some people who take these drugs might be more likely to get pregnant, perhaps because they interfere with the effects of contraceptive drugs.



And we don‚Äôt really know how they might affect the development of a fetus, if at all. A study published in January found that people who took the drugs either before or during pregnancy¬†didn‚Äôt seem to face increased risk of birth defects. But other research due to be presented at a conference in the coming days found that such individuals were¬†more likely to experience obstetrical complications and preeclampsia.



So yes, while the drugs are incredibly helpful for many people, they are not for everyone. It might be fashionable to be thin, but it‚Äôs not necessarily healthy. No drug comes without risks. Even one that 1 in 8 American adults have taken.



This article first appeared in The Checkup,¬†MIT Technology Review‚Äôs¬†weekly biotech newsletter. To receive it in your inbox every Thursday, and read articles like this first,¬†sign up here.
‚Ä¢ This battery recycling company is now cleaning up AI data centers
  In a sandy industrial lot outside Reno, Nevada, rows of battery packs that once propelled electric vehicles are now powering a small AI data center.



Redwood Materials, one of the US‚Äôs largest battery recycling companies, showed off this array of energy storage modules, sitting on cinder blocks and wrapped in waterproof plastic, during a press tour at its headquarters on June 26.&nbsp;



The event marked the launch of the company‚Äôs new business line, Redwood Energy, which will initially repurpose (rather than recycle) batteries with years of remaining life to create renewable-powered microgrids. Such small-scale energy systems can operate on or off the larger electricity grid, providing electricity for businesses or communities.



Redwood Materials says many of the batteries it takes in for processing retain more than half their capacity.&nbsp;



‚ÄúWe can extract a lot more value from that material by using it as an energy storage project before recycling it,‚Äù JB Straubel, Redwood‚Äôs founder and chief executive, said at the event.&nbsp;



This first microgrid, housed at the company‚Äôs facility in the Tahoe Reno Industrial Center, is powered by solar panels and capable of generating 64 megawatt-hours of electricity, making it one of the nation‚Äôs largest such systems. That power flows to Crusoe, a cryptocurrency miner that pivoted into developing AI data centers, which has built a facility with 2,000 graphics processing units adjacent to the lot of repurposed EV batteries.&nbsp;



(That‚Äôs tiny as modern data centers go: Crusoe is developing a $500 billion AI data center for OpenAI and others in Abilene, Texas, where it expects to install 100,000 GPUs across its first two facilities by the end of the year, according to Forbes.)



Redwood‚Äôs project underscores a growing interest in powering data centers partially or entirely outside the electric grid. Not only would such microgrids be quicker to build than conventional power plants, but consumer ratepayers wouldn‚Äôt be on the hook for the cost of new grid-connected power plants developed to serve AI data centers.¬†Since Redwood‚Äôs batteries are used, and have already been removed from vehicles, the company says its microgrids should also be substantially cheaper than ones assembled from new batteries.



COURTESY REDWOOD MATERIALS




Redwood Energy‚Äôs microgrids could generate electricity for any kind of operation. But the company stresses they‚Äôre an ideal fit for addressing the growing energy needs and climate emissions of data centers. The energy consumption of such facilities could double by 2030, mainly due to the ravenous appetite of AI, according to an April report by the International Energy Agency.



‚ÄúStorage is this perfectly positioned technology, especially low-cost storage, to attack each of those problems,‚Äù Straubel says.



The Tahoe Reno Industrial Center is the epicenter of a data center development boom in northern Nevada that has sparked growing concerns about climate emissions and excessive demand for energy and water, as MIT Technology Review recently reported.



Straubel says the litany of data centers emerging around it ‚Äúwould be logical targets‚Äù for its new business line, but adds there are growth opportunities across the expanding data center clusters in Texas, Virginia, and the Midwest as well.



‚ÄúWe‚Äôre talking to a broad cross section of those companies,‚Äù he says.



Crusoe, which also provides cloud services, recently announced a joint venture with the investment firm Engine No. 1 to provide ‚Äúpowered data center real estate solutions‚Äù to AI companies by constructing 4.5 gigawatts of new natural-gas plants.



Redwood‚Äôs microgrid should provide more than 99% of the electricity Crusoe‚Äôs local facilities need. In the event of extended periods with little sunlight, a rarity in the Nevada desert, the company could still draw from the standard power grid.



Cully Cavness, cofounder and operating chief of Crusoe, says the company is already processing AI queries and producing conclusions for its customers at the Nevada facility. (Its larger data centers are dedicated to the more computationally intensive process of training AI models.)



Redwood‚Äôs new business division offers a test case for a strategy laid out in a paper late last year, which highlighted the potential for solar-powered microgrids to supply the energy that AI data centers need.



The authors of that paper found that microgrids could be built much faster than natural-gas plants and would generally be only a little more expensive as an energy source for data centers, so long as the facilities could occasionally rely on natural-gas generators to get them through extended periods of low sunlight.



If solar-powered microgrids were used to power 30 gigawatts of new AI data centers, with just 10% backup from natural gas, it would eliminate 400 million tons of carbon dioxide emissions relative to running the centers entirely on natural gas, the study found.&nbsp;



‚ÄúHaving a data center running off solar and storage is more or less what we were advocating for in our paper,‚Äù says Zeke Hausfather, climate lead at the payments company Stripe and a coauthor of the paper. He hopes that Redwood‚Äôs new microgrid will establish that ‚Äúthese sorts of systems work in the real world‚Äù and encourage other data center developers to look for similar solutions.¬†



Redwood Materials says electric vehicles are its fastest-growing source of used batteries, and it estimates that more than 100,000 EVs will come off US roads this year.



The company says it tests each battery to determine whether it can be reused. Those that qualify will be integrated into its modular storage systems, which can then store up energy from wind and solar installations or connect to the grid. As those batteries reach the end of their life, they‚Äôll be swapped out of the microgrids and moved into the company‚Äôs recycling process.&nbsp;



Redwood says it already has enough reusable batteries to build a gigawatt-hour‚Äôs worth of microgrids, capable of powering a little more than a million homes for an hour. In addition, the company‚Äôs new division has begun designing microgrids that are 10 times larger than the one it unveiled this week.



Straubel expects Redwood Energy to become a major business line, conceivably surpassing the company‚Äôs core recycling operation someday.



‚ÄúWe‚Äôre confident this is the lowest-cost solution out there,‚Äù he says.
‚Ä¢ The Download: Google DeepMind‚Äôs DNA AI, and heatwaves‚Äô impact on the grid
  This is today&#8217;s edition of¬†The Download,¬†our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



Google‚Äôs new AI will help researchers understand how our genes work



When scientists first sequenced the human genome in 2003, they revealed the full set of DNA instructions that make a person. But we still didn‚Äôt know what all those 3 billion genetic letters actually do.Now Google‚Äôs DeepMind division says it‚Äôs made a leap in trying to understand the code with AlphaGenome, an AI model that predicts what effects small changes in DNA will have on an array of molecular processes, such as whether a gene‚Äôs activity will go up or down.It‚Äôs just the sort of question biologists regularly assess in lab experiments, and is an attempt to further smooth biologists‚Äô work by answering basic questions about how changing DNA letters alters gene activity and, eventually, how genetic mutations affect our health. Read the full story.



‚ÄîAntonio Regalado







It‚Äôs officially summer, and the grid is stressed



It‚Äôs crunch time for the grid this week. Large swaths of the US have reached or exceeded record-breaking temperatures. Spain recently went through a dramatic heat wave too, as did the UK, which is bracing for another one soon.We rely on electricity to keep ourselves comfortable, and more to the point, safe. These are the moments we design the grid for: when need is at its very highest. The key to keeping everything running smoothly during these times might be just a little bit of flexibility. But demand for electricity from major grids is already peaking, and that&#8217;s a good reason to be a little nervous. Read the full story.



‚ÄîCasey Crownhart



This article is from The Spark, MIT Technology Review‚Äôs weekly climate newsletter. To receive it in your inbox every Wednesday, sign up here.







MIT Technology Review Narrated: How did China come to dominate the world of electric cars?



From generous government subsidies to support for lithium batteries, here are the keys to understanding how China managed to build a world-leading industry in electric vehicles.This is our latest story to be turned into a MIT Technology Review Narrated podcast, which¬†we‚Äôre publishing each week on Spotify and Apple Podcasts. Just navigate to MIT Technology Review Narrated on either platform, and follow us to get all our new content as it‚Äôs released.







Inside OpenAI‚Äôs empire with Karen Hao



Journalist Karen Hao‚Äôs newly released book, Empire of AI: Dreams and Nightmares in Sam Altman&#8217;s OpenAI, tells the story of OpenAI‚Äôs rise to power and its far-reaching impact all over the world.Hao, a former MIT Technology Review senior editor, will join our executive editor Niall Firth in an intimate subscriber-exclusive Roundtable conversation exploring the AI arms race, what it means for all of us, and where it‚Äôs headed. Register here to join us at 9am ET on Monday June 30th June.



Special giveaway: Attendees will have the chance to receive a free copy of Hao&#8217;s book. See registration form for details.







The must-reads



I‚Äôve combed the internet to find you today‚Äôs most fun/important/scary/fascinating stories about technology.



1 Meta has won an AI copyright case against authorsThe judge said the authors hadn‚Äôt presented enough evidence to back up their case. (TechCrunch)+ It‚Äôs not an entirely decisive victory for Meta, though. (Wired $)+ It‚Äôs the second lawsuit in favor of AI giants this week. (Insider $)



2 The US will stop contributing towards a global vaccine allianceRFK Jr made unsubstantiated claims about Gavi‚Äôs safety record. (WP $)+ Kennedy‚Äôs newly-assembled vaccine panel is reviewing its guidelines for children. (Vox)+ Experts are worried the once-influential panel will cause irreparable harm. (Ars Technica)+ How measuring vaccine hesitancy could help health professionals tackle it. (MIT Technology Review)



3 Jeff Bezos is cozying up to Donald TrumpIf the Trump administration happens to need a new space company, he‚Äôs ready and willing to supply it. (WSJ $)+ Meanwhile, a private astronaut mission is on its way to the ISS. (CNN)



4 Taiwan is working on suicide drones to defend itself from ChinaThe country is taking a leaf out of Ukraine‚Äôs defense book. (FT $)+ This giant microwave may change the future of war. (MIT Technology Review)



5 Biohackers are feeling emboldened by the Trump administrationThey welcome lower barriers to entry for their unorthodox treatments. (Wired $)+ The first US hub for experimental medical treatments is coming. (MIT Technology Review)



6 A UK cyberattack on a health firm contributed to a patient‚Äôs deathThe ransomware attack disrupted blood services at London hospitals. (BBC)+ A Russian hacking gang is to blame for the incident. (Bloomberg $)



7 Take a look inside Amazon‚Äôs colossal new data centerFour construction teams are working around the clock to finish it. (NYT $)+ Generating video is the most energy-intensive AI prompt. (WSJ $)+ We did the math on AI‚Äôs energy footprint. Here‚Äôs the story you haven‚Äôt heard. (MIT Technology Review)



8 The debate around dark energy is intensifyingNew research suggests it evolves over time. But not everyone agrees. (Undark)



9 Trump Mobile is no longer claiming to be ‚Äòmade in the USA‚ÄôIt‚Äôs now &#8220;designed with American values in mind‚Äù instead. (Ars Technica)



10 It‚Äôs official: The Social Network is getting a sequelZuck goes MAGA? (Deadline $)







Quote of the day



‚ÄúBy training generative AI models with copyrighted works, companies are creating something that often will dramatically undermine the market for those works, and thus dramatically undermine the incentive for human beings to create things the old-fashioned way.&#8221;



‚ÄîUS district judge Vince Chhabria, who presided over a copyright lawsuit brought against Meta by a group of authors, warns of the implications of the company‚Äôs actions, the Guardian reports.







One more thing







Beyond gene-edited babies: the possible paths for tinkering with human evolution



Editing human embryos is restricted in much of the world‚Äîand making an edited baby is fully illegal in most countries surveyed by legal scholars. But advancing technology could render the embryo issue moot.



New ways of adding CRISPR, the revolutionary gene editing tool, to the bodies of people already born could let them easily receive changes as well. It‚Äôs possible that in 125 years, many people will be the beneficiaries of multiple rare, but useful, gene mutations currently found in only small segments of the population.&nbsp;



These could protect us against common diseases and infections, but eventually they could also yield improvements in other traits, such as height, metabolism, or even cognition. But humanity won‚Äôt necessarily do things the right way. Read the full story.



‚ÄîAntonio Regalado







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)



+ Amazing things are happening in New York‚Äôs Central Park.+ A newly-discovered species of dinosaur has gone on display in London, and it‚Äôs small but perfectly formed.+ Cool‚ÄîBob Dylan is releasing a new art book, this time of his drawings.+ Iron Maiden bassist Steve Harris has a secret second career‚Äîas a footballer
‚Ä¢ Kayak and Expedia race to build AI travel agents that turn social posts into itineraries
  Kayak and Expedia reimagine the travel agent as an AI agent . Kayak
‚Ä¢ From chatbots to collaborators: How AI agents are reshaping enterprise work
  Anthropic's Scott White explains how AI agents evolved from chatbots to autonomous workers .
‚Ä¢ Between utopia and collapse: Navigating AI‚Äôs murky middle future
  AI is disrupting the world, but it also presents an opportunity to ask what we are
‚Ä¢ Identity theft hits 1.1M reports ‚Äî and authentication fatigue is only getting worse
  The tug-of-war between friction and freedom will be won by those who can
‚Ä¢ From hallucinations to hardware: Lessons from a real-world computer vision project gone sideways
  What we tried, what didn't work and how a combination of approaches helped us build
‚Ä¢ AI agents are hitting a liability wall. Mixus has a plan to overcome it using human overseers on high-risk workflows
  Mixus's "colleague-in-the-loop" model blends automation with
‚Ä¢ CTGT wins Best Presentation Style award at VB Transform 2025
  CTGT won the Best Presentation Style award at VB Transform 2025 in San Francisco . Founded by 23
‚Ä¢ Catio wins ‚Äòcoolest tech‚Äô award at VB Transform 2025
  Catio also announced the upcoming launch of Archie, a conversational, multi-agent
‚Ä¢ Retail Resurrection: David‚Äôs Bridal bets its future on AI after double bankruptcy
  How AI-driven personalization, knowledge graphs and a two-sided marketplace are creating
‚Ä¢ How runtime attacks turn profitable AI into budget black holes
  AI inference attacks drain enterprise budgets, derail regulatory compliance and destroy new AI deployment ROI
‚Ä¢ Model minimalism: The new AI strategy saving companies millions
  LLMs changed how enterprises build applications, but smaller AI models provide power and reduces cost
‚Ä¢ The inference trap: How cloud providers are eating your AI margins
  If you‚Äôre unsure about the load of different AI workloads, start with
‚Ä¢ The rise of prompt ops: Tackling hidden AI costs from bad inputs and context bloat
  Prompt ops can help manage, measure, monitor and tune prompts . AI models can get
‚Ä¢ Scaling smarter: How enterprise IT teams can right-size their compute for AI
  IT and business leaders plan and choose infrastructure can keep them from being doomed to pilot pur
‚Ä¢ Why your enterprise AI strategy needs both open and closed models: The TCO reality check
  Learn how enterprises evaluate open versus closed AI models to optimize costs, security, and performance
‚Ä¢ CFOs want AI that pays: real metrics, not marketing demos
  CFOs who master new eval frameworks will drive the next wave of AI adoption .
‚Ä¢ From pilot to profit: The real path to scalable, ROI-positive AI
  Organizations that delay systematic AI deployment risk being left behind by competitors who have already mastered the
‚Ä¢ Kumo‚Äôs ‚Äòrelational foundation model‚Äô predicts the future your LLM can‚Äôt see
  Forecasting is a fundamentally new capability that is missing from the current purview of gener
‚Ä¢ Can AI run a physical shop? Anthropic‚Äôs Claude tried and the results were gloriously, hilariously bad
  Anthropic's Claude ran a vending machine business for a month selling tungsten cubes
‚Ä¢ OpenAI‚Äôs API lead explains how enterprises are already succeeding with its Agents SDK and Responses API
  OpenAI now includes tracing and eval tools with the API stack to help teams define what
‚Ä¢ How Highmark Health and Google Cloud are using Gen AI to streamline medical claims and improve care: 6 key lessons
  Success in generative AI isn't reserved for those with the biggest budgets, but for
‚Ä¢ The hidden scaling cliff that‚Äôs about to break your agent rollouts
  Enterprise teams hit a scaling wall when managing AI agents across departments . Writer's May Hab
‚Ä¢ Walmart cracks enterprise AI at scale: Thousands of use cases, one framework
  Walmart VP Desir√©e Gosby reveals how the retailer scales AI across 255M customers
‚Ä¢ What enterprise leaders can learn from LinkedIn‚Äôs success with AI agents
  LinkedIn scientists share how they have found success with their LinkedIn hiring assistant . The AI agent
‚Ä¢ Lessons learned from agentic AI leaders reveal critical deployment strategies for enterprises
  What does it take to actually engineer AI agents to get the best return on investment?
‚Ä¢ Get paid faster: How Intuit‚Äôs new AI agents help businesses get funds up to 5 days faster and save 12 hours a month with autonomous workflows
  Intuit has integrated agentic AI deeply into multiple processes to help businesses get things done

üîí Cybersecurity & Privacy
‚Ä¢ Senator Chides FBI for Weak Advice on Mobile Security
  Agents with the Federal Bureau of Investigation (FBI) briefed Capitol Hill staff recently on hardening the security of their mobile devices, after a contacts list stolen from the personal phone of the White House Chief of Staff Susie Wiles was reportedly used to fuel a series of text messages and phone calls impersonating her to U.S. lawmakers. But in a letter this week to the FBI, one of the Senate&#8217;s most tech-savvy lawmakers says the feds aren&#8217;t doing enough to recommend more appropriate security protections that are already built into most consumer mobile devices.
A screenshot of the first page from Sen. Wyden&#8217;s letter to FBI Director Kash Patel.
On May 29, The Wall Street Journal reported that federal authorities were investigating a clandestine effort to impersonate Ms. Wiles via text messages and in phone calls that may have used AI to spoof her voice. According to The Journal, Wiles told associates her cellphone contacts were hacked, giving the impersonator access to the private phone numbers of some of the country&#8217;s most influential people.
The execution of this phishing and impersonation campaign &#8212; whatever its goals may have been &#8212; suggested the attackers were financially motivated, and not particularly sophisticated.
&#8220;It became clear to some of the lawmakers that the requests were suspicious when the impersonator began asking questions about Trump that Wiles should have known the answers to‚Äîand in one case, when the impersonator asked for a cash transfer, some of the people said,&#8221; the Journal wrote. &#8220;In many cases, the impersonator‚Äôs grammar was broken and the messages were more formal than the way Wiles typically communicates, people who have received the messages said. The calls and text messages also didn‚Äôt come from Wiles‚Äôs phone number.&#8221;
Sophisticated or not, the impersonation campaign was soon punctuated by the murder of Minnesota House of Representatives Speaker Emerita Melissa Hortman and her husband, and the shooting of Minnesota State Senator John Hoffman and his wife. So when FBI agents offered in mid-June to brief U.S. Senate staff on mobile threats, more than 140 staffers took them up on that invitation (a remarkably high number considering that no food was offered at the event).
But according to Sen. Ron Wyden (D-Ore.), the advice the FBI provided to Senate staffers was largely limited to remedial tips, such as not clicking on suspicious links or attachments, not using public wifi networks, turning off bluetooth, keeping phone software up to date, and rebooting regularly.
&#8220;This is insufficient to protect Senate employees and other high-value targets against foreign spies using advanced cyber tools,&#8221; Wyden wrote in a letter sent today to FBI Director Kash Patel. &#8220;Well-funded foreign intelligence agencies do not have to rely on phishing messages and malicious attachments to infect unsuspecting victims with spyware. Cyber mercenary companies sell their government customers advanced &#8216;zero-click&#8217; capabilities to deliver spyware that do not require any action by the victim.&#8221;
Wyden stressed that to help counter sophisticated attacks, the FBI should be encouraging lawmakers and their staff to enable anti-spyware defenses that are built into Apple&#8217;s iOS and Google&#8217;s Android phone software.
These include Apple&#8217;s Lockdown Mode, which is designed for users who are worried they may be subject to targeted attacks. Lockdown Mode restricts non-essential iOS features to reduce the device&#8217;s overall attack surface. Google Android devices carry a similar feature called Advanced Protection Mode.
Wyden also urged the FBI to update its training to recommend a number of other steps that people can take to make their mobile devices less trackable, including the use of ad blockers to guard against malicious advertisements, disabling ad tracking IDs in mobile devices, and opting out of commercial data brokers (the suspect charged in the Minnesota shootings reportedly used multiple people-search services to find the home addresses of his targets).
The senator&#8217;s letter notes that while the FBI has recommended all of the above precautions in various advisories issued over the years, the advice the agency is giving now to the nation&#8217;s leaders needs to be more comprehensive, actionable and urgent.
&#8220;In spite of the seriousness of the threat, the FBI has yet to provide effective defensive guidance,&#8221; Wyden said.
Nicholas Weaver is a researcher with the International Computer Science Institute, a nonprofit in Berkeley, Calif. Weaver said Lockdown Mode or Advanced Protection will mitigate many vulnerabilities, and should be the default setting for all members of Congress and their staff.
&#8220;Lawmakers are at exceptional risk and need to be exceptionally protected,&#8221; Weaver said. &#8220;Their computers should be locked down and well administered, etc. And the same applies to staffers.&#8221;
Weaver noted that Apple&#8217;s Lockdown Mode has a track record of blocking zero-day attacks on iOS applications; in September 2023, Citizen Lab documented how Lockdown Mode foiled a zero-click flaw capable of installing spyware on iOS devices without any interaction from the victim.

Earlier this month, Citizen Lab researchers documented a zero-click attack used to infect the iOS devices of two journalists with Paragon&#8217;s Graphite spyware. The vulnerability could be exploited merely by sending the target a booby-trapped media file delivered via iMessage. Apple also recently updated its advisory for the zero-click flaw (CVE-2025-43200), noting that it was mitigated as of iOS 18.3.1, which was released in February 2025.
Apple has not commented on whether CVE-2025-43200 could be exploited on devices with Lockdown Mode turned on. But HelpNetSecurity observed that at the same time Apple addressed CVE-2025-43200 back in February, the company fixed another vulnerability flagged by Citizen Lab researcher Bill Marczak: CVE-2025-24200, which Apple said was used in an extremely sophisticated physical attack against specific targeted individuals that allowed attackers to disable USB Restricted Mode on a locked device.
In other words, the flaw could apparently be exploited only if the attacker had physical access to the targeted vulnerable device. And as the old infosec industry adage goes, if an adversary has physical access to your device, it&#8217;s most likely not your device anymore.
I can&#8217;t speak to Google&#8217;s Advanced Protection Mode personally, because I don&#8217;t use Google or Android devices. But I have had Apple&#8217;s Lockdown Mode enabled on all of my Apple devices since it was first made available in September 2022. I can only think of a single occasion when one of my apps failed to work properly with Lockdown Mode turned on, and in that case I was able to add a temporary exception for that app in Lockdown Mode&#8217;s settings.
My main gripe with Lockdown Mode was captured in a March 2025 column by TechCrunch&#8217;s Lorenzo Francheschi-Bicchierai, who wrote about its penchant for periodically sending mystifying notifications that someone has been blocked from contacting you, even though nothing then prevents you from contacting that person directly. This has happened to me at least twice, and in both cases the person in question was already an approved contact, and said they had not attempted to reach out.
Although it would be nice if Apple&#8217;s Lockdown Mode sent fewer, less alarming and more informative alerts, the occasional baffling warning message is hardly enough to make me turn it off.

üéì University AI
No updates.

üè¢ Corporate AI
‚Ä¢ AI Testing and Evaluation: Learnings from genome editing
  Generative AI presents a unique challenge and opportunity to reexamine governance practices for the responsible development, deployment, and use of AI. To advance thinking in this space, Microsoft has tapped into the experience and knowledge of experts across domains‚Äîfrom genome editing to cybersecurity‚Äîto investigate the role of testing and evaluation as a governance tool. AI Testing and Evaluation: Learnings from Science and Industry, hosted by Microsoft Research‚Äôs Kathleen Sullivan, explores what the technology industry and policymakers can learn from these fields and how that might help shape the course of AI development.



In this episode, Alta Charo (opens in new tab), emerita professor of law and bioethics at the University of Wisconsin‚ÄìMadison, joins Sullivan for a conversation on the evolving landscape of genome editing and its regulatory implications. Drawing on decades of experience in biotechnology policy, Charo emphasizes the importance of distinguishing between hazards and risks and describes the field&#8217;s approach to regulating applications of technology rather than the technology itself. The discussion also explores opportunities and challenges in biotech‚Äôs multi-agency oversight model and the role of international coordination. Later, Daniel Kluttz (opens in new tab), a partner general manager in Microsoft&#8217;s Office of Responsible AI, joins Sullivan to discuss how insights from genome editing could inform more nuanced and robust governance frameworks for emerging technologies like AI.








Learn more:



Learning from other Domains to Advance AI Evaluation and Testing: Governance of Genome Edition in Human Therapeutics and Agricultural ApplicationsCase study | January 2025&nbsp;



Learning from other domains to advance AI evaluation and testing&nbsp;Microsoft Research Blog | June 2025&nbsp;



Responsible AI: Ethical policies and practices | Microsoft AI&nbsp;



AI and Microsoft Research&nbsp;









	
		
			Subscribe to the Microsoft Research Podcast:		
		
							
					
						  
						Apple Podcasts
					
				
			
							
					
						
						Email
					
				
			
							
					
						
						Android
					
				
			
							
					
						
						Spotify
					
				
			
							
					
						
						RSS Feed
					
				
					
	




	
		
			
				
					

Transcript



[MUSIC]



KATHLEEN SULLIVAN: Welcome to AI Testing and Evaluation: Learnings from Science and Industry. I&#8217;m your host, Kathleen Sullivan.



As generative AI continues to advance, Microsoft has gathered a range of experts‚Äîfrom genome editing to cybersecurity‚Äîto share how their fields approach evaluation and risk assessment. Our goal is to learn from their successes and their stumbles to move the science and practice of AI testing forward. In this series, we&#8217;ll explore how these insights might help guide the future of AI development, deployment, and responsible use.



[MUSIC ENDS]



Today I&#8217;m excited to welcome R. Alta Charo, the Warren P. Knowles Professor Emerita of Law and Bioethics at the University of Wisconsin‚ÄìMadison, to explore testing and risk assessment in genome editing.



Professor Charo has been at the forefront of biotechnology policy and governance for decades, advising former President Obama&#8217;s transition team on issues of medical research and public health, as well as serving as a senior policy advisor at the Food and Drug Administration. She consults on gene therapy and genome editing for various companies and organizations and has held positions on a number of advisory committees, including for the National Academy of Sciences. Her committee work has spanned women&#8217;s health, stem cell research, genome editing, biosecurity, and more.



After our conversation with Professor Charo, we&#8217;ll hear from Daniel Kluttz, a partner general manager in Microsoft&#8217;s Office of Responsible AI, about what these insights from biotech regulation could mean for AI governance and risk assessment and his team&#8217;s work governing sensitive AI uses and emerging technologies.



Alta, thank you so much for being here today. I&#8217;m a follower of your work and have really been looking forward to our conversation.



				
				
					



ALTA CHARO: It&#8217;s my pleasure. Thanks for having me.



SULLIVAN: Alta, I&#8217;d love to begin by stepping back in time a bit before you became a leading figure in bioethics and legal policy. You&#8217;ve shared that your interest in science was really inspired by your brothers‚Äô interest in the topic and that your upbringing really helped shape your perseverance and resilience. Can you talk to us about what put you on the path to law and policy?



CHARO: Well, I think it&#8217;s true that many of us are strongly influenced by our families and certainly my family had, kind of, a science-y, techy orientation. My father was a refugee, you know, escaping the Nazis, and when he finally was able to start working in the United States, he took advantage of the G.I. Bill to learn how to repair televisions and radios, which were really just coming in in the 1950s. So he was, kind of, technically oriented.



My mother retrained from being a talented amateur artist to becoming a math teacher, and not surprisingly, both my brothers began to aim toward things like engineering and chemistry and physics. And our form of entertainment was to watch PBS or Star Trek. [LAUGHTER]



And so the interest comes from that background coupled with, in the 1960s, this enormous surge of interest in the so-called nature-versus-nurture debate about the degree to which we are destined by our biology or shaped by our environments. It was a heady debate, and one that perfectly combined the two interests in politics and science.



SULLIVAN: For listeners who are brand new to your field in genomic editing, can you give us what I&#8217;ll call a ‚Äú90-second survey‚Äù of the space in perhaps plain language and why it&#8217;s important to have a framework for ensuring its responsible use.



CHARO: Well, you know, genome editing is both very old and very new. At base, what we&#8217;re talking about is a way to either delete sections of the genome, our collection of genes, or to add things or to alter what&#8217;s there. The goal is simply to be able to take what might not be healthy and make it healthy, whether it&#8217;s a plant, an animal, or a human.



Many people have compared it to a word processor, where you can edit text by swapping things in and out. You could change the letter g to the letter h in every word, and in our genomes, you can do similar kinds of things.



But because of this, we have a responsibility to make sure that whatever we change doesn&#8217;t become dangerous and that it doesn&#8217;t become socially disruptive. Now the earliest forms of genome editing were very inefficient, and so we didn&#8217;t worry that much. But with the advances that were spearheaded by people like Jennifer Doudna and Emmanuelle Charpentier, who won the Nobel Prize for their work in this area, genome editing has become much easier to do.



It&#8217;s become more efficient. It doesn&#8217;t require as much sophisticated laboratory equipment. It&#8217;s moved from being something that only a few people can do to something that we&#8217;re going to be seeing in our junior high school biology labs. And that means you have to pay attention to who&#8217;s doing it, why are they doing it, what are they releasing, if anything, into the environment, what are they trying to sell, and is it honest and is it safe?



SULLIVAN: How would you describe the risks, and are there, you know, sort of, specifically inherent risks in the technology itself, or do those risks really emerge only when it&#8217;s applied in certain contexts, like CRISPR in agriculture or CRISPR for human therapies?



CHARO: Well, to answer that, I&#8217;m going to do something that may seem a little picky, even pedantic. [LAUGHTER] But I&#8217;m going to distinguish between hazards and risks. So there are certain intrinsic hazards. That is, there are things that can go wrong.



You want to change one particular gene or one particular portion of a gene, and you might accidentally change something else, a so-called off-target effect. Or you might change something in a gene expecting a certain effect but not necessarily anticipating that there&#8217;s going to be an interaction between what you changed and what was there, a gene-gene interaction, that might have an unanticipated kind of result, a side effect essentially.



So there are some intrinsic hazards, but risk is a hazard coupled with the probability that it&#8217;s going to actually create something harmful. And that really depends upon the application.



If you are doing something that is making a change in a human being that is going to be a lifelong change, that enhances the significance of that hazard. It amplifies what I call the risk because if something goes wrong, then its consequences are greater.



It may also be that in other settings, what you&#8217;re doing is going to have a much lower risk because you&#8217;re working with a more familiar substance, your predictive power is much greater, and it&#8217;s not going into a human or an animal or into the environment. So I think that you have to say that the risk and the benefits, by the way, all are going to depend upon the particular application.



SULLIVAN: Yeah, I think on this point of application, there&#8217;s many players involved in that, right. Like, we often hear about this puzzle of who&#8217;s actually responsible for ensuring safety and a reasonable balance between risks and benefits or hazards and benefits, to quote you. Is it the scientists, the biotech companies, government agencies? And then if you could touch upon, as well, maybe how does the nature of genome editing risks ‚Ä¶ how do those responsibilities get divvied up?



CHARO: Well, in the 1980s, we had a very significant policy discussion about whether we should regulate the technology‚Äîno matter how it&#8217;s used or for whatever purpose‚Äîor if we should simply fold the technology in with all the other technologies that we currently have and regulate its applications the way we regulate applications generally. And we went for the second, the so-called coordinated framework.



So what we have in the United States is a system in which if you use genome editing in purely laboratory-based work, then you will be regulated the way we regulate laboratories.



There&#8217;s also, at most universities because of the way the government works with this, something called Institutional Biosafety Committees, IBCs. You want to do research that involves recombinant DNA and modern biotechnology, including genome editing but not limited to it, you have to go first to your IBC, and they look and see what you&#8217;re doing to decide if there&#8217;s a danger there that you have not anticipated that requires special attention.



If what you&#8217;re doing is going to get released into the environment or it&#8217;s going to be used to change an animal that&#8217;s going to be in the environment, then there are agencies that oversee the safety of our environment, predominantly the Environmental Protection Agency and the U.S. Department of Agriculture.



If you&#8217;re working with humans and you&#8217;re doing medical therapies, like you&#8217;re doing the gene therapies that just have been developed for things like sickle cell anemia, then you have to go through a very elaborate regulatory process that&#8217;s overseen by the Food and Drug Administration and also seen locally at the research stages overseen by institutional review boards that make sure the people who are being recruited into research understand what they&#8217;re getting into, that they&#8217;re the right people to be recruited, etc.



So we do have this kind of Jenga game ‚Ä¶



SULLIVAN: [LAUGHS] Yeah, sounds like it.



CHARO: ‚Ä¶ of regulatory agencies. And on top of all that, most of this involves professionals who&#8217;ve had to be licensed in some way. There may be state laws specifically on licensing. If you are dealing with things that might cross national borders, there may be international treaties and agreements that cover this.



And, of course, the insurance industry plays a big part because they decide whether or not what you&#8217;re doing is safe enough to be insured. So all of these things come together in a way that is not at all easy to understand if you&#8217;re not, kind of, working in the field. But the bottom-line thing to remember, the way to really think about it is, we don&#8217;t regulate genome editing; we regulate the things that use genome editing.



SULLIVAN: Yeah, that makes a lot of sense. Actually, maybe just following up a little bit on this notion of a variety of different, particularly like government agencies being involved. You know, in this multi-stakeholder model, where do you see gaps today that need to be filled, some of the pros and cons to keep in mind, and, you know, just as we think about distributing these systems at a global level, like, what are some of the considerations you are keeping in mind on that front?



CHARO: Well, certainly there are times where the way the statutes were written that govern the regulation of drugs or the regulation of foods did not anticipate this tremendous capacity we now have in the area of biotechnology generally or genome editing in particular. And so you can find that there are times where it feels a little bit ambiguous, and the agencies have to figure out how to apply their existing rules.



So an example. If you&#8217;re going to make alterations in an animal, right, we have a system for regulating drugs, including veterinary drugs. But we didn&#8217;t have something that regulated genome editing of animals. But in a sense, genome editing of an animal is the same thing as using a veterinary drug. You&#8217;re trying to affect the animal&#8217;s physical constitution in some fashion.



And it took a long time within the FDA to, sort of, work out how the regulation of veterinary drugs would apply if you think about the genetic construct that&#8217;s being used to alter the animal as the same thing as injecting a chemically based drug. And on that basis, they now know here&#8217;s the regulatory path‚Äîhere are the tests you have to do; here are the permissions you have to do; here&#8217;s the surveillance you have to do after it goes on the market.



Even there, sometimes, it was confusing. What happens when it&#8217;s not the kind of animal you&#8217;re thinking about when you think about animal drugs? Like, we think about pigs and dogs, but what about mosquitoes?



Because there, you&#8217;re really thinking more about pests, and if you&#8217;re editing the mosquito so that it can&#8217;t, for example, transmit dengue fever, right, it feels more like a public health thing than it is a drug for the mosquito itself, and it, kind of, fell in between the agencies that possibly had jurisdiction. And it took a while for the USDA, the Department of Agriculture, and the Food and Drug Administration to work out an agreement about how they would share this responsibility. So you do get those kinds of areas in which you have at least ambiguity.



We also have situations where frankly the fact that some things can move across national borders means you have to have a system for harmonizing or coordinating national rules. If you want to, for example, genetically engineer mosquitoes that can&#8217;t transmit dengue, mosquitoes have a tendency to fly. [LAUGHTER] And so &#8230; they can&#8217;t fly very far. That&#8217;s good. That actually makes it easier to control.



But if you&#8217;re doing work that&#8217;s right near a border, then you have to be sure that the country next to you has the same rules for whether it&#8217;s permitted to do this and how to surveil what you&#8217;ve done in order to be sure that you got the results you wanted to get and no other results. And that also is an area where we have a lot of work to be done in terms of coordinating across government borders and harmonizing our rules.



SULLIVAN: Yeah, I mean, you&#8217;ve touched on this a little bit, but there is such this striking balance between advancing technology, ensuring public safety, and sometimes, I think it feels just like you&#8217;re walking a tightrope where, you know, if we clamp down too hard, we&#8217;ll stifle innovation, and if we&#8217;re too lax, we risk some of these unintended consequences. And on a global scale like you just mentioned, as well. How has the field of genome editing found its balance?



CHARO: It&#8217;s still being worked out, frankly, but it&#8217;s finding its balance application by application. So in the United States, we have two very different approaches on regulation of things that are going to go into the market.



Some things can&#8217;t be marketed until they&#8217;ve gotten an approval from the government. So you come up with a new drug, you can&#8217;t sell that until it&#8217;s gone through FDA approval.



On the other hand, for most foods that are made up of familiar kinds of things, you can go on the market, and it&#8217;s only after they&#8217;re on the market that the FDA can act to withdraw it if a problem arises. So basically, we have either pre-market controls: you can&#8217;t go on without permission. Or post-market controls: we can take you off the market if a problem occurs.



How do we decide which one is appropriate for a particular application? It&#8217;s based on our experience. New drugs typically are both less familiar than existing things on the market and also have a higher potential for injury if they, in fact, are not effective or they are, in fact, dangerous and toxic.



If you have foods, even bioengineered foods, that are basically the same as foods that are already here, it can go on the market with notice but without a prior approval. But if you create something truly novel, then it has to go through a whole long process.



And so that is the way that we make this balance. We look at the application area. And we&#8217;re just now seeing in the Department of Agriculture a new approach on some of the animal editing, again, to try and distinguish between things that are simply a more efficient way to make a familiar kind of animal variant and those things that are genuinely novel and to have a regulatory process that is more rigid the more unfamiliar it is and the more that we see a risk associated with it.



SULLIVAN: I know we&#8217;re at the end of our time here and maybe just a quick kind of lightning-round of a question. For students, young scientists, lawyers, or maybe even entrepreneurs listening who are inspired by your work, what&#8217;s the single piece of advice you give them if they&#8217;re interested in policy, regulation, the ethical side of things in genomics or other fields?



CHARO: I&#8217;d say be a bio-optimist and read a lot of science fiction. Because it expands your imagination about what the world could be like. Is it going to be a world in which we&#8217;re now going to be growing our buildings instead of building them out of concrete?



Is it going to be a world in which our plants will glow in the evening so we don&#8217;t need to be using batteries or electrical power from other sources but instead our environment is adapting to our needs?



You know, expand your imagination with a sense of optimism about what could be and see ethics and regulation not as an obstacle but as a partner to bringing these things to fruition in a way that&#8217;s responsible and helpful to everyone.



[TRANSITION MUSIC]



SULLIVAN: Wonderful. Well, Alta, this has been just an absolute pleasure. So thank you.



CHARO: It was my pleasure. Thank you for having me.



SULLIVAN: Now, I&#8217;m happy to bring in Daniel Kluttz. As a partner general manager in Microsoft&#8217;s Office of Responsible AI, Daniel leads the group‚Äôs Sensitive Uses and Emerging Technologies program.



Daniel, it&#8217;s great to have you here. Thanks for coming in.



DANIEL KLUTTZ: It&#8217;s great to be here, Kathleen.



SULLIVAN: Yeah. So maybe before we unpack Alta Charo‚Äôs insights, I&#8217;d love to just understand the elevator pitch here. What exactly is [the] Sensitive Uses and Emerging Tech program, and what was the impetus for establishing it?



KLUTTZ: Yeah. So the Sensitive Uses and Emerging Technologies program sits within our Office of Responsible AI at Microsoft. And inherent in the name, there are two real core functions. There&#8217;s the sensitive uses and emerging technologies. What does that mean?



Sensitive uses, think of that as Microsoft&#8217;s internal consulting and oversight function for our higher-risk, most impactful AI system deployments. And so my team is a team of multidisciplinary experts who engages in sort of a white-glove-treatment sort of way with product teams at Microsoft that are designing, building, and deploying these higher-risk AI systems, and where that sort of consulting journey culminates is in a set of bespoke requirements tailored to the use case of that given system that really implement and apply our more standardized, generalized requirements that apply across the board.



Then the emerging technologies function of my team faces a little bit further out, trying to look around corners to see what new and novel and emerging risks are coming out of new AI technologies with the idea that we work with our researchers, our engineering partners, and, of course, product leaders across the company to understand where Microsoft is going with those emerging technologies, and we&#8217;re developing sort of rapid, quick-fire early-steer guidance that implements our policies ahead of that formal internal policymaking process, which can take a bit of time. So it&#8217;s designed to, sort of, both afford that innovation speed that we like to optimize for at Microsoft but also integrate our responsible AI commitments and our AI principles into emerging product development.



SULLIVAN: That segues really nicely, actually, as we met with Professor Charo and she was, you know, talking about the field of genome editing and the governing at the application level. I&#8217;d love to just understand how similar or not is that to managing the risks of AI in our world?



KLUTTZ: Yeah. I mean, Professor Charo‚Äôs comments were music to my ears because, you know, where we make our bread and butter, so to speak, in our team is in applying to use cases. AI systems, especially in this era of generative AI, are almost inherently multi-use, dual use. And so what really matters is how you&#8217;re going to apply that more general-purpose technology. Who&#8217;s going to use it? In what domain is it going to be deployed? And then tailor that oversight to those use cases. Try to be risk proportionate.



Professor Charo talked a little bit about this, but if it&#8217;s something that&#8217;s been done before and it&#8217;s just a new spin on an old thing, maybe we&#8217;re not so concerned about how closely we need to oversee and gate that application of that technology, whereas if it&#8217;s something new and novel or some new risk that might be posed by that technology, we take a little bit closer look and we are overseeing that in a more sort of high-touch way.



SULLIVAN: Maybe following up on that, I mean, how do you define sensitive use or maybe like high-impact application, and once that&#8217;s labeled, what happens? Like, what kind of steps kick in from there?



KLUTTZ: Yeah. So we have this Sensitive Uses program that&#8217;s been at Microsoft since 2019. I came to Microsoft in 2019 when we were starting this program in the Office of Responsible AI, and it had actually been incubated in Microsoft Research with our Aether community of colleagues who are experts in sociotechnical approaches to responsible AI, as well. Once we put it in the Office of Responsible AI, I came over. I came from academia. I was a researcher myself ‚Ä¶



SULLIVAN: At Berkeley, right?



KLUTTZ: At Berkeley. That&#8217;s right. Yep. Sociologist by training and a lawyer in a past life. [LAUGHTER] But that has helped sort of bridge those fields for me.



But Sensitive Uses, we force all of our teams when they&#8217;re envisioning their system design to think about, could the reasonably foreseeable use or misuse of the system that they&#8217;re developing in practice result in three really major, sort of, risk types. One is, could that deployment result in a consequential impact on someone&#8217;s legal position or life opportunity? Another category we have is, could that foreseeable use or misuse result in significant psychological or physical injury or harm? And then the third really ties in with a longstanding commitment we&#8217;ve had to human rights at Microsoft. And so could that system in it&#8217;s reasonably foreseeable use or misuse result in human rights impacts and injurious consequences to folks along different dimensions of human rights? 



Once you decide, we have a process to reporting that project into my office, and we will triage that project, working with the product team, for example, and our Responsible AI Champs community, which are folks who are dispersed throughout the ecosystem at Microsoft and educated in our responsible AI program, and then determine, OK, is it in scope for our program? If it is, say, OK, we&#8217;re going to go along for that ride with you, and then we get into that whole sort of consulting arrangement that then culminates in this set of bespoke use-case-based requirements applying our AI principles.



SULLIVAN: That&#8217;s super fascinating. What are some of the approaches in the governance of genome editing are you maybe seeing happening in AI governance or maybe just, like, bubbling up in conversations around it?



KLUTTZ: Yeah, I mean, I think we&#8217;ve learned a lot from fields like genome editing that Professor Charo talked about and others. And again, it gets back to this, sort of, risk-proportionate-based approach. It&#8217;s a balancing test. It&#8217;s a tradeoff of trying to, sort of, foster innovation and really look for the beneficial uses of these technologies. I appreciated her speaking about that. What are the intended uses of the system, right? And then getting to, OK, how do we balance trying to, again, foster that innovation in a very fast-moving space, a pretty complex space, and a very unsettled space contrasting to other, sort of, professional fields or technological fields that have a long history and are relatively settled from an oversight and regulatory standpoint? This one is not, and for good reason. It is still developing.



And I think, you know, there are certain oversight and policy regimes that exist today that can be applied. Professor Charo talked about this, as well, where, you know, maybe you have certain policy and oversight regimes that, depending on how the application of that technology is applied, applies there versus some horizontal, overarching regulatory sort of framework. And I think that applies from an internal governance standpoint, as well.



SULLIVAN: Yeah. It&#8217;s a great point. So what isn&#8217;t being explored from genome editing that, you know, maybe we think could be useful to AI governance, or as we think about the evolving frameworks ‚Ä¶



KLUTTZ: Yeah.



SULLIVAN: ‚Ä¶ what maybe we should be taking into account from what Professor Charo shared with us?



KLUTTZ: So one of the things I&#8217;ve thought about and took from Professor Charo‚Äôs discussion was she had just this amazing way of framing up how genome editing regulation is done. And she said, you know, we don&#8217;t regulate genome editing; we regulate the things that use genome editing. And while it&#8217;s not a one-to-one analogy with the AI space because we do have this sort of very general model level distinction versus application layer and even platform layer distinctions, I think it&#8217;s fair to say, you know, we don&#8217;t regulate AI applications writ large. We regulate the things that use AI in a very similar way. And that&#8217;s how we think of our internal policy and oversight process at Microsoft, as well.



And maybe there are things that we regulated and oversaw internally at the first instance and the first time we saw it come through, and it graduates into more of a programmatic framework for how we manage that. So one good example of that is some of our higher-risk AI systems that we offer out of Azure at the platform level. When I say that, I mean APIs that you call that developers can then build their own applications on top of. We were really deep in evaluating and assessing mitigations on those platform systems in the first instance, but we also graduated them into what we call our Limited Access AI services program.



And some of the things that Professor Charo discussed really resonated with me. You know, she had this moment where she was mentioning how, you know, you want to know who&#8217;s using your tools and how they&#8217;re being used. And it&#8217;s the same concepts. We want to have trust in our customers, we want to understand their use cases, and we want to apply technical controls that, sort of, force those use cases or give us signal post-deployment that use cases are being done in a way that may give us some level of concern, to reach out and understand what those use cases are.



SULLIVAN: Yeah, you&#8217;re hitting on a great point. And I love this kind of layered approach that we&#8217;re taking and that Alta highlighted, as well. Maybe to double-click a little bit just on that post-market control and what we&#8217;re tracking, kind of, once things are out and being used by our customers. How do we take some of that deployment data and bring it back in to maybe even better inform upfront governance or just how we think about some of the frameworks that we&#8217;re operating in?



KLUTTZ: It&#8217;s a great question. The number one thing is for us at Microsoft, we want to know the voice of our customer. We want our customers to talk to us. We don&#8217;t want to just understand telemetry and data. But it&#8217;s really getting out there and understanding from our customers and not just our customers. I would say our stakeholders is maybe a better term because that includes civil society organizations. It includes governments. It includes all of these non, sort of, customer actors that we care about and that we&#8217;re trying to sort of optimize for, as well. It includes end users of our enterprise customers. If we can gather data about how our products are being used and trying to understand maybe areas that we didn&#8217;t foresee how customers or users might be using those things, and then we can tune those systems to better align with what both customers and users want but also our own AI principles and policies and programs.



SULLIVAN: Daniel, before coming to Microsoft, you led social science research and sociotechnical applications of AI-driven tech at Berkeley. What do you think some of the biggest challenges are in defining and maybe even just, kind of, measuring at, like, a societal level some of the impacts of AI more broadly?



KLUTTZ: Measuring social phenomenon is a difficult thing. And one of the things that, as social scientists, you&#8217;re very interested in is scientifically observing and measuring social phenomena. Well, that sounds great. It sounds also very high level and jargony. What do we mean by that? You know, it&#8217;s very easy to say that you&#8217;re collecting data and you&#8217;re measuring, I don&#8217;t know, trust in AI, right? That&#8217;s a very fuzzy concept.



SULLIVAN: Right. Definitely.



KLUTTZ: It is a concept that we want to get to, but we have to unpack that, and we have to develop what we call measurable constructs. What are the things that we might observe that could give us an indication toward what is a very fuzzy and general concept. And there&#8217;s challenges with that everywhere. And I&#8217;m extremely fortunate to work at Microsoft with some of the world&#8217;s leading sociotechnical researchers and some of these folks who are thinking about‚Äîyou know, very steeped in measurement theory, literally PhDs in these fields‚Äîhow to both measure and allow for a scalable way to do that at a place the size of Microsoft. And that is trying to develop frameworks that are scalable and repeatable and put into our platform that then serves our product teams. Are we providing, as a platform, a service to those product teams that they can plug in and do their automated evaluations at scale as much as possible and then go back in over the top and do some of your more qualitative targeted testing and evaluations.



SULLIVAN: Yeah, makes a lot of sense. Before we close out, if you&#8217;re game for it, maybe we do a quick lightning round. Just 30-second answers here. Favorite real-world sensitive use case you&#8217;ve ever reviewed.



KLUTTZ: Oh gosh. Wow, this is where I get to be the social scientist.



SULLIVAN: [LAUGHS] Yes.



KLUTTZ: It‚Äôs like, define favorite, Kathleen. [LAUGHS] Most memorable, most painful.



SULLIVAN: Let&#8217;s do most memorable.



KLUTTZ: We‚Äôll do most memorable.



SULLIVAN: Yeah.



KLUTTZ: You know, I would say the most memorable project I worked on was when we rolled out the new Bing Chat, which is no longer called Bing Chat, because that was the first really big cross-company effort to deploy GPT-4, which was, you know, the next step up in AI innovation from our partners at OpenAI. And I really value working hand in hand with engineering teams and with researchers and that was us at our best and really sort of turbocharged the model that we have.



SULLIVAN: Wonderful. What&#8217;s one of the most overused phrases that you have in your AI governance meetings?



KLUTTZ: Gosh. [LAUGHS] If I hear ‚ÄúWe need to get aligned; we need to align on this more‚Äù ‚Ä¶



SULLIVAN: [LAUGHS] Right.



KLUTTZ: But, you know, it&#8217;s said for a reason. And I think it sort of speaks to that clever nature. That&#8217;s one that comes to mind.



SULLIVAN: That&#8217;s great. And then maybe, maybe last one. What are you most excited about in the next, I don&#8217;t know, let&#8217;s say three months? This world is moving so fast!



KLUTTZ: You know, the pace of innovation, as you just said, is just staggering. It is unbelievable. And sometimes it can feel overwhelming in my space. But what I am most excited about is how we are building up this Emerging ‚Ä¶ I mentioned this Emerging Technologies program in my team as a, sort of, formal program is relatively new. And I really enjoy being able to take a step back and think a little bit more about the future and a little bit more holistically. And I love working with engineering teams and sort of strategic visionaries who are thinking about what we&#8217;re doing a year from now or five years from now, or even 10 years from now, and I get to be a part of those conversations. And that really gives me energy and helps me ‚Ä¶ helps keep me grounded and not just dealing with the day to day, and, you know, various fire drills that you may run. It&#8217;s thinking strategically and having that foresight about what&#8217;s to come. And it&#8217;s exciting.



SULLIVAN: Great. Well, Daniel, just thanks so much for being here. I had such a wonderful discussion with you, and I think the thoughtfulness in our discussion today I hope resonates with our listeners. And again, thanks to Alta for setting the stage and sharing her really amazing, insightful thoughts here, as well. So thank you.



[MUSIC]



KLUTTZ: Thank you, Kathleen. I appreciate it. It&#8217;s been fun.



SULLIVAN: And to our listeners, thanks for tuning in. You can find resources related to this podcast in the show notes. And if you want to learn more about how Microsoft approaches AI governance, you can visit microsoft.com/RAI.



See you next time!&nbsp;



[MUSIC FADES]

				
			
			
				Show more			
		
	

Opens in a new tabThe post AI Testing and Evaluation: Learnings from genome editing appeared first on Microsoft Research.
‚Ä¢ PadChest-GR: A bilingual grounded radiology reporting benchmark for chest X-rays
  In our ever-evolving journey to enhance healthcare through technology, we‚Äôre announcing a unique new benchmark for grounded radiology report generation‚ÄîPadChest-GR (opens in new tab). The world‚Äôs first multimodal, bilingual sentence-level radiology report dataset, developed&nbsp;by the University of Alicante with Microsoft Research, University Hospital Sant Joan d‚ÄôAlacant and MedBravo, is set to redefine how AI and radiologists interpret radiological images. Our work demonstrates how collaboration between humans and AI can create powerful feedback loops‚Äîwhere new datasets drive better AI models, and those models, in turn, inspire richer datasets. We&#8217;re excited to share this progress in‚ÄØNEJM AI, highlighting both the clinical relevance and research excellence of this initiative.&nbsp;



A new frontier in radiology report generation&nbsp;



It is estimated that over half of people visiting hospitals have radiology scans that must be interpreted by a clinical professional. Traditional radiology reports often condense multiple findings into unstructured narratives. In contrast, grounded radiology reporting demands that each finding be described and localized individually.



This can mitigate the risk of AI fabrications and enable new interactive capabilities that enhance clinical and patient interpretability. PadChest-GR is the first bilingual dataset to address this need with 4,555 chest X-ray studies complete with Spanish and English sentence-level descriptions and precise spatial (bounding box) annotations for both positive and negative findings. It is the first public benchmark that enables us to evaluate generation of fully grounded radiology reports in chest X-rays.&nbsp;



Figure 1. Example of a grounded report from PadChest-GR. The original free-text report in Spanish was ‚ÄùMotivo de consulta: Preoperatorio. Rx PA t√≥rax: Impresi√≥n diagn√≥stica: Ateromatosis a√≥rtica calcificada. Engrosamiento pleural biapical. Atelectasia laminar basal izquierda. Elongaci√≥n a√≥rtica. Sin otros hallazgos radiol√≥gicos significativos.‚Äù



This benchmark isn‚Äôt standing alone‚Äîit plays a critical role in powering our state-of-the-art multimodal report generation model, MAIRA-2. Leveraging the detailed annotations of PadChest-GR, MAIRA-2 represents our commitment to building more interpretable and clinically useful AI systems. You can explore our work on MAIRA-2 on our project web page, including recent user research conducted with clinicians in healthcare settings.



PadChest-GR is a testament to the power of collaboration. Aurelia Bustos at MedBravo and Antonio Pertusa at the University of Alicante published the original&nbsp;PadChest dataset (opens in new tab) in 2020,&nbsp;with the help of Jose Mar√≠a Salinas from Hospital San Juan de Alicante and Mar√≠a de la Iglesia Vay√° from the Center of Excellence in Biomedical Imaging at the Ministry of Health in Valencia, Spain. We started to look at PadChest and were deeply impressed by the scale, depth, and diversity of the data.



As we worked more closely with the dataset, we realized the opportunity to develop this for grounded radiology reporting research and worked with the team at the University of Alicante to determine how to approach this together. Our complementary expertise was a nice fit. At Microsoft Research, our mission is to push the boundaries of medical AI through innovative, data-driven solutions. The University of Alicante, with its deep clinical expertise, provided critical insights that greatly enriched the dataset‚Äôs relevance and utility. The result of this collaboration is the PadChest-GR dataset.



A significant enabler of our annotation process was Centaur Labs. The team of senior and junior radiologists from the University Hospital Sant Joan d‚ÄôAlacant, coordinated by Joaquin Galant,&nbsp;used this HIPAA-compliant labeling platform to&nbsp;perform rigorous study-level quality control and bounding box annotations. The annotation protocol implemented ensured that each annotation was accurate and consistent, forming the backbone of a dataset designed for the next generation of grounded radiology report generation models.&nbsp;



Accelerating PadChest-GR dataset annotation with AI&nbsp;



Our approach integrates advanced large language models with comprehensive manual annotation:&nbsp;



Data Selection & Processing: Leveraging Microsoft Azure OpenAI Service (opens in new tab) with GPT-4, we extracted sentences describing individual positive and negative findings from raw radiology reports, translated them from Spanish to English, and linked each sentence to the existing expert labels from PadChest. This was done for a selected subset of the full PadChest dataset, carefully curated to reflect a realistic distribution of clinically relevant findings.&nbsp;



Manual Quality Control & Annotation: The processed studies underwent meticulous quality checks on the Centaur Labs platform by radiologist from Hospital San Juan de Alicante. Each positive finding was then annotated with bounding boxes to capture critical spatial information.&nbsp;



Standardization & Integration: All annotations were harmonized into coherent grounded reports, preserving the structure and context of the original findings while enhancing interpretability.&nbsp;



Figure 2. Overview of the data curation pipeline.



Impact and future directions&nbsp;



PadChest-GR not only sets a new benchmark for grounded radiology reporting, but also serves as the foundation for our MAIRA-2 model, which already showcases the potential of highly interpretable AI in clinical settings. While we developed PadChest-GR to help train and validate our own models, we believe the research community will greatly benefit from this dataset for many years to come. We look forward to seeing the broader research community build on this‚Äîimproving grounded reporting AI models and using PadChest-GR as a standard for evaluation. We believe that by fostering open collaboration and sharing our resources, we can accelerate progress in medical imaging AI and ultimately improve patient care together with the community.



The collaboration between Microsoft Research and the University of Alicante highlights the transformative power of working together across disciplines. With our publication in NEJM-AI and the integral role of PadChest-GR in the development of MAIRA-2 (opens in new tab) and RadFact (opens in new tab), we are excited about the future of AI-empowered radiology. We invite researchers and industry experts to explore PadChest-GR and MAIRA-2, contribute innovative ideas, and join us in advancing the field of grounded radiology reporting.&nbsp;



Papers already using PadChest-GR:




[2406.04449] MAIRA-2: Grounded Radiology Report Generation (opens in new tab)



RadVLM: A Multitask Conversational Vision-Language Model for Radiology (opens in new tab)



Enhancing Abnormality Grounding for Vision Language Models with Knowledge Descriptions (opens in new tab)



Visual Prompt Engineering for Vision Language Models in Radiology (opens in new tab)




For further details or to download PadChest-GR, please visit the BIMCV PadChest-GR Project (opens in new tab).&nbsp;



Models in the Azure Foundry that can do Grounded Reporting:&nbsp;




How to deploy and use CXRReportGen healthcare AI model with Azure AI Foundry &#8211; Azure AI Foundry | Microsoft Learn (opens in new tab)



Healthcare Orchestrator &#8211; Healthcare agent service | Microsoft Learn (opens in new tab)




Acknowledgement




Authors: Daniel C. Castro (opens in new tab), Aurelia Bustos (opens in new tab), Shruthi Bannur (opens in new tab), Stephanie L. Hyland (opens in new tab), Kenza Bouzid (opens in new tab), Maria Teodora Wetscherek (opens in new tab), Maria Dolores S√°nchez-Valverde (opens in new tab), Lara Jaques-P√©rez (opens in new tab), Lourdes P√©rez-Rodr√≠guez (opens in new tab), Kenji Takeda (opens in new tab), Jos√© Mar√≠a Salinas (opens in new tab), Javier Alvarez-Valle (opens in new tab), Joaqu√≠n Galant Herrero (opens in new tab), Antonio Pertusa (opens in new tab)&nbsp;





MSR Health Futures UK: Hannah Richardson, Valentina Salvatelli, Harshita Sharma, Sam Bond-Taylor, Max Ilse, Fernando Perez-Garcia, Anton Schwaighofer, Jonathan Carlson¬†





MSR Flow: Kenji Takeda, Evelyn Viegas, Ashley Llorens





HLS: Matthew Lungren, Naiteek Sangani, Shrey Jain, Ivan Tarapov, Will Guyman, Mert Oez, Chris Burt, David Ardman

Opens in a new tabThe post PadChest-GR: A bilingual grounded radiology reporting benchmark for chest X-rays appeared first on Microsoft Research.
‚Ä¢ Build and deploy AI inference workflows with new enhancements to the Amazon SageMaker Python SDK
  Amazon SageMaker Inference has been a popular tool for deploying advanced machine learning (ML) and generative AI models at scale. As AI applications become increasingly complex, customers want to deploy multiple models in a coordinated group that collectively process inference requests for an application. In addition, with the evolution of generative AI applications, many use cases now require inference workflows‚Äîsequences of interconnected models operating in predefined logical flows. This trend drives a growing need for more sophisticated inference offerings. 
To address this need, we are introducing a new capability in the SageMaker Python SDK that revolutionizes how you build and deploy inference workflows on SageMaker. We will take Amazon Search as an example to show case how this feature is used in helping customers building inference workflows. This new Python SDK capability provides a streamlined and simplified experience that abstracts away the underlying complexities of packaging and deploying groups of models and their collective inference logic, allowing you to focus on what matter most‚Äîyour business logic and model integrations. 
In this post, we provide an overview of the user experience, detailing how to set up and deploy these workflows with multiple models using the SageMaker Python SDK. We walk through examples of building complex inference workflows, deploying them to SageMaker endpoints, and invoking them for real-time inference. We also show how customers like Amazon Search plan to use SageMaker Inference workflows to provide more relevant search results to Amazon shoppers. 
Whether you are building a simple two-step process or a complex, multimodal AI application, this new feature provides the tools you need to bring your vision to life. This tool aims to make it easy for developers and businesses to create and manage complex AI systems, helping them build more powerful and efficient AI applications. 
In the following sections, we dive deeper into details of the SageMaker Python SDK, walk through practical examples, and showcase how this new capability can transform your AI development and deployment process. 
Key improvements and user experience 
The SageMaker Python SDK now includes new features for creating and managing inference workflows. These additions aim to address common challenges in developing and deploying inference workflows: 
 
 Deployment of multiple models&nbsp;‚Äì The core of this new experience is the deployment of multiple models as&nbsp;inference components&nbsp;within a single SageMaker endpoint. With this approach, you can create a more unified inference workflow. By consolidating multiple models into one endpoint, you can reduce the number of endpoints that need to be managed. This consolidation can also improve operational tasks, resource utilization, and potentially costs. 
 Workflow definition with workflow mode&nbsp;‚Äì The new workflow mode extends the existing&nbsp;Model Builder capabilities. It allows for the definition of inference workflows using Python code. Users familiar with the ModelBuilder class might find this feature to be an extension of their existing knowledge. This mode enables creating multi-step workflows, connecting models, and specifying the data flow between different models in the workflows. The goal is to reduce the complexity of managing these workflows and enable you to focus more on the logic of the resulting compound AI system. 
 Development and deployment options&nbsp;‚Äì A new deployment option has been introduced for the development phase. This feature is designed to allow for quicker deployment of workflows to development environments. The intention is to enable faster testing and refinement of workflows. This could be particularly relevant when experimenting with different configurations or adjusting models. 
 Invocation flexibility&nbsp;‚Äì The SDK now provides options for invoking individual models or entire workflows. You can choose to call a specific inference component used in a workflow or the entire workflow. This flexibility can be useful in scenarios where access to a specific model is needed, or when only a portion of the workflow needs to be executed. 
 Dependency management&nbsp;‚Äì You can use SageMaker&nbsp;Deep Learning Containers&nbsp;(DLCs) or the SageMaker distribution that comes preconfigured with various model serving libraries and tools. These are intended to serve as a starting point for common use cases. 
 
To get started, use the SageMaker Python SDK to deploy your models as inference components. Then, use the workflow mode to create an inference workflow, represented as Python code using the container of your choice. Deploy the workflow container as another inference component on the same endpoints as the models or a dedicated endpoint. You can run the workflow by invoking the inference component that represents the workflow. The user experience is entirely code-based, using the SageMaker Python SDK. This approach allows you to define, deploy, and manage inference workflows using SDK abstractions offered by this feature and Python programming. The workflow mode provides flexibility to specify complex sequences of model invocations and data transformations, and the option to deploy as components or endpoints caters to various scaling and integration needs. 
Solution overview 
The following diagram illustrates a reference architecture using the SageMaker Python SDK. 
 
The improved SageMaker Python SDK introduces a more intuitive and flexible approach to building and deploying AI inference workflows. Let‚Äôs explore the key components and classes that make up the experience: 
 
 ModelBuilder simplifies the process of packaging individual models as inference components. It handles model loading, dependency management, and container configuration automatically. 
 The CustomOrchestrator class provides a standardized way to define custom inference logic that orchestrates multiple models in the workflow. Users implement the handle() method to specify this logic and can use an orchestration library or none at all (plain Python). 
 A single deploy() call handles the deployment of the components and workflow orchestrator. 
 The Python SDK supports invocation against the custom inference workflow or individual inference components. 
 The Python SDK supports both synchronous and streaming inference. 
 
CustomOrchestrator&nbsp;is an abstract base class that serves as a template for defining custom inference orchestration logic. It standardizes the structure of entry point-based inference scripts, making it straightforward for users to create consistent and reusable code. The handle method in the class is an abstract method that users implement to define their custom orchestration logic. 
 
  
  class CustomOrchestrator (ABC):
"""
Templated class used to standardize the structure of an entry point based inference script.
"""

    @abstractmethod
    def handle(self, data, context=None):
        """abstract class for defining an entrypoint for the model server"""
        return NotImplemented 
  
 
With this templated class, users can integrate into their custom workflow code, and then point to this code in the model builder using a file path or directly using a class or method name. Using this class and the&nbsp;ModelBuilder&nbsp;class, it enables a more streamlined workflow for AI inference: 
 
 Users define their custom workflow by implementing the CustomOrchestrator class. 
 The custom CustomOrchestrator is passed to ModelBuilder using the ModelBuilder inference_spec parameter. 
 ModelBuilder packages the CustomOrchestrator along with the model artifacts. 
 The packaged model is deployed to a SageMaker endpoint (for example, using a TorchServe container). 
 When invoked, the SageMaker endpoint uses the custom handle() function defined in the CustomOrchestrator to handle the input payload. 
 
In the follow sections, we provide two examples of custom workflow orchestrators implemented with plain Python code. For simplicity, the examples use two inference components. 
We explore how to create a simple workflow that deploys two large language models (LLMs) on SageMaker Inference endpoints along with a simple Python orchestrator that calls the two models. We create an IT customer service workflow where one model processes the initial request and another suggests solutions. You can find the example notebook in the&nbsp;GitHub repo. 
Prerequisites 
To run the example notebooks, you need an AWS account with an&nbsp;AWS Identity and Access Management&nbsp;(IAM) role with&nbsp;least-privilege permissions&nbsp;to manage resources created. For details, refer to&nbsp;Create an AWS account. You might need to request a service quota increase for the corresponding SageMaker hosting instances. In this example, we host multiple models on the same SageMaker endpoint, so we use two ml.g5.24xlarge SageMaker hosting instances. 
Python inference orchestration 
First, let‚Äôs define our custom orchestration class that inherits from CustomOrchestrator. The workflow is structured around a custom inference entry point that handles the request data, processes it, and retrieves predictions from the configured model endpoints. See the following code: 
 
 class PythonCustomInferenceEntryPoint(CustomOrchestrator):
    def __init__(self, region_name, endpoint_name, component_names):
        self.region_name = region_name
        self.endpoint_name = endpoint_name
        self.component_names = component_names
    
    def preprocess(self, data):
        payload = {
            "inputs": data.decode("utf-8")
        }
        return json.dumps(payload)

    def _invoke_workflow(self, data):
        # First model (Llama) inference
        payload = self.preprocess(data)
        
        llama_response = self.client.invoke_endpoint(
            EndpointName=self.endpoint_name,
            Body=payload,
            ContentType="application/json",
            InferenceComponentName=self.component_names[0]
        )
        llama_generated_text = json.loads(llama_response.get('Body').read())['generated_text']
        
        # Second model (Mistral) inference
        parameters = {
            "max_new_tokens": 50
        }
        payload = {
            "inputs": llama_generated_text,
            "parameters": parameters
        }
        mistral_response = self.client.invoke_endpoint(
            EndpointName=self.endpoint_name,
            Body=json.dumps(payload),
            ContentType="application/json",
            InferenceComponentName=self.component_names[1]
        )
        return {"generated_text": json.loads(mistral_response.get('Body').read())['generated_text']}
    
    def handle(self, data, context=None):
        return self._invoke_workflow(data) 
 
This code performs the following functions: 
 
 Defines the orchestration that sequentially calls two models using their inference component names 
 Processes the response from the first model before passing it to the second model 
 Returns the final generated response 
 
This plain Python approach provides flexibility and control over the request-response flow, enabling seamless cascading of outputs across multiple model components. 
Build and deploy the workflow 
To deploy the workflow, we first create our inference components and then build the custom workflow. One inference component will host a Meta Llama 3.1 8B model, and the other will host a Mistral 7B model. 
 
  
  from sagemaker.serve import ModelBuilder
from sagemaker.serve.builder.schema_builder import SchemaBuilder

# Create a ModelBuilder instance for Llama 3.1 8B
# Pre-benchmarked ResourceRequirements will be taken from JumpStart, as Llama-3.1-8b is a supported model.
llama_model_builder = ModelBuilder(
    model="meta-textgeneration-llama-3-1-8b",
    schema_builder=SchemaBuilder(sample_input, sample_output),
    inference_component_name=llama_ic_name,
    instance_type="ml.g5.24xlarge"
)

# Create a ModelBuilder instance for Mistral 7B model.
mistral_mb = ModelBuilder(
    model="huggingface-llm-mistral-7b",
    instance_type="ml.g5.24xlarge",
    schema_builder=SchemaBuilder(sample_input, sample_output),
    inference_component_name=mistral_ic_name,
    resource_requirements=ResourceRequirements(
        requests={
           "memory": 49152,
           "num_accelerators": 2,
           "copies": 1
        }
    ),
    instance_type="ml.g5.24xlarge"
) 
  
 
Now we can tie it all together to create one more ModelBuilder to which we pass the modelbuilder_list, which contains the ModelBuilder objects we just created for each inference component and the custom workflow. Then we call the build() function to prepare the workflow for deployment. 
 
  
  # Create workflow ModelBuilder
orchestrator= ModelBuilder(
    inference_spec=PythonCustomInferenceEntryPoint(
        region_name=region,
        endpoint_name=llama_mistral_endpoint_name,
        component_names=[llama_ic_name, mistral_ic_name],
    ),
    dependencies={
        "auto": False,
        "custom": [
            "cloudpickle",
            "graphene",
            # Define other dependencies here.
        ],
    },
    sagemaker_session=Session(),
    role_arn=role,
    resource_requirements=ResourceRequirements(
        requests={
           "memory": 4096,
           "num_accelerators": 1,
           "copies": 1,
           "num_cpus": 2
        }
    ),
    name=custom_workflow_name, # Endpoint name for your custom workflow
    schema_builder=SchemaBuilder(sample_input={"inputs": "test"}, sample_output="Test"),
    modelbuilder_list=[llama_model_builder, mistral_mb] # Inference Component ModelBuilders created in Step 2
)
# call the build function to prepare the workflow for deployment
orchestrator.build() 
  
 
In the preceding code snippet, you can comment out the section that defines the resource_requirements to have the custom workflow deployed on a separate endpoint instance, which can be a dedicated CPU instance to handle the custom workflow payload. 
By calling the deploy() function, we deploy the custom workflow and the inference components to your desired instance type, in this example ml.g5.24.xlarge. If you choose to deploy the custom workflow to a separate instance, by default, it will use the ml.c5.xlarge instance type. You can set inference_workflow_instance_type and inference_workflow_initial_instance_count to configure the instances required to host the custom workflow. 
 
  
  predictors = orchestrator.deploy(
    instance_type="ml.g5.24xlarge",
    initial_instance_count=1,
    accept_eula=True, # Required for Llama3
    endpoint_name=llama_mistral_endpoint_name
    # inference_workflow_instance_type="ml.t2.medium", # default
    # inference_workflow_initial_instance_count=1 # default
) 
  
 
Invoke the endpoint 
After you deploy the workflow, you can invoke the endpoint using the predictor object: 
 
 from sagemaker.serializers import JSONSerializer
predictors[-1].serializer = JSONSerializer()
predictors[-1].predict("Tell me a story about ducks.") 
 
You can also invoke each inference component in the deployed endpoint. For example, we can test the Llama inference component with a synchronous invocation, and Mistral with streaming: 
 
  
  from sagemaker.predictor import Predictor
# create predictor for the inference component of Llama model
llama_predictor = Predictor(endpoint_name=llama_mistral_endpoint_name, component_name=llama_ic_name)
llama_predictor.content_type = "application/json"

llama_predictor.predict(json.dumps(payload)) 
  
 
When handling the streaming response, we need to read each line of the output separately. The following example code demonstrates this streaming handling by checking for newline characters to separate and print each token in real time: 
 
  
  mistral_predictor = Predictor(endpoint_name=llama_mistral_endpoint_name, component_name=mistral_ic_name)
mistral_predictor.content_type = "application/json"

body = json.dumps({
    "inputs": prompt,
    # specify the parameters as needed
    "parameters": parameters
})

for line in mistral_predictor.predict_stream(body):
    decoded_line = line.decode('utf-8')
    if '\n' in decoded_line:
        # Split by newline to handle multiple tokens in the same line
        tokens = decoded_line.split('\n')
        for token in tokens[:-1]:  # Print all tokens except the last one with a newline
            print(token)
        # Print the last token without a newline, as it might be followed by more tokens
        print(tokens[-1], end='')
    else:
        # Print the token without a newline if it doesn't contain '\n'
        print(decoded_line, end='') 
  
 
So far, we have walked through the example code to demonstrate how to build complex inference logic using Python orchestration, deploy them to SageMaker endpoints, and invoke them for real-time inference. The Python SDK automatically handles the following: 
 
 Model packaging and container configuration 
 Dependency management and environment setup 
 Endpoint creation and component coordination 
 
Whether you‚Äôre building a simple workflow of two models or a complex multimodal application, the new SDK provides the building blocks needed to bring your inference workflows to life with minimal boilerplate code. 
Customer story: Amazon Search 
Amazon Search is a critical component of the Amazon shopping experience, processing an enormous volume of queries across billions of products across diverse categories. At the core of this system are sophisticated matching and ranking workflows, which determine the order and relevance of search results presented to customers. These workflows execute large deep learning models in predefined sequences, often sharing models across different workflows to improve price-performance and accuracy. This approach makes sure that whether a customer is searching for electronics, fashion items, books, or other products, they receive the most pertinent results tailored to their query. 
The SageMaker Python SDK enhancement offers valuable capabilities that align well with Amazon Search‚Äôs requirements for these ranking workflows. It provides a standard interface for developing and deploying complex inference workflows crucial for effective search result ranking. The enhanced Python SDK enables efficient reuse of shared models across multiple ranking workflows while maintaining the flexibility to customize logic for specific product categories. Importantly, it allows individual models within these workflows to scale independently, providing optimal resource allocation and performance based on varying demand across different parts of the search system. 
Amazon Search&nbsp;is exploring the broad adoption of these Python SDK enhancements across their search ranking infrastructure. This initiative aims to further refine and improve search capabilities, enabling the team to build, version, and catalog workflows that power search ranking more effectively across different product categories. The ability to share models across workflows and scale them independently offers new levels of efficiency and adaptability in managing the complex search ecosystem. 
Vaclav Petricek, Sr. Manager of Applied Science at Amazon Search, highlighted the potential impact of these SageMaker Python SDK enhancements: ‚ÄúThese capabilities represent a significant advancement in our ability to develop and deploy sophisticated inference workflows that power search matching and ranking. The flexibility to build workflows using Python, share models across workflows, and scale them independently is particularly exciting, as it opens up new possibilities for optimizing our search infrastructure and rapidly iterating on our matching and ranking algorithms as well as new AI features. Ultimately, these SageMaker Inference enhancements will allow us to more efficiently create and manage the complex algorithms powering Amazon‚Äôs search experience, enabling us to deliver even more relevant results to our customers.‚Äù 
The following diagram illustrates a sample solution architecture used by Amazon Search. 
 
Clean up 
When you‚Äôre done testing the models, as a best practice, delete the endpoint to save costs if the endpoint is no longer required. You can follow the cleanup section the demo notebook or use following code to delete the model and endpoint created by the demo: 
 
  
  mistral_predictor.delete_predictor()
llama_predictor.delete_predictor()
llama_predictor.delete_endpoint()
workflow_predictor.delete_predictor() 
  
 
Conclusion 
The new SageMaker Python SDK enhancements for inference workflows mark a significant advancement in the development and deployment of complex AI inference workflows. By abstracting the underlying complexities, these enhancements empower inference customers to focus on innovation rather than infrastructure management. This feature bridges sophisticated AI applications with the robust SageMaker infrastructure, enabling developers to use familiar Python-based tools while harnessing the powerful inference capabilities of SageMaker. 
Early adopters, including Amazon Search, are already exploring how these capabilities can drive major improvements in AI-powered customer experiences across diverse industries. We invite all SageMaker users to explore this new functionality, whether you‚Äôre developing classic ML models, building generative AI applications or multi-model workflows, or tackling multi-step inference scenarios. The enhanced SDK provides the flexibility, ease of use, and scalability needed to bring your ideas to life. As AI continues to evolve, SageMaker Inference evolves with it, providing you with the tools to stay at the forefront of innovation. Start building your next-generation AI inference workflows today with the enhanced SageMaker Python SDK. 
 
About the authors 
Melanie Li, PhD, is a Senior Generative AI Specialist Solutions Architect at AWS based in Sydney, Australia, where her focus is on working with customers to build solutions leveraging state-of-the-art AI and machine learning tools. She has been actively involved in multiple Generative AI initiatives across APJ, harnessing the power of Large Language Models (LLMs). Prior to joining AWS, Dr. Li held data science roles in the financial and retail industries. 
Saurabh Trikande is a Senior Product Manager for Amazon Bedrock and SageMaker Inference. He is passionate about working with customers and partners, motivated by the goal of democratizing AI. He focuses on core challenges related to deploying complex AI applications, inference with multi-tenant models, cost optimizations, and making the deployment of Generative AI models more accessible. In his spare time, Saurabh enjoys hiking, learning about innovative technologies, following TechCrunch, and spending time with his family. 
Osho Gupta&nbsp;is a Senior Software Developer at AWS SageMaker. He is passionate about ML infrastructure space, and is motivated to learn &amp; advance underlying technologies that optimize Gen AI training &amp; inference performance. In his spare time, Osho enjoys paddle boarding, hiking, traveling, and spending time with his friends &amp; family. 
Joseph Zhang is a software engineer at AWS. He started his AWS career at EC2 before eventually transitioning to SageMaker, and now works on developing GenAI-related features. Outside of work he enjoys both playing and watching sports (go Warriors!), spending time with family, and making coffee. 
Gary Wang is a Software Developer at AWS SageMaker. He is passionate about AI/ML operations and building new things. In his spare time, Gary enjoys running, hiking, trying new food, and spending time with his friends and family. 
James Park&nbsp;is a Solutions Architect at Amazon Web Services. He works with Amazon.com to design, build, and deploy technology solutions on AWS, and has a particular interest in AI and machine learning. In h is spare time he enjoys seeking out new cultures, new experiences, &nbsp;and staying up to date with the latest technology trends. You can find him on LinkedIn. 
Vaclav Petricek is a Senior Applied Science Manager at Amazon Search, where he led teams that built Amazon Rufus and now leads science and engineering teams that work on the next generation of Natural Language Shopping. He is passionate about shipping AI experiences that make people‚Äôs lives better. Vaclav loves off-piste skiing, playing tennis, and backpacking with his wife and three children. 
Wei Li is a Senior Software Dev Engineer in Amazon Search. She is passionate about Large Language Model training and inference technologies, and loves integrating these solutions into Search Infrastructure to enhance natural language shopping experiences. During her leisure time, she enjoys gardening, painting, and reading. 
Brian Granger is a Senior Principal Technologist at Amazon Web Services and a professor of physics and data science at Cal Poly State University in San Luis Obispo, CA. He works at the intersection of UX design and engineering on tools for scientific computing, data science, machine learning, and data visualization. Brian is a co-founder and leader of Project Jupyter, co-founder of the Altair project for statistical visualization, and creator of the PyZMQ project for ZMQ-based message passing in Python. At AWS he is a technical and open source leader in the AI/ML organization. Brian also represents AWS as a board member of the PyTorch Foundation. He is a winner of the 2017 ACM Software System Award and the 2023 NASA Exceptional Public Achievement Medal for his work on Project Jupyter. He has a Ph.D. in theoretical physics from the University of Colorado.
‚Ä¢ Context extraction from image files in Amazon Q Business using LLMs
  To effectively convey complex information, organizations increasingly rely on visual documentation through diagrams, charts, and technical illustrations. Although text documents are well-integrated into modern knowledge management systems, rich information contained in diagrams, charts, technical schematics, and visual documentation often remains inaccessible to search and AI assistants. This creates significant gaps in organizational knowledge bases, leading to interpreting visual data manually and preventing automation systems from using critical visual information for comprehensive insights and decision-making. While Amazon Q Business already handles embedded images within documents, the custom document enrichment (CDE) feature extends these capabilities significantly by processing standalone image files (for example, JPGs and PNGs). 
In this post, we look at a step-by-step implementation for using the CDE feature within an Amazon Q Business application. We walk you through an AWS Lambda function configured within CDE to process various image file types, and we showcase an example scenario of how this integration enhances the Amazon Q Business ability to provide comprehensive insights. By following this practical guide, you can significantly expand your organization‚Äôs searchable knowledge base, enabling more complete answers and insights that incorporate both textual and visual information sources. 
Example scenario: Analyzing regional educational demographics 
Consider a scenario where you‚Äôre working for a national educational consultancy that has charts, graphs, and demographic data across different AWS Regions stored in an Amazon Simple Storage Service (Amazon S3) bucket. The following image shows student distribution by age range across various cities using a bar chart. The insights in visualizations like this are valuable for decision-making but traditionally locked within image formats in your S3 buckets and other storage. 
With Amazon Q Business and CDE, we show you how to enable natural language queries against such visualizations. For example, your team could ask questions such as ‚ÄúWhich city has the highest number of students in the 13‚Äì15 age range?‚Äù or ‚ÄúCompare the student demographics between City 1 and City 4‚Äù directly through the Amazon Q Business application interface. 
 
You can bridge this gap using the Amazon Q Business CDE feature to: 
 
 Detect and process image files during the document ingestion process 
 Use Amazon Bedrock with AWS Lambda to interpret the visual information 
 Extract structured data and insights from charts and graphs 
 Make this information searchable using natural language queries 
 
Solution overview 
In this solution, we walk you through how to implement a CDE-based solution for your educational demographic data visualizations. The solution empowers organizations to extract meaningful information from image files using the CDE capability of Amazon Q Business. When Amazon Q Business encounters the S3 path during ingestion, CDE rules automatically trigger a Lambda function. The Lambda function identifies the image files and calls the Amazon Bedrock API, which uses multimodal large language models (LLMs) to analyze and extract contextual information from each image. The extracted text is then seamlessly integrated into the knowledge base in Amazon Q Business. End users can then quickly search for valuable data and insights from images based on their actual context. By bridging the gap between visual content and searchable text, this solution helps organizations unlock valuable insights previously hidden within their image repositories. 
The following figure shows the high-level architecture diagram used for this solution. 
 
For this use case, we use Amazon S3 as our data source. However, this same solution is adaptable to other data source types supported by Amazon Q Business, or it can be implemented with custom data sources as needed.To complete the solution, follow these high-level implementation steps: 
 
 Create an Amazon Q Business application and sync with an S3 bucket. 
 Configure the Amazon Q Business application CDE for the Amazon S3 data source. 
 Extract context from the images. 
 
Prerequisites 
The following prerequisites are needed for implementation: 
 
 An AWS account. 
 At least one Amazon Q Business Pro user that has admin permissions to set up and configure Amazon Q Business. For pricing information, refer to Amazon Q Business pricing. 
 AWS Identity and Access Management (IAM) permissions to create and manage IAM roles and policies. 
 A supported data source to connect, such as an S3 bucket containing your public documents. 
 Access to an Amazon Bedrock LLM in the required AWS Region. 
 
Create an Amazon Q Business application and sync with an S3 bucket 
To create an Amazon Q Business application and connect it to your S3 bucket, complete the following steps. These steps provide a general overview of how to create an Amazon Q Business application and synchronize it with an S3 bucket. For more comprehensive, step-by-step guidance, follow the detailed instructions in the blog post Discover insights from Amazon S3 with Amazon Q S3 connector. 
 
 Initiate your application setup through either the AWS Management Console or AWS Command Line Interface (AWS CLI). 
 Create an index for your Amazon Q Business application. 
 Use the built-in Amazon S3 connector to link your application with documents stored in your organization‚Äôs S3 buckets. 
 
Configure the Amazon Q Business application CDE for the Amazon S3 data source 
With the CDE feature of Amazon Q Business, you can make the most of your Amazon S3 data sources by using the sophisticated capabilities to modify, enhance, and filter documents during the ingestion process, ultimately making enterprise content more discoverable and valuable. When connecting Amazon Q Business to S3 repositories, you can use CDE to seamlessly transform your raw data, applying modifications that significantly improve search quality and information accessibility. This powerful functionality extends to extracting context from binary files such as images through integration with Amazon Bedrock services, enabling organizations to unlock insights from previously inaccessible content formats. By implementing CDE for Amazon S3 data sources, businesses can maximize the utility of their enterprise data within Amazon Q, creating a more comprehensive and intelligent knowledge base that responds effectively to user queries.To configure the Amazon Q Business application CDE for the Amazon S3 data source, complete the following steps: 
 
 Select your application and navigate to Data sources. 
 Choose your existing Amazon S3 data source or create a new one. Verify that Audio/Video under Multi-media content configuration is not enabled. 
 In the data source configuration, locate the Custom Document Enrichment section. 
 Configure the pre-extraction rules to trigger a Lambda function when specific S3 bucket conditions are satisfied. Check the following screenshot for an example configuration. 
 
 Pre-extraction rules are executed before Amazon Q Business processes files from your S3 bucket. 
Extract context from the images 
To extract insights from an image file, the Lambda function makes an Amazon Bedrock API call using Anthropic‚Äôs Claude 3.7 Sonnet model. You can modify the code to use other Amazon Bedrock models based on your use case. 
Constructing the prompt is a critical piece of the code. We recommend trying various prompts to get the desired output for your use case. Amazon Bedrock offers the capability to optimize a prompt that you can use to enhance your use case specific input. 
Examine the following Lambda function code snippets, written in Python, to understand the Amazon Bedrock model setup along with a sample prompt to extract insights from an image. 
In the following code snippet, we start by importing relevant Python libraries, define constants, and initialize AWS SDK for Python (Boto3) clients for Amazon S3 and Amazon Bedrock runtime. For more information, refer to the Boto3 documentation. 
 
 import boto3
import logging
import json
from typing import List, Dict, Any
from botocore.config import Config

MODEL_ID = "us.anthropic.claude-3-7-sonnet-20250219-v1:0"
MAX_TOKENS = 2000
MAX_RETRIES = 2
FILE_FORMATS = ("jpg", "jpeg", "png")

logger = logging.getLogger()
logger.setLevel(logging.INFO)
s3 = boto3.client('s3')
bedrock = boto3.client('bedrock-runtime', config=Config(read_timeout=3600, region_name='us-east-1')) 
 
The prompt passed to the Amazon Bedrock model, Anthropic‚Äôs Claude 3.7 Sonnet in this case, is broken into two parts: prompt_prefix and prompt_suffix. The prompt breakdown makes it more readable and manageable. Additionally, the Amazon Bedrock prompt caching feature can be used to reduce response latency as well as input token cost. You can modify the prompt to extract information based on your specific use case as needed. 
 
 prompt_prefix = """You are an expert image reader tasked with generating detailed descriptions for various """
"""types of images. These images may include technical diagrams,"""
""" graphs and charts, categorization diagrams, data flow and process flow diagrams,"""
""" hierarchical and timeline diagrams, infographics, """
"""screenshots and product diagrams/images from user manuals. """
""" The description of these images needs to be very detailed so that user can ask """
""" questions based on the image, which can be answered by only looking at the descriptions """
""" that you generate.
Here is the image you need to analyze:

&lt;image&gt;
"""

prompt_suffix = """
&lt;/image&gt;

Please follow these steps to analyze the image and generate a comprehensive description:

1. Image type: Classify the image as one of technical diagrams, graphs and charts, categorization diagrams, data flow and process flow diagrams, hierarchical and timeline diagrams, infographics, screenshots and product diagrams/images from user manuals. The description of these images needs to be very detailed so that user can ask questions based on the image, which can be answered by only looking at the descriptions that you generate or other.

2. Items:
&nbsp;&nbsp; Carefully examine the image and extract all entities, texts, and numbers present. List these elements in &lt;image_items&gt; tags.

3. Detailed Description:
&nbsp;&nbsp; Using the information from the previous steps, provide a detailed description of the image. This should include the type of diagram or chart, its main purpose, and how the various elements interact or relate to each other. &nbsp;Capture all the crucial details that can be used to answer any followup questions. Write this description in &lt;image_description&gt; tags.

4. Data Estimation (for charts and graphs only):
&nbsp;&nbsp; If the image is a chart or graph, capture the data in the image in CSV format to be able to recreate the image from the data. Ensure your response captures all relevant details from the chart that might be necessary to answer any follow up questions from the chart.
&nbsp;&nbsp; If exact values cannot be inferred, provide an estimated range for each value in &lt;estimation&gt; tags.
&nbsp;&nbsp; If no data is present, respond with "No data found".

Present your analysis in the following format:

&lt;analysis&gt;
&lt;image_type&gt;
[Classify the image type here]
&lt;/image_type&gt;

&lt;image_items&gt;
[List all extracted entities, texts, and numbers here]
&lt;/image_items&gt;

&lt;image_description&gt;
[Provide a detailed description of the image here]
&lt;/image_description&gt;

&lt;data&gt;
[If applicable, provide estimated number ranges for chart elements here]
&lt;/data&gt;
&lt;/analysis&gt;

Remember to be thorough and precise in your analysis. If you're unsure about any aspect of the image, state your uncertainty clearly in the relevant section.
"""
 
 
The lambda_handler is the main entry point for the Lambda function. While invoking this Lambda function, the CDE passes the data source‚Äôs information within event object input. In this case, the S3 bucket and the S3 object key are retrieved from the event object along with the file format. Further processing of the input happens only if the file_format matches the expected file types. For production ready code, implement proper error handling for unexpected errors. 
 
 def lambda_handler(event, context):
&nbsp;&nbsp; &nbsp;logger.info("Received event: %s" % json.dumps(event))
&nbsp;&nbsp; &nbsp;s3Bucket = event.get("s3Bucket")
&nbsp;&nbsp; &nbsp;s3ObjectKey = event.get("s3ObjectKey")
&nbsp;&nbsp; &nbsp;metadata = event.get("metadata")
&nbsp;&nbsp; &nbsp;file_format = s3ObjectKey.lower().split('.')[-1]
&nbsp;&nbsp; &nbsp;new_key = 'cde_output/' + s3ObjectKey + '.txt'
&nbsp;&nbsp; &nbsp;if (file_format in FILE_FORMATS):
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;afterCDE = generate_image_description(s3Bucket, s3ObjectKey, file_format)
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;s3.put_object(Bucket = s3Bucket, Key = new_key, Body=afterCDE)
&nbsp;&nbsp; &nbsp;return {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"version" : "v0",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"s3ObjectKey": new_key,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"metadataUpdates": []
&nbsp;&nbsp; &nbsp;} 
 
The generate_image_description function calls two other functions: first to construct the message that is passed to the Amazon Bedrock model and second to invoke the model. It returns the final text output extracted from the image file by the model invocation. 
 
 def generate_image_description(s3Bucket: str, s3ObjectKey: str, file_format: str) -&gt; str:
&nbsp;&nbsp; &nbsp;"""
&nbsp;&nbsp; &nbsp;Generate a description for an image.
&nbsp;&nbsp; &nbsp;Inputs:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;image_file: str - Path to the image file
&nbsp;&nbsp; &nbsp;Output:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;str - Generated image description
&nbsp;&nbsp; &nbsp;"""
&nbsp;&nbsp; &nbsp;messages = _llm_input(s3Bucket, s3ObjectKey, file_format)
&nbsp;&nbsp; &nbsp;response = _invoke_model(messages)
&nbsp;&nbsp; &nbsp;return response['output']['message']['content'][0]['text']
 
 
The _llm_input function takes in the S3 object‚Äôs details passed as input along with the file type (png, jpg) and builds the message in the format expected by the model invoked by Amazon Bedrock. 
 
 def _llm_input(s3Bucket: str, s3ObjectKey: str, file_format: str) -&gt; List[Dict[str, Any]]:
&nbsp;&nbsp; &nbsp;s3_response = s3.get_object(Bucket = s3Bucket, Key = s3ObjectKey)
&nbsp;&nbsp; &nbsp;image_content = s3_response['Body'].read()
&nbsp;&nbsp; &nbsp;message = {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"role": "user",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"content": [
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;{"text": prompt_prefix},
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;{
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"image": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"format": file_format,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"source": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"bytes": image_content
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;},
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;{"text": prompt_suffix}
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;]
&nbsp;&nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp;return [message]
 
 
The _invoke_model function calls the converse API using the Amazon Bedrock runtime client. This API returns the response generated by the model. The values within inferenceConfig settings for maxTokens and temperature are used to limit the length of the response and make the responses more deterministic (less random) respectively. 
 
 def _invoke_model(messages: List[Dict[str, Any]]) -&gt; Dict[str, Any]:
&nbsp;&nbsp; &nbsp;"""
&nbsp;&nbsp; &nbsp;Call the Bedrock model with retry logic.
&nbsp;&nbsp; &nbsp;Input:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;messages: List[Dict[str, Any]] - Prepared messages for the model
&nbsp;&nbsp; &nbsp;Output:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;Dict[str, Any] - Model response
&nbsp;&nbsp; &nbsp;"""
&nbsp;&nbsp; &nbsp;for attempt in range(MAX_RETRIES):
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;try:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;response = bedrock.converse(
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;modelId=MODEL_ID,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;messages=messages,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;inferenceConfig={
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"maxTokens": MAX_TOKENS,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"temperature": 0,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;)
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;return response
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;except Exception as e:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;print(e)
&nbsp;&nbsp; &nbsp;
&nbsp;&nbsp; &nbsp;raise Exception(f"Failed to call model after {MAX_RETRIES} attempts") 
 
Putting all the preceding code pieces together, the full Lambda function code is shown in the following block: 
 
 # Example Lambda function for image processing
import boto3
import logging
import json
from typing import List, Dict, Any
from botocore.config import Config

MODEL_ID = "us.anthropic.claude-3-7-sonnet-20250219-v1:0"
MAX_TOKENS = 2000
MAX_RETRIES = 2
FILE_FORMATS = ("jpg", "jpeg", "png")

logger = logging.getLogger()
logger.setLevel(logging.INFO)
s3 = boto3.client('s3')
bedrock = boto3.client('bedrock-runtime', config=Config(read_timeout=3600, region_name='us-east-1'))

prompt_prefix = """You are an expert image reader tasked with generating detailed descriptions for various """
"""types of images. These images may include technical diagrams,"""
""" graphs and charts, categorization diagrams, data flow and process flow diagrams,"""
""" hierarchical and timeline diagrams, infographics, """
"""screenshots and product diagrams/images from user manuals. """
""" The description of these images needs to be very detailed so that user can ask """
""" questions based on the image, which can be answered by only looking at the descriptions """
""" that you generate.
Here is the image you need to analyze:

&lt;image&gt;
"""

prompt_suffix = """
&lt;/image&gt;

Please follow these steps to analyze the image and generate a comprehensive description:

1. Image type: Classify the image as one of technical diagrams, graphs and charts, categorization diagrams, data flow and process flow diagrams, hierarchical and timeline diagrams, infographics, screenshots and product diagrams/images from user manuals. The description of these images needs to be very detailed so that user can ask questions based on the image, which can be answered by only looking at the descriptions that you generate or other.

2. Items:
&nbsp;&nbsp; Carefully examine the image and extract all entities, texts, and numbers present. List these elements in &lt;image_items&gt; tags.

3. Detailed Description:
&nbsp;&nbsp; Using the information from the previous steps, provide a detailed description of the image. This should include the type of diagram or chart, its main purpose, and how the various elements interact or relate to each other. &nbsp;Capture all the crucial details that can be used to answer any followup questions. Write this description in &lt;image_description&gt; tags.

4. Data Estimation (for charts and graphs only):
&nbsp;&nbsp; If the image is a chart or graph, capture the data in the image in CSV format to be able to recreate the image from the data. Ensure your response captures all relevant details from the chart that might be necessary to answer any follow up questions from the chart.
&nbsp;&nbsp; If exact values cannot be inferred, provide an estimated range for each value in &lt;estimation&gt; tags.
&nbsp;&nbsp; If no data is present, respond with "No data found".

Present your analysis in the following format:

&lt;analysis&gt;
&lt;image_type&gt;
[Classify the image type here]
&lt;/image_type&gt;

&lt;image_items&gt;
[List all extracted entities, texts, and numbers here]
&lt;/image_items&gt;

&lt;image_description&gt;
[Provide a detailed description of the image here]
&lt;/image_description&gt;

&lt;data&gt;
[If applicable, provide estimated number ranges for chart elements here]
&lt;/data&gt;
&lt;/analysis&gt;

Remember to be thorough and precise in your analysis. If you're unsure about any aspect of the image, state your uncertainty clearly in the relevant section.
"""

def _llm_input(s3Bucket: str, s3ObjectKey: str, file_format: str) -&gt; List[Dict[str, Any]]:
&nbsp;&nbsp; &nbsp;s3_response = s3.get_object(Bucket = s3Bucket, Key = s3ObjectKey)
&nbsp;&nbsp; &nbsp;image_content = s3_response['Body'].read()
&nbsp;&nbsp; &nbsp;message = {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"role": "user",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"content": [
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;{"text": prompt_prefix},
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;{
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"image": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"format": file_format,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"source": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"bytes": image_content
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;},
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;{"text": prompt_suffix}
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;]
&nbsp;&nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp;return [message]

def _invoke_model(messages: List[Dict[str, Any]]) -&gt; Dict[str, Any]:
&nbsp;&nbsp; &nbsp;"""
&nbsp;&nbsp; &nbsp;Call the Bedrock model with retry logic.
&nbsp;&nbsp; &nbsp;Input:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;messages: List[Dict[str, Any]] - Prepared messages for the model
&nbsp;&nbsp; &nbsp;Output:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;Dict[str, Any] - Model response
&nbsp;&nbsp; &nbsp;"""
&nbsp;&nbsp; &nbsp;for attempt in range(MAX_RETRIES):
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;try:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;response = bedrock.converse(
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;modelId=MODEL_ID,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;messages=messages,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;inferenceConfig={
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"maxTokens": MAX_TOKENS,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"temperature": 0,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;)
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;return response
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;except Exception as e:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;print(e)
&nbsp;&nbsp; &nbsp;
&nbsp;&nbsp; &nbsp;raise Exception(f"Failed to call model after {MAX_RETRIES} attempts")

def generate_image_description(s3Bucket: str, s3ObjectKey: str, file_format: str) -&gt; str:
&nbsp;&nbsp; &nbsp;"""
&nbsp;&nbsp; &nbsp;Generate a description for an image.
&nbsp;&nbsp; &nbsp;Inputs:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;image_file: str - Path to the image file
&nbsp;&nbsp; &nbsp;Output:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;str - Generated image description
&nbsp;&nbsp; &nbsp;"""
&nbsp;&nbsp; &nbsp;messages = _llm_input(s3Bucket, s3ObjectKey, file_format)
&nbsp;&nbsp; &nbsp;response = _invoke_model(messages)
&nbsp;&nbsp; &nbsp;return response['output']['message']['content'][0]['text']

def lambda_handler(event, context):
&nbsp;&nbsp; &nbsp;logger.info("Received event: %s" % json.dumps(event))
&nbsp;&nbsp; &nbsp;s3Bucket = event.get("s3Bucket")
&nbsp;&nbsp; &nbsp;s3ObjectKey = event.get("s3ObjectKey")
&nbsp;&nbsp; &nbsp;metadata = event.get("metadata")
&nbsp;&nbsp; &nbsp;file_format = s3ObjectKey.lower().split('.')[-1]
&nbsp;&nbsp; &nbsp;new_key = 'cde_output/' + s3ObjectKey + '.txt'
&nbsp;&nbsp; &nbsp;if (file_format in FILE_FORMATS):
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;afterCDE = generate_image_description(s3Bucket, s3ObjectKey, file_format)
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;s3.put_object(Bucket = s3Bucket, Key = new_key, Body=afterCDE)
&nbsp;&nbsp; &nbsp;return {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"version" : "v0",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"s3ObjectKey": new_key,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"metadataUpdates": []
&nbsp;&nbsp; &nbsp;} 
 
We strongly recommend testing and validating code in a nonproduction environment before deploying it to production. In addition to Amazon Q pricing, this solution will incur charges for AWS Lambda and Amazon Bedrock. For more information, refer to AWS Lambda pricing and Amazon Bedrock pricing. 
After the Amazon S3 data is synced with the Amazon Q index, you can prompt the Amazon Q Business application to get the extracted insights as shown in the following section. 
Example prompts and results 
The following question and answer pairs refer the Student Age Distribution graph at the beginning of this post. 
Q: Which City has the highest number of students in the 13-15 age range? 
 
Q: Compare the student demographics between City 1 and City 4? 
 
In the original graph, the bars representing student counts lacked explicit numerical labels, which could make data interpretation challenging on a scale. However, with Amazon Q Business and its integration capabilities, this limitation can be overcome. By using Amazon Q Business to process these visualizations with Amazon Bedrock LLMs using the CDE feature, we‚Äôve enabled a more interactive and insightful analysis experience. The service effectively extracts the contextual information embedded in the graph, even when explicit labels are absent. This powerful combination means that end users can ask questions about the visualization and receive responses based on the underlying data. Rather than being limited by what‚Äôs explicitly labeled in the graph, users can now explore deeper insights through natural language queries. This capability demonstrates how Amazon Q Business transforms static visualizations into queryable knowledge assets, enhancing the value of your existing data visualizations without requiring additional formatting or preparation work. 
Best practices for Amazon S3 CDE configuration 
When setting up CDE for your Amazon S3 data source, consider these best practices: 
 
 Use conditional rules to only process specific file types that need transformation. 
 Monitor Lambda execution with Amazon CloudWatch to track processing errors and performance. 
 Set appropriate timeout values for your Lambda functions, especially when processing large files. 
 Consider incremental syncing to process only new or modified documents in your S3 bucket. 
 Use document attributes to track which documents have been processed by CDE. 
 
Cleanup 
Complete the following steps to clean up your resources: 
 
 Go to the Amazon Q Business application and select Remove and unsubscribe for users and groups. 
 Delete the Amazon Q Business application. 
 Delete the Lambda function. 
 Empty and delete the S3 bucket. For instructions, refer to Deleting a general purpose bucket. 
 
Conclusion 
This solution demonstrates how combining Amazon Q Business, custom document enrichment, and Amazon Bedrock can transform static visualizations into queryable knowledge assets, significantly enhancing the value of existing data visualizations without additional formatting work. By using these powerful AWS services together, organizations can bridge the gap between visual information and actionable insights, enabling users to interact with different file types in more intuitive ways. 
Explore What is Amazon Q Business? and Getting started with Amazon Bedrock in the documentation to implement this solution for your specific use cases and unlock the potential of your visual data. 
About the Authors 
 
About the authors 
Amit Chaudhary Amit Chaudhary is a Senior Solutions Architect at Amazon Web Services. His focus area is AI/ML, and he helps customers with generative AI, large language models, and prompt engineering. Outside of work, Amit enjoys spending time with his family. 
Nikhil Jha Nikhil Jha is a Senior Technical Account Manager at Amazon Web Services. His focus areas include AI/ML, building Generative AI resources, and analytics. In his spare time, he enjoys exploring the outdoors with his family.
‚Ä¢ Build AWS architecture diagrams using Amazon Q CLI and MCP
  Creating professional AWS architecture diagrams is a fundamental task for solutions architects, developers, and technical teams. These diagrams serve as essential communication tools for stakeholders, documentation of compliance requirements, and blueprints for implementation teams. However, traditional diagramming approaches present several challenges: 
 
 Time-consuming process ‚Äì Creating detailed architecture diagrams manually can take hours or even days 
 Steep learning curve ‚Äì Learning specialized diagramming tools requires significant investment 
 Inconsistent styling ‚Äì Maintaining visual consistency across multiple diagrams is difficult 
 Outdated AWS icons ‚Äì Keeping up with the latest AWS service icons and best practices challenging. 
 Difficult maintenance ‚Äì Updating diagrams as architectures evolve can become increasingly burdensome 
 
Amazon Q Developer CLI with the Model Context Protocol (MCP) offers a streamlined approach to creating AWS architecture diagrams. By using generative AI through natural language prompts, architects can now generate professional diagrams in minutes rather than hours, while adhering to AWS best practices. 
In this post, we explore how to use Amazon Q Developer CLI with the AWS Diagram MCP and the AWS Documentation MCP servers to create sophisticated architecture diagrams that follow AWS best practices. We discuss techniques for basic diagrams and real-world diagrams, with detailed examples and step-by-step instructions. 
Solution overview 
Amazon Q Developer CLI is a command line interface that brings the generative AI capabilities of Amazon Q directly to your terminal. Developers can interact with Amazon Q through natural language prompts, making it an invaluable tool for various development tasks. 
Developed by Anthropic as an open protocol, the Model Context Protocol (MCP) provides a standardized way to connect AI models to virtually any data source or tool. Using a client-server architecture (as illustrated in the following diagram), the MCP helps developers expose their data through lightweight MCP servers while building AI applications as MCP clients that connect to these servers. 
The MCP uses a client-server architecture containing the following components: 
 
 Host ‚Äì A program or AI tool that requires access to data through the MCP protocol, such as Anthropic‚Äôs Claude Desktop, an integrated development environment (IDE), AWS MCP CLI, or other AI applications 
 Client ‚Äì Protocol clients that maintain one-to-one connections with server 
 Server ‚Äì Lightweight programs that expose capabilities through standardized MCP or act as tools 
 Data sources ‚Äì Local data sources such as databases and file systems, or external systems available over the internet through APIs (web APIs) that MCP servers can connect with 
 
 
As announced in April 2025, MCP enables Amazon Q Developer to connect with specialized servers that extend its capabilities beyond what‚Äôs possible with the base model alone. MCP servers act as plugins for Amazon Q, providing domain-specific knowledge and functionality. The AWS Diagram MCP server specifically enables Amazon Q to generate architecture diagrams using the Python diagrams package, with access to the complete AWS icon set and architectural best practices. 
Prerequisites 
To implement this solution, you must have an AWS account with appropriate permissions and follow the steps below. 
Set up your environment 
Before you can start creating diagrams, you need to set up your environment with Amazon Q CLI, the AWS Diagram MCP server, and AWS Documentation MCP server. This section provides detailed instructions for installation and configuration. 
Install Amazon Q Developer CLI 
Amazon Q Developer CLI is available as a standalone installation. Complete the following steps to install it: 
 
 Download and install Amazon Q Developer CLI. For instructions, see Using Amazon Q Developer on the command line. 
 Verify the installation by running the following command: q --version You should see output similar to the following: Amazon Q Developer CLI version 1.x.x 
 Configure Amazon Q CLI with your AWS credentials: q login 
 Choose the login method suitable for you: 
   
   Use for free with AWS Builder ID 
   Use with Pro license 
    
 
Set up MCP servers 
Complete the following steps to set up your MCP servers: 
 
 Install uv using the following command: pip install uv 
 Install Python 3.10 or newer: uv python install 3.10 
 Install GraphViz for your operating system. 
 Add the servers to your ~/.aws/amazonq/mcp.json file: 
 
{
  "mcpServers": {
    "awslabs.aws-diagram-mcp-server": {
      "command": "uvx",
      "args": ["awslabs.aws-diagram-mcp-server"],
      "env": {
        "FASTMCP_LOG_LEVEL": "ERROR"
      },
      "autoApprove": [],
      "disabled": false
    },
    "awslabs.aws-documentation-mcp-server": {
      "command": "uvx",
      "args": ["awslabs.aws-documentation-mcp-server@latest"],
      "env": {
        "FASTMCP_LOG_LEVEL": "ERROR"
      },
      "autoApprove": [],
      "disabled": false
    }
  }
}
 
Now, Amazon Q CLI automatically discovers MCP servers in the ~/.aws/amazonq/mcp.json file. 
Understanding MCP server tools 
The AWS Diagram MCP server provides several powerful tools: 
 
 list_icons ‚Äì Lists available icons from the diagrams package, organized by provider and service category 
 get_diagram_examples ‚Äì Provides example code for different types of diagrams (AWS, sequence, flow, class, and others) 
 generate_diagram ‚Äì Creates a diagram from Python code using the diagrams package 
 
The AWS Documentation MCP server provides the following useful tools: 
 
 search_documentation ‚Äì Searches AWS documentation using the official AWS Documentation Search API 
 read_documentation ‚Äì Fetches and converts AWS documentation pages to markdown format 
 recommend ‚Äì Gets content recommendations for AWS documentation pages 
 
These tools work together to help you create accurate architecture diagrams that follow AWS best practices. 
Test your setup 
Let‚Äôs verify that everything is working correctly by generating a simple diagram: 
 
 Start the Amazon Q CLI chat interface and verify the output shows the MCP servers being loaded and initialized: q chat  
 In the chat interface, enter the following prompt: Please create a diagram showing an EC2 instance in a VPC connecting to an external S3 bucket. Include essential networking components (VPC, subnets, Internet Gateway, Route Table), security elements (Security Groups, NACLs), and clearly mark the connection between EC2 and S3. Label everything appropriately concisely and indicate that all resources are in the us-east-1 region. Check for AWS documentation to ensure it adheres to AWS best practices before you create the diagram. 
 Amazon Q CLI will ask you to trust the tool that is being used; enter t to trust it.Amazon Q CLI will generate and display a simple diagram showing the requested architecture. Your diagram should look similar to the following screenshot, though there might be variations in layout, styling, or specific details because it‚Äôs created using generative AI. The core architectural components and relationships will be represented, but the exact visual presentation might differ slightly with each generation.  If you see the diagram, your environment is set up correctly. If you encounter issues, verify that Amazon Q CLI can access the MCP servers by making sure you installed the necessary tools and the servers are in the ~/.aws/amazonq/mcp.json file. 
 
Configuration options 
The AWS Diagram MCP server supports several configuration options to customize your diagramming experience: 
 
 Output directory ‚Äì By default, diagrams are saved in a generated-diagrams directory in your current working directory. You can specify a different location in your prompts. 
 Diagram format ‚Äì The default output format is PNG, but you can request other formats like SVG in your prompts. 
 Styling options ‚Äì You can specify colors, shapes, and other styling elements in your prompts. 
 
Now that our environment is set up, let‚Äôs create more diagrams. 
Create AWS architecture diagrams 
In this section, we walk through the process of multiple AWS architecture diagrams using Amazon Q CLI with the AWS Diagram MCP server and AWS Documentation MCP server to make sure our requirements follow best practices. 
When you provide a prompt to Amazon Q CLI, the AWS Diagram and Documentation MCP servers complete the following steps: 
 
 Interpret your requirements. 
 Check for best practices on the AWS documentation. 
 Generate Python code using the diagrams package. 
 Execute the code to create the diagram. 
 Return the diagram as an image. 
 
This process happens seamlessly, so you can focus on describing what you want rather than how to create it. 
AWS architecture diagrams typically include the following components: 
 
 Nodes ‚Äì AWS services and resources 
 Edges ‚Äì Connections between nodes showing relationships or data flow 
 Clusters ‚Äì Logical groupings of nodes, such as virtual private clouds (VPCs), subnets, and Availability Zones 
 Labels ‚Äì Text descriptions for nodes and connections 
 
Example 1: Create a web application architecture 
Let‚Äôs create a diagram for a simple web application hosted on AWS. Enter the following prompt: 
Create a diagram for a simple web application with an Application Load Balancer, two EC2 instances, and an RDS database. Check for AWS documentation to ensure it adheres to AWS best practices before you create the diagram 
 
 After you enter your prompt, Amazon Q CLI will search AWS documentation for best practices using the search_documentation tool from awslabsaws_documentation_mcp_server.  
  Following the search of the relevant AWS documentation, it will read the documentation using the read_documentation tool from the MCP server awslabsaws_documentation_mcp_server.  
 Amazon Q CLI will then list the needed AWS service icons using the list_icons tool, and will use generate_diagram with awslabsaws_diagram_mcp_server.  
 You should receive an output with a description of the diagram created based on the prompt along with the location of where the diagram was saved.  
 Amazon Q CLI will generate and display the diagram. 
  
 
The generated diagram shows the following key components: 
 
 An Application Load Balancer as the entry point 
 Two Amazon Elastic Compute Cloud (Amazon EC2) instances for the application tier 
 An Amazon Relational Database Service (Amazon RDS) instance for the database tier 
 Connections showing the flow of traffic 
 
Example 2: Create a multi-tier architecture 
Multi-tier architectures separate applications into functional layers (presentation, application, and data) to improve scalability and security. We use the following prompt to create our diagram: 
Create a diagram for a three-tier web application with a presentation tier (ALB and CloudFront), application tier (ECS with Fargate), and data tier (Aurora PostgreSQL). Include VPC with public and private subnets across multiple AZs. Check for AWS documentation to ensure it adheres to AWS best practices before you create the diagram. 
 
The diagram shows the following key components: 
 
 A presentation tier in public subnets 
 An application tier in private subnets 
 A data tier in isolated private subnets 
 Proper security group configurations 
 Traffic flow between tiers 
 
Example 3: Create a serverless architecture 
We use the following prompt to create a diagram for a serverless architecture: 
Create a diagram for a serverless web application using API Gateway, Lambda, DynamoDB, and S3 for static website hosting. Include Cognito for user authentication and CloudFront for content delivery. Check for AWS documentation to ensure it adheres to AWS best practices before you create the diagram. 
 
The diagram includes the following key components: 
 
 Amazon Simple Storage Service (Amazon S3) hosting static website content 
 Amazon CloudFront distributing content globally 
 Amazon API Gateway handling API requests 
 AWS Lambda functions implementing business logic 
 Amazon DynamoDB storing application data 
 Amazon Cognito managing user authentication 
 
Example 4: Create a data processing diagram 
We use the following prompt to create a diagram for a data processing pipeline: 
Create a diagram for a data processing pipeline with components organized in clusters for data ingestion, processing, storage, and analytics. Include Kinesis, Lambda, S3, Glue, and QuickSight. Check for AWS documentation to ensure it adheres to AWS best practices before you create the diagram. 
 
The diagram organizes components into distinct clusters: 
 
 Data ingestion ‚Äì Amazon Kinesis Data Streams, Amazon Data Firehose, Amazon Simple Queue Service 
 Data processing ‚Äì Lambda functions, AWS Glue jobs 
 Data storage ‚Äì S3 buckets, DynamoDB tables 
 Data analytics ‚Äì AWS Glue, Amazon Athena, Amazon QuickSight 
 
Real-world examples 
Let‚Äôs explore some real-world architecture patterns and how to create diagrams for them using Amazon Q CLI with the AWS Diagram MCP server. 
Ecommerce platform 
Ecommerce platforms require scalable, resilient architectures to handle variable traffic and maintain high availability. We use the following prompt to create an example diagram: 
Create a diagram for an e-commerce platform with microservices architecture. Include components for product catalog, shopping cart, checkout, payment processing, order management, and user authentication. Ensure the architecture follows AWS best practices for scalability and security. Check for AWS documentation to ensure it adheres to AWS best practices before you create the diagram. 
 
The diagram includes the following key components: 
 
 API Gateway as the entry point for client applications 
 Microservices implemented as containers in Amazon Elastic Container Service (Amazon ECS) with AWS Fargate 
 RDS databases for product catalog, shopping cart, and order data 
 Amazon ElastiCache for product data caching and session management 
 Amazon Cognito for authentication 
 Amazon Simple Queue Service (Amazon SQS) and Amazon Simple Notification Service (Amazon SNS) for asynchronous communication between services 
 CloudFront for content delivery and static assets from Amazon S3 
 Amazon Route 53 for DNS management 
 AWS WAF for web application security 
 AWS Lambda functions for serverless microservice implementation 
 AWS Secrets Manager for secure credential storage 
 Amazon CloudWatch for monitoring and observability 
 
Intelligent document processing solution 
We use the following prompt to create a diagram for an intelligent document processing (IDP) architecture: 
Create a diagram for an intelligent document processing (IDP) application on AWS. Include components for document ingestion, OCR and text extraction, intelligent data extraction (using NLP and/or computer vision), human review and validation, and data output/integration. Ensure the architecture follows AWS best practices for scalability and security, leveraging services like S3, Lambda, Textract, Comprehend, SageMaker (for custom models, if applicable), and potentially Augmented AI (A2I). Check for AWS documentation related to intelligent document processing best practices to ensure it adheres to AWS best practices before you create the diagram. 
 
The diagram includes the following key components: 
 
 Amazon API Gateway as the entry point for client applications, providing a secure and scalable interface 
 Microservices implemented as containers in ECS with Fargate, enabling flexible and scalable processing 
 Amazon RDS databases for product catalog, shopping cart, and order data, providing reliable structured data storage 
 Amazon ElastiCache for product data caching and session management, improving performance and user experience 
 Amazon Cognito for authentication, ensuring secure access control 
 Amazon Simple Queue Service and Amazon Simple Notification Service for asynchronous communication between services, enabling decoupled and resilient architecture 
 Amazon CloudFront for content delivery and static assets from S3, optimizing global performance 
 Amazon Route53 for DNS management, providing reliable routing 
 AWS WAF for web application security, protecting against common web exploits 
 AWS Lambda functions for serverless microservice implementation, offering cost-effective scaling 
 AWS Secrets Manager for secure credential storage, enhancing security posture 
 Amazon CloudWatch for monitoring and observability, providing insights into system performance and health. 
 
Clean up 
If you no longer need to use the AWS Cost Analysis MCP server with Amazon Q CLI, you can remove it from your configuration: 
 
 Open your ~/.aws/amazonq/mcp.json file. 
 Remove or comment out the MCP server entries. 
 Save the file. 
 
This will prevent the server from being loaded when you start Amazon Q CLI in the future. 
Conclusion 
In this post, we explored how to use Amazon Q CLI with the AWS Documentation MCP and AWS Diagram MCP servers to create professional AWS architecture diagrams that adhere to AWS best practices referenced from official AWS documentation. This approach offers significant advantages over traditional diagramming methods: 
 
 Time savings ‚Äì Generate complex diagrams in minutes instead of hours 
 Consistency ‚Äì Make sure diagrams follow the same style and conventions 
 Best practices ‚Äì Automatically incorporate AWS architectural guidelines 
 Iterative refinement ‚Äì Quickly modify diagrams through simple prompts 
 Validation ‚Äì Check architectures against official AWS documentation and recommendations 
 
As you continue your journey with AWS architecture diagrams, we encourage you to deepen your knowledge by learning more about the Model Context Protocol (MCP) to understand how it enhances the capabilities of Amazon Q. When seeking inspiration for your own designs, the AWS Architecture Center offers a wealth of reference architectures that follow best practices. For creating visually consistent diagrams, be sure to visit the AWS Icons page, where you can find the complete official icon set. And to stay at the cutting edge of these tools, keep an eye on updates to the official AWS MCP Servers‚Äîthey‚Äôre constantly evolving with new features to make your diagramming experience even better. 
 
About the Authors 
Joel Asante, an Austin-based Solutions Architect at Amazon Web Services (AWS), works with GovTech (Government Technology) customers. With a strong background in data science and application development, he brings deep technical expertise to creating secure and scalable cloud architectures for his customers. Joel is passionate about data analytics, machine learning, and robotics, leveraging his development experience to design innovative solutions that meet complex government requirements. He holds 13 AWS certifications and enjoys family time, fitness, and cheering for the Kansas City Chiefs and Los Angeles Lakers in his spare time. 
Dunieski Otano is a Solutions Architect at Amazon Web Services based out of Miami, Florida. He works with World Wide Public Sector MNO (Multi-International Organizations) customers. His passion is Security, Machine Learning and Artificial Intelligence, and Serverless. He works with his customers to help them build and deploy high available, scalable, and secure solutions. Dunieski holds 14 AWS certifications and is an AWS Golden Jacket recipient. In his free time, you will find him spending time with his family and dog, watching a great movie, coding, or flying his drone. 
Varun Jasti&nbsp;is a Solutions Architect at Amazon Web Services, working with AWS Partners to design and scale artificial intelligence solutions for public sector use cases to meet compliance standards. With a background in Computer Science, his work covers broad range of ML use cases primarily focusing on LLM training/inferencing and computer vision. In his spare time, he loves playing tennis and swimming.
‚Ä¢ AWS costs estimation using Amazon Q CLI and AWS Cost Analysis MCP
  Managing and optimizing AWS infrastructure costs is a critical challenge for organizations of all sizes. Traditional cost analysis approaches often involve the following: 
 
 Complex spreadsheets ‚Äì Creating and maintaining detailed cost models, which requires significant effort 
 Multiple tools ‚Äì Switching between the AWS Pricing Calculator, AWS Cost Explorer, and third-party tools 
 Specialized knowledge ‚Äì Understanding the nuances of AWS pricing across services and AWS Regions 
 Time-consuming analysis ‚Äì Manually comparing different deployment options and scenarios 
 Delayed optimization ‚Äì Cost insights often come too late to inform architectural decisions 
 
Amazon Q Developer CLI with the Model Context Protocol (MCP) offers a revolutionary approach to AWS cost analysis. By using generative AI through natural language prompts, teams can now generate detailed cost estimates, comparisons, and optimization recommendations in minutes rather than hours, while providing accuracy through integration with official AWS pricing data. 
In this post, we explore how to use Amazon Q CLI with the AWS Cost Analysis MCP server to perform sophisticated cost analysis that follows AWS best practices. We discuss basic setup and advanced techniques, with detailed examples and step-by-step instructions. 
Solution overview 
Amazon Q Developer CLI is a command line interface that brings the generative AI capabilities of Amazon Q directly to your terminal. Developers can interact with Amazon Q through natural language prompts, making it an invaluable tool for various development tasks. Developed by Anthropic as an open protocol, the Model Context Protocol (MCP) provides a standardized way to connect AI models to different data sources or tools. Using a client-server architecture (as illustrated in the following diagram), the MCP helps developers expose their data through lightweight MCP servers while building AI applications as MCP clients that connect to these servers. 
The MCP uses a client-server architecture containing the following components: 
 
 Host ‚Äì A program or AI tool that requires access to data through the MCP protocol, such as Anthropic‚Äôs Claude Desktop, an integrated development environment (IDE), or other AI applications 
 Client ‚Äì Protocol clients that maintain one-to-one connections with servers 
 Server ‚Äì Lightweight programs that expose capabilities through standardized MCP or act as tools 
 Data sources ‚Äì Local data sources such as databases and file systems, or external systems available over the internet through APIs (web APIs) that MCP servers can connect with 
 
 
As announced in April 2025, the MCP enables Amazon Q Developer to connect with specialized servers that extend its capabilities beyond what‚Äôs possible with the base model alone. MCP servers act as plugins for Amazon Q, providing domain-specific knowledge and functionality. The AWS Cost Analysis MCP server specifically enables Amazon Q to generate detailed cost estimates, reports, and optimization recommendations using real-time AWS pricing data. 
Prerequisites 
To implement this solution, you must have an AWS account with appropriate permissions and follow the steps below. 
Set up your environment 
Before you can start analyzing costs, you need to set up your environment with Amazon Q CLI and the AWS Cost Analysis MCP server. This section provides detailed instructions for installation and configuration. 
Install Amazon Q Developer CLI 
Amazon Q Developer CLI is available as a standalone installation. Complete the following steps to install it: 
 
 Download and install Amazon Q Developer CLI. For instructions, see Using Amazon Q Developer on the command line. 
 Verify the installation by running the following command: q --version You should see output similar to the following: Amazon Q Developer CLI version 1.x.x 
 Configure Amazon Q CLI with your AWS credentials: q login 
 Choose the login method suitable for you: 
   
   Use for free with AWS Builder ID 
   Use with Pro license 
    
 
Set up MCP servers 
Before using the AWS Cost Analysis MCP server with Amazon Q CLI, you must install several tools and configure your environment. The following steps guide you through installing the necessary tools and setting up the MCP server configuration: 
 
 Install Panoc using the following command (you can install with brew as well), converting the output to PDF: pip install pandoc 
 Install uv with the following command: pip install uv 
 Install Python 3.10 or newer: uv python install 3.10 
 Add the servers to your ~/.aws/amazonq/mcp.json file: {
  "mcpServers": {
    "awslabs.cost-analysis-mcp-server": {
      "command": "uvx",
      "args": ["awslabs.cost-analysis-mcp-server"],
      "env": {
        "FASTMCP_LOG_LEVEL": "ERROR"
      },
      "autoApprove": [],
      "disabled": false
    }
  }
}
 Now, Amazon Q CLI automatically discovers MCP servers in the ~/.aws/amazonq/mcp.json file. 
 
Understanding MCP server tools 
The AWS Cost Analysis MCP server provides several powerful tools: 
 
 get_pricing_from_web ‚Äì Retrieves pricing information from AWS pricing webpages 
 get_pricing_from_api ‚Äì Fetches pricing data from the AWS Price List API 
 generate_cost_report ‚Äì Creates detailed cost analysis reports with breakdowns and visualizations 
 analyze_cdk_project ‚Äì Analyzes AWS Cloud Development Kit (AWS CDK) projects to identify services used and estimate costs 
 analyze_terraform_project ‚Äì Analyzes Terraform projects to identify services used and estimate costs 
 get_bedrock_patterns ‚Äì Retrieves architecture patterns for Amazon Bedrock with cost considerations 
 
These tools work together to help you create accurate cost estimates that follow AWS best practices. 
Test your setup 
Let‚Äôs verify that everything is working correctly by generating a simple cost analysis: 
 
 Start the Amazon Q CLI chat interface and verify the output shows the MCP server being loaded and initialized: q chat 
 In the chat interface, enter the following prompt:Please create a cost analysis for a simple web application with an Application Load Balancer, two t3.medium EC2 instances, and an RDS db.t3.medium MySQL database. Assume 730 hours of usage per month and moderate traffic of about 100 GB data transfer. Convert estimation to a PDF format. 
 Amazon Q CLI will ask for permission to trust the tool that is being used; enter t to trust it. Amazon Q should generate and display a detailed cost analysis. Your output should look like the following screenshot.  If you see the cost analysis report, your environment is set up correctly. If you encounter issues, verify that Amazon Q CLI can access the MCP servers by making sure you installed install the necessary tools and the servers are in the ~/.aws/amazonq/mcp.json file. 
 
Configuration options 
The AWS Cost Analysis MCP server supports several configuration options to customize your cost analysis experience: 
 
 Output format ‚Äì Choose between markdown, CSV formats, or PDF (which we installed the package for) for cost reports 
 Pricing model ‚Äì Specify on-demand, reserved instances, or savings plans 
 Assumptions and exclusions ‚Äì Customize the assumptions and exclusions in your cost analysis 
 Detailed cost data ‚Äì Provide specific usage patterns for more accurate estimates 
 
Now that our environment is set up, let‚Äôs create more cost analyses. 
Create AWS Cost Analysis reports 
In this section, we walk through the process of creating AWS cost analysis reports using Amazon Q CLI with the AWS Cost Analysis MCP server. 
When you provide a prompt to Amazon Q CLI, the AWS Cost Analysis MCP server completes the following steps: 
 
 Interpret your requirements. 
 Retrieve pricing data from AWS pricing sources. 
 Generate a detailed cost analysis report. 
 Provide optimization recommendations. 
 
This process happens seamlessly, so you can focus on describing what you want rather than how to create it. 
AWS Cost Analysis reports typically include the following information: 
 
 Service costs ‚Äì Breakdown of costs by AWS service 
 Unit pricing ‚Äì Detailed unit pricing information 
 Usage quantities ‚Äì Estimated usage quantities for each service 
 Calculation details ‚Äì Step-by-step calculations showing how costs were derived 
 Assumptions ‚Äì Clearly stated assumptions used in the analysis 
 Exclusions ‚Äì Costs that were not included in the analysis 
 Recommendations ‚Äì Cost optimization suggestions 
 
Example 1: Analyze a serverless application 
Let‚Äôs create a cost analysis for a simple serverless application. Use the following prompt: 
Create a cost analysis for a serverless application using API Gateway, Lambda, and DynamoDB. Assume 1 million API calls per month, average Lambda execution time of 200ms with 512MB memory, and 10GB of DynamoDB storage with 5 million read requests and 1 million write requests per month. Convert estimation to a PDF format. 
Upon entering your prompt, Amazon Q CLI will retrieve pricing data using the get_pricing_from_web or get_pricing_from_api tools, and will use generate_cost_report with awslabscost_analysis_mcp_server. 
 
You should receive an output giving a detailed cost breakdown based on the prompt along with optimization recommendations. 
 
 
The generated cost analysis shows the following information: 
 
 Amazon API Gateway costs for 1 million requests 
 AWS Lambda costs for compute time and requests 
 Amazon DynamoDB costs for storage, read, and write capacity 
 Total monthly cost estimate 
 Cost optimization recommendations 
 
Example 2: Analyze multi-tier architectures 
Multi-tier architectures separate applications into functional layers (presentation, application, and data) to improve scalability and security. This example analyzes costs for implementing such an architecture on AWS with components for each tier: 
Create a cost analysis for a three-tier web application with a presentation tier (ALB and CloudFront), application tier (ECS with Fargate), and data tier (Aurora PostgreSQL). Include costs for 2 Fargate tasks with 1 vCPU and 2GB memory each, an Aurora db.r5.large instance with 100GB storage, an Application Load Balancer with 10 
This time, we are formatting it into both PDF and DOCX. 
 
 
The cost analysis shows the following information: 
 
 Presentation tier costs (Application Load Balancer and AWS CloudFront) 
 Application tier costs (Amazon Elastic Container Service (Amazon ECS) and AWS Fargate) 
 Data tier costs (Amazon Aurora PostgreSQL-Compatible Edition) 
 Detailed breakdown of each component‚Äôs pricing 
 Total monthly cost estimate 
 Cost optimization recommendations for each tier 
 
Example 3: Compare deployment options 
When deploying containers on AWS, choosing between Amazon ECS with Amazon Elastic Compute Cloud (Amazon EC2) or Fargate involves different cost structures and management overhead. This example compares these options to determine the most cost-effective solution for a specific workload: 
Compare the costs between running a containerized application on ECS with EC2 launch type versus Fargate launch type. Assume 4 containers each needing 1 vCPU and 2GB memory, running 24/7 for a month. For EC2, use t3.medium instances. Provide a recommendation on which option is more cost-effective for this workload. Convert estimation to a HTML webpage. 
This time, we are formatting it into a HTML webpage. 
 
 
The cost comparison includes the following information: 
 
 Amazon ECS with Amazon EC2 launch type costs 
 Amazon ECS with Fargate launch type costs 
 Detailed breakdown of each option‚Äôs pricing components 
 Side-by-side comparison of total costs 
 Recommendations for the most cost-effective option 
 Considerations for when each option might be preferred 
 
Real-world examples 
Let‚Äôs explore some real-world architecture patterns and how to analyze their costs using Amazon Q CLI with the AWS Cost Analysis MCP server. 
Ecommerce platform 
Ecommerce platforms require scalable, resilient architectures with careful cost management. These systems typically use microservices to handle various functions independently while maintaining high availability. This example analyzes costs for a complete ecommerce solution with multiple components serving moderate traffic levels: 
Create a cost analysis for an e-commerce platform with microservices architecture. Include components for product catalog, shopping cart, checkout, payment processing, order management, and user authentication. Assume moderate traffic of 500,000 monthly active users, 2 million page views per day, and 50,000 orders per month. Ensure the analysis follows AWS best practices for cost optimization. Convert estimation to a PDF format. 
 
The cost analysis includes the following key components: 
 
 Frontend delivery costs (Amazon Simple Storage Service (Amazon S3) and CloudFront) 
 API Gateway and Lambda costs for serverless components 
 Container costs for microservices (Amazon Elastic Kubernetes Service (Amazon EKS) and Amazon ECS) 
 Database costs (Amazon Relational Database Service (Amazon RDS) and DynamoDB) 
 Caching costs (Amazon ElastiCache) 
 Storage and data transfer costs 
 Monitoring and security costs 
 Total monthly cost estimate 
 Cost optimization recommendations for each component 
 Reserved instance and savings plan opportunities 
 
Data analytics platform 
Modern data analytics platforms need to efficiently ingest, store, process, and visualize large volumes of data while managing costs effectively. This example examines the AWS services and costs involved in building a complete analytics pipeline handling significant daily data volumes with multiple user access requirements: 
Create a cost analysis for a data analytics platform processing 500GB of new data daily. Include components for data ingestion (Kinesis), storage (S3), processing (EMR), and visualization (QuickSight). Assume 50 users accessing dashboards daily and data retention of 90 days. Ensure the analysis follows AWS best practices for cost optimization and includes recommendations for cost-effective scaling. Convert estimation to a HTML webpage. 
 
The cost analysis includes the following key components: 
 
 Data ingestion costs (Amazon Kinesis Data Streams and Amazon Data Firehose) 
 Storage costs (Amazon S3 with lifecycle policies) 
 Processing costs (Amazon EMR cluster) 
 Visualization costs (Amazon QuickSight) 
 Data transfer costs between services 
 Total monthly cost estimate 
 Cost optimization recommendations for each component 
 Scaling considerations and their cost implications 
 
Clean up 
If you no longer need to use the AWS Cost Analysis MCP server with Amazon Q CLI, you can remove it from your configuration: 
 
 Open your ~/.aws/amazonq/mcp.json file. 
 Remove or comment out the ‚Äúawslabs.cost-analysis-mcp-server‚Äù entry. 
 Save the file. 
 
This will prevent the server from being loaded when you start Amazon Q CLI in the future. 
Conclusion 
In this post, we explored how to use Amazon Q CLI with the AWS Cost Analysis MCP server to create detailed cost analyses that use accurate AWS pricing data. This approach offers significant advantages over traditional cost estimation methods: 
 
 Time savings ‚Äì Generate complex cost analyses in minutes instead of hours 
 Accuracy ‚Äì Make sure estimates use the latest AWS pricing information 
 Comprehensive ‚Äì Include relevant cost components and considerations 
 Actionable ‚Äì Receive specific optimization recommendations 
 Iterative ‚Äì Quickly compare different scenarios through simple prompts 
 Validation ‚Äì Check estimates against official AWS pricing 
 
As you continue exploring AWS cost analysis, we encourage you to deepen your knowledge by learning more about the Model Context Protocol (MCP) to understand how it enhances the capabilities of Amazon Q. For hands-on cost estimation, the AWS Pricing Calculator offers an interactive experience to model and compare different deployment scenarios. To make sure your architectures follow financial best practices, the AWS Well-Architected Framework Cost Optimization Pillar provides comprehensive guidance on building cost-efficient systems. And to stay at the cutting edge of these tools, keep an eye on updates to the official AWS MCP servers‚Äîthey‚Äôre constantly evolving with new features to make your cost analysis experience even more powerful and accurate. 
 
About the Authors 
Joel Asante, an Austin-based Solutions Architect at Amazon Web Services (AWS), works with GovTech (Government Technology) customers. With a strong background in data science and application development, he brings deep technical expertise to creating secure and scalable cloud architectures for his customers. Joel is passionate about data analytics, machine learning, and robotics, leveraging his development experience to design innovative solutions that meet complex government requirements. He holds 13 AWS certifications and enjoys family time, fitness, and cheering for the Kansas City Chiefs and Los Angeles Lakers in his spare time. 
Dunieski Otano is a Solutions Architect at Amazon Web Services based out of Miami, Florida. He works with World Wide Public Sector MNO (Multi-International Organizations) customers. His passion is Security, Machine Learning and Artificial Intelligence, and Serverless. He works with his customers to help them build and deploy high available, scalable, and secure solutions. Dunieski holds 14 AWS certifications and is an AWS Golden Jacket recipient. In his free time, you will find him spending time with his family and dog, watching a great movie, coding, or flying his drone. 
Varun Jasti&nbsp;is a Solutions Architect at Amazon Web Services, working with AWS Partners to design and scale artificial intelligence solutions for public sector use cases to meet compliance standards. With a background in Computer Science, his work covers broad range of ML use cases primarily focusing on LLM training/inferencing and computer vision. In his spare time, he loves playing tennis and swimming.
‚Ä¢ Tailor responsible AI with new safeguard tiers in Amazon Bedrock Guardrails
  Amazon Bedrock Guardrails provides configurable safeguards to help build trusted generative AI applications at scale. It provides organizations with integrated safety and privacy safeguards that work across multiple foundation models (FMs), including models available in Amazon Bedrock, as well as models hosted outside Amazon Bedrock from other model providers and cloud providers. With the standalone ApplyGuardrail API, Amazon Bedrock Guardrails offers a model-agnostic and scalable approach to implementing responsible AI policies for your generative AI applications. Guardrails currently offers six key safeguards: content filters, denied topics, word filters, sensitive information filters, contextual grounding checks, and Automated Reasoning checks (preview), to help prevent unwanted content and align AI interactions with your organization‚Äôs responsible AI policies. 
As organizations strive to implement responsible AI practices across diverse use cases, they face the challenge of balancing safety controls with varying performance and language requirements across different applications, making a one-size-fits-all approach ineffective. To address this, we‚Äôve introduced safeguard tiers for Amazon Bedrock Guardrails, so you can choose appropriate safeguards based on your specific needs. For instance, a financial services company can implement comprehensive, multi-language protection for customer-facing AI assistants while using more focused, lower-latency safeguards for internal analytics tools, making sure each application upholds responsible AI principles with the right level of protection without compromising performance or functionality. 
In this post, we introduce the new safeguard tiers available in Amazon Bedrock Guardrails, explain their benefits and use cases, and provide guidance on how to implement and evaluate them in your AI applications. 
Solution overview 
Until now, when using Amazon Bedrock Guardrails, you were provided with a single set of the safeguards associated to specific AWS Regions and a limited set of languages supported. The introduction of safeguard tiers in Amazon Bedrock Guardrails provides three key advantages for implementing AI safety controls: 
 
 A tier-based approach that gives you control over which guardrail implementations you want to use for content filters and denied topics, so you can select the appropriate protection level for each use case. We provide more details about this in the following sections. 
 Cross-Region Inference Support (CRIS) for Amazon Bedrock Guardrails, so you can use compute capacity across multiple Regions, achieving better scaling and availability for your guardrails. With this, your requests get automatically routed during guardrail policy evaluation to the optimal Region within your geography, maximizing available compute resources and model availability. This helps maintain guardrail performance and reliability when demand increases. There‚Äôs no additional cost for using CRIS with Amazon Bedrock Guardrails, and you can select from specific guardrail profiles for controlling model versioning and future upgrades. 
 Advanced capabilities as a configurable tier option for use cases where more robust protection or broader language support are critical priorities, and where you can accommodate a modest latency increase. 
 
Safeguard tiers are applied at the guardrail policy level, specifically for content filters and denied topics. You can tailor your protection strategy for different aspects of your AI application. Let‚Äôs explore the two available tiers: 
 
 Classic tier (default): 
   
   Maintains the existing behavior of Amazon Bedrock Guardrails 
   Limited language support: English, French, and Spanish 
   Does not require CRIS for Amazon Bedrock Guardrails 
   Optimized for lower-latency applications 
    
 Standard tier: 
   
   Provided as a new capability that you can enable for existing or new guardrails 
   Multilingual support for more than 60 languages 
   Enhanced robustness against prompt typos and manipulated inputs 
   Enhanced prompt attack protection covering modern jailbreak and prompt injection techniques, including token smuggling, AutoDAN, and many-shot, among others 
   Enhanced topic detection with improved understanding and handling of complex topics 
   Requires the use of CRIS for Amazon Bedrock Guardrails and might have a modest increase in latency profile compared to the Classic tier option 
    
 
You can select each tier independently for content filters and denied topics policies, allowing for mixed configurations within the same guardrail, as illustrated in the following hierarchy. With this flexibility, companies can implement the right level of protection for each specific application. 
 
 Policy: Content filters 
   
   Tier: Classic or Standard 
    
 Policy: Denied topics 
   
   Tier: Classic or Standard 
    
 Other policies: Word filters, sensitive information filters, contextual grounding checks, and Automated Reasoning checks (preview) 
 
To illustrate how these tiers can be applied, consider a global financial services company deploying AI in both customer-facing and internal applications: 
 
 For their customer service AI assistant, they might choose the Standard tier for both content filters and denied topics, to provide comprehensive protection across many languages. 
 For internal analytics tools, they could use the Classic tier for content filters prioritizing low latency, while implementing the Standard tier for denied topics to provide robust protection against sensitive financial information disclosure. 
 
You can configure the safeguard tiers for content filters and denied topics in each guardrail through the AWS Management Console, or programmatically through the Amazon Bedrock SDK and APIs. You can use a new or existing guardrail. For information on how to create or modify a guardrail, see Create your guardrail. 
Your existing guardrails are automatically set to the Classic tier by default to make sure you have no impact on your guardrails‚Äô behavior. 
Quality enhancements with the Standard tier 
According to our tests, the new Standard tier improves harmful content filtering recall by more than 15% with a more than 7% gain in balanced accuracy compared to the Classic tier. A key differentiating feature of the new Standard tier is its multilingual support, maintaining strong performance with over 78% recall and over 88% balanced accuracy for the most common 14 languages.The enhancements in protective capabilities extend across several other aspects. For example, content filters for prompt attacks in the Standard tier show a 30% improvement in recall and 16% gain in balanced accuracy compared to the Classic tier, while maintaining a lower false positive rate. For denied topic detection, the new Standard tier delivers a 32% increase in recall, resulting in an 18% improvement in balanced accuracy.These substantial evolutions in detection capabilities for Amazon Bedrock Guardrails, combined with consistently low false positive rates and robust multilingual performance, also represent a significant advancement in content protection technology compared to other commonly available solutions. The multilingual improvements are particularly noteworthy, with the new Standard tier in Amazon Bedrock Guardrails showing consistent performance gains of 33‚Äì49% in recall across different language evaluations compared to other competitors‚Äô options. 
Benefits of safeguard tiers 
Different AI applications have distinct safety requirements based on their audience, content domain, and geographic reach. For example: 
 
 Customer-facing applications often require stronger protection against potential misuse compared to internal applications 
 Applications serving global customers need guardrails that work effectively across many languages 
 Internal enterprise tools might prioritize controlling specific topics in just a few primary languages 
 
The combination of the safeguard tiers with CRIS for Amazon Bedrock Guardrails also addresses various operational needs with practical benefits that go beyond feature differences: 
 
 Independent policy evolution ‚Äì Each policy (content filters or denied topics) can evolve at its own pace without disrupting the entire guardrail system. You can configure these with specific guardrail profiles in CRIS for controlling model versioning in the models powering your guardrail policies. 
 Controlled adoption ‚Äì You decide when and how to adopt new capabilities, maintaining stability for production applications. You can continue to use Amazon Bedrock Guardrails with your previous configurations without changes and only move to the new tiers and CRIS configurations when you consider it appropriate. 
 Resource efficiency ‚Äì You can implement enhanced protections only where needed, balancing security requirements with performance considerations. 
 Simplified migration path ‚Äì When new capabilities become available, you can evaluate and integrate them gradually by policy area rather than facing all-or-nothing choices. This also simplifies testing and comparison mechanisms such as A/B testing or blue/green deployments for your guardrails. 
 
This approach helps organizations balance their specific protection requirements with operational considerations in a more nuanced way than a single-option system could provide. 
Configure safeguard tiers on the Amazon Bedrock console 
On the Amazon Bedrock console, you can configure the safeguard tiers for your guardrail in the Content filters tier or Denied topics tier sections by selecting your preferred tier. 
 
 
Use of the new Standard tier requires setting up cross-Region inference for Amazon Bedrock Guardrails, choosing the guardrail profile of your choice. 
 
Configure safeguard tiers using the AWS SDK 
You can also configure the guardrail‚Äôs tiers using the AWS SDK. The following is an example to get started with the Python SDK: 
 
 import boto3
import json

bedrock = boto3.client(
    "bedrock",
    region_name="us-east-1"
)

# Create a guardrail with Standard tier for both Content Filters and Denied Topics
response = bedrock.create_guardrail(
    name="enhanced-safety-guardrail",
    # cross-Region is required for STANDARD tier
    crossRegionConfig={
        'guardrailProfileIdentifier': 'us.guardrail.v1:0'
    },
    # Configure Denied Topics with Standard tier
    topicPolicyConfig={
        "topicsConfig": [
            {
                "name": "Financial Advice",
                "definition": "Providing specific investment advice or financial recommendations",
                "type": "DENY",
                "inputEnabled": True,
                "inputAction": "BLOCK",
                "outputEnabled": True,
                "outputAction": "BLOCK"
            }
        ],
        "tierConfig": {
            "tierName": "STANDARD"
        }
    },
    # Configure Content Filters with Standard tier
    contentPolicyConfig={
        "filtersConfig": [
            {
                "inputStrength": "HIGH",
                "outputStrength": "HIGH",
                "type": "SEXUAL"
            },
            {
                "inputStrength": "HIGH",
                "outputStrength": "HIGH",
                "type": "VIOLENCE"
            }
        ],
        "tierConfig": {
            "tierName": "STANDARD"
        }
    },
    blockedInputMessaging="I cannot respond to that request.",
    blockedOutputsMessaging="I cannot provide that information."
) 
 
Within a given guardrail, the content filter and denied topic policies can be configured with its own tier independently, giving you granular control over how guardrails behave. For example, you might choose the Standard tier for content filtering while keeping denied topics in the Classic tier, based on your specific requirements. 
For migrating existing guardrails‚Äô configurations to use the Standard tier, add the sections highlighted in the preceding example for crossRegionConfig and tierConfig to your current guardrail definition. You can do this using the UpdateGuardrail API, or create a new guardrail with the CreateGuardrail API. 
Evaluating your guardrails 
To thoroughly evaluate your guardrails‚Äô performance, consider creating a test dataset that includes the following: 
 
 Safe examples ‚Äì Content that should pass through guardrails 
 Harmful examples ‚Äì Content that should be blocked 
 Edge cases ‚Äì Content that tests the boundaries of your policies 
 Examples in multiple languages ‚Äì Especially important when using the Standard tier 
 
You can also rely on openly available datasets for this purpose. Ideally, your dataset should be labeled with the expected response for each case for assessing accuracy and recall of your guardrails. 
With your dataset ready, you can use the Amazon Bedrock ApplyGuardrail API as shown in the following example to efficiently test your guardrail‚Äôs behavior for user inputs without invoking FMs. This way, you can save the costs associated with the large language model (LLM) response generation. 
 
 import boto3
import json

bedrock_runtime = boto3.client(
    "bedrock-runtime",
    region_name="us-east-1"
)

# Test the guardrail with potentially problematic content
content = [
    {
        "text": {
            "text": "Your test prompt here"
        }
    }
]

response = bedrock_runtime.apply_guardrail(
    content=content,
    source="INPUT",
    guardrailIdentifier="your-guardrail-id",
    guardrailVersion="DRAFT"
)

print(json.dumps(response, indent=2, default=str)) 
 
Later, you can repeat the process for the outputs of the LLMs if needed. For this, you can use the ApplyGuardrail API if you want an independent evaluation for models in AWS or outside in another provider, or you can directly use the Converse API if you intend to use models in Amazon Bedrock. When using the Converse API, the inputs and outputs are evaluated with the same invocation request, optimizing latency and reducing coding overheads. 
Because your dataset is labeled, you can directly implement a mechanism for assessing the accuracy, recall, and potential false negatives or false positives through the use of libraries like SKLearn Metrics: 
 
 # scoring script
# labels and preds store list of ground truth label and guardrails predictions

from sklearn.metrics import confusion_matrix

tn, fp, fn, tp = confusion_matrix(labels, preds, labels=[0, 1]).ravel()

recall = tp / (tp + fn) if (tp + fn) != 0 else 0
fpr = fp / (fp + tn) if (fp + tn) != 0 else 0
balanced_accuracy = 0.5 * (recall + 1 - fpr) 
 
Alternatively, if you don‚Äôt have labeled data or your use cases have subjective responses, you can also rely on mechanisms such as LLM-as-a-judge, where you pass the inputs and guardrails‚Äô evaluation outputs to an LLM for assessing a score based on your own predefined criteria. For more information, see Automate building guardrails for Amazon Bedrock using test-drive development. 
Best practices for implementing tiers 
We recommend considering the following aspects when configuring your tiers for Amazon Bedrock Guardrails: 
 
 Start with staged testing ‚Äì Test both tiers with a representative sample of your expected inputs and responses before making broad deployment decisions. 
 Consider your language requirements ‚Äì If your application serves users in multiple languages, the Standard tier‚Äôs expanded language support might be essential. 
 Balance safety and performance ‚Äì Evaluate both the accuracy improvements and latency differences to make informed decisions. Consider if you can afford a few additional milliseconds of latency for improved robustness with the Standard tier or prefer a latency-optimized option for more straight forward evaluations with the Classic tier. 
 Use policy-level tier selection ‚Äì Take advantage of the ability to select different tiers for different policies to optimize your guardrails. You can choose separate tiers for content filters and denied topics, while combining with the rest of the policies and features available in Amazon Bedrock Guardrails. 
 Remember cross-Region requirements ‚Äì The Standard tier requires cross-Region inference, so make sure your architecture and compliance requirements can accommodate this. With CRIS, your request originates from the Region where your guardrail is deployed, but it might be served from a different Region from the ones included in the guardrail inference profile for optimizing latency and availability. 
 
Conclusion 
The introduction of safeguard tiers in Amazon Bedrock Guardrails represents a significant step forward in our commitment to responsible AI. By providing flexible, powerful, and evolving safety tools for generative AI applications, we‚Äôre empowering organizations to implement AI solutions that are not only innovative but also ethical and trustworthy. This capabilities-based approach enables you to tailor your responsible AI practices to each specific use case. You can now implement the right level of protection for different applications while creating a path for continuous improvement in AI safety and ethics.The new Standard tier delivers significant improvements in multilingual support and detection accuracy, making it an ideal choice for many applications, especially those serving diverse global audiences or requiring enhanced protection. This aligns with responsible AI principles by making sure AI systems are fair and inclusive across different languages and cultures. Meanwhile, the Classic tier remains available for use cases prioritizing low latency or those with simpler language requirements, allowing organizations to balance performance with protection as needed. 
By offering these customizable protection levels, we‚Äôre supporting organizations in their journey to develop and deploy AI responsibly. This approach helps make sure that AI applications are not only powerful and efficient but also align with organizational values, comply with regulations, and maintain user trust. 
To learn more about safeguard tiers in Amazon Bedrock Guardrails, refer to Detect and filter harmful content by using Amazon Bedrock Guardrails, or visit the Amazon Bedrock console to create your first tiered guardrail. 
 
About the Authors 
 Koushik Kethamakka is a Senior Software Engineer at AWS, focusing on AI/ML initiatives. At Amazon, he led real-time ML fraud prevention systems for Amazon.com before moving to AWS to lead development of AI/ML services like Amazon Lex and Amazon Bedrock. His expertise spans product and system design, LLM hosting, evaluations, and fine-tuning. Recently, Koushik‚Äôs focus has been on LLM evaluations and safety, leading to the development of products like Amazon Bedrock Evaluations and Amazon Bedrock Guardrails. Prior to joining Amazon, Koushik earned his MS from the University of Houston. 
Hang Su is a Senior Applied Scientist at AWS AI. He has been leading the Amazon Bedrock Guardrails Science team. His interest lies in AI safety topics, including harmful content detection, red-teaming, sensitive information detection, among others. 
Shyam Srinivasan is on the Amazon Bedrock product team. He cares about making the world a better place through technology and loves being part of this journey. In his spare time, Shyam likes to run long distances, travel around the world, and experience new cultures with family and friends. 
Aartika Sardana Chandras is a Senior Product Marketing Manager for AWS Generative AI solutions, with a focus on Amazon Bedrock. She brings over 15 years of experience in product marketing, and is dedicated to empowering customers to navigate the complexities of the AI lifecycle. Aartika is passionate about helping customers leverage powerful AI technologies in an ethical and impactful manner. 
Satveer Khurpa is a Sr. WW Specialist Solutions Architect, Amazon Bedrock at Amazon Web Services, specializing in Amazon Bedrock security. In this role, he uses his expertise in cloud-based architectures to develop innovative generative AI solutions for clients across diverse industries. Satveer‚Äôs deep understanding of generative AI technologies and security principles allows him to design scalable, secure, and responsible applications that unlock new business opportunities and drive tangible value while maintaining robust security postures. 
Antonio Rodriguez is a Principal Generative AI Specialist Solutions Architect at Amazon Web Services. He helps companies of all sizes solve their challenges, embrace innovation, and create new business opportunities with Amazon Bedrock. Apart from work, he loves to spend time with his family and play sports with his friends.
‚Ä¢ Structured data response with Amazon Bedrock: Prompt Engineering and Tool Use
  Generative AI is revolutionizing industries by streamlining operations and enabling innovation. While textual chat interactions with GenAI remain popular, real-world applications often depend on structured data for APIs, databases, data-driven workloads, and rich user interfaces. Structured data can also enhance conversational AI, enabling more reliable and actionable outputs. A key challenge is that LLMs (Large Language Models) are inherently unpredictable, which makes it difficult for them to produce consistently structured outputs like JSON. This challenge arises because their training data mainly includes unstructured text, such as articles, books, and websites, with relatively few examples of structured formats. As a result, LLMs can struggle with precision when generating JSON outputs, which is crucial for seamless integration into existing APIs and databases. Models vary in their ability to support structured responses, including recognizing data types and managing complex hierarchies effectively. These capabilities can make a difference when choosing the right model. 
This blog demonstrates how Amazon Bedrock, a managed service for securely accessing top AI models, can help address these challenges by showcasing two alternative options: 
 
 Prompt Engineering:&nbsp;A straightforward approach to shaping structured outputs using well-crafted prompts. 
 Tool Use with the Bedrock Converse API:&nbsp;An advanced method that enables better control, consistency, and native JSON schema integration. 
 
We will use a customer review analysis example to demonstrate how Bedrock generates structured outputs, such as sentiment scores, with simplified Python code. 
Building a prompt engineering solution 
This section will demonstrate how to use prompt engineering effectively to generate structured outputs using Amazon Bedrock. Prompt engineering involves crafting precise input prompts to guide large language models (LLMs) in producing consistent and structured responses. It is a fundamental technique for developing Generative AI applications, particularly when structured outputs are required.Here are the five key steps we will follow: 
 
 Configure the Bedrock client and runtime parameters. 
 Create a JSON schema for structured outputs. 
 Craft a prompt and guide the model with clear instructions and examples. 
 Add a customer review as input data to analyse. 
 Invoke Bedrock, call the model, and process the response. 
 
While we demonstrate customer review analysis to generate a JSON output, these methods can also be used with other formats like XML or CSV. 
Step 1: Configure Bedrock 
To begin, we‚Äôll set up some constants and initialize a Python Bedrock client connection object using the Python Boto3 SDK for Bedrock runtime, which facilitates interaction with Bedrock: 
 
The REGION specifies the AWS region for model execution, while the MODEL_ID identifies the specific Bedrock model. The TEMPERATURE constant controls the output randomness, where higher values increase creativity, and lower values maintain precision, such as when generating structured output. MAX_TOKENS determines the output length, balancing cost-efficiency and data completeness. 
Step 2: Define the Schema 
Defining a schema is essential for facilitating structured and predictable model outputs, maintaining data integrity, and enabling seamless API integration. Without a well-defined schema, models may generate inconsistent or incomplete responses, leading to errors in downstream applications. The JSON standard schema used in the code below serves as a blueprint for structured data generation, guiding the model on how to format its output with explicit instructions. 
Let‚Äôs create a JSON schema for customer reviews with three required fields:&nbsp;reviewId&nbsp;(string, max 50 chars),&nbsp;sentiment (number, -1 to 1), and&nbsp;summary&nbsp;(string, max 200 chars). 
 
Step 3: Craft the Prompt text 
To generate consistent, structured, and accurate responses, prompts must be clear and well-structured, as LLMs rely on precise input to produce reliable outputs. Poorly designed prompts can lead to ambiguity, errors, or formatting issues, disrupting structured workflows, so we follow these best practices: 
 
 Clearly outline the AI‚Äôs role and objectives to avoid ambiguity. 
 Divide tasks into smaller, manageable numbered steps for clarity. 
 Indicate that a JSON schema will be provided (see Step 5 below) to maintain a consistent and valid structure. 
 Use one-shot prompting with a sample output to guide the model; add more examples if needed for consistency, but avoid too many, as they may limit the model‚Äôs ability to handle new inputs. 
 Define how to handle missing or invalid data. 
 
 
Step 4: Integrate Input Data 
For demonstration purposes, we‚Äôll include a review text in the prompt as a Python variable: 
 
Separating the input data with &lt;input&gt; tags improve readability and clarity, making it straightforward to identify and reference. This hardcoded input simulates real-world data integration. For production use, you might dynamically populate input data from APIs or user submissions. 
Step 5: Call Bedrock 
In this section, we construct a Bedrock request by defining a body object that includes the JSON schema, prompt, and input review data from previous steps. This structured request makes sure the model receives clear instructions, adheres to a predefined schema, and processes sample input data correctly. Once the request is prepared, we invoke Amazon Bedrock to generate a structured JSON response. 
 
We reuse the&nbsp;MAX_TOKENS,&nbsp;TEMPERATURE, and&nbsp;MODEL_ID constants defined in Step 1. The&nbsp;body&nbsp;object has essential inference configurations like&nbsp;anthropic_version&nbsp;for model compatibility and the&nbsp;messages&nbsp;array, which includes a single message to provide the model with task instructions, the schema, and the input data. The role defines the ‚Äúspeaker‚Äù in the interaction context, with user value representing the program sending the request. Alternatively, we could simplify the input by combining instructions, schema, and data into one text prompt, which is straightforward to manage but less modular. 
Finally, we use the&nbsp;client.invoke_model&nbsp;method to send the request. After invoking, the model processes the request, and the JSON data must be properly (not explained here) extracted from the Bedrock response. For example: 
 
Tool Use with the Amazon Bedrock Converse API 
In the previous chapter, we explored a solution using Bedrock Prompt Engineering. Now, let‚Äôs look at an alternative approach for generating structured responses with Bedrock. 
We will extend the previous solution by using the Amazon Bedrock Converse API, a consistent interface designed to facilitate multi-turn conversations with Generative AI models. The API abstracts model-specific configurations, including inference parameters, simplifying integration. 
A key feature of the Converse API is Tool Use (also known as Function Calling), which enables the model to execute external tools, such as calling an external API. This method supports standard JSON schema integration directly into tool definitions, facilitating output alignment with predefined formats. Not all Bedrock models support Tool Use, so make sure you check which models are compatible with these feature. 
Building on the previously defined data, the following code provides a straightforward example of Tool Use tailored to our curstomer review use case: 
 
In this code the&nbsp;tool_list&nbsp;defines a custom customer review analysis tool with its input schema and purpose, while the&nbsp;messages&nbsp;provide the earlier defined instructions and input data. Unlike in the previous prompt engineering example we used the earlier defined JSON schema in the definition of a tool. Finally, the&nbsp;client.converse&nbsp;call combines these components, specifying the tool to use and inference configurations, resulting in outputs tailored to the given schema and task. After exploring&nbsp;Prompt Engineering&nbsp;and&nbsp;Tool Use&nbsp;in Bedrock solutions for structured response generation, let‚Äôs now evaluate how different foundation models perform across these approaches. 
Test Results: Claude Models on Amazon Bedrock 
Understanding the capabilities of foundation models in structured response generation is essential for maintaining reliability, optimizing performance, and building scalable, future-proof Generative AI applications with Amazon Bedrock. To evaluate how well models handle structured outputs, we conducted extensive testing of Anthropic‚Äôs Claude models, comparing&nbsp;prompt-based&nbsp;and&nbsp;tool-based&nbsp;approaches across 1,000 iterations per model. Each iteration processed 100 randomly generated items, providing broad test coverage across different input variations.The examples shown earlier in this blog are intentionally simplified for demonstration purposes, where Bedrock performed seamlessly with no issues. To better assess the models under real-world challenges, we used a more complex schema that featured nested structures, arrays, and diverse data types to identify edge cases and potential issues. The outputs were validated for adherence to the JSON format and schema, maintaining consistency and accuracy. The following diagram summarizes the results, showing the number of successful, valid JSON responses for each model across the two demonstrated approaches: Prompt Engineering and Tool Use. 
 
The results demonstrated that all models achieved over 93% success across both approaches, with Tool Use methods consistently outperforming prompt-based ones. While the evaluation was conducted using a highly complex JSON schema, simpler schemas result in significantly fewer issues, often nearly none. Future updates to the models are expected to further enhance performance. 
Final Thoughts 
In conclusion, we demonstrated two methods for generating structured responses with Amazon Bedrock: Prompt Engineering and Tool Use with the Converse API. Prompt Engineering is flexible, works with Bedrock models (including those without Tool Use support), and handles various schema types (e.g., Open API schemas), making it a great starting point. However, it can be fragile, requiring exact prompts and struggling with complex needs. On the other hand, Tool Use offers greater reliability, consistent results, seamless API integration, and runtime validation of JSON schema for enhanced control. 
For simplicity, we did not demonstrate a few areas in this blog. Other techniques for generating structured responses include using models with built-in support for configurable response formats, such as JSON, when invoking models, or leveraging constraint decoding techniques with third-party libraries like LMQL. Additionally, generating structured data with GenAI can be challenging due to issues like invalid JSON, missing fields, or formatting errors. To maintain data integrity and handle unexpected outputs or API failures, effective error handling, thorough testing, and validation are essential. 
To try the Bedrock techniques demonstrated in this blog, follow the steps to Run example Amazon Bedrock API requests through the AWS SDK for Python (Boto3). With pay-as-you-go pricing, you‚Äôre only charged for API calls, so little to no cleanup is required after testing. For more details on best practices, refer to the Bedrock prompt engineering guidelines and model-specific documentation, such as Anthropic‚Äôs best practices. 
Structured data is key to leveraging Generative AI in real-world scenarios like APIs, data-driven workloads, and rich user interfaces beyond text-based chat. Start using Amazon Bedrock today to unlock its potential for reliable structured responses. 
 
About the authors 
Adam Nemeth is a Senior Solutions Architect at AWS, where he helps global financial customers embrace cloud computing through architectural guidance and technical support. With over 24 years of IT expertise, Adam previously worked at UBS before joining AWS. He lives in Switzerland with his wife and their three children. 
Dominic Searle is a Senior Solutions Architect at Amazon Web Services, where he has had the pleasure of working with Global Financial Services customers as they explore how Generative AI can be integrated into their technology strategies. Providing technical guidance, he enjoys helping customers effectively leverage AWS Services to solve real business problems.
‚Ä¢ Using Amazon SageMaker AI Random Cut Forest for NASA‚Äôs Blue Origin spacecraft sensor data
  The successful deorbit, descent, and landing of spacecraft on the Moon requires precise control and monitoring of vehicle dynamics. Anomaly detection provides a unique utility for identifying important states that might represent vehicle behaviors of interest. By producing unique vehicle behavior points, critical spacecraft system states can be identified to be more appropriately addressed and potentially better understood. These identified states can be invaluable for efforts such as system failure mitigation, engineering design improvements, and mission planning. Today, space missions have become more frequent and complex, and the volume of telemetry data generated has grown exponentially. With this growth, methods of analyzing this data for anomalies need to effectively scale and without risking missing subtle, but important deviations in spacecraft behavior. Fortunately, AWS uses powerful AI/ML applications within Amazon SageMaker AI that can address these needs. 
In this post, we demonstrate how to use SageMaker AI to apply the Random Cut Forest (RCF) algorithm to detect anomalies in spacecraft position, velocity, and quaternion orientation data from NASA and Blue Origin‚Äôs demonstration of lunar Deorbit, Descent, and Landing Sensors (BODDL-TP). The presented analysis focuses on detecting anomalies in spacecraft dynamics data, including positions, velocities, and quaternion orientations. 
Solution overview 
This solution provides an effective approach to anomaly detection in spacecraft data. We begin with data preprocessing and cleaning to produce quality input for our analysis. Using SageMaker AI, we train an RCF model specifically for detecting anomalies in complex spacecraft dynamics data. To handle the substantial volume of telemetry data efficiently, we implement batch processing for anomaly detection across large datasets. 
After the model is trained and anomalies are detected, this solution produces robust visualization capabilities, presenting results with highlighted anomalies for clear interpretation of the findings. We use Amazon Simple Storage Service (Amazon S3) for seamless data storage and retrieval, including both raw data and generated plots. Throughout the implementation, we maintain careful cost management of SageMaker AI instances by deleting resources after they‚Äôre used to achieve efficient utilization while maintaining performance. 
This combination of features creates a scalable, efficient pipeline for processing and analyzing spacecraft dynamics data, making it particularly suitable for space mission applications where reliability and precision are crucial. 
Key concepts 
In this section, we discuss some key concepts of spacecraft dynamics and machine learning (ML) in this solution. 
Position and velocity in spacecraft dynamics 
Position and velocity vectors in our NASA Blue Origin DDL data are represented in the Earth-Centered Earth-Fixed (ECEF) coordinate system. This reference frame rotates with the Earth, making it ideal for tracking spacecraft relative to landing sites on the lunar surface. The position vector [x, y, z] in ECEF pinpoints the spacecraft‚Äôs location in three-dimensional space. Its origin is at Earth‚Äôs center, with the X-axis intersecting the prime meridian at the equator, the Y-axis 90 degrees east in the equatorial plane, and the Z-axis aligned with Earth‚Äôs rotational axis. Measured in meters, this position data can reveal crucial information about orbital descent trajectories, landing approach paths, terminal descent profiles, and final touchdown positioning. Complementing position data, the velocity vector [vx, vy, vz] represents the spacecraft‚Äôs rate of position change in each direction. Measured in meters per second, this velocity data is vital for monitoring descent rates, maintaining safe approach speeds, controlling deceleration profiles, and verifying landing constraints. Our RCF algorithm scrutinizes both position and velocity data for anomalies. In position data, it looks for anomalies that might be caused by unexpected trajectory deviations, unrealistic position jumps, sensor glitches, or data recording errors. For velocity, its detected anomalies might be due to sudden speed changes, unusual acceleration patterns, potential thruster misfires, or navigation system issues. The fusion of position and velocity data offers a comprehensive view of the spacecraft‚Äôs translational motion. When combined with quaternion data describing rotational state, we obtain a complete picture of the spacecraft‚Äôs dynamic state during critical mission phases. These metrics play essential roles in mission planning, real-time monitoring, post-flight analysis, safety verification, C2 (command and control), and overall system performance evaluation. By using these rich datasets and advanced anomaly detection techniques, we enhance our ability to achieve mission success and spacecraft safety throughout the dynamic phases of lunar deorbit, descent, and landing. 
Quaternions in spacecraft dynamics 
Quaternions play a crucial role in spacecraft attitude (orientation) representation. Although Euler angles (roll, pitch, and yaw) are more intuitive, they can suffer from gimbal lock‚Äîa loss of one degree of freedom in certain orientations. Quaternions solve this problem by using a four-parameter representation that avoids such singularities. This representation consists of one scalar component (q0) and three vector components (q1, q2, q3), providing a robust mathematical framework for describing spacecraft orientation. In our NASA Blue Origin DDL data, quaternions serve a vital purpose: they represent the rotation from the spacecraft‚Äôs body-fixed coordinate system (CON) to the ECEF frame. This transformation is fundamental to several critical aspects of spacecraft operation, including maintaining precise attitude control during descent, preserving correct thrust vector orientation, facilitating accurate sensor measurements, and computing landing trajectories. For reliable anomaly detection, quaternion values must satisfy two essential mathematical properties. First, they must maintain unit magnitude, meaning the sum of their squared components (q0¬≤ + q1¬≤ + q2¬≤ + q3¬≤ = 1) equals one. Second, they must demonstrate continuity, avoiding sudden jumps that would indicate physically impossible rotations. These properties help confirm the validity of our orientation measurements and the effectiveness of our anomaly detection system. When our RCF algorithm identifies anomalies in quaternion data, these could signal various issues requiring attention. Such anomalies might indicate sensor malfunctions, attitude control system issues, data transmission errors, or actual problems with spacecraft orientation. By carefully monitoring these quaternion components alongside position and velocity data, we develop a comprehensive understanding of the spacecraft‚Äôs dynamic state during the critical phases of deorbit, descent, and landing. 
The Random Cut Forest algorithm 
Random Cut Forest is an unsupervised algorithm for detecting anomalies in high-dimensional data. The algorithm‚Äôs construction begins by creating multiple decision trees, each built through a process of repeatedly cutting the data space with random hyperplanes. This partitioning continues until each data point is isolated, creating a forest of trees that captures the underlying structure of the data. The novelty of RCF lies in the scoring mechanism. Points located in sparse regions of the data space that require fewer cuts to isolate score higher, while points in dense regions that need more cuts score lower. This fundamental principle allows the algorithm to assign anomaly scores inversely proportional to the number of cuts needed to isolate each point. Higher scores, therefore, indicate potential anomalies, making it straightforward to identify unusual patterns in the data. 
In our spacecraft dynamics context, we apply RCF to 10-dimensional vectors that combine position (three dimensions), velocity (three dimensions), and quaternion orientation (four dimensions). Each vector represents a specific moment in time during the spacecraft‚Äôs mission states. The flight patterns create dense regions in this high-dimensional space, while anomalies appear as isolated points in sparse regions. This data is high-dimensional, multivariate time series, and has no labels, which RCF handles fairly well while maintaining computational efficiency and handling sensor noise. For this use case, RCF is able to detect subtle deviations between data points of spacecraft dynamics while handling the complex relationships between position, velocity, and orientation parameters. These features of RCF make it an effective tool for spacecraft dynamics monitoring analysis and anomaly detection. 
Solution architecture 
The solution architecture implements anomaly detection for NASA-Blue Origin Lunar DDL data using the RCF algorithm, as illustrated in the following diagram. 
 
Our solution‚Äôs data flow begins with public DDL (Deorbit, Descent, and Landing) data securely stored in an S3 bucket. This data is then accessed through a SageMaker AI domain using JupyterLab, providing a powerful and flexible environment for data scientists and engineers. Within JupyterLab, we use a custom notebook to process the raw data and implement our anomaly detection algorithms. 
The core of our solution lies in the processing pipeline. It starts in the JupyterLab notebook, where we train an RCF model using SageMaker AI. After it‚Äôs trained, this model is deployed to a SageMaker AI endpoint, creating a scalable and responsive anomaly detection service. We then feed our spacecraft dynamics data through this model to identify potential anomalies. The pipeline concludes by generating detailed visualizations of these anomalies, providing clear and actionable insights. 
For output, our system saves both the detected anomaly data and the generated plots back to Amazon S3. This makes sure the results are securely stored and accessible for further analysis or reporting. Additionally, we preserve all training data and model outputs in Amazon S3, enabling reproducibility and facilitating iterative improvements to our anomaly detection process. Throughout these operations, we maintain robust security measures, using Amazon Virtual Private Cloud (Amazon VPC) to enforce data privacy and integrity at every step of the process. 
Prerequisites 
Before standing up the project, you must set up the necessary tools and access rights: 
 
 The AWS environment should include an active AWS account with appropriate permissions for running ML workloads, along with the AWS Command Line Interface (AWS CLI) for command line operations installed 
 Access to SageMaker AI is essential for the ML implementation 
 On the development side, Python 3.7 or later needs to be installed, along with several key Python packages: 
   
   Boto3 for AWS service integration 
   Pandas for data manipulation 
   Matplotlib for visualization 
   NumPy for numerical operations 
   The SageMaker AI Python SDK for interacting with the SageMaker services 
    
 
Set up the solution 
The setup process includes accessing the SageMaker AI environment, where all the data analysis and model training is executed. 
 
 On the SageMaker AI console, open the SageMaker domain details page. 
 Open JupyterLab, then create a new Python notebook instance for this project. 
 When the environment is ready, open a terminal in SageMaker AI JupyterLab to clone the project repository using the following commands: 
 
 
 git clone https://github.com/aws-samples/sample-SageMaker-ai-rcf-anomaly-detection-lunar-spacecraft.git
cd sample-SageMaker-ai-rcf-anomaly-detection-lunar-spacecraft 
 
 
 Install the required Python libraries: 
 
pip install -r requirements.txt 
This process will set up the necessary dependencies for running anomaly detection analysis on the spacecraft data. 
Execute anomaly detection 
Update the bucket_name and file_name variables in the script with your S3 bucket and data file names. 
Run the script in JupyterLab as a Jupyter notebook or run as a Python script: python Lunar_DDL_AD.py 
Upon execution, the notebook or script performs a series of automated tasks to analyze the spacecraft data. It begins by loading and preprocessing the raw data, making sure it‚Äôs in the correct format for analysis. Next, it trains and deploys an RCF model using SageMaker AI, establishing the foundation for our anomaly detection system. When the model is operational, it processes the spacecraft dynamics data to identify potential anomalies in position, velocity, and quaternion measurements. Finally, the script generates detailed visualizations of these findings and automatically uploads both the plots and analysis results to Amazon S3 for secure storage and straightforward access. 
Code structure 
The Python implementation centers around an anomaly detection pipeline, structured in the main script. At its core is the AnomalyDetector class, which orchestrates the entire workflow from data ingestion to visualization. This class contains several methods that together process spacecraft telemetry data and identify anomalies. 
The load_and_prepare_data method handles the initial data ingestion and preprocessing, making sure spacecraft measurements are properly formatted for analysis. After the data is prepared, train_and_deploy_model trains the RCF model and deploys it as a SageMaker endpoint. The predict_anomalies method then uses this trained model to identify unusual patterns in the spacecraft‚Äôs position, velocity, and quaternion data. 
For visualization and storage, the plot_results method creates detailed graphs highlighting detected anomalies, and upload_plot_to_s3 makes sure these visualizations are securely stored in Amazon S3 for future reference and centralized access. 
Together, these components create a comprehensive pipeline for processing spacecraft telemetry data and identifying potential anomalies that might warrant further investigation. 
Configuration 
Adjust the following parameters in the script as needed: 
 
 threshold_percentile for the threshold for anomaly classification 
 RCF hyperparameters in train_and_deploy_model: 
   
   feature_dim: Number of input features 
   num_samples_per_tree: Random data points per tree 
   num_trees: Number of trees in the algorithmic forest 
    
 batch_size in predict_anomalies for large datasets 
 
For RCF applications, the hyperparameters and threshold configuration significantly influence anomaly detections. We use the following configuration values for this example: 
 
 threshold_percentile=0.9 
 RCF hyperparameters in train_and_deploy_model(): 
   
   feature_dim=10 
   num_samples_per_tree=512 
   num_trees=100 
    
 batch_size=1000 in predict_anomalies() 
 
SageMaker AI instance type size for training and inference can affect anomaly results, processing time, and cost. In this example, we used an ml.m5.4xlarge instance for both training and inference. 
In addition, SageMaker AI can be integrated with various security features for protecting sensitive data and models. It‚Äôs possible to operate in no internet or VPC only modes so SageMaker AI instances remain isolated within your Amazon VPC. Secure data access can also be achieved through AWS PrivateLink, enabling private connections to Amazon S3 without internet exposure. Also, integration with AWS Identity and Access Management (IAM) provides fine-grained access control through custom user profiles, enforcing data privacy and adhering to the principle of least privilege, such as when using sensitive spacecraft telemetry data. These are some of the security enhancement services that can be applied according to your appropriate use case with SageMaker AI. 
Data 
The script uses public NASA-Blue Origin Demo of Lunar Deorbit, Descent, and Landing Sensors (BODDL-TP) data, which you can download. Make sure your data is in the correct format with columns for timestamps, positions, velocities, and quaternions. 
Results 
The script generates plots for positions, velocities, and quaternions. The respective data is plotted and the anomalies are plotted as an overlay in red. The plots are saved to the specified S3 bucket. Due to the small scale, the positions plot is difficult to observe anomalies. However, the SageMaker AI RCF algorithm can detect them and are highlighted in red. In the following plots, the sharp changes in velocities and quaternions correspond with the anomalies shown. 
 
Unlike the positions plot, the velocities plot shows discontinuities, which are detected as anomalies. This is likely due to rate changes for vehicle maneuvers during the deorbit, descent, and landing demonstration stages. 
 
Similarly to the velocities plot, the quaternions plot shows sharp changes, which are also detected as anomalies. This is likely due to rotational accelerations during vehicle maneuvers of the deorbit, descent, and landing demonstration stages. 
 
These anomalies most likely represent the lunar spacecraft vehicle dynamics at key maneuver stages of the deorbit, descent, and landing demonstration. Momentum wheels, thrusters, and various other C2 applications could be the cause of the observed abrupt positional, velocity, and quaternion changes being detected as anomalous. By having these results, data points of interest are indicated for more precise and potentially valuable analysis for improved vehicle health and status awareness. 
Clean up 
The provided script includes SageMaker AI endpoint deletion after training and inference to avoid any unnecessary charges. If you‚Äôre using JupyterLab and want to further avoid charges, stop the SageMaker AI instance running the RCF JupyterLab Python notebook. 
Conclusion 
In this post, we demonstrated how the SageMaker AI RCF algorithm can effectively detect anomalies in spacecraft dynamics data from NASA and Blue Origin‚Äôs lunar Deorbit, Descent, and Landing demonstration. By detecting anomalies for position, velocity, and quaternion orientation data, we‚Äôve shown how ML can enhance space mission analysis, situational awareness, and autonomy. The built-in algorithm processes complex, multi-dimensional spacecraft telemetry data. Through efficient batch processing, we can analyze large-scale mission data effectively, and our visualization approach enables quick identification of potential issues in spacecraft dynamics. From there, the solution‚Äôs scalability shows the ability adapt to handle varying data volumes and mission durations, making it potentially suitable for a wide range of space applications. Although this solution applies to a lunar mission demonstration, the approach could have broad applications throughout the space industry. You can adapt the same architecture for various space operations, such as landing missions on other celestial bodies, orbital rendezvous, space station docking, and satellite constellations. This integration of AWS services with aerospace applications creates a robust, secure, and scalable platform for space mission analytics, which is becoming increasingly valuable as we continue to execute missions in the space environment. Looking forward, this solution opens many possibilities for enhancement and expansion. Real-time anomaly detection could be implemented for live mission data, providing immediate insights during critical operations. Also, the system could be enhanced by incorporating additional spacecraft parameters and sensor data, and automated alert services could be developed to provide immediate notification of detected anomalies. In addition, further developments might include extending the analysis to incorporate predictive ML models and creating custom metrics tailored to specific mission requirements. These potential advancements would continue to build upon the foundation we‚Äôve established, creating even more powerful tools for spacecraft mission analysis. 
The code and implementation details are available in our GitHub repository, enabling you to adapt and enhance the solution for your specific needs. 
For space operations, the combination of cloud computing and ML have strong potential to play an increasingly crucial role in ensuring mission success. This solution demonstrates just one of many possible applications of AWS services for improving spacecraft mission compute and data analysis. 
To learn more about the AWS services used in this solution, refer to Guide to getting set up with Amazon SageMaker AI, Train a Model with Amazon SageMaker, and the JupyterLab user guide. 
 
About the authors 
Dr. Ian Lunsford is an Aerospace AI Engineer at AWS Professional Services. He integrates cloud services into aerospace applications. Additionally, Ian focuses on building AI/ML solutions using AWS services. 
Nick Biso is a Machine Learning Engineer at AWS Professional Services. He solves complex organizational and technical challenges using data science and engineering. In addition, he builds and deploys AI/ML models on the AWS Cloud. His passion extends to his proclivity for travel and diverse cultural experiences.

‚∏ª