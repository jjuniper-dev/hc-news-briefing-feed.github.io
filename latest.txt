‚úÖ Morning News Briefing ‚Äì October 04, 2025 10:39

üìÖ Date: 2025-10-04 10:39
üè∑Ô∏è Tags: #briefing #ai #publichealth #digitalgov

‚∏ª

üßæ Weather
‚Ä¢ No watches or warnings in effect, Pembroke
  No watches or warnings in effect. No warnings or watches or watches in effect . Watch or warnings are no longer in effect in the U.S. No watches, warnings are in effect for the rest of the day . No watches and warnings are still in effect, but no watches are in place for the day's events . The weather is not expected to be affected by the weather .
‚Ä¢ Current Conditions: Fog Patches, 8.1¬∞C
  Observed at Garrison Petawawa 6:00 AM EDT Saturday 4 October 2025 . Temperature: 8.1&deg;C Pressure: 102.2 kPa Visibility: 11 km Visibility : 11 km Humidity: 92 % Dewpoint: 6.8&deg:C Wind: WSW 4 km/h . Air Quality Health Index: n/a . Air
‚Ä¢ Saturday: Mainly sunny. High 27.
  Mainly sunny. High 27. Humidex 31. UV index 5 or moderate. Mainly cloudy. High 28.50 degrees Fahrenheit in the early morning hours of Saturday morning . Forecast issued 5:00 AM EDT Saturday 4 October 2025. For the rest of the day, see www.jenn.com/jennennenn.org for the latest weather forecast .

üåç International News
No updates.

üçÅ Canadian News
No updates.

üá∫üá∏ U.S. Top Stories
‚Ä¢ Lou Ye's acclaimed 'An Unfinished Film' remains unfinished
  Lou Ye's An Unfinished Film is not a masterpiece, but why do so many seem to demand it to be? Why not so many demand it be a masterpiece? Lou Ye: 'An Unfinished film' is not an unfinished film, but it is a masterpiece . Lou Ye is a Chinese director and director of a German film company that specializes in Chinese films and Chinese films
‚Ä¢ Japan's ruling party elects Sanae Takaichi as leader, likely to become first female PM
  Sanae Takaichi is a hard-line ultra-conservative and China hawk . She is likely to become the country's first female prime minister . Sanae is an ultra-hard-liners and China hawks who oppose China's influence in the country . She will likely become the first female Prime Minister of Japan, according to Japan's governing party . Japan's ruling party elected Sana
‚Ä¢ NPR's Student Podcast Challenge: Here are our fourth grade winners!
  We heard fun and engaging podcasts on topics including how math teaching has evolved, what its like to disengage from technology, and, who has it better: kids or grownups? We also heard how math teacher has evolved and what it's like to engage in technology . Listen to the full transcript of each episode of this week's featured episodes of iReporters on CNN iReport .
‚Ä¢ The Federal Election Commission is down to 2 members. So its work is at a standstill
  The FEC has been without a quorum for months, leaving the agency unable to do much of its work . The FEC regulates campaign finance, but it has lost another member . The agency has been unable to work for months because of the loss of a member of its quorum, which means it can't do so much of it's work on the issue of campaign finance and campaign finance
‚Ä¢ 8 low-effort ways to make Spooky Season feel cozy and festive
  We've got a list of simple ideas to make your lead-up to Halloween feel warm, restful and eerie . Halloween is a great time to celebrate all the good things the season brings . We've also got some Halloween-themed ideas to help you celebrate the holiday with a little eerie decorating and Halloween-related activities in the lead up to the big day . The Halloween season

üß† Artificial Intelligence
No updates.

üíª Digital Strategy
‚Ä¢ India's tech talent pipeline is sputtering
  Shubh Kumar graduated from IIT Patna, one of India's famed Institutes of Technology . The IITs attract millions of applicants but admit only 18,000 undergraduates . AI and new wave of offshoring mean graduates can't get gigs without a good education . The world's top universities attract millions applicants but only admit just 18,500 undergraduates per year . The
‚Ä¢ ICE plans to scour Facebook, TikTok, X, and even defunct Google+ for illegal immigration leads
  Draft solicitation calls for nearly 30 contractors to mine social media and other open-source data . US Immigration and Customs Enforcement (ICE) is seeking contractors to trawl social media for potential immigration enforcement leads . ICE hopes public posts can yield actionable intelligence, assuming public posts will yield intelligence . ICE: Social media posts should yield intelligence; public posts may yield leads to actionable information.‚Ä¶
‚Ä¢ Bezos plan for solar powered datacenters is out of this world‚Ä¶ literally
  Aspiring Bond villain believes the best place to train our AI overlords is in orbit . Within two decades, gigawatt-scale datacenters powered by a continuous stream of photons from the sun will fill Earth's orbit . Amazon founder and executive chair Jeff Bezos says within two decades the datacomes will fill the world's orbit and be powered by the sun's solar rays .
‚Ä¢ No suds for you! Asahi brewery attack leaves Japanese drinkers dry
  Ransomware has left Japan's biggest brewer struggling to ship beer . Asahi warning domestic customers to brace for patchy supplies while its core systems stay offline . One week after the blitz, beer biz is still stymied - one week after it was hit by the cyber-wansomware attack . Japan‚Äôs biggest brewer, Asahi, warns domestic customers of patch
‚Ä¢ 'Retired' cybercrime group demands $989M not to leak 1B Salesforce records
  Scattered LAPSUS$ Hunters has reemerged with a data-leak site listing about 40 companies‚Äô Salesforce environments . The group is demanding $989.45 to prevent what it claims is about 1 billion stolen records from being published online . CRM giant insists its platform wasn‚Äôt breached and says it‚Äôs not a breach of its platform . The

üè• Public Health
No updates.

üî¨ Science
‚Ä¢ Determinants of low fifth minute Apgar scores among newborns at North Shoa Zone Public Hospitals in Ethiopia
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ The primary hospitals should take gait speed as a routine test for elderly patients
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Quantifying population-level sexual risk behavior through HSV-2 transmission dynamics in the United States, 1950‚Äì2020
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Persistence and clearance of oral human papillomavirus among a multi-national cohort of men
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Application of machine learning models for predicting depression among older adults with non-communicable diseases in India
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations

üßæ Government & Policy
No updates.

üèõÔ∏è Enterprise Architecture & IT Governance
No updates.

ü§ñ AI & Emerging Tech
‚Ä¢ The Download: using AI to discover ‚Äúzero day‚Äù vulnerabilities, and Apple‚Äôs ICE app removal
  This is today&#8217;s edition of¬†The Download,¬†our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



Microsoft says AI can create ‚Äúzero day‚Äù threats in biology



A team at Microsoft says it used artificial intelligence to discover a &#8220;zero day&#8221; vulnerability in the biosecurity systems used to prevent the misuse of DNA.These screening systems are designed to stop people from purchasing genetic sequences that could be used to create deadly toxins or pathogens. But now researchers say they have figured out how to bypass the protections in a way previously unknown to defenders. Read the full story.



‚ÄîAntonio Regalado



If you‚Äôre interested in learning more about AI and biology, check out:



+ AI-designed viruses are here and already killing bacteria. Read the full story.+ OpenAI is making a foray into longevity science with an AI built to help manufacture stem cells.+ AI is dreaming up drugs that no one has ever seen. Now we‚Äôve got to see if they work.







The must-reads



I‚Äôve combed the internet to find you today‚Äôs most fun/important/scary/fascinating stories about technology.



1 Apple removed an app for reporting ICE officer sightingsThe US Attorney General requested it take down ICEBlock‚Äîand Apple complied. (Insider $)+ Apple says the removal was down to the safety risk it posed. (Bloomberg $)+ The company had a similar explanation for removing a Hong Kong map app back in 2019. (The Verge)



2 OpenAI‚Äôs parental controls are easily circumvented¬†Its alerts about teenagers‚Äô concerning conversations also took hours to deliver. (WP $)+ The looming crackdown on AI companionship. (MIT Technology Review)



3 VCs have sunk a record amount into AI startups this year¬†To the tune of $192.7 billion so far. (Bloomberg $)+ The AI bubble is looking increasingly precarious, though. (FT $)+ How to fine-tune AI for prosperity. (MIT Technology Review)



4 The US federal vaccination schedule is still waiting for an updateOfficials are yet to sign off on recommendations for this year&#8217;s updated Covid shots. (Ars Technica)+ Many people have been left unable to get vaccinated. (NPR)



5 The US Department of Energy has canceled yet more clean energy projectsIn mostly blue states. (TechCrunch)+ More than 300 funding awards have been axed. (CNBC)+ How to make clean energy progress under Trump in the states. (MIT Technology Review)



6 TikTok recommends pornography to children‚Äôs accountsDespite activating its &#8220;restricted mode‚Äù to prevent sexualized content. (BBC)



7 China has launched a new skilled worker visa programIn the wake of the US H-1B visa clampdown. (Wired $)+ The initiative hasn‚Äôt gone down well with locals. (BBC)



8 Flights were grounded in Germany after several drone sightingsNATO members are worried about suspected Russian incursions in their skies. (WSJ $)+ It‚Äôs the latest in a string of airspace sightings. (FT $)



9 How YouTube is shaking up HollywoodIts powerful creators are starting to worry the entertainment establishment‚Äîand Netflix. (FT $)



10 Anti-robocall tools are getting betterCall screening features are a useful first line of defense. (NYT $)







Quote of the day



‚ÄúCapitulating to an authoritarian regime is never the right move.‚Äù



‚ÄîJoshua Aaron, the developer of ICEBlock, the app that crowdsources sightings of ICE officials, hits back at Apple‚Äôs decision to remove it from the App Store, 404 Media reports.







One more thing







How AI can help supercharge creativityExisting generative tools can automate a striking range of creative tasks and offer near-instant gratification‚Äîbut at what cost? Some artists and researchers fear that such technology could turn us into passive consumers of yet more AI slop.And so they are looking for ways to inject human creativity back into the process: working on what‚Äôs known as co-¬≠creativity or more-than-human creativity. The aim is to develop AI tools that augment our creativity rather than strip it from us. Read the full story.



‚ÄîWill Douglas Heaven







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)



+ Congratulations to Fizz, the very handsome UK cat of the year! + What it took to transform actor Jeremy Allan White into the one and only Boss in his new film, Deliver Me from Nowhere.+ Divers have salvaged more than 1,000 gold and silver coins from a 1715 shipwreck off the east coast of Florida.+ The internet is obsessed with crabs. But why?
‚Ä¢ Microsoft says AI can create ‚Äúzero day‚Äù threats in biology
  A team at Microsoft says it used artificial intelligence to discover a &#8220;zero day&#8221; vulnerability in the biosecurity systems used to prevent the misuse of DNA.



These screening systems are designed to stop people from purchasing genetic sequences that could be used to create deadly toxins or pathogens. But now researchers led by Microsoft‚Äôs chief scientist, Eric Horvitz, say they have figured out how to bypass the protections in a way previously unknown to defenders.¬†



The team described its work today in the journal Science.



Horvitz and his team focused on generative AI algorithms that propose new protein shapes. These types of programs are already fueling the hunt for new drugs at well-funded startups like Generate Biomedicines and Isomorphic Labs, a spinout of Google.&nbsp;



The problem is that such systems are potentially ‚Äúdual use.‚Äù They can use their training sets to generate both beneficial molecules and harmful ones.



Microsoft says it began a ‚Äúred-teaming‚Äù test of AI‚Äôs dual-use potential in 2023 in order to determine whether ‚Äúadversarial AI protein design‚Äù could help bioterrorists manufacture harmful proteins.&nbsp;



The safeguard that Microsoft attacked is what‚Äôs known as biosecurity screening software. To manufacture a protein, researchers typically need to order a corresponding DNA sequence from a commercial vendor, which they can then install in a cell. Those vendors use screening software to compare incoming orders with known toxins or pathogens. A close match will set off an alert.





To design its attack, Microsoft used several generative protein models (including its own, called EvoDiff) to redesign toxins‚Äîchanging their structure in a way that let them slip past screening software but was predicted to keep their deadly function intact.



The researchers say the exercise was entirely digital and they never produced any toxic proteins. That was to avoid any perception that the company was developing bioweapons.&nbsp;



Before publishing the results, Microsoft says, it alerted the US government and software makers, who‚Äôve already patched their systems, although some AI-designed molecules can still escape detection.&nbsp;



‚ÄúThe patch is incomplete, and the state of the art is changing. But this isn‚Äôt a one-and-done thing. It‚Äôs the start of even more testing,‚Äù says Adam Clore, director of technology R&amp;D at Integrated DNA Technologies, a large manufacturer of DNA, who is a coauthor on the Microsoft report. ‚ÄúWe‚Äôre in something of an arms race.‚Äù



To make sure nobody misuses the research, the researchers say, they‚Äôre not disclosing some of their code and didn‚Äôt reveal what toxic proteins they asked the AI to redesign. However, some dangerous proteins are well known, like ricin‚Äîa poison found in castor beans‚Äîand the infectious prions that are the cause of mad-cow disease.



‚ÄúThis finding, combined with rapid advances in AI-enabled biological modeling, demonstrates the clear and urgent need for enhanced nucleic acid synthesis screening procedures coupled with a reliable enforcement and verification mechanism,‚Äù says Dean Ball, a fellow at the Foundation for American Innovation, a think tank in San Francisco.



Ball notes that the US government already considers screening of DNA orders a key line of security. Last May, in an executive order on biological research safety, President Trump called for an overall revamp of that system, although so far the White House hasn‚Äôt released new recommendations.



Others doubt that commercial DNA synthesis is the best point of defense against bad actors. Michael Cohen, an AI-safety researcher at the University of California, Berkeley, believes there will always be ways to disguise sequences and that Microsoft could have made its test harder.



‚ÄúThe challenge appears weak, and their patched tools fail a lot,‚Äù says Cohen. ‚ÄúThere seems to be an unwillingness to admit that sometime soon, we‚Äôre going to have to retreat from this supposed choke point, so we should start looking around for ground that we can actually hold.‚Äù&nbsp;



Cohen says biosecurity should probably be built into the AI systems themselves‚Äîeither directly or via controls over what information they give.&nbsp;



But Clore says monitoring gene synthesis is still a practical approach to detecting biothreats, since the manufacture of DNA in the US is dominated by a few companies that work closely with the government. By contrast, the technology used to build and train AI models is more widespread. ‚ÄúYou can‚Äôt put that genie back in the bottle,‚Äù says Clore. ‚ÄúIf you have the resources to try to trick us into making a DNA sequence, you can probably train a large language model.‚Äù
‚Ä¢ The Download: RIP EV tax credits, and OpenAI‚Äôs new valuation
  This is today&#8217;s edition of¬†The Download,¬†our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



EV tax credits are dead in the US. Now what?



Federal EV tax credits in the US officially came to an end yesterday.Those credits, expanded and extended in the 2022 Inflation Reduction Act, gave drivers up to $7,500 toward the purchase of a new electric vehicle. They‚Äôve been a major force in cutting the up-front costs of EVs, pushing more people toward purchasing them and giving automakers confidence that demand would be strong.The tax credits‚Äô demise comes at a time when battery-electric vehicles still make up a small percentage of new vehicle sales in the country. So what‚Äôs next for the US EV market?



‚ÄîCasey Crownhart



This article is from The Spark, MIT Technology Review‚Äôs weekly climate newsletter. To receive it in your inbox every Wednesday, sign up here.



If you‚Äôre interested in reading more about EVs and clean energy, take a look at:



+ The US could really use an affordable electric truck. Ford recently announced plans for a $30,000 electric pickup, which could be the shot in the arm that the slowing US EV market needs. Read the full story.+ What role should oil and gas companies play in climate tech, really?+ China is an EV-building powerhouse. These three charts explain its energy dominance. Read the full story.+ Supporting new technologies like EVs can be expensive, but deciding when to wean the public off incentives can be a difficult balancing act. Read the full story.







The must-reads



I‚Äôve combed the internet to find you today‚Äôs most fun/important/scary/fascinating stories about technology.



1 OpenAI has become the world‚Äôs most valuable startupMove aside, SpaceX. (Bloomberg $)+ OpenAI is now valued at an eye-watering $500 billion. (FT $)+ The valuation came after workers sold around $6.6 billion in shares. (Reuters)



2 Music labels are close to striking AI licensing dealsUniversal and Warner are trying their best to avoid the mis-steps of the internet era. (FT $)+ AI is coming for music, too. (MIT Technology Review)



3 Facebook‚Äôs political ads are full of spam and scamsAnd deepfake technology is making them more convincing than ever. (NYT $)+ Meta will start using conversations with its chatbots to personalize ads. (WSJ $)4 China is forging ahead with integrating AI tools into children‚Äôs livesBut educators worry they‚Äôll harm youngsters&#8217; learning and social skills. (Rest of World)+ Chinese universities want students to use more AI, not less. (MIT Technology Review)



5 The batteries of the future could be created by AI¬†Researchers including Microsoft are experimenting with materials suggested by models. (IEEE Spectrum)+ This startup wants to use the Earth as a massive battery. (MIT Technology Review)



6 A historian claims to have used AI to identify an anonymous NaziDigital tools helped J√ºrgen Matth√§us to pinpoint the person photographed beside a mass grave. (The Guardian)



7 The Pentagon is interested in AI-powered machine guns that shoot dronesSteven Simoni‚Äôs Allen Control Systems is part of Silicon Valley‚Äôs new military pivot. (Reuters)+ We saw a demo of the new AI system powering Anduril‚Äôs vision for war. (MIT Technology Review)



8 One of Saturn‚Äôs moons may have once hosted life Enceladus has all the necessary keystones to support life, and future missions could uncover it. (Scientific American $)+ Meanwhile, Blue Origin has won a NASA rover contract. (Wired $)+ The case against humans in space. (MIT Technology Review)



9 Chatbots exercise all sorts of tricks to keep you talkingThey don‚Äôt want the conversation to end, a new study has found. (Wired $)



10 What it‚Äôs like to become a viral memeDrew Scanlon, aka ‚ÄúBlinking Guy,‚Äù is leveraging his fame for a good cause. (SF Gate)







Quote of the day



‚ÄúI cannot overstate how disgusting I find this kind of ‚ÄòAI‚Äô dog shit in the first place, never mind under these circumstances.‚Äù



‚ÄîWriter Luke O‚ÄôNeil tells 404 Media his feelings about an AI-generated ‚Äúbiography‚Äù of journalist Kaleb Horton, who recently died.







One more thing







A day in the life of a Chinese robotaxi driver



When Liu Yang started his current job, he found it hard to go back to driving his own car: ‚ÄúI instinctively went for the passenger seat. Or when I was driving, I would expect the car to brake by itself,‚Äù says the 33-year-old Beijing native, who joined the Chinese tech giant Baidu in January 2021 as a robotaxi driver.



Liu is one of the hundreds of safety operators employed by Baidu, ‚Äúdriving‚Äù five days a week in Shougang Park. But despite having only worked for the company for 19 months, he already has to think about his next career move, as his job will likely be eliminated within a few years. Read the full story.



‚ÄîZeyi Yang







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)



+ Congratulations are in order for 32 Chunk, winner of this year‚Äôs highly prestigious Fat Bear Week competition + Here‚Äôs how 10 women artists got their days off to the best start possible.+ This Instagram account documenting the worldly travels of a cassette player is fab.+ Brb, I‚Äôm off to listen to Arctic Outpost Radio, spinning records from the very top of the world.
‚Ä¢ EV tax credits are dead in the US. Now what?
  On Wednesday, federal EV tax credits in the US officially came to an end.



Those credits, expanded and extended in the 2022 Inflation Reduction Act, gave drivers up to $7,500 in credits toward the purchase of a new electric vehicle. They‚Äôve been a major force in cutting the up-front costs of EVs, pushing more people toward purchasing them and giving automakers confidence that demand would be strong.





The tax credits‚Äô demise comes at a time when battery-electric vehicles still make up a small percentage of new vehicle sales in the country. And transportation is a major contributor to US climate pollution, with cars, trucks, ships, trains, and planes together making up roughly 30% of total greenhouse-gas emissions.



To anticipate what‚Äôs next for the US EV market, we can look to countries like Germany, which have ended similar subsidy programs. (Spoiler alert: It‚Äôs probably going to be a rough end to the year.)



When you factor in fuel savings, the lifetime cost of an EV can already be lower than that of a gas-powered vehicle today. But EVs can have a higher up-front cost, which is why some governments offer a tax credit or rebate that can help boost adoption for the technology.



In 2016, Germany kicked off a national incentive program to encourage EV sales. While the program was active, drivers could get grants of up to about ‚Ç¨6,000 toward the purchase of a new battery-electric or plug-in hybrid vehicle.



Eventually, the government began pulling back the credits. Support for plug-in hybrids ended in 2022, and commercial buyers lost eligibility in September 2023. Then the entire program came to a screeching halt in December 2023, when the government announced it would be ending the incentives with about one week‚Äôs notice.



Monthly sales data shows the fingerprints of those changes. In each case where there‚Äôs a contraction of public support, there‚Äôs a peak in sales just before a cutback, then a crash after. These short-term effects can be dramatic: There were about half as many battery-electric vehicles sold in Germany in January 2024 than there were in December 2023.&nbsp;





We‚Äôre already seeing the first half of this sort of boom-bust cycle in the US: EV sales ticked up in August, making up about 10% of all new vehicle sales, and analysts say September will turn out to be a record-breaking month. People rushed to take advantage of the credits while they still could.



Next comes the crash‚Äîthe next few months will probably be very slow for EVs. One analyst predicted to the Washington Post that the figure could plummet to the low single digits, ‚Äúlike 1 or 2%.‚Äù



Ultimately, it‚Äôs not terribly surprising that there are local effects around these policy changes. ‚ÄúThe question is really how long this decline will last, and how slowly any recovery in the growth will be,‚Äù Robbie Andrew, a senior researcher at the CICERO Center for International Climate Research in Norway who collects EV sales data, said in an email.&nbsp;



When I spoke to experts (including Andrew) for a story last year, several told me that Germany‚Äôs subsidies were ending too soon, and that they were concerned about what cutting off support early would mean for the long-term prospects of the technology in the country. And Germany was much further along than the US, with EVs making up 20% of new vehicle sales‚Äîtwice the American proportion.



EV growth did see a longer-term backslide in Germany after the end of the subsidies. Battery-electric vehicles made up 13.5% of new registrations in 2024, down from 18.5% the year before, and the UK also passed Germany to become Europe‚Äôs largest EV market.&nbsp;



Things have improved this year, with sales in the first half beating records set in 2023. But growth would need to pick up significantly for Germany to reach its goal of getting 15 million battery-electric vehicles registered in the country by 2030. As of January 2025, that number was just 1.65 million.&nbsp;



According to early projections, the end of tax credits in the US could significantly slow progress on EVs and, by extension, on cutting emissions. Sales of battery-electric vehicles could be about 40% lower in 2030 without the credits than what we‚Äôd see with them, according to one analysis by Princeton University‚Äôs Zero Lab.



Some US states still have their own incentive programs for people looking to buy electric vehicles. But without federal support, the US is likely to continue lagging behind global EV leaders like China.&nbsp;



As Andrew put it: ‚ÄúFrom a climate perspective, with road transport responsible for almost a quarter of US total emissions, leaving the low-hanging fruit on the tree is a significant setback.‚Äù&nbsp;



This article is from The Spark, MIT Technology Review‚Äôs weekly climate newsletter. To receive it in your inbox every Wednesday, sign up here.
‚Ä¢ Turning migration into modernization
  In late 2023, a long-trusted virtualization staple became the biggest open question on the enterprise IT roadmap . CIOs contending with pricing hikes and product roadmap opacity face a daunting choice: double‚Äëdown on a familiar but costlier stack, or use the disruption to rethink how‚Äîand where‚Äîcritical workloads should run . Forrester predicted that one in five large VMware customers would begin moving away from the platform in 2024 .

üîí Cybersecurity & Privacy
No updates.

üéì University AI
No updates.

üè¢ Corporate AI
‚Ä¢ Unlock global AI inference scalability using new global cross-Region inference on Amazon Bedrock  with Anthropic‚Äôs Claude Sonnet 4.5
  Organizations are increasingly integrating generative AI capabilities into their applications to enhance customer experiences, streamline operations, and drive innovation. As generative AI workloads continue to grow in scale and importance, organizations face new challenges in maintaining consistent performance, reliability, and availability of their AI-powered applications. Customers are looking to scale their AI inference workloads across multiple AWS Regions to support consistent performance and reliability. 
To address this need, we introduced cross-Region inference (CRIS) for Amazon Bedrock. This managed capability automatically routes inference requests across multiple Regions, enabling applications to handle traffic bursts seamlessly and achieve higher throughput without requiring developers to predict demand fluctuations or implement complex load-balancing mechanisms. CRIS works through inference profiles, which define a foundation model (FM) and the Regions to which requests can be routed. 
We are excited to announce availability of global cross-Region inference with Anthropic‚Äôs Claude Sonnet 4.5 on Amazon Bedrock. Now, with cross-Region inference, you can choose either a geography-specific inference profile or a global inference profile. This evolution from geography-specific routing provides greater flexibility for organizations because Amazon Bedrock automatically selects the optimal commercial Region within that geography to process your inference request. Global CRIS further enhances cross-Region inference by enabling the routing of inference requests to supported commercial Regions worldwide, optimizing available resources and enabling higher model throughput. This helps support consistent performance and higher throughput, particularly during unplanned peak usage times. Additionally, global CRIS supports key Amazon Bedrock features, including prompt caching, batch inference, Amazon Bedrock Guardrails, Amazon Bedrock Knowledge Bases, and more. 
In this post, we explore how global cross-Region inference works, the benefits it offers compared to Regional profiles, and how you can implement it in your own applications with Anthropic‚Äôs Claude Sonnet 4.5 to improve your AI applications‚Äô performance and reliability. 
Core functionality of global cross-Region inference 
Global cross-Region inference helps organizations manage unplanned traffic bursts by using compute resources across different Regions. This section explores how this feature works and the technical mechanisms that power its functionality. 
Understanding inference profiles 
An inference profile in Amazon Bedrock defines an FM and one or more Regions to which it can route model invocation requests. The global cross-Region inference profile for Anthropic‚Äôs Claude Sonnet 4.5 extends this concept beyond geographic boundaries, allowing requests to be routed to one of the supported Amazon Bedrock commercial Regions globally, so you can prepare for unplanned traffic bursts by distributing traffic across multiple Regions. 
Inference profiles operate on two key concepts: 
 
 Source Region ‚Äì The Region from which the API request is made 
 Destination Region ‚Äì A Region to which Amazon Bedrock can route the request for inference 
 
At the time of writing, global CRIS supports over 20 source Regions, and the destination Region is a supported commercial Region dynamically chosen by Amazon Bedrock. 
Intelligent request routing 
Global cross-Region inference uses an intelligent request routing mechanism that considers multiple factors, including model availability, capacity, and latency, to route requests to the optimal Region. The system automatically selects the optimal available Region for your request without requiring manual configuration: 
 
 Regional capacity ‚Äì The system considers the current load and available capacity in each potential destination Region. 
 Latency considerations ‚Äì Although the system prioritizes availability, it also takes latency into account. By default, the service attempts to fulfill requests from the source Region when possible, but it can seamlessly route requests to other Regions as needed. 
 Availability metrics ‚Äì The system continuously monitors the availability of FMs across Regions to support optimal routing decisions. 
 
This intelligent routing system enables Amazon Bedrock to distribute traffic dynamically across the AWS global infrastructure, facilitating optimal availability for each request and smoother performance during high-usage periods. 
Monitoring and logging 
When using global cross-Region inference, Amazon CloudWatch and AWS CloudTrail continue to record log entries only in the source Region where the request originated. This simplifies monitoring and logging by maintaining all records in a single Region regardless of where the inference request is ultimately processed. To track which Region processed a request, CloudTrail events include an additionalEventData field with an inferenceRegion key that specifies the destination Region. Organizations can monitor and analyze the distribution of their inference requests across the AWS global infrastructure. 
Data security and compliance 
Global cross-Region inference maintains high standards for data security. Data transmitted during cross-Region inference is encrypted and remains within the secure AWS network. Sensitive information remains protected throughout the inference process, regardless of which Region processes the request. Because security and compliance is a shared responsibility, you must also consider legal or compliance requirements that come with processing inference request in a different geographic location. Because global cross-Region inference allows requests to be routed globally, organizations with specific data residency or compliance requirements can elect, based on their compliance needs, to use geography-specific inference profiles to make sure data remains within certain Regions. This flexibility helps businesses balance redundancy and compliance needs based on their specific requirements. 
Implement global cross-Region inference 
To use global cross-Region inference with Anthropic‚Äôs Claude Sonnet 4.5, developers must complete the following key steps: 
 
 Use the global inference profile ID ‚Äì When making API calls to Amazon Bedrock, specify the global Anthropic‚Äôs Claude Sonnet 4.5 inference profile ID (global.anthropic.claude-sonnet-4-5-20250929-v1:0) instead of a Region-specific model ID. This works with both InvokeModel and Converse APIs. 
 Configure IAM permissions ‚Äì Grant appropriate AWS Identity and Access Management (IAM) permissions to access the inference profile and FMs in potential destination Regions. In the next section, we provide more details. You can also read more about prerequisites for inference profiles. 
 
Implementing global cross-Region inference with Anthropic‚Äôs Claude Sonnet 4.5 is straightforward, requiring only a few changes to your existing application code. The following is an example of how to update your code in Python: 
 
 import boto3
import json
bedrock = boto3.client('bedrock-runtime', region_name='us-east-1')


model_id = "global.anthropic.claude-sonnet-4-5-20250929-v1:0"&nbsp;&nbsp;



response = bedrock.converse(
&nbsp; &nbsp;&nbsp;messages=[{"role": "user", "content": [{"text": "Explain cloud computing in 2 sentences."}]}],
&nbsp;&nbsp; &nbsp;modelId=model_id,
)

print("Response:", response['output']['message']['content'][0]['text'])
print("Tokens used:", result.get('usage', {})) 
 
If you‚Äôre using the Amazon Bedrock InvokeModel API, you can quickly switch to a different model by changing the model ID, as shown in Invoke model code examples. 
IAM policy requirements for global CRIS 
In this section, we discuss the IAM policy requirements for global CRIS. 
Enable global CRIS 
To enable global CRIS for your users, you must apply a three-part IAM policy to the role. The following is an example IAM policy to provide granular control. You can replace &lt;REQUESTING REGION&gt; in the example policy with the Region you are operating in. 
 
 {
&nbsp;&nbsp; &nbsp;"Version": "2012-10-17",
&nbsp;&nbsp; &nbsp;"Statement": [
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;{
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"Sid": "GrantGlobalCrisInferenceProfileRegionAccess",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"Effect": "Allow",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"Action": "bedrock:InvokeModel",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"Resource": [
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"arn:aws:bedrock:&lt;REQUESTING REGION&gt;:&lt;ACCOUNT&gt;:inference-profile/global.&lt;MODEL NAME&gt;"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;],
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"Condition": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"StringEquals": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"aws:RequestedRegion": "&lt;REQUESTING REGION&gt;"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;},
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;{
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"Sid": "GrantGlobalCrisInferenceProfileInRegionModelAccess",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"Effect": "Allow",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"Action": "bedrock:InvokeModel",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"Resource": [
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"arn:aws:bedrock:&lt;REQUESTING REGION&gt;::foundation-model/&lt;MODEL NAME&gt;"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;],
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"Condition": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"StringEquals": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"aws:RequestedRegion": "&lt;REQUESTING REGION&gt;",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"bedrock:InferenceProfileArn": "arn:aws:bedrock:&lt;REQUESTING REGION&gt;:&lt;ACCOUNT&gt;:inference-profile/global.&lt;MODEL NAME&gt;"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;},
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;{
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"Sid": "GrantGlobalCrisInferenceProfileGlobalModelAccess",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"Effect": "Allow",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"Action": "bedrock:InvokeModel",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"Resource": [
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"arn:aws:bedrock:::foundation-model/&lt;MODEL NAME&gt;"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;],
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"Condition": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"StringEquals": {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; "aws:RequestedRegion":&nbsp;"unspecified",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"bedrock:InferenceProfileArn": "arn:aws:bedrock:&lt;REQUESTING REGION&gt;:&lt;ACCOUNT&gt;:inference-profile/global.&lt;MODEL NAME&gt;"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp;]
} 
 
The first part of the policy grants access to the Regional inference profile in your requesting Region. This policy allows users to invoke the specified global CRIS inference profile from their requesting Region. The second part of the policy provides access to the Regional FM resource, which is necessary for the service to understand which model is being requested within the Regional context. The third part of the policy grants access to the global FM resource, which enables the cross-Region routing capability that makes global CRIS function. When implementing these policies, make sure all three resource Amazon Resource Names (ARNs) are included in your IAM statements: 
 
 The Regional inference profile ARN follows the pattern arn:aws:bedrock:REGION:ACCOUNT:inference-profile/global.MODEL-NAME. This is used to give access to the global inference profile in the source Region. 
 The Regional FM uses arn:aws:bedrock:REGION::foundation-model/MODEL-NAME. This is used to give access to the FM in the source Region. 
 The global FM requires arn:aws:bedrock:::foundation-model/MODEL-NAME. This is used to give access to the FM in different global Regions. 
 
The global FM ARN has no Region or account specified, which is intentional and required for the cross-Region functionality. 
To simplify onboarding, global CRIS doesn‚Äôt require complex changes to an organization‚Äôs existing Service Control Policies (SCPs) that might deny access to services in certain Regions. When you opt in to global CRIS using this three-part policy structure, Amazon Bedrock will process inference requests across commercial Regions without validating against Regions denied in other parts of SCPs. This prevents workload failures that could occur when global CRIS routes inference requests to new or previously unused Regions that might be blocked in your organization‚Äôs SCPs. However, if you have data residency requirements, you should carefully evaluate your use cases before implementing global CRIS, because requests might be processed in any supported commercial Region. 
Disable global CRIS 
You can choose from two primary approaches to implement deny policies to global CRIS for specific IAM roles, each with different use cases and implications: 
 
 Remove an IAM policy ‚Äì The first method involves removing one or more of the three required IAM policies from user permissions. Because global CRIS requires all three policies to function, removing a policy will result in denied access. 
 Implement a deny policy ‚Äì The second approach is to implement an explicit deny policy that specifically targets global CRIS inference profiles. This method provides clear documentation of your security intent and makes sure that even if someone accidentally adds the required allow policies later, the explicit deny will take precedence. The deny policy should use a StringEquals condition matching the pattern "aws:RequestedRegion": "unspecified". This pattern specifically targets inference profiles with the global prefix. 
 
When implementing deny policies, it‚Äôs crucial to understand that global CRIS changes how the aws:RequestedRegion field behaves. Traditional Region-based deny policies that use StringEquals conditions with specific Region names such as "aws:RequestedRegion": "us-west-2" will not work as expected with global CRIS because the service sets this field to global rather than the actual destination Region. However, as mentioned earlier, "aws:RequestedRegion": "unspecified" will result in the deny effect. 
Note: To simplify customer onboarding, global CRIS has been designed to work without requiring complex changes to an organization‚Äôs existing SCPs that may deny access to services in certain Regions. When customers opt in to global CRIS using the three-part policy structure described above, Amazon Bedrock will process inference requests across supported AWS commercial Regions without validating against regions denied in any other parts of SCPs. This prevents workload failures that could occur when global CRIS routes inference requests to new or previously unused Regions that might be blocked in your organization‚Äôs SCPs. However, customers with data residency requirements should evaluate their use cases before implementing global CRIS, because requests may be processed in any supported commercial Regions. As a best practice, organizations who use geographic CRIS but want to opt out from global CRIS should implement the second approach. 
Request limit increases for global CRIS with Anthropic‚Äôs Claude Sonnet 4.5 
When using global CRIS inference profiles, it‚Äôs important to understand that service quota management is centralized in the US East (N. Virginia) Region. However, you can use global CRIS from over 20 supported source Regions. Because this will be a global limit, requests to view, manage, or increase quotas for global cross-Region inference profiles must be made through the Service Quotas console or AWS Command Line Interface (AWS CLI) specifically in the US East (N. Virginia) Region. Quotas for global CRIS inference profiles will not appear on the Service Quotas console or AWS CLI for other source Regions, even when they support global CRIS usage. This centralized quota management approach makes it possible to access your limits globally without estimating usage in individual Regions. If you don‚Äôt have access to US East (N. Virginia), reach out to your account teams or AWS support. 
Complete the following steps to request a limit increase: 
 
 Sign in to the Service Quotas console in your AWS account.  
 Make sure your selected Region is US East (N. Virginia). 
 In the navigation pane, choose AWS services. 
 From the list of services, find and choose Amazon Bedrock. 
 In the list of quotas for Amazon Bedrock, use the search filter to find the specific global CRIS quotas. For example: 
   
   Global cross-Region model inference tokens per day for Anthropic Claude Sonnet 4.5 V1 
   Global cross-Region model inference tokens per minute for Anthropic Claude Sonnet 4.5 V1 
    
 Select the quota you want to increase. 
 Choose Request increase at account level.  
 Enter your desired new quota value. 
 Choose Request to submit your request. 
 
Use global cross-Region inference with Anthropic‚Äôs Claude Sonnet 4.5 
Claude Sonnet 4.5 is Anthropic‚Äôs most intelligent model (at the time of writing), and is best for coding and complex agents. Anthropic‚Äôs Claude Sonnet 4.5 demonstrates advancements in agent capabilities, with enhanced performance in tool handling, memory management, and context processing. The model shows marked improvements in code generation and analysis, including identifying optimal improvements and exercising stronger judgment in refactoring decisions. It particularly excels at autonomous long-horizon coding tasks, where it can effectively plan and execute complex software projects spanning hours or days while maintaining consistent performance and reliability throughout the development cycle. 
Global cross-Region inference for Anthropic‚Äôs Claude Sonnet 4.5 delivers multiple advantages over traditional geographic cross-Region inference profiles: 
 
 Enhanced throughput during peak demand ‚Äì Global cross-Region inference provides improved resilience during periods of peak demand by automatically routing requests to Regions with available capacity. This dynamic routing happens seamlessly without additional configuration or intervention from developers. Unlike traditional approaches that might require complex client-side load balancing between Regions, global cross-Region inference handles traffic spikes automatically. This is particularly important for business-critical applications where downtime or degraded performance can have significant financial or reputational impacts. 
 Cost-efficiency ‚Äì Global cross-Region inference for Anthropic‚Äôs Claude Sonnet 4.5 offers approximately 10% savings on both input and output token pricing compared to geographic cross-Region inference. The price is calculated based on the Region from which the request is made (source Region). This means organizations can benefit from improved resilience with even lower costs. This pricing model makes global cross-Region inference a cost-effective solution for organizations looking to optimize their generative AI deployments. By improving resource utilization and enabling higher throughput without additional costs, it helps organizations maximize the value of their investment in Amazon Bedrock. 
 Streamlined monitoring ‚Äì When using global cross-Region inference, CloudWatch and CloudTrail continue to record log entries in your source Region, simplifying observability and management. Even though your requests are processed across different Regions worldwide, you maintain a centralized view of your application‚Äôs performance and usage patterns through your familiar AWS monitoring tools. 
 On-demand quota flexibility ‚Äì With global cross-Region inference, your workloads are no longer limited by individual Regional capacity. Instead of being restricted to the capacity available in a specific Region, your requests can be dynamically routed across the AWS global infrastructure. This provides access to a much larger pool of resources, making it less complicated to handle high-volume workloads and sudden traffic spikes. 
 
If you‚Äôre currently using Anthropic‚Äôs Sonnet models on Amazon Bedrock, upgrading to Claude Sonnet 4.5 is a great opportunity to enhance your AI capabilities. It offers a significant leap in intelligence and capability, offered as a straightforward, drop-in replacement at a comparable price point as Sonnet 4. The primary reason to switch is Sonnet 4.5‚Äôs superior performance across critical, high-value domains. It is Anthropic‚Äôs most powerful model so far for building complex agents, demonstrating state-of-the-art performance in coding, reasoning, and computer use. Furthermore, its advanced agentic capabilities, such as extended autonomous operation and more effective use of parallel tool calls, enable the creation of more sophisticated AI workflows. 
Conclusion 
Amazon Bedrock global cross-Region inference for Anthropic‚Äôs Claude Sonnet 4.5 marks a significant evolution in AWS generative AI capabilities, enabling global routing of inference requests across the AWS worldwide infrastructure. With straightforward implementation and comprehensive monitoring through CloudTrail and CloudWatch, organizations can quickly use this powerful capability for their AI applications, high-volume workloads, and disaster recovery scenarios.We encourage you to try global cross-Region inference with Anthropic‚Äôs Claude Sonnet 4.5 in your own applications and experience the benefits firsthand. Start by updating your code to use the global inference profile ID, configure appropriate IAM permissions, and monitor your application‚Äôs performance as it uses the AWS global infrastructure to deliver enhanced resilience. 
For more information about global cross-Region inference for Anthropic‚Äôs Claude Sonnet 4.5 in Amazon Bedrock, refer to Increase throughput with cross-Region inference, Supported Regions and models for inference profiles, and Use an inference profile in model invocation. 
 
About the authors 
Melanie Li, PhD, is a Senior Generative AI Specialist Solutions Architect at AWS based in Sydney, Australia, where her focus is on working with customers to build solutions using state-of-the-art AI/ML tools. She has been actively involved in multiple generative AI initiatives across APJ, harnessing the power of LLMs. Prior to joining AWS, Dr. Li held data science roles in the financial and retail industries. 
Saurabh Trikande&nbsp;is a Senior Product Manager for Amazon Bedrock and Amazon SageMaker Inference. He is passionate about working with customers and partners, motivated by the goal of democratizing AI. He focuses on core challenges related to deploying complex AI applications, inference with multi-tenant models, cost optimizations, and making the deployment of generative AI models more accessible. In his spare time, Saurabh enjoys hiking, learning about innovative technologies, following TechCrunch, and spending time with his family. 
Derrick Choo&nbsp;is a Senior Solutions Architect at AWS who accelerates enterprise digital transformation through cloud adoption, AI/ML, and generative AI solutions. He specializes in full-stack development and ML, designing end-to-end solutions spanning frontend interfaces, IoT applications, data integrations, and ML models, with a particular focus on computer vision and multi-modal systems. 
Satveer Khurpa&nbsp;is a Sr. WW Specialist Solutions Architect, Amazon Bedrock at Amazon Web Services. In this role, he uses his expertise in cloud-based architectures to develop innovative generative AI solutions for clients across diverse industries. Satveer‚Äôs deep understanding of generative AI technologies allows him to design scalable, secure, and responsible applications that unlock new business opportunities and drive tangible value. 
Jared Dean is a Principal AI/ML Solutions Architect at AWS. Jared works with customers across industries to develop machine learning applications that improve efficiency. He is interested in all things AI, technology, and BBQ. 
Jan Catarata is a software engineer working on Amazon Bedrock, where he focuses on designing robust distributed systems. When he‚Äôs not building scalable AI solutions, you can find him strategizing his next move with friends and family at game night.
‚Ä¢ Secure ingress connectivity to Amazon Bedrock AgentCore Gateway using interface VPC endpoints
  Agentic AI applications represent a significant development in enterprise automation, where intelligent agents autonomously execute complex workflows, access sensitive datasets, and make real-time decisions across your organization‚Äôs infrastructure. Amazon Bedrock AgentCore accelerates enterprise AI transformation by providing fully managed services that remove infrastructure complexity, maintain session isolation, and enable seamless integration with enterprise tools so organizations can deploy trustworthy AI agents at scale. AgentCore Gateway, a modular service under AgentCore, simplifies integration by securely transforming APIs, AWS Lambda functions, and services into Model Context Protocol (MCP)-compatible tools and making them available to agents through a unified endpoint, with built-in authentication and serverless infrastructure that minimizes operational overhead. 
In production environments, AI agents are typically deployed within virtual private clouds (VPCs) to maintain secure, isolated network access and to meet enterprise security and compliance requirements. Amazon Web Services (AWS) interface VPC endpoints can enhance agentic AI security by creating private connections between VPC-hosted agents and AgentCore Gateway, keeping sensitive communications within the secure infrastructure of AWS. These endpoints use dedicated network interfaces with private IP addresses to deliver reduced latency and superior performance through direct connectivity. Additionally, VPC interface endpoints offer granular access control through endpoint policies, streamline operations by avoiding proxy server management, reduce data transfer costs, and establish the secure foundation that autonomous AI systems require when processing confidential data in regulated environments at enterprise scale. 
In this post, we demonstrate how to access AgentCore Gateway through a VPC interface endpoint from an Amazon Elastic Compute Cloud (Amazon EC2) instance in a VPC. We also show how to configure your VPC endpoint policy to provide secure access to the AgentCore Gateway while maintaining the principle of least privilege access. 
Architecture overview 
This architecture diagram illustrates a user accessing an application supported by backend agents deployed across various AWS compute services, including EC2 instances, Lambda functions, Amazon Elastic Kubernetes Service (Amazon EKS), or Amazon Elastic Container Service (Amazon ECS), all operating within a VPC environment. These agents communicate with AgentCore Gateway to discover, access, and invoke external tools and services that have been transformed into agent-compatible resources, such as enterprise APIs and Lambda functions. In the standard configuration, agent requests to AgentCore Gateway traverse the public internet. By implementing interface VPC endpoints, organizations can route these communications through the AWS secure internal network backbone instead, delivering significant benefits that can include enhanced security, reduced latency, and improved compliance alignment for regulated workloads that require strict network isolation and data protection standards. The solution follows this workflow: 
 
 AI agent interaction ‚Äì An agent running within the VPC obtains the required inbound authorization from identity providers, authenticates with Gateway, and sends a tool-use request (invokes the MCP tool) to the gateway through the interface VPC endpoint. 
 Gateway processing: Gateway manages OAuth authorization to make sure only valid users and agents can access tools and resources. The inbound request is authorized by Gateway. Converts agent requests using protocols like Model Context Protocol (MCP) into API requests and Lambda invocations 
 Secure access: The gateway handles credential injection for each tool, enabling agents to use tools with different authentication requirements seamlessly. It uses AgentCore Identity to securely access backend resources (the targets) on behalf of the agent. 
 Target execution: The gateway data plane invokes the target, which can be a Lambda function, an OpenAPI specification, or a Smithy model. 
 Monitoring: AgentCore Gateway provides built-in observability and auditing. Additionally, AWS PrivateLink publishes metrics to Amazon CloudWatch for monitoring interface endpoints. You can optionally enable VPC Flow Logs for logging IP traffic to AgentCore Gateway. 
 
 
Be aware of the following key considerations: 
 
 Private and public network communication ‚Äì The interface VPC endpoint enables secure communication for inbound traffic from agents to AgentCore Gateway through AWS PrivateLink, making sure this traffic remains within the private network. However, authentication workflows‚Äîincluding OAuth access token retrieval and credential exchange processes between agents and external Identity Provider systems for both inbound and outbound flows‚Äîand outbound access from the gateway to MCP tools continue to require internet connectivity for establishing secure sessions with identity systems and external resources hosted outside the AWS environment. 
 Data plane scope ‚Äì It‚Äôs important to understand that, currently, the interface VPC endpoint support is applicable only to the data plane endpoints of your gateway‚Äîthe runtime endpoints where your applications interact with agent tools. To clarify the distinction: although you can now access your gateway‚Äôs runtime endpoint through the interface VPC endpoint, the control plane operations, such as creating gateways, managing tools, and configuring security settings, must still be performed through the standard public AgentCore control plane endpoint (for example, bedrock-agentcore-control.&lt;region&gt;.amazonaws.com) 
 
Prerequisites 
To perform the solution, you need the following prerequisites: 
 
 An AWS account with appropriate AWS Identity and Access Management (IAM) permissions for VPC and Amazon Elastic Compute Cloud (Amazon EC2) management 
 Existing VPC setup with subnet configuration and route tables 
 AgentCore Gateway already provisioned and configured in your AWS account 
 Basic understanding of VPC networking concepts and security group configurations 
 
Solution walkthrough 
In the following sections, we demonstrate how to configure the interface VPC endpoint using the AWS Management Console and establish secure connectivity from a test EC2 instance within the VPC to AgentCore Gateway. 
Create a security group for the EC2 instance 
To create a security group for the EC2 instance, follow these steps, as shown in the following screenshot: 
 
 Navigate to the Amazon EC2 console in your preferred AWS Region and choose Security Groups in the navigation pane under Network &amp; Security. 
 Choose Create security group. 
 For Security group name, enter a descriptive name such as ec2-agent-sg. 
 For Description, enter a meaningful description such as Security group for EC2 instances running AI agents. 
 For VPC, choose your target VPC. 
 Add relevant Inbound rules for the EC2 instance management such as SSH (port 22) from your management network or bastion host. 
 Leave Outbound rules as default (allows all outbound traffic) to make sure agents can communicate with necessary services. 
 Choose Create security group. 
 
 
Create a security group for the interface VPC endpoint 
To create a security group for the interface VPC endpoint, follow these steps: 
Create a second security group named vpce-agentcore-sg that will be attached to the AgentCore Gateway interface VPC endpoint using similar steps to the preceding instructions and selecting the same VPC. For this security group, configure the following rules to enable secure and restricted access: 
 
 Inbound rules ‚Äì Allow HTTPS (port 443) for secure communication to the AgentCore Gateway 
 Source ‚Äì Select the EC2 security group (ec2-agent-sg) you created in the preceding section to allow traffic only from authorized agent instances 
 Outbound rules ‚Äì Leave as default (all traffic allowed) to support response traffic 
 
This security group configuration implements the principle of least privilege by making sure only EC2 instances with the agent security group can access the VPC endpoint while blocking unauthorized access from other resources in the VPC. These steps are illustrated by the following screenshot. 
 
Provision an EC2 instance within the VPC 
Provision an EC2 instance in the same VPC and select an appropriate Availability Zone for your workload requirements. Configure the instance with the network settings shown in the following list, making sure you select the same VPC and note the chosen subnet for VPC endpoint configuration: 
 
 VPC ‚Äì Select your target VPC 
 Subnet ‚Äì Choose a private subnet for enhanced security (note this subnet for VPC endpoint configuration) 
 Security group ‚Äì Attach the EC2 security group (ec2-agent-sg) you created in the previous steps 
 IAM role ‚Äì Configure an IAM role with necessary permissions for Amazon Bedrock and AgentCore Gateway access 
 Instance type ‚Äì Choose an appropriate instance type based on your agent workload requirements 
 
Remember the chosen subnet because you‚Äôll need to configure the VPC endpoint in the same subnet to facilitate optimal network routing and minimal latency. These configurations are shown in the following screenshot. 
 
Create an interface VPC endpoint 
Create an interface VPC endpoint using Amazon Virtual Private Cloud (Amazon VPC) that automatically uses AWS PrivateLink technology, enabling secure communication from your EC2 instance to AgentCore Gateway without traversing the public internet. Follow these steps: 
 
 Navigate to the Amazon VPC console and choose Endpoints in the navigation pane under the PrivateLink and Lattice section. 
 Choose Create endpoint. 
 For Name tag, enter a descriptive name (for example, vpce-agentcore-gateway). 
 For Service category, choose AWS services. 
 For Services, search for and choose com.amazonaws.&lt;region&gt;.bedrock-agentcore.gateway (replace &lt;region&gt; with your actual AWS Region). 
 
These settings are shown in the following screenshot. 
 
 
 Set the VPC to the same VPC you‚Äôve been working with throughout this setup. 
 Select Enable DNS name to allow access to the AgentCore Gateway using its default domain name, which simplifies application configuration and maintains compatibility with existing code. 
 Specify the subnet where the EC2 instance is running to maintain optimal network routing and minimal latency, as shown in the following screenshot. 
 
 
 
 Set the security group to the VPC endpoint security group (vpce-agentcore-sg) you created earlier to control access to the endpoint. 
 For initial testing, leave the policy set to Full access to allow agents within your VPC to communicate with AgentCore Gateway in your AWS account. In production environments, implement more restrictive policies based on the principle of least privilege. 
 
 
After you create the endpoint, it will take approximately 2‚Äì5 minutes to become available. You can monitor the status on the Amazon VPC console, and when it shows as Available, you can proceed with testing the connection. 
Test the connection 
Log in to the EC2 instance to perform following the tests. 
Check traffic flow over an interface VPC endpoint 
To confirm the traffic flow through the Amazon Bedrock AgentCore Gateway endpoint, check the IP address of the source resource that connects to the AgentCore Gateway endpoint. When you set up an interface VPC endpoint, AWS deploys an elastic network interface with a private IP address in the subnet. This deployment allows communication with AgentCore Gateway from resources within the Amazon VPC and on-premises resources that connect to the interface VPC endpoint through AWS Direct Connect or AWS Site-to-Site VPN. It also allows communication with resources in other Amazon VPC endpoints when you use centralized interface VPC endpoint architecture patterns. 
Check whether you turned on private DNS for the AgentCore Gateway endpoint. If you turn on private DNS, then AgentCore Gateway endpoints resolve to the private endpoint IP addresses. For AgentCore Gateway, enabling private DNS means your agents can continue using the standard gateway endpoint URL while benefiting from private network routing through the VPC endpoint. 
Before VPC interface endpoint, as shown in the following example, the DNS resolves to a public IP address for AgentCore Gateway endpoint: 
 
 nslookup gateway.bedrock-agentcoreamazonaws.com

Non-authoritative answer:
Name: gateway.bedrock-agentcore..amazonaws.com
Address: 52.86.152.150 
 
After VPC interface endpoint creation with private DNS resolution, as shown in the following example, the DNS resolves to private IP address from the CIDR range of the subnet of the VPC in which the VPC endpoint was created. 
 
 nslookup .gateway.bedrock-agentcore..amazonaws.com

Non-authoritative answer:
Name: .gateway.bedrock-agentcore..amazonaws.com
Address: 172.31.91.174 
 
When you select Enable DNS name for AgentCore Gateway VPC interface endpoints, by default AWS turns on the Enable private DNS only for inbound endpoints option. 
Private DNS enabled (cURL) (recommended) 
When private DNS is enabled, your applications can seamlessly use the standard gateway URL endpoint in the format https://{gateway-id}.gateway.bedrock-agentcore.{region}.amazonaws.com while traffic automatically routes through the VPC endpoint. 
The following is a sample cURL request to be executed from a resource within the VPC. The command sends a JSON-RPC POST request to retrieve available tools from the AgentCore Gateway: 
 
 curl -sS -i -X POST https://&lt;gatewayid&gt;.gateway.bedrock-agentcore.&lt;region&gt;.amazonaws.com/mcp \
--header 'Content-Type: application/json' \ 
--header "Authorization: Bearer $TOKEN" \ 
--data '{
"jsonrpc": "2.0",
"id": "'"$UNIQUE_ID"'",
"method": "tools/list",
"params": {}
}' 

 
 
This cURL command sends a JSON-RPC 2.0 POST request to the AgentCore Gateway MCP endpoint to retrieve a list of available tools. It uses bearer token authentication and includes response headers in the output, calling the tools/list method to discover what tools are accessible through the gateway. 
Private DNS disabled (Python) 
When Private DNS is disabled, you can‚Äôt access the gateway directly through the standard AgentCore Gateway endpoint. Instead, you must route traffic through the VPC DNS name shown in the following screenshot and include the original gateway domain name in the Host header. 
 
 
 curl -sS -i -X POST https://&lt;vpce-dns-name&gt;/mcp \
  --header 'Host: &lt;gatewayid&gt;.gateway.bedrock-agentcore.&lt;region&gt;.amazonaws.com \
  --header 'Content-Type: application/json' \
  --header "Authorization: Bearer $TOKEN" \
  --data '{
    "jsonrpc": "2.0",
    "id": "'$UNIQUE_ID'",
    "method": "tools/list",
    "params": {}
  }'
 
 
The following steps below walk through executing a Python script that uses the Host header: 
 
 Access your EC2 instance. Log in to your EC2 instance that has access to the VPC endpoint. 
 Configure the required environment variables for the connection: 
 GATEWAY_URL ‚Äì The VPC endpoint URL used to access the AgentCore Gateway through your private network connection 
 TOKEN ‚Äì Your authentication bearer token for accessing the gateway 
 GATEWAY_HOST ‚Äì The original AgentCore Gateway domain name that must be included in the Host header when Private DNS is disabled 
 
For example: 
 
 export GATEWAY_URL=https://&lt;vpce_id&gt;.gateway.bedrock-agentcore.ap-southeast-2.vpce.amazonaws.com/mcp
export TOKEN=&lt;your-token-here&gt;
export GATEWAY_HOST=&lt;gateway_id&gt;.gateway.bedrock-agentcore.ap-southeast-2.amazonaws.com
 
 
 
 Create and execute the test script. 
   
   Copy the following Python code into a file named agent.py. This code tests the AgentCore Gateway workflow by discovering available tools, creating a Strands Agent with the tools, and then testing both conversational interactions (tool listing and weather queries) and direct MCP tool calls. Copy the code: 
    
 
 
 from strands.models import BedrockModel
from mcp.client.streamable_http import streamablehttp_client
from strands.tools.mcp.mcp_client import MCPClient
from strands import Agent
import logging
import os

# Read authentication token and gateway URL from environment variables
token = os.getenv('TOKEN')
gatewayURL = os.getenv('GATEWAY_URL')  #vpc endpoint url
gatewayHost = os.getenv('GATEWAY_HOST') #domain name of the agentcore gateway

def create_streamable_http_transport():
    """Create HTTP transport with proper authentication headers"""
    return streamablehttp_client(
        gatewayURL, 
        headers={
            "Authorization": f"Bearer {token}",
            "Host":gatewayHost
        }
    )
# Initialize MCP client with the transport
client = MCPClient(create_streamable_http_transport)

# Configure Bedrock model - ensure IAM credentials in ~/.aws/credentials have Bedrock access
yourmodel = BedrockModel(
    model_id="amazon.nova-pro-v1:0",
    temperature=0.7,
)

# Configure logging for debugging and monitoring
logging.getLogger("strands").setLevel(logging.INFO)
logging.basicConfig(
    format="%(levelname)s | %(name)s | %(message)s",
    handlers=[logging.StreamHandler()]
)

# Test the complete agent workflow
with client:
    targetname = 'TestGatewayTarget36cb2ebf'
    
    # List available tools from the MCP server
    tools = client.list_tools_sync()
    
    # Create an Agent with the model and available tools
    agent = Agent(model=yourmodel, tools=tools)
    print(f"Tools loaded in the agent: {agent.tool_names}")
    
    # Test agent with a simple query to list available tools
    response1 = agent("Hi, can you list all tools available to you?")
    print(f"Agent response for tool listing: {response1}")
    
    # Test agent with a tool invocation request
    response2 = agent("Get the current weather for Seattle and show me the exact response from the tool")
    print(f"Agent response for weather query: {response2}")
    
    # Direct MCP tool invocation for validation
    result = client.call_tool_sync(
        tool_use_id="get-weather-seattle-call-1",  # Unique identifier for this call
        name=f"{targetname}___get_weather",  # Tool name format for Lambda targets
        arguments={"location": "Seattle"}
    )
    print(f"Direct MCP tool response: {result}") 
 
 
 Invoke the script using the following command: 
 
python3 agent.py 
Advanced configuration: VPC endpoint access policies 
A VPC endpoint policy is a resource-based policy that controls access to AWS services through the endpoint. Unlike identity-based policies, endpoint policies provide an additional layer of access control at the network level. You can configure access policies for AgentCore Gateway VPC endpoints with specific considerations.When creating endpoint policies for AgentCore Gateway, consider these key elements: 
 
 Principal configuration ‚Äì The Principal field can‚Äôt be modified because AgentCore Gateway doesn‚Äôt use IAM for authentication. Authentication is handled through bearer tokens rather than IAM principals. 
 Resource specification ‚Äì Clearly define the Resource field if you want to restrict access to specific gateway endpoints. Use the full Amazon Resource Name (ARN) format to target particular gateways within your account as shown in the following sample policy structure. 
 Action permissions ‚Äì For the Action field, avoid specifying control plane operations. Use a wildcard (*) to allow the necessary data plane operations for gateway functionality. 
 
Here is a sample policy structure: 
 
 {
"Version": "2012-10-17",
"Statement": [
{
"Principal": "*",
"Effect": "Allow",
"Action": "*",
"Resource": "arn:aws:bedrock-agentcore:&lt;region&gt;:&lt;AWS_Account_ID&gt;:gateway/&lt;gateway_id&gt;"
}
]
}
 
 
When the VPC endpoint policy blocks a request, you will see error responses such as: 
 
 {"jsonrpc":"2.0","id":2,"error":{"code":-32002,"message":"Authorization error - Insufficient permissions"}} 
 
Policy caching behavior 
AgentCore Gateway implements a caching mechanism for access policies that introduces a delay of up to 15 minutes before policy changes take effect. Although this caching significantly improves gateway performance, it means that policy modifications might not be immediately reflected in access controls. To work effectively with this behavior, you should allow at least 15 minutes for policy changes to fully propagate throughout the system after making updates. When possible, schedule policy modifications during planned maintenance windows to minimize operational impact. Always test policy changes in nonproduction environments before applying them to production gateways and factor in the caching delay when diagnosing access-related issues to avoid premature troubleshooting efforts. 
Advanced patterns 
In a shared gateway, multiple agents pattern, multiple agents from different services access a single centralized gateway through a shared VPC endpoint, simplifying network architecture while maintaining security through token-based authentication. This pattern is illustrated in the following diagram. 
 
In a multi-gateway, multi-agent pattern, which is shown in the following diagram, multiple agents across different applications access multiple specialized gateways through dedicated VPC endpoints, providing maximum security isolation with access control per gateway. 
 
In a cross-VPC gateway access pattern, shown in the following diagram, agents in multiple VPCs can access AgentCore Gateway through VPC peering or AWS Transit Gateway connections, allowing centralized gateway access across network boundaries while maintaining isolation. 
 
In a hybrid cloud gateway pattern, on-premises agents can access cloud-based gateways through VPC endpoints with private DNS disabled, enabling hybrid cloud deployments through Direct Connect or VPN connections. The following diagram illustrates this pattern. 
 
Clean up 
To avoid ongoing charges and maintain good resource hygiene, clean up your resources by completing the following steps in order:Delete the EC2 instance: 
 
 Navigate to the Amazon EC2 console and select your test instance 
 Choose Instance state and Stop instance, then wait for it to stop 
 Choose Instance state and Terminate instance to permanently delete the instance 
 
Delete the VPC endpoint: 
 
 Navigate to the Amazon VPC console and choose Endpoints 
 Select the VPC endpoint (vpce-agentcore-gateway) you created 
 Choose Actions and Delete VPC endpoints 
 Confirm the deletion 
 
Delete the security groups: 
 
 Navigate to the Amazon EC2 console and choose Security groups 
 Select the EC2 security group (ec2-agent-sg) you created 
 Choose Actions and Delete security groups 
 Repeat for the VPC endpoint security group (vpce-agentcore-sg) 
 
Conclusion 
In this post, we demonstrated how to establish secure, private connectivity between VPC-hosted resources and Amazon Bedrock AgentCore Gateway using VPC interface endpoints and AWS PrivateLink. This architecture delivers comprehensive benefits for enterprise agentic AI deployments by implementing networks that are isolated from the internet, providing enhanced security through dedicated private network paths. The solution implements a robust data perimeter through VPC endpoint policies, which create granular access controls that establish strict data boundaries around your AI resources. Additionally, the architecture enables private connectivity to Gateway endpoints for on-premises environments, supporting distributed AI architectures that span cloud and on-premises infrastructure. For organizations deploying autonomous AI systems at scale, implementing VPC interface endpoints creates the secure networking foundation necessary for efficient agent operations while delivering reduced latency through optimized network paths. This enterprise-grade approach helps enable your agentic AI applications to achieve improved performance and reduced response times while meeting security and compliance requirements. 
To learn more about implementing these patterns and best practices, visit the Amazon Bedrock documentation and AWS PrivateLink documentation for comprehensive guidance on AI deployments. 
 
About the authors 
Dhawal Patel is a Principal Machine Learning Architect at Amazon Web Services (AWS). He has worked with organizations ranging from large enterprises to midsized startups on problems related to distributed computing and AI. He focuses on deep learning, including natural language processing (NLP) and computer vision domains. He helps customers achieve high-performance model inference on Amazon SageMaker. 
Sindhura Palakodety is a Senior Solutions Architect at Amazon Web Services (AWS) and Single-Threaded Leader (STL) for ISV Generative AI, where she is dedicated to empowering customers in developing enterprise-scale, Well-Architected solutions. She specializes in generative AI and data analytics domains, enabling organizations to leverage innovative technologies for transformative business outcomes. 
Thomas Mathew Veppumthara is a Sr. Software Engineer at Amazon Web Services (AWS) with Amazon Bedrock AgentCore. He has previous generative AI leadership experience in Amazon Bedrock Agents and nearly a decade of distributed systems expertise across Amazon eCommerce Services and Amazon Elastic Block Store (Amazon EBS). He holds multiple patents in distributed systems, storage, and generative AI technologies. 
June Won is a Principal Product Manager with Amazon SageMaker JumpStart. He focuses on making foundation models (FMs) easily discoverable and usable to help customers build generative AI applications. His experience at Amazon also includes mobile shopping applications and last-mile delivery.
‚Ä¢ Enhance agentic workflows with enterprise search using Kore.ai and Amazon Q Business
  This post was written with Meghana Chintalapudi and Surabhi Sankhla of Kore.ai. 
As organizations struggle with exponentially growing volumes of data distributed across multiple repositories and applications, employees lose significant time‚Äîapproximately 30% according to the International Data Corporation (IDC)‚Äîsearching for information that could be spent on higher-value work. The complexity of modern enterprise data networks demands solutions that can efficiently integrate, process, and deliver actionable insights across disparate systems. 
In this post, we demonstrate how organizations can enhance their employee productivity by integrating Kore.ai‚Äôs AI for Work platform with Amazon Q Business. We show how to configure AI for Work as a data accessor for Amazon Q index for independent software vendors (ISVs), so employees can search enterprise knowledge and execute end-to-end agentic workflows involving search, reasoning, actions, and content generation. We explore the key benefits of this integration, including advanced search capabilities across more than 90 enterprise connectors and how to extend agentic experiences on top of a search foundation. The post includes a step-by-step implementation guide to help you set up this integration in your environment. 
Components of the integration 
Kore.ai is a leading Enterprise AI platform consistently recognized by Gartner as a leader in conversational AI. With three key Kore.ai offerings, AI for Work, AI for Process, and AI for Service, enterprises can build and deploy AI solutions based on their business needs. The AI for Work platform helps employees be more productive by making it possible to search across applications, take context-aware actions, generate content, and automate repetitive tasks. The platform goes beyond standalone search to deliver comprehensive agentic orchestration and workflows, helping employees follow up with clients, send weekly updates, or research and write marketing content with a single command. With AI for Work, your employees can create simple no-code agents while your admins have the flexibility to create more advanced low-code or pro-code agents. AI for Process, on the other hand, automates knowledge-intensive business processes end-to-end. AI for Service helps organizations deliver differentiated customer service experiences through self-service, proactive outreach campaigns, and agent assistance. 
Amazon Q index for ISVs is a powerful, managed vector search service that supports seamless integration of generative AI applications with customers‚Äô enterprise data through a unified, secure index. ISVs can access and retrieve relevant content through the SearchRelevantContent API for cross-application data retrieval without needing direct access or individual indexing of each data source, while customers retain full control over data access and governance. 
When combined with additional search connectors offered by AI for Work platform and its ability to create and orchestrate agents, organizations gain a complete solution that transforms how employees access enterprise data and execute tasks end-to-end. The following video shows one such agentic experience in action, where the AI for Work interface seamlessly orchestrates agents to help a sales executive prepare for a client meeting‚Äîcompiling information from Amazon Q index and AI for Work connectors, summarizing talking points, and sending them as an email, all from a single query. 

 
  
 
 
Benefits for enterprises 
Enterprises often struggle with fragmented data access and repetitive manual tasks that slow down critical business processes. For example, imagine a scenario where a product manager needs to compile quarterly feature requests‚Äîwith the integration of Kore.ai‚Äôs AI for Work and Amazon Q index, they can instantly gather requests from Salesforce, support tickets, and JIRA; automatically generate a structured roadmap; and schedule stakeholder meetings, all with a single query. This seamless integration changes the way enterprises interact with enterprise systems, through multiple key advantages: 
 
 Improved search capabilities ‚Äì Amazon Q index augments the generative AI experience by providing semantically relevant enterprise content across connected systems through its distributed vector database, delivering query responses at enterprise scale. Now, together with AI for Work, your employees can search data from over 90 connectors, integrating with enterprise systems like Microsoft 365, Salesforce, and Workday while also connecting with custom internal knowledge systems and third-party search providers. AI for Work‚Äôs orchestrator manages complex query processing and agent routing across multiple data sources, resulting in contextually appropriate and actionable results that significantly reduce search time while also enabling intelligent automations that extend far beyond traditional search capabilities. 
 Enhanced data processing ‚Äì The system continuously ingests and analyzes data through the document processing pipeline in Amazon Q index, which automatically handles multiple formats using intelligent chunking algorithms that preserve semantic context. The AI for Work platform unifies search, content generation, and actions in a single interface, to support the creation of multi-step agentic experiences grounded in search. Through real-time incremental indexing that processes only changed content, the system maintains data freshness while converting siloed raw data into actionable insights and multi-step business processes that can be saved and reused across the organization. 
 Cost optimization ‚Äì Organizations can achieve significant cost savings by streamlining routine tasks through agents that reduce operational overhead and improve resource allocation. AI for Work supports a wide range of agent-building options, from no-code and low-code to pro-code, for both non-technical employees and technical experts to build agents for themselves and to share across the organization, so teams can accomplish more with existing resources and benefit from sustained productivity improvements. 
 Security benefits ‚Äì Security remains paramount, with Amazon Q index implementing vector-level security through end-to-end encryption using AWS Key Management Service (AWS KMS) customer managed keys and document-level access controls that filter search results based on user identity and group membership. The joint solution implements robust role-based access control and audit trails. This zero-trust security approach maintains compliance with industry standards while providing granular control over sensitive enterprise data, making sure users only see information from documents they have explicit permissions to access while maintaining complete data sovereignty. With AI for Work‚Äôs robust security and governance tools enterprises can manage permissions and agent access, monitor usage, and enforce guardrails for secure, enterprise-wide deployment of AI solutions at scale. 
 
Solution overview 
The Amazon Q Business data accessor provides a secure interface that integrates Kore.ai‚Äôs AI for Work platform with Amazon Q index. The integration delivers a robust solution that uses enterprise data across multiple systems to power intelligent agentic actions and content generation capabilities that transform how organizations handle routine tasks and automate complex processes end-to-end. 
When a user submits a query through AI for Work, its orchestrator intelligently routes requests between Kore.ai‚Äôs native retrievers and Amazon Q index based on predefined routing rules and advanced intent recognition algorithms. For Amazon Q index requests, the architecture implements secure cross-account API calls using OAuth 2.0 tokens that transform into temporary AWS credentials, supporting both security and optimal performance while maintaining strict access controls throughout the entire system. With AI for Work‚Äôs agents, users can take follow up actions, such as drafting proposals or submitting tickets‚Äîdirectly on top of search results, for end-to-end task completion in a single interface. Users can also build personalized workflows of pre-defined steps and execute them from a single query to further save time. 
This supports use cases such as automated roadmap generation, where a product manager can query feature requests across multiple systems and receive a structured roadmap complete with stakeholder notifications, or RFP response automation, where sales executives can generate comprehensive proposals by pulling compliance documentation and tailoring responses based on client requirements. 
The following diagram illustrates the solution architecture. 
 
Prerequisites 
Before enabling the Amazon Q index integration with Kore.ai‚Äôs AI for Work, you must have the following components in place: 
 
 An AWS account with appropriate service access 
 Amazon Q Business set up with AWS IAM Identity Center for user authentication 
 Access to Kore.ai‚Äôs AI for Work (as a workspace admin) 
 
With these prerequisites met, you can complete the basic configuration steps on both the Amazon Q Business and Kore.ai consoles to get started. 
Add Kore.ai as a data accessor 
After creating an Amazon Q Business application with AWS IAM Identity Center, administrators can configure Kore.ai as a data accessor through the Amazon Q Business console. Complete the following steps: 
 
 On the Amazon Q Business console, choose Data accessors in the navigation pane. 
 Choose Add data accessor. 
 Choose Kore.ai as your data accessor. You must retrieve tenantID, a unique identifier for your application tenant. Refer to Prerequisites for instructions to retrieve the TenantId for your application. Similar instructions are also listed later in this post. 
 For Data source access, configure your level of access. You can select specific data sources from your Amazon Q index to be available through the data accessor. This makes it possible to control which content is surfaced in the AI for Work environment. 
 For User access, specify which users or groups can access the Amazon Q index through the data accessor. This option makes it possible to configure granular permissions for data accessor accessibility and manage organizational access controls. 
 
 
After you have added the data accessor, the Amazon Q Business console displays configuration details that you need to share with Kore.ai to complete the setup. 
 
 Note down the following information for the next step: 
   
   Amazon Q Business application ID 
   AWS Region of the Amazon Q Business application 
   Amazon Q Business retriever ID 
   Region for IAM Identity Center instance 
    
 
Configure Amazon Q index in Kore.ai‚Äôs AI for Work 
Kore.ai‚Äôs AI for Work supports flexible integration with Amazon Q index based on your enterprise search needs. There are two configuration options: configuring Amazon Q index as the primary enterprise knowledge source or configuring it as a search agent. We provide instructions for both options in this post. 
Option 1: Configure Amazon Q index as the primary enterprise knowledge source 
If you want Amazon Q index to act as the primary fallback search layer, coming into play, complete the following steps: 
 
 In AI for Work, go to Workspaces on the admin console. Then navigate to Enterprise Workspace, which is the default workspace. 
 
 
 
 Choose Configure to configure an enterprise knowledge data source. 
 On the Create New dropdown menu, choose Amazon Q. 
 
 
 
 Enter a source name and brief description. 
 Copy the tenant ID displayed‚Äîthis is required during the setup of the data accessor in AWS, as described in the previous section. 
 Enter the details captured earlier: 
   
   Amazon Q Business application ID 
   Region of the Amazon Q Business application 
   Amazon Q Business retriever ID 
   Region for IAM Identity Center instance 
    
 Choose Continue to save and complete the configuration. 
 
 
The new knowledge source now shows as Active. 
 
Option 2: Configure Amazon Q index as a search agent 
If you already have a primary search index, you can configure Amazon Q index as a search agent: 
 
 In AI for Work, go to Workspaces on the admin console. 
 Choose the workspace where you want to add Amazon Q index. (Enterprise Workspace is used by default). 
 Under AI Agents in the navigation pane, choose Search Agent 
 Choose Create agent. 
 
 
 
 Provide an agent name and purpose. This helps define when the search agent should be invoked. 
 Choose Continue to move to configuration. 
 For Select Search Index, choose Amazon Q. 
 
 
 
 Copy the tenant ID displayed‚Äîit is required during the setup of the data accessor in AWS. 
 
 
 
 Preview and test the agent. 
 After you have validated the agent, publish it to selected users or groups. 
 
Your integration is now complete. You can now access the assistant application and start asking questions in the AI for Work console. If you‚Äôve created a search agent, you can also access it from the list of agents and start interacting with it directly. 
Clean up 
When you are finished using this solution, clean up your resources to avoid additional costs: 
 
 Disable the Amazon Q index configuration within AI for Work‚Äôs settings. 
 Delete the Kore.ai data accessor from the Amazon Q Business console, which will remove permissions and access for users. 
 Delete the Amazon Q Business application to remove the associated index and data source connectors, on your AWS account. 
 
Conclusion 
The combination of Kore.ai‚Äôs AI for Work and Amazon Q index offers enterprises a transformative approach to boost employee productivity leveraging comprehensive search capabilities while streamlining repetitive tasks and processes. By integrating Kore.ai‚Äôs advanced agentic platform with the robust search infrastructure of Amazon Q index, organizations can now execute context aware actions by accessing relevant information across disparate systems while maintaining data ownership and security. This supports faster problem-solving, enhanced productivity, and better collaboration across the organization. 
In this post, we explored how enterprises can use the integration between Kore.ai‚Äôs AI for Work and Amazon Q Business to streamline their operational processes and unlock valuable productivity gains. We demonstrated how organizations can set up this integration using an Amazon Q data accessor, helping teams access critical information securely and cost-effectively. 
Unlock the full potential of your organization‚Äôs data and agentic workflows today with the Amazon Q index and Kore.ai‚Äôs AI for Work‚Äôs unified solution by following the steps in Amazon Q integration with AI for Work. 
 
About the authors 
Siddhant Gupta is a Software Development Manager on the Amazon Q team based in Seattle, WA. He is driving innovation and development in cutting-edge AI-powered solutions. 
Chinmayee Rane is a Generative AI Specialist Solutions Architect at AWS, with a core focus on generative AI. She helps ISVs accelerate the adoption of generative AI by designing scalable and impactful solutions. With a strong background in applied mathematics and machine learning, she specializes in intelligent document processing and AI-driven innovation. Outside of work, she enjoys salsa and bachata dancing. 
Bobby Williams is a Senior Solutions Architect at AWS. He has decades of experience designing, building, and supporting enterprise software solutions that scale globally. He works on solutions across industry verticals and horizontals and is driven to create a delightful experience for every customer. 
Santhosh Urukonda is a Senior PACE (Prototyping &amp; Cloud Engineering) Architect at AWSs with two decades of experience. He specializes in helping customers develop innovative, first-to-market solutions with a focus on generative AI. 
Nikhil Kumar Goddeti is a Cloud Support Engineer II at AWS. He specializes in AWS Data Analytics services with emphasis on Amazon OpenSearch Service, Amazon Q Business, Amazon Kinesis, Amazon MSK, Amazon AppFlow, and Amazon Kendra. He is a Subject Matter Expert of OpenSearch. Outside of work, he enjoys travelling with his friends and playing cricket. 
Meghana Chintalapudi is a Product Manager at Kore.ai, driving the development of search and agentic AI solutions for the AI for Work platform. She has led large-scale AI implementations for Fortune 500 clients, evolving from deterministic NLP and intent-detection models to advanced large language model deployments, with a strong emphasis on enterprise-grade security and scalability. Outside of work, Meghana is a dancer and takes movement workshops in Hyderabad, India. 
Surabhi Sankhla is a VP of Product at Kore.ai, where she leads the AI for Work platform to help enterprises boost employee productivity. With over 13 years of experience in product management and technology, she has launched AI products from the ground up and scaled them to millions of users. At Kore.ai, she drives product strategy, client implementations, and go-to-market execution in partnership with cross-functional teams. Based in San Francisco, Surabhi is passionate about making AI accessible and impactful for all.
‚Ä¢ Accelerate development with the Amazon Bedrock AgentCore MCP server
  Today, we‚Äôre excited to announce the Amazon Bedrock AgentCore Model Context Protocol (MCP) Server. With built-in support for runtime, gateway integration, identity management, and agent memory, the AgentCore MCP Server is purpose-built to speed up creation of components compatible with Bedrock AgentCore. You can use the AgentCore MCP server for rapid prototyping, production AI solutions, or to scale your agent infrastructure for your enterprise. 
Agentic IDEs like Kiro, Amazon Q Developer for CLI, Claude Code, GitHub Copilot, and Cursor, along with sophisticated MCP servers are transforming how developers build AI agents. What typically takes significant time and effort, for example learning about Bedrock AgentCore services, integrating Runtime and Tools Gateway, managing security configurations, and deploying to production can now be completed in minutes through conversational commands with your coding assistant. 
In this post we introduce the new AgentCore MCP server and walk through the installation steps so you can get started. 
AgentCore MCP server capabilities 
The AgentCore MCP server brings a new agentic development experience to AWS, providing specialized tools that automate the complete agent lifecycle, eliminate the steep learning curve, and reduce development friction that can slow innovation cycles.&nbsp;To address specific agent development challenges the AgentCore MCP server: 
 
 Transforms agents for AgentCore Runtime integration by providing guidance to your coding assistant on the minimum functionality changes needed‚Äîadding Runtime library imports, updating dependencies, initializing apps with BedrockAgentCoreApp(), converting entrypoints to decorators, and changing direct agent calls to payload handling‚Äîwhile preserving your existing agent logic and Strands Agents features. 
 Automates development environment provisioning by handling the complete setup process through your coding assistant: installing required dependencies (bedrock-agentcore SDK, bedrock-agentcore-starter-toolkit CLI helpers, strands-agents SDK), configuring AWS credentials and AWS Regions, defining execution roles with Bedrock AgentCore permissions, setting up ECR repositories, and creating .bedrock_agentcore.yaml configuration files. 
 Simplifies tool integration with Bedrock AgentCore Gateway for seamless agent-to-tool communication in the cloud environment. 
 Enables simple agent invocation and testing by providing natural language commands through your coding assistant to invoke provisioned agents on AgentCore Runtime and verify the complete workflow, including calls to AgentCore Gateway tools when applicable. 
 
Layered approach 
When using the AgentCore MCP server with your favorite client, we encourage you to consider a layered architecture designed to provide comprehensive AI agent development support: 
 
 Layer 1: Agentic IDE or client ‚Äì Use Kiro, Amazon Q Developer for CLI, Claude Code, Cursor, VS Code extensions, or another natural language interface for developers. For very simple tasks, agentic IDEs are equipped with the right tools to look up documentation and perform tasks specific to Bedrock AgentCore. However, with this layer alone, developers may observe sub-optimal performance across AgentCore developer paths. 
 Layer 2: AWS service documentation ‚Äì Install the AWS Documentation MCP Server&nbsp;for comprehensive AWS service documentation, including context about Bedrock AgentCore. 
 Layer 3: Framework documentation ‚Äì Install the Strands, LangGraph, or other framework docs MCP servers or use the llms.txt for framework-specific context. 
 Layer 4: SDK documentation ‚Äì Install the MCP or use the llms.txt for the Agent Framework SDK and Bedrock AgentCore SDK for a combined documentation layer that covers the Strands Agents SDK documentation and Bedrock AgentCore API references. 
 Layer 5: Steering files ‚Äì Task-specific guidance for more complex and repeated workflows. Each IDE has a different approach to using steering files (for example, see Steering in the Kiro documentation). 
 
Each layer builds upon the previous one, providing increasingly specific context so your coding assistant can handle everything from basic AWS operations to complex agent transformations and deployments. 
Installation 
To get started with the Amazon Bedrock AgentCore MCP server you can use the one-click install on the Github repository. 
Each IDE integrates with an MCP differently using the mcp.json file. Review the MCP documentation for your IDE, such as Kiro, Cursor, Amazon Q Developer for CLI, and Claude Code to determine the location of the mcp.json. 
 
  
   
   Client 
   Location of mcp.json 
   Documentation 
   
   
   Kiro 
   .kiro/settings/mcp.json 
   https://kiro.dev/docs/mcp/ 
   
   
   Cursor 
   .cursor/mcp.json 
   https://cursor.com/docs/context/mcp 
   
   
   Amazon Q Developer for CLI 
   ~/.aws/amazonq/mcp.json 
   https://docs.aws.amazon.com/amazonq/latest/qdeveloper-ug/qdev-mcp.html 
   
   
   Claude Code 
   ~/.claude/mcp.json 
   https://docs.claude.com/en/docs/claude-code/mcp 
   
  
 
Use the following in your mcp.json: 
 
 {
&nbsp;&nbsp;"mcpServers": {
&nbsp;&nbsp; &nbsp;"awslabs.amazon-bedrock-agentcore-mcp-server": {
&nbsp;&nbsp; &nbsp; &nbsp;"command": "uvx",
&nbsp;&nbsp; &nbsp; &nbsp;"args": ["awslabs.amazon-bedrock-agentcore-mcp-server@latest"],
&nbsp;&nbsp; &nbsp; &nbsp;"env": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"FASTMCP_LOG_LEVEL": "ERROR"
&nbsp;&nbsp; &nbsp; &nbsp;},
&nbsp;&nbsp; &nbsp; &nbsp;"disabled": false,
&nbsp;&nbsp; &nbsp; &nbsp;"autoApprove": []
&nbsp;&nbsp; &nbsp;}
&nbsp;&nbsp;}
} 
 
For example, here is what the IDE looks like on Kiro, with the AgentCore MCP server and the two tools, search_agentcore_docs and fetch_agentcore_doc, connected: 
 
Using the AgentCore MCP server for agent development 
While we show demos for various use cases below using the Kiro IDE, the AgentCore MCP server has also been tested to work on Claude Code, Amazon Q CLI, Cursor, and the VS Code Q plugin. First, let‚Äôs take a look at a typical agent development lifecycle using AgentCore services (remember that this is only one example with the tools available, and you are free to explore more such use cases simply by instructing the agent in your favorite Agentic IDE): 
 
The agent development lifecycle follows these steps: 
 
 The user takes a local set of tools or MCP servers and 
   
   Creates a lambda target for AgentCore Gateway; or 
   Deploys the MCP server as-is on AgentCore Runtime 
    
 The user prepares the actual agent code using a preferred framework like Strands Agents or LangGraph. The user can either: 
   
   Start from scratch (the server can fetch docs from the Strands Agents or LangGraph documentation) 
   Start from fully or partially working agent code 
    
 The user asks the agent to transform the code into a format compatible with AgentCore Runtime with the intention to deploy the agent later. This causes the agent to: 
   
   Write an appropriate requirements.txt&nbsp;file 
   import necessary libraries including bedrock_agentcore 
   decorate the main handler (or create one) to access the core agent calling logic or input handler 
    
 The user may then ask the agent to deploy to AgentCore Runtime. The agent can look up documentation and can use the AgentCore CLI to deploy the agent code to Runtime 
 The user can test the agent by asking the agent to do so. The AgentCore CLI command required for this is written and executed by the client 
 The user then asks to modify the code to use the deployed AgentCore Gateway MCP server within this AgentCore Runtime agent. 
   
   The agent modifies the original code to add an MCP client that can call the deployed gateway 
   The agent then deploys a new version v2 of the agent to Runtime 
   The agent then tests this integration with a new prompt 
    
 
Here is a demo of the MCP server working with Cursor IDE. We see the agent perform the following steps: 
 
 Transform the weather_agent.py to be compatible with AgentCore runtime 
 Use the AgentCore CLI to deploy the agent 
 Test the deployed agent with a successful prompt 
 
 

 
  
 

 
Here‚Äôs another example of deploying a LangGraph agent to AgentCore Runtime with the Cursor IDE performing similar steps as seen above. 

 
  
 
 
Clean up 
If you‚Äôd like to uninstall the MCP server, follow the MCP documentation for your IDE, such as Kiro, Amazon Q Developer for CLI, Cursor, and Claude Code for instructions. 
Conclusion 
In this post, we showed how you can use the AgentCore MCP server with your favorite Agentic IDE of choice to speed up your development workflows. 
We encourage you to review the Github repository, as well read through and use the following resources in your development: 
 
 Amazon Bedrock AgentCore CLI documentation 
 Strands Agents MCP Server 
 LangGraph llms.txt 
 
We encourage you to try out the AgentCore MCP server and provide any feedback through issues in our GitHub repository. 
 
About the authors 
 
  
  
   
   
  Shreyas Subramanian 
  Shreyas is a Principal Data Scientist and helps customers by using Generative AI to solve their business challenges using the AWS platform. Shreyas has a background in large scale optimization and Deep Learning, and he is a researcher studying the use of Machine Learning and Reinforcement Learning for accelerating learning and optimization tasks. Shreyas is also an Amazon best-selling book author with several research papers and patents to his name. 
  
  
  
   
   
  Primo Mu 
  Primo is a Software Development Engineer on the Agentic AI Foundation team at AWS, where he builds foundational systems and infrastructure that power intelligent AI applications. He has extensive experience working on backend stateless orchestration services behind products like Kiro and Q Dev CLI. He focuses on creating scalable frameworks and robust architectures that enable developers to build sophisticated agentic systems.
‚Ä¢ How Hapag-Lloyd improved schedule reliability with ML-powered vessel schedule predictions using Amazon SageMaker
  This post is cowritten with Thomas Voss and Bernhard Hersberger from Hapag-Lloyd. 
Hapag-Lloyd is one of the world‚Äôs leading shipping companies with more than 308 modern vessels, 11.9 million TEUs (twenty-foot equivalent units) transported per year, and 16,700 motivated employees in more than 400 offices in 139 countries. They connect continents, businesses, and people through reliable container transportation services on the major trade routes across the globe. 
In this post, we share how Hapag-Lloyd developed and implemented a machine learning (ML)-powered assistant predicting vessel arrival and departure times that revolutionizes their schedule planning. By using Amazon SageMaker AI and implementing robust MLOps practices, Hapag-Lloyd has enhanced its schedule reliability‚Äîa key performance indicator in the industry and quality promise to their customers. 
For Hapag-Lloyd, accurate vessel schedule predictions are crucial for maintaining schedule reliability, where schedule reliability is defined as percentage of vessels arriving within 1 calendar day (earlier or later) of their estimated arrival time, communicated around 3 to 4 weeks before arrival. 
Prior to developing the new ML solution, Hapag-Lloyd relied on simple rule-based and statistical calculations, based on historical transit patterns for vessel schedule predictions. While this statistical method provided basic predictions, it couldn‚Äôt effectively account for real-time conditions such as port congestion, requiring significant manual intervention from operations teams. 
Developing a new ML solution to replace the existing system presented several key challenges: 
 
 Dynamic shipping conditions ‚Äì The estimated time of arrival (ETA) prediction model needs to account for numerous variables that affect journey duration, including weather conditions, port-related delays such as congestion, labor strikes, and unexpected events that force route changes. For example, when the Suez Canal was blocked by the Ever Given container ship in March 2021, vessels had to be rerouted around Africa, adding approximately 10 days to their journey times. 
 Data integration at scale ‚Äì The development of accurate models requires integration of large volumes of historical voyage data with external real-time data sources including port congestion information and vessel position tracking (AIS). The solution needs to scale across 120 vessel services or lines and 1,200 unique port-to-port routes. 
 Robust MLOps infrastructure ‚Äì A robust MLOps infrastructure is required to continuously monitor model performance and quickly deploy updates whenever needed. This includes capabilities for regular model retraining to adapt to changing patterns, comprehensive performance monitoring, and maintaining real-time inference capabilities for immediate schedule adjustments. 
 
Hapag-Llyod‚Äôs previous approach to schedule planning couldn‚Äôt effectively address these challenges. A comprehensive solution that could handle both the complexity of vessel schedule prediction and provide the infrastructure needed to sustain ML operations at global scale was needed. 
The Hapag-Lloyd network consists of over 308 vessels and many more partner vessels that continuously circumnavigate the globe on predefined service routes, resulting in more than 3,500 port arrivals per month. Each vessel operates on a fixed service line, making regular round trips between a sequence of ports. For instance, a vessel might repeatedly sail a route from Southampton to Le Havre, Rotterdam, Hamburg, New York, and Philadelphia before starting the cycle again. For each port arrival, an ETA must be provided multiple weeks in advance to arrange critical logistics, including berth windows at ports and onward transportation of containers by sea, land or air transport. The following table shows an example where a vessel travels from Southampton to New York through Le Havre, Rotterdam, and Hamburg. The vessel‚Äôs time until arrival at the New York port can be calculated as the sum of ocean to port time to Southampton, and the respective berth times and port-to-port times for the intermediate ports called while sailing to New York. If this vessel encounters a delay in Rotterdam, it affects its arrival in Hamburg and cascades through the entire schedule, impacting arrivals in New York and beyond as shown in the following table. This ripple effect can disrupt carefully planned transshipment connections and require extensive replanning of downstream operations. 
 
  
   
   Port 
   Terminal call 
   Scheduled arrival 
   Scheduled departure 
   
   
   SOUTHAMPTON 
   1 
   2025-07-29 07:00 
   2025-07-29 21:00 
   
   
   LE HAVRE 
   2 
   2025-07-30 16:00 
   2025-07-31 16:00 
   
   
   ROTTERDAM 
   3 
   2025-08-03 18:00 
   2025-08-05 03:00 
   
   
   HAMBURG 
   4 
   2025-08-07 07:00 
   2025-08-08 07:00 
   
   
   NEW YORK 
   5 
   2025-08-18 13:00 
   2025-08-21 13:00 
   
   
   PHILADELPHIA 
   6 
   2025-08-22 06:00 
   2025-08-24 16:30 
   
   
   SOUTHAMPTON 
   7 
   2025-09-01 08:00 
   2025-09-02 20:00 
   
  
 
When a vessel departs Rotterdam with a delay, new ETAs must be calculated for the remaining ports. For Hamburg, we only need to estimate the remaining sailing time from the vessel‚Äôs current position. However, for subsequent ports like New York, the prediction requires multiple components: the remaining sailing time to Hamburg, the duration of port operations in Hamburg, and the sailing time from Hamburg to New York. 
Solution overview 
As an input to the vessel ETA prediction, we process the following two data sources: 
 
 Hapag-Lloyd‚Äôs internal data, which is stored in a data lake. This includes detailed vessel schedules and routes, port and terminal performance information, real-time port congestion and waiting times, and vessel characteristics datasets. This data is prepared for model training using AWS Glue jobs. 
 Automatic Identification System (AIS) data, which provides streaming updates on the vessel movements. This AIS data ingestion is batched every 20 minutes using AWS Lambda and includes crucial information such as latitude, longitude, speed, and direction of vessels. New batches are processed using AWS Glue and Iceberg to update the existing AIS database‚Äîcurrently holding around 35 million observations. 
 
These data sources are combined to create training datasets for the ML models. We carefully consider the timing of available data through temporal splitting to avoid data leakage. Data leakage occurs when using information that wouldn‚Äôt be available at prediction time in the real world. For example, when training a model to predict arrival time in Hamburg for a vessel currently in Rotterdam, we can‚Äôt use actual transit times that were only known after the vessel reached Hamburg. 
A vessel‚Äôs journey can be divided into different legs, which led us to develop a multi-step solution using specialized ML models for each leg, which are orchestrated as hierarchical models to retrieve the overall ETA: 
 
 The Ocean to Port (O2P) model predicts the time needed for a vessel to reach its next port from its current position at sea. The model uses features such as remaining distance to destination, vessel speed, journey progress metrics, port congestion data, and historical sea leg durations. 
 The Port to Port (P2P) model forecasts sailing time between any two ports for a given date, considering key features such as ocean distance between ports, recent transit time trends, weather, and seasonal patterns. 
 The Berth Time model estimates how long a vessel will spend at port. The model uses vessel characteristics (such as tonnage and load capacity), planned container load, and historical port performance. 
 The Combined model takes as input the predictions from the O2P, P2P, and Berth Time models, along with the original schedule. Rather than predicting absolute arrival times, it computes the expected deviation from the original schedule by learning patterns in historical prediction accuracy and specific voyage conditions. These computed deviations are then used to update ETAs for the upcoming ports in a vessel‚Äôs schedule. 
 
 
All four models are trained using the XGBoost algorithm built into SageMaker, chosen for its ability to handle complex relationships in tabular data and its robust performance with mixed numerical and categorical features. Each model has a dedicated training pipeline in SageMaker Pipelines, handling data preprocessing steps and model training. The following diagram shows the data processing pipeline, which generates the input datasets for ML training. 
 
As an example, this diagram shows the training pipeline of the Berth model. The steps in the SageMaker training pipelines of the Berth, P2P, O2P, and Combined models are identical. Therefore, the training pipeline is implemented once as a blueprint and re-used across the other models, enabling a fast turn-around time of the implementation. 
 
Because the Combined model depends on outputs from the other three specialized models, we use AWS Step Functions to orchestrate the SageMaker pipelines for training. This helps ensure that the individual models are updated in the correct sequence and maintains prediction consistency across the system. The orchestration of the training pipelines is shown in the following pipeline architecture. 
 The individual workflow begins with a data processing pipeline that prepares the input data (vessel schedules, AIS data, port congestion, and port performance metrics) and splits it into dedicated datasets. This feeds into three parallel SageMaker training pipelines for our base models (O2P, P2P, and Berth), each following a standardized process of feature encoding, hyperparameter optimization, model evaluation, and registration using SageMaker Processing and hyperparameter turning jobs and SageMaker Model Registry. After training, each base model runs a SageMaker batch transform job to generate predictions that serve as input features for the combined model training. The performance of the latest Combined model version is tested on the last 3 months of data with known ETAs, and performance metrics (R¬≤, mean absolute error (MAE)) are computed. If the model‚Äôs performance is below a set MAE threshold, the entire training process fails and the model version is automatically discarded, preventing the deployment of models that don‚Äôt meet the minimum performance threshold. 
All four models are versioned and stored as separate model package groups in the SageMaker Model Registry, enabling systematic version control and deployment. This orchestrated approach helps ensure that our models are trained in the correct sequence using parallel processing, resulting in an efficient and maintainable training process.The hierarchical model approach helps further ensure that a degree of explainability comparable to the current statistical and rule-based solution is maintained‚Äîavoiding ML black box behavior. For example, it becomes possible to highlight unusually long berthing time predictions when discussing predictions results with business experts. This helps increase transparency and build trust, which in turn increases acceptance within the company. 
Inference solution walkthrough 
The inference infrastructure implements a hybrid approach combining batch processing with real-time API capabilities as shown in Figure 5. Because most data sources update daily and require extensive preprocessing, the core predictions are generated through nightly batch inference runs. These pre-computed predictions are complemented by a real-time API that implements business logic for schedule changes and ETA updates. 
 
 Daily batch Inference: 
   
   Amazon EventBridge triggers a Step Functions workflow every day. 
   The Step Functions workflow orchestrates the data and inference process: 
     
     Lambda copies internal Hapag-Lloyd data from the data lake to Amazon Simple Storage Service (Amazon S3). 
     AWS Glue jobs combine the different data sources and prepare inference inputs 
     SageMaker inference executes in sequence: 
       
       Fallback predictions are computed from historical averages and written to Amazon Relational Database Service (Amazon RDS). Fallback predictions are used in case of missing data or a downstream inference failure. 
       Preprocessing data for the four specialized ML models. 
       O2P, P2P, and Berth model batch transforms. 
       The Combined model batch transform generates final ETA predictions, which are written to Amazon RDS. 
       Input features and output files are stored in Amazon S3 for analytics and monitoring. 
        
      
   For operational reliability, any failures in the inference pipeline trigger immediate email notifications to the on-call operations team through Amazon Simple Email Service (Amazon SES). 
    
 Real-time API: 
   
   Amazon API Gateway receives client requests containing the current schedule and an indication for which vessel-port combinations an ETA update is required. By receiving the current schedule through the client request, we can take care of intraday schedule updates while doing daily batch transform updates. 
   The API Gateway triggers a Lambda function calculating the response. The Lambda function constructs the response by linking the ETA predictions (stored in Amazon RDS) with the current schedule using custom business logic, so that we can take care of short-term schedule changes unknown at inference time. Typical examples of short-term schedule changes are port omissions (for example, due to port congestion) and one-time port calls. 
    
 
This architecture enables millisecond response times to custom requests while achieving a 99.5% availability (a maximum 3.5 hours downtime per month). 
 
Conclusion 
Hapag Lloyd‚Äôs ML powered vessel scheduling assistant outperforms the current solution in both accuracy and response time. Typical API response times are in the order of hundreds of milliseconds, helping to ensure a real-time user experience and outperforming the current solution by more than 80%. Low response times are crucial because, in addition to fully automated schedule updates, business experts require low response times to work with the schedule assistant interactively. In terms of accuracy, the MAE of the ML-powered ETA predictions outperform the current solution by approximately 12%, which translates into climbing by two positions in the international ranking of schedule reliability on average. This is one of the key performance metrics in liner shipping, and this is a significant improvement within the industry. 
To learn more about architecting and governing ML workloads at scale on AWS, see the AWS blog post Governing the ML lifecycle at scale, Part 1: A framework for architecting ML workloads using Amazon SageMaker and the accompanying AWS workshop AWS Multi-Account Data &amp; ML Governance Workshop. 
Acknowledgement 
We acknowledge the significant and valuable work of Michal Papaj and Piotr Zielinski from Hapag-Lloyd in the data science and data engineering areas of the project. 
About the authors 
Thomas Voss Thomas Voss works at Hapag-Lloyd as a data scientist. With his background in academia and logistics, he takes pride in leveraging data science expertise to drive business innovation and growth through the practical design and modeling of AI solutions. 
Bernhard Hersberger Bernhard Hersberger works as a data scientist at Hapag-Lloyd, where he heads the AI Hub team in Hamburg. He is enthusiastic about integrating AI solutions across the company, taking comprehensive responsibility from identifying business issues to deploying and scaling AI solutions worldwide. 
Gabija Pasiunaite At AWS, Gabija Pasiunaite was a Machine Learning Engineer at AWS Professional Services based in Zurich. She specialized in building scalable ML and data solutions for AWS Enterprise customers, combining expertise in data engineering, ML automation and cloud infrastructure. Gabija has contributed to the AWS MLOps Framework used by AWS customers globally. Outside work, Gabija enjoys exploring new destinations and staying active through hiking, skiing, and running. 
Jean-Michel Lourier Jean-Michel Lourier is a Senior Data Scientist within AWS Professional Services. He leads teams implementing data driven applications side by side with AWS customers to generate business value out of their data. He‚Äôs passionate about diving into tech and learning about AI, machine learning, and their business applications. He is also an enthusiastic cyclist. 
Mousam Majhi Mousam Majhi is a Senior ProServe Cloud Architect focusing on Data &amp; AI within AWS Professional Services. He works with Manufacturing and Travel, Transportation &amp; Logistics customers in DACH to achieve their business outcomes by leveraging data and AI powered solutions. Outside of work, Mousam enjoys hiking in the Bavarian Alps.

‚∏ª