‚úÖ Morning News Briefing ‚Äì October 24, 2025 10:46

üìÖ Date: 2025-10-24 10:46
üè∑Ô∏è Tags: #briefing #ai #publichealth #digitalgov

‚∏ª

üßæ Weather
‚Ä¢ Current Conditions:  0.1¬∞C
  Observed at: Pembroke 6:00 AM EDT Friday 24 October 2025 Temperature: 0.1&deg;C Pressure / Tendency: 101.6 kPa rising Humidity: 98 % Dewpoint: -0.2&deg:C Wind: WSW 5 km/h . Air Quality Health Index: n/a . Air quality Health Index is n
‚Ä¢ Friday: Chance of rain showers or flurries. High 8. POP 30%
  Cloudy with 30 percent chance of flurries early this morning and 30 percent . chance of rain showers this afternoon . Wind becoming northwest 20 km/h late this morning . High 8.5C with a low UV index 2 or low . Forecast issued 5:00 AM EDT Friday 24 October 2025 . Snowfall expected to continue throughout the day . Snow expected to fall overnight .
‚Ä¢ Friday night: Chance of showers. Low minus 1. POP 30%
  Cloudy. 30 percent chance of showers this evening . Low minus 1.50 degrees Fahrenheit . Cloudy, with rain likely of rain in the morning . Showers are possible in the afternoon . Forecast issued 5:00 AM EDT Friday 24 October 2025 . Weather forecasters predict temperatures will drop to minus 1 degrees Fahrenheit in the early hours of Friday morning and Saturday night. Forecasters

üåç International News
No updates.

üçÅ Canadian News
No updates.

üá∫üá∏ U.S. Top Stories
‚Ä¢ Boo to spooky chocolate prices! It's the year of chewy, fruity Halloween treats
  Sales of non-chocolate candy are growing faster than those of chocolate . With cocoa in shortage, manufacturers are changing pack sizes, adding fillers and dipping candy in "white creme" Manufacturers are adding fillingers, adding candy in white creme, dipping candy into "whithered creme," according to the candy industry . Sales of chocolate candy have increased in recent years
‚Ä¢ It's supposed to be payday for many federal workers. Instead, they're getting nothing
  Roughly 1.4 million federal workers are going without pay due to the government shutdown . About half of them are furloughed, while the other half has been deemed essential and is working without pay . Roughly half of the federal workers have been deemed 'essential' and are not receiving their paychecks . The government shutdown is the longest in the history of the U.S
‚Ä¢ No historic museums were harmed in the making of this quiz. Can you score 11?
  From brazen jewel heists to internet meltdowns, this week brought travesties galore . This week's travesty included jewel thefts, jewel thefts and online meltdowns . From jewel thefts to internet meltsdowns, the week was full of travestys galore in the news this week . Read the rest of the week's featured news stories here: CNN iReport.com
‚Ä¢ Here's what experts say 'A House of Dynamite' gets wrong (and right) about nuclear war
  A House of Dynamite is available online for streaming on October 24 . Experts say the sets, such as the White House situation room's watch floor, are "scarily authentic" Some praised realistic elements like the depiction of the situation room . But others said parts of the plot didn't ring true, and that some parts of it didn't seem to ring true . The series is available
‚Ä¢ Not sure how to support a friend with cancer? Survivors have advice
  To help a loved one with cancer, think about what you're good at ‚Äî and what they need . Are you organized? Plan their meals? Go with them to appointments? Plan meals? Plan appointments? Share ideas with CNN iReport.com/Cancer.com: Share your help with your loved ones with cancer. Back to Mail Online home . Back to the page you came

üß† Artificial Intelligence
No updates.

üíª Digital Strategy
‚Ä¢ BBC probe finds AI chatbots mangle nearly half of news summaries
  Four of the most popular AI chatbots routinely serve up inaccurate or misleading news content to users, according to a wide-reaching investigation . Google Gemini worst offender with 76% error rate with 76%. Google Gemini is the worst offender, with the worst rate of error rate at least 76% . The investigation was carried out by Google, Facebook, Google and Microsoft . The results come from a
‚Ä¢ How do you solve a problem like Discovery?
  Request For Ideas: How would you move a retired orbiter across the US? The White House's Office of Management and Budget is grappling with how to transport Space Shuttle Discovery from the Smithsonian Museum in Virginia to Space Center Houston . How would it move the shuttle from Virginia to Houston? Do you know what you would do? Share your ideas with CNN.com/Heroes and others .
‚Ä¢ Shield AI shows off not-at-all-terrifying autonomous VTOL combat drone
  US defense technology biz Shield AI claims it can build a jet-powered vertical take-off and landing (VTOL) autonomous fighter drone that doesn't need a runway to operate . Shield AI: Where we're going, we don't need runways to operate.‚Ä¶‚Ä¶‚Ä¶ Where we are going, where we don‚Äôt need to runways? Shield AI says it
‚Ä¢ Britain's Ministry of Justice just signed up to ChatGPT Enterprise
  OpenAI has signed up the UK's Ministry of Justice as the latest public sector customer for ChatGPT Enterprise . OpenAI sweetens the deal with data residency with the government's data residency . The company has also signed up a contract with the Department of Justice in the UK to use its own data residency data to make its data residency available to the public sector . The move is the
‚Ä¢ New boss took charge of project code and sent two billion unwanted emails
  Techie summoned at 02:00am to sort things out sent another 2 billion trying to fix it . Techie summons at 02-00am sent another two billion trying a fix it out . On Call is The Register's weekly wander through your tales of tech support . Read the latest instalment of On Call from The Register . Back to the page you came from, read it

üè• Public Health
No updates.

üî¨ Science
‚Ä¢ Reducing annotation burden in physical activity research using vision language models
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Association of dietary omega-3 fatty acids intake with all-cause and cardiovascular disease-specific mortality among individuals with cardiovascular disease
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Volume‚Äíoutcome relationships in bariatric surgery: a rapid review
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Ambient biothermal stress, preconceptional thyrotropin abnormalities, and the risk of preterm birth: a nationwide Chinese cohort study
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Predicting hypotension, syncope, and fracture risk in patients indicated for antihypertensive treatment: the STRATIFY models
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations

üßæ Government & Policy
No updates.

üèõÔ∏è Enterprise Architecture & IT Governance
No updates.

ü§ñ AI & Emerging Tech
‚Ä¢ An AI app to measure pain is here
  How are you feeling?



I‚Äôm genuinely interested in the well-being of all my treasured Checkup readers, of course. But this week I‚Äôve also been wondering how science and technology can help answer that question‚Äîespecially when it comes to pain.¬†In the latest issue of MIT Technology Review magazine, Deena Mousa describes how an AI-powered smartphone app is being used to assess how much pain a person is in.



The app, and other tools like it, could help doctors and caregivers. They could be especially useful in the care of people who aren‚Äôt able to tell others how they are feeling.



But they are far from perfect. And they open up all kinds of thorny questions about how we experience, communicate, and even treat pain.





Pain can be notoriously difficult to describe, as almost everyone who has ever been asked to will know. At a recent medical visit, my doctor asked me to rank my pain on a scale from 1 to 10. I found it incredibly difficult to do. A 10, she said, meant ‚Äúthe worst pain imaginable,‚Äù which brought back unpleasant memories of having appendicitis.



A short while before the problem that brought me in, I‚Äôd broken my toe in two places, which had hurt like a mother‚Äîbut less than appendicitis. If appendicitis was a 10, breaking a toe was an 8, I figured. If that was the case, maybe my current pain was a 6. As a pain score, it didn‚Äôt sound as bad as I actually felt. I couldn‚Äôt help wondering if I might have given a higher score if my appendix were still intact. I wondered, too, how someone else with my medical issue might score their pain.



In truth, we all experience pain in our own unique ways. Pain is subjective, and it is influenced by our past experiences, our¬†moods, and our expectations. The way people describe their pain can vary tremendously, too.



We‚Äôve known this for ages. In the 1940s, the anesthesiologist Henry Beecher noted that¬†wounded soldiers were much less likely to ask for pain relief than similarly injured people in civilian hospitals. Perhaps they were putting on a brave face, or maybe they just felt lucky to be alive, given their circumstances. We have no way of knowing how much pain they were really feeling.



Given this messy picture, I can see the appeal of a simple test that can score pain and help medical professionals understand how best to treat their patients. That‚Äôs what is being offered by PainChek, the smartphone app Deena wrote about. The app works by assessing small facial movements, such as lip raises or brow pinches. A user is then required to fill a separate checklist to identify other signs of pain the patient might be displaying. It seems to work well, and it is already being used in hospitals and care settings.





But the app is judged against subjective reports of pain. It might be useful for assessing the pain of people who can‚Äôt describe it themselves‚Äîperhaps because they have dementia, for example‚Äîbut it won‚Äôt add much to assessments from people who can already communicate their pain levels.



There are other complications. Say a test could spot that a person was experiencing pain. What can a doctor do with that information? Perhaps prescribe pain relief‚Äîbut most of the pain-relieving drugs we have were designed to treat acute, short-term pain. If a person is grimacing from a chronic pain condition, the treatment options are more limited, says Stuart Derbyshire, a pain neuroscientist at the National University of Singapore.



The last time I spoke to Derbyshire was back in 2010, when I covered¬†work by researchers in London who were using brain scans to measure pain. That was 15 years ago. But pain-measuring brain scanners are yet to become a routine part of clinical care.



That scoring system was also built on subjective pain reports. Those reports are, as Derbyshire puts it, ‚Äúbaked into the system.‚Äù It‚Äôs not ideal, but when it comes down to it, we must rely on these wobbly, malleable, and sometimes incoherent self-descriptions of pain. It‚Äôs the best we have.



Derbyshire says he doesn‚Äôt think we‚Äôll ever have a ‚Äúpain-o-meter‚Äù that can tell you what a person is truly experiencing. ‚ÄúSubjective report is the gold standard, and I think it always will be,‚Äù he says.



This article first appeared in The Checkup,¬†MIT Technology Review‚Äôs¬†weekly biotech newsletter. To receive it in your inbox every Thursday, and read articles like this first,¬†sign up here.
‚Ä¢ What‚Äôs next for carbon removal?
  MIT Technology Review‚Äôs What‚Äôs Next series looks across industries, trends, and technologies to give you a first look at the future. You can read the rest of them here.In the early 2020s, a little-known aquaculture company in Portland, Maine, snagged more than $50 million by pitching a plan to harness nature to fight back against climate change. The company, Running Tide, said it could sink enough kelp to the seafloor to sequester a billion tons of carbon dioxide by this year, according to one of its early customers.



Instead, the business shut down its operations last summer, marking the biggest bust to date in the nascent carbon removal sector.



Its demise was the most obvious sign of growing troubles and dimming expectations for a space that has spawned hundreds of startups over the last few years. A handful of other companies have shuttered, downsized, or pivoted in recent months as well. Venture investments have flagged. And the collective industry hasn‚Äôt made a whole lot more progress toward that billion-ton benchmark.The hype phase is over and the sector is sliding into the turbulent business trough that follows, warns Robert H√∂glund, cofounder of CDR.fyi, a public-benefit corporation that provides data and analysis on the carbon removal industry.



‚ÄúWe‚Äôre past the peak of expectations,‚Äù he says. ‚ÄúAnd with that, we could see a lot of companies go out of business, which is natural for any industry.‚Äù



The open question is: If the carbon removal sector is heading into a painful if inevitable clearing-out cycle, where will it go from there?&nbsp;



The odd quirk of carbon removal is that it never made a lot of sense as a business proposition: It‚Äôs an atmospheric cleanup job, necessary for the collective societal good of curbing climate change. But it doesn‚Äôt produce a service or product that any individual or organization strictly needs‚Äîor is especially eager to pay for.



To date, a number of businesses have voluntarily agreed to buy tons of carbon dioxide that companies intend to eventually suck out of the air. But whether they‚Äôre motivated by sincere climate concerns or pressures from investors, employees, or customers, corporate do-goodism will only scale any industry so far.&nbsp;



Most observers argue that whether carbon removal continues to bobble along or transforms into something big enough to make a dent in climate change will depend largely on whether governments around the world decide to pay for a whole, whole lot of it‚Äîor force polluters to.&nbsp;



‚ÄúPrivate-sector purchases will never get us there,‚Äù says Erin Burns, executive director of Carbon180, a nonprofit that advocates for the removal and reuse of carbon dioxide. ‚ÄúWe need policy; it has to be policy.‚Äù



What‚Äôs the problem?



The carbon removal sector began to scale up in the early part of this decade, as increasingly grave climate studies revealed the need to dramatically cut emissions and suck down vast amounts of carbon dioxide to keep global warming in check.



Specifically, nations may have to continually remove as much as 11 billion tons of carbon dioxide per year by around midcentury to have a solid chance of keeping the planet from warming past 2 ¬∞C over preindustrial levels, according to a UN climate panel report in 2022.



A number of startups sprang up to begin developing the technology and building the infrastructure that would be needed, trying out a variety of approaches like sinking seaweed or building carbon-dioxide-sucking factories.





And they soon attracted customers. Companies including Stripe, Google, Shopify, Microsoft, and others began agreeing to pre-purchase tons of carbon removal, hoping to stand up the nascent industry and help offset their own climate emissions. Venture investments also flooded into the space, peaking in 2023 at nearly $1 billion, according to data provided by PitchBook.



From early on, players in the emerging sector sought to draw a sharp distinction between conventional carbon offset projects, which studies have shown frequently exaggerate climate benefits, and ‚Äúdurable‚Äù carbon removal that could be relied upon to suck down and store away the greenhouse gas for decades to centuries. There‚Äôs certainly a big difference in the price: While buying carbon offsets through projects that promise to preserve forests or plant trees might cost a few dollars per ton, a ton of carbon removal can run hundreds to thousands of dollars, depending on the approach.&nbsp;



That high price, however, brings big challenges. Removing 10 billion tons of carbon dioxide a year at, say, $300 a ton adds up to a global price tag of $3 trillion‚Äîa year.&nbsp;



Which brings us back to the fundamental question: Who should or would foot the bill to develop and operate all the factories, pipelines, and wells needed to capture, move, and bury billions upon billions of tons of carbon dioxide?



The state of the market



The market is still growing, as companies voluntarily purchase tons of carbon removal to make strides toward their climate goals. In fact, sales reached an all-time high in the second quarter of this year, mostly thanks to several massive purchases by Microsoft.





But industry sources fear that demand isn‚Äôt growing fast enough to support a significant share of the startups that have formed or even the projects being built, undermining the momentum required to scale the sector up to the size needed by midcentury.



To date, all those hundreds of companies that have spun up in recent years have disclosed deals to sell some 38 million tons of carbon dioxide pulled from the air, according to CDR.fyi. That‚Äôs roughly the amount the US pumps out in energy-related emissions every three days.&nbsp;



And they‚Äôve only delivered around 940,000 tons of carbon removal. The US emits that much carbon dioxide in less than two hours. (Not every transaction is publicly announced or revealed to CDR.fyi, so the actual figures could run a bit higher.)



Another concern is that the same handful of big players continue to account for the vast majority of the overall purchases, leaving the health and direction of the market dependent on their whims and fortunes.&nbsp;



Most glaringly, Microsoft has agreed to buy 80% of all the carbon removal purchased to date, according to&nbsp; CDR.fyi. The second-biggest buyer is Frontier, a coalition of companies that includes Google, Meta, Stripe, and Shopify, which has committed to spend $1 billion.



If you strip out those two buyers, the market shrinks from 16 million tons under contract during the first half of this year to just 1.2 million, according to data provided to MIT Technology Review by CDR.fyi.&nbsp;



Signs of trouble



Meanwhile, the investor appetite for carbon removal is cooling. For the 12-month period ending in the second quarter of 2025, venture capital investments in the sector fell more than 13% from the same period last year, according to data provided by PitchBook. That tightening funding will make it harder and harder for companies that aren‚Äôt bringing in revenue to stay afloat.



Companies that have already shut down also include the carbon removal marketplace Noya and Alkali Earth, which was attempting to use industrial by-products to tie up carbon dioxide.



Still other businesses are struggling. Climeworks, one of the first companies to build direct-air-capture (DAC) factories, announced it was laying off 10% of its staff in May, as it grapples with challenges on several fronts.



The company‚Äôs plans to collaborate on the development of a major facility in the US have been at least delayed as the Trump administration has held back tens of millions of dollars in funding granted in 2023 under the Department of Energy‚Äôs Regional Direct Air Capture Hubs program. It now appears the government could terminate the funding altogether, along with perhaps tens of billions of dollars‚Äô worth of additional grants previously awarded for a variety of other US carbon removal and climate tech projects.



‚ÄúMarket rumors have surfaced, and Climeworks is prepared for all scenarios,‚Äù Christoph Gebald, one of the company‚Äôs co-CEOs, said in a previous statement to MIT Technology Review. ‚ÄúThe need for DAC is growing as the world falls short of its climate goals and we‚Äôre working to achieve the gigaton capacity that will be needed.‚Äù



But purchases from direct-air-capture projects fell nearly 16% last year and account for just 8% of all carbon removal transactions to date. Buyers are increasingly looking to categories that promise to deliver tons faster and for less money, notably including burying biochar or installing carbon capture equipment on bioenergy plants. (Read more in my recent story on that method of carbon removal, known as BECCS, here.)



CDR.fyi recently described the climate for direct air capture in grim terms: ‚ÄúThe sector has grown rapidly, but the honeymoon is over: Investment and sales are falling, while deployments are delayed across almost every company.‚Äù‚ÄúMost DAC companies,‚Äù the organization added, ‚Äúwill fold or be acquired.‚Äù



What‚Äôs next?



In the end, most observers believe carbon removal isn‚Äôt really going to take off unless governments bring their resources and regulations to bear. That could mean making direct purchases, subsidizing these sectors, or getting polluters to pay the costs to do so‚Äîfor instance, by folding carbon removal into market-based emissions reductions mechanisms like cap-and-trade systems.&nbsp;



More government support does appear to be on the way. Notably, the European Commission recently proposed allowing ‚Äúdomestic carbon removal‚Äù within its EU Emissions Trading System after 2030, integrating the sector into one of the largest cap-and-trade programs. The system forces power plants and other polluters in member countries to increasingly cut their emissions or pay for them over time, as the cap on pollution tightens and the price on carbon rises.&nbsp;



That could create incentives for more European companies to pay direct-air-capture or bioenergy facilities to draw down carbon dioxide as a means of helping them meet their climate obligations.



There are also indications that the International Civil Aviation Organization, a UN organization that establishes standards for the aviation industry, is considering incorporating carbon removal into its market-based mechanism for reducing the sector‚Äôs emissions. That might take several forms, including allowing airlines to purchase carbon removal to offset their use of traditional jet fuel or requiring the use of carbon dioxide obtained through direct air capture in some share of sustainable aviation fuels.



Meanwhile, Canada has committed to spend $10 million on carbon removal and is developing a protocol to allow direct air capture in its national offsets program. And Japan will begin accepting several categories of carbon removal in its emissions trading system.&nbsp;



Despite the Trump administration‚Äôs efforts to claw back funding for the development of carbon-sucking projects, the US does continue to subsidize storage of carbon dioxide, whether it comes from power plants, ethanol refineries, direct-air-capture plants, or other facilities. The so-called 45Q tax credit, which is worth up to $180 a ton, was among the few forms of government support for climate-tech-related sectors that survived in the 2025 budget reconciliation bill. In fact, the subsidies for putting carbon dioxide to other uses increased.



Even in the current US political climate, Burns is hopeful that local or federal legislators will continue to enact policies that support specific categories of carbon removal in the regions where they make the most sense, because the projects can provide economic growth and jobs as well as climate benefits.



‚ÄúI actually think there are lots of models for what carbon removal policy can look like that aren‚Äôt just things like tax incentives,‚Äù she says. ‚ÄúAnd I think that this particular political moment gives us the opportunity in a unique way to start to look at what those regionally specific and pathway specific policies look like.‚Äù



The dangers ahead



But even if more nations do provide the money or enact the laws necessary to drive the business of durable carbon renewal forward, there are mounting concerns that a sector conceived as an alternative to dubious offset markets could increasingly come to replicate their problems.



Various incentives are pulling in that direction.



Financial pressures are building on suppliers to deliver tons of carbon removal. Corporate buyers are looking for the fastest and most affordable way of hitting their climate goals. And the organizations that set standards and accredit carbon removal projects often earn more money as the volume of purchases rises, creating clear conflicts of interest.



Some of the same carbon registries that have long signed off on carbon offset projects have begun creating standards or issuing credits for various forms of carbon removal, including Verra and Gold Standard.





‚ÄúReliable assurance that a project‚Äôs declared ton of carbon savings equates to a real ton of emissions removed, reduced, or avoided is crucial,‚Äù Cynthia Giles, a senior EPA advisor under President Biden, and Cary Coglianese, a law professor at the University of Pennsylvania, wrote in a recent editorial in Science. ‚ÄúYet extensive research from many contexts shows that auditors selected and paid by audited organizations often produce results skewed toward those entities‚Äô interests.‚Äù



Noah McQueen, the director of science and innovation at Carbon180, has stressed that the industry must strive to counter the mounting credibility risks, noting in a recent LinkedIn post: ‚ÄúGrowth matters, but growth without integrity isn‚Äôt growth at all.‚Äù



In an interview, McQueen said that heading off the problem will require developing and enforcing standards to truly ensure that carbon removal projects deliver the climate benefits promised. McQueen added that to gain trust, the industry needs to earn buy-in from the communities in which these projects are built and avoid the environmental and health impacts that power plants and heavy industry have historically inflicted on disadvantaged communities.



Getting it right will require governments to take a larger role in the sector than just subsidizing it, argues David Ho, a professor at the University of Hawai ªi at MƒÅnoa who focuses&nbsp; on ocean-based carbon removal.



He says there should be a massive, multinational research drive to determine the most effective ways of mopping up the atmosphere with minimal environmental or social harm, likening it to a Manhattan Project (minus the whole nuclear bomb bit).



‚ÄúIf we‚Äôre serious about doing this, then let‚Äôs make it a government effort,‚Äù he says, ‚Äúso that you can try out all the things, determine what works and what doesn‚Äôt, and you don‚Äôt have to please your VCs or concentrate on developing [intellectual property] so you can sell yourself to a fossil-fuel company.‚Äù



Ho adds that there‚Äôs a moral imperative for the world‚Äôs historically biggest climate polluters to build and pay for the carbon-sucking and storage infrastructure required to draw down billions of tons of greenhouse gas. That‚Äôs because the world‚Äôs poorest, hottest nations, which have contributed the least to climate change, will nevertheless face the greatest dangers from intensifying heat waves, droughts, famines, and sea-level rise.



‚ÄúIt should be seen as waste management for the waste we‚Äôre going to dump on the Global South,‚Äù he says, ‚Äúbecause they‚Äôre the people who will suffer the most from climate change.‚Äù
‚Ä¢ Redefining data engineering in the age of AI
  MIT Technology Review Insights: Data engineers play a pivotal role in their organizations as enablers of AI . The share of time data engineers spend each day on AI projects has nearly doubled in the past two years, from an average of 19% in 2023 to 37% in 2025 . Data engineers are being asked to do more today than ever before, and that‚Äôs not likely to change .
‚Ä¢ The Download: aluminium‚Äôs potential as a zero-carbon fuel, and what‚Äôs next for energy storage
  This is today&#8217;s edition of¬†The Download,¬†our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



This startup is about to conduct the biggest real-world test of aluminum as a zero-carbon fuel



Found Energy, a startup in Boston, aims to harness the energy in scraps of aluminum metal to power industrial processes without fossil fuels. Since 2022, the company has worked to develop ways to rapidly release energy from aluminum on a small scale.Now it‚Äôs just switched on a much larger version of its aluminum-powered engine, which it claims is the largest aluminum-water reactor ever built.Early next year, it will be installed to supply heat and hydrogen to a tool manufacturing facility in the southeastern US, using the aluminum waste produced by the plant itself as fuel.If everything works as planned, this technology, which uses a catalyst to unlock the energy stored within aluminum metal, could transform a growing share of aluminum scrap into a zero-carbon fuel. Read the full story.



‚ÄîJames Dinneen







What a massive thermal battery means for energy storage



Rondo Energy just turned on what it says is the world‚Äôs largest thermal battery, an energy storage system that can take in electricity and provide a consistent source of heat.



The concept behind a thermal battery is overwhelmingly simple: Use electricity to heat up some cheap, sturdy material (like bricks) and keep it hot until you want to use that heat later, either directly in an industrial process or to produce electricity.¬†



Thermal batteries could be a major tool in cutting emissions: 20% of total energy demand today is used to provide heat for industrial processes, and most of that is generated by burning fossil fuels. But the company is using its battery for enhanced oil recovery‚Äîa process that critics argue keep polluting infrastructure running longer. Read the full story.



‚ÄîCasey Crownhart



This article is from The Spark, MIT Technology Review‚Äôs weekly climate newsletter. To receive it in your inbox every Wednesday, sign up here.







The must-reads



I‚Äôve combed the internet to find you today‚Äôs most fun/important/scary/fascinating stories about technology.



1 ChatGPT‚Äôs suicide discussion rules were loosened twice before a teen took his own life¬†¬†¬†¬†The parents of Adam Raine claim the changes OpenAI made equate to a weakening in its suicide protection for users. (WSJ $)+ It did so to increase use of the chatbot, they allege in an amended lawsuit. (FT $)+ The family is accusing OpenAI of intentional misconduct rather than reckless indifference. (Rolling Stone $)2 Google claims its new quantum algorithm outperforms a supercomputerIt could accelerate advances in drug discovery and new building materials. (Ars Technica)+ Its Willow chip is at the heart of the advance. (NYT $)+ But real-world use of quantum computing is still likely to be years away. (The Guardian)



3 Reddit is suing AI search engine PerplexityFor allegedly illegally scraping its data to train the model powering Perplexity&#8217;s engine. (FT $)+ Reddit‚Äôs also seeking a permanent injunction on companies selling its data. (Engadget)+ What comes next for AI copyright lawsuits? (MIT Technology Review)



4 China has a five-year plan to become technologically self-reliantAnd semiconductors and AI will play key roles. (Bloomberg $)+ China is winning the trade war with America. (Economist $)5 DeepSeek is taking off in AfricaIts decision to make its AI cheaper and less power-intensive is paying off. (Bloomberg $)+ How DeepSeek ripped up the AI playbook. (MIT Technology Review)



6 Elon Musk is building a robot armyHe envisions his Optimus robot becoming an ‚Äúincredible surgeon.‚Äù (Wired $)+ Will we ever trust robots? (MIT Technology Review)



7 Apple has pulled a pair of controversial dating apps from the App StoreTea and TeaOnHer fell short of its privacy and content moderation rules. (TechCrunch)



8 Tesla‚Äôs profits are massively downEven though it sold more cars than during its previous quarter. (NYT $)+ The company has been forced to recall tens of thousands of Cybertrucks. (Reuters)+ What happens when your EV becomes obsolete? (The Atlantic $)



9 An unexpected victim of the AWS outage? Smart beds Some unlucky owners‚Äô beds blared alarms and became unbearably warm. (WP $)+ If the internet stays the way it is, more bed outages could be on their way. (The Atlantic $)



10 The appeal of incredibly basic softwareApple‚Äôs TextEdit does exactly what it says on the tin. (New Yorker $)







Quote of the day



‚ÄúI‚Äôm very excited that nerds are having our moment.‚Äù



‚ÄîMadhavi Sewak, a Google DeepMind researcher, says she‚Äôs glad that AI experts are being recognized, the Wall Street Journal reports.







One more thing







Inside the hunt for new physics at the world‚Äôs largest particle colliderIn 2012, using data from CERN‚Äôs Large Hadron Collider, researchers discovered a particle called the Higgs boson. In the process, they answered a nagging question: Where do fundamental particles, such as the ones that make up all the protons and neutrons in our bodies, get their mass?When the particle was finally found, scientists celebrated with champagne. A Nobel for two of the physicists who predicted the Higgs boson soon followed.More than a decade later, there is a sense of unease. That‚Äôs because there are still so many unanswered questions about the fundamental constituents of the universe.So researchers are trying something new. They are repurposing detectors to search for unusual-looking particles, squeezing what they can out of the data with machine learning, and planning for entirely new kinds of colliders. Read the full story.



‚ÄîDan Garisto







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)+ Mexico City is already getting into the Halloween spirit: its annual zombie parade took place over the weekend.+ Everything you need to know before travelling to Japan.+ The most stylish people alive? I‚Äôll be the judge of that.+ Here‚Äôs something you don‚Äôt expect archeologists to uncover: Neolithic chewing gum.
‚Ä¢ What a massive thermal battery means for energy storage
  Rondo Energy just turned on what it says is the world‚Äôs largest thermal battery, an energy storage system that can take in electricity and provide a consistent source of heat . The company announced last week that its first full-scale system is operational, with 100 megawatt-hours of capacity . The battery is being used for enhanced oil recovery, a process where steam is injected down into wells to get stubborn oil out of the ground .

üîí Cybersecurity & Privacy
‚Ä¢ Canada Fines Cybercrime Friendly Cryptomus $176M
  Financial regulators in Canada this week levied $176 million in fines against Cryptomus, a digital payments platform that supports dozens of Russian cryptocurrency exchanges and websites hawking cybercrime services . The penalties for violating Canada&#8217;s anti money-laundering laws come ten months after KrebsOnSecurity noted the Vancouver address was home to dozens of foreign currency dealers, money transfer businesses, and cryptocurrency exchanges .

üéì University AI
No updates.

üè¢ Corporate AI
‚Ä¢ Tell me when: Building agents that can wait, monitor, and act
  Modern&nbsp;LLM&nbsp;Agents&nbsp;can debug code, analyze spreadsheets, and book complex travel.&nbsp;Given those capabilities, it‚Äôs reasonable to assume that they could handle something simpler:&nbsp;waiting.&nbsp;Ask an agent to&nbsp;monitor&nbsp;your email for a colleague‚Äôs response or watch for a price drop over several days, and it will fail. Not because it&nbsp;can‚Äôt&nbsp;check email or scrape prices. It can do both. It fails&nbsp;because it&nbsp;doesn‚Äôt&nbsp;know&nbsp;when&nbsp;to check.&nbsp;Agents either&nbsp;give up after a few attempts or burn through their context window, checking obsessively. Neither&nbsp;work.&nbsp;



This matters because monitoring tasks&nbsp;are&nbsp;everywhere. We track emails for specific information, watch news&nbsp;feeds for updates, and&nbsp;monitor&nbsp;prices for sales. Automating these tasks would save hours, but current&nbsp;agents&nbsp;aren‚Äôt&nbsp;built for patience.



To address this, we are introducing&nbsp;SentinelStep (opens in new tab),&nbsp;a&nbsp;mechanism&nbsp;that&nbsp;enables&nbsp;agents&nbsp;to complete long-running monitoring&nbsp;tasks.&nbsp;The&nbsp;approach is simple.&nbsp;SentinelStep&nbsp;wraps the agent in a workflow with dynamic&nbsp;polling&nbsp;and&nbsp;careful context&nbsp;management.&nbsp;This&nbsp;enables&nbsp;the&nbsp;agent&nbsp;to&nbsp;monitor&nbsp;conditions for&nbsp;hours&nbsp;or&nbsp;days&nbsp;without getting&nbsp;sidetracked.&nbsp;We&#8217;ve&nbsp;implemented&nbsp;SentinelStep&nbsp;in&nbsp;Magentic-UI,&nbsp;our research&nbsp;prototype&nbsp;agentic system,&nbsp;to enable&nbsp;users&nbsp;to&nbsp;build agents for&nbsp;long-running&nbsp;tasks,&nbsp;whether they&nbsp;involve web&nbsp;browsing, coding, or external&nbsp;tools.&nbsp;



	
		

		
		Spotlight: AI-POWERED EXPERIENCE
	
	
	
						
				
					
				
			
			
			

									Microsoft research copilot experience
				
								Discover more about research at Microsoft through our AI-powered experience
				
								
					
						
							Start now						
					
				
							
	
Opens in a new tab	
	


How it works



The core&nbsp;challenge is&nbsp;polling frequency. Poll too often,&nbsp;and&nbsp;tokens get&nbsp;wasted. Poll too infrequently, and the user‚Äôs notification gets delayed.&nbsp;SentinelStep&nbsp;makes&nbsp;an educated guess&nbsp;at&nbsp;the&nbsp;polling interval based on the task at hand‚Äîchecking email gets different treatment&nbsp;than&nbsp;monitoring&nbsp;quarterly earnings‚Äîthen dynamically adjusts&nbsp;based on&nbsp;observed&nbsp;behavior.&nbsp;



There‚Äôs&nbsp;a second challenge: context overflow.&nbsp;Because monitoring tasks can run for days,&nbsp;context overflow&nbsp;becomes inevitable.&nbsp;SentinelStep&nbsp;handles&nbsp;this by saving the agent state after the first check, then&nbsp;using&nbsp;that state for each subsequent check.





















These demonstrations capture&nbsp;Magentic-UI with&nbsp;SentinelStep&nbsp;at work, completing a range of tasks in a timelapse sequence.&nbsp;



Core components



As&nbsp;the name&nbsp;suggests,&nbsp;SentinelStep&nbsp;consists of&nbsp;individual steps&nbsp;taken as part of&nbsp;an&nbsp;agent‚Äôs broader&nbsp;workflow.&nbsp;As illustrated in Figure 1, there are three main components:&nbsp;the&nbsp;actions necessary to collect information, the condition that determines&nbsp;when&nbsp;the task&nbsp;is complete,&nbsp;and the polling interval&nbsp;that&nbsp;determines&nbsp;timing.&nbsp;Once&nbsp;these components&nbsp;are&nbsp;identified, the&nbsp;system‚Äôs&nbsp;behavior is simple:&nbsp;every&nbsp;[polling interval]&nbsp;do&nbsp;[actions]&nbsp;until&nbsp;[condition]&nbsp;is satisfied.&nbsp;



Figure&nbsp;1.&nbsp;SentinelSteps‚Äôs&nbsp;three main components&nbsp;in&nbsp;Magentic-UI‚Äôs&nbsp;co-planning interface.&nbsp;



These three components are defined and exposed in the co-planning interface of Magentic-UI. Given a user prompt, Magentic-UI proposes a complete multi-step plan, including pre-filled parameters for any monitoring steps. Users can accept the plan or adjust as needed.



Processing



Once a run starts, Magentic-UI assigns the most appropriate agent from a team of agents to perform each action. This team includes agents capable of web surfing, code execution, and calling arbitrary MCP servers.



When the workflow reaches a monitoring step, the flow is straightforward. The assigned agent collects the necessary information through the actions described in the plan. The Magentic-UI orchestrator then checks whether the condition is satisfied. If it is, the SentinelStep is complete, and the orchestrator moves to the next step. If not, the orchestrator determines the timestamp for the next check and resets the agent‚Äôs state to prevent context overflow.



Evaluation



Evaluating&nbsp;monitoring tasks in real-world settings&nbsp;is&nbsp;nearly impossible.&nbsp;Consider a simple example: monitoring the Magentic-UI repository on GitHub&nbsp;until&nbsp;it reaches&nbsp;10,000 stars&nbsp;(a measure of how many people have bookmarked it). That event occurs only once and can‚Äôt be repeated.&nbsp;Most&nbsp;real-world monitoring tasks share this limitation, making systematic bench marking very challenging.



In response, we&nbsp;are developing&nbsp;SentinelBench, a suite of synthetic&nbsp;web environments for evaluating monitoring tasks. These environments make experiments repeatable. SentinelBench&nbsp;currently&nbsp;supports&nbsp;28&nbsp;configurable scenarios, each&nbsp;allowing the user to schedule exactly when&nbsp;a&nbsp;target&nbsp;event&nbsp;should&nbsp;occur. It includes setups like GitHub Watcher, which&nbsp;simulates a repository accumulating stars over time;&nbsp;Teams Monitor, which models incoming messages, some&nbsp;urgent; and&nbsp;Flight Monitor, which&nbsp;replicates&nbsp;evolving&nbsp;flight-availability&nbsp;dynamics.&nbsp;



Initial&nbsp;tests&nbsp;show clear benefits.&nbsp;As shown in&nbsp;Figure&nbsp;2, success rates&nbsp;remain&nbsp;high for short tasks (30&nbsp;sec&nbsp;and 1&nbsp;min) regardless of&nbsp;whether&nbsp;SentinelStep&nbsp;is&nbsp;used.&nbsp;For longer tasks,&nbsp;SentinelStep&nbsp;markedly&nbsp;improves reliability: at 1 hour, task reliability rises from 5.6% without&nbsp;SentinelStep&nbsp;to&nbsp;33.3% with&nbsp;it;&nbsp;and at 2 hours,&nbsp;it rises&nbsp;from 5.6% to 38.9%. These gains&nbsp;demonstrate&nbsp;that&nbsp;SentinelStep&nbsp;effectively addresses the challenge of maintaining performance over extended durations.



Figure&nbsp;2.&nbsp;SentinelStep&nbsp;improves&nbsp;success rates&nbsp;on longer running tasks (1‚Äì2&nbsp;hours)&nbsp;while&nbsp;maintaining&nbsp;comparable performance&nbsp;on shorter tasks.&nbsp;&nbsp;



Impact and availability



SentinelStep is a first step toward practical, proactive, longer‚Äërunning agents. By embedding patience into plans, agents can responsibly monitor conditions and act when it matters‚Äîstaying proactive without wasting resources. This lays the groundwork for always‚Äëon assistants that stay efficient, respectful of limits, and aligned with user intent.



We‚Äôve open-sourced SentinelStep as part of Magentic-UI, available on GitHub (opens in new tab) or via pip install magnetic-ui. As with any new technique, production deployment should be preceded through&nbsp;testing and validation&nbsp;for the specific use case.&nbsp;For&nbsp;guidance on&nbsp;intended use,&nbsp;privacy&nbsp;considerations,&nbsp;and safety&nbsp;guidelines,&nbsp;see&nbsp;the&nbsp;Magentic-UI&nbsp;Transparency&nbsp;Note. (opens in new tab)&nbsp;



Our goal is to&nbsp;make it easier to implement agents that can&nbsp;handle&nbsp;long-running&nbsp;monitoring&nbsp;tasks&nbsp;and&nbsp;lay&nbsp;the groundwork for&nbsp;systems that&nbsp;anticipate, adapt, and&nbsp;evolve&nbsp;to meet real-world needs.&nbsp;
Opens in a new tabThe post Tell me when: Building agents that can wait, monitor, and act appeared first on Microsoft Research.
‚Ä¢ Generate Gremlin queries using Amazon Bedrock models
  Graph databases have revolutionized how organizations manage complex, interconnected data. However, specialized query languages such as Gremlin often create a barrier for teams looking to extract insights efficiently. Unlike traditional relational databases with well-defined schemas, graph databases lack a centralized schema, requiring deep technical expertise for effective querying. 
To address this challenge, we explore an approach that converts natural language to Gremlin queries, using Amazon Bedrock models such as Amazon Nova Pro. This approach helps business analysts, data scientists, and other non-technical users access and interact with graph databases seamlessly. 
In this post, we outline our methodology for generating Gremlin queries from natural language, comparing different techniques and demonstrating how to evaluate the effectiveness of these generated queries using large language models (LLMs) as judges. 
Solution overview 
Transforming natural language queries into Gremlin queries requires a deep understanding of graph structures and the domain-specific knowledge encapsulated within the graph database. To achieve this, we divided our approach into three key steps: 
 
 Understanding and extracting graph knowledge 
 Structuring the graph similar to text-to-SQL processing 
 Generating and executing Gremlin queries 
 
The following diagram illustrates this workflow. 
 
Step 1: Extract graph knowledge 
A successful query generation framework must integrate both graph knowledge and domain knowledge to accurately translate natural language queries. Graph knowledge encompasses structural and semantic information extracted directly from the graph database. Specifically, it includes: 
 
 Vertex labels and properties ‚Äì A listing of vertex types, names, and their associated attributes 
 Edge labels and properties ‚Äì Information about edge types and their attributes 
 One-hop neighbors for each vertex ‚Äì Capturing local connectivity information, such as direct relationships between vertices 
 
With this graph-specific knowledge, the framework can effectively reason about the heterogeneous properties and complex connections inherent to graph databases. 
Domain knowledge captures additional context that augments the graph knowledge and is tailored specifically to the application domain. It is sourced in two ways: 
 
 Customer-provided domain knowledge ‚Äì For example, the customer kscope.ai helped specify those vertices that represent metadata and should never be queried. Such constraints are encoded to guide the query generation process. 
 LLM-generated descriptions ‚Äì To enhance the system‚Äôs understanding of vertex labels and their relevance to specific questions, we use an LLM to generate detailed semantic descriptions of vertex names, properties, and edges. These descriptions are stored within the domain knowledge repository and provide additional context to improve the relevance of the generated queries. 
 
Step 2: Structure the graph as a text-to-SQL schema 
To improve the model‚Äôs comprehension of graph structures, we adopt an approach similar to text-to-SQL processing, where we construct a schema representing vertex types, edges, and properties. This structured representation enhances the model‚Äôs ability to interpret and generate meaningful queries. 
The question processing component transforms natural language input into structured elements for query generation. It operates in three stages: 
 
 Entity recognition and classification ‚Äì Identifies key database elements in the input question (such as vertices, edges, and properties) and categorizes the question based on its intent 
 Context enhancement ‚Äì Enriches the question with relevant information from the knowledge component, so both graph-specific and domain-specific context is properly captured 
 Query planning ‚Äì Maps the enhanced question to specific database elements needed for query execution 
 
The context generation component makes sure the generated queries accurately reflect the underlying graph structure by assembling the following: 
 
 Element properties ‚Äì Retrieves attributes of vertices and edges along with their data types 
 Graph structure ‚Äì Facilitates alignment with the database‚Äôs topology 
 Domain rules ‚Äì Applies business constraints and logic 
 
Step 3: Generate and execute Gremlin queries 
The final step is query generation, where the LLM constructs a Gremlin query based on the extracted context. The process follows these steps: 
 
 The LLM generates an initial Gremlin query. 
 The query is executed within a Gremlin engine. 
 If the execution is successful, results are returned. 
 If execution fails, an error message parsing mechanism analyzes the returned errors and refines the query using LLM-based feedback. 
 
This iterative refinement makes sure the generated queries align with the database‚Äôs structure and constraints, improving overall accuracy and usability. 
Prompt template 
Our final prompt template is as follows: 
 
 ## Request
Please write a gremlin query to answer the given question:
{{question}}
You will be provided with couple relevant vertices, together with their 
schema and other information.
Please choose the most relevant vertex according to its schema and other 
information to make the gremlin query correct.


## Instructions
1. Here are related vertices and their details:
{{schema}}
2. Don't rename properties.
3. Don't change lines (using slash n) in the generated query.


## IMPORTANT
Return the results in the following XML format:

&lt;Results&gt;
    &lt;Query&gt;INSERT YOUR QUERY HERE&lt;/Query&gt;
    &lt;Explanation&gt;
        PROVIDE YOUR EXPLANATION ON HOW THIS QUERY WAS GENERATED 
        AND HOW THE PROVIDED SCHEMA WAS LEVERAGED
    &lt;/Explanation&gt;
&lt;/Results&gt; 
 
Comparing LLM-generated queries to ground truth 
We implemented an LLM-based evaluation system using Anthropic‚Äôs Claude 3.5 Sonnet on Amazon Bedrock as a judge to assess both query generation and execution results for Amazon Nova Pro and a benchmark model. The system operates in two key areas: 
 
 Query evaluation ‚Äì Assesses correctness, efficiency, and similarity to ground-truth queries; calculates exact matching component percentages; and provides an overall rating based on predefined rules developed with domain experts 
 Execution evaluation ‚Äì Initially used a single-stage approach to compare generated results with ground truth, then enhanced to a two-stage evaluation process: 
   
   Item-by-item verification against ground truth 
   Calculation of overall match percentage 
    
 
Testing across 120 questions demonstrated the framework‚Äôs ability to effectively distinguish correct from incorrect queries. The two-stage approach particularly improved the reliability of execution result evaluation by conducting thorough comparison before scoring. 
Experiments and results 
In this section, we discuss the experiments we conducted and their results. 
Query similarity 
In the query evaluation case, we propose two metrics: query exact match and query overall rating. An exact match score is calculated by identifying matching vs. non-matching components between generated and ground truth queries. The following table summarizes the scores for query exact match. 
 
  
   
    
   Easy 
   Medium 
   Hard 
   Overall 
   
   
   Amazon Nova Pro 
   82.70% 
   61% 
   46.60% 
   70.36% 
   
   
   Benchmark Model  
   92.60% 
   68.70% 
   56.20% 
   78.93% 
   
  
 
An overall rating is provided after considering factors including query correctness, efficiency, and completeness as instructed in the prompt. The overall rating is on scale 1‚Äì10. The following table summarizes the scores for query overall rating. 
 
  
   
    
   Easy 
   Medium 
   Hard 
   Overall 
   
   
   Amazon Nova Pro 
   8.7 
   7 
   5.3 
   7.6 
   
   
   Benchmark Model 
   9.7 
   8 
   6.1 
   8.5 
   
  
 
One limitation in the current query evaluation setup is that we rely solely on the LLM‚Äôs ability to compare ground truth against LLM-generated queries and arrive at the final scores. As a result, the LLM can fail to align with human preferences and under- or over-penalize the generated query. To address this, we recommend working with a subject matter expert to include domain-specific rules in the evaluation prompt. 
Execution accuracy 
To calculate accuracy, we compare the results of the LLM-generated Gremlin queries against the results of ground truth queries. If the results from both queries match exactly, we count the instance as correct; otherwise, it is considered incorrect. Accuracy is then computed as the ratio of correct query executions to the total number of queries tested. This metric provides a straightforward evaluation of how well the model-generated queries retrieve the expected information from the graph database, facilitating alignment with the intended query logic. 
The following table summarizes the scores for execution results count match. 
 
  
   
    
   Easy 
   Medium 
   Hard 
   Overall 
   
   
   Amazon Nova Pro 
   80% 
   50% 
   10% 
   60.42% 
   
   
   Benchmark Model 
   90% 
   70% 
   30% 
   74.83% 
   
  
 
Query execution latency 
In addition to accuracy, we evaluate the efficiency of generated queries by measuring their runtime and comparing it with the ground truth queries. For each query, we record the runtime in milliseconds and analyze the difference between the generated query and the corresponding ground truth query. A lower runtime indicates a more optimized query, whereas significant deviations might suggest inefficiencies in query structure or execution planning. By considering both accuracy and runtime, we gain a more comprehensive assessment of query quality, making sure the generated queries are correct and performant within the graph database. The following box plot showcases query execution latency with respect to time for the ground truth query and the query generated by Amazon Nova Pro. As illustrated, all three types of queries exhibit comparable runtimes, with similar median latencies and overlapping interquartile ranges. Although the ground truth queries display a slightly wider range and a higher outlier, the median values across all three groups remain close. This suggests that the model-generated queries are at the same level as human-written ones in terms of execution efficiency, supporting the claim that AI-generated queries are of similar quality and don‚Äôt incur additional latency overhead. 
 
Query generation latency and cost 
Finally, we compare the time taken to generate each query and calculate the cost based on token consumption. More specifically, we measure the query generation time and track the number of tokens used, because most LLM-based APIs charge based on token usage. By analyzing both the generation speed and token cost, we can determine whether the model is efficient and cost-effective. These results provide insights in selecting the optimal model that balances query accuracy, execution efficiency, and economic feasibility. 
As shown in the following plots, Amazon Nova Pro consistently outperforms the benchmark model in both generation latency and cost. In the left plot, which depicts query generation latency, Amazon Nova Pro demonstrates a significantly lower median generation time, with most values clustered between 1.8‚Äì4 seconds, compared to the benchmark model‚Äôs broader range from around 5‚Äì11 seconds. The right plot, illustrating query generation cost, shows that Amazon Nova Pro maintains a much smaller cost per query‚Äîcentered well below $0.005‚Äîwhereas the benchmark model incurs higher and more variable costs, reaching up to $0.025 in some cases. These results highlight Amazon Nova Pro‚Äôs advantage in terms of both speed and affordability, making it a strong candidate for deployment in time-sensitive or large-scale systems. 
 
Conclusion 
We experimented with all 120 ground truth queries provided to us by kscope.ai and achieved an overall accuracy of 74.17% in generating correct results. The proposed framework demonstrates its potential by effectively addressing the unique challenges of graph query generation, including handling heterogeneous vertex and edge properties, reasoning over complex graph structures, and incorporating domain knowledge. Key components of the framework, such as the integration of graph and domain knowledge, the use of Retrieval Augmented Generation (RAG) for query plan creation, and the iterative error-handling mechanism for query refinement, have been instrumental in achieving this performance. 
In addition to improving accuracy, we are actively working on several enhancements. These include refining the evaluation methodology to handle deeply nested query results more effectively and further optimizing the use of LLMs for query generation. Moreover, we are using the RAGAS-faithfulness metric to improve the automated evaluation of query results, resulting in greater reliability and consistency in assessing the framework‚Äôs outputs. 
 
About the authors 
Mengdie (Flora) Wang is a Data Scientist at AWS Generative AI Innovation Center, where she works with customers to architect and implement scalable Generative AI solutions that address their unique business challenges. She specializes in model customization techniques and agent-based AI systems, helping organizations harness the full potential of generative AI technology. Prior to AWS, Flora earned her Master‚Äôs degree in Computer Science from the University of Minnesota, where she developed her expertise in machine learning and artificial intelligence. 
Jason Zhang&nbsp;has expertise in machine learning, reinforcement learning, and generative AI. He earned his Ph.D. in Mechanical Engineering in 2014, where his research focused on applying reinforcement learning to real-time optimal control problems. He began his career at Tesla, applying machine learning to vehicle diagnostics, then advanced NLP research at Apple and Amazon Alexa. At AWS, he worked as a Senior Data Scientist on generative AI solutions for customers. 
Rachel Hanspal is a Deep Learning Architect at AWS Generative AI Innovation Center, specializing in end-to-end GenAI solutions with a focus on frontend architecture and LLM integration. She excels in translating complex business requirements into innovative applications, leveraging expertise in natural language processing, automated visualization, and secure cloud architectures. 
Zubair Nabi is the CTO and Co-Founder of Kscope, an Integrated Security Posture Management (ISPM) platform. His expertise lies at the intersection of Big Data, Machine Learning, and Distributed Systems, with over a decade of experience building software, data, and AI platforms. Zubair is also an adjunct faculty member at George Washington University and the author of Pro Spark Streaming: The Zen of Real-Time Analytics Using Apache Spark. He holds an MPhil from the University of Cambridge. 
Suparna Pal  ‚Äì CEO &amp; Co-Founder of kscope.ai ‚Äì 20+ years of journey of building innovative platforms &amp; solutions for Industrial, Health Care and IT operations at PTC, GE, and Cisco. 
Wan Chen is an Applied Science Manager at AWS Generative AI Innovation Center. As a ML/AI veteran in tech industry, she has wide range of expertise on traditional machine learning, recommender system, deep learning and Generative AI. She is a stronger believer of Superintelligence and is very passionate to push the boundary of AI research and application to enhance human life and drive business growth. She holds Ph.D in Applied Mathematics from University of British Columbia and had worked as postdoctoral fellow in Oxford University. 
Mu Li is a Principal Solutions Architect with AWS Energy. He‚Äôs also the Worldwide Tech Leader for the AWS Energy &amp; Utilities Technical Field Community (TFC), a community of 300+ industry and technical experts. Li is passionate about working with customers to achieve business outcomes using technology. Li has worked with customers to migrate all-in to AWS from on-prem and Azure, launch the Production Monitoring and Surveillance industry solution, deploy ION/OpenLink Endur on AWS, and implement AWS-based IoT and machine learning workloads. Outside of work, Li enjoys spending time with his family, investing, following Houston sports teams, and catching up on business and technology.
‚Ä¢ Incorporating responsible AI into generative AI project prioritization
  Over the past two years, companies have seen an increasing need to develop a project prioritization methodology for generative AI. There is no shortage of generative AI use cases to consider. Rather, companies want to evaluate the business value against the cost, level of effort, and other concerns, for a large number of potential generative AI projects. One new concern for generative AI compared to other domains is considering issues like hallucination, generative AI agents making incorrect decisions and then acting on those decisions through tool calls to downstream systems, and dealing with the rapidly changing regulatory landscape. In this post we describe how to incorporate responsible AI practices into a prioritization method to systematically address these types of concerns. 
Responsible AI overview 
The AWS Well-Architected Framework defines responsible AI as ‚Äúthe practice of designing, developing, and using AI technology with the goal of maximizing benefits and minimizing risks.‚Äù The AWS responsible AI framework begins by defining eight dimensions of responsible AI: fairness, explainability, privacy and security, safety, controllability, veracity and robustness, governance, and transparency. At key points in the development lifecycle, a generative AI team should consider the possible harms or risks for each dimension (inherent and residual risks), implements risk mitigations, and monitors risk on an ongoing basis. Responsible AI applies across the entire development lifecycle and should be considered during initial project prioritization. That‚Äôs especially true for generative AI projects, where there are novel types of risks to consider, and mitigations might not be as well understood or researched. Considering responsible AI up front gives a more accurate picture of project risk and mitigation level of effort and reduces the chance of costly rework if risks are uncovered later in the development lifecycle. In addition to potentially delayed projects due to rework, unmitigated concerns might also harm customer trust, result in representational harm, or fail to meet regulatory requirements. 
Generative AI prioritization 
While most companies have their own prioritization methods, here we‚Äôll demonstrate how to use the weighted shortest job first (WSJF) method from the Scaled Agile system. WSJF assigns a priority using this formula: 
Priority = (cost of delay) / (job size) 
The cost of delay is a measure of business value. It includes the direct value (for example, additional revenue or cost savings), the timeliness (such as, is shipping this project worth a lot more today than a year from now), and the adjacent opportunities (such as, would delivering this project open up other opportunities down the road). 
The job size is where you consider the level of effort to deliver the project. That normally includes direct development costs and paying for any infrastructure or software you need. The job size is where you can include the results of the initial responsible AI risk assessment and expected mitigations. For example, if the initial assessment uncovers three risks that require mitigation, you include the development cost for those mitigations in the job size. You can also qualitatively assess that a project with ten high-priority risks is more complex than a project with only two high-priority risks. 
Example scenario 
Now, let‚Äôs walk through a prioritization exercise that compares two generative AI projects. The first project uses a large language model (LLM) to generate product descriptions. A marketing team will use this application to automatically create production descriptions that go into the online product catalog website. The second project uses a text-to-image model to generate new visuals for advertising campaigns and the product catalog. The marketing team will use this application to more quickly create customized brand assets. 
First pass prioritization 
First, we‚Äôll go through the prioritization method without considering responsible AI, assigning a score of 1‚Äì5 for each part of the WSJF formula. The specific scores vary by organization. Some companies prefer to use t-shirt sizing (S, M, L, and XL), others prefer a score of 1‚Äì5, and others will use a more granular score. A score of 1‚Äì5 is a common and straightforward way to start. For example, the direct value scores can be calculated as: 
1 = no direct value 
2 = 20% improvement in KPI (time to create high-quality descriptions) 
3 = 40% improvement in KPI 
4 = 80% improvement in KPI 
5 = 100% or more improvement in KPI 
 
  
   
    
   Project 1: Automated product descriptions (scored from 1‚Äì5) 
   Project 2: Creating visual brand assets (scored from 1‚Äì5) 
   
   
   Direct value 
   3: Helps marketing team create higher quality descriptions more quickly 
   3: Helps marketing team create higher quality assets more quickly 
   
   
   Timeliness 
   2: Not particularly urgent 
   4: New ad campaign planned this quarter; without this project, cannot create enough brand assets without hiring a new agency to supplement the team 
   
   
   Adjacent opportunities 
   2: Might be able to reuse for similar scenarios) 
   3: Experience gained in image generation will build competence for future projects 
   
   
   Job size 
   2: Basic, well-known pattern 
   2: Basic, well-known pattern 
   
   
   Score 
   (3+2+2)/2 = 3.5 
   (3+4+3)/2 = 5 
   
  
 
At first glance, it looks like Project 2 is more compelling. Intuitively that makes sense‚Äîit takes people a lot longer to make high-quality visuals than to create textual product descriptions. 
Risk assessment 
Now let‚Äôs go through a risk assessment for each project. The following table lists a brief overview of the outcome of a risk assessment along each of the AWS responsible AI dimensions, along with a t-shirt size (S, M, L, and XL) severity level. The table also includes suggested mitigations. 
 
  
   
    
   Project 1: Automated product descriptions 
   Project 2: Creating visual brand assets 
   
   
   Fairness 
   L: Are descriptions appropriate in terms of gender and demographics? Mitigate using guardrails. 
   L: Images must not portray particular demographics in a biased way. Mitigate using human and automated checks. 
   
   
   Explainability 
   No risks identified. 
   No risks identified. 
   
   
   Privacy and security 
   L: Some product information is proprietary and cannot be listed on a public site. Mitigate using data governance. 
   L: Model must not be trained on any images that contain proprietary information. Mitigate using data governance. 
   
   
   Safety 
   M: Language must be age-appropriate and not cover offensive topics. Mitigate using guardrails. 
   L: Images must not contain adult content or images of drugs, alcohol, or weapons. Mitigate using guardrails. 
   
   
   Controllability 
   S: Need to track customer feedback on the descriptions. Mitigate using customer feedback collection. 
   L: Do images align to our brand guidelines? Mitigate using human and automated checks. 
   
   
   Veracity and robustness 
   M: Will the system hallucinate and imply product capabilities that aren‚Äôt real? Mitigate using guardrails. 
   L: Are images realistic enough to avoid uncanny valley effects? Mitigate using human and automated checks. 
   
   
   Governance 
   M: Prefer LLM providers that offer copyright indemnification. Mitigate using LLM provider selection. 
   L: Require copyright indemnification and image source attribution. Mitigate using model provider selection. 
   
   
   Transparency 
   S: Disclose that descriptions are AI generated. 
   S: Disclose that descriptions are AI generated. 
   
  
 
The risks and mitigations are use-case specific. The preceding table is for illustrative purposes only. 
Second pass prioritization 
How does the risk assessment affect the prioritization? 
 
  
   
    
   Project 1: Automated product descriptions (scored from 1‚Äì5) 
   Project 2: Creating visual brand assets (scored from 1‚Äì5) 
   
   
   Job size 
   3: Basic, well-known pattern; requires fairly standard guardrails, governance, and feedback collection. 
   5: Basic, well-known pattern. Requires advanced image guardrails with human oversight, and a more expensive commercial model. Research spike needed. 
   
   
   Score 
   (3+2+2)/3 = 2.3 
   (3+4+3)/5 = 2 
   
  
 
Now it looks like Project 1 is a better one to start with. Intuitively, after you consider responsible AI, that makes sense. Poorly crafted or offensive images are more noticeable and have a larger impact than a poorly phrased product description. And the guardrails you can use for maintaining image safety are less mature than the equivalent guardrails for text, particularly in ambiguous cases like adhering to brand guidelines. In fact, an image guardrail system might require training a monitoring model or using people to spot-check some percentage of the output. You might need to dedicate a small science team to study this problem first. 
Conclusion 
In this post, you saw how to include responsible AI considerations in a generative AI project prioritization method. You saw how conducting a responsible AI risk assessment in the initial prioritization phase can change the outcome by uncovering a substantial amount of mitigation work. Moving forward, you should develop your own responsible AI policy and start adopting responsible AI practices for generative AI projects. You can find additional details and resources at Transform responsible AI from theory into practice. 
 
About the author 
Randy DeFauw is a Sr. Principal Solutions Architect at AWS. He has over 20 years of experience in technology, starting with his university work on autonomous vehicles. He has worked with and for customers ranging from startups to Fortune 50 companies, launching Big Data and Machine Learning applications. He holds an MSEE and an MBA, serves as a board advisor to K-12 STEM education initiatives, and has spoken at leading conferences including Strata and GlueCon. He is the co-author of the books SageMaker Best Practices and Generative AI Cloud Solutions. Randy currently acts as a technical advisor to AWS‚Äô director of technology in North America.
‚Ä¢ Build scalable creative solutions for product teams with Amazon Bedrock
  Creative teams and product developers are constantly seeking ways to streamline their workflows and reduce time to market while maintaining quality and brand consistency. This post demonstrates how to use AWS services, particularly Amazon Bedrock, to transform your creative processes through generative AI. You can implement a secure, scalable solution that accelerates your creative workflow, such as managing product launches, creating marketing campaigns, or developing multimedia content. 
This post examines how product teams can deploy a generative AI application that enables rapid content iteration across formats. The solution addresses comprehensive needs‚Äîfrom product descriptions and marketing copy to visual concepts and video content for social media. By integrating with brand guidelines and compliance requirements, teams can significantly reduce time to market while maintaining creative quality and consistency. 
Solution overview 
Consider a product development team at an ecommerce company creating multimedia marketing campaigns for their seasonal product launches. Their traditional workflow has bottlenecks due to lengthy revisions, manual compliance reviews, and complex coordination across creative teams. The team is exploring solutions to rapidly iterate through creative concepts, generate multiple variations of marketing materials. 
By using Amazon Bedrock and Amazon Nova models, the team can transform its creative process. Amazon Nova models enable the generation of product descriptions and marketing copy. The team creates concept visuals and product mockups with Amazon Nova Canvas, and uses Amazon Nova Reel to produce engaging video content for social media presence. Amazon Bedrock Guardrails can help the team maintain consistent brand guidelines with configurable safeguards and governance for its generative AI applications at scale. 
The team can further enhance its brand consistency with Amazon Bedrock Knowledge Bases, which can serve as a centralized repository for brand style guides, visual identity documentation, and successful campaign materials. This comprehensive knowledge base makes sure generated content is informed by the organization‚Äôs historical success and established brand standards. Product specifications, market research, and approved messaging are seamlessly integrated into the creative process, enabling more relevant and effective content generation. 
With this solution, the team can simultaneously develop materials for multiple channels while maintaining consistent brand voice across their content. Creative professionals can now focus their energy on strategic decisions rather than repetitive tasks, leading to higher-quality outputs and improved team satisfaction. 
The following sample application creates a scalable environment that streamlines the creative workflow. It helps product teams move seamlessly from initial concept to market-ready materials with automated systems handling compliance and consistency checks throughout the journey. 
 
The solution‚Äôs workflow begins with the application engineer‚Äôs setup: 
 
 Creative assets and brand guidelines are securely stored in encrypted Amazon Simple Storage Service (Amazon S3) buckets. This content is then indexed in Amazon OpenSearch Service to create a comprehensive knowledge base. 
 Guardrails are configured to enforce brand standards and compliance requirements. 
 
The user experience flows from authentication to content delivery: 
 
 Creative team members access the interface through a secure portal hosted in Amazon S3. 
 Authentication is managed through Amazon Cognito. 
 Team members‚Äô submitted creative briefs or requirements are routed to Amazon API Gateway. 
 An AWS Lambda function queries relevant brand guidelines and assets from the knowledge base. 
 The Lambda function sends the contextual information from the knowledge base to Amazon Bedrock, along with the user‚Äôs creative briefs. 
 The prompt and generated response are filtered through Amazon Bedrock Guardrails. 
 Amazon Polly converts text into lifelike speech, generating audio streams that can be played immediately and stored in S3 buckets for later use. 
 The models‚Äô generated content is delivered to the user. 
 Chat history stored in Amazon DynamoDB. 
 
Prerequisites 
The following prerequisites are required before continuing: 
 
 An AWS account 
 An AWS Identity and Access Management (IAM) role with permission to manage AWS Marketplace subscriptions and AWS services 
 AWS services: 
   
   AWS CloudFormation 
   Amazon API Gateway 
   AWS CloudFormation 
   Amazon Cognito 
   Amazon DynamoDB 
   Amazon Polly 
   Amazon S3 
   Amazon Virtual Private Cloud (Amazon VPC) with two public subnets 
    
 Amazon Bedrock models enabled: 
   
   Amazon Nova Canvas 
   Amazon Nova Reels 
   Amazon Nova Pro 
   Amazon Nova Lite 
    
 Anthropic models (optional): 
   
   Anthropic‚Äôs Claude 3 Sonnet 
    
 
Select the Models to Use in Amazon Bedrock 
When working with Amazon Bedrock for generative AI applications, one of the Ô¨Årst steps is selecting which foundation models you want to access. Amazon Bedrock oÔ¨Äers a variety of models from diÔ¨Äerent providers, and you‚Äôll need to explicitly enable the ones we plan to use in this blog. 
 
 In the Amazon Bedrock console, find and select Model access from the navigation menu on the left. 
 Click the Modify model access button to begin selecting your models. 
 Select the following Amazon models: 
   
   Nova Canvas 
   Nova Premier Cross-region inference Nova Pro 
   Titan Embeddings G1 ‚Äì Text 
   Titan Text Embeddings V2 
    
 Select the Anthropic Claude 3.7 Sonnet model. 
 Choose Next. 
 Review your selections carefully on the summary page, then choose&nbsp;Submit to confirm your choices. 
 
Set up the CloudFormation template 
We use a use a CloudFormation template to deploy all necessary solution resources. Follow these steps to prepare your installation files: 
 
 Clone the GitHub repository: 
   
   git clone https://github.com/aws-samples/aws-service-catalog-reference-architectures.git
 
    
 Navigate to the solution directory: 
   
   cd aws-service-catalog-reference-architectures/blog_content/bedrock_genai
 
   (Make note of this location as you‚Äôll need it in the following steps) 
 Sign in to your AWS account with administrator privileges to ensure you can create all required AWS resources. 
 Create an S3 bucket in the AWS Region where you plan to deploy this solution. Remember the bucket name for later steps. 
 Upload the entire content folder to your newly created S3 bucket. 
 Navigate to the content/genairacer/src folder in your S3 bucket. 
 Copy the URL for the content/genairacer/src/genairacer_setup.json file. You‚Äôll need this URL for the deployment phase. 
 
Deploy the CloudFormation template 
Complete the following steps to use the provided CloudFormation template to automatically create and configure the application components within your AWS account: 
 
 On the CloudFormation console, choose Stacks in navigation pane. 
 Choose Create stack and select with new resources (standard). 
 On the Create stack page, under Specify template, for Object URL, enter the URL copied from the previous step, then choose Next. 
 On the Specify stack details page, enter a stack name. 
 Under Parameters, choose Next. 
 On the Configure stack options page, choose Next. 
 On the Review page, select the acknowledgement check boxes and choose Submit. 
 
Sign in to the Amazon Bedrock generative AI application 
Accessing your newly deployed application is simple and straightforward. Follow these steps to log in for the Ô¨Årst time and start exploring the Amazon Bedrock generative AI interface. 
 
 On the CloudFormation console, select the stack you deployed and select the Outputs tab. 
 Find the FrontendURL value and open the provided link. 
 When the sign-in screen displays, enter the username you speciÔ¨Åed during the CloudFormation deployment process. 
 Enter the temporary password that was sent to the email address you provided during setup. 
 After you sign in, follow the prompts to change your password. 
 Choose Send to conÔ¨Årm your new credentials. 
 
Once authenticated, you‚Äôll be directed to the main Amazon Bedrock generative AI dashboard, where you can begin exploring all the features and capabilities of your new application. 
Using the application 
Now that the application has been deployed, you can use it for text, image, and audio management. In the following sections, we explore some sample use cases. 
Text generation 
The creative team at the ecommerce company wants to draft compelling product descriptions. By inputting the basic product features and desired tone, the LLM generates engaging and persuasive text that highlights the unique selling points of each item, making sure the online store‚Äôs product pages are both informative and captivating for potential customers. 
To use the text generation feature and perform actions with the supported text models using Amazon Bedrock, follow these steps: 
 
 On the AWS CloudFormation console, go to the stack you created. 
 Choose the Outputs tab. 
 Choose the link for FrontendURL. 
 Log in using the credentials sent to the email you provided during the stack deployment process. 
 On the Text tab, enter your desired prompt in the input field. 
 Choose the specific model ID you want Amazon Bedrock to use from the available options. 
 Choose Run. 
 
Repeat this process for any additional prompts you want to process. 
 
Image generation 
The creative team can now conceptualize and produce stunning product images. By describing the desired scene, style, and product placement, they can enhance the online shopping experience and increase the likelihood of customer engagement and purchase.To use the image generation feature, follow these steps: 
 
 In the UI, choose the Images tab. 
 Enter your desired text-to-image prompt in the input field. 
 Choose the specific model ID you want Amazon Bedrock to utilize from the available options. 
 Optionally, choose the desired style of the image from the provided style options. 
 Choose Generate Image. 
 
Repeat this process for any additional prompts you want to process. 
  
Audio generation 
The ecommerce company‚Äôs creative team wants to develop audio content for marketing campaigns. By specifying the message, brand voice, target demographic, and audio components, they can compose scripts and generate voiceovers for promotional videos and audio ads, resulting in consistent and professional audio materials that effectively convey the brand‚Äôs message and values.To use the audio generation feature, follow these steps: 
 
 In the UI, choose the Audio tab. 
 Enter your desired prompt in the input field. 
 Choose Run. An audio file will appear and start to play. 
 Choose the file (right-click) and choose Save Audio As to save the file. 
 
 
Amazon Bedrock Knowledge Bases 
With Amazon Bedrock Knowledge Bases, you can provide foundation models (FMs) and agents with contextual information from your organization‚Äôs private data sources, to deliver more relevant, accurate, and tailored responses. It is a powerful and user-friendly implementation of the Retrieval Augmented Generation (RAG) approach. The application showcased in this post uses the Amazon Bedrock components in the backend, simplifying the process to merely uploading a document using the application‚Äôs GUI, and then entering a prompt that will query the documents you upload. 
For our example use case, the creative team now needs to research information about internal processes and customer data, which are typically stored in documentation. When this documentation is stored in the knowledge base, they can query it on the KnowledgeBase tab. The queries executed on this tab will search the documents for the specific information they are looking for. 
Manage documents 
The documents you have uploaded will be listed on the KnowledgeBase tab. To add more, complete the following steps: 
 
 In the UI, choose the KnowledgeBase tab. 
 Choose Manage Document. 
 Choose Browse, then choose a file. 
 Choose Upload. 
 
You will see a message confirming that the file was uploaded successfully.The Amazon Bedrock Knowledge Bases syncing process is triggered when the file is uploaded. The application will be ready for queries against the new document within a minute. 
Query the knowledge base 
To query the knowledge base, complete the following steps: 
 
 In the UI, choose the KnowledgeBase tab. 
 Enter your query in the input field. 
 For Model, choose the model you want Amazon Bedrock to use for performing the query. 
 Choose Run. 
 
The generated text response from Amazon Bedrock will appear. 
Amazon Bedrock guardrails 
You can use the Guardrails tab to manage your guardrails, and create and remove guardrails as needed. Guardrails are used on the Text tab when performing queries. 
Create a guardrail 
Complete the following steps to create a new guardrail: 
 
 In the UI, choose the Guardrails tab. 
 Enter the required fields or choose the appropriate options. 
 Choose the type of guardrail under Content Filter Type. 
 Choose Create Guardrail. 
 
The newly created guardrail will appear in the right pane. 
Delete a guardrail 
Complete the following steps to delete a guardrail: 
 
 In the UI, choose the Guardrails tab. 
 Choose the guardrail you want to delete in the right pane. 
 Choose the X icon next to the guardrail. 
 
By following these steps, you can effectively manage your guardrails, for a seamless and controlled experience when performing queries in the Text tab. 
Use guardrails 
The creative team requires access to information about internal processes and customer data, which are securely stored in documentation within the knowledge base. To enforce compliance with personally identifiable information (PII) guardrails, queries executed using the Text tab are designed to search documents for specific, non-sensitive information while preventing the exposure or inclusion of PII in both prompts and answers. This approach helps the team retrieve necessary data without compromising privacy or security standards. 
To use the guardrails feature, complete the following steps: 
 
 In the UI, choose the Text tab. 
 Enter your prompt in the input field. 
 For Model ID, choose the specific model ID you want Amazon Bedrock to use. 
 Turn on Guardrails. 
 For Select Filter, choose the guardrail you want to use. 
 Choose Run. 
 
The generated text from Amazon Bedrock will appear within a few seconds. Repeat this process for any additional prompts you want to process. 
 
Clean up 
To avoid incurring costs, delete resources that are no longer needed. If you no longer need the solution, complete the following steps to delete all resources you created from your AWS account: 
 
 On the AWS CloudFormation console, choose Stacks in the navigation pane. 
 Select the stack you deployed and choose Delete. 
 
Conclusion 
By combining Amazon Bedrock, Knowledge Bases, and Guardrails with Cognito, API Gateway, and Lambda, organizations can give employees powerful AI tools for text, image, and data work. This serverless approach integrates generative AI into daily workflows securely and scalably, boosting productivity and innovation across teams.. 
For more information about generative AI and Amazon Bedrock, refer to the Amazon Bedrock category in the AWS News Blog. 
 
About the authors 
 Kenneth Walsh is a Senior AI Acceleration Architect based in New York who transforms AWS builder productivity through innovative generative AI automation tools. With a strategic focus on standardized frameworks, Kenneth accelerates partner adoption of generative AI technologies at scale. As a trusted advisor, he guides customers through their GenAI journeys with both technical expertise and genuine passion. Outside the world of artiÔ¨Åcial intelligence, Kenneth enjoys crafting culinary creations, immersing himself in audiobooks, and cherishing quality time with his family and dog. 
Wanjiko Kahara is a New York‚Äìbased Solutions Architect with a interest area in generative AI. Wanjiko is excited about learning new technology to help her customers be successful. Outside of work, Wanjiko loves to travel, explore the outdoors, and read. 
Greg Medard is a Solutions Architect with AWS. Greg guides clients in architecting, designing, and developing cloud-optimized infrastructure solutions. His drive lies in fostering cultural shifts by embracing DevOps principles that overcome organizational hurdles. Beyond work, he cherishes quality time with loved ones, tinkering with the latest tech gadgets, or embarking on adventures to discover new destinations and culinary delights. 
Bezuayehu Wate is a Specialist Solutions Architect at AWS, with a focus on big data analytics. Passionate about helping customers design, build, and modernize their cloud-based analytics solutions, she Ô¨Ånds joy in learning and exploring new technologies. Outside of work, Bezuayehu enjoys quality time with family and traveling. 
Nicole Murray is a generative AI Senior Solutions Architect at AWS, specializing in MLOps and Cloud Operations for AI startups. With 17 years of experience‚Äîincluding helping government agencies design secure, compliant applications on AWS‚Äîshe now partners with startup founders to build and scale innovative AI/ML solutions. Nicole helps teams navigate secure cloud management, technical strategy, and regulatory best practices in the generative AI space, and is also a passionate speaker and educator known for making complex cloud and AI topics accessible.
‚Ä¢ Build a proactive AI cost management system for Amazon Bedrock ‚Äì Part 2
  In Part 1 of our series, we introduced a proactive cost management solution for Amazon Bedrock, featuring a robust cost sentry mechanism designed to enforce real-time token usage limits. We explored the core architecture, token tracking strategies, and initial budget enforcement techniques that help organizations control their generative AI expenses. 
Building upon that foundation, this post explores advanced cost monitoring strategies for generative AI deployments. We introduce granular custom tagging approaches for precise cost allocation, and develop comprehensive reporting mechanisms. 
Solution overview 
The cost sentry solution introduced in Part 1 was developed as a centralized mechanism to proactively limit generative AI usage to adhere to prescribed budgets. The following diagram illustrates the core components of the solution, adding in cost monitoring through AWS Billing and Cost Management. 
 
Invocation-level tagging for enhanced traceability 
Invocation-level tagging extends our solution‚Äôs capabilities by attaching rich metadata to every API request, creating a comprehensive audit trail within Amazon CloudWatch logs. This becomes particularly valuable when investigating budget-related decisions, analyzing rate-limiting impacts, or understanding usage patterns across different applications and teams. To support this, the main AWS Step Functions workflow was updated, as illustrated in the following figure. 
 
Enhanced API input 
We also evolved the API input to support custom tagging. The new input structure introduces optional parameters for model-specific configurations and custom tagging: 
 
 {
&nbsp;&nbsp;"model": "string", &nbsp; &nbsp; // e.g., "claude-3" or "anthropic.claude-3-sonnet-20240229-v1:0"
&nbsp;&nbsp;"prompt": {
&nbsp;&nbsp; &nbsp;"messages": [
&nbsp;&nbsp; &nbsp; &nbsp;{
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"role": "string", &nbsp; &nbsp;// "system", "user", or "assistant"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"content": "string"
&nbsp;&nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp;],
&nbsp;&nbsp; &nbsp;"parameters": {
&nbsp;&nbsp; &nbsp; &nbsp;"max_tokens": number, &nbsp;&nbsp; // Optional, model-specific defaults
&nbsp;&nbsp; &nbsp; &nbsp;"temperature": number, &nbsp; // Optional, model-specific defaults
&nbsp;&nbsp; &nbsp; &nbsp;"top_p": number, &nbsp; &nbsp; &nbsp; &nbsp; // Optional, model-specific defaults
&nbsp;&nbsp; &nbsp; &nbsp;"top_k": number &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;// Optional, model-specific defaults
&nbsp;&nbsp; &nbsp;}
&nbsp;&nbsp;},
&nbsp;&nbsp;"tags": {
&nbsp;&nbsp; &nbsp;"applicationId": "string", &nbsp;// Required
&nbsp;&nbsp; &nbsp;"costCenter": "string", &nbsp; &nbsp; // Optional
&nbsp;&nbsp; &nbsp;"environment": "string" &nbsp; &nbsp;&nbsp;// Optional - dev/staging/prod
&nbsp;&nbsp;}
} 
 
The input structure comprises three key components: 
 
 model ‚Äì Maps simple names (for example, claude-3) to full Amazon Bedrock model IDs (for example, anthropic.claude-3-sonnet-20240229-v1:0) 
 input ‚Äì Provides a messages array for prompts, supporting both single-turn and multi-turn conversations 
 tags ‚Äì Supports application-level tracking, with applicationId as the required field and costCenter and environment as optional fields 
 
In this example, we use different cost centers for sales, services, and support to simulate the use of a business attribute to track usage and spend for inference in Amazon Bedrock. For example: 
 
 {
&nbsp;&nbsp;"model": "claude-3-5-haiku",
&nbsp;&nbsp;"prompt": {
&nbsp;&nbsp; &nbsp;"messages": [
&nbsp;&nbsp; &nbsp; &nbsp;{
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"role": "user",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"content": "Explain the benefits of using S3 using only 100 words."
&nbsp;&nbsp; &nbsp; &nbsp;},
&nbsp;&nbsp; &nbsp; &nbsp;{
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"role": "assistant",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"content": "You are a helpful AWS expert."
&nbsp;&nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp;],
&nbsp;&nbsp; &nbsp;"parameters": {
&nbsp;&nbsp; &nbsp; &nbsp;"max_tokens": 2000,
&nbsp;&nbsp; &nbsp; &nbsp;"temperature": 0.7,
&nbsp;&nbsp; &nbsp; &nbsp;"top_p": 0.9,
&nbsp;&nbsp; &nbsp; &nbsp;"top_k": 50
&nbsp;&nbsp; &nbsp;}
&nbsp;&nbsp;},
&nbsp;&nbsp;"tags": {
&nbsp;&nbsp; &nbsp;"applicationId": "aws-documentation-helper",
&nbsp;&nbsp; &nbsp;"costCenter": "support",
&nbsp;&nbsp; &nbsp;"environment": "production"
&nbsp;&nbsp;}
} 
 
Validation and tagging 
A new validation step was added to the workflow for tagging. This step uses an AWS Lambda function to add validation checks and maps the model requested to the specific model ID in Amazon Bedrock. It supplements the tags object with tags that will be required for downstream analysis. 
The following code is an example of a simple map to get the appropriate model ID from the model specified: 
 
 MODEL_ID_MAPPING =&nbsp;{
&nbsp;&nbsp; &nbsp;"nova-lite": "amazon.nova-lite-v1:0",
&nbsp;&nbsp; &nbsp;"nova-micro": "amazon.nova-micro-v1:0",
&nbsp;&nbsp; &nbsp;"claude-2": "anthropic.claude-v2:0",
&nbsp;&nbsp; &nbsp;"claude-3-haiku": "anthropic.claude-3-haiku-20240307-v1:0",
&nbsp;&nbsp; &nbsp;"claude-3-5-sonnet-v2": "us.anthropic.claude-3-5-sonnet-20241022-v2:0",
&nbsp;&nbsp; &nbsp;"claude-3-5-haiku": "us.anthropic.claude-3-5-haiku-20241022-v1:0"
} 
 
Logging and analysis 
By using CloudWatch metrics with custom-generated tags and dimensions, you can track detailed metrics across multiple dimensions such as model type, cost center, application, and environment. Custom tags and dimensions show how teams use AI services. To see this analysis, steps were implemented to generate custom tags, store metric data, and analyze metric data: 
 
 We include a unique set of tags that capture contextual information. This can include user-supplied tags as well as ones that are dynamically generated, such as requestId and timestamp: 
   
   &nbsp;&nbsp;"tags": {
&nbsp;&nbsp; &nbsp;"requestId": "ded98994-eb76-48d9-9dbc-f269541b5e49",
&nbsp;&nbsp; &nbsp;"timestamp": "2025-01-31T14:05:26.854682",
&nbsp;&nbsp; &nbsp;"applicationId": "aws-documentation-helper",
&nbsp;&nbsp; &nbsp;"costCenter": "support",
&nbsp;&nbsp; &nbsp;"environment": "production"
} 
    
 As each workflow is executed, the limit for each model will be evaluated to make sure the request is within budgetary guidelines. The workflow will end based on three possible outcomes: 
   
   Rate limit approved and invocation successful 
   Rate limit approved and invocation unsuccessful 
   Rate limit denied 
   The custom metric data is saved in CloudWatch in the GenAIRateLimiting namespace. This namespace includes the following key metrics: 
   
   TotalRequests ‚Äì Counts every invocation attempt regardless of outcome 
   RateLimitApproved ‚Äì Tracks requests that passed rate limiting checks 
   RateLimitDenied ‚Äì Tracks requests blocked by rate limiting 
   InvocationFailed ‚Äì Counts requests that failed during model invocation 
   InputTokens ‚Äì Measures input token consumption for successful requests 
   OutputTokens ‚Äì Measures output token consumption for successful requests 
   Each metric includes dimensions for Model, ModelId, CostCenter, Application, and Environment for data analysis. 
 We use CloudWatch metrics query capabilities with math expressions to analyze the data collected by the workflow. The data can be displayed in a variety of visual formats to get a granular view of requests by the dimensions provided, such as model or cost center. The following screenshot shows an example dashboard that displays invocation metrics where one model has reached its limit. 
 
 
Additional Amazon Bedrock analytics 
In addition to the custom metrics dashboard, CloudWatch provides automatic dashboards for monitoring Amazon Bedrock performance and usage. The Bedrock dashboard offers visibility into key performance metrics and operational insights, as shown in the following screenshot. 
 
Cost tagging and reporting 
Amazon Bedrock has introduced application inference profiles, a new capability that organizations can use to apply custom cost allocation tags to track and manage their on-demand foundation model (FM) usage. This feature addresses a previous limitation where tagging wasn‚Äôt possible for on-demand FMs, making it difficult to track costs across different business units and applications. You can now create custom inference profiles for base FMs and apply cost allocation tags like department, team, and application identifiers. These tags integrate with AWS cost management tools including AWS Cost Explorer, AWS Budgets, and AWS Cost Anomaly Detection, enabling detailed cost analysis and budget control. 
Application inference profiles 
To start, you must create application inference profiles for each type of usage you want to track. In this case, the solution defines custom tags for costCenter, environment, and applicationId. An inference profile will also be based on an existing Amazon Bedrock model profile, so you must combine the desired tags and model into the profile. At the time of writing, you must use the AWS Command Line Interface (AWS CLI) or AWS API to create one. See the following example code: 
 
 aws bedrock create-inference-profile \
&nbsp;&nbsp;--inference-profile-name "aws-docs-sales-prod" \
&nbsp;&nbsp;--model-source '{"copyFrom": &nbsp;"arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-3-haiku-20240307-v1:0"}' \
&nbsp;&nbsp;--tags '[
&nbsp;&nbsp; &nbsp;{"key": "applicationId", "value": "aws-documentation-helper"},
&nbsp;&nbsp; &nbsp;{"key": "costCenter", "value": "sales"},
&nbsp;&nbsp; &nbsp;{"key": "environment", "value": "production"}
&nbsp;&nbsp;]' 
 
This command creates a profile for the sales cost center and production environment using Anthropic‚Äôs Claude Haiku 3.5 model. The output from this command is an Amazon Resource Name (ARN) that you will use as the model ID. In this solution, the ValidateAndSetContext Lambda function was modified to allow for specifying the model by cost center (for example, sales). To see which profiles you created, use the following command: 
aws bedrock list-inference-profiles --type-equals APPLICATION 
After the profiles have been created and the validation has been updated to map cost centers to the profile ARNs, the workflow will start running inference requests with the aligned profile. For example, when the user submits a request, they will specify the model as sales, services, or support to align with the three cost centers defined. The following code is a similar map to the previous example: 
 
 MODEL_ID_MAPPING =&nbsp;{
&nbsp;&nbsp; &nbsp;"sales": "arn:aws:bedrock:&lt;region&gt;:&lt;account&gt;:application-inference-profile/&lt;unique id1&gt;",
&nbsp;&nbsp; &nbsp;"services": "arn:aws:bedrock:&lt;region&gt;:&lt;account&gt;:application-inference-profile/&lt;unique id2&gt;",
&nbsp;&nbsp; &nbsp;"support": "arn:aws:bedrock:&lt;region&gt;:&lt;account&gt;:application-inference-profile/&lt;unique id3&gt;"
&nbsp;&nbsp; } 
 
To query CloudWatch metrics for the model usage correctly when using application inference profiles, you must specify the unique ID for the profile (the last part of the ARN). CloudWatch will store metrics like token usage based on the unique ID. To support both profile and direct model usage, the Lambda function was modified to add a new tag for modelMetric to be the appropriate term to use to query for token usage. See the following code: 
 
 &nbsp;&nbsp;"tags": {
&nbsp;&nbsp; &nbsp;"requestId": "ded98994-eb76-48d9-9dbc-f269541b5e49",
&nbsp;&nbsp; &nbsp;"timestamp": "2025-01-31T14:05:26.854682",
&nbsp;&nbsp; &nbsp;"applicationId": "aws-documentation-helper",
&nbsp;&nbsp; &nbsp;"costCenter": "support",
&nbsp;&nbsp; &nbsp;"environment": "production",&nbsp;&nbsp; &nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;"modelMetric": "&lt;unique id&gt; | &lt;model id&gt;"
&nbsp; } 
 
Cost Explorer 
Cost Explorer is a powerful cost management tool that provides comprehensive visualization and analysis of your cloud spending across AWS services, including Amazon Bedrock. It offers intuitive dashboards to track historical costs, forecast future expenses, and gain insights into your cloud consumption. With Cost Explorer, you can break down expenses by service, tags, and custom dimensions, for detailed financial analysis. The tool updates on a daily basis. 
When you use application inference profiles with Amazon Bedrock, your AI service usage is automatically tagged and flows directly into Billing and Cost Management. These tags enable detailed cost tracking across different dimensions like cost center, application, and environment. This means you can generate reports that break down Amazon Bedrock AI expenses by specific business units, projects, or organizational hierarchies, providing clear visibility into your generative AI spending. 
Cost allocation tags 
Cost allocation tags are key-value pairs that help you categorize and track AWS resource costs across your organization. In the context of Amazon Bedrock, these tags can include attributes like application name, cost center, environment, or project ID. To activate a cost allocation tag, you must first enable it on the Billing and Cost Management console. After they‚Äôre activated, these tags will appear in your AWS Cost and Usage Report (CUR), helping you break down Amazon Bedrock expenses with granular detail. 
To activate a cost allocation tag, complete the following steps: 
 
 On the Billing and Cost Management console, in the navigation pane, choose Cost Allocation Tags. 
 Locate your tag (for this example, it‚Äôs named costCenter) and choose Activate. 
 Confirm the activation. 
 
After activation, the costCenter tag will appear in your CUR and will be used in Cost Explorer. It might take 24 hours for the tag to become fully active in your billing reports. 
 
Cost Explorer reporting 
To create an Amazon Bedrock usage report in Cost Explorer based on your tag, complete the following steps: 
 
 On the Billing and Cost Management console, choose Cost Explorer in the navigation pane. 
 Set your desired date range (relative time range or custom period). 
 Select Daily or Monthly granularity. 
 On the Group by dropdown menu, choose Tag. 
 Choose costCenter as the tag key. 
 Review the displayed Amazon Bedrock costs broken down by each unique cost center value. 
 Optionally, filter the values by applying a filter in the Filters section: 
   
   Choose Tag filter. 
   Choose the costCenter tag. 
   Choose specific cost center values you want to analyze. 
    
 
The resulting report will provide a detailed view of Amazon Bedrock AI service expenses, helping you compare spending across different organizational units or projects with precision. 
 
Summary 
The AWS Cost and Usage Reports (including budgets) act as trailing edge indicators because they show what you‚Äôve already spent on Amazon Bedrock after the fact. By blending real-time alerts from Step Functions with comprehensive cost reports, you can get a 360-degree view of your Amazon Bedrock usage. This reporting can alert you before you overspend and help you understand your actual consumption. This approach gives you the power to manage AI resources proactively, keeping your innovation budget on track and your projects running smoothly. 
Try out this cost management approach for your own use case, and share your feedback in the comments. 
 
About the Author 
Jason Salcido is a Startups Senior Solutions Architect with nearly 30 years of experience pioneering innovative solutions for organizations from startups to enterprises. His expertise spans cloud architecture, serverless computing, machine learning, generative AI, and distributed systems. Jason combines deep technical knowledge with a forward-thinking approach to design scalable solutions that drive value, while translating complex concepts into actionable strategies.

‚∏ª