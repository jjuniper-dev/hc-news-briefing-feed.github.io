‚úÖ Morning News Briefing ‚Äì August 27, 2025 10:43

üìÖ Date: 2025-08-27 10:43
üè∑Ô∏è Tags: #briefing #ai #publichealth #digitalgov

‚∏ª

üßæ Weather
‚Ä¢ Current Conditions:  10.9¬∞C
  Temperature: 10.9&deg;C Pressure / Tendency: 101.8 kPa rising Humidity: 82 % Dewpoint: 8.0&deg:C Wind: WNW 7 km/h Air Quality Health Index: n/a Observed at: Pembroke 6:00 AM EDT Wednesday 27 August 2025 . Weather: 10/10.9/
‚Ä¢ Wednesday: Mainly sunny. High 21.
  Fog patches dissipating this morning . Mainly sunny this morning with a high of 21 degrees Celsius . Wind becoming northwest 20 km/h late this morning, with the risk of rain . UV index 6 or high for the north-to-south-east Canada, with an average of 6 degrees Celsius in the mid to mid-90s . Forecast issued 5:00 AM EDT
‚Ä¢ Wednesday night: Chance of showers. Low 13. POP 40%
  A few clouds. Increasing cloudiness late this evening then 40 percent chance of showers before morning . Wind southwest 20 km/h becoming light late this night . Wind becoming southwest 30 before morning. Low 13.50 degrees Celsius in New York City on August 27, 2025 . Forecast issued 5:00 AM EDT Wednesday 27 August 2025. Weather forecast: Showers, rain, snow,

üåç International News
No updates.

üçÅ Canadian News
No updates.

üá∫üá∏ U.S. Top Stories
‚Ä¢ Are you a new grandparent? NPR wants to hear from you for National Grandparent's Day
  National Grandparents Day is Sep. 7 . NPR wants to hear from new grandparents about how your life has changed . NPR also wants to know how your grandparents have changed your life since you were a child . National Grandparent Day is September 7. For more information, visit NPR iReport.com/GrandparentsDay.com . In the U.S. call the National National Grand
‚Ä¢ In the brain, a lost limb is never really gone
  Even years after an arm is amputated, the brain maintains a detailed map of the limb and tries to interact with this phantom appendage . The brain is still mapping the limb years after it was amputated . Even years later an arm can still be seen as a phantom limb in the brain as a result of the amputation . Brain maps the limb in a detailed way that the brain
‚Ä¢ Drowning prevention program comes to a halt at the CDC
  A few years in, a CDC program was ready to share its findings on how to mitigate the leading cause of death among young children . Then, the administration terminated that staff . The program was about to discuss how to prevent drowning deaths among children, but was terminated by the Obama administration . Drowning is a leading cause for death among children in the U.S., according to the
‚Ä¢ Flag burning has a long history in the U.S. ‚Äî and legal protections from the Supreme Court
  President Trump's executive order challenges a landmark Supreme Court decision, according to free speech attorneys . Free speech attorneys say the executive order is a violation of free speech rights . The White House is expected to challenge a landmark decision by the Supreme Court in the next two weeks . The president's order is the first of its kind in the history of the White House to address the issue of gay marriage
‚Ä¢ What's tea? No, seriously. What's 'tea'?
  How did a word that simply referred to a millennia-old beverage come to be the latest iteration of "what's up?" "What's up" was the first word used to refer to an ancient beverage . The word was first used in the name of a centuries-old drink, but has never been used in a modern day of the word "what is up" "What is

üß† Artificial Intelligence
No updates.

üíª Digital Strategy
‚Ä¢ Unlike most of Musk's other ventures, Starship keeps it together for Flight Test 10
  SpaceX has finally managed a test flight of Starship without anything creating an impromptu firework display . Explosions all expected and on schedule this time this time . SpaceX finally managed to fly the rocket without creating a display of fireworks during the test flight . SpaceX has now managed to successfully test the Starship without creating any firework displays . SpaceX says it will be the first time it has flown without
‚Ä¢ Who are you again? Infosec experiencing 'Identity crisis' amid rising login attacks
  Infosec pros losing confidence in identity providers' ability to keep attackers out . Cisco-owned Duo warning that the industry is facing what it calls "identidentidentity crisis" Vendor insists passkeys are the future, but getting workers on board is proving difficult . Duo: Industry is facing an identity crisis, with the industry facing "an identity crisis" The company says passkeys will
‚Ä¢ Datacenters face rising thirst as Europe dries up
  Analysts warn cooling demands could outstrip supplies as heatwaves intensify . Water scarcity is rising up the agenda as one of the major concerns for datacenters in Europe . Hot and dry summer marked by intense heatwaves in southern parts of the continent . Analysts say cooling demands may outstrip supply of water in Europe following unusually hot and dry summers . Data centers in Europe could be hit
‚Ä¢ More than 100 companies are chasing an AI chip gold rush. Few will surive
  The number of companies developing AI processor chips now numbers well over a hundred, according to new research . Quick, get some investment money before the bubble bursts before it bursts, say experts . AI processor chip makers now have more than 100 companies working on the chip chips, new research says . The chip makers say they're working on a billion dollars a year to develop the chips that can be
‚Ä¢ Intel pitches Clearwater Forest as a consolidation play for all you hoarding ancient Xeons
  The first datacenter silicon to use Intel's two-nanometer-class 18A process tech won't arrive for a while yet, but that's not stopping the struggling x86 giant from making its sales pitch early . Chipzilla's part to use 18A is another core-packed monster Hot Chips . It's not the latest in a long line of core-pack monster Hot

üè• Public Health
No updates.

üî¨ Science
‚Ä¢ Emotional AI is here ‚Äî let‚Äôs shape it, not shun it
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ What USAID cuts mean for future mortality rates
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Repeated heatwaves can age you as much as smoking or drinking
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Mental health symptoms in Chinese children with sleep disorders and association with parental emotions
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Assessing the impact of interregional mobility on COVID19 spread in Spain using transfer entropy
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations

üßæ Government & Policy
No updates.

üèõÔ∏è Enterprise Architecture & IT Governance
No updates.

ü§ñ AI & Emerging Tech
‚Ä¢ India is still working on sewer robots
  The practice of extracting human excreta from toilets, sewers, or septic tanks is still practiced widely in India . Manual scavenging was responsible for more than 500 deaths between 2018 and 2023 . People who enter clogged sewers face the risk of asphyxiation from exposure to toxic gases like ammonia and methane . Several companies have emerged to offer alternatives at a wide range of technical complexity .
‚Ä¢ AI comes for the job market, security, and prosperity: The Debrief
  When I picked up my daughter from summer camp, we settled in for an eight-hour drive through the Appalachian mountains, heading from North Carolina to her grandparents‚Äô home in Kentucky. With little to no cell service for much of the drive, we enjoyed the rare opportunity to have a long, thoughtful conversation, uninterrupted by devices. The subject, naturally, turned to AI.&nbsp;







‚ÄúNo one my age wants AI. No one is excited about it,‚Äù she told me of her high-school-age peers. Why not? I asked. ‚ÄúBecause,‚Äù she replied, ‚Äúit seems like all the jobs we thought we wanted to do are going to go away.‚Äù&nbsp;



I was struck by her pessimism, which she told me was shared by friends from California to Georgia to New Hampshire. In an already fragile world, one increasingly beset by climate change and the breakdown of the international order, AI looms in the background, threatening young people‚Äôs ability to secure a prosperous future.



It‚Äôs an understandable concern. Just a few days before our drive, OpenAI CEO Sam Altman was telling the US Federal Reserve‚Äôs board of governors that AI agents will leave entire job categories ‚Äújust like totally, totally gone.‚Äù Anthropic CEO Dario Amodei told Axios he believes AI will wipe out half of all entry-level white-collar jobs in the next five years. Amazon CEO Andy Jassy said the company will eliminate jobs in favor of AI agents in the coming years. Shopify CEO Tobi L√ºtke told staff they had to prove that new roles couldn‚Äôt be done by AI before making a hire. And the view is not limited to tech. Jim Farley, the CEO of Ford, recently said he expects AI to replace half of all white-collar jobs in the US.&nbsp;



These are no longer mere theoretical projections. There is already evidence that AI is affecting employment. Hiring of new grads is down, for example, in sectors like tech and finance. While that is not entirely due to AI, the technology is almost certainly playing a role.&nbsp;



For Gen Z, the issue is broader than employment. It also touches on another massive generational challenge: climate change. AI is computationally intensive and requires massive data centers. Huge complexes have already been built all across the country, from Virginia in the east to Nevada in the west. That buildout is only going to accelerate as companies race to be first to create superintelligence. Meta and OpenAI have announced plans for data centers that will require five gigawatts of power just for their ¬≠computing‚Äîenough to power the entire state of Maine in the summertime.&nbsp;



It‚Äôs very likely that utilities will turn to natural gas to power these facilities; some already have. That means more carbon dioxide emissions for an already warming world. Data centers also require vast amounts of water. There are communities right now that are literally running out of water because it‚Äôs being taken by nearby data centers, even as climate change makes that resource more scarce.&nbsp;



Proponents argue that AI will make the grid more efficient, that it will help us achieve technological breakthroughs leading to cleaner energy sources and, I don‚Äôt know, more butterflies and bumblebees? But xAI is belching CO2 into the Memphis skies from its methane-fueled generators right now. Google‚Äôs electricity demand and emissions are skyrocketing today.&nbsp;



Things would be different, my daughter told me, if it were obviously useful. But for much of her generation, she argued, it‚Äôs a looming threat with ample costs and no obvious utility: ‚ÄúIt‚Äôs not good for research because it‚Äôs not highly accurate. You can‚Äôt use it for writing because it‚Äôs banned‚Äîand people get zeros on papers who haven‚Äôt even used it because of AI detectors. And it seems like it‚Äôs going to take all the good jobs. One teacher told us we‚Äôre all going to be janitors.‚Äù &nbsp;



It would be na√Øve to think we are going back to a world without AI. We‚Äôre not. And yet there are other urgent problems that we need to address to build security and prosperity for coming generations. This September/October issue is about our attempts to make the world more secure. From missiles. From asteroids. From the unknown. From threats both existential and trivial.&nbsp;



We‚Äôre also introducing three new columns in this issue, from some of our leading writers: The Algorithm, which covers AI; The Checkup, on biotech; and The Spark, on energy and climate. You‚Äôll see these in future issues, and you can also subscribe online to get them in your inbox every week.¬†



Stay safe out there.&nbsp;
‚Ä¢ Job titles of the future: Satellite streak astronomer
  Up to 40% of images captured by the Vera Rubin Observatory within its first 10 years of operation will be marred by their sunlight-reflecting streaks . Satellites could also confuse astronomers when the sudden brightening they cause gets mistaken for astronomical phenomena . An entirely new subdiscipline of astronomical image processing has emerged, focusing on techniques to remove satellite light pollution from the data and designing observation protocols to prevent too-bright satellites from spoiling the views .
‚Ä¢ 3 Things James O‚ÄôDonnell is into right now
  Overthink is a podcast in which two smart people draw unexpected philosophical connections between facets of modern life . Ellie Anderson and David Pe√±a-Guzm√°n have done hour-long episodes on everything from mommy issues to animal justice, with particularly sharp segments on tech-adjacent issues like biohacking and the relationship between AI and art . Mountainhead, a darkly funny film by the creator of Succession, follows four unlikable tech founders as they watch the world collapse under political turmoil and violence caused by AI deepfakes .
‚Ä¢ Reimagining sound and space
  On a typical afternoon, MIT‚Äôs new Edward and Joyce Linde Music Building hums with life. On the fourth floor, a jazz combo works through a set in a rehearsal suite as engineers adjust microphone levels in a nearby control booth. Downstairs, the layered rhythms of Senegalese drumming pulse through a room built to absorb its force. In the building‚Äôs makerspace, students solder circuits, prototype sensor systems, and build instruments. Just off the main lobby, beneath the 50-foot ¬≠ceiling of the circular Thomas Tull Concert Hall, another group tests how the room, whose acoustics can be calibrated to shift with each performance, responds to its sound.



Situated behind Kresge Auditorium on the site of a former parking lot, the Linde building doesn‚Äôt mark the beginning of a serious commitment to music at MIT‚Äîit amplifies an already strong program. Every year, more than 1,500 students enroll in music classes, and over 500 take part in one of the Institute‚Äôs 30 ensembles, from the MIT Symphony Orchestra to the Fabulous MIT Laptop Ensemble, which creates electronic music using laptops and synthesizers. They rehearse and perform in venues across campus, including Killian Hall, Kresge, and a network of practice rooms, but the Linde Building provides a dedicated home to meet the depth, range, and ambition of music at MIT.



‚ÄúIt would be very difficult to teach biology or engineering in a studio designed for dance or music,‚Äù Jay Scheib, section head for Music and Theater Arts, told MIT News shortly before the building officially opened. ‚ÄúThe same goes for teaching music in a mathematics or chemistry classroom. In the past, we‚Äôve done it, but it did limit us.‚Äù He said the new space would allow MIT musicians to hear their music as it was intended to be heard and ‚Äúprovide an opportunity to convene people to inhabit the same space, breathe the same air, and exchange ideas and perspectives.‚Äù



The building, made possible by a gift from the late philanthropists Edward ‚Äô62 and Joyce Linde, has already transformed daily music life on campus. Musicians, engineers, and designers now cross paths more often as they make use of its rehearsal rooms, performance spaces, studios, and makerspace, and their ideas have begun converging in distinctly MIT ways. Antonis Christou, a second-year master‚Äôs student in the Opera of the Future group at the MIT Media Lab and an Emerson/Harris Scholar, says he‚Äôs there ‚Äúall the time‚Äù for classes, rehearsals, and composing.



‚ÄúIt‚Äôs really nice to have a dedicated space for music on campus. MIT does have very strong music and arts programs, so I think it reflects the strength of those programs,‚Äù says Valerie Chen ‚Äô22, MEng ‚Äô23, a cellist and PhD candidate in electrical engineering who works on interactive robotics. ‚ÄúBut more than that, I think it makes a statement that technology and the arts, and music in particular, are very interconnected.‚Äù



A building tuned for acoustics and performance



Acoustic innovation shaped every aspect of the building‚Äôs 35,000 square feet of space. From the outset, the design team faced a fundamental challenge: how to create a facility where radically different types of music could coexist without interference. Keeril Makan, the Michael (1949) and Sonja Koerner Music Composition Professor and associate dean of MIT‚Äôs School of Humanities, Arts, and Social Sciences (SHASS), helped lead that effort.



‚ÄúIt was important to me that we could have classical music happening in one space, world music in another space, jazz somewhere else, and also very fine measurements of sound all happening at the same time. And it really does that,‚Äù says Makan. ‚ÄúBut it took a lot of work to get there.‚Äù



Keeril Makan, professor of composition and associate dean of SHASS, helped spearhead the effort to create a building in which radically different kinds of musicmaking could happen simultaneously.WINSLOW TOWNSON




That work resulted in a building made up of three artfully interconnected blocks, creating three acoustically isolated zones: the Thomas Tull Concert Hall, the Erdely Music and Culture Space, and the Lim Music Maker Pavilion. Thick double shells of concrete enclose each zone, and their physical separation minimizes vibration transfer between them. One space for world music rests on a floating slab above the building‚Äôs underground parking garage and is constructed using a box-in-box method, with its inner room structurally isolated from the rest of the building. Other rooms use related techniques, with walls, floors, and ceilings separated by layers of sound-dampening materials and structural isolation systems to reduce sound transmission.



The building was designed by the Japanese architecture firm SANAA, in close collaboration with Nagata Acoustics, the team behind Berlin‚Äôs Pierre Boulez Saal. Inspired in part by that German hall, the 390-seat Thomas Tull Concert Hall is meant to serve musicians‚Äô varying acoustic needs. Inside, ceiling baffles and perimeter curtains make it possible to adapt the room on demand, shifting the acoustics from resonant and open for chamber music and classical performances to drier and more controlled for jazz or electronic music.



Makan and the acoustics team pushed for a 50-foot ceiling, a requirement from Nagata for acoustic flexibility and performance quality. The result is a concert hall that breaks from traditional form. Instead of occupying a raised stage facing rows of seats, performers in Tull Hall are positioned at the bottom of the space, with the audience seated around and above them. This layout alters the relationship between listeners and performers; audience members can choose to sit next to the string section or behind the pianist, experiencing sounds and sights typically reserved for musicians. The circular configuration encourages movement, intimacy, and a more immersive musical experience.&nbsp;



‚ÄúIt‚Äôs a big opportunity for creativity,‚Äù says Ian Hattwick, a lecturer in music technology. ‚ÄúYou can distribute musicians around the hall in interesting ways. I really encourage people in electronic music concerts to come up and get close. You can come up and peer over somebody‚Äôs shoulder while they‚Äôre playing. It‚Äôs definitely different. But I think it‚Äôs beautiful.‚Äù



That sense of openness shaped one of the first performances in the new hall. As part of the building‚Äôs opening-weekend event in February, called ‚ÄúSonic Jubilance,‚Äù the Fabulous MIT Laptop Ensemble (FaMLE), directed by Hattwick, took the stage, testing the venue‚Äôs variable acoustics and capacity for spatial experimentation as it employed laptops, gestural controllers, and other electronic devices to improvise and perform electronic music.



‚ÄúI was really struck by how good it sounded for what I do and for what FaMLE does,‚Äù says Hattwick. ‚ÄúThere‚Äôs a surround system of speakers. It was really fun and really satisfying, so I‚Äôm super excited to spend some more time working on spatial audio applications.‚Äù That evening, a concert featured performances by a diverse array of additional ensembles and world premieres by four MIT composers. It was the first moment many performers heard what the hall could do‚Äîand the first time they‚Äôd shared a space designed for all of them.






JONATHAN SACHS






JONATHAN SACHS






The community joined MIT music faculty, staff, and students for special workshops and short performances at the building‚Äôs public opening in February.




Since then, the hall has hosted a wide range of performances, from student recitals to concerts featuring guest artists. In the span of two weeks in March, the Boston Chamber Music Society celebrated the music of Faur√© and the Boston Symphony Chamber Players performed works by Aaron Copland, Brahms, and MIT‚Äôs own Makan. Other concerts have featured student compositions, historical instruments, and multichannel electronic works.&nbsp;



Just a few steps from the entrance to Tull Concert Hall, across the brick- and glass-lined lobby, the Beatrice and Stephen Erdely Music and Culture Space supports a different kind of sound. It‚Äôs designed to host rehearsals of percussion groups like Rambax MIT, the Institute‚Äôs Senegalese drumming ensemble, which uses hand-carved sabar drums, each played with a stick and open palm to produce tightly woven polyrhythms. At other times, students gather there around bronze-keyed instruments as they play with the Gamelan Galak Tika ensemble, practicing the interlocking patterns of Balinese kotekan.&nbsp;



Such music was originally meant to be performed in the open. The Music and Culture Space provides the physical and sonic headroom these traditions require, using materials chosen not only to isolate sound but also to let it breathe. Inside, the room thrums with rhythm, while just outside its walls, the rest of the building stays silent.



‚ÄúWe can imagine [world music] growing with this new home,‚Äù says Makan. Previously, these ensembles had rehearsed in a converted space inside the old MIT Museum building on Massachusetts Avenue, separated from the rest of the music program.&nbsp;



‚ÄúThey deserved their own space for so long,‚Äù says Hattwick, ‚Äúand it‚Äôs really fantastic that they managed to get it and that it is integrated in the music building the way that it is.‚Äù&nbsp;



The soaring ceiling of the Beatrice and Stephen Erdely Music and Culture Space provides the physical and sonic headroom for percussion ensembles.ADAM DETOUR




The building‚Äôs commitment to sound isolation extends beyond its rehearsal and performance spaces, and for faculty working in sound design and music technology, it has changed their daily rhythms. Mark Rau, an assistant professor of music technology with a joint appointment in electrical engineering and computer science (EECS), regularly uses speakers at high volume in his office‚Äîsomething that he says wouldn‚Äôt have been possible in MIT‚Äôs previous facilities.



‚ÄúAll the rooms in the building have good sound isolation, even the offices‚Äînot just the performance rooms, which is pretty great,‚Äù says Rau, whose second-floor office in the Jae S. and Kyuho Lim Music Maker Pavilion features gray acoustic panels lining the walls and ceiling. ‚ÄúTo be able to test the algorithms that I‚Äôm working on and things for homework assignments, and not bother my neighbors, is important.‚Äù&nbsp;



The attention to acoustic detail continues upstairs. On the fourth floor, Rau ran the first two sessions in the building‚Äôs new recording facilities, which were purpose-¬≠built to support both ensemble work and critical listening. He says they offer professional-¬≠quality recording.



The recording suite includes a large main room that can accommodate up to a dozen players, a smaller isolation booth for separating instruments or voices, and a control room designed for precise monitoring. Each space is acoustically treated and linked to the building‚Äôs dedicated audio network, so sound can be routed from any room in the building to any other in real time. &nbsp;



In the music technology research lab, undergraduate researchers (from left) Mouhammad Seck ‚Äô27, Anthony Wang ‚Äô28, and Alex Jin ‚Äô27 model the sounds of historic instruments‚Äî many of which are unplayable‚Äîfrom the collection of the MFA Boston.ADAM DETOUR




‚ÄúYou could record an entire symphony orchestra, and almost everybody could be in a different room,‚Äù says Hattwick. Or you could have the orchestra playing together in the concert hall and record it in one of the studios. The whole building uses a digital audio protocol called Dante, which allows low-latency, high-fidelity ¬≠transmission over Ethernet.



MIT multimedia specialist Cuco Daglio, who helped oversee technical planning, advocated for that level of fidelity. ‚ÄúIt‚Äôs a beautifully designed acoustic space,‚Äù says Hattwick.&nbsp;



The building‚Äôs exterior reflects a similar attention to performance. The arch above its entryway facing the Johnson Athletic Center and the Zesiger Sports and Fitness Center forms a conical shell that shapes and reflects sound, creating a natural stage. On warm days, music drifts out into the open air as groups rehearse beneath the overhang or students gather to play informally in small groups.&nbsp;



New program, new space



This fall, MIT is launching a new one-year master‚Äôs program in music technology, bringing together faculty from engineering and the arts. The Linde Music Building serves as the program‚Äôs home base, providing studios, tools, and collaborative spaces that students will use to design new instruments, software, and performance systems. Eran Egozy ‚Äô93, MEng ‚Äô95, professor of the practice in music technology and cofounder of Harmonix Music Systems, which developed Guitar Hero and Rock Band, directs the program. He developed the curriculum with Anna Huang, SM ‚Äô08, an associate professor with a joint appointment in music and EECS who did research on human-AI music collaboration technologies at Google, and he, Huang, and Rau are among its faculty.



Eran Egozy ‚Äô93, MEng ‚Äô95, professor of the practice in music technology and one of the masterminds behind Guitar Hero and Rock Band, directs the Institute‚Äôs new master‚Äôs program in music technology.KATE LEMMON




‚ÄúIt‚Äôs really about inventing new things,‚Äù says Egozy. ‚ÄúAsking questions like: What would the future musician want? What kinds of tools will a composer want?‚Äù



Rachel Loh ‚Äô25, who double-majored in computer science and engineering and music, will be part of the inaugural cohort. A vocalist with Syncopasian, MIT‚Äôs East Asian a cappella group, she draws on performance experience in her research. Her current project explores how AI systems improvise alongside human musicians, using visualizations to provide insight into machine decision-making.



‚ÄúIn high school, I knew I wanted to work at the intersection of music and computer science,‚Äù she says. ‚ÄúNow, this new music tech program is the perfect thing for me.‚Äù



A performance in the Thomas Tull Concert Hall.KATE LEMMON




A flexible workshop on the Music Maker Pavilion‚Äôs second floor will serve as a core space for the new program, outfitted with essentials like soldering stations, a laser cutter, and testing gear but left unfinished by design. Hattwick and Rau, who oversee the space, are allowing its exact form to emerge over time.&nbsp;



‚ÄúWe‚Äôve been spending this year outfitting it and starting to think about how we make all of these resources available to our students, and what the best way is to utilize this opportunity in this space,‚Äù Hattwick says. ‚Äú[The makerspace] directly supports research and our specific coursework.‚Äù&nbsp;



Already, students have begun to push the makerspace into new territory. Some are designing analog circuits and signal-¬≠boosting devices known as preamplifiers for musical instrument sensors. Others are experimenting with embedded systems that blur the boundary between physical and digital sound. In one class, students are building custom digital instruments from scratch‚Äîtools that don‚Äôt yet exist, shaped to suit musical ideas still in formation. The building‚Äôs infrastructure, including features like Dante, gives these projects unusual flexibility.



In March, the building served as a backdrop for large-scale projections of animated visuals created by students in MIT‚Äôs Interactive Design and Projection for Live Performance class.AV PRODUCTIONS




Ayyub Abdulrezak ‚Äô24, MEng ‚Äô25, one of Egozy‚Äôs students, worked in the makerspace to develop compact sensor boxes that combine a microphone, a Raspberry Pi board, and custom signal-processing software. Each device logs when and how long a campus piano is played, sending the data to a central server. The resulting heat maps could help inform tuning schedules, improve access, or guide planning for music spaces across MIT.



The makerspace also supports repair, maintenance, and modification. Hattwick describes it as a place to ‚Äúbuild and fix and maintain and explore new kinds of instruments,‚Äù where students can learn what it means to refine a musical system‚Äînot just in theory but in screws, solder, and code. Rau, who also builds guitars, is incorporating more hands-on fabrication into his courses, merging electronics with instrument making and repair to yield a unified design practice.






Alex Mazurenko ‚Äô28 is an undergraduate researcher working on slip casting, impedance testing, and musical instrument accessory designs. Here, he uses CAD software to design a custom saxophone mouthpiece.ADAM DETOUR






After 3D-printing his model, Mazurenko reviews the design with his advisor, senior postdoctoral associate Benjamin Sabatini.ADAM DETOUR














He then refines the prototype using tools in the makerspace, a workshop where students can fabricate analog circuits, musical sensors, and even custom instruments.ADAM DETOUR






Mazurenko brings the prototype to the Laboratory for Manufacturing and Productivity, where he images it in an x-ray CT scanner built by Lumafield, a startup founded by MIT alumni. He will use the scan to create a digital model for further testing and iteration.ADAM DETOUR











While the space is still growing into its full potential, its ethos is clear: experimentation at the intersection of sound, system, and student agency. These kinds of projects rely not only on equipment but on space where musicians can experiment, fail, and refine. As the new master‚Äôs program takes shape, that environment will be central to how students learn and create.



Building sound and community



For the first time, MIT musicians, technologists, composers, and researchers share a space designed to bring their disciplines into conversation. The building‚Äôs form encourages these exchanges. Its three wings connect through a glass-lined lobby filled with daylight and movement. Students pause there to talk, overhear a rehearsal in progress, or catch sight of a friend heading to a practice room.&nbsp;



Curves abound in the brick- and glass-lined lobby of the Edward and Joyce Linde Music Building. ADAM DETOUR




‚ÄúMusic is such a community thing,‚Äù says Christou. ‚ÄúI‚Äôve learned about concerts, or that someone is coming to visit, or I‚Äôve seen friends just studying or practicing. It‚Äôs really nice to have a hub with musical activity.‚Äù



Egozy sees these exchanges as central to the building‚Äôs mission. ‚ÄúIt‚Äôs the idea cross-pollination that happens when you just happen to run into someone you know, literally by the water cooler, and you‚Äôre just chatting about this or that,‚Äù he says. ‚ÄúThat‚Äôs my favorite part.‚Äù



Many of these encounters occur in the makerspace, where students working on entirely different projects end up asking each other questions, swapping tools, or launching ideas together.&nbsp;



‚ÄúLots of students from all different walks of life have been building instruments, prototyping different devices,‚Äù says Makan, who adds that he wants the new building to be ‚Äúa place for people to gather and hang out.‚Äù Many ensembles that once rehearsed in classrooms scattered across campus now work in adjoining rooms. ‚ÄúYou feel like something is always happening,‚Äù Christou says. ‚ÄúIt‚Äôs not just your practice or your rehearsal. It‚Äôs this sense of a shared rhythm.‚Äù



New frontiers for MIT‚Äôs music culture



Already, the Linde Music Building is affecting how music is conceived, taught, and experienced at MIT. Faculty members are rethinking syllabi to take advantage of the building‚Äôs multi-room routing capability and to delve more into spatial acoustics, interactive sound design, and even instrument making. Students are beginning to compose with acoustics in mind, treating the building itself as part of their instrument.



For example, Rau is engaging students in projects that explore room dynamics and acoustics as integral to music. In one class, students listen for differences in how music sounds in various parts of Tull Hall and observe changes when the curtains are used. Then they conduct acoustic measurements of the hall‚Äôs reverberation and build a digital copy of the hall, creating a sonic blueprint of the space that lets them produce artificial reverberation. Egozy, meanwhile, is developing tools that let performers engage audiences in new ways.&nbsp;



This June, one of those ideas was scaled up. As part of the International Computer Music Conference, MIT premiered a piece that invited audience members to shape the sound in real time using their phones. Musicians performed in Tull Hall, surrounded by a circular array of 24 speakers, with the audio shifting throughout the space in response to the audience input.&nbsp;



Undulating walls and an overhanging ring of glass panels help engineers customize the acoustics for each performance in the Thomas Tull Concert Hall.ADAM DETOUR




Performances like these are fueling growing interest in the building‚Äôs creative potential at MIT and beyond. Visiting composers have proposed site-specific works. Local ensembles are booking time to record in Tull Hall. Faculty are exploring how the building might support residencies that pair MIT researchers with performers working at the leading edges of both sound and computation.



The circular Tull Hall allows countless configurations for both performers and audiences. Here singers perform from the upper level of the hall while instrumentalists play from center stage at the base of the room.CAROLINE ALDEN




‚ÄúThis hall is really special. There‚Äôs nothing like it anywhere in the Boston area,‚Äù Egozy says. ‚ÄúWe will have a lot of really amazing events that will draw people into MIT. We‚Äôre excited about what it‚Äôs going to do for the MIT students, but it‚Äôs also going to do a lot just for the whole Boston area.‚Äù



Each day, students and faculty explore its possibilities‚Äîlinking rehearsal with recording, sound design with performance, tradition with experiment.



MIT is ‚Äúa place to enable exploration of new vistas, and really letting everyone pursue their path to what their vision is,‚Äù Hattwick says. ‚ÄúThe music building is just going to be like a huge boost to doing even more cool things in the future.‚Äù

üîí Cybersecurity & Privacy
‚Ä¢ DSLRoot, Proxies, and the Threat of ‚ÄòLegal Botnets‚Äô
  The cybersecurity community on Reddit responded in disbelief this month when a self-described Air National Guard member with top secret security clearance began questioning the arrangement they&#8217;d made with company called DSLRoot, which was paying $250 a month to plug a pair of laptops into the Redditor&#8217;s high-speed Internet connection in the United States. This post examines the history and provenance of DSLRoot, one of the oldest &#8220;residential proxy&#8221; networks with origins in Russia and Eastern Europe.

The query about DSLRoot came from a Reddit user &#8220;Sacapoopie,&#8221; who did not respond to questions. This user has since deleted the original question from their post, although some of their replies to other Reddit cybersecurity enthusiasts remain in the thread. The original post was indexed here by archive.is, and it began with a question:
&#8220;I have been getting paid 250$ a month by a residential IP network provider named DSL root to host devices in my home,&#8221; Sacapoopie wrote. &#8220;They are on a separate network than what we use for personal use. They have dedicated DSL connections (one per host) to the ISP that provides the DSL coverage. My family used Starlink. Is this stupid for me to do? They just sit there and I get paid for it. The company pays the internet bill too.&#8221;
Many Redditors said they assumed Sacapoopie&#8217;s post was a joke, and that nobody with a cybersecurity background and top-secret (TS/SCI) clearance would agree to let some shady residential proxy company introduce hardware into their network. Other readers pointed to a slew of posts from Sacapoopie in the Cybersecurity subreddit over the past two years about their work on cybersecurity for the Air National Guard.
When pressed for more details by fellow Redditors, Sacapoopie described the equipment supplied by DSLRoot as &#8220;just two laptops hardwired into a modem, which then goes to a dsl port in the wall.&#8221;

&#8220;When I open the computer, it looks like [they] have some sort of custom application that runs and spawns several cmd prompts,&#8221; the Redditor explained. &#8220;All I can infer from what I see in them is they are making connections.&#8221;
When asked how they became acquainted with DSLRoot, Sacapoopie told another user they discovered the company and reached out after viewing an advertisement on a social media platform.
&#8220;This was probably 5-6 years ago,&#8221; Sacapoopie wrote. &#8220;Since then I just communicate with a technician from that company and I help trouble shoot connectivity issues when they arise.&#8221;
Reached for comment, DSLRoot said its brand has been unfairly maligned thanks to that Reddit discussion. The unsigned email said DSLRoot is fully transparent about its goals and operations, adding that it operates under full consent from its &#8220;regional agents,&#8221; the company&#8217;s term for U.S. residents like Sacapoopie.
&#8220;As although we support honest journalism, we&#8217;re against of all kinds of &#8216;low rank/misleading Yellow Journalism&#8217; done for the sake of cheap hype,&#8221; DSLRoot wrote in reply. &#8220;It&#8217;s obvious to us that whoever is doing this, is either lacking a proper understanding of the subject or doing it intentionally to gain exposure by misleading those who lack proper understanding,&#8221; DSLRoot wrote in answer to questions about the company&#8217;s intentions.
&#8220;We monitor our clients and prohibit any illegal activity associated with our residential proxies,&#8221; DSLRoot continued. &#8220;We honestly didn&#8217;t know that the guy who made the Reddit post was a military guy. Be it an African-American granny trying to pay her rent or a white kid trying to get through college, as long as they can provide an Internet line or host phones for us &#8212; we&#8217;re good.&#8221;
WHAT IS DSLROOT?
DSLRoot is sold as a residential proxy service on the forum BlackHatWorld under the name DSLRoot and GlobalSolutions. The company is based in the Bahamas and was formed in 2012. The service is advertised to people who are not in the United States but who want to seem like they are. DSLRoot pays people in the United States to run the company&#8217;s hardware and software &#8212; including 5G mobile devices &#8212; and in return it rents those IP addresses as dedicated proxies to customers anywhere in the world &#8212; priced at $190 per month for unrestricted access to all locations.
The DSLRoot website.
The GlobalSolutions account on BlackHatWorld lists a Telegram account and a WhatsApp number in Mexico. DSLRoot&#8217;s profile on the marketing agency digitalpoint.com from 2010 shows their previous username on the forum was &#8220;Incorptoday.&#8221; GlobalSolutions user accounts at bitcointalk[.]org and roclub[.]com include the email clickdesk@instantvirtualcreditcards[.]com.
Passive DNS records from DomainTools.com show instantvirtualcreditcards[.]com shared a host back then &#8212; 208.85.1.164 &#8212; with just a handful of domains, including dslroot[.]com, regacard[.]com, 4groot[.]com, residential-ip[.]com, 4gemperor[.]com, ip-teleport[.]com, proxysource[.]net and proxyrental[.]net.
Cyber intelligence firm Intel 471 finds GlobalSolutions registered on BlackHatWorld in 2016 using the email address prepaidsolutions@yahoo.com. This user shared that their birthday is March 7, 1984.
Several negative reviews about DSLRoot on the forums noted that the service was operated by a BlackHatWorld user calling himself &#8220;USProxyKing.&#8221; Indeed, Intel 471 shows this user told fellow forum members in 2013 to contact him at the Skype username &#8220;dslroot.&#8221;
USProxyKing on BlackHatWorld, soliciting installations of his adware via torrents and file-sharing sites.
USProxyKing had a reputation for spamming the forums with ads for his residential proxy service, and he ran a &#8220;pay-per-install&#8221; program where he paid affiliates a small commission each time one of their websites resulted in the installation of his unspecified &#8220;adware&#8221; programs &#8212; presumably a program that turned host PCs into proxies. On the other end of the business, USProxyKing sold that pay-per-install access to others wishing to distribute questionable software &#8212; at $1 per installation.
Private messages indexed by Intel 471 show USProxyKing also raised money from nearly 20 different BlackHatWorld members who were promised shareholder positions in a new business that would offer robocalling services capable of placing 2,000 calls per minute.
Constella Intelligence, a platform that tracks data exposed in breaches, finds that same IP address GlobalSolutions used to register at BlackHatWorld was also used to create accounts at a handful of sites, including a GlobalSolutions user account at WebHostingTalk that supplied¬†the email address incorptoday@gmail.com. Also registered to incorptoday@gmail.com are the domains dslbay[.]com, dslhub[.]net, localsim[.]com, rdslpro[.]com, virtualcards[.]biz/cc, and virtualvisa[.]cc.
Recall that DSLRoot&#8217;s profile on digitalpoint.com was previously named Incorptoday. DomainTools says incorptoday@gmail.com is associated with almost two dozen domains going back to 2008, including incorptoday[.]com, a website that offers to incorporate businesses in several states, including Delaware, Florida and Nevada, for prices ranging from $450 to $550.
As we can see in this archived copy of the site from 2013, IncorpToday also offered a premiere service for $750 that would allow the customer&#8217;s new company to have a retail checking account, with no questions asked.
Global Solutions is able to provide access to the U.S. banking system by offering customers prepaid cards that can be loaded with a variety of virtual payment instruments that were popular in Russian-speaking countries at the time, including WebMoney. The cards are limited to $500 balances, but non-Westerners can use them to anonymously pay for goods and services at a variety of Western companies. Cardnow[.]ru, another domain registered to incorptoday@gmail.com, demonstrates this in action.
A copy of Incorptoday&#8217;s website from 2013 offers non-US residents a service to incorporate a business in Florida, Delaware or Nevada, along with a no-questions-asked checking account, for $750.
WHO IS ANDREI HOLAS?
The oldest domain (2008) registered to incorptoday@gmail.com is andrei[.]me; another is called andreigolos[.]com. DomainTools says these and other domains registered to that email address include the registrant name Andrei Holas, from Huntsville, Ala.
Public records indicate Andrei Holas has lived with his brother &#8212; Aliaksandr Holas &#8212; at two different addresses in Alabama. Those records state that Andrei Holas&#8217; birthday is in March 1984, and that his brother is slightly younger. The younger brother did not respond to a request for comment.
Andrei Holas maintained an account on the Russian social network Vkontakte under the email address ryzhik777@gmail.com, an address that shows up in numerous records hacked and leaked from Russian government entities over the past few years.
Those records indicate Andrei Holas and his brother are from Belarus and have maintained an address in Moscow for some time (that address is roughly three blocks away from the main headquarters of the Russian FSB, the successor intelligence agency to the KGB). Hacked Russian banking records show Andrei Holas&#8217; birthday is March 7, 1984 &#8212; the same birth date listed by GlobalSolutions on BlackHatWorld.
A 2010 post by ryzhik777@gmail.com at the Russian-language forum Ulitka explains that the poster was having trouble getting his B1/B2 visa to visit his brother in the United States, even though he&#8217;d previously been approved for two separate guest visas and a student visa. It remains unclear if one, both, or neither of the Holas brothers still lives in the United States. Andrei explained in 2010 that his brother was an American citizen.
LEGAL BOTNETS
We can all wag our fingers at military personnel who should undoubtedly know better than to install Internet hardware from strangers, but in truth there is an endless supply of U.S. residents who will resell their Internet connection if it means they can make a few bucks out of it. And these days, there are plenty of residential proxy providers who will make it worth your while.
Traditionally, residential proxy networks have been constructed using malicious software that quietly turns infected systems into traffic relays that are then sold in shadowy online forums. Most often, this malware gets bundled with popular cracked software and video files that are uploaded to file-sharing networks and that secretly turn the host device into a traffic relay. In fact, USPRoxyKing bragged that he routinely achieved thousands of installs per week via this method alone.
These days, there a number of residential proxy networks that entice users to monetize their unused bandwidth (inviting you to violate the terms of service of your ISP in the process); others, like DSLRoot, act as a communal VPN, and by using the service you gain access to the connections of other proxies (users) by default, but you also agree to share your connection with others.
Indeed, Intel 471&#8217;s archives show the GlobalSolutions and DSLRoot accounts routinely received private messages from forum users who were college students or young people trying to make ends meet. Those messages show that many of DSLRoot&#8217;s &#8220;regional agents&#8221; often sought commissions to refer friends interested in reselling their home Internet connections (DSLRoot would offer to cover the monthly cost of the agent&#8217;s home Internet connection).
But in an era when North Korean hackers are relentlessly posing as Western IT workers by paying people to host laptop farms in the United States, letting strangers run laptops, mobile devices or any other hardware on your network seems like an awfully risky move regardless of your station in life. As several Redditors pointed out in Sacapoopie&#8217;s thread, an Arizona woman was sentenced in July 2025 to 102 months in prison for hosting a laptop farm that helped North Korean hackers secure jobs at more than 300 U.S. companies, including Fortune 500 firms.
Lloyd Davies is the founder of Infrawatch, a London-based security startup that tracks residential proxy networks. Davies said he reverse engineered the software that powers DSLRoot&#8217;s proxy service, and found it phones home to the aforementioned domain proxysource[.]net, which sells a service that promises to &#8220;get your ads live in multiple cities without getting banned, flagged or ghosted&#8221; (presumably a reference to CraigsList ads).
Davies said he found the DSLRoot installer had capabilities to remotely control residential networking equipment across multiple vendor brands.
Image: Infrawatch.app.
&#8220;The software employs vendor-specific exploits and hardcoded administrative credentials, suggesting DSLRoot pre-configures equipment before deployment,&#8221; Davies wrote in an analysis published today. He said the software performs WiFi network enumeration to identify nearby wireless networks, thereby &#8220;potentially expanding targeting capabilities beyond the primary internet connection.&#8221;
It&#8217;s unclear exactly when the USProxyKing was usurped from his throne, but DSLRoot and its proxy offerings are not what they used to be. Davies said the entire DSLRoot network now has fewer than 300 nodes nationwide, mostly systems on DSL providers like CenturyLink and Frontier.
On Aug. 17, GlobalSolutions posted to BlackHatWorld saying, &#8220;We&#8217;re restructuring our business model by downgrading to &#8216;DSL only&#8217; lines (no mobile or cable).&#8221; Asked via email about the changes, DSLRoot blamed the decline in his customers on the proliferation of residential proxy services.
&#8220;These days it has become almost impossible to compete in this niche as everyone is selling residential proxies and many companies want you to install a piece of software on your phone or desktop so they can resell your residential IPs on a much larger scale,&#8221; DSLRoot explained. &#8220;So-called &#8216;legal botnets&#8217; as we see them.&#8221;

üéì University AI
No updates.

üè¢ Corporate AI
‚Ä¢ Crescent library brings privacy to digital identity systems
  Digital identities, the electronic credentials embedded in phone wallets, workplace logins, and other apps, are becoming ubiquitous. While they offer unprecedented convenience, they also create new privacy risks, particularly around tracking and surveillance.&nbsp;



One of these risks is linkability, the ability to associate one or more uses of a credential to a specific person. Currently, when people use their mobile driver&#8217;s license or log into various apps, hidden identifiers can link these separate activities together, building detailed profiles of user behavior.&nbsp;&nbsp;



To address this, we have released Crescent (opens in new tab), a cryptographic library that adds unlinkability to widely used identity formats, protecting privacy. These include JSON Web Tokens (the authentication standard behind many app logins) and mobile driver&#8217;s licenses. Crescent also works without requiring the organizations that issue these credentials to update their systems. &nbsp;



The protection goes beyond existing privacy features. Some digital identity systems already offer selective disclosure, allowing users to share only specific pieces of information in each interaction. &nbsp;



But even with selective disclosure, credentials can still be linked through serial numbers, cryptographic signatures, or embedded identifiers. Crescent&#8217;s unlinkability feature is designed to prevent anything in the credential, beyond what a user explicitly chooses to reveal, from being used to connect their separate digital interactions.



Figure 1: Unlinkability between a credential issuance and presentation



Two paths to unlinkability&nbsp;



To understand how Crescent works, it helps to examine the two main approaches researchers have developed for adding unlinkability to identity systems:&nbsp;




Specialized cryptographic signature schemes. These schemes can provide unlinkability but require extensive changes to existing infrastructure. New algorithms must be standardized, implemented, and integrated into software and hardware platforms. For example, the BBS (opens in new tab) signature scheme is currently being standardized by the Internet Engineering Task Force (IETF), but even after completion, adoption may be slow.&nbsp;&nbsp;&nbsp;





Zero-knowledge proofs with existing credentials. This approach, used by Crescent (opens in new tab), allows users to prove specific facts about their credentials without revealing the underlying data that could enable tracking. For example, someone could prove they hold a valid driver&#8217;s license and live in a particular ZIP code without exposing any other personal information or identifiers that could link this interaction to future ones.&nbsp;




Zero-knowledge proofs have become more practical since they were first developed 40 years ago but they are not as efficient as the cryptographic algorithms used in today‚Äôs credentials. Crescent addresses this computational challenge through preprocessing, performing the most complex calculations once in advance so that later proof generation is quick and efficient for mobile devices.&nbsp;



Beyond unlinkability, Crescent supports selective disclosure, allowing users to prove specific facts without revealing unnecessary details. For example, it can confirm that a credential is valid and unexpired without disclosing the exact expiration date, which might otherwise serve as a unique identifier. These privacy protections work even when credentials are stored in a phone&#8217;s secure hardware, which keeps them tied to the device and prevents unauthorized access.



	
		

		
		Spotlight: Microsoft research newsletter
	
	
	
						
				
					
				
			
			
			

									Microsoft Research Newsletter
				
								Stay connected to the research community at Microsoft.
				
								
					
						
							Subscribe today						
					
				
							
	
Opens in a new tab	
	


Behind the cryptographic curtain&nbsp;



At its core, Crescent uses a sophisticated form of cryptographic proof called a zero-knowledge SNARK (Zero-Knowledge Succinct Noninteractive Argument of Knowledge). This method allows one party to prove possession of information or credentials without revealing the underlying data itself.&nbsp;



Crescent specifically uses the Groth16 proof system, one of the first practical implementations of this technology. What makes Groth16 particularly useful is that its proofs are small in size, quick to verify, and can be shared in a single step without back-and-forth communication between the user and verifier.&nbsp;



The system works by first establishing shared cryptographic parameters based on a credential template. Multiple organizations issuing similar credentials, such as different state motor vehicle departments issuing mobile driver&#8217;s licenses, can use the same parameters as long as they follow compatible data formats and security standards.&nbsp;



The mathematical rules that define what each proof will verify are written using specialized programming tools that convert them into a Rank-1 Constraint System (R1CS), a mathematical framework that describes exactly what needs to be proven about a credential.&nbsp;



To make the system fast enough for real-world use, Crescent splits the proof generation into two distinct stages:&nbsp;




Prepare stage. This step runs once and generates cryptographic values that can be stored on the user&#8217;s device for repeated use.&nbsp;





Show stage. When a user needs to present their credential, this quicker step takes the stored values and randomizes them to prevent any connection to previous presentations. It also creates a compact cryptographic summary that reveals only the specific information needed for that particular interaction.&nbsp;




Figures 2 and 3 illustrate this credential-proving workflow and the division between the prepare and show steps.



Figure 2: Crescent‚Äôs credential-proving workflow includes a compilation of a circuit to R1CS, followed by the prepare and show steps. The output zero-knowledge proof is sent to the verifier. 



Figure 3: The Crescent presentation steps show the division between prepare and show steps.



A sample application&nbsp;



To demonstrate how Crescent works, we created a sample application covering two real-world scenarios: verifying employment and proving age for online access. The application includes sample code for setting up fictional issuers and verifiers as Rust servers, along with a browser-extension wallet for the user. The step numbers correspond to the steps in Figure 4.&nbsp;



Setup&nbsp;




A Crescent service pre-generates the zero-knowledge parameters for creating and verifying proofs from JSON Web Tokens and mobile driver‚Äôs licenses.&nbsp;





The user obtains a mobile driver‚Äôs license from their Department of Motor Vehicles.&nbsp;





The user obtains a proof-of-employment JSON Web Token from their employer, Contoso.&nbsp;





These credentials and their private keys are stored in the Crescent wallet.&nbsp;




Scenarios&nbsp;




Employment verification: The user presents their JSON Web Token to Fabrikam, an online health clinic, to prove they are employed at Contoso and eligible for workplace benefits. Fabrikam learns that the user works at Contoso but not the user&#8217;s identity, while Contoso remains unaware of the interaction.&nbsp;





Age verification: The user presents their mobile driver‚Äôs license to a social network, proving they are over 18. The proof confirms eligibility without revealing their age or identity.&nbsp;




Across both scenarios, Crescent ensures that credential presentations remain unlinkable, preventing any party from connecting them to the user.&nbsp;



For simplicity, the sample defines its own issuance and presentation protocol, but it could be integrated into higher-level identity frameworks such as OpenID/OAuth, Verifiable Credentials, or the mobile driver‚Äôs license ecosystem.



Figure 4. The sample architecture, from credential issuance to presentation.



To learn more about the project, visit the Crescent project GitHub (opens in new tab) page, or check out our recent presentations given at the Real-Word Crypto 2025 (opens in new tab) and North Sec 2025 (opens in new tab) conferences.¬†




Opens in a new tabThe post Crescent library brings privacy to digital identity systems appeared first on Microsoft Research.
‚Ä¢ Learn how Amazon Health Services improved discovery in Amazon search using AWS ML and gen AI
  Healthcare discovery on ecommerce domains presents unique challenges that traditional product search wasn‚Äôt designed to handle. Unlike searching for books or electronics, healthcare queries involve complex relationships between symptoms, conditions, treatments, and services, requiring sophisticated understanding of medical terminology and customer intent. 
This challenge became particularly relevant for Amazon as we expanded beyond traditional ecommerce into comprehensive healthcare services. Amazon now offers direct access to prescription medications through Amazon Pharmacy, primary care through One Medical, and specialized care partnerships through Health Benefits Connector. These healthcare offerings represent a significant departure from traditional Amazon.com products, presenting both exciting opportunities and unique technical challenges. 
In this post, we show you how Amazon Health Services (AHS) solved discoverability challenges on Amazon.com search using AWS services such as Amazon SageMaker, Amazon Bedrock, and Amazon EMR. By combining machine learning (ML), natural language processing, and vector search capabilities, we improved our ability to connect customers with relevant healthcare offerings. This solution is now used daily for health-related search queries, helping customers find everything from prescription medications to primary care services. 
At AHS, we‚Äôre on a mission to transform how people access healthcare. We strive to make healthcare more straightforward for customers to find, choose, afford, and engage with the services, products, and professionals they need to get and stay healthy. 
Challenges 
Integrating healthcare services into the ecommerce business of Amazon presented two unique opportunities to enhance search for customers on healthcare journeys: understanding health search intent in queries and matching up customer query intent with the most relevant healthcare products and services. 
The challenge in understanding health search intent lies in the relationships between symptoms (such as back pain or sore throat), conditions (such as a herniated disc or the common cold), treatments (such as physical therapy or medication), and the healthcare services Amazon offers. This requires sophisticated query understanding capabilities that can parse medical terminology and map it to common search terminology that a layperson outside of the medical field might use to search. 
AHS offerings also present unique challenges for search matching. For example, a customer searching for ‚Äúback pain treatment‚Äù might be looking for a variety of solutions, from over-the-counter pain relievers like Tylenol or prescription medications such as cyclobenzaprine (a muscle relaxant), to scheduling a doctor‚Äôs appointment or accessing virtual physical therapy. Existing search algorithms optimized for physical products might not match these service-based health offerings, potentially missing relevant results such as One Medical‚Äôs primary care services or Hinge Health‚Äôs virtual physical therapy program that helps reduce joint and muscle pain through personalized exercises and 1-on-1 support from dedicated therapists. This unique nature of healthcare offerings called for developing specialized approaches to connect customers with relevant services. 
Solution overview 
To address these challenges, we developed a comprehensive solution that combines ML for query understanding, vector search for product matching, and large language models (LLMs) for relevance optimization. The solution consists of three main components: 
 
 Query understanding pipeline ‚Äì Uses ML models to identify and classify health-related searches, distinguishing between specific medication queries and broader health condition searches 
 Product knowledge base ‚Äì Combines existing product metadata with LLM-enhanced health information to create comprehensive product embeddings for semantic search 
 Relevance optimization ‚Äì Implements a hybrid approach using both human labeling and LLM-based classification to produce high-quality matches between searches and healthcare offerings 
 
The solution is built entirely on AWS services, with Amazon SageMaker powering our ML models, Amazon Bedrock providing LLM capabilities, and Amazon EMR and Amazon Athena handling our data processing needs. 
Solution architecture 
Now let‚Äôs examine the technical implementation details of our architecture, exploring how each component was engineered to address the unique challenges of healthcare search on Amazon.com. 
Query understanding: Identification of health searches 
We approached the customer search journey by recognizing its two distinct ends of the spectrum. On one end are what we call ‚Äúspearfishing queries‚Äù or lower funnel searches, where customers have a clear product search intent with specific knowledge about attributes. For Amazon Health Services, these typically include searches for specific prescription medications with precise dosages and form factors, such as ‚Äúatorvastatin 40 mg‚Äù or ‚Äúlisinopril 20 mg.‚Äù 
On the other end are broad, upper funnel queries where customers seek inspiration, information, or recommendations with general product search intent that might encompass multiple product types. Examples include searches like ‚Äúback pain relief,‚Äù ‚Äúacne,‚Äù or ‚Äúhigh blood pressure.‚Äù Building upon Amazon search capabilities, we developed additional query understanding models to serve the full spectrum of healthcare searches. 
For identifying spearfishing search intent, we analyzed anonymized customer search engagement data for Amazon products and trained a classification model to understand which search keywords exclusively lead to engagement with Amazon Pharmacy Amazon Standard Identification Numbers (ASINs). This process used PySpark on Amazon EMR and Athena to collect and process Amazon search data at scale. The following diagram shows this architecture. 
 
For identifying broad health search intent, we trained a named entity recognition (NER) model to annotate search keywords at a medical terminology level. To build this capability, we used a corpus of health ontology data sources to identify concepts such as health conditions, diseases, treatments, injuries, and medications. For health concepts where we did not have enough alternate terms in our knowledge base, we used LLMs to expand our knowledge base. For example, alternate terms for the condition ‚Äúacid reflux‚Äù might be ‚Äúheart burn‚Äù, ‚ÄúGERD‚Äù, ‚Äúindigestion‚Äù, etc. We gated this NER model behind health-relevant product types predicted by Amazon search query-to-product-type models. The following diagram shows the training process for the NER model. 
 
The following image is an example of a query identification task in practice. In the example on the left, the pharmacy classifier predicts that ‚Äúatorvastatin 40 mg‚Äù is a query with intent for a prescription drug and triggers a custom search experience geared towards AHS products. In the example on the right, we detect the broad ‚Äúhigh blood pressure‚Äù symptom but don‚Äôt know the customer‚Äôs intention. So, we trigger an experience that gives them multiple options to make the search more specific. 
 
For those interested in implementing similar medical entity recognition capabilities, Amazon Comprehend Medical offers powerful tools for detecting medical entities in text spans. 
Building product knowledge 
With our ability to identify health-related searches in place, we needed to build comprehensive knowledge bases for our healthcare products and services. We started with our existing offerings and collected all available product knowledge information that best described each product or service. 
To enhance this foundation, we used a large language model (LLM) with a fine-tuned prompt and few-shot examples to layer in additional relevant health conditions, symptoms, and treatment-related keywords for each product or service. We did this using the Amazon Bedrock batch inference capability. This approach meant that we significantly expanded our product knowledge with medically relevant information. 
The entire knowledge base was then converted into embeddings using Facebook AI Similarity Search (FAISS), and we created an index file to enable efficient similarity searches. We maintained careful mappings from each embedding back to the original knowledge base items, making sure we could perform accurate reverse lookups when needed. 
This process used several AWS services, including Amazon Simple Storage Service (Amazon S3) for storage of the knowledge base and the embeddings files. Note that Amazon OpenSearch Service is also a viable option for vector database capabilities. Large-scale knowledge base embedding jobs were executed with scheduled SageMaker Notebook Jobs. Through the combination of these technologies, we built a robust foundation of healthcare product knowledge that could be efficiently searched and matched to customer queries. 
The following diagram illustrates how we built the product knowledge base using Amazon catalog data, and then used that to prepare a FAISS index file. 
 
Mapping health search intent to the most relevant products and services 
A core component of our solution was implementing the Retrieval Augmented Generation (RAG) design pattern. The first step in this pattern was to identify a set of known keywords and Amazon products, establishing the initial ground truth for our solution. 
With our product knowledge base built from Amazon catalog metadata and ASIN attributes, we were ready to support new queries from customers. When a customer search query arrived, we converted it to an embedding and used it as a search key for matching against our index. This similarity search used FAISS with matching criteria based on the threshold against the similarity score. 
To verify the quality of these query-product pairs identified for health search keywords, we needed to maintain the relevance of each pair. To achieve this, we implemented a two-pronged approach to relevance labeling. We used an established scheme to tag each offering as exact, substitute, complement, or irrelevant to the keyword. Referred to as the exact, substitute, complement, irrelevant (ESCI) framework established through academic research. For more information, refer to the ESCI challenge and esci-data GitHub repository. 
First, we worked with a human labeling team to establish ground truth on a substantial sample size, creating a reliable benchmark for our system‚Äôs performance using this scheme. The labeling team was given guidance based on the ESCI framework and tailored towards AHS products and services. 
Second, we implemented LLM-based labeling using Amazon Bedrock and batch jobs. After matches were found in the previous step, we retrieved the top products and used them as prompt context for our generative model. We included few-shot examples of ESCI guidance as part of the prompt. This way, we conducted large-scale inference across the top health searches, connecting them to the most relevant offerings using similarity search. We performed this at scale for the query-product pairs identified as relevant to AHS and stored the outputs in Amazon S3. 
The following diagram shows our query retrieval, re-ranking and ESCI labeling pipeline. 
 
Using a mix of high-confidence human and LLM-based labels, we established a true ground truth. Through this process, we successfully identified relevant product offerings for customers using only semantic data from aggregated search keywords and product metadata. 
How did this help customers? 
We‚Äôre on a mission to make it more straightforward for people to find, choose, afford, and engage with the services, products, and professionals they need to get and stay healthy. Today, customers searching for health solutions on Amazon‚Äîwhether for acute conditions like acne, strep throat, and fever or chronic conditions such as arthritis, high blood pressure, and diabetes‚Äîwill begin to see medically vetted and relevant offerings alongside other relevant products and services available on Amazon.com. 
Customers can now quickly find and choose to meet with doctors, get their prescription medications, and access other healthcare services through a familiar experience. By extending the powerful ecommerce search capabilities of Amazon to address healthcare-specific opportunities, we‚Äôve created additional discovery pathways for relevant health services. 
We‚Äôve used semantic understanding of health queries and comprehensive product knowledge to create connections that help customers find the right healthcare solutions at the right time. 
Amazon Health Services Offerings 
Here is a little more information about three healthcare services you can use directly through Amazon: 
 
 Amazon Pharmacy (AP) provides a full-service, online pharmacy experience with transparent medication pricing, convenient home delivery at no additional cost, ongoing delivery updates, 24/7 pharmacist support, and insurance plan acceptance, which supports access and medication adherence. Prime members enjoy special savings with Prime Rx, RxPass, and automatic coupons, making medications more affordable. 
 One Medical Membership and Amazon One Medical Pay Per Visit offer flexible health solutions, from in-office and virtual primary care to condition-based telehealth. Membership offers convenient access to preventive, quality primary care and the option to connect with your care team virtually in the One Medical app. Pay-per-visit is a one-time virtual visit option to find treatment for more than 30 common conditions like acne, pink eye, and sinus infections. 
 Health Benefits Connector matches customers to digital health companies outside of Amazon that are covered by their employer. This program has been expanding over the past year, offering access to specialized care through partners like Hinge Health for musculoskeletal care, Rula and Talkspace for mental health support, and Omada for diabetes treatment. 
 
Key takeaways 
As we reflect on our journey to enhance healthcare discovery on Amazon, several key insights stand out that might be valuable for others working on similar challenges: 
 
 Using domain-specific ontology ‚Äì We began by developing a deep understanding of customer health searches, specifically identifying what kinds of conditions, symptoms, and treatments customers were seeking. By using established health ontology datasets, we enriched a NER model to detect these entities in search queries, providing a foundation for better matching. 
 Similarity search on product knowledge ‚Äì We used existing product knowledge along with LLM-augmented real-world knowledge to build a comprehensive corpus of data that could be mapped to our offerings. Through this approach, we created semantic connections between customer queries and relevant healthcare solutions without relying on individual customer data. 
 Generative AI is more than just chatbots ‚Äì Throughout this project, we relied on various AWS services that proved instrumental to our success. Amazon SageMaker provided the infrastructure for our ML models. However, using Amazon Bedrock batch inference was a key differentiator. It provided us with powerful LLMs for knowledge augmentation and relevance labeling, and services such as Amazon S3 and Amazon EMR supported our data storage and processing needs. Scaling this process manually would have required orders of magnitude more financial budget. Consider generative AI applications at scale beyond merely chat assistants. 
 
By combining these approaches, we‚Äôve created a more intuitive and effective way for customers to discover healthcare offerings on Amazon. 
Implementation considerations 
If you‚Äôre looking to implement a similar solution for healthcare or search, consider the following: 
 
 Security and compliance: Make sure your solution adheres to healthcare data privacy regulations like Health Insurance Portability and Accountability Act (HIPAA). Our approach doesn‚Äôt use individual customer data. 
 Cost optimization: 
   
   Use Amazon EMR on EC2 Spot Instances for batch processing jobs 
   Implement caching for frequently searched queries 
   Choose appropriate instance types for your workload 
    
 Scalability: 
   
   Design your vector search infrastructure to handle peak traffic 
   Use auto scaling for your inference endpoints 
   Implement proper monitoring and alerting 
    
 Maintenance: 
   
   Regularly update your health ontology datasets 
   Monitor model performance and retrain as needed 
   Keep your product knowledge base current 
    
 
Conclusion 
In this post, we demonstrated how Amazon Health Services used AWS ML and generative AI services to solve the unique challenges of healthcare discovery on Amazon.com, illustrating how you can build sophisticated domain-specific search experiences using Amazon SageMaker, Amazon Bedrock, and Amazon EMR. We showed how to create a query understanding pipeline to identify health-related searches, build comprehensive product knowledge bases enhanced with LLM capabilities, and implement semantic matching using vector search and the ESCI relevance framework to connect customers with relevant healthcare offerings. 
This scalable, AWS based approach demonstrates how ML and generative AI can transform specialized search experiences, advancing our mission to make healthcare more straightforward for customers to find, choose, afford, and engage with. We encourage you to explore how these AWS services can address similar challenges in your own healthcare or specialized search applications. For more information about implementing healthcare solutions on AWS, visit the AWS for Healthcare &amp; Life Sciences page. 
 
About the authors 
K. Faryab Haye is an Applied Scientist II at Amazon Health located in Seattle, WA, where he leads search and query understanding initiatives for healthcare AI. His work spans the complete ML lifecycle from large-scale data processing to deploying production systems that serve millions of customers. Faryab earned his MS in Computer Science with a Machine Learning specialization from the University of Michigan and co-founded the Applied Science Club at Amazon Health. When not building ML systems, he can be found hiking mountains, cycling, skiing, or playing volleyball. 
Vineeth Harikumar is a Principal Engineer at Amazon Health Services working on growth and engagement tech initiatives for Amazon One Medical (primary care and telehealth services), Pharmacy prescription delivery, and Health condition programs. Prior to working in healthcare, he worked on building large-scale backend systems in Amazon‚Äôs global inventory, supply chain and fulfillment network, Kindle devices, and Digital commerce businesses (such as Prime Video, Music, and eBooks).
‚Ä¢ Enhance Geospatial Analysis and GIS Workflows with Amazon Bedrock Capabilities
  As data becomes more abundant and information systems grow in complexity, stakeholders need solutions that reveal quality insights. Applying emerging technologies to the geospatial domain offers a unique opportunity to create transformative user experiences and intuitive workstreams for users and organizations to deliver on their missions and responsibilities. 
In this post, we explore how you can integrate existing systems with Amazon Bedrock to create new workflows to unlock efficiencies insights. This integration can benefit technical, nontechnical, and leadership roles alike. 
Introduction to geospatial data 
Geospatial data is associated with a position relative to Earth (latitude, longitude, altitude). Numerical and structured geospatial data formats can be categorized as follows: 
 
 Vector data ‚Äì Geographical features, such as roads, buildings, or city boundaries, represented as points, lines, or polygons 
 Raster data ‚Äì Geographical information, such as satellite imagery, temperature, or elevation maps, represented as a grid of cells 
 Tabular data ‚Äì Location-based data, such as descriptions and metrics (average rainfall, population, ownership), represented in a table of rows and columns 
 
Geospatial data sources might also contain natural language text elements for unstructured attributes and metadata for categorizing and describing the record in question. Geospatial Information Systems (GIS) provide a way to store, analyze, and display geospatial information. In GIS applications, this information is frequently presented with a map to visualize streets, buildings, and vegetation. 
LLMs and Amazon Bedrock 
Large language models (LLMs) are a subset of foundation models (FMs) that can transform input (usually text or image, depending on model modality) into outputs (generally text) through a process called generation. Amazon Bedrock is a comprehensive, secure, and flexible service for building generative AI applications and agents. 
LLMs work in many generalized tasks involving natural language. Some common LLM use cases include: 
 
 Summarization ‚Äì Use a model to summarize text or a document. 
 Q&amp;A ‚Äì Use a model to answer questions about data or facts from context provided during training or inference using Retrieval Augmented Generation (RAG). 
 Reasoning ‚Äì Use a model to provide chain of thought reasoning to assist a human with decision-making and hypothesis evaluation. 
 Data generation ‚Äì Use a model to generate synthetic data for testing simulations or hypothetical scenarios. 
 Content generation ‚Äì Use a model to draft a report from insights derived from an Amazon Bedrock knowledge base or a user‚Äôs prompt. 
 AI agent and tool orchestration ‚Äì Use a model to plan the invocation of other systems and processes. After other systems are invoked by an agent, the agent‚Äôs output can then be used as context for further LLM generation. 
 
GIS can implement these capabilities to create value and improve user experiences. Benefits can include: 
 
 Live decision-making ‚Äì Taking real-time insights to support immediate decision-making, such as emergency response coordination and traffic management 
 Research and analysis ‚Äì In-depth analysis that humans or systems can identify, such as trend analysis, patterns and relationships, and environmental monitoring 
 Planning ‚Äì Using research and analysis for informed long-term decision-making, such as infrastructure development, resource allocation, and environmental regulation 
 
Augmenting GIS and workflows with LLM capabilities leads to simpler analysis and exploration of data, discovery of new insights, and improved decision-making. Amazon Bedrock provides a way to host and invoke models as well as integrate the AI models with surrounding infrastructure, which we elaborate on in this post. 
Combining GIS and AI through RAG and agentic workflows 
LLMs are trained with large amounts of generalized information to discover patterns in how language is produced. To improve the performance of LLMs for specific use cases, approaches such as RAG and agentic workflows have been created. Retrieving policies and general knowledge for geospatial use cases can be accomplished with RAG, whereas calculating and analyzing GIS data would require an agentic workflow. In this section, we expand upon both RAG and agentic workflows in the context of geospatial use cases. 
Retrieval Augmented Generation 
With RAG, you can dynamically inject contextual information from a knowledge base during model invocation. 
RAG supplements a user-provided prompt with data sourced from a knowledge base (collection of documents). Amazon Bedrock offers managed knowledge bases to data sources, such as Amazon Simple Storage Service (Amazon S3) and SharePoint, so you can provide supplemental information, such as city development plans, intelligence reports, or policies and regulations, when your AI assistant is generating a response for a user. 
Knowledge bases are ideal for unstructured documents with information stored in natural language. When your AI model responds to a user with information sourced from RAG, it can provide references and citations to its source material. The following diagram shows how the systems connect together. 
 
Because geospatial data is often structured and in a GIS, you can connect the GIS to the LLM using tools and agents instead of knowledge bases. 
Tools and agents (to control a UI and a system) 
Many LLMs, such as Anthropic‚Äôs Claude on Amazon Bedrock, make it possible to provide a description of tools available so your AI model can generate text to invoke external processes. These processes might retrieve live information, such as the current weather in a location or querying a structured data store, or might control external systems, such as starting a workflow or adding layers to a map. Some common geospatial functionality that you might want to integrate with your LLM using tools include: 
 
 Performing mathematical calculations like the distance between coordinates, filtering datasets based on numeric values, or calculating derived fields 
 Deriving information from predictive analysis models 
 Looking up points of interest in structured data stores 
 Searching content and metadata in unstructured data stores 
 Retrieving real-time geospatial data, like traffic, directions, or estimated time to reach a destination 
 Visualizing distances, points of interest, or paths 
 Submitting work outputs such as analytic reports 
 Starting workflows, like ordering supplies or adjusting supply chain 
 
Tools are often implemented in AWS Lambda functions. Lambda runs code without the complexity and overhead of running servers. It handles the infrastructure management, enabling faster development, improved performance, enhanced security, and cost-efficiency. 
Amazon Bedrock offers the feature Amazon Bedrock Agents to simplify the orchestration and integration with your geospatial tools. Amazon Bedrock agents follow instructions for LLM reasoning to break down a user prompt into smaller tasks and perform actions against identified tasks from action providers. The following diagram illustrates how Amazon Bedrock Agents works. 
 
The following diagram shows how Amazon Bedrock Agents can enhance GIS solutions. 
 
Solution overview 
The following demonstration applies the concepts we‚Äôve discussed to an earthquake analysis agent as an example. This example deploys an Amazon Bedrock agent with a knowledge base based on Amazon Redshift. The Redshift instance has two tables. One table is for earthquakes, which includes date, magnitude, latitude, and longitude. The second table holds the counites in California, described as polygon shapes. The geospatial capabilities of Amazon Redshift can relate these datasets to answer queries like which county had the most recent earthquake or which county has had the most earthquakes in the last 20 years. The Amazon Bedrock agent can generate these geospatially based queries based on natural language. 
This script creates an end-to-end pipeline that performs the following steps: 
 
 Processes geospatial data. 
 Sets up cloud infrastructure. 
 Loads and configures the spatial database. 
 Creates an AI agent for spatial analysis. 
 
In the following sections, we create this agent and test it out. 
Prerequisites 
To implement this approach, you must have an AWS account with the appropriate AWS Identity and Access Management (IAM) permissions for Amazon Bedrock, Amazon Redshift, and Amazon S3. 
Additionally, complete the following steps to set up the AWS Command Line Interface (AWS CLI): 
 
 Confirm you have access to the latest version of the AWS CLI. 
 Sign in to the AWS CLI with your credentials. 
 Make sure ./jq is installed. If not, use the following command: 
 
 
 yum -y install jq 
 
Set up error handling 
Use the following code for the initial setup and error handling: 
 
 #!/usr/bin/env bash
set -ex

LOG_FILE="deployment_$(date +%Y%m%d_%H%M%S).log"
touch "$LOG_FILE"

handle_error() {
&nbsp;&nbsp; &nbsp;local exit_code=$?
&nbsp;&nbsp; &nbsp;local line_number=$1
&nbsp;&nbsp; &nbsp;if [ $exit_code -ne 0 ]; then
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;log_error "Failed at line $line_number with exit code $exit_code"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;exit $exit_code
&nbsp;&nbsp; &nbsp;fi
}
trap 'handle_error $LINENO' ERR 
 
This code performs the following functions: 
 
 Creates a timestamped log file 
 Sets up error trapping that captures line numbers 
 Enables automatic script termination on errors 
 Implements detailed logging of failures 
 
Validate the AWS environment 
Use the following code to validate the AWS environment: 
 
 AWS_VERSION=$(aws --version 2&gt;&amp;1)
log "INFO" "AWS CLI version: $AWS_VERSION"

if ! aws sts get-caller-identity &amp;&gt;/dev/null; then
&nbsp;&nbsp; &nbsp;log_error "AWS CLI is not configured with valid credentials"
&nbsp;&nbsp; &nbsp;exit 1
fi

AWS_REGION="us-east-1"
AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text) 
 
This code performs the essential AWS setup verification: 
 
 Checks AWS CLI installation 
 Validates AWS credentials 
 Retrieves account ID for resource naming 
 
Set up Amazon Redshift and Amazon Bedrock variables 
Use the following code to create Amazon Redshift and Amazon Bedrock variables: 
 
 REDSHIFT_CLUSTER_IDENTIFIER="geo-analysis-cluster"
REDSHIFT_DATABASE="geo_db"
REDSHIFT_MASTER_USER= [Create username]
REDSHIFT_MASTER_PASSWORD= [Create Password]
REDSHIFT_NODE_TYPE="dc2.large"
REDSHIFT_CLUSTER_TYPE="single-node"
BEDROCK_ROLE_NAME="BedrockGeospatialRole"
# Bedrock Configuration
AGENT_NAME="GeoAgentRedshift"
KNOWLEDGE_BASE_NAME="GeospatialKB" 
 
Create IAM roles for Amazon Redshift and Amazon S3 
Use the following code to set up IAM roles for Amazon S3 and Amazon Redshift: 
 
 if aws iam get-role --role-name "$REDSHIFT_ROLE_NAME" &amp;&gt;/dev/null; then
    REDSHIFT_ROLE_ARN=$(aws iam get-role --role-name "$REDSHIFT_ROLE_NAME" --query 'Role.Arn' --output text)
    log "INFO" "Using existing role ARN: $REDSHIFT_ROLE_ARN"
else
    # Create trust policy document
    cat &gt; /tmp/trust-policy.json &lt;&lt; EOF
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "Service": "redshift.amazonaws.com"
      },
      "Action": "sts:AssumeRole"
    }
  ]
}
EOF
    # Create role
    CREATE_ROLE_OUTPUT=$(aws iam create-role \
        --role-name "$REDSHIFT_ROLE_NAME" \
        --assume-role-policy-document "file:///tmp/trust-policy.json" \
        --description "Role for Redshift to access S3" 2&gt;&amp;1)
    
    REDSHIFT_ROLE_ARN=$(aws iam get-role --role-name "$REDSHIFT_ROLE_NAME" --query 'Role.Arn' --output text)
    if [ $? -ne 0 ]; then
        log_error "Failed to create role:"
        exit 1
    fi
    REDSHIFT_ROLE_ARN=$(echo "$CREATE_ROLE_OUTPUT" | jq -r '.Role.Arn')
    # Wait for role to be available
    sleep 10
fi
ATTACH_POLICY_OUTPUT=$(aws iam attach-role-policy \
    --role-name "$REDSHIFT_ROLE_NAME" \
    --policy-arn "arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess" 2&gt;&amp;1)
if [ $? -ne 0 ]; then
    if echo "$ATTACH_POLICY_OUTPUT" | grep -q "EntityAlreadyExists"; then
    else
        exit 1
    fi
fi 
 
Prepare the data and Amazon S3 
Use the following code to prepare the data and Amazon S3 storage: 
 
 DATA_BUCKET="geospatial-bedrock-demo-data-${AWS_ACCOUNT_ID}"
aws s3 mb s3://$DATA_BUCKET

# Download source data
curl -o earthquakes.csv https://raw.githubusercontent.com/Esri/gis-tools-for-hadoop/master/samples/data/earthquake-data/earthquakes.csv
curl -o california-counties.json https://raw.githubusercontent.com/Esri/gis-tools-for-hadoop/master/samples/data/counties-data/california-counties.json 
 
This code sets up data storage and retrieval through the following steps: 
 
 Creates a unique S3 bucket 
 Downloads earthquake and county boundary data 
 Prepares for data transformation 
 
Transform geospatial data 
Use the following code to transform the geospatial data: 
 
 INPUT_FILE="california-counties.json"
OUTPUT_FILE="california-counties.csv"

# Create CSV header
echo "OBJECTID,AREA,PERIMETER,CO06_D00_,CO06_D00_I,STATE,COUNTY,NAME,LSAD,LSAD_TRANS,Shape_Length,Shape_Area,WKT" &gt; "$OUTPUT_FILE"

# Function to convert ESRI rings to WKT POLYGON format
esri_to_wkt() {
    local rings=$1
    
    # Extract the first ring (exterior ring)
    local exterior_ring=$(echo "$rings" | jq -c '.[0]')
    
    if [ "$exterior_ring" = "null" ] || [ -z "$exterior_ring" ]; then
        echo "POLYGON EMPTY"
        return
    fi
    
    # Start building the WKT string
    local wkt="POLYGON (("
    
    # Process each coordinate pair in the ring
    local coords=$(echo "$exterior_ring" | jq -r '.[] | "\(.[0]) \(.[1])"')
    local first_coord=""
    local result=""
    
    while IFS= read -r coord; do
        if [ -z "$result" ]; then
            result="$coord"
            first_coord="$coord"
        else
            result="$result, $coord"
        fi
    done &lt;&lt;&lt; "$coords"
    
    # Close the ring by adding the first coordinate again if needed
    if [ "$first_coord" != "$(echo "$coords" | tail -1)" ]; then
        result="$result, $first_coord"
    fi
    
    wkt="${wkt}${result}))"
    echo "$wkt"
}

# Process each feature in the JSON file
jq -c '.features[]' "$INPUT_FILE" | while read -r feature; do
    # Extract attributes
    OBJECTID=$(echo "$feature" | jq -r '.attributes.OBJECTID // empty')
    AREA=$(echo "$feature" | jq -r '.attributes.AREA // empty')
    PERIMETER=$(echo "$feature" | jq -r '.attributes.PERIMETER // empty')
    CO06_D00_=$(echo "$feature" | jq -r '.attributes.CO06_D00_ // empty')
    CO06_D00_I=$(echo "$feature" | jq -r '.attributes.CO06_D00_I // empty')
    STATE=$(echo "$feature" | jq -r '.attributes.STATE // empty')
    COUNTY=$(echo "$feature" | jq -r '.attributes.COUNTY // empty')
    NAME=$(echo "$feature" | jq -r '.attributes.NAME // empty')
    LSAD=$(echo "$feature" | jq -r '.attributes.LSAD // empty')
    LSAD_TRANS=$(echo "$feature" | jq -r '.attributes.LSAD_TRANS // empty')
    Shape_Length=$(echo "$feature" | jq -r '.attributes.Shape_Length // empty')
    Shape_Area=$(echo "$feature" | jq -r '.attributes.Shape_Area // empty')
    
    # Extract geometry and convert to WKT
    if echo "$feature" | jq -e '.geometry.rings' &gt; /dev/null 2&gt;&amp;1; then
        rings=$(echo "$feature" | jq -c '.geometry.rings')
        WKT=$(esri_to_wkt "$rings")
    else
        WKT="POLYGON EMPTY"
    fi
    
    # Escape any commas in the fields
    NAME=$(echo "$NAME" | sed 's/,/\\,/g')
    LSAD=$(echo "$LSAD" | sed 's/,/\\,/g')
    LSAD_TRANS=$(echo "$LSAD_TRANS" | sed 's/,/\\,/g')
    
     # Write to CSV - wrap WKT field in quotes
    echo "$OBJECTID,$AREA,$PERIMETER,$CO06_D00_,$CO06_D00_I,$STATE,$COUNTY,$NAME,$LSAD,$LSAD_TRANS,$Shape_Length,$Shape_Area,\"$WKT\"" &gt;&gt; "$OUTPUT_FILE"
done

echo "Conversion complete. Output saved to $OUTPUT_FILE"

# Upload data files to S3
aws s3 cp earthquakes.csv s3://$DATA_BUCKET/earthquakes/
aws s3 cp california-counties.csv s3://$DATA_BUCKET/counties/ 
 
This code performs the following actions to convert the geospatial data formats: 
 
 Transforms ESRI JSON to WKT format 
 Processes county boundaries into CSV format 
 Preserves spatial information for Amazon Redshift 
 
Create a Redshift cluster 
Use the following code to set up the Redshift cluster: 
 
 # Create Redshift cluster
aws redshift create-cluster \
&nbsp;&nbsp; &nbsp;--cluster-identifier "$REDSHIFT_CLUSTER_IDENTIFIER" \
&nbsp;&nbsp; &nbsp;--node-type "$REDSHIFT_NODE_TYPE" \
&nbsp;&nbsp; &nbsp;--cluster-type single-node \
&nbsp;&nbsp; &nbsp;--master-username "$REDSHIFT_MASTER_USER" \
&nbsp;&nbsp; &nbsp;--master-user-password "$REDSHIFT_MASTER_PASSWORD" \
&nbsp;&nbsp; &nbsp;--db-name "$REDSHIFT_DATABASE" \
&nbsp;&nbsp; &nbsp;--cluster-subnet-group-name "$SUBNET_GROUP_NAME" \
&nbsp;&nbsp; &nbsp;--vpc-security-group-ids "$SG_ID" \
&nbsp;&nbsp; &nbsp;--iam-roles "$REDSHIFT_ROLE_ARN"

# Wait for cluster availability
while true; do
&nbsp;&nbsp; &nbsp;CLUSTER_STATUS=$(aws redshift describe-clusters \
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;--cluster-identifier "$REDSHIFT_CLUSTER_IDENTIFIER" \
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;--query 'Clusters[0].ClusterStatus' \
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;--output text)
&nbsp;&nbsp; &nbsp;if [ "$CLUSTER_STATUS" = "available" ]; then
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;break
&nbsp;&nbsp; &nbsp;fi
&nbsp;&nbsp; &nbsp;sleep 30
done 
 
This code performs the following functions: 
 
 Sets up a single-node cluster 
 Configures networking and security 
 Waits for cluster availability 
 
Create a database schema 
Use the following code to create the database schema: 
 
 aws redshift-data execute-statement \
&nbsp;&nbsp; &nbsp;--cluster-identifier "$REDSHIFT_CLUSTER_IDENTIFIER" \
&nbsp;&nbsp; &nbsp;--database "$REDSHIFT_DATABASE" \
&nbsp;&nbsp; &nbsp;--sql "
CREATE TABLE IF NOT EXISTS counties (
&nbsp;&nbsp; &nbsp;OBJECTID INTEGER PRIMARY KEY,
&nbsp;&nbsp; &nbsp;AREA DOUBLE PRECISION,
&nbsp;&nbsp; &nbsp;NAME VARCHAR(100),
&nbsp;&nbsp; &nbsp;geom GEOMETRY
);

CREATE TABLE IF NOT EXISTS earthquakes (
&nbsp;&nbsp; &nbsp;earthquake_date VARCHAR(50),
&nbsp;&nbsp; &nbsp;latitude double precision,
&nbsp;&nbsp; &nbsp;longitude double precision,
&nbsp;&nbsp; &nbsp;magnitude double precision
);" 
 
This code performs the following functions: 
 
 Creates a counties table with spatial data 
 Creates an earthquakes table 
 Configures appropriate data types 
 
Create an Amazon Bedrock knowledge base 
Use the following code to create a knowledge base: 
 
 # Create knowledge base
aws bedrock-agent create-knowledge-base \
&nbsp;&nbsp; &nbsp;--name "$KNOWLEDGE_BASE_NAME" \
&nbsp;&nbsp; &nbsp;--knowledge-base-configuration "{
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;\"type\": \"SQL\",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;\"sqlKnowledgeBaseConfiguration\": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\"type\": \"REDSHIFT\"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp;}" \
&nbsp;&nbsp; &nbsp;--region "$AWS_REGION"

# Create data source
aws bedrock-agent create-data-source \
&nbsp;&nbsp; &nbsp;--knowledge-base-id "$KB_ID" \
&nbsp;&nbsp; &nbsp;--name "EarthquakeDataSource" \
&nbsp;&nbsp; &nbsp;--data-source-configuration "{\"type\": \"REDSHIFT_METADATA\"}" 
 
This code performs the following functions: 
 
 Creates an Amazon Bedrock knowledge base 
 Sets up an Amazon Redshift data source 
 Enables spatial queries 
 
Create an Amazon Bedrock agent 
Use the following code to create and configure an agent: 
 
 # Create agent
aws bedrock-agent create-agent \
&nbsp;&nbsp; &nbsp;--agent-name "$AGENT_NAME" \
&nbsp;&nbsp; &nbsp;--instruction "You are a geospatial analysis assistant..." \
&nbsp;&nbsp; &nbsp;--foundation-model "anthropic.claude-3-sonnet-20240229-v1:0"

# Associate knowledge base
aws bedrock-agent associate-agent-knowledge-base \
&nbsp;&nbsp; &nbsp;--agent-id "$AGENT_ID" \
&nbsp;&nbsp; &nbsp;--knowledge-base-id "$KB_ID" \
&nbsp;&nbsp; &nbsp;--description "Earthquake data knowledge base" \
&nbsp;&nbsp; &nbsp;--agent-version "DRAFT" 
 
This code performs the following functions: 
 
 Creates an Amazon Bedrock agent 
 Associates the agent with the knowledge base 
 Configures the AI model and instructions 
 
Test the solution 
Let‚Äôs observe the system behavior with the following natural language user inputs in the chat window. 
Example 1: Summarization and Q&amp;A 
For this example, we use the prompt ‚ÄúSummarize which zones allow for building of an apartment.‚Äù 
The LLM performs retrieval with a RAG approach, then uses the retrieved residential code documents as context to answer the user‚Äôs query in natural language. 
 
This example demonstrates the LLM capabilities for hallucination mitigation, RAG, and summarization. 
Example 2: Generate a draft report 
Next, we input the prompt ‚ÄúWrite me a report on how various zones and related housing data can be utilized to plan new housing development to meet high demand.‚Äù 
The LLM retrieves relevant urban planning code documents, then summarizes the information into a standard reporting format as described in its system prompt. 
 
This example demonstrates the LLM capabilities for prompt templates, RAG, and summarization. 
Example 3: Show places on the map 
For this example, we use the prompt ‚ÄúShow me the low density properties on Abbeville street in Macgregor on the map with their address.‚Äù 
The LLM creates a chain of thought to look up which properties match the user‚Äôs query and then invokes the draw marker tool on the map. The LLM provides tool invocation parameters in its scratchpad, awaits the completion of these tool invocations, then responds in natural language with a bulleted list of markers placed on the map. 
 
 
This example demonstrates the LLM capabilities for chain of thought reasoning, tool use, retrieval systems using agents, and UI control. 
Example 4: Use the UI as context 
For this example, we choose a marker on a map and input the prompt ‚ÄúCan I build an apartment here.‚Äù 
The ‚Äúhere‚Äù is not contextualized from conversation history but rather from the state of the map view. Having a state engine that can relay information from a frontend view to the LLM input adds a richer context. 
The LLM understands the context of ‚Äúhere‚Äù based on the selected marker, performs retrieval to see the land development policy, and responds to the user in simple natural language, ‚ÄúNo, and here is why‚Ä¶‚Äù 
 
This example demonstrates the LLM capabilities for UI context, chain of thought reasoning, RAG, and tool use. 
Example 5: UI context and UI control 
Next, we choose a marker on the map and input the prompt ‚Äúdraw a .25 mile circle around here so I can visualize walking distance.‚Äù 
The LLM invokes the draw circle tool to create a layer on the map centered at the selected marker, contextualized by ‚Äúhere.‚Äù 
 
This example demonstrates the LLM capabilities for UI context, chain of thought reasoning, tool use, and UI control. 
Clean up 
To clean up your resources and prevent AWS charges from being incurred, complete the following steps: 
 
 Delete the Amazon Bedrock knowledge base. 
 Delete the Redshift cluster. 
 Delete the S3 bucket. 
 
Conclusion 
The integration of LLMs with GIS creates intuitive systems that help users of different technical levels perform complex spatial analysis through natural language interactions. By using RAG and agent-based workflows, organizations can maintain data accuracy while seamlessly connecting AI models to their existing knowledge bases and structured data systems. Amazon Bedrock facilitates this convergence of AI and GIS technology by providing a robust platform for model invocation, knowledge retrieval, and system control, ultimately transforming how users visualize, analyze, and interact with geographical data. 
For further exploration, Earth on AWS has videos and articles you can explore to understand how AWS is helping build GIS applications on the cloud. 
 
About the Authors 
Dave Horne&nbsp;is a Sr. Solutions Architect supporting Federal System Integrators at AWS. He is based in Washington, DC, and has 15 years of experience building, modernizing, and integrating systems for public sector customers. Outside of work, Dave enjoys playing with his kids, hiking, and watching Penn State football! 
Kai-Jia Yue&nbsp;is a solutions architect on the Worldwide Public Sector Global Systems Integrator Architecture team at Amazon Web Services (AWS). She has a focus in data analytics and helping customer organizations make data-driven decisions. Outside of work, she loves spending time with friends and family and traveling. 
Brian Smitches is the Head of Partner Deployed Engineering at Windsurf focusing on how partners can bring organizational value through the adoption of Agentic AI software development tools like Windsurf and Devin. Brian has a background in Cloud Solutions Architecture from his time at AWS, where he worked in the&nbsp;AWS Federal Partner ecosystem. In his personal time, Brian enjoys skiing, water sports, and traveling with friends and family.
‚Ä¢ Beyond the basics: A comprehensive foundation model selection framework for generative AI
  Most organizations evaluating foundation models limit their analysis to three primary dimensions: accuracy, latency, and cost. While these metrics provide a useful starting point, they represent an oversimplification of the complex interplay of factors that determine real-world model performance. 
Foundation models have revolutionized how enterprises develop generative AI applications, offering unprecedented capabilities in understanding and generating human-like content. However, as the model landscape expands, organizations face complex scenarios when selecting the right foundation model for their applications. In this blog post we present a systematic evaluation methodology for Amazon Bedrock users, combining theoretical frameworks with practical implementation strategies that empower data scientists and machine learning (ML) engineers to make optimal model selections. 
The challenge of foundation model selection 
Amazon Bedrock is a fully managed service that offers a choice of high-performing foundation models from leading AI companies such as&nbsp;AI21 Labs,&nbsp;Anthropic,&nbsp;Cohere,&nbsp;DeepSeek,&nbsp;Luma,&nbsp;Meta,&nbsp;Mistral AI,&nbsp;poolside&nbsp;(coming soon),&nbsp;Stability AI,&nbsp;TwelveLabs&nbsp;(coming soon),&nbsp;Writer, and&nbsp;Amazon&nbsp;through a single API, along with a broad set of capabilities you need to build generative AI applications with security, privacy, and responsible AI. The service‚Äôs API-driven approach allows seamless model interchangeability, but this flexibility introduces a critical challenge: which model will deliver optimal performance for a specific application while meeting operational constraints? 
Our research with enterprise customers reveals that many early generative AI projects select models based on either limited manual testing or reputation, rather than systematic evaluation against business requirements. This approach frequently results in: 
 
 Over-provisioning computational resources to accommodate larger models than required 
 Sub-optimal performance because of misalignment between model strengths and use case requirements 
 Unnecessarily high operational costs because of inefficient token utilization 
 Production performance issues discovered too late in the development lifecycle 
 
In this post, we outline a comprehensive evaluation methodology optimized for Amazon Bedrock implementations using Amazon Bedrock Evaluations while providing forward-compatible patterns as the foundation model landscape evolves. To read more about on how to evaluate large language model (LLM) performance, see LLM-as-a-judge on Amazon Bedrock Model Evaluation. 
A multidimensional evaluation framework‚ÄîFoundation model capability matrix 
Foundation models vary significantly across multiple dimensions, with performance characteristics that interact in complex ways. Our capability matrix provides a structured view of critical dimensions to consider when evaluating models in Amazon Bedrock. Below are four core dimensions (in no specific order) ‚Äì Task performance, Architectural characteristics, Operational considerations, and Responsible AI attributes. 
Task performance 
Evaluating the models based on the task performance is crucial for achieving direct impact on business outcomes, ROI, user adoption and trust, and competitive advantage. 
 
 Task-specific accuracy: Evaluate models using benchmarks relevant to your use case (MMLU, HELM, or domain-specific benchmarks). 
 Few-shot learning capabilities: Strong few-shot performers require minimal examples to adapt to new tasks, leading to cost efficiency, faster time-to-market, resource optimization, and operational benefits. 
 Instruction following fidelity: For the applications that require precise adherence to commands and constraints, it is critical to evaluate model‚Äôs instruction following fidelity. 
 Output consistency: Reliability and reproducibility across multiple runs with identical prompts. 
 Domain-specific knowledge: Model performance varies dramatically across specialized fields based on training data. Evaluate the models base on your domain-specific use-case scenarios. 
 Reasoning capabilities: Evaluate the model‚Äôs ability to perform logical inference, causal reasoning, and multi-step problem-solving. This can include reasoning such as deductive and inductive, mathematical, chain-of-thought, and so on. 
 
Architectural characteristics 
Architectural characteristics for evaluating the models are important as they directly impact the model‚Äôs performance, efficiency, and suitability for specific tasks. 
 
 Parameter count (model size): Larger models typically offer more capabilities but require greater computational resources and may have higher inference costs and latency. 
 Training data composition: Models trained on diverse, high-quality datasets tend to have better generalization abilities across different domains. 
 Model architecture: Decoder-only models excel at text generation, encoder-decoder architectures handle translation and summarization more effectively, while mixture of experts (MoE) architectures can be a powerful tool for improving the performance of both decoder-only and encoder-decoder models. Some specialized architectures focus on enhancing reasoning capabilities through techniques like chain-of-thought prompting or recursive reasoning. 
 Tokenization methodology: The way models process text affects performance on domain-specific tasks, particularly with specialized vocabulary. 
 Context window capabilities: Larger context windows enable processing more information at once, critical for document analysis and extended conversations. 
 Modality: Modality refers to type of data a model can process and generate, such as text, image, audio, or video. Consider the modality of the models depending on the use case, and choose the model optimized for that specific modality. 
 
Operational considerations 
Below listed operational considerations are critical for model selection as they directly impact the real-world feasibility, cost-effectiveness, and sustainability of AI deployments. 
 
 Throughput and latency profiles: Response speed impacts user experience and throughput determines scalability. 
 Cost structures: Input/output token pricing significantly affects economics at scale. 
 Scalability characteristics: Ability to handle concurrent requests and maintain performance during traffic spikes. 
 Customization options: Fine-tuning capabilities and adaptation methods for tailoring to specific use cases or domains. 
 Ease of integration: Ease of integration into existing systems and workflow is an important consideration. 
 Security: When dealing with sensitive data, model security‚Äîincluding data encryption, access control, and vulnerability management‚Äîis a crucial consideration. 
 
Responsible AI attributes 
As AI becomes increasingly embedded in business operations and daily lives, evaluating models on responsible AI attributes isn‚Äôt just a technical consideration‚Äîit‚Äôs a business imperative. 
 
 Hallucination propensity: Models vary in their tendency to generate plausible but incorrect information. 
 Bias measurements: Performance across different demographic groups affects fairness and equity. 
 Safety guardrail effectiveness: Resistance to generating harmful or inappropriate content. 
 Explainability and privacy: Transparency features and handling of sensitive information. 
 Legal Implications: Legal considerations should include data privacy, non-discrimination, intellectual property, and product liability. 
 
Agentic AI considerations for model selection 
The growing popularity of agentic AI applications introduces evaluation dimensions beyond traditional metrics. When assessing models for use in autonomous agents, consider these critical capabilities: 
Agent-specific evaluation dimensions 
 
 Planning and reasoning capabilities: Evaluate chain-of-thought consistency across complex multi-step tasks and self-correction mechanisms that allow agents to identify and fix their own reasoning errors. 
 Tool and API integration: Test function calling capabilities, parameter handling precision, and structured output consistency (JSON/XML) for seamless tool use. 
 Agent-to-agent communication: Assess protocol adherence to frameworks like A2A and efficient contextual memory management across extended multi-agent interactions. 
 
Multi-agent collaboration testing for applications using multiple specialized agents 
 
 Role adherence: Measure how well models maintain distinct agent personas and responsibilities without role confusion. 
 Information sharing efficiency: Test how effectively information flows between agent instances without critical detail loss. 
 Collaborative intelligence: Verify whether multiple agents working together produce better outcomes than single-model approaches. 
 Error propagation resistance: Assess how robustly multi-agent systems contain and correct errors rather than amplifying them. 
 
A four-phase evaluation methodology 
Our recommended methodology progressively narrows model selection through increasingly sophisticated assessment techniques: 
Phase 1: Requirements engineering 
Begin with a precise specification of your application‚Äôs requirements: 
 
 Functional requirements: Define primary tasks, domain knowledge needs, language support, output formats, and reasoning complexity. 
 Non-functional requirements: Specify latency thresholds, throughput requirements, budget constraints, context window needs, and availability expectations. 
 Responsible AI requirements: Establish hallucination tolerance, bias mitigation needs, safety requirements, explainability level, and privacy constraints. 
 Agent-specific requirements: For agentic applications, define tool-use capabilities, protocol adherence standards, and collaboration requirements. 
 
Assign weights to each requirement based on business priorities to create your evaluation scorecard foundation. 
Phase 2: Candidate model selection 
Use the Amazon Bedrock model information API to filter models based on hard requirements. This typically reduces candidates from dozens to 3‚Äì7 models that are worth detailed evaluation. 
Filter options include but aren‚Äôt limited to the following: 
 
 Filter by modality support, context length, and language capabilities 
 Exclude models that don‚Äôt meet minimum performance thresholds 
 Calculate theoretical costs at projected scale so that you can exclude options that exceed the available budget 
 Filter for customization requirements such as fine-tuning capabilities 
 For agentic applications, filter for function calling and multi-agent protocol support 
 
Although the Amazon Bedrock model information API might not provide the filters you need for candidate selection, you can use the Amazon Bedrock model catalog (shown in the following figure) to obtain additional information about these models. 
 
Phase 3: Systematic performance evaluation 
Implement structured evaluation using Amazon Bedrock Evaluations: 
 
 Prepare evaluation datasets: Create representative task examples, challenging edge cases, domain-specific content, and adversarial examples. 
 Design evaluation prompts: Standardize instruction format, maintain consistent examples, and mirror production usage patterns. 
 Configure metrics: Select appropriate metrics for subjective tasks (human evaluation and reference-free quality), objective tasks (precision, recall, and F1 score), and reasoning tasks (logical consistency and step validity). 
 For agentic applications: Add protocol conformance testing, multi-step planning assessment, and tool-use evaluation. 
 Execute evaluation jobs: Maintain consistent parameters across models and collect comprehensive performance data. 
 Measure operational performance: Capture throughput, latency distributions, error rates, and actual token consumption costs. 
 
Phase 4: Decision analysis 
Transform evaluation data into actionable insights: 
 
 Normalize metrics: Scale all metrics to comparable units using min-max normalization. 
 Apply weighted scoring: Calculate composite scores based on your prioritized requirements. 
 Perform sensitivity analysis: Test how robust your conclusions are against weight variations. 
 Visualize performance: Create radar charts, efficiency frontiers, and tradeoff curves for clear comparison. 
 Document findings: Detail each model‚Äôs strengths, limitations, and optimal use cases. 
 
Advanced evaluation techniques 
Beyond standard procedures, consider the following approaches for evaluating models. 
A/B testing with production traffic 
Implement comparative testing using Amazon Bedrock‚Äôs routing capabilities to gather real-world performance data from actual users. 
Adversarial testing 
Test model vulnerabilities through prompt injection attempts, challenging syntax, edge case handling, and domain-specific factual challenges. 
Multi-model ensemble evaluation 
Assess combinations such as sequential pipelines, voting ensembles, and cost-efficient routing based on task complexity. 
Continuous evaluation architecture 
Design systems to monitor production performance with: 
 
 Stratified sampling of production traffic across task types and domains 
 Regular evaluations and trigger-based reassessments when new models emerge 
 Performance thresholds and alerts for quality degradation 
 User feedback collection and failure case repositories for continuous improvement 
 
Industry-specific considerations 
Different sectors have unique requirements that influence model selection: 
 
 Financial services: Regulatory compliance, numerical precision, and personally identifiable information (PII) handling capabilities 
 Healthcare: Medical terminology understanding, HIPAA adherence, and clinical reasoning 
 Manufacturing: Technical specification comprehension, procedural knowledge, and spatial reasoning 
 Agentic systems: Autonomous reasoning, tool integration, and protocol conformance 
 
Best practices for model selection 
Through this comprehensive approach to model evaluation and selection, organizations can make informed decisions that balance performance, cost, and operational requirements while maintaining alignment with business objectives. The methodology makes sure that model selection isn‚Äôt a one-time exercise but an evolving process that adapts to changing needs and technological capabilities. 
 
 Assess your situation thoroughly: Understand your specific use case requirements and available resources 
 Select meaningful metrics: Focus on metrics that directly relate to your business objectives 
 Build for continuous evaluation: Design your evaluation process to be repeatable as new models are released 
 
Looking forward: The future of model selection 
As foundation models evolve, evaluation methodologies must keep pace. Below are further considerations (By no means this list of considerations is exhaustive and is subject to ongoing updates as technology evolves and best practices emerge), you should take into account while selecting the best model(s) for your use-case(s). 
 
 Multi-model architectures: Enterprises will increasingly deploy specialized models in concert rather than relying on single models for all tasks. 
 Agentic landscapes: Evaluation frameworks must assess how models perform as autonomous agents with tool-use capabilities and inter-agent collaboration. 
 Domain specialization: The growing landscape of domain-specific models will require more nuanced evaluation of specialized capabilities. 
 Alignment and control: As models become more capable, evaluation of controllability and alignment with human intent becomes increasingly important. 
 
Conclusion 
By implementing a comprehensive evaluation framework that extends beyond basic metrics, organizations can informed decisions about which foundation models will best serve their requirements. For agentic AI applications in particular, thorough evaluation of reasoning, planning, and collaboration capabilities is essential for success. By approaching model selection systematically, organizations can avoid the common pitfalls of over-provisioning, misalignment with use case needs, excessive operational costs, and late discovery of performance issues. The investment in thorough evaluation pays dividends through optimized costs, improved performance, and superior user experiences. 
 
About the author 
Sandeep Singh is a Senior Generative AI Data Scientist at Amazon Web Services, helping businesses innovate with generative AI. He specializes in generative AI, machine learning, and system design. He has successfully delivered state-of-the-art AI/ML-powered solutions to solve complex business problems for diverse industries, optimizing efficiency and scalability.
‚Ä¢ Accelerate intelligent document processing with generative AI on AWS
  Every day, organizations process millions of documents, including invoices, contracts, insurance claims, medical records, and financial statements. Despite the critical role these documents play, an estimated 80‚Äì90% of the data they contain is unstructured and largely untapped, hiding valuable insights that could transform business outcomes. Despite advances in technology, many organizations still rely on manual data entry, spending countless hours extracting information from PDFs, scanned images, and forms. This manual approach is time-consuming, error-prone, and prevents organizations from scaling their operations and responding quickly to business demands. 
Although generative AI has made it easier to build proof-of-concept document processing solutions, the journey from proof of concept to production remains fraught with challenges. Organizations often find themselves rebuilding from scratch when they discover their prototype can‚Äôt handle production volumes, lacks proper error handling, doesn‚Äôt scale cost-effectively, or fails to meet enterprise security and compliance requirements. What works in a demo with a handful of documents often breaks down when processing thousands of documents daily in a production environment. 
In this post, we introduce our open source GenAI IDP Accelerator‚Äîa tested solution that we use to help customers across industries address their document processing challenges. Automated document processing workflows accurately extract structured information from documents, reducing manual effort. We will show you how this ready-to-deploy solution can help you build those workflows with generative AI on AWS in days instead of months. 
Understanding intelligent document processing 
Intelligent document processing (IDP) encompasses the technologies and techniques used to extract and process data from various document types. Common IDP tasks include: 
 
 OCR (Optical Character Recognition) ‚Äì Converting scanned documents and images into machine-readable text 
 Document classification ‚Äì Automatically identifying document types (such as invoices, contracts, or forms) 
 Data extraction ‚Äì Pulling structured information from unstructured documents 
 Assessment ‚Äì Evaluating the quality and confidence of extracted data 
 Summarization ‚Äì Creating concise summaries of document content 
 Evaluation ‚Äì Measuring accuracy and performance against expected outcomes 
 
These capabilities are critical across industries. In financial services, organizations use IDP to process loan applications, extract data from bank statements, and validate insurance claims. Healthcare providers rely on IDP to extract patient information from medical records, process insurance forms, and handle lab results efficiently. Manufacturing and logistics companies use IDP to process invoices and purchase orders, extract shipping information, and handle quality certificates. Government agencies use IDP to process citizen applications, extract data from tax forms, manage permits and licenses, and enforce regulatory compliance. 
The generative AI revolution in IDP 
Traditional IDP solutions relied on template-based extraction, regular expressions, and classical machine learning (ML) models. Though functional, these approaches required extensive setup, struggled with document variations, and achieved limited accuracy on complex documents. 
The emergence of large language models (LLMs) and generative AI has fundamentally transformed IDP capabilities. Modern AI models can understand document context, handle variations without templates, achieve near-human accuracy on complex extractions, and adapt to new document types with minimal examples. This shift from rule-based to intelligence-based processing means organizations can now process different document types with high accuracy, dramatically reducing the time and cost of implementation. 
GenAI IDP Accelerator 
We‚Äôre excited to share the GenAI IDP Accelerator‚Äîan open source solution that transforms how organizations handle document processing by dramatically reducing manual effort and improving accuracy. This serverless foundation offers processing patterns which use Amazon Bedrock Data Automation for rich out-of-the-box document processing features, high accuracy, ease of use, and straightforward per-page pricing, Amazon Bedrock state-of-the-art foundation models (FMs) for complex documents requiring custom logic, and other AWS AI services to provide a flexible, scalable starting point for enterprises to build document automation tailored to their specific needs. 
The following is a short demo of the solution in action, in this case showcasing the default Amazon Bedrock Data Automation processing pattern. 

 
  
 
 
Real-world impact 
The GenAI IDP Accelerator is already transforming document processing for organizations across industries. 
Competiscan: Transforming marketing intelligence at scale 
Competiscan, a leader in competitive marketing intelligence, faced a massive challenge: processing 35,000‚Äì45,000 marketing campaigns daily while maintaining a searchable archive of 45 million campaigns spanning 15 years. 
Using the GenAI IDP Accelerator, Competiscan achieved the following: 
 
 85% classification and extraction accuracy across diverse marketing materials 
 Increased scalability to handle 35,000‚Äì45,000 daily campaigns 
 Removal of critical bottlenecks, facilitating business growth 
 Production deployment in just 8 weeks from initial concept 
 
Ricoh: Scaling document processing 
Ricoh, a global leader in document management, implemented the GenAI IDP Accelerator to transform healthcare document processing for their clients. Processing over 10,000 healthcare documents monthly with potential to scale to 70,000, they needed a solution that could handle complex medical documentation with high accuracy. 
The results speak for themselves: 
 
 Savings potential of over 1,900 person-hours annually through automation 
 Achieved extraction accuracy to help minimize financial penalties from processing errors 
 Automated classification of grievances vs. appeals 
 Created a reusable framework deployable across multiple healthcare customers 
 Integrated with human-in-the-loop review for cases requiring expert validation 
 Leveraged modular architecture to integrate with existing systems, enabling custom document splitting and large-scale document processing 
 
Solution overview 
The GenAI IDP Accelerator is a modular, serverless solution that automatically converts unstructured documents into structured, actionable data. Built entirely on AWS services, it provides enterprise-grade scalability, security, and cost-effectiveness while requiring minimal setup and maintenance. Its configuration-driven design helps teams quickly adapt prompts, extraction templates, and validation rules for their specific document types without touching the underlying infrastructure. 
The solution follows a modular pipeline that enriches documents at each stage, from OCR to classification, to extraction, to assessment, to summarization, and ending with evaluation. 
You can deploy and customize each step independently, so you can optimize for your specific use cases while maintaining the benefits of the integrated workflow. 
The following diagram illustrates the solution architecture, showing the default Bedrock Data Automation workflow (Pattern-1). 
 
Refer to the GitHub repo for additional details and processing patterns. 
Some of the key features of the solution include: 
 
 Serverless architecture ‚Äì Built on AWS Lambda, AWS Step Functions, and other serverless technologies for queueing, concurrency management, and retries to provide automatic scaling and pay-per-use pricing for production workloads of many sizes 
 Generative AI-powered document packet splitting and classification ‚Äì Intelligent document classification using Amazon Bedrock Data Automation or Amazon Bedrock multimodal FMs, including support for multi-document packets and packet splitting 
 Advanced AI key information extraction ‚Äì Key information extraction using Amazon Bedrock Data Automation or Amazon Bedrock multimodal FMs 
 Multiple processing patterns ‚Äì Choose from pre-built patterns optimized for different workloads with different configurability, cost, and accuracy requirements, or extend the solution with additional patterns: 
   
   Pattern 1 ‚Äì Uses Amazon Bedrock Data Automation, a fully managed service that offers rich out-of-the-box features, ease of use, and straightforward per-page pricing. This pattern is recommended for most use cases. 
   Pattern 2 ‚Äì Uses Amazon Textract and Amazon Bedrock with Amazon Nova, Anthropic‚Äôs Claude, or custom fine-tuned Amazon Nova models. This pattern is ideal for complex documents requiring custom logic. 
   Pattern 3 ‚Äì Uses Amazon Textract, Amazon SageMaker with a fine-tuned model for classification, and Amazon Bedrock for extraction. This pattern is ideal for documents requiring specialized classification. 
    
 
We expect to add more pattern options to handle additional real-world document processing needs, and to take advantage of ever-improving state-of-the-art capabilities: 
 
 Few-shot learning ‚Äì Improve accuracy for classification and extraction by providing few-shot examples to guide the AI models 
 Confidence assessment ‚Äì AI-powered quality assurance that evaluates extraction field confidence, used to indicate documents for human review 
 Human-in-the-loop (HITL) review ‚Äì Integrated workflow for human review of low-confidence extractions using Amazon SageMaker Augmented AI (Amazon A2I), currently available for Pattern 1, with support for Patterns 2 and 3 coming soon 
 Web user interface ‚Äì Responsive web UI for monitoring document processing, viewing results, and managing configurations 
 Knowledge base integration ‚Äì Query processed documents using natural language through Amazon Bedrock Knowledge Bases 
 Built-in evaluation ‚Äì Framework to evaluate and improve accuracy against baseline data 
 Analytics and reporting database ‚Äì Centralized analytics database for tracking processing metrics, accuracy trends, and cost optimization across document workflows, and for analyzing extracted document content using Amazon Athena 
 No-code configuration ‚Äì Customize document types, extraction fields, and processing logic through configuration, editable in the web UI 
 Developer-friendly python package ‚Äì For data science and engineering teams who want to experiment, optimize, or integrate the IDP capabilities directly into their workflows, the solution‚Äôs core logic is available through the idp_common Python package 
 
Prerequisites 
Before you deploy the solution, make sure you have an AWS account with administrator permissions and access to Amazon and Anthropic models on Amazon Bedrock. For more details, see Access Amazon Bedrock foundation models. 
Deploy the GenAI IDP Accelerator 
To deploy the GenAI IDP Accelerator, you can use the provided AWS CloudFormation template. For more details, see the quick start option on the GitHub repo. The high-level steps are as follows: 
 
 Log in to your AWS account. 
 Choose Launch Stack for your preferred AWS Region: 
 
 
  
   
   Region 
   Launch Stack 
   
  
  
   
   US East (N. Virginia) 
    
   
   
   US West (Oregon) 
    
   
  
 
 
 Enter your email address and choose your processing pattern (default is Pattern 1, using Amazon Bedrock Data Automation). 
 Use defaults for all other configuration parameters. 
 Deploy the stack. 
 
The stack takes approximately 15‚Äì20 minutes to deploy the resources. After deployment, you will receive an email with login credentials for the web interface. 
Process documents 
After you deploy the solution, you can start processing documents: 
 
 Use the web interface to upload a sample document (you can use the provided sample: lending_package.pdf). 
 
In production, you typically automate loading your documents directly to the Amazon Simple Storage Service (Amazon S3) input bucket, automatically triggering processing. To learn more, see Testing without the UI. 
 
 
 Select your document from the document list and choose View Processing Flow to watch as your document flows through the pipeline. 
 
 
 
 Examine the extracted data with confidence scores. 
 
 
 
 Use the knowledge base feature to ask questions about processed content. 
 
 
Alternative deployment methods 
You can build the solution from source code if you need to deploy the solution to additional Regions or build and deploy code changes.  
We hope to add support for AWS Cloud Development Kit (AWS CDK) and Terraform deployments. Follow the GitHub repository for updates, or contact AWS Professional Services for implementation assistance. 
Update an existing GenAI IDP Accelerator stack 
You can update your existing GenAI IDP Accelerator stack to the latest release. For more details, see Updating an Existing Stack. 
Clean up 
When you‚Äôre finished experimenting, clean up your resources by using the AWS CloudFormation console to delete the IDP stack that you deployed. 
Conclusion 
In this post, we discussed the GenAI IDP Accelerator, a new approach to document processing that combines the power of generative AI with the reliability and scale of AWS. You can process hundreds or even millions of documents to achieve better results faster and more cost-effectively than traditional approaches. 
Visit the GitHub repository for detailed guides and examples and choose watch to stay informed on new releases and features. AWS Professional Services and AWS Partners are available to help with implementation. You can also join the GitHub community to contribute improvements and share your experiences. 
 
About the Authors 
Bob Strahan is a Principal Solutions Architect in the AWS Generative AI Innovation Center. 
Joe King is a Senior Data Scientist in the AWS Generative AI Innovation Center. 
Mofijul Islam is an Applied Scientist in the AWS Generative AI Innovation Center. 
Vincil Bishop is a Senior Deep Learning Architect in the AWS Generative AI Innovation Center. 
David Kaleko is a Senior Applied Scientist in the AWS Generative AI Innovation Center. 
Rafal Pawlaszek is a Senior Cloud Application Architect in the AWS Generative AI Innovation Center. 
Spencer Romo is a Senior Data Scientist in the AWS Generative AI Innovation Center. 
Vamsi Thilak Gudi is a Solutions Architect in the AWS World Wide Public Sector team. 
 
 
Acknowledgments 
We would like to thank&nbsp;Abhi Sharma, Akhil Nooney, Aleksei Iancheruk, Ava Kong, Boyi Xie, Diego Socolinsky, Guillermo Tantachuco, Ilya Marmur, Jared Kramer, Jason Zhang, Jordan Ratner, Mariano Bellagamba, Mark Aiyer, Niharika Jain, Nimish Radia, Shean Sager, Sirajus Salekin, Yingwei Yu, and many others in our expanding community, for their unwavering vision, passion, contributions, and guidance throughout.

‚∏ª