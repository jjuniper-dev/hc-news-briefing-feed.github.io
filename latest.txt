‚úÖ Morning News Briefing ‚Äì October 23, 2025 10:45

üìÖ Date: 2025-10-23 10:45
üè∑Ô∏è Tags: #briefing #ai #publichealth #digitalgov

‚∏ª

üßæ Weather
‚Ä¢ Current Conditions:  5.4¬∞C
  Temperature: 5.4&deg;C Pressure / Tendency: 100.3 kPa rising Humidity: 95 % Dewpoint: 4.7&deg:C Wind: WSW 6 km/h . Air Quality Health Index: n/a . Pembroke 6:00 AM EDT Thursday 23 October 2025 . Weather forecast: 5/4¬∞C Pem
‚Ä¢ Thursday: Chance of showers. High 9. POP 30%
  A few showers beginning early this morning and ending near noon then 30 percent chance of showers this afternoon . Cloudy. Wind becoming northwest 30 km/h gusting to 50 this morning . High 9. UV index 2 or low, with a low of UV index of 2 or a low in the afternoon . Forecast issued 5:00 AM EDT Thursday 23 October 2025. For more information
‚Ä¢ Thursday night: Chance of rain showers or flurries. Low zero. POP 30%
  Mainly cloudy. 30 percent chance of rain showers changing to 30% chance of flurries or rain showers overnight . Wind west 30 km/h becoming light this evening. Low zero. Mainly . cloudy. Low 0.50¬∞F . Forecast issued 5:00 AM EDT Thursday 23 October 2025. Forecast: Snow, rain, snowfall, rain and flur

üåç International News
No updates.

üçÅ Canadian News
No updates.

üá∫üá∏ U.S. Top Stories
‚Ä¢ This 'magical' dinosaur specimen emerged from the ground like a polished jewel
  The word rinpoche is Tibetan for "precious one" and refers to the domed skull . The new pachycephalosaur was officially named Zavacephale rin pachycephale, a new species of dinosaur that was found in the Himalayan region of Tibet . It was discovered in the journal Nature, which has published a number of scientific journals
‚Ä¢ Your ballot or other mail may not get postmarked by USPS the day it's dropped off
  To make sure it gets a postmark on time under the latest USPS changes, you may want to send it early or visit a post office . You may also want to check your ballot, tax return or legal document on time to make sure your postmark is on time . The new USPS changes are changing the rules on how to post your tax returns, tax returns and legal documents .
‚Ä¢ Starter homes are scarce. But homebuyers can improve their odds
  Construction of smaller, entry-level homes has been has been falling for decades . But there are ways to find an affordable first home, and the changing market may help . The market is changing, and it's changing how to find a first home in the U.S. For decades, construction of smaller entry level homes has fallen, but there are still ways to get a home .
‚Ä¢ Antidepressant side effects differ greatly depending on the drug, study finds
  Researchers studied 30 different antidepressants and found side effects vary from drug to drug . Millions of Americans take antidepressants and they come with side effects . Side effects vary between drug and drug depending on the medication . Researchers say side effects of antidepressants vary from the drug to the drug and the side effects are similar to those of other drugs . Back to Mail Online home .Back to the page you came from
‚Ä¢ What Jared Kushner brings to the negotiating table in the Middle East
  President Trump's son-in-law says his relationships in the Middle East helped him broker a deal between Hamas and Israel . But his business ties also present a potential conflict, he says . Trump's business ties present a potentially conflict between his business and his relationship with business partners in Israel and the U.S. President Barack Obama is expected to make a speech at the White House on

üß† Artificial Intelligence
No updates.

üíª Digital Strategy
‚Ä¢ Microsoft threatens to ram Copilot into Exchange Server on-prem
  Survey probes interest in AI assistance for locally hosted email setups . Microsoft's mission to "Copilot all the things" has reached Exchange Server, with a survey asking if admins want the AI assistant on-prem.‚Ä¶‚Ä¶‚Ä¶ Survey also asks if administrators want the assistant to be in-premised, locally-hosted email setups, with the help of an AI assistant in-built
‚Ä¢ SpaceX pulls plug on 2,500 Starlink terminals tied to Myanmar fraud farms
  Criminal outfits had been using the broadband beacons to run cyber-slavery scams across Southeast Asia . SpaceX says it has shut down thousands of Starlink terminals that were powering Myanmar's notorious scam compounds after its satellite network was found to be keeping human trafficking and cyber-fraud operations online in the country's lawless border zones . The move comes after it was found that human trafficking
‚Ä¢ Microsoft finance slang defines the eternal optimist: The 'hockey stick on wheels'
  The wheels on Copilot's hockey stick must be giving off smoke by now . Microsoft's finance division has a term for an overly optimistic projection that seems to march backward year after year: the hockey stick on wheels . Copilot is Microsoft's version of a hockey stick with wheels on wheels, but the wheels on the wheels are giving off a lot of smoke . Microsoft: "Copilot
‚Ä¢ OpenBSD 7.8 out now, and you're not seeing double, 9front releases 'Release'
  New version includes multithreaded TCP/IP and Raspberry Pi 5 support . The 59th version of the OpenBSD operating system is here, six months after 7.7, with multiple improvements in various areas . The new version of OpenBOS is based on the open-source software OpenBASB 7.8, which was released in December . It is the first version
‚Ä¢ UK.gov vows to hack through regulation to get benefit from AI
  The UK government is pushing ahead with AI "sandboxing" and backing a raft of projects it claims could benefit from red-tape cutting . Civil services claims 75,000 days could be saved by the tech each year . Meanwhile, civil services claims the tech could be used to save the country's days by cutting red tape . The government is backing a range of projects that could benefit

üè• Public Health
No updates.

üî¨ Science
‚Ä¢ Ambient biothermal stress, preconceptional thyrotropin abnormalities, and the risk of preterm birth: a nationwide Chinese cohort study
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Predicting hypotension, syncope, and fracture risk in patients indicated for antihypertensive treatment: the STRATIFY models
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ We need more than good science to fight infectious disease
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ A guide to the Nature Index
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Measles, polio, tuberculosis: what‚Äôs causing spikes in infectious diseases?
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations

üßæ Government & Policy
No updates.

üèõÔ∏è Enterprise Architecture & IT Governance
No updates.

ü§ñ AI & Emerging Tech
‚Ä¢ What a massive thermal battery means for energy storage
  ‚ÄòI‚Äôve been following the way‚Äôs not the way, and this is the way that its first full-scale system is to show that it can‚Äôt be ‚Äòcan‚Äôn‚Äôv‚Äôm not the first of the ‚Äòvirus‚Äô and ‚Äòt‚Äôh‚Äô can be a member of the public‚Äô¬†‚Äòvila‚Äô will be able to bring up a new ‚Äòm‚Äô rather than a member‚Äô of the company that the company has the responsibility to make the public, and not the company, that the public should have the responsibility for the public to look for the future, and that it is not the public that it should look at the future of the nation, and the public can look for a future and the future .
‚Ä¢ This startup is about to conduct the biggest real-world test of aluminum as a zero-carbon fuel
  The crushed-up soda can disappears in a cloud of steam and‚Äîthough it‚Äôs not visible‚Äîhydrogen gas. ‚ÄúI can just keep this reaction going by adding more water,‚Äù says Peter Godart, squirting some into the steaming beaker. ‚ÄúThis is room-temperature water, and it‚Äôs immediately boiling. Doing this on your stove would be slower than this.‚Äù&nbsp;



Godart is the founder and CEO of Found Energy, a startup in Boston that aims to harness the energy in scraps of aluminum metal to power industrial processes without fossil fuels. Since 2022, the company has worked to develop ways to rapidly release energy from aluminum on a small scale. Now it‚Äôs just switched on a much larger version of its aluminum-powered engine, which Godart claims is the largest aluminum-water reactor ever built.&nbsp;



Early next year, it will be installed to supply heat and hydrogen to a tool manufacturing facility in the southeastern US, using the aluminum waste produced by the plant itself as fuel. (The manufacturer did not want to be named until the project is formally announced.)



If everything works as planned, this technology, which uses a catalyst to unlock the energy stored within aluminum metal, could transform a growing share of aluminum scrap into a zero-carbon fuel. The high heat generated by the engine could be especially valuable to reduce the substantial greenhouse-gas emissions generated by industrial processes, like cement production and metal refining, that are difficult to power with electricity directly.



‚ÄúWe invented the fuel, which is a blessing and a curse,‚Äù says Godart, surrounded by the pipes and wires of the experimental reactor. ‚ÄúIt‚Äôs a huge opportunity for us, but it also means we do have to develop all of the systems around us. We‚Äôre redefining what even is an engine.‚Äù



Engineers have long eyed using aluminum as a fuel thanks to its superior energy density. Once it has been refined and smelted from ore, aluminum metal contains more than twice as much energy as diesel fuel by volume and almost eight times as much as hydrogen gas. When it reacts with oxygen in water or air, it forms aluminum oxides. This reaction releases heat and hydrogen gas, which can be tapped for zero-carbon power.



Liquid metal



The trouble with aluminum as a fuel‚Äîand the reason your soda can doesn‚Äôt spontaneously combust‚Äîis that as soon as the metal starts to react, an oxidized layer forms across its surface that prevents the rest of it from reacting. It‚Äôs like a fire that puts itself out as it generates ash. ‚ÄúPeople have tried it and abandoned this idea many, many times,‚Äù says Godart.



Some believe using aluminum as a fuel remains a fool‚Äôs errand. ‚ÄúThis potential use of aluminum crops up every few years and has no possibility of success even if aluminum scrap is used as the fuel source,‚Äù says Geoff Scamans, a metallurgist at Brunel University of London who spent a decade working on using aluminum to power vehicles in the 1980s. He says the aluminum-water reaction isn‚Äôt efficient enough for the metal to make sense as a fuel given how much energy it takes to refine and smelt aluminum from ore to begin with: ‚ÄúA crazy idea is always a crazy idea.‚Äù



But Godart believes he and his company have found a way to make it work. ‚ÄúThe real breakthrough was thinking about catalysis in a different way,‚Äù he says: Instead of trying to speed up the reaction by bringing water and aluminum together onto a catalyst, they ‚Äúflipped it around‚Äù and ‚Äúfound a material that we could actually dissolve into the aluminum.‚Äù



JAMES DINNEEN




The liquid metal catalyst at the heart of the company‚Äôs approach ‚Äúpermeates the microstructure‚Äù of the aluminum, says Godart. As the aluminum reacts with water, the catalyst forces the metal to froth and split open, exposing more unreacted aluminum to the water.¬†



The composition of the catalyst is proprietary, but Godart says it is a ‚Äúlow-melting-point liquid metal that‚Äôs not mercury.‚Äù His dissertation research focused on using a liquid mixture of gallium and indium as the catalyst, and he says the principle behind the current material is the same.



During a visit in early October, Godart demonstrated the central reaction in the Found R&amp;D lab, which after the company‚Äôs $12 million seed round last year now fills the better part of two floors of an industrial building in Boston‚Äôs Charlestown neighborhood. Using a pair of tongs to avoid starting the reaction with the moisture on his fingers, he placed a pellet of aluminum treated with the secret catalyst in a beaker and then added water. Immediately, the metal began to bubble with hydrogen. Then the water steamed away, leaving behind a frothing gray mass of aluminum hydroxide.









‚ÄúOne of the impediments to this technology taking off is that [the aluminum-water reaction] was just too sluggish,‚Äù says Godart. ‚ÄúBut you can see here we‚Äôre making steam. We just made a boiler.‚Äù



From Europa to Earth



Godart was a scientist at NASA when he first started thinking about fresh ways to unlock the energy stored in aluminum. He was working on building aluminum robots that could consume themselves for fuel when roving on Jupiter‚Äôs icy moon Europa. But that work was cut short when Congress reduced funding for the mission.



‚ÄúI was sort of having this little mini crisis where I was like, I need to do something about climate change, about Earth problems,‚Äù says Godart. ‚ÄúAnd I was like, you know‚ÄîI bet this aluminum technology would be even better for Earth applications.‚Äù After completing a dissertation on aluminum fuels at MIT, he started Found Energy in his house in Cambridge in 2022 (the next year, he earned a place on MIT Technology Review‚Äôs annual 35 Innovators under 35 list).



Until this year, the company was working at a tiny scale, tweaking the catalyst and testing different conditions within a small 10-kilowatt reactor to make the reaction release more heat and hydrogen more quickly. Then, in January, it began designing an engine that‚Äôs 10 times larger, big enough to supply a useful amount of power for industrial processes beyond the lab.



This larger engine took up most of the lab on the second floor. The reactor vessel resembled a water boiler turned on its side, with piping and wires connected to monitoring equipment that took up almost as much space as the engine itself. On one end, there was a pipe to inject water and a piston to deliver pellets of aluminum fuel into the reactor at variable rates. On the other end, outflow pipes carried away the reaction products: steam, hydrogen gas, aluminum hydroxide, and the recovered catalyst. Godart says none of the catalyst is lost in the reaction, so it can be used again to make more fuel.



FOUND ENERGY




The company first switched on the engine to begin testing in July. In September, it managed to power it up to its targeted power of 100 kilowatts‚Äîroughly as much as can be supplied by the diesel engine in a small pickup truck. In early 2026, it plans to install the 100-kilowatt engine to supply heat and hydrogen to the tool manufacturing facility. This pilot project is meant to serve as the proof of concept needed to raise the money for a 1-megawatt reactor, 10 times larger again.



The initial pilot will use the engine to supply hot steam and hydrogen. But the energy released in the reactor could be put to use in a variety of ways across a range of temperatures, according to Godart. The hot steam could spin a turbine to produce electricity, or the hydrogen could produce electricity in a fuel cell. By burning the hydrogen within the steam, the engine can produce superheated steam as hot as 1,300 ¬∞C, which could be used to generate electricity more efficiently or refine chemicals. Burning the hydrogen alone could generate temperatures of 2,400 ¬∞C, hot enough to make steel.



Picking up scrap



Godart says he and his colleagues hope the engine will eventually power many different industrial processes, but the initial target is the aluminum refining and recycling industry itself, as it already handles scrap metal and aluminum oxide supply chains. ‚ÄúAluminum recyclers are coming to us, asking us to take their aluminum waste that‚Äôs difficult to recycle and then turn that into clean heat that they can use to re-melt other aluminum,‚Äù he says. ‚ÄúThey are begging us to implement this for them.‚Äù



Citing nondisclosure agreements, he wouldn‚Äôt name any of the companies offering up their unrecyclable aluminum, which he says is something of a ‚Äúdirty secret‚Äù for an industry that‚Äôs supposed to be recycling all it collects. But estimates from the International Aluminium Institute, an industry group, suggest that globally a little over 3 million metric tons of aluminum collected for recycling currently goes unrecycled each year; another 9 million metric tons isn‚Äôt collected for recycling at all or is incinerated with other waste. Together, that‚Äôs a little under a third of the estimated 43 million metric tons of aluminum scrap that currently gets recycled each year.



Even if all that unused scrap was recovered for fuel, it would still supply only a fraction of the overall industrial demand for heat, let alone the overall industrial demand for energy. But the plan isn‚Äôt to be limited by available scrap. Eventually, Godart says, the hope is to ‚Äúrecharge‚Äù the aluminum hydroxide that comes out of the reactor by using clean electricity to convert it back into aluminum metal and react it again. According to the company‚Äôs estimates, this ‚Äúclosed loop‚Äù approach could supply all global demand for industrial heat by using and reusing a total of around 300 million metric tons of aluminum‚Äîaround 4% of Earth‚Äôs abundant aluminum reserves.&nbsp;



However, all that recharging would require a lot of energy. ‚ÄúIf you‚Äôre doing that, [aluminum fuel] is an energy storage technology, not so much an energy providing technology,‚Äù says Jeffrey Rissman, who studies industrial decarbonization at Energy Innovation, a think tank in California. As with other forms of energy storage like thermal batteries or green hydrogen, he says, that could still make sense if the fuel can be recharged using low-cost, clean electricity. But that will be increasingly hard to come by amid the scramble for clean power for everything from AI data centers to heat pumps.



Despite these obstacles, Godart is confident his company will find a way to make it work. The existing engine may already be able to squeeze out more power from aluminum than anticipated. ‚ÄúWe actually believe this can probably do half a megawatt,‚Äù he says. ‚ÄúWe haven‚Äôt fully throttled it.‚Äù



James Dinneen is a science and environmental journalist based in New York City.&nbsp;
‚Ä¢ Introducing: the body issue
  This is today&#8217;s edition of¬†The Download,¬†our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



Introducing: the body issue



We‚Äôre thrilled to share the latest edition of MIT Technology Review magazine, digging into the future of the human body, and how it could change in the years ahead thanks to scientific and technological tinkering.The below stories are just a taste of what you can expect from this fascinating issue. To read the full thing, subscribe now if you haven‚Äôt already.



+ A new field of science claims to be able to predict aesthetic traits, intelligence, and even moral&nbsp;



character in embryos. But is this the next step in human evolution or something more dangerous? Read the full story.



+ How aging clocks can help us understand why we age‚Äîand if we could ever reverse it. Read the full story.



+ Instead of relying on the same old recipe biology follows, stem-cell scientist Jacob Hanna is coaxing the beginnings of animal bodies directly from stem cells. But should he?



+ The more we move, the more our muscle cells begin to make a memory of that exercise. Bonnie Tsui‚Äôs piece digs into how our bodies learn to remember.







MIT Technology Review Narrated: How Antarctica‚Äôs history of isolation is ending‚Äîthanks to Starlink



‚ÄúThis is one of the least visited places on planet Earth and I got to open the door,‚Äù Matty Jordan, a construction specialist at New Zealand‚Äôs Scott Base in Antarctica, wrote in the caption to the video he posted to Instagram and TikTok in October 2023.&nbsp;



In the video, he guides viewers through the hut, pointing out where the men of Ernest Shackleton‚Äôs 1907 expedition lived and worked.&nbsp;



The video has racked up millions of views from all over the world. It‚Äôs also kind of a miracle: until very recently, those who lived and worked on Antarctic bases had no hope of communicating so readily with the outside world. That‚Äôs starting to change, thanks to Starlink, the satellite constellation developed by Elon Musk‚Äôs company SpaceX to service the world with high-speed broadband internet.



This is our latest story to be turned into a MIT Technology Review Narrated podcast, which we‚Äôre publishing each week on Spotify and Apple Podcasts. Just navigate to MIT Technology Review Narrated on either platform, and follow us to get all our new content as it‚Äôs released.







The must-reads



I‚Äôve combed the internet to find you today‚Äôs most fun/important/scary/fascinating stories about technology.



1 OpenAI has launched its own web browser¬†¬†Atlas has an Ask ChatGPT sidebar and an agent mode to complete certain tasks. (TechCrunch)+ It runs on Chromium, the open-source engine that powers Google&#8217;s Chrome. (Axios)+ OpenAI believes the future of web browsing will involve chatting to its interface. (Ars Technica)+ AI means the end of internet search as we‚Äôve known it. (MIT Technology Review)



2 China is demanding US chip firms share their sales dataIt‚Äôs conducting a probe into American suppliers, and it wants answers. (Bloomberg $)3 AI pioneers are among those calling for a ban on superintelligent systemsIncluding Geoffrey Hinton and Yoshua Bengio. (The Guardian)+ Prominent Chinese scientists have also signed the statement. (FT $)+ Read our interview with Hinton on why he‚Äôs now scared of AI. (MIT Technology Review)



4 Anthropic promises its AI is not wokeDespite what the Trump administration‚Äôs ‚ÄúAI Czar‚Äù says. (404 Media)+ Its CEO insists the company shares the same goals as the Trump administration. (CNBC)+ Why it‚Äôs impossible to build an unbiased AI language model. (MIT Technology Review)



5 Climate scientists expect we‚Äôll see more solar geoengineering attemptsBut it‚Äôs a risky intervention with potentially huge repercussions. (New Scientist $)+ The hard lessons of Harvard‚Äôs failed geoengineering experiment. (MIT Technology Review)6 Why Silicon Valley is so fixated on ChinaIt marvels at the country‚Äôs ability to move fast and break things‚Äîbut should it?(NYT $)+ How Trump is helping China extend its massive lead in clean energy. (MIT Technology Review)



7 YouTube has launched a likeness detector to foil AI doppelg√§ngersBut that doesn‚Äôt guarantee that the fake videos will be removed. (Ars Technica)



8 Bots are threatening Reddit‚Äôs status as an oasis of human chatCan it keep fighting off the proliferation of AI slop? (WP $)+ It‚Äôs not just Reddit either‚Äîemployers are worried about ‚Äòworkslop‚Äô too. (FT $)+ AI trained on AI garbage spits out AI garbage. (MIT Technology Review)



9 This AI-powered pet toy is surprisingly cuteMoflin is a guinea pig-like creature that learns to become more expressive. (TechCrunch)+ AI toys are all the rage in China‚Äîand now they‚Äôre appearing on shelves in the US too. (MIT Technology Review)



10 You don‚Äôt need to know a lot about AI to get a job in AIMake of that what you will. (Fast Company $)







Quote of the day



‚ÄúIt&#8217;s wild that Google wrote the Transformers paper (that birthed GPTs) AND open sourced Chromium, both of which will (eventually) lead to the downfall of their search monopoly. History lesson in there somewhere.‚Äù



‚ÄîInvestor Nikunj Kothari ponders the future of Google‚Äôs empire in the wake of the announcement of OpenAI‚Äôs new web browser in a post on X.







One more thing







The quest to protect farmworkers from extreme heatEven as temperatures rise each summer, the people working outdoors to pick fruits, vegetables, and flowers have to keep laboring.The consequences can be severe, leading to illnesses such as heat exhaustion, heatstroke and even acute kidney injury.Now, researchers are developing an innovative sensor that tracks multiple vital signs with a goal of anticipating when a worker is at risk of developing heat illness and issuing an alert. If widely adopted and consistently used, it could represent a way to make workers safer on farms even without significant heat protections. Read the full story.



‚ÄîKalena Thomhave







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)



+ Netflix is making a film based on the hit board game Catan, for some reason.+ Why it‚Äôs time to embrace the beauty of slow running.+ The Satellite Crayon Project takes colors from the natural world and turns them into vibrant drawing implements.+ Mamma Mia has never sounded better.
‚Ä¢ 3 Things Stephanie Arnett is into right now
  The latest version of the birding app Merlin helps ease the transition to birding . The app can analyze which birds are singing in real time with step-by-step questions, photos, or audio . It's open-source and files are stored on my device, so I don't have to worry about whether I‚Äôm sharing my private thoughts with a company that might scrape them for AI .
‚Ä¢ Dispatch: Partying at one of Africa‚Äôs largest AI gatherings
  Deep Learning Indaba is an annual AI conference where Africans present their research and technologies they‚Äôve built . The event began in 2017 from a nucleus of 300 people gathered in Johannesburg, South Africa . This year, nearly 3,000 people applied to join the Indaba; about 1,300 were accepted . The main ‚Äòprize‚Äô for many attendees is to be hired by a tech company or accepted into a PhD program, says Nyalleng Moorosi .

üîí Cybersecurity & Privacy
‚Ä¢ Canada Fines Cybercrime Friendly Cryptomus $176M
  Financial regulators in Canada this week levied $176 million in fines against Cryptomus, a digital payments platform that supports dozens of Russian cryptocurrency exchanges and websites hawking cybercrime services . The penalties for violating Canada&#8217;s anti money-laundering laws come ten months after KrebsOnSecurity noted the Vancouver address was home to dozens of foreign currency dealers, money transfer businesses, and cryptocurrency exchanges .

üéì University AI
No updates.

üè¢ Corporate AI
‚Ä¢ Tell me when: Building agents that can wait, monitor, and act
  Modern&nbsp;LLM&nbsp;Agents&nbsp;can debug code, analyze spreadsheets, and book complex travel.&nbsp;Given those capabilities, it‚Äôs reasonable to assume that they could handle something simpler:&nbsp;waiting.&nbsp;Ask an agent to&nbsp;monitor&nbsp;your email for a colleague‚Äôs response or watch for a price drop over several days, and it will fail. Not because it&nbsp;can‚Äôt&nbsp;check email or scrape prices. It can do both. It fails&nbsp;because it&nbsp;doesn‚Äôt&nbsp;know&nbsp;when&nbsp;to check.&nbsp;Agents either&nbsp;give up after a few attempts or burn through their context window, checking obsessively. Neither&nbsp;work.&nbsp;



This matters because monitoring tasks&nbsp;are&nbsp;everywhere. We track emails for specific information, watch news&nbsp;feeds for updates, and&nbsp;monitor&nbsp;prices for sales. Automating these tasks would save hours, but current&nbsp;agents&nbsp;aren‚Äôt&nbsp;built for patience.



To address this, we are introducing&nbsp;SentinelStep (opens in new tab),&nbsp;a&nbsp;mechanism&nbsp;that&nbsp;enables&nbsp;agents&nbsp;to complete long-running monitoring&nbsp;tasks.&nbsp;The&nbsp;approach is simple.&nbsp;SentinelStep&nbsp;wraps the agent in a workflow with dynamic&nbsp;polling&nbsp;and&nbsp;careful context&nbsp;management.&nbsp;This&nbsp;enables&nbsp;the&nbsp;agent&nbsp;to&nbsp;monitor&nbsp;conditions for&nbsp;hours&nbsp;or&nbsp;days&nbsp;without getting&nbsp;sidetracked.&nbsp;We&#8217;ve&nbsp;implemented&nbsp;SentinelStep&nbsp;in&nbsp;Magentic-UI,&nbsp;our research&nbsp;prototype&nbsp;agentic system,&nbsp;to enable&nbsp;users&nbsp;to&nbsp;build agents for&nbsp;long-running&nbsp;tasks,&nbsp;whether they&nbsp;involve web&nbsp;browsing, coding, or external&nbsp;tools.&nbsp;



	
		

		
		PODCAST SERIES
	
	
	
						
				
					
				
			
			
			

									AI Testing and Evaluation: Learnings from Science and Industry
				
								Discover how Microsoft is learning from other domains to advance evaluation and testing as a pillar of AI governance.
				
								
					
						
							Listen now						
					
				
							
	
Opens in a new tab	
	


How it works



The core&nbsp;challenge is&nbsp;polling frequency. Poll too often,&nbsp;and&nbsp;tokens get&nbsp;wasted. Poll too infrequently, and the user‚Äôs notification gets delayed.&nbsp;SentinelStep&nbsp;makes&nbsp;an educated guess&nbsp;at&nbsp;the&nbsp;polling interval based on the task at hand‚Äîchecking email gets different treatment&nbsp;than&nbsp;monitoring&nbsp;quarterly earnings‚Äîthen dynamically adjusts&nbsp;based on&nbsp;observed&nbsp;behavior.&nbsp;



There‚Äôs&nbsp;a second challenge: context overflow.&nbsp;Because monitoring tasks can run for days,&nbsp;context overflow&nbsp;becomes inevitable.&nbsp;SentinelStep&nbsp;handles&nbsp;this by saving the agent state after the first check, then&nbsp;using&nbsp;that state for each subsequent check.





















These demonstrations capture&nbsp;Magentic-UI with&nbsp;SentinelStep&nbsp;at work, completing a range of tasks in a timelapse sequence.&nbsp;



Core components



As&nbsp;the name&nbsp;suggests,&nbsp;SentinelStep&nbsp;consists of&nbsp;individual steps&nbsp;taken as part of&nbsp;an&nbsp;agent‚Äôs broader&nbsp;workflow.&nbsp;As illustrated in Figure 1, there are three main components:&nbsp;the&nbsp;actions necessary to collect information, the condition that determines&nbsp;when&nbsp;the task&nbsp;is complete,&nbsp;and the polling interval&nbsp;that&nbsp;determines&nbsp;timing.&nbsp;Once&nbsp;these components&nbsp;are&nbsp;identified, the&nbsp;system‚Äôs&nbsp;behavior is simple:&nbsp;every&nbsp;[polling interval]&nbsp;do&nbsp;[actions]&nbsp;until&nbsp;[condition]&nbsp;is satisfied.&nbsp;



Figure&nbsp;1.&nbsp;SentinelSteps‚Äôs&nbsp;three main components&nbsp;in&nbsp;Magentic-UI‚Äôs&nbsp;co-planning interface.&nbsp;



These three components are defined and exposed in the co-planning interface of Magentic-UI. Given a user prompt, Magentic-UI proposes a complete multi-step plan, including pre-filled parameters for any monitoring steps. Users can accept the plan or adjust as needed.



Processing



Once a run starts, Magentic-UI assigns the most appropriate agent from a team of agents to perform each action. This team includes agents capable of web surfing, code execution, and calling arbitrary MCP servers.



When the workflow reaches a monitoring step, the flow is straightforward. The assigned agent collects the necessary information through the actions described in the plan. The Magentic-UI orchestrator then checks whether the condition is satisfied. If it is, the SentinelStep is complete, and the orchestrator moves to the next step. If not, the orchestrator determines the timestamp for the next check and resets the agent‚Äôs state to prevent context overflow.



Evaluation



Evaluating&nbsp;monitoring tasks in real-world settings&nbsp;is&nbsp;nearly impossible.&nbsp;Consider a simple example: monitoring the Magentic-UI repository on GitHub&nbsp;until&nbsp;it reaches&nbsp;10,000 stars&nbsp;(a measure of how many people have bookmarked it). That event occurs only once and can‚Äôt be repeated.&nbsp;Most&nbsp;real-world monitoring tasks share this limitation, making systematic bench marking very challenging.



In response, we&nbsp;are developing&nbsp;SentinelBench, a suite of synthetic&nbsp;web environments for evaluating monitoring tasks. These environments make experiments repeatable. SentinelBench&nbsp;currently&nbsp;supports&nbsp;28&nbsp;configurable scenarios, each&nbsp;allowing the user to schedule exactly when&nbsp;a&nbsp;target&nbsp;event&nbsp;should&nbsp;occur. It includes setups like GitHub Watcher, which&nbsp;simulates a repository accumulating stars over time;&nbsp;Teams Monitor, which models incoming messages, some&nbsp;urgent; and&nbsp;Flight Monitor, which&nbsp;replicates&nbsp;evolving&nbsp;flight-availability&nbsp;dynamics.&nbsp;



Initial&nbsp;tests&nbsp;show clear benefits.&nbsp;As shown in&nbsp;Figure&nbsp;2, success rates&nbsp;remain&nbsp;high for short tasks (30&nbsp;sec&nbsp;and 1&nbsp;min) regardless of&nbsp;whether&nbsp;SentinelStep&nbsp;is&nbsp;used.&nbsp;For longer tasks,&nbsp;SentinelStep&nbsp;markedly&nbsp;improves reliability: at 1 hour, task reliability rises from 5.6% without&nbsp;SentinelStep&nbsp;to&nbsp;33.3% with&nbsp;it;&nbsp;and at 2 hours,&nbsp;it rises&nbsp;from 5.6% to 38.9%. These gains&nbsp;demonstrate&nbsp;that&nbsp;SentinelStep&nbsp;effectively addresses the challenge of maintaining performance over extended durations.



Figure&nbsp;2.&nbsp;SentinelStep&nbsp;improves&nbsp;success rates&nbsp;on longer running tasks (1‚Äì2&nbsp;hours)&nbsp;while&nbsp;maintaining&nbsp;comparable performance&nbsp;on shorter tasks.&nbsp;&nbsp;



Impact and availability



SentinelStep is a first step toward practical, proactive, longer‚Äërunning agents. By embedding patience into plans, agents can responsibly monitor conditions and act when it matters‚Äîstaying proactive without wasting resources. This lays the groundwork for always‚Äëon assistants that stay efficient, respectful of limits, and aligned with user intent.



We‚Äôve open-sourced SentinelStep as part of Magentic-UI, available on GitHub (opens in new tab) or via pip install magnetic-ui. As with any new technique, production deployment should be preceded through&nbsp;testing and validation&nbsp;for the specific use case.&nbsp;For&nbsp;guidance on&nbsp;intended use,&nbsp;privacy&nbsp;considerations,&nbsp;and safety&nbsp;guidelines,&nbsp;see&nbsp;the&nbsp;Magentic-UI&nbsp;Transparency&nbsp;Note. (opens in new tab)&nbsp;



Our goal is to&nbsp;make it easier to implement agents that can&nbsp;handle&nbsp;long-running&nbsp;monitoring&nbsp;tasks&nbsp;and&nbsp;lay&nbsp;the groundwork for&nbsp;systems that&nbsp;anticipate, adapt, and&nbsp;evolve&nbsp;to meet real-world needs.&nbsp;
Opens in a new tabThe post Tell me when: Building agents that can wait, monitor, and act appeared first on Microsoft Research.
‚Ä¢ Build scalable creative solutions for product teams with Amazon Bedrock
  Creative teams and product developers are constantly seeking ways to streamline their workflows and reduce time to market while maintaining quality and brand consistency. This post demonstrates how to use AWS services, particularly Amazon Bedrock, to transform your creative processes through generative AI. You can implement a secure, scalable solution that accelerates your creative workflow, such as managing product launches, creating marketing campaigns, or developing multimedia content. 
This post examines how product teams can deploy a generative AI application that enables rapid content iteration across formats. The solution addresses comprehensive needs‚Äîfrom product descriptions and marketing copy to visual concepts and video content for social media. By integrating with brand guidelines and compliance requirements, teams can significantly reduce time to market while maintaining creative quality and consistency. 
Solution overview 
Consider a product development team at an ecommerce company creating multimedia marketing campaigns for their seasonal product launches. Their traditional workflow has bottlenecks due to lengthy revisions, manual compliance reviews, and complex coordination across creative teams. The team is exploring solutions to rapidly iterate through creative concepts, generate multiple variations of marketing materials. 
By using Amazon Bedrock and Amazon Nova models, the team can transform its creative process. Amazon Nova models enable the generation of product descriptions and marketing copy. The team creates concept visuals and product mockups with Amazon Nova Canvas, and uses Amazon Nova Reel to produce engaging video content for social media presence. Amazon Bedrock Guardrails can help the team maintain consistent brand guidelines with configurable safeguards and governance for its generative AI applications at scale. 
The team can further enhance its brand consistency with Amazon Bedrock Knowledge Bases, which can serve as a centralized repository for brand style guides, visual identity documentation, and successful campaign materials. This comprehensive knowledge base makes sure generated content is informed by the organization‚Äôs historical success and established brand standards. Product specifications, market research, and approved messaging are seamlessly integrated into the creative process, enabling more relevant and effective content generation. 
With this solution, the team can simultaneously develop materials for multiple channels while maintaining consistent brand voice across their content. Creative professionals can now focus their energy on strategic decisions rather than repetitive tasks, leading to higher-quality outputs and improved team satisfaction. 
The following sample application creates a scalable environment that streamlines the creative workflow. It helps product teams move seamlessly from initial concept to market-ready materials with automated systems handling compliance and consistency checks throughout the journey. 
 
The solution‚Äôs workflow begins with the application engineer‚Äôs setup: 
 
 Creative assets and brand guidelines are securely stored in encrypted Amazon Simple Storage Service (Amazon S3) buckets. This content is then indexed in Amazon OpenSearch Service to create a comprehensive knowledge base. 
 Guardrails are configured to enforce brand standards and compliance requirements. 
 
The user experience flows from authentication to content delivery: 
 
 Creative team members access the interface through a secure portal hosted in Amazon S3. 
 Authentication is managed through Amazon Cognito. 
 Team members‚Äô submitted creative briefs or requirements are routed to Amazon API Gateway. 
 An AWS Lambda function queries relevant brand guidelines and assets from the knowledge base. 
 The Lambda function sends the contextual information from the knowledge base to Amazon Bedrock, along with the user‚Äôs creative briefs. 
 The prompt and generated response are filtered through Amazon Bedrock Guardrails. 
 Amazon Polly converts text into lifelike speech, generating audio streams that can be played immediately and stored in S3 buckets for later use. 
 The models‚Äô generated content is delivered to the user. 
 Chat history stored in Amazon DynamoDB. 
 
Prerequisites 
The following prerequisites are required before continuing: 
 
 An AWS account 
 An AWS Identity and Access Management (IAM) role with permission to manage AWS Marketplace subscriptions and AWS services 
 AWS services: 
   
   AWS CloudFormation 
   Amazon API Gateway 
   AWS CloudFormation 
   Amazon Cognito 
   Amazon DynamoDB 
   Amazon Polly 
   Amazon S3 
   Amazon Virtual Private Cloud (Amazon VPC) with two public subnets 
    
 Amazon Bedrock models enabled: 
   
   Amazon Nova Canvas 
   Amazon Nova Reels 
   Amazon Nova Pro 
   Amazon Nova Lite 
    
 Anthropic models (optional): 
   
   Anthropic‚Äôs Claude 3 Sonnet 
    
 
Select the Models to Use in Amazon Bedrock 
When working with Amazon Bedrock for generative AI applications, one of the Ô¨Årst steps is selecting which foundation models you want to access. Amazon Bedrock oÔ¨Äers a variety of models from diÔ¨Äerent providers, and you‚Äôll need to explicitly enable the ones we plan to use in this blog. 
 
 In the Amazon Bedrock console, find and select Model access from the navigation menu on the left. 
 Click the Modify model access button to begin selecting your models. 
 Select the following Amazon models: 
   
   Nova Canvas 
   Nova Premier Cross-region inference Nova Pro 
   Titan Embeddings G1 ‚Äì Text 
   Titan Text Embeddings V2 
    
 Select the Anthropic Claude 3.7 Sonnet model. 
 Choose Next. 
 Review your selections carefully on the summary page, then choose&nbsp;Submit to confirm your choices. 
 
Set up the CloudFormation template 
We use a use a CloudFormation template to deploy all necessary solution resources. Follow these steps to prepare your installation files: 
 
 Clone the GitHub repository: 
   
   git clone https://github.com/aws-samples/aws-service-catalog-reference-architectures.git
 
    
 Navigate to the solution directory: 
   
   cd aws-service-catalog-reference-architectures/blog_content/bedrock_genai
 
   (Make note of this location as you‚Äôll need it in the following steps) 
 Sign in to your AWS account with administrator privileges to ensure you can create all required AWS resources. 
 Create an S3 bucket in the AWS Region where you plan to deploy this solution. Remember the bucket name for later steps. 
 Upload the entire content folder to your newly created S3 bucket. 
 Navigate to the content/genairacer/src folder in your S3 bucket. 
 Copy the URL for the content/genairacer/src/genairacer_setup.json file. You‚Äôll need this URL for the deployment phase. 
 
Deploy the CloudFormation template 
Complete the following steps to use the provided CloudFormation template to automatically create and configure the application components within your AWS account: 
 
 On the CloudFormation console, choose Stacks in navigation pane. 
 Choose Create stack and select with new resources (standard). 
 On the Create stack page, under Specify template, for Object URL, enter the URL copied from the previous step, then choose Next. 
 On the Specify stack details page, enter a stack name. 
 Under Parameters, choose Next. 
 On the Configure stack options page, choose Next. 
 On the Review page, select the acknowledgement check boxes and choose Submit. 
 
Sign in to the Amazon Bedrock generative AI application 
Accessing your newly deployed application is simple and straightforward. Follow these steps to log in for the Ô¨Årst time and start exploring the Amazon Bedrock generative AI interface. 
 
 On the CloudFormation console, select the stack you deployed and select the Outputs tab. 
 Find the FrontendURL value and open the provided link. 
 When the sign-in screen displays, enter the username you speciÔ¨Åed during the CloudFormation deployment process. 
 Enter the temporary password that was sent to the email address you provided during setup. 
 After you sign in, follow the prompts to change your password. 
 Choose Send to conÔ¨Årm your new credentials. 
 
Once authenticated, you‚Äôll be directed to the main Amazon Bedrock generative AI dashboard, where you can begin exploring all the features and capabilities of your new application. 
Using the application 
Now that the application has been deployed, you can use it for text, image, and audio management. In the following sections, we explore some sample use cases. 
Text generation 
The creative team at the ecommerce company wants to draft compelling product descriptions. By inputting the basic product features and desired tone, the LLM generates engaging and persuasive text that highlights the unique selling points of each item, making sure the online store‚Äôs product pages are both informative and captivating for potential customers. 
To use the text generation feature and perform actions with the supported text models using Amazon Bedrock, follow these steps: 
 
 On the AWS CloudFormation console, go to the stack you created. 
 Choose the Outputs tab. 
 Choose the link for FrontendURL. 
 Log in using the credentials sent to the email you provided during the stack deployment process. 
 On the Text tab, enter your desired prompt in the input field. 
 Choose the specific model ID you want Amazon Bedrock to use from the available options. 
 Choose Run. 
 
Repeat this process for any additional prompts you want to process. 
 
Image generation 
The creative team can now conceptualize and produce stunning product images. By describing the desired scene, style, and product placement, they can enhance the online shopping experience and increase the likelihood of customer engagement and purchase.To use the image generation feature, follow these steps: 
 
 In the UI, choose the Images tab. 
 Enter your desired text-to-image prompt in the input field. 
 Choose the specific model ID you want Amazon Bedrock to utilize from the available options. 
 Optionally, choose the desired style of the image from the provided style options. 
 Choose Generate Image. 
 
Repeat this process for any additional prompts you want to process. 
  
Audio generation 
The ecommerce company‚Äôs creative team wants to develop audio content for marketing campaigns. By specifying the message, brand voice, target demographic, and audio components, they can compose scripts and generate voiceovers for promotional videos and audio ads, resulting in consistent and professional audio materials that effectively convey the brand‚Äôs message and values.To use the audio generation feature, follow these steps: 
 
 In the UI, choose the Audio tab. 
 Enter your desired prompt in the input field. 
 Choose Run. An audio file will appear and start to play. 
 Choose the file (right-click) and choose Save Audio As to save the file. 
 
 
Amazon Bedrock Knowledge Bases 
With Amazon Bedrock Knowledge Bases, you can provide foundation models (FMs) and agents with contextual information from your organization‚Äôs private data sources, to deliver more relevant, accurate, and tailored responses. It is a powerful and user-friendly implementation of the Retrieval Augmented Generation (RAG) approach. The application showcased in this post uses the Amazon Bedrock components in the backend, simplifying the process to merely uploading a document using the application‚Äôs GUI, and then entering a prompt that will query the documents you upload. 
For our example use case, the creative team now needs to research information about internal processes and customer data, which are typically stored in documentation. When this documentation is stored in the knowledge base, they can query it on the KnowledgeBase tab. The queries executed on this tab will search the documents for the specific information they are looking for. 
Manage documents 
The documents you have uploaded will be listed on the KnowledgeBase tab. To add more, complete the following steps: 
 
 In the UI, choose the KnowledgeBase tab. 
 Choose Manage Document. 
 Choose Browse, then choose a file. 
 Choose Upload. 
 
You will see a message confirming that the file was uploaded successfully.The Amazon Bedrock Knowledge Bases syncing process is triggered when the file is uploaded. The application will be ready for queries against the new document within a minute. 
Query the knowledge base 
To query the knowledge base, complete the following steps: 
 
 In the UI, choose the KnowledgeBase tab. 
 Enter your query in the input field. 
 For Model, choose the model you want Amazon Bedrock to use for performing the query. 
 Choose Run. 
 
The generated text response from Amazon Bedrock will appear. 
Amazon Bedrock guardrails 
You can use the Guardrails tab to manage your guardrails, and create and remove guardrails as needed. Guardrails are used on the Text tab when performing queries. 
Create a guardrail 
Complete the following steps to create a new guardrail: 
 
 In the UI, choose the Guardrails tab. 
 Enter the required fields or choose the appropriate options. 
 Choose the type of guardrail under Content Filter Type. 
 Choose Create Guardrail. 
 
The newly created guardrail will appear in the right pane. 
Delete a guardrail 
Complete the following steps to delete a guardrail: 
 
 In the UI, choose the Guardrails tab. 
 Choose the guardrail you want to delete in the right pane. 
 Choose the X icon next to the guardrail. 
 
By following these steps, you can effectively manage your guardrails, for a seamless and controlled experience when performing queries in the Text tab. 
Use guardrails 
The creative team requires access to information about internal processes and customer data, which are securely stored in documentation within the knowledge base. To enforce compliance with personally identifiable information (PII) guardrails, queries executed using the Text tab are designed to search documents for specific, non-sensitive information while preventing the exposure or inclusion of PII in both prompts and answers. This approach helps the team retrieve necessary data without compromising privacy or security standards. 
To use the guardrails feature, complete the following steps: 
 
 In the UI, choose the Text tab. 
 Enter your prompt in the input field. 
 For Model ID, choose the specific model ID you want Amazon Bedrock to use. 
 Turn on Guardrails. 
 For Select Filter, choose the guardrail you want to use. 
 Choose Run. 
 
The generated text from Amazon Bedrock will appear within a few seconds. Repeat this process for any additional prompts you want to process. 
 
Clean up 
To avoid incurring costs, delete resources that are no longer needed. If you no longer need the solution, complete the following steps to delete all resources you created from your AWS account: 
 
 On the AWS CloudFormation console, choose Stacks in the navigation pane. 
 Select the stack you deployed and choose Delete. 
 
Conclusion 
By combining Amazon Bedrock, Knowledge Bases, and Guardrails with Cognito, API Gateway, and Lambda, organizations can give employees powerful AI tools for text, image, and data work. This serverless approach integrates generative AI into daily workflows securely and scalably, boosting productivity and innovation across teams.. 
For more information about generative AI and Amazon Bedrock, refer to the Amazon Bedrock category in the AWS News Blog. 
 
About the authors 
 Kenneth Walsh is a Senior AI Acceleration Architect based in New York who transforms AWS builder productivity through innovative generative AI automation tools. With a strategic focus on standardized frameworks, Kenneth accelerates partner adoption of generative AI technologies at scale. As a trusted advisor, he guides customers through their GenAI journeys with both technical expertise and genuine passion. Outside the world of artiÔ¨Åcial intelligence, Kenneth enjoys crafting culinary creations, immersing himself in audiobooks, and cherishing quality time with his family and dog. 
Wanjiko Kahara is a New York‚Äìbased Solutions Architect with a interest area in generative AI. Wanjiko is excited about learning new technology to help her customers be successful. Outside of work, Wanjiko loves to travel, explore the outdoors, and read. 
Greg Medard is a Solutions Architect with AWS. Greg guides clients in architecting, designing, and developing cloud-optimized infrastructure solutions. His drive lies in fostering cultural shifts by embracing DevOps principles that overcome organizational hurdles. Beyond work, he cherishes quality time with loved ones, tinkering with the latest tech gadgets, or embarking on adventures to discover new destinations and culinary delights. 
Bezuayehu Wate is a Specialist Solutions Architect at AWS, with a focus on big data analytics. Passionate about helping customers design, build, and modernize their cloud-based analytics solutions, she Ô¨Ånds joy in learning and exploring new technologies. Outside of work, Bezuayehu enjoys quality time with family and traveling. 
Nicole Murray is a generative AI Senior Solutions Architect at AWS, specializing in MLOps and Cloud Operations for AI startups. With 17 years of experience‚Äîincluding helping government agencies design secure, compliant applications on AWS‚Äîshe now partners with startup founders to build and scale innovative AI/ML solutions. Nicole helps teams navigate secure cloud management, technical strategy, and regulatory best practices in the generative AI space, and is also a passionate speaker and educator known for making complex cloud and AI topics accessible.
‚Ä¢ Build a proactive AI cost management system for Amazon Bedrock ‚Äì Part 2
  In Part 1 of our series, we introduced a proactive cost management solution for Amazon Bedrock, featuring a robust cost sentry mechanism designed to enforce real-time token usage limits. We explored the core architecture, token tracking strategies, and initial budget enforcement techniques that help organizations control their generative AI expenses. 
Building upon that foundation, this post explores advanced cost monitoring strategies for generative AI deployments. We introduce granular custom tagging approaches for precise cost allocation, and develop comprehensive reporting mechanisms. 
Solution overview 
The cost sentry solution introduced in Part 1 was developed as a centralized mechanism to proactively limit generative AI usage to adhere to prescribed budgets. The following diagram illustrates the core components of the solution, adding in cost monitoring through AWS Billing and Cost Management. 
 
Invocation-level tagging for enhanced traceability 
Invocation-level tagging extends our solution‚Äôs capabilities by attaching rich metadata to every API request, creating a comprehensive audit trail within Amazon CloudWatch logs. This becomes particularly valuable when investigating budget-related decisions, analyzing rate-limiting impacts, or understanding usage patterns across different applications and teams. To support this, the main AWS Step Functions workflow was updated, as illustrated in the following figure. 
 
Enhanced API input 
We also evolved the API input to support custom tagging. The new input structure introduces optional parameters for model-specific configurations and custom tagging: 
 
 {
&nbsp;&nbsp;"model": "string", &nbsp; &nbsp; // e.g., "claude-3" or "anthropic.claude-3-sonnet-20240229-v1:0"
&nbsp;&nbsp;"prompt": {
&nbsp;&nbsp; &nbsp;"messages": [
&nbsp;&nbsp; &nbsp; &nbsp;{
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"role": "string", &nbsp; &nbsp;// "system", "user", or "assistant"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"content": "string"
&nbsp;&nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp;],
&nbsp;&nbsp; &nbsp;"parameters": {
&nbsp;&nbsp; &nbsp; &nbsp;"max_tokens": number, &nbsp;&nbsp; // Optional, model-specific defaults
&nbsp;&nbsp; &nbsp; &nbsp;"temperature": number, &nbsp; // Optional, model-specific defaults
&nbsp;&nbsp; &nbsp; &nbsp;"top_p": number, &nbsp; &nbsp; &nbsp; &nbsp; // Optional, model-specific defaults
&nbsp;&nbsp; &nbsp; &nbsp;"top_k": number &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;// Optional, model-specific defaults
&nbsp;&nbsp; &nbsp;}
&nbsp;&nbsp;},
&nbsp;&nbsp;"tags": {
&nbsp;&nbsp; &nbsp;"applicationId": "string", &nbsp;// Required
&nbsp;&nbsp; &nbsp;"costCenter": "string", &nbsp; &nbsp; // Optional
&nbsp;&nbsp; &nbsp;"environment": "string" &nbsp; &nbsp;&nbsp;// Optional - dev/staging/prod
&nbsp;&nbsp;}
} 
 
The input structure comprises three key components: 
 
 model ‚Äì Maps simple names (for example, claude-3) to full Amazon Bedrock model IDs (for example, anthropic.claude-3-sonnet-20240229-v1:0) 
 input ‚Äì Provides a messages array for prompts, supporting both single-turn and multi-turn conversations 
 tags ‚Äì Supports application-level tracking, with applicationId as the required field and costCenter and environment as optional fields 
 
In this example, we use different cost centers for sales, services, and support to simulate the use of a business attribute to track usage and spend for inference in Amazon Bedrock. For example: 
 
 {
&nbsp;&nbsp;"model": "claude-3-5-haiku",
&nbsp;&nbsp;"prompt": {
&nbsp;&nbsp; &nbsp;"messages": [
&nbsp;&nbsp; &nbsp; &nbsp;{
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"role": "user",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"content": "Explain the benefits of using S3 using only 100 words."
&nbsp;&nbsp; &nbsp; &nbsp;},
&nbsp;&nbsp; &nbsp; &nbsp;{
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"role": "assistant",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"content": "You are a helpful AWS expert."
&nbsp;&nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp;],
&nbsp;&nbsp; &nbsp;"parameters": {
&nbsp;&nbsp; &nbsp; &nbsp;"max_tokens": 2000,
&nbsp;&nbsp; &nbsp; &nbsp;"temperature": 0.7,
&nbsp;&nbsp; &nbsp; &nbsp;"top_p": 0.9,
&nbsp;&nbsp; &nbsp; &nbsp;"top_k": 50
&nbsp;&nbsp; &nbsp;}
&nbsp;&nbsp;},
&nbsp;&nbsp;"tags": {
&nbsp;&nbsp; &nbsp;"applicationId": "aws-documentation-helper",
&nbsp;&nbsp; &nbsp;"costCenter": "support",
&nbsp;&nbsp; &nbsp;"environment": "production"
&nbsp;&nbsp;}
} 
 
Validation and tagging 
A new validation step was added to the workflow for tagging. This step uses an AWS Lambda function to add validation checks and maps the model requested to the specific model ID in Amazon Bedrock. It supplements the tags object with tags that will be required for downstream analysis. 
The following code is an example of a simple map to get the appropriate model ID from the model specified: 
 
 MODEL_ID_MAPPING =&nbsp;{
&nbsp;&nbsp; &nbsp;"nova-lite": "amazon.nova-lite-v1:0",
&nbsp;&nbsp; &nbsp;"nova-micro": "amazon.nova-micro-v1:0",
&nbsp;&nbsp; &nbsp;"claude-2": "anthropic.claude-v2:0",
&nbsp;&nbsp; &nbsp;"claude-3-haiku": "anthropic.claude-3-haiku-20240307-v1:0",
&nbsp;&nbsp; &nbsp;"claude-3-5-sonnet-v2": "us.anthropic.claude-3-5-sonnet-20241022-v2:0",
&nbsp;&nbsp; &nbsp;"claude-3-5-haiku": "us.anthropic.claude-3-5-haiku-20241022-v1:0"
} 
 
Logging and analysis 
By using CloudWatch metrics with custom-generated tags and dimensions, you can track detailed metrics across multiple dimensions such as model type, cost center, application, and environment. Custom tags and dimensions show how teams use AI services. To see this analysis, steps were implemented to generate custom tags, store metric data, and analyze metric data: 
 
 We include a unique set of tags that capture contextual information. This can include user-supplied tags as well as ones that are dynamically generated, such as requestId and timestamp: 
   
   &nbsp;&nbsp;"tags": {
&nbsp;&nbsp; &nbsp;"requestId": "ded98994-eb76-48d9-9dbc-f269541b5e49",
&nbsp;&nbsp; &nbsp;"timestamp": "2025-01-31T14:05:26.854682",
&nbsp;&nbsp; &nbsp;"applicationId": "aws-documentation-helper",
&nbsp;&nbsp; &nbsp;"costCenter": "support",
&nbsp;&nbsp; &nbsp;"environment": "production"
} 
    
 As each workflow is executed, the limit for each model will be evaluated to make sure the request is within budgetary guidelines. The workflow will end based on three possible outcomes: 
   
   Rate limit approved and invocation successful 
   Rate limit approved and invocation unsuccessful 
   Rate limit denied 
   The custom metric data is saved in CloudWatch in the GenAIRateLimiting namespace. This namespace includes the following key metrics: 
   
   TotalRequests ‚Äì Counts every invocation attempt regardless of outcome 
   RateLimitApproved ‚Äì Tracks requests that passed rate limiting checks 
   RateLimitDenied ‚Äì Tracks requests blocked by rate limiting 
   InvocationFailed ‚Äì Counts requests that failed during model invocation 
   InputTokens ‚Äì Measures input token consumption for successful requests 
   OutputTokens ‚Äì Measures output token consumption for successful requests 
   Each metric includes dimensions for Model, ModelId, CostCenter, Application, and Environment for data analysis. 
 We use CloudWatch metrics query capabilities with math expressions to analyze the data collected by the workflow. The data can be displayed in a variety of visual formats to get a granular view of requests by the dimensions provided, such as model or cost center. The following screenshot shows an example dashboard that displays invocation metrics where one model has reached its limit. 
 
 
Additional Amazon Bedrock analytics 
In addition to the custom metrics dashboard, CloudWatch provides automatic dashboards for monitoring Amazon Bedrock performance and usage. The Bedrock dashboard offers visibility into key performance metrics and operational insights, as shown in the following screenshot. 
 
Cost tagging and reporting 
Amazon Bedrock has introduced application inference profiles, a new capability that organizations can use to apply custom cost allocation tags to track and manage their on-demand foundation model (FM) usage. This feature addresses a previous limitation where tagging wasn‚Äôt possible for on-demand FMs, making it difficult to track costs across different business units and applications. You can now create custom inference profiles for base FMs and apply cost allocation tags like department, team, and application identifiers. These tags integrate with AWS cost management tools including AWS Cost Explorer, AWS Budgets, and AWS Cost Anomaly Detection, enabling detailed cost analysis and budget control. 
Application inference profiles 
To start, you must create application inference profiles for each type of usage you want to track. In this case, the solution defines custom tags for costCenter, environment, and applicationId. An inference profile will also be based on an existing Amazon Bedrock model profile, so you must combine the desired tags and model into the profile. At the time of writing, you must use the AWS Command Line Interface (AWS CLI) or AWS API to create one. See the following example code: 
 
 aws bedrock create-inference-profile \
&nbsp;&nbsp;--inference-profile-name "aws-docs-sales-prod" \
&nbsp;&nbsp;--model-source '{"copyFrom": &nbsp;"arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-3-haiku-20240307-v1:0"}' \
&nbsp;&nbsp;--tags '[
&nbsp;&nbsp; &nbsp;{"key": "applicationId", "value": "aws-documentation-helper"},
&nbsp;&nbsp; &nbsp;{"key": "costCenter", "value": "sales"},
&nbsp;&nbsp; &nbsp;{"key": "environment", "value": "production"}
&nbsp;&nbsp;]' 
 
This command creates a profile for the sales cost center and production environment using Anthropic‚Äôs Claude Haiku 3.5 model. The output from this command is an Amazon Resource Name (ARN) that you will use as the model ID. In this solution, the ValidateAndSetContext Lambda function was modified to allow for specifying the model by cost center (for example, sales). To see which profiles you created, use the following command: 
aws bedrock list-inference-profiles --type-equals APPLICATION 
After the profiles have been created and the validation has been updated to map cost centers to the profile ARNs, the workflow will start running inference requests with the aligned profile. For example, when the user submits a request, they will specify the model as sales, services, or support to align with the three cost centers defined. The following code is a similar map to the previous example: 
 
 MODEL_ID_MAPPING =&nbsp;{
&nbsp;&nbsp; &nbsp;"sales": "arn:aws:bedrock:&lt;region&gt;:&lt;account&gt;:application-inference-profile/&lt;unique id1&gt;",
&nbsp;&nbsp; &nbsp;"services": "arn:aws:bedrock:&lt;region&gt;:&lt;account&gt;:application-inference-profile/&lt;unique id2&gt;",
&nbsp;&nbsp; &nbsp;"support": "arn:aws:bedrock:&lt;region&gt;:&lt;account&gt;:application-inference-profile/&lt;unique id3&gt;"
&nbsp;&nbsp; } 
 
To query CloudWatch metrics for the model usage correctly when using application inference profiles, you must specify the unique ID for the profile (the last part of the ARN). CloudWatch will store metrics like token usage based on the unique ID. To support both profile and direct model usage, the Lambda function was modified to add a new tag for modelMetric to be the appropriate term to use to query for token usage. See the following code: 
 
 &nbsp;&nbsp;"tags": {
&nbsp;&nbsp; &nbsp;"requestId": "ded98994-eb76-48d9-9dbc-f269541b5e49",
&nbsp;&nbsp; &nbsp;"timestamp": "2025-01-31T14:05:26.854682",
&nbsp;&nbsp; &nbsp;"applicationId": "aws-documentation-helper",
&nbsp;&nbsp; &nbsp;"costCenter": "support",
&nbsp;&nbsp; &nbsp;"environment": "production",&nbsp;&nbsp; &nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;"modelMetric": "&lt;unique id&gt; | &lt;model id&gt;"
&nbsp; } 
 
Cost Explorer 
Cost Explorer is a powerful cost management tool that provides comprehensive visualization and analysis of your cloud spending across AWS services, including Amazon Bedrock. It offers intuitive dashboards to track historical costs, forecast future expenses, and gain insights into your cloud consumption. With Cost Explorer, you can break down expenses by service, tags, and custom dimensions, for detailed financial analysis. The tool updates on a daily basis. 
When you use application inference profiles with Amazon Bedrock, your AI service usage is automatically tagged and flows directly into Billing and Cost Management. These tags enable detailed cost tracking across different dimensions like cost center, application, and environment. This means you can generate reports that break down Amazon Bedrock AI expenses by specific business units, projects, or organizational hierarchies, providing clear visibility into your generative AI spending. 
Cost allocation tags 
Cost allocation tags are key-value pairs that help you categorize and track AWS resource costs across your organization. In the context of Amazon Bedrock, these tags can include attributes like application name, cost center, environment, or project ID. To activate a cost allocation tag, you must first enable it on the Billing and Cost Management console. After they‚Äôre activated, these tags will appear in your AWS Cost and Usage Report (CUR), helping you break down Amazon Bedrock expenses with granular detail. 
To activate a cost allocation tag, complete the following steps: 
 
 On the Billing and Cost Management console, in the navigation pane, choose Cost Allocation Tags. 
 Locate your tag (for this example, it‚Äôs named costCenter) and choose Activate. 
 Confirm the activation. 
 
After activation, the costCenter tag will appear in your CUR and will be used in Cost Explorer. It might take 24 hours for the tag to become fully active in your billing reports. 
 
Cost Explorer reporting 
To create an Amazon Bedrock usage report in Cost Explorer based on your tag, complete the following steps: 
 
 On the Billing and Cost Management console, choose Cost Explorer in the navigation pane. 
 Set your desired date range (relative time range or custom period). 
 Select Daily or Monthly granularity. 
 On the Group by dropdown menu, choose Tag. 
 Choose costCenter as the tag key. 
 Review the displayed Amazon Bedrock costs broken down by each unique cost center value. 
 Optionally, filter the values by applying a filter in the Filters section: 
   
   Choose Tag filter. 
   Choose the costCenter tag. 
   Choose specific cost center values you want to analyze. 
    
 
The resulting report will provide a detailed view of Amazon Bedrock AI service expenses, helping you compare spending across different organizational units or projects with precision. 
 
Summary 
The AWS Cost and Usage Reports (including budgets) act as trailing edge indicators because they show what you‚Äôve already spent on Amazon Bedrock after the fact. By blending real-time alerts from Step Functions with comprehensive cost reports, you can get a 360-degree view of your Amazon Bedrock usage. This reporting can alert you before you overspend and help you understand your actual consumption. This approach gives you the power to manage AI resources proactively, keeping your innovation budget on track and your projects running smoothly. 
Try out this cost management approach for your own use case, and share your feedback in the comments. 
 
About the Author 
Jason Salcido is a Startups Senior Solutions Architect with nearly 30 years of experience pioneering innovative solutions for organizations from startups to enterprises. His expertise spans cloud architecture, serverless computing, machine learning, generative AI, and distributed systems. Jason combines deep technical knowledge with a forward-thinking approach to design scalable solutions that drive value, while translating complex concepts into actionable strategies.
‚Ä¢ Build a proactive AI cost management system for Amazon Bedrock ‚Äì Part 1
  As organizations embrace generative AI powered by Amazon Bedrock, they face the challenge of managing costs associated with the token-based pricing model. Amazon Bedrock offers a pay-as-you-go pricing structure that can potentially lead to unexpected and excessive bills if usage is not carefully monitored. Traditional methods of cost monitoring, such as budget alerts and cost anomaly detection, can help spot unexpectedly high usage but are reactive in nature. To address costs proactively, it is vital to use both leading and trailing indicators. 
Leading indicators are predictive signals that help you anticipate future trends or potential issues before they fully materialize. These indicators provide proactive insights that allow for timely intervention. In contrast, trailing indicators are retrospective measurements that confirm what has already occurred. By understanding and tracking both types of indicators, organizations can develop more strategic and responsive decision-making processes. 
In this two-part series, we introduce a comprehensive solution for proactively managing Amazon Bedrock inference costs. Our approach features a cost sentry mechanism designed to establish and enforce token usage limits, providing organizations with a robust framework for controlling generative AI expenses. In this post, we focus on core architecture, cost sentry design, token usage tracking, and initial budget enforcement strategies. In Part 2, we explore advanced monitoring techniques, custom tagging, reporting, and long-term cost optimization best practices. The goal is to deliver a predictable, cost-effective approach to Amazon Bedrock deployments that aligns with organizational financial constraints. 
Solution overview 
Amazon Bedrock is billed on a token usage-based policy with charges based on the input and output tokens used. The rate charged depends on the model used and AWS Region where inference is performed. Developers must implement robust token management strategies in their applications to help prevent runaway costs, making sure generative AI applications include circuit breakers and consumption limits that align with budgetary constraints. 
To address this, you can configure Amazon CloudWatch alarms or monitor costs with billing alerts and budgets, but these mechanisms look at incurred costs or usage after the fact. Another option is the Generative AI Gateway Solution in the AWS Solutions Library, which uses LiteLLM to enforce budgetary limits for Amazon Bedrock and other model providers. 
This solution was developed to identify a proactive, centralized mechanism that could limit the generative AI usage to a specific budget that can be adjusted. This approach uses serverless workflows and native Amazon Bedrock integration that offers less operational complexity while providing large-scale performance and scaling. 
When building applications with Amazon Bedrock, it is common practice to access the service through a developed API, either synchronously through a REST API or asynchronously through a queuing system. The following diagram compares these architectures. 
 
For synchronous interactions, clients make direct REST API calls to Amazon Bedrock, passing in the necessary parameters. In an asynchronous architecture, clients submit inference requests to a queue or message broker, such as Amazon Simple Queue Service (Amazon SQS). A backend processing system, often implemented as a serverless function or a containerized application, continuously monitors the queue and processes incoming requests. This approach decouples the client from the inference processing, enabling scalability and resilience in handling bursts of requests. 
This solution is a centralized mechanism that can be used to interact with Amazon Bedrock to serve as a proactive cost sentry. It is designed using a serverless architecture that uses AWS Step Functions to orchestrate a workflow that validates token usage against configured limits before allowing Amazon Bedrock inference requests to proceed. This solution makes sure that generative AI applications stay within predefined budgetary boundaries, providing cost predictability and control. 
The following diagram illustrates the architecture we build in this post. 
 
The core components of this solution include: 
 
 Rate limiter workflow ‚Äì A Step Functions workflow that retrieves current token usage metrics from CloudWatch, compares them against predefined limits stored in Amazon DynamoDB, and determines whether to proceed with or deny the Amazon Bedrock inference request. 
 Amazon Bedrock model router ‚Äì A separate Step Functions state machine that acts as a centralized gateway for invoking various Amazon Bedrock models. This component abstracts the complexity of handling different I/O parameters required by each model. 
 Token usage tracking ‚Äì Uses CloudWatch metrics integration with Amazon Bedrock to retrieve current token usage data for input and output tokens across all or specific models. 
 Budget configuration ‚Äì Allows setting token usage limits on a per-model basis by storing the desired budget values in DynamoDB. A default limit can also be set to apply to models without specific budgets defined. 
 Cost and usage visibility ‚Äì Provides visibility for AI usage with CloudWatch dashboards and cost over time reporting in AWS Cost Explorer. 
 
The solution follows a serverless architecture approach, using managed AWS services like Step Functions, AWS Lambda, DynamoDB, and CloudWatch to provide a scalable, extensible, and cost-effective implementation. 
The goal is to provide a proactive method of setting generative AI usage limits that operate as a leading indicator to limit usage: 
 
 Proactive budgeting ‚Äì Enforces token usage limits before allowing inference requests, helping prevent accidental overspending 
 Model-specific budgets ‚Äì Supports setting individual budgets for different Amazon Bedrock models based on their pricing and usage patterns 
 Default budget fallback ‚Äì If no specific budget is defined for a model, a default limit can be applied to enable cost control 
 Monitoring ‚Äì Uses CloudWatch metrics integration to track token usage, enabling accurate budget enforcement 
 Serverless architecture ‚Äì Uses Step Functions, Lambda, DynamoDB, and CloudWatch for a scalable and cost-effective solution 
 Extensibility ‚Äì The modular design allows for seamless integration of additional Amazon Bedrock models or alternative inference methods 
 
Step Functions workflows 
In this section, we explore how the solution uses Step Functions to implement rate limiting and model routing workflows. 
Rate limiting workflow 
The rate limiting workflow is designed to take a minimal JSON document as input with the following format: 
 
 {
&nbsp; "modelId": "string", &nbsp; &nbsp; &nbsp;&nbsp;// e.g. "anthropic.claude-3-sonnet-20240229-v1:0"
&nbsp;&nbsp;"prompt": {
&nbsp;&nbsp; &nbsp;"messages": [
&nbsp;&nbsp; &nbsp; &nbsp;{
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"role": "string", &nbsp; &nbsp;// "system", "user", or "assistant"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"content": "string"
&nbsp;&nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp;]
&nbsp; }
} 
 
This workflow is the core component that enforces budgetary controls. The key steps are as follows: 
 
 A Lambda function retrieves the start and end dates for the current month, which is used to query token usage metrics for the appropriate time range. 
 The workflow queries CloudWatch to retrieve the current month‚Äôs token usage metrics for the specified Amazon Bedrock model. 
 The workflow retrieves the configured token usage limit for the specified Amazon Bedrock model from DynamoDB. If no specific limit is found, it falls back to retrieving the default limit. 
 The workflow compares the current token usage against the configured limit to determine if the budget has been exceeded or not. 
 If the token usage is within the budget, this step invokes the Amazon Bedrock model router state machine to perform the actual inference request. 
 Depending on the outcome of the budget check, the workflow returns either the formatted inference result or an error indicating that the budget has been exceeded. 
 
The following diagram illustrates the Step Functions workflow. 
 
Amazon Bedrock model router workflow 
The Amazon Bedrock model router workflow is a separate Step Functions state machine responsible for invoking the appropriate Amazon Bedrock model based on the request parameters. It abstracts the complexity of handling different I/O formats required by various Amazon Bedrock models and combines the result into a standardized format. 
The key steps in the workflow include: 
 
 Based on the provided model ID, the workflow determines the specific Amazon Bedrock model to be invoked. 
 The workflow calls the appropriate Amazon Bedrock model with the required input parameters. 
 The workflow normalizes the output from the Amazon Bedrock model to a consistent format for further processing or returning to the client. 
 The workflow returns the transformed inference result to the calling workflow (budget sentry workflow). 
 
The following diagram illustrates the Step Functions workflow. 
 
You can implement additional steps to handle error conditions and format the output appropriately. In this example, the Anthropic flow includes error processing. 
Token usage tracking with CloudWatch metrics 
The Amazon Bedrock cost sentry uses the CloudWatch integration with Amazon Bedrock to retrieve current token usage metrics. These metrics are used to enforce budgetary limits proactively. For example, see the following query: 
 
 {
&nbsp;&nbsp; &nbsp;"sparkline": false,
&nbsp;&nbsp; &nbsp;"metrics": [
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;[ { "expression": "SEARCH('{AWS/Bedrock} MetricName=\"InputTokenCount\"', 'Sum', 60)", "region": "us-east-1" } ],
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;[ { "expression": "SEARCH('{AWS/Bedrock} MetricName=\"OutputTokenCount\"', 'Sum', 60)", "region": "us-east-1" } ]
&nbsp;&nbsp; &nbsp;],
&nbsp;&nbsp; &nbsp;"legend": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"position": "right"
&nbsp;&nbsp; &nbsp;},
&nbsp;&nbsp; &nbsp;"title": "InputTokenCount, OutputTokenCount",
&nbsp;&nbsp; &nbsp;"region": "us-east-1",
&nbsp;&nbsp; &nbsp;"liveData": true,
&nbsp;&nbsp; &nbsp;"view": "gauge",
&nbsp;&nbsp; &nbsp;"stacked": false,
&nbsp;&nbsp; &nbsp;"period": 2592000,
&nbsp;&nbsp; &nbsp;"table": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"summaryColumns": [
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"SUM"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;]
&nbsp;&nbsp; &nbsp;},
&nbsp;&nbsp; &nbsp;"yAxis": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"left": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"min": 0,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"max": 1000000
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp;},
&nbsp;&nbsp; &nbsp;"setPeriodToTimeRange": true,
&nbsp;&nbsp; &nbsp;"trend": false,
&nbsp;&nbsp; &nbsp;"startTime": "2024-05-01T00:00:00Z",
&nbsp;&nbsp; &nbsp;"endTime": "2024-05-30T23:59:59Z"
} 
 
This CloudWatch metric query retrieves the total input and output token counts for a specified time range, allowing the rate limiter workflow to accurately enforce budgets based on real-time usage data. 
Budget configuration with DynamoDB 
The Amazon Bedrock cost sentry stores token usage limits in a DynamoDB table, providing seamless configuration and updates to individual model budgets or the default limit. For example, see the following code: 
 
 {
&nbsp;&nbsp; &nbsp;"modelId": "anthropic.claude-3-sonnet-20240229-v1:0",
&nbsp;&nbsp; &nbsp;"limit": {
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;"input": 1000000,
&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;"output": 3000000
&nbsp;&nbsp; &nbsp;}
} 
 
In this example, the token usage limit for the specified Amazon Bedrock model (anthropic.claude-3-sonnet-20240229-v1:0) is set to 1,000,000 input tokens and 3,000,000 output tokens. 
Administrators can quickly update these limits by modifying the corresponding DynamoDB records, providing flexibility in adjusting budgets as needed. 
Performance analysis of the rate limiter workflow 
To assess the performance impact of introducing the workflow, we used an array of inference requests. Test cases included various prompts designed to generate responses ranging from concise answers to detailed explanations over 500 words, effectively testing the workflow‚Äôs performance across different output token sizes. The workflow demonstrated exceptional performance characteristics across 501 successful executions, handling a diverse set of inference requests from brief responses to extensive content generation. 
The workflow maintains consistent execution patterns while processing requests ranging from 6.76 seconds to 32.24 seconds in total duration, with the variation primarily reflecting the different output token requirements of each request: 
 
 Quick responses (under 10 seconds) ‚Äì Typically handling concise answers and simple queries 
 Medium-length content (11‚Äì22 seconds) ‚Äì Common for detailed explanations and multi-paragraph responses 
 Extended generation (up to 32 seconds) ‚Äì Handling comprehensive responses requiring more than 500 words 
 
The following diagram illustrates our time distribution findings. 
 
The time distribution analysis reveals highly optimized resource utilization: 
 
 Amazon Bedrock model router ‚Äì 5.80‚Äì31.99 seconds (98.26% of runtime) 
 Other workflow steps ‚Äì 0.11‚Äì4.74 seconds (1.65% of runtime) 
 System overhead ‚Äì 0.02 seconds average (0.09% of runtime) 
 
This performance profile aligns with best practices for workflow orchestration, where minimizing overhead and maintaining consistent execution patterns are crucial for reliability. The workflow‚Äôs efficiency is evidenced by its remarkably low system overhead of just 0.09%, demonstrating effective use of the built-in controls and state management capabilities of Step Functions regardless of the response size being generated. 
The execution consistency is particularly noteworthy, with a predictable event pattern of 47‚Äì49 events per execution, regardless of the inference request complexity or output size. This predictability is essential for workload management and resource planning, especially when handling varied request complexities and token outputs. 
These metrics indicate a well-architected workflow that effectively uses Step Functions Express workflow capabilities for high-volume event processing while maintaining minimal overhead and consistent performance characteristics across both simple queries and complex, token-intensive inference requests. 
Cost analysis 
To analyze the cost implications, estimates were generated using the AWS Pricing Calculator for both Standard and Express Step Functions workflows, assuming 100,000 requests per month. The following table summarizes these estimates. 
 
  
   
   Detailed Estimate 
    
    
    
    
    
    
    
   
   
   Region 
   Description 
   Service 
   Upfront 
   Monthly 
   First 12 Months Total 
   Currency 
   Configuration Summary 
   
   
   US East (Ohio) 
   Step Functions Standard 
   Step Functions ‚Äì Standard Workflows 
   0 
   $37.40 
   $448.80 
   USD 
   Workflow requests (100,000 per month)State transitions per workflow (15) 
   
   
   US East (Ohio) 
   Step Functions Express 
   Step Functions ‚Äì Express Workflows 
   0 
   $3.75 
   $45 
   USD 
   Duration of each workflow (35,000)Memory consumed by each workflow (64 MB)Workflow requests (100,000 per month) 
   
  
 
The cost analysis revealed that the Step Functions Express workflow offers a more cost-effective solution compared to the Standard workflow, with potential cost savings of up to 90% for the same workload. There is a potential for cost reduction for Standard if the number of steps can be optimized. For example, a few formatting pass steps could potentially be removed, but these steps help format the downstream input to later steps. 
Consult the AWS Pricing Calculator for more details on pricing and to run your own scenario. 
Conclusion 
In this solution, we used Step Functions to build a system that serves as a leading indicator because it tracks rate limiting and token usage, warning us immediately when we‚Äôre approaching our usage limits. In Part 2, we discuss combining this with trailing indicators to stay aware of usage and costs. 
 
About the author 
Jason Salcido is a Startups Senior Solutions Architect with nearly 30 years of experience pioneering innovative solutions for organizations from startups to enterprises. His expertise spans cloud architecture, serverless computing, machine learning, generative AI, and distributed systems. Jason combines deep technical knowledge with a forward-thinking approach to design scalable solutions that drive value, while translating complex concepts into actionable strategies.
‚Ä¢ Streamline code migration using Amazon Nova Premier with an agentic workflow
  Many enterprises are burdened with mission-critical systems built on outdated technologies that have become increasingly difficult to maintain and extend. 
This post demonstrates how you can use the Amazon Bedrock Converse API with Amazon Nova Premier within an agentic workflow to systematically migrate legacy C code to modern Java/Spring framework applications. By breaking down the migration process into specialized agent roles and implementing robust feedback loops, organizations can accomplish the following: 
 
 Reduce migration time and cost ‚Äì Automation handles repetitive conversion tasks while human engineers focus on high-value work. 
 Improve code quality ‚Äì Specialized validation agents make sure the migrated code follows modern best practices. 
 Minimize risk ‚Äì The systematic approach prevents critical business logic loss during migration. 
 Enable cloud integration ‚Äì The resulting Java/Spring code can seamlessly integrate with AWS services. 
 
Challenges 
Code migration from legacy systems to modern frameworks presents several significant challenges that require a balanced approach combining AI capabilities with human expertise: 
 
 Language paradigm differences ‚Äì Converting C code to Java involves navigating fundamental differences in memory management, error handling, and programming paradigms. C‚Äôs procedural nature and direct memory manipulation contrast sharply with Java‚Äôs object-oriented approach and automatic memory management. Although AI can handle many syntactic transformations automatically, developers must review and validate the semantic correctness of these conversions. 
 Architectural complexity ‚Äì Legacy systems often feature complex interdependencies between components that require human analysis and planning. In our case, the C code base contained intricate relationships between modules, with some TPs (Transaction Programs) connected to as many as 12 other modules. Human developers must create dependency mappings and determine migration order, typically starting from leaf nodes with minimal dependencies. AI can assist in identifying these relationships, but the strategic decisions about migration sequencing require human judgment. 
 Maintaining business logic ‚Äì Making sure critical business logic is accurately preserved during translation requires continuous human oversight. Our analysis showed that although automatic migration is highly successful for simple, well-structured code, complex business logic embedded in larger files (over 700 lines) requires careful human review and often manual refinement to prevent errors or omissions. 
 Inconsistent naming and structures ‚Äì Legacy code often contains inconsistent naming conventions and structures that must be standardized during migration. AI can handle many routine transformations‚Äîconverting alphanumeric IDs in function names, transforming C-style error codes to Java exceptions, and converting C structs into Java classes‚Äîbut human developers must establish naming standards and review edge cases where automated conversion may be ambiguous. 
 Integration complexity ‚Äì After converting individual files, human-guided integration is essential for creating a cohesive application. Variable names that were consistent across the original C files often become inconsistent during individual file conversion, requiring developers to perform reconciliation work and facilitate proper inter-module communication. 
 Quality assurance ‚Äì Validating that converted code maintains functional equivalence with the original requires a combination of automated testing and human verification. This is particularly critical for complex business logic, where subtle differences can lead to significant issues. Developers must design comprehensive test suites and perform thorough code reviews to ensure migration accuracy. 
 
These challenges necessitate a systematic approach that combines the pattern recognition capabilities of large language models (LLMs) with structured workflows and essential human oversight to produce successful migration outcomes. The key is using AI to handle routine transformations while keeping humans in the loop for strategic decisions, complex logic validation, and quality assurance. 
Solution overview 
The solution employs the Amazon Bedrock Converse API with Amazon Nova Premier to convert legacy C code to modern Java/Spring framework code through a systematic agentic workflow. This approach breaks down the complex migration process into manageable steps, allowing for iterative refinement and handling of token limitations. The solution architecture consists of several key components: 
 
 Code analysis agent ‚Äì Analyzes C code structure and dependencies 
 Conversion agent ‚Äì Transforms C code to Java/Spring code 
 Security assessment agent ‚Äì Identifies vulnerabilities in legacy and migrated code 
 Validation agent ‚Äì Verifies conversion completeness and accuracy 
 Refine agent ‚Äì Rewrites the code based on the feedback from the validation agent 
 Integration agent ‚Äì Combines individually converted files 
 
Our agentic workflow is implemented using a Strands Agents framework combined with the Amazon Bedrock Converse API for robust agent orchestration and LLM inference. The architecture (as shown in the following diagram) uses a hybrid approach that combines Strands‚Äôs session management capabilities with custom BedrockInference handling for token continuation. 
 
The solution uses the following core technologies: 
 
 Strands Agents framework (v1.1.0+) ‚Äì Provides agent lifecycle management, session handling, and structured agent communication 
 Amazon Bedrock Converse API ‚Äì Powers the LLM inference with Amazon Nova Premier model 
 Custom BedrockInference class ‚Äì Handles token limitations through text prefilling and response continuation 
 Asyncio-based orchestration ‚Äì Enables concurrent processing and non-blocking agent execution 
 
The workflow consists of the following steps: 
1. Code analysis: 
 
 Code analysis agent ‚Äì Performs input code analysis to understand the conversion requirements. Examines C code base structure, identifies dependencies, and assesses complexity. 
 Framework integration ‚Äì Uses Strands for session management while using BedrockInference for analysis. 
 Output ‚Äì JSON-structured analysis with dependency mapping and conversion recommendations. 
 
2. File categorization and metadata creation: 
 
 Implementation ‚Äì FileMetadata data class with complexity assessment. 
 Categories ‚Äì Simple (0‚Äì300 lines), Medium (300‚Äì700 lines), Complex (over 700 lines). 
 File types ‚Äì Standard C files, header files, and database I/O (DBIO) files. 
 
3. Individual file conversion: 
 
 Conversion agent ‚Äì Performs code migration on individual files based on the information from the code analysis agent. 
 Token handling ‚Äì Uses the stitch_output() method for handling large files that exceed token limits. 
 
4. Security assessment phase: 
 
 Security assessment agent ‚Äì Performs comprehensive vulnerability analysis on both legacy C code and converted Java code. 
 Risk categorization ‚Äì Classifies security issues by severity (Critical, High, Medium, Low). 
 Mitigation recommendations ‚Äì Provides specific code fixes and security best practices. 
 Output ‚Äì Detailed security report with actionable remediation steps. 
 
5. Validation and feedback loop: 
 
 Validation agent ‚Äì Analyzes conversion completeness and accuracy. 
 Refine agent ‚Äì Applies iterative improvements based on validation results. 
 Iteration control ‚Äì Maximum five feedback iterations with early termination on satisfactory results. 
 Session persistence ‚Äì Strands framework maintains conversation context across iterations. 
 
6. Integration and finalization: 
 
 Integration agent ‚Äì Attempts to combine individually converted files. 
 Consistency resolution ‚Äì Standardizes variable naming and provides proper dependencies. 
 Output generation ‚Äì Creates cohesive Java/Spring application structure. 
 
7. DBIO conversion (specialized) 
 
 Purpose ‚Äì Converts SQL DBIO C source code to MyBatis XML mapper files. 
 Framework ‚Äì Uses the same Strands and BedrockInference hybrid approach for consistency. 
 
The solution consists of the following key orchestration features: 
 
 Session persistence ‚Äì Each conversion maintains session state across agent interactions. 
 Error recovery ‚Äì Comprehensive error handling with graceful degradation. 
 Performance tracking ‚Äì Built-in metrics for processing time, iteration counts, and success rates. 
 Token continuation ‚Äì Seamless handling of large files through response stitching. 
 
This framework-specific implementation facilitates reliable, scalable code conversion while maintaining the flexibility to handle diverse C code base structures and complexities. 
Prerequisites 
Before implementing this code conversion solution, make sure you have the following components configured: 
 
 AWS environment: 
   
   AWS account with appropriate permissions for Amazon Bedrock with Amazon Nova Premier model access 
   Amazon Elastic Compute Cloud (Amazon EC2) instance (t3.medium or larger) for development and testing or development environment in local machine 
    
 Development setup: 
   
   Python 3.10+ installed with Boto3 SDK and Strands Agents 
   AWS Command Line Interface (AWS CLI) configured with appropriate credentials and AWS Region 
   Git for version control of legacy code base and converted code 
   Text editor or integrated development environment (IDE) capable of handling both C and Java code bases 
    
 Source and target code base requirements: 
   
   C source code organized in a structured directory format 
   Java 11+ and Maven/Gradle build tools 
   Spring Framework 5.x or Spring Boot 2.x+ dependencies 
    
 
The source code and prompts used in the post can be found in the GitHub repo. 
Agent-based conversion process 
The solution uses a sophisticated multi-agent system implemented using the Strands framework, where each agent specializes in a specific aspect of the code conversion process. This distributed approach provides thorough analysis, accurate conversion, and comprehensive validation while maintaining the flexibility to handle diverse code structures and complexities. 
Strands framework integration 
Each agent extends the BaseStrandsConversionAgent class, which provides a hybrid architecture combining Strands session management with custom BedrockInference capabilities: 
 
 class BaseStrandsConversionAgent(ABC):
&nbsp;&nbsp; &nbsp;def (self, name: str, bedrock_inference, system_prompt: str):
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;self.name = name
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;self.bedrock = bedrock_inference &nbsp;# Custom BedrockInference for token handling
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;self.system_prompt = system_prompt
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;# Create strands agent for session management
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;self.strands_agent = Agent(name=name, system_prompt=system_prompt)
&nbsp;&nbsp; &nbsp;
&nbsp;&nbsp; &nbsp;async def execute_async(self, context: ConversionContext) -&gt; Dict[str, Any]:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;# Implemented by each specialized agent
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;pass 
 
Code analysis agent 
The code analysis agent examines the structure of the C code base, identifying dependencies between files and determining the optimal conversion strategy. This agent helps prioritize which files to convert first and identifies potential challenges. The following is the prompt template for the code analysis agent: 
 
 You are a Code Analysis Agent with expertise in legacy C codebases and modern Java/Spring architecture.

&lt;c_codebase&gt;
{c_code}
&lt;/c_codebase&gt;

## TASK
Your task is to analyze the provided C code to prepare for migration.

Perform a comprehensive analysis and provide the following:
## INSTRUCTIONS

1. DEPENDENCY ANALYSIS:
&nbsp;&nbsp; - Identify all file dependencies (which files include or reference others)
&nbsp;&nbsp; - Map function calls between files
&nbsp;&nbsp; - Detect shared data structures and global variables

2. COMPLEXITY ASSESSMENT:
&nbsp;&nbsp; - Categorize each file as Simple (0-300 lines), Medium (300-700 lines), or Complex (700+ lines)
&nbsp;&nbsp; - Identify files with complex control flow, pointer manipulation, or memory management
&nbsp;&nbsp; - Flag any platform-specific or hardware-dependent code

3. CONVERSION PLANNING:
&nbsp;&nbsp; - Recommend a conversion sequence (which files to convert first)
&nbsp;&nbsp; - Suggest logical splitting points for large files
&nbsp;&nbsp; - Identify common patterns that can be standardized during conversion

4. RISK ASSESSMENT:
&nbsp;&nbsp; - Highlight potential conversion challenges (e.g., pointer arithmetic, bitwise operations)
&nbsp;&nbsp; - Identify business-critical sections requiring special attention
&nbsp;&nbsp; - Note any undocumented assumptions or behaviors

5. ARCHITECTURE RECOMMENDATIONS:
&nbsp;&nbsp; - Suggest appropriate Java/Spring components for each C module
&nbsp;&nbsp; - Recommend DTO structure and service organization
&nbsp;&nbsp; - Propose database access strategy using a persistence framework

Format your response as a structured JSON document with these sections. 
 
Conversion agent 
The conversion agent handles the actual transformation of C code to Java/Spring code. This agent is assigned the role of a senior software developer with expertise in both C and Java/Spring frameworks. The prompt template for the conversion agent is as follows: 
 
 You are a Senior Software Developer with 15+ years of experience in both C and Java Spring framework. 

&lt;c_file&gt;
{c_code}
&lt;/c_file&gt;

## TASK
Your task is to convert legacy C code to modern Java Spring code with precision and completeness.

## CONVERSION GUIDELINES:

1. CODE STRUCTURE:
&nbsp;&nbsp; - Create appropriate Java classes (Service, DTO, Mapper interfaces)
&nbsp;&nbsp; - Preserve original function and variable names unless they conflict with Java conventions
&nbsp;&nbsp; - Use Spring annotations appropriately (@Service, @Repository, etc.)
&nbsp;&nbsp; - Implement proper package structure based on functionality

2. JAVA BEST PRACTICES:
&nbsp;&nbsp; - Use Lombok annotations (@Data, @Slf4j, @RequiredArgsConstructor) to reduce boilerplate
&nbsp;&nbsp; - Implement proper exception handling instead of error codes
&nbsp;&nbsp; - Replace pointer operations with appropriate Java constructs
&nbsp;&nbsp; - Convert C-style arrays to Java collections where appropriate

3. SPRING FRAMEWORK INTEGRATION:
&nbsp;&nbsp; - Use dependency injection instead of global variables
&nbsp;&nbsp; - Implement a persistence framework mappers for database operations
&nbsp;&nbsp; - Replace direct SQL calls with mapper interfaces
&nbsp;&nbsp; - Use Spring's transaction management

4. SPECIFIC TRANSFORMATIONS:
&nbsp;&nbsp; - Replace PFM_TRY/PFM_CATCH with Java try-catch blocks
&nbsp;&nbsp; - Convert mpfmdbio calls to a persistence framework mapper method calls
&nbsp;&nbsp; - Replace mpfm_dlcall with appropriate Service bean injections
&nbsp;&nbsp; - Convert NGMHEADER references to input.getHeaderVo() calls
&nbsp;&nbsp; - Replace PRINT_ and PFM_DBG macros with SLF4J logging
&nbsp;&nbsp; - Convert ngmf_ methods to CommonAPI.ngmf method calls

5. DATA HANDLING:
&nbsp;&nbsp; - Create separate DTO classes for input and output structures
&nbsp;&nbsp; - Use proper Java data types (String instead of char arrays, etc.)
&nbsp;&nbsp; - Implement proper null handling and validation
&nbsp;&nbsp; - Remove manual memory management code

## OUTPUT FORMAT:
&nbsp;&nbsp; - Include filename at the top of each Java file: #filename: [filename].java
&nbsp;&nbsp; - Place executable Java code inside &lt;java&gt;&lt;/java&gt; tags
&nbsp;&nbsp; - Organize multiple output files clearly with proper headers

Generate complete, production-ready Java code that fully implements all functionality from the original C code. 
 
Security assessment agent 
The security assessment agent performs comprehensive vulnerability analysis on the original C code and the converted Java code, identifying potential security risks and providing specific mitigation strategies. This agent is crucial for making sure security vulnerabilities are not carried forward during migration and new code follows security best practices. The following is the prompt template for the security assessment agent: 
 
 You are a Security Assessment Agent with expertise in identifying vulnerabilities in both C and Java codebases, specializing in secure code migration practices.

ORIGINAL C CODE:
&lt;c_code&gt;
{c_code}
&lt;/c_code&gt;

CONVERTED JAVA CODE:
&lt;java_code&gt;
{java_code}
&lt;/java_code&gt;

## TASK
Your task is to perform comprehensive security analysis on both the legacy C code and converted Java code, identifying vulnerabilities and providing specific mitigation recommendations.

## SECURITY ANALYSIS FRAMEWORK

1. **LEGACY C CODE VULNERABILITIES:**
&nbsp;&nbsp; - Buffer overflow risks (strcpy, strcat, sprintf usage)
&nbsp;&nbsp; - Memory management issues (dangling pointers, memory leaks)
&nbsp;&nbsp; - Integer overflow/underflow vulnerabilities
&nbsp;&nbsp; - Format string vulnerabilities
&nbsp;&nbsp; - Race conditions in multi-threaded code
&nbsp;&nbsp; - Improper input validation and sanitization
&nbsp;&nbsp; - SQL injection risks in database operations
&nbsp;&nbsp; - Insecure cryptographic implementations

2. **JAVA CODE SECURITY ASSESSMENT:**
&nbsp;&nbsp; - Input validation and sanitization gaps
&nbsp;&nbsp; - SQL injection vulnerabilities in persistence framework queries
&nbsp;&nbsp; - Improper exception handling that leaks sensitive information
&nbsp;&nbsp; - Authentication and authorization bypass risks
&nbsp;&nbsp; - Insecure deserialization vulnerabilities
&nbsp;&nbsp; - Cross-site scripting (XSS) prevention in web endpoints
&nbsp;&nbsp; - Logging of sensitive data
&nbsp;&nbsp; - Dependency vulnerabilities in Spring framework usage

3. **MIGRATION-SPECIFIC RISKS:**
&nbsp;&nbsp; - Security assumptions that don't translate between languages
&nbsp;&nbsp; - Privilege escalation through improper Spring Security configuration
&nbsp;&nbsp; - Data exposure through overly permissive REST endpoints
&nbsp;&nbsp; - Session management vulnerabilities
&nbsp;&nbsp; - Configuration security (hardcoded credentials, insecure defaults)

4. **COMPLIANCE AND BEST PRACTICES:**
&nbsp;&nbsp; - OWASP Top 10 compliance assessment
&nbsp;&nbsp; - Spring Security best practices implementation
&nbsp;&nbsp; - Secure coding standards adherence
&nbsp;&nbsp; - Data protection and privacy considerations

## OUTPUT FORMAT
Provide your analysis as a structured JSON with these fields:
- "critical_vulnerabilities": array of critical security issues requiring immediate attention
- "security_risk_issues": array of security concerns
- "secure_code_recommendations": specific code changes to implement security fixes
- "spring_security_configurations": recommended Spring Security configurations
- "compliance_gaps": areas where code doesn't meet security standards
- "migration_security_notes": security considerations specific to the C-to-Java migration

For each vulnerability, include:
- Description of the security risk
- Potential impact and attack vectors
- Specific line numbers or code sections affected
- Detailed remediation steps with code examples
- Priority level and recommended timeline for fixes

Be thorough in identifying both obvious and subtle security issues that could be exploited in production environments. 
 
Validation agent 
The validation agent reviews the converted code to identify missing or incorrectly converted components. This agent provides detailed feedback that is used in subsequent conversion iterations. The prompt template for the validation agent is as follows: 
 
 You are a Code Validation Agent specializing in verifying C to Java/Spring migrations.

ORIGINAL C CODE:
&lt;c_code&gt;
{c_code}
&lt;/c_code&gt;

CONVERTED JAVA CODE:
&lt;java_code&gt;
{java_code}
&lt;/java_code&gt;

## TASK
Your task is to thoroughly analyze the conversion quality and identify any issues or omissions.
Perform a comprehensive validation focusing on these aspects:

## INSTRUCTIONS
1. COMPLETENESS CHECK:
&nbsp;&nbsp; - Verify all functions from C code are implemented in Java
&nbsp;&nbsp; - Confirm all variables and data structures are properly converted
&nbsp;&nbsp; - Check that all logical branches and conditions are preserved
&nbsp;&nbsp; - Ensure all error handling paths are implemented

2. CORRECTNESS ASSESSMENT:
&nbsp;&nbsp; - Identify any logical errors in the conversion
&nbsp;&nbsp; - Verify proper transformation of C-specific constructs (pointers, structs, etc.)
&nbsp;&nbsp; - Check for correct implementation of memory management patterns
&nbsp;&nbsp; - Validate proper handling of string operations and byte manipulation

3. SPRING FRAMEWORK COMPLIANCE:
&nbsp;&nbsp; - Verify appropriate use of Spring annotations and patterns
&nbsp;&nbsp; - Check proper implementation of dependency injection
&nbsp;&nbsp; - Validate correct use of persistence framework mappers
&nbsp;&nbsp; - Ensure proper service structure and organization

4. CODE QUALITY EVALUATION:
&nbsp;&nbsp; - Assess Java code quality and adherence to best practices
&nbsp;&nbsp; - Check for proper exception handling
&nbsp;&nbsp; - Verify appropriate logging implementation
&nbsp;&nbsp; - Evaluate overall code organization and readability

## OUTPUT FORMAT
Provide your analysis as a structured JSON with these fields:
- "complete": boolean indicating if conversion is complete
- "missing_elements": array of specific functions, variables, or logic blocks that are missing
- "incorrect_transformations": array of elements that were incorrectly transformed
- "spring_framework_issues": array of Spring-specific implementation issues
- "quality_concerns": array of code quality issues
- "recommendations": specific, actionable recommendations for improvement

Be thorough and precise in your analysis, as your feedback will directly inform the next iteration of the conversion process. 
 
Feedback loop implementation with refine agent 
The feedback loop is a critical component that enables iterative refinement of the converted code. This process involves the following steps: 
 
 Initial conversion by the conversion agent. 
 Security assessment by the security assessment agent. 
 Validation by the validation agent. 
 Feedback incorporation by the refine agent (incorporating both validation and security feedback). 
 Repeat until satisfactory results are achieved. 
 
The refine agent incorporates security vulnerability fixes alongside functional improvements, and security assessment results are provided to development teams for final review and approval before production deployment. The following code is the prompt template for code refinement: 
 
 You are a Senior Software Developer specializing in C to Java/Spring migration with expertise in secure coding practices.
ORIGINAL C CODE:
&lt;c_code&gt;
{c_code}
&lt;/c_code&gt;

YOUR PREVIOUS JAVA CONVERSION:
&lt;previous_java&gt;
{previous_java_code}
&lt;/previous_java&gt;

VALIDATION FEEDBACK:
&lt;validation_feedback&gt;
{validation_feedback}
&lt;/validation_feedback&gt;

SECURITY ASSESSMENT:
&lt;security_feedback&gt;
{security_feedback}
&lt;/security_feedback&gt;

## TASK
You've previously converted C code to Java, but validation and security assessment have identified issues that need to be addressed. Your task is to improve the conversion by addressing all identified functional and security issues while maintaining complete functionality.
## INSTRUCTIONS
1. ADDRESSING MISSING ELEMENTS:
&nbsp;&nbsp; - Implement any functions, variables, or logic blocks identified as missing
&nbsp;&nbsp; - Ensure all control flow paths from the original code are preserved
&nbsp;&nbsp; - Add any missing error handling or edge cases
2. CORRECTING TRANSFORMATIONS:
&nbsp;&nbsp; - Fix any incorrectly transformed code constructs
&nbsp;&nbsp; - Correct any logical errors in the conversion
&nbsp;&nbsp; - Properly implement C-specific patterns in Java
3. IMPLEMENTING SECURITY FIXES:
&nbsp;&nbsp; - Address all critical and high-risk security vulnerabilities identified
&nbsp;&nbsp; - Implement secure coding practices (input validation, parameterized queries, etc.)
&nbsp;&nbsp; - Replace insecure patterns with secure Java/Spring alternatives
&nbsp;&nbsp; - Add proper exception handling that does not leak sensitive information
4. IMPROVING SPRING IMPLEMENTATION:
&nbsp;&nbsp; - Correct any issues with Spring annotations or patterns
&nbsp;&nbsp; - Ensure proper dependency injection and service structure
&nbsp;&nbsp; - Fix persistence framework mapper implementations if needed
&nbsp;&nbsp; - Implement Spring Security configurations as recommended
5. MAINTAINING CONSISTENCY:
&nbsp;&nbsp; - Ensure naming conventions are consistent throughout the code
&nbsp;&nbsp; - Maintain consistent patterns for similar operations
&nbsp;&nbsp; - Preserve the structure of the original code where appropriate
## OUTPUT FORMAT
Output the improved Java code inside &lt;java&gt;&lt;/java&gt; tags, with appropriate file headers. Ensure all security vulnerabilities are addressed while maintaining complete functionality from the original C code. 
 
Integration agent 
The integration agent combines individually converted Java files into a cohesive application, resolving inconsistencies in variable naming and providing proper dependencies. The prompt template for the integration agent is as follows: 
 
 You are an Integration Agent specializing in combining individually converted Java files into a cohesive Spring application. 

CONVERTED JAVA FILES:
&lt;converted_files&gt;
{converted_java_files}
&lt;/converted_files&gt;

ORIGINAL FILE RELATIONSHIPS:
&lt;relationships&gt;
{file_relationships}
&lt;/relationships&gt;

## TASK
Your task is to integrate multiple Java files that were converted from C, ensuring they work together properly.

Perform the following integration tasks:
## INSTRUCTIONS
1. DEPENDENCY RESOLUTION:
&nbsp;&nbsp; - Identify and resolve dependencies between services and components
&nbsp;&nbsp; - Ensure proper autowiring and dependency injection
&nbsp;&nbsp; - Verify that service method signatures match their usage across files

2. NAMING CONSISTENCY:
&nbsp;&nbsp; - Standardize variable and method names that should be consistent across files
&nbsp;&nbsp; - Resolve any naming conflicts or inconsistencies
&nbsp;&nbsp; - Ensure DTO field names match across related classes

3. PACKAGE ORGANIZATION:
&nbsp;&nbsp; - Organize classes into appropriate package structure
&nbsp;&nbsp; - Group related functionality together
&nbsp;&nbsp; - Ensure proper import statements across all files

4. SERVICE COMPOSITION:
&nbsp;&nbsp; - Implement proper service composition patterns
&nbsp;&nbsp; - Ensure services interact correctly with each other
&nbsp;&nbsp; - Verify that data flows correctly between components

5. COMMON COMPONENTS:
&nbsp;&nbsp; - Extract and standardize common utility functions
&nbsp;&nbsp; - Ensure consistent error handling across services
&nbsp;&nbsp; - Standardize logging patterns

6. CONFIGURATION:
&nbsp;&nbsp; - Create necessary Spring configuration classes
&nbsp;&nbsp; - Set up appropriate bean definitions
&nbsp;&nbsp; - Configure any required properties or settings

Output the integrated Java code as a set of properly organized files, each with:
- Appropriate package declarations
- Correct import statements
- Proper Spring annotations
- Clear file headers (#filename: [filename].java)

Place each file's code inside &lt;java&gt;&lt;/java&gt; tags. Ensure the integrated application maintains all functionality from the individual components while providing a cohesive structure. 
 
DBIO conversion agent 
This specialized agent handles the conversion of SQL DBIO C source code to XML files compatible with persistence framework in the Java Spring framework. The following is the prompt template for the DBIO conversion agent: 
 
 You are a Database Integration Specialist with expertise in converting C-based SQL DBIO code to persistence framework XML mappings for Spring applications. 

SQL DBIO C SOURCE CODE:
&lt;sql_dbio&gt;
{sql_dbio_code}
&lt;/sql_dbio&gt;

## TASK
Your task is to transform the provided SQL DBIO C code into properly structured persistence framework XML files.

Perform the conversion following these guidelines:

## INSTRUCTIONS
1. XML STRUCTURE:
&nbsp;&nbsp; - Create a properly formatted persistence framework mapper XML file
&nbsp;&nbsp; - Include appropriate namespace matching the Java mapper interface
&nbsp;&nbsp; - Set correct resultType or resultMap attributes for queries
&nbsp;&nbsp; - Use proper persistence framework XML structure and syntax

2. SQL TRANSFORMATION:
&nbsp;&nbsp; - Preserve the exact SQL logic from the original code
&nbsp;&nbsp; - Convert any C-specific SQL parameter handling to persistence framework parameter markers
&nbsp;&nbsp; - Maintain all WHERE clauses, JOIN conditions, and other SQL logic
&nbsp;&nbsp; - Preserve any comments explaining SQL functionality

3. PARAMETER HANDLING:
&nbsp;&nbsp; - Convert C variable bindings to persistence framework parameter references (#{param})
&nbsp;&nbsp; - Handle complex parameters using appropriate persistence framework techniques
&nbsp;&nbsp; - Ensure parameter types match Java equivalents (String instead of char[], etc.)

4. RESULT MAPPING:
&nbsp;&nbsp; - Create appropriate resultMap elements for complex result structures
&nbsp;&nbsp; - Map column names to Java DTO property names
&nbsp;&nbsp; - Handle any type conversions needed between database and Java types

5. DYNAMIC SQL:
&nbsp;&nbsp; - Convert any conditional SQL generation to persistence framework dynamic SQL elements
&nbsp;&nbsp; - Use &lt;if&gt;, &lt;choose&gt;, &lt;where&gt;, and other dynamic elements as appropriate
&nbsp;&nbsp; - Maintain the same conditional logic as the original code

6. ORGANIZATION:
&nbsp;&nbsp; - Group related queries together
&nbsp;&nbsp; - Include clear comments explaining the purpose of each query
&nbsp;&nbsp; - Follow persistence framework best practices for mapper organization

## OUTPUT FORMAT
Output the converted persistence framework XML inside &lt;xml&gt;&lt;/xml&gt; tags. Include a filename comment at the top: #filename: [EntityName]Mapper.xml

Ensure the XML is well-formed, properly indented, and follows persistence framework conventions for Spring applications. 
 
Handling token limitations 
To address token limitations in the Amazon Bedrock Converse API, we implemented a text prefilling technique that allows the model to continue generating code where it left off. This approach is particularly crucial for large files that exceed the model‚Äôs context window and represents a key technical innovation in our Strands-based implementation. 
Technical implementation 
The following code implements the BedrockInference class with continuation support: 
 
 class BedrockInference:
&nbsp;&nbsp; &nbsp;def __init__(self, region_name: str = "us-east-1", model_id: str = "us.amazon.nova-premier-v1:0"):
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;self.config = Config(read_timeout=300)
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;self.client = boto3.client("bedrock-runtime", config=self.config, region_name=region_name)
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;self.model_id = model_id
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;self.continue_prompt = {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"role": "user",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"content": [{"text": "Continue the code conversion from where you left off."}]
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp;
&nbsp;&nbsp; &nbsp;def run_converse_inference_with_continuation(self, prompt: str, system_prompt: str) -&gt; List[str]:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"""Run inference with continuation handling for large outputs"""
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;ans_list = []
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;messages = [{"role": "user", "content": [{"text": prompt}]}]
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;response, stop = self.generate_conversation([{'text': system_prompt}], messages)
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;ans = response['output']['message']['content'][0]['text']
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;ans_list.append(ans)
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;while stop == "max_tokens":
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;logger.info("Response truncated, continuing generation...")
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;messages.append(response['output']['message'])
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;messages.append(self.continue_prompt)
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# Extract last few lines for continuation context
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;sec_last_line = '\n'.join(ans.rsplit('\n', 3)[1:-1]).strip()
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;messages.append({"role": "assistant", "content": [{"text": sec_last_line}]})
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;response, stop = self.generate_conversation([{'text': system_prompt}], messages)
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;ans = response['output']['message']['content'][0]['text']
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;del messages[-1] &nbsp;# Remove the prefill message
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;ans_list.append(ans)
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;return ans_list 
 
Continuation strategy details 
The continuation strategy consists of the following steps: 
 
 Response monitoring: 
   
   The system monitors the stopReason field in Amazon Bedrock responses. 
   When stopReason equals max_tokens, continuation is triggered automatically. This makes sure no code generation is lost due to token limitations. 
    
 Context preservation: 
   
   The system extracts the last few lines of generated code as continuation context. 
   It uses text prefilling to maintain code structure and formatting. It preserves variable names, function signatures, and code patterns across continuations. 
    
 Response stitching: 
 
 
 def stitch_output(self, prompt: str, system_prompt: str, tag: str = "java") -&gt; str:
&nbsp;&nbsp; &nbsp;"""Stitch together multiple responses and extract content within specified tags"""
&nbsp;&nbsp; &nbsp;ans_list = self.run_converse_inference_with_continuation(prompt, system_prompt)
&nbsp;&nbsp; &nbsp;
&nbsp;&nbsp; &nbsp;if len(ans_list) == 1:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;final_ans = ans_list[0]
&nbsp;&nbsp; &nbsp;else:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;final_ans = ans_list[0]
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;for i in range(1, len(ans_list)):
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# Seamlessly combine responses by removing overlap
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;final_ans = final_ans.rsplit('\n', 1)[0] + ans_list[i]
&nbsp;&nbsp; &nbsp;
&nbsp;&nbsp; &nbsp;# Extract content within specified tags (java, xml, etc.)
&nbsp;&nbsp; &nbsp;if f'&lt;{tag}&gt;' in final_ans and f'&lt;/{tag}&gt;' in final_ans:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;final_ans = final_ans.split(f'&lt;{tag}&gt;')[-1].split(f'&lt;/{tag}&gt;')[0].strip()
&nbsp;&nbsp; &nbsp;
&nbsp;&nbsp; &nbsp;return final_ans 
 
Optimizing conversion quality 
Through our experiments, we identified several factors that significantly impact conversion quality: 
 
 File size management ‚Äì Files with more than 300 lines of code benefit from being broken into smaller logical units before conversion. 
 Focused conversion ‚Äì Converting different file types (C, header, DBIO) separately yields better results as each file type has distinct conversion patterns. During conversion, C functions are transformed into Java methods within classes, and C structs become Java classes. However, because files are converted individually without cross-file context, achieving optimal object-oriented design might require human intervention to consolidate related functionality, establish proper class hierarchies, and facilitate appropriate encapsulation across the converted code base. 
 Iterative refinement ‚Äì Multiple feedback loops (4‚Äì5 iterations) produce more comprehensive conversions. 
 Role assignment ‚Äì Assigning the model a specific role (senior software developer) improves output quality. 
 Detailed instructions ‚Äì Providing specific transformation rules for common patterns improves consistency. 
 
Assumptions 
This migration strategy makes the following key assumptions: 
 
 Code quality ‚Äì Legacy C code follows reasonable coding practices with discernible structure. Obfuscated or poorly structured code might require preprocessing before automated conversion. 
 Scope limitations ‚Äì This approach targets business logic conversion rather than low-level system code. C code with hardware interactions or platform-specific features might require manual intervention. 
 Test coverage ‚Äì Comprehensive test cases exist for the legacy application to validate functional equivalence after migration. Without adequate tests, additional validation steps are necessary. 
 Domain knowledge ‚Äì Although the agentic workflow reduces the need for expertise in both C and Java, access to subject matter experts who understand the business domain is required to validate preservation of critical business logic. 
 Phased migration ‚Äì The approach assumes an incremental migration strategy is acceptable, where components can be converted and validated individually rather than a full project level migration. 
 
Results and performance 
To evaluate the effectiveness of our migration approach powered by Amazon Nova Premier, we measured performance across enterprise-grade code bases representing typical customer scenarios. Our assessment focused on two success factors: structural completeness (preservation of all business logic and functions) and framework compliance (adherence to Spring Boot best practices and conventions). 
Migration accuracy by code base complexity 
The agentic workflow demonstrated varying effectiveness based on file complexity, with all results validated by subject matter experts. The following table summarizes the results. 
 
  
   
   File Size Category 
   Structural Completeness 
   Framework Compliance 
   Average Processing Time 
   
   
   Small (0‚Äì300 lines) 
   93% 
   100% 
   30 ‚Äì40 seconds 
   
   
   Medium (300‚Äì700 lines) 
   81%* 
   91%* 
   7 minutes 
   
   
   Large (more than 700 lines) 
   62%* 
   84%* 
   21 minutes 
   
  
 
*After multiple feedback cycles 
Key insights for enterprise adoption 
These results reveal an important pattern: the agentic approach excels at handling the bulk of migration work (small to medium files) while still providing significant value for complex files that require human oversight. This creates a hybrid approach where AI handles routine conversions and security assessments, and developers focus on integration and architectural decisions. 
Conclusion 
Our solution demonstrates that the Amazon Bedrock Converse API with Amazon Nova Premier, when implemented within an agentic workflow, can effectively convert legacy C code to modern Java/Spring framework code. The approach handles complex code structures, manages token limitations, and produces high-quality conversions with minimal human intervention. The solution breaks down the conversion process into specialized agent roles, implements robust feedback loops, and handles token limitations through continuation techniques. This approach accelerates the migration process, improves code quality, and reduces the potential for errors. Try out the solution for your own use case, and share your feedback and questions in the comments. 
 
About the authors 
Aditya Prakash&nbsp;is a Senior Data Scientist at the Amazon Generative AI Innovation Center. He helps customers leverage AWS AI/ML services to solve business challenges through generative AI solutions. Specializing in code transformation, RAG systems, and multimodal applications, Aditya enables organizations to implement practical AI solutions across diverse industries. 
Jihye Seo&nbsp;is a Senior Deep Learning Architect who specializes in designing and implementing generative AI solutions. Her expertise spans model optimization, distributed training, RAG systems, AI agent development, and real-time data pipeline construction across manufacturing, healthcare, gaming, and e-commerce sectors. As an AI/ML consultant, Jihye has delivered production-ready solutions for clients, including smart factory control systems, predictive maintenance platforms, demand forecasting models, recommendation engines, and MLOps frameworks 
Yash Shah&nbsp;is a Science Manager in the AWS Generative AI Innovation Center. He and his team of applied scientists, architects and engineers work on a range of machine learning use cases from healthcare, sports, automotive and manufacturing, helping customers realize art of the possible with GenAI. Yash is a graduate of Purdue University, specializing in human factors and statistics. Outside of work, Yash enjoys photography, hiking and cooking.

‚∏ª