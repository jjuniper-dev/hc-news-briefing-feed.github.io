‚úÖ Morning News Briefing ‚Äì July 19, 2025 10:55

üìÖ Date: 2025-07-19 10:55
üè∑Ô∏è Tags: #briefing #ai #publichealth #digitalgov

‚∏ª

üßæ Weather
‚Ä¢ No watches or warnings in effect, Pembroke
  No watches or warnings in effect. No warnings or watches or watches in effect . Watch or warnings are no longer in effect in the U.S. No watches, warnings are in effect for the rest of the day . No watches and warnings are still in effect, but no watches are in place for the day's events . The weather is not expected to be affected by the weather .
‚Ä¢ Current Conditions:  13.8¬∞C
  Temperature: 13.8&deg;C Pressure / Tendency: 101.8 kPa falling Humidity: 89 % Dewpoint: 12.0&deg:C Wind: SW calm km/h Air Quality Health Index: n/a . Pembroke 6:00 AM EDT Saturday 19 July 2025 . Weather forecast: July 19, 2025, July 20, July
‚Ä¢ Saturday: Chance of showers. High 24. POP 70%
  30 percent chance of showers early this morning, 70 percent chance for showers this afternoon . High 24. Humidex 26. UV index 8 or very high. UV Index 8 or high . Showers will be felt throughout the day, with highs of 24.50 degrees warmer than normal . Forecast issued at 5:00 AM EDT Saturday 19 July 2025, with rain likely in the

üåç International News
No updates.

üçÅ Canadian News
No updates.

üá∫üá∏ U.S. Top Stories
‚Ä¢ Congo and Rwanda-backed rebels sign declaration of principles for permanent ceasefire
  Congo and Rwanda-backed rebels on Saturday signed a declaration of principles in Qatar to end fighting in eastern Congo . The declaration commits them to a permanent ceasefire to be signed in one month . The two sides are expected to sign an agreement in a month to end the fighting in the east of the country . The agreement was reached in Qatar in order to end violence in the eastern Congo region .
‚Ä¢ Muddy boots and AI are helping this threatened frog to make a comeback
  Scientists monitor the populations of the threatened California red-legged frog . They use AI to help monitor the population of the endangered species of the frog . The frog population has been threatened in the U.S. for more than a century . Scientists are monitoring the population with the help of AI to monitor the frog population in a new way of monitoring the species' natural habitat in the wild .
‚Ä¢ Are high-protein snacks worth the hype? Here's how to assess
  Protein has taken over the packaged-snack aisles at the grocery store . But do you need extra protein in your chips and muffins? Do you need it? Share your photos with CNN iReport.com . Back to the page you came from, please submit your best shots of the day to see if you need a new look at the latest iReporter photos .
‚Ä¢ Should you buy it? If you answer 'yes' to these questions, probably not
  Sometimes we want to buy things we want, not what we need, and that's OK . When you're unsure whether to swipe that card or walk away, this guide can help you make a mindful decision you won't regret . It's OK to buy what you want, but don't always buy things you don't need, or what you need, at the end of the day
‚Ä¢ The USDA wants states to hand over food stamp data by the end of July
  The USDA has set a deadline of July 30 for states to hand over the sensitive data of tens of millions of people who applied for federal food assistance . A lawsuit is trying to stop the collection of the data, while a lawsuit is being filed to stop it . The USDA set the July 30 deadline for the data collection of food assistance applicants . The lawsuit is currently underway to try to stop

üß† Artificial Intelligence
No updates.

üíª Digital Strategy
‚Ä¢ Ex-IDF cyber chief on Iran, Scattered Spider, and why social engineering worries him more than 0-days
  Scattered Spider and Iranian government-backed cyber units have more in common than a recent uptick in hacking activity . Ariel Parnes, a former colonel in the Israeli Defense Forces' cyber unit 8200, says the Iranian government has more to common than recent uptick of hacking activity, he says . The former colonel says the two cyber units are backed by the government of Israel and Iran .
‚Ä¢ Republican calls out Trump admin's decision to resume GPU sales to China
  The Republican chair of the US House Select Committee on China has protested the Trump administration's decision to lift restrictions on the sale of Nvidia H20 GPUs and similar processors . He says the chips could be used to advance Chinese AI and military interests . Moolenaar demands answers from Commerce Secretary, calling on Commerce Secretary to explain his decision to relax restrictions on sale of the chips in the US
‚Ä¢ Meta declines to abide by voluntary EU AI safety guidelines
  European Commission issued voluntary guidelines for providers of general-purpose AI models . Meta refused to sign, arguing that the extra measures introduce "legal uncertainties" beyond the law's scope . The EU AI Act takes effect next month, but Meta says extra measures are 'unfairly beyond the scope' of the law . The European Commission is expected to issue voluntary guidelines to providers of AI models next
‚Ä¢ Foundry competition heats up as Japan‚Äôs Rapidus says 2nm chip tech on track for 2027
  Rapidus says it's on track to begin volume production of 2nm process tech . Foundry upstart Rapidus is two years behind everyone else . Rapidus achieved a major milestone this week at the start of production process process tech in Japan . It is the first major Japanese foundry to produce a process process process two years after its first 2nm technology was developed in 2011 . Rapid
‚Ä¢ Coldplay kiss-cam flap proves we‚Äôre already our own surveillance state
  A tech executive's alleged affair exposed on a stadium jumbotron is ripe fodder for the gossip rags, but it exhibits something else: proof that we need not wait for an AI-fueled dystopian surveillance state to descend on us - we're perfectly able and willing to surveil ourselves . And we‚Äôre the ones building it .‚Ä¶ And we're the ones that

üè• Public Health
No updates.

üî¨ Science
‚Ä¢ Effectiveness of leading pedestrian intervals for city walkers‚Äô safety
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Adherence to different types of sports shapes motor competence development in preschool children
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Personality traits and adherence on COVID-19 preventive measures in a two-year follow-up study
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Development and performance verification of an isometric dynamometer for lower extremity
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Breast cancer survival and mortality among women with type 2 diabetes: a retrospective cohort study
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations

üßæ Government & Policy
No updates.

üèõÔ∏è Enterprise Architecture & IT Governance
No updates.

ü§ñ AI & Emerging Tech
‚Ä¢ A major AI training data set contains millions of examples of personal data
  Millions of images of passports, credit cards, birth certificates, and other documents containing personally identifiable information are likely included in one of the biggest open-source AI training sets, new research has found.



Thousands of images‚Äîincluding identifiable faces‚Äîwere found in a small subset of DataComp CommonPool, a major AI training set for image generation scraped from the web. Because the researchers audited just 0.1% of CommonPool‚Äôs data, they estimate that the real number of images containing personally identifiable information, including faces and identity documents, is in the hundreds of millions. The study that details the breach was published on arXiv earlier this month.



The bottom line, says William Agnew, a postdoctoral fellow in AI ethics at Carnegie Mellon University and one of the coauthors, is that ‚Äúanything you put online can [be] and probably has been scraped.‚Äù



The researchers found thousands of instances of validated identity documents‚Äîincluding images of credit cards, driver‚Äôs licenses, passports, and birth certificates‚Äîas well as over 800 validated job application documents (including r√©sum√©s and cover letters), which were confirmed through LinkedIn and other web searches as being associated with real people. (In many more cases, the researchers did not have time to validate the documents or were unable to because of issues like image clarity.)&nbsp;



A number of the r√©sum√©s disclosed sensitive information including disability status, the results of background checks, birth dates and birthplaces of dependents, and race. When r√©sum√©s were linked to people with online presences, researchers also found contact information, government identifiers, sociodemographic information, face photographs, home addresses, and the contact information of other people (like references).



Examples of identity-related documents found in CommonPool‚Äôs small-scale data set show a credit card, a Social Security number, and a driver‚Äôs license. For each sample, the type of URL site is shown at the top, the image in the middle, and the caption in quotes below. All personal information has been replaced, and text has been paraphrased to avoid direct quotations. Images have been redacted to show the presence of faces without identifying the individuals.COURTESY OF THE RESEARCHERS




When it was released in 2023, DataComp CommonPool, with its 12.8 billion data samples, was the largest existing data set of publicly available image-text pairs, which are often used to train generative text-to-image models. While its curators said that CommonPool was intended for academic research, its license does not prohibit commercial use as well.&nbsp;



CommonPool was created as a follow-up to the LAION-5B data set, which was used to train models including Stable Diffusion and Midjourney. It draws on the same data source: web scraping done by the nonprofit Common Crawl between 2014 and 2022.&nbsp;



While commercial models often do not disclose what data sets they are trained on, the shared data sources of DataComp CommonPool and LAION-5B mean that the data sets are similar, and that the same personally identifiable information likely appears in LAION-5B, as well as in other downstream models trained on CommonPool data. CommonPool researchers did not respond to emailed questions.



And since DataComp CommonPool has been downloaded more than 2 million times over the past two years, it is likely that ‚Äúthere [are]many downstream models that are all trained on this exact data set,‚Äù says Rachel Hong, a PhD student in computer science at the University of Washington and the paper‚Äôs lead author. Those would duplicate similar privacy risks.



Good intentions are not enough



‚ÄúYou can assume that any large-scale web-scraped data always contains content that shouldn‚Äôt be there,‚Äù says Abeba Birhane, a cognitive scientist and tech ethicist who leads Trinity College Dublin‚Äôs AI Accountability Lab‚Äîwhether it‚Äôs personally identifiable information (PII), child sexual abuse imagery, or hate speech (which Birhane‚Äôs own research into LAION-5B has found).&nbsp;



Indeed, the curators of DataComp CommonPool were themselves aware it was likely that PII would appear in the data set and did take some measures to preserve privacy, including automatically detecting and blurring faces. But in their limited data set, Hong‚Äôs team found and validated over 800 faces that the algorithm had missed, and they estimated that overall, the algorithm had missed 102 million faces in the entire data set. On the other hand, they did not apply filters that could have recognized known PII character strings, like emails or Social Security numbers.&nbsp;



‚ÄúFiltering is extremely hard to do well,‚Äù says Agnew. ‚ÄúThey would have had to make very significant advancements in PII detection and removal that they haven‚Äôt made public to be able to effectively filter this.‚Äù&nbsp;&nbsp;



Examples of r√©sum√© documents and personal disclosures found in CommonPool‚Äôs small-scale data set. For each sample, the type of URL site is shown at the top, the image in the middle, and the caption in quotes below. All personal information has been replaced, and text has been paraphrased to avoid direct quotations. Images have been redacted to show the presence of faces without identifying the individuals. Image courtesy of the researchers.COURTESY OF THE RESEARCHERS




There are other privacy issues that the face blurring doesn‚Äôt address. While the blurring filter is automatically applied, it is optional and can be removed. Additionally, the captions that often accompany the photos, as well as the photos‚Äô metadata, often contain even more personal information, such as names and exact locations.



Another privacy mitigation measure comes from Hugging Face, a platform that distributes training data sets and hosts CommonPool, which integrates with a tool that theoretically allows people to search for and remove their own information from a data set. But as the researchers note in their paper, this would require people to know that their data is there to start with. When asked for comment, Florent Daudens of Hugging Face said that ‚Äúmaximizing the privacy of data subjects across the AI ecosystem takes a multilayered approach, which includes but is not limited to the widget mentioned,‚Äù and that the platform is ‚Äúworking with our community of users to move the needle in a more privacy-grounded direction.‚Äù&nbsp;



In any case, just getting your data removed from one data set probably isn‚Äôt enough. ‚ÄúEven if someone finds out their data was used in a training data sets and ‚Ä¶ exercises their right to deletion, technically the law is unclear about what that means,‚Äù ¬†says Tiffany Li, an associate professor of law at the University of San Francisco School of Law. ‚ÄúIf the organization only deletes data from the training data sets‚Äîbut does not delete or retrain the already trained model‚Äîthen the harm will nonetheless be done.‚Äù



The bottom line, says Agnew, is that ‚Äúif you web-scrape, you‚Äôre going to have private data in there. Even if you filter, you‚Äôre still going to have private data in there, just because of the scale of this. And that‚Äôs something that we [machine-learning researchers], as a field, really need to grapple with.‚Äù



Reconsidering consent



CommonPool was built on web data scraped between 2014 and 2022, meaning that many of the images likely date to before 2020, when ChatGPT was released. So even if it‚Äôs theoretically possible that some people consented to having their information publicly available to anyone on the web, they could not have consented to having their data used to train large AI models that did not yet exist.





And with web scrapers often scraping data from each other, an image that was originally uploaded by the owner to one specific location would often find its way into other image repositories. ‚ÄúI might upload something onto the internet, and then ‚Ä¶ a year or so later, [I] want to take it down, but then that [removal] doesn‚Äôt necessarily do anything anymore,‚Äù says Agnew.



The researchers also found numerous examples of children‚Äôs personal information, including depictions of birth certificates, passports, and health status, but in contexts suggesting that they had been shared for limited purposes.



‚ÄúIt really illuminates the original sin of AI systems built off public data‚Äîit‚Äôs extractive, misleading, and dangerous to people who have been using the internet with one framework of risk, never assuming it would all be hoovered up by a group trying to create an image generator,‚Äù says Ben Winters, the director of AI and privacy at the Consumer Federation of America.



Finding a policy that fits



Ultimately, the paper calls for the machine-learning community to rethink the common practice of indiscriminate web scraping and also lays out the possible violations of current privacy laws represented by the existence of PII in massive machine-learning data sets, as well as the limitations of those laws‚Äô ability to protect privacy.



‚ÄúWe have the GDPR in Europe, we have the CCPA in California, but there‚Äôs still no federal data protection law in America, which also means that different Americans have different rights protections,‚Äù says Marietje Schaake, a Dutch lawmaker turned tech policy expert who currently serves as a fellow at Stanford‚Äôs Cyber Policy Center.&nbsp;



Besides, these privacy laws apply to companies that meet certain criteria for size and other characteristics. They do not necessarily apply to researchers like those who were responsible for creating and curating DataComp CommonPool.



And even state laws that do address privacy, like California‚Äôs consumer privacy act, have carve-outs for ‚Äúpublicly available‚Äù information. Machine-learning researchers have long operated on the principle that if it‚Äôs available on the internet, then it is public and no longer private information, but Hong, Agnew, and their colleagues hope that their research challenges this assumption.&nbsp;



‚ÄúWhat we found is that ‚Äòpublicly available‚Äô includes a lot of stuff that a lot of people might consider private‚Äîr√©sum√©s, photos, credit card numbers, various IDs, news stories from when you were a child, your family blog. These are probably not things people want to just be used anywhere, for anything,‚Äù says Hong.&nbsp;&nbsp;



Hopefully, Schaake says, this research ‚Äúwill raise alarm bells and create change.‚Äù&nbsp;



This article previously misstated Tiffany Li&#8217;s affiliation. This has been fixed.
‚Ä¢ The Download: how to run an LLM, and a history of ‚Äúthree-parent babies‚Äù
  This is today&#8217;s edition of¬†The Download,¬†our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



How to run an LLM on your laptop



In the early days of large language models, there was a high barrier to entry: it used to be impossible to run anything useful on your own computer without investing in pricey GPUs. But researchers have had so much success in shrinking down and speeding up models that anyone with a laptop, or even a smartphone, can now get in on the action.For people who are concerned about privacy, want to break free from the control of the big LLM companies, or just enjoy tinkering, local models offer a compelling alternative to ChatGPT and its web-based peers. Here‚Äôs how to get started running a useful model from the safety and comfort of your own computer. Read the full story.‚ÄîGrace Huckins



This story is part of MIT Technology Review‚Äôs How To series, helping you get things done. You can check out the rest of the series here.







A brief history of ‚Äúthree-parent babies‚Äù



This week we heard that eight babies have been born in the UK following an experimental form of IVF that involves DNA from three people. The approach was used to prevent women with genetic mutations from passing mitochondrial diseases to their children.But these eight babies aren‚Äôt the first ‚Äúthree-parent‚Äù children out there. Over the last decade, several teams have been using variations of this approach to help people have babies. But the procedure is not without controversy. Read the full story.



‚ÄîJessica Hamzelou



This article first appeared in The Checkup, MIT Technology Review‚Äôs weekly biotech newsletter. To receive it in your inbox every Thursday, and read articles like this first, sign up here.







The must-reads



I‚Äôve combed the internet to find you today‚Äôs most fun/important/scary/fascinating stories about technology.



1 OpenAI has launched ChatGPT Agent¬†It undertakes tasks on your behalf by building its own ‚Äúvirtual computer.‚Äù (The Verge)+ It may take a while to actually complete them. (Wired $)+ Are we ready to hand AI agents the keys? (MIT Technology Review)



2 The White House is going after ‚Äúwoke AI‚ÄùIt‚Äôs preparing an executive order preventing companies with ‚Äúliberal bias‚Äù in their models from landing federal contracts. (WSJ $)+ Why it‚Äôs impossible to build an unbiased AI language model. (MIT Technology Review)



3 A new law in Russia criminalizes certain online searchesLooking up LGBT content, for example, could land Russians in big trouble. (WP $)+ Dozens of Russian regions have been hit with cellphone internet shutdowns. (ABC News)



4 Elon Musk wants to detonate SpaceX rockets over Hawaii‚Äôs watersEven though the proposed area is a sacred Hawaiian religious site. (The Guardian)+ Rivals are rising to challenge the dominance of SpaceX. (MIT Technology Review) 



5 Meta‚Äôs privacy violation trial is overThe shareholders suing Mark Zuckerberg and other officials have settled for a (likely very hefty) payout. (Reuters)



6 Inside ICE‚Äôs powerful facial recognition appMobile Fortify can check a person‚Äôs face against a database of 200 million images. (404 Media)+ The department has unprecedented access to Medicaid data, too. (Wired $)



7 DOGE has left federal workers exhausted and anxiousSix months in, workers are struggling to cope with the fall out. (Insider $)+ DOGE‚Äôs tech takeover threatens the safety and stability of our critical data. (MIT Technology Review)



8 Netflix has used generative AI in a show for the first timeTo cut costs, apparently. (BBC)



9 Does AI really spell the end of loneliness?Virtual companions aren‚Äôt always what they‚Äôre cracked up to be. (New Yorker $)+ The AI relationship revolution is already here. (MIT Technology Review)



10 Flip phones are back with a vengeanceAt least they‚Äôre more interesting to look at than a conventional smartphone. (Vox)+ Triple-folding phones might be a bridge too far, though. (The Verge)







Quote of the day



‚ÄúIt is far from perfect.‚Äù



‚ÄîKevin Weil, OpenAI‚Äôs chief product officer, acknowledges that its new agent still requires a lot of work, Bloomberg reports.







One more thing







GMOs could reboot chestnut treesLiving as long as a thousand years, the American chestnut tree once dominated parts of the Eastern forest canopy, with many Native American nations relying on them for food. But by 1950, the tree had largely succumbed to a fungal blight probably introduced by Japanese chestnuts.As recently as last year, it seemed the 35-year effort to revive the American chestnut might grind to a halt. Now, American Castanea, a new biotech startup, has created more than 2,500 transgenic chestnut seedlings‚Äî likely the first genetically modified trees to be considered for federal regulatory approval as a tool for ecological restoration. Read the full story.



¬†‚ÄîAnya Kamenetz







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)



+ This stained glass embedded into a rusted old Porsche is strangely beautiful.+ Uhoh: here comes the next annoying group of people to avoid, the Normans.+ I bet Dolly Parton knows a thing or two about how to pack for a trip.+ Aww‚Äîorcas have been known to share food with humans in the wild.
‚Ä¢ A brief history of ‚Äúthree-parent babies‚Äù
  This week we heard that eight babies have been born in the UK following an experimental form of IVF that involves DNA from three people. The approach was used to prevent women with genetic mutations from passing mitochondrial diseases to their children. You can read all about the results, and the reception to them, here.¬†



But these eight babies aren‚Äôt the first ‚Äúthree-parent‚Äù children out there. Over the last decade, several teams have been using variations of this approach to help people have babies. This week, let‚Äôs consider the other babies born from three-person IVF.





I can‚Äôt go any further without talking about the term we use to describe these children. Journalists, myself included, have called them ‚Äúthree-parent babies‚Äù because they are created using DNA from three people. Briefly, the approach typically involves using the DNA from the nuclei of the intended parents‚Äô egg and sperm cells. That‚Äôs where most of the DNA in a cell is found.



But it also makes use of mitochondrial DNA (mtDNA)‚Äîthe DNA found in the energy-producing organelles of a cell‚Äîfrom a third person. The idea is to avoid using the mtDNA from the intended mother, perhaps because it is carrying genetic mutations. Other teams have done this in the hope of treating infertility.



mtDNA, which is usually inherited from a person‚Äôs mother, makes up a tiny fraction of total inherited DNA. It includes only 37 genes, all of which are thought to play a role in how mitochondria work (as opposed to, say, eye color or height).



That‚Äôs why some scientists despise the term ‚Äúthree-parent baby.‚Äù Yes, the baby has DNA from three people, but those three can‚Äôt all be considered parents, critics argue. For the sake of argument, this time around I‚Äôll use the term ‚Äúthree-person IVF‚Äù from here on out.



So, about these babies. The first were reported back in the 1990s. Jacques Cohen, then at Saint Barnabas Medical Center in Livingston, New Jersey, and his colleagues thought they might be able to treat some cases of infertility by injecting the mitochondria-containing cytoplasm of healthy eggs into eggs from the intended mother.¬†Seventeen babies were ultimately born this way, according to the team. (Side note: In¬†their paper, the authors describe potential resulting children as ‚Äúthree-parental individuals.‚Äù)



But two fetuses appeared to have genetic abnormalities. And one of the children started to show signs of a developmental disorder. In 2002, the US Food and Drug Administration put a stop to the research.





The babies born during that study¬†are in their 20s now. But scientists still don‚Äôt know why they saw those abnormalities. Some think that mixing mtDNA from two people might be problematic.



Newer approaches to three-person IVF aim to include mtDNA from just the donor, completely bypassing the intended mother‚Äôs mtDNA. John Zhang at the New Hope Fertility Center in New York City tried this approach for a Jordanian couple in 2016. The woman carried genes for a fatal mitochondrial disease and had already lost two children to it. She wanted to avoid passing it on to another child.



Zhang took the nucleus of the woman‚Äôs egg and inserted it into a donor egg that had had its own nucleus removed‚Äîbut still had its mitochondria-containing cytoplasm. That egg was then fertilized with the woman‚Äôs husband‚Äôs sperm.



Because it was still illegal in the US, Zhang controversially did the procedure in Mexico, where,¬†as he told me at the time, ‚Äúthere are no rules.‚Äù The couple eventually welcomed a healthy baby boy. Less than 1% of the boy‚Äôs mitochondria carried his mother‚Äôs mutation, so the procedure was deemed a success.



There was a fair bit of outrage from the scientific community, though. Mitochondrial donation had been made legal in the UK the previous year, but no clinic had yet been given a license to do it. Zhang‚Äôs experiment seemed to have been conducted with no oversight. Many questioned how ethical it was, although Sian Harding, who reviewed the ethics of the UK procedure, then told me it was ‚Äúas good as or better than what we‚Äôll do in the UK.‚Äù



The scandal had barely died down by the time the next ‚Äúthree-person IVF‚Äù babies were announced. In 2017, a team at the Nadiya Clinic in Ukraine¬†announced the birth of a little girl to parents who‚Äôd had the treatment for infertility. The news brought more outrage from some quarters, as scientists argued that the experimental procedure should only be used to prevent severe mitochondrial diseases.



It wasn‚Äôt until later that year that the UK‚Äôs fertility authority granted a team in Newcastle a license to perform mitochondrial donation. That team launched a trial in 2017. It was big news‚Äîthe first ‚Äúofficial‚Äù trial to test whether the approach could safely prevent mitochondrial disease.





But it was slow going. And meanwhile, other teams were making progress. The Nadiya Clinic continued to trial the procedure in couples with infertility. Pavlo Mazur, a former embryologist who worked at that clinic, tells me that 10 babies were born there as a result of mitochondrial donation.



Mazur then moved to another clinic in Ukraine, where he says he used a different type of mitochondrial donation to achieve another five healthy births for people with infertility. ‚ÄúIn total, it‚Äôs 15 kids made by me,‚Äù he says.



But he adds that other clinics in Ukraine are also using mitochondrial donation, without sharing their results. ‚ÄúWe don‚Äôt know the actual number of those kids in Ukraine,‚Äù says Mazur. ‚ÄúBut there are dozens of them.‚Äù



In 2020, Nuno Costa-Borges of Embryotools in Barcelona, Spain, and his colleagues described¬†another trial of mitochondrial donation. This trial, performed in Greece, was also designed to test the procedure for people with infertility. It involved 25 patients. So far,¬†seven children have been born. ‚ÄúI think it‚Äôs a bit strange that they aren‚Äôt getting more credit,‚Äù says Heidi Mertes, a medical ethicist at Ghent University in Belgium.



The newly announced UK births are only the latest ‚Äúthree-person IVF‚Äù babies. And while their births are being heralded as a success story for mitochondrial donation, the story isn‚Äôt quite so simple. Three of the eight babies were born with a non-insignificant proportion of mutated mitochondria, ranging between 5% and 20%, depending on the baby and the sample.



Dagan Wells of the University of Oxford, who is involved in the Greece trial, says that two of the seven babies in their study also appear to have inherited mtDNA from their intended mothers. Mazur says he has seen several cases of this ‚Äúreversal‚Äù too.



This isn‚Äôt a problem for babies whose mothers don‚Äôt carry genes for mitochondrial disease. But it might be for those whose mothers do.



I don‚Äôt want to pour cold water over the new UK results. It was great to finally see the results of a trial that‚Äôs been running for eight years. And the births of healthy babies are something to celebrate. But it‚Äôs not a simple success story. Mitochondrial donation doesn‚Äôt guarantee a healthy baby. We still have more to learn, not only from these babies, but from the others that have already been born.



This article first appeared in The Checkup,¬†MIT Technology Review‚Äôs¬†weekly biotech newsletter. To receive it in your inbox every Thursday, and read articles like this first,¬†sign up here.
‚Ä¢ Finding value from AI agents from day one
  Imagine AI so sophisticated it could read a customer‚Äôs mind? Or identify and close a cybersecurity loophole weeks before hackers strike? How about a team of AI agents equipped to restructure a global supply chain and circumnavigate looming geopolitical disruption? Such disruptive possibilities explain why agentic AI is sending ripples of excitement through corporate boardrooms.&nbsp;







Although still so early in its development that there lacks consensus on a single, shared definition, agentic AI refers loosely to a suite of AI systems capable of connected and autonomous decision-making with zero or limited human intervention. In scenarios where traditional AI typically requires explicit prompts or instructions for each step, agentic AI will independently execute tasks, learning and adapting to its environment to refine decisions over time.&nbsp;



From assuming oversight for complex workflows, such as procurement or recruitment, to carrying out proactive cybersecurity checks or automating support, enterprises are abuzz at the potential use cases for agentic AI.&nbsp;



According to one Capgemini survey, 50% of business executives are set to invest in and implement AI agents in their organizations in 2025, up from just 10% currently. Gartner has also forecast that 33% of enterprise software applications will incorporate agentic AI by 2028. For context, in 2024 that proportion was less than 1%.&nbsp;



‚ÄúIt‚Äôs creating such a buzz ‚Äì software enthusiasts seeing the possibilities unlocked by LLMs, venture capitalists wanting to find the next big thing, companies trying to find the ‚Äòkiller app,‚Äù says Matt McLarty, chief technology officer at Boomi. But, he adds, ‚Äúright now organizations are struggling to get out of the starting blocks.‚Äù&nbsp;



The challenge is that many organizations are so caught up in the excitement that they risk attempting to run before they can walk when it comes to deployment of agentic AI, believes McLarty. And in so doing they risk turning it from potential business breakthrough into a source of cost, complexity, and confusion.



Keeping agentic AI simple&nbsp;



The heady capabilities of agentic AI have created understandable temptation for senior business leaders to rush in, acting on impulse rather than insight risks turning the technology into a solution in search of a problem, points out McLarty.&nbsp;



It‚Äôs a scenario that‚Äôs unfolded with previous technologies. The decoupling of Blockchain from Bitcoin in 2014 paved the way for a Blockchain 2.0 boom in which organizations rushed to explore the applications for a digital, decentralized ledger beyond currency. But a decade on, the technology has fallen far short of forecasts at the time, dogged by technology limitations and obfuscated use cases.&nbsp;



‚ÄúI do see Blockchain as a cautionary tale,‚Äù says McLarty. ‚ÄúThe hype and ultimate lack of adoption is definitely a path the agentic AI movement should avoid.‚Äù He explains, ‚ÄúThe problem with Blockchain is that people struggle to find use cases where it applies as a solution, and even when they find the use cases, there is often a simpler and cheaper solution,‚Äù he adds. ‚ÄúI think agentic AI can do things no other solution can, in terms of contextual reasoning and dynamic execution. But as technologists, we get so excited about the technology, sometimes we lose sight of the business problem.‚Äù



Instead of diving in headfirst, McLarty advocates for an iterative attitude toward applications of agentic AI, targeting ‚Äúlow-hanging fruit‚Äù and incremental use cases. This includes focusing investment on the worker agents that are set to make up the components of more sophisticated, multi-agent agentic systems further down the road.&nbsp;



However, with a narrower, more prescribed remit, these AI agents with agentic capabilities can add instant value. Enabled with natural language processing (NLP) they can be used to bridge the linguistic shortfalls in current chat agents for example or adaptively carry out rote tasks via dynamic automation.&nbsp;



‚ÄúCurrent rote automation processes generate a lot of value for organizations today, but they can lead to a lot of manual exception processing,‚Äù points out McLarty. ‚ÄúAgentic exception handling agents can eliminate a lot of that.‚Äù&nbsp;



It‚Äôs also essential to avoid use cases for agentic AI that could be addressed with a cheaper and simpler technology. ‚ÄúConfiguring a self-manager, ephemeral agent swarm may sound exciting and be exhilarating to build, but maybe you can just solve the problem with a simple reasoning agent that has access to some in-house contextual data and API-based tools,‚Äù says McLarty. ‚ÄúLet‚Äôs call it the KASS principle: Keep agents simple, stupid.‚Äù



Connecting the dots



The future value of agentic AI will lie in its interoperability and organizations that prioritize this pillar at the earliest phase of their adoption will find themselves ahead of the curve.&nbsp;



As McLarty explains, the usefulness of agentic AI agents in scenarios like customer support chats lies in their combination of four elements: a defined business scope, large language models (LLM), the wider context derived from an organization‚Äôs existing data, and capabilities executed through its core applications. These latter two rely on in-built interoperability. For example, an AI agent tasked with onboarding new employees will require access to updated HR policies, asset catalogs and IT. ‚ÄúOrganizations can get a massive head start on business value through AI agents by having interoperable data and applications to plug and play with agents,‚Äù he says.&nbsp;



Agent-to-agent frameworks like the model context protocol (MCP) ‚Äì an open and standardized plug-and-play that connects AI models to internal (or external) information sources ‚Äì can be layered onto an existing API architecture to embed connectedness from the outset. And while it might feel like an additional hurdle now, in the longer-term those organizations that make this investment early will reap the benefits.&nbsp;



‚ÄúThe icing on the cake for interoperability is that all the work you do to connect agents to data and applications now will help you prepare for the multi-agent future where interoperability between agents will be essential,‚Äù says McLarty.&nbsp;



In this future, multi-agent systems will work collectively on more intricate, cross-functional tasks. Agentic systems will draw on AI agents across inventory, logistics and production to coordinate and optimize supply chain management for example or perform complex assembly tasks.&nbsp;



Conscious that this is where the technology is headed, third-party developers are already beginning to offer multi-agent capability. In December, Amazon launched such a tool for its Bedrock service, providing users access to specialized agents coordinated by a supervisor agent capable of breaking down requests, delegating tasks and consolidating outputs.&nbsp;



But though such an off-the-rack solution has the advantage of allowing enterprises to bypass both the risk and complexity in leveraging such capabilities, the digital heterogeneity of larger organizations in particular will likely mean ‚Äì in the longer-term at least ‚Äì they‚Äôll need to rely on their own API architecture to realize the full potential in multi-agent systems.



McLarty‚Äôs advice is simple, ‚ÄúThis is definitely a time to ground yourself in the business problem, and only go as far as you need to with the solution.‚Äù



This content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review‚Äôs editorial staff.



This content was researched, designed, and written entirely by human writers, editors, analysts, and illustrators. This includes the writing of surveys and collection of data for surveys. AI tools that may have been used were limited to secondary production processes that passed thorough human review.
‚Ä¢ How to run an LLM on your laptop
  MIT Technology Review‚Äôs How To series helps you get things done.&nbsp;



Simon Willison has a plan for the end of the world. It‚Äôs a USB stick, onto which he has loaded a couple of his favorite open-weight LLMs‚Äîmodels that have been shared publicly by their creators and that can, in principle, be downloaded and run with local hardware. If human civilization should ever collapse, Willison plans to use all the knowledge encoded in their billions of parameters for help. ‚ÄúIt‚Äôs like having a weird, condensed, faulty version of Wikipedia, so I can help reboot society with the help of my little USB stick,‚Äù he says.



But you don‚Äôt need to be planning for the end of the world to want to run an LLM on your own device. Willison, who writes a popular blog about local LLMs and software development, has plenty of compatriots: r/LocalLLaMA, a subreddit devoted to running LLMs on your own hardware, has half a million members.



For people who are concerned about privacy, want to break free from the control of the big LLM companies, or just enjoy tinkering, local models offer a compelling alternative to ChatGPT and its web-based peers.



The local LLM world used to have a high barrier to entry: In the early days, it was impossible to run anything useful without investing in pricey GPUs. But researchers have had so much success in shrinking down and speeding up models that anyone with a laptop, or even a smartphone, can now get in on the action. ‚ÄúA couple of years ago, I‚Äôd have said personal computers are not powerful enough to run the good models. You need a $50,000 server rack to run them,‚Äù Willison says. ‚ÄúAnd I kept on being proved wrong time and time again.‚Äù





Why you might want to download your own LLM



Getting into local models takes a bit more effort than, say, navigating to ChatGPT‚Äôs online interface. But the very accessibility of a tool like ChatGPT comes with a cost. ‚ÄúIt‚Äôs the classic adage: If something‚Äôs free, you‚Äôre the product,‚Äù says Elizabeth Seger, the director of digital policy at Demos, a London-based think tank.&nbsp;



OpenAI, which offers both paid and free tiers, trains its models on users‚Äô chats by default. It‚Äôs not too difficult to opt out of this training, and it also used to be possible to remove your chat data from OpenAI‚Äôs systems entirely, until a recent legal decision in the New York Times‚Äô ongoing lawsuit against OpenAI required the company to maintain all user conversations with ChatGPT.



Google, which has access to a wealth of data about its users, also trains its models on both free and paid users‚Äô interactions with Gemini, and the only way to opt out of that training is to set your chat history to delete automatically‚Äîwhich means that you also lose access to your previous conversations. In general, Anthropic does not train its models using user conversations, but it will train on conversations that have been ‚Äúflagged for Trust &amp; Safety review.‚Äù¬†



Training may present particular privacy risks because of the ways that models internalize, and often recapitulate, their training data. Many people trust LLMs with deeply personal conversations‚Äîbut if models are trained on that data, those conversations might not be nearly as private as users think, according to some experts.



‚ÄúSome of your personal stories may be cooked into some of the models, and eventually be spit out in bits and bytes somewhere to other people,‚Äù says Giada Pistilli, principal ethicist at the company Hugging Face, which runs a huge library of freely downloadable LLMs and other AI resources.



For Pistilli, opting for local models as opposed to online chatbots has implications beyond privacy. ‚ÄúTechnology means power,‚Äù she says. ‚ÄúAnd so who[ever] owns the technology also owns the power.‚Äù States, organizations, and even individuals might be motivated to disrupt the concentration of AI power in the hands of just a few companies by running their own local models.



Breaking away from the big AI companies also means having more control over your LLM experience. Online LLMs are constantly shifting under users‚Äô feet: Back in April, ChatGPT suddenly started sucking up to users far more than it had previously, and just last week Grok started calling itself MechaHitler on X.



Providers tweak their models with little warning, and while those tweaks might sometimes improve model performance, they can also cause undesirable behaviors. Local LLMs may have their quirks, but at least they are consistent. The only person who can change your local model is you.



Of course, any model that can fit on a personal computer is going to be less powerful than the premier online offerings from the major AI companies. But there‚Äôs a benefit to working with weaker models‚Äîthey can inoculate you against the more pernicious limitations of their larger peers. Small models may, for example, hallucinate more frequently and more obviously than Claude, GPT, and Gemini, and seeing those hallucinations can help you build up an awareness of how and when the larger models might also lie.



‚ÄúRunning local models is actually a really good exercise for developing that broader intuition for what these things can do,‚Äù Willison says.



How to get started



Local LLMs aren‚Äôt just for proficient coders. If you‚Äôre comfortable using your computer‚Äôs command-line interface, which allows you to browse files and run apps using text prompts, Ollama is a great option. Once you‚Äôve installed the software, you can download and run any of the hundreds of models they offer with a single command.&nbsp;



If you don‚Äôt want to touch anything that even looks like code, you might opt for LM Studio, a user-friendly app that takes a lot of the guesswork out of running local LLMs. You can browse models from Hugging Face from right within the app, which provides plenty of information to help you make the right choice. Some popular and widely used models are tagged as ‚ÄúStaff Picks,‚Äù and every model is labeled according to whether it can be run entirely on your machine‚Äôs speedy GPU, needs to be shared between your GPU and slower CPU, or is too big to fit onto your device at all. Once you‚Äôve chosen a model, you can download it, load it up, and start interacting with it using the app‚Äôs chat interface.



As you experiment with different models, you‚Äôll start to get a feel for what your machine can handle. According to Willison, every billion model parameters require about one GB of RAM to run, and I found that approximation to be accurate: My own 16 GB laptop managed to run Alibaba‚Äôs Qwen3 14B as long as I quit almost every other app. If you run into issues with speed or usability, you can always go smaller‚ÄîI got reasonable responses from Qwen3 8B as well.



And if you go really small, you can even run models on your cell phone. My beat-up iPhone 12 was able to run Meta‚Äôs Llama 3.2 1B using an app called LLM Farm. It‚Äôs not a particularly good model‚Äîit very quickly goes off into bizarre tangents and hallucinates constantly‚Äîbut trying to coax something so chaotic toward usability can be entertaining. If I‚Äôm ever on a plane sans Wi-Fi and desperate for a probably false answer to a trivia question, I now know where to look.



Some of the models that I was able to run on my laptop were effective enough that I can imagine using them in my journalistic work. And while I don‚Äôt think I‚Äôll depend on phone-based models for anything anytime soon, I really did enjoy playing around with them. ‚ÄúI think most people probably don‚Äôt need to do this, and that‚Äôs fine,‚Äù Willison says. ‚ÄúBut for the people who want to do this, it‚Äôs so much fun.‚Äù

üîí Cybersecurity & Privacy
‚Ä¢ Poor Passwords Tattle on AI Hiring Bot Maker Paradox.ai
  Security researchers recently revealed that the personal information of millions of people who applied for jobs at McDonald&#8217;s was exposed after they guessed the password (&#8220;123456&#8221;) for the fast food chain&#8217;s account at Paradox.ai, a company that makes artificial intelligence based hiring chatbots used by many Fortune 500 firms. Paradox.ai said the security oversight was an isolated incident that did not affect its other customers, but recent security breaches involving its employees in Vietnam tell a more nuanced story.
A screenshot of the paradox.ai homepage showing its AI hiring chatbot &#8220;Olivia&#8221; interacting with potential hires.
Earlier this month, security researchers Ian Carroll and Sam Curry wrote about simple methods they found to access the backend of the AI chatbot platform on McHire.com, the McDonald&#8217;s website that many of its franchisees use to screen job applicants. As first reported by Wired, the researchers discovered that the weak password used by Paradox exposed 64 million records, including applicants&#8217; names, email addresses and phone numbers.
Paradox.ai acknowledged the researchers&#8217; findings but said the company&#8217;s other client instances were not affected, and that no sensitive information &#8212; such as Social Security numbers &#8212; was exposed.
&#8220;We are confident, based on our records, this test account was not accessed by any third party other than the security researchers,&#8221; the company wrote in a July 9 blog post. &#8220;It had not been logged into since 2019 and frankly, should have been decommissioned. We want to be very clear that while the researchers may have briefly had access to the system containing all chat interactions (NOT job applications), they only viewed and downloaded five chats in total that had candidate information within. Again, at no point was any data leaked online or made public.&#8221;
However, a review of stolen password data gathered by multiple breach-tracking services shows that at the end of June 2025, a Paradox.ai administrator in Vietnam suffered a malware compromise on their device that stole usernames and passwords for a variety of internal and third-party online services. The results were not pretty.
The password data from the Paradox.ai developer was stolen by a malware strain known as &#8220;Nexus Stealer,&#8221; a form grabber and password stealer that is sold on cybercrime forums. The information snarfed by stealers like Nexus is often recovered and indexed by data leak aggregator services like Intelligence X, which reports that the malware on the Paradox.ai developer&#8217;s device exposed hundreds of mostly poor and recycled passwords (using the same base password but slightly different characters at the end).
Those purloined credentials show the developer in question at one point used the same seven-digit password to log in to Paradox.ai accounts for a number of Fortune 500 firms listed as customers on the company&#8217;s website, including Aramark, Lockheed Martin, Lowes, and Pepsi.
Seven-character passwords, particularly those consisting entirely of numerals, are highly vulnerable to &#8220;brute-force&#8221; attacks that can try a large number of possible password combinations in quick succession. According to a much-referenced password strength guide maintained by Hive Systems, modern password-cracking systems can work out a seven number password more or less instantly.
Image: hivesystems.com.
In response to questions from KrebsOnSecurity, Paradox.ai confirmed that the password data was recently stolen by a malware infection on the personal device of a longtime Paradox developer based in Vietnam, and said the company was made aware of the compromise shortly after it happened. Paradox maintains that few of the exposed passwords were still valid, and that a majority of them were present on the employee&#8217;s personal device only because he had migrated the contents of a password manager from an old computer.
Paradox also pointed out that it has been requiring single sign-on (SSO) authentication since 2020 that enforces multi-factor authentication for its partners. Still, a review of the exposed passwords shows they included the Vietnamese administrator&#8217;s credentials to the company&#8217;s SSO platform &#8212; paradoxai.okta.com. The password for that account ended in 202506 &#8212; possibly a reference to the month of June 2025 &#8212; and the digital cookie left behind after a successful Okta login with those credentials says it was valid until December 2025.
Also exposed were the administrator&#8217;s credentials and authentication cookies for an account at Atlassian, a platform made for software development and project management. The expiration date for that authentication token likewise was December 2025.
Infostealer infections are among the leading causes of data breaches and ransomware attacks today, and they result in the theft of stored passwords and any credentials the victim types into a browser. Most infostealer malware also will siphon authentication cookies stored on the victim&#8217;s device, and depending on how those tokens are configured thieves may be able to use them to bypass login prompts and/or multi-factor authentication.
Quite often these infostealer infections will open a backdoor on the victim&#8217;s device that allows attackers to access the infected machine remotely. Indeed, it appears that remote access to the Paradox administrator&#8217;s compromised device was offered for sale recently.
In February 2019, Paradox.ai announced it had successfully completed audits for two fairly comprehensive security standards (ISO 27001 and SOC 2 Type II). Meanwhile, the company&#8217;s security disclosure this month says the test account with the atrocious 123456 username and password was last accessed in 2019, but somehow missed in their annual penetration tests. So how did it manage to pass such stringent security audits with these practices in place?
Paradox.ai told KrebsOnSecurity that at the time of the 2019 audit, the company&#8217;s various contractors were not held to the same security standards the company practices internally. Paradox emphasized that this has changed, and that it has updated its security and password requirements multiple times since then.
It is unclear how the Paradox developer in Vietnam infected his computer with malware, but a closer review finds a Windows device for another Paradox.ai employee from Vietnam was compromised by similar data-stealing malware at the end of 2024 (that compromise included the victim&#8217;s GitHub credentials). In the case of both employees, the stolen credential data includes Web browser logs that indicate the victims repeatedly downloaded pirated movies and television shows, which are often bundled with malware disguised as a video codec needed to view the pirated content.
‚Ä¢ DOGE Denizen Marko Elez Leaked API Key for xAI
  Marko Elez, a 25-year-old employee at Elon Musk&#8217;s Department of Government Efficiency (DOGE), has been granted access to sensitive databases at the U.S. Social Security Administration, the Treasury and Justice departments, and the Department of Homeland Security. So it should fill all Americans with a deep sense of confidence to learn that Mr. Elez over the weekend inadvertently published a private key that allowed anyone to interact directly with more than four dozen large language models (LLMs) developed by Musk&#8217;s artificial intelligence company xAI.
Image: Shutterstock, @sdx15.
On July 13, Mr. Elez committed a code script to GitHub called &#8220;agent.py&#8221; that included a private application programming interface (API) key for xAI. The inclusion of the private key was first flagged by GitGuardian, a company that specializes in detecting and remediating exposed secrets in public and proprietary environments. GitGuardian‚Äôs systems constantly scan GitHub and other code repositories for exposed API keys, and fire off automated alerts to affected users.
Philippe Caturegli, ‚Äúchief hacking officer‚Äù at the security consultancy Seralys,¬†said the exposed API key allowed access to at least 52 different LLMs used by xAI. The most recent LLM in the list was called &#8220;grok-4-0709&#8221; and was created on July 9, 2025.
Grok, the generative AI chatbot developed by xAI and integrated into Twitter/X, relies on these and other LLMs (a query to Grok before publication shows Grok currently uses Grok-3, which was launched in Feburary 2025). Earlier today, xAI announced that the Department of Defense will begin using Grok as part of a contract worth up to $200 million. The contract award came less than a week after Grok began spewing antisemitic rants and invoking Adolf Hitler.
Mr. Elez did not respond to a request for comment. The code repository containing the private xAI key was removed shortly after Caturegli notified Elez via email. However, Caturegli said the exposed API key still works and has not yet been revoked.
&#8220;If a developer can&#8217;t keep an API key private, it raises questions about how they&#8217;re handling far more sensitive government information behind closed doors,&#8221; Caturegli told KrebsOnSecurity.
Prior to joining DOGE, Marko Elez worked for a number of Musk&#8217;s companies. His DOGE career began at the Department of the Treasury, and a legal battle over DOGE&#8217;s access to Treasury databases showed Elez was sending unencrypted personal information in violation of the agency&#8217;s policies.
While still at Treasury, Elez resigned after The Wall Street Journal linked him to social media posts that advocated racism and eugenics. When Vice President J.D. Vance lobbied for Elez to be rehired, President Trump agreed and Musk reinstated him.
Since his re-hiring as a DOGE employee, Elez has been granted access to databases at one federal agency after another. TechCrunch reported in February 2025 that he was working at the Social Security Administration. In March, Business Insider found Elez was part of a DOGE detachment assigned to the Department of Labor.
Marko Elez, in a photo from a social media profile.
In April, The New York Times reported that Elez held positions at the U.S. Customs and Border Protection and the Immigration and Customs Enforcement (ICE) bureaus, as well as the Department of Homeland Security. The Washington Post later reported that Elez, while serving as a DOGE advisor at the Department of Justice, had gained access to the Executive Office for Immigration Review&#8217;s Courts and Appeals System (EACS).
Elez is not the first DOGE worker to publish internal API keys for xAI: In May, KrebsOnSecurity detailed how another DOGE employee leaked a private xAI key on GitHub for two months, exposing LLMs that were custom made for working with internal data from Musk&#8217;s companies, including SpaceX, Tesla and Twitter/X.
Caturegli said it&#8217;s difficult to trust someone with access to confidential government systems when they can&#8217;t even manage the basics of operational security.
&#8220;One leak is a mistake,&#8221; he said. &#8220;But when the same type of sensitive key gets exposed again and again, it‚Äôs not just bad luck, it‚Äôs a sign of deeper negligence and a broken security culture.&#8221;

üéì University AI
No updates.

üè¢ Corporate AI
‚Ä¢ CollabLLM: Teaching LLMs to collaborate with users
  Large language models (LLMs) can solve complex puzzles in seconds, yet they sometimes struggle over simple conversations. When these AI tools make assumptions, overlook key details, or neglect to ask clarifying questions, the result can erode trust and derail real-world interactions, where nuance is everything.



A key reason these models behave this way lies in how they‚Äôre trained and evaluated. Most benchmarks use isolated, single-turn prompts with clear instructions. Training methods tend to optimize for the model&#8217;s next response, not its contribution to a successful, multi-turn exchange. But real-world interaction is dynamic and collaborative. It relies on context, clarification, and shared understanding.



User-centric approach to training&nbsp;



To address this, we‚Äôre exploring ways to train LLMs with users in mind. Our approach places models in simulated environments that reflect the back-and-forth nature of real conversations. Through reinforcement learning, these models improve through trial and error, for example, learning when to ask questions and how to adapt tone and communication style to different situations. This user-centric approach helps bridge the gap between how LLMs are typically trained and how people actually use them.¬†¬†



This is the concept behind CollabLLM (opens in new tab), recipient of an ICML (opens in new tab) Outstanding Paper Award (opens in new tab). This training framework helps LLMs improve through simulated multi-turn interactions, as illustrated in Figure 1. The core insight behind CollabLLM is simple: in a constructive collaboration, the value of a response isn‚Äôt just in its immediate usefulness, but in how it contributes to the overall success of the conversation. A clarifying question might seem like a delay but often leads to better outcomes. A quick answer might appear useful but can create confusion or derail the interaction.



Figure 1. Diagram comparing two training approaches for LLMs. (a) The standard method lacks user-agent collaboration and uses single-turn rewards, leading to an inefficient conversation. (b) In contrast, CollabLLM simulates multi-turn user-agent interactions during training, enabling it to learn effective collaboration strategies and produce more efficient dialogues.



CollabLLM puts this collaborative approach into practice with a simulation-based training loop, illustrated in Figure 2. At any point in a conversation, the model generates multiple possible next turns by engaging in a dialogue with a simulated user.



Figure 2: Simulation-based training process used in CollabLLM



The system uses a sampling method to extend conversations turn by turn, choosing likely responses for each participant (the AI agent or the simulated user), while adding some randomness to vary the conversational paths. The goal is to expose the model to a wide variety of conversational scenarios, helping it learn more effective collaboration strategies.



	
		

		
		PODCAST SERIES
	
	
	
						
				
					
				
			
			
			

									AI Testing and Evaluation: Learnings from Science and Industry
				
								Discover how Microsoft is learning from other domains to advance evaluation and testing as a pillar of AI governance.
				
								
					
						
							Listen now						
					
				
							
	
Opens in a new tab	
	


To each simulated conversation, we applied multiturn-aware reward (MR) functions, which assess how the model‚Äôs response at the given turn influences the entire trajectory of the conversation. We sampled multiple conversational follow-ups from the model, such as statements, suggestions, questions, and used MR to assign a reward to each based on how well the conversation performed in later turns. We based these scores on automated metrics that reflect key factors like goal completion, conversational efficiency, and user engagement.



To score the sampled conversations, we used task-specific metrics and metrics from an LLM-as-a-judge framework, which supports efficient and scalable evaluation. For metrics like engagement, a judge model rates each sampled conversation on a scale from 0 to 1.



The MR of each model response was computed by averaging the scores from the sampled conversations, originating from the model response. Based on the score, the model updates its parameters using established reinforcement learning algorithms like Proximal Policy Optimization (PPO) or Direct Preference Optimization (DPO).



We tested CollabLLM through a combination of automated and human evaluations, detailed in the paper. One highlight is a user study involving 201 participants in a document co-creation task, shown in Figure 3. We compared CollabLLM to a baseline trained with single-turn rewards and to a second, more proactive baseline prompted to ask clarifying questions and take other proactive steps. CollabLLM outperformed both, producing higher-quality documents, better interaction ratings, and faster task completion times.



Figure 3: Results of the user study in a document co-creation task comparing CollabLLM to a baseline trained with single-turn rewards.



Designing for real-world collaboration



Much of today‚Äôs AI research focuses on fully automated tasks, models working without input from or interaction with users. But many real-world applications depend on people in the loop: as users, collaborators, or decision-makers. Designing AI systems that treat user input not as a constraint, but as essential, leads to systems that are more accurate, more helpful, and ultimately more trustworthy.



This work is driven by a core belief: the future of AI depends not just on intelligence, but on the ability to collaborate effectively. And that means confronting the communication breakdowns in today‚Äôs systems.



We see CollabLLM as a step in that direction, training models to engage in meaningful multi-turn interactions, ask clarifying questions, and adapt to context. In doing so, we can build systems designed to work with people‚Äînot around them.
Opens in a new tabThe post CollabLLM: Teaching LLMs to collaborate with users appeared first on Microsoft Research.
‚Ä¢ AI Testing and Evaluation: Learnings from cybersecurity
  Generative AI presents a unique challenge and opportunity to reexamine governance practices for the responsible development, deployment, and use of AI. To advance thinking in this space, Microsoft has tapped into the experience and knowledge of experts across domains‚Äîfrom genome editing to cybersecurity‚Äîto investigate the role of testing and evaluation as a governance tool.&nbsp;AI Testing and Evaluation: Learnings from Science and Industry,&nbsp;hosted by Microsoft Research‚Äôs&nbsp;Kathleen Sullivan, explores what the technology industry and policymakers can learn from these fields and how that might help shape the course of AI development.



In this episode, Sullivan speaks with Professor Ciaran Martin (opens in new tab) of the University of Oxford about risk assessment and testing in the field of cybersecurity. They explore the importance of differentiated standards for organizations of varying sizes, the role of public-private partnerships, and the opportunity to embed security into emerging technologies from the outset. Later, Tori Westerhoff (opens in new tab), a principal director on the Microsoft AI Red Team, joins Sullivan to talk about identifying vulnerabilities in AI products and services. Westerhoff describes AI security in terms she‚Äôs heard cybersecurity professionals use for their work‚Äîa team sport‚Äîand points to cybersecurity‚Äôs establishment of a shared language and understanding of risk as a model for AI security.








Learn more:




Introducing AI Red Teaming Agent: Accelerate your AI safety and security journey with Azure AI Foundry (opens in new tab)Azure AI Foundry Blog | April 2025



Lessons From Red Teaming 100 Generative AI ProductsPublication | January 2025



Learning from other domains to advance AI evaluation and testingMicrosoft Research Blog | June 2025



Responsible AI: Ethical policies and practices | Microsoft AI



AI and Microsoft Research










	
		
			Subscribe to the Microsoft Research Podcast:		
		
							
					
						  
						Apple Podcasts
					
				
			
							
					
						
						Email
					
				
			
							
					
						
						Android
					
				
			
							
					
						
						Spotify
					
				
			
							
					
						
						RSS Feed
					
				
					
	




	
		
			
				
					

Transcript



[MUSIC]



KATHLEEN SULLIVAN: Welcome to AI Testing and Evaluation: Learnings from Science and Industry. I&#8217;m your host, Kathleen Sullivan.



As generative AI continues to advance, Microsoft has gathered a range of experts‚Äîfrom genome editing to cybersecurity‚Äîto share how their fields approach evaluation and risk assessment. Our goal is to learn from their successes and their stumbles to move the science and practice of AI testing forward. In this series, we&#8217;ll explore how these insights might help guide the future of AI development, deployment, and responsible use.



[MUSIC ENDS]



Today, I&#8217;m excited to welcome Ciaran Martin to the podcast to explore testing and risk assessment in cybersecurity. Ciaran is a professor of practice in the management of public organizations at the University of Oxford. He had previously founded and served as chief executive of the National Cyber Security Centre within the UK&#8217;s intelligence, security, and cyber agency.



And after our conversation, we&#8217;ll talk to Microsoft&#8217;s Tori Westerhoff, a principal director on Microsoft‚Äôs AI Red Team, about how we should think about these insights in the context of AI.



Hi, Ciaran. Thank you so much for being here today.



				
				
					



CIARAN MARTIN: Well, thanks so much for inviting me. It‚Äôs great to be here.



SULLIVAN: Ciaran, before we get into some regulatory specifics, it&#8217;d be great to hear a little bit more about your origin story, and just take us to that day‚Äîwho tapped you on the shoulder and said, ‚ÄúCiaran, we need you to run a national cyber center! Do you fancy building one?‚Äù



MARTIN: You could argue that I owe my job to Edward Snowden. Not an obvious thing to say. So the National Cyber Security Centre, which didn&#8217;t exist at the time‚ÄîI was invited to join the British government&#8217;s cybersecurity effort in a leadership role‚Äîis now a subset of GCHQ. That&#8217;s the digital intelligence agency. The equivalent in the US obviously is the NSA [National Security Agency]. It had been convulsed by the Snowden disclosures. It was an unprecedented challenge.



I was a 17-year career government fixer with some national security experience. So I was asked to go out and help with the policy response, the media response, the legal response. But I said, look, any crisis, even one as big as this, is over one way or the other in six months. What should I do long term? And they said, well, we were thinking of asking you to try to help transform our cybersecurity mission. So the National Cyber Security Centre was born, and I was very proud to lead it, and all in all, I did it for seven years from startup to handing it on to somebody else.



SULLIVAN: I mean, it&#8217;s incredible. And just building on that, people spend a significant portion of their lives online now with a variety of devices, and maybe for listeners who are newer to cybersecurity, could you give us the 90-second lightning talk? Kind of, what does risk assessment and testing look like in this space?



MARTIN: Well, risk assessment and testing, I think, are two different things. You can&#8217;t defend everything. If you defend everything, you&#8217;re defending nothing. So broadly speaking, organizations face three threats. One is complete disruption of their systems. So just imagine not being able to access your system. The second is data protection, and that could be sensitive customer information. It could be intellectual property. And the third is, of course, you could be at risk of just straightforward being stolen from. I mean, you don&#8217;t want any of them to happen, but you have to have a hierarchy of harm.



SULLIVAN: Yes.



MARTIN: So that&#8217;s your risk assessment.



The testing side, I think, is slightly different. One of the paradoxes, I think, of cybersecurity is for such a scientific, data-rich subject, the sort of metrics about what works are very, very hard to come by. So you&#8217;ve got boards and corporate leadership and senior governmental structures, and they say, ‚ÄúLook, how do I run this organization safely and securely?‚Äù And a cybersecurity chief within the organization will say, ‚ÄúWell, we could get this capability in.‚Äù Well, the classic question for a leadership team to ask is, well, what risk and harm will this reduce, by how much, and what&#8217;s the cost-benefit analysis? And we find that really hard.



So that&#8217;s really where testing and assurance comes in. And also as technology changes so fast, we have to figure out, well, if we&#8217;re worried about post-quantum cryptography, for example, what standards does it have to meet? How do you assess whether it&#8217;s meeting those standards? So it&#8217;s a huge issue in cybersecurity and one that we&#8217;re always very conscious of. It‚Äôs really hard.



SULLIVAN: Given the scope of cybersecurity, are there any differences in testing, let&#8217;s say, for maybe a small business versus a critical infrastructure operator? Are there any, sort of, metrics we can look at in terms of distinguishing risk or assessment?



MARTIN: There have to be. One of the reasons I think why we have to be is that no small business can be expected to take on a hostile nation-state that&#8217;s well equipped. You have to be realistic.



If you look at government guidance, certainly in the UK 15 years ago on cybersecurity, you were telling small businesses that are living hand to mouth, week by week, trying to make payments at the end of each month, we were telling them they needed sort of nation-state-level cyber defenses. That was never going to happen, even if they could afford it, which they couldn&#8217;t. So you have to have some differentiation. So again, you&#8217;ve got assessment frameworks and so forth where you have to meet higher standards. So there absolutely has to be that distinction. Otherwise, you end up in a crazy world of crippling small businesses with just unmanageable requirements which they&#8217;re never going to meet.



SULLIVAN: It&#8217;s such a great point. You touched on this a little bit earlier, as well, but just cybersecurity governance operates in a fast-moving technology and threat environment. How have testing standards evolved, and where do new technical standards usually originate?



MARTIN: I keep saying this is very difficult, and it is. [LAUGHTER] So I think there are two challenges. One is actually about the balance, and this applies to the technology of today as well as the technology of tomorrow. This is about, how do you make sure things are good enough without crowding out new entrants? You want people to be innovative and dynamic. You want disruptors in this business.



But if you say to them, ‚ÄúLook, well, you have to meet these 14 impossibly high technical standards before you can even sell to anybody or sell to the government,‚Äù whatever, then you&#8217;ve got a problem. And I think we&#8217;ve wrestled with that, and there&#8217;s no perfect answer. You just have to try and go to ‚Ä¶ find the sweet spot between two ends of a spectrum. And that&#8217;s going to evolve.



The second point, which in some respects if you&#8217;ve got the right capabilities is slightly easier but still a big call, is around, you know, those newer and evolving technologies. And here, having, you know, been a bit sort of gloomy and pessimistic, here I think is actually an opportunity. So one of the things we always say in cybersecurity is that the internet was built and developed without security in mind. And that was kind of true in the ‚Äô90s and the noughties, as we call them over here.



But I think as you move into things like post-quantum computing, applied use of AI, and so on, you can actually set the standards at the beginning. And that&#8217;s really good because it&#8217;s saying to people that these are the things that are going to matter in the post-quantum age. Here&#8217;s the outline of the standards you&#8217;re going to have to meet; start looking at them. So there&#8217;s an opportunity actually to make technology safer by design, by getting ahead of it. And I think that&#8217;s the era we&#8217;re in now.



SULLIVAN: That makes a lot of sense. Just building on that, do businesses and the public trust these standards? And I guess, which standard do you wish the world would just adopt already, and what&#8217;s the real reason they haven&#8217;t?



MARTIN: Well, again, where do you start? I mean, most members of the public quite rightly haven&#8217;t heard of any of these standards. I think public trust and public capital in any society matters. But I think it is important that these things are credible.



And there&#8217;s quite a lot of convergence between, you know, the top-level frameworks. And obviously in the US, you know, the NIST [National Institute of Standards and Technology] framework is the one that&#8217;s most popular for cybersecurity, but it bears quite a strong resemblance to the international one, ISO[/IEC] 27001, and there are others, as well. But fundamentally, they boil down to kind of five things. Do a risk assessment; work out what your crown jewels are. Protect your perimeter as best you can. Those are the first two.



The third one then is when your perimeter&#8217;s breached, be able to detect it more times than not. And when you can&#8217;t do that, you go to the fourth one, which is, can you mitigate it? And when all else fails, how quickly can you recover and manage it? I mean, all the standards are expressed in way more technical language than that, but fundamentally, if everybody adopted those five things and operated them in a simple way, you wouldn&#8217;t eliminate the harm, but you would reduce it quite substantially.



SULLIVAN: Which policy initiatives are most promising for incentivizing companies to undertake, you know, these cybersecurity testing parameters that you‚Äôve just outlined? Governments, including the UK, have used carrots and sticks, but what do you think will actually move the needle?



MARTIN: I think there are two answers to that, and it comes back to your split between smaller businesses and critically important businesses. In the critically important services, I think it&#8217;s easier because most industries are looking for a level playing field. In other words, they realize there have to be rules and they want to apply them to everyone.



We had a fascinating experience when I was in government back in around 2018 where the telecom sector, they came to us and they said, we&#8217;ve got a very good cooperative relationship with the British government, but it needs to be put on a proper legal footing because you&#8217;re just asking us nicely to do expensive things. And in a regulated sector, if you actually put in some rules‚Äîand please develop them jointly with us; that&#8217;s the crucial part‚Äîthen that will help because it means that we&#8217;re not going to our boards and saying, or our shareholders, and saying that we should do this, and they&#8217;re saying, ‚ÄúWell, do you have to do it? Are our competitors doing it?‚Äù And if the answer to that is, yes, we have to, and, yes, our competitors are doing it, then it tends to be OK.



The harder nut to crack is the smaller business. And I think there&#8217;s a real mystery here: why has nobody cracked a really good and easy solution for small business? We need to be careful about this because, you know, you can&#8217;t throttle small businesses with onerous regulation. At the same time, we&#8217;re not brilliant, I think, in any part of the world at using the normal corporate governance rules to try and get people to figure out how to do cybersecurity.



There are initiatives there that are not the sort of pretty heavy stick that you might have to take to a critical function, but they could help. But that is a hard nut to crack. And I look around the world, and, you know, I think if this was easy, somebody would have figured it out by now. I think most of the developed economies around the world really struggle with cybersecurity for smaller businesses.



SULLIVAN: Yeah, it&#8217;s a great point. Actually building on one of the comments you made on the role of, kind of, government, how do you see the role of private-public partnerships scaling and strengthening, you know, robust cybersecurity testing?



MARTIN: I think they&#8217;re crucial, but they have to be practical. I&#8217;ve got a slight, sort of, high horse on this, if you don&#8217;t mind, Kathleen. It&#8217;s sort of ‚Ä¶ [LAUGHS]



SULLIVAN: Of course.



MARTIN: I think that there are two types of public-private partnership. One involves committees saying that we should strengthen partnerships and we should all work together and collaborate and share stuff. And we tried that for a very long time, and it didn&#8217;t get us very far. There are other types.



We had some at the National Cyber Security Centre where we paid companies to do spectacularly good technical work that the market wouldn&#8217;t provide. So I think it&#8217;s sort of partnership with a purpose. I think sometimes, and I understand the human instinct to do this, particularly in governments and big business, they think you need to get around a table and work out some grand strategy to fix everything, and the scale of the ‚Ä¶ not just the problem but the scale of the whole technology is just too big to do that.



So pick a bit of the problem. Find some ways of doing it. Don&#8217;t over-lawyer it. [LAUGHTER] I think sometimes people get very nervous. Oh, well, is this our role? You know, should we be doing this, that, and the other? Well, you know, sometimes certainly in this country, you think, well, who&#8217;s actually going to sue you over this, you know? So I wouldn&#8217;t over-programmatize it. Just get stuck practically into solving some problems.



SULLIVAN: I love that. Actually, [it] made me think, are there any surprising allies that you&#8217;ve gained‚Äîyou know, maybe someone who you never expected to be a cybersecurity champion‚Äîthrough your work?



MARTIN: Ooh! That&#8217;s a ‚Ä¶ that&#8217;s a‚Ä¶ what a question! To give you a slightly disappointing answer, but it relates to your previous question. In the early part of my career, I was working in institutions like the UK Treasury long before I was in cybersecurity, and the treasury and the British civil service in general, but the treasury in particular sort of trained you to believe that the private sector was amoral, not immoral, amoral. It just didn&#8217;t have values. It just had bottom line, and, you know, its job essentially was to provide employment and revenue then for the government to spend on good things that people cared about. And when I got into cybersecurity and people said, look, you need to develop relations with this cybersecurity company, often in the US, actually. I thought, well, what&#8217;s in it for them?



And, sure, sometimes you were paying them for specific services, but other times, there was a real public spiritedness about this. There was a realization that if you tried to delineate public-private boundaries, that it wouldn&#8217;t really work. It was a shared risk. And you could analyze where the boundaries fell or you could actually go on and do something about it together. So I was genuinely surprised at the allyship from the cybersecurity sector. Absolutely, I really, really was. And I think it&#8217;s a really positive part of certainly the UK cybersecurity ecosystem.



SULLIVAN: Wonderful. Well, we&#8217;re coming to the end of our time here, but is there any maybe last thoughts or perhaps requests you have for our listeners today?



MARTIN: I think that standards, assurance, and testing really matter, but it&#8217;s a bit like the discussion we&#8217;re having over AI. Get all these things to take you 80, 90% of the way and then really apply your judgment. There&#8217;s been some bad regulation under the auspices of standards and assurance. First of all, it‚Äôs, have you done this assessment? Have you done that? Have you looked at this? Well, fine. And you can tick that box, but what does it actually mean when you do it? What bits that you know in your heart of hearts are really important to the defense of your organization that may not be covered by this and just go and do those anyway. Because sure it helps, but it&#8217;s not everything.



SULLIVAN: No. Great, great closing sentiment. Well, Ciaran, thank you for joining us today. This has been just a super fun conversation and really insightful. Just really enjoyed the conversation. Thank you.



MARTIN: My pleasure, Kathleen, thank you.



[TRANSITION MUSIC]



SULLIVAN: Now, I&#8217;m happy to introduce Tori Westerhoff. As a principal director on the Microsoft AI Red Team, Tori leads all AI security and safety red team operations, as well as dangerous capability testing, to directly inform C-suite decision-makers.



So, Tori, welcome!



TORI WESTERHOFF: Thanks. I am so excited to be here.



SULLIVAN: I&#8217;d love to just start a little bit more learning about your background. You&#8217;ve worn some very intriguing hats. I mean, cognitive neuroscience grad from Yale, national security consultant, strategist in augmented and virtual reality ‚Ä¶ how do those experiences help shape the way you lead the Microsoft AI Red Team?



WESTERHOFF: I always joke this is the only role I think will always combine the entire patchwork LinkedIn r√©sum√©. [LAUGHS]



I think I use those experiences to help me understand the really broad approach that AI Red Team‚Äîartist also known as AIRT; I&#8217;m sure I&#8217;ll slip into our acronym‚Äîhow we frame up the broad security implications of AI. So I think the cognitive neuroscience element really helped me initially approach AI hacking, right. There&#8217;s a lot of social engineering and manipulation within chat interfaces that are enabled by AI. And also, kind of, this, like, metaphor for understanding how to find soft spots in the way that you see human heuristics show up, too. And so I think that was actually my personal ‚Äúin‚Äù to getting hooked into AI red teaming generally.



But my experience in national security and I&#8217;d also say working through the AR/VR/metaverse space at the time where I was in it helped me balance both how our impact is framed, how we&#8217;re thinking about critical industries, how we&#8217;re really trying to push our understanding of where security of AI can help people the most. And also do it in a really breakneck speed in an industry that&#8217;s evolving all of the time, that&#8217;s really pushing you to always be at the bleeding edge of your understanding. So I draw a lot of the energy and the mission criticality and the speed from those experiences as we&#8217;re shaping up how we approach it.



SULLIVAN: Can you just give us a quick rundown? What does the Red Team do? What actually, kind of, is involved on a day-to-day basis? And then as we think about, you know, our engagements with large enterprises and companies, how do we work alongside some of those companies in terms of testing?



WESTERHOFF: The way I see our team is almost like an indicator light that works really part and parcel with product development. So the way we&#8217;ve organized our expert red teaming efforts is that we work with product development before anything ships out to anyone who can use it. And our job is to act as expert AI manipulators, AI hackers. And we are supposed to take the theories and methods and new research and harness it to find examples of vulnerabilities or soft spots in products to enable product teams to harden those soft spots before anything actually reaches someone who wants to use it.



So if we&#8217;re the indicator light, we are also not the full workup, right. I see that as measurement and evals. And we also are not the mechanic, which is that product development team that&#8217;s creating mitigations. It&#8217;s platform-security folks who are creating mitigations at scale. And there&#8217;s a really great throughput of insights from those groups back into our area where we love to inform about them, but we also love to add on to, how do we break the next thing, right? So it&#8217;s a continuous cycle.



And part of that is just being really creative and thinking outside of a traditional cybersecurity box. And part of that is also really thinking about how we pull in research‚Äîwe have a research function within our AI Red Team‚Äîand how we automate and scale. This year, we&#8217;ve pulled a lot of those assets and insights into the Azure [AI] Foundry AI Red Teaming Agent (opens in new tab). And so folks can now access a lot of our mechanisms through that. So you can get a little taste of what we do day to day in the AI Red Teaming Agent.



SULLIVAN: You recently‚Äîactually, with your team‚Äîpublished a report that outlined lessons from testing over a hundred generative AI products. But could you share a bit about what you learned? What were some of the important lessons? Where do you see opportunities to improve the state of red teaming as a method for probing AI safety?



WESTERHOFF: I think the most important takeaway from those lessons is that AI security is truly a team sport. You&#8217;ll hear cybersecurity folks say that a lot. And part of the rationale there is that the defense in depth and integrating and a view towards AI security through the entire development of AI systems is really the way that we&#8217;re going to approach this with intentionality and responsibility.



So in our space, we really focus on novel harm categories. We are pushing bleeding edge, and we also are pushing iterative and, like, contextually based red teaming in product dev. So outside of those hundred that we&#8217;ve done, there&#8217;s a community [LAUGHS] through the entire, again, multistage life cycle of a product that is really trying to push the cost of attacking those AI systems higher and higher with all of the expertise they bring. So we may be, like, the experts in AI hacking in that line, but there are also so many partners in the Microsoft ecosystem who are thinking about their market context or they really, really know the people who love their products. How are they using it?



And then when you bubble out, you also have industry and government who are working together to push towards the most secure AI implementation for people, right? And I think our team in particular, we feel really grateful to be part of the big AI safety and security ecosystem at Microsoft and also to be able to contribute to the industry writ large. 



SULLIVAN: As you know, we had a chance to speak with Professor Ciaran Martin from the University of Oxford about the cybersecurity industry and governance there. What are some of the ideas and tools from that space that are surfacing in how we think about approaching red teaming and AI governance broadly?



WESTERHOFF: Yeah, I think it&#8217;s such a broad set of perspectives to bring in, in the AI instance. Something that I&#8217;ve noticed interjecting into security at the AI junction, right, is that cybersecurity has so many decades of experience of working through how to build trustworthy computing, for example, or bring an entire industry to bear in that way. And I think that AI security and safety can learn a lot of lessons of how to bring clarity and transparency across the industry to push universal understanding of where the threats really are.



So frameworks coming out of NIST, coming out of MITRE that help us have a universal language that inform governance, I think, are really important because it brings clarity irrespective of where you are looking into AI security, irrespective of your company size, what you&#8217;re working on. It means you all understand, ‚ÄúHey, we are really worried about this fundamental impact.‚Äù And I think cybersecurity has done a really good job of driving towards impact as their organizational vector. And I am starting to see that in the AI space, too, where we&#8217;re trying to really clarify terms and threats.&nbsp;And you see it in updates of those frameworks, as well, that I really love.



So I think that the innovation is in transparency to folks who are really innovating and doing the work so we all have a shared language, and from that, it really creates communal goals across security instead of a lot of people being worried about the same thing and talking about it in a different way.



SULLIVAN: Mm-hmm. In the cybersecurity context, Ciaran really stressed matching risk frameworks to an organization&#8217;s role and scale. Microsoft plays many roles, including building models and shipping applications. How does your red teaming approach shift across those layers?&nbsp;



WESTERHOFF: I love this question also because I love it as part of our work. So one of the most fascinating things about working on this team has been the diversity of the technology that we end up red teaming and testing. And it feels like we&#8217;re in the crucible in that way. Because we see AI applied to so many different architectures, tech stacks, individual features, models, you name it.



Part of my answer is that we still care about the highest-impact things. And so irrespective of the iteration, which is really fascinating and I love, I still think that our team drives to say, ‚ÄúOK, what is that critical vulnerability that is going to affect people in the largest ways, and can we battle test to see if that can occur?‚Äù



So in some ways, the task is always the same. I think in the ways that we change our testing, we customize a lot to the access to systems and data and also people&#8217;s trust almost as different variables that could affect the impact, right.



So a good example is if we&#8217;re thinking through agentic frameworks that have access to functions and tools and preferential ability to act on data, it&#8217;s really different to spaces where that action may not be feasible, right. And so I think the tailoring of the way to get to that impact is hyper-custom every time we start an engagement. And part of it is very thesis driven and almost mechanizing empathy.



You almost need to really focus on how people could use, or misuse, in such a way that you can emulate it before to a really great signal to product development, to say this is truly what people could do and we want to deliver the highest-impact scenarios so you can solve for those and also solve the underlying patterns, actually, that could contribute to maybe that one piece of evidence but also all the related pieces of evidence. So singular drive but like hyper-, hyper-customization to what that piece of tech could do and has access to.



SULLIVAN: What are some of the unexplored testing approaches or considerations from cybersecurity that you think we should encourage AI technologists, policymakers, and other stakeholders to focus on? 



WESTERHOFF: I do love that AI humbles us each and every day with new capabilities and the potential for new capabilities. It&#8217;s not just saying, ‚ÄúHey, there&#8217;s one test that we want to try,‚Äù but more, ‚ÄúHey, can we create a methodology that we feel really, really solid about so that when we are asked a question we haven&#8217;t even thought of, we feel confident that we have the resources and the system?‚Äù



So part of me is really intrigued by the process that we&#8217;re asked to make without knowing what those capabilities are really going to bring. And then I think tactically, AIRT is really pushing on how we create new research methodologies. How are we investing in, kind of, these longer-term iterations of red teaming? So we&#8217;re really excited about pushing out those insights in an experimental and longer-term way.



I think another element is a little bit of that evolution of how industry standards and frameworks are updating to the AI moment and really articulating where AI is either furthering adversarial ability to create those harms or threats or identifying where AI has a net new harm. And I think that demystifies a little bit about what we talked about in terms of the lessons learned, that fundamentally, a lot of the things that we talk about are traditional security vulnerabilities, and we are standing on kind of that cybersecurity shoulder. And I&#8217;m starting to see those updates translate in spaces that are already considered trustworthy and kind of the basis on which not only cybersecurity folks build their work but also business decision-makers make decisions on those frameworks.



So to me, integration of AI into those frameworks by those same standards means that we&#8217;re evolving security to include AI. We aren&#8217;t creating an entirely new industry of AI security and that, I think, really helps anchor people in the really solid foundation that we have in cybersecurity anyways.



I think there&#8217;s also some work around how the cyber, like, defenses will actually benefit from AI. So we think a lot about threats because that&#8217;s our job. But the other side of cybersecurity is offense. And I&#8217;m seeing a ton of people come out with frameworks and methodologies, especially in the research space, on how defensive networks are going to be benefited from things like agentic systems.



Generally speaking, I think the best practice is to realize that we&#8217;re fundamentally still talking about the same impacts, and we can use the same avenues, conversations, and frameworks. We just really want them to be crisply updated with that understanding of AI applications.



SULLIVAN: How do you think about bringing others into the fold there? I think those standards and frameworks are often informed by technologists. But I&#8217;d love for you to expand [that to] policymakers or other kind of stakeholders in our ecosystem, even, you know, end consumers of these products. Like, how do we communicate some of this to them in a way that resonates and it has an impactful meaning?



WESTERHOFF: I&#8217;ve found the AI security-safety space to be one of the more collaborative. I actually think the fact that I&#8217;m talking to you today is probably evidence that a ton of people are bringing in perspectives that don&#8217;t only come from a long-term cybersecurity view. And I see that as a trend in how AI is being approached opposed to how those areas were moving earlier. So I think that speed and the idea of conversations and not always having the perfect answer but really trying to be transparent with what everyone does know is kind of a communal energy in the communities, at least, where we&#8217;re playing. [LAUGHS] So I am pretty biased but at least the spaces where we are.



SULLIVAN: No, I think we&#8217;re seeing that across the board. I mean, I&#8217;d echo [that] sitting in research, as well, like, that ability to have impact now and at speed to getting the amazing technology and models that we&#8217;re creating into the hands of our customers and partners and ecosystem is just underscored.



So on the note of speed, let&#8217;s shift gears a little bit to just a quick lightning round. I&#8217;d love to get maybe some quick thoughts from you, just 30-second answers here. I&#8217;ll start with one.



Which headline-grabbing AI threat do you think is mostly hot air?



WESTERHOFF: I think we should pay attention to it all. I&#8217;m a red team lead. I love a good question to see if we can find an answer in real life. So no hot air, just questions.



SULLIVAN: Is there some sort of maybe new tool that you can&#8217;t wait to sneak into the red team arsenal?



WESTERHOFF: I think there are really interesting methodologies that break our understanding of cybersecurity by looking at the intersection between different layers of AI and how you can manipulate AI-to-AI interaction, especially now when we&#8217;re looking at agentic systems. So I would say a method, not a tool.



SULLIVAN: So maybe ending on a little bit of a lighter note, do you have a go-to snack during an all-night red teaming session?



WESTERHOFF: Always coffee. I would love it to be a protein smoothie, but honestly, it is probably Trader Joe&#8217;s elote chips. Like the whole bag. [LAUGHTER] It‚Äôs going to get me through. I&#8217;m going to not love that I did it.



[MUSIC]



SULLIVAN: Amazing. Well, Tori, thanks so much for joining us today, and just a huge thanks also to Ciaran for his insights, as well.



WESTERHOFF: Thank you so much for having me. This was a joy.



SULLIVAN: And to our listeners, thanks for tuning in. You can find resources related to this podcast in the show notes. And if you want to learn more about how Microsoft approaches AI governance, you can visit microsoft.com/RAI.



See you next time!‚ÄØ



[MUSIC FADES]

				
			
			
				Show more			
		
	





AI Testing and Evaluation podcast series

Opens in a new tabThe post AI Testing and Evaluation: Learnings from cybersecurity appeared first on Microsoft Research.
‚Ä¢ Build real-time travel recommendations using AI agents on Amazon Bedrock
  Generative AI is transforming how businesses deliver personalized experiences across industries, including travel and hospitality. Travel agents are enhancing their services by offering personalized holiday packages, carefully curated for customer‚Äôs unique preferences, including accessibility needs, dietary restrictions, and activity interests. Meeting these expectations requires a solution that combines comprehensive travel knowledge with real-time pricing and availability information. 
In this post, we show how to build a generative AI solution using Amazon Bedrock that creates bespoke holiday packages by combining customer profiles and preferences with real-time pricing data. We demonstrate how to use Amazon Bedrock Knowledge Bases for travel information, Amazon Bedrock Agents for real-time flight details, and Amazon OpenSearch Serverless for efficient package search and retrieval. 
Solution overview 
Travel agencies face increasing demands for personalized recommendations while struggling with real-time data accuracy and scalability. Consider a travel agency that needs to offer accessible holiday packages: they need to match specific accessibility requirements with real-time flight and accommodation availability but are constrained by manual processing times and outdated information in traditional systems. This AI-powered solution combines personalization with real-time data integration, enabling the agency to automatically match accessibility requirements with current travel options, delivering accurate recommendations in minutes rather than hours.The solution uses a three-layer architecture to help travel agents create personalized holiday recommendations: 
 
 Frontend layer ‚Äì Provides an interface where travel agents input customer requirements and preferences 
 Orchestration layer ‚Äì Processes request and enriches them with customer data 
 Recommendation layer ‚Äì Combines two key components: 
   
   Travel data storage ‚Äì Maintains a searchable repository of travel packages 
   Real-time information retrieval ‚Äì Fetches current flight details through API integration 
    
 
The following diagram illustrates this architecture. 
 
With this layered approach, travel agents can capture customer requirements, enrich them with stored preferences, integrate real-time data, and deliver personalized recommendations that match customer needs. The following diagram illustrates how these components are implemented using AWS services. 
 
The AWS implementation includes: 
 
 Amazon API Gateway ‚Äì Receives requests and routes them to AWS Lambda functions facilitating secure API calls for retrieving recommendations 
 AWS Lambda ‚Äì Processes input data, creates the enriched prompt, and executes the recommendation workflow 
 Amazon DynamoDB ‚Äì Stores customer preferences and travel history 
 Amazon Bedrock Knowledge Bases ‚Äì Helps travel agents build a curated database of destinations, travel packages, and deals, making sure recommendations are based on reliable and up-to-date information 
 Amazon OpenSearch Serverless ‚Äì Enables simple, scalable, and high-performing vector search 
 Amazon Simple Storage Service (Amazon S3) ‚Äì Stores large datasets such as flight schedules and promotional materials 
 Amazon Bedrock Agents ‚Äì Integrates real-time information retrieval, making sure recommended itineraries reflect current availability, pricing, and scheduling through external API integrations 
 
This solution uses a AWS CloudFormation template that automatically provisions and configures the required resources. The template handles the complete setup process, including service configurations and necessary permissions. 
For the latest information about service quotas that might affect your deployment, refer to AWS service quotas. 
Prerequisites 
To deploy and use this solution, you must have the following: 
 
 An AWS account with access to Amazon Bedrock 
 Permissions to create and manage the following services: 
   
   Amazon Bedrock 
   Amazon OpenSearch Serverless 
   Lambda 
   DynamoDB 
   Amazon S3 
   API Gateway 
    
 Access to foundation models in Amazon Bedrock for Amazon Titan Text Embeddings V2 and Anthropic Claude 3 Haiku models 
 
Deploy the CloudFormation stack 
You can deploy this solution in your AWS account using AWS CloudFormation. Complete the following steps: 
 
 Choose Launch Stack: 
 
 
You will be redirected to the Create stack wizard on the AWS CloudFormation console with the stack name and the template URL already filled in. 
 
 Leave the default settings and complete the stack creation. 
 Choose View stack events to go to the AWS CloudFormation console to see the deployment details. 
 
The stack takes around 10 minutes to create the resources. Wait until the stack status is CREATE_COMPLETE before continuing to the next steps. 
The CloudFormation template automatically creates and configures components for data storage and management, Amazon Bedrock, and the API and interface. 
Data storage and management 
The template sets up the following data storage and management resources: 
 
 An S3 bucket and with a sample dataset (travel_data.json and promotions.csv), prompt template, and the API schema 
 
 
 
 DynamoDB tables populated with sample user profiles and travel history 
 
 
 
 An OpenSearch Serverless collection with optimized settings for travel package searches 
 
 
 
 A vector index with settings compatible with the Amazon Bedrock knowledge base 
 
 
Amazon Bedrock configuration 
For Amazon Bedrock, the CloudFormation template creates the following resources: 
 
 A knowledge base with the travel dataset and data sources ingested from Amazon S3 with automatic synchronization 
 
 
 
 An Amazon Bedrock agent, which is automatically prepared 
 
 
 
 A new version and alias for the agent 
 
 
 
 Agent action groups with mock flight data integration 
 
 
 
 An action group invocation, configured with the FlightPricingLambda Lambda function and the API schema retrieved from the S3 bucket 
 
 
API and interface setup 
To enable API access and the UI, the template configures the following resources: 
 
 API Gateway endpoints 
 Lambda functions with a mock flight API for demonstration purposes 
 A web interface for travel agents 
 
Verify the setup 
After stack creation is complete, you can verify the setup on the Outputs tab of the AWS CloudFormation console, which provides the following information: 
 
 WebsiteURL ‚Äì Access the travel agent interface 
 ApiEndpoint ‚Äì Use for programmatic access to the recommendation system 
 
 
Test the endpoints 
The web interface provides an intuitive form where travel agents can input customer requirements, including: 
 
 Customer ID (for example, Joe or Will) 
 Travel budget 
 Preferred dates 
 Number of travelers 
 Travel style 
 
 
You can call the API directly using the following code: 
 
 curl -X POST \
&nbsp;&nbsp;&lt;ApiEndpoint&gt; \
&nbsp;&nbsp;-H 'Content-Type: application/json' \
&nbsp;&nbsp;-d '{
&nbsp;&nbsp; &nbsp;"userId": "Joe",
&nbsp;&nbsp; &nbsp;"budget": "3000 GBP",
&nbsp;&nbsp; &nbsp;"duration": "7 days",
&nbsp;&nbsp; &nbsp;"travelDate": "2025-07-15",
&nbsp;&nbsp; &nbsp;"numberOfTravelers": 2
&nbsp;&nbsp;}' 
 
Test the solution 
For demonstration purposes, we create sample user profiles in the UserPreferences and TravelHistory tables in DynamoDB. 
The UserPreferences table stores user-specific travel preferences. For instance, Joe represents a luxury traveler with wheelchair accessibility requirements. 
 
Will represents a budget traveler with elderly-friendly needs. These profiles help showcase how the system handles different customer requirements and preferences. 
 
The TravelHistory table stores past trips taken by users. The following tables show the past trips taken by the user Joe, showing destinations, trip durations, ratings, and travel dates. 
 
Let‚Äôs walk through a typical use case to demonstrate how a travel agent can use this solution to create personalized holiday recommendations.Consider a scenario where a travel agent is helping Joe, a customer who requires wheelchair accessibility, plan a luxury vacation. The travel agent enters the following information: 
 
 Customer ID: Joe 
 Budget: 4,000 GBP 
 Duration: 5 days 
 Travel dates: July 15, 2025 
 Number of travelers: 2 
 Travel style: Luxury 
 
 
When a travel agent submits a request, the system orchestrates a series of actions through the PersonalisedHolidayFunction Lambda function, which will query the knowledge base, check real-time flight information using the mock API, and return personalized recommendations that match the customer‚Äôs specific needs and preferences. The recommendation layer uses the following prompt template: 
 
 Based on the profile and requirements:

User Preferences:
- Travel Preferences: {travelStyle}
- Interests: {interests}
- Dietary Restrictions: {dietaryRestrictions}
- Accessibility Needs: {accessibility}

Current Request:
- Budget: {budget}
- Duration: {duration}
- Travel Date: {travelDate}
- Number of Travelers: {numberOfTravelers}

Previous Destinations: {previousDestinations}

Instructions:
1. Match the user's budget, travel style and interests
2. Consider dietary restrictions and accessibility needs
3. Avoid previously visited destinations
4. Include:
&nbsp;&nbsp; - Recommended destinations
&nbsp;&nbsp; - Suitable accommodations
&nbsp;&nbsp; - Relevant activities and experiences
&nbsp;&nbsp; - Transportation options
&nbsp;&nbsp; - Estimated cost breakdown
&nbsp;&nbsp; - Travel tips

Please follow the &lt;Instructions&gt; and provide a personalized holiday recommendation in the below format:
Destination: [Primary recommended destination]

[Detailed recommendation] 
 
The system retrieves Joe‚Äôs preferences from the user profile, including: 
 
 {
&nbsp;&nbsp; &nbsp;"userPreferences": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"preferences": "Prefer warm climate and cultural experiences",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"budget": 3000,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"duration": "5 days",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"travelDate": "2025-03-04",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"interests": [
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"photography",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"food",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"beach"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;],
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"travelStyle": "Luxury",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"numberOfTravelers": 2,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"dietaryRestrictions": [
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"plant based",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"vegetarian"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;],
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"accessibility": [
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"wheelchair-accessible"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;],
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"previousDestinations": [
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"Maldives",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"Bali"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;]
&nbsp;&nbsp; &nbsp;}
} 
 
The system then generates personalized recommendations that consider the following: 
 
 Destinations with proven wheelchair accessibility 
 Available luxury accommodations 
 Flight details for the recommended destination 
 
Each recommendation includes the following details: 
 
 Detailed accessibility information 
 Real-time flight pricing and availability 
 Accommodation details with accessibility features 
 Available activities and experiences 
 Total package cost breakdown 
 
Clean up 
To avoid incurring future charges, delete the CloudFormation stack. For more information, see Delete a stack from the CloudFormation console. 
The template includes proper deletion policies, making sure the resources you created, including S3 buckets, DynamoDB tables, and OpenSearch collections, are properly removed. 
Next steps 
To further enhance this solution, consider the following: 
 
 Explore multi-agent capabilities: 
   
   Create specialized agents for different travel aspects (hotels, activities, local transport) 
   Enable agent-to-agent communication for complex itinerary planning 
   Implement an orchestrator agent to coordinate responses and resolve conflicts 
    
 Implement multi-language support using multi-language foundation models in Amazon Bedrock 
 Integrate with customer relationship management (CRM) systems 
 
Conclusion 
In this post, you learned how to build an AI-powered holiday recommendation system using Amazon Bedrock that helps travel agents deliver personalized experiences. Our implementation demonstrated how combining Amazon Bedrock Knowledge Bases with Amazon Bedrock Agents effectively bridges historical travel information with real-time data needs, while using serverless architecture and vector search for efficient matching of customer preferences with travel packages.The solution shows how travel recommendation systems can balance comprehensive travel knowledge, real-time data accuracy, and personalization at scale. This approach is particularly valuable for travel organizations needing to integrate real-time pricing data, handle specific accessibility requirements, or scale their personalized recommendations. This solution provides a practical starting point with clear paths for enhancement based on specific business needs, from modernizing your travel planning systems or handling complex customer requirements. 
Related resources 
To learn more, refer to the following resources: 
 
 Documentation: 
   
   Amazon Bedrock Documentation 
   Automate tasks in your application using AI agents 
   Retrieve data and generate AI responses with Amazon Bedrock Knowledge Bases 
   Amazon OpenSearch Serverless Developer Guide 
   Building Lambda functions with Python 
    
 Code samples: 
   
   Amazon Bedrock RAG with Knowledge Bases and Agents 
   Amazon Bedrock Samples Repository 
   Amazon Bedrock Agent Samples Repository 
    
 Additional learning: 
   
   AWS Machine Learning Blog 
   AWS Training and Certification 
    
 
 
About the Author 
Vishnu Vardhini 
Vishnu Vardhini is a Solutions Architect at AWS based in Scotland, focusing on SMB customers across industries. With expertise in Security, Cloud Engineering and DevOps, she architects scalable and secure AWS solutions. She is passionate about helping customers leverage Machine Learning and Generative AI to drive business value.
‚Ä¢ Deploy a full stack voice AI agent with Amazon Nova Sonic
  AI-powered speech solutions are transforming contact centers by enabling natural conversations between customers and AI agents, shortening wait times, and dramatically reducing operational costs‚Äîall without sacrificing the human-like interaction customers expect. With the recent launch of Amazon Nova Sonic in Amazon Bedrock, you can now build sophisticated conversational AI agents that communicate naturally through voice, without the need for separate speech recognition and text-to-speech components. Amazon Nova Sonic is a speech-to-speech model in Amazon Bedrock that enables real-time, human-like voice conversations. 
Whereas many early Amazon Nova Sonic implementations focused on local development, this solution provides a complete cloud-deployed architecture that you can use as a foundation for building real proof of concept applications. This asset is deployable through the AWS Cloud Development Kit (AWS CDK) and provides a foundation for building further Amazon Nova use cases using preconfigured infrastructure components, while allowing you to customize the architecture to address your specific business requirements. 
In this post, we show how to create an AI-powered call center agent for a fictional company called AnyTelco. The agent, named Telly, can handle customer inquiries about plans and services while accessing real-time customer data using custom tools implemented with the Model Context Protocol (MCP) framework. 
Solution overview 
The following diagram provides an overview of the deployable solution. 
 
The solution is composed of the following layers: 
 
 Frontend layer ‚Äì The frontend layer of this system is built with scalability and performance in mind: 
   
   Amazon CloudFront distribution serves as the content delivery network for the web application. 
   Amazon Simple Storage Service (Amazon S3) hosts static assets. 
   The UI handles audio streaming and user interaction. 
    
 Communication layer ‚Äì The communication layer facilitates seamless real-time interactions: 
   
   Network Load Balancer manages WebSocket connections. WebSockets enable two-way interactive communication sessions between a user‚Äôs browser and the server, which is essential for real-time audio streaming applications. 
   Amazon Cognito provides user authentication and JSON web token (JWT) validation. Amazon Cognito provides user authentication, authorization, and user management for web and mobile applications, alleviating the need to build and maintain your own identity systems. 
    
 Processing layer ‚Äì The processing layer forms the computational backbone of the system: 
   
   Amazon Elastic Container Service (Amazon ECS) runs the containerized backend service. 
   AWS Fargate provides the serverless compute backend. Orchestration is provided by the Amazon ECS engine. 
   The Python backend processes audio streams and manages Amazon Nova Sonic interactions. 
    
 Intelligence layer ‚Äì The intelligence layer uses AI and data technologies to power the core functionalities: 
   
   The Amazon Nova Sonic model in Amazon Bedrock handles speech processing. 
   Amazon DynamoDB stores customer information. 
   Amazon Bedrock Knowledge Bases connects foundation models (FMs) with your organization‚Äôs data sources, allowing AI applications to reference accurate, up-to-date information specific to your business. 
    
 
The following sequence diagram highlights the flow when a user initiates conversation. The user only signs in one time, but authentication Steps 3 and 4 happen every time the user starts a new session. The conversational loop in Steps 6‚Äì12 is repeated throughout the conversational interaction. Steps a‚Äìc only happen when the Amazon Nova Sonic agent decides to use a tool. In scenarios without tool use, the flow goes directly from Step 9 to Step 10. 
 
Prerequisites 
Before getting started, verify that you have the following: 
 
 Python 3.12 
 Node.js v20 
 npm v10.8 
 An AWS account 
 The AWS CDK set up (for prerequisites and installation instructions, see Getting started with the AWS CDK) 
 Amazon Nova Sonic enabled in Amazon Bedrock (for more information, see Add or remove access to Amazon Bedrock foundation models) 
 Chrome or Safari browser environment (Firefox is not supported at the time of writing) 
 A working microphone and speakers 
 
Deploy the solution 
You can find the solution and full deployment instructions on the GitHub repository. The solution uses the AWS CDK to automate infrastructure deployment. Use the following code terminal commands to get started in your AWS Command Line Interface (AWS CLI) environment: 
 
 git clone https://github.com/aws-samples/sample-sonic-cdk-agent.git 
cd nova-s2s-call-center 

# Configure environment variables
cp template.env .env

# Edit .env with your settings

# Deploy the solution 
./deploy.sh  
 
The deployment creates two AWS CloudFormation stacks: 
 
 Network stack for virtual private cloud (VPC) and networking components 
 Stack for application resources 
 
The output of the second stack gives you a CloudFront distribution link, which takes you to the login page. 
 
You can create an Amazon Cognito admin user with the following AWS CLI command: 
 
 aws cognito-idp admin-create-user \
  --user-pool-id YOUR_USER_POOL_ID \
  --username USERNAME \
  --user-attributes Name=email,Value=USER_EMAIL \
  --temporary-password TEMPORARY_PASSWORD \
  --region YOUR_AWS_REGION 
 
The preceding command uses the following parameters: 
 
 YOUR_USER_POOL_ID: The ID of your Amazon Cognito user pool 
 USERNAME: The desired user name for the user 
 USER_EMAIL: The email address of the user 
 TEMPORARY_PASSWORD: A temporary password for the user 
 YOUR_AWS_REGION: Your AWS Region (for example, us-east-1) 
 
Log in with your temporary password from the CloudFront distribution link, and you will be asked to set a new password. 
You can choose Start Session to start a conversation with your assistant. Experiment with prompts and different tools for your use case. 
 
Customizing the application 
A key feature of this solution is its flexibility‚Äîyou can tailor the AI agent‚Äôs capabilities to your specific use case. The sample implementation demonstrates this extensibility through custom tools and knowledge integration: 
 
 Customer information lookup ‚Äì Retrieves customer profile data from DynamoDB using phone numbers as keys 
 Knowledge base search ‚Äì Queries an Amazon Bedrock knowledge base for company information, plan details, and pricing 
 
These features showcase how to enhance the functionality of Amazon Nova Sonic with external data sources and domain-specific knowledge. The architecture is designed for seamless customization in several key areas. 
Modifying the system prompt 
The solution includes a UI in which you can adjust the AI agent‚Äôs behavior by modifying its system prompt. This enables rapid iteration on the agent‚Äôs personality, knowledge base, and conversation style without redeploying the entire application. 
 
Adding new tools 
You can also extend the AI agent‚Äôs capabilities by implementing additional tools using the MCP framework. The process involves: 
 
 Implementing the tool logic, typically as a new Python module 
 Registering the tool with the MCP server by using the @mcp_server.tool custom decorator and defining the tool specification, including its name, description, and input schema in /backend/tools/mcp_tool_registry.py 
 
For example, the following code illustrates how to add a knowledge base lookup tool: 
 
 @mcp_server.tool(
    name="lookup",
    description="Runs query against a knowledge base to retrieve information."
)
async def lookup_tool(
    query: Annotated[str, Field(description="the query to search")]
) -&gt; dict:
    """Look up information in the knowledge base"""
    results = knowledge_base_lookup.main(query)
    return results 
 
The decorator handles registration with the MCP server, and the function body contains your tool‚Äôs implementation logic. 
Expanding the knowledge base 
The solution uses Amazon Bedrock Knowledge Bases to provide the AI agent with company-specific information. You can update this knowledge base with: 
 
 Frequently asked questions and their answers 
 Product catalogs and specifications 
 Company policies and procedures 
 
Clean up 
You can remove the stacks with the following command: 
 
 # move to the cdk folder, assuming you are in the project root folder
cd cdk
# Removes both stacks sequentially
npx cdk destroy --all 
 
Conclusion 
AI agents are transforming how organizations approach customer service, with solutions offering the ability to handle multiple conversations simultaneously, provide consistent service around the clock, and scale instantly while maintaining quality and reducing operational costs. This solution makes those benefits accessible by providing a deployable foundation for Amazon Nova Sonic applications on AWS. The solution demonstrates how AI agents can effectively handle customer inquiries, access real-time data, and provide personalized service‚Äîall while maintaining the natural conversational flow that customers expect. 
By combining the Amazon Nova Sonic model with a robust cloud architecture, secure authentication, and flexible tool integration, organizations can quickly move from concept to proof of concept. This solution is not just helping build voice AI applications, it‚Äôs helping companies drive better customer satisfaction and productivity across a range of industries. 
To learn more, refer to the following resources: 
 
 Introducing Amazon Nova Sonic: Human-like voice conversations for generative AI applications 
 Using the Amazon Nova Sonic Speech-to-Speech model 
 Amazon Nova Sonic Workshop 
 
 
 
About the authors 
Reilly Manton is a Solutions Architect in AWS Telecoms Prototyping. He combines visionary thinking and technical expertise to build innovative solutions. Focusing on generative AI and machine learning, he empowers telco customers to enhance their technological capabilities. 
Shuto Araki is a Software Development Engineer at AWS. He works with customers in telecom industry focusing on AI security and networks. Outside of work, he enjoys cycling throughout the Netherlands. 
Ratan Kumar is a Principal Solutions Architect at Amazon Web Services.A trusted technology advisor with over 20 years of experience working across a range of industry domains, Ratan‚Äôs passion lies in empowering enterprise customers innovate and transform their business by unlocking the potential of AWS cloud. 
Chad Hendren is a Principal Solutions Architect at Amazon Web Services. His passion is AI/ML and Generative AI applied to Customer Experience. He is a published author and inventor with 30 years of telecommunications experience.
‚Ä¢ Manage multi-tenant Amazon Bedrock costs using application inference profiles
  Successful generative AI software as a service (SaaS) systems require a balance between service scalability and cost management. This becomes critical when building a multi-tenant generative AI service designed to serve a large, diverse customer base while maintaining rigorous cost controls and comprehensive usage monitoring. 
Traditional cost management approaches for such systems often reveal limitations. Operations teams encounter challenges in accurately attributing costs across individual tenants, particularly when usage patterns demonstrate extreme variability. Enterprise clients might have different consumption behaviors‚Äîsome experiencing sudden usage spikes during peak periods, whereas others maintain consistent resource consumption patterns. 
A robust solution requires a context-driven, multi-tiered alerting system that exceeds conventional monitoring standards. By implementing graduated alert levels‚Äîfrom green (normal operations) to red (critical interventions)‚Äîsystems can develop intelligent, automated responses that dynamically adapt to evolving usage patterns. This approach enables proactive resource management, precise cost allocation, and rapid, targeted interventions that help prevent potential financial overruns. 
The breaking point often comes when you experience significant cost overruns. These overruns aren‚Äôt due to a single factor but rather a combination of multiple enterprise tenants increasing their usage while your monitoring systems fail to catch the trend early enough. Your existing alerting system might only provide binary notifications‚Äîeither everything is fine or there‚Äôs a problem‚Äîthat lack the nuanced, multi-level approach needed for proactive cost management. The situation is further complicated by a tiered pricing model, where different customers have varying SLA commitments and usage quotas. Without a sophisticated alerting system that can differentiate between normal usage spikes and genuine problems, your operations team might find itself constantly taking reactive measures rather than proactive ones. 
This post explores how to implement a robust monitoring solution for multi-tenant AI deployments using a feature of Amazon Bedrock called application inference profiles. We demonstrate how to create a system that enables granular usage tracking, accurate cost allocation, and dynamic resource management across complex multi-tenant environments. 
What are application inference profiles? 
Application inference profiles in Amazon Bedrock enable granular cost tracking across your deployments. You can associate metadata with each inference request, creating a logical separation between different applications, teams, or customers accessing your foundation models (FMs). By implementing a consistent tagging strategy with application inference profiles, you can systematically track which tenant is responsible for each API call and the corresponding consumption. 
For example, you can define key-value pair tags such as TenantID, business-unit, or ApplicationID and send these tags with each request to partition your usage data. You can also send the application inference profile ID with your request. When combined with AWS resource tagging, these tag-enabled profiles provide visibility into the utilization of Amazon Bedrock models. This tagging approach introduces accurate chargeback mechanisms to help you allocate costs proportionally based on actual usage rather than arbitrary distribution approaches. To attach tags to the inference profile, see Tagging Amazon Bedrock resources and Organizing and tracking costs using AWS cost allocation tags. Furthermore, you can use application inference profiles to identify optimization opportunities specific to each tenant, helping you implement targeted improvements for the greatest impact to both performance and cost-efficiency. 
Solution overview 
Imagine a scenario where an organization has multiple tenants, each with their respective generative AI applications using Amazon Bedrock models. To demonstrate multi-tenant cost management, we provide a sample, ready-to-deploy solution&nbsp;on GitHub. It deploys two tenants with two applications, each within a single AWS Region. The solution uses application inference profiles for cost tracking, Amazon Simple Notification Service (Amazon SNS) for notifications, and Amazon CloudWatch to produce tenant-specific dashboards. You can modify the source code of the solution to suit your needs. 
The following diagram illustrates the solution architecture. 
 
The solution handles the complexities of collecting and aggregating usage data across tenants, storing historical metrics for trend analysis, and presenting actionable insights through intuitive dashboards. This solution provides the visibility and control needed to manage your Amazon Bedrock costs while maintaining the flexibility to customize components to match your specific organizational requirements. 
In the following sections, we walk through the steps to deploy the solution. 
Prerequisites 
Before setting up the project, you must have the following prerequisites: 
 
 AWS account ‚Äì An active AWS account with permissions to create and manage resources such as Lambda functions, API Gateway endpoints, CloudWatch dashboards, and SNS alerts 
 Python environment ‚Äì Python 3.12 or higher installed on your local machine 
 Virtual environment ‚Äì It‚Äôs recommended to use a virtual environment to manage project dependencies 
 
Create the virtual environment 
The first step is to clone the GitHub repo or copy the code into a new project to create the virtual environment. 
 
Update models.json 
Review and update the models.json file to reflect the correct input and output token pricing based on your organization‚Äôs contract, or use the default settings. Verifying you have the right data at this stage is critical for accurate cost tracking. 
 
Update config.json 
Modify config.json to define the profiles you want to set up for cost tracking. Each profile can have multiple key-value pairs for tags. For every profile, each tag key must be unique, and each tag key can have only one value. Each incoming request should contain these tags or the profile name as HTTP headers at runtime. 
As part of the solution, you also configure a unique Amazon Simple Storage Service (Amazon S3) bucket for saving configuration artifacts and an admin email alias that will receive alerts when a particular threshold is breached. 
 
Create user roles and deploy solution resources 
After you modify config.json and models.json, run the following command in the terminal to create the assets, including the user roles: 
python setup.py --create-user-roles 
Alternately, you can create the assets without creating user roles by running the following command: 
python setup.py 
Make sure that you are executing this command from the project directory. Note that full access policies are not advised for production use cases. 
The setup command triggers the process of creating the inference profiles, building a CloudWatch dashboard to capture the metrics for each profile, deploying the inference Lambda function that executes the Amazon Bedrock Converse API and extracts the inference metadata and metrics related to the inference profile, sets up the SNS alerts, and finally creates the API Gateway endpoint to invoke the Lambda function. 
 
When the setup is complete, you will see the inference profile IDs and API Gateway ID listed in the config.json file. (The API Gateway ID will also be listed in the final part of the output in the terminal) 
 
When the API is live and inferences are invoked from it, the CloudWatch dashboard will show cost tracking. If you experience significant traffic, the alarms will trigger an SNS alert email. 
 
For a video version of this walkthrough, refer to Track, Allocate, and Manage your Generative AI cost &amp; usage with Amazon Bedrock. 
You are now ready to use Amazon Bedrock models with this cost management solution. Make sure that you are using the API Gateway endpoint to consume these models and send the requests with the tags or application inference profile IDs as headers, which you provided in the config.json file. This solution will automatically log the invocations and track costs for your application on a per-tenant basis. 
Alarms and dashboards 
The solution creates the following alarms and dashboards: 
 
 BedrockTokenCostAlarm-{profile_name} ‚Äì Alert when total token cost for {profile_name} exceeds {cost_threshold} in 5 minutes 
 BedrockTokensPerMinuteAlarm-{profile_name} ‚Äì Alert when tokens per minute for {profile_name} exceed {tokens_per_min_threshold} 
 BedrockRequestsPerMinuteAlarm-{profile_name} ‚Äì Alert when requests per minute for {profile_name} exceed {requests_per_min_threshold} 
 
You can monitor and receive alerts about your AWS resources and applications across multiple Regions. 
A metric alarm has the following possible states: 
 
 OK ‚Äì The metric or expression is within the defined threshold 
 ALARM ‚Äì The metric or expression is outside of the defined threshold 
 INSUFFICIENT_DATA ‚Äì The alarm has just started, the metric is not available, or not enough data is available for the metric to determine the alarm state 
 
After you add an alarm to a dashboard, the alarm turns gray when it‚Äôs in the INSUFFICIENT_DATA state and red when it‚Äôs in the ALARM state. The alarm is shown with no color when it‚Äôs in the OK state. 
An alarm invokes actions only when the alarm changes state from OK to ALARM. In this solution, an email is sent to through your SNS subscription to an admin as specified in your config.json file. You can specify additional actions when the alarm changes state between OK, ALARM, and INSUFFICIENT_DATA. 
Considerations 
Although the API Gateway maximum integration timeout (30 seconds) is lower than the Lambda timeout (15 minutes), long-running model inference calls might be cut off by API Gateway. Lambda and Amazon Bedrock enforce strict payload and token size limits, so make sure your requests and responses fit within these boundaries. For example, the maximum payload size is 6 MB for synchronous Lambda invocations and the combined request line and header values can‚Äôt exceed 10,240 bytes for API Gateway payloads. If your workload can work within these limits, you will be able to use this solution. 
Clean up 
To delete your assets, run the following command: 
python unsetup.py 
Conclusion 
In this post, we demonstrated how to implement effective cost monitoring for multi-tenant Amazon Bedrock deployments using application inference profiles, CloudWatch metrics, and custom CloudWatch dashboards. With this solution, you can track model usage, allocate costs accurately, and optimize resource consumption across different tenants. You can customize the solution according to your organization‚Äôs specific needs. 
This solution provides the framework for building an intelligent system that can understand context‚Äîdistinguishing between a gradual increase in usage that might indicate healthy business growth and sudden spikes that could signal potential issues. An effective alerting system needs to be sophisticated enough to consider historical patterns, time of day, and customer tier when determining alert levels. Furthermore, these alerts can trigger different types of automated responses based on the alert level: from simple notifications, to automatic customer communications, to immediate rate-limiting actions. 
Try out the solution for your own use case, and share your feedback and questions in the comments. 
 
About the authors 
Claudio Mazzoni is a Sr Specialist Solutions Architect on the Amazon Bedrock GTM team. Claudio exceeds at guiding costumers through their Gen AI journey. Outside of work, Claudio enjoys spending time with family, working in his garden, and cooking Uruguayan food. 
Fahad Ahmed is a Senior Solutions Architect at AWS and assists financial services customers. He has over 17 years of experience building and designing software applications. He recently found a new passion of making AI services accessible to the masses. 
 
Manish Yeladandi&nbsp;is a Solutions Architect at AWS, specializing in AI/ML, containers, and security. Combining deep cloud expertise with business acumen, Manish architects secure, scalable solutions that help organizations optimize their technology investments and achieve transformative business outcomes. 
Dhawal Patel is a Principal Machine Learning Architect at AWS. He has worked with organizations ranging from large enterprises to mid-sized startups on problems related to distributed computing and artificial intelligence. He focuses on deep learning, including NLP and computer vision domains. He helps customers achieve high-performance model inference on Amazon SageMaker. 
James Park&nbsp;is a Solutions Architect at Amazon Web Services. He works with Amazon.com to design, build, and deploy technology solutions on AWS, and has a particular interest in AI and machine learning. In h is spare time he enjoys seeking out new cultures, new experiences, &nbsp;and staying up to date with the latest technology trends. You can find him on LinkedIn. 
Abhi Shivaditya&nbsp;is a Senior Solutions Architect at AWS, working with strategic global enterprise organizations to facilitate the adoption of AWS services in areas such as Artificial Intelligence, distributed computing, networking, and storage. His expertise lies in Deep Learning in the domains of Natural Language Processing (NLP) and Computer Vision. Abhi assists customers in deploying high-performance machine learning models efficiently within the AWS ecosystem.

‚∏ª