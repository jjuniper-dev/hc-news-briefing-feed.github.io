‚úÖ Morning News Briefing ‚Äì September 15, 2025 10:44

üìÖ Date: 2025-09-15 10:44
üè∑Ô∏è Tags: #briefing #ai #publichealth #digitalgov

‚∏ª

üßæ Weather
‚Ä¢ Current Conditions:  6.3¬∞C
  Observed at: Pembroke 6:00 AM EDT Monday 15 September 2025 . Temperature: 6.3&deg;C Pressure / Tendency: 102.7 kPa rising Humidity: 99 % Humidity : 99 % Dewpoint: 6 .2&deg:C Wind: NW 3 km/h . Air Quality Health Index: n/a. Air Quality
‚Ä¢ Monday: Sunny. High 23.
  Fog patches dissipating this morning. Sunny. High 23. Humidex 25. UV index 6 or high. High in the mid to mid-90s . Fog patches are dissipating today morning, but rain is expected later this morning . Forecast issued 5:00 AM EDT Monday 15 September 2025. For the rest of the year, see www.jenn.com/
‚Ä¢ Monday night: Clear. Low 8.
  Fog patches developing overnight . Clear. Clear. Low 8.70s . Clear . Fog patches expected overnight . Forecast issued 5:00 AM EDT Monday 15 September 2025 . Lowest of 10 is expected to be below 8 p.m. Forecast: Clear, sunny, cloudy, warm, warm . Low 5:70s. Clear, cold, sunny . Low forecast:

üåç International News
No updates.

üçÅ Canadian News
No updates.

üá∫üá∏ U.S. Top Stories
‚Ä¢ Melting glaciers mean an uncertain future for Europe's rivers
  Europe is warming faster than any continent . That's speeding up glacial melt, which is already threatening Europe's largest rivers . Europe's biggest rivers are already threatened by the melting of glacial glacial ice . Europe is also becoming more and more vulnerable to the threat posed by melting glaciers, scientists say . The melting of the glaciers is speeding up the threat of Europe's rivers and streams
‚Ä¢ A Utah church embarks on a healing journey in the aftermath of the Kirk assassination
  A church in Orem, Utah, where Charlie Kirk was killed, brings healing during its Sunday service . "We're going to be navigating this as a community and as a church family for months to come," says church leader Marisa Pe√±aloza . The church says it will be "navigating this" for months, "for months, come" to deal with the tragedy
‚Ä¢ A record number of Congressional lawmakers aren't running for reelection in 2026. Here's the list
  NPR tracking the record number of lawmakers in Congress who have already announced they don't plan to run for reelection ahead of the 2026 midterms . NPR is tracking the number of Congressmen who have announced they won't run for re-election in the U.S. The midterms will be held on November 26, 2026, in both chambers of Congress . The midterm elections will
‚Ä¢ Trump, fascinated by the royals, gets a rare second U.K. state visit
  Trump has long expressed fascination with the royal family . The U.K. government will tap into that this week during a state visit that combines pageantry and politics . The state visit is expected to be a political moment for the first time in the U.S. and the second in a row of events in the UK . Trump has expressed interest in the royals and has expressed fascination
‚Ä¢ Is it the magnesium or the massage? The facts about magnesium lotion for sleep
  Social media influencers claim that using lotion with magnesium promotes sleep . But there is little evidence magnesium taken in this way is effective . There are no studies that show magnesium taken with lotion is effective in sleep sleep-inducing forms of type of sleep-inspiring lotions, such as lotion lotions - but there is no evidence this is effective, according to experts at the

üß† Artificial Intelligence
No updates.

üíª Digital Strategy
‚Ä¢ Starlink outage knocks tens of thousands offline worldwide
  Elon Musk's Starlink satellite broadband network went dark today as thousands of users around the globe reported connectivity issues . Downdetector logged 40,000 reports before service flickered back after service went dark . Dowdetector logged 40,500 reports of connectivity issues before service was restored . Elon Musk tweeted that the network was working to fix the problem . Musk's Twitter account
‚Ä¢ Jaguar Land Rover supply chain workers must get Covid-style support, says union
  The UK's chief automotive workers' union is calling on the government to establish a Covid-esque furlough scheme . Thousands of individuals face losing their jobs due to the cyber-related downtime at Jaguar Land Rover . The labor org argues UK goverment should step in to help the thousands of individuals who face losing jobs . The UK government is considering setting up a similar furl
‚Ä¢ Bharti big shots storm BT boardroom after ¬£3.6B raid
  Bharti Global, its largest shareholder, are joining the board with immediate effect . Sign 'relationship agreement' as Mittal and Vittal take non-exec directorships . BT - Britain's former state-owned telecoms monopoly - confirmed that execs will join board with 'immediate effect' Vittal and Mittal will be joined by Bharta Mittal on board
‚Ä¢ UK.gov decides tech projects worth billions are major but not 'mega'
  Ministers concerned Treasury governance team may be distracted about supervising vital efforts . Treasury takes greater role in so-called "mega-projects" UK ministers question government's decision to seemingly downgrade huge public sector tech projects . HM Treasury takes a greater role as Treasury takes more role in 'mega-projects' UK ministers have questioned the decision to downgrade huge tech projects as HM Treasury take a greater
‚Ä¢ UK Lords take aim at Ofcom's 'child-protection' upgrades to Online Safety Act
  Peers will quiz campaigners on whether Ofcom's new measures will actually work, or just add more compliance pain . The House of Lords is about to put the latest child-protection plans of UK regulator the Office of Communications (Ofcom) under the microscope . Ofcom is set to introduce new measures to help children in need of parents' protection . The proposals will be put under the

üè• Public Health
No updates.

üî¨ Science
‚Ä¢ Realizing the full potential of Our Future Health through data linkage and trans-biobank efforts
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Influence of the SARS-CoV-2 pandemic and infection on musculoskeletal function
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Geographic restrictions in stimulus spending mitigated COVID-19 transmission in Seoul
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Functions of the global health system in a new era
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations

üßæ Government & Policy
No updates.

üèõÔ∏è Enterprise Architecture & IT Governance
No updates.

ü§ñ AI & Emerging Tech
‚Ä¢ The Download: America‚Äôs gun crisis, and how AI video models work
  This is today&#8217;s edition of¬†The Download,¬†our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



We can‚Äôt ‚Äúmake American children healthy again‚Äù without tackling the gun crisis



This week, the Trump administration released a strategy for improving the health and well-being of American children. The report was titled‚Äîyou guessed it‚ÄîMake Our Children Healthy Again. It suggests American children should be eating more healthily. And they should be getting more exercise.



But there‚Äôs a glaring omission. The leading cause of death for American children and teenagers isn‚Äôt ultraprocessed food or exposure to some chemical. It‚Äôs gun violence.&nbsp;



This week‚Äôs news of yet more high-profile shootings at schools in the US throws this disconnect into even sharper relief. Experts believe it is time to treat gun violence in the US as what it is: a public health crisis. Read the full story.



‚ÄîJessica Hamzelou



This article first appeared in The Checkup, MIT Technology Review‚Äôs weekly biotech newsletter. To receive it in your inbox every Thursday, and read articles like this first, sign up here.







How do AI models generate videos?



It‚Äôs been a big year for video generation. In the last nine months OpenAI made Sora public, Google DeepMind launched Veo 3, and the video startup Runway launched Gen-4. All can produce video clips that are (almost) impossible to distinguish from actual filmed footage or CGI animation.The downside is that creators are competing with AI slop, and social media feeds are filling up with faked news footage. Video generation also uses up a huge amount of energy, many times more than text or image generation.With AI-generated videos everywhere, let&#8217;s take a moment to talk about the tech that makes them work. Read the full story.‚ÄîWill Douglas Heaven



This article is part of MIT Technology Review Explains, our series untangling the complex, messy world of technology to help you understand what‚Äôs coming next. You can read more from the series here.







Meet our 2025 Innovator of the Year: Sneha Goenka



Up to a quarter of children entering intensive care have undiagnosed genetic conditions. To be treated properly, they must first get diagnoses‚Äîwhich means having their genomes sequenced. This process typically takes up to seven weeks. Sadly, that‚Äôs often too slow to save a critically ill child.Hospitals may soon have a faster option, thanks to a groundbreaking system built in part by Sneha Goenka, an assistant professor of electrical and computer engineering at Princeton‚Äîand MIT Technology Review‚Äôs 2025 Innovator of the Year. Read all about Goenka and her work in this profile.



‚ÄîHelen Thomson



As well as our Innovator of the Year, Goenka is one of the biotech honorees on our 35 Innovators Under 35 list for 2025. Meet the rest of our biotech and materials science innovators, and the full list here.&nbsp;







The must-reads



I‚Äôve combed the internet to find you today‚Äôs most fun/important/scary/fascinating stories about technology.



1 OpenAI and Microsoft have agreed a revised dealBut haven‚Äôt actually revealed any details of said deal. (Axios)+ The news comes as OpenAI keeps pursuing its for-profit pivot. (Ars Technica)+ The world‚Äôs largest startup is going to need more paying users soon. (WSJ $)2 A child has died from a measles complication in Los AngelesThey had contracted the virus before they were old enough to be vaccinated. (Ars Technica)+ Infants are best protected by community immunity. (LA Times $)+ They‚Äôd originally recovered from measles before developing the condition. (CNN)+ Why childhood vaccines are a public health success story. (MIT Technology Review)



3 Ukrainian drone attacks triggered internet blackouts in RussiaThe Kremlin cut internet access in a bid to thwart the mobile-guided drones. (FT $)+ The UK is poised to mass-produce drones to aid Ukraine. (Sky News)+ On the ground in Ukraine‚Äôs largest Starlink repair shop. (MIT Technology Review)



4 Demis Hasabis says AI may slash drug discovery time to under a yearOr perhaps even faster. (Bloomberg $)+ But there‚Äôs good reason to be skeptical of that claim. (FT $)+ An AI-driven ‚Äúfactory of drugs‚Äù claims to have hit a big milestone. (MIT Technology Review)



5 How chatbots alter how we thinkWe shouldn&#8217;t outsource our critical thinking to them. (Undark)+ AI companies have stopped warning you that their chatbots aren‚Äôt doctors. (MIT Technology Review)



6 Fraudsters are threatening small businesses with one-star reviewsOnline reviews can make or break fledgling enterprises, and scammers know it. (NYT $)



7 Why humanoid robots aren‚Äôt taking off any time soonThe industry has a major hype problem. (IEEE Spectrum)+ Chinese tech giant Ant Group showed off its own humanoid machine. (The Verge)+ Why the humanoid workforce is running late. (MIT Technology Review)



8 Encyclopedia Britannica and Merriam-Webster are suing PerplexityIn yet another case of alleged copyright infringement. (Reuters)+ What comes next for AI copyright lawsuits? (MIT Technology Review)



9 Where we‚Äôre most likely to find extraterrestrial life in the next decadeWarning: Hollywood may have given us unrealistic expectations. (BBC)10 Want to build a trillion-dollar company?Then kiss your social life goodbye. (WSJ $)







Quote of the day



&#8220;Nooooo I&#8217;m going to have to use my brain again and write 100% of my code like a caveman from December 2024.&#8221;



‚ÄîA Hacker News commenter jokes about a service outage that left Anthropic users unable to access its AI coding tools, Ars Technica reports.







One more thing







What Africa needs to do to become a major AI playerAfrica is still early in the process of adopting AI technologies. But researchers say the continent is uniquely hospitable to it for several reasons, including a relatively young and increasingly well-educated population, a rapidly growing ecosystem of AI startups, and lots of potential consumers.However, ambitious efforts to develop AI tools that answer the needs of Africans face numerous hurdles. Read our story to learn what they are, and how they could be overcome.



‚ÄîAbdullahi Tsanni







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)+ The fascinating, unexpected origins of everyone‚Äôs favorite pastime‚Äîkaraoke.+ Why the Twilight juggernaut just refuses to die.+ If you‚Äôre among the mass of excited Hollow Knight fans, here‚Äôs a few tips to get through the early stages of the new Silksong game.+ A sloe gin bramble pie sounds like the perfect way to welcome fall.
‚Ä¢ How do AI models generate videos?
  MIT Technology Review Explains: Let our writers untangle the complex, messy world of technology to help you understand what‚Äôs coming next. You can read more from the series here.



It‚Äôs been a big year for video generation. In the last nine months OpenAI made Sora public, Google DeepMind launched Veo 3, the video startup Runway launched Gen-4. All can produce video clips that are (almost) impossible to distinguish from actual filmed footage or CGI animation. This year also saw Netflix debut an AI visual effect in its show The Eternaut, the first time video generation has been used to make mass-market TV.



Sure, the clips you see in demo reels are cherry-picked to showcase a company‚Äôs models at the top of their game. But with the technology in the hands of more users than ever before‚ÄîSora and Veo 3 are available in the ChatGPT and Gemini apps for paying subscribers‚Äîeven the most casual filmmaker can now knock out something remarkable.&nbsp;



The downside is that creators are competing with AI slop, and social media feeds are filling up&nbsp;with faked news footage. Video generation also uses up a huge amount of energy, many times more than text or image generation.&nbsp;



With AI-generated videos everywhere, let&#8217;s take a moment to talk about the tech that makes them work.



How do you generate a video?



Let‚Äôs assume you‚Äôre a casual user. There are now a range of high-end tools that allow pro video makers to insert video generation models into their workflows. But most people will use this technology in an app or via a website. You know the drill: ‚ÄúHey, Gemini, make me a video of a unicorn eating spaghetti. Now make its horn take off like a rocket.‚Äù What you get back will be hit or miss, and you‚Äôll typically need to ask the model to take another pass or 10 before you get more or less what you wanted.&nbsp;









So what‚Äôs going on under the hood? Why is it hit or miss‚Äîand why does it take so much energy? The latest wave of video generation models are what‚Äôs known as latent diffusion transformers. Yes, that‚Äôs quite a mouthful. Let‚Äôs unpack each part in turn, starting with diffusion.&nbsp;



What‚Äôs a diffusion model?



Imagine taking an image and adding a random spattering of pixels to it. Take that pixel-spattered image and spatter it again and then again. Do that enough times and you will have turned the initial image into a random mess of pixels, like static on an old TV set.&nbsp;



A diffusion model is a neural network trained to reverse that process, turning random static into images. During training, it gets shown millions of images in various stages of pixelation. It learns how those images change each time new pixels are thrown at them and, thus, how to undo those changes.&nbsp;



The upshot is that when you ask a diffusion model to generate an image, it will start off with a random mess of pixels and step by step turn that mess into an image that is more or less similar to images in its training set.&nbsp;









But you don‚Äôt want any image‚Äîyou want the image you specified, typically with a text prompt. And so the diffusion model is paired with a second model‚Äîsuch as a large language model (LLM) trained to match images with text descriptions‚Äîthat guides each step of the cleanup process, pushing the diffusion model toward images that the large language model considers a good match to the prompt.&nbsp;



An aside: This LLM isn‚Äôt pulling the links between text and images out of thin air. Most text-to-image and text-to-video models today are trained on large data sets that contain billions of pairings of text and images or text and video scraped from the internet (a practice many creators are very unhappy about). This means that what you get from such models is a distillation of the world as it‚Äôs represented online, distorted by prejudice (and pornography).



It&#8217;s easiest to imagine diffusion models working with images. But the technique can be used with many kinds of data, including audio and video. To generate movie clips, a diffusion model must clean up sequences of images‚Äîthe consecutive frames of a video‚Äîinstead of just one image.&nbsp;



What‚Äôs a latent diffusion model?&nbsp;



All this takes a huge amount of compute (read: energy). That‚Äôs why most diffusion models used for video generation use a technique called latent diffusion. Instead of processing raw data‚Äîthe millions of pixels in each video frame‚Äîthe model works in what‚Äôs known as a latent space, in which the video frames (and text prompt) are compressed into a mathematical code that captures just the essential features of the data and throws out the rest.&nbsp;



A similar thing happens whenever you stream a video over the internet: A video is sent from a server to your screen in a compressed format to make it get to you faster, and when it arrives, your computer or TV will convert it back into a watchable video.&nbsp;





And so the final step is to decompress what the latent diffusion process has come up with. Once the compressed frames of random static have been turned into the compressed frames of a video that the LLM guide considers a good match for the user‚Äôs prompt, the compressed video gets converted into something you can watch.&nbsp;&nbsp;



With latent diffusion, the diffusion process works more or less the way it would for an image. The difference is that the pixelated video frames are now mathematical encodings of those frames rather than the frames themselves. This makes latent diffusion far more efficient than a typical diffusion model. (Even so, video generation still uses more energy than image or text generation. There‚Äôs just an eye-popping amount of computation involved.)&nbsp;



What‚Äôs a latent diffusion transformer?



Still with me? There‚Äôs one more piece to the puzzle‚Äîand that‚Äôs how to make sure the diffusion process produces a sequence of frames that are consistent, maintaining objects and lighting and so on from one frame to the next. OpenAI did this with Sora by combining its diffusion model with another kind of model called a transformer. This has now become standard in generative video.&nbsp;



Transformers are great at processing long sequences of data, like words. That has made them the special sauce inside large language models such as OpenAI‚Äôs GPT-5 and Google DeepMind‚Äôs Gemini, which can generate long sequences of words that make sense, maintaining consistency across many dozens of sentences.&nbsp;



But videos are not made of words. Instead, videos get cut into chunks that can be treated as if they were. The approach that OpenAI came up with was to dice videos up across both space and time. ‚ÄúIt‚Äôs like if you were to have a stack of all the video frames and you cut little cubes from it,‚Äù says Tim Brooks, a lead researcher on Sora.





A selection of videos generated with Veo 3 and Midjourney. The clips have been enhanced in postproduction with Topaz, an AI video-editing tool. Credit: VaigueMan



Using transformers alongside diffusion models brings several advantages. Because they are designed to process sequences of data, transformers also help the diffusion model maintain consistency across frames as it generates them. This makes it possible to produce videos in which objects don‚Äôt pop in and out of existence, for example.&nbsp;



And because the videos are diced up, their size and orientation do not matter. This means that the latest wave of video generation models can be trained on a wide range of example videos, from short vertical clips shot with a phone to wide-screen cinematic films. The greater variety of training data has made video generation far better than it was just two years ago. It also means that video generation models can now be asked to produce videos in a variety of formats.&nbsp;



What about the audio?&nbsp;



A big advance with Veo 3 is that it generates video with audio, from lip-synched dialogue to sound effects to background noise. That‚Äôs a first for video generation models. As Google DeepMind CEO Demis Hassabis put it at this year‚Äôs Google I/O: ‚ÄúWe‚Äôre emerging from the silent era of video generation.‚Äù&nbsp;









The challenge was to find a way to line up video and audio data so that the diffusion process would work on both at the same time. Google DeepMind‚Äôs breakthrough was a new way to compress audio and video into a single piece of data inside the diffusion model. When Veo 3 generates a video, its diffusion model produces audio and video together in a lockstep process, ensuring that the sound and images are synched.¬†¬†



You said that diffusion models can generate different kinds of data. Is this how LLMs work too?&nbsp;



No‚Äîor at least not yet. Diffusion models are most often used to generate images, video, and audio. Large language models‚Äîwhich generate text (including computer code)‚Äîare built using transformers. But the lines are blurring. We‚Äôve seen how transformers are now being combined with diffusion models to generate videos. And this summer Google DeepMind revealed that it was building an experimental large language model that used a diffusion model instead of a transformer to generate text.&nbsp;



Here‚Äôs where things start to get confusing: Though video generation (which uses diffusion models) consumes a lot of energy, diffusion models themselves are in fact more efficient than transformers. Thus, by using a diffusion model instead of a transformer to generate text, Google DeepMind‚Äôs new LLM could be a lot more efficient than existing LLMs. Expect to see more from diffusion models in the near future!
‚Ä¢ We can‚Äôt ‚Äúmake American children healthy again‚Äù without tackling the gun crisis
  Note for readers: This newsletter discusses gun violence, a raw and tragic issue in America. It was already in progress on Wednesday when a school shooting occurred at Evergreen High School in Colorado and Charlie Kirk was shot and killed at Utah Valley University.&nbsp;



Earlier this week, the Trump administration‚Äôs Make America Healthy Again movement released a strategy for improving the health and well-being of American children. The report was titled‚Äîyou guessed it‚ÄîMake Our Children Healthy Again.



Robert F. Kennedy Jr., who leads the Department of Health and Human Services, and his colleagues are focusing on four key aspects of child health: diet, exercise, chemical exposure, and overmedicalization.



Anyone who‚Äôs been listening to RFK Jr. posturing on health and wellness won‚Äôt be surprised by these priorities. And the first two are pretty obvious. On the whole, American children should be eating more healthily. And they should be getting more exercise.



But there‚Äôs a glaring omission. The leading cause of death for American children and teenagers isn‚Äôt ultraprocessed food or exposure to some chemical. It‚Äôs gun violence.&nbsp;



Yesterday‚Äôs news of yet more high-profile shootings at schools in the US throws this disconnect into even sharper relief. Experts believe it is time to treat gun violence in the US as what it is: a public health crisis.





I live in London, UK, with my husband and two young children. We don‚Äôt live in a particularly fancy part of the city‚Äîin one recent ranking of London boroughs from most to least posh, ours came in at 30th out of 33. I do worry about crime. But I don‚Äôt worry about gun violence.



That changed when I temporarily moved my family to the US a couple of years ago. We rented the ground-floor apartment of a lovely home in Cambridge, Massachusetts‚Äîa beautiful area with good schools, pastel-colored houses, and fluffy rabbits hopping about. It wasn‚Äôt until after we‚Äôd moved in that my landlord told me he had guns in the basement.



My daughter joined the kindergarten of a local school that specialized in music, and we took her younger sister along to watch the kids sing songs about friendship. It was all so heartwarming‚Äîuntil we noticed the school security officer at the entrance carrying a gun.



Later in the year, I received an email alert from the superintendent of the Cambridge Public Schools. ‚ÄúAt approximately 1:45 this afternoon, a Cambridge Police Department Youth Officer assigned to Cambridge Rindge and Latin School accidentally discharged their firearm while using a staff bathroom inside the school,‚Äù the message began. ‚ÄúThe school day was not disrupted.‚Äù



These experiences, among others, truly brought home to me the cultural differences over firearms between the US and the UK (along with most other countries). For the first time, I worried about my children‚Äôs exposure to them. I banned my children from accessing parts of the house. I felt guilty that my four-year-old had to learn what to do if a gunman entered her school.&nbsp;



But it‚Äôs the statistics that are the most upsetting.



In 2023, 46,728 people died from gun violence in the US, according to a report published in June by the Johns Hopkins Bloomberg School of Public Health. That includes both homicides and suicides, and it breaks down to 128 deaths per day, on average. The majority of those who die from gun violence are adults. But the figures for children are sickening, too. In 2023, 2,566 young people died from gun violence. Of those, 234 were under the age of 10.



Gun death rates among children have more than doubled since 2013. Firearms are involved in more child deaths than cancer or car crashes.



Many other children survive gun violence with nonfatal‚Äîbut often life-changing‚Äîinjuries. And the impacts are felt beyond those who are physically injured. Witnessing gun violence or hearing gunshots can understandably cause fear, sadness, and distress.&nbsp;&nbsp;





That‚Äôs worth bearing in mind when you consider that there have been 434 school shootings in the US since Columbine in 1999. The Washington Post estimates that 397,000 students have experienced gun violence at school in that period. Another school shooting took place at Evergreen High School in Colorado on Wednesday, adding to that total.



‚ÄúBeing indirectly exposed to gun violence takes its toll on our mental health and children‚Äôs ability to learn,‚Äù says Daniel Webster, Bloomberg Professor of American Health at the Johns Hopkins Center for Gun Violence Solutions in Baltimore.



The MAHA report states that ‚ÄúAmerican youth face a mental health crisis,‚Äù going on to note that ‚Äúsuicide deaths among 10- to 24-year-olds increased by 62% from 2007 to 2021‚Äù and that ‚Äúsuicide is now the leading cause of death in teens aged 15-19.‚Äù What it doesn‚Äôt say is that around half of these suicides involve guns.



‚ÄúWhen you add all these dimensions, [gun violence is] a very huge public health problem,‚Äù says Webster.



Researchers who study gun violence have been saying the same thing for years. And in 2024, then US Surgeon General Vivek Murthy declared it a public health crisis. ‚ÄúWe don‚Äôt have to subject our children to the ongoing horror of firearm violence in America,‚Äù Murthy said in a statement at the time. Instead, he argued, we should tackle the problem using a public health approach.



Part of that approach involves identifying who is at the greatest risk and offering support to lower that risk, says Webster. Young men who live in poor communities tend to have the highest risk of gun violence, he says, as do those who experience crisis or turmoil. Trying to mediate conflicts or limit access to firearms, even temporarily, can help lower the incidence of gun violence, he says.



There‚Äôs an element of social contagion, too, adds Webster. Shooting begets more shooting. He likens it to the outbreak of an infectious disease. ‚ÄúWhen more people get vaccinated ‚Ä¶ infection rates go down,‚Äù he says. ‚ÄúAlmost exactly the same thing happens with gun violence.‚Äù



But existing efforts are already under threat. The Trump administration has eliminated hundreds of millions of dollars in grants for organizations working to reduce gun violence.



Webster thinks the MAHA report has ‚Äúmissed the mark‚Äù when it comes to the health and well-being of children in the US. ‚ÄúThis document is almost the polar opposite to how many people in public health think,‚Äù he says. ‚ÄúWe have to acknowledge that injuries and deaths from firearms are a big threat to the health and safety of children and adolescents.‚Äù



This article first appeared in The Checkup,¬†MIT Technology Review‚Äôs¬†weekly biotech newsletter. To receive it in your inbox every Thursday, and read articles like this first,¬†sign up here.
‚Ä¢ Partnering with generative AI in the finance function
  Generative AI has the potential to transform the finance function . By taking on some of the more mundane tasks that can occupy a lot of time, generative AI tools can help free up capacity for more high-value strategic work . Deloitte‚Äôs fourth-quarter 2024 North American CFO Signals survey found that 46% of CFOs who responded expect deployment of AI in finance to increase in the next 12 months .
‚Ä¢ The Download: Trump‚Äôs impact on science, and meet our climate and energy honorees
  This is today&#8217;s edition of¬†The Download,¬†our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



How Trump‚Äôs policies are affecting early-career scientists‚Äîin their own words



Every year MIT Technology Review celebrates accomplished young scientists, entrepreneurs, and inventors from around the world in our Innovators Under 35 list. We‚Äôve just published the 2025 edition. This year, though, the context is different: The US scientific community is under attack.Since Donald Trump took office in January, his administration has fired top government scientists, targeted universities and academia, and made substantial funding cuts to the country‚Äôs science and technology infrastructure.



We asked our six most recent cohorts about both positive and negative impacts of the administration‚Äôs new policies. Their responses provide a glimpse into the complexities of building labs, companies, and careers in today‚Äôs political climate. Read the full story.‚ÄîEileen Guo &amp; Amy Nordrum



This story is part of MIT Technology Review‚Äôs &#8220;America Undone‚Äù series, examining how the foundations of US success in science and innovation are currently under threat. You can read the rest here.







This Ethiopian entrepreneur is reinventing ammonia production



In the small town in Ethiopia where he grew up, Iwnetim Abate‚Äôs family had electricity, but it was unreliable. So, for several days each week when they were without power, Abate would finish his homework by candlelight.



Growing up without the access to electricity that many people take for granted shaped the way Abate thinks about energy issues. Today, the 32-year old is an assistant professor at MIT in the department of materials science and engineering.&nbsp;



Part of his research focuses on sodium-ion batteries, which could be cheaper than the lithium-based ones that typically power electric vehicles and grid installations. He‚Äôs also pursuing a new research path, examining how to harness the heat and pressure under the Earth‚Äôs surface to make ammonia, a chemical used in fertilizer and as a green fuel. Read the full story.



‚ÄîCasey Crownhart



Abate is one of the climate and energy honorees on our 35 Innovators Under 35 list for 2025. Meet the rest of our climate and energy innovators here, and the full list‚Äîincluding our innovator of the year‚Äîhere.&nbsp;







Texas banned lab-grown meat. What‚Äôs next for the industry?



Last week, a legal battle over lab-grown meat kicked off in Texas. On September 1, a two-year ban on the technology went into effect across the state; the following day, two companies filed a lawsuit against state officials.



The two companies, Wildtype Foods and Upside Foods, are part of a growing industry that aims to bring new types of food to people‚Äôs plates. These products, often called cultivated meat by the industry, take live animal cells and grow them in the lab to make food products without the need to slaughter animals.Texas joins six other US states and the country of Italy in banning these products‚Äîadding barriers to an industry that‚Äôs still in its infancy, and already faces plenty of challenges before it can reach consumers in a meaningful way. Read the full story.



‚ÄîCasey Crownhart



This article is from The Spark, MIT Technology Review‚Äôs weekly climate newsletter. To receive it in your inbox every Wednesday, sign up here.







The must-reads



I‚Äôve combed the internet to find you today‚Äôs most fun/important/scary/fascinating stories about technology.



1 Videos of Charlie Kirk‚Äôs shooting are everywhere on social mediaIt demonstrates just how poorly equipped platforms are to stop the spread of violent material. (NYT $)+ Why social media can‚Äôt get on top of its graphic video problem. (NY Mag $)+ Here‚Äôs how platforms say they‚Äôll treat the videos. (The Verge)+ Far-right communities reacted to Kirk‚Äôs murder by calling for more violence. (Wired $)



2 NASA has uncovered the clearest sign of life on Mars to dateSome unusual rocks may have been formed by ancient microbes. (WP $)+ Scientists are very excited by the possibility they were created by living organisms. (New Scientist $)



3 A California bill to regulate AI companion chatbots is close to passingIt would become the first US state to make chatbot operators legally accountable. (TechCrunch)+ Wall Street is only now starting to worry about ‚ÄúAI psychosis.‚Äù (Insider $)+ AI companions are the final stage of digital addiction, and lawmakers are taking aim. (MIT Technology Review)



4 Larry Ellison briefly overtook Elon Musk as the world‚Äôs richest personHis firm Oracle reported far better-than expected results. (The Guardian)+ Oracle is riding high on a surge of demand for its data centers. (BBC)+ But its continued success will depend on its ability to deliver promised hardware. (FT $)



5 The ousted CDC director is set to testify before the US SenateRFK Jr repeatedly called Susan Monarez a liar during a hearing last week. (Ars Technica)+ The backlash to Kennedy‚Äôs actions is intensifying. (NY Mag $)



6 A new system can pinpoint the best spot to hit an asteroidMaking destroying them a whole lot safer, in theory. (New Scientist $)+ Meet the researchers testing the ‚ÄúArmageddon‚Äù approach to asteroid defense. (MIT Technology Review)



7 Saudi Arabia is building some of the world‚Äôs biggest solar farms It needs plenty more electricity for its new resorts and data centers. (WSJ $)+ AI is changing the grid. Could it help more than it harms? (MIT Technology Review)



8 CRISPR could help to combat diabetesScientists successfully implanted insulin-producing edited cells into a man‚Äôs pancreas. (Wired $)+ A US court just put ownership of CRISPR back in play. (MIT Technology Review)



9 How to save oyster reefs Conservation projects are helping to rebuild destroyed populations. (Knowable Magazine)+ How the humble sea creature could hold the key to restoring coastal waters. (MIT Technology Review)



10 Bluesky is not as fun as it should beIt fosters a culture of reactionary scolding that‚Äôs driving some users back to X. (New Yorker $)







Quote of the day



‚ÄúFor the love of God and Charlie‚Äôs family, just stop.‚Äù



‚ÄîA poster on X begs fellow social media users to stop sharing images and videos of conservative activist Charlie Kirk‚Äôs murder online, the Associated Press reports.







One more thing







This giant microwave may change the future of warImagine: China deploys hundreds of thousands of autonomous drones in the air, on the sea, and under the water‚Äîall armed with explosive warheads or small missiles. These machines descend in a swarm toward military installations on Taiwan and nearby US bases, and over the course of a few hours, a single robotic blitzkrieg overwhelms the US Pacific force before it can even begin to fight back.The proliferation of cheap drones means just about any group with the wherewithal to assemble and launch a swarm could wreak havoc, no expensive jets or massive missile installations required.The US armed forces are now hunting for a solution‚Äîand they want it fast. One of these is microwaves: high-powered electronic devices that push out kilowatts of power to zap the circuits of a drone as if it were the tinfoil you forgot to take off your leftovers when you heated them up. Read the full story.



‚ÄîSam Dean







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)



+ They‚Äôve finally done it‚Äîthe Stephen King novel they claimed was impossible to adapt is coming to the big screen.+ Do you have more zucchinis than you know what to do with? This tasty bread is one solution.+ How The Penguin‚Äôs production designers transformed NYC into spooky, dirty Gotham.+ This fascinating website shows you what today‚Äôs date looks like on dozens of different calendars and clocks.

üîí Cybersecurity & Privacy
‚Ä¢ Bulletproof Host Stark Industries Evades EU Sanctions
  In May 2025, the European Union levied financial sanctions on the owners of¬†Stark Industries Solutions Ltd., a bulletproof hosting provider that materialized two weeks before Russia invaded Ukraine and quickly became a top source of Kremlin-linked cyberattacks and disinformation campaigns. But new findings show those sanctions have done little to stop Stark from simply rebranding and transferring their assets to other corporate entities controlled by its original hosting providers.
Image: Shutterstock.
Materializing just two weeks before Russia invaded Ukraine in 2022, Stark Industries Solutions became a frequent source of massive DDoS attacks, Russian-language proxy and VPN services, malware tied to Russia-backed hacking groups, and fake news. ISPs like Stark are called &#8220;bulletproof&#8221; providers when they cultivate a reputation for ignoring any abuse complaints or police inquiries about activity on their networks.
In May 2025, the European Union sanctioned one of Stark&#8217;s two main conduits to the larger Internet &#8212; Moldova-based PQ Hosting &#8212; as well as the company&#8217;s Moldovan owners Yuri and Ivan Neculiti. The EU Commission said the Neculiti brothers and PQ Hosting were linked to Russia&#8217;s hybrid warfare efforts.
But a new report from Recorded Future finds that just prior to the sanctions being announced, Stark rebranded to¬†the[.]hosting, under control of the Dutch entity WorkTitans BV¬†(AS209847) on June 24, 2025. The Neculiti brothers reportedly got a heads up roughly 12 days before the sanctions were announced, when Moldovan and EU media reported on the forthcoming inclusion of the Neculiti brothers in the sanctions package.
In response, the Neculiti brothers moved much of Stark&#8217;s considerable address space and other resources over to a new company in Moldova called PQ Hosting Plus S.R.L., an entity reportedly connected to the Neculiti brothers thanks to the re-use of a phone number from the original PQ Hosting.
&#8220;Although the majority of associated infrastructure remains attributable to Stark Industries, these changes likely reflect an attempt to obfuscate ownership and sustain hosting services under new legal and network entities,&#8221; Recorded Future observed.
Neither the Recorded Future report nor the May 2025 sanctions from the EU mentioned a second critical pillar of Stark&#8217;s network that KrebsOnSecurity identified in a May 2024 profile on the notorious bulletproof hoster: The Netherlands-based hosting provider MIRhosting.
MIRhosting is operated by 38-year old Andrey Nesterenko, whose personal website says he is an accomplished concert pianist who began performing publicly at a young age. DomainTools says mirhosting[.]com is registered to Mr. Nesterenko and to Innovation IT Solutions Corp, which lists addresses in London and in Nesterenko‚Äôs stated hometown of Nizhny Novgorod, Russia.
Image credit: correctiv.org.
According to the book Inside Cyber Warfare by Jeffrey Carr, Innovation IT Solutions Corp. was responsible for hosting StopGeorgia[.]ru, a hacktivist website for organizing cyberattacks against Georgia that appeared at the same time Russian forces invaded the former Soviet nation in 2008. That conflict was thought to be the first war ever fought in which a notable cyberattack and an actual military engagement happened simultaneously.
Mr. Nesterenko did not respond to requests for comment. In May 2024, Mr. Nesterenko said he couldn&#8217;t verify whether StopGeorgia was ever a customer because they didn&#8217;t keep records going back that far. But he maintained that Stark Industries Solutions was merely one client of many, and claimed MIRhosting had not received any actionable complaints about abuse on Stark.
However, it appears that MIRhosting is once again the new home of Stark Industries, and that MIRhosting employees are managing both the[.]hosting and WorkTitans &#8212; the primary beneficiaries of Stark&#8217;s assets.
A copy of the incorporation documents for WorkTitans BV obtained from the Dutch Chamber of Commerce shows WorkTitans also does business under the names Misfits Media and and WT Hosting (considering Stark&#8217;s historical connection to Russian disinformation websites, &#8220;Misfits Media&#8221; is a bit on the nose).
An incorporation document for WorkTitans B.V. from the Netherlands Chamber of Commerce.
The incorporation document says the company was formed in 2019 by a y.zinad@worktitans.nl. That email address corresponds to a LinkedIn account for a Youssef Zinad, who says their personal websites are worktitans[.]nl and custom-solution[.]nl. The profile also links to a website (etripleasims dot nl) that LinkedIn currently blocks as malicious. All of these websites are or were hosted at MIRhosting.
Although Mr. Zinad&#8217;s LinkedIn profile does not mention any employment at MIRhosting, virtually all of his LinkedIn posts over the past year have been reposts of advertisements for MIRhosting&#8217;s services.
Mr. Zinad&#8217;s LinkedIn profile is full of posts for MIRhosting&#8217;s services.
A Google search for Youssef Zinad reveals multiple startup-tracking websites that list him as the founder of the[.]hosting, which censys.io finds is hosted by PQ Hosting Plus S.R.L.
The Dutch Chamber of Commerce document says WorkTitans&#8217; sole shareholder is a company in Almere, Netherlands called Fezzy B.V. Who runs Fezzy? The phone number listed in a Google search for Fezzy B.V. &#8212; 31651079755 &#8212; also was used to register a Facebook profile for a Youssef Zinad from the same town, according to the breach tracking service Constella Intelligence.
In a series of email exchanges leading up to KrebsOnSecurity&#8217;s May 2024 deep dive on Stark, Mr. Nesterenko included Mr. Zinad in the message thread (youssef@mirhosting.com), referring to him as part of the company&#8217;s legal team. The Dutch website stagemarkt[.]nl lists Youssef Zinad as an official contact for MIRhosting&#8217;s offices in Almere. Mr. Zinad did not respond to requests for comment.

Given the above, it is difficult to argue with the Recorded Future report on Stark&#8217;s rebranding, which concluded that &#8220;the EU‚Äôs sanctioning of Stark Industries was largely ineffective, as affiliated infrastructure remained operational and services were rapidly re-established under new branding, with no significant or lasting disruption.&#8221;

üéì University AI
No updates.

üè¢ Corporate AI
‚Ä¢ Tool-space interference in the MCP era: Designing for agent compatibility at scale
  This year&nbsp;we‚Äôve&nbsp;seen&nbsp;remarkable&nbsp;advances in agentic AI, including&nbsp;systems that conduct deep research,&nbsp;operate&nbsp;computers, complete substantial software engineering tasks, and tackle a range of other complex,&nbsp;multi-step goals. In each case,&nbsp;the industry relied&nbsp;on careful vertical integration: tools and agents were co-designed, co-trained, and tested together&nbsp;for peak&nbsp;performance. For example,&nbsp;OpenAI&#8217;s&nbsp;recent models&nbsp;presume&nbsp;the&nbsp;availability&nbsp;of web search and document retrieval&nbsp;tools (opens in new tab). Likewise,&nbsp;the prompts and actions&nbsp;of&nbsp;Magentic-One&nbsp;are&nbsp;set up to make hand-offs easy‚Äîfor example, allowing the WebSurfer agent to pass downloaded files to the Coder agent.‚ÄØ&nbsp;But‚ÄØas agents proliferate, we anticipate strategies relying heavily on vertical integration will not age well.&nbsp;Agents&nbsp;from&nbsp;different&nbsp;developers&nbsp;or companies will&nbsp;increasingly&nbsp;encounter&nbsp;each other and&nbsp;must&nbsp;work together to complete tasks, in what we refer to as a&nbsp;society of agents.&nbsp;These systems can vary in how coordinated they are, how aligned their goals are, and how much information they share. Can heterogenous agents and tools cooperate&nbsp;in this&nbsp;setting, or will they hinder one another and slow progress?



Early clues have&nbsp;emerged&nbsp;from an&nbsp;unexpected&nbsp;source:&nbsp;namely,&nbsp;Model Context Protocol (opens in new tab)&nbsp;(MCP). Since January 2025, MCP has‚ÄØgrown from a&nbsp;promising spec to a&nbsp;thriving&nbsp;market&nbsp;of&nbsp;tool&nbsp;servers.&nbsp;As an example,‚ÄØZapier boasts a catalog of 30,000 tools (opens in new tab)&nbsp;across 7,000 services.&nbsp;Composio&nbsp;provide over 100 managed MCP servers (opens in new tab), surfacing hundreds of tools. Hugging&nbsp;Face is now serving&nbsp;many&nbsp;Spaces&nbsp;apps over MCP (opens in new tab), and‚ÄØShopify has enabled MCP for millions of storefronts (opens in new tab).&nbsp;A&nbsp;society of&nbsp;tools&nbsp;is already here, and it promises to&nbsp;extend&nbsp;agent capabilities through&nbsp;cross-provider&nbsp;horizontal integration.&nbsp;



So,&nbsp;what does MCP have to say about&nbsp;horizontal integration? As catalogs grow,&nbsp;we expect some new failure modes to surface.&nbsp;This&nbsp;blog&nbsp;post introduces&nbsp;these&nbsp;as‚ÄØtool-space interference, and sketches both early observations‚ÄØand some pragmatic interventions to keep the society&nbsp;we‚Äôre&nbsp;building&nbsp;from stepping on its own feet.&nbsp;



Tool-space interference describes situations where otherwise reasonable tools or agents, when co-present, reduce end-to-end effectiveness. This can look like longer action sequences, higher token cost, brittle recovery from errors, or, in some cases, task failure.



A framing example



Consider MCP as a means for extending Magentic-One, a generalist multi-agent system we released last year, to cover more software engineering tasks. Magentic-One ships with agents to write code, interact with the computer terminal, browse the web, and access local files. To help Magentic-One navigate version control, find issues to solve, and make pull requests, we could add an agent equipped with the GitHub MCP Server. However, now each time the team encounters a task involving GitHub, it must choose whether to visit github.com in the browser, execute a git command at the command line, or engage the GitHub MCP server. As the task progresses, agent understanding of state can also diverge: changing the branch in the browser won‚Äôt change the branch in the terminal, and an authorized MCP tool does not imply authorization in the browser.&nbsp;Thus, while any single agent might complete the task efficiently, the larger set of agents might misunderstand or interfere with one another, leading to additional rounds of debugging, or even complete task failure.



Figure 1: We can extend&nbsp;Magentic-One by adding an agent that equips the GitHub MCP server. However, on every turn involving a git-related task, the orchestrator will need to decide between messaging the Computer Terminal agent (with access to the git command line interface), WebSurfer agent (with access to github.com), and the agent with the GitHub MCP server. This overlap raises the possibility that they will interfere with one another.&nbsp;&nbsp;



Tool-space interference, through the lens of MCP



To better understand the potential interference patterns and the current state of the MCP ecosystem, we conducted a survey of MCP servers listed on two registries: smithery.ai (opens in new tab) and Docker MCP Hub (opens in new tab). Smithery is an MCP Server registry with over 7,000 first-party and community-contributed servers, which we sampled from the Smithery API. Likewise, Docker MCP Hub is a registry that distributes MCP servers as Docker images, and we manually collected popular entries. We then launched each server for inspection. After excluding servers that were empty or failed to launch, and deduplicating servers with identical features, 1,470 servers remained in our catalog.



To&nbsp;automate the&nbsp;inspection&nbsp;of&nbsp;running MCP servers,&nbsp;we developed an&nbsp;MCP&nbsp;Interviewer&nbsp;tool.&nbsp;The MCP&nbsp;Interviewer&nbsp;begins by cataloging the server‚Äôs tools, prompts, resources, resource templates, and capabilities.&nbsp;From&nbsp;this catalog we can compute&nbsp;descriptive statistics&nbsp;such as the number of tools, or the depth of the parameter&nbsp;schemas.&nbsp;&nbsp;Then, given the list of available tools, the interviewer uses&nbsp;an LLM (in our case,&nbsp;OpenAI&#8217;s GPT-4.1)&nbsp;to construct a functional testing&nbsp;plan&nbsp;that&nbsp;calls each tool at least once, collecting outputs, errors, and statistics along the way. Finally,&nbsp;the&nbsp;interviewer&nbsp;can&nbsp;also&nbsp;grade&nbsp;more qualitative&nbsp;criteria&nbsp;by&nbsp;using&nbsp;an LLM&nbsp;to&nbsp;apply purpose-built rubrics&nbsp;to&nbsp;tool&nbsp;schemas&nbsp;and&nbsp;tool call outputs.&nbsp;&nbsp;We are excited to&nbsp;release the MCP Interviewer&nbsp;as an open-source CLI&nbsp;tool (opens in new tab), so server developers can automatically evaluate their MCP servers with agent usability in mind,&nbsp;and users can&nbsp;validate&nbsp;new servers.&nbsp;



While our survey provides informative initial results, it also faces significant limitations, the most obvious of which is authorization: many of the most popular MCP servers provide access to services that require authorization to use, hindering automated analysis. We are often still able to collect static features from these servers but are limited in the functional testing that can be done.



One-size fits all (but some more than others)



So, what does our survey of MCP servers tell us about the MCP ecosystem? We will get into the numbers in a moment, but as we contemplate the statistics, there is one overarching theme to keep in mind: MCP servers do not know which clients or models they are working with, and present one common set of tools, prompts, and resources to everyone. However, some models handle long contexts and large tool spaces better than others (with diverging hard limits), and respond quite differently to common prompting patterns. For example, OpenAI‚Äôs guide on function calling (opens in new tab) advises developers to:



‚ÄúInclude examples and edge cases, especially to rectify any recurring failures. (Note: Adding examples may hurt performance for reasoning models).‚Äù



So already, this places MCP at a disadvantage over vertical integrations that optimize to the operating environment. And with that, let‚Äôs dive into more numbers.



Tool count



While models generally vary in their proficiency for tool calling, the general trend has been that performance drops as the number of tools increases. For example, OpenAI limits developers to 128 tools, but recommends (opens in new tab) that developers:



‚ÄúKeep the number of functions small for higher accuracy. Evaluate your performance with different numbers of functions. Aim for fewer than 20 functions at any one time, though this is just a soft suggestion.‚Äù



While we expect this to improve with each new model generation, at present, large tool spaces can lower performance by up to 85% for some models (opens in new tab). Thankfully, the majority of servers in our survey contain four or fewer tools. But there are outliers: the largest MCP server we cataloged adds 256 distinct tools, while the 10 next-largest servers add more than 100 tools each. Further down the list we find popular servers like Playwright-MCP (opens in new tab) (29 tools, at the time of this writing), and GitHub MCP (91 tools, with subsets available at alternative endpoint URLs), which might be too large for some models.



Figure 2: The number of tools listed by each catalogued server directly after initialization. Note: servers can change the tools they list at any time, but only 226 servers in our catalog declare this capability.



Response length



Tools are generally called in agentic loops, where the output is then fed back into the model as input context. Models have hard limits on input context, but even within these limits, large contexts can drive costs up and performance down, so practical limits can be much lower (opens in new tab). MCP offers no guidance on how many tokens a tool call can produce, and the size of some responses can come as a surprise. In our analysis, we consider the 2,443 tool calls across 1,312 unique tools that the MCP Interviewer was able to call successfully during the active testing phase of server inspection. While a majority of tools produced 98 or fewer tokens  (opens in new tab), some tools are extraordinarily heavyweight: the top tool returned an average of 557,766 tokens, which is enough to swamp the context windows of many popular models like GPT-5. Further down the list, we find that 16 tools produce more than 128,000 tokens, swamping GPT-4o and other popular models. Even when responses fit into the context window length, overly long responses can significantly degrade performance (up to 91% in one study (opens in new tab)), and limit the number of future calls that can be made. Of course, agents are free to implement their own context management strategies, but this behavior is left undefined in the MCP specification and server developers cannot count on any particular client behavior or strategy.



# of tools that would overflow context inModelContext Window1 call2 calls3-5 calls6-10 callsGPT 4.11,000,00001711GPT 5400,000171525GPT-4o, Llama 3.1,128,00016153340Qwen 332,00056378690Phi-416,0009360116109



Figure 3: Tool call response length averages, in tokens, as&nbsp;observed&nbsp;by the MCP Interviewer‚Äôs functional test plan. Only successful tool calls are considered. Horizontal lines&nbsp;indicate&nbsp;context window limits for GPT-4o and GPT-5.



Tool parameter complexity



Mirroring the challenges from increasing&nbsp;the&nbsp;number of tools,&nbsp;increasing the complexity of a tool‚Äôs parameter space can also lead to degradation.&nbsp;For example, while MCP tools can take complex object types and structures as parameters,&nbsp;composio (opens in new tab)&nbsp;found that&nbsp;flattening the parameter space could improve tool-calling performance&nbsp;by 47%&nbsp;compared to baseline performance.&nbsp;&nbsp;In our analysis, we&nbsp;find&nbsp;numerous examples of deeply nested structure‚Äîin&nbsp;one&nbsp;case, going&nbsp;20&nbsp;levels deep.



Figure 4: The maximum depth of each tool‚Äôs input properties schema. A depth of 0&nbsp;indicates&nbsp;a tool with no properties. A depth of 1&nbsp;indicates&nbsp;a tool with named properties but no annotations (e.g., no description or type). A depth of 2&nbsp;indicates&nbsp;a tool with named and annotated properties.&nbsp;&nbsp;A depth of 3+&nbsp;indicates&nbsp;a tool with structured properties that have&nbsp;additional&nbsp;nested annotations.&nbsp;



Namespacing issues and naming ambiguity



Another often-cited issue with the current MCP specification is the lack of a formal namespace mechanism (opens in new tab). If two servers are registered to the same agent or application, and the servers have tool names in common, then disambiguation becomes impossible. Libraries like the OpenAI Agents SDK raise an error (opens in new tab) under this circumstance. Clients, like Claude Code, prefix tool names with unique identifiers to work around this issue. In our analysis of MCP servers, we found name collisions between 775 tools. The most common collision was ‚Äúsearch‚Äù, which appears across 32 distinct MCP servers. The following table lists the top 10 collisions.



Tool NameNumber of Instancessearch32get_user11execute_query11list_tables10update_task9generate_image9send_message9execute_command8list_tasks8search_files8



Even when names are unique, they can be semantically similar. If these tools behave similarly, then the redundancy may not be immediately problematic, but if you are expecting to call a particular tool then the name similarities raise the potential for confusion. The following table lists some examples of semantically similar tool names relating to web search:



websearchbrave_web_searchsearch-webtavily_web_searchweb_searchgoogle_news_searchsearch_webgoogle-play-searchsearch_webkrgoogle_search_parsedgoogle_searchsearch_google_imagessearch_googleget_webset_search_exaai_web_searchsearch_google_scholarweb_search_exaduckduckgo_web_searchsearch_web_toolgoogle_search_scraperweb_search_agentanswer_query_websearchbatch-web-search&nbsp;



Errors and error messages



Like all software libraries, MCP will occasionally encounter error conditions. In these cases, it is important to provide sufficient information for the agent to handle the error and plan next steps. In our analysis, we found this was not always the case. While MCP provides an ‚ÄúIsError‚Äù flag to signal errors, we found that it was common for servers to handle errors by returning strings while leaving this flag set to false, signaling a normal exit. Out of 5,983 tool call results with no error flag, GPT-4.1 judged that 3,536 indicated errors in their content. More worrisome: the error messages were often of low quality. For instance, one tool providing web search capabilities failed with the string ‚Äúerror: job,‚Äù while another tool providing academic search returned ‚ÄúPlease retry with 0 or fewer IDs.‚Äù



Resource sharing conventions



Finally, in addition to tools, MCP allows servers to share resources and resource templates with clients. In our survey, only 112 (7.6%) servers reported any resources, while 74 (5%) provided templates. One potential reason for low adoption is that the current MCP specification provides limited guidance for when resources are retrieved, or how they are incorporated into context. One clearcut situation where a client might retrieve a resource is in response to a tool returning a resource_link (opens in new tab) as a result &#8212; but only 4 tools exhibited this behavior in our survey (arguably, this would be the ideal behavior for tools that return very long, document-like responses, as outlined earlier).



Conversely, a whole different set of issues arises when there is a need to share resources from the client to the server. Consider for example a tool that provides some analysis of a local PDF file. In the case of a local MCP server utilizing STDIO transport, a local file path can be provided as an argument to the tool, but no similar conventions exist for delivering a local file to a remote MCP server. These issues are challenging enough when implementing a single server. When multiple tools or servers need to interact within the same system, the risk of interoperability errors compounds.



Recommendations



On balance, along any given dimension, the average MCP server is quite reasonable‚Äîbut, as we have seen, outliers and diverging assumptions can introduce trouble. While we expect many of these challenges to improve with time, we are comfortable making small recommendations that we feel are evergreen. We organize them below by audience.



Protocol developers



We recognize the advantages of keeping MCP relatively lightweight, avoiding being overly prescriptive in an environment where AI models and use cases are rapidly changing. However, a few small recommendations are warranted. First, we believe MCP should be extended to include a specification for client-provided resources so that tools on remote servers have a mechanism for operating on specified local files or documents. This would more effectively position MCP as a clearinghouse for resources passed between steps of agentic workflows. The MCP specification would also benefit from taking a more opinionated stance on when resources are retrieved and used overall.



Likewise, we believe&nbsp;MCP should&nbsp;quickly move to&nbsp;provide formal namespaces&nbsp;to eliminate tool name collisions.&nbsp;If namespaces&nbsp;are hierarchical, then this also provides a way of organizing large catalogs&nbsp;of functions&nbsp;into thematically&nbsp;related tool&nbsp;sets.&nbsp;Tool sets, as an organizing principle,&nbsp;are already showing some promise&nbsp;in&nbsp;GitHub MCP Server‚Äôs&nbsp;dynamic tool discovery, (opens in new tab)&nbsp;and VS Code‚Äôs&nbsp;tool grouping (with virtual tools) (opens in new tab),&nbsp;where agents or users&nbsp;can&nbsp;enable and disable tools&nbsp;as needed.&nbsp;&nbsp;In the future,&nbsp;a standardized mechanism for grouping tools would allow&nbsp;clients&nbsp;to engage in hierarchical tool-calling,&nbsp;where they first select a category, then select a tool, without needing to keep all possible&nbsp;tools in context.



Server developers



While our MCP Interviewer tool can catalog many outward-facing properties of MCP servers, developers are often in a much better position to characterize the nature of their tools. To this end, we believe developers should publish an MCP Server card alongside their servers or services, clearly outlining the runtime characteristics of the tools (e.g., the expected number of tokens generated, or expected latency of a tool call). Ideally developers should also indicate which models, agents and clients the server was tested with, how the tools were tested (e.g., provide sample tasks), list any known incompatibilities, and be mindful of limitations of various models throughout development.



Client developers



Client developers have the opportunity to experiment with various mitigations or optimizations that might help the average MCP server work better for a given system or environment. For example, clients could cache tool schemas, serving them as targets for prompt optimizations, or as an index for RAG-like tool selection approaches. To this end, Anthropic recently reported using a tool testing agent (opens in new tab) to rewrite the prompts of defective MCP servers, improving task completion time by 40%. Likewise, rather than waiting for the protocol to evolve, clients could take proactive steps to resolve name collisions‚Äî for example, generating namespaces from server names‚Äîand could reduce token outputs by summarizing or paginating long tool results.



Market developers



Finally, we see an opportunity for marketplaces to codify best-practices, spot compatibility issues at a global level, and perhaps centralize the generation and serving of model or agent-specific optimizations. Mirroring how a market like PyPI distributes Python wheels matched to a developer‚Äôs operating system or processor (opens in new tab), an MCP marketplace could serve tool schemas optimized for a developer‚Äôs chosen LLM, agent or client library. We are already seeing small steps in this direction, with registries like Smithery providing customized launch configurations to match users‚Äô clients.



Conclusion



In summary, the MCP&nbsp;ecosystem offers significant value for AI agent development,&nbsp;despite&nbsp;some&nbsp;early&nbsp;growing pains.&nbsp;Grounded in insights from the&nbsp;MCP Interviewer (opens in new tab)&nbsp;and our survey of live servers, the evidence is clear: horizontal integration is expanding capability, yet it also exposes forms of toolspace interference that can erode end to end effectiveness. Anticipating rapid advances in model capability and growing architectural diversity, the recommendations provided here aim to ensure that protocol, server, client, and marketplace developers are&nbsp;well positioned&nbsp;to adapt and thrive. Key steps include implementing formal namespaces to&nbsp;eliminate&nbsp;collisions, enhancing protocol support for&nbsp;client provided&nbsp;resources, and encouraging transparent server documentation to foster interoperability and robust development practices across the ecosystem.&nbsp;



By embracing these evergreen recommendations and proactively addressing compatibility, usability, and optimization issues, the AI agent community can create a more reliable, scalable, and efficient infrastructure that benefits both developers and end users. The future of MCP is bright, with ample opportunities for experimentation, standardization, and collective progress.
Opens in a new tabThe post Tool-space interference in the MCP era: Designing for agent compatibility at scale appeared first on Microsoft Research.
‚Ä¢ RenderFormer: How neural networks are reshaping 3D rendering
  3D rendering‚Äîthe process of converting three-dimensional models into two-dimensional images‚Äîis a foundational technology in computer graphics, widely used across gaming, film, virtual reality, and architectural visualization. Traditionally, this process has depended on physics-based techniques like ray tracing and rasterization, which simulate light behavior through mathematical formulas and expert-designed models.



Now, thanks to advances in AI, especially neural networks, researchers are beginning to replace these conventional approaches with machine learning (ML). This shift is giving rise to a new field known as neural rendering.



Neural rendering combines deep learning with traditional graphics techniques, allowing models to simulate complex light transport without explicitly modeling physical optics. This approach offers significant advantages: it eliminates the need for handcrafted rules, supports end-to-end training, and can be optimized for specific tasks. Yet, most current neural rendering methods rely on 2D image inputs, lack support for raw 3D geometry and material data, and often require retraining for each new scene‚Äîlimiting their generalizability.



RenderFormer: Toward a general-purpose neural rendering model



To overcome these limitations, researchers at Microsoft Research have developed RenderFormer, a new neural architecture designed to support full-featured 3D rendering using only ML‚Äîno traditional graphics computation required. RenderFormer is the first model to demonstrate that a neural network can learn a complete graphics rendering pipeline, including support for arbitrary 3D scenes and global illumination, without relying on ray tracing or rasterization. This work has been accepted at SIGGRAPH 2025 and is open-sourced on GitHub (opens in new tab).



Architecture overview



As shown in Figure 1, RenderFormer represents the entire 3D scene using triangle tokens‚Äîeach one encoding spatial position, surface normal, and physical material properties such as diffuse color, specular color, and roughness. Lighting is also modeled as triangle tokens, with emission values indicating intensity.



Figure 1. Architecture of RenderFormer



To describe the viewing direction, the model uses ray bundle tokens derived from a ray map‚Äîeach pixel in the output image corresponds to one of these rays. To improve computational efficiency, pixels are grouped into rectangular blocks, with all rays in a block processed together.



The model outputs a set of tokens that are decoded into image pixels, completing the rendering process entirely within the neural network.



	
		

		
		PODCAST SERIES
	
	
	
						
				
					
				
			
			
			

									The AI Revolution in Medicine, Revisited
				
								Join Microsoft‚Äôs Peter Lee on a journey to discover how AI is impacting healthcare and what it means for the future of medicine.
				
								
					
						
							Listen now						
					
				
							
	
Opens in a new tab	
	


Dual-branch design for view-independent and view-dependent effects



The RenderFormer architecture is built around two transformers: one for view-independent features and another for view-dependent ones.




The view-independent transformer captures scene information unrelated to viewpoint, such as shadowing and diffuse light transport, using self-attention between triangle tokens.



The view-dependent transformer models effects like visibility, reflections, and specular highlights through cross-attention between triangle and ray bundle tokens.




Additional image-space effects, such as anti-aliasing and screen-space reflections, are handled via self-attention among ray bundle tokens.



To validate the architecture, the team conducted ablation studies and visual analyses, confirming the importance of each component in the rendering pipeline.



Table 1. Ablation study analyzing the impact of different components and attention mechanisms on the final performance of the trained network. 



To test the capabilities of the view-independent transformer, researchers trained a decoder to produce diffuse-only renderings. The results, shown in Figure 2, demonstrate that the model can accurately simulate shadows and other indirect lighting effects.



Figure 2. View-independent rendering effects decoded directly from the view-independent transformer, including diffuse lighting and coarse shadow effects. 



The view-dependent transformer was evaluated through attention visualizations. For example, in Figure 3, the attention map reveals a pixel on a teapot attending to its surface triangle and to a nearby wall‚Äîcapturing the effect of specular reflection. These visualizations also show how material changes influence the sharpness and intensity of reflections.



Figure 3. Visualization of attention outputs



Training methodology and dataset design



RenderFormer was trained using the Objaverse dataset, a collection of more than 800,000 annotated 3D objects that is designed to advance research in 3D modeling, computer vision, and related fields. The researchers designed four scene templates, populating each with 1‚Äì3 randomly selected objects and materials. Scenes were rendered in high dynamic range (HDR) using Blender‚Äôs Cycles renderer, under varied lighting conditions and camera angles.



The base model, consisting of 205 million parameters, was trained in two phases using the AdamW optimizer:




500,000 steps at 256√ó256 resolution with up to 1,536 triangles



100,000 steps at 512√ó512 resolution with up to 4,096 triangles




The model supports arbitrary triangle-based input and generalizes well to complex real-world scenes. As shown in Figure 4, it accurately reproduces shadows, diffuse shading, and specular highlights.



Figure 4. Rendered results of different 3D scenes generated by RenderFormer 



RenderFormer can also generate continuous video by rendering individual frames, thanks to its ability to model viewpoint changes and scene dynamics.



3D animation sequence rendered by RenderFormer 



Looking ahead: Opportunities and challenges



RenderFormer represents a significant step forward for neural rendering. It demonstrates that deep learning can replicate and potentially replace the traditional rendering pipeline, supporting arbitrary 3D inputs and realistic global illumination‚Äîall without any hand-coded graphics computations.



However, key challenges remain. Scaling to larger and more complex scenes with intricate geometry, advanced materials, and diverse lighting conditions will require further research. Still, the transformer-based architecture provides a solid foundation for future integration with broader AI systems, including video generation, image synthesis, robotics, and embodied AI.¬†



Researchers hope that RenderFormer will serve as a building block for future breakthroughs in both graphics and AI, opening new possibilities for visual computing and intelligent environments.
Opens in a new tabThe post RenderFormer: How neural networks are reshaping 3D rendering appeared first on Microsoft Research.
‚Ä¢ Automate advanced agentic RAG pipeline with Amazon SageMaker AI
  Retrieval Augmented Generation (RAG) is a fundamental approach for building advanced generative AI applications that connect large language models (LLMs) to enterprise knowledge. However, crafting a reliable RAG pipeline is rarely a one-shot process. Teams often need to test dozens of configurations (varying chunking strategies, embedding models, retrieval techniques, and prompt designs) before arriving at a solution that works for their use case. Furthermore, management of high-performing RAG pipeline involves complex deployment, with teams often using manual RAG pipeline management, leading to inconsistent results, time-consuming troubleshooting, and difficulty in reproducing successful configurations. Teams struggle with scattered documentation of parameter choices, limited visibility into component performance, and the inability to systematically compare different approaches. Additionally, the lack of automation creates bottlenecks in scaling the RAG solutions, increases operational overhead, and makes it challenging to maintain quality across multiple deployments and environments from development to production. 
In this post, we walk through how to streamline your RAG development lifecycle from experimentation to automation, helping you operationalize your RAG solution for production deployments with Amazon SageMaker AI, helping your team experiment efficiently, collaborate effectively, and drive continuous improvement. By combining experimentation and automation with SageMaker AI, you can verify that the entire pipeline is versioned, tested, and promoted as a cohesive unit. This approach provides comprehensive guidance for traceability, reproducibility, and risk mitigation as the RAG system advances from development to production, supporting continuous improvement and reliable operation in real-world scenarios. 
Solution overview 
By streamlining both experimentation and operational workflows, teams can use SageMaker AI to rapidly prototype, deploy, and monitor RAG applications at scale. Its integration with SageMaker managed MLflow provides a unified platform for tracking experiments, logging configurations, and comparing results, supporting reproducibility and robust governance throughout the pipeline lifecycle. Automation also minimizes manual intervention, reduces errors, and streamlines the process of promoting the finalized RAG pipeline from the experimentation phase directly into production. With this approach, every stage from data ingestion to output generation operates efficiently and securely, while making it straightforward to transition validated solutions from development to production deployment. 
For automation, Amazon SageMaker Pipelines orchestrates end-to-end RAG workflows from data preparation and vector embedding generation to model inference and evaluation all with repeatable and version-controlled code. Integrating continuous integration and delivery (CI/CD) practices further enhances reproducibility and governance, enabling automated promotion of validated RAG pipelines from development to staging or production environments. Promoting an entire RAG pipeline (not just an individual subsystem of the RAG solution like a chunking layer or orchestration layer) to higher environments is essential because data, configurations, and infrastructure can vary significantly across staging and production. In production, you often work with live, sensitive, or much larger datasets, and the way data is chunked, embedded, retrieved, and generated can impact system performance and output quality in ways that are not always apparent in lower environments. Each stage of the pipeline (chunking, embedding, retrieval, and generation) must be thoroughly evaluated with production-like data for accuracy, relevance, and robustness. Metrics at every stage (such as chunk quality, retrieval relevance, answer correctness, and LLM evaluation scores) must be monitored and validated before the pipeline is trusted to serve real users. 
The following diagram illustrates the architecture of a scalable RAG pipeline built on SageMaker AI, with MLflow experiment tracking seamlessly integrated at every stage and the RAG pipeline automated using SageMaker Pipelines. SageMaker managed MLflow provides a unified platform for centralized RAG experiment tracking across all pipeline stages. Every MLflow execution run whether for RAG chunking, ingestion, retrieval, or evaluation sends execution logs, parameters, metrics, and artifacts to SageMaker managed MLflow. The architecture uses SageMaker Pipelines to orchestrate the entire RAG workflow through versioned, repeatable automation. These RAG pipelines manage dependencies between critical stages, from data ingestion and chunking to embedding generation, retrieval, and final text generation, supporting consistent execution across environments. Integrated with CI/CD practices, SageMaker Pipelines enable seamless promotion of validated RAG configurations from development to staging and production environments while maintaining infrastructure as code (IaC) traceability. 
 
For the operational workflow, the solution follows a structured lifecycle: During experimentation, data scientists iterate on pipeline components within Amazon SageMaker Studio notebooks while SageMaker managed MLflow captures parameters, metrics, and artifacts at every stage. Validated workflows are then codified into SageMaker Pipelines and versioned in Git. The automated promotion phase uses CI/CD to trigger pipeline execution in target environments, rigorously validating stage-specific metrics (chunk quality, retrieval relevance, answer correctness) against production data before deployment. The other core components include: 
 
 Amazon SageMaker JumpStart for accessing the latest LLM models and hosting them on SageMaker endpoints for model inference with the embedding model huggingface-textembedding-all-MiniLM-L6-v2 and text generation model deepseek-llm-r1-distill-qwen-7b. 
 Amazon OpenSearch Service as a vector database to store document embeddings with the OpenSearch index configured for k-nearest neighbors (k-NN) search. 
 The Amazon Bedrock model anthropic.claude-3-haiku-20240307-v1:0 as an LLM-as-a-judge component for all the MLflow LLM evaluation metrics. 
 A SageMaker Studio notebook for a development environment to experiment and automate the RAG pipelines with SageMaker managed MLflow and SageMaker Pipelines. 
 
You can implement this agentic RAG solution code from the GitHub repository. In the following sections, we use snippets from this code in the repository to illustrate RAG pipeline experiment evolution and automation. 
Prerequisites 
You must have the following prerequisites: 
 
 An AWS account with billing enabled. 
 A SageMaker AI domain. For more information, see Use quick setup for Amazon SageMaker AI. 
 Access to a running SageMaker managed MLflow tracking server in SageMaker Studio. For more information, see the instructions for setting up a new MLflow tracking server. 
 Access to SageMaker JumpStart to host LLM embedding and text generation models. 
 Access to the Amazon Bedrock foundation models (FMs) for RAG evaluation tasks. For more details, see Subscribe to a model. 
 
SageMaker MLFlow RAG experiment 
SageMaker managed MLflow provides a powerful framework for organizing RAG experiments, so teams can manage complex, multi-stage processes with clarity and precision. The following diagram illustrates the RAG experiment stages with SageMaker managed MLflow experiment tracking at every stage. This centralized tracking offers the following benefits: 
 
 Reproducibility: Every experiment is fully documented, so teams can replay and compare runs at any time 
 Collaboration: Shared experiment tracking fosters knowledge sharing and accelerates troubleshooting 
 Actionable insights: Visual dashboards and comparative analytics help teams identify the impact of pipeline changes and drive continuous improvement 
 
The following diagram illustrates the solution workflow. 
 
Each RAG experiment in MLflow is structured as a top-level run under a specific experiment name. Within this top-level run, nested runs are created for each major pipeline stage, such as data preparation, data chunking, data ingestion, RAG retrieval, and RAG evaluation. This hierarchical approach allows for granular tracking of parameters, metrics, and artifacts at every step, while maintaining a clear lineage from raw data to final evaluation results. 
The following screenshot shows an example of the experiment details in MLflow. 
 
The various RAG pipeline steps defined are: 
 
 Data preparation: Logs dataset version, preprocessing steps, and initial statistics 
 Data chunking: Records chunking strategy, chunk size, overlap, and resulting chunk counts 
 Data ingestion: Tracks embedding model, vector database details, and document ingestion metrics 
 RAG retrieval: Captures retrieval model, context size, and retrieval performance metrics 
 RAG evaluation: Logs evaluation metrics (such as answer similarity, correctness, and relevance) and sample results 
 
This visualization provides a clear, end-to-end view of the RAG pipeline‚Äôs execution, so you can trace the impact of changes at any stage and achieve full reproducibility. The architecture supports scaling to multiple experiments, each representing a distinct configuration or hypothesis (for example, different chunking strategies, embedding models, or retrieval parameters). MLflow‚Äôs experiment UI visualizes these experiments side by side, enabling side-by-side comparison and analysis across runs. This structure is especially valuable in enterprise settings, where dozens or even hundreds of experiments might be conducted to optimize RAG performance. 
We use MLflow experimentation throughout the RAG pipeline to log metrics and parameters, and the different experiment runs are initialized as shown in the following code snippet: 
 
 with mlflow.start_run() as run:
&nbsp;&nbsp; &nbsp;main_run_id = run.info.run_id
&nbsp;&nbsp; &nbsp;print("mlflow_run", run_id)
&nbsp;&nbsp; &nbsp;with mlflow.start_run(run_name="DataPreparation", nested=True): 
 
RAG pipeline experimentation 
The key components of the RAG workflow are ingestion, chunking, retrieval, and evaluation, which we explain in this section. The MLflow dashboard makes it straightforward to visualize and analyze these parameters and metrics, supporting data-driven refinement of the chunking stage within the RAG pipeline. 
 
Data ingestion and preparation 
In the RAG workflow, rigorous data preparation is foundational to downstream performance and reliability. Tracking detailed metrics on data quality, such as the total number of question-answer pairs, the count of unique questions, average context length, and initial evaluation predictions, provides essential visibility into the dataset‚Äôs structure and suitability for RAG tasks. These metrics help validate the dataset is comprehensive, diverse, and contextually rich, which directly impacts the relevance and accuracy of the RAG system‚Äôs responses. Additionally, logging critical RAG parameters like the data source, detected personally identifiable information (PII) types, and data lineage information is vital for maintaining compliance, reproducibility, and trust in enterprise environments. Capturing this metadata in SageMaker managed MLflow supports robust experiment tracking, auditability, efficient comparison, and root cause analysis across multiple data preparation runs, as visualized in the MLflow dashboard. This disciplined approach to data preparation lays the groundwork for effective experimentation, governance, and continuous improvement throughout the RAG pipeline. The following screenshot shows an example of the experiment run details in MLflow. 
 
Data chunking 
After data preparation, the next step is to split documents into manageable chunks for efficient embedding and retrieval. This process is pivotal, because the quality and granularity of chunks directly affect the relevance and completeness of answers returned by the RAG system. The RAG workflow in this post supports experimentation and RAG pipeline automation with both fixed-size and recursive chunking strategies for comparison and validations. However, this RAG solution can be expanded to many other chucking techniques. 
 
 FixedSizeChunker divides text into uniform chunks with configurable overlap 
 RecursiveChunker splits text along logical boundaries such as paragraphs or sentences 
 
Tracking detailed chunking metrics such as total_source_contexts_entries, total_contexts_chunked, and total_unique_chunks_final is crucial for understanding how much of the source data is represented, how effectively it is segmented, and whether the chunking approach is yielding the desired coverage and uniqueness. These metrics help diagnose issues like excessive duplication or under-segmentation, which can impact retrieval accuracy and model performance. 
Additionally, logging parameters such as chunking_strategy_type (for example, FixedSizeChunker), chunking_strategy_chunk_size (for example, 500 characters), and chunking_strategy_chunk_overlap provide transparency and reproducibility for each experiment. Capturing these details in SageMaker managed MLflow helps teams systematically compare the impact of different chunking configurations, optimize for efficiency and contextual relevance, and maintain a clear audit trail of how chunking decisions evolve over time. The MLflow dashboard makes it straightforward to visualize and analyze these parameters and metrics, supporting data-driven refinement of the chunking stage within the RAG pipeline. The following screenshot shows an example of the experiment run details in MLflow. 
 
After the documents are chunked, the next step is to convert these chunks into vector embeddings using a SageMaker embedding endpoint, after which the embeddings are ingested into a vector database such as OpenSearch Service for fast semantic search. This ingestion phase is crucial because the quality, completeness, and traceability of what enters the vector store directly determine the effectiveness and reliability of downstream retrieval and generation stages. 
Tracking ingestion metrics such as the number of documents and chunks ingested provides visibility into pipeline throughput and helps identify bottlenecks or data loss early in the process. Logging detailed parameters, including the embedding model ID, endpoint used, and vector database index, is essential for reproducibility and auditability. This metadata helps teams trace exactly which model and infrastructure were used for each ingestion run, supporting root cause analysis and compliance, especially when working with evolving datasets or sensitive information. 
Retrieval and generation 
For a given query, we generate an embedding and retrieve the top-k relevant chunks from OpenSearch Service. For answer generation, we use a SageMaker LLM endpoint. The retrieved context and the query are combined into a prompt, and the LLM generates an answer. Finally, we orchestrate retrieval and generation using LangGraph, enabling stateful workflows and advanced tracing: 
 
 graph_builder = StateGraph(State).add_sequence([retrieve, generate])
graph_with_context = graph_builder.compile() 
 
With the GenerativeAI agent defined with LangGraph framework, the agentic layers are evaluated for each iteration of RAG development, verifying the efficacy of the RAG solution for agentic applications. Each retrieval and generation run is logged to SageMaker managed MLflow, capturing the prompt, generated response, and key metrics and parameters such as retrieval performance, top-k values, and the specific model endpoints used. Tracking these details in MLflow is essential for evaluating the effectiveness of the retrieval stage, making sure the returned documents are relevant and that the generated answers are accurate and complete. It is equally important to track the performance of the vector database during retrieval, including metrics like query latency, throughput, and scalability. Monitoring these system-level metrics alongside retrieval relevance and accuracy makes sure the RAG pipeline delivers correct and relevant answers and meets production requirements for responsiveness and scalability. The following screenshot shows an example of the Langraph RAG retrieval tracing in MLflow. 
 
RAG Evaluation 
Evaluation is conducted on a curated test set, and results are logged to MLflow for quick comparison and analysis. This helps teams identify the best-performing configurations and iterate toward production-grade solutions. With MLflow you can evaluate the RAG solution with heuristics metrics, content similarity metrics and LLM-as-a-judge. In this post, we evaluate the RAG pipeline using advanced LLM-as-a-judge MLflow metrics (answer similarity, correctness, relevance, faithfulness): 
 
 metrics_genai_only = [answer_correctness_aws, answer_similarity_aws, answer_relevance_aws, answer_faithfulness_aws] 
 
The following screenshot shows an RAG evaluation stage experiment run details in MLflow. 
 
You can use MLflow to log all metrics and parameters, enabling quick comparison of different experiment runs. See the following code for reference: 
 
 with mlflow.start_run(run_id=main_run_id) as run:
&nbsp;&nbsp; &nbsp;with mlflow.start_run(run_name="RAGEvaluation", nested=True):
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;results = mlflow.evaluate(
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ...&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# Other parameters
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;extra_metrics=metrics_genai_only,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;evaluator_config={
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ...&nbsp;# Config parameters
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;) 
 
By using MLflow‚Äôs evaluation capabilities (such as mlflow.evaluate()), teams can systematically assess retrieval quality, identify potential gaps or misalignments in chunking or embedding strategies, and compare the performance of different retrieval and generation configurations. MLflow‚Äôs flexibility allows for seamless integration with external libraries and evaluation libraries such as RAGAS for comprehensive RAG pipeline assessment. RAGAS is an open source library that provide tools specifically for evaluation of LLM applications and generative AI agents. RAGAS includes the method ragas.evaluate() to run evaluations for LLM agents with the choice of LLM models (evaluators) for scoring the evaluation, and an extensive list of default metrics. To incorporate RAGAS metrics into your MLflow experiments, refer to the following GitHub repository. 
Comparing experiments 
In the MLflow UI, you can compare runs side by side. For example, comparing FixedSizeChunker and RecursiveChunker as shown in the following screenshot reveals differences in metrics such as answer_similarity (a difference of 1 point), providing actionable insights for pipeline optimization. 
 
Automation with Amazon SageMaker pipelines 
After systematically experimenting with and optimizing each component of the RAG workflow through SageMaker managed MLflow, the next step is transforming these validated configurations into production-ready automated pipelines. Although MLflow experiments help identify the optimal combination of chunking strategies, embedding models, and retrieval parameters, manually reproducing these configurations across environments can be error-prone and inefficient. 
To produce the automated RAG pipeline, we use SageMaker Pipelines, which helps teams codify their experimentally validated RAG workflows into automated, repeatable pipelines that maintain consistency from development through production. By converting the successful MLflow experiments into pipeline definitions, teams can make sure the exact same chunking, embedding, retrieval, and evaluation steps that performed well in testing are reliably reproduced in production environments. 
SageMaker Pipelines offers a serverless workflow orchestration for converting experimental notebook code into a production-grade pipeline, versioning and tracking pipeline configurations alongside MLflow experiments, and automating the end-to-end RAG workflow. The automated Sagemaker pipeline-based RAG workflow offers dependency management, comprehensive custom testing and validation before production deployment, and CI/CD integration for automated pipeline promotion. 
With SageMaker Pipelines, you can automate your entire RAG workflow, from data preparation to evaluation, as reusable, parameterized pipeline definitions. This provides the following benefits: 
 
 Reproducibility ‚Äì Pipeline definitions capture all dependencies, configurations, and executions logic in version-controlled code 
 Parameterization ‚Äì Key RAG parameters (chunk sizes, model endpoints, retrieval settings) can be quickly modified between runs 
 Monitoring ‚Äì Pipeline executions provide detailed logs and metrics for each step 
 Governance ‚Äì Built-in lineage tracking supports full audibility of data and model artifacts 
 Customization ‚Äì Serverless workflow orchestration is customizable to your unique enterprise landscape, with scalable infrastructure and flexibility with instances optimized for CPU, GPU, or memory-intensive tasks, memory configuration, and concurrency optimization 
 
To implement a RAG workflow in SageMaker pipelines, each major component of the RAG process (data preparation, chunking, ingestion, retrieval and generation, and evaluation) is included in a SageMaker processing job. These jobs are then orchestrated as steps within a pipeline, with data flowing between them, as shown in the following screenshot. This structure allows for modular development, quick debugging, and the ability to reuse components across different pipeline configurations. 
 
The key RAG configurations are exposed as pipeline parameters, enabling flexible experimentation with minimal code changes. For example, the following code snippets showcase the modifiable parameters for RAG configurations, which can be used as pipeline configurations: 
 
 processor&nbsp;&nbsp;PyTorchProcessor(
&nbsp; &nbsp; ...
&nbsp; &nbsp; arguments[
&nbsp; &nbsp; "--experiment-name",&nbsp;experiment_name,
&nbsp; &nbsp; "--mlflow-tracking-uri",&nbsp;mlflow_tracking_uri,
&nbsp; &nbsp; "--embedding-endpoint-name",&nbsp;embedding_endpoint_name,
&nbsp; &nbsp; "--text-endpoint-name",&nbsp;text_endpoint_name,
&nbsp; &nbsp; "--domain-name",&nbsp;domain_name,
&nbsp; &nbsp; "--index-name",&nbsp;index_name,
&nbsp; &nbsp; "--chunking-strategy",&nbsp;chunking_strategy,
&nbsp; &nbsp; "--chunk-size",&nbsp;chunk_size,
&nbsp; &nbsp; "--chunk-overlap",&nbsp;chunk_overlap,
&nbsp; &nbsp; "--context-retrieval-size",&nbsp;context_retrieval_size,
&nbsp; &nbsp; "--embedding-model-id",&nbsp;embedding_model_id,
&nbsp; &nbsp; "--text-model-id",&nbsp;text_model_id,
&nbsp; &nbsp; "--output-data-path",&nbsp;"/opt/ml/processing/output",
&nbsp; &nbsp; "--role-arn",&nbsp;role
&nbsp; &nbsp; ],
) 
 
In this post, we provide two agentic RAG pipeline automation approaches to building the SageMaker pipeline, each with own benefits: single-step SageMaker pipelines and multi-step pipelines. 
The single-step pipeline approach is designed for simplicity, running the entire RAG workflow as one unified process. This setup is ideal for straightforward or less complex use cases, because it minimizes pipeline management overhead. With fewer steps, the pipeline can start quickly, benefitting from reduced execution times and streamlined development. This makes it a practical option when rapid iteration and ease of use are the primary concerns. 
The multi-step pipeline approach is preferred for enterprise scenarios where flexibility and modularity are essential. By breaking down the RAG process into distinct, manageable stages, organizations gain the ability to customize, swap, or extend individual components as needs evolve. This design enables plug-and-play adaptability, making it straightforward to reuse or reconfigure pipeline steps for various workflows. Additionally, the multi-step format allows for granular monitoring and troubleshooting at each stage, providing detailed insights into performance and facilitating robust enterprise management. For enterprises seeking maximum flexibility and the ability to tailor automation to unique requirements, the multi-step pipeline approach is the superior choice. 
CI/CD for an agentic RAG pipeline 
Now we integrate the SageMaker RAG pipeline with CI/CD. CI/CD is important for making a RAG solution enterprise-ready because it provides faster, more reliable, and scalable delivery of AI-powered workflows. Specifically for enterprises, CI/CD pipelines automate the integration, testing, deployment, and monitoring of changes in the RAG system, which brings several key benefits, such as faster and more reliable updates, version control and traceability, consistency across environments, modularity and flexibility for customization, enhanced collaboration and monitoring, risk mitigation, and cost savings. This aligns with general CI/CD benefits in software and AI systems, emphasizing automation, quality assurance, collaboration, and continuous feedback essential to enterprise AI readiness. 
When your SageMaker RAG pipeline definition is in place, you can implement robust CI/CD practices by integrating your development workflow and toolsets already enabled at your enterprise. This setup makes it possible to automate code promotion, pipeline deployment, and model experimentation through simple Git triggers, so changes are versioned, tested, and systematically promoted across environments. For demonstration, in this post, we show the CI/CD integration using GitHub Actions and by using GitHub Actions as the CI/CD orchestrator. Each code change, such as refining chunking strategies or updating pipeline steps, triggers an end-to-end automation workflow, as shown in the following screenshot. You can use the same CI/CD pattern with your choice of CI/CD tool instead of GitHub Actions, if needed. 
 
Each GitHub Actions CI/CD execution automatically triggers the SageMaker pipeline (shown in the following screenshot), allowing for seamless scaling of serverless compute infrastructure. 
 
Throughout this cycle, SageMaker managed MLflow records every executed pipeline (shown in the following screenshot), so you can seamlessly review results, compare performance across different pipeline runs, and manage the RAG lifecycle. 
 
After an optimal RAG pipeline configuration is determined, the new desired configuration (Git version tracking captured in MLflow as shown in the following screenshot) can be promoted to higher stages or environments directly through an automated workflow, minimizing manual intervention and reducing risk. 
 
Clean up 
To avoid unnecessary costs, delete resources such as the SageMaker managed MLflow tracking server, SageMaker pipelines, and SageMaker endpoints when your RAG experimentation is complete. You can visit the SageMaker Studio console to destroy resources that aren‚Äôt needed anymore or call appropriate AWS APIs actions. 
Conclusion 
By integrating SageMaker AI, SageMaker managed MLflow, and Amazon OpenSearch Service, you can build, evaluate, and deploy RAG pipelines at scale. This approach provides the following benefits: 
 
 Automated and reproducible workflows with SageMaker Pipelines and MLflow, minimizing manual steps and reducing the risk of human error 
 Advanced experiment tracking and comparison for different chunking strategies, embedding models, and LLMs, so every configuration is logged, analyzed, and reproducible 
 Actionable insights from both traditional and LLM-based evaluation metrics, helping teams make data-driven improvements at every stage 
 Seamless deployment to production environments, with automated promotion of validated pipelines and robust governance throughout the workflow 
 
Automating your RAG pipeline with SageMaker Pipelines brings additional benefits: it enables consistent, version-controlled deployments across environments, supports collaboration through modular, parameterized workflows, and supports full traceability and auditability of data, models, and results. With built-in CI/CD capabilities, you can confidently promote your entire RAG solution from experimentation to production, knowing that each stage meets quality and compliance standards. 
Now it‚Äôs your turn to operationalize RAG workflows and accelerate your AI initiatives. Explore SageMaker Pipelines and managed MLflow using the solution from the GitHub repository to unlock scalable, automated, and enterprise-grade RAG solutions. 
 
About the authors 
Sandeep Raveesh is a GenAI Specialist Solutions Architect at AWS. He works with customers through their AIOps journey across model training, generative AI applications like agents, and scaling generative AI use cases. He also focuses on Go-To-Market strategies, helping AWS build and align products to solve industry challenges in the generative AI space. You can find Sandeep on LinkedIn. 
Blake Shin is an Associate Specialist Solutions Architect at AWS who enjoys learning about and working with new AI/ML technologies. In his free time, Blake enjoys exploring the city and playing music.
‚Ä¢ Unlock model insights with log probability support for Amazon Bedrock Custom Model Import
  You can use Amazon Bedrock Custom Model Import to seamlessly integrate your customized models‚Äîsuch as Llama, Mistral, and Qwen‚Äîthat you have fine-tuned elsewhere into Amazon Bedrock. The experience is completely serverless, minimizing infrastructure management while providing your imported models with the same unified API access as native Amazon Bedrock models. Your custom models benefit from automatic scaling, enterprise-grade security, and native integration with Amazon Bedrock features such as Amazon Bedrock Guardrails and Amazon Bedrock Knowledge Bases. 
Understanding how confident a model is in its predictions is essential for building reliable AI applications, particularly when working with specialized custom models that might encounter domain-specific queries. 
With log probability support now added to Custom Model Import, you can access information about your models‚Äô confidence in their predictions at the token level. This enhancement provides greater visibility into model behavior and enables new capabilities for model evaluation, confidence scoring, and advanced filtering techniques. 
In this post, we explore how log probabilities work with imported models in Amazon Bedrock. You will learn what log probabilities are, how to enable them in your API calls, and how to interpret the returned data. We also highlight practical applications‚Äîfrom detecting potential hallucinations to optimizing RAG systems and evaluating fine-tuned models‚Äîthat demonstrate how these insights can improve your AI applications, helping you build more trustworthy solutions with your custom models. 
Understanding log probabilities 
In language models, a log probability represents the logarithm of the probability that the model assigns to a token in a sequence. These values indicate how confident the model is about each token it generates or processes. Log probabilities are expressed as negative numbers, with values closer to zero indicating higher confidence. For example, a log probability of -0.1 corresponds to approximately 90% confidence, while a value of -3.0 corresponds to about 5% confidence. By examining these values, you can identify when a model is highly certain versus when it‚Äôs making less confident predictions. Log probabilities provide a quantitative measure of how likely the model considered each generated token, offering valuable insight into the confidence of its output. By analyzing them you can, 
 
 Gauge confidence across a response: Assess how confident the model was in different sections of its output, helping you identify where it was certain versus uncertain. 
 Score and compare outputs: Compare overall sequence likelihood (by adding or averaging log probabilities) to rank or filter multiple model outputs. 
 Detect potential hallucinations: Identify sudden drops in token-level confidence, which can flag segments that might require verification or review. 
 Reduce RAG costs with early pruning:&nbsp;Run short, low-cost draft generations based on retrieved contexts, compute log probabilities for those drafts, and discard low-scoring candidates early, avoiding&nbsp;unnecessary full-length generations or expensive reranking while keeping only the most promising contexts in the pipeline. 
 Build confidence-aware applications: Adapt system behavior based on certainty levels‚Äîfor example, trigger clarifying prompts, provide fallback responses, or flagging for human review. 
 
Overall, log probabilities are a powerful tool for interpreting and debugging model responses with measurable certainty‚Äîparticularly valuable for applications where understanding why a model responded in a certain way can be as important as the response itself. 
Prerequisites 
To use log probability support with custom model import in Amazon Bedrock, you need: 
 
 An active AWS account with access to Amazon Bedrock 
 A custom model created in Amazon Bedrock using the Custom Model Import feature after July 31, 2025, when the log probabilities support was released 
 Appropriate AWS Identity and Access Management (IAM) permissions to invoke models through the Amazon Bedrock Runtime 
 
Introducing log probabilities support in Amazon Bedrock 
With this release, Amazon Bedrock now allows models imported using the Custom Model Import feature to return token-level log probabilities as part of the inference response. 
When invoking a model through Amazon Bedrock InvokeModel API, you can access token log probabilities by setting "return_logprobs": true in the JSON request body. With this flag enabled, the model‚Äôs response will include additional fields providing log probabilities for both the prompt tokens and the generated tokens, so that customers can analyze the model‚Äôs confidence in its predictions.&nbsp;These log probabilities let you quantitatively assess how confident your custom models are when processing inputs and generating responses. The granular metrics allow for better evaluation of response quality, troubleshooting of unexpected outputs, and optimization of prompts or model configurations. 
Let‚Äôs walk through an example of invoking a custom model on Amazon Bedrock with log probabilities enabled and examine the output format. Suppose you have already imported a custom model (for instance, a fine-tuned Llama 3.2 1B model) into Amazon Bedrock and have its model Amazon Resource Name (ARN). You can invoke this model using the Amazon Bedrock Runtime SDK (Boto3 for Python in this example) as shown in the following example: 
 
 import boto3, json

bedrock_runtime = boto3.client('bedrock-runtime') &nbsp;
model_arn = "arn:aws:bedrock:&lt;&lt;aws-region&gt;&gt;:&lt;&lt;account-id&gt;&gt;:imported-model/your-model-id"

# Define the request payload with log probabilties enabled
request_payload = {
&nbsp;&nbsp; &nbsp;"prompt": "The quick brown fox jumps",
&nbsp;&nbsp; &nbsp;"max_gen_len": 50,
&nbsp;&nbsp; &nbsp;"temperature": 0.5,
&nbsp;&nbsp; &nbsp;"stop": [".", "\n"],
&nbsp;&nbsp; &nbsp;"return_logprobs": True &nbsp; # Request log probabilities
}

response = bedrock_runtime.invoke_model(
&nbsp;&nbsp; &nbsp;modelId=model_arn,
&nbsp;&nbsp; &nbsp;body=json.dumps(request_payload),
&nbsp;&nbsp; &nbsp;contentType="application/json",
&nbsp;&nbsp; &nbsp;accept="application/json"
)

# Parse the JSON response
result = json.loads(response["body"].read())
print(json.dumps(result, indent=2)) 
 
In the preceding code, we send a prompt‚Äî"The quick brown fox jumps"‚Äîto our custom imported model. We configure standard inference parameters: a maximum generation length of 50 tokens, a moderate temperature of 0.5 for moderate randomness, and a stop condition (either a period or a newline). The "return_logprobs":True parameter tells Amazon Bedrock to return log probabilities in the response. 
The InvokeModel API returns a JSON response containing three main components: the standard generated text output, metadata about the generation process, and now log probabilities for both prompt and generated tokens. These values reveal the model‚Äôs internal confidence for each token prediction, so you can understand not just what text was produced, but how certain the model was at each step of the process. The following is an example response from the "quick brown fox jumps" prompt, showing log probabilities (appearing as negative numbers): 
 
 {
  'prompt_logprobs': [
    None,
    {'791': -3.6223082542419434, '14924': -1.184808373451233},
    {'4062': -9.256651878356934, '220': -3.6941518783569336},
    {'14198': -4.840845108032227, '323': -1.7158453464508057},
    {'39935': -0.049946799874305725},
    {'35308': -0.2087990790605545}
  ],
  'generation': ' over the lazy dog',
  'prompt_token_count': 6,
  'generation_token_count': 5,
  'stop_reason': 'stop',
  'logprobs': [
    {'927': -0.04093993827700615},
    {'279': -0.0728893131017685},
    {'16053': -0.02005653828382492},
    {'5679': -0.03769925609230995},
    {'627': -1.194122076034546}
  ]
} 
 
The raw API response provides token IDs paired with their log probabilities. To make this data interpretable, we need to first decode the token IDs using the appropriate tokenizer (in this case, the Llama 3.2 1B tokenizer), which maps each ID back to its actual text token. Then we convert log probabilities to probabilities by applying the exponential function, translating these values into more intuitive probabilities between 0 and 1. We have implemented these transformations using custom code (not shown here) to produce a human-readable format where each token appears alongside its probability, making the model‚Äôs confidence in its predictions immediately clear. 
 
 {'prompt_logprobs': [None,
&nbsp;&nbsp;{'791': "'The' (p=0.0267)", '14924': "'Question' (p=0.3058)"},
&nbsp;&nbsp;{'4062': "' quick' (p=0.0001)", '220': "' ' (p=0.0249)"},
&nbsp;&nbsp;{'14198': "' brown' (p=0.0079)", '323': "' and' (p=0.1798)"},
&nbsp;&nbsp;{'39935': "' fox' (p=0.9513)"},
&nbsp;&nbsp;{'35308': "' jumps' (p=0.8116)"}],
&nbsp;'generation': ' over the lazy dog',
&nbsp;'prompt_token_count': 6,
&nbsp;'generation_token_count': 5,
&nbsp;'stop_reason': 'stop',
&nbsp;'logprobs': [{'927': "' over' (p=0.9599)"},
&nbsp;&nbsp;{'279': "' the' (p=0.9297)"},
&nbsp;&nbsp;{'16053': "' lazy' (p=0.9801)"},
&nbsp;&nbsp;{'5679': "' dog' (p=0.9630)"},
&nbsp;&nbsp;{'627': "'.\n' (p=0.3030)"}]} 
 
Let‚Äôs break down what this tells us about the model‚Äôs internal processing: 
 
 generation: This is the actual text generated by the model (in our example, it‚Äôs a continuation of the prompt that we sent to the model). This is the same field you would get normally from any model invocation. 
 prompt_token_count and generation_token_count: These indicate the number of tokens in the input prompt and in the output, respectively. In our example, the prompt was tokenized into six tokens, and the model generated five tokens in its completion. 
 stop_reason: The reason the generation stopped ("stop" means the model naturally stopped at a stop sequence or end-of-text, "length" means it hit the max token limit, and so on). In our case it shows "stop", indicating the model stopped on its own or because of the stop condition we provided. 
 prompt_logprobs: This array provides log probabilities for each token in the prompt. As the model processes your input, it continuously predicts what should come next based on what it has seen so far. These values measure which tokens in your prompt were expected or surprising to the model. 
   
   The first entry is None because the very first token has no preceding context. The model cannot predict anything without prior information. Each subsequent entry contains token IDs mapped to their log probabilities. We have converted these IDs to readable text and transformed the log probabilities into percentages for easier understanding. 
   You can observe the model‚Äôs increasing confidence as it processes familiar sequences. For example, after seeing The quick brown, the model predicted fox with 95.1% confidence. After seeing the full context up to fox, it predicted jumps with 81.1% confidence. 
   Many positions show multiple tokens with their probabilities, revealing alternatives the model considered. For instance, at the second position, the model evaluated both The (2.7%) and Question (30.6%), which means the model considered both tokens viable at that position. This added visibility helps you understand where the model weighted alternatives and can reveal when it was more uncertain or had difficulty choosing from multiple options. 
   Notably low probabilities appear for some tokens‚Äîquick received just 0.01%‚Äîindicating the model found these words unexpected in their context. 
   The overall pattern tells a clear story: individual words initially received low probabilities, but as the complete quick brown fox jumps phrase emerged, the model‚Äôs confidence increased dramatically, showing it recognized this as a familiar expression. 
   When multiple tokens in your prompt consistently receive low probabilities, your phrasing might be unusual for the model. This uncertainty can affect the quality of completions. Using these insights, you can reformulate prompts to better align with patterns the model encountered in its training data. 
    
 logprobs: This array contains log probabilities for each token in the model‚Äôs generated output.&nbsp;The format is similar: a dictionary mapping token IDs to their corresponding log probabilities. 
   
   After decoding these values, we can see that the tokens over, the, lazy, and dog all have high probabilities. This demonstrates the model recognized it was completing the well-known phrase the quick brown fox jumps over the lazy dog‚Äîa common pangram that the model appears to have strong familiarity with. 
   In contrast, the final period (newline) token has a much lower probability (30.3%), revealing the model‚Äôs uncertainty about how to conclude the sentence. This makes sense because the model had multiple valid options: ending the sentence with a period, continuing with additional content, or choosing another punctuation mark altogether. 
    
 
Practical use cases of log probabilities 
Token-level log probabilities from the Custom Model Import feature provide valuable insights into your model‚Äôs decision-making process. These metrics transform how you interact with your custom models by revealing their confidence levels for each generated token. Here are impactful ways to use these insights: 
Ranking multiple completions 
You can use log probabilities to quantitatively rank multiple generated outputs for the same prompt. When your application needs to choose between different possible completions‚Äîwhether for summarization, translation, or creative writing‚Äîyou can calculate each completion‚Äôs overall likelihood by averaging or adding the log probabilities across all its tokens. 
Example: 
Prompt: Translate the phrase "Battre le fer pendant qu'il est chaud" 
 
 Completion A: "Strike while the iron is hot" (Average log probability: -0.39) 
 Completion B: "Beat the iron while it is hot." (Average log probability: -0.46) 
 
In this example, Completion A receives a higher log probability score (closer to zero), indicating the model found this idiomatic translation more natural than the more literal Completion B. This numerical approach enables your application to automatically select the most probable output or present multiple candidates ranked by the model‚Äôs confidence level. 
This ranking capability extends beyond translation to many scenarios where multiple valid outputs exist‚Äîincluding content generation, code completion, and creative writing‚Äîproviding an objective quality metric based on the model‚Äôs confidence rather than relying solely on subjective human judgment. 
Detecting hallucinations and low-confidence answers 
Models might produce hallucinations‚Äîplausible-sounding but factually incorrect statements‚Äîwhen handling ambiguous prompts, complex queries, or topics outside their expertise. Log probabilities provide a practical way to detect these instances by revealing the model‚Äôs internal uncertainty, helping you identify potentially inaccurate information even when the output appears confident. 
By analyzing token-level log probabilities, you can identify which parts of a response the model was potentially uncertain about, even when the text appears confident on the surface. This capability is especially valuable in retrieval-augmented generation (RAG) systems, where responses should be grounded in retrieved context. When a model has relevant information available, it typically generates answers with higher confidence. Conversely, low confidence across multiple tokens suggests the model might be generating content without sufficient supporting information. 
Example: 
 
 Prompt: 
   
   "Explain how the Portfolio Synergy Quotient (PSQ) is applied in multi-asset investment
 strategies?" 
    
 Model output: 
   
   "The PSQ is a measure of the diversification benefits of combining different asset 
 classes in a portfolio." 
    
 
In this example, we intentionally asked about a fictional metric‚ÄîPortfolio Synergy Quotient (PSQ)‚Äîto demonstrate how log probabilities reveal uncertainty in model responses. Despite producing a professional-sounding definition for this non-existent financial concept, the token-level confidence scores tell a revealing story. The confidence scores shown below are derived by applying the exponential function to the log probabilities returned by the model. 
 
 PSQ shows medium confidence (63.8%), indicating that the model recognized the acronym format but wasn‚Äôt highly certain about this specific term. 
 Common finance terminology like classes (98.2%) and portfolio (92.8%) exhibit high confidence, likely because these are standard concepts widely used in financial contexts. 
 Critical connecting concepts show notably low confidence: measure (14.0%) and diversification (31.8%), reveal the model‚Äôs uncertainty when attempting to explain what PSQ means or does. 
 Functional words like is (45.9%) and of (56.6%) hover in the medium confidence levels, suggesting uncertainty about the overall structure of the explanation. 
 
By identifying these low-confidence segments, you can implement targeted safeguards in your applications‚Äîsuch as flagging content for verification, retrieving additional context, generating clarifying questions, or applying confidence thresholds for sensitive information. This approach helps create more reliable AI systems that can distinguish between high-confidence knowledge and uncertain responses. 
Monitoring prompt quality 
When engineering prompts for your application, log probabilities reveal how well the model understands your instructions. If the first few generated tokens show unusually low probabilities, it often signals that the model struggled to interpret what you are asking. 
By tracking the average log probability of the initial tokens‚Äîtypically the first 5‚Äì10 generated tokens‚Äîyou can quantitatively measure prompt clarity. Well-structured prompts with clear context typically produce higher probabilities because the model immediately knows what to do. Vague or underspecified prompts often yield lower initial token likelihoods as the model hesitates or searches for direction. 
Example: 
Prompt comparison for customer service responses: 
 
 Basic prompt: 
   
   "Write a response to this customer complaint: I ordered a laptop 2 weeks ago and it 
 still hasn't arrived." 
   
   
   Average log probability of first five tokens: -1.215 (lower confidence) 
    
 Optimized prompt: 
   
   "You are a senior customer service manager with expertise in conflict resolution and 
 customer retention. You work for a reputable electronics retailer that values 
 customer satisfaction above all else. Your task is to respond to the following 
 customer complaint with professionalism and empathy. 
 Customer Complaint: I ordered a laptop 2 weeks ago and it still hasn't arrived." 
   
   
   Average log probability of first five tokens: -0.333 (higher confidence) 
    
 
The optimized prompt generates higher log probabilities, demonstrating that precise instructions and clear context reduce the model‚Äôs uncertainty. Rather than making absolute judgments about prompt quality, this approach lets you measure relative improvement between versions. You can directly observe how specific elements‚Äîrole definitions, contextual details, and explicit expectations‚Äîincrease model confidence. By systematically measuring these confidence scores across different prompt iterations, you build a quantitative framework for prompt engineering that reveals exactly when and how your instructions become unclear to the model, enabling continuous data-driven refinement. 
Reducing RAG costs with early pruning 
In traditional RAG implementations, systems retrieve 5‚Äì20 documents and generate complete responses using these retrieved contexts. This approach drives up inference costs because every retrieved context consumes tokens regardless of actual usefulness. 
Log probabilities enable a more cost-effective alternative through early pruning. Instead of immediately processing the retrieved documents in full: 
 
 Generate draft responses based on each retrieved context 
 Calculate the average log probability across these short drafts 
 Rank contexts by their average log probability scores 
 Discard low-scoring contexts that fall below a confidence threshold 
 Generate the complete response using only the highest-confidence contexts 
 
This approach works because contexts that contain relevant information produce higher log probabilities in the draft generation phase. When the model encounters helpful context, it generates text with greater confidence, reflected in log probabilities closer to zero. Conversely, irrelevant or tangential contexts produce more uncertain outputs with lower log probabilities. 
By filtering contexts before full generation, you can reduce token consumption while maintaining or even improving answer quality. This shifts the process from a brute-force approach to a targeted pipeline that directs full generation only toward contexts where the model demonstrates genuine confidence in the source material. 
Fine-tuning evaluation 
When you have fine-tuned a model for your specific domain, log probabilities offer a quantitative way to assess the effectiveness of your training. By analyzing confidence patterns in responses, you can determine if your model has developed proper calibration‚Äîshowing high confidence for correct domain-specific answers and appropriate uncertainty elsewhere. 
A well-calibrated fine-tuned model should assign higher probabilities to accurate information within its specialized area while maintaining lower confidence when operating outside its training domain. Problems with calibration appear in two main forms. Overconfidence occurs when the model assigns high probabilities to incorrect responses, suggesting it hasn‚Äôt properly learned the boundaries of its knowledge. Under confidence manifests as consistently low probabilities despite generating accurate answers, indicating that training might not have sufficiently reinforced correct patterns. 
By systematically testing your model across various scenarios and analyzing the log probabilities, you can identify areas needing additional training or detect potential biases in your current approach. This creates a data-driven feedback loop for iterative improvements, making sure your model performs reliably within its intended scope while maintaining appropriate boundaries around its expertise. 
Getting started 
Here‚Äôs how to start using log probabilities with models imported through the Amazon Bedrock Custom Model Import feature: 
 
 Enable log probabilities in your API calls: Add "return_logprobs": true to your request payload when invoking your custom imported model. This parameter works with both the InvokeModel and InvokeModelWithResponseStream APIs. Begin with familiar prompts to observe which tokens your model predicts with high confidence compared to which it finds surprising. 
 Analyze confidence patterns in your custom models: Examine how your fine-tuned or domain-adapted models respond to different inputs. The log probabilities reveal whether your model is appropriately calibrated for your specific domain‚Äîshowing high confidence where it should be certain. 
 Develop confidence-aware applications: Implement practical use cases such as hallucination detection, response ranking, and content verification to make your applications more robust. For example, you can flag low-confidence sections of responses for human review or select the highest-confidence response from multiple generations. 
 
Conclusion 
Log probability support for Amazon Bedrock Custom Model Import offers enhanced visibility into model decision-making. This feature transforms previously opaque model behavior into quantifiable confidence metrics that developers can analyze and use. 
Throughout this post, we have demonstrated how to enable log probabilities in your API calls, interpret the returned data, and use these insights for practical applications. From detecting potential hallucinations and ranking multiple completions to optimizing RAG systems and evaluating fine-tuning quality, log probabilities offer tangible benefits across diverse use cases. 
For customers working with customized foundation models like Llama, Mistral, or Qwen, these insights address a fundamental challenge: understanding not just what a model generates, but how confident it is in its output. This distinction becomes critical when deploying AI in domains requiring high reliability‚Äîsuch as finance, healthcare, or enterprise applications‚Äîwhere incorrect outputs can have significant consequences. 
By revealing confidence patterns across different types of queries, log probabilities help you assess how well your model customizations have affected calibration, highlighting where your model excels and where it might need refinement. Whether you are evaluating fine-tuning effectiveness, debugging unexpected responses, or building systems that adapt to varying confidence levels, this capability represents an important advancement in bringing greater transparency and control to generative AI development on Amazon Bedrock. 
We look forward to seeing how you use log probabilities to build more intelligent and trustworthy applications with your custom imported models. This capability demonstrates the commitment from Amazon Bedrock to provide developers with tools that enable confident innovation while delivering the scalability, security, and simplicity of a fully managed service. 
 
About the authors 
Manoj Selvakumar is a Generative AI Specialist Solutions Architect at AWS, where he helps organizations design, prototype, and scale AI-powered solutions in the cloud. With expertise in deep learning, scalable cloud-native systems, and multi-agent orchestration, he focuses on turning emerging innovations into production-ready architectures that drive measurable business value. He is passionate about making complex AI concepts practical and enabling customers to innovate responsibly at scale‚Äîfrom early experimentation to enterprise deployment. Before joining AWS, Manoj worked in consulting, delivering data science and AI solutions for enterprise clients, building end-to-end machine learning systems supported by strong MLOps practices for training, deployment, and monitoring in production. 
Yanyan Zhang&nbsp;is a Senior Generative AI Data Scientist at Amazon Web Services, where she has been working on cutting-edge AI/ML technologies as a Generative AI Specialist, helping customers use generative AI to achieve their desired outcomes. Yanyan graduated from Texas A&amp;M University with a PhD in Electrical Engineering. Outside of work, she loves traveling, working out, and exploring new things. 
Lokeshwaran Ravi is a Senior Deep Learning Compiler Engineer at AWS, specializing in ML optimization, model acceleration, and AI security. He focuses on enhancing efficiency, reducing costs, and building secure ecosystems to democratize AI technologies, making cutting-edge ML accessible and impactful across industries. 
Revendra Kumar is a Senior Software Development Engineer at Amazon Web Services. In his current role, he focuses on model hosting and inference MLOps on Amazon Bedrock. Prior to this, he worked as an engineer on hosting Quantum computers on the cloud and developing infrastructure solutions for on-premises cloud environments. Outside of his professional pursuits, Revendra enjoys staying active by playing tennis and hiking.
‚Ä¢ Migrate from Anthropic‚Äôs Claude 3.5 Sonnet to Claude 4 Sonnet on Amazon Bedrock
  This post is co-written with Gareth Jones from Anthropic. 
Anthropic‚Äôs Claude 4 Sonnet model has launched on Amazon Bedrock, marking a significant advancement in foundation model capabilities. Consequently, the deprecation timeline for Anthropic‚Äôs Claude 3.5 Sonnet (v1 and v2) was announced. This evolution creates a dual imperative for production AI applications: the opportunity to harness enhanced performance and the operational necessity to migrate before deprecation. Organizations must treat model migrations as a core component of their AI inference strategy, because poor execution can result in service disruptions, performance regressions, and cost overruns. 
This post provides a systematic approach to migrating from Anthropic‚Äôs Claude 3.5 Sonnet to Claude 4 Sonnet on Amazon Bedrock. We examine the key model differences, highlight essential migration considerations, and deliver proven best practices to transform this necessary transition into a strategic advantage that drives measurable value for your organization. 
Overview of model differences 
Understanding the specific changes between model versions is the first step in planning a successful migration. The migration from Anthropic‚Äôs Claude Sonnet 3.5 Sonnet to Claude 4 Sonnet introduces capability and behavioral shifts that you can take advantage of: 
 
 Increased context window ‚Äì Anthropic‚Äôs Claude 4 Sonnet expands the context window from 200,000 tokens to 1 million tokens (beta). This allows applications to process and reason over codebases, financial reports, or lengthy legal documents in a single prompt, simplifying complex workflows. 
 Native reasoning mechanisms ‚Äì Whereas Anthropic‚Äôs Claude 3.5 Sonnet models rely on the chain-of-thought (CoT) prompting technique, Claude 4 models introduce built-in, API-enabled reasoning features like extended thinking and interleaved thinking. These give the model dedicated computational time to reason before answering, dramatically improving performance on complex problems. 
 Advanced tool use ‚Äì Anthropic‚Äôs Claude 4 Sonnet model significantly upgrades tool use capabilities. They can execute multiple tools in parallel and use extended thinking between tool calls, enabling more sophisticated and efficient agentic workflows compared to the sequential tool use of older models. 
 
To find out more about the model differences, refer to the Complete Model Comparison Guide. 
A successful migration hinges on a few critical technical and strategic considerations, which we discuss in the following sections. Addressing these points proactively can minimize risk and accelerate your path to production. 
Prerequisites 
Before you can start using Anthropic‚Äôs Claude 4 Sonnet model, you must enable access to these models in Amazon Bedrock. For instructions, see Request access to an Amazon Bedrock foundation model. Make sure you review and accept the model‚Äôs End User License Agreement (EULA) proceeding with the model access request. It is crucial to first confirm that the Claude 4 Sonnet is available in your intended AWS Region, because model support can differ depending on location. For an updated list of supported Regions, refer to Model support by AWS Region in Amazon Bedrock. Additionally, you can use cross-Region inference (CRIS) by specifying an inference profile when running model inference, which can help improve throughput and maximize resource availability. For a list of supported cross-Region inference profiles, refer to Supported Regions and models for inference profiles. 
API changes and code updates 
When migrating on Amazon Bedrock, you can use either the model-specific InvokeModel API or the unified Converse API. 
If you use the InvokeModel API, the migration is straightforward. You only need to update the modelId in your code, because the request body structure for Anthropic‚Äôs Messages API remains consistent between versions: 
 
 Old model ID ‚Äì 'anthropic.claude-3-5-sonnet-20240620-v1:0' or 'anthropic.claude-3-5-sonnet-20241022-v2:0' 
 New model ID ‚Äì 'anthropic.claude-4-sonnet-20240514-v1:0' 
 
If you use a CRIS profile for the model, be sure to specify the correct inference profile ID in one of the source Regions, for example, 'us.anthropic.claude-sonnet-4-20250514-v1:0'. 
This migration is an excellent opportunity to switch to the Converse API. It provides a standardized request/response format for language models on Amazon Bedrock, making future migrations to different models or providers much simpler. See the following code: 
 
 import&nbsp;boto3

bedrock_runtime = boto3.client(service_name='bedrock-runtime')
# Simply change the model ID to switch models
response = bedrock_runtime.converse(
&nbsp;&nbsp; &nbsp;modelId='us.anthropic.claude-sonnet-4-20250514-v1:0',
&nbsp;&nbsp; &nbsp;messages=[{'role': 'user', 'content': [{'text': "Your prompt here"}]}],
&nbsp; &nbsp; inferenceConfig={'maxTokens': 1024}
)

print(response['output']['message']['content'][0]['text']) 
 
The following are some key changes: 
 
 Updated text editor tool ‚Äì Developers using Anthropic‚Äôs built-in text editor tool for agentic workflows must be aware that the tool definition has been updated. The tool type is now text_editor_20250124 and tool name is now str_replace_based_edit_tool. The computer-use-2024-10-22 in the anthropic_beta header can be removed because the text editor tool is generally available in Anthropic‚Äôs Claude 4. Any code using this tool must be updated to reflect these changes. For more details, see Anthropic defined tools. 
 Removed undo_edit command ‚Äì The undo_edit command is no longer supported in Anthropic‚Äôs Claude 4 Sonnet. Any code using this command should be removed. 
 New refusal stop reason ‚Äì The model now introduces a new refusal stop reason. This reason is returned when the model declines to generate content due to safety policies. Application logic that handles model responses must be updated to recognize and manage this new stop reason to help prevent unexpected behavior or errors. For example, when you receive a refusal stop reason in a multi-turn interaction, you must reset the conversation context by either removing or updating the turn that was refused. 
 
Prompt engineering and behavioral shifts 
Don‚Äôt assume your existing prompts will work perfectly with the new model. Adherence to model-specific best practices is key. 
Refer to Claude 4 prompt engineering best practices released by Anthropic to achieve optimal results. For example, Anthropic‚Äôs Claude models perform best when prompts are structured with XML tags (for example, &lt;document&gt;, &lt;/document&gt;) to clearly separate different parts of the input. 
Anthropic‚Äôs Claude 4 Sonnet is designed to follow instructions more precisely. This is a major advantage, but might mean it is less verbose or elaborative than Claude 3.5 Sonnet unless explicitly prompted to be. Your system prompts defining the AI‚Äôs persona might need adjustments. 
New reasoning features 
Anthropic‚Äôs Claude 4 Sonnet model‚Äôs built-in extended thinking is a powerful capability. To unlock the deep reasoning capabilities of the new models, developers can enable extended thinking by including the thinking keyword argument in their API call. But make sure to use it strategically‚Äîextended thinking incurs additional costs because reasoning tokens are billed as output tokens at standard model rates. You will be charged for the full thinking process (not just the summary you see), and costs can scale with the thinking budget you set. 
We recommend enabling extended thinking for tasks requiring deep, multi-step analysis where accuracy is paramount (for example, complex scientific problems or advanced coding challenges). 
We recommend disabling extended thinking for simple queries or latency-sensitive applications. The feature adds to both cost (reasoning tokens are billed) and response time. For many tasks, a well-crafted CoT prompt is still the most efficient choice. 
To use extended thinking, simply update the Converse API call by adding the additionalModelRequestFields parameter with the thinking configuration. You can specify the maximum thinking tokens that the model can use by setting a value for budget_tokens. Note that maxTokens should be larger than budget_tokens for extended thinking. See the following code: 
 
 import boto3

bedrock_runtime = boto3.client(service_name='bedrock-runtime')

# Add configurations to enable extended thinking
response = bedrock_runtime.converse(
&nbsp;&nbsp; &nbsp;modelId='us.anthropic.claude-sonnet-4-20250514-v1:0',
&nbsp;&nbsp; &nbsp;messages=[{'role': 'user', 'content': [{'text': "Your prompt here"}]}],
&nbsp;&nbsp; &nbsp;inferenceConfig={'maxTokens': 2048},
&nbsp;&nbsp; &nbsp;additionalModelRequestFields={"thinking":{"type": "enabled",
&nbsp;&nbsp;&nbsp;&nbsp; "budget_tokens": 1024}}
)

reasoning_text&nbsp;=&nbsp;response['output']['message']['content'][0]['reasoningContent']['reasoningText']['text']
assistant_response =&nbsp;response['output']['message']['content'][1]['text'] 
 
The following table shows an example of extended thinking and how it can improve response accuracy. We prompt Anthropic‚Äôs Claude 3.5 Sonnet v2 and Claude 4 Sonnet (with thinking enabled) with an example prompt: 
 
 Alan, Bob, Colin, Dave and Emily are standing in a circle. Alan is on Bob's immediate left. Bob is on Colin's immediate left. Colin is on Dave's immediate left. Dave is on Emily's immediate left. Who is on Alan's immediate right? 
 
The table shows responses from the two models. With extended thinking, the model is able to carefully reason through the problem and arrive at the correct answer. 
 
  
   
   Model 
   Extended Thinking 
   Output 
   Correct Answer 
   
   
   Claude 3.5 Sonnet v2 
   N/A 
   Let me solve this step by step. First, let‚Äôs understand what we know: Alan is on Bob‚Äôs immediate left Bob is on Colin‚Äôs immediate left Colin is on Dave‚Äôs immediate left Dave is on Emily‚Äôs immediate left Starting with any person, let‚Äôs arrange them in order: If Dave is on Emily‚Äôs immediate left, then Emily is on Dave‚Äôs immediate right If Colin is on Dave‚Äôs immediate left, then Dave is on Colin‚Äôs immediate right If Bob is on Colin‚Äôs immediate left, then Colin is on Bob‚Äôs immediate right If Alan is on Bob‚Äôs immediate left, then Bob is on Alan‚Äôs immediate right So the order going clockwise is: Alan ‚Üí Bob ‚Üí Colin ‚Üí Dave ‚Üí Emily Since they‚Äôre in a circle, Emily must be next to Alan to complete the circle. Therefore, Emily is on Alan‚Äôs immediate right. The answer is Emily. 
   No 
   
   
   Claude 4 Sonnet 
   Let me work through this step by step. We have 5 people standing in a circle: Alan, Bob, Colin, Dave, and Emily. Let me trace through the given information: Alan is on Bob‚Äôs immediate left Bob is on Colin‚Äôs immediate left Colin is on Dave‚Äôs immediate left Dave is on Emily‚Äôs immediate left Since they‚Äôre in a circle, I need to figure out the order. Let me start with one person and work around. If Alan is on Bob‚Äôs immediate left, that means going clockwise: Alan, then Bob. If Bob is on Colin‚Äôs immediate left, that means going clockwise: Bob, then Colin. If Colin is on Dave‚Äôs immediate left, that means going clockwise: Colin, then Dave. If Dave is on Emily‚Äôs immediate left, that means going clockwise: Dave, then Emily. So going clockwise around the circle: Alan ‚Üí Bob ‚Üí Colin ‚Üí Dave ‚Üí Emily But wait, since it‚Äôs a circle, Emily must connect back to someone. Since we have Alan ‚Üí Bob ‚Üí Colin ‚Üí Dave ‚Üí Emily, and this is a complete circle of 5 people, Emily must be followed by Alan. So the complete circle clockwise is: Alan ‚Üí Bob ‚Üí Colin ‚Üí Dave ‚Üí Emily ‚Üí Alan This means Emily is on Alan‚Äôs immediate left, and therefore Alan‚Äôs immediate right would be‚Ä¶ let me think about this carefully. If we go clockwise: Alan ‚Üí Bob ‚Üí Colin ‚Üí Dave ‚Üí Emily ‚Üí (back to Alan) So Alan‚Äôs immediate right (going clockwise from Alan) is Bob. And Alan‚Äôs immediate left (going counter-clockwise from Alan) is Emily. Wait, let me double-check this by verifying the given conditions: Alan is on Bob‚Äôs immediate left ‚úì (Alan comes right before Bob going clockwise) Bob is on Colin‚Äôs immediate left ‚úì (Bob comes right before Colin going clockwise) Colin is on Dave‚Äôs immediate left ‚úì (Colin comes right before Dave going clockwise) Dave is on Emily‚Äôs immediate left ‚úì (Dave comes right before Emily going clockwise) I confirm Bob sits directly to Alan‚Äôs right in the circular arrangement. 
   Looking at this step by step: Given information: Alan is on Bob‚Äôs immediate left Bob is on Colin‚Äôs immediate left Colin is on Dave‚Äôs immediate left Dave is on Emily‚Äôs immediate left Let me trace the clockwise order around the circle: Alan ‚Üí Bob (since Alan is on Bob‚Äôs immediate left) Bob ‚Üí Colin (since Bob is on Colin‚Äôs immediate left) Colin ‚Üí Dave (since Colin is on Dave‚Äôs immediate left) Dave ‚Üí Emily (since Dave is on Emily‚Äôs immediate left) Since they‚Äôre standing in a circle, Emily must connect back to complete the circle: Emily ‚Üí Alan So the complete clockwise order is: Alan ‚Üí Bob ‚Üí Colin ‚Üí Dave ‚Üí Emily ‚Üí Alan Therefore, Bob is on Alan‚Äôs immediate right. 
   Yes 
   
  
 
With extended thinking enabled, the API will, by default, return a summary of the model‚Äôs full thought process rather than the entire chain of reasoning. This is a safeguard designed to prevent misuse while still providing the full intelligence benefits. Developers should be aware that this can result in a chunky or delayed pattern in streaming responses and should account for this in their application‚Äôs user experience. 
Anthropic‚Äôs Claude 4 models feature the interleaved thinking capability when using extended thinking with tools, allowing the model to perform intermediate reasoning between tool calls and develop more nuanced analytical conclusions based on the tool results received. To enable interleaved thinking for tool calling, add the additional parameter "anthropic_beta": ["interleaved-thinking-2025-05-14"] to additionalModelRequestFields in the Converse API request. 
For more details on using extended thinking, refer to Extended thinking. 
Robust evaluation is non-negotiable 
You must verify that the new model performs as well as or better than the old one on your specific tasks. Create a curated set of prompts and expected outputs that are highly representative of your production traffic. This custom benchmark is far more valuable than reported benchmark scores. 
Integrate this dataset into an automated evaluation pipeline. This pipeline should become part of your continuous integration and continuous delivery (CI/CD) process, serving as a permanent regression suite for future model or prompt changes. For tooling, teams can use built-in solutions like Amazon Bedrock evaluations or open source evaluation frameworks like RAGAS and DeepEval. 
Managing integrated safety and governance 
A model‚Äôs safety profile changes with each version. This must be tested as part of an integrated system. Never test the new model in isolation. Your migration test plan is only valid if it evaluates the new model with the exact same guardrail configuration you use in production, for example, Amazon Bedrock Guardrails. A new model‚Äôs conversational style might trigger existing guardrails in unexpected ways, leading to a spike in blocked responses. 
Implementing safe deployment strategies 
When deploying Anthropic‚Äôs Claude 4 Sonnet to production, implement a phased rollout strategy to minimize risks. Consider shadow testing to compare model performance using mirrored traffic without impacting users, then conduct A/B testing to measure impact on business KPIs. For the actual rollout, use either a canary release approach by gradually exposing a small percentage of users to the new model, or a blue/green deployment strategy that maintains parallel environments for instant rollback capability. This structured approach facilitates safe validation of the new model‚Äôs performance while maintaining business continuity. 
Conclusion 
By treating the transition from Anthropic‚Äôs Claude 3.5 Sonnet (v1 and v2) to Claude 4 Sonnet as a structured engineering project, you can de-risk the migration process and unlock significant benefits. Understanding the key model differences, adapting your prompts and API calls, and implementing a robust, automated evaluation strategy are the pillars of a successful upgrade. 
This transition is an opportunity to maintain business continuity and enhance your application with next-generation capabilities. We encourage you to begin your analysis and testing today. 
For more details, refer to Migrating to Claude 4 and Anthropic‚Äôs Claude in Amazon Bedrock, and reach out to us for migration-specific inquiries. 
 
About the authors 
Melanie Li, PhD, is a Senior Generative AI Specialist Solutions Architect at AWS based in Sydney, Australia, where her focus is on working with customers to build solutions using state-of-the-art AI/ML tools. She has been actively involved in multiple generative AI initiatives across APJ, harnessing the power of LLMs. Prior to joining AWS, Dr. Li held data science roles in the financial and retail industries. 
Deepak Dalakoti, PhD, is a Deep Learning Architect at the Generative AI Innovation Centre in Sydney, Australia. With expertise in AI, he partners with clients to accelerate their generative AI adoption through customized, innovative solutions. Outside the world of AI, he enjoys exploring new activities and experiences. 
Mahsa Paknezhad, PhD, is a Deep Learning Architect at the AWS Generative AI Innovation Center. With a focus on scalability and production readiness, Mahsa helps organizations across diverse industries harness advanced generative AI models to achieve meaningful business outcomes. Mahsa has a proven track record of successfully delivering projects specifically within the mining industry and the healthcare sector. 
Nicholas Moore is a Solutions Architect at AWS, helping startups and businesses of all sizes turn ideas into reality. He specializes in cloud solutions with a focus on AI, analytics, and modern application development. 
Derrick Choo is a Senior Solutions Architect at AWS who accelerates enterprise digital transformation through cloud adoption, AI/ML, and generative AI solutions. He specializes in full-stack development and ML, designing end-to-end solutions spanning frontend interfaces, IoT applications, data integrations, and ML models, with a particular focus on computer vision and multi-modal systems. 
Sovik Kumar Nath is an AI/ML and Generative AI Senior Solutions Architect with AWS. He has extensive experience designing end-to-end ML and business analytics solutions in finance, operations, marketing, healthcare, supply chain management, and IoT. He has master‚Äôs degrees from the University of South Florida and University of Fribourg, Switzerland, and a bachelor‚Äôs degree from the Indian Institute of Technology, Kharagpur. Outside of work, Sovik enjoys traveling, taking ferry rides, and watching movies. 
Saurabh Trikande is a Senior Product Manager for Amazon Bedrock and Amazon SageMaker Inference. He is passionate about working with customers and partners, motivated by the goal of democratizing AI. He focuses on core challenges related to deploying complex AI applications, inference with multi-tenant models, cost optimizations, and making the deployment of generative AI models more accessible. In his spare time, Saurabh enjoys hiking, learning about innovative technologies, following TechCrunch, and spending time with his family. 
Gareth Jones is a Product Manager at Anthropic focused on the Claude API. His work includes collaboration with AWS on Amazon Bedrock to make Claude widely accessible to developers.

‚∏ª