‚úÖ Morning News Briefing ‚Äì November 01, 2025 10:41

üìÖ Date: 2025-11-01 10:41
üè∑Ô∏è Tags: #briefing #ai #publichealth #digitalgov

‚∏ª

üßæ Weather
‚Ä¢ No watches or warnings in effect, Pembroke
  No watches or warnings in effect. No warnings or watches or watches in effect . Watch or warnings are no longer in effect in the U.S. No watches, warnings are in effect for the rest of the day . No watches and warnings are still in effect, but no watches are in place for the day's events . The weather is not expected to be affected by the weather .
‚Ä¢ Current Conditions:  3.0¬∞C
  Observed at: Pembroke 6:00 AM EDT Saturday 1 November 2025 Temperature: 3.0&deg;C Pressure / Tendency: 100.7 kPa rising Humidity: 80 % Dewpoint: -0.1&deg:C Wind: NW 12 km/h Air Quality Health Index: n/a . Observed in the UK at 6:
‚Ä¢ Saturday: Cloudy. High 6.
  Cloudy. High 6. UV index 2 or low . Cloudy . High 6 or low for the rest of the day . Forecast issued 5:00 AM EDT Saturday 1 November 2025 . Weather will be mostly sunny and breezy in the afternoon . Forecasters predict the weather will be cooler in the next few days . For the most recent forecast, see www.wbbbb

üåç International News
No updates.

üçÅ Canadian News
No updates.

üá∫üá∏ U.S. Top Stories
‚Ä¢ How a great-grandmother helped researchers unravel a dinosaur mummy mystery
  A paleontologist was trying to locate the site of a famous 1908 discovery when a rancher in Wyoming shared an important clue . The discovery was made in 1908 when the discovery was discovered by a scientist in 1908 . A rancher shared a key clue to the discovery of a dinosaur in Wyoming, Wyoming, a century before the discovery made famous discovery of the fossil car car-sized dinosaur
‚Ä¢ In redistricting vote, some Californians fear losing out, others want to face Trump
  California's vote on redistricting congressional seats could be important in determining who controls Congress after the 2026 midterm elections . California will vote Tuesday on whether to redistrict congressional seats in the state . The vote could determine who controls the House of Representatives after the midterms in 2026 . The state will hold a vote on the redistricting issue on Tuesday . The California vote could help determine who
‚Ä¢ As SNAP benefits run dry, 'grocery buddies' are footing their neighbors' food bills
  With federal food aid frozen during the government shutdown, there has been a wave of people rushing to help . People are sending gift cards or buying groceries for SNAP recipients in their community . The government shutdown has frozen food aid for some of the country's SNAP recipients . People have been rushing to send gift cards, buy groceries or send groceries to those in need of food assistance in their communities .
‚Ä¢ Could smaller families 'rewild' the planet ‚Äî and make humans happier?
  Many economists and business leaders are raising alarms about falling birthrates . But advocates for lower human populations say a less crowded world will be happier and more sustainable . Advocates for a world of fewer people say it will be more sustainable and less crowded than the world's current population growth rate is too much to blame . The world's population will need to be less densely populated, less crowded,
‚Ä¢ Fewer students are missing school. These state policies may have helped
  A new study says several states are doing the right things to get students to show up to school regularly . Study: States are doing what they need to do to keep students in school on the weekends . The study says students need to be in school regularly on time to get ready for school in order to get back to school . The report says students should be encouraged to attend school on time

üß† Artificial Intelligence
No updates.

üíª Digital Strategy
‚Ä¢ Robotic lawnmower uses AI to dodge cats, toys
  The Sunseeker Elite X5 can mow on its own, but it doesn't come cheap . We tested the robotic mower to see what happens when artificial intelligence meets whirling blades of doom . The mower uses machine learning to steer around your lawn, and it's cheaper to buy than a lawnmower . It's the latest in a series of robotic lawnmowers
‚Ä¢ AI blew open software security, now OpenAI wants to fix it with an agent called Aardvark
  OpenAI has thrown a bone to cyber defenders . AI promises to find bugs and gaps in your apps . AI services are prone to data poisoning and prompt injection . OpenAI's OpenAI says its AI services can be used to identify bugs in apps and find gaps in apps . The company has launched an AI service called OpenAI, which will be available in the U.S. for
‚Ä¢ Datacenter biz and nuke startup join forces for Texas AI ranch
  Texas is set to get another nuclear-powered datacenter project thanks to Blue Energy and Crusoe . The bit barn will run on gas power first, but any atomic action isn't likely until the next decade . Blue Energy: Texas will be the first state to build a nuclear power facility in the state to be powered by natural gas . The project is expected to be completed in the
‚Ä¢ Ransomware gang runs ads for Microsoft Teams to pwn victims
  You click and think you're getting a download page, but get malware instead . The Rhysida ransomware gang has been placing fake ads for Microsoft Teams in search engines and then infecting victims who make the mistake of clicking them . The group has stolen millions of people's info, including millions of users' info, and has been using malware to infect millions of them . You click click
‚Ä¢ YouTube's AI moderator pulls Windows 11 workaround videos, calls them dangerous
  YouTube has started pulling videos that show users how to sidestep Microsoft's setup restrictions . YouTube's AI moderation system seems to think installing Windows 11 with a local account or on unsupported hardware is harmful or dangerous . Users can install Windows 11 on their local accounts or using unsupported hardware on unsupported PCs, or on Windows 11 without having to install it on the same account or using it on unsupported

üè• Public Health
No updates.

üî¨ Science
‚Ä¢ Longer walks beat shorter strolls for heart health
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Prevalence and predictors of burnout in critical care health workers in palestine
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ The nature of the relationship between obesity and mental health: an IMI2 SOPHIA qualitative study
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Assessment of knowledge, attitudes and behaviors related to self-medication practices among the adult population during the COVID-19 pandemic in Italy
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ A moderated mediation approach to enhancing autonomous learning in university physical education
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations

üßæ Government & Policy
No updates.

üèõÔ∏è Enterprise Architecture & IT Governance
No updates.

ü§ñ AI & Emerging Tech
‚Ä¢ Here‚Äôs the latest company planning for gene-edited babies
  A West Coast biotech entrepreneur says he‚Äôs secured $30 million to form a public-benefit company to study how to safely create genetically edited babies, marking the largest known investment into the taboo technology.&nbsp;&nbsp;



The new company, called Preventive, is being formed to research so-called ‚Äúheritable genome editing,‚Äù in which the DNA of embryos would be modified by correcting harmful mutations or installing beneficial genes. The goal would be to prevent disease.



Preventive was founded by the gene-editing scientist Lucas Harrington, who described his plans yesterday in a blog post announcing the venture. Preventive, he said, will not rush to try out the technique but instead will dedicate itself ‚Äúto rigorously researching whether heritable genome editing can be done safely and responsibly.‚Äù



Creating genetically edited humans remains controversial, and the first scientist to do it, in China, was imprisoned for three years. The procedure remains illegal in many countries, including the US, and doubts surround its usefulness as a form of medicine.





Still, as gene-editing technology races forward, the temptation to shape the future of the species may prove irresistible, particularly to entrepreneurs keen to put their stamp on the human condition. In theory, even small genetic tweaks could create people who never get heart disease or Alzheimer‚Äôs, and who would pass those traits on to their own offspring.



According to Harrington, if the technique proves safe, it ‚Äúcould become one of the most important health technologies of our time.‚Äù He has estimated that editing an embryo would cost only about $5,000 and believes regulations could change in the future.&nbsp;



Preventive is the third US startup this year to say it is pursuing technology to produce gene-edited babies. The first, Bootstrap Bio, based in California, is reportedly seeking seed funding and has an interest in enhancing intelligence. Another, Manhattan Genomics, is also in the formation stage but has not announced funding yet.



As of now, none of these companies have significant staff or facilities, and they largely lack any credibility among mainstream gene-editing scientists. Reached by email, Fyodor Urnov, an expert in gene editing at the University of California, Berkeley, where Harrington studied, said he believes such ventures should not move forward.



Urnov has been a pointed critic of the concept of heritable genome editing, calling it dangerous, misguided, and a distraction from the real benefits of gene editing to treat adults and children.&nbsp;



In his email, Urnov said the launch of still another venture into the area made him want to ‚Äúhowl with pain.‚Äù&nbsp;&nbsp;





Harrinton‚Äôs venture was incorporated in Delaware in May 2025,under the name Preventive Medicine PBC. As a public-benefit corporation, it is organized to put its public mission above profits. ‚ÄúIf our research shows [heritable genome editing] cannot be done safely, that conclusion is equally valuable to the scientific community and society,‚Äù Harrington wrote in his post.



Harrington is a cofounder of Mammoth Biosciences, a gene-editing company pursuing drugs for adults, and remains a board member there.



In recent months, Preventive has sought endorsements from leading figures in genome editing, but according to its post, it had secured only one‚Äîfrom Paula Amato, a fertility doctor at Oregon Health Sciences University, who said she had agreed to act as an advisor to the company.



Amato is a member of a US team that has researched embryo editing in the country since 2017, and she has promoted the technology as a way to increase IVF success. That could be the case if editing could correct abnormal embryos, making more available for use in trying to create a pregnancy.



It remains unclear where Preventive‚Äôs funding is coming from. Harrington said the $30 million was gathered from ‚Äúprivate funders who share our commitment to pursuing this research responsibly.‚Äù But he declined to identify those investors other than SciFounders, a venture firm he runs with his personal and business partner Matt Krisiloff, the CEO of the biotech company Conception, which aims to create human eggs from stem cells.



That‚Äôs yet another technology that could change reproduction, if it works. Krisiloff is listed as a member of Preventive‚Äôs founding team.



The idea of edited babies has received growing attention from figures in the cryptocurrency business. These include Brian Armstrong, the billionaire founder of Coinbase, who has held a series of off-the-record dinners to discuss the technology (which Harrington attended). Armstrong previously argued that the ‚Äútime is right‚Äù for a startup venture in the area.





Will Harborne, a crypto entrepreneur and partner at LongGame Ventures, says he‚Äôs ‚Äúthrilled‚Äù to see Preventive launch. If the technology proves safe, he argues, ‚Äúwidespread adoption is inevitable,‚Äù calling its use a ‚Äúsocietal obligation.‚Äù



Harborne‚Äôs fund has invested in Herasight, a company that uses genetic tests to rank IVF embryos for future IQ and other traits. That‚Äôs another hotly debated technology, but one that has already reached the market, since such testing isn‚Äôt strictly regulated. Some have begun to use the term ‚Äúhuman enhancement companies‚Äù to refer to such ventures.



What‚Äôs still lacking is evidence that leading gene-editing specialists support these ventures. Preventive was unsuccessful in establishing a collaboration with at least one key research group, and Urnov says he had harsh words for Manhattan Genomics when that company reached out to him about working together. ‚ÄúI encourage you to stop,‚Äù he wrote back. ‚ÄúYou will cause zero good and formidable harm.‚Äù



Harrington thinks Preventive could change such attitudes, if it shows that it is serious about doing responsible research. ‚ÄúMost scientists I speak with either accept embryo editing as inevitable or are enthusiastic about the potential but hesitate to voice these opinions publicly,‚Äù he told MIT Technology Review earlier this year. ‚ÄúPart of being more public about this is to encourage others in the field to discuss this instead of ignoring it.‚Äù
‚Ä¢ The Download: down the Mandela effect rabbit hole, and the promise of a vaccine for colds
  This is today&#8217;s edition of¬†The Download,¬†our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



Why do so many people think the Fruit of the Loom logo had a cornucopia?



Quick question: Does the Fruit of the Loom logo feature a cornucopia?Many of us have been wearing the company‚Äôs T-shirts for decades, and yet the question of whether there is a woven brown horn of plenty on the logo is surprisingly contentious.According to a 2022 poll, 55% of Americans believe the logo does include a cornucopia, 25% are unsure, and only 21% are confident that it doesn‚Äôt, even though this last group is correct.There‚Äôs a name for what‚Äôs happening here: the ‚ÄúMandela effect,‚Äù or collective false memory, so called because a number of people misremember that Nelson Mandela died in prison. Yet while many find it easy to let their unconfirmable beliefs go, some spend years seeking answers‚Äîand vindication. Read the full story.



‚ÄîAmelia Tait



This story is part of MIT Technology Review‚Äôs series ‚ÄúThe New Conspiracy Age,‚Äù on how the present boom in conspiracy theories is reshaping science and technology.







Here‚Äôs why we don‚Äôt have a cold vaccine. Yet.



For those of us in the Northern Hemisphere, it‚Äôs the season of the sniffles. As the weather turns, we‚Äôre all spending more time indoors. The kids have been back at school for a couple of months. And cold germs are everywhere.



So why can‚Äôt we get a vaccine to protect us against the common cold? Scientists have been working on this for decades, but it turns out that creating a cold vaccine is hard. Really hard. But not impossible. There‚Äôs still hope. Read the full story.



‚ÄîJessica Hamzelou



This article first appeared in The Checkup, MIT Technology Review‚Äôs weekly biotech newsletter. To receive it in your inbox every Thursday, and read articles like this first, sign up here.







Inside the archives of the NASA Ames Research Center



At the southern tip of San Francisco Bay, surrounded by the tech giants Google, Apple, and Microsoft, sits the historic NASA Ames Research Center. Its rich history includes a grab bag of fascinating scientific research involving massive wind tunnels, experimental aircraft, supercomputing, astrobiology, and more.A collection of 5,000 images from NASA Ames‚Äôs archives paints a vivid picture of bleeding-edge work at the heart of America‚Äôs technology hub. Read the full story.‚ÄîJon Keegan



This story is from the latest print issue of MIT Technology Review magazine, which is full of stories about the body. If you haven‚Äôt already, subscribe now to receive future issues once they land.







The must-reads



I‚Äôve combed the internet to find you today‚Äôs most fun/important/scary/fascinating stories about technology.



1 The US government is considering banning TP-Link routersAn investigation has raised concerns over the company‚Äôs links to China. (WP $)+ Lawmakers are worried its equipment is vulnerable to hacking. (Bloomberg $)2 ICE has proposed building a deportation network in TexasThe 24/7 operation would transfer detained immigrants into holding facilities. (Wired $)+ But US citizens keep being detained, too. (NY Mag $)+ Inside the operation giving ICE a run for its money. (Slate $)+ Another effort to track ICE raids was just taken offline. (MIT Technology Review)



3 Ukrainian drone teams are gamifying their war effortsOfficials say rewarding soldiers for successful attacks keeps them motivated. (NYT $)+ A Peter Thiel-backed drone startup crashed and burned during military trials. (FT $)+ Meet the radio-obsessed civilian shaping Ukraine‚Äôs drone defense. (MIT Technology Review)



4 Meta has denied torrenting porn to train its AI modelsInstead, it claims, the downloads were for someone‚Äôs ‚Äúprivate personal use.‚Äù (Ars Technica)



5 Bird flu is getting harder to keep tabs onThe virus has wreaked havoc on the US poultry industry for close to four years. (Vox)+ A new biosensor can detect bird flu in five minutes. (MIT Technology Review)



6 AI browsers are a cybersecurity nightmareThey‚Äôre a hotbed of known‚Äîand unknown‚Äîrisks. (The Verge)+ I tried OpenAI‚Äôs new Atlas browser but I still don‚Äôt know what it‚Äôs for. (MIT Technology Review)



7 Robots are starting to do more jobs across AmericaBut they‚Äôre still proving buggy and expensive to run. (WSJ $)+ When you might start speaking to robots. (MIT Technology Review)



8 These are the jobs that AI builtFrom conversation designer to adoption strategist. (WP $)+ if you fancy landing a job in quantum computing, here‚Äôs how to do it. (IEEE Spectrum)



9 Computer vision is getting much, much better Their blind spots are rapidly being eliminated. (Knowable Magazine)



10 A lock-cracking YouTuber is being sued by a lockmaking company ¬†It‚Äôs arguing he defamed the company, even though he didn‚Äôt say a word during the clip. (Ars Technica)







Quote of the day



‚ÄúYes, we‚Äôve been to the Moon before‚Ä¶ six times!‚Äù



‚ÄîNASA‚Äôs acting administrator Sean Duffy reacts to Kim Kardashian‚Äôs belief that man has never set foot on the moon, the Guardian reports.







One more thing







What happens when you donate your body to science



Rebecca George doesn‚Äôt mind the vultures that complain from the trees that surround the Western Carolina University body farm. Her arrival has interrupted their breakfast. George studies human decomposition, and part of decomposing is becoming food. Scavengers are welcome.



In the US, about 20,000 people or their families donate their bodies to scientific research and education each year. Whatever the reason, the decision becomes a gift. Western Carolina‚Äôs FOREST is among the places where watchful caretakers know that the dead and the living are deeply connected, and the way you treat the first reflects how you treat the second. Read the full story.



‚ÄîAbby Ohlheiser







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)+ Zoo animals across the world are getting into the Halloween spirit with some tasty pumpkins.+ If you‚Äôre stuck for something suitably spooky to watch tonight, this list is a great place to start.+ New York‚Äôs historic Morris-Jumel Mansion is seriously beautiful‚Äîand seriously haunted.+ Salem‚Äôs Lucipurr is on the prowl!
‚Ä¢ Here‚Äôs why we don‚Äôt have a cold vaccine. Yet.
  For those of us in the Northern Hemisphere, it‚Äôs the season of the sniffles. As the weather turns, we‚Äôre all spending more time indoors. The kids have been back at school for a couple of months. And cold germs are everywhere.



My youngest started school this year, and along with artwork and seedlings, she has also been bringing home lots of lovely bugs to share with the rest of her family. As she coughed directly into my face for what felt like the hundredth time, I started to wonder if there was anything I could do to stop this endless cycle of winter illnesses. We all got our flu jabs a month ago. Why couldn‚Äôt we get a vaccine to protect us against the common cold, too?





Scientists have been working on this for decades. It turns out that creating a cold vaccine is hard. Really hard.



But not impossible. There‚Äôs still hope. Let me explain.



Technically, colds are infections that affect your nose and throat, causing symptoms like sneezing, coughing, and generally feeling like garbage. Unlike some other infections,‚Äîcovid-19, for example‚Äîthey aren‚Äôt defined by the specific virus that causes them.



That‚Äôs because there are a lot of viruses that cause colds, including rhinoviruses, adenoviruses, and even seasonal coronaviruses (they don‚Äôt all cause covid!). Within those virus families, there are many different variants.



Take rhinoviruses, for example. These viruses are thought to be behind most colds. They‚Äôre human viruses‚Äîover the course of evolution, they have become perfectly adapted to infecting us, rapidly multiplying in our noses and airways to make us sick. There are around 180 rhinovirus variants, says Gary McLean, a molecular immunologist at Imperial College London in the UK.



Once you factor in the other cold-causing viruses, there are around 280 variants all told. That‚Äôs 280 suspects behind the cough that my daughter sprayed into my face. It‚Äôs going to be really hard to make a vaccine that will offer protection against all of them.



The second challenge lies in the prevalence of those variants.



Scientists tailor flu and covid vaccines to whatever strain happens to be circulating. Months before flu season starts, the World Health Organization advises countries on which strains their vaccines should protect against. Early recommendations for the Northern Hemisphere can be based on which strains seem to be dominant in the Southern Hemisphere, and vice versa.





That approach wouldn‚Äôt work for the common cold, because all those hundreds of variants are circulating all the time, says McLean.



That‚Äôs not to say that people haven‚Äôt tried to make a cold vaccine. There was a flurry of interest in the 1960s and ‚Äô70s, when scientists made valiant efforts to develop vaccines for the common cold. Sadly, they all failed. And we haven‚Äôt made much progress since then.



In 2022, a team of researchers reviewed all the research that had been published up to that year. They¬†only identified one clinical trial‚Äîand it was conducted back in 1965.



Interest has certainly died down since then, too. Some question whether a cold vaccine is even worth the effort. After all, most colds don‚Äôt require much in the way of treatment and don‚Äôt last more than a week or two. There are many, many more dangerous viruses out there we could be focusing on.



And while cold viruses do mutate and evolve, no one really expects them to cause the next pandemic, says McLean. They‚Äôve evolved to cause mild disease in humans‚Äîsomething they‚Äôve been doing successfully for a long, long time. Flu viruses‚Äîwhich can cause serious illness, disability, or even death‚Äîpose a much bigger risk, so they probably deserve more attention.



But colds are still irritating, disruptive, and potentially harmful. Rhinoviruses are¬†considered to be the leading cause of human infectious disease. They can cause pneumonia in children and older adults. And once you add up doctor visits, medication, and missed work, the economic cost of colds is pretty hefty: a¬†2003 study put it at $40 billion per year for the US alone.



So it‚Äôs reassuring that we needn‚Äôt abandon all hope: Some scientists are making progress! McLean and his colleagues are working on ways to prepare the immune systems of people with asthma and lung diseases to potentially protect them from cold viruses. And a team at Emory University has developed a vaccine that appears to protect monkeys from around a third of rhinoviruses.



There‚Äôs still a long way to go. Don‚Äôt expect a cold vaccine to materialize in the next five years, at least. ‚ÄúWe‚Äôre not quite there yet,‚Äù says Michael Boeckh, an infectious-disease researcher at Fred Hutch Cancer Center in Seattle, Washington. ‚ÄúBut will it at some point happen? Possibly.‚Äù



At the end of our Zoom call, perhaps after reading the disappointed expression on my sniffling, cold-riddled face (yes, I did end up catching my daughter‚Äôs cold), McLean told me he hoped he was ‚Äúpositive enough.‚Äù He admitted that he used to be more optimistic about a cold vaccine. But he hasn‚Äôt given up hope. He‚Äôs even running a trial of a potential new vaccine in people, although he wouldn‚Äôt reveal the details.



‚ÄúIt could be done,‚Äù he said.



This article first appeared in The Checkup,¬†MIT Technology Review‚Äôs¬†weekly biotech newsletter. To receive it in your inbox every Thursday, and read articles like this first,¬†sign up here.
‚Ä¢ The Download: Introducing the new conspiracy age
  This is today&#8217;s edition of¬†The Download,¬†our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



Introducing: the new conspiracy age



Everything is a conspiracy theory now. Conspiracists are all over the White House, turning fringe ideas into dangerous policy. America‚Äôs institutions are crumbling under the weight of deep suspicion and the lasting effects of covid isolation. Online echo chambers are getting harder to escape, and generative AI is altering the fabric of truth. A mix of technology and politics has given an unprecedented boost to once-fringe ideas‚Äîbut they are pretty much the same fantasies that have been spreading for hundreds of years.



MIT Technology Review helps break down how this moment is changing science and technology‚Äîand how we can make it through. We‚Äôre thrilled to present The New Conspiracy Age, a new series digging into how the present boom in conspiracy theories is reshaping science and technology.&nbsp;



To kick us off, check out Dorian Lynskey‚Äôs fascinating piece explaining why it‚Äôs never been easier to be a conspiracy theorist. And stay tuned‚Äîwe‚Äôll be showcasing a different story from the package each day in the next few editions of The Download!







Four thoughts from Bill Gates on climate tech



Bill Gates doesn‚Äôt shy away or pretend modesty when it comes to his stature in the climate world today. ‚ÄúWell, who‚Äôs the biggest funder of climate innovation companies?‚Äù he asked a handful of journalists at a media roundtable event last week. ‚ÄúIf there‚Äôs someone else, I‚Äôve never met them.‚Äù



The former Microsoft CEO has spent the last decade investing in climate technology through Breakthrough Energy, which he founded in 2015. Ahead of the UN climate meetings kicking off next week, Gates published a memo outlining what he thinks activists and negotiators should focus on and how he‚Äôs thinking about the state of climate tech right now. Here‚Äôs what he had to say.



‚ÄîCasey Crownhart



This article is from The Spark, MIT Technology Review‚Äôs weekly climate newsletter. To receive it in your inbox every Wednesday, sign up here.







The must-reads



I‚Äôve combed the internet to find you today‚Äôs most fun/important/scary/fascinating stories about technology.



1 US Homeland Security shared false videos of immigration operationsThey claimed to show recent operations but used footage that was old, or recorded thousands of miles away. (WP $)+ ICE is scanning pedestrians‚Äô faces to verify their citizenship. (404 Media)



2 Character.AI is banning under-18s from talking to its virtual companionsIt‚Äôs currently facing several lawsuits from families who claim its chatbots have harmed their children. (NYT $)+ The company says it‚Äôs introducing age assurance functionality. (FT $)+ Teenage boys are using chatbots to roleplay as girlfriends. (The Guardian)+ The looming crackdown on AI companionship. (MIT Technology Review)



3 Trump directed the Pentagon to resume nuclear weapons testingAmerica hasn‚Äôt conducted such tests for more than 30 years. (BBC)+ The US President made multiple incorrect assertions in his statement. (The Verge)+ He doesn‚Äôt seem to even know why he wants to resume the tests himself. (The Atlantic $)



4 A Google DeepMind AI model accurately predicted Hurricane Melissa‚Äôs severityIt‚Äôs the first time the US National Hurricane Center has deployed it. (Nature $)+ Here‚Äôs how to actually help the people affected by its extensive damage. (Vox)+ Google DeepMind‚Äôs new AI model is the best yet at weather forecasting. (MIT Technology Review)



5 A major record label has signed a deal with AI music firm UdioUniversal Music Group had previously sued it for copyright infringement. (WSJ $)+ AI is coming for music, too. (MIT Technology Review)6 Are companies using AI as a fig leaf to lay workers off?It‚Äôs sure starting to look that way. (NBC News)+ Big Tech is going to keep spending billions on AI, regardless. (WP $)



7 Meta Ray-Ban users are filming themselves in massage parlorsThey‚Äôre harassing workers, who appear unaware they‚Äôre being recorded. (404 Media)+ China‚Äôs smart glasses makers are keen to capture the market. (FT $)



8 Just three countries dominate the world‚Äôs space launchesWhat will it take to get some other nations in the mix? (Rest of World)



9 Why you shouldn‚Äôt hire an AI agentTheir freelancing capabilities are‚Ä¶ limited. (Wired $)+ The people paid to train AI are outsourcing their work‚Ä¶ to AI. (MIT Technology Review)



10 This app‚Äôs AI-generated podcasting dog videos are a big hit But DogPack wants to make sure viewers know it‚Äôs not trying to trick them. (Insider $)







Quote of the day



‚ÄúZuck spent five years and $70 billion dollars to build a business that loses $4.4 billion/year to create only $470 million in revenue. So bad you can&#8217;t give it away, I guess.‚Äù



‚ÄîGreg Linden, a former data scientist at Microsoft, pokes fun at Meta‚Äôs beleaguered Reality Labs‚Äô earnings in a post on Bluesky.







One more thing







How scientists want to make you young againA little over 15 years ago, scientists at Kyoto University in Japan made a remarkable discovery. When they added just four proteins to a skin cell and waited about two weeks, some of the cells underwent an unexpected and astounding transformation: they became young again. They turned into stem cells almost identical to the kind found in a days-old embryo, just beginning life‚Äôs journey.At least in a petri dish, researchers using the procedure can take withered skin cells from a 101-year-old and rewind them so they act as if they‚Äôd never aged at all.Now, after more than a decade of studying and tweaking so-called cellular reprogramming, a number of biotech companies and research labs say they have tantalizing hints that the process could be the gateway to an unprecedented new technology for age reversal. Read the full story.¬†



‚ÄîAntonio Regalado







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)



+ 2025‚Äôs Comedy Wildlife Award winners and finalists are classics of the genre.+ This Instagram account shared the same video of Thomas the Tank Engine‚Äôs daring railway stunts every day, and I think that‚Äôs just beautiful.+ How to get more of that elusive deep sleep.+ Here‚Äôs an interesting take on why we still find dragons so fascinating
‚Ä¢ Leveraging the clinician‚Äôs expertise with agentic AI
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations

üîí Cybersecurity & Privacy
‚Ä¢ Aisuru Botnet Shifts from DDoS to Residential Proxies
  Aisuru, the botnet responsible for a series of record-smashing distributed denial-of-service (DDoS) attacks this year, recently was overhauled to support a more low-key, lucrative and sustainable business: Renting hundreds of thousands of infected Internet of Things (IoT) devices to proxy services that help cybercriminals anonymize their traffic. Experts say a glut of proxies from Aisuru and other sources is fueling large-scale data harvesting efforts tied to various artificial intelligence (AI) projects, helping content scrapers evade detection by routing their traffic through residential connections that appear to be regular Internet users.

First identified in August 2024, Aisuru has spread to at least 700,000 IoT systems, such as poorly secured Internet routers and security cameras. Aisuru&#8217;s overlords have used their massive botnet to clobber targets with headline-grabbing DDoS attacks, flooding targeted hosts with blasts of junk requests from all infected systems simultaneously.
In June, Aisuru hit KrebsOnSecurity.com with a DDoS clocking at 6.3 terabits per second &#8212; the biggest attack that Google had ever mitigated at the time. In the weeks and months that followed, Aisuru&#8217;s operators demonstrated DDoS capabilities of nearly 30 terabits of data per second &#8212; well beyond the attack mitigation capabilities of most Internet destinations.
These digital sieges have been particularly disruptive this year for U.S.-based Internet service providers (ISPs), in part because Aisuru recently succeeded in taking over a large number of IoT devices in the United States. And when Aisuru launches attacks, the volume of outgoing traffic from infected systems on these ISPs is often so high that it can disrupt or degrade Internet service for adjacent (non-botted) customers of the ISPs.
&#8220;Multiple broadband access network operators have experienced significant operational impact due to outbound DDoS attacks in excess of 1.5Tb/sec launched from Aisuru botnet nodes residing on end-customer premises,&#8221; wrote Roland Dobbins, principal engineer at Netscout, in a recent executive summary on Aisuru. &#8220;Outbound/crossbound attack traffic exceeding 1Tb/sec from compromised customer premise equipment (CPE) devices has caused significant disruption to wireline and wireless broadband access networks. High-throughput attacks have caused chassis-based router line card failures.&#8221;
The incessant attacks from Aisuru have caught the attention of federal authorities in the United States and Europe (many of Aisuru&#8217;s victims are customers of ISPs and hosting providers based in Europe). Quite recently, some of the world&#8217;s largest ISPs have started informally sharing block lists identifying the rapidly shifting locations of the servers that the attackers use to control the activities of the botnet.
Experts say the Aisuru botmasters recently updated their malware so that compromised devices can more easily be rented to so-called &#8220;residential proxy&#8221; providers. These proxy services allow paying customers to route their Internet communications through someone else&#8217;s device, providing anonymity and the ability to appear as a regular Internet user in almost any major city worldwide.

From a website‚Äôs perspective, the IP traffic of a residential proxy network user appears to originate from the rented residential IP address, not from the proxy service customer. Proxy services can be used in a legitimate manner for several business purposes ‚Äî such as price comparisons or sales intelligence. But they are massively abused for hiding cybercrime activity (think advertising fraud, credential stuffing) because they can make it difficult to trace malicious traffic to its original source.
And as we&#8217;ll see in a moment, this entire shadowy industry appears to be shifting its focus toward enabling aggressive content scraping activity that continuously feeds raw data into large language models (LLMs) built to support various AI projects.
&#8216;INSANE&#8217; GROWTH
Riley Kilmer is co-founder of spur.us, a service that tracks proxy networks. Kilmer said all of the top proxy services have grown exponentially over the past six months &#8212; with some adding between 10 to 200 times more proxies for rent.
&#8220;I just checked, and in the last 90 days we&#8217;ve seen 250 million unique residential proxy IPs,&#8221; Kilmer said. &#8220;That is insane. That is so high of a number, it&#8217;s unheard of. These proxies are absolutely everywhere now.&#8221;
To put Kilmer&#8217;s comments in perspective, here was Spur&#8217;s view of the Top 10 proxy networks by approximate install base, circa May 2025:
AUPROXIES_PROXY¬† 66,097
RAYOBYTE_PROXY¬† ¬† 43,894
OXYLABS_PROXY¬† ¬†43,008
WEBSHARE_PROXY¬† ¬†39,800
IPROYAL_PROXY¬† ¬† 32,723
PROXYCHEAP_PROXY¬† ¬† 26,368
IPIDEA_PROXY¬† ¬† 26,202
MYPRIVATEPROXY_PROXY¬† 25,287
HYPE_PROXY¬† ¬† 18,185
MASSIVE_PROXY¬† ¬† 17,152
Today, Spur says it is tracking an unprecedented spike in available proxies across all providers, including;
LUMINATI_PROXY¬† ¬† 11,856,421
NETNUT_PROXY¬† ¬† 10,982,458
ABCPROXY_PROXY¬† ¬† 9,294,419
OXYLABS_PROXY¬† ¬† ¬†6,754,790
IPIDEA_PROXY¬† ¬† ¬†3,209,313
EARNFM_PROXY¬† ¬† 2,659,913
NODEMAVEN_PROXY¬† ¬† 2,627,851
INFATICA_PROXY¬† ¬† 2,335,194
IPROYAL_PROXY¬† ¬† 2,032,027
YILU_PROXY¬† ¬† 1,549,155
Reached for comment about the apparent rapid growth in their proxy network, Oxylabs (#4 on Spur&#8217;s list) said while their proxy pool did grow recently, it did so at nowhere near the rate cited by Spur.
&#8220;We don‚Äôt systematically track other providers‚Äô figures, and we‚Äôre not aware of any instances of 10√ó or 100√ó growth, especially when it comes to a few bigger companies that are legitimate businesses,&#8221; the company said in a written statement.
Bright Data was formerly known as Luminati Networks, the name that is currently at the top of Spur&#8217;s list of the biggest residential proxy networks, with more than 11 million proxies. Bright Data likewise told KrebsOnSecurity that Spur&#8217;s current estimates of its proxy network are dramatically overstated and inaccurate.
&#8220;We did not actively initiate nor do we see any 10x or 100x expansion of our network, which leads me to believe that someone might be presenting these IPs as Bright Data&#8217;s in some way,&#8221; said Rony Shalit, Bright Data&#8217;s chief compliance and ethics officer. &#8220;In many cases in the past, due to us being the leading data collection proxy provider, IPs were falsely tagged as being part of our network, or while being used by other proxy providers for malicious activity.&#8221;
&#8220;Our network is only sourced from verified IP providers and a robust opt-in only residential peers, which we work hard and in complete transparency to obtain,&#8221; Shalit continued. &#8220;Every DC, ISP or SDK partner is reviewed and approved, and every residential peer must actively opt in to be part of our network.&#8221;
HK NETWORK
Even Spur acknowledges that Luminati and Oxylabs are unlike most other proxy services on their top proxy providers list, in that these providers actually adhere to &#8220;know-your-customer&#8221; policies, such as requiring video calls with all customers, and strictly blocking customers from reselling access.
Benjamin Brundage is founder of Synthient, a startup that helps companies detect proxy networks. Brundage said if there is increasing confusion around which proxy networks are the most worrisome, it&#8217;s because nearly all of these lesser-known proxy services have evolved into highly incestuous bandwidth resellers. What&#8217;s more, he said, some proxy providers do not appreciate being tracked and have been known to take aggressive steps to confuse systems that scan the Internet for residential proxy nodes.
Brundage said most proxy services today have created their own software development kit or SDK that other app developers can bundle with their code to earn revenue. These SDKs quietly modify the user&#8217;s device so that some portion of their bandwidth can be used to forward traffic from proxy service customers.
&#8220;Proxy providers have pools of constantly churning IP addresses,&#8221; he said. &#8220;These IP addresses are sourced through various means, such as bandwidth-sharing apps, botnets, Android SDKs, and more. These providers will often either directly approach resellers or offer a reseller program that allows users to resell bandwidth through their platform.&#8221;
Many SDK providers say they require full consent before allowing their software to be installed on end-user devices. Still, those opt-in agreements and consent checkboxes may be little more than a formality for cybercriminals like the Aisuru botmasters, who can earn a commission each time one of their infected devices is forced to install some SDK that enables one or more of these proxy services.
Depending on its structure, a single provider may operate hundreds of different proxy pools at a time &#8212; all maintained through other means, Brundage said.
&#8220;Often, you&#8217;ll see resellers maintaining their own proxy pool in addition to an upstream provider,&#8221; he said. &#8220;It allows them to market a proxy pool to high-value clients and offer an unlimited bandwidth plan for cheap reduce their own costs.&#8221;
Some proxy providers appear to be directly in league with botmasters. Brundage identified one proxy provider that was aggressively advertising cheap and plentiful bandwidth to content scraping companies. After scanning that provider&#8217;s pool of available proxies, Brundage said he found a one-to-one match with IP addresses he&#8217;d previously mapped to the Aisuru botnet.
Brundage says that by almost any measurement, the world&#8217;s largest residential proxy service is IPidea, a China-based proxy network. IPidea is #5 on Spur&#8217;s Top 10, and Brundage said its brands include ABCProxy (#3), Roxlabs, LunaProxy, PIA S5 Proxy, PyProxy, 922Proxy, 360Proxy, IP2World, and Cherry Proxy.¬†Spur&#8217;s Kilmer said they also track Yilu Proxy¬†(#10) as IPidea.
Brundage said all of these providers operate under a corporate umbrella known on the cybercrime forums as &#8220;HK Network.&#8221;
&#8220;The way it works is there&#8217;s this whole reseller ecosystem, where IPidea will be incredibly aggressive and approach all these proxy providers with the offer, &#8216;Hey, if you guys buy bandwidth from us, we&#8217;ll give you these amazing reseller prices,'&#8221; Brundage explained. &#8220;But they&#8217;re also very aggressive in recruiting resellers for their apps.&#8221;
A graphic depicting the relationship between proxy providers that Synthient found are white labeling IPidea proxies. Image: Synthient.com.
Those apps include a range of low-cost and &#8220;free&#8221; virtual private networking (VPN) services that indeed allow users to enjoy a free VPN, but which also turn the user&#8217;s device into a traffic relay that can be rented to cybercriminals, or else parceled out to countless other proxy networks.
&#8220;They have all this bandwidth to offload,&#8221; Brundage said of IPidea and its sister networks. &#8220;And they can do it through their own platforms, or they go get resellers to do it for them by advertising on sketchy hacker forums to reach more people.&#8221;
One of IPidea&#8217;s core brands is 922S5Proxy, which is a not-so-subtle nod to the 911S5Proxy service that was hugely popular between 2015 and 2022. In July 2022, KrebsOnSecurity published a deep dive into 911S5Proxy&#8217;s origins and apparent owners in China. Less than a week later, 911S5Proxy announced it was closing down after the company&#8217;s servers were massively hacked.
That 2022 story named Yunhe Wang from Beijing as the apparent owner and/or manager of the 911S5 proxy service. In May 2024, the U.S. Department of Justice arrested Mr Wang, alleging that his network was used to steal billions of dollars from financial institutions, credit card issuers, and federal lending programs. At the same time, the U.S. Treasury Department announced sanctions against Wang and two other Chinese nationals for operating 911S5Proxy.
The website for 922Proxy.
DATA SCRAPING FOR AI
In recent months, multiple experts who track botnet and proxy activity have shared that a great deal of content scraping which ultimately benefits AI companies is now leveraging these proxy networks to further obfuscate their aggressive data-slurping activity. That&#8217;s because by routing it through residential IP addresses, content scraping firms can make their traffic far trickier to filter out.
&#8220;It&#8217;s really difficult to block, because there&#8217;s a risk of blocking real people,&#8221; Spur&#8217;s Kilmer said of the LLM scraping activity that is fed through individual residential IP addresses, which are often shared by multiple customers at once.
Kilmer says the AI industry has brought a veneer of legitimacy to residential proxy business, which has heretofore mostly been associated with sketchy affiliate money making programs, automated abuse, and unwanted Internet traffic.
&#8220;Web crawling and scraping has always been a thing, but AI made it like a commodity, data that had to be collected,&#8221; Kilmer said. &#8220;Everybody wanted to monetize their own data pots, and how they monetize that is different across the board.&#8221;
Kilmer said many LLM-related scrapers rely on residential proxies in cases where the content provider has restricted access to their platform in some way, such as forcing interaction through an app, or keeping all content behind a login page with multi-factor authentication.
&#8220;Where the cost of data is out of reach &#8212; there is some exclusivity or reason they can&#8217;t access the data &#8212; they&#8217;ll turn to residential proxies so they look like a real person accessing that data,&#8221; Kilmer said of the content scraping efforts.
Aggressive AI crawlers increasingly are overloading community-maintained infrastructure, causing what amounts to persistent DDoS attacks on vital public resources. A report earlier this year from LibreNews found some open-source projects now see as much as 97 percent of their traffic originating from AI company bots, dramatically increasing bandwidth costs, service instability, and burdening already stretched-thin maintainers.
Cloudflare is now experimenting with tools that will allow content creators to charge a fee to AI crawlers to scrape their websites. The company&#8217;s &#8220;pay-per-crawl&#8221; feature is currently in a private beta, and it lets publishers set their own prices that bots must pay before scraping content.
On October 22, the social media and news network Reddit sued Oxylabs (PDF) and several other proxy providers, alleging that their systems enabled the mass-scraping of Reddit user content even though Reddit had taken steps to block such activity.
&#8220;Recognizing that Reddit denies scrapers like them access to its site, Defendants scrape the data from Google‚Äôs search results instead,&#8221; the lawsuit alleges. &#8220;They do so by masking their identities, hiding their locations, and disguising their web scrapers as regular people (among other techniques) to circumvent or bypass the security restrictions meant to stop them.&#8221;
Denas Grybauskas, chief governance and strategy officer at Oxylabs, said the company was shocked and disappointed by the lawsuit.
&#8220;Reddit has made no attempt to speak with us directly or communicate any potential concerns,&#8221; Grybauskas said in a written statement. &#8220;Oxylabs has always been and will continue to be a pioneer and an industry leader in public data collection, and it will not hesitate to defend itself against these allegations. Oxylabs‚Äô position is that no company should claim ownership of public data that does not belong to them. It is possible that it is just an attempt to sell the same public data at an inflated price.&#8221;
As big and powerful as Aisuru may be, it is hardly the only botnet that is contributing to the overall broad availability of residential proxies. For example, on June 5 the FBI‚Äôs Internet Crime Complaint Center warned that an IoT malware threat dubbed BADBOX 2.0 had compromised millions of smart-TV boxes, digital projectors, vehicle infotainment units, picture frames, and other IoT devices.
In July, Google filed a lawsuit in New York federal court against the Badbox botnet&#8217;s alleged perpetrators. Google said the Badbox 2.0 botnet &#8220;compromised more than 10 million uncertified devices running Android&#8217;s open-source software, which lacks Google&#8217;s security protections. Cybercriminals infected these devices with pre-installed malware and exploited them to conduct large-scale ad fraud and other digital crimes.&#8221;
A FAMILIAR DOMAIN NAME
Brundage said the Aisuru botmasters have their own SDK, and for some reason part of its code tells many newly-infected systems to query the domain name fuckbriankrebs[.]com. This may be little more than an elaborate &#8220;screw you&#8221; to this site&#8217;s author: One of the botnet&#8217;s alleged partners goes by the handle &#8220;Forky,&#8221; and was identified in June by KrebsOnSecurity as a young man from Sao Paulo, Brazil.
Brundage noted that only systems infected with Aisuru&#8217;s Android SDK will be forced to resolve the domain. Initially, there was some discussion about whether the domain might have some utility as a &#8220;kill switch&#8221; capable of disrupting the botnet&#8217;s operations, although Brundage and others interviewed for this story say that is unlikely.
A tiny sample of the traffic after a DNS server was enabled on the newly registered domain fuckbriankrebs dot com. Each unique IP address requested its own unique subdomain. Image: Seralys.
For one thing, they said, if the domain was somehow critical to the operation of the botnet, why was it still unregistered and actively for-sale? Why indeed, we asked. Happily, the domain name was deftly snatched up last week by Philippe Caturegli, &#8220;chief hacking officer&#8221; for the security intelligence company Seralys.
Caturegli enabled a passive DNS server on that domain and within a few hours received more than 700,000 requests for unique subdomains on fuckbriankrebs[.]com.
But even with that visibility into Aisuru, it is difficult to use this domain check-in feature to measure its true size, Brundage said. After all, he said, the systems that are phoning home to the domain are only a small portion of the overall botnet.
&#8220;The bots are hardcoded to just spam lookups on the subdomains,&#8221; he said. &#8220;So anytime an infection occurs or it runs in the background, it will do one of those DNS queries.&#8221;
Caturegli briefly configured all subdomains on fuckbriankrebs dot com to display this ASCII art image to visiting systems today.
The domain fuckbriankrebs[.]com has a storied history. On its initial launch in 2009, it was used to spread malicious software by the Cutwail spam botnet. In 2011, the domain was involved in a notable DDoS against this website from a botnet powered by Russkill (a.k.a. &#8220;Dirt Jumper&#8221;).
Domaintools.com finds that in 2015, fuckbriankrebs[.]com was registered to an email address attributed to David &#8220;Abdilo&#8221; Crees, a 27-year-old Australian man sentenced in May 2025 to time served for cybercrime convictions related to the Lizard Squad hacking group.

üéì University AI
No updates.

üè¢ Corporate AI
‚Ä¢ Build reliable AI systems with Automated Reasoning on Amazon Bedrock ‚Äì Part 1
  Enterprises in regulated industries often need mathematical certainty that every AI response complies with established policies and domain knowledge. Regulated industries can‚Äôt use traditional quality assurance methods that test only a statistical sample of AI outputs and make probabilistic assertions about compliance. When we launched Automated Reasoning checks in Amazon Bedrock Guardrails in preview at AWS re:Invent 2024, it offered a novel solution by applying formal verification techniques to systematically validate AI outputs against encoded business rules and domain knowledge. These techniques make the validation output transparent and explainable. 
Automated Reasoning checks are being used in workflows across industries. Financial institutions verify AI-generated investment advice meets regulatory requirements with mathematical certainty. Healthcare organizations make sure patient guidance aligns with clinical protocols. Pharmaceutical companies confirm marketing claims are supported by FDA-approved evidence. Utility companies validate emergency response protocols during disasters, while legal departments verify AI tools capture mandatory contract clauses. 
With the general availability of Automated Reasoning, we have increased document handling and added new features like scenario generation, which automatically creates examples that demonstrate your policy rules in action. With the enhanced test management system, domain experts can build, save, and automatically execute comprehensive test suites to maintain consistent policy enforcement across model and application versions. 
In the first part of this two-part technical deep dive, we‚Äôll explore the technical foundations of Automated Reasoning checks in Amazon Bedrock Guardrails and demonstrate how to implement this capability to establish mathematically rigorous guardrails for generative AI applications. 
In this post, you will learn how to: 
 
 Understand the formal verification techniques that enable mathematical validation of AI outputs 
 Create and refine an Automated Reasoning policy from natural language documents 
 Design and implement effective test cases to validate AI responses against business rules 
 Apply policy refinement through annotations to improve policy accuracy 
 Integrate Automated Reasoning checks into your AI application workflow using Bedrock Guardrails, following AWS best practices to maintain high confidence in generated content 
 
By following this implementation guide, you can systematically help prevent factual inaccuracies and policy violations before they reach end users, a critical capability for enterprises in regulated industries that require high assurance and mathematical certainty in their AI systems. 
Core capabilities of Automated Reasoning checks 
In this section, we explore the capabilities of Automated Reasoning checks, including the console experience for policy development, document processing architecture, logical validation mechanisms, test management framework, and integration patterns. Understanding these core components will provide the foundation for implementing effective verification systems for your generative AI applications. 
Console experience 
The Amazon Bedrock Automated Reasoning checks console organizes policy development into logical sections, guiding you through the creation, refinement, and testing process. The interface includes clear rule identification with unique IDs and direct use of variable names within the rules, making complex policy structures understandable and manageable. 
Document processing capacity 
Document processing supports up to 120K tokens (approximately 100 pages), so you can encode substantial knowledge bases and complex policy documents into your Automated Reasoning policies. Organizations can incorporate comprehensive policy manuals, detailed procedural documentation, and extensive regulatory guidelines. With this capacity you can work with complete documents within a single policy. 
Validation capabilities 
The validation API includes ambiguity detection that identifies statements requiring clarification, counterexamples for invalid findings that demonstrate why validation failed, and satisfiable findings with both valid and invalid examples to help understand boundary conditions. These features provide context around validation results, to help you understand why specific responses were flagged and how they can be improved. The system can also express its confidence in translations between natural language and logical structures to set appropriate thresholds for specific use cases. 
Iterative feedback and refinement process 
Automated Reasoning checks provide detailed, auditable findings that explain why a response failed validation, to support an iterative refinement process instead of simply blocking non-compliant content. This information can be fed back to your foundation model, allowing it to adjust responses based on specific feedback until they comply with policy rules. This approach is particularly valuable in regulated industries where factual accuracy and compliance must be mathematically verified rather than estimated. 
 
Finding types using a policy example 
Consider the example of a policy for determining days off. When implementing Automated Reasoning checks, a policy consists of both a schema of variables (defining concepts like employee type, years of service, and available leave days) and a set of logical rules that establish relationships between these variables (such as eligibility conditions for different types of time off). During validation, the system uses this schema and rule structure to evaluate whether foundation model responses comply with your defined policy constraints. 
We want to validate the following input that a user asked the foundation model (FM) powered application and the generated output. 
 
 Input:
"Is Thursday a day off if it's a public holiday?"

Output:
"Yes, Thursday would be a day off if it's a public holiday, since all public holidays are considered days off regardless of the day of the week." 
 
Premises are statements from which a conclusion is drawn and the claim is an assertion of the truth. In this example, the premises inferred are day is equal to ‚ÄúThursday‚Äù and is_public_holiday is true, and the claim is made that is_day_off is true. 
This reasoning follows the automated reasoning policy: 
 
 Days Off Policy:
-&nbsp;All weekend days (Saturday and Sunday) are days off
-&nbsp;All public holidays are days off
- A day is considered a day off if it is either a weekend day or a public holiday
 
 
With general availability, Automated Reasoning checks now produces seven distinct finding types that offer precise insights into the validation process of a FM generated response: 
 
VALID: Confirms Input&nbsp;and&nbsp;Output&nbsp;fully aligns with policy rules, providing confidence that the information in the generated response is correct according to your defined policies. The claims are inferred from the premises and validated by the AR policy to be true, and there are no alternative answers that contradict these claims. 
 
 Input:
Today is Sunday, do I have the day off? 

Output:
Yes, Sunday is a weekend day, you have the day off.

Premises:
day: Sunday
is_weekend: true

Claim:
is_day_off: true

Explanation:
Given the assigned variables and policy rules that weekends are days off, the only possible answer is that it is a day off. 
 
SATISFIABLE: Recognizes that Input&nbsp;and Output could be true or false depending on specific assumptions. These findings help you understand boundary conditions where a response is valid only under certain conditions, so that you can decide whether those assumptions are reasonable in your context. If the required assumptions are false, then an alternative claim consistent with the premises may exist. 
 
 Input:
Today is Tuesday, do I have the day off?

Output:
Yes
Premises:
day: Tuesday

Claim:
is_day_off: true

Explanation: 
If we assume is_public_holiday=true, this is correct, but if we assume is_public_holiday=false, the answer would be incorrect since Tuesday is not a weekend. 
 
INVALID: Identifies Input and Output have policy inaccuracies or factual errors, enhanced with counter-examples that explicitly demonstrate why the validation failed. The claims are not implied by the premises and AR policy, and there exist different claims that would be consistent with the premises and AR policy. 
 
 Input:
Today is Sunday, do I have the day off?

Output:
No you do&nbsp;not have the day off.

Premises:
day: Sunday

Claim:
is_day_off: false

Explanation:
This is invalid because the policy states weekends are days off. The correct claim would be is_day_off = true since Sunday is a weekend day 
 
IMPOSSIBLE: Indicates when no valid&nbsp;Claims&nbsp;can be generated because the premises conflict with the AR policy or the policy contains internal contradictions. This finding occurs when the constraints defined in the policy create a logical impossibility. 
 
 Input: 
Today is Sunday and not a weekend day, do I have the day off?

Output:
Yes

Premises:
day: Sunday
is_weekend: false

Claim:
is_day_off: true

Explanation: 
Sunday is always a weekend day, so the premises contain a contradiction. No valid claim can exist given these contradictory premises. 
 
NO_TRANSLATIONS: Occurs when the Input and Output contains no information that can be translated into relevant data for the AR policy evaluation. This typically happens when the text is entirely unrelated to the policy domain or contains no actionable information. 
 
 Input: 
How many legs does the average cat have?

Output:
Less than 4

Explanation:
The AR policy is about days off, so there is no relevant translation for content about cats. The input has no connection to the policy domain. 
 
TRANSLATION_AMBIGUOUS: Identifies when ambiguity in the Input&nbsp;and Output prevents definitive translation into logical structures. This finding suggests that additional context or follow-up questions may be needed to proceed with validation. 
 
 Input: 
I won! Today is Winsday, do I get the day off?

Output:
Yes, you get the day off!

Explanation: 
"Winsday" is not a recognized day in the AR policy, creating ambiguity. Automated reasoning cannot proceed without clarification of what day is being referenced. 
 
TOO_COMPLEX: Signals that the Input&nbsp;and Output contains too much information to process within latency limits. This finding occurs with extremely large or complex inputs that exceed the system‚Äôs current processing capabilities. 
 
 Input:
Can you tell me which days are off for all 50 states plus territories for the next 3 years, accounting for federal, state, and local holidays? Include exceptions for floating holidays and special observances.

Output:
I have analyzed the holiday calendars for all 50 states. In Alabama, days off include...

Explanation: 
This use case contains too many variables and conditions for AR checks to process while maintaining accuracy and response time requirements. 
 
Scenario generation 
You can now generate scenarios directly from your policy, which creates test samples that conform to your policy rules, helps identify edge cases, and supports verification of your policy‚Äôs business logic implementation. With this capability policy authors can see concrete examples of how their rules work in practice before deployment, reducing the need for extensive manual testing. The scenario generation also highlights potential conflicts or gaps in policy coverage that might not be apparent from examining individual rules. 
Test management system 
A new test management system allows you to save and annotate policy tests, build test libraries for consistent validation, execute tests automatically to verify policy changes, and maintain quality assurance across policy versions. This system includes versioning capabilities that track test results across policy iterations, making it easier to identify when changes might have unintended consequences. You can now also export test results for integration into existing quality assurance workflows and documentation processes. 
Expanded options with direct guardrail integration 
Automated Reasoning checks now integrates with Amazon Bedrock APIs, enabling validation of AI generated responses against established policies throughout complex interactions. This integration extends to both the Converse and RetrieveAndGenerate actions, allowing policy enforcement across different interaction modalities. Organizations can configure validation confidence thresholds appropriate to their domain requirements, with options for stricter enforcement in regulated industries or more flexible application in exploratory contexts. 
Solution ‚Äì AI-powered hospital readmission risk assessment system 
Now that we have explained the capabilities of Automated Reasoning checks, let‚Äôs work through a solution by considering the use case of an AI-powered hospital readmission risk assessment system. This AI system automates hospital readmission risk assessment by analyzing patient data from electronic health records to classify patients into risk categories (Low, Intermediate, High) and recommends personalized intervention plans based on CDC-style guidelines. The objective of this AI system is to reduce the 30-day hospital readmission rates by supporting early identification of high-risk patients and implementing targeted interventions. This application is an ideal candidate for Automated Reasoning checks because the healthcare provider prioritizes verifiable accuracy and explainable recommendations that can be mathematically proven to comply with medical guidelines, supporting both clinical decision-making and satisfying the strict auditability requirements common in healthcare settings. 
Note: The referenced policy document is an example created for demonstration purposes only and should not be used as an actual medical guideline or for clinical decision-making. 
Prerequisites 
To use Automated Reasoning checks in Amazon Bedrock, verify you have met the following prerequisites: 
 
 An active AWS account 
 Confirmation of AWS Regions where Automated Reasoning checks is available 
 Appropriate IAM permissions to create, test, and invoke Automated Reasoning policies (Note: The IAM policy should be fine-grained and limited to necessary resources using proper ARN patterns for production usage): 
 
 
  {  
  "Sid": "OperateAutomatedReasoningChecks",  
  "Effect": "Allow",  
  "Action": [  
    "bedrock:CancelAutomatedReasoningPolicyBuildWorkflow",  
    "bedrock:CreateAutomatedReasoningPolicy",
    "bedrock:CreateAutomatedReasoningPolicyTestCase",  
    "bedrock:CreateAutomatedReasoningPolicyVersion",
    "bedrock:CreateGuardrail",
    "bedrock:DeleteAutomatedReasoningPolicy",  
    "bedrock:DeleteAutomatedReasoningPolicyBuildWorkflow",  
    "bedrock:DeleteAutomatedReasoningPolicyTestCase",
    "bedrock:ExportAutomatedReasoningPolicyVersion",  
    "bedrock:GetAutomatedReasoningPolicy",  
    "bedrock:GetAutomatedReasoningPolicyAnnotations",  
    "bedrock:GetAutomatedReasoningPolicyBuildWorkflow",  
    "bedrock:GetAutomatedReasoningPolicyBuildWorkflowResultAssets",  
    "bedrock:GetAutomatedReasoningPolicyNextScenario",  
    "bedrock:GetAutomatedReasoningPolicyTestCase",  
    "bedrock:GetAutomatedReasoningPolicyTestResult",
    "bedrock:InvokeAutomatedReasoningPolicy",  
    "bedrock:ListAutomatedReasoningPolicies",  
    "bedrock:ListAutomatedReasoningPolicyBuildWorkflows",  
    "bedrock:ListAutomatedReasoningPolicyTestCases",  
    "bedrock:ListAutomatedReasoningPolicyTestResults",
    "bedrock:StartAutomatedReasoningPolicyBuildWorkflow",  
    "bedrock:StartAutomatedReasoningPolicyTestWorkflow",
    "bedrock:UpdateAutomatedReasoningPolicy",  
    "bedrock:UpdateAutomatedReasoningPolicyAnnotations",  
    "bedrock:UpdateAutomatedReasoningPolicyTestCase",
    "bedrock:UpdateGuardrail"
  ],  
  "Resource": [
  "arn:aws:bedrock:\${aws:region}:\${aws:accountId}:automated-reasoning-policy/*",
  "arn:aws:bedrock:\${aws:region}:\${aws:accountId}:guardrail/*"
]
} 
 
 
 Key service limits: Be aware of the service limits when implementing Automated Reasoning checks. 
 With Automated Reasoning checks, you pay based on the amount of text processed. For more information, see Amazon Bedrock pricing. For more information, see Amazon Bedrock pricing. 
 
Use case and policy dataset overview 
The full policy document used in this example can be accessed from the Automated Reasoning GitHub repository.&nbsp; To validate the results from Automated Reasoning checks, being familiar with the policy is helpful. Moreover, refining the policy that is created by Automated Reasoning is key in achieving a soundness of over 99%. 
Let‚Äôs review the main details of the sample medical policy that we are using in this post. As we start validating responses, it is helpful to verify it against the source document. 
 
 Risk assessment and stratification:&nbsp;Healthcare facilities must implement a standardized risk scoring system based on demographic, clinical, utilization, laboratory, and social factors, with patients classified into Low (0-3 points), Intermediate (4-7 points), or High Risk (8+ points) categories. 
 Mandatory interventions:&nbsp;Each risk level requires specific interventions, with higher risk levels incorporating lower-level interventions plus additional measures, while certain conditions trigger automatic High Risk classification regardless of score. 
 Quality metrics and compliance:&nbsp;Facilities must achieve specific completion rates including 95%+ risk assessment within 24 hours of admission and 100% completion before discharge, with High Risk patients requiring documented discharge plans. 
 Clinical oversight:&nbsp;While the scoring system is standardized, attending physicians maintain override authority with proper documentation and approval from the discharge planning coordinator. 
 
Create and test an Automated Reasoning checks‚Äô policy&nbsp;using the Amazon Bedrock console 
The first step is to encode your knowledge‚Äîin this case, the sample medical policy‚Äîinto an Automated Reasoning policy. Complete the following steps to create an Automated Reasoning policy: 
 
 On the Amazon Bedrock console, choose Automated Reasoning under Build in the navigation pane. 
 Choose Create policy.  
 
 
 Provide a policy name and policy description.  
 
 
 Add source content from which Automated Reasoning will generate your policy. You can either upload document (pdf, txt) or enter text as the ingest method.   
 Include a description of the intent of the Automated Reasoning policy you‚Äôre creating. The intent is optional but provides valuable information to the Large Language Models that are translating the natural language based document into a set of rules that can be used for mathematical verification. For the sample policy, you can use the following intent: This logical policy validates claims about the clinical practice guideline providing evidence-based recommendations for healthcare facilities to systematically assess and mitigate hospital readmission risk through a standardized risk scoring system, risk-stratified interventions, and quality assurance measures, with the goal of reducing 30-day readmissions by 15-23% across participating healthcare systems.

Following is an example patient profile and the corresponding classification.

&lt;Patient Profile&gt;Age: 82 years

Length of stay: 10 days

Has heart failure

One admission within last 30 days

Lives alone without caregiver

&lt;Classification&gt; High Risk  
 Once the policy has been created, we can inspect the definitions to see which rules, variables and types have been created from the natural language document to represent the knowledge into logic. 
 
 
 You may see differences in the number of rules, variables, and types generated compared to what is shown in this example. This is due to the non-deterministic processing of the supplied document. To address this, the recommended guidance is to perform a human-in-the-loop review of the generated information in the policy before using it with other systems. 
Exploring the Automated Reasoning checks‚Äô definition 
A Variable in automated reasoning for policy documents is a named container that holds a specific type of information (like Integer, Real Number, or Boolean) and represents a distinct concept or measurement from the policy. Variables act as building blocks for rules and can be used to track, measure, and evaluate policy requirements. From the image below, we can see examples like admissionsWithin30Days&nbsp;(an Integer variable tracking previous hospital admissions), ageRiskPoints&nbsp;(an Integer variable storing age-based risk scores), and conductingMonthlyHighRiskReview (a Boolean variable indicating whether monthly reviews are being performed). Each variable has a clear description of its purpose and the specific policy concept it represents, making it possible to use these variables within rules to enforce policy requirements and measure compliance. Issues also highlight that some variables are unused. It is particularly important to verify which concepts these variables represent and to identify if rules are missing. 
 
In the Definitions, we see ‚ÄòRules‚Äô, ‚ÄòVariables‚Äô and ‚ÄòTypes‚Äô. A rule is an unambiguous logical statement that Automated Reasoning extracts from your source document. Consider this simple rule that has been created:&nbsp;followupAppointmentsScheduledRate is at least 90.0&nbsp; ‚Äì This rule has been created from the Section III A Process Measures, which states that healthcare facilities should monitor various process indications, requiring that follow up appointments scheduled prior to discharge should be 90% or higher. 
Let‚Äôs look at a more complex rule: 
 
 comorbidityRiskPoints is equal to(ite&nbsp;hasDiabetesMellitus&nbsp;1 0) + (ite hasHeartFailure 2 0) + (ite&nbsp;hasCOPD&nbsp;1 0) + (ite hasChronicKidneyDisease 1 0) 
 
 
 Where ‚Äúite‚Äù is ‚ÄúIf then else‚Äù 
 
This rule calculates a patient‚Äôs risk points based on their existing medical conditions (comorbidities) as specified in the policy document. When evaluating a patient, the system checks for four specific conditions: diabetes mellitus of any type (worth 1 point), heart failure of any classification (worth 2 points), chronic obstructive pulmonary disease (worth 1 point), and chronic kidney disease stages 3-5 (worth 1 point). The rule adds these points together by using boolean logic ‚Äì meaning it multiplies each condition (represented as true=1 or false=0) by its assigned point value, then sums all values to generate a total comorbidity risk score. For instance, if a patient has both heart failure and diabetes, they would receive 3 total points (2 points for heart failure plus 1 point for diabetes). This comorbidity score then becomes part of the larger risk assessment framework used to determine the patient‚Äôs overall readmission risk category. 
 
The Definitions also include custom variable types.&nbsp;Custom variable types, also known as enumerations (ENUMs), are specialized data structures that define a fixed set of allowable values for specific policy concepts.&nbsp;These custom types maintain consistency and accuracy in data collection and rule enforcement by limiting values to predefined options that align with the policy requirements. In the sample policy, we can see that four custom variable types have been identified: 
 
 AdmissionType: This defines the possible types of hospital admissions (MEDICAL, SURGICAL, MIXED_MEDICAL_SURGICAL, PSYCHIATRIC) that determine whether a patient is eligible for the readmission risk assessment protocol. 
 HealthcareFacilityType: This specifies the types of healthcare facilities (ACUTE_CARE_HOSPITAL_25PLUS, CRITICAL_ACCESS_HOSPITAL) where the readmission risk assessment protocol may be implemented. 
 LivingSituation: This categorizes a patient‚Äôs living arrangement (LIVES_ALONE_NO_CAREGIVER, LIVES_ALONE_WITH_CAREGIVER) which is a critical factor in determining social support and risk levels. 
 RiskCategory: This defines the three possible risk stratification levels (LOW_RISK, INTERMEDIATE_RISK, HIGH_RISK) that can be assigned to a patient based on their total risk score. 
 
 
An important step in improving soundness (accuracy of Automated Reasoning checks when it says VALID), is the policy refinement step of making sure that the rules, variable, and types that are captured best represent the source of truth. In order to do this, we will head over to the test suite and explore how to add tests, generate tests and use the results from the tests to apply annotations that will update the rules. 
Testing the Automated Reasoning policy and policy refinement 
The test suite in Automated Reasoning provides test capabilities for two purposes: First, we want to run different scenarios and test the various rules and variables in the Automated Reasoning policy and refine them so that they accurately represent the ground truth. This policy refinement step is important to improving the soundness of Automated Reasoning checks. Second, we want metrics to understand how well the Automated Reasoning checks performs for the defined policy and the use case. To do so, we can open the Tests tab on Automated Reasoning console. 
 
Test samples can be added manually by using the Add button. To scale up the testing, we can generate tests from the policy rules. This testing approach helps verify both the semantic correctness of your policy (making sure rules accurately represent intended policy constraints) and the natural language translation capabilities (confirming the system can correctly interpret the language your users will use when interacting with your application). In the image below, we can see a test sample generated and before adding it to the test suite, the SME should indicate if this test sample is possible (thumbs up) or not possible (thumbs up). The test sample can then be saved to the test suite. 
 
 
Once the test sample is created, it possible to run this test sample alone, or all the test samples in the test suite by choosing on Validate all tests. Upon executing, we see that this test passed successfully. 
 
You can manually create tests by providing an input (optional) and output. These are translated into logical representations before validation occurs. 
How translation works: 
Translation converts your natural language tests into logical representations that can be mathematically verified against your policy rules: 
 
 Automated Reasoning Checks uses multiple LLMs to translate your input/output into logical findings 
 Each translation receives a confidence vote indicating translation quality 
 You can set a confidence threshold to control which findings are validated and returned 
 
Confidence threshold behavior: 
The confidence threshold controls which translations are considered reliable enough for validation, balancing strictness with coverage: 
 
 Higher threshold: Greater certainty in translation accuracy but also higher chance of no findings being validated. 
 Lower threshold: &nbsp;Greater chance of getting validated findings returned, but potentially less certain translations 
 Threshold = 0: All findings are validated and returned regardless of confidence 
 
Ambiguous results: 
When no finding meets your confidence threshold, Automated Reasoning Checks returns ‚ÄúTranslation Ambiguous,‚Äù indicating uncertainty in the content‚Äôs logical interpretation.The test case we will create and validate is: 
 
 Input:
Patient A
Age: 82
Length of stay: 16 days
Diabetes Mellitus: Yes
Heart Failure: Yes
Chronic Kidney Disease: Yes
Hemoglobin: 9.2 g/dL
eGFR: 28 ml/min/1.73m^2
Sodium: 146 mEq/L
Living Situation: Lives alone without caregiver
Has established PCP: No
Insurance Status: Medicaid
Admissions within 30 days: 1

Output:
Final&nbsp;Classification:&nbsp;INTERMEDIATE RISK 
 
 
We see that this test passed upon running it, the result of ‚ÄòINVALID‚Äô matches our expected results. Additionally Automated Reasoning checks also shows that 12 rules were contradicting the premises and claims, which lead to the output of the test sample being ‚ÄòINVALID‚Äô 
 
Let‚Äôs examine some of the visible contradicting rules: 
 
 Age risk: Patient is 82 years old 
   
   Rule triggers: ‚Äúif patientAge is at least 80, then ageRiskPoints is equal to 3‚Äù 
    
 Length of stay risk: Patient stayed 16 days 
   
   Rule triggers: ‚Äúif lengthOfStay is greater than 14, then lengthOfStayRiskPoints is equal to 3‚Äù 
    
 Comorbidity risk: Patient has multiple conditions 
   
   Rule calculates: ‚ÄúcomorbidityRiskPoints = (hasDiabetesMellitus √ó 1) + (hasHeartFailure √ó 2) + (hasCOPD √ó 1) + (hasChronicKidneyDisease √ó 1)‚Äù 
    
 Utilization risk: Patient has 1 admission within 30 days 
   
   Rule triggers: ‚Äúif admissionsWithin30Days is at least 1, then utilizationRiskPoints is at least 3‚Äù 
    
 Laboratory risk: Patient‚Äôs eGFR is 28 
   
   Rule triggers: ‚Äúif eGFR is less than 30.0, then laboratoryRiskPoints is at least 2‚Äù 
    
 
These rules are likely producing conflicting risk scores, making it impossible for the system to determine a valid final risk category. These contradictions show us which rules where used to determine that the input text of the test is INVALID. 
 
Let‚Äôs add another test to the test suite, as shown in the screenshot below: 
 
 Input:
Patient&nbsp;profile
Age: 83
Length of stay: 16 days
Diabetes Mellitus: Yes
Heart Failure: Yes
Chronic Kidney Disease: Yes
Hemoglobin: 9.2 g/dL
eGFR: 28 ml/min/1.73m^2
Sodium: 146 mEq/L
Living Situation: Lives alone without caregiver
Has established PCP: No
Insurance Status: Medicaid
Admissions within 30 days: 1
Admissions within 90 days:&nbsp;2

Output:
Final&nbsp;Classification:&nbsp;HIGH RISK 
 
 
 
When this test is executed, we see that each of the patient details are extracted as premises, to validate the claim that the risk of readmission if high. We see that 8 rules have been applied to verify this claim. The key rules and their validations include: 
 
 Age risk: Validates that patient age ‚â• 80 contributes 3 risk points 
 Length of stay risk: Confirms that stay &gt;14 days adds 3 risk points 
 Comorbidity risk: Calculated based on presence of&nbsp;Diabetes Mellitus, Heart Failure, Chronic Kidney Disease 
 Utilization risk: Evaluates admissions history 
 Laboratory risk: Evaluates risk based on&nbsp;Hemoglobin level of 9.2 and eGFR of 28 
 
Each premise was evaluated as true, with multiple risk factors present (advanced age, extended stay, multiple comorbidities, concerning lab values, living alone without caregiver, and lack of PCP), supporting the overall Valid classification of this HIGH RISK assessment. 
 
Moreover, the Automated Reasoning engine performed an extensive validation of this test sample using 93 different assignments to increase the soundness that the HIGH RISK classification is correct.&nbsp;Various related rules from the Automated Reasoning policy are used to validate the samples against 93 different scenarios and variable combinations. In this manner, Automated Reasoning checks confirms that there is no possible situation under which this patient‚Äôs HIGH RISK classification could be invalid. This thorough verification process affirms the reliability of the risk assessment for this elderly patient with multiple chronic conditions and complex care needs.In the event of a test sample failure, the 93 assignments would serve as an important diagnostic tool, pinpointing specific variables and their interactions that conflict with the expected outcome, thereby enabling subject matter experts (SMEs) to analyze the relevant rules and their relationships to determine if adjustments are needed in either the clinical logic or risk assessment criteria. In the next section, we will look at policy refinement and how SMEs can apply annotations to improve and correct the rules, variables, and custom types of the Automated Reasoning policy. 
Policy refinement through annotations 
Annotations provide a powerful improvement mechanism for Automated Reasoning policies when tests fail to produce expected results. Through annotations, SMEs can systematically refine policies by: 
 
 Correcting problematic rules by modifying their logic or conditions 
 Adding missing variables essential to the policy definition 
 Updating variable descriptions for greater precision and clarity 
 Resolving translation issues where original policy language was ambiguous 
 Deleting redundant or conflicting elements from the policy 
 
This iterative process of testing, annotating, and updating creates increasingly robust policies that accurately encode domain expertise. As shown in the figure below, annotations can be applied to modify various policy elements, after which the refined policy can be exported as a JSON file for deployment. 
 
In the following figure, we can see how annotations are being applied, and rules are deleted in the policy. Similarly, additions and updates can be made to rules, variables, or the custom types. 
 
 
When the subject matter expert has validated the Automated Reasoning policy through testing, applying annotations, and validating the rules, it is possible to export the policy as a JSON file. 
 
Using Automated Reasoning checks at inference 
To use the Automated Reasoning checks with the created policy, we can now navigate to Amazon Bedrock&nbsp;Guardrails,&nbsp;and create a new guardrail by entering the name, description, and the messaging that will be displayed when the guardrail intervenes and blocks a prompt or a output from the AI system. 
 
Now, we can attach Automated Reasoning check by using the toggle to Enable Automated Reasoning policy. We can set a confidence threshold, which determines how strictly the policy should be enforced. This threshold ranges from 0.00 to 1.00, with 1.00 being the default and most stringent setting. Each guardrail can accommodate up to two separate automated reasoning policies for enhanced validation flexibility. In the following figure, we are attaching the draft version of the medical policy related to patient hospital readmission risk assessment. 
 
Now we can create the guardrail. Once you‚Äôve established the guardrail and linked your automated reasoning policies, verify your setup by reviewing the guardrail details page to confirm all policies are properly attached. 
 
Clean up 
When you‚Äôre finished with your implementation, clean up your resources by deleting the guardrail and automated reasoning policies you created. Before deleting a guardrail, be sure to disassociate it from all resources or applications that use it. 
Conclusion 
In this first part of our blog, we explored how Automated Reasoning checks in Amazon Bedrock Guardrails help maintain the reliability and accuracy of generative AI applications through mathematical verification. You can use increased document processing capacity, advanced validation mechanisms, and comprehensive test management features to validate AI outputs against business rules and domain knowledge. This approach addresses key challenges facing enterprises deploying generative AI systems, particularly in regulated industries where factual accuracy and policy compliance are essential. Our hospital readmission risk assessment demonstration shows how this technology supports the validation of complex decision-making processes, helping transform generative AI into systems suitable for critical business environments. You can use these capabilities through both the AWS Management Console and APIs to establish quality control processes for your AI applications. 
To learn more, and build secure and safe AI applications, see the technical documentation and the GitHub code samples, or access to the Amazon Bedrock console. 
 
About the authors 
Adewale Akinfaderin&nbsp;is a Sr. Data Scientist‚ÄìGenerative AI, Amazon Bedrock, where he contributes to cutting edge innovations in foundational models and generative AI applications at AWS. His expertise is in reproducible and end-to-end AI/ML methods, practical implementations, and helping global customers formulate and develop scalable solutions to interdisciplinary problems. He has two graduate degrees in physics and a doctorate in engineering. 
Bharathi Srinivasan is a Generative AI Data Scientist at the AWS Worldwide Specialist Organization. She works on developing solutions for Responsible AI, focusing on algorithmic fairness, veracity of large language models, and explainability. Bharathi guides internal teams and AWS customers on their responsible AI journey. She has presented her work at various learning conferences. 
Nafi Diallo &nbsp;is a Senior Automated Reasoning Architect at Amazon Web Services, where she advances innovations in AI safety and Automated Reasoning systems for generative AI applications. Her expertise is in formal verification methods, AI guardrails implementation, and helping global customers build trustworthy and compliant AI solutions at scale. She holds a PhD in Computer Science with research in automated program repair and formal verification, and an MS in Financial Mathematics from WPI.
‚Ä¢ Custom Intelligence: Building AI that matches your business DNA
  In 2024, we launched the Custom Model Program within the AWS Generative AI Innovation Center to provide comprehensive support throughout every stage of model customization and optimization. Over the past two years, this program has delivered exceptional results by partnering with global enterprises and startups across diverse industries‚Äîincluding legal, financial services, healthcare and life sciences, software development, telecommunications, and manufacturing. These partnerships have produced tailored AI solutions that capture each organization‚Äôs unique data expertise, brand voice, and specialized business requirements. They operate more efficiently than off-the-shelf alternatives, delivering increased alignment and relevance with significant cost savings on inference operations. 
 
As organizations mature past proof-of-concept projects and basic chatbots, we‚Äôre seeing increased adoption of advanced personalization and optimization strategies beyond prompt engineering and retrieval augmented generation (RAG). Our approach encompasses creating specialized models for specific tasks and brand alignment, distilling larger models into smaller, faster, more cost-effective versions, implementing deeper adaptations through mid-training modifications, and optimizing hardware and accelerators to increase throughput while reducing costs. 
Strategic upfront investment pays dividends throughout a model‚Äôs production lifecycle, as demonstrated by Cosine AI‚Äôs results. Cosine AI is the developer of an AI developer platform and software engineering agent designed to integrate seamlessly into their users‚Äô workflows. They worked with the Innovation Center to fine-tune Nova Pro, an Amazon Nova foundation model, using Amazon SageMaker AI for their AI engineering assistant, Genie, achieving remarkable results including a 5x increase in A/B testing capability, a 10x faster developer iterations, and a 4x overall project speed improvement. The return on investment becomes even more compelling as companies transition toward agentic systems and workflows, where latency task specificity, performance, and depth are critical and compound across complex processes. 
In this post, we‚Äôll share key learnings and actionable strategies for leaders looking to use customization for maximum ROI while avoiding common implementation pitfalls. 
Five tips for maximizing value from training and tuning generative AI models 
The Innovation Center recommends the following top tips to maximize value from training and tuning AI models: 
1. Don‚Äôt start from a technical approach; work backwards from business goals 
This may seem obvious, but after working with over a thousand customers, we‚Äôve found that working backwards from business goals is a critical factor in why projects supported by the Innovation Center achieve a 65% production success rate, with some launching within 45 days. We apply this same strategy to every customization project by first identifying and prioritizing tangible business outcomes that a technical solution will drive. Success must be measurable and deliver real business value, helping avoid flashy experiments that end up sitting on a shelf instead of producing results. In the Custom Model Program, many customers initially approach us seeking specific technical solutions‚Äîsuch as jumping directly into model pre-training or continued pre-training‚Äîwithout having defined downstream use cases, data strategies, or evaluation plans. By starting with clear business objectives first, we make sure that technical decisions align with strategic goals and create meaningful impact for the organization. 
2. Pick the right customization approach 
Start with a baseline customization approach and exhaust simpler approaches before diving into deep model customization. The first question we ask customers seeking custom model development is ‚ÄúWhat have you already tried?‚Äù We recommend establishing this baseline with prompt engineering and RAG before exploring more complex techniques. While there‚Äôs a spectrum of model optimization approaches that can achieve higher performance, sometimes the simplest solution is the most effective. Once you establish this baseline, identify remaining gaps and opportunities to determine whether advancing to the next level makes strategic sense. 
 
Customization options range from lightweight approaches like supervised fine-tuning to ground-up model development. We typically advise starting with lighter-weight solutions that require smaller amounts of data and compute, then progressing to more complex techniques only when specific use cases or remaining gaps justify the investment: 
 
 Supervised fine-tuning sharpens the model‚Äôs focus for specific use cases, for example delivering consistent customer service responses or adapting to your organization‚Äôs preferred phrasing, structure and reasoning patterns. Volkswagen, one of the world‚Äôs largest automobile manufacturers, achieved an ‚Äúimprovement in AI-powered brand consistency checks, increasing accuracy in identifying on-brand images from 55% to 70%,‚Äù notes Dr. Philip Trempler, Technical Lead AI &amp; Cloud Engineering at Volkswagen Group Services. 
 Model efficiency and deployment tuning supports organizations like Robin AI, a leader in AI-powered legal contract technology, to create tailored models that speed up human verification. Organizations can also use techniques like quantization, pruning, and system optimizations to improve model performance and reduce infrastructure costs. 
 Reinforcement learning uses reward functions or preference data to align models to preferred behavior. This approach is often combined with supervised fine-tuning so organizations like Cosine AI can refine their models‚Äô decision making to match organizational preferences. 
 Continued pre-training allow organizations like Athena RC, a leading research center in Greece, to build Greek-first foundation models that expand language capabilities beyond English. By continually pre-training large language models on extensive Greek data, Athena RC strengthens the models‚Äô core understanding of the Greek language, culture, and usage ‚Äì not just their domain knowledge. Their Meltemi-7B and Llama-Krikri-8B models demonstrate how continued pre-training and instruction tuning can create open, high-quality Greek models for applications across research, education, industry, and society. 
 Domain-specific foundation model development enables organizations like TGS, a leading energy data, insights, and technology provider, to build custom AI models from scratch, ideal for those with highly specialized requirements and substantial volume of proprietary data. TGS helps energy companies make smarter exploration and development decisions by solving some of the industry‚Äôs toughest challenges in understanding what lies beneath the Earth‚Äôs surface. TGS has enhanced its Seismic Foundation Models (SFMs) to more reliably detect underground geological structures‚Äîsuch as faults and reservoirs‚Äîthat indicate potential oil and gas deposits. The benefit is clear: operators can reduce uncertainty, lower exploration costs, and make faster investment decisions. 
 
Data quality and accessibility will be a major consideration in determining feasibility of each customization technique. Clean, high-quality data is essential both for model improvement and measuring progress. While some Innovation Center customers achieve performance gains with relatively smaller volumes of fine-tuning training pairs on instruction-tuned foundation models, approaches like continued pre-training typically require large volumes of training tokens. This reinforces the importance of starting simple‚Äîas you test lighter-weight model tuning, you can collect and process larger data volumes in parallel for future phases. 
3. Define measures for what good looks like 
Success needs to be measurable, regardless of which technical approach you choose. It‚Äôs critical to establish clear methods for measuring both overall business outcomes and the technical solution‚Äôs performance. At the model or application level, teams typically optimize across some combination of relevance, latency, and cost. However, the metrics for your production application won‚Äôt be general leaderboard metrics‚Äîthey must be unique to what matters for your business. 
Customers developing content generation systems prioritize metrics like relevance, clarity, style, and tone. Consider this example from Volkswagen Group: ‚ÄúWe fine-tuned Nova Pro in SageMaker AI using our marketing experts‚Äô knowledge. This improved the model‚Äôs ability to identify on-brand images, achieving stronger alignment with Volkswagen‚Äôs brand guidelines,‚Äù according to Volkswagen‚Äôs Dr. Trempler. ‚ÄúWe are building on these results to enable Volkswagen Group‚Äôs vision to scale high-quality, brand-compliant content creation across our diverse automotive markets worldwide using generative AI.‚Äù Developing an automated evaluation process is critical for supporting iterative solution improvements. 
For qualitative use cases, it‚Äôs essential to align automated evaluations with human experts, particularly in specialized domains. A common solution involves using LLM as judge to review another model or system responses. For instance, when fine-tuning a generation model for a RAG application, you might use an LLM judge to compare the fine-tuned model response to your existing baseline. However, LLM judges come with intrinsic biases and may not align with your internal team‚Äôs human preferences or domain expertise. Robin AI partnered with the Innovation Center to develop Legal LLM-as-Judge, an AI model for legal contract review. Emulating expert methodology and creating ‚Äúa panel of trained judges‚Äù using fine-tuning techniques, they obtained smaller and faster models that maintain accuracy while reviewing documents ranging from NDAs to merger agreements. The solution achieved an 80% faster contract review process, enabling lawyers to focus on strategic work while AI handles detailed analysis. 
4. Consider hardware-level optimizations for training and inference 
If you‚Äôre using a managed service like Amazon Bedrock, you can take advantage of built-in optimizations out of the box. However, if you have a more bespoke solution or are operating at a lower level of the technology stack, there are several areas to consider for optimization and efficiency gains. For instance, TGS‚Äôs SFMs process massive 3D seismic images (essentially giant CAT scans of the Earth) that can cover tens of thousands of square kilometers. Each dataset is measured in petabytes, far beyond what traditional manual or even semi-automated interpretation methods can handle. By rebuilding their AI models on AWS‚Äôs high-performance GPU training infrastructure, TGS achieved near-linear scaling, meaning that adding more computing power results in almost proportional speed increases while maintaining &gt;90% GPU efficiency. As a result, TGS can now deliver actionable subsurface insights, such as identifying drilling targets or de-risking exploration zones, to customers in days instead of weeks. 
Over the life of a model, resource requirements are generally driven by inference requests, and any efficiency gains you can achieve will pay dividends during the production phase. One approach to reduce inference demands is model distillation to reduce the model size itself, but in some cases, there are additional gains to be had by digging deeper into the infrastructure. A recent example is Synthesia, the creator of a leading video generation platform where users can create professional videos without the need for mics, cameras, or actors. Synthesia is continually looking for ways to elevate their user experience, including by decreasing generation times for content. They worked with the Innovation Center to optimize the Variational Autoencoder decoder of their already efficient video generation pipeline. Strategic optimization of the model‚Äôs causal convolution layers unlocked powerful compiler performance gains, while asynchronous video chunk writing eliminated GPU idle time ‚Äì together delivering a dramatic reduction in end-to-end latency and a 29% increase in decoding throughput. 
5. One size doesn‚Äôt fit all 
The one size doesn‚Äôt fit all principle applies to both model size and family. Some models excel out of the box for specific tasks like code generation, tool usage, document processing, or summarization. With the rapid pace of innovation, the best foundation model for a given use case today likely won‚Äôt be the best tomorrow. Model size corresponds to the number of parameters and often determines its ability to complete a broad set of general tasks and capabilities. However, larger models require more compute resources at inference time and can be expensive to run at production scale. Many applications don‚Äôt need a model that excels at everything but rather one that performs exceptionally well at a more limited set of tasks or domain-specific capabilities. 
Even within a single application, optimization may require using multiple model providers depending on the specific task, complexity level, and latency requirements. In agentic applications, you might use a lightweight model for specialized agent tasks while requiring a more powerful generalist model to orchestrate and supervise those agents. Architecting your solution to be modular and resilient to changing model providers or versions helps you adapt quickly and capitalize on improvements. Services like Amazon Bedrock facilitate this approach by providing a unified API experience across a broad range of model families, including custom versions of many models. 
How the Innovation Center can help 
The Custom Model Program by the Innovation Center provides end-to-end expert support from model selection to customization, delivering performance improvements, and reducing time-to-market and value realization. Our process works backwards from customer business needs, strategy and goals, and starts with a use case and generative AI capability review by an experienced generative AI strategist. Specialist hands-on-keyboard applied scientists and engineers embed with customer teams to train and tune models for customers and integrate into applications without data ever needing to leave customer VPCs. This end-to-end support has helped organizations across industries successfully transform their AI vision into real business outcomes. 
 
Want to learn more? Contact your account manager to learn more about the Innovation Center or come see us at re:Invent at the AWS Village in the Expo. 
 
About the authors 
Sri Elaprolu serves as Director of the AWS Generative AI Innovation Center, where he leverages nearly three decades of technology leadership experience to drive artificial intelligence and machine learning innovation. In this role, he leads a global team of machine learning scientists and engineers who develop and deploy advanced generative and agentic AI solutions for enterprise and government organizations facing complex business challenges. Throughout his nearly 13-year tenure at AWS, Sri has held progressively senior positions, including leadership of ML science teams that partnered with high-profile organizations such as the NFL, Cerner, and NASA. These collaborations enabled AWS customers to harness AI and ML technologies for transformative business and operational outcomes. Prior to joining AWS, he spent 14 years at Northrop Grumman, where he successfully managed product development and software engineering teams. Sri holds a Master‚Äôs degree in Engineering Science and an MBA with a concentration in general management, providing him with both the technical depth and business acumen essential for his current leadership role. 
Hannah Marlowe leads the Model Customization and Optimization program for the AWS Generative AI Innovation Center. Her global team of strategists, specialized scientists, and engineers embeds directly with AWS customers, developing custom model solutions optimized for relevance, latency, and cost to drive business outcomes and capture ROI. Previous roles at Amazon include Senior Practice Manager for Advanced Computing and Principal Lead for Computer Vision and Remote Sensing. Dr. Marlowe completed her PhD in Physics at the University of Iowa in modeling and simulation of astronomical X-ray sources and instrumentation development for satellite-based payloads. 
 Rohit Thekkanal serves as ML Engineering Manager for Model Customization at the AWS Generative AI Innovation Center, where he leads the development of scalable generative AI applications focused on model optimization. With nearly a decade at Amazon, he has contributed to machine learning initiatives that significantly impact Amazon‚Äôs retail catalog. Rohit holds an MBA from The University of Chicago Booth School of Business and a Master‚Äôs degree from Carnegie Mellon University. 
Alexandra Fedorova leads Growth for the Model Customization and Optimization program for the AWS Generative AI Innovation Center. Previous roles at Amazon include Global GenAI Startups Practice Leader with the AWS Generative AI Innovation Center, and Global Leader, Startups Strategic Initiatives and Growth. Alexandra holds an MBA degree from Southern Methodist University, and BS in Economics and Petroleum Engineering from Gubkin Russian State University of Oil and Gas.
‚Ä¢ Clario streamlines clinical trial software configurations using Amazon Bedrock
  This post was co-written with Kim Nguyen and Shyam Banuprakash from Clario. 
Clario is a leading provider of endpoint data solutions for systematic collection, management, and analysis of specific, predefined outcomes (endpoints) to evaluate a treatment‚Äôs safety and effectiveness in the clinical trials industry, generating high-quality clinical evidence for life sciences companies seeking to bring new therapies to patients. Since Clario‚Äôs founding more than 50 years ago, the company‚Äôs endpoint data solutions have supported clinical trials more than 30,000 times with over 700 regulatory approvals across more than 100 countries. 
This post builds upon our previous post discussing how Clario developed an AI solution powered by Amazon Bedrock to accelerate clinical trials. Since then, Clario has further enhanced their AI capabilities, focusing on innovative solutions that streamline the generation of software configurations and artifacts for clinical trials while delivering high-quality clinical evidence. 
Business challenge 
In clinical trials, designing and customizing various software systems configurations to manage and optimize the different stages of a clinical trial efficiently is critical. These configurations can range from basic study setup to more advanced features like data collection customization and integration with other systems. Clario uses data from multiple sources to build specific software configurations for clinical trials. The traditional workflow involved manual extraction of necessary data from individual forms. These forms contained vital information about exams, visits, conditions, and interventions. Additionally, the process required the need to incorporate study-related information such as study plans, participation criteria, sponsors, collaborators, and standardized exam protocols from multiple enterprise data providers. 
The manual nature of this process created several challenges: 
 
 Manual data extraction ‚Äì Team members manually review PDF documents to extract structured data. 
 Transcript challenges&nbsp;‚Äì The manual transfer of data from source forms into configuration documents presents opportunities for improvement, particularly in reducing transcription inconsistencies and enhancing standardization. 
 Version control challenges ‚Äì When studies required iterations or updates, maintaining consistency between documents and systems became increasingly complicated. 
 Fragmented information flow ‚Äì Data existed in disconnected silos, including PDFs, study detail database records, and other standalone documents. 
 Software build timelines ‚Äì The configuration process directly impacted the timeline for generating the necessary software builds. 
 
For clinical trials where timing is essential and accuracy is non-negotiable, Clario has implemented rigorous quality control measures to minimize the risks associated with manual processes. While these efforts are substantial, they underscore a business challenge of ensuring precision and consistency across complex study configurations. 
Solution overview 
To address the business challenge, Clario developed a generative AI-powered solution that Clario refers to as the Clario‚Äôs Genie AI Service on AWS. This solution uses the capabilities of large language models (LLMs), specifically Anthropic‚Äôs Claude 3.7 Sonnet on Amazon Bedrock. The process is orchestrated using Amazon Elastic Container Service (Amazon ECS) to transform how Clario handled software configuration for clinical trials. 
Clario‚Äôs approach uses a custom data parser using Amazon Bedrock to automatically structure information from PDF transmittal forms into validated tables. The Genie AI Service centralizes data from multiple sources, including transmittal forms, study details, standard exam protocols, and additional configuration parameters. An interactive review dashboard helps stakeholders verify AI-extracted information and make necessary corrections before finalizing the validated configuration. Post-validation, the system automatically generates a Software Configuration Specification (SCS) document as a comprehensive record of the software configuration. The process culminates with generative AI-powered XML generation, which is then released into Clario‚Äôs proprietary medical imaging software for study builds, creating an end-to-end solution that drastically reduces manual effort while improving accuracy in clinical trial software configurations. 
The Genie AI Service architecture consists of several interconnected components that work together in a clear workflow sequence, as illustrated in the following diagram. 
 
The workflow consists of the following steps: 
 
 Initiate the study and collect data. 
 Extract the data using Amazon Bedrock. 
 Review and validate the AI-generated output. 
 Generate essential documentation and code artifacts. 
 
In the following sections, we discuss the workflow steps in more detail. 
Study initiation and data collection 
The workflow begins with gathering essential study information through multiple integrated steps: 
 
 Study code lookup ‚Äì Users begin by entering a study code that uniquely identifies the clinical trial. 
 API integration with study database ‚Äì The study lookup operation makes an API call to fetch study details such as such as study plan, participation criteria, sponsors, collaborators, and more from the study database, establishing the foundation for the configuration. 
 Transmittal form processing ‚Äì Users upload transmittal forms containing study parameters such as information about exams, visits, conditions, and interventions to the Genie AI Service using the web UI through a secure AWS Direct Connect network. 
 Data structuring ‚Äì The system organizes information into key categories: 
   
   Visit information (scheduling, procedures) 
   Exam specifications (protocols, requirements) 
   Study-specific custom fields (vitals, dosing information, and so on) 
    
 
Data extraction 
The solution uses Anthropic‚Äôs Claude Sonnet on Amazon Bedrock through API calls to perform the following actions: 
 
 Parse and extract structured data from transmittal forms 
 Identify key fields and tables within the documents 
 Organize the information into standardized formats 
 Apply domain-specific rules to properly categorize clinical trial visits 
 Extract and validate demographic fields while maintaining proper data types and formats 
 Handle specialized formatting rules for medical imaging parameters 
 Manage document-specific adaptations (such as different processing for phantom vs. subject scans) 
 
Review and validation 
The solution provides a comprehensive review interface for stakeholders to validate and refine the AI-generated configurations through the following steps: 
 
 Interactive review process ‚Äì Reviewers access the Genie AI Service interface to perform the following actions: 
   
   Examine the AI-generated output 
   Make corrections or adjustments to the data as necessary 
   Add comments and highlight adjustments made as a feedback mechanism 
   Validate the configuration accuracy 
    
 Data storage ‚Äì Reviewed and approved software configurations are saved to Clario‚Äôs Genie Database, creating a central, authoritative, auditable source of configuration data 
 
Document and code generation 
After the configuration data is validated, the solution automates the creation of essential documentation and code artifacts through a structured workflow: 
 
 SCS document creation ‚Äì Reviewers access the Genie AI Service interface to finalize the software configurations by generating an SCS document using the validated data. 
 XML generation workflow ‚Äì After the SCS document is finalized, the workflow completes the following steps: 
   
   The workflow fetches the configuration details from the Genie database. 
   The SCSXMLConverter, an internal microservice of the Genie AI Service, processes both SCS document and study configurations. This microservice invokes Anthropic‚Äôs Claude 3.7 Sonnet through API calls to generate a standardized SCS XML file. 
   Validation checks are performed on the generated XML to make sure it meets the structural and content requirements of Clario‚Äôs clinical study software. 
   The final XML output is created for use in the software build process with detailed logs of the conversion process. 
    
 
Benefits and results 
The solution enhanced data extraction quality while providing teams with a streamlined dashboard that accelerates the validation process. 
By implementing consistent extraction logic and minimizing manual data entry, the solution has reduced potential transcription errors. Additionally, built-in validation safeguards now help identify potential issues early in the process, preventing problems from propagating downstream. 
The solution has also transformed how teams collaborate. By providing centralized review capabilities and giving cross-functional teams access to the same solution, communication has become more transparent and efficient. The standardized workflows have created clearer channels for information sharing and decision-making. 
From an operational perspective, the new approach offers greater scalability across studies while supporting iterations as studies evolve. This standardization has laid a strong foundation for expanding these capabilities to other operational areas within the organization. 
Importantly, the solution maintains strong compliance and auditability through complete audit trails and reproducible processes. Key outcomes include: 
 
 Study configuration execution time has been reduced while improving overall quality 
 Teams can focus more on value-added activities like study design optimization. 
 
Lessons learned 
Clario‚Äôs journey to transform software configuration through generative AI has taught them valuable lessons that will inform future initiatives. 
Generative AI implementation insights 
The following key learnings emerged specifically around working with generative AI technology: 
 
 Prompt engineering is foundational ‚Äì Few-shot prompting with domain knowledge is essential. The team discovered that providing detailed examples and explicit business rules in the prompts was necessary for success. Rather than simple instructions, Clario‚Äôs prompts include comprehensive business logic, edge case handling, and exact output formatting requirements to guide the AI‚Äôs understanding of clinical trial configurations. 
 Prompt engineering requires iteration ‚Äì The quality of data extraction depends heavily on well-crafted prompts that encode domain expertise. Clario‚Äôs team spent significant time refining these prompts through multiple iterations and testing different approaches to capture complex business rules about visit sequencing, demographic requirements, and field formatting. 
 Human oversight within a validation workflow ‚Äì Although generative AI dramatically accelerates extraction, human review remains necessary within a structured validation workflow. The Genie AI Service interface was specifically designed to highlight potential inconsistencies and provide convenient editing capabilities for reviewers to apply their expertise efficiently. 
 
Integration challenges 
Some important challenges surfaced during system integration: 
 
 Two-system synchronization ‚Äì One of the biggest challenges has been verifying that changes made in the SCS documents are reflected in the solution. This bidirectional integration is still being refined. 
 System transition strategy ‚Äì Moving from the proof-of-concept scripts to fully integrated solution functionality requires careful planning to avoid disruption. 
 
Process adaptation 
The team identified the following key factors for successful process change: 
 
 Phased Implementation ‚Äì Clario rolled out the solution in stages, beginning with pilot teams who could validate functionality and serve as internal advocates to help teams transition from familiar document-centric workflows to the new solution. 
 Workflow optimization is iterative ‚Äì The initial workflow design has evolved based on user feedback and real-world usage patterns. 
 Training requirements ‚Äì Even with an intuitive interface, proper training makes sure users can take full advantage of the solution‚Äôs capabilities. 
 
Technical considerations 
Implementation revealed several important technical aspects to consider: 
 
 Data formatting variability ‚Äì Transmittal forms vary significantly across different therapeutic areas (oncology, neurology, and so on) and even between studies within the same area. This variability creates challenges when the AI model encounters form structures or terminology it hasn‚Äôt seen before. Clario‚Äôs prompt engineering requires continuous iteration as they discover new patterns and edge cases in transmittal forms, creating a feedback loop where human experts identify missed or misinterpreted data points that inform future prompt refinements. 
 Performance optimization ‚Äì Processing times for larger documents required optimization to maintain a smooth user experience. 
 Error handling robustness ‚Äì Building resilient error handling into the generative AI processing flow was essential for production reliability. 
 
Strategic insights 
The project yielded valuable strategic lessons that will inform future initiatives: 
 
 Start with well-defined use cases ‚Äì Beginning with the software configuration process gave Clario a concrete, high-value target for demonstrating generative AI benefits. 
 Build for extensibility ‚Äì Designing the architecture with future expansion in mind has positioned them well for extending these capabilities to other areas. 
 Measure concrete outcomes ‚Äì Tracking specific metrics like processing time and error rates has helped quantify the return on the generative AI investment. 
 
These lessons have been invaluable for refining the current solution and informing the approach to future generative AI implementations across the organization. 
Conclusion 
The transformation of the software configuration process through generative AI represents more than just a technical achievement for Clario‚Äîit reflects a fundamental shift in how the company approaches data processing and knowledge work in clinical trials. By combining the pattern recognition and processing power of LLMs available in Amazon Bedrock with human expertise for validation and decision-making, Clario created a hybrid workflow that delivers the best of both worlds, orchestrated through Amazon ECS for reliable, scalable execution. 
The success of this initiative demonstrates how generative AI on AWS is a practical tool that can deliver tangible benefits. By focusing on specific, well-defined processes with clear pain points, Clario has implemented the solution Genie AI Service powered by Amazon Bedrock in a way that creates immediate value while establishing a foundation for broader transformation. 
For organizations considering similar transformations, the experience highlights the importance of starting with concrete use cases, building for human-AI collaboration and maintaining a focus on measurable business outcomes. With these principles in mind, generative AI can become a genuine catalyst for organizational evolution. 
 
About the authors 
 Kim Nguyen serves as the Sr Director of Data Science at Clario, where he leads a team of data scientists in developing innovative AI/ML solutions for the healthcare and clinical trials industry. With over a decade of experience in clinical data management and analytics, Kim has established himself as an expert in transforming complex life sciences data into actionable insights that drive business outcomes. His career journey includes leadership roles at&nbsp;Clario and Gilead Sciences, where he consistently pioneered data automation and standardization initiatives across multiple functional teams. Kim holds a Master‚Äôs degree in Data Science and Engineering from UC San Diego and a Bachelor‚Äôs degree from the University of California, Berkeley, providing him with the technical foundation to excel in developing predictive models and data-driven strategies. Based in San Diego, California, he leverages his expertise to drive forward-thinking approaches to data science in the clinical research space. 
Shyam Banuprakash serves as the Senior Vice President of Data Science and Delivery at Clario, where he leads complex analytics programs and develops innovative data solutions for the medical imaging sector. With nearly 12 years of progressive experience at Clario, he has demonstrated exceptional leadership in data-driven decision making and business process improvement. His expertise extends beyond his primary role, as he contributes his knowledge as an Advisory Board Member for both Modal and UC Irvine‚Äôs Customer Experience Program. Shyam holds a Master of Advanced Study in Data Science and Engineering from UC San Diego, complemented by specialized training from MIT in data science and big data analytics. His career exemplifies the powerful intersection of healthcare, technology, and data science, positioning him as a thought leader in leveraging analytics to transform clinical research and medical imaging. 
Praveen Haranahalli is a Senior Solutions Architect at Amazon Web Services (AWS), where he architects secure, scalable cloud solutions and provides strategic guidance to diverse enterprise customers. With nearly two decades of IT experience including over a decade specializing in cloud computing, Praveen has delivered transformative implementations across multiple industries. As a trusted technical advisor, Praveen partners with customers to implement robust DevSecOps pipelines, establish comprehensive security guardrails, and develop innovative AI/ML solutions. He is passionate about solving complex business challenges through cutting-edge cloud architectures and empowering organizations to achieve successful digital transformations powered by artificial intelligence and machine learning.
‚Ä¢ Introducing Amazon Bedrock cross-Region inference for Claude Sonnet 4.5 and Haiku 4.5 in Japan and Australia
  „Åì„Çì„Å´„Å°„ÅØ, G‚Äôday. 
The recent launch of Anthropic‚Äôs Claude Sonnet 4.5 and Claude Haiku 4.5, now available on Amazon Bedrock, marks a significant leap forward in generative AI models. These state-of-the-art models excel at complex agentic tasks, coding, and enterprise workloads, offering enhanced capabilities to developers. Along with the new models, we are&nbsp;thrilled to announce that customers in Japan and Australia can now access Anthropic Claude Sonnet 4.5 and Anthropic Claude Haiku 4.5 in Amazon Bedrock while processing the data in their specific geography by using Cross-Region inference (CRIS). This can be useful when customers need to meet the requirements to process data locally. 
This post will explore the new geographic-specific cross-Region inference profile in Japan and Australia for Claude Sonnet 4.5 and Claude Haiku 4.5. We will delve into the details of these geographic-specific CRIS profiles, provide guidance for migrating from older models, and show you how to get started with this new capability to unlock the full potential of these models for your generative AI applications. 
Japan and Australia Cross-Region inference 
With Japan and Australia cross-Region inference you can make calls to Anthropic Claude Sonnet 4.5 or Claude Haiku 4.5 within your local geography. By using CRIS Amazon Bedrock processes the inference requests within the geographic boundaries, either Japan or Australia, through the entire inference request lifecycle. 
How Cross-Region inference works 
Cross-Region inference in Amazon Bedrock operates through the AWS Global Network with end-to-end encryption for data in transit and at rest. When a customer submits an inference request in the source AWS Region, Amazon Bedrock automatically evaluates available capacity in each&nbsp;potential destination Region and routes their request to the optimal destination Region. The traffic flows exclusively over the AWS Global Network without traversing the public internet between Regions listed as destination for your source Region, using the AWS internal service-to-service communication patterns. Following the same design, the Japan and Australia GEO CRIS use the secure AWS Global Network to automatically route traffic between Regions within their respective geographies ‚Äì between Tokyo and Osaka in Japan, and between Sydney and Melbourne in Australia. CRIS uses intelligent routing that distributes traffic dynamically across multiple Regions within the same geography, without requiring manual user configuration or intervention. 
Cross-Region inference configuration 
The CRIS configurations for Japan and Australia are described in the following tables. 
Japan CRIS:&nbsp;For organizations operating within Japan, the CRIS system provides routing between Tokyo and Osaka Regions. 
 
  
   
   Source Region 
   Destination Region 
   Description 
   
   
   ap-northeast-1 (Tokyo) 
   ap-northeast-1 (Tokyo)ap-northeast-3 (Osaka) 
   Requests from the Tokyo Region can be automatically routed to either Tokyo or Osaka Regions. 
   
   
   ap-northeast-3 (Osaka) 
   ap-northeast-1 (Tokyo)ap-northeast-3 (Osaka) 
   Requests from the Osaka Region can be automatically routed to either Tokyo or Osaka Regions. 
   
  
 
Australia CRIS:&nbsp;For organizations operating within Australia, the CRIS system provides routing between Sydney and Melbourne Regions. 
 
  
   
   Source Region 
   Destination Region 
   Description 
   
   
   ap-southeast-2 (Sydney) 
   ap-southeast-2 (Sydney)ap-southeast-4 (Melbourne) 
   Requests from the Sydney Region can be automatically routed to either Sydney or Melbourne Regions. 
   
   
   ap-southeast-4 (Melbourne) 
   ap-southeast-2 (Sydney)ap-southeast-4 (Melbourne) 
   Requests from the&nbsp;Melbourne Region can be automatically routed to either Sydney or Melbourne Regions. 
   
  
 
Note: A list of destination Regions is listed for each source Region within your inference profile. 
Getting started 
To get started with Australia or Japan CRIS, follow these steps using&nbsp;Amazon Bedrock&nbsp;inference profiles. 
 
 Configure IAM Permission: Verify your IAM role or user has the necessary permissions to invoke Amazon Bedrock models using a cross-Region inference profile. To allow an IAM user or role to invoke a geographic-specific cross-Region inference profile, you can use the following example policy.The first statement in the policy allows Amazon Bedrock InvokeModel&nbsp;API access to the GEO specific cross-Region inference profile resource for requests originating from the nominated Region. GEO specific inference profiles are prefix by the Region code (‚Äújp‚Äù for Japan and ‚Äúau‚Äù for Australia). In this example, the nominated requesting Region is ap-northeast-1 (Tokyo)&nbsp;and the inference profile is&nbsp;jp.anthropic.claude-sonnet-4-5-20250929-v1:0.The second statement allows the GEO specific cross-Region inference profile to access and invoke the matching foundation models in the Region where the GEO specific inference profile will route to. In this example, the Japan cross-Region inference profiles can route to either ap-northeast-1 (Tokyo) or ap-northeast-3 (Osaka). 
   
   {
    "Version":"2012-10-17",                   
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "bedrock:InvokeModel*"
            ],
            "Resource": [
                "arn:aws:bedrock:ap-northeast-1:&lt;your-account-id&gt;:inference-profile/jp.anthropic.claude-sonnet-4-5-20250929-v1:0"
            ]
        },
        {
            "Effect": "Allow",
            "Action": [
                "bedrock:InvokeModel*"
            ],
            "Resource": [
                "arn:aws:bedrock:ap-northeast-1::foundation-model/anthropic.claude-sonnet-4-5-20250929-v1:0",
                "arn:aws:bedrock:ap-northeast-3::foundation-model/anthropic.claude-sonnet-4-5-20250929-v1:0"
            ],
            "Condition": {
                "StringLike": {
                    "bedrock:InferenceProfileArn": "arn:aws:bedrock:ap-northeast-1:&lt;your-account-id&gt;:inference-profile/jp.anthropic.claude-sonnet-4-5-20250929-v1:0"
                }
            }
        }
    ]
} 
    
 Use cross-Region inference profile: Configure your application to use the relevant inference profile ID. This works for both the InvokeModel&nbsp;and Converse&nbsp;APIs. 
 
Inference Profiles for Anthropic Claude Sonnet 4.5 
 
  
   
   Region 
   Inference Profile ID 
   
   
   Australia 
   au.anthropic.claude-sonnet-4-5-20250929-v1:0 
   
   
   Japan 
   jp.anthropic.claude-sonnet-4-5-20250929-v1:0 
   
  
 
Inference Profiles for Anthropic Claude Haiku 4.5 
 
  
   
   Region 
   Inference Profile ID 
   
   
   Australia 
   au.anthropic.claude-haiku-4-5-20251001-v1:0 
   
   
   Japan 
   jp.anthropic.claude-haiku-4-5-20251001-v1:0 
   
  
 
Example Code 
Using the Converse API (Python) with Japan CRIS inference profile. 
 
 import boto3

# Initialize Bedrock Runtime client
bedrock_runtime = boto3.client(
&nbsp;&nbsp; &nbsp;service_name="bedrock-runtime",
&nbsp;&nbsp; &nbsp;region_name="ap-northeast-1"&nbsp;&nbsp;# Your originating Region
)

# Define the inference profile ID
inference_profile_id = "jp.anthropic.claude-sonnet-4-5-20250929-v1:0"

# Prepare the conversation
response = bedrock_runtime.converse(
&nbsp;&nbsp; &nbsp;modelId=inference_profile_id,
&nbsp;&nbsp; &nbsp;messages=[
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;{
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"role": "user",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"content": [{"text": "What is Amazon Bedrock?"}]
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp;],
&nbsp;&nbsp; &nbsp;inferenceConfig={
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"maxTokens": 512,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"temperature": 0.7
&nbsp;&nbsp; &nbsp;}
)

# Print the response
print(f"Response: {response['output']['message']['content'][0]['text']}") 
 
Quota management 
When using CRIS, it is important to understand how quotas are managed. For geographic-specific CRIS, quota management is performed at the source Region level. This means that quota increases requested from the source Region will only apply to requests originating from that Region. For example, if you request a quota increase from the Tokyo (ap-northeast-1) Region, it will only apply to requests that originate from the Tokyo Region. Similarly, quota increase requests from Osaka only apply to requests originating from Osaka. When requesting a quota increase, organizations should consider their regional usage patterns and request increases in the appropriate source Regions through the AWS Service Quotas console. This Region-specific quota management allows for more granular control over resource allocation while maintaining data local processing requirements. 
Requesting a quota increase 
For requesting quota increases for CRIS in Japan and Australia, organizations should use the AWS Service Quotas console in their respective source Regions (Tokyo/Osaka for Japan, and Sydney/Melbourne for Australia). Organizations and customers can search for specific quotas related to Claude Sonnet 4.5 or Claude Haiku 4.5 model inference tokens (per day and per minute) and submit increase requests based on their workload requirements in the specific Region. 
 
Quota management best practices 
To manage your quotas, follow these best practices: 
 
 Request increase proactively: Each organization receives default quota allocations based on their account history and usage patterns. These quotas are measured in tokens per minute (TPM) and requests per minute (RPM).&nbsp;For Claude Sonnet 4.5 and Claude Haiku 4.5, quotas typically start at conservative levels and can be increased based on demonstrated need and usage patterns. If you anticipate high usage, request quota increase through the AWS Service Quotas console before your deployment. 
 Monitor utilization: Implement monitoring of your quota usage to minimize the chances of reaching quota limits to help prevent service interruptions and optimize resource allocation. AWS provides CloudWatch metrics that track quota utilization in real-time, allowing organizations to set up alerts when usage approaches defined thresholds. The monitoring system should track both current usage and historical patterns to identify trends and predict future quota needs. This data is essential for planning quota increase requests and optimizing application behavior to work within available limits. Organizations should also monitor quota usage across different time periods to identify peak usage patterns and plan accordingly. 
 Test at scale: Before production deployment, conduct load testing to understand your quota requirements under realistic conditions. Testing at scale requires establishing realistic scenarios that mirror production traffic patterns, including peak usage periods and concurrent user loads.&nbsp;Implement progressive load testing while&nbsp;monitoring response times, error rates, and quota utilization. 
 
Important: When&nbsp;calculating your required quota increase, you need to take into account for the burndown rate, defined as the rate at which input and output tokens are converted into token quota usage for the throttling system. The following models have a 5x burn down rate for output tokens&nbsp;(1 output token consumes 5 tokens from your quotas): 
 
 Anthropic Claude Opus 4 
 Anthropic Claude Sonnet 4.5 
 Anthropic Claude Sonnet 4 
 Anthropic Claude 3.7 Sonnet 
 
For other models, the burndown rate is 1:1 (1 output token consumes 1 token from your quota). For input tokens, the token to quota ratio is 1:1. The calculation for the total number of tokens per request is as follows: 
Input token count + Cache write input tokens + (Output token count x Burndown rate) 
Migrating from Claude 3.5 to Claude 4.5 
Organizations currently using Claude Sonnet 3.5 (v1 and v2) and Claude Haiku 3.5 models should plan their migration to Claude Sonnet 4.5 and Claude Haiku 4.5 respectively. Claude Sonnet 4.5 and Haiku 4.5 are hybrid reasoning models that represents a substantial advancement over its predecessors. They feature advanced capabilities in tool handling with improvements in memory management and context processing. This migration presents an opportunity to use enhanced capabilities while maintaining compliance with data local processing requirements through CRIS. 
Key Migration Considerations 
The transition from Claude 3.5 to 4.5 involves several critical factors beyond simple model replacement. 
 
 Performance benchmarking should be your first priority, as Claude 4.5 demonstrates significant improvements in agentic tasks, coding capabilities, and enterprise workloads compared to its predecessors. Organizations should establish standardized benchmarks specific to their use cases to make sure the new model meets or exceeds current performance requirements. 
 Claude 4.5 introduces several advanced technical capabilities. The enhanced context processing enables more sophisticated prompt optimization, requiring organizations to refine their existing prompts to fully leverage the model‚Äôs capabilities. The model supports more complex tool integration patterns and demonstrates improved performance in multi-modal tasks. 
 Cost optimization represents another crucial consideration. Organizations should conduct thorough cost-benefit analysis including potential quota increases and capacity planning requirements. 
 
For more technical implementation guidance, organizations should reference the AWS blog post, Migrate from Anthropic‚Äôs Claude 3.5 Sonnet to Claude 4 Sonnet on Amazon Bedrock, which provides essential best practices that are also valid for the migration to the new Claude Sonnet 4.5 model. Additionally, Anthropic‚Äôs migration documentation offers model-specific optimization strategies and considerations for transitioning to Claude 4.5 models. 
Given the accelerated pace of generative AI model evolution, organizations should adopt agile migration processes. Industry standards now expect model migrations every six to twelve months, making it essential to develop systematic approaches rather than over-optimizing for specific model versions. 
Choosing between Global Cross-Region inference or GEO Cross-Region inference 
Amazon Bedrock offers two types of cross-Region inference profile to help you scale AI workflows during high demand. While both automatically distribute traffic across multiple Regions, they differ in their geographical scope and pricing models. 
For customers who need to process data locally within specific geographical boundaries, GEO CRIS is the recommended option, as it makes sure inference processing stays within the geography boundaries of the specified GEO. 
For customers without data residency or cross-GEO constraints, Global CRIS scales and routes to supported AWS commercial Regions for customers who need higher throughput at a lower price for Claude 4.5 models compared to GEO CRIS. 
Conclusion 
In this post, we introduced the availability of Anthropic‚Äôs Claude Sonnet 4.5 and Claude Haiku 4.5 on Amazon Bedrock with cross-Region inference capabilities for Japan and Australia. We discussed how organizations can harness advanced AI capabilities while adhering to local data processing requirements, making sure the inference requests remain within geographical boundaries. This new feature is recommended for sectors such as financial institutions, healthcare providers, and government agencies handling sensitive data. We also provided guidance on how to get started and covered quota management strategies, as well as migration guidance from older Claude models to Claude 4.5 models. To understand more of the pricing for Claude Sonnet 4.5 and Claude Haiku 4.5 on Bedrock, please refer to Amazon Bedrock pricing. 
Through this capability, organizations can now confidently implement production applications with Claude Sonnet 4.5 and Claude Haiku 4.5 that not only meet their performance requirements but also the local data processing requirements, marking a significant advancement in the responsible deployment of AI technology in these countries. 
 
About the authors 
Derrick Choo&nbsp;is a Senior Solutions Architect at AWS who accelerates enterprise digital transformation through cloud adoption, AI/ML, and generative AI solutions. He specializes in full-stack development and ML, designing end-to-end solutions spanning frontend interfaces, IoT applications, data integrations, and ML models, with a particular focus on computer vision and multi-modal systems. 
Melanie Li, PhD, is a Senior Generative AI Specialist Solutions Architect at AWS based in Sydney, Australia, where her focus is on working with customers to build solutions using state-of-the-art AI/ML tools. She has been actively involved in multiple generative AI initiatives across APJ, harnessing the power of LLMs. Prior to joining AWS, Dr. Li held data science roles in the financial and retail industries. 
Saurabh Trikande&nbsp;is a Senior Product Manager for Amazon Bedrock and Amazon SageMaker Inference. He is passionate about working with customers and partners, motivated by the goal of democratizing AI. He focuses on core challenges related to deploying complex AI applications, inference with multi-tenant models, cost optimizations, and making the deployment of generative AI models more accessible. In his spare time, Saurabh enjoys hiking, learning about innovative technologies, following TechCrunch, and spending time with his family. 
Jared Dean is a Principal AI/ML Solutions Architect at AWS. Jared works with customers across industries to develop machine learning applications that improve efficiency. He is interested in all things AI, technology, and BBQ. 
Stephanie Zhao is a Generative AI GTM &amp; Capacity Lead for AWS in Asia Pacific and Japan. She champions the voice of the customer to drive the roadmap for AWS Generative AI services including Amazon Bedrock and Amazon EC2 GPUs across AWS Regions in APJ. Outside of work, she enjoys using Generative AI creative models to make portraits of her shiba inu and cat. 
Kazuki Motohashi, Ph.D. is an AI/ML GTM Specialist Solutions Architect at AWS Japan. He has been working in the AI/ML field for more than 8 years and currently supports Japanese enterprise customers and partners who utilize AWS generative AI/ML services in their businesses. He‚Äôs seeking time to play Final Fantasy Tactics, but hasn‚Äôt even started it yet.
‚Ä¢ Reduce CAPTCHAs for AI agents browsing the web with Web Bot Auth (Preview) in Amazon Bedrock AgentCore Browser
  AI agents need to browse the web on your behalf. When your agent visits a website to gather information, complete a form, or verify data, it encounters the same defenses designed to stop unwanted bots: CAPTCHAs, rate limits, and outright blocks. 
Today, we are excited to share that AWS has a solution. Amazon Bedrock AgentCore Browser, our secure, cloud-based browser for AI agents to interact with websites, now supports Web Bot Auth (in preview), a draft IETF protocol that gives agents verifiable cryptographic identities. 
CAPTCHA friction 
Customers tell us that CAPTCHA friction is one of the biggest obstacles to reliable browser-based agentic workflows. Your agent halts mid-task, waiting for human intervention to solve a puzzle that proves you‚Äôre not a bot ‚Äì except your agent is a bot, and that‚Äôs the point. CAPTCHAs exist for good reason. Websites face constant challenges protecting their content, inventory and reviews. Web Application Firewalls (WAFs) and bot detection services protect these sites, but they treat nearly all automated traffic as suspicious because they have no reliable way to distinguish legitimate agents from malicious ones. 
Some automation providers try to solve CAPTCHAs programmatically ‚Äì using computer vision models to read distorted text or clicking through image grids until the puzzle clears. This approach is brittle, expensive, and is bypassing controls that domain owners intended for their content. Other approaches rely on IP allowlists or User-Agent strings. IP allowlists break when you run agents in cloud environments where addresses change frequently. User-Agent strings can be spoofed by anyone, so they provide no verification, and pose a risk of people emulating well trusted strings. Both methods require manual coordination with every website you want to access, which does not scale. 
Web Bot Auth: Cryptographic identity for agents browsing the web 
Web Bot Auth is a draft IETF protocol that gives agents verifiable cryptographic identities. When you enable Web Bot Auth in AgentCore Browser, we issue cryptographic credentials that websites can verify. The agent presents these credentials with every request. The WAF may now additionally check the signature, confirm it matches a trusted directory, and allow the request through if verified bots are allowed by the domain owner and other WAF checks are clear. 
AgentCore is working with Cloudflare, HUMAN Security, and Akamai Technologies to support this verification flow. These providers protect millions of websites. When you create an AgentCore Browser with signing enabled in the configuration, we automatically register your agent‚Äôs signature directory with these providers. Many domains already configure their WAFs to allow verified bots by default, which means you can see immediate CAPTCHA reduction without additional setup in the cases that this happens. 
How domain owners control access 
WAF providers give website owners three levels of control using Web Bot Auth: 
 
 Block all bots ‚Äì Some sites choose to block automated traffic entirely. Web Bot Auth does not bypass this ‚Äì if a domain wants no automation, that choice is respected. 
 Allow verified bots ‚Äì Many domains configure their WAF to allow any bot that presents a valid cryptographic signature. This is the default policy for a growing number of sites protected by Cloudflare, HUMAN Security, and Akamai Technologies. When you enable signing, as a parameter in the AgentCore Browser configuration, this policy will apply to your agents. 
 Allow specific verified bots to conduct only specific actions ‚Äì For example, a financial services company automating vendor portal access can share its unique directory with those vendors. The vendor can create rules like ‚Äúallow FinCo agents at 100 requests per minute, don‚Äôt allow them to create new accounts, and block all other signed agents.‚Äù This gives websites granular control while preserving the benefits of cryptographic verification. 
 
Today‚Äôs preview release of Web Both Auth support in AgentCore Browser helps reduce friction with CAPTCHAs on domains that allow verified bots, by making your agent appear as a verified bot. Once the Web Bot Auth protocol is finalized, AgentCore intends to transition to customer-specific keys, so AgentCore users can use the tier of control that allows only specified verified bots. 
Using the Web Bot Auth protocol 
To enable the browser to sign requests using the Web Bot Auth protocol, create a browser tool with the browserSigning configuration: 
 
 import boto3
cp_client = boto3.client('bedrock-agentcore-control')
response = cp_client.create_browser(
    name="signed_browser",
    description="Browser tool with Web Bot Auth enabled",
    networkConfiguration={
        "networkMode": "PUBLIC"
    },
    executionRoleArn="arn:aws:iam::123456789012:role/AgentCoreExecutionRole",
    browserSigning={
        "enabled": True
    }
)
browserId = response['browserId']
 
 
Pass the browser identifier to your agent framework. Here is an example using Strands Agents: 
 
 from strands import Agent
from strands_tools.browser import AgentCoreBrowser
agent_core_browser = AgentCoreBrowser(
    region="us-west-2",
    identifier=browserId
)
strands_agent = Agent(
    tools=[agent_core_browser.browser],
    model="anthropic.claude-4-5-haiku-20251001-v1:0",
    system_prompt="You are a website analyst. Use the browser tool efficiently."
)
result = strands_agent("Analyze the website at &lt;https://example.com/&gt;")
 
 
The agent is now configured to use the new browser tool that signs every HTTP request. Websites protected by Cloudflare, HUMAN Security, or Akamai Technologies can verify the signature and allow the request through without presenting a CAPTCHA, if the domain owner allows verified bots. 
Protocol development 
The Web Bot Auth protocol is gaining industry momentum because it solves a real problem: legitimate automation is indistinguishable from abuse without verifiable identity. You can read the draft protocol specification, HTTP Message Signatures for automated traffic Architecture. The architecture defines how agents generate signatures, how WAFs verify them, and how key directories enable discovery. Amazon is working with Cloudflare and many popular WAF providers to help finalize the customer-specific key directory format and work towards finalizing the draft. 
Conclusion 
Amazon Bedrock AgentCore Browser is generally available, with the Web Bot Auth feature available in preview. AgentCore Browser signing requests using the Web Bot Auth protocol help reduce friction with CAPTCHA across domains that allow verified bots. As the protocol finalizes, AgentCore Browser intends to issue customer-specific keys and directories, so you can prove your agent‚Äôs identity to specific websites and establish trust relationships directly with the domains you need to access. 
Web Bot Auth enables agents to prove their identity when challenged, reduces operational friction in automated workflows, and gives website owners control over which agents access their resources. Amazon Bedrock AgentCore Browser support for Web Bot Auth (Preview) provides the infrastructure layer that makes this possible. Try out AgentCore Browser in your account, with our&nbsp;samples on Github. 
 
About the authors 
Veda Raman is a Senior Specialist Solutions Architect for generative AI and machine learning at AWS. Veda works with customers to help them architect efficient, secure, and scalable machine learning applications. Veda specializes in generative AI services like Amazon Bedrock and Amazon SageMaker. 
Kosti Vasilakakis is a Principal PM at AWS on the Agentic AI team, where he has led the design and development of several Bedrock AgentCore services from the ground up, including Runtime, Browser, Code Interpreter, and Identity. He previously worked on Amazon SageMaker since its early days, launching AI/ML capabilities now used by thousands of companies worldwide. Earlier in his career, Kosti was a data scientist. Outside of work, he builds personal productivity automations, plays tennis, and enjoys life with his wife and kids. 
Joshua Samuel is a Senior AI/ML Specialist Solutions Architect at AWS who accelerates enterprise transformation through AI/ML, and generative AI solutions, based in Melbourne, Australia. A passionate disrupter, he specializes in agentic AI and coding techniques ‚Äì Anything that makes builders faster and happier.

‚∏ª