‚úÖ Morning News Briefing ‚Äì September 12, 2025 10:42

üìÖ Date: 2025-09-12 10:42
üè∑Ô∏è Tags: #briefing #ai #publichealth #digitalgov

‚∏ª

üßæ Weather
‚Ä¢ Current Conditions:  4.1¬∞C
  Temperature: 4.1&deg;C Pressure / Tendency: 102.6 kPa rising Humidity: 96 % Humidity : 96 % Dewpoint: 3.5&deg:C Wind: W 5 km/h Air Quality Health Index: n/a . Pembroke 6:00 AM EDT Friday 12 September 2025 Temperature: . 4/1/C
‚Ä¢ Friday: Sunny. High 22.
  Sunny. Sunny. High 22. UV index 6 or high . Sunny . High 22 . Sunny.UV index 6 . High . UV index six or high for high . Forecast issued 5:00 AM EDT Friday 12 Sept. 12 September 2025 . Weather forecast: Thursday, Friday, Saturday, September 12, and Sunday, September 11, 2018 . Weather conditions: Friday, Thursday,
‚Ä¢ Friday night: Chance of showers. Low 11. POP 30%
  Increasing cloudiness. 30 percent chance of showers overnight. Low 11.50/60/60 . Rainy showers expected to fall through the night. Showery showers possible in the morning . Forecast issued 5:00 AM EDT Friday 12 Sept. 12 September 2025. Showers expected to be felt throughout the day. Low 60/60 is below the 60/70s .

üåç International News
No updates.

üçÅ Canadian News
No updates.

üá∫üá∏ U.S. Top Stories
‚Ä¢ Police share video of Charlie Kirk shooting person of interest. And, prices rise
  Police shared photos and a video of a person fleeing from the scene after Charlie Kirk was fatally shot . Plus, inflation is rising, and the job market is weakening . So why are stocks soaring? So why do you think stocks are soaring? Share your thoughts with iReport.com/news@dailymailonline.com and may you know more about this week's featured news stories .
‚Ä¢ Prince Harry makes surprise visit to Ukraine in support of wounded troops
  Britain's Prince Harry has arrived in Ukraine for a surprise visit in support of wounded service members . Prince Harry is visiting Ukraine to support injured service members from the armed forces . The prince is visiting the country to support the wounded and wounded members of the Ukrainian armed forces. Prince Harry's visit comes as part of a tour of the country's military bases in Ukraine and Afghanistan. Prince William is
‚Ä¢ Why mothers in the U.S. are scaling back on their work lives
  Workforce participation for moms in the U.S. has been dropping for most of this year . The reasons are more complicated than return-to-office mandates . The team from "The Indicator" explains why the drop in workforce participation is more complicated . The Indicator: Moms need to make sure they're ready to be ready to work in the workplace for their families .
‚Ä¢ Insect populations drop even without direct human interference, a new study finds
  Research published this month found that even in pristine areas, insect populations are still on the decline . Climate change is a likely culprit, and climate change is likely to be a major factor in the decline of insect populations . Even in pristine, untouched areas, insects are still declining, according to new research . The decline is due to climate change, and scientists say it could be a factor in
‚Ä¢ Something big collapsed this week ‚Äî again. Do you know what? Take our quiz
  This week, we had news of a drawing that may or may not be the president's, and of a rock that may indicate that Mars had life . We also had news that a rock may have been found on Mars may have led to the discovery of life on the planet, but it's not clear if it was a rock or a drawing of the president . We are happy to

üß† Artificial Intelligence
No updates.

üíª Digital Strategy
‚Ä¢ Privacy activists warn digital ID won‚Äôt stop small boats ‚Äì but will enable mass surveillance
  Big Brother Watch says a so-called BritCard could turn daily life into one long identity check . Warns that Whitehall can‚Äôt be trusted to run a national digital ID could hand the government the tools for population-wide surveillance ‚Äì and if history is anything to go by, ministers probably couldn't run it without cocking it up .‚Ä¶‚Ä¶‚Ä¶ If history isn't
‚Ä¢ Hack to school: Parents told to keep their little script kiddies in line
  UK data watchdog says students behind most education cyberattacks . More than half of cyberattacks in schools are caused by students, UK data protection watchdog says . Parents should act early to prevent their offspring from falling into the wrong crowds, says watchdog . UK data agency says parents should take action to prevent children falling into wrong crowds . For confidential support call the Samaritans on 08457 90 90 90
‚Ä¢ Terminators: AI-driven robot war machines on the march
  I've read military science fiction since I was a kid . Robert A. Heinlein's Starship Troopers, Joe Haldeman's The Forever War, and David Drake's Hammer's Slammers books . Even if you've never read war science fiction, you certainly at least know about Terminators . But what was once science fiction is now reality on the Ukrainian battlefields. It won't stop there.
‚Ä¢ Huntress's 'hilarious' attacker surveillance splits infosec community
  Security outfit Huntress forced onto the defensive after latest research split opinion . Ethical concerns raised after crook offered themselves up on silver platter . Senior staff described by senior staff as "hilarious" and split opinion across the cybersecurity community . Huntress's latest research has been described as 'incredible' and 'incredibly funny' by senior members of staff . Security firm Hunt
‚Ä¢ ‚ÄòIT manager‚Äô needed tech support because they had never heard of a command line
  The Register's weekly tales of your support experiences reflect your working lives . Traceroute was also a mystery to this mountebank On Call . The Register is built on the premise that our readers know quite a lot about information technology, and that stories featured each Friday in On Call reflect your work lives .‚Ä¶‚Ä¶‚Ä¶ The Register‚Äôs On Call is based in London and features

üè• Public Health
No updates.

üî¨ Science
‚Ä¢ Influence of the SARS-CoV-2 pandemic and infection on musculoskeletal function
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Geographic restrictions in stimulus spending mitigated COVID-19 transmission in Seoul
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Functions of the global health system in a new era
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ A call for action: closing the gap on ethnic disparities in oral cavity cancer care
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Home-based care improves blood pressure control
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations

üßæ Government & Policy
No updates.

üèõÔ∏è Enterprise Architecture & IT Governance
No updates.

ü§ñ AI & Emerging Tech
‚Ä¢ How do AI models generate videos?
  MIT Technology Review Explains: Let our writers untangle the complex, messy world of technology to help you understand what‚Äôs coming next. You can read more from the series here.



It‚Äôs been a big year for video generation. In the last nine months OpenAI made Sora public, Google DeepMind launched Veo 3, the video startup Runway launched Gen-4. All can produce video clips that are (almost) impossible to distinguish from actual filmed footage or CGI animation. This year also saw Netflix debut an AI visual effect in its show The Eternaut, the first time video generation has been used to make mass-market TV.



Sure, the clips you see in demo reels are cherry-picked to showcase a company‚Äôs models at the top of their game. But with the technology in the hands of more users than ever before‚ÄîSora and Veo 3 are available in the ChatGPT and Gemini apps for paying subscribers‚Äîeven the most casual filmmaker can now knock out something remarkable.&nbsp;



The downside is that creators are competing with AI slop, and social media feeds are filling up&nbsp;with faked news footage. Video generation also uses up a huge amount of energy, many times more than text or image generation.&nbsp;



With AI-generated videos everywhere, let&#8217;s take a moment to talk about the tech that makes them work.



How do you generate a video?



Let‚Äôs assume you‚Äôre a casual user. There are now a range of high-end tools that allow pro video makers to insert video generation models into their workflows. But most people will use this technology in an app or via a website. You know the drill: ‚ÄúHey, Gemini, make me a video of a unicorn eating spaghetti. Now make its horn take off like a rocket.‚Äù What you get back will be hit or miss, and you‚Äôll typically need to ask the model to take another pass or 10 before you get more or less what you wanted.&nbsp;









So what‚Äôs going on under the hood? Why is it hit or miss‚Äîand why does it take so much energy? The latest wave of video generation models are what‚Äôs known as latent diffusion transformers. Yes, that‚Äôs quite a mouthful. Let‚Äôs unpack each part in turn, starting with diffusion.&nbsp;



What‚Äôs a diffusion model?



Imagine taking an image and adding a random spattering of pixels to it. Take that pixel-spattered image and spatter it again and then again. Do that enough times and you will have turned the initial image into a random mess of pixels, like static on an old TV set.&nbsp;



A diffusion model is a neural network trained to reverse that process, turning random static into images. During training, it gets shown millions of images in various stages of pixelation. It learns how those images change each time new pixels are thrown at them and, thus, how to undo those changes.&nbsp;



The upshot is that when you ask a diffusion model to generate an image, it will start off with a random mess of pixels and step by step turn that mess into an image that is more or less similar to images in its training set.&nbsp;









But you don‚Äôt want any image‚Äîyou want the image you specified, typically with a text prompt. And so the diffusion model is paired with a second model‚Äîsuch as a large language model (LLM) trained to match images with text descriptions‚Äîthat guides each step of the cleanup process, pushing the diffusion model toward images that the large language model considers a good match to the prompt.&nbsp;



An aside: This LLM isn‚Äôt pulling the links between text and images out of thin air. Most text-to-image and text-to-video models today are trained on large data sets that contain billions of pairings of text and images or text and video scraped from the internet (a practice many creators are very unhappy about). This means that what you get from such models is a distillation of the world as it‚Äôs represented online, distorted by prejudice (and pornography).



It&#8217;s easiest to imagine diffusion models working with images. But the technique can be used with many kinds of data, including audio and video. To generate movie clips, a diffusion model must clean up sequences of images‚Äîthe consecutive frames of a video‚Äîinstead of just one image.&nbsp;



What‚Äôs a latent diffusion model?&nbsp;



All this takes a huge amount of compute (read: energy). That‚Äôs why most diffusion models used for video generation use a technique called latent diffusion. Instead of processing raw data‚Äîthe millions of pixels in each video frame‚Äîthe model works in what‚Äôs known as a latent space, in which the video frames (and text prompt) are compressed into a mathematical code that captures just the essential features of the data and throws out the rest.&nbsp;



A similar thing happens whenever you stream a video over the internet: A video is sent from a server to your screen in a compressed format to make it get to you faster, and when it arrives, your computer or TV will convert it back into a watchable video.&nbsp;





And so the final step is to decompress what the latent diffusion process has come up with. Once the compressed frames of random static have been turned into the compressed frames of a video that the LLM guide considers a good match for the user‚Äôs prompt, the compressed video gets converted into something you can watch.&nbsp;&nbsp;



With latent diffusion, the diffusion process works more or less the way it would for an image. The difference is that the pixelated video frames are now mathematical encodings of those frames rather than the frames themselves. This makes latent diffusion far more efficient than a typical diffusion model. (Even so, video generation still uses more energy than image or text generation. There‚Äôs just an eye-popping amount of computation involved.)&nbsp;



What‚Äôs a latent diffusion transformer?



Still with me? There‚Äôs one more piece to the puzzle‚Äîand that‚Äôs how to make sure the diffusion process produces a sequence of frames that are consistent, maintaining objects and lighting and so on from one frame to the next. OpenAI did this with Sora by combining its diffusion model with another kind of model called a transformer. This has now become standard in generative video.¬†



Transformers are great at processing long sequences of data, like words. That has made them the special sauce inside large language models such as OpenAI‚Äôs GPT-5 and Google DeepMind‚Äôs Gemini, which can generate long sequences of words that make sense, maintaining consistency across many dozens of sentences.&nbsp;



But videos are not made of words. Instead, videos get cut into chunks that can be treated as if they were. The approach that OpenAI came up with was to dice videos up across both space and time. ‚ÄúIt‚Äôs like if you were to have a stack of all the video frames and you cut little cubes from it,‚Äù says Tim Brooks, a lead researcher on Sora.





A selection of videos generated with Veo 3 and Midjourney. The videos have been enhanced in postproduction with Topaz, an AI video-editing tool. Credit: VaigueMan 



Using transformers alongside diffusion models brings several advantages. Because they are designed to process sequences of data, transformers also help the diffusion model maintain consistency across frames as it generates them. This makes it possible to produce videos in which objects don‚Äôt pop in and out of existence, for example.&nbsp;



And because the videos are diced up, their size and orientation do not matter. This means that the latest wave of video generation models can be trained on a wide range of example videos, from short vertical clips shot with a phone to wide-screen cinematic films. The greater variety of training data has made video generation far better than it was just two years ago. It also means that video generation models can now be asked to produce videos in a variety of formats.&nbsp;



What about the audio?&nbsp;



A big advance with Veo 3 is that it generates video with audio, from lip-synched dialogue to sound effects to background noise. That‚Äôs a first for video generation models. As Google DeepMind CEO Demis Hassabis put it at this year‚Äôs Google I/O: ‚ÄúWe‚Äôre emerging from the silent era of video generation.‚Äù&nbsp;









The challenge was to find a way to line up video and audio data so that the diffusion process would work on both at the same time. Google DeepMind‚Äôs breakthrough was a new way to compress audio and video into a single piece of data inside the diffusion model. When Veo 3 generates a video, its diffusion model produces audio and video together in a lockstep process, ensuring that the sound and images are synched exactly.&nbsp;&nbsp;



You said that diffusion models can generate different kinds of data. Is this how LLMs work too?&nbsp;



No‚Äîor at least not yet. Diffusion models are most often used to generate images, video, and audio. Large language models‚Äîwhich generate text (including computer code)‚Äîare built using transformers. But the lines are blurring. We‚Äôve seen how transformers are now being combined with diffusion models to generate videos. And this summer Google DeepMind revealed that it was building an experimental large language model that used a diffusion model instead of a transformer to generate text.&nbsp;



Here‚Äôs where things start to get confusing: Though video generation (which uses diffusion models) consumes a lot of energy, diffusion models themselves are in fact more efficient than transformers. Thus, by using a diffusion model instead of a transformer to generate text, Google DeepMind‚Äôs new LLM could be a lot more efficient than existing LLMs. Expect to see more from diffusion models in the near future!
‚Ä¢ We can‚Äôt ‚Äúmake American children healthy again‚Äù without tackling the gun crisis
  Note for readers: This newsletter discusses gun violence, a raw and tragic issue in America. It was already in progress on Wednesday when a school shooting occurred at Evergreen High School in Colorado and Charlie Kirk was shot and killed at Utah Valley University.&nbsp;



Earlier this week, the Trump administration‚Äôs Make America Healthy Again movement released a strategy for improving the health and well-being of American children. The report was titled‚Äîyou guessed it‚ÄîMake Our Children Healthy Again.



Robert F. Kennedy Jr., who leads the Department of Health and Human Services, and his colleagues are focusing on four key aspects of child health: diet, exercise, chemical exposure, and overmedicalization.



Anyone who‚Äôs been listening to RFK Jr. posturing on health and wellness won‚Äôt be surprised by these priorities. And the first two are pretty obvious. On the whole, American children should be eating more healthily. And they should be getting more exercise.



But there‚Äôs a glaring omission. The leading cause of death for American children and teenagers isn‚Äôt ultraprocessed food or exposure to some chemical. It‚Äôs gun violence.&nbsp;



Yesterday‚Äôs news of yet more high-profile shootings at schools in the US throws this disconnect into even sharper relief. Experts believe it is time to treat gun violence in the US as what it is: a public health crisis.





I live in London, UK, with my husband and two young children. We don‚Äôt live in a particularly fancy part of the city‚Äîin one recent ranking of London boroughs from most to least posh, ours came in at 30th out of 33. I do worry about crime. But I don‚Äôt worry about gun violence.



That changed when I temporarily moved my family to the US a couple of years ago. We rented the ground-floor apartment of a lovely home in Cambridge, Massachusetts‚Äîa beautiful area with good schools, pastel-colored houses, and fluffy rabbits hopping about. It wasn‚Äôt until after we‚Äôd moved in that my landlord told me he had guns in the basement.



My daughter joined the kindergarten of a local school that specialized in music, and we took her younger sister along to watch the kids sing songs about friendship. It was all so heartwarming‚Äîuntil we noticed the school security officer at the entrance carrying a gun.



Later in the year, I received an email alert from the superintendent of the Cambridge Public Schools. ‚ÄúAt approximately 1:45 this afternoon, a Cambridge Police Department Youth Officer assigned to Cambridge Rindge and Latin School accidentally discharged their firearm while using a staff bathroom inside the school,‚Äù the message began. ‚ÄúThe school day was not disrupted.‚Äù



These experiences, among others, truly brought home to me the cultural differences over firearms between the US and the UK (along with most other countries). For the first time, I worried about my children‚Äôs exposure to them. I banned my children from accessing parts of the house. I felt guilty that my four-year-old had to learn what to do if a gunman entered her school.&nbsp;



But it‚Äôs the statistics that are the most upsetting.



In 2023, 46,728 people died from gun violence in the US, according to a report published in June by the Johns Hopkins Bloomberg School of Public Health. That includes both homicides and suicides, and it breaks down to 128 deaths per day, on average. The majority of those who die from gun violence are adults. But the figures for children are sickening, too. In 2023, 2,566 young people died from gun violence. Of those, 234 were under the age of 10.



Gun death rates among children have more than doubled since 2013. Firearms are involved in more child deaths than cancer or car crashes.



Many other children survive gun violence with nonfatal‚Äîbut often life-changing‚Äîinjuries. And the impacts are felt beyond those who are physically injured. Witnessing gun violence or hearing gunshots can understandably cause fear, sadness, and distress.&nbsp;&nbsp;





That‚Äôs worth bearing in mind when you consider that there have been 434 school shootings in the US since Columbine in 1999. The Washington Post estimates that 397,000 students have experienced gun violence at school in that period. Another school shooting took place at Evergreen High School in Colorado on Wednesday, adding to that total.



‚ÄúBeing indirectly exposed to gun violence takes its toll on our mental health and children‚Äôs ability to learn,‚Äù says Daniel Webster, Bloomberg Professor of American Health at the Johns Hopkins Center for Gun Violence Solutions in Baltimore.



The MAHA report states that ‚ÄúAmerican youth face a mental health crisis,‚Äù going on to note that ‚Äúsuicide deaths among 10- to 24-year-olds increased by 62% from 2007 to 2021‚Äù and that ‚Äúsuicide is now the leading cause of death in teens aged 15-19.‚Äù What it doesn‚Äôt say is that around half of these suicides involve guns.



‚ÄúWhen you add all these dimensions, [gun violence is] a very huge public health problem,‚Äù says Webster.



Researchers who study gun violence have been saying the same thing for years. And in 2024, then US Surgeon General Vivek Murthy declared it a public health crisis. ‚ÄúWe don‚Äôt have to subject our children to the ongoing horror of firearm violence in America,‚Äù Murthy said in a statement at the time. Instead, he argued, we should tackle the problem using a public health approach.



Part of that approach involves identifying who is at the greatest risk and offering support to lower that risk, says Webster. Young men who live in poor communities tend to have the highest risk of gun violence, he says, as do those who experience crisis or turmoil. Trying to mediate conflicts or limit access to firearms, even temporarily, can help lower the incidence of gun violence, he says.



There‚Äôs an element of social contagion, too, adds Webster. Shooting begets more shooting. He likens it to the outbreak of an infectious disease. ‚ÄúWhen more people get vaccinated ‚Ä¶ infection rates go down,‚Äù he says. ‚ÄúAlmost exactly the same thing happens with gun violence.‚Äù



But existing efforts are already under threat. The Trump administration has eliminated hundreds of millions of dollars in grants for organizations working to reduce gun violence.



Webster thinks the MAHA report has ‚Äúmissed the mark‚Äù when it comes to the health and well-being of children in the US. ‚ÄúThis document is almost the polar opposite to how many people in public health think,‚Äù he says. ‚ÄúWe have to acknowledge that injuries and deaths from firearms are a big threat to the health and safety of children and adolescents.‚Äù



This article first appeared in The Checkup,¬†MIT Technology Review‚Äôs¬†weekly biotech newsletter. To receive it in your inbox every Thursday, and read articles like this first,¬†sign up here.
‚Ä¢ Partnering with generative AI in the finance function
  Generative AI has the potential to transform the finance function . By taking on some of the more mundane tasks that can occupy a lot of time, generative AI tools can help free up capacity for more high-value strategic work . Deloitte‚Äôs fourth-quarter 2024 North American CFO Signals survey found that 46% of CFOs who responded expect deployment of AI in finance to increase in the next 12 months .
‚Ä¢ The Download: Trump‚Äôs impact on science, and meet our climate and energy honorees
  This is today&#8217;s edition of¬†The Download,¬†our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



How Trump‚Äôs policies are affecting early-career scientists‚Äîin their own words



Every year MIT Technology Review celebrates accomplished young scientists, entrepreneurs, and inventors from around the world in our Innovators Under 35 list. We‚Äôve just published the 2025 edition. This year, though, the context is different: The US scientific community is under attack.Since Donald Trump took office in January, his administration has fired top government scientists, targeted universities and academia, and made substantial funding cuts to the country‚Äôs science and technology infrastructure.



We asked our six most recent cohorts about both positive and negative impacts of the administration‚Äôs new policies. Their responses provide a glimpse into the complexities of building labs, companies, and careers in today‚Äôs political climate. Read the full story.‚ÄîEileen Guo &amp; Amy Nordrum



This story is part of MIT Technology Review‚Äôs &#8220;America Undone‚Äù series, examining how the foundations of US success in science and innovation are currently under threat. You can read the rest here.







This Ethiopian entrepreneur is reinventing ammonia production



In the small town in Ethiopia where he grew up, Iwnetim Abate‚Äôs family had electricity, but it was unreliable. So, for several days each week when they were without power, Abate would finish his homework by candlelight.



Growing up without the access to electricity that many people take for granted shaped the way Abate thinks about energy issues. Today, the 32-year old is an assistant professor at MIT in the department of materials science and engineering.&nbsp;



Part of his research focuses on sodium-ion batteries, which could be cheaper than the lithium-based ones that typically power electric vehicles and grid installations. He‚Äôs also pursuing a new research path, examining how to harness the heat and pressure under the Earth‚Äôs surface to make ammonia, a chemical used in fertilizer and as a green fuel. Read the full story.



‚ÄîCasey Crownhart



Abate is one of the climate and energy honorees on our 35 Innovators Under 35 list for 2025. Meet the rest of our climate and energy innovators here, and the full list‚Äîincluding our innovator of the year‚Äîhere.&nbsp;







Texas banned lab-grown meat. What‚Äôs next for the industry?



Last week, a legal battle over lab-grown meat kicked off in Texas. On September 1, a two-year ban on the technology went into effect across the state; the following day, two companies filed a lawsuit against state officials.



The two companies, Wildtype Foods and Upside Foods, are part of a growing industry that aims to bring new types of food to people‚Äôs plates. These products, often called cultivated meat by the industry, take live animal cells and grow them in the lab to make food products without the need to slaughter animals.Texas joins six other US states and the country of Italy in banning these products‚Äîadding barriers to an industry that‚Äôs still in its infancy, and already faces plenty of challenges before it can reach consumers in a meaningful way. Read the full story.



‚ÄîCasey Crownhart



This article is from The Spark, MIT Technology Review‚Äôs weekly climate newsletter. To receive it in your inbox every Wednesday, sign up here.







The must-reads



I‚Äôve combed the internet to find you today‚Äôs most fun/important/scary/fascinating stories about technology.



1 Videos of Charlie Kirk‚Äôs shooting are everywhere on social mediaIt demonstrates just how poorly equipped platforms are to stop the spread of violent material. (NYT $)+ Why social media can‚Äôt get on top of its graphic video problem. (NY Mag $)+ Here‚Äôs how platforms say they‚Äôll treat the videos. (The Verge)+ Far-right communities reacted to Kirk‚Äôs murder by calling for more violence. (Wired $)



2 NASA has uncovered the clearest sign of life on Mars to dateSome unusual rocks may have been formed by ancient microbes. (WP $)+ Scientists are very excited by the possibility they were created by living organisms. (New Scientist $)



3 A California bill to regulate AI companion chatbots is close to passingIt would become the first US state to make chatbot operators legally accountable. (TechCrunch)+ Wall Street is only now starting to worry about ‚ÄúAI psychosis.‚Äù (Insider $)+ AI companions are the final stage of digital addiction, and lawmakers are taking aim. (MIT Technology Review)



4 Larry Ellison briefly overtook Elon Musk as the world‚Äôs richest personHis firm Oracle reported far better-than expected results. (The Guardian)+ Oracle is riding high on a surge of demand for its data centers. (BBC)+ But its continued success will depend on its ability to deliver promised hardware. (FT $)



5 The ousted CDC director is set to testify before the US SenateRFK Jr repeatedly called Susan Monarez a liar during a hearing last week. (Ars Technica)+ The backlash to Kennedy‚Äôs actions is intensifying. (NY Mag $)



6 A new system can pinpoint the best spot to hit an asteroidMaking destroying them a whole lot safer, in theory. (New Scientist $)+ Meet the researchers testing the ‚ÄúArmageddon‚Äù approach to asteroid defense. (MIT Technology Review)



7 Saudi Arabia is building some of the world‚Äôs biggest solar farms It needs plenty more electricity for its new resorts and data centers. (WSJ $)+ AI is changing the grid. Could it help more than it harms? (MIT Technology Review)



8 CRISPR could help to combat diabetesScientists successfully implanted insulin-producing edited cells into a man‚Äôs pancreas. (Wired $)+ A US court just put ownership of CRISPR back in play. (MIT Technology Review)



9 How to save oyster reefs Conservation projects are helping to rebuild destroyed populations. (Knowable Magazine)+ How the humble sea creature could hold the key to restoring coastal waters. (MIT Technology Review)



10 Bluesky is not as fun as it should beIt fosters a culture of reactionary scolding that‚Äôs driving some users back to X. (New Yorker $)







Quote of the day



‚ÄúFor the love of God and Charlie‚Äôs family, just stop.‚Äù



‚ÄîA poster on X begs fellow social media users to stop sharing images and videos of conservative activist Charlie Kirk‚Äôs murder online, the Associated Press reports.







One more thing







This giant microwave may change the future of warImagine: China deploys hundreds of thousands of autonomous drones in the air, on the sea, and under the water‚Äîall armed with explosive warheads or small missiles. These machines descend in a swarm toward military installations on Taiwan and nearby US bases, and over the course of a few hours, a single robotic blitzkrieg overwhelms the US Pacific force before it can even begin to fight back.The proliferation of cheap drones means just about any group with the wherewithal to assemble and launch a swarm could wreak havoc, no expensive jets or massive missile installations required.The US armed forces are now hunting for a solution‚Äîand they want it fast. One of these is microwaves: high-powered electronic devices that push out kilowatts of power to zap the circuits of a drone as if it were the tinfoil you forgot to take off your leftovers when you heated them up. Read the full story.



‚ÄîSam Dean







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)



+ They‚Äôve finally done it‚Äîthe Stephen King novel they claimed was impossible to adapt is coming to the big screen.+ Do you have more zucchinis than you know what to do with? This tasty bread is one solution.+ How The Penguin‚Äôs production designers transformed NYC into spooky, dirty Gotham.+ This fascinating website shows you what today‚Äôs date looks like on dozens of different calendars and clocks.
‚Ä¢ Texas banned lab-grown meat. What‚Äôs next for the industry?
  Last week, a legal battle over lab-grown meat kicked off in Texas. On September 1, a two-year ban on the technology went into effect across the state; the following day, two companies filed a lawsuit against state officials.



The two companies, Wildtype Foods and Upside Foods, are part of a growing industry that aims to bring new types of food to people‚Äôs plates. These products, often called cultivated meat by the industry, take live animal cells and grow them in the lab to make food products without the need to slaughter animals.





Texas joins six other US states and the country of Italy in banning these products. These legal challenges are adding barriers to an industry that‚Äôs still in its infancy and already faces plenty of challenges before it can reach consumers in a meaningful way.



The agriculture sector makes up a hefty chunk of global greenhouse-gas emissions, with livestock alone accounting for somewhere between 10% and 20% of climate pollution. Alternative meat products, including those grown in a lab, could help cut the greenhouse gases from agriculture.



The industry is still in its early days, though. In the US, just a handful of companies can legally sell products including cultivated chicken, pork fat, and salmon. Australia, Singapore, and Israel also allow a few companies to sell within their borders.



Upside Foods, which makes cultivated chicken, was one of the first to receive the legal go-ahead to sell its products in the US, in 2022. Wildtype Foods, one of the latest additions to the US market, was able to start selling its cultivated salmon in June.



Upside, Wildtype, and other cultivated-meat companies are still working to scale up production. Products are generally available at pop-up events or on special menus at high-end restaurants. (I visited San Francisco to try Upside‚Äôs cultivated chicken at a Michelin-starred restaurant a few years ago.)



Until recently, the only place you could reliably find lab-grown meat in Texas was a sushi restaurant in Austin. Otoko featured Wildtype‚Äôs cultivated salmon on a special tasting menu starting in July. (The chef told local publication Culture Map Austin that the cultivated fish tastes like wild salmon, and it was included in a dish with grilled yellowtail to showcase it side-by-side with another type of fish.)



The as-yet-limited reach of lab-grown meat didn‚Äôt stop state officials from moving to ban the technology, effective from now until September 2027.



The office of state senator Charles Perry, the author of the bill, didn‚Äôt respond to requests for comment. Neither did the Texas and Southwestern Cattle Raisers Association, whose president, Carl Ray Polk Jr., testified in support of the bill in a March committee hearing.



‚ÄúThe introduction of lab-grown meat could disrupt traditional livestock markets, affecting rural communities and family farms,‚Äù Perry said during the meeting.





In an interview with the Texas Tribune, Polk said the two-year moratorium would help the industry put checks and balances in place before the products could be sold. He also expressed concern about how clearly cultivated-meat companies will be labeling their products.



‚ÄúThe purpose of these bans is to try to kill the cultivated-meat industry before it gets off the ground,‚Äù said Myra Pasek, general counsel of Upside Foods, via email. The company is working to scale up its manufacturing and get the product on the market, she says, ‚Äúbut that can&#8217;t happen if we‚Äôre not allowed to compete in the marketplace.‚Äù



Others in the industry have similar worries. ‚ÄúMoratoriums on sale like this not only deny Texans new choices and economic growth, but they also send chilling signals to researchers and entrepreneurs across the country,‚Äù said Pepin Andrew Tuma, the vice president of policy and government relations for the Good Food Institute, a nonprofit think tank focused on alternative proteins, in a statement. (The group isn‚Äôt involved in the lawsuit.)&nbsp;



One day after the moratorium took effect on September 1, Wildtype Foods and Upside Foods filed a lawsuit challenging the ban, naming Jennifer Shuford, commissioner of the Texas Department of State Health Services, among other state officials.



A lawsuit wasn‚Äôt necessarily part of the scale-up plan. ‚ÄúThis was really a last resort for us,‚Äù says Justin Kolbeck, cofounder and CEO of Wildtype.



Growing cells to make meat in the lab isn‚Äôt easy‚Äîsome companies have spent a decade or more trying to make significant amounts of a product that people want to eat. These legal battles certainly aren‚Äôt going to help.&nbsp;



This article is from The Spark, MIT Technology Review‚Äôs weekly climate newsletter. To receive it in your inbox every Wednesday, sign up here.

üîí Cybersecurity & Privacy
‚Ä¢ Bulletproof Host Stark Industries Evades EU Sanctions
  In May 2025, the European Union levied financial sanctions on the owners of¬†Stark Industries Solutions Ltd., a bulletproof hosting provider that materialized two weeks before Russia invaded Ukraine and quickly became a top source of Kremlin-linked cyberattacks and disinformation campaigns. But new findings show those sanctions have done little to stop Stark from simply rebranding and transferring their assets to other corporate entities controlled by its original hosting providers.
Image: Shutterstock.
Materializing just two weeks before Russia invaded Ukraine in 2022, Stark Industries Solutions became a frequent source of massive DDoS attacks, Russian-language proxy and VPN services, malware tied to Russia-backed hacking groups, and fake news. ISPs like Stark are called &#8220;bulletproof&#8221; providers when they cultivate a reputation for ignoring any abuse complaints or police inquiries about activity on their networks.
In May 2025, the European Union sanctioned one of Stark&#8217;s two main conduits to the larger Internet &#8212; Moldova-based PQ Hosting &#8212; as well as the company&#8217;s Moldovan owners Yuri and Ivan Neculiti. The EU Commission said the Neculiti brothers and PQ Hosting were linked to Russia&#8217;s hybrid warfare efforts.
But a new report from Recorded Future finds that just prior to the sanctions being announced, Stark rebranded to¬†the[.]hosting, under control of the Dutch entity WorkTitans BV¬†(AS209847) on June 24, 2025. The Neculiti brothers reportedly got a heads up roughly 12 days before the sanctions were announced, when Moldovan and EU media reported on the forthcoming inclusion of the Neculiti brothers in the sanctions package.
In response, the Neculiti brothers moved much of Stark&#8217;s considerable address space and other resources over to a new company in Moldova called PQ Hosting Plus S.R.L., an entity reportedly connected to the Neculiti brothers thanks to the re-use of a phone number from the original PQ Hosting.
&#8220;Although the majority of associated infrastructure remains attributable to Stark Industries, these changes likely reflect an attempt to obfuscate ownership and sustain hosting services under new legal and network entities,&#8221; Recorded Future observed.
Neither the Recorded Future report nor the May 2025 sanctions from the EU mentioned a second critical pillar of Stark&#8217;s network that KrebsOnSecurity identified in a May 2024 profile on the notorious bulletproof hoster: The Netherlands-based hosting provider MIRhosting.
MIRhosting is operated by 38-year old Andrey Nesterenko, whose personal website says he is an accomplished concert pianist who began performing publicly at a young age. DomainTools says mirhosting[.]com is registered to Mr. Nesterenko and to Innovation IT Solutions Corp, which lists addresses in London and in Nesterenko‚Äôs stated hometown of Nizhny Novgorod, Russia.
Image credit: correctiv.org.
According to the book Inside Cyber Warfare by Jeffrey Carr, Innovation IT Solutions Corp. was responsible for hosting StopGeorgia[.]ru, a hacktivist website for organizing cyberattacks against Georgia that appeared at the same time Russian forces invaded the former Soviet nation in 2008. That conflict was thought to be the first war ever fought in which a notable cyberattack and an actual military engagement happened simultaneously.
Mr. Nesterenko did not respond to requests for comment. In May 2024, Mr. Nesterenko said he couldn&#8217;t verify whether StopGeorgia was ever a customer because they didn&#8217;t keep records going back that far. But he maintained that Stark Industries Solutions was merely one client of many, and claimed MIRhosting had not received any actionable complaints about abuse on Stark.
However, it appears that MIRhosting is once again the new home of Stark Industries, and that MIRhosting employees are managing both the[.]hosting and WorkTitans &#8212; the primary beneficiaries of Stark&#8217;s assets.
A copy of the incorporation documents for WorkTitans BV obtained from the Dutch Chamber of Commerce shows WorkTitans also does business under the names Misfits Media and and WT Hosting (considering Stark&#8217;s historical connection to Russian disinformation websites, &#8220;Misfits Media&#8221; is a bit on the nose).
An incorporation document for WorkTitans B.V. from the Netherlands Chamber of Commerce.
The incorporation document says the company was formed in 2019 by a y.zinad@worktitans.nl. That email address corresponds to a LinkedIn account for a Youssef Zinad, who says their personal websites are worktitans[.]nl and custom-solution[.]nl. The profile also links to a website (etripleasims dot nl) that LinkedIn currently blocks as malicious. All of these websites are or were hosted at MIRhosting.
Although Mr. Zinad&#8217;s LinkedIn profile does not mention any employment at MIRhosting, virtually all of his LinkedIn posts over the past year have been reposts of advertisements for MIRhosting&#8217;s services.
Mr. Zinad&#8217;s LinkedIn profile is full of posts for MIRhosting&#8217;s services.
A Google search for Youssef Zinad reveals multiple startup-tracking websites that list him as the founder of the[.]hosting, which censys.io finds is hosted by PQ Hosting Plus S.R.L.
The Dutch Chamber of Commerce document says WorkTitans&#8217; sole shareholder is a company in Almere, Netherlands called Fezzy B.V. Who runs Fezzy? The phone number listed in a Google search for Fezzy B.V. &#8212; 31651079755 &#8212; also was used to register a Facebook profile for a Youssef Zinad from the same town, according to the breach tracking service Constella Intelligence.
In a series of email exchanges leading up to KrebsOnSecurity&#8217;s May 2024 deep dive on Stark, Mr. Nesterenko included Mr. Zinad in the message thread (youssef@mirhosting.com), referring to him as part of the company&#8217;s legal team. The Dutch website stagemarkt[.]nl lists Youssef Zinad as an official contact for MIRhosting&#8217;s offices in Almere. Mr. Zinad did not respond to requests for comment.

Given the above, it is difficult to argue with the Recorded Future report on Stark&#8217;s rebranding, which concluded that &#8220;the EU‚Äôs sanctioning of Stark Industries was largely ineffective, as affiliated infrastructure remained operational and services were rapidly re-established under new branding, with no significant or lasting disruption.&#8221;
‚Ä¢ Microsoft Patch Tuesday, September 2025 Edition
  Microsoft Corp. today issued security updates to fix more than 80 vulnerabilities in its Windows operating systems and software. There are no known &#8220;zero-day&#8221; or actively exploited vulnerabilities in this month&#8217;s bundle from Redmond, which nevertheless includes patches for 13 flaws that earned Microsoft&#8217;s most-dire &#8220;critical&#8221; label. Meanwhile, both Apple and Google recently released updates to fix zero-day bugs in their devices.

Microsoft assigns security flaws a &#8220;critical&#8221; rating when malware or miscreants can exploit them to gain remote access to a Windows system with little or no help from users. Among the more concerning critical bugs quashed this month is CVE-2025-54918. The problem here resides with Windows NTLM, or NT LAN Manager, a suite of code for managing authentication in a Windows network environment.
Redmond rates this flaw as &#8220;Exploitation More Likely,&#8221; and although it is listed as a privilege escalation vulnerability, Kev Breen at Immersive says this one is actually exploitable over the network or the Internet.
&#8220;From Microsoft‚Äôs limited description, it appears that if an attacker is able to send specially crafted packets over the network to the target device, they would have the ability to gain SYSTEM-level privileges on the target machine,&#8221; Breen said. &#8220;The patch notes for this vulnerability state that &#8216;Improper authentication in Windows NTLM allows an authorized attacker to elevate privileges over a network,&#8217; suggesting an attacker may already need to have access to the NTLM hash or the user&#8217;s credentials.&#8221;
Breen said another patch &#8212; CVE-2025-55234, a 8.8 CVSS-scored flaw affecting the Windows SMB client for sharing files across a network &#8212; also is listed as privilege escalation bug but is likewise remotely exploitable. This vulnerability was publicly disclosed prior to this month.
&#8220;Microsoft says that an attacker with network access would be able to perform a replay attack against a target host, which could result in the attacker gaining additional privileges, which could lead to code execution,&#8221; Breen noted.
CVE-2025-54916 is an &#8220;important&#8221; vulnerability in Windows NTFS &#8212; the default filesystem for all modern versions of Windows &#8212; that can lead to remote code execution. Microsoft likewise thinks we are more than likely to see exploitation of this bug soon: The last time Microsoft patched an NTFS bug was in March 2025 and it was already being exploited in the wild as a zero-day.
&#8220;While the title of the CVE says &#8216;Remote Code Execution,&#8217; this exploit is not remotely exploitable over the network, but instead needs an attacker to either have the ability to run code on the host or to convince a user to run a file that would trigger the exploit,&#8221; Breen said. &#8220;This is commonly seen in social engineering attacks, where they send the user a file to open as an attachment or a link to a file to download and run.&#8221;
Critical and remote code execution bugs tend to steal all the limelight, but Tenable Senior Staff Research Engineer Satnam Narang notes that nearly half of all vulnerabilities fixed by Microsoft this month are privilege escalation flaws that require an attacker to have gained access to a target system first before attempting to elevate privileges.
&#8220;For the third time this year, Microsoft patched more elevation of privilege vulnerabilities than remote code execution flaws,&#8221; Narang observed.
On Sept. 3, Google fixed two flaws that were detected as exploited in zero-day attacks, including¬†CVE-2025-38352, an elevation of privilege in the Android kernel, and CVE-2025-48543, also an elevation of privilege problem in the Android Runtime component.
Also, Apple recently patched its seventh zero-day (CVE-2025-43300) of this year. It was part of an exploit chain used along with a vulnerability in the WhatsApp (CVE-2025-55177) instant messenger to hack Apple devices. Amnesty International reports that the two zero-days have been used in &#8220;an advanced spyware campaign&#8221; over the past 90 days. The issue is fixed in iOS 18.6.2, iPadOS 18.6.2, iPadOS 17.7.10, macOS Sequoia 15.6.1, macOS Sonoma 14.7.8, and macOS Ventura 13.7.8.
The SANS Internet Storm Center has a clickable breakdown of each individual fix from Microsoft, indexed by severity and CVSS score. Enterprise Windows admins involved in testing patches before rolling them out should keep an eye on askwoody.com, which often has the skinny on wonky updates.
AskWoody also reminds us that we&#8217;re now just two months out from Microsoft discontinuing free security updates for Windows 10 computers. For those interested in safely extending the lifespan and usefulness of these older machines, check out last month&#8217;s Patch Tuesday coverage for a few pointers.
As ever, please don&#8217;t neglect to back up your data (if not your entire system) at regular intervals, and feel free to sound off in the comments if you experience problems installing any of these fixes.
‚Ä¢ 18 Popular Code Packages Hacked, Rigged to Steal Crypto
  At least 18 popular JavaScript code packages that are collectively downloaded more than two billion times each week were briefly compromised with malicious software today, after a developer involved in maintaining the projects was phished. The attack appears to have been quickly contained and was narrowly focused on stealing cryptocurrency. But experts warn that a similar attack with a slightly more nefarious payload could lead to a disruptive malware outbreak that is far more difficult to detect and restrain.
This phishing email lured a developer into logging in at a fake NPM website and supplying a one-time token for two-factor authentication. The phishers then used that developer&#8217;s NPM account to add malicious code to at least 18 popular JavaScript code packages.
Aikido is a security firm in Belgium that monitors new code updates to major open-source code repositories, scanning any code updates for suspicious and malicious code. In a blog post published today, Aikido said its systems found malicious code had been added to at least 18 widely-used code libraries available on NPM (short for) &#8220;Node Package Manager,&#8221; which acts as a central hub for JavaScript development and the latest updates to widely-used JavaScript components.
JavaScript is a powerful web-based scripting language used by countless websites to build a more interactive experience with users, such as entering data into a form. But there&#8217;s no need for each website developer to build a program from scratch for entering data into a form when they can just reuse already existing packages of code at NPM that are specifically designed for that purpose.
Unfortunately, if cybercriminals manage to phish NPM credentials from developers, they can introduce malicious code that allows attackers to fundamentally control what people see in their web browser when they visit a website that uses one of the affected code libraries.
According to Aikido, the attackers injected a piece of code that silently intercepts cryptocurrency activity in the browser, &#8220;manipulates wallet interactions, and rewrites payment destinations so that funds and approvals are redirected to attacker-controlled accounts without any obvious signs to the user.&#8221;
&#8220;This malware is essentially a browser-based interceptor that hijacks both network traffic and application APIs,&#8221; Aikido researcher Charlie Eriksen wrote. &#8220;What makes it dangerous is that it operates at multiple layers: Altering content shown on websites, tampering with API calls, and manipulating what users‚Äô apps believe they are signing. Even if the interface looks correct, the underlying transaction can be redirected in the background.&#8221;
Aikido said it used the social network Bsky to notify the affected developer, Josh Junon, who quickly replied that he was aware of having just been phished. The phishing email that Junon fell for was part of a larger campaign that spoofed NPM and told recipients they were required to update their two-factor authentication (2FA) credentials. The phishing site mimicked NPM&#8217;s login page, and intercepted Junon&#8217;s credentials and 2FA token. Once logged in, the phishers then changed the email address on file for Junon&#8217;s NPM account, temporarily locking him out.
Aikido notified the maintainer on Bluesky, who replied at 15:15 UTC that he was aware of being compromised, and starting to clean up the compromised packages.
Junon also issued a mea culpa on HackerNews, telling the community&#8217;s coder-heavy readership, &#8220;Hi, yep I got pwned.&#8221;
&#8220;It looks and feels a bit like a targeted attack,&#8221; Junon wrote. &#8220;Sorry everyone, very embarrassing.&#8221;
Philippe Caturegli, &#8220;chief hacking officer&#8221; at the security consultancy Seralys, observed that the attackers appear to have registered their spoofed website &#8212; npmjs[.]help &#8212; just two days before sending the phishing email. The spoofed website used services from dnsexit[.]com, a &#8220;dynamic DNS&#8221; company that also offers &#8220;100% free&#8221; domain names that can instantly be pointed at any IP address controlled by the user.
Junon&#8217;s mea cupla on Hackernews today listed the affected packages.
Caturegli said it&#8217;s remarkable that the attackers in this case were not more ambitious or malicious with their code modifications.
&#8220;The crazy part is they compromised billions of websites and apps just to target a couple of cryptocurrency things,&#8221; he said. &#8220;This was a supply chain attack, and it could easily have been something much worse than crypto harvesting.&#8221;
Aikido&#8217;s Eriksen agreed, saying countless websites dodged a bullet because this incident was handled in a matter of hours. As an example of how these supply-chain attacks can escalate quickly, Eriksen pointed to another compromise of an NPM developer in late August that added malware to &#8220;nx,&#8221; an open-source code development toolkit with as many as six million weekly downloads.
In the nx compromise, the attackers introduced code that scoured the user&#8217;s device for authentication tokens from programmer destinations like GitHub and NPM, as well as SSH and API keys. But instead of sending those stolen credentials to a central server controlled by the attackers, the malicious code created a new public repository in the victim&#8217;s GitHub account, and published the stolen data there for all the world to see and download.
Eriksen said coding platforms like GitHub and NPM should be doing more to ensure that any new code commits for broadly-used packages require a higher level of attestation that confirms the code in question was in fact submitted by the person who owns the account, and not just by that person&#8217;s account.
&#8220;More popular packages should require attestation that it came through trusted provenance and not just randomly from some location on the Internet,&#8221; Eriksen said. &#8220;Where does the package get uploaded from, by GitHub in response to a new pull request into the main branch, or somewhere else? In this case, they didn&#8217;t compromise the target&#8217;s GitHub account. They didn&#8217;t touch that. They just uploaded a modified version that didn&#8217;t come where it&#8217;s expected to come from.&#8221;
Eriksen said code repository compromises can be devastating for developers, many of whom end up abandoning their projects entirely after such an incident.
&#8220;It&#8217;s unfortunate because one thing we&#8217;ve seen is people have their projects get compromised and they say, &#8216;You know what, I don&#8217;t have the energy for this and I&#8217;m just going to deprecate the whole package,'&#8221; Eriksen said.
Kevin Beaumont, a frequently quoted security expert who writes about security incidents at the blog doublepulsar.com, has been following this story closely today in frequent updates to his account on Mastodon. Beaumont said the incident is a reminder that much of the planet still depends on code that is ultimately maintained by an exceedingly small number of people who are mostly overburdened and under-resourced.
&#8220;For about the past 15 years every business has been developing apps by pulling in 178 interconnected libraries written by 24 people in a shed in Skegness,&#8221; Beaumont wrote on Mastodon. &#8220;For about the past 2 years orgs have been buying AI vibe coding tools, where some exec screams &#8216;make online shop&#8217; into a computer and 389 libraries are added and an app is farted out. The output = if you want to own the world&#8217;s companies, just phish one guy in Skegness.&#8221;
Image: https://infosec.exchange/@GossiTheDog@cyberplace.social.
Aikido recently launched a product that aims to help development teams ensure that every code library used is checked for malware before it can be used or installed. Nicholas Weaver, a researcher with the International Computer Science Institute, a nonprofit in Berkeley, Calif., said Aikido&#8217;s new offering exists because many organizations are still one successful phishing attack away from a supply-chain nightmare.
Weaver said these types of supply-chain compromises will continue as long as people responsible for maintaining widely-used code continue to rely on phishable forms of 2FA.
&#8220;NPM should only support phish-proof authentication,&#8221; Weaver said, referring to physical security keys that are phish-proof &#8212; meaning that even if phishers manage to steal your username and password, they still can&#8217;t log in to your account without also possessing that physical key.
&#8220;All critical infrastructure needs to use phish-proof 2FA, and given the dependencies in modern software, archives such as NPM are absolutely critical infrastructure,&#8221; Weaver said. &#8220;That NPM does not require that all contributor accounts use security keys or similar 2FA methods should be considered negligence.&#8221;

üéì University AI
No updates.

üè¢ Corporate AI
‚Ä¢ Tool-space interference in the MCP era: Designing for agent compatibility at scale
  This year&nbsp;we‚Äôve&nbsp;seen&nbsp;remarkable&nbsp;advances in agentic AI, including&nbsp;systems that conduct deep research,&nbsp;operate&nbsp;computers, complete substantial software engineering tasks, and tackle a range of other complex,&nbsp;multi-step goals. In each case,&nbsp;the industry relied&nbsp;on careful vertical integration: tools and agents were co-designed, co-trained, and tested together&nbsp;for peak&nbsp;performance. For example,&nbsp;OpenAI&#8217;s&nbsp;recent models&nbsp;presume&nbsp;the&nbsp;availability&nbsp;of web search and document retrieval&nbsp;tools (opens in new tab). Likewise,&nbsp;the prompts and actions&nbsp;of&nbsp;Magentic-One&nbsp;are&nbsp;set up to make hand-offs easy‚Äîfor example, allowing the WebSurfer agent to pass downloaded files to the Coder agent.‚ÄØ&nbsp;But‚ÄØas agents proliferate, we anticipate strategies relying heavily on vertical integration will not age well.&nbsp;Agents&nbsp;from&nbsp;different&nbsp;developers&nbsp;or companies will&nbsp;increasingly&nbsp;encounter&nbsp;each other and&nbsp;must&nbsp;work together to complete tasks, in what we refer to as a&nbsp;society of agents.&nbsp;These systems can vary in how coordinated they are, how aligned their goals are, and how much information they share. Can heterogenous agents and tools cooperate&nbsp;in this&nbsp;setting, or will they hinder one another and slow progress?



Early clues have&nbsp;emerged&nbsp;from an&nbsp;unexpected&nbsp;source:&nbsp;namely,&nbsp;Model Context Protocol (opens in new tab)&nbsp;(MCP). Since January 2025, MCP has‚ÄØgrown from a&nbsp;promising spec to a&nbsp;thriving&nbsp;market&nbsp;of&nbsp;tool&nbsp;servers.&nbsp;As an example,‚ÄØZapier boasts a catalog of 30,000 tools (opens in new tab)&nbsp;across 7,000 services.&nbsp;Composio&nbsp;provide over 100 managed MCP servers (opens in new tab), surfacing hundreds of tools. Hugging&nbsp;Face is now serving&nbsp;many&nbsp;Spaces&nbsp;apps over MCP (opens in new tab), and‚ÄØShopify has enabled MCP for millions of storefronts (opens in new tab).&nbsp;A&nbsp;society of&nbsp;tools&nbsp;is already here, and it promises to&nbsp;extend&nbsp;agent capabilities through&nbsp;cross-provider&nbsp;horizontal integration.&nbsp;



So,&nbsp;what does MCP have to say about&nbsp;horizontal integration? As catalogs grow,&nbsp;we expect some new failure modes to surface.&nbsp;This&nbsp;blog&nbsp;post introduces&nbsp;these&nbsp;as‚ÄØtool-space interference, and sketches both early observations‚ÄØand some pragmatic interventions to keep the society&nbsp;we‚Äôre&nbsp;building&nbsp;from stepping on its own feet.&nbsp;



Tool-space interference describes situations where otherwise reasonable tools or agents, when co-present, reduce end-to-end effectiveness. This can look like longer action sequences, higher token cost, brittle recovery from errors, or, in some cases, task failure.



A framing example



Consider MCP as a means for extending Magentic-One, a generalist multi-agent system we released last year, to cover more software engineering tasks. Magentic-One ships with agents to write code, interact with the computer terminal, browse the web, and access local files. To help Magentic-One navigate version control, find issues to solve, and make pull requests, we could add an agent equipped with the GitHub MCP Server. However, now each time the team encounters a task involving GitHub, it must choose whether to visit github.com in the browser, execute a git command at the command line, or engage the GitHub MCP server. As the task progresses, agent understanding of state can also diverge: changing the branch in the browser won‚Äôt change the branch in the terminal, and an authorized MCP tool does not imply authorization in the browser.&nbsp;Thus, while any single agent might complete the task efficiently, the larger set of agents might misunderstand or interfere with one another, leading to additional rounds of debugging, or even complete task failure.



Figure 1: We can extend&nbsp;Magentic-One by adding an agent that equips the GitHub MCP server. However, on every turn involving a git-related task, the orchestrator will need to decide between messaging the Computer Terminal agent (with access to the git command line interface), WebSurfer agent (with access to github.com), and the agent with the GitHub MCP server. This overlap raises the possibility that they will interfere with one another.&nbsp;&nbsp;



Tool-space interference, through the lens of MCP



To better understand the potential interference patterns and the current state of the MCP ecosystem, we conducted a survey of MCP servers listed on two registries: smithery.ai (opens in new tab) and Docker MCP Hub (opens in new tab). Smithery is an MCP Server registry with over 7,000 first-party and community-contributed servers, which we sampled from the Smithery API. Likewise, Docker MCP Hub is a registry that distributes MCP servers as Docker images, and we manually collected popular entries. We then launched each server for inspection. After excluding servers that were empty or failed to launch, and deduplicating servers with identical features, 1,470 servers remained in our catalog.



To&nbsp;automate the&nbsp;inspection&nbsp;of&nbsp;running MCP servers,&nbsp;we developed an&nbsp;MCP&nbsp;Interviewer&nbsp;tool.&nbsp;The MCP&nbsp;Interviewer&nbsp;begins by cataloging the server‚Äôs tools, prompts, resources, resource templates, and capabilities.&nbsp;From&nbsp;this catalog we can compute&nbsp;descriptive statistics&nbsp;such as the number of tools, or the depth of the parameter&nbsp;schemas.&nbsp;&nbsp;Then, given the list of available tools, the interviewer uses&nbsp;an LLM (in our case,&nbsp;OpenAI&#8217;s GPT-4.1)&nbsp;to construct a functional testing&nbsp;plan&nbsp;that&nbsp;calls each tool at least once, collecting outputs, errors, and statistics along the way. Finally,&nbsp;the&nbsp;interviewer&nbsp;can&nbsp;also&nbsp;grade&nbsp;more qualitative&nbsp;criteria&nbsp;by&nbsp;using&nbsp;an LLM&nbsp;to&nbsp;apply purpose-built rubrics&nbsp;to&nbsp;tool&nbsp;schemas&nbsp;and&nbsp;tool call outputs.&nbsp;&nbsp;We are excited to&nbsp;release the MCP Interviewer&nbsp;as an open-source CLI&nbsp;tool (opens in new tab), so server developers can automatically evaluate their MCP servers with agent usability in mind,&nbsp;and users can&nbsp;validate&nbsp;new servers.&nbsp;



While our survey provides informative initial results, it also faces significant limitations, the most obvious of which is authorization: many of the most popular MCP servers provide access to services that require authorization to use, hindering automated analysis. We are often still able to collect static features from these servers but are limited in the functional testing that can be done.



One-size fits all (but some more than others)



So, what does our survey of MCP servers tell us about the MCP ecosystem? We will get into the numbers in a moment, but as we contemplate the statistics, there is one overarching theme to keep in mind: MCP servers do not know which clients or models they are working with, and present one common set of tools, prompts, and resources to everyone. However, some models handle long contexts and large tool spaces better than others (with diverging hard limits), and respond quite differently to common prompting patterns. For example, OpenAI‚Äôs guide on function calling (opens in new tab) advises developers to:



‚ÄúInclude examples and edge cases, especially to rectify any recurring failures. (Note: Adding examples may hurt performance for reasoning models).‚Äù



So already, this places MCP at a disadvantage over vertical integrations that optimize to the operating environment. And with that, let‚Äôs dive into more numbers.



Tool count



While models generally vary in their proficiency for tool calling, the general trend has been that performance drops as the number of tools increases. For example, OpenAI limits developers to 128 tools, but recommends (opens in new tab) that developers:



‚ÄúKeep the number of functions small for higher accuracy. Evaluate your performance with different numbers of functions. Aim for fewer than 20 functions at any one time, though this is just a soft suggestion.‚Äù



While we expect this to improve with each new model generation, at present, large tool spaces can lower performance by up to 85% for some models (opens in new tab). Thankfully, the majority of servers in our survey contain four or fewer tools. But there are outliers: the largest MCP server we cataloged adds 256 distinct tools, while the 10 next-largest servers add more than 100 tools each. Further down the list we find popular servers like Playwright-MCP (opens in new tab) (29 tools, at the time of this writing), and GitHub MCP (91 tools, with subsets available at alternative endpoint URLs), which might be too large for some models.



Figure 2: The number of tools listed by each catalogued server directly after initialization. Note: servers can change the tools they list at any time, but only 226 servers in our catalog declare this capability.



Response length



Tools are generally called in agentic loops, where the output is then fed back into the model as input context. Models have hard limits on input context, but even within these limits, large contexts can drive costs up and performance down, so practical limits can be much lower (opens in new tab). MCP offers no guidance on how many tokens a tool call can produce, and the size of some responses can come as a surprise. In our analysis, we consider the 2,443 tool calls across 1,312 unique tools that the MCP Interviewer was able to call successfully during the active testing phase of server inspection. While a majority of tools produced 98 or fewer tokens  (opens in new tab), some tools are extraordinarily heavyweight: the top tool returned an average of 557,766 tokens, which is enough to swamp the context windows of many popular models like GPT-5. Further down the list, we find that 16 tools produce more than 128,000 tokens, swamping GPT-4o and other popular models. Even when responses fit into the context window length, overly long responses can significantly degrade performance (up to 91% in one study (opens in new tab)), and limit the number of future calls that can be made. Of course, agents are free to implement their own context management strategies, but this behavior is left undefined in the MCP specification and server developers cannot count on any particular client behavior or strategy.



# of tools that would overflow context inModelContext Window1 call2 calls3-5 calls6-10 callsGPT 4.11,000,00001711GPT 5400,000171525GPT-4o, Llama 3.1,128,00016153340Qwen 332,00056378690Phi-416,0009360116109



Figure 3: Tool call response length averages, in tokens, as&nbsp;observed&nbsp;by the MCP Interviewer‚Äôs functional test plan. Only successful tool calls are considered. Horizontal lines&nbsp;indicate&nbsp;context window limits for GPT-4o and GPT-5.



Tool parameter complexity



Mirroring the challenges from increasing&nbsp;the&nbsp;number of tools,&nbsp;increasing the complexity of a tool‚Äôs parameter space can also lead to degradation.&nbsp;For example, while MCP tools can take complex object types and structures as parameters,&nbsp;composio (opens in new tab)&nbsp;found that&nbsp;flattening the parameter space could improve tool-calling performance&nbsp;by 47%&nbsp;compared to baseline performance.&nbsp;&nbsp;In our analysis, we&nbsp;find&nbsp;numerous examples of deeply nested structure‚Äîin&nbsp;one&nbsp;case, going&nbsp;20&nbsp;levels deep.



Figure 4: The maximum depth of each tool‚Äôs input properties schema. A depth of 0&nbsp;indicates&nbsp;a tool with no properties. A depth of 1&nbsp;indicates&nbsp;a tool with named properties but no annotations (e.g., no description or type). A depth of 2&nbsp;indicates&nbsp;a tool with named and annotated properties.&nbsp;&nbsp;A depth of 3+&nbsp;indicates&nbsp;a tool with structured properties that have&nbsp;additional&nbsp;nested annotations.&nbsp;



Namespacing issues and naming ambiguity



Another often-cited issue with the current MCP specification is the lack of a formal namespace mechanism (opens in new tab). If two servers are registered to the same agent or application, and the servers have tool names in common, then disambiguation becomes impossible. Libraries like the OpenAI Agents SDK raise an error (opens in new tab) under this circumstance. Clients, like Claude Code, prefix tool names with unique identifiers to work around this issue. In our analysis of MCP servers, we found name collisions between 775 tools. The most common collision was ‚Äúsearch‚Äù, which appears across 32 distinct MCP servers. The following table lists the top 10 collisions.



Tool NameNumber of Instancessearch32get_user11execute_query11list_tables10update_task9generate_image9send_message9execute_command8list_tasks8search_files8



Even when names are unique, they can be semantically similar. If these tools behave similarly, then the redundancy may not be immediately problematic, but if you are expecting to call a particular tool then the name similarities raise the potential for confusion. The following table lists some examples of semantically similar tool names relating to web search:



websearchbrave_web_searchsearch-webtavily_web_searchweb_searchgoogle_news_searchsearch_webgoogle-play-searchsearch_webkrgoogle_search_parsedgoogle_searchsearch_google_imagessearch_googleget_webset_search_exaai_web_searchsearch_google_scholarweb_search_exaduckduckgo_web_searchsearch_web_toolgoogle_search_scraperweb_search_agentanswer_query_websearchbatch-web-search&nbsp;



Errors and error messages



Like all software libraries, MCP will occasionally encounter error conditions. In these cases, it is important to provide sufficient information for the agent to handle the error and plan next steps. In our analysis, we found this was not always the case. While MCP provides an ‚ÄúIsError‚Äù flag to signal errors, we found that it was common for servers to handle errors by returning strings while leaving this flag set to false, signaling a normal exit. Out of 5,983 tool call results with no error flag, GPT-4.1 judged that 3,536 indicated errors in their content. More worrisome: the error messages were often of low quality. For instance, one tool providing web search capabilities failed with the string ‚Äúerror: job,‚Äù while another tool providing academic search returned ‚ÄúPlease retry with 0 or fewer IDs.‚Äù



Resource sharing conventions



Finally, in addition to tools, MCP allows servers to share resources and resource templates with clients. In our survey, only 112 (7.6%) servers reported any resources, while 74 (5%) provided templates. One potential reason for low adoption is that the current MCP specification provides limited guidance for when resources are retrieved, or how they are incorporated into context. One clearcut situation where a client might retrieve a resource is in response to a tool returning a resource_link (opens in new tab) as a result &#8212; but only 4 tools exhibited this behavior in our survey (arguably, this would be the ideal behavior for tools that return very long, document-like responses, as outlined earlier).



Conversely, a whole different set of issues arises when there is a need to share resources from the client to the server. Consider for example a tool that provides some analysis of a local PDF file. In the case of a local MCP server utilizing STDIO transport, a local file path can be provided as an argument to the tool, but no similar conventions exist for delivering a local file to a remote MCP server. These issues are challenging enough when implementing a single server. When multiple tools or servers need to interact within the same system, the risk of interoperability errors compounds.



Recommendations



On balance, along any given dimension, the average MCP server is quite reasonable‚Äîbut, as we have seen, outliers and diverging assumptions can introduce trouble. While we expect many of these challenges to improve with time, we are comfortable making small recommendations that we feel are evergreen. We organize them below by audience.



Protocol developers



We recognize the advantages of keeping MCP relatively lightweight, avoiding being overly prescriptive in an environment where AI models and use cases are rapidly changing. However, a few small recommendations are warranted. First, we believe MCP should be extended to include a specification for client-provided resources so that tools on remote servers have a mechanism for operating on specified local files or documents. This would more effectively position MCP as a clearinghouse for resources passed between steps of agentic workflows. The MCP specification would also benefit from taking a more opinionated stance on when resources are retrieved and used overall.



Likewise, we believe&nbsp;MCP should&nbsp;quickly move to&nbsp;provide formal namespaces&nbsp;to eliminate tool name collisions.&nbsp;If namespaces&nbsp;are hierarchical, then this also provides a way of organizing large catalogs&nbsp;of functions&nbsp;into thematically&nbsp;related tool&nbsp;sets.&nbsp;Tool sets, as an organizing principle,&nbsp;are already showing some promise&nbsp;in&nbsp;GitHub MCP Server‚Äôs&nbsp;dynamic tool discovery, (opens in new tab)&nbsp;and VS Code‚Äôs&nbsp;tool grouping (with virtual tools) (opens in new tab),&nbsp;where agents or users&nbsp;can&nbsp;enable and disable tools&nbsp;as needed.&nbsp;&nbsp;In the future,&nbsp;a standardized mechanism for grouping tools would allow&nbsp;clients&nbsp;to engage in hierarchical tool-calling,&nbsp;where they first select a category, then select a tool, without needing to keep all possible&nbsp;tools in context.



Server developers



While our MCP Interviewer tool can catalog many outward-facing properties of MCP servers, developers are often in a much better position to characterize the nature of their tools. To this end, we believe developers should publish an MCP Server card alongside their servers or services, clearly outlining the runtime characteristics of the tools (e.g., the expected number of tokens generated, or expected latency of a tool call). Ideally developers should also indicate which models, agents and clients the server was tested with, how the tools were tested (e.g., provide sample tasks), list any known incompatibilities, and be mindful of limitations of various models throughout development.



Client developers



Client developers have the opportunity to experiment with various mitigations or optimizations that might help the average MCP server work better for a given system or environment. For example, clients could cache tool schemas, serving them as targets for prompt optimizations, or as an index for RAG-like tool selection approaches. To this end, Anthropic recently reported using a tool testing agent (opens in new tab) to rewrite the prompts of defective MCP servers, improving task completion time by 40%. Likewise, rather than waiting for the protocol to evolve, clients could take proactive steps to resolve name collisions‚Äî for example, generating namespaces from server names‚Äîand could reduce token outputs by summarizing or paginating long tool results.



Market developers



Finally, we see an opportunity for marketplaces to codify best-practices, spot compatibility issues at a global level, and perhaps centralize the generation and serving of model or agent-specific optimizations. Mirroring how a market like PyPI distributes Python wheels matched to a developer‚Äôs operating system or processor (opens in new tab), an MCP marketplace could serve tool schemas optimized for a developer‚Äôs chosen LLM, agent or client library. We are already seeing small steps in this direction, with registries like Smithery providing customized launch configurations to match users‚Äô clients.



Conclusion



In summary, the MCP&nbsp;ecosystem offers significant value for AI agent development,&nbsp;despite&nbsp;some&nbsp;early&nbsp;growing pains.&nbsp;Grounded in insights from the&nbsp;MCP Interviewer (opens in new tab)&nbsp;and our survey of live servers, the evidence is clear: horizontal integration is expanding capability, yet it also exposes forms of toolspace interference that can erode end to end effectiveness. Anticipating rapid advances in model capability and growing architectural diversity, the recommendations provided here aim to ensure that protocol, server, client, and marketplace developers are&nbsp;well positioned&nbsp;to adapt and thrive. Key steps include implementing formal namespaces to&nbsp;eliminate&nbsp;collisions, enhancing protocol support for&nbsp;client provided&nbsp;resources, and encouraging transparent server documentation to foster interoperability and robust development practices across the ecosystem.&nbsp;



By embracing these evergreen recommendations and proactively addressing compatibility, usability, and optimization issues, the AI agent community can create a more reliable, scalable, and efficient infrastructure that benefits both developers and end users. The future of MCP is bright, with ample opportunities for experimentation, standardization, and collective progress.
Opens in a new tabThe post Tool-space interference in the MCP era: Designing for agent compatibility at scale appeared first on Microsoft Research.
‚Ä¢ RenderFormer: How neural networks are reshaping 3D rendering
  3D rendering‚Äîthe process of converting three-dimensional models into two-dimensional images‚Äîis a foundational technology in computer graphics, widely used across gaming, film, virtual reality, and architectural visualization. Traditionally, this process has depended on physics-based techniques like ray tracing and rasterization, which simulate light behavior through mathematical formulas and expert-designed models.



Now, thanks to advances in AI, especially neural networks, researchers are beginning to replace these conventional approaches with machine learning (ML). This shift is giving rise to a new field known as neural rendering.



Neural rendering combines deep learning with traditional graphics techniques, allowing models to simulate complex light transport without explicitly modeling physical optics. This approach offers significant advantages: it eliminates the need for handcrafted rules, supports end-to-end training, and can be optimized for specific tasks. Yet, most current neural rendering methods rely on 2D image inputs, lack support for raw 3D geometry and material data, and often require retraining for each new scene‚Äîlimiting their generalizability.



RenderFormer: Toward a general-purpose neural rendering model



To overcome these limitations, researchers at Microsoft Research have developed RenderFormer, a new neural architecture designed to support full-featured 3D rendering using only ML‚Äîno traditional graphics computation required. RenderFormer is the first model to demonstrate that a neural network can learn a complete graphics rendering pipeline, including support for arbitrary 3D scenes and global illumination, without relying on ray tracing or rasterization. This work has been accepted at SIGGRAPH 2025 and is open-sourced on GitHub (opens in new tab).



Architecture overview



As shown in Figure 1, RenderFormer represents the entire 3D scene using triangle tokens‚Äîeach one encoding spatial position, surface normal, and physical material properties such as diffuse color, specular color, and roughness. Lighting is also modeled as triangle tokens, with emission values indicating intensity.



Figure 1. Architecture of RenderFormer



To describe the viewing direction, the model uses ray bundle tokens derived from a ray map‚Äîeach pixel in the output image corresponds to one of these rays. To improve computational efficiency, pixels are grouped into rectangular blocks, with all rays in a block processed together.



The model outputs a set of tokens that are decoded into image pixels, completing the rendering process entirely within the neural network.



	
		

		
		Spotlight: AI-POWERED EXPERIENCE
	
	
	
						
				
					
				
			
			
			

									Microsoft research copilot experience
				
								Discover more about research at Microsoft through our AI-powered experience
				
								
					
						
							Start now						
					
				
							
	
Opens in a new tab	
	


Dual-branch design for view-independent and view-dependent effects



The RenderFormer architecture is built around two transformers: one for view-independent features and another for view-dependent ones.




The view-independent transformer captures scene information unrelated to viewpoint, such as shadowing and diffuse light transport, using self-attention between triangle tokens.



The view-dependent transformer models effects like visibility, reflections, and specular highlights through cross-attention between triangle and ray bundle tokens.




Additional image-space effects, such as anti-aliasing and screen-space reflections, are handled via self-attention among ray bundle tokens.



To validate the architecture, the team conducted ablation studies and visual analyses, confirming the importance of each component in the rendering pipeline.



Table 1. Ablation study analyzing the impact of different components and attention mechanisms on the final performance of the trained network. 



To test the capabilities of the view-independent transformer, researchers trained a decoder to produce diffuse-only renderings. The results, shown in Figure 2, demonstrate that the model can accurately simulate shadows and other indirect lighting effects.



Figure 2. View-independent rendering effects decoded directly from the view-independent transformer, including diffuse lighting and coarse shadow effects. 



The view-dependent transformer was evaluated through attention visualizations. For example, in Figure 3, the attention map reveals a pixel on a teapot attending to its surface triangle and to a nearby wall‚Äîcapturing the effect of specular reflection. These visualizations also show how material changes influence the sharpness and intensity of reflections.



Figure 3. Visualization of attention outputs



Training methodology and dataset design



RenderFormer was trained using the Objaverse dataset, a collection of more than 800,000 annotated 3D objects that is designed to advance research in 3D modeling, computer vision, and related fields. The researchers designed four scene templates, populating each with 1‚Äì3 randomly selected objects and materials. Scenes were rendered in high dynamic range (HDR) using Blender‚Äôs Cycles renderer, under varied lighting conditions and camera angles.



The base model, consisting of 205 million parameters, was trained in two phases using the AdamW optimizer:




500,000 steps at 256√ó256 resolution with up to 1,536 triangles



100,000 steps at 512√ó512 resolution with up to 4,096 triangles




The model supports arbitrary triangle-based input and generalizes well to complex real-world scenes. As shown in Figure 4, it accurately reproduces shadows, diffuse shading, and specular highlights.



Figure 4. Rendered results of different 3D scenes generated by RenderFormer 



RenderFormer can also generate continuous video by rendering individual frames, thanks to its ability to model viewpoint changes and scene dynamics.



3D animation sequence rendered by RenderFormer 



Looking ahead: Opportunities and challenges



RenderFormer represents a significant step forward for neural rendering. It demonstrates that deep learning can replicate and potentially replace the traditional rendering pipeline, supporting arbitrary 3D inputs and realistic global illumination‚Äîall without any hand-coded graphics computations.



However, key challenges remain. Scaling to larger and more complex scenes with intricate geometry, advanced materials, and diverse lighting conditions will require further research. Still, the transformer-based architecture provides a solid foundation for future integration with broader AI systems, including video generation, image synthesis, robotics, and embodied AI.¬†



Researchers hope that RenderFormer will serve as a building block for future breakthroughs in both graphics and AI, opening new possibilities for visual computing and intelligent environments.
Opens in a new tabThe post RenderFormer: How neural networks are reshaping 3D rendering appeared first on Microsoft Research.
‚Ä¢ Breaking the¬†networking¬†wall¬†in¬†AI infrastructure
  Memory and network bottlenecks are increasingly limiting AI system performance by reducing GPU&nbsp;utilization&nbsp;and overall efficiency,&nbsp;ultimately preventing&nbsp;infrastructure from reaching its full potential&nbsp;despite enormous investments.&nbsp;At the&nbsp;core&nbsp;of this challenge is a fundamental trade-off in the communication technologies used for memory and network interconnects.



Datacenters typically deploy two types of physical cables&nbsp;for&nbsp;communication between&nbsp;GPUs.&nbsp;Traditional copper links&nbsp;are power-efficient and&nbsp;reliable,&nbsp;but&nbsp;limited to&nbsp;very short&nbsp;distances&nbsp;(simultaneously.&nbsp;This approach leverages a hardware-system co-design and adopts&nbsp;a wide-and-slow design with hundreds of parallel low-speed channels using&nbsp;microLEDs.&nbsp;



The fundamental trade-off&nbsp;among&nbsp;power, reliability, and reach&nbsp;stems from&nbsp;the&nbsp;narrow-and-fast&nbsp;architecture&nbsp;deployed&nbsp;in&nbsp;today&#8217;s copper and optical links,&nbsp;comprising&nbsp;a few channels&nbsp;operating&nbsp;at&nbsp;very high&nbsp;data rates. For example,&nbsp;an&nbsp;800 Gbps link&nbsp;consists of eight 100 Gbps channels.&nbsp;With&nbsp;copper links, higher channel speeds lead to greater signal integrity challenges, which limits their reach.&nbsp;With optical&nbsp;links,&nbsp;high-speed transmission is inherently inefficient, requiring power-hungry laser drivers and&nbsp;complex electronics&nbsp;to compensate for transmission impairments. These challenges&nbsp;grow&nbsp;as speeds increase&nbsp;with&nbsp;every&nbsp;generation&nbsp;of networks.&nbsp;Transmitting at high speeds also pushes the limits of optical components, reducing&nbsp;systems&nbsp;margins&nbsp;and increasing failure rates.&nbsp;



These limitations force systems designers to make unpleasant&nbsp;choices,&nbsp;limiting the scalability of AI infrastructure.&nbsp;For example,&nbsp;scale-up networks connecting AI accelerators at&nbsp;multi-Tbps&nbsp;bandwidth&nbsp;typically&nbsp;must&nbsp;rely on&nbsp;copper links&nbsp;to meet&nbsp;the&nbsp;power budget,&nbsp;requiring&nbsp;ultra-dense racks that&nbsp;consume&nbsp;hundreds of kilowatts&nbsp;per rack. This creates significant challenges in cooling&nbsp;and&nbsp;mechanical design,&nbsp;which constrain&nbsp;the practical scale of these networks and end-to-end performance. This imbalance&nbsp;ultimately&nbsp;erects&nbsp;a&nbsp;networking wall&nbsp;akin&nbsp;to the&nbsp;memory wall, in&nbsp;which CPU speeds have outstripped memory speeds, creating performance bottlenecks.



A technology offering copper-like power efficiency and reliability over long distances can overcome this networking¬†wall,¬†enabling¬†multi-rack¬†scale-up domains and unlocking¬†new architectures. This is a highly active R&amp;D area, with many candidate technologies currently being developed across the industry.¬†In¬†our recent¬†paper,¬†‚ÄúMOSAIC: Breaking the Optics versus Copper Trade-off with a Wide-and-Slow Architecture and MicroLEDs‚Äù, which received the Best Paper award at ACM SIGCOMM (opens in new tab), we present¬†one such promising¬†approach¬†that is¬†the result of a multi-year collaboration between Microsoft Research,¬†Azure, and M365.¬†This¬†work is¬†centered around¬†an optical¬†wide-and-slow architecture, shifting from a small number of high-speed serial channels towards¬†hundreds of parallel low-speed channels.¬†This¬†would be impractical¬†to realize with today‚Äôs copper and optical technologies because of¬†i)¬†electromagnetic interference challenges in high-density copper cables and ii) the¬†high cost¬†and power consumption of lasers¬†in optical links,¬†as well as the increase in packaging complexity.¬†MOSAIC overcomes these issues by¬†leveraging¬†directly modulated¬†microLEDs, a technology originally developed for¬†screen¬†displays.¬†



MicroLEDs&nbsp;are significantly smaller than traditional LEDs (ranging from a few to tens of&nbsp;microns) and, due to their&nbsp;small size,&nbsp;they&nbsp;can be modulated at several Gbps.&nbsp;They&nbsp;are manufactured in large arrays,&nbsp;with over half a million&nbsp;in a small physical footprint for high-resolution displays&nbsp;like&nbsp;head-mounted devices or smartwatches. For example, assuming 2 Gbps per&nbsp;microLED&nbsp;channel, an 800 Gbps MOSAIC link can be realized by using a 20√ó20&nbsp;microLED&nbsp;array, which can fit in less than 1 mm√ó1 mm&nbsp;silicon&nbsp;die.&nbsp;



MOSAIC‚Äôs&nbsp;wide-and-slow&nbsp;design&nbsp;provides four core benefits.




Operating&nbsp;at low speed improves power efficiency&nbsp;by&nbsp;eliminating&nbsp;the need for&nbsp;complex&nbsp;electronics&nbsp;and&nbsp;reducing optical power requirements.



By&nbsp;leveraging&nbsp;optical transmission (via&nbsp;microLEDs),&nbsp;MOSAIC&nbsp;sidesteps&nbsp;copper‚Äôs reach issues, supporting distances up to 50 meters,&nbsp;or&nbsp;> 10x&nbsp;further&nbsp;than copper.



MicroLEDs‚Äô&nbsp;simpler structure&nbsp;and temperature insensitivity&nbsp;make them more reliable than lasers. The parallel nature of&nbsp;wide-and-slow&nbsp;also&nbsp;makes it easy to add redundant channels, further increasing reliability, up to two orders of magnitude higher than optical links.&nbsp;



The&nbsp;approach is also scalable, as higher aggregate speeds (e.g.,&nbsp;1.6&nbsp;Tbps&nbsp;or 3.2&nbsp;Tbps) can be achieved by increasing the number of&nbsp;channels and/or raising per-channel speed&nbsp;(e.g., to 4-8 Gbps).&nbsp;




Further,&nbsp;MOSAIC is fully compatible with today‚Äôs pluggable transceivers‚Äô form&nbsp;factor&nbsp;and it provides a drop-in replacement for today‚Äôs copper and optical cables, without requiring any changes to existing server and network infrastructure.&nbsp;MOSAIC is protocol-agnostic, as it simply relays bits from one endpoint to another without&nbsp;terminating&nbsp;or inspecting the connection&nbsp;and, hence,&nbsp;it‚Äôs&nbsp;fully compatible with today‚Äôs protocols (e.g.,&nbsp;Ethernet, PCIe, CXL).&nbsp;We are currently working with our suppliers to&nbsp;productize&nbsp;this technology and&nbsp;scale&nbsp;to mass production.&nbsp;



While&nbsp;conceptually simple, realizing this architecture posed a few key challenges&nbsp;across the stack, which&nbsp;required&nbsp;a multi-disciplinary team with&nbsp;expertise&nbsp;spanning across integrated photonics, lens design, optical transmission, and&nbsp;analog&nbsp;and digital design.&nbsp;For example, using individual&nbsp;fibers&nbsp;per channel would be prohibitively complex and costly due to the&nbsp;large number&nbsp;of channels. We addressed this by employing imaging&nbsp;fibers,&nbsp;which are typically used for medical applications (e.g., endoscopy).&nbsp;They&nbsp;can support thousands of cores&nbsp;per&nbsp;fiber, enabling multiplexing&nbsp;of&nbsp;many channels within a single&nbsp;fiber.&nbsp;Also,&nbsp;microLEDs&nbsp;are a less pure light source&nbsp;than lasers,&nbsp;with&nbsp;a larger beam shape (which complicates&nbsp;fiber&nbsp;coupling) and&nbsp;a broader spectrum (which&nbsp;degrades&nbsp;fiber&nbsp;transmission due to chromatic dispersion).&nbsp;We tackled these issues through&nbsp;a novel&nbsp;microLED and&nbsp;optical lens design,&nbsp;and&nbsp;a power-efficient&nbsp;analog-only electronic back&nbsp;end, which does not require any expensive digital signal processing.&nbsp;&nbsp;



Based on our current estimates, this approach can save&nbsp;up to 68% of power, i.e., more&nbsp;than 10W per cable while reducing failure rates by up to 100x. With global annual shipments of optical cables&nbsp;reaching into&nbsp;the tens of millions, this translates to over 100MW of power savings per year,&nbsp;enough to power more than 300,000 homes. While these immediate gains are already significant, the unique combination of low power consumption, reduced cost, high reliability, and long reach opens up exciting new opportunities&nbsp;to rethink&nbsp;AI&nbsp;infrastructure from network and cluster architectures to compute and memory designs.



For example,&nbsp;by&nbsp;supporting&nbsp;low-power,&nbsp;high-bandwidth connectivity at long reach,&nbsp;MOSAIC&nbsp;removes the need for ultra-dense racks and&nbsp;enables&nbsp;novel network topologies, which would be impractical today. The resulting redesign could&nbsp;reduce&nbsp;resource fragmentation and&nbsp;simplify&nbsp;collective optimization.&nbsp;Similarly,&nbsp;on the&nbsp;compute&nbsp;front,&nbsp;the ability&nbsp;to&nbsp;connect&nbsp;silicon&nbsp;dies at low power over long distances&nbsp;could&nbsp;enable&nbsp;resource&nbsp;disaggregation, shifting from today‚Äôs&nbsp;large,&nbsp;multi-die packages to&nbsp;smaller, more cost-effective, ones.&nbsp;Bypassing packaging area constraints would also make it possible to drastically increase&nbsp;GPU&nbsp;memory&nbsp;capacity and bandwidth,&nbsp;while&nbsp;facilitating&nbsp;adoption of&nbsp;novel memory technologies.&nbsp;



Historically, step changes in network technology have unlocked entirely new classes of applications and workloads. While our SIGCOMM paper provides&nbsp;possible future&nbsp;directions, we hope this work sparks broader discussion and collaboration across the research and industry communities.
Opens in a new tabThe post Breaking the¬†networking¬†wall¬†in¬†AI infrastructure¬† appeared first on Microsoft Research.
‚Ä¢ Enhance video understanding with Amazon Bedrock Data Automation and open-set object detection
  In real-world video and image analysis, businesses often face the challenge of detecting objects that weren‚Äôt part of a model‚Äôs original training set. This becomes especially difficult in dynamic environments where new, unknown, or user-defined objects frequently appear. For example, media publishers might want to track emerging brands or products in user-generated content; advertisers need to analyze product appearances in influencer videos despite visual variations; retail providers aim to support flexible, descriptive search; self-driving cars must identify unexpected road debris; and manufacturing systems need to catch novel or subtle defects without prior labeling.In all these cases, traditional closed-set object detection (CSOD) models‚Äîwhich only recognize a fixed list of predefined categories‚Äîfail to deliver. They either misclassify the unknown objects or ignore them entirely, limiting their usefulness for real-world applications.Open-set object detection (OSOD) is an approach that enables models to detect both known and previously unseen objects, including those not encountered during training. It supports flexible input prompts, ranging from specific object names to open-ended descriptions, and can adapt to user-defined targets in real time without requiring retraining. By combining visual recognition with semantic understanding‚Äîoften through vision-language models‚ÄîOSOD helps users query the system broadly, even if it‚Äôs unfamiliar, ambiguous, or entirely new. 
In this post, we explore how Amazon Bedrock Data Automation uses OSOD to enhance video understanding. 
Amazon Bedrock Data Automation and video blueprints with OSOD 
Amazon Bedrock Data Automation is a cloud-based service that extracts insights from unstructured content like documents, images, video and audio. Specifically, for video content, Amazon Bedrock Data Automation supports functionalities such as chapter segmentation, frame-level text detection, chapter-level classification Interactive Advertising Bureau (IAB) taxonomies, and frame-level OSOD. For more information about Amazon Bedrock Data Automation, see Automate video insights for contextual advertising using Amazon Bedrock Data Automation. 
Amazon Bedrock Data Automation video blueprints support OSOD on the frame level. You can input a video along with a text prompt specifying the desired objects to detect. For each frame, the model outputs a dictionary containing bounding boxes in XYWH format (the x and y coordinates of the top-left corner, followed by the width and height of the box), along with corresponding labels and confidence scores. You can further customize the output based on their needs‚Äîfor instance, filtering by high-confidence detections when precision is prioritized. 
The input text is highly flexible, so you can define dynamic fields in the Amazon Bedrock Data Automation video blueprints powered by OSOD. 
Example use cases 
In this section, we explore some examples of different use cases for Amazon Bedrock Data Automation video blueprints using OSOD. The following table summarizes the functionality of this feature. 
 
  
   
   Functionality 
   Sub-functionality 
   Examples 
   
   
   Multi-granular visual comprehension 
   Object detection from fine-grained object reference 
   "Detect the apple in the video." 
   
   
    
   Object detection from cross-granularity object reference 
   "Detect all the fruit items in the image." 
   
   
    
   Object detection from open questions 
   "Find and detect the most visually important elements in the image." 
   
   
   Visual hallucination detection 
   Identify and flag object mentionings in the input text that do not correspond to actual content in the given image. 
   "Detect if apples appear in the image." 
   
  
 
Ads analysis 
Advertisers can use this feature to compare the effectiveness of various ad placement strategies across different locations and conduct A/B testing to identify the most optimal advertising approach. For example, the following image is the output in response to the prompt ‚ÄúDetect the locations of echo devices.‚Äù 
 
Smart resizing 
By detecting key elements in the video, you can choose appropriate resizing strategies for devices with different resolutions and aspect ratios, making sure important visual information is preserved. For example, the following image is the output in response to the prompt ‚ÄúDetect the key elements in the video.‚Äù 
 
Surveillance with intelligent monitoring 
In home security systems, producers or users can take advantage of the model‚Äôs high-level understanding and localization capabilities to maintain safety, without the need to manually enumerate all possible scenarios. For example, the following image is the output in response to the prompt ‚ÄúCheck dangerous elements in the video.‚Äù 
 
Custom labels 
You can define your own labels and search through videos to retrieve specific, desired results. For example, the following image is the output in response to the prompt ‚ÄúDetect the white car with red wheels in the video.‚Äù 
 
Image and video editing 
With flexible text-based object detection, you can accurately remove or replace objects in photo editing software, minimizing the need for imprecise, hand-drawn masks that often require multiple attempts to achieve the desired result. For example, the following image is the output in response to the prompt ‚ÄúDetect the people riding motorcycles in the video.‚Äù 
Sample video blueprint input and output 
The following example demonstrates how to define an Amazon Bedrock Data Automation video blueprint to detect visually prominent objects at the chapter level, with sample output including objects and their bounding boxes. 
The following code is our example blueprint schema: 
 
 blueprint&nbsp;=&nbsp;{
&nbsp;&nbsp;"$schema": "http://json-schema.org/draft-07/schema#",
&nbsp;&nbsp;"description": "This blueprint enhances the searchability and discoverability of video content by providing comprehensive object detection and scene analysis.",
&nbsp;&nbsp;"class": "media_search_video_analysis",
&nbsp;&nbsp;"type": "object",
&nbsp;&nbsp;"properties": {
&nbsp;&nbsp; &nbsp;# Targeted Object Detection: Identifies visually prominent objects in the video
&nbsp;&nbsp; &nbsp;# Set granularity to chapter level for more precise object detection
&nbsp;&nbsp; &nbsp;"targeted-object-detection": {
&nbsp;&nbsp; &nbsp; &nbsp;"type": "array",
&nbsp;&nbsp; &nbsp; &nbsp;"instruction": "Please detect all the visually prominent objects in the video",
&nbsp;&nbsp; &nbsp; &nbsp;"items": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"$ref": "bedrock-data-automation#/definitions/Entity"
&nbsp;&nbsp; &nbsp; &nbsp;},
&nbsp;&nbsp; &nbsp; &nbsp;"granularity": ["chapter"] &nbsp;# Chapter-level granularity provides per-scene object detection
&nbsp;&nbsp; &nbsp;},&nbsp;&nbsp;
&nbsp;&nbsp;}
} 
 
The following code is out example video custom output: 
 
 "chapters": [
&nbsp; &nbsp; &nbsp; &nbsp; .....,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;{
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"inference_result": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"emotional-tone": "Tension and suspense"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;},
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"frames": [
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;{
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"frame_index": 10289,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"inference_result": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"targeted-object-detection": [
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;{
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"label": "man",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"bounding_box": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"left": 0.6198254823684692,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"top": 0.10746771097183228,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"width": 0.16384708881378174,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"height": 0.7655990719795227
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;},
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"confidence": 0.9174646443068981
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;},
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;{
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"label": "ocean",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"bounding_box": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"left": 0.0027531087398529053,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"top": 0.026655912399291992,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"width": 0.9967235922813416,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"height": 0.7752640247344971
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;},
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"confidence": 0.7712276351034641
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;},
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;{
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"label": "cliff",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"bounding_box": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"left": 0.4687306359410286,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"top": 0.5707792937755585,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"width": 0.168929323554039,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"height": 0.20445972681045532
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;},
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"confidence": 0.719932173293829
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;],
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;},
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"timecode_smpte": "00:05:43;08",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"timestamp_millis": 343276
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;],
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"chapter_index": 11,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"start_timecode_smpte": "00:05:36;16",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"end_timecode_smpte": "00:09:27;14",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"start_timestamp_millis": 336503,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"end_timestamp_millis": 567400,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"start_frame_index": 10086,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"end_frame_index": 17006,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"duration_smpte": "00:03:50;26",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"duration_millis": 230897,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"duration_frames": 6921
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;},
&nbsp; &nbsp; &nbsp; &nbsp; ..........
] 
 
For the full example, refer to the following GitHub repo. 
Conclusion 
The OSOD capability within Amazon Bedrock Data Automation significantly enhances the ability to extract actionable insights from video content. By combining flexible text-driven queries with frame-level object localization, OSOD helps users across industries implement intelligent video analysis workflows‚Äîranging from targeted ad evaluation and security monitoring to custom object tracking. Integrated seamlessly into the broader suite of video analysis tools available in Amazon Bedrock Data Automation, OSOD not only streamlines content understanding but also help reduce the need for manual intervention and rigid pre-defined schemas, making it a powerful asset for scalable, real-world applications. 
To learn more about Amazon Bedrock Data Automation video and audio analysis, see New Amazon Bedrock Data Automation capabilities streamline video and audio analysis. 
 
About the authors 
Dongsheng An is an Applied Scientist at AWS AI, specializing in face recognition, open-set object detection, and vision-language models. He received his Ph.D. in Computer Science from Stony Brook University, focusing on optimal transport and generative modeling. 
Lana Zhang is a Senior Solutions Architect in the AWS World Wide Specialist Organization AI Services team, specializing in AI and generative AI with a focus on use cases including content moderation and media analysis. She‚Äôs dedicated to promoting AWS AI and generative AI solutions, demonstrating how generative AI can transform classic use cases by adding business value. She assists customers in transforming their business solutions across diverse industries, including social media, gaming, ecommerce, media, advertising, and marketing. 
Raj Jayaraman is a Senior Generative AI Solutions Architect at AWS, bringing over a decade of experience in helping customers extract valuable insights from data. Specializing in AWS AI and generative AI solutions, Raj‚Äôs expertise lies in transforming business solutions through the strategic application of AWS‚Äôs AI capabilities, ensuring customers can harness the full potential of generative AI in their unique contexts. With a strong background in guiding customers across industries in adopting AWS Analytics and Business Intelligence services, Raj now focuses on assisting organizations in their generative AI journey‚Äîfrom initial demonstrations to proof of concepts and ultimately to production implementations.
‚Ä¢ How Skello uses Amazon Bedrock to query data in a multi-tenant environment while keeping logical boundaries
  This is a guest post co-written with Skello. 
Skello is a leading human resources (HR) software as a service (SaaS) solution focusing on employee scheduling and workforce management. Catering to diverse sectors such as hospitality, retail, healthcare, construction, and industry, Skello offers features including schedule creation, time tracking, and payroll preparation. With approximately 20,000 customers and 400,000 daily users across Europe as of 2024, Skello continually innovates to meet its clients‚Äô evolving needs. 
One such innovation is the implementation of an AI-powered assistant to enhance user experience and data accessibility. In this post, we explain how Skello used Amazon Bedrock to create this AI assistant for end-users while maintaining customer data safety in a multi-tenant environment. Amazon Bedrock is a fully managed service that offers a choice of high-performing foundation models (FMs) through a single API, along with a broad set of capabilities to build generative AI applications with security, privacy, and responsible AI. 
We dive deep into the challenges of implementing large language models (LLMs) for data querying, particularly in the context of a French company operating under the General Data Protection Regulation (GDPR). Our solution demonstrates how to balance powerful AI capabilities with strict data protection requirements. 
Challenges with multi-tenant data access 
As Skello‚Äôs platform grew to serve thousands of businesses, we identified a critical need: our users needed better ways to access and understand their workforce data. Many of our customers, particularly those in HR and operations roles, found traditional database querying tools too technical and time-consuming. This led us to identify two key areas for improvement: 
 
 Quick access to non-structured data ‚Äì Our users needed to find specific information across various data types‚Äîemployee records, scheduling data, attendance logs, and performance metrics. Traditional search methods often fell short when users had complex questions like ‚ÄúShow me all part-time employees who worked more than 30 hours last month‚Äù or ‚ÄúWhat‚Äôs the average sick leave duration in the retail department?‚Äù 
 Visualization of data through graphs for analytics ‚Äì Although our platform collected comprehensive workforce data, users struggled to transform this raw information into actionable insights. They needed an intuitive way to create visual representations of trends and patterns without writing complex SQL queries or learning specialized business intelligence tools. 
 
To address these challenges, we needed a solution that could: 
 
 Understand natural language questions about complex workforce data 
 Correctly interpret context and intent from user queries 
 Generate appropriate database queries while respecting data access rules 
 Return results in user-friendly formats, including visualizations 
 Handle variations in how users might phrase similar questions 
 Process queries about time-based data and trends 
 
LLMs emerged as the ideal solution for this task. Their ability to understand natural language and context, combined with their capability to generate structured outputs, made them perfectly suited for translating user questions into precise database queries. However, implementing LLMs in a business-critical application required careful consideration of security, accuracy, and performance requirements. 
Solution overview 
Using LLMs to generate structured queries from natural language input is an emerging area of interest. This process enables the transformation of user requests into organized data structures, which can then be used to query databases automatically. 
The following diagram of Skello‚Äôs high-level architecture illustrates this user request transformation process. 
 
The implementation using AWS Lambda and Amazon Bedrock provides several advantages: 
 
 Scalability through serverless architecture 
 Cost-effective processing with pay-as-you-go pricing 
 Low-latency performance 
 Access to advanced language models like Anthropic‚Äôs Claude 3.5 Sonnet 
 Rapid deployment capabilities 
 Flexible integration options 
 
Basic query generation process 
The following diagram illustrates how we transform natural language queries into structured database requests. For this example, the user asks ‚ÄúGive me the gender parity.‚Äù 
 
The process works as follows: 
 
 The authentication service validates the user‚Äôs identity and permissions. 
 The LLM converts the natural language to a structured query format. 
 The query validation service enforces compliance with security policies. 
 The database access layer executes the query within the user‚Äôs permitted scope. 
 
Handling complex queries 
For more sophisticated requests like ‚ÄúGive me the worked hours per week per position for the last 3 months,‚Äù our system completes the following steps: 
 
 Extract query components: 
   
   Target metric: worked hours 
   Aggregation levels: week, position 
   Time frame: 3 months 
    
 Generate temporal calculations: 
   
   Use relative time expressions instead of hard-coded dates 
   Implement standardized date handling patterns 
    
 
 
Data schema optimization 
To make our system as efficient and user-friendly as possible, we carefully organized our data structure‚Äîthink of it as creating a well-organized filing system for a large office. 
We created standardized schema definitions, establishing consistent ways to store similar types of information. For example, date-related fields (hire dates, shift times, vacation periods) follow the same format. This helps prevent confusion when users ask questions like ‚ÄúShow me all events from last week.‚Äù It‚Äôs similar to having all calendars in your office using the same date format instead of some using MM/DD/YY and others using DD/MM/YY. 
Our system employs consistent naming conventions with clear, predictable names for all data fields. Instead of technical abbreviations like emp_typ_cd, we use clear terms like employee_type. This makes it straightforward for the AI to understand what users mean when they ask questions like ‚ÄúShow me all full-time employees.‚Äù 
For optimized search patterns, we strategically organized our data to make common searches fast and efficient. This is particularly important because it directly impacts user experience and system performance. We analyzed usage patterns to identify the most frequently requested information and designed our database indexes accordingly. Additionally, we created specialized data views that pre-aggregate common report requests. This comprehensive approach means questions like ‚ÄúWho‚Äôs working today?‚Äù get answered almost instantly. 
We also established clear data relationships by mapping out how different pieces of information relate to each other. For example, we clearly connect employees to their departments, shifts, and managers. This helps answer complex questions like ‚ÄúShow me all department managers who have team members on vacation next week.‚Äù 
These optimizations deliver real benefits to our users: 
 
 Faster response times when asking questions 
 More accurate answers to queries 
 Less confusion when referring to specific types of data 
 Ability to ask more complex questions about relationships between different types of information 
 Consistent results when asking similar questions in different ways 
 
For example, whether a user asks ‚ÄúShow me everyone‚Äôs vacation time‚Äù or ‚ÄúDisplay all holiday schedules,‚Äù the system understands they‚Äôre looking for the same type of information. This reliability makes the system more trustworthy and easier to use for everyone, regardless of their technical background. 
Graph generation and display 
One of the most powerful features of our system is its ability to turn data into meaningful visual charts and graphs automatically. This consists of the following actions: 
 
 Smart label creation ‚Äì The system understands what your data means and creates clear, readable labels. For example, if you ask ‚ÄúShow me employee attendance over the last 6 months,‚Äù the horizontal axis automatically labels the months (January through June), the vertical axis shows attendance numbers with simple-to-read intervals, and the title clearly states what you‚Äôre looking at: ‚ÄúEmployee Attendance Trends.‚Äù 
 Automatic legend creation ‚Äì The system creates helpful legends that explain what each part of the chart means. For instance, if you ask ‚ÄúCompare sales across different departments,‚Äù different departments get different colors, a clear legend shows which color represents which department, and additional information like ‚ÄúDashed lines show previous year‚Äù is automatically added when needed. 
 Choosing the right type of chart ‚Äì The system is smart about picking the best way to show your information. For example, it uses bar charts for comparing different categories (‚ÄúShow me sales by department‚Äù), line graphs for trends over time (‚ÄúHow has attendance changed this year?‚Äù), pie charts for showing parts of a whole (‚ÄúWhat‚Äôs the breakdown of full-time vs. part-time staff?‚Äù), and heat maps for complex patterns (‚ÄúShow me busiest hours per day of the week‚Äù). 
 Smart sizing and scaling ‚Äì The system automatically adjusts the size and scale of charts to make them simple to read. For example, if numbers range from 1‚Äì100, it might show intervals of 10; if you‚Äôre looking at millions, it might show them in a more readable way (1M, 2M, etc.); charts automatically resize to show patterns clearly; and important details are never too small to see. 
 
All of this happens automatically‚Äîyou ask your question, and the system handles the technical details of creating a clear, professional visualization. For example, the following figure is an example for the question ‚ÄúHow many hours my employees worked over the past 7 weeks?‚Äù 
 
Security-first architecture 
Our implementation adheres to OWASP best practices (specifically LLM06) by maintaining complete separation between security controls and the LLM. 
Through dedicated security services, user authentication and authorization checks are performed before LLM interactions, with user context and permissions managed through Amazon Bedrock SessionParameters, keeping security information entirely outside of LLM processing. 
Our validation layer uses Amazon Bedrock Guardrails to protect against prompt injection, inappropriate content, and forbidden topics such as racism, sexism, or illegal content. 
The system‚Äôs architecture implements strict role-based access controls through a detailed permissions matrix, so users can only access data within their authorized scope. For authentication, we use industry-standard JWT and SAML protocols, and our authorization service maintains granular control over data access permissions. 
This multi-layered approach prevents potential security bypasses through prompt manipulation or other LLM-specific attacks. The system automatically enforces data boundaries at both database and API levels, effectively preventing cross-contamination between different customer accounts. For instance, department managers can only access their team‚Äôs data, with these restrictions enforced through database compartmentalization. 
Additionally, our comprehensive audit system maintains immutable logs of all actions, including timestamps, user identifiers, and accessed resources, stored separately to protect their integrity. This security framework operates seamlessly in the background, maintaining robust protection of sensitive information without disrupting the user experience or legitimate workflows. 
Benefits 
Creating data visualizations has never been more accessible. Even without specialized expertise, you can now produce professional-quality charts that communicate your insights effectively. The streamlined process makes sure your visualizations remain consistently clear and intuitive, so you can concentrate on exploring your data questions instead of spending time on presentation details. 
The solution works through simple conversational requests that require no technical knowledge or specialized software. You simply describe what you want to visualize using everyday language and the system interprets your request and creates the appropriate visualization. There‚Äôs no need to learn complex software interfaces, remember specific commands, or understand data formatting requirements. The underlying technology handles the data processing, chart selection, and professional formatting automatically, transforming your spoken or written requests into polished visual presentations within moments. 
Your specific information needs to drive how the data is displayed, making the insights more relevant and actionable. When it‚Äôs time to share your findings, these visualizations seamlessly integrate into your reports and presentations with polished formatting that enhances your overall message. This democratization of data visualization empowers everyone to tell compelling data stories. 
Conclusion 
In this post, we explored Skello‚Äôs implementation of an AI-powered assistant using Amazon Bedrock and Lambda. We saw how end-users can query their own data in a multi-tenant environment while maintaining logical boundaries and complying with GDPR regulations. The combination of serverless architecture and advanced language models proved effective in enhancing data accessibility and user experience. 
We invite you to explore the AWS Machine Learning Blog for more insights on AI solutions and their potential business applications. If you‚Äôre interested in learning more about Skello‚Äôs journey in modernizing HR software, check out our blog post series on the topic. 
If you have any questions or suggestions about implementing similar solutions in your own multi-tenant environment, please feel free to share them in the comments section. 
 
About the authors 
Nicolas de Place is a Data &amp; AI Solutions Architect specializing in machine learning strategy for high-growth startups. He empowers emerging companies to harness the full potential of artificial intelligence and advanced analytics, designing scalable ML architectures and data-driven solutions 
C√©dric Peruzzi is a Software Architect at Skello, where he focuses on designing and implementing Generative AI features. Before his current role, he worked as a software engineer and architect, bringing his experience to help build better software solutions.

‚∏ª