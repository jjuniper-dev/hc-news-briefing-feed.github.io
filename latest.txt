‚úÖ Morning News Briefing ‚Äì July 29, 2025 10:50

üìÖ Date: 2025-07-29 10:50
üè∑Ô∏è Tags: #briefing #ai #publichealth #digitalgov

‚∏ª

üßæ Weather
‚Ä¢ No watches or warnings in effect, Pembroke
  No watches or warnings in effect. No warnings or watches or watches in effect . Watch or warnings are no longer in effect in the U.S. No watches, warnings are in effect for the rest of the day . No watches and warnings are still in effect, but no watches are in place for the day's events . The weather is not expected to be affected by the weather .
‚Ä¢ Current Conditions: Mainly Sunny, 18.4¬∞C
  Temperature: 18.4&deg;C Pressure: 101.7 kPa Visibility: 19 km Humidity: 83 % Dewpoint: 15.5&deg:C Wind: NW 13 km/h Air Quality Health Index: n/a . Conditions: Mainly Sunny at Garrison Petawawa 6:00 AM EDT Tuesday 29 July 2025 . Weather forecast: "Main
‚Ä¢ Tuesday: Chance of showers. High 30. POP 30%
  Fog patches dissipating this morning then a mix of sun and cloud with 30 percent chance of showers late this afternoon . Risk of a thunderstorm late today afternoon . Humidex 35. UV index 9 or very high. Risk of thunderstorm later this afternoon with risk of rain in the UK . High 30. Wind north 20 km/h becoming light late this morning with wind north 20

üåç International News
No updates.

üçÅ Canadian News
No updates.

üá∫üá∏ U.S. Top Stories
‚Ä¢ 'Worst-case scenario of famine' unfolding in Gaza, warns U.N.-backed report
  A UN-backed food security body says famine is unfolding in Gaza, with thousands of children malnourished and hunger-related deaths rising . The group warns that airdrops alone won't prevent a "humanitarian catastrophe" Only immediate, unrestricted aid access can prevent a humanitarian catastrophe, the group says . The agency warns that aid aid can't be given without immediate access to
‚Ä¢ As Gaza starves, the next generation may also endure the consequences
  U.N. officials say many people in Gaza are experiencing "famine-like conditions" Health experts who have studied past famines warn that the fallout can reverberate across generations . Health experts have warned of the impact of the Gaza crisis on generations of people . Gaza has been hit by a wave of violence since the start of last year's Gaza strike that left many Palestinians homeless .
‚Ä¢ Trump lawsuit against Murdoch and 'Wall Street Journal' turns personal
  Trump says he personally told his "very good friend Rupert Murdoch" that he had not sent a racy birthday greeting two decades ago to Jeffrey Epstein . Murdoch's Journal reported it anyway . Trump: I personally told Rupert Murdoch that I didn't send a birthday greeting to Epstein, but he didn't do it . Trump says it was reported anyway by Murdoch's newspaper, which he said was
‚Ä¢ DHS is urging DACA recipients to self-deport
  "DACA does not confer any form of legal status in this country," said DHS assistant press secretary Tricia McLaughlin . McLaughlin encouraged "every person here illegally" to self-deport from the U.S. "DREAMers," encouraged them to leave the country . DHS official: "Every person here illegal" should be allowed to stay in the United States. DHS
‚Ä¢ Hall of Fame second baseman Ryne Sandberg has died after battling cancer
  Ryne Sandberg was a Hall of Fame second baseman for the Chicago Cubs . He was one of baseball's best all-around players during his playing career . Sandberg, 65, died at the age of 65 on Monday, October 8, in a hospitable home in Chicago, Illinois . He is remembered for his all-round contributions to baseball's baseballing greats during his

üß† Artificial Intelligence
No updates.

üíª Digital Strategy
‚Ä¢ How Google profits even as its AI summaries reduce website ad link clicks
  In its latest quarterly report, Google's revenue grew 14 percent year-over-year to $96.4 billion . While Google's cloud revenue, $13.6 billion, with 31 percent growth, is growing fast . The bulk of Google's cash, $54.2 billion worth, still comes from advertising . Google's parent company, Alphabet, is making money hand over fist
‚Ä¢ Publishers cry foul over W3C crusade to rid web of third-party cookies
  Movement for an Open Web (MOW) has filed a complaint with the UK's Competition and Markets Authority . MOW challenges the World Wide Web Consortium's (W3C) call to eliminate third-party cookies . The CMA's proposal favors Google Exclusive, the advocacy group says . The proposal is being investigated by the UK competition watchdog, the CMA, the UK Competition and
‚Ä¢ UK buys time with ¬£1.65B extension to G-Cloud framework
  UK government extending two cloud purchasing agreements due to delayed replacement arrangements . Replacement deal not ready, so old one gets 6 more months and budget bump . UK government is extending two major cloud contracts worth an additional ¬£1.65 billion . The new framework could be worth an extra ¬£1 billion, according to the UK government . The UK government has extended two cloud deals worth ¬£2.2
‚Ä¢ Microsoft bolts Copilot Mode onto Edge to chase AI-browser crowd
  Microsoft introduces Copilot Mode in its Edge browser, a way to use voice or text commands to automate web-based tasks via AI . 'Edge, order two tons of creamed corn...' Microsoft on Monday introduced Copilot mode in the browser . Copilot is a way of using voice commands to order web-related tasks via voice commands . It's the latest in a series of
‚Ä¢ Microsoft spotlights Apple bug patched in March as SharePoint exploits continue
  Microsoft says it spotted a MacOS vulnerability some months ago that could allow attackers to steal private data . Redmond reported the bug to Cupertino, which issued a fix back in March . The bug is now under mass exploitation and being abused for espionage, data theft, and ransomware infections, Microsoft says . Microsoft says the bug was spotted in March and is now being exploited for espionage and data

üè• Public Health
No updates.

üî¨ Science
‚Ä¢ Additive effect of wildfires on hospital admission in the Pantanal wetland, Brazil
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Investigating the influence of urban land use on asbestos concentration and identifying the most vulnerable areas in Shiraz, Iran
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ A network analysis of carbapenem-resistant Klebsiella pneumoniae among healthcare facilities
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Peace in Colombia requires healing the scars of conflict on the mind and brain
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Modeling behavioral intention of using health-related WeChat official accounts through ELM and SCT factors using the PLS-SEM approach
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations

üßæ Government & Policy
No updates.

üèõÔ∏è Enterprise Architecture & IT Governance
No updates.

ü§ñ AI & Emerging Tech
‚Ä¢ What you may have missed about Trump‚Äôs AI Action Plan
  A number of the executive orders and announcements coming from the White House since Donald Trump returned to office have painted an ambitious vision for America‚Äôs AI future‚Äîcrushing competition with China, abolishing ‚Äúwoke‚Äù AI models that suppress conservative speech, jump-starting power-hungry AI data centers. But the details have been sparse.&nbsp;



The White House‚Äôs AI Action Plan, released last week, is meant to fix that. Many of the points in the plan won‚Äôt come as a surprise, and you‚Äôve probably heard of the big ones by now. Trump wants to boost the buildout of data centers by slashing environmental rules; withhold funding from states that pass ‚Äúburdensome AI regulations‚Äù; and contract only with AI companies whose models are ‚Äúfree from top-down ideological bias.‚Äù



But if you dig deeper, certain parts of the plan that didn‚Äôt pop up in any headlines reveal more about where the administration‚Äôs AI plans are headed. Here are three of the most important issues to watch.&nbsp;



Trump is escalating his fight with the Federal Trade Commission



When Americans get scammed, they‚Äôre supposed to be helped by the Federal Trade Commission. As I wrote last week, the FTC under President Biden increasingly targeted AI companies that overhyped the accuracy of their systems, as well as deployments of AI it found to have harmed consumers.&nbsp;



The Trump plan vows to take a fresh look at all the FTC actions under the previous administration as part of an effort to get rid of ‚Äúonerous‚Äù regulation that it claims is hampering AI‚Äôs development. The administration may even attempt to repeal some of the FTC‚Äôs actions entirely. This would weaken a major AI watchdog agency, but it‚Äôs just the latest in the Trump administration‚Äôs escalating attacks on the FTC. Read more in my story.&nbsp;



The White House is very optimistic about AI for science



The opening to the AI Action Plan describes a future where AI is doing everything from discovering new materials and drugs to ‚Äúunraveling ancient scrolls once thought unreadable‚Äù to making breakthroughs in science and math.&nbsp;



That type of unbounded optimism about AI for scientific discovery echoes what tech companies are promising. Some of that optimism is grounded in reality: AI‚Äôs role in predicting protein structures has indeed led to material scientific wins (and just last week, Google DeepMind released a new AI meant to help interpret ancient Latin engravings). But the idea that large language models‚Äîessentially very good text prediction machines‚Äîwill act as scientists in their own right has less merit so far.&nbsp;



Still, the plan shows that the Trump administration wants to award money to labs trying to make it a reality, even as it has worked to slash the funding the National Science Foundation makes available to human scientists, some of whom are now struggling to complete their research.&nbsp;



And some of the steps the plan proposes are likely to be welcomed by researchers, like funding to build AI systems that are more transparent and interpretable.





The White House‚Äôs messaging on deepfakes is confused



Compared with President Biden‚Äôs executive orders on AI, the new action plan is mostly devoid of anything related to making AI safer.&nbsp;



However, there‚Äôs a notable exception: a section in the plan that takes on the harms posed by deepfakes. In May, Trump signed legislation to protect people from nonconsensual sexually explicit deepfakes, a growing concern for celebrities and everyday people alike as generative video gets more advanced and cheaper to use. The law had bipartisan support.



Now, the White House says it‚Äôs concerned about the issues deepfakes could pose for the legal system. For example, it says, ‚Äúfake evidence could be used to attempt to deny justice to both plaintiffs and defendants.‚Äù It calls for new standards for deepfake detection and asks the Department of Justice to create rules around it. Legal experts I‚Äôve spoken with are more concerned with a different problem: Lawyers are adopting AI models that make errors such as citing cases that don‚Äôt exist, which judges may not catch. This is not addressed in the action plan.&nbsp;



It‚Äôs also worth noting that just days before releasing a plan that targets ‚Äúmalicious deepfakes,‚Äù President Trump shared a fake AI-generated video of former president Barack Obama being arrested in the Oval Office.



Overall, the AI Action Plan affirms what President Trump and those in his orbit have long signaled: It‚Äôs the defining social and political weapon of our time. They believe that AI, if harnessed correctly, can help them win everything from culture wars to geopolitical conflicts. The right AI, they argue, will help defeat China. Government pressure on leading companies can force them to purge ‚Äúwoke‚Äù ideology from their models.¬†



The plan includes crowd-pleasers‚Äîlike cracking down on deepfakes‚Äîbut overall, it reflects how tech giants have cozied up to the Trump administration. The fact that it contains almost no provisions challenging their power shows how their investment in this relationship is paying off.



This story originally appeared in The Algorithm, our weekly newsletter on AI. To get stories like this in your inbox first, sign up here.
‚Ä¢ This startup wants to use the Earth as a massive battery
  Texas-based startup Quidnet Energy completed a test showing it can store energy for up to six months by pumping water underground . In June, the company was able to discharge 35 megawatt-hours of energy from the well . The company hopes its approach could help bring cheap, long-duration energy storage to new places . It could qualify for federal tax credits to help make it even cheaper than lithium-ion batteries .
‚Ä¢ The Download: how China‚Äôs universities approach AI, and the pitfalls of welfare algorithms
  This is today&#8217;s edition of¬†The Download,¬†our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



Chinese universities want students to use more AI, not less



Just two years ago, students in China were told to avoid using AI for their assignments. At the time, to get around a national block on ChatGPT, students had to buy a mirror-site version from a secondhand marketplace. Its use was common, but it was at best tolerated and more often frowned upon. Now, professors no longer warn students against using AI. Instead, they‚Äôre encouraged to use it‚Äîas long as they follow best practices.Just like those in the West, Chinese universities are going through a quiet revolution. The use of generative AI on campus has become nearly universal. However, there‚Äôs a crucial difference. While many educators in the West see AI as a threat they have to manage, more Chinese classrooms are treating it as a skill to be mastered. Read the full story.



‚ÄîCaiwei Chen



If you‚Äôre interested in reading more about how AI is affecting education, check out:



+ Here‚Äôs how ed-tech companies are pitching AI to teachers.+ AI giants like OpenAI and Anthropic say their technologies can help students learn‚Äînot just cheat. But real-world use suggests otherwise. Read the full story.+ The narrative around cheating students doesn‚Äôt tell the whole story. Meet the teachers who think generative AI could actually make learning better. Read the full story.+ This AI system makes human tutors better at teaching children math. Called Tutor CoPilot, it demonstrates how AI could enhance, rather than replace, educators‚Äô work. Read the full story.







Why it‚Äôs so hard to make welfare AI fair



There are plenty of stories about AI that‚Äôs caused harm when deployed in sensitive situations, and in many of those cases, the systems were developed without much concern to what it meant to be fair or how to implement fairness.But the city of Amsterdam did spend a lot of time and money to try to create ethical AI‚Äîin fact, it followed every recommendation in the responsible AI playbook. But when it deployed it in the real world, it still couldn‚Äôt remove biases. So why did Amsterdam fail? And more importantly: Can this ever be done right?Join our editor Amanda Silverman, investigative reporter Eileen Guo and Gabriel Geiger, an investigative reporter from Lighthouse Reports, for a subscriber-only Roundtables conversation at 1pm ET on Wednesday July 30 to explore if algorithms can ever be fair. Register here!







The must-reads



I‚Äôve combed the internet to find you today‚Äôs most fun/important/scary/fascinating stories about technology.



1 The US has frozen tech export restrictions to China¬†Donald Trump is attempting to thrash out a favorable deal with Beijing. (FT $)



2 Microsoft‚Äôs early cybersecurity alert system may have tipped off hackersIt‚Äôs investigating whether the program inadvertently leaked flaws in its SharePoint service. (Bloomberg $)+ But how did the hackers know how to exploit them? (The Register)3 This may be the last time humans beat AI at mathThe world‚Äôs brightest teenagers are still outwitting AI models‚Äîbut for how long? (WSJ $)+ What‚Äôs next for AI and math. (MIT Technology Review)



4 Google is putting a vibe coding app through its pacesOpal is the company‚Äôs answer to the likes of Cursor and Lovable. (TechCrunch)+ What is vibe coding, exactly? (MIT Technology Review)



5 What the future of satellite-on-satellite warfare may look likeAmerica is preparing for combat in low-Earth orbit. (Economist $)



6 San Francisco is becoming a proper tech hub once againThe city is finally revitalizing post-pandemic. (WP $)



7 A women‚Äôs dating safety app database has been exposedAnd the womens‚Äô data shared to 4Chan. (404 Media)+ More than 72,000 images were stolen in the breach. (Reuters)+ Interest in the app has skyrocketed in the past week. (NYT $)



8 Optimists are using AI to manifest their dream livesFor when your Pinterest vision board is no longer cutting it. (NYT $)



9 A new kind of aerogel could help make saltwater drinkableAnd, unlike previous aerogels, it works on a scale large enough to matter. (Ars Technica)



10 How AI is changing video gamesExperts are bracing themselves for a complete industry takeover. (NYT $)+ How generative AI could reinvent what it means to play. (MIT Technology Review)







Quote of the day



&#8220;Let&#8217;s face it, you can&#8217;t have the Chinese have an app on 100 million American phones, that is just not okay.&#8221;



‚ÄîHoward Lutnick, the US secretary of commerce, explains why he thinks TikTok must be sold to an American owner, Reuters reports.







One more thing







Is the digital dollar dead?In 2020, digital currencies were one of the hottest topics in town. China was well on its way to launching its own central bank digital currency, or CBDC, and many other countries launched CBDC research projects, including the US.How things change. Years later, the digital dollar‚Äîeven though it doesn‚Äôt exist‚Äîhas become political red meat, as some politicians label it a dystopian tool for surveillance. And late last year, the Boston Fed quietly stopped working on its CBDC project. So is the dream of the digital dollar dead? Read the full story.



‚ÄîMike Orcutt







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)



+ How Canada is working with First Nations to connect ecological hotspots.+ Meet the dedicated followers of fashion running some of the most popular celebrity style Instagram accounts.+ The most worthless kitchen tools and gadgets, according to pro chefs.+ This clever interactive map pinpoints the locations of films, TV shows, books and games.
‚Ä¢ Chinese universities want students to use more AI, not less
  Just two years ago, Lorraine He, now a 24-year-old law student,&nbsp; was told to avoid using AI for her assignments. At the time, to get around a national block on ChatGPT, students had to buy a mirror-site version from a secondhand marketplace. Its use was common, but it was at best tolerated and more often frowned upon. Now, her professors no longer warn students against using AI. Instead, they‚Äôre encouraged to use it‚Äîas long as they follow best practices.



She is far from alone. Just like those in the West, Chinese universities are going through a quiet revolution. According to a recent survey by the Mycos Institute, a Chinese higher-education research group, the use of generative AI on campus has become nearly universal. The same survey reports that just 1% of university faculty and students in China reported never using AI tools in their studies or work. Nearly 60% said they used them frequently‚Äîeither multiple times a day or several times a week.



However, there‚Äôs a crucial difference. While many educators in the West see AI as a threat they have to manage, more Chinese classrooms are treating it as a skill to be mastered. In fact, as the Chinese-developed model DeepSeek gains in popularity globally, people increasingly see it as a source of national pride. The conversation in Chinese universities has gradually shifted from worrying about the implications for academic integrity to encouraging literacy, productivity, and staying ahead.&nbsp;



The cultural divide is even more apparent in public sentiment. A report on global AI attitudes from Stanford University‚Äôs Institute for Human-Centered Artificial Intelligence (HAI) found that China leads the world in enthusiasm. About 80% of Chinese respondents said they were ‚Äúexcited‚Äù about new AI services‚Äîcompared with just 35% in the US and 38% in the UK.



‚ÄúThis attitude isn‚Äôt surprising,‚Äù says Fang Kecheng, a professor in communications at the Chinese University of Hong Kong. ‚ÄúThere‚Äôs a long tradition in China of believing in technology as a driver of national progress, tracing back to the 1980s, when Deng Xiaoping was already saying that science and technology are primary productive forces.‚Äù



From taboo to toolkit



Liu Bingyu, one of He‚Äôs professors at the China University of Political Science and Law, says AI can act as ‚Äúinstructor, brainstorm partner, secretary, and devil‚Äôs advocate.‚Äù She added a full session on AI guidelines to her lecture series this year, after the university encouraged ‚Äúresponsible and confident‚Äù use of AI.&nbsp;



Liu recommends that students use generative AI to write literature reviews, draft abstracts, generate charts, and organize thoughts. She‚Äôs created slides that lay out detailed examples of good and bad prompts, along with one core principle: AI can‚Äôt replace human judgment. ‚ÄúOnly high-quality input and smart prompting can lead to good results,‚Äù she says.





‚ÄúThe ability to interact with machines is one of the most important skills in today‚Äôs world,‚Äù Liu told her class. ‚ÄúAnd instead of having students do it privately, we should talk about it out in the open.‚Äù



This reflects a growing trend across the country. MIT Technology Review reviewed the AI strategies of 46 top Chinese universities and found that almost all of them have added interdisciplinary AI general‚Äëeducation classes, AI related degree programs and AI literacy modules in the past year. Tsinghua, for example, is establishing a new undergraduate general education college to train students in AI plus another traditional discipline, like biology, healthcare, science, or humanities.



Major institutions like Remin, Nanjing, and Fudan Universities have rolled out general-access AI courses and degree programs that are open to all students, not reserved for computer science majors like the traditional machine-learning classes. At Zhejiang University, an introductory AI class will become mandatory for undergraduates starting in 2024.&nbsp;



Lin Shangxin, principal of Renmin University of China recently told local media that AI was an ‚Äúunprecedented opportunity‚Äù for humanities and social sciences. ‚ÄúIntead of a challenge, I believe AI would empower humanities studies,‚Äù Lin said told The Paper.



The collective action echoes a central government push. In April 2025, the Ministry of Education released new national guidelines calling for sweeping ‚ÄúAI+ education‚Äù reforms, aimed at cultivating critical thinking, digital fluency, and real‚Äêworld skills at all education levels. Earlier this year, the Beijing municipal government mandated AI education across all schools in the city‚Äîfrom universities to K‚Äì12.



Fang believes that more formal AI literacy education will help bridge an emerging divide between students. ‚ÄúThere‚Äôs a big gap in digital literacy,‚Äù he says. ‚ÄúSome students are fluent in AI tools. Others are lost.‚Äù



Building the AI university



In the absence of Western tools like ChatGPT and Claude, many Chinese universities have begun deploying local versions of DeepSeek on campus servers to support students. Many top universities have deployed their own locally hosted versions of Deepseek. These campus-specific AI systems‚Äìoften referred to as the ‚Äúfull-blood version‚Äù of Deepseek‚Äîoffer longer context windows, unlimited dialogue rounds and broader functionality than public-facing free versions.&nbsp;





This mirrors a broader trend in the West, where companies like OpenAI and Anthropic are rolling out campus-wide education tiers‚ÄîOpenAI recently offered free ChatGPT Plus to all U.S. and Canadian college students, while Anthropic launched Claude for Education with partners like Northeastern and LSE. But in China, the initiative is typically university-led rather than driven by the companies themselves.



The goal, according to Zhejiang University, is to offer students full access to AI tools so they can stay up to date with the fast-changing technology. Students can use their ID to access the models for free.&nbsp;



Yanyan Li and Meifang Zhuo, two researchers at Warwick University who have studied students‚Äô use of AI at universities in the UK, believe that AI literacy education has become crucial to students‚Äô success.&nbsp;



With their colleague Gunisha Aggarwal, they conducted focus groups including college students from different backgrounds and levels to find out how AI is used in academic studies. They found that students‚Äô knowledge of how to use AI comes mainly from personal exploration. ‚ÄúWhile most students understand that AI output is not always trustworthy, we observed a lot of anxiety on how to use it right,‚Äù says Li.



‚ÄúThe goal shouldn‚Äôt be preventing students from using AI but guiding them to harness it for effective learning and higher-order thinking,‚Äù says Zhuo.&nbsp;



That lesson has come slowly. A student at Central China Normal University in Wuhan told MIT Technology Review that just a year ago, most of his classmates paid for mirror websites of ChatGPT, using VPNs or semi-legal online marketplaces to access Western models. ‚ÄúNow, everyone just uses DeepSeek and Doubao,‚Äù he said. ‚ÄúIt‚Äôs cheaper, it works in Chinese, and no one‚Äôs worried about getting flagged anymore.‚Äù



Still, even with increased institutional support, many students feel anxious about whether they‚Äôre using AI correctly‚Äîor ethically. The use of AI detection tools has created an informal gray economy, where students pay hundreds of yuan to freelancers promising to ‚ÄúAI-detection-proof‚Äù their writing, according to a Rest of World report. Three students told MIT Technology Review that this environment has created confusion, stress, and increased anxiety. Across the board, they said they appreciate it when their professor offers clear policies and practical advice, not just warnings.



He, the law student in Beijing, recently joined a career development group to learn more AI skills to prepare for the job market. To many like her, understanding how to use AI better is not just a studying hack but a necessary skill in China‚Äôs fragile job market. Eighty percent of job openings available to fresh graduates listed AI-related skills as a plus in 2025, according to a report by the Chinese media outlet YiCai. In a slowed-down economy and a competitive job market, many students see AI as a lifeline.&nbsp;



&nbsp;‚ÄúWe need to rethink what is considered ‚Äòoriginal work‚Äô in the age of AI‚Äù says Zhuo, ‚Äúand universities are a crucial site of that conversation‚Äù.
‚Ä¢ The Download: saving the US climate programs, and America‚Äôs AI protections are under threat
  This is today&#8217;s edition of¬†The Download,¬†our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



How nonprofits and academia are stepping up to salvage US climate programs



Nonprofits are trying to preserve a US effort to modernize greenhouse-gas measurements, amid growing fears that the Trump administration‚Äôs dismantling of federal programs will obscure the nation‚Äôs contributions to climate change.



The Data Foundation, a Washington, DC, nonprofit, is fundraising for an initiative that will coordinate efforts among nonprofits, technical experts, and companies to improve the accuracy and accessibility of climate emissions information. It will build on an effort to improve the collection of emissions data that former president Joe Biden launched in 2023‚Äîand which President Trump nullified on his first day in office.&nbsp;



The new greenhouse-gas coalition is one of a growing number of nonprofit and academic groups that have spun up or shifted focus to keep essential climate monitoring and research efforts going amid the Trump administration‚Äôs assault on environmental funding, staffing, and regulations. Read the full story.



‚ÄîJames Temple







America‚Äôs AI watchdog is losing its bite



Most Americans encounter the Federal Trade Commission only if they‚Äôve been scammed: It handles identity theft, fraud, and stolen data. During the Biden administration, the agency went after AI companies for scamming customers with deceptive advertising or harming people by selling irresponsible technologies.&nbsp;



With the announcement of President Trump‚Äôs AI Action Plan, that era may now be over.The new plan suggests that the Trump administration believes the agency‚Äôs previous actions went too far, and that it would be reviewing all FTC actions taken under the Biden administration.The move is the latest in its evolving attack on the agency, which provides a significant route of redress for people harmed by AI in the US. It‚Äôs likely to result in faster deployment of AI with fewer checks on accuracy, fairness, or consumer harm. Read the full story.



‚ÄîJames O‚ÄôDonnell







Trump‚Äôs AI Action Plan is a distraction



‚ÄîAsad Ramzanali is the director of artificial intelligence and technology policy at the Vanderbilt Policy Accelerator.



On Wednesday, President Trump issued three executive orders, delivered a speech, and released an action plan, all on the topic of continuing American leadership in AI.



This flurry of actions made for glitzy press moments, including an hour-long speech from the president and onstage signings. But while the tech industry cheered these announcements (which will swell their coffers), they obscured the fact that the administration is currently decimating the very policies that enabled America to become the world leader in AI in the first place. Read the full story.







The deadly saga of the controversial gene therapy Elevidys



It has been a grim few months for the Duchenne muscular dystrophy (DMD) community. There had been some excitement when, a couple of years ago, a gene therapy for the disorder was approved by the US Food and Drug Administration for the first time. That drug, Elevidys, has now been implicated in the deaths of two teenage boys.



The drug‚Äôs approval was always controversial‚Äîthere was a lack of evidence that it actually worked, for starters. But the agency that once rubber-stamped the drug has now turned on its manufacturer, Sarepta Therapeutics. In a remarkable chain of events, the FDA asked the company to stop shipping the drug on July 18. Sarepta refused to comply.In the days since, the company has acquiesced. But its reputation has already been hit. And the events have dealt a devastating blow to people desperate for treatments that might help them, their children, or other family members with DMD. Read the full story.



‚ÄîJessica Hamzelou



A version of this article first appeared in The Checkup, MIT Technology Review‚Äôs weekly biotech newsletter. To receive it in your inbox every Thursday, and read articles like this first, sign up here.







The must-reads



I‚Äôve combed the internet to find you today‚Äôs most fun/important/scary/fascinating stories about technology.



1 Corporate America is paying the price for Trump‚Äôs tariffsUS businesses are absorbing the costs‚Äîfor now. (WSJ $)+ Inflation is likely to hit hard in the fall. (Vox)+ Sweeping tariffs could threaten the US manufacturing rebound. (MIT Technology Review)



2 GPT-5 is reportedly launching next monthAfter some unexpected setbacks. (The Verge)



3 Meta is hosting ads crowdfunding for IDF dronesA watchdog has identified more than 100 ads seeking donations for the army. (The Guardian)4 AI is helping researchers to combat long covid and MEA new platform spots biological markers of the conditions in patients. (FT $)+ Scientists are finding signals of long covid in blood. They could lead to new treatments. (MIT Technology Review)5 Demand is surging for banned chip repair expertise in China¬†While most of Nvidia‚Äôs chips aren‚Äôt allowed in the country, there‚Äôs a booming industry for fixing them once they break. (Reuters)+ Nvidia‚Äôs chips are being smuggled in through the black market. (FT $)



6 ChatGPT can offer up instructions for self harm and devil worshipIt guides users through self-mutilation rituals, despite violating its own policies. (The Atlantic $)



7 We have more steel than we can possibly useBut countries are worried about the optics of ceasing production. (NYT $)+ This startup just hit a big milestone for green steel production. (MIT Technology Review)



8 Internet age checks are comingA swath of child protection laws are forcing a profound shift across the web. (Wired $)+ Child online safety laws will actually hurt kids, critics say. (MIT Technology Review)



9 An Italian rocket maker wants to conduct launches in the USAvio SpA is keen to launch flights from Wallops Island in Virginia. (Bloomberg $)+ Rivals are rising to challenge the dominance of SpaceX. (MIT Technology Review)



10 This app allows women to check a potential date‚Äôs historyBut men say there‚Äôs no recourse to address false posts about them. (WP $)







Quote of the day



&#8220;If OpenAI‚Äôs ChatGPT or Google‚Äôs Gemini had responded that it was trained to appeal to the left, congressional Republicans would have been outraged and opened an investigation. Instead, they were silent.‚Äù



‚ÄîSenator Ed Markey urges the CEOs of major tech companies to fight Donald Trump&#8217;s anti-woke AI order, Ars Technica reports.







One more thing







The great AI consciousness conundrumAI consciousness isn‚Äôt just a devilishly tricky intellectual puzzle; it‚Äôs a morally weighty problem with potentially dire consequences that philosophers, cognitive scientists, and engineers alike are currently grappling with.Fail to identify a conscious AI, and you might unintentionally subjugate a being whose interests ought to matter. Mistake an unconscious AI for a conscious one, and you risk compromising human safety and happiness for the sake of an unthinking, unfeeling hunk of silicon and code.Over the past few decades, a small research community has doggedly attacked the question of what consciousness is and how it works. The effort has yielded real progress. And now, with the rapid advance of AI technology, these insights could offer our only guide to the untested, morally fraught waters of artificial consciousness. Read the full story.



‚ÄîGrace Huckins







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)



+ Crank it up to 11! Spinal Tap is back, baby.+ The tale of New York‚Äôs first real architectural firm.+ Crazy Train as played by a class of children on the xylophone is a real delight.+ Here‚Äôs how to easily add some cheap and accessible superfoods to zhuzh up your daily diet.

üîí Cybersecurity & Privacy
‚Ä¢ Phishers Target Aviation Execs to Scam Customers
  KrebsOnSecurity recently heard from a reader whose boss&#8217;s email account got phished and was used to trick one of the company&#8217;s customers into sending a large payment to scammers. An investigation into the attacker&#8217;s infrastructure points to a long-running Nigerian cybercrime ring that is actively targeting established companies in the transportation and aviation industries.
Image: Shutterstock, Mr. Teerapon Tiuekhom.
A reader who works in the transportation industry sent a tip about a recent successful phishing campaign that tricked an executive at the company into entering their credentials at a fake Microsoft 365 login page. From there, the attackers quickly mined the executive&#8217;s inbox for past communications about invoices, copying and modifying some of those messages with new invoice demands that were sent to some of the company&#8217;s customers and partners.
Speaking on condition of anonymity, the reader said the resulting phishing emails to customers came from a newly registered domain name that was remarkably similar to their employer&#8217;s domain, and that at least one of their customers fell for the ruse and paid a phony invoice. They said the attackers had spun up a look-alike domain just a few hours after the executive&#8217;s inbox credentials were phished, and that the scam resulted in a customer suffering a six-figure financial loss.
The reader also shared that the email addresses in the registration records for the imposter domain &#8212; roomservice801@gmail.com &#8212; is tied to many such phishing domains. Indeed, a search on this email address at DomainTools.com finds it is associated with at least 240 domains registered in 2024 or 2025. Virtually all of them mimic legitimate domains for companies in the aerospace and transportation industries worldwide.
An Internet search for this email address reveals a humorous blog post from 2020 on the Russian forum hackware[.]ru, which found roomservice801@gmail.com was tied to a phishing attack that used the lure of phony invoices to trick the recipient into logging in at a fake Microsoft login page. We&#8217;ll come back to this research in a moment.
JUSTY JOHN
DomainTools shows that some of the early domains registered to roomservice801@gmail.com in 2016 include other useful information. For example, the WHOIS records for alhhomaidhicentre[.]biz reference the technical contact of &#8220;Justy John&#8221; and the email address justyjohn50@yahoo.com.
A search at DomainTools found justyjohn50@yahoo.com has been registering one-off phishing domains since at least 2012. At this point, I was convinced that some security company surely had already published an analysis of this particular threat group, but I didn&#8217;t yet have enough information to draw any solid conclusions.
DomainTools says the Justy John email address is tied to more than two dozen domains registered since 2012, but we can find hundreds more phishing domains and related email addresses simply by pivoting on details in the registration records for these Justy John domains. For example, the street address used by the Justy John domain axisupdate[.]net &#8212; 7902 Pelleaux Road in Knoxville, TN &#8212; also appears in the registration records for accountauthenticate[.]com, acctlogin[.]biz, and loginaccount[.]biz, all of which at one point included the email address rsmith60646@gmail.com.
That Rsmith Gmail address is connected to the 2012 phishing domain alibala[.]biz (one character off of the Chinese e-commerce giant alibaba.com, with a different top-level domain of .biz). A search in DomainTools on the phone number in those domain records &#8212; 1.7736491613 &#8212; reveals even more phishing domains as well as the Nigerian phone number &#8220;2348062918302&#8221; and the email address michsmith59@gmail.com.
DomainTools shows michsmith59@gmail.com appears in the registration records for the domain seltrock[.]com, which was used in the phishing attack documented in the 2020 Russian blog post mentioned earlier. At this point, we are just two steps away from identifying the threat actor group.
The same Nigerian phone number shows up in dozens of domain registrations that reference the email address sebastinekelly69@gmail.com, including 26i3[.]net, costamere[.]com, danagruop[.]us, and dividrilling[.]com. A Web search on any of those domains finds they were indexed in an &#8220;indicator of compromise&#8221; list on GitHub maintained by Palo Alto Networks&#8216; Unit 42 research team.
SILVERTERRIER
According to Unit 42, the domains are the handiwork of a vast cybercrime group based in Nigeria that it dubbed &#8220;SilverTerrier&#8221; back in 2014. In an October 2021 report, Palo Alto said SilverTerrier excels at so-called &#8220;business e-mail compromise&#8221; or BEC scams, which target legitimate business email accounts through social engineering or computer intrusion activities. BEC criminals use that access to initiate or redirect the transfer of business funds for personal gain.
Palo Alto says SilverTerrier encompasses hundreds of BEC fraudsters, some of whom have been arrested in various international law enforcement operations by Interpol. In 2022, Interpol and the Nigeria Police Force arrested 11 alleged SilverTerrier members, including a prominent SilverTerrier leader who&#8217;d been flaunting his wealth on social media for years. Unfortunately, the lure of easy money, endemic poverty and corruption, and low barriers to entry for cybercrime in Nigeria conspire to provide a constant stream of new recruits.
BEC scams were the 7th most reported crime tracked by the FBI&#8217;s Internet Crime Complaint Center (IC3) in 2024, generating more than 21,000 complaints. However, BEC scams were the second most costly form of cybercrime reported to the feds last year, with nearly $2.8 billion in claimed losses.¬†In its 2025 Fraud and Control Survey Report, the Association for Financial Professionals found 63 percent of organizations experienced a BEC last year.
Poking at some of the email addresses that spool out from this research reveals a number of Facebook accounts for people residing in Nigeria or in the United Arab Emirates, many of whom do not appear to have tried to mask their real-life identities. Palo Alto&#8217;s Unit 42 researchers reached a similar conclusion, noting that although a small subset of these crooks went to great lengths to conceal their identities, it was usually simple to learn their identities on social media accounts and the major messaging services.
Palo Alto said BEC actors have become far more organized over time, and that while it remains easy to find actors working as a group, the practice of using one phone number, email address or alias to register malicious infrastructure in support of multiple actors has made it far more time consuming (but not impossible) for cybersecurity and law enforcement organizations to sort out which actors committed specific crimes.
&#8220;We continue to find that SilverTerrier actors, regardless of geographical location, are often connected through only a few degrees of separation on social media platforms,&#8221; the researchers wrote.
FINANCIAL FRAUD KILL CHAIN
Palo Alto has published a useful list of recommendations that organizations can adopt to minimize the incidence and impact of BEC attacks. Many of those tips are prophylactic, such as conducting regular employee security training and reviewing network security policies.
But one recommendation &#8212; getting familiar with a process known as the &#8220;financial fraud kill chain&#8221; or FFKC &#8212; bears specific mention because it offers the single best hope for BEC victims who are seeking to claw back payments made to fraudsters, and yet far too many victims don&#8217;t know it exists until it is too late.
Image: ic3.gov.
As explained in this FBI primer, the International Financial Fraud Kill Chain is a partnership between federal law enforcement and financial entities whose purpose is to freeze fraudulent funds wired by victims. According to the FBI, viable victim complaints filed with ic3.gov promptly after a fraudulent transfer (generally less than 72 hours) will be automatically triaged by the Financial Crimes Enforcement Network (FinCEN).
The FBI noted in its IC3 annual report (PDF) that the FFKC had a 66 percent success rate in 2024. Viable ic3.gov complaints involve losses of at least $50,000, and include all records from the victim or victim bank, as well as a completed FFKC form (provided by FinCEN) containing victim information, recipient information, bank names, account numbers, location, SWIFT, and any additional information.

üéì University AI
No updates.

üè¢ Corporate AI
‚Ä¢ Navigating medical education in the era of generative AI
  In November 2022, OpenAI‚Äôs ChatGPT kick-started a new era in AI. This was followed less than a half year later by the release of GPT-4. In the months leading up to GPT-4‚Äôs public release, Peter Lee, president of Microsoft Research, cowrote a book full of optimism for the potential of advanced AI models to transform the world of healthcare. What has happened since? In this special podcast series, The AI Revolution in Medicine, Revisited, Lee revisits the book, exploring how patients, providers, and other medical professionals are experiencing and using generative AI today while examining what he and his coauthors got right‚Äîand what they didn‚Äôt foresee.&nbsp;&nbsp;&nbsp;



In this episode, Dr. Morgan Cheatham (opens in new tab) and Daniel Chen (opens in new tab), two rising physicians and experts in both medicine and technology, join Lee to explore how generative AI is reshaping medical education. Cheatham, a partner and head of healthcare and life sciences at Breyer Capital and a resident physician at Boston Children‚Äôs Hospital, discusses how AI is changing how clinicians acquire and apply medical knowledge at the point of care, emphasizing the need for training and curriculum changes to help ensure AI is used responsibly and that clinicians are equipped to maximize its potential. Chen, a medical student at the Kaiser Permanente Bernard J. Tyson School of Medicine, shares how he and his peers use AI tools as study aids, clinical tutors, and second opinions and reflects on the risks of overreliance and the importance of preserving critical thinking.







Learn more:



Perspectives on the Current and Future State of Artificial Intelligence in Medical Genetics (opens in new tab)&nbsp;(Cheatham)Publication | May 2025



Performance of ChatGPT on USMLE: Potential for AI-assisted medical education using large language models (opens in new tab) (Cheatham)&nbsp;Publication | February 2023&nbsp;



The AI Revolution in Medicine: GPT-4 and Beyond&nbsp;Book | Peter Lee, Carey Goldberg, Isaac Kohane | April 2023&nbsp;




	
		
			Subscribe to the Microsoft Research Podcast:		
		
							
					
						  
						Apple Podcasts
					
				
			
							
					
						
						Email
					
				
			
							
					
						
						Android
					
				
			
							
					
						
						Spotify
					
				
			
							
					
						
						RSS Feed
					
				
					
	




	
		
			
				
					

Transcript



[MUSIC]‚ÄØ‚ÄØ‚ÄØ‚ÄØ&nbsp;



[BOOK PASSAGE]&nbsp;



PETER LEE: ‚ÄúMedicine often uses the training approach when trying to assess multipurpose talent. To ensure students can safely and effectively take care of patients, we have them jump through quite a few hoops, &#8230; [and] they need good evaluations once they reach the clinic, passing grades on more exams like the USMLE [United States Medical Licensing Examination]. &#8230; [But] GPT-4 gets more than 90 percent of questions on licensing exams correct. &#8230; Does that provide any level of comfort in using GPT-4 in medicine?‚Äù&nbsp;



[END OF BOOK PASSAGE]‚ÄØ‚ÄØ‚ÄØ‚ÄØ&nbsp;



[THEME MUSIC]‚ÄØ‚ÄØ‚ÄØ‚ÄØ&nbsp;



This is The AI Revolution in Medicine, Revisited. I‚Äôm your host, Peter Lee.‚ÄØ‚ÄØ‚ÄØ‚ÄØ&nbsp;



Shortly after OpenAI&#8217;s GPT-4 was publicly released, Carey Goldberg, Dr. Zak Kohane, and I published The AI Revolution in Medicine to help educate the world of healthcare and medical research about the transformative impact this new generative AI technology could have. But because we wrote the book when GPT-4 was still a secret, we had to speculate. Now, two years later, what did we get right, and what did we get wrong?‚ÄØ‚ÄØ‚ÄØ‚ÄØ‚ÄØ&nbsp;



In this series, we‚Äôll talk to clinicians, patients, hospital administrators, and others to understand the reality of AI in the field and where we go from here.



				
				
					



[THEME MUSIC FADES]‚ÄØ&nbsp;



The book passage I read at the top is from Chapter 4, ‚ÄúTrust but Verify.‚Äù In it, we explore how AI systems like GPT-4 should be evaluated for performance, safety, and reliability and compare this to how humans are both trained and assessed for readiness to deliver healthcare.&nbsp;



In previous conversations with guests, we‚Äôve spoken a lot about AI in the clinic as well as in labs and companies developing AI-driven tools. We‚Äôve also talked about AI in the hands of patients and consumers. But there has been some discussion also about AI‚Äôs role in medical training. And, as a founding board member of a new medical school at Kaiser Permanente, I definitely have my own thoughts about this. But today, I‚Äôm excited to welcome two guests who represent the next generation of medical professionals for their insights, Morgan Cheatham and Daniel Chen.&nbsp;



Morgan Cheatham is a graduate of Brown University&#8217;s Warren Alpert Medical School with clinical training in genetics at Harvard and is a clinical fellow at Boston Children&#8217;s Hospital. While Morgan is a bona fide doctor in training, he‚Äôs also amazingly an influential health technology strategist. He was recently named partner and head of healthcare and life sciences at Breyer Capital and has led investments in several healthcare AI companies that have eclipsed multibillion-dollar valuations.&nbsp;&nbsp;



Daniel Chen is finishing his second year as a medical student at the Kaiser Permanente Bernard J. Tyson School of Medicine. He holds a neuroscience degree from the University of Washington and was a research assistant in the Raskind Lab at the UW School of Medicine, working with imaging and genetic data analyses for biomedical research. Prior to med school, Daniel pursued experiences that cultivated his interest in the application of AI in medical practice and education.&nbsp;&nbsp;



Daniel and Morgan exemplify the real-world future of healthcare, a student entering his third year of medical school and a fresh medical-school graduate who is starting a residency while at the same time continuing his work on investing in healthcare startups.&nbsp;



[TRANSITION MUSIC]‚ÄØ



Here is my interview with Morgan Cheatham:&nbsp;



LEE: Morgan, thanks for joining. Really, really looking forward to this chat.&nbsp;



MORGAN CHEATHAM: Peter, it&#8217;s a privilege to be here with you. Thank you.&nbsp;



LEE: So are there any other human beings who are partners at big-league venture firms, residents at, you know, a Harvard-affiliated medical center, author, editor for a leading medical journal? I mean, who are your ‚Ä¶ who&#8217;s your cohort? Who are your peers?&nbsp;



CHEATHAM: I love this question. There are so many people who I consider peers that I look up to who have paved this path. And I think what is distinct about each of them is they have this physician-plus orientation. They are multilingual in terms of knowing the language of medicine but having learned other disciplines. And we share a common friend, Dr. Zak Kohane, who was among the first to really show how you can meld two worlds as a physician and make significant contributions to the intersections thereof.&nbsp;&nbsp;



I also deeply, in the world of business, respect physicians like Dr. Krishna Yeshwant at Google Ventures, who simultaneously pursued residency training and built what is now, you know, a large and enduring venture firm.&nbsp;&nbsp;



So there are plenty of people out there who&#8217;ve carved their own path and become these multilingual beings, and I aspire to be one.&nbsp;



LEE: So, you know, one thing I&#8217;ve been trying to explore with people are their origins with respect to the technology of AI. And there&#8217;s two eras for that. There&#8217;s AI before ChatGPT and before, you know, generative AI really became a big thing, and then afterwards.&nbsp;&nbsp;



So let&#8217;s start first before ChatGPT. You know, what was your contact? What was, you know, your knowledge of AI and machine learning?&nbsp;



CHEATHAM: Sure, so my experiences in computer science date back to high school. I went to Thomas Jefferson, which is a high school in Alexandria, Virginia, that prides itself on requiring students to take computer science in their first year of high school as kind of a required torturous experience. [LAUGHTER] And I remember that fondly. Our final project was Brick Breaker. It was actually, I joke, all hard coded. So there was nothing intelligent about the Brick Breaker that we built. But that was my first exposure.&nbsp;&nbsp;



I was a classics nerd, and I was really interested in biology and chemistry as a pre-medical student. So I really wouldn&#8217;t intersect with this field again until I was shadowing at Inova Hospital, which was a local hospital near me. And it was interesting because, at the time‚ÄîI was shadowing in the anesthesia department‚Äîthey were actually implementing their first instance of Epic.&nbsp;



LEE: Mmm. Wow.&nbsp;



CHEATHAM: And I remember that experience fondly because the entire hospital was going from analog‚Äîthey were going from paper-based charts‚Äîto this new digital system. And I didn&#8217;t quite know in that moment what it would mean for the field or for my career, but I knew it was a big deal because a lot of people had a lot of emotion around what was going on, and it was in that experience that I kind of decided to attach myself to the intersection of computation and medicine. So when I got to undergrad, I was a pre-medical student. I was very passionate about studying the sacred physician-patient relationship and everything that had to go on in that exam room to provide excellent care.&nbsp;&nbsp;



But there were a few formative experiences: one, working at a physician-founded startup that was using at the time we called it big data, if you remember, ‚Ä¶&nbsp;



LEE: Yup.&nbsp;



CHEATHAM: ‚Ä¶ to match the right patient to the right provider at the right time. And it was in that moment that I realized that as a physician, I could utilize technology to scale that sacred one-to-one patient-provider interaction in nonlinear ways. So that was, kind of, the first experience where I saw deployed systems that were using patient data and clinical information in an intelligent format.&nbsp;



LEE: Yeah. And so you&#8217;re a pre-medical student, but you have this computer science understanding. You have an intuition, I guess is the right way to say it, that the clinical data becoming digital is going to be important. So then what happens from there to your path to medical school?&nbsp;



CHEATHAM: Yeah, so I had a few formative research experiences in my undergraduate years. You know, nothing that ever amounted to a significant publication, but I was toying around with SVMs [support vector machines] for sepsis and working with the MIMIC [Medical Information Mart for Intensive Care] database early days and really just trying to understand what it meant that medical data was becoming digitized.&nbsp;&nbsp;



And at the same time, again, I was rather unsatisfied doing that purely in an academic context. And I so early craved seeing how this would roll out in the wild, roll out in a clinical setting that I would soon occupy. And that was really what drove me to work at this company called Kyruus [Health] and understand how these systems, you know, scaled. Obviously, that&#8217;s something with AI that we&#8217;re now grappling with in a real way because it looks much different.&nbsp;&nbsp;



LEE: Right. Yep.&nbsp;



CHEATHAM: So the other experience I had, which is less relevant to AI, but I did do a summer in banking. And I mention this because what I learned in the experience was ‚Ä¶ it was a master class in business. And I learned that there was another scaling factor that I should appreciate as we think about medicine, and that was capital and business formation. And that was also something that could scale nonlinearly.&nbsp;&nbsp;



So when you married that with technology, it was, kind of, a natural segue for me before going to med school to think about venture capital and partnering with founders who were going to be building these technologies for the long term. And so that&#8217;s how I landed on the venture side.&nbsp;



LEE: And then how long of a break before you started your medical studies?&nbsp;



CHEATHAM: It was about four years. Originally, it was going to be a two-year deferral, and the pandemic happened. Our space became quite active in terms of new companies and investment. So it was about four years before I went back.&nbsp;



LEE: I see. And so you&#8217;re in medical school. ChatGPT happened while you were in medical school, is that right?&nbsp;



CHEATHAM: That&#8217;s right. That&#8217;s right. Right before I was studying for Step 1. So the funny story, Peter, that I like to share with folks is ‚Ä¶&nbsp;



LEE: Yeah.&nbsp;



CHEATHAM: ‚Ä¶ I was just embarking on designing my Step 1 study plan with my mentor. And I went to NeurIPS [Conference] for the first time. And that was in 2022, when, of course, ChatGPT was released.&nbsp;&nbsp;



And for the remainder of that fall period, you know, I should have been studying for these shelf exams and, you know, getting ready ‚Ä¶&nbsp;&nbsp;



LEE: Yeah.&nbsp;



CHEATHAM: ‚Ä¶ for this large board exam. And I was fortunate to partner, actually, with one of our portfolio company CEOs who is a physician‚Äîhe is an MD/PhD‚Äîto work on the first paper that showed that ChatGPT could pass the US Medical Licensing Exam (opens in new tab).&nbsp;&nbsp;



LEE: Yes.&nbsp;



CHEATHAM: And that was a riveting experience for a number of reasons. I joke with folks that it was both the best paper I was ever, you know, a part of and proud to be a coauthor of, but also the worst for a lot of reasons that we could talk about.&nbsp;&nbsp;



It was the best in terms of canonical metrics like citations, but the worst in terms of, wow, did we spend six months as a field thinking this was the right benchmark ‚Ä¶ [LAUGHTER]&nbsp;



LEE: Right.&nbsp;



CHEATHAM: ‚Ä¶ for how to assess the performance of these models. And I&#8217;m so encouraged ‚Ä¶&nbsp;



LEE: You shouldn&#8217;t feel bad that way because, you know, at that time, I was secretly, you know, assessing what we now know of as GPT-4 in that period. And what was the first thing I tried to do? Step 1 medical exam.&nbsp;&nbsp;



By the way, just for our listeners who don&#8217;t understand about medical education‚Äîin the US, there&#8217;s a three-part exam that extends over a couple of years of medical school. Step 1, Step 2, Step 3. And Step 1 and Step 2 in particular are multiple-choice exams.&nbsp;



And they are very high stakes when you&#8217;re in medical school. And you really have to have a command of quite a lot of clinical knowledge to pass these. And it&#8217;s funny to hear you say what you were just sharing there because it was also the first impulse I had with GPT-4. And in retrospect, I feel silly about that.&nbsp;



CHEATHAM: I think many of us do, but I&#8217;ve been encouraged over the last two years, to your point, that we really have moved our discourse beyond these exams to thinking about more robust systems for the evaluation of performance, which becomes even more interesting as you and I have spoken about these multi-agent frameworks that we are now, you know, compelled to explore further.&nbsp;



LEE: Yeah. Well, and even though I know you&#8217;re a little sheepish about it now, I think in the show notes, we&#8217;ll link to that paper because it really was one of the seminal moments when we think about AI, AI in medicine.



And so you&#8217;re seeing this new technology, and it&#8217;s happening at a moment when you yourself have to confront taking the Step 1 exam. So how did that feel?&nbsp;



CHEATHAM: It was humbling. It was shocking. What I had worked two years for, grueling over textbooks and, you know, flashcards and all of the things we do as medical students, to see a system emerge out of thin air that was arguably going to perform far better than I ever would, no matter how much ‚Ä¶&nbsp;&nbsp;



LEE: Yeah.&nbsp;



CHEATHAM: ‚Ä¶ I was going to study for that exam, it set me back. It forced me to interrogate what my role in medicine would be. And it dramatically changed the specialties that I considered for myself long term.&nbsp;&nbsp;



And I hope we talk about, you know, how I stumbled upon genetics and why I&#8217;m so excited about that field and its evolution in this computational landscape. I had to do a lot of soul searching to relinquish what I thought it meant to be a physician and how I would adapt in this new environment.&nbsp;



LEE: You know, one of the things that we wrote in our book, I think it was in a chapter that I contributed, I was imagining that students studying for Step 1 would be able to do it more actively.&nbsp;&nbsp;



Or you could even do sort of a pseudo-Step 3 exam by having a conversation. You provide the presentation of a patient and then have an encounter, you know, where the ChatGPT is the patient, and then you pretend to be the doctor.&nbsp;And then in the example that we published, then you say, ‚ÄúEnd of encounter.‚Äù And then you ask ChatGPT for an assessment of things.&nbsp;&nbsp;



So, you know, maybe it all came too late for Step 1 for you because you were already very focused and, you know, had your own kind of study framework. But did you have an influence or use of this kind of technology for Step 2 and Step 3?&nbsp;



CHEATHAM: So even for Step 1, I would say, it [ChatGPT], you know, dropped in November. I took it [Step 1 exam] in the spring, so I was able to use it to study. But the lesson I learned in that moment, Peter, was really about the importance of trust with AI and clinicians or clinicians in training, because we all have the same resources that we use for these board exams, right. UWorld is this question bank. It&#8217;s been around forever. If you&#8217;re not using UWorld, like, good luck. And so why would you deviate off of a well-trodden path to study for this really important exam?&nbsp;&nbsp;



And so I kind of adjunctively used GPT alongside UWorld to come up with more personalized explanations for concepts that I wasn&#8217;t understanding, and I found that it was pretty good and it was certainly helpful for me.&nbsp;&nbsp;



Fortunately, I was, you know, able to pass, but I was very intentional about dogfooding AI when I was a medical student, and part of that was because I had been a venture capitalist, and I&#8217;d made investments in companies whose products I could actually use.&nbsp;&nbsp;



And so, you know, Abridge is a company in the scribing space that you and I have talked about.&nbsp;&nbsp;



LEE: Yeah.&nbsp;



CHEATHAM: I was so fortunate in the early days of their product to not just be a user but to get to bring their product across the hospital. I could bring the product to the emergency department one week, to neurology another week, to the PICU [pediatric intensive care unit], you know, the next week, and assess the relative performance of, you know, how it handled really complex genetics cases ‚Ä¶&nbsp;



LEE: Yeah.&nbsp;



CHEATHAM: ‚Ä¶ versus these very challenging social situations that you often find yourself navigating in primary care. So not only was I emotional about this technology, but I was a voracious adopter in the moment.&nbsp;



LEE: Yeah, right. And you had a financial interest then on top of that, right?&nbsp;



CHEATHAM: I was not paid by Abridge to use the product, but, you know, I joke that the team was probably sick of me. [LAUGHTER]&nbsp;



LEE: No, no, but you were working for a venture firm that was invested in these, right? So all of these things are wrapped up together. You know, you&#8217;re having to get licensed as a doctor while doing all of this.&nbsp;&nbsp;



So I want to get into that investment and new venture stuff there, but let&#8217;s stick just for a few more minutes on medical education. So I mentioned, you know, what we wrote in the book, and I remember writing the example, you know, of an encounter. Is that at all realistic? Is anything like that ‚Ä¶ that was pure speculation on our part. What&#8217;s really happening?&nbsp;&nbsp;



And then after we talk about what&#8217;s really happening, what do you think should happen in medical education given the reality of generative AI?&nbsp;



CHEATHAM: I&#8217;ve been pleasantly surprised talking with my colleagues about AI in clinical settings, how curious people are and how curious they&#8217;ve been over the last two years. I think, oftentimes, we say, oh, you know, this technology really is stratified by age and the younger clinicians are using it more and the older physicians are ignoring it. And, you know, maybe that&#8217;s true in some regards, but I&#8217;ve seen, you know, many, you know, senior attendings pulling up Perplexity, GPT, more recently OpenEvidence (opens in new tab), which has been a really essential tool for me personally at the point of care, to come up with the best decisions for our patients.&nbsp;&nbsp;



The general skepticism arises when people reflect on their own experience in training and they think, ‚ÄúWell, I had to learn how to do it this way.‚Äù&nbsp;&nbsp;



LEE: Yeah.&nbsp;



CHEATHAM: ‚ÄúAnd therefore, you using an AI scribe to document this encounter doesn&#8217;t feel right to me because I didn&#8217;t get to do that.‚Äù And I did face some of those critiques or criticisms, where you need to learn how to do it the old-school way first and then you can use an AI scribe.&nbsp;&nbsp;



And I haven&#8217;t yet seen‚Äîmaybe even taking a step back‚ÄîI haven&#8217;t seen a lot of integration of AI into the core medical curriculum, period.&nbsp;&nbsp;



LEE: Yeah.&nbsp;



CHEATHAM: And, as you know, if you want to add something to medical school curriculum, you can get in a long line of people who also want to do that.&nbsp;



LEE: Yes. Yeah.



CHEATHAM: But it is urgent that our medical schools do create formalized required trainings for this technology because people are already using it.&nbsp;&nbsp;



LEE: Yes.&nbsp;



CHEATHAM: I think what we will need to determine is how much of the old way do people need to learn in order to earn the right to use AI at the point of care and how much of that old understanding, that prior experience, is essential to be able to assess the performance of these tools and whether or not they are having the intended outcome.&nbsp;&nbsp;



I kind of joke it&#8217;s like learning cursive, right?&nbsp;



LEE: Yes.&nbsp;



CHEATHAM: I&#8217;m old enough to have had to learn cursive. I don&#8217;t think people really have to learn it these days. When do I use it? Well, when I&#8217;m signing something. I don&#8217;t even really sign checks anymore, but &#8230;&nbsp;



LEE: Well ‚Ä¶ the example I&#8217;ve used, which you&#8217;ve heard, is, I&#8217;m sure you were still taught the technique of manual palpation, even though ‚Ä¶&nbsp;



CHEATHAM: Of course.&nbsp;



LEE: ‚Ä¶ you have access to technologies like ultrasound. And in fact, you would use ultrasound in many cases.&nbsp;&nbsp;



And so I need to pin you down. What is your opinion on these things? Do you need to be trained in the old ways?&nbsp;



CHEATHAM: When it comes to understanding the architecture of the medical note, I believe it is important for clinicians in training to know how that information is generated, how it&#8217;s characterized, and how to go from a very broad-reaching conversation to a distilled clinical document that serves many functions.&nbsp;



Does that mean that you should be forced to practice without an AI scribe for the entirety of your medical education? No. And I think that as you are learning the architecture of that document, you should be handed an AI scribe and you should be empowered to have visits with patients both in an analog setting‚Äîwhere you are transcribing and generating that note‚Äîand soon thereafter, I&#8217;m talking in a matter of weeks, working with an AI scribe. That&#8217;s my personal belief.&nbsp;&nbsp;



LEE: Yeah, yeah. So you&#8217;re going to &#8230; well, first off, congratulations on landing a residency at Boston Children&#8217;s [Hospital].&nbsp;&nbsp;



CHEATHAM: Thank you, Peter.&nbsp;



LEE: I understand there were only two people selected for this and super competitive. You know, with that perspective, you know, how do you see your future in medicine, just given everything that&#8217;s happening with AI right now?&nbsp;&nbsp;



And are there things that you would urge, let&#8217;s say, the dean of the Brown Medical School to consider or to change? Or maybe not the dean of Brown but the head of the LCME [Liaison Committee on Medical Education], the accrediting body for US medical schools. What in your mind needs to change?&nbsp;



CHEATHAM: Sure. I&#8217;ll answer your first question first and then talk about the future.&nbsp;&nbsp;



LEE: Yeah.&nbsp;



CHEATHAM: For me personally, I fell into the field of genomics. And so my training program will cover both pediatrics as well as clinical genetics and genomics.&nbsp;&nbsp;



And I alluded to this earlier, but one of the reasons I&#8217;m so excited to join the field is because I really feel like the field of genetics not only is focused on a very underserved patient population, but not in how we typically think of underserved. I&#8217;m talking about underserved as in patients who don&#8217;t always have answers. Patients for whom the current guidelines don&#8217;t offer information or comfort or support.&nbsp;



Those are patients that are extremely underserved. And I think in this moment of AI, there&#8217;s a unique opportunity to utilize the computational systems that we now have access to, to provide these answers more precisely, more quickly.&nbsp;&nbsp;



And so I&#8217;m excited to marry those two fields. And genetics has long been a field that has adopted technology. We just think about the foundational technology of genomic sequencing and variant interpretation. And so it&#8217;s a kind of natural evolution of the field, I believe, to integrate AI and specifically generative AI.&nbsp;



If I were speaking directly to the LCME, I mean, I would just have to encourage the organization, as well as medical societies who partner with attending physicians across specialties, to lean in here.



When I think about prior revolutions in technology and medicine, physicians were not always at the helm. We have a unique opportunity now, and you talk about companies like Abridge in the AI space, companies like Viz.ai, Cleerly‚ÄîI mean, I could go on: Iterative Health ‚Ä¶ I could list 20 organizations that are bringing AI to the point of care that are founded by physicians.



This is our moment to have a seat at the table and to shape not only the discourse but the deployment. And the unique lens, of course, that a physician brings is that of prioritizing the patient, and with AI and this time around, we have to do that.



LEE: So LCME for our listeners is, I think it stands for the Liaison Committee on Medical Education (opens in new tab). It&#8217;s basically the accrediting body for US medical schools, and it&#8217;s very high stakes. It&#8217;s very, very rigorous, which I think is a good thing, but it&#8217;s also a bit of a straitjacket.&nbsp;&nbsp;



So if you are on the LCME, are there specific new curricular elements that you would demand that LCME, you know, add to its accreditation standards?&nbsp;



CHEATHAM: We need to unbundle the different components of the medical appointment and think about the different functions of a human clinician to answer that question.&nbsp;&nbsp;



There are a couple of areas that are top of mind for me, the first being medical search. There are large organizations and healthcare incumbents that have been around for many decades, companies like UpToDate or even, you know, the guidelines that are housed by our medical societies, that need to respond to the information demands of clinicians at the point of care in a new way with AI.&nbsp;&nbsp;



And so I would love to see our medical institutions teaching more students how to use AI for medical search problems at the point of care. How to not only, you know, from a prompt perspective, ask questions about patients in a high-efficacy way, but also to interpret the outputs of these systems to inform downstream clinical decision-making.&nbsp;&nbsp;



People are already adopting, as you know, GPT, OpenEvidence, Perplexity, all of these tools to make these decisions now.&nbsp;&nbsp;



And so by not‚Äîagain, it&#8217;s a moral imperative of the LCME‚Äîby not having curriculum and support for clinicians doing that, we run the risk of folks not utilizing these tools properly but also to their greatest potential.&nbsp;



LEE: Yeah, then, but zooming forward then, what about board certification?&nbsp;



CHEATHAM: Board certification today is already transitioning to an open-book format for many specialties, is my understanding. And in talking to some of my fellow geneticists, who, you know, that&#8217;s a pretty challenging board exam in clinical genetics or biochemical genetics. They are using OpenEvidence during those open-book exams.&nbsp;&nbsp;



So what I would like to see us do is move from a system of rote memorization and regurgitation of fact to an assessment framework that is adaptive, is responsive, and assesses for your ability to use the tools that we now have at our disposal to make sound clinical decisions.&nbsp;



LEE: Yeah. We&#8217;ve heard from Sara Murray, you know, that when she&#8217;s doing her rounds, she consults routinely with ChatGPT. And that was something we also predicted, especially Carey Goldberg in our book, you know, wrote this fictional account.&nbsp;&nbsp;



Is that the primary real-world use of AI? Not only by clinicians, but also by medical students &#8230; are medical students, you know, engaged with ChatGPT or, you know, similar?&nbsp;



CHEATHAM: Absolutely. I&#8217;ve listed some of the tools. I think there, in general, Peter, there is this new clinician stack that is emerging of these tools that people are trying, and I think the cycles of iteration are quick, right. Some folks are using Claude [Claude.ai] one week, and they&#8217;re trying Perplexity, or they&#8217;re trying OpenEvidence, they&#8217;re trying GPT for a different task.¬†¬†



There&#8217;s this kind of moment in medicine that every clinician experiences where you&#8217;re on rounds, and there&#8217;s that very senior attending. And you&#8217;ve just presented a patient to them, and you think you did an elegant job, and you&#8217;ve summarized all the information, and you really feel good about your differential, and they ask you, like, the one question you didn&#8217;t think to address. [LAUGHTER]&nbsp;



And I&#8217;ll tell you, some of the funniest moments I&#8217;ve had using AI in the hospital has been, and let me take a step back, that process of an attending physician interrogating a medical student is called ‚Äúpimping,‚Äù for lack of a better phrase.&nbsp;&nbsp;



And some of the funniest use cases I&#8217;ve had for AI in that setting is actually using OpenEvidence or GPT as defense against pimping. [LAUGHTER] So quickly while they&#8217;re asking me the question, I put it in, and I&#8217;m actually able to answer it right away. So it&#8217;s been effective for that. But I would say, you know, [in] the halls of most of the hospitals where I&#8217;ve trained, I&#8217;m seeing this technology in the wild.



LEE: So now you&#8217;re so tech-forward, but that off-label use of AI, we also, when we wrote our book, we weren&#8217;t sure that at least top health systems would tolerate this. Do you have an opinion about this? Should these things be better regulated or controlled by the CIOs of Boston Children&#8217;s?&nbsp;



CHEATHAM: I&#8217;m a big believer that transparency encourages good behaviors.&nbsp;



And so the first time I actually tried to use ChatGPT in a clinical setting, it was at a hospital in Rhode Island. I will not name which hospital. But the site was actually blocked. I wasn&#8217;t able to access it from a desktop. That was the hospital&#8217;s first response to this technology was, let&#8217;s make sure none of our clinicians can access it. It has so much potential for medicine. The irony of that today.&nbsp;&nbsp;



And it&#8217;s since, you know, become unblocked. But I was able to use it on my phone. So, to your point, if there&#8217;s a will, there&#8217;s a way. And we will utilize this technology if we are seeing perceived value.&nbsp;



LEE: So, yeah, no, absolutely. So now, you know, in some discussions, one superpower that seems to be common across people who are really leading the charge here is they seem to be very good readers and students.&nbsp;&nbsp;



And I understand you also as a voracious reader. In fact, you&#8217;re even on an editorial team for a major medical journal. To what extent does that help?&nbsp;&nbsp;



And then from your vantage point at New England Journal of Medicine AI‚Äîand I&#8217;ll have a chance to ask Zak Kohane as the editor in chief the same question‚Äîyou know, what&#8217;s your assessment as you reflect over the last two years for the submitted manuscripts? Are you overall surprised at what you&#8217;re seeing? Disappointed? Any kind of notable hits or misses, just in the steady stream of research papers that are getting submitted to that leading journal?&nbsp;



CHEATHAM: I would say overall, the field is becoming more expansive in the kinds of questions that people are asking.&nbsp;&nbsp;



Again, when we started, it was this very myopic approach of: ‚ÄúCan we pass these medical licensing exams? Can we benchmark this technology to how we benchmark our human clinicians?‚Äù I think that&#8217;s a trap. Some folks call this the Turing Trap, right, of let&#8217;s just compare everything to what a human is capable of.&nbsp;&nbsp;



Instead of interrogating what is the unique, as you all talk about in the book, what are the unique attributes of this new substrate for computation and what new behaviors emerge from it, whether that&#8217;s from a workflow perspective in the back office, or‚Äîas I&#8217;m personally more passionate and as we&#8217;re seeing more people focus on in the literature‚Äîwhat are the diagnostic capabilities, right.&nbsp;



I love Eric Topol&#8217;s framework for ‚Äúmachine eyes,‚Äù right, as this notion of like, yes, we as humans have eyes, and we have looked at medical images for many, many decades, but these machines can take a different approach to a retinal image, right.&nbsp;&nbsp;



It&#8217;s not just what you can diagnose in terms of an ophthalmological disease but maybe a neurologic disease or, you know, maybe liver disease, right.&nbsp;



So I think the literature is, in general, moving to this place of expansion, and I&#8217;m excited by that.&nbsp;



LEE: Yeah, I kind of have referred to that as triangulation. You know, one of the things I think a trap that specialists in medicine can fall into, like a cardiologist will see everything in terms of the cardiac system. And ‚Ä¶ whereas a nephrologist will see things in a certain lens.&nbsp;&nbsp;



And one of the things that you oftentimes see in the responses from a large language model is that more expansive view. At the same time, you know, I wonder ‚Ä¶ we have medical specialties for good reason. And, you know, at times I do wonder, you know, if there can be confusion that builds up.&nbsp;&nbsp;



CHEATHAM: This is an under-discussed area of AI‚ÄîAI collapses medical specialties onto themselves, right.&nbsp;



You have the canonical example of the cardiologist, you know, arguing that, you know, we should diuresis and maybe the nephrologist arguing that we should, you know, protect the kidneys. And how do two disciplines disagree on what is right for the patient when in theory, there is an objective best answer given that patient&#8217;s clinical status?&nbsp;



My understanding is that the emergence of medical specialties was a function of the cognitive overload of medicine in general and how difficult it was to keep all of the specifics of a given specialty in the mind. Of course, general practitioners are tasked with doing this at some level, but they&#8217;re also tasked with knowing when they&#8217;ve reached their limit and when they need to refer to a specialist.&nbsp;&nbsp;



So I&#8217;m interested in this question of whether medical specialties themselves need to evolve.&nbsp;&nbsp;



And if we look back in the history of medical technology, there are many times where a new technology forced a medical specialty to evolve, whether it was certain diagnostic tools that have been introduced or, as we&#8217;re seeing now with GLP-1s, the entire cardiometabolic field ‚Ä¶&nbsp;



LEE: Right.&nbsp;



CHEATHAM: ‚Ä¶ is having to really reimagine itself with these new tools. So I think AI will look very similar, and we should not hold on to this notion of classical medical specialties simply out of convention.&nbsp;&nbsp;



LEE: Right. All right. So now you&#8217;re starting your residency. You&#8217;re, you know, basically leading a charge in health and life sciences for a leading venture firm. I&#8217;d like you to predict what the world of healthcare is going to look like, you know, two years from now, five years from now, 10 years from now. And to frame that, to make it a little more specific, you know, what do you think will be possible that you, as a doctor and an investor, will be able to do two years from now, five years from now, 10 years from now that you can&#8217;t do today?&nbsp;&nbsp;



CHEATHAM: Two years from now, I&#8217;m optimistic we&#8217;ll have greater adoption of AI by clinicians, both for back-office use cases. So whether that&#8217;s the scribe and the generation of the note for billing purposes, but also now thinking about that for patient-facing applications.&nbsp;&nbsp;



We&#8217;re already doing this with drafting of notes. I think we&#8217;ll see greater proliferation of those more obvious use cases over the next two years. And hopefully we&#8217;re seeing that across hospital systems, not just large well-funded academics, but really reaching our community hospitals, our rural hospitals, our under-resourced settings.&nbsp;&nbsp;



I think hopefully we&#8217;ll see greater conversion. Right now, we have this challenge of ‚Äúpilotitis,‚Äù right. A lot of people are trying things, but the data shows that only one in three pilots are really converting to production use. So hopefully we&#8217;ll kind of move things forward that are working and pare back on those that are not.&nbsp;



We will not solve the problem of payment models in the next two years. That is a prediction I have.&nbsp;&nbsp;



Over the next five years, I suspect that, with the help of regulators, we will identify better payment mechanisms to support the adoption of AI because it cannot and will not sustain itself simply by asking health systems and hospitals to pay for it. That is not a scalable solution.&nbsp;&nbsp;



LEE: Yes. Right. Yep. In fact, I think there have to be new revenue-positive incentives if providers are asked to do more in the adoption of technology.&nbsp;



CHEATHAM: Absolutely. But as we appreciate, some of the most promising applications of AI have nothing to do with revenue. It might simply be providing a diagnosis to somebody, you know, for whom that might drive additional intervention, but may also not.&nbsp;&nbsp;



And we have to be OK with that because that&#8217;s the right thing to do. It&#8217;s our moral imperative as clinicians to implement this where it provides value to the patient.



Over the next 10 years, what I‚Äîagain, being a techno-optimist‚Äîam hopeful we start to see is a dissolving of the barrier that exists between care delivery and biomedical discovery.&nbsp;&nbsp;



This is the vision of the learning health system that was written over 10 years ago, and we have not realized it in practice. I&#8217;m a big proponent of ensuring that every single patient that enters our healthcare system not only receives the best care, but that we learn from the experiences of that individual to help the next.&nbsp;



And in our current system, that is not how it works. But, with AI, that now becomes possible.&nbsp;



LEE: Well, I think connecting healthcare experiences to medical discovery‚ÄîI think that that is really such a great vision for the future. And I do agree [that] AI really gives us real hope that we can make it true.&nbsp;&nbsp;



Morgan, I think we could talk for a few hours more. It&#8217;s just incredible what you&#8217;re up to nowadays. Thank you so much for this conversation. I&#8217;ve learned a lot talking to you.&nbsp;



CHEATHAM: Peter, thank you so much for your time. I will be clutching my signed copy of The AI Revolution in Medicine for many years to come.&nbsp;&nbsp;



[TRANSITION MUSIC]&nbsp;



LEE: Morgan obviously is not an ordinary med school graduate. In previous episodes, one of the things we&#8217;ve seen is that people on the leading edge of real-world AI in medicine oftentimes are both practicing doctors as well as technology developers. Morgan is another example of this type of polymath, being both a med student and a venture capitalist.&nbsp;



One thing that struck me about Morgan is he&#8217;s just incredibly hands-on. He goes out, finds leading-edge tools and technologies, and often these things, even though they&#8217;re experimental, he takes them into his education and into his clinical experiences. I think this highlights a potentially important point for medical schools, and that is, it might be incredibly important to provide the support‚Äîand, let&#8217;s be serious, the permission‚Äîto students to access and use new tools and technologies. Indeed, the insight for me when I interact with Morgan is that in these early days of AI in medicine, there is no substitute for hands-on experimentation, and that is likely best done while in medical school.



Here&#8217;s my interview with Daniel Chen:&nbsp;



LEE: Daniel, it&#8217;s great to have you here.&nbsp;



DANIEL CHEN: Yeah, it&#8217;s a pleasure being here.&nbsp;



LEE: Well, you know, I normally get started in these conversations by asking, you know, how do you explain to your mother what you do all day? And the reason that that&#8217;s a good question is a lot of the people we have on this podcast have fancy titles and unusual jobs, but I&#8217;m guessing that your mother would have already a preconceived notion of what a medical student does. So I&#8217;d like to twist the question around a little bit for you and ask, what does your mother not realize about how you spend your days at school?&nbsp;&nbsp;



Or does she get it all right? [LAUGHS]&nbsp;



CHEN: Oh, no, she is very observant. I&#8217;ll say that off the bat. But I think something that she might not realize, is the amount of efforts spent, kind of, outside the classroom or outside the hospital. You know, she&#8217;s always, like, saying you have such long days in the hospital. You&#8217;re there so early in the morning.&nbsp;&nbsp;



But what she doesn&#8217;t realize is that maybe when I come back from the hospital, it&#8217;s not just like, oh, I&#8217;m done for the day. Let&#8217;s wind down, go to bed. But it&#8217;s more like, OK, I have some more practice questions I need to get through; I didn&#8217;t get through my studying. Let me write on, like wrap up this research project I&#8217;m working on, get that back to the PI [principal investigator]. It&#8217;s never ending to a certain extent. Those are some things she doesn&#8217;t realize.&nbsp;



LEE: Yeah, I think, you know, all the time studying, I think, is something that people expect of second-year medical students. And even nowadays at the top medical schools like this one, being involved in research is also expected.&nbsp;&nbsp;



I think one thing that is a little unusual is that you are actually in clinic, as well, as a second-year student. How has that gone for you?&nbsp;



CHEN: Yeah, I mean, it&#8217;s definitely interesting. I would say I spend my time, especially this year, it&#8217;s kind of three things. There&#8217;s the preclinical stuff I&#8217;m doing. So that&#8217;s your classic, you know, you&#8217;re learning from the books, though I don&#8217;t feel like many of us do have textbooks anymore. [LAUGHTER]&nbsp;



There&#8217;s the clinical aspect, which you mentioned, which is we have an interesting model, longitudinal integrated clerkships. We can talk about that. And the last component is the research aspect, right. The extracurriculars.&nbsp;&nbsp;



But I think starting out as a second year and doing your rotations, probably early on in, kind of, the clinical medical education, has been really interesting, especially with our format, because typically med students have two years to read up on all the material and, like, get some foundational knowledge. With us, it&#8217;s a bit more, we have maybe one year under our belt before we&#8217;re thrown into like, OK, go talk to this patient; they have ankle pain, right. But we might have not even started talking about ankle pain in class, right. Well, where do I begin?&nbsp;&nbsp;



So I think starting out, it&#8217;s kind of, like, you know, the classic drinking from a fire hydrant. But you also, kind of, have that embarrassment of you&#8217;re talking to the patient like, I have no clue what&#8217;s happening [LAUGHTER] or you might have ‚Ä¶ my differentials all over the place, right.&nbsp;&nbsp;



But I think the beauty of the longitudinal aspect is that now that we&#8217;re, like, in our last trimester, everything&#8217;s kind of coming together. Like, OK, I can start to see, you know, here&#8217;s what you&#8217;re telling me. Here&#8217;s what the physical exam findings are. I&#8217;m starting to form a differential. Like, OK, I think these are the top three things.&nbsp;



But in addition to that, I think these are the next steps you should take so we can really focus and hone in on what exact diagnosis this might be.&nbsp;



LEE: All right. So, of course, what we&#8217;re trying to get into is about AI.&nbsp;&nbsp;



And, you know, the funny thing about AI and the book that Carey, Zak, and I wrote is we actually didn&#8217;t think too much about medical education, although we did have some parts of our book where we, well, first off, we made the guess that medical students would find AI to be useful. And we even had some examples, you know, where, you know, you would have a vignette of a mythical patient, and you would ask the AI to pretend to be that patient.&nbsp;&nbsp;



And then you would have an interaction and have to have an encounter. And so I want to delve into whether any of that is happening. How real it is. But before we do that, let&#8217;s get into first off, your own personal contact with AI. So let me start with a very simple question. Do you ever use generative AI systems like ChatGPT or similar?&nbsp;



CHEN: All the time, if not every day.&nbsp;



LEE: [LAUGHS] Every day, OK. And when did that start?&nbsp;



CHEN: I think when it first launched with GPT-3.5, I was, you know, curious. All my friends work in tech. You know, they&#8217;re either software engineers, PMs. They&#8217;re like, ‚ÄúHey, Daniel, take a look at this,‚Äù and at first, I thought it was just more of a glorified search engine. You know, I was actually looking back.&nbsp;&nbsp;



My first question to ChatGPT was, what was the weather going to be like the next week, you know? Something very, like, something easily you could have looked up on Google or your phone app, right.&nbsp;&nbsp;



I was like, oh, this is pretty cool. But then, kind of, fast-forwarding to, I think, the first instance I was using it in med school. I think the first, like, thing that really helped me was actually a coding problem. It was for a research project. I was trying to use SQL.&nbsp;&nbsp;



Obviously, I&#8217;ve never taken a SQL class in my life. So I asked Chat like, ‚ÄúHey, can you write me this code to maybe morph two columns together,‚Äù right? Something that might have taken me hours to maybe Google on YouTube or like try to read some documentation which just goes through my head.



But ChatGPT was able to, you know, not only produce the code, but, like, walk me through like, OK, you&#8217;re going to launch SQL. You&#8217;re going to click on this menu, [LAUGHTER] put the code in here, make sure your file names are correct. And it worked.&nbsp;&nbsp;



So it&#8217;s been a very powerful tool in that way in terms of, like, giving me expertise in something that maybe I traditionally had no training in.&nbsp;



LEE: And so while you&#8217;re doing this, I assume you had fellow students, friends, and others. And so what were you observing about their contact with AI? I assume you weren&#8217;t alone in this.&nbsp;



CHEN: Yeah, yeah, I think, &#8230; I&#8217;m not too sure in terms of what they were doing when it first came out, but I think if we were talking about present day, um, a lot of it&#8217;s kind of really spot on to what you guys talked about in the book.&nbsp;&nbsp;



Um, I think the idea around this personal tutor, personal mentor, is something that we&#8217;re seeing a lot. Even if we&#8217;re having in-class discussions, the lecturer might be saying something, right. And then I might be or I see a friend in ChatGPT or some other model looking up a question.&nbsp;&nbsp;



And you guys talked about, you know, how it can, like, explain a concept at different levels, right. But honestly, sometimes if there&#8217;s a complex topic, I ask ChatGPT, like, can you explain this to me as if I was a 6-year-old?&nbsp;&nbsp;



LEE: Yeah. [LAUGHS]&nbsp;&nbsp;



CHEN: Breaking down complex topics. Yeah. So I think it&#8217;s something that we see in the pre-clinical space, in lecture, but also even in the clinical space, there&#8217;s a lot of teaching, as well.&nbsp;



Sometimes if my preceptor is busy with patients, but I had maybe a question, I would maybe converse with ChatGPT, like, ‚ÄúHey, what are your thoughts about this?‚Äù Or, like, a common one is, like, medical doctors love to use abbreviations, ‚Ä¶&nbsp;



LEE: Yes.&nbsp;&nbsp;



CHEN: ‚Ä¶ and these abbreviations are sometimes only very niche and unique to their specialty, right. [LAUGHTER]&nbsp;



And I was reading this note from a urogynecologist. [In] the entire first sentence, I think there were, like, 10 abbreviations. Obviously, I compile lists and ask ChatGPT, like, ‚ÄúHey, in the context of urogynecology, can you define what these could possibly mean,‚Äù right? Instead of hopelessly searching in a Google or maybe, embarrassing, asking the preceptor. So in these instances, it&#8217;s played a huge role.&nbsp;



LEE: Yeah. And when you&#8217;re doing things like that, it can make mistakes. And so what are your views of the reliability of generative AI, at least in the form of ChatGPT?&nbsp;



CHEN: Yeah, I think into the context of medicine, right, we fear a lot about the hallucinations that these models might have. And it&#8217;s something I&#8217;m always checking for. When I talk with peers about this, we find it most helpful when the model gives us a source linking it back. I think the gold standard nowadays in medicine is using something called UpToDate (opens in new tab) that&#8217;s written by clinicians, for clinicians.&nbsp;



But sometimes searching on UpToDate can be a lot of time as well because it&#8217;s a lot of information to, like, sort through. But nowadays a lot of us are using something called OpenEvidence, which is also an LLM. But they always cite their citations with, like, published literature, right.&nbsp;&nbsp;



So I think being able to be conscious of the downfalls of these models and also being able to have the critical understanding of, like, analyzing the actual literature. I think double checking is just something that we&#8217;ve been also getting really good at.&nbsp;



LEE: How would you assess student attitudes‚Äîmed student attitudes‚Äîabout AI? Is it ‚Ä¶ the way you&#8217;re coming across is it&#8217;s just a natural part of life. But do people have firm opinions, you know, pro or con, when it comes to AI, and especially AI in medicine?&nbsp;



CHEN: I think it&#8217;s pretty split right now. I think there&#8217;s the half, kind of, like us, where we&#8217;re very optimistic‚Äîcautiously optimistic about, you know, the potential of this, right. It&#8217;s able to, you know, give us that extra information, of being that extra tutor, right. It&#8217;s also able to give us information very quickly, as well.&nbsp;&nbsp;&nbsp;



But I think the other flip side of what a lot of students hesitate to, which I agree, is this loss of the ability to critically think. Something that you can easily do is, you know, give these models, like, relevant information about the patient history and be like, ‚ÄúGive me a 10-list differential,‚Äù right.



LEE: Yes.&nbsp;&nbsp;



CHEN: And I think it&#8217;s very easy as a student to, you know, [say], ‚ÄúThis is difficult. Let me just use what the model says, and we&#8217;ll go with that,‚Äù right.&nbsp;



So I think being able to separate that, you know, medical school is a time where, you know, you&#8217;re learning to become a good doctor. And part of that requires the ability to be observant and critically think. Having these models simultaneously might hinder the ability to do that.&nbsp;&nbsp;



LEE: Yeah.&nbsp;



CHEN: So I think, you know, the next step is, like, these models can be great‚Äîa great tool, absolutely wonderful. But how do you make sure that it&#8217;s not hindering these abilities to critically think?&nbsp;



LEE: Right. And so when you&#8217;re doing your LIC [longitudinal integrated clerkship] work, these longitudinal experiences, and you&#8217;re in clinic, are you pulling the phone out of your pocket and consulting with AI?&nbsp;



CHEN: Definitely. And I think my own policy for this, to kind of counter this, is that the night before when I&#8217;m looking over the patient list, the clinic [schedule] of who&#8217;s coming, I&#8217;m always giving it my best effort first.&nbsp;&nbsp;



Like, OK, the chief complaint is maybe just a runny nose for a kid in a pediatric clinic. What could this possibly be? Right? At this point, we&#8217;ve seen a lot. Like, OK, it could be URI [upper respiratory infection], it could be viral, it could be bacterial, you know, and then I go through the‚Äîyou know, I try to do my due diligence of, like, going through the history and everything like that, right.&nbsp;



But sometimes if it&#8217;s a more complex case, something maybe a presentation I&#8217;ve never seen before, I&#8217;ll still kind of do my best coming up with maybe a differential that might not be amazing. But then I&#8217;ll ask, you know, ChatGPT like, OK, in addition to these ideas, what do you think?&nbsp;&nbsp;



LEE: Yeah.&nbsp;



CHEN: Am I missing something? You know, and usually, it gives a pretty good response.&nbsp;



LEE: You know, that particular idea is something that I think Carey, Zak, and I thought would be happening a lot more today than we&#8217;re observing. And it&#8217;s the idea of a second set of eyes on your work. And somehow, at least our observation is that that isn&#8217;t happening quite as much by today as we thought it might.&nbsp;&nbsp;



And it just seems like one of the really safest and most effective use cases. When you go and you&#8217;re looking at yourself and other fellow medical students, other second-year students, what do you see when it comes to the ‚Äúsecond set of eyes‚Äù idea?&nbsp;



CHEN: I think, like, a lot of students are definitely consulting ChatGPT in that regard because, you know, even in the very beginning, we&#8217;re taught to be, like, never miss these red flags, right. So these red flags are always on our differential, but sometimes, it can be difficult to figure out where to place them on that, right.&nbsp;&nbsp;



So I think in addition to, you know, coming up with these differentials, something I&#8217;ve been finding a lot of value [in] is just chatting with these tools to get their rationale behind their thinking, you know.&nbsp;&nbsp;



Something I find really helpful‚ÄîI think this is also a part of the, kind of, art of medicine‚Äîis figuring out what to order, right, what labs to order.&nbsp;&nbsp;



LEE: Right.



CHEN: Obviously, you have your order sets that automate some of the things, like in the ED [emergency department], or, like, there are some gold standard imaging things you should do for certain presentations.&nbsp;



But then you chat to, like, 10 different physicians on maybe the next steps after that, and they give you 10 different answers.&nbsp;&nbsp;



LEE: Yes.&nbsp;&nbsp;



CHEN: But there&#8217;s never &#8230; I never understand exactly why. It&#8217;s always like, I&#8217;ve just been doing this for all my training, or that&#8217;s how I was taught.&nbsp;&nbsp;



So asking ChatGPT, like, ‚ÄúWhy would you do this next?‚Äù Or, like, ‚ÄúIs this a good idea?‚Äù And seeing the pros and cons has also been really helpful in my learning.&nbsp;



LEE: Yeah, wow, that&#8217;s super interesting. So now, you know, I&#8217;d like to get into the education you&#8217;re receiving. And, you know, I think it&#8217;s fair to say Kaiser Permanente is very progressive in really trying to be very cutting-edge in how the whole curriculum is set up.&nbsp;&nbsp;



And for the listeners who don&#8217;t know this, I&#8217;m actually on the board of directors of the school and have been since the founding of the school. And I think one of the reasons why I was invited to be on the board is the school really wanted to think ahead and be cutting edge when it comes to technology.&nbsp;&nbsp;



So from where I&#8217;ve sat, I&#8217;ve never been completely satisfied with the amount of tech that has made it into the curriculum. But at the same time, I&#8217;ve also made myself feel better about that just understanding that it&#8217;s sort of unstoppable, that students are so tech-forward already.&nbsp;&nbsp;



But I wanted to delve into a little bit here into what your honest opinions are and your fellow students&#8217; opinions are about whether you feel like you&#8217;re getting adequate training and background formally as part of your medical education when it comes to things like artificial intelligence or other technologies.&nbsp;&nbsp;



What do you think? Are you ‚Ä¶ would you wish the curriculum would change?&nbsp;



CHEN: Yeah, I think that&#8217;s a great question.&nbsp;&nbsp;



I think from a tech perspective, the school is very good about implementing, you know, opportunities for us to learn. Like, for example, learning how to use Epic, right, or at Kaiser Permanente, what we call HealthConnect, right. These electronic health records. That, my understanding is, a lot of schools maybe don&#8217;t teach that.&nbsp;&nbsp;



That&#8217;s something where we get training sessions maybe once or twice a year, like, ‚ÄúHey, here&#8217;s how to make a shortcut in the environment,‚Äù right.&nbsp;&nbsp;



So I think from that perspective, the school is really proactive in providing those opportunities, and they make it very easy to find resources for that, too. I think it &#8230;&nbsp;



LEE: Yeah, I think you&#8217;re pretty much guaranteed to be an Epic black belt by the time you [LAUGHS] finish your degree.&nbsp;&nbsp;



CHEN: Yes, yes.&nbsp;&nbsp;



But then I think in terms of the aspects of artificial intelligence, I think the school&#8217;s taken a more cautiously optimistic viewpoint. They&#8217;re just kind of looking around right now.&nbsp;&nbsp;



Formally in the curriculum, there hasn&#8217;t been anything around this topic. I believe the fourth-year students last year got a student-led lecture around this topic.&nbsp;&nbsp;



But talking to other peers at other institutions, it looks like it&#8217;s something that&#8217;s very slowly being built into the curriculum, and it seems like a lot of it is actually student-led, you know.&nbsp;&nbsp;



You know, my friend at Feinberg [School of Medicine] was like we just got a session before clerkship about best practices on how to use these tools.&nbsp;&nbsp;



I have another friend at Pitt talking about how they‚Äôre leading efforts of maybe incorporating some sort of LLM into their in-house curriculum where students can, instead of clicking around the website trying to find the exact slide, they can just ask this tool, like, ‚ÄúOK. We had class this day. They talked about this &#8230; but can you provide more information?‚Äù and it can pull from that.&nbsp;&nbsp;



So I think a lot of this, a lot of it is student-driven. Which I think is really exciting because it begs the question, I think, you know current physicians may not be very well equipped with these tools as well, right?&nbsp;&nbsp;



So maybe they don&#8217;t have a good idea of what exactly is the next steps or what does the curriculum look like. So I think the future in terms of this AI curriculum is really student-led, as well.&nbsp;



LEE: Yeah, yeah, it&#8217;s really interesting.&nbsp;&nbsp;



I think one of the reasons I think also that that happens is [that] it&#8217;s not just necessarily the curriculum that lags but the accreditation standards. You know, accreditation is really important for medical schools because you want to make sure that anyone who holds an MD, you know, is a bona fide doctor, and so accreditation standards are pretty strictly monitored in most countries, including the United States.&nbsp;&nbsp;



And I think accreditation standards are also‚Äîmy observation‚Äîare slow to understand how to adopt or integrate AI. And it&#8217;s not meant as a criticism. It&#8217;s a big unknown. No one knows exactly what to do and how to do. And so it&#8217;s really interesting to see that, as far as I can tell, I&#8217;ve observed the same thing that you just have seen, that most of the innovation in this area about how AI should be integrated into medical education is coming from the students themselves.&nbsp;&nbsp;



It seems, I think, I&#8217;d like to think it&#8217;s a healthy development. [LAUGHS]



CHEN: Something tells me maybe the students are a bit better at using these tools, as well.&nbsp;&nbsp;



You know, I talk to my preceptors because KP [Kaiser Permanente] also has their own version ‚Ä¶&nbsp;



LEE: Preceptor, maybe we should explain what that is.&nbsp;



CHEN: Yeah, sorry. So a preceptor is an attending physician, fully licensed, finished residency, and they are essentially your kind of teacher in the clinical environment.&nbsp;&nbsp;



So KP has their own version of some ambient documentation device, as well. And something I always like to ask, you know, like, ‚ÄúHey, what are your thoughts on these tools,‚Äù right?&nbsp;&nbsp;



And it&#8217;s always so polarizing, as well, even among the same specialty. Like, if you ask psychiatrists, which I think is a great use case of these tools, right. My preceptor hates it. Another preceptor next door loves it. [LAUGHTER]&nbsp;



So I think a lot of it‚Äôs, like, it‚Äôs still, like, a lot of unknowns, like you were mentioning.&nbsp;



LEE: Right. Well, in fact, I&#8217;m glad you brought that up because one thing that we&#8217;ve been hearing from previous guests a lot when it comes to AI in clinic is about ambient listening by AI, for example, to help set up a clinical note or even write a clinical note.&nbsp;&nbsp;



And another big use case that we heard a lot about that seems to be pretty popular is the use of generative AI to respond to patient messages.&nbsp;&nbsp;



So let&#8217;s start with the clinical note thing. First off, do you have opinions about that technology?&nbsp;



CHEN: I think it&#8217;s definitely good.&nbsp;&nbsp;



I think especially where, you know, if you&#8217;re in the family medicine environment or pediatric environment where you&#8217;re spending so much time with patients, a note like that is great, right.&nbsp;



I think coming from a strictly medical student standpoint, I think it&#8217;s‚Äîhonestly, it&#8217;d be great to have‚Äîbut I think there&#8217;s a lot of learning when you write the note, you know. There&#8217;s a lot of, you know, all of my preceptors talk about, like, when I read your note, you should present it in a way where I can see your thoughts and then once I get to the assessment and plan, it&#8217;s kind of funneling down towards a single diagnosis or a handful of diagnoses. And that&#8217;s, I think, a skill that requires you to practice over time, right.&nbsp;&nbsp;



So a part of me thinks, like, if I had this tool where [it] can just automatically give me a note as a first year, then it takes away from that learning experience, you know.&nbsp;



Even during our first year throughout school, we frequently get feedback from professors and doctors about these notes. And it&#8217;s a lot of feedback. [LAUGHTER] It&#8217;s like, ‚ÄúI don&#8217;t think you should have written that,‚Äù ‚ÄúThat should be in this section ‚Äù ‚Ä¶ you know, like a medical note or a SOAP note [Subjective, Objective, Assessment, and Plan], where, you know, the subjective is, like, what the patient tells you. Objective is what the physical findings are, and then your assessment of what&#8217;s happening, and then your plan. Like, it&#8217;s very particular, and then I think medicine is so structured in a way, that&#8217;s kind of, like, how everyone does it, right. So kind of going back to the question, I think it&#8217;s a great tool, but I don&#8217;t think it&#8217;s appropriate for a medical student.&nbsp;



LEE: Yeah, it&#8217;s so interesting to hear you say that. I was ‚Ä¶ one of our previous guests is the head of R&amp;D at Epic, Seth Hain. He said, ‚ÄúYou know, Peter, doctors do a lot of their thinking when they write the note.‚Äù&nbsp;



And, of course, Epic is providing ambient, you know, clinical notetaking automation. But he was urging caution because, you know, you&#8217;re saying, well, this is where you&#8217;re learning a lot. But actually, it&#8217;s also a point where, as a doctor, you&#8217;re thinking about the patient. And we do probably have to be careful with how we automate parts of that.&nbsp;&nbsp;



All right. So you&#8217;re gearing up for Step 1 of the USMLE [United States Medical Licensing Examination]. That&#8217;ll be a big multiple-choice exam. Then Step 2 is similar: very, very focused on advanced clinical knowledge. And then Step 3, you know, is a little more interactive.&nbsp;&nbsp;



And so one question that people have had about AI is, you know, how do we regulate the use of AI in medicine? And one of the famous papers that came out of both academia and industry was the concept that you might be able to treat AI like a person and have it go through the same licensing. And this is something that Carey, Zak, and I contemplated in our book.&nbsp;&nbsp;



In the end, at the time we wrote the book, I personally rejected the idea, but I think it&#8217;s still alive. And so I‚Äôve wondered if you have any &#8230; you know, first off, are you opinionated at all about, what should the requirements be for the allowable use of AI in the kind of work that you&#8217;re going to be doing?&nbsp;



CHEN: Yeah, I think it&#8217;s a tough question because, like, where do you draw that line, right? If you apply the human standards of it&#8217;s passing exams, then yes, in theory, it could be maybe a medical doctor, as well, right? It&#8217;s more empathetic than medical doctors, right? So where do you draw that line?&nbsp;&nbsp;



I think, you know, part of me thinks it&#8217;s maybe it is that human aspect that patients like to connect with, right. And maybe this really is just, like, these tools are just aids in helping, you know, maybe load off some cognitive load, right.&nbsp;&nbsp;



But I think the other part of me, I&#8217;m thinking about this is the next generation who are growing up with this technology, right. They&#8217;re interacting with applications all day. Maybe they&#8217;re on their iPads. They&#8217;re talking to chatbots. They&#8217;re using ChatGPT. This is, kind of, the environment they grew up with. Does that mean they also have increased, like, trust in these tools that maybe our generation or the generations above us don&#8217;t have that value that human connection? Would they value human connection less?&nbsp;&nbsp;



You know, I think those are some troubling thoughts that, you know, yes, at end of the day, maybe I&#8217;m not as smart as these tools, but I can still provide that human comfort. But if, at the end of the day, the future generation doesn&#8217;t really care about that or they perfectly trust these tools because that&#8217;s all they&#8217;ve kind of known, then where do human doctors stand?&nbsp;&nbsp;



I think part of that is, there would be certain specialties where maybe the human connection is more important. The longitudinal aspect of building that trust, I think is important. Family medicine is a great example. I think hematology oncology with cancer treatment.&nbsp;&nbsp;



Obviously, I think anyone&#8217;s not going to be thrilled to hear cancer diagnosis, but something tells me that seeing that on a screen versus maybe a physician prompting you and telling you about that tells me that maybe in those aspects, you know, the human nature, the human touch plays an important role there, too.&nbsp;



LEE: Yeah, you know, I think it strikes me that it&#8217;s going to be your generation that really is going to set the pattern probably for the next 50 years about how this goes. And it&#8217;s just so interesting because I think a lot will depend on your reactions to things.&nbsp;&nbsp;



So, for example, you know, one thing that is already starting to happen are patients who are coming in armed, you know, with a differential [LAUGHS], you know, that they&#8217;ve developed themselves with the help of ChatGPT. So let me &#8230; you must have thought about these things. So, in fact, has it happened in your clinical work already?&nbsp;



CHEN: Yeah, I&#8217;ve seen people come into the ED during my ED shift, like emergency department, and they‚Äôll be like, ‚ÄúOh, I have neck pain and here are all the things that, you know, Chat told me, ChatGPT told me. What do you think &#8230; do I need? I want this lab ordered, that lab ordered.‚Äù&nbsp;&nbsp;



LEE: Right.&nbsp;



CHEN: And I think my initial reaction is, ‚ÄúGreat. Maybe we should do that.‚Äù But I think the other reaction is understanding that not everyone has the clinical background of understanding what&#8217;s most important, what do we need to absolutely rule out, right?&nbsp;&nbsp;&nbsp;



So, I think in some regards, I would think that maybe ChatGPT errs on the side of caution, &#8230;&nbsp;



LEE: Yes.&nbsp;&nbsp;



CHEN: &#8230; giving maybe patients more extreme examples of what this could be just to make sure that it&#8217;s, in a way, is not missing any red flags as well, right.&nbsp;&nbsp;



LEE: Right. Yeah.&nbsp;&nbsp;



CHEN: But I think a lot of this is ‚Ä¶ what we&#8217;ve been learning is it&#8217;s all about shared decision making with the patient, right. Being able to acknowledge like, ‚ÄúYeah, [in] that list, most of the stuff is very plausible, but maybe you didn&#8217;t think about this one symptom you have.‚Äù&nbsp;&nbsp;



So I think part of it, maybe it&#8217;s a sidebar here, is the idea of prompting, right. You know, they&#8217;ve always talked about all these, you know, prompt engineers, you know, how well can you, like, give it context to answer your question?&nbsp;



LEE: Yeah.&nbsp;



CHEN: So I think being able to give these models the correct information and the relevant information and keyword relevant, because relevant is, I guess, where your clinical expertise comes in. Like, what do you give the model, what do you not give? So I think that difference between a medical provider versus maybe your patients is ultimately the difference.&nbsp;



LEE: Let me press on that a little bit more because you brought up the issue of trust, and trust is so essential for patients to feel good about their medical care.&nbsp;&nbsp;



And I can imagine you&#8217;re a medical student seeing a patient for the first time. So you don&#8217;t have a trust relationship with that patient. And the patient comes in maybe trusting ChatGPT more than you.&nbsp;



CHEN: Very valid. No. I mean, I get that a lot, surprisingly, you know. [LAUGHTER] Sometimes [they‚Äôre] like, ‚ÄúOh, I don&#8217;t want to see the medical student,‚Äù because we always give the patient an option, right. Like, it&#8217;s their time, whether it&#8217;s a clinic visit.&nbsp;&nbsp;



But yeah, those patients, I think it&#8217;s perfectly reasonable. If I heard a second-year medical student was going to be part of my care team, taking that history, I&#8217;d be maybe a little bit concerned, too. Like, are they asking all the right questions? Are they relaying that information back to their attending physician correctly?&nbsp;&nbsp;



So I think a lot of it is, at least from a medical student perspective, is framing it so the patient understands that this is a learning opportunity for the students. And something I do a lot is tell them like, ‚ÄúHey, like, you know, at the end of the day, there is someone double-checking all my work.‚Äù&nbsp;&nbsp;



LEE: Yeah.&nbsp;



CHEN: But for those that come in with a list, I sometimes sit down with them, and we&#8217;ll have a discussion, honestly.&nbsp;&nbsp;



I‚Äôll be like, ‚ÄúI don&#8217;t think you have meningitis because you&#8217;re not having a fever. Some of the physical exam maneuvers we did were also negative. So I don&#8217;t think you have anything to worry about that,‚Äù you know.&nbsp;&nbsp;



So I think it&#8217;s having that very candid conversation with the patient that helps build that initial trust. Telling them like, ‚ÄúHey ‚Ä¶ ‚Äù&nbsp;



LEE: It&#8217;s impressive to hear how even keeled you are about this. You know, I think, of course, and you&#8217;re being very humble saying, well, you know, as a second-year medical student, of course, someone might not, you know, have complete trust. But I think that we will be entering into a world where no doctor is going to be, no matter how experienced or how skilled, is going to be immune from this issue.&nbsp;



So we&#8217;re starting to run toward the end of our time together. And I like to end with one or two more provocative questions.&nbsp;&nbsp;



And so let me start with this one. Undoubtedly, I mean, you&#8217;re close enough to tech and digital stuff, digital health, that you&#8217;re undoubtedly familiar with famous predictions, you know, by Turing and Nobel laureates that someday certain medical specialties, most notably radiology, would be completely supplanted by machines. And more recently, there have been predictions by others, like, you know, Elon Musk, that maybe even some types of surgery would be replaced by machines.&nbsp;&nbsp;



What do you think? Do you have an opinion?&nbsp;



CHEN: I think replace is a strong term, right. To say that doctors are completely obsolete, I think, is unlikely.&nbsp;&nbsp;



If anything, I think there might be a shift maybe in what it means to be a doctor, right. Undoubtedly, maybe the demands of radiologists are going to go down because maybe more of the simple things can truly be automated, right. And you just have a supervising radiologist whose output is maybe 10 times as maybe 10 single radiologists, right.&nbsp;&nbsp;



So I definitely see a future where the demand of certain specialties might go down.&nbsp;&nbsp;



And I think when I talk about a shift of what it means to be a physician, maybe it&#8217;s not so much diagnostic anymore, right, if these models get so good at, like, just taking in large amounts of information, but maybe it pivots to being really good at understanding the limitations of these models and knowing when to intervene is what it means to be the kind of the next generation of physicians.&nbsp;&nbsp;



I think in terms of surgery, yeah, I think it&#8217;s a concern, but maybe not in the next 50 years. Like those Da Vinci robots are great. I think out of Mayo Clinic, they were demoing some videos of these robots leveraging computer vision to, like, close portholes, like laparoscopic scars. And that&#8217;s something I do in the OR [operating room], right. And we&#8217;re at the same level at this point. [LAUGHTER] So at that point, maybe.&nbsp;&nbsp;



But I think robotics still has to address the understanding of like, what if something goes wrong, right? Who&#8217;s responsible? And I don&#8217;t see a future where a robot is able to react to these, you know, dangerous situations when maybe something goes wrong. You still have to have a surgeon on board to, kind of, take over. So in that regard, that&#8217;s kind of where I see maybe the future going.&nbsp;



LEE: So last question. You know, when you are thinking about the division of time, one of the themes that we&#8217;ve seen in the previous guests is more and more doctors are doing more technology work, like writing code and so on. And more and more technologists are thinking deeply and getting educated in clinical and preclinical work.&nbsp;&nbsp;



So for you, let&#8217;s look ahead 10 years. What do you see your division of labor to be? Or, you know, how would you ‚Ä¶ what would you tell your mom then about how you spend a typical day?&nbsp;



CHEN: Yeah, I mean, I think for me, technology is something I definitely want to be involved in in my line of work, whether it&#8217;s, you know, AI work, whether it&#8217;s improving quality of healthcare through technology.&nbsp;&nbsp;



My perfect division would be maybe still being able to see patients but also balancing some maybe more of these higher-level kind of larger projects. But I think having that division would be something nice.&nbsp;



LEE: Yeah, well, I think you would be great just from the little bit I know about you. And, Daniel, it&#8217;s been really great chatting with you. I wish you the best of luck, you know, with your upcoming exams and getting past this year two of your medical studies. And perhaps someday I&#8217;ll be your patient.&nbsp;



[TRANSITION MUSIC]‚ÄØ&nbsp;



CHEN: Thank you so much.&nbsp;



LEE: You know, one of the lucky things about my job is that I pretty regularly get to talk to students at all levels, spanning high school to graduate school. And when I get to talk especially to med students, I&#8217;m always impressed with their intelligence, just how serious they are, and their high energy levels. Daniel is absolutely a perfect example of all that.&nbsp;&nbsp;



Now, it comes across as trite to say that the older generation is less adept at technology adoption than younger people. But actually, there probably is a lot of truth to that. And in the conversation with Daniel, I think he was actually being pretty diplomatic but also clear that he and his fellow med students don&#8217;t necessarily expect the professors in their med school to understand AI as well as they do.&nbsp;



There&#8217;s no doubt in my mind that medical education will have to evolve a lot to help prepare doctors and nurses for an AI future. But where will this evolution come from?&nbsp;&nbsp;



As I reflect on my conversations with Morgan and Daniel, I start to think that it&#8217;s most likely to come from the students themselves. And when you meet people like Morgan and Daniel, it&#8217;s impossible to not be incredibly optimistic about the next generation of clinicians.&nbsp;



[THEME MUSIC]&nbsp;



Another big thank-you to Morgan and Daniel for taking time to share their experiences with us. And to our listeners, thank you for joining us. We have just a couple of episodes left, one on AI‚Äôs impact on the operation of public health departments and healthcare systems and another coauthor roundtable. We hope you&#8217;ll continue to tune in.&nbsp;&nbsp;



Until next time.&nbsp;



[MUSIC FADES]&nbsp;

				
			
			
				Show more			
		
	





AI Revolution in Medicine podcast series

Opens in a new tabThe post Navigating medical education in the era of generative AI appeared first on Microsoft Research.
‚Ä¢ Build a drug discovery research assistant using Strands Agents and Amazon Bedrock
  Drug discovery is a complex, time-intensive process that requires researchers to navigate vast amounts of scientific literature, clinical trial data, and molecular databases. Life science customers like Genentech and AstraZeneca are using AI agents and other generative AI tools to increase the speed of scientific discovery. Builders at these organizations are already using the fully managed features of Amazon Bedrock to quickly deploy domain-specific workflows for a variety of use cases, from early drug target identification to healthcare provider engagement. 
However, more complex use cases might benefit from using the open source Strands Agents SDK. Strands Agents takes a model-driven approach to develop and run AI agents. It works with most model providers, including custom and internal large language model (LLM) gateways, and agents can be deployed where you would host a Python application. 
In this post, we demonstrate how to create a powerful research assistant for drug discovery using Strands Agents and Amazon Bedrock. This AI assistant can search multiple scientific databases simultaneously using the Model Context Protocol (MCP), synthesize its findings, and generate comprehensive reports on drug targets, disease mechanisms, and therapeutic areas. This assistant is available as an example in the open-source&nbsp;healthcare and life sciences agent toolkit&nbsp;for you to use and adapt. 
Solution overview 
This solution uses Strands Agents to connect high-performing foundation models (FMs) with common life science data sources like arXiv, PubMed, and ChEMBL. It demonstrates how to quickly create MCP servers to query data and view the results in a conversational interface. 
Small, focused AI agents that work together can often produce better results than a single, monolithic agent. This solution uses a team of sub-agents, each with their own FM, instructions, and tools. The following flowchart shows how the orchestrator agent (shown in orange) handles user queries and routes them to sub-agents for either information retrieval (green) or planning, synthesis, and report generation (purple). 
 
This post focuses on building with Strands Agents in your local development environment. Refer to the Strands Agents documentation to deploy production agents on AWS Lambda, AWS Fargate, Amazon Elastic Kubernetes Service (Amazon EKS), or Amazon Elastic Compute Cloud (Amazon EC2). 
In the following sections, we show how to create the research assistant in Strands Agents by defining an FM, MCP tools, and sub-agents. 
Prerequisites 
This solution requires Python 3.10+, strands-agents, and several additional Python packages. We strongly recommend using a virtual environment like venv or uv to manage these dependencies. 
Complete the following steps to deploy the solution to your local environment: 
 
 Clone the code repository from GitHub. 
 Install the required Python dependencies with pip install -r requirements.txt. 
 Configure your AWS credentials by setting them as environment variables, adding them to a credentials file, or following another supported process. 
 Save your Tavily API key to a .env file in the following format: TAVILY_API_KEY="YOUR_API_KEY". 
 
You also need access to the following Amazon Bedrock FMs in your AWS account: 
 
 Anthropic‚Äôs Claude 3.7 Sonnet 
 Anthropic‚Äôs Claude 3.5 Sonnet 
 Anthropic‚Äôs Claude 3.5 Haiku 
 
Define the foundation model 
We start by defining a connection to an FM in Amazon Bedrock using the Strands Agents BedrockModel class. We use Anthropic‚Äôs Claude 3.7 Sonnet as the default model. See the following code: 
 
 from strands import Agent, tool
from strands.models import BedrockModel
from strands.agent.conversation_manager import SlidingWindowConversationManager
from strands.tools.mcp import MCPClient
# Model configuration with Strands using Amazon Bedrock's foundation models
def get_model():
    model = BedrockModel(
        boto_client_config=Config(
            read_timeout=900,
            connect_timeout=900,
            retries=dict(max_attempts=3, mode="adaptive"),
        ),
        model_id="us.anthropic.claude-3-7-sonnet-20250219-v1:0",
        max_tokens=64000,
        temperature=0.1,
        top_p=0.9,
        additional_request_fields={
            "thinking": {
                "type": "disabled"  # Can be enabled for reasoning mode
            }
        }
    )
    return model 
 
Define MCP tools 
MCP provides a standard for how AI applications interact with their external environments. Thousands of MCP servers already exist, including those for life science tools and datasets. This solution provides example MCP servers for: 
 
 arXiv ‚Äì Open-access repository of scholarly articles 
 PubMed ‚Äì Peer-reviewed citations for biomedical literature 
 ChEMBL ‚Äì Curated database of bioactive molecules with drug-like properties 
 ClinicalTrials.gov ‚Äì US government database of clinical research studies 
 Tavily Web Search ‚Äì API to find recent news and other content from the public internet 
 
Strands Agents streamlines the definition of MCP clients for our agent. In this example, you connect to each tool using standard I/O. However, Strands Agents also supports remote MCP servers with Streamable-HTTP Events transport. See the following code: 
 
 # MCP Clients for various scientific databases
tavily_mcp_client = MCPClient(lambda: stdio_client(
    StdioServerParameters(command="python", args=["application/mcp_server_tavily.py"])
))
arxiv_mcp_client = MCPClient(lambda: stdio_client(
    StdioServerParameters(command="python", args=["application/mcp_server_arxiv.py"])
))
pubmed_mcp_client = MCPClient(lambda: stdio_client(
    StdioServerParameters(command="python", args=["application/mcp_server_pubmed.py"])
))
chembl_mcp_client = MCPClient(lambda: stdio_client(
    StdioServerParameters(command="python", args=["application/mcp_server_chembl.py"])
))
clinicaltrials_mcp_client = MCPClient(lambda: stdio_client(
    StdioServerParameters(command="python", args=["application/mcp_server_clinicaltrial.py"])
)) 
 
Define specialized sub-agents 
The planning agent looks at user questions and creates a plan for which sub-agents and tools to use: 
 
 @tool
def planning_agent(query: str) -&gt; str:
    """
    A specialized planning agent that analyzes the research query and determines
    which tools and databases should be used for the investigation.
    """
    planning_system = """
    You are a specialized planning agent for drug discovery research. Your role is to:
    
    1. Analyze research questions to identify target proteins, compounds, or biological mechanisms
    2. Determine which databases would be most relevant (Arxiv, PubMed, ChEMBL, ClinicalTrials.gov)
    3. Generate specific search queries for each relevant database
    4. Create a structured research plan
    """
    model = get_model()
    planner = Agent(
        model=model,
        system_prompt=planning_system,
    )
    response = planner(planning_prompt)
    return str(response) 
 
Similarly, the synthesis agent integrates findings from multiple sources into a single, comprehensive report: 
 
 @tool
def synthesis_agent(research_results: str) -&gt; str:
    """
    Specialized agent for synthesizing research findings into a comprehensive report.
    """
    system_prompt = """
    You are a specialized synthesis agent for drug discovery research. Your role is to:
    
    1. Integrate findings from multiple research databases
    2. Create a comprehensive, coherent scientific report
    3. Highlight key insights, connections, and opportunities
    4. Organize information in a structured format:
       - Executive Summary (300 words)
       - Target Overview
       - Research Landscape
       - Drug Development Status
       - References
    """
    model = get_model()
    synthesis = Agent(
        model=model,
        system_prompt=system_prompt,
    )
    response = synthesis(synthesis_prompt)
    return str(response) 
 
Define the orchestration agent 
We also define an orchestration agent to coordinate the entire research workflow. This agent uses the SlidingWindowConversationManager class from Strands Agents to store the last 10 messages in the conversation. See the following code: 
 
 def create_orchestrator_agent(
    history_mode,
    tavily_client=None,
    arxiv_client=None,
    pubmed_client=None,
    chembl_client=None,
    clinicaltrials_client=None,
):
    system = """
    You are an orchestrator agent for drug discovery research. Your role is to coordinate a multi-agent workflow:
    
    1. COORDINATION PHASE:
       - For simple queries: Answer directly WITHOUT using specialized tools
       - For complex research requests: Initiate the multi-agent research workflow
    
    2. PLANNING PHASE:
       - Use the planning_agent to determine which databases to search and with what queries
    
    3. EXECUTION PHASE:
       - Route specialized search tasks to the appropriate research agents
    
    4. SYNTHESIS PHASE:
       - Use the synthesis_agent to integrate findings into a comprehensive report
       - Generate a PDF report when appropriate
    """
    # Aggregate all tools from specialized agents and MCP clients
    tools = [planning_agent, synthesis_agent, generate_pdf_report, file_write]
    # Dynamically load tools from each MCP client
    if tavily_client:
        tools.extend(tavily_client.list_tools_sync())
    # ... (similar for other clients)
    conversation_manager = SlidingWindowConversationManager(
        window_size=10,  # Maintains context for the last 10 exchanges
    )
    orchestrator = Agent(
        model=model,
        system_prompt=system,
        tools=tools,
        conversation_manager=conversation_manager
    )
    return orchestrator 
 
Example use case: Explore recent breast cancer research 
To test out the new assistant, launch the chat interface by running streamlit run application/app.py and opening the local URL (typically http://localhost:8501) in your web browser. The following screenshot shows a typical conversation with the research agent. In this example, we ask the assistant, ‚ÄúPlease generate a report for HER2 including recent news, recent research, related compounds, and ongoing clinical trials.‚Äù The assistant first develops a comprehensive research plan using the various tools at its disposal. It decides to start with a web search for recent news about HER2, as well as scientific articles on PubMed and arXiv. It also looks at HER2-related compounds in ChEMBL and ongoing clinical trials. It synthesizes these results into a single report and generates an output file of its findings, including citations. 
 
The following is an excerpt of a generated report: 
 
 Comprehensive Scientific Report: HER2 in Breast Cancer Research and Treatment
1. Executive Summary
Human epidermal growth factor receptor 2 (HER2) continues to be a critical target in breast cancer research and treatment development. This report synthesizes recent findings across the HER2 landscape highlighting significant advances in understanding HER2 biology and therapeutic approaches. The emergence of antibody-drug conjugates (ADCs) represents a paradigm shift in HER2-targeted therapy, with trastuzumab deruxtecan (T-DXd, Enhertu) demonstrating remarkable efficacy in both early and advanced disease settings. The DESTINY-Breast11 trial has shown clinically meaningful improvements in pathologic complete response rates when T-DXd is followed by standard therapy in high-risk, early-stage HER2+ breast cancer, potentially establishing a new treatment paradigm. 
 
Notably, you don‚Äôt have to define a step-by-step process to accomplish this task. By providing the assistant with a well-documented list of tools, it can decide which to use and in what order. 
Clean up 
If you followed this example on your local computer, you will not create new resources in your AWS account that you need to clean up. If you deployed the research assistant using one of those services, refer to the relevant service documentation for cleanup instructions. 
Conclusion 
In this post, we showed how Strands Agents streamlines the creation of powerful, domain-specific AI assistants. We encourage you to try this solution with your own research questions and extend it with new scientific tools. The combination of Strands Agents‚Äôs orchestration capabilities, streaming responses, and flexible configuration with the powerful language models of Amazon Bedrock creates a new paradigm for AI-assisted research. As the volume of scientific information continues to grow exponentially, frameworks like Strands Agents will become essential tools for drug discovery. 
To learn more about building intelligent agents with Strands Agents, refer to Introducing Strands Agents, an Open Source AI Agents SDK, Strands Agents SDK, and the GitHub repository. You can also find more sample agents for healthcare and life sciences built on Amazon Bedrock. 
For more information about implementing AI-powered solutions for drug discovery on AWS, visit us at AWS for Life Sciences. 
 
About the authors 
Hasun Yu&nbsp;is an AI/ML Specialist Solutions Architect with extensive expertise in designing, developing, and deploying AI/ML solutions for healthcare and life sciences. He supports the adoption of advanced AWS AI/ML services, including generative and agentic AI. 
Brian Loyal is a Principal AI/ML Solutions Architect in the Global Healthcare and Life Sciences team at Amazon Web Services. He has more than 20 years‚Äô experience in biotechnology and machine learning and is passionate about using AI to improve human health and well-being.
‚Ä¢ Amazon Nova Act SDK (preview): Path to production for browser automation agents
  In early 2025, we introduced the Amazon Nova Act SDK as a research preview to help developers build agents that reliably complete tasks in a web browser. Now, we are excited to work with customers to take their agents to production in a limited preview, using new AWS integrations including AWS Identity and Access Management (IAM) for secure credentialing, Amazon Simple Storage Service (Amazon S3) for data storage and policy control, and the new Amazon Bedrock AgentCore Browser Tool for scalable, cloud-based browser execution. 
In this post, we walk through what makes the Amazon Nova Act SDK unique, how it works, and how teams across industries are already using it to automate browser-based workflows at scale. 
Challenges with traditional automated business workflows 
Many day-to-day business operations require a browser, such as submitting time-off requests, processing invoices, accessing vendor portals, or reviewing dashboards. Lack of API coverage often means workflows are done manually: teams copy-paste data across tabs, follow multi-step flows, and click through countless interfaces to get work done. 
Traditional rules-based browser automation frameworks often face challenges in dynamic web environments. Teams can spend more time on ongoing maintenance than on building new automations, because changes in page structure (for example, newly added form fields or dropdown options) break brittle selectors. Most importantly, these frameworks are difficult to scale. If one use case is performed on 50 different sites (for example, professional license verification on state websites), teams must build 50 site-specific automations, because rules-based frameworks don‚Äôt generalize. 
As humans, our ability to perform tasks adapts across different tools and interfaces. For example, once you know how to draft an email in Outlook, you can easily do the same in Gmail‚Äîeven if you‚Äôve never used it before. Large language models (LLMs), trained on millions of examples of UIs, offer the potential to create a similar type of pattern recognition for AI agents. They‚Äôve brought us this far‚Äîpowering chat, summarization, coding copilots, and more‚Äîby interpreting language, following instructions, and reasoning across domains. Now, we‚Äôre entering the next phase of generative AI: one centered on action. Agentic AI builds on the foundation of LLMs to move from understanding to execution. These systems are designed to complete dynamic, multi-step workflows‚Äîlike filling out complex forms, interacting with evolving UI elements, or performing real-world business tasks at scale. Agentic AI doesn‚Äôt replace LLMs‚Äîit extends them, unlocking new automation capabilities that bring us closer to real task completion in enterprise environments. 
Agentic AI with the Amazon Nova Act SDK 
With the Amazon Nova Act SDK, you can build and deploy reliable browser agents powered by the Amazon Nova Act model‚Äîpurpose-built and fine-tuned for agentic AI. Trained with reinforcement learning and extensive in-domain browser interaction data, it executes step-by-step workflows with precision. With this latest version, we‚Äôve extended those capabilities with AWS integrations so you can take your agents from prototype to production. You can install the SDK with a single command, write agents in Python and natural language, debug in real time, and integrate directly into continuous integration and delivery (CI/CD) pipelines. With enterprise-grade security, observability, and infrastructure now available through AWS, the Amazon Nova Act SDK provides a fast, flexible path for teams looking to build agents that act‚Äîand deliver‚Äîat scale. You can use the Amazon Nova Act SDK to automate real-world workflows where traditional scripts or general-purpose models aren‚Äôt reliable or scalable enough. You can install it with a single command, write agents using a combination of Python and natural language, debug while the workflow runs, and deploy through CI/CD pipelines. 
The Amazon Nova Act SDK also integrates with the new Amazon Bedrock AgentCore Browser Tool‚Äîa fast, secure, cloud-based browser that enables AI agents to interact with websites at scale. It includes enterprise-grade security features, including virtual machine-level isolation and federated identity integration. The tool offers built-in observability through live viewing, AWS CloudTrail logging, and session replay to troubleshoot, maintain quality, and support compliance. 
Benefits of the Amazon Nova Act SDK 
The Amazon Nova Act SDK is reliable, fast to deploy, and built for secure, large-scale browser automation use cases. In this section, we discuss some of the benefits of the SDK in more detail. 
Reliability: Build robust browser automation with high accuracy and repeatability 
With the Amazon Nova Act SDK, developers can break down complex workflows into reliable atomic commands (for example, collect all form elements of a webpage and return a string with all required fields of the form). It supports the addition of detailed instructions to refine those commands when needed (for example, dismiss any popup banners), the ability to call APIs, and the option to alternate direct browser manipulation through Playwright to improve reliability (for example, for entering passwords). Developers can interleave Python code‚Äîsuch as tests, breakpoints, assertions, or thread pools for parallelization‚Äîto optimize performance, especially because even the fastest agents are constrained by webpage load times. With this latest version, the Amazon Nova Act SDK is already demonstrating over 90% reliability across early enterprise workflows, including automated quality assurance, complex form handling, and process execution. Improvements to reasoning and recovery help agents adapt to changing UIs and complete complex sequences consistently and accurately. 
Speed-to-market: Move from prototype to production in days‚Äînot weeks 
The Amazon Nova Act SDK is designed to help you build automation quickly, without relying on brittle scripts. You can install the SDK with a single command. You can define agents using Python, natural language, or both. You can debug flows while they run, inspect the DOM, pause between steps, and iterate rapidly. The Amazon Nova Act SDK supports the following features: 
 
 Git-based CI/CD pipelines 
 Reusable agent modules 
 Playwright fallbacks for sensitive tasks (for example, password input) 
 Threading and parallelization for fast execution 
 
You don‚Äôt have to change your infrastructure or rebuild your internal tools. Agents built with Amazon Nova Act fit into existing dev workflows and allow you to move from experimentation to production quickly. 
Security: Deploy automations you can trust‚Äîpowered by AWS 
The Amazon Nova Act SDK integrates with IAM for access control, and access to the model is managed just like access from other AWS services. It supports execution on macOS, Linux, Windows, and WSL2. Runtime environments are isolated, and encryption is supported for sensitive inputs and outputs. The Amazon Nova Act SDK was designed to work inside enterprise environments‚Äîwith the reliability, observability, and security that production systems require. 
See it in action: Automating information gathering to help streamline financial decisions 
In financial services‚Äîespecially investment banking, M&amp;A advisory, and strategic research‚Äîsuccess often depends on how fast and accurately teams can turn fragmented public data into actionable insight. The following demo shows the Amazon Nova Act SDK in action. 
 
Where the Amazon Nova Act SDK can make an impact 
Browser-based workflows are common in today‚Äôs businesses, yet many remain manual, repetitive, and prone to error. The Amazon Nova Act SDK helps organizations automate these tasks, freeing up teams to focus on higher-value work, improve accuracy, and reduce operational delays. Its reliability makes it a fit across industries and use cases. In this section, we provide some examples of what our early customers are building. 
Automated data entry and form filling 
The Amazon Nova Act SDK reduces repetitive manual input across web-based systems‚Äîlike CRMs, HR tools, and finance platforms‚Äîby automating form submissions, uploads, and updates. In healthcare, staff assist members with complex, state-specific benefit applications. Public sector caseworkers also re-enter household data across multiple systems. The Amazon Nova Act SDK handles these dynamic flows reliably‚Äînavigating shifting fields, dropdowns, and popups without brittle scripts or custom code. 
  
Rackspace Technology, a leading hybrid and AI solutions provider, is working with Alvee Health to automatically register members for public benefits using the Amazon Nova Act SDK. ‚ÄúMany registration forms for public programs are long and confusing, so members often don‚Äôt apply for the help they need,‚Äù said Nicole Cook, CEO at Alvee. ‚ÄúWith the Amazon Nova Act SDK and harnessing information already in Alvee‚Äôs system, we‚Äôre not just simplifying paperwork‚Äîwe‚Äôre helping provide timely, accurate access to the resources that support healthier lives. We expect this innovation to increase successful benefit registrations by 30%, and improve overall case load by up to tenfold, allowing healthcare providers to focus more on patient care and less on administration. This is a prime example of how AI can be used to support well-being and improve overall health for communities.‚Äù 
Customer support augmentation 
Customer support teams across retail, travel, and software as a service (SaaS) often move between internal tools and third-party portals to resolve tickets. For example, a retail associate might submit a return on a partner site. A travel agent might log in to an airline dashboard to request compensation. A support rep might reset a license key in a customer admin console. The Amazon Nova Act SDK automates these browser-based tasks, helping agents stay focused on customer conversations while backend steps are executed reliably and at speed. 
 
Navan, a leading travel and expense management platform, uses the Amazon Nova Act SDK to simplify its travel agents‚Äô workflows by automating how they fill out payment forms across a wide range of vendors. 
Yuval Refua, SVP of Product, said, ‚ÄúAdding the Amazon Nova Act SDK to our agents‚Äô workflows has helped us reduce repetitive tasks‚Äîan essential step in scaling our operations to serve more customers. We tried other computer use tools, and Amazon Nova Act‚Äôs reliability and flexibility enabled a single script work across diverse payment forms from a range of hotel brands. We‚Äôre now expanding this automation to cover even more vendors, which we expect will increase our operational capacity and help us meet growing customer demand more efficiently.‚Äù 
Automating high-stakes administrative workflows 
Credential verification, identity checks, and other compliance-heavy tasks often involve navigating hundreds of third-party portals with inconsistent layouts. The Amazon Nova Act SDK makes it possible to automate these workflows with high accuracy, flexibility, and full control‚Äîhelping teams scale while maintaining precision. 
 
Automation Anywhere, a global leader in Agentic Process Automation (APA), is expanding its automation capabilities through the Amazon Nova Act SDK, starting with professional credential verification‚Äîa high-stakes, repetitive task that‚Äôs essential for compliance, member safety, and day-to-day operations. 
‚ÄúBy deeply integrating the Amazon Nova Act SDK into our Process Reasoning Engine (PRE), we‚Äôve unlocked a major leap forward in computer use for enterprise automation,‚Äù said Adi Kuruganti, Chief Product Officer at Automation Anywhere. ‚ÄúOur goal-oriented AI agents don‚Äôt just mimic clicks, they reason through UI-based processes in real time, navigating complex websites with human-like expertise. This opens the door to automating previously out-of-reach use cases like healthcare program enrollment testing, where accuracy and scale are essential.‚Äù 
UX and QA testing across dynamic interfaces 
UX and QA testing often involves simulating real user interactions on frequently changing websites‚Äîespecially in sectors like banking, insurance, and government. With the Amazon Nova Act SDK, teams can write and update tests using natural language or Python, adapting quickly to UI changes without brittle selectors or manual rewrites. 
 
Tyler Technologies, a leading provider of integrated software and technology services to the public sector, is using the Amazon Nova Act SDK to automate software testing and improve the reliability of its releases. ‚ÄúAmazon Nova Act‚Äôs natural-language interface lets us convert our manual test plans into automated suites in minutes‚Äîwithout writing a single line of code, saving us hundreds of hours while expanding test coverage and increasing product quality.‚Äù said Franklin Williams, President of Data &amp; Insights at Tyler Technologies. ‚ÄúWe‚Äôre now looking to expand the use of Nova Act SDK across our portfolio.‚Äù 
What‚Äôs next for the Amazon Nova Act SDK 
We‚Äôre working closely with early AWS customers to inform our roadmap. Although today‚Äôs focus is on browser-based workflows, the Amazon Nova Act SDK is part of a broader effort to build agents that can operate reliably across diverse environments. We‚Äôre continuing to expand the model‚Äôs reach beyond the web, applying reinforcement learning to more complex, real-world tasks. We‚Äôre also deepening integration across the AWS ecosystem to help developers move faster‚Äîfrom prototyping to deploying secure, scalable agentic systems. 
Get started with the Amazon Nova Act SDK 
If you‚Äôre a technical leader or developer and want to start prototyping with the research preview of the Amazon Nova Act SDK, visit Amazon Nova Act. You will get access to early tooling designed for reliable, step-by-step browser automation‚Äîbuilt for real-world workflows, not just demos. 
The Amazon AGI Lab is Amazon‚Äôs applied research group focused on building useful AI agents that can take real-world actions in digital and physical environments. Their work spans LLMs, reinforcement learning, world modeling, and more. To learn more and keep up with their latest innovations, visit Amazon AGI Labs. 
Contact us to express interest in working with us to productionize your agent (limited preview). 
 
About the authors 
Lori Knapp&nbsp;is a Principal Product Manager with Amazon Nova. She leads product efforts to define how foundation models can power intelligent agents across diverse real-world use cases. Prior to this role, Lori‚Äôs experience spanned scaling adaptive voice experiences at Alexa, product strategy at Microsoft, and behavioral science consulting. Outside of work, she enjoys exploring new cities, hosting dinner parties, and solving crossword puzzles. 
Tara Raj is an Engineering Manager at Amazon working on Nova Act. In her current role she‚Äôs focused on developer experience, from building nova.amazon.com/act to the Nova Act SDK with the software engineers on her team to driving adoption of Amazon Nova Act with her solutions architect team. Tara has over 10 years of experience in engineering roles bringing products from vision to launch including Nova multimodal capabilities at Amazon and the Windows Subsystem for Linux and Visual Studio Code at Microsoft. Outside of work you can find her traveling, dancing, and trying new restaurants.
‚Ä¢ Optimizing enterprise AI assistants: How Crypto.com uses LLM reasoning and feedback for enhanced efficiency
  This post is co-written with Jessie Jiao from Crypto.com. Crypto.com is a crypto exchange and comprehensive trading service serving 140 million users in 90 countries. To improve the service quality of Crypto.com, the firm implemented generative AI-powered assistant services on AWS. 
Modern AI assistants‚Äîartificial intelligence systems designed to interact with users through natural language, answer questions, and even perform tasks‚Äîface increasingly complex challenges in production environments. Beyond handling basic FAQs, they must now execute meaningful actions, adhere to company policies, implement content filtering, escalate to human operators when needed, and manage follow-up tasks. These requirements demand sophisticated systems capable of handling diverse scenarios while maintaining consistency and compliance. 
To address these challenges, a modular subsystem architecture proves invaluable. This architectural approach divides an AI system into separate, specialized components that can function independently while working together as a cohesive whole. Such design allows for flexible integration of different processing logics, such as intelligent routing between knowledge bases, dynamic prioritization of information sources, and seamless incorporation of business rules and policies. Each subsystem can be independently developed and optimized for specific tasks while maintaining overall system coherence. 
As AI assistant systems grow in complexity, with multiple subsystems handling various workloads, prompt engineering emerges as a critical discipline. This art of carefully crafting input text guides language model responses and facilitates consistent behavior across interconnected components. Crafting effective prompts that work across different subsystems while maintaining consistency and accuracy is both critical and time-intensive. This challenge is particularly acute in enterprise environments where precision and reliability are paramount. 
In this post, we explore how we used user and system feedback to continuously improve and optimize our instruction prompts. This feedback-driven approach has enabled us to create more effective prompts that adapt to various subsystems while maintaining high performance across different use cases. 
Feedback and reasoning: The key to LLM performance improvement 
Although large language models (LLMs) have demonstrated remarkable capabilities, they can sometimes struggle with complex or ambiguous inputs. This is where feedback mechanisms become essential. By incorporating feedback loops, LLMs can learn from their mistakes, refine the instruction, and adapt to challenging scenarios. 
One powerful approach is critiquing, where LLMs are paired with an external feedback mechanism that provide critiques or feedback. For instance, when processing documents, if an LLM generates an incorrect summary, a fact-checking tool can identify inaccuracies and provide feedback. The model can then revise its output, leading to improved accuracy and reliability. This iterative process mirrors human learning, where feedback drives continuous improvement. Consider an example where a customer asks an enterprise AI assistant, ‚ÄúI need to increase my credit limit immediately for an emergency purchase.‚Äù The assistant might initially respond with approval steps without verification, but a critique system would flag: ‚ÄúResponse bypasses required identity verification protocol and fails to assess qualification criteria per company policy.‚Äù With this feedback, the assistant can revise its response to include proper authentication steps, eligibility checking, and alternative options for emergency situations‚Äîdemonstrating how critiquing facilitates adherence to business rules while maintaining helpful customer service. 
Unlike traditional machine learning (ML) processes where feedback serves as a loss function to update model weights, these feedback mechanisms operate differently in inference-time LLM applications. Rather than modifying the underlying model parameters, feedback provides supplementary instructions that dynamically guide the model‚Äôs behavior. This approach allows for behavioral adaptation without the computational expense of retraining, effectively creating a flexible instruction layer that shapes model outputs while preserving the core capabilities of the pre-trained model. Such runtime adaptability represents a significant advancement in making LLMs more responsive to specific requirements without architectural modifications. 
The effectiveness of feedback mechanisms extends beyond simple error correction, enabling LLMs to develop a nuanced understanding of task requirements. Through iterative feedback cycles, models can learn to interpret ambiguous instructions more effectively, identify implicit context, and adapt their processing strategies accordingly. This capability is particularly valuable in enterprise settings where complex, domain-specific tasks require precise interpretation of instructions. By analyzing feedback patterns over time, LLMs can even anticipate potential misunderstandings and proactively adjust their approach, leading to more efficient and accurate outcomes. In our research implementing this approach for financial services classification tasks, we observed substantial performance improvements‚Äîfrom initial accuracy rates of 60% to eventually achieving 100% through systematic feedback incorporation. Each iteration addressed specific weaknesses identified in previous rounds, demonstrating how structured critique leads to continuous model improvement. 
For deeper insights into these mechanisms, we recommend exploring two key research papers: CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing, which demonstrates how LLMs can self-correct with tool-interactive critiquing, and Reflexion: Language Agents with Verbal Reinforcement Learning, which explores language agents with verbal reinforcement learning. The following figure provides a visual representation of this feedback process. 
 
Recent developments in reasoning capabilities have made this feedback process even more powerful. Modern LLMs can now engage in sophisticated analysis of their own outputs, breaking down complex problems into manageable components and systematically evaluating each aspect of their performance. To learn more, see Anthropic‚Äôs Claude 3.7 Sonnet hybrid reasoning model is now available in Amazon Bedrock and DeepSeek-R1 now available as a fully managed serverless model in Amazon Bedrock. This self-analysis capability, combined with external feedback, creates a robust framework for continuous improvement. 
Consider a scenario where an LLM is tasked with sentiment analysis. Initially, when classifying a mixed review like ‚ÄúThe product worked as advertised, but customer service was disappointing,‚Äù the model might incorrectly label it as positive. Through error analysis and verification, a critique mechanism (powered by a separate reasoning model) can provide targeted feedback, explaining that negative statements about service quality significantly impact overall sentiment. This feedback doesn‚Äôt modify the model‚Äôs weights but instead serves as supplementary instruction that enriches the original prompt template, helping the model properly weigh contrasting sentiments within the same text. 
Over multiple feedback iterations, the LLM employs reasoning capabilities to incorporate this external feedback and develop more sophisticated classification heuristics. With the critique system continuously verifying outputs and providing constructive guidance, the model learns to identify why certain patterns lead to misclassifications and refines its approach accordingly. When encountering new ambiguous reviews, it can now apply these learned insights to correctly interpret subtle emotional nuances. This demonstrates how reasoning-based feedback effectively modifies the instruction context without requiring parameter adjustments, allowing for continuous improvement through analytical understanding rather than mechanical optimization. 
In the next section, we explore how these feedback mechanisms and reasoning capability can be operationalized to enhance workflows. 
Solution overview 
The integration of feedback and reasoning creates a powerful learning loop: feedback identifies areas for improvement, reasoning capabilities analyze the root causes of issues, and the resulting insights drive specific, actionable changes. This systematic approach to improvement makes sure that each iteration brings the model closer to optimal performance, while maintaining transparency and accountability in the development process. 
 
For practical examples and complete implementation code of this process, check out our GitHub repository. This repository includes sample datasets, evaluation frameworks, and ready-to-use templates for each step of the optimization workflow. 
Our proposed solution uses two foundation models (FMs) through Amazon Bedrock: Amazon Nova for executing instructional tasks and optimizing the instruction prompt, and Anthropic‚Äôs Claude 3.7 or DeepSeek-R1 for error analysis and feedback generation. Amazon Bedrock, a fully managed service, provides access to high-performance FMs from leading AI companies, enabling flexible model selection and testing. You can explore illustration_notebook_optimization_prompt.ipynb for a quick walkthrough of the high-level process for LLM optimization, which demonstrates key concepts and implementation details in an accessible format. 
LLM optimization workflow 
The following is the high-level process for LLM optimization: 
 
 The process begins with a precise articulation of task requirements and success criteria. This crucial first step involves three key components: defining specific task objectives, crafting a well-structured prompt template with clear instructions, and assembling a comprehensive evaluation dataset with verified ground truth labels. During this phase, we establish quantifiable success metrics and acceptance criteria to measure improvement effectively. The Amazon Nova Pro understanding model is configured to provide both task outputs and detailed explanations for its decisions, enabling transparency in the evaluation process. 
 
For illustration, we started with a simple prompt template to categorize customer inquiries into multiple classes, such as PASSWORD_RESET, ESCALATION, and OUT_OF_SCOPE. This initial template provided only basic category definitions without detailed guidance on edge cases or classification priorities, serving as our baseline for improvement. You can refer to the test case dataset and initial template. 
 
 Following the setup, we conduct rigorous testing against ground truth data to evaluate model performance. This evaluation focuses on both successful and failed cases, with particular emphasis on analyzing misclassifications. The model‚Äôs generated explanations for each decision serve as valuable insights into its reasoning process. We collect both quantitative performance metrics (accuracy, precision, recall) and qualitative insights into error patterns, creating a comprehensive performance baseline. 
 
During this step, we compare model predictions to ground truth labels and record both quantitative metrics and detailed error cases. For example, when a customer urgently reports unauthorized account changes with ‚ÄúSomeone must have accessed my account‚Ä¶I need this fixed immediately‚Äù, the model might incorrectly classify it as CARD_DISPUTE instead of the correct ESCALATION category. Each prediction is logged with its success status (true/false), the model‚Äôs explanation, and the correct label. This comprehensive analysis creates a structured dataset of both successful classifications and failure cases, providing critical input for the reasoning-based optimization in the next step. 
 
 The key step of our optimization process lies in systematic error analysis using a dedicated reasoning framework. This framework examines the model‚Äôs explanations for each error case, identifying root causes and pattern recognition failures. Beyond individual error analysis, we employ pattern recognition to identify systemic issues across multiple cases. The reasoning model, in our case Anthropic‚Äôs Claude 3.7, incorporates historical feedback and learning patterns to generate specific, actionable feedback for prompt improvement. This critical step produces structured, detailed recommendations for prompt optimization. 
 
The reasoning model analyzed classification performance through a structured framework that identified error patterns, investigated prompt-specific root causes, considered historical context from previous iterations, and suggested targeted improvements. This methodical approach focused exclusively on enhancing prompt clarity, structure, and precision‚Äîavoiding model or data modifications outside the scope of prompt engineering. By systematically addressing ambiguities and refining classification criteria, we achieved progressively better performance with each iteration. See the following code: 
 
 critique_prompt_template = """
    Analyze classification performance and provide reasoning for prompt improvements:
    Current Template: ${input_current_template}
    Evaluation Results: ${evaluation_results}
    
    Follow these thinking steps:
    1. Error Pattern Analysis:
    2. Root Cause Investigation:
    3. Historical Context Review:
       ‚Ä¢ Previous suggestions: ${suggestion_history}
    4. Prompt Improvement Ideas:
     
    Output final suggestions between &lt;suggestion&gt; &lt;/suggestion&gt; tags
""" 
 
You can see the detailed implementation in error_analysis_with_reasoning.py. 
 
 Using the structured feedback from the reasoning framework, we implement targeted modifications to the prompt template. These refinements might include enhancing instruction clarity, adjusting classification parameters, or restructuring the prompt format. Each modification directly addresses specific issues identified in the analysis phase, making sure changes are evidence-based and purposeful. The focus remains on improving the instruction layer rather than modifying the underlying model architecture. 
 
To implement these structured improvements, we developed a systematic prompt rewriting mechanism encoded in our prompt_rewrite.py module. This component transforms analytical feedback into concrete prompt enhancements through a dedicated template-based approach. The rewriting process follows a methodical workflow: it preserves essential components like placeholders, incorporates specific improvements identified in the analysis, and makes sure modifications directly address root causes from the feedback. This systematic rewriting approach guarantees that each iteration builds upon previous learnings rather than making arbitrary changes. 
 
 rewrite_prompt_template = """
TASK: Improve the prompt template based on critique feedback.
INPUT:
- Current Template: ${input_current_template}
- Critique Analysis: ${critique_feedbacks}
INSTRUCTIONS:
1. Preserve the current template structure and all placeholders
2. Implement specific improvements identified in the critique
3. Focus on addressing root causes of errors
4. Create a complete, ready-to-use improved template
OUTPUT FORMAT:
- Root cause summary
- Improved template incorporating all recommended changes
The improved template should directly address identified issues while remaining concise and effective.
""" 
 
 
 The optimization process concludes each iteration by testing the refined prompt against the evaluation dataset. We measure performance improvements through comparative analysis of key metrics and conduct quality assessments of new outputs. This phase initiates the next iteration cycle, where successful changes are incorporated into the baseline, and newly identified challenges inform the next round of optimization. This creates a sustainable improvement loop that progressively enhances prompt effectiveness while maintaining detailed documentation of successful strategies. 
 
Through our iterative refinement process, we transformed a basic prompt into a highly effective instruction set for LLMs. Each iteration strategically addressed specific weaknesses identified through our structured analysis framework. For complete documentation of each iteration‚Äôs analysis and improvements, see iteration_log. 
What began as a simple prompt evolved into a comprehensive set of instructions incorporating nuanced task boundaries, explicit priority rules for edge cases, hierarchical decision criteria, and precise handling instructions for corner cases. Rather than modify model weights or architecture, our approach used targeted feedback from a critique mechanism to enhance the instruction layer, effectively guiding model behavior without retraining. Each iteration built upon lessons from previous rounds, systematically addressing error patterns revealed through our critique framework. The feedback served as supplementary instructions that enriched the original prompt template, allowing the model to develop increasingly sophisticated processing heuristics over time. 
Results 
Through these iterative approaches, we benchmarked the solution on the production system. Our comparative analysis between the initial and final prompts revealed several important patterns: 
 
 Boundary confusion was resolved by adding explicit prioritization rules between overlapping categories 
 Edge case handling improved by incorporating specific examples that defined thresholds for categorization 
 Decision transparency increased through structured reasoning requirements in the output format 
 Classification consistency was enhanced by adding counterexamples to help prevent overcategorization in sensitive areas 
 
Through 10 deliberate iterations and the incorporation of detailed task-specific instructions, we achieved a remarkable 34-percentage-point improvement in task effectiveness, transforming a basic prompt with 60% accuracy into a robust classification system with 94% accuracy on challenging cases. This validates not only our iterative optimization strategy but demonstrates how systematic prompt refinement can dramatically enhance LLM model performance without modifying the underlying model architecture. 
Conclusion 
The integration of feedback mechanisms into AI assistant systems represents a significant leap forward in conversational AI capabilities. By implementing robust feedback loops, we‚Äôve demonstrated how AI assistants can evolve from static question-answering systems to dynamic, self-improving resources. The modular subsystem architecture, combined with continuous prompt optimization through feedback, enables AI assistants to handle increasingly complex tasks while maintaining compliance and accuracy. 
As we‚Äôve shown through practical examples and research insights, feedback-driven systems not only produce better outputs but also allow for more effective and streamlined input instructions over time. This efficiency gain is particularly valuable in enterprise environments where precision and adaptability are crucial, and where model retraining is costly or impractical. Each iteration builds upon lessons from previous rounds, systematically addressing error patterns revealed through our critique framework. 
Looking ahead, the continued refinement of feedback mechanisms and prompt engineering techniques will be essential for developing next-generation AI assistant systems. By embracing these approaches, organizations can create AI assistants that not only meet current demands but also adapt to future challenges, delivering increasingly sophisticated and reliable interactions. We invite you to try our proposed feedback-driven prompt optimization approach in your own applications. For those interested in implementing these techniques, Amazon Bedrock provides an ideal landscape for exploring these methods in your specific business contexts, offering a selection of FMs with flexible deployment options. 
 
About the authors 
Jessie Jiao is a Senior Software Engineer at crypto.com, where she leverages her extensive experience in designing, building, and implementing enterprise applications with LLM models and AI technologies. She is passionate about harnessing the power of AI to drive business transformation and enhance operational efficiency. 
Gary Lo is a Solutions Architect at AWS based in Hong Kong. He is a highly passionate IT professional with over 10 years of experience in designing and implementing critical and complex solutions for distributed systems, web applications, and mobile platforms for startups and enterprise companies. Outside of the office, he enjoys cooking and sharing the latest technology trends and insights on his social media platforms with thousands of followers. 
Yanwei Cui, PhD, is a Senior Machine Learning Specialist Solutions Architect at AWS. He started machine learning research at IRISA (Research Institute of Computer Science and Random Systems), and has several years of experience building AI-powered industrial applications in computer vision, natural language processing, and online user behavior prediction. At AWS, he shares his domain expertise and helps customers unlock business potentials and drive actionable outcomes with machine learning at scale. Outside of work, he enjoys reading and traveling. 
Michelle Hong, PhD, works as Prototyping Solutions Architect at Amazon Web Services, where she helps customers build innovative applications using a variety of AWS components. She demonstrated her expertise in machine learning, particularly in natural language processing, to develop data-driven solutions that optimize business processes and improve customer experiences.
‚Ä¢ Build modern serverless solutions following best practices using Amazon Q Developer CLI and MCP
  Building modern serverless applications on AWS requires navigating best practices to manage the integration between multiple services, such as&nbsp;AWS Lambda,&nbsp;Amazon API Gateway,&nbsp;Amazon DynamoDB, and&nbsp;Amazon EventBridge. Security considerations, performance optimization, and implementing a comprehensive monitoring systems adds further requirements to build a serverless architecture while adhering to AWS best practices. 
Amazon Q Developer CLI&nbsp;with&nbsp;Model Context Protocol (MCP)&nbsp;supports getting started with serverless applications using natural language conversations that produce infrastructure configurations and architectural guidance aligned with AWS best practices. This reduces the need to manually research documentation or piece together solutions from disparate sources. 
This post explores how the AWS Serverless MCP server accelerates development throughout the serverless lifecycle, from making architectural decisions with tools like get_iac_guidance and get_lambda_guidance, to streamlining development with get_serverless_templates, sam_init, to deployment with SAM integration, webapp_deployment_help, and configure_domain. We show how this conversational AI approach transforms the entire process, from architecture design through operations, dramatically accelerating AWS serverless projects while adhering to architectural principles. 
Solution overview 
Amazon Q Developer CLI serves as a powerful terminal-based gateway to the AI capabilities of Amazon Q, bringing conversational intelligence directly to your command line environment. Although you can also apply this approach to conversational AI in the interactive development environment‚Äôs (IDE‚Äôs) agentic chat, this innovative interface transforms how developers work by enabling natural language interactions with AWS services and development tools without leaving the terminal. By typing queries or requests in plain English, developers can harness the full potential of generative AI with Amazon Q to solve complex problems, generate code snippets, troubleshoot issues, and access AWS best practices‚Äîall from the familiar comfort of their preferred terminal application. 
However, although Amazon Q CLI provides the conversational interface, it needs a way to connect with external tools and data sources. This is where MCP comes in. 
MCP is a groundbreaking open standard created by Anthropic that bridges the gap between AI models and external data sources or tools. This protocol establishes a unified framework allowing large language models (LLMs) to seamlessly interact with diverse information systems and specialized capabilities. When applied to serverless development workflows, this capability transforms how developers build AWS applications. 
MCP transforms Amazon Q CLI for serverless development by bridging the gap between traditional approaches and modern AI-assisted workflows. This integration replaces manual documentation research and trial-and-error development with conversational access to AWS expertise, automated code generation, and intelligent configuration assistance‚Äîdirectly from the command line. 
Now that we have discussed the conceptual benefits of Amazon Q CLI and MCP, let‚Äôs explore how to set up these tools in your development environment. 
Prerequisites 
You must have the following prerequisites: 
 
 An AWS account with credentials configured 
 The AWS Serverless Application Model Command Line Interface (AWS SAM CLI) installed 
 The AWS Command Line Interface (AWS CLI) installed 
 
Prepare your environment 
Before you can start constructing this serverless solution, you must configure your IDE by installing and setting up Amazon Q CLI and the AWS Serverless MCP server. This section provides comprehensive guidance for properly installing and configuring the required components. 
Install Amazon Q Developer CLI You can install Amazon Q Developer CLI independently on your system. To get started, complete the following steps: 
 
 Confirm successful installation with this terminal command: 
 
 
 q --version  
 
The system should now show something similar to the following output: 
 
 q 1.x.x 
 
 
 Log in to your AWS account: 
 
 
 q login 
 
 
 Select your preferred authentication option: 
   
   Use for free with Builder ID 
   Pro license 
    
 
Set up MCP servers 
Before you can start using MCP servers with Amazon Q CLI, you must install several prerequisites and configure your environment. Complete the following steps: 
Install uv with the following command: 
 
 pip install uv 
 
Install Python 3.10 or newer using the following command: 
 
 uv python install 3.10 
 
Add the following code to your MCP client global configuration (~/.aws/amazonq/mcp.json file). This can be configured by project (.amazonq/mcp.json) in the current folder. The AWS Serverless MCP server uses the default AWS profile by default. Specify a value in AWS_PROFILE if you want to use a different profile. Similarly, adjust the AWS Region and log-level values as needed: 
 
 {
    "mcpServers": {
        "awslabs.aws-serverless-mcp": {
            "command": "uvx",
            "args": [
                "awslabs.aws-serverless-mcp-server"
            ],
            "env": {
                "AWS_PROFILE": "your-aws-profile",
                "AWS_REGION": "us-east-1",
                "FASTMCP_LOG_LEVEL": "ERROR"
            }
        }
    }
} 
 
Amazon Q CLI seamlessly detects and integrates with MCP servers configured in the ~/.aws/amazonq/mcp.json file. 
The MCP servers support several configuration options to customize your development experience: 
 
 Auto-approve ‚Äì Configure which tools can be used without prompting for approval 
 Environment variables ‚Äì Customize server behavior through environment variables 
 Disabled flag ‚Äì Temporarily disable specific servers when not needed 
 
The following is an example of how to configure your AWS Serverless MCP server using the preceding options: 
 
 {
    "mcpServers": {
        "awslabs.aws-serverless-mcp": {
            "command": "uvx",
            "args": [
                "awslabs.aws-serverless-mcp-server"
            ],
            "env": {
                "AWS_PROFILE": "your-aws-profile",
                "AWS_REGION": "us-east-1",
                "FASTMCP_LOG_LEVEL": "ERROR"
            },
            "disabled": false,
            "autoApprove": [
                "search_documentation",
                "read_documentation",
                "recommend"
            ],
            "trusted": [
                "search_documentation",
                "read_documentation",
                "recommend"
            ]
        }
    }
} 
 Test your setup 
 To verify that everything is working correctly, generate a simple serverless function. Start the Amazon Q CLI chat interface and verify the output shows MCP servers being loaded and initialized: 
  
  q chat 
  
  
 The message in the preceding screenshot might not match what you see for your use case, especially when your MCP servers take longer to load. 
 Now that you‚Äôve confirmed your environment is properly configured, you can begin exploring the serverless development workflow. The first step in this journey is making informed architectural decisions that will shape your application. 
 Architecture and design phase 
 The architecture and design phase is where your serverless application takes shape. Making the right decisions at this stage can significantly impact your project‚Äôs success, scalability, and maintainability. The AWS Serverless MCP server provides powerful tools to guide these critical early decisions. 
 When starting a new serverless project, choosing the right infrastructure as code (IaC) approach is crucial for long-term success. The AWS Serverless MCP server‚Äôs get_iac_guidance_tool transforms this decision-making process from hours of research into a simple conversation. This powerful tool provides comprehensive, contextual guidance tailored to your specific needs by analyzing factors such as: 
  
  Project complexity and scale 
  Team expertise and preferences 
  Required deployment flexibility 
  Local testing requirements 
  Long-term maintenance considerations 
  
 Let‚Äôs see how this works in practice by asking Amazon Q about the best IaC approach for your serverless application: 
 What's the best IaC approach for my serverless application? 
 Because Amazon Q is using get_iac_guidance for the first time, it will ask you to enter t to trust this tool for the entire session, y to allow the execution of this tool only one time, or n to reject the execution of this tool. 
 The following example is for an ecommerce application.  
 After approving the tool usage, you will receive a comprehensive, contextual response tailored to your specific architecture needs. For example: 
  
   
   AWS SAM for Your E-Commerce Microservices

Based on your e-commerce microservices architecture, I recommend AWS SAM (Serverless Application Model) as your primary IaC tool for the serverless components of your application. 

Here is why it is particularly well-suited for your needs:

1. Perfect for Your Serverless Components: SAM is specifically designed for the serverless parts of your architecture like the Payment Service (Lambda) and notification systems.

2. Simplified Syntax: SAM provides a simplified way to define Lambda functions, API Gateway, and DynamoDB tables - all key components in your architecture.

3. Local Testing: You can test your Lambda functions locally before deployment, which is crucial for your payment processing service.

4. Integration with Your Architecture: SAM works seamlessly with the API Gateway and Lambda components shown in your diagram.

5. Deployment Independence: Supports your microservices approach where "each microservice can be deployed independently" as mentioned in your documentation.

Example SAM Template for Your Payment Service:

AWSTemplateFormatVersion: "2010-09-09"
Transform: AWS::Serverless-2016-10-31

Resources:
  PaymentFunction:
    Type: AWS::Serverless::Function
    Properties:
      CodeUri: ./payment-service/
      Handler: index.handler
      Runtime: nodejs22.x
      Policies:
        - SQSPollerPolicy:
            QueueName: !GetAtt PaymentQueue.QueueName
      Events:
        SQSEvent:
          Type: SQS
          Properties:
            Queue: !GetAtt PaymentQueue.Arn
            BatchSize: 10

  PaymentQueue:
    Type: AWS::SQS::Queue
    Properties:
      VisibilityTimeout: 300 
   
  Development acceleration phase 
  After you‚Äôve made key architectural decisions, you enter the development phase. This is often the most time-consuming part of the process, requiring significant effort to implement patterns correctly, handle edge cases, and follow best practices. The AWS Serverless MCP server provides tools that dramatically accelerate this phase by using proven patterns and templates. 
  The get_serverless_template tool alleviates the need to start from scratch by providing access to production-ready serverless patterns from the AWS Serverless Land GitHub repository. These patterns are tested, follow AWS best practices, and can be customized to meet your specific requirements. 
  Let‚Äôs say you want to understand more about serverless architecture. With this tool, you can discover and implement common serverless architectures with a simple conversation. For example, you can ask Amazon Q the following question: 
  Show me Node.js API templates for serverless applications 
  Because Amazon Q is using get_serverless_templates for the first time, it will ask you to enter t to trust this tool for the session, y to trust the tool only for this time, or n to reject the use of this tool.  
  You will have to trust at least this tool if not more. After trusting the tool, you should get a response similar to the following: 
   
   Here are some Node.js API templates for serverless applications on AWS:

1. API Gateway to SQS Integration

This template creates a REST API Gateway that directly integrates with AWS SQS to read messages:

Key Features:
- Direct service integration between API Gateway and SQS
- No Lambda function required
- Support for query string parameters
- Ideal for retrieving messages via HTTPS without AWS SDK

Use Cases:
- Web applications polling SQS queues
- Mobile apps requiring secure access to SQS
- Third-party integrations

GitHub Link: https://github.com/aws-samples/serverless-patterns/tree/main/APIGateway-SQS-ReceiveMessages

2. Application Load Balancer with Lambda REST API

While this example uses Python, the pattern is applicable to Node.js as well:

Key Features:
- Path-based listener rules on ALB
- Lambda function as the target
- RESTful API implementation

GitHub Link: https://github.com/aws-samples/serverless-patterns/tree/main/alb-lambda-rest-api-sam-py

3. Full-Stack Integration: Amplify + Cognito + API Gateway + Lambda

This template demonstrates a complete serverless stack with authentication:

Key Features:
- AWS Amplify frontend
- Cognito for authentication
- API Gateway with Cognito authorizer
- Lambda backend
- Environment variables shared between frontend and backend

GitHub Link: https://github.com/aws-samples/serverless-patterns/tree/main/amplify_cognito_apigateway_lambda_envvariables 
   
  Development and testing phase 
  The development and testing phase is where your serverless application takes shape through code. This phase requires tools that streamline the creation, building, and testing of your application components. The AWS Serverless MCP server provides a suite of tools that integrate seamlessly with the AWS SAM CLI, enabling a smooth development experience from project initialization to local testing. 
  Beginning a new serverless project with the right structure and configuration can be challenging. The sam_init tool reduces this hurdle by creating well-structured projects that follow AWS best practices out of the box. This tool provides templates for various serverless application types, helping you start with a solid foundation. 
  For example, you can ask the following question: 
  Create a new SAM project for a Python REST API 
  Because Amazon Q is using sam_init for the first time, it will ask you to enter t to trust this tool for the session, y to trust the tool only for this time, or n to reject the use of this tool.  
  The sam_init tool is used to initialize a new serverless application using AWS SAM. It creates a complete project structure with the necessary files to develop, test, and deploy serverless applications on AWS. 
  The tool has the following key capabilities: 
   
   It creates a new project directory with a standardized structure for serverless applications 
   It generates an AWS SAM template file (template.yaml) that defines your infrastructure as code 
   It sets up the initial code for Lambda functions in your chosen runtime (Python, Node.js, and so on) 
   It configures the project with the appropriate dependency management system 
   
  For your ecommerce microservices architecture, this tool would be particularly useful for: 
   
   Creating individual microservices (like your product catalog service or payment service) 
   Setting up serverless API endpoints through API Gateway 
   Establishing the foundation for event-driven communication between services 
   
  The tool supports various application templates and runtimes, helping you quickly bootstrap different types of serverless applications, from simple APIs to more complex event-processing systems. 
  Deployment and configuration phase 
  After you have developed and tested your serverless application locally, the next step is deploying it to the AWS Cloud. This phase often involves complex, multi-step processes to make sure the components are properly deployed, configured, and integrated. The AWS Serverless MCP server provides specialized tools that simplify these processes, enabling smooth deployments with minimal manual intervention. 
  Streamline full-stack deployments 
  For this use case, we‚Äôre building a full-stack serverless application with a React frontend, Node.js backend APIs, and DynamoDB database. Traditionally, this would require manually configuring multiple AWS services, managing deployment pipelines, and validating proper integration between components. 
  The webapp_deployment_help tool transforms this complex process into a single, streamlined operation that handles the different aspects of deployment while following AWS best practices. 
  You can ask Amazon Q the following question: 
  Deploy my React application to AWS with serverless backend 
  Because Amazon Q is using webapp_deployment_help for the first time, it will ask you to enter t to trust this tool for the session, y to trust the tool only for this time, or n to reject the use of this tool. For this example, trust this tool for the session.  
  This tool supports three deployment types: 
   
   Backend only ‚Äì Deploys a backend application to Lambda with API Gateway 
   Frontend only ‚Äì Deploys a frontend application to Amazon Simple Storage Service (Amazon S3) and Amazon CloudFront 
   Full stack ‚Äì Deploys both backend and frontend components together 
   
  It uses AWS Lambda Web Adapter, so you can deploy standard web applications (like Express.js, Flask, and React) to Lambda without needing to modify your code to follow Lambda-specific conventions. 
  For your ecommerce microservices architecture, this tool is particularly useful because it aligns with several components in your diagram: 
   
   It can deploy your frontend to CloudFront 
   It can deploy your microservices as Lambda functions or containers 
   It integrates with API Gateway for API management 
   
  The tool requires you to build your application first using your framework‚Äôs build command (like npm run build for React), then it handles the deployment to AWS, including setting up the necessary infrastructure. 
  Set up custom domains 
  Although your application is now deployed and accessible using AWS generated URLs, professional applications typically require branded, custom domain names for a better user experience. However, configuring custom domains traditionally involves multiple complex steps across several AWS services. The configure_domain tool simplifies this process into a single, intuitive conversation. 
  You can ask Amazon Q the following question: 
  Configure my custom domain example.com for my serverless application 
  Because Amazon Q is using configure_domain for the first time, it will ask you to enter t to trust this tool for the session, y to trust the tool only for this time, or n to reject the use of this tool.  
  Get the steps to configure your domain as follows: 
   
   Steps to Configure a Custom Domain for Your Serverless Application

1. Prerequisites 
- Ensure you own the domain name (example.com) 
- Have a Route 53 hosted zone for your domain 
- Have deployed your serverless application (e.g., your e-commerce-app)

2. Create an SSL Certificate with AWS Certificate Manager (ACM)
- bash
# Request a certificate for your domain. Region must be us-east-1 for CloudFront distributions
- aws acm request-certificate --domain-name example.com --validation-method DNS --region us-east-1
# Note the certificate ARN from the output

3. Validate the Certificate 
- Go to the ACM console 
- Find your certificate and click on it 
- Create the DNS validation records in Route 53 
- Wait for validation to complete (can take up to 30 minutes) 
   
  The tool orchestrates a comprehensive domain setup process that includes the following steps: 
   
   Creating and validating AWS Certificate Manager (ACM) certificates for secure HTTPS connections 
   Configuring Amazon Route 53 DNS records to point to your application 
   Setting up API Gateway custom domain mappings 
   Associating CloudFront distributions with your domain 
   Establishing proper cache behaviors and security settings 
   
  This automation alleviates the need to navigate between multiple AWS console pages and services, reducing the risk of configuration errors while making sure your domain setup follows security best practices. The tool also handles the complexities of certificate validation and DNS propagation, providing clear guidance throughout the process. 
  With configure_domain, you can transform your application‚Äôs user experience from generic AWS endpoints to professional, branded domains in minutes rather than hours. 
  Having walked through the complete serverless development lifecycle‚Äîfrom architecture decisions to deployment and configuration‚Äîwe can now appreciate the transformative impact of Amazon Q CLI with MCP on the entire process. 
  Next steps 
  To continue your serverless journey with Amazon Q CLI and MCP, we recommend a progressive approach to building your expertise: 
   
   Begin with a simple project like a basic Lambda function triggered by an API Gateway endpoint or Amazon S3 event to familiarize yourself with the conversational development workflow and gain confidence in the generated solutions. 
   Explore different serverless patterns by experimenting with event-driven architectures using EventBridge for decoupled communication, orchestration workflows with AWS Step Functions, or building comprehensive API-backed applications with API Gateway and Lambda.\ 
   As your applications mature, integrate the generated code into your continuous integration and deployment pipelines using services like AWS CodePipeline or AWS CodeBuild to automate testing and deployment processes. 
   Enhance your applications with robust observability by implementing monitoring, logging, and tracing using Amazon CloudWatch, AWS X-Ray, and Powertools for AWS Lambda (Python) to gain insights into performance and troubleshoot issues effectively. 
   Share your experiences and suggestions with the community to contribute to the improvement of MCP servers and help shape the future of AI-assisted serverless development. 
   
  Conclusion 
  By using Amazon Q CLI with MCP servers, you can accelerate your serverless application development while adhering to AWS best practices. This approach helps you focus on your business logic while Amazon Q handles the implementation details, resulting in more robust, scalable, and secure applications. 
  The ability to generate code, configurations, and architecture recommendations in minutes rather than hours creates a comprehensive development experience that makes sure the different components adhere to best practices while providing actionable, specific implementation guidance. The iterative nature of this approach helps teams quickly compare different architectural decisions through simple prompts, while continuously validating their implementations against official AWS documentation. 
  Having explored the capabilities and benefits of this approach, you‚Äôre now ready to apply these concepts in your own projects. 
  As you continue exploring serverless application development with Amazon Q CLI and MCP, we encourage you to deepen your knowledge with the following resources: 
   
   The MCP documentation provides valuable insights into how MCP enhances the capabilities of Amazon Q beyond basic interactions. 
   The AWS SAM documentation offers comprehensive guidance for serverless application development patterns and deployment strategies. 
   To learn more about Amazon Q CLI, refer to Using Amazon Q Developer on the command line. 
   When focusing on architectural excellence, the AWS Well-Architected Framework provides foundational principles for implementing security, reliability, performance efficiency, cost optimization, and operational excellence. 
   For serverless-specific architectural guidance that addresses the unique characteristics of this computing model, refer to the specialized Serverless Lens for Well-Architected Framework tailored specifically for serverless applications. 
   To enhance your development workflow, Powertools for AWS Lambda (Python) offers ready-to-use utilities for structured logging, metrics collection, and distributed tracing. Additionally, the AWS MCP servers documentation offers domain-specific assistance tailored to your project requirements. 
   The serverless community provides valuable resources through AWS Serverless Land, a curated collection of serverless resources, patterns, and examples, and Serverless Stack, an open source framework for building production-ready serverless applications with integrated best practices. 
   
  Start building your next serverless application with Amazon Q CLI and MCP today. 
   
  About the authors 
  Dunieski Otano is a Solutions Architect at Amazon Web Services based out of Miami, Florida. He works with World Wide Public Sector MNO (Multi-International Organizations) customers. His passion is Security, Machine Learning and Artificial Intelligence, and Serverless. He works with his customers to help them build and deploy high available, scalable, and secure solutions. Dunieski holds 14 AWS certifications and is an AWS Golden Jacket recipient. In his free time, you will find him spending time with his family and dog, watching a great movie, coding, or flying his drone. 
  Varun Jasti is a Solutions Architect at Amazon Web Services, working with AWS Partners to design and scale artificial intelligence solutions for public sector use cases to meet compliance standards. With a background in Computer Science, his work covers broad range of ML use cases primarily focusing on LLM training/inferencing and computer vision. In his spare time, he loves playing tennis and swimming. 
  Joel Asante, an Austin-based Solutions Architect at Amazon Web Services (AWS), works with GovTech (Government Technology) customers. With a strong background in data science and application development, he brings deep technical expertise to creating secure and scalable cloud architectures for his customers. Joel is passionate about data analytics, machine learning, and robotics, leveraging his development experience to design innovative solutions that meet complex government requirements. He holds 13 AWS certifications and enjoys family time, fitness, and cheering for the Kansas City Chiefs and Los Angeles Lakers in his spare time.

‚∏ª