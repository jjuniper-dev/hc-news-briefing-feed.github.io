‚úÖ Morning News Briefing ‚Äì October 09, 2025 10:45

üìÖ Date: 2025-10-09 10:45
üè∑Ô∏è Tags: #briefing #ai #publichealth #digitalgov

‚∏ª

üßæ Weather
‚Ä¢ Current Conditions:  -2.0¬∞C
  Temperature: -2.0&deg;C Pressure / Tendency: 103.4 kPa rising Humidity: 97 % Humidity : 97 % Dewpoint: - 2.4&deg:C Wind:  calm km/h Air Quality Health Index: n/a . Pembroke 6:00 AM EDT Thursday 9 October 2025 Temperature:   -2
‚Ä¢ Thursday: Sunny. High 11.
  Wind chill minus 4 this morning . Wind up to 15 km/h. Sunny. High 11. UV index 4 or moderate. Wind chiller minus 4 today. Wind chill plus minus 4 in the morning morning . High of 11.5 degrees Celsius in Ottawa, Canada, on Thursday 9 October 2025. Forecast issued 5:00 AM EDT Thursday 9 Oct 2025. Weather forecast:
‚Ä¢ Thursday night: Clear. Low minus 2.
  Clear. Clear. Wind up to 15 km/h. Low minus 2. Wind chill minus 4 overnight. Clear . Wind chill below zero overnight. Wind chiller minus 4 in wind chill minus 2 overnight. Forecast issued 5:00 AM EDT Thursday 9 October 2025. Forecasts issued for October 9, 2025. For the rest of the year, see www.cnn.

üåç International News
No updates.

üçÅ Canadian News
No updates.

üá∫üá∏ U.S. Top Stories
‚Ä¢ Pope Leo issues new document on poverty
  The apostolic exhortation is Pope Leo's first since his election in May . Pope Leo was elected to the Vatican in May and has been in office since then . The apostolation is the first since the pope's election to the world's highest-ranking church leader in May 2013 . The pope is expected to make a speech at the Vatican's Vatican headquarters in Rome on Thursday
‚Ä¢ In Utah, a group that helped prompt the redistricting says it's acting on faith
  Mormon Women for Ethical Government was one of the plaintiffs in a lawsuit that could overturn Utah's Republican-leaning map for U.S. House seats . That could matter in next year's elections . Utah's GOP-leaning congressional map could be a factor in the next election, especially in Utah's congressional districts . The map could also affect the balance of Congress in the coming months .
‚Ä¢ Need a laptop? This retiree refurbishes laptops, gives them away to those in need
  Craig Clark, 79, refurbishes old laptops and gives them away to people who need them . Clark spends his time refurbishing laptops and giving them away for free to people . Clark: "Tech Fairy" is a volunteer who refurbishes and gives laptops away to those who need to have them . He also refurbishes laptops to give them to people in need of needing them, Clark says
‚Ä¢ Renewable energy outpaces coal for electricity generation in historic first, report says
  Renewable energy generated more electricity for the planet than coal for the first time on record, a new report says . Renewable electricity generated more power for the world than coal, it says . The report was published by the World Institute for Renewable Energy in New York City, New York, USA, on Monday . It says renewable energy generated the most electricity in the world ever recorded .
‚Ä¢ This 4-year-old's heart is failing. A federal grant that might help him was canceled
  Cornell University researcher has been developing an artificial heart for children for more than 20 years . Now, his research is on hold and his lab is shut down . Cornell University researchers have been developing artificial hearts for kids for over 20 years and are now on hold . His lab is being shut down and his research on artificial hearts is being conducted in the U.S. for the first time .

üß† Artificial Intelligence
No updates.

üíª Digital Strategy
‚Ä¢ Zero-day lets nation-state spies cross-examine elite US law firm Williams &amp; Connolly
  Williams &amp; Connolly has confirmed that attackers exploited a zero-day vulnerability to access a handful of attorney email accounts in what it believes was a nation-state-linked cyberattack . The firm represented Bill Clinton, Elizabeth Holmes and Bill Clinton at the time of the attack . It believes the attack was the work of a Chinese state-run state-sponsored hackers . The law firm
‚Ä¢ McKinsey wonders how to sell AI apps with no measurable benefits
  Consultant says software vendors risk hiking prices without cutting costs or boosting productivity . Software vendors keen to monetize AI risk inflating costs for customers without delivering any promised benefits such as reducing employee head count . Consultant: Software vendors should tread cautiously, not overpromising benefits of cutting costs, cutting productivity or reducing head count of staff in the process of developing AI software to boost productivity .
‚Ä¢ Hobble your AI agents to prevent them from hurting you too badly
  CTO of AI security company Zenity makes surprise admission at AI security summit . Michael Bargury, CTO at Zenity, talks to attendees at the company's AI Agent Security Summit on Wednesday . Bargury: AI security is the future of the AI agent security industry in the U.S. and the future is in the form of machine intelligence and AI security . The summit is
‚Ä¢ CoreWeave bets on serverless agent builder to woo penny-pinching enterprises
  CoreWeave has introduced a platform that aims to make reinforcement learning more accessible to enterprise customers . The company says it hopes to make the platform accessible to enterprises . It's a move into the AI services arena with the introduction of a platform aimed at making it easier to use for consumption-based pricing models . The platform will be available in the U.S. market for $1,
‚Ä¢ 3 more infamous cybercrime crews team up to 'maximize income' in 'challenging' ransomware biz
  Ransomware-as-a-service giants DragonForce, Qilin, and LockBit claim to be collaborating on ransomware attacks . Following in the footsteps of an earlier unholy alliance between three other cybercrime crews, three other crews claim to have collaborated on the attacks . The latest in a series of unholy alliances between cybercrime groups, LockBit, DragonForce and Q

üè• Public Health
No updates.

üî¨ Science
‚Ä¢ Feasibility, acceptability and clinical outcomes of the BabyScreen+ genomic newborn screening study
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Unveiling the sedentary epidemic through insights from college students in Guangdong
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Ultra-sensitive DNA sequencing maps mutations that precede cancer
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Exposure to fake news on social media, coping mechanisms, and mental health impact among Vietnamese adolescents and young adults
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ A One Health framework for global and local stewardship across the antimicrobial lifecycle
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations

üßæ Government & Policy
No updates.

üèõÔ∏è Enterprise Architecture & IT Governance
No updates.

ü§ñ AI & Emerging Tech
‚Ä¢ 3 takeaways about climate tech right now
  On Monday, we published our 2025 edition of Climate Tech Companies to Watch. This marks the third time we‚Äôve put the list together, and it‚Äôs become one of my favorite projects to work on every year.¬†



In the journalism world, it‚Äôs easy to get caught up in the latest news, whether it‚Äôs a fundraising round, research paper, or startup failure. Curating this list gives our team a chance to take a step back and consider the broader picture. What industries are making progress or lagging behind? Which countries or regions are seeing quick changes? Who‚Äôs likely to succeed?&nbsp;



This year is an especially interesting moment in the climate tech world, something we grappled with while choosing companies. Here are three of my takeaways from the process of building this list.&nbsp;



1. It‚Äôs hard to overstate China‚Äôs role in energy technology right now.&nbsp;



To put it bluntly, China‚Äôs progress on cleantech is wild. The country is dominating in installing wind and solar power and building EVs, and it‚Äôs also pumping government money into emerging technologies like fusion energy.&nbsp;





We knew we wanted this list to reflect China‚Äôs emergence as a global energy superpower, and we ended up including two Chinese firms in key industries: renewables and batteries.



In 2024, China accounted for the top four wind turbine makers worldwide. Envision was in the second spot, with 19.3 gigawatts of new capacity added last year. But the company isn‚Äôt limited to wind; it‚Äôs working to help power heavy industries like steel and chemicals with technology like green hydrogen.¬†



Batteries are also a hot industry in China, and we‚Äôre seeing progress in tech beyond the lithium-ion cells that currently dominate EVs and energy storage on the grid. We represent that industry with HiNa Battery Technology, a leading startup building sodium-ion batteries, which could be cheaper than today‚Äôs options. The company‚Äôs batteries are already being used in electric mopeds and grid installations.¬†



2. Energy demand from data centers and AI is on everyone‚Äôs mind, especially in the US.&nbsp;



Another trend we noticed this year was a fixation on the growing energy demand of data centers, including massive planned dedicated facilities that power AI models. (Here‚Äôs another nudge to check out our Power Hungry series on AI and energy, in case you haven‚Äôt explored it already.)¬†



Even if their technology has nothing to do with data centers, companies are trying to show how they can be valuable in this age of rising energy demand. Some are signing lucrative deals with tech giants that could provide the money needed to help bring their product to market.&nbsp;



Kairos Power hopes to be one such energy generator, building next-generation nuclear reactors. Last year, the company signed an agreement with Google that will see the company buy up to 500 megawatts of electricity from Kairos‚Äôs first reactors through 2035.¬†



In a more direct play, Redwood Materials is stringing together used EV batteries to build microgrids that could power‚Äîyou guessed it‚Äîdata centers. The company‚Äôs first installation fired up this year, and while it‚Äôs small, it‚Äôs an interesting example of a new use for old technology.¬†



3. Materials continue to be an area that‚Äôs ripe for innovation.&nbsp;



In a new essay that accompanies the list, Bill Gates lays out the key role of innovation in making progress on climate technology. One thing that jumped out at me while I was reading that piece was a number: 30% of global greenhouse-gas emissions come from manufacturing, including cement and steel production.¬†





I‚Äôve obviously covered materials and heavy industry for years. But it still strikes me just how much innovation we still need in the most important materials we use to scaffold our world.&nbsp;



Several companies on this year‚Äôs list focus on materials: We‚Äôve once again represented cement, a material that accounts for 7% of global greenhouse-gas emissions. Cemvision is working to use alternative fuel sources and starting materials to clean up the dirty industry.¬†



And Cyclic Materials is trying to reclaim and recycle rare earth magnets, a crucial technology that underpins everything from speakers to EVs and wind turbines. Today, only about 0.2% of rare earths from recycled devices are recycled, but the company is building multiple facilities in North America in hopes of changing that.¬†



Our list of 10 Climate Tech Companies to Watch highlights businesses we think have a shot at helping the world address and adapt to climate change with the help of everything from established energy technologies to novel materials. It‚Äôs a representation of this moment, and I hope you enjoy taking a spin through it.
‚Ä¢ How healthy am I? My immunome knows the score.
  The story is a collaboration between MIT Technology Review and Aventine, a non-profit research foundation that creates and supports content about how technology and science are changing the way we live.



It‚Äôs not often you get a text about the robustness of your immune system, but that‚Äôs what popped up on my phone last spring. Sent by John Tsang, an immunologist at Yale, the text came after his lab had put my blood through a mind-boggling array of newfangled tests. The result‚Äîthink of it as a full-body, high-resolution CT scan of my immune system‚Äîwould reveal more about the state of my health than any test I had ever taken. And it could potentially tell me far more than I wanted to know.



‚ÄúDavid,‚Äù the text read, ‚Äúyou are the red dot.‚Äù



Tsang was referring to an image he had attached to the text that showed a graph with a scattering of black dots representing other people whose immune systems had been evaluated‚Äîand a lone red one. There also was a score: 0.35.



I had no idea what any of this meant.



The red dot was the culmination of an immuno-quest I had begun on an autumn afternoon a few months earlier, when a postdoc in Tsang‚Äôs lab drew several vials of my blood. It was also a significant milestone in a decades-long journey I‚Äôve taken as a journalist covering life sciences and medicine. Over the years, I‚Äôve offered myself up as a human guinea pig for hundreds of tests promising new insights into my health and mortality. In 2001, I was one of the first humans to have my DNA sequenced. Soon after, in the early 2000s, researchers tapped into my proteome‚Äîproteins circulating in my blood. Then came assessments of my microbiome, metabolome, and much more. I have continued to test-drive the latest protocols and devices, amassing tens of terabytes of data on myself, and I‚Äôve reported on the results in dozens of articles and a book called Experimental Man. Over time, the tests have gotten better and more informative, but no test I had previously taken promised to deliver results more comprehensive or closer to revealing the truth about my underlying state of health than what John Tsang was offering.




Over the years, I‚Äôve offered myself up as a human guinea pig for hundreds of tests promising new insights into my health and mortality. But no test I had previously taken promised to deliver results more comprehensive or closer to revealing the truth about my underlying state of health.




It also was not lost on me that I‚Äôm now 20-plus years older than I was when I took those first tests. Back in my 40s, I was ridiculously healthy. Since then, I‚Äôve been battered by various pathogens, stresses, and injuries, including two bouts of covid and long covid‚Äîand, well, life.



But I‚Äôd kept my apprehensions to myself as Tsang, a slim, perpetually smiling man who directs the Yale Center for Systems and Engineering Immunology, invited me into his office in New Haven to introduce me to something called the human immunome.



John Tsang has helped create a new test for your immune system.
 JULIE BIDWELL




Made up of 1.8 trillion cells and trillions more proteins, metabolites, mRNA, and other biomolecules, every person‚Äôs immunome is different, and it is constantly changing. It‚Äôs shaped by our DNA, past illnesses, the air we have breathed, the food we have eaten, our age, and the traumas and stresses we have experienced‚Äîin short, everything we have ever been exposed to physically and emotionally. Right now, your immune system is hard at work identifying and fending off viruses and rogue cells that threaten to turn cancerous‚Äîor maybe already have. And it is doing an excellent job of it all, or not, depending on how healthy it happens to be at this particular moment.





Yet as critical as the immunome is to each of us, this universe of cells and molecules has remained largely beyond the reach of modern medicine‚Äîa vast yet inaccessible operating system that powerfully influences everything from our vulnerability to viruses and cancer to how well we age to whether we tolerate certain foods better than others.



Now, thanks to a slew of new technologies and to scientists like Tsang, who is on the Steering Committee of the Chan Zuckerberg Biohub New York, understanding this vital and mysterious system is within our grasp, paving the way for powerful new tools and tests to help us better assess, diagnose and treat diseases.



Already, new research is revealing patterns in the ways our bodies respond to stress and disease. Scientists are creating contrasting portraits of weak and robust immunomes‚Äîportraits that someday, it‚Äôs hoped, could offer new insights into patient care and perhaps detect illnesses before symptoms appear. There are plans afoot to deploy this knowledge and technology on a global scale, which would enable scientists to observe the effects of climate, geography, and countless other factors on the immunome. The results could transform what it means to be healthy and how we identify and treat disease.



It all begins with a test that can tell you whether your immune system is healthy or not.



Reading the immunome



Sitting in his office last fall, Tsang‚Äîa systems immunologist whose expertise combines computer science and immunology‚Äî began my tutorial in immunomics by introducing me to a study that he and his team wrote up in a 2024 paper published in Nature Medicine. It described the results of measurements made on blood samples taken from 270 subjects‚Äîtests similar to the ones Tsang‚Äôs team would be running on me. In the study, Tsang and his colleagues looked at the immune systems of 228 patients diagnosed with a variety of genetic disorders and a control group of 42 healthy people.



To help me visualize what my results might look like, Tsang opened his laptop to reveal several colorful charts from the study, punctuated by black dots representing each person evaluated. The results reminded me vaguely of abstract paintings by Joan Mir√≥. But in place of colorful splotches, whirls, and circles were an assortment of scatter plots, Gantt charts, and heat maps tinted in greens, blues, oranges, and purples.



It all looked like gibberish to me.



Luckily, Tsang was willing to serve as my guide. Flashing his perpetually patient smile, he explained that these colorful jumbles depicted what his team had uncovered about each subject after taking blood samples and assessing the details of how well their immune cells, proteins, mRNA, and other immune system components were doing their job.



IBRAHIM RAYINTAKATH




The results placed people‚Äîrepresented by the individual dots‚Äîon a left-to-right continuum, ranging from those with unhealthy immunomes on the left to those with healthy immunomes on the right. Background colors, meanwhile, were used to identify people with different medical conditions affecting their immune systems. For example, olive-green indicated those with auto-immune disorders; orange backgrounds were designated for individuals with no known disease history. Tsang said he and his team would be placing me on a similar graph after they finished analyzing my blood.



Tsang‚Äôs measurements go significantly beyond what can be discerned from the handful of immune biomarkers that people routinely get tested for today. ‚ÄúThe main immune cell panel typically ordered by a physician is called a CBC differential,‚Äù he told me. CBC, which stands for ‚Äúcomplete blood count,‚Äù is a decades-old type of analysis that counts levels of red blood cells, hemoglobin, and basic immune cell types (neutrophils, lymphocytes, monocytes, basophils, and eosinophils). Changes in these levels can indicate whether a person‚Äôs immune system might be reacting to a virus or other infection, cancer, or something else. Other blood tests‚Äîlike one that looks for elevated levels of C-reactive protein, which can indicate inflammation associated with heart disease‚Äîare more specific than the CBC. But they still rely on blunt counting‚Äîin this case of certain proteins.



Tsang‚Äôs assessment, by contrast, tests up to a million cells, proteins, mRNA and immune biomolecules‚Äîsignificantly more than the CBC and others. His protocol is designed to paint a more holistic portrait of a person‚Äôs immune system by not only counting cells and molecules but also by assessing their interactions. The CBC ‚Äúdoesn‚Äôt tell me as a physician what the cells being counted are doing,‚Äù says Rachel Sparks, a clinical immunologist who was the lead author of the Nature Medicine study and is now a translational medicine physician with the drug giant AstraZeneca. ‚ÄúI just know that there are more neutrophils than normal, which may or may not indicate that they‚Äôre behaving badly. We now have technology that allows us to see at a granular level what a cell is actually doing when a virus appears‚Äîhow it‚Äôs changing and reacting.‚Äù




Tsang‚Äôs measurements go significantly beyond what can be discerned from the handful of immune biomarkers that people routinely get tested for today. His assessment tests up to a million cells, proteins, mRNA and immune biomolecules.




Such breakthroughs have been made possible thanks to a raft of new and improved technologies that have evolved over the past decade, allowing scientists like Tsang and Sparks to explore the intricacies of the immunome with newfound precision. These include devices that can count myriad different types of cells and biomolecules, as well as advanced sequencers that identify and characterize DNA, RNA, proteins, and other molecules. There are now instruments that also can measure thousands of changes and reactions that occur inside a single immune cell as it reacts to a virus or other threat.



Tsang and Spark‚Äôs‚Äô team used data generated by such measurements to identify and characterize a series of signals distinctive to unhealthy immune systems. Then they used the presence or absence of these signals to create a numerical assessment of the health of a person‚Äôs immunome‚Äîa score they call an ‚Äúimmune health metric,‚Äù or IHM.



Clinical immunologist Rachel Sparks hopes new tests can improve medical care. JARED SOARES




To make sense of the crush of data being collected, Tsang‚Äôs team used machine-learning algorithms that correlated the results of the many measurements with a patient‚Äôs known health status and age. They also used AI to compare their findings with immune system data collected elsewhere. All this allowed them to determine and validate an IHM score for each person, and to place it on their spectrum, identifying that person as healthy or not.



It all came together for the first time with the publication of the Nature Medicine paper, in which Tsang and his colleagues reported the results from testing multiple immune variables in the 270 subjects. They also announced a remarkable discovery: Patients with different kinds of diseases reacted with similar disruptions to their immunomes. For instance, many showed a lower level of the aptly named natural killer immune cells, regardless of what they were suffering from. Critically, the immune profiles of those with diagnosed diseases tended to look very different from those belonging to the outwardly healthy people in the study. And, as expected, immune health declined in the older patients.





But then the results got really interesting. In a few cases, the immune systems of &nbsp;unhealthy and healthy people looked similar, with some people appearing near the ‚Äúhealthy‚Äù area of the chart even though they were known to have diseases. Most likely this was because their symptoms were in remission and not causing an immune reaction at the moment when their blood was drawn, Tsang told me.&nbsp;



In other cases, people without a known disease showed up on the chart closer to those who were known to be sick. ‚ÄúSome of these people who appear to be in good health are overlapping with pathology that traditional metrics can‚Äôt spot,‚Äù says Tsang, whose Nature Medicine paper reported that roughly half the healthy individuals in the study had IHM scores that overlapped with those of people known to be sick. Either these seemingly healthy people had normal immune systems that were busy fending off, say, a passing virus, or&nbsp; their immune systems had been impacted by aging and the vicissitudes of life. Potentially more worrisome, they were harboring an illness or stress that was not yet making them ill but might do so eventually.



These findings have obvious implications for medicine. Spotting a low immune score in a seemingly healthy person could make it possible to identify and start treating an illness before symptoms appear, diseases worsen, or tumors grow and metastasize. IHM-style evaluations could also provide clues as to why some people respond differently to viruses like the one that causes covid, and why vaccines‚Äîwhich are designed to activate a healthy immune system‚Äîmight not work as well in people whose immune systems are compromised.




Spotting a low immune score in a seemingly healthy person could make it possible to identify and start treating an illness before symptoms appear, diseases worsen, or tumors grow and metastasize.




‚ÄúOne of the more surprising things about the last pandemic was that all sorts of random younger people who seemed very healthy got sick and then they were gone,‚Äù says Mark Davis, a Stanford immunologist who helped pioneer the science being developed in labs like Tsang‚Äôs. ‚ÄúSome had underlying conditions like obesity and diabetes, but some did not. So the question is, could we have pointed out that something was off with these folks‚Äô immune systems? Could we have diagnosed that and warned people to take extra precautions?‚Äù



Tsang‚Äôs IHM test is designed to answer a simple question: What is the relative health of your immune system? But there are other assessments being developed to provide more detailed information on how the body is doing. Tsang‚Äôs own team is working on a panel of additional scores aimed at getting finer detail on specific immune conditions. These include a test that measures the health of a person‚Äôs bone marrow, which makes immune cells. ‚ÄúIf you have a bone marrow stress or inflammatory condition in the bone marrow, you could have lower capacity to produce cells, which will be reflected by this score,‚Äù he says. Another detailed metric will measure protein levels to predict how a person will respond to a virus.



Tsang hopes that an IHM-style test will one day be part of a standard physical exam‚Äîa snapshot of a patient‚Äôs immune system that could inform care. For instance, has a period of intense stress compromised the immune system, making it less able to fend off this season‚Äôs flu? Will someone‚Äôs score predict a better or worse response to a vaccine or a cancer drug? How does a person‚Äôs immune system change with age?



Or, as I anxiously wondered while waiting to learn my own score, will the results reveal an underlying disorder or disease, silently ticking away until it shows itself?



Toward a human immunome project&nbsp;&nbsp;



The quest to create advanced tests like the IHM for the immune system began more than 15 years ago, when scientists like Mark Davis became frustrated with a field in which research‚Äîprimarily in mice‚Äîwas focused mostly on individual immune cells and proteins. In 2007 he launched the Stanford Human Immune Monitoring Center, one of the first efforts to conceptualize the human immunome as a holistic, body-wide network in human beings. Speaking by Zoom from his office in Palo Alto, California, Davis told me that the effort had spawned other projects, including a landmark twin study showing that a lot of immune variation is not genetic, which was then the prevailing theory, but is heavily influenced by environmental factors‚Äîa major shift in scientists‚Äô understanding.



Shai Shen-Orr sees a day when people will check their immune scores on an app. COURTESY OF SHAI SHEN-ORR




Davis and others also laid the groundwork for tests like John Tsang‚Äôs by discovering how a T cell‚Äîamong the most common and important immune players‚Äîcan recognize pathogens, cancerous cells, and other threats, triggering defensive measures that can include destroying the threat. This and other discoveries have revealed many of the basic mechanics of how immune cells work, says Davis, ‚Äúbut there‚Äôs still a lot we have to learn.‚Äù



One researcher working with Davis in those early days was Shai Shen-Orr, who is now director of the Zimin Institute for AI Solutions in Healthcare at the Technion-Israel Institute of Technology, based in Haifa, Israel. (He‚Äôs also a frequent collaborator with Tsang.) Shen-Orr, like Tsang, is a systems immunologist. He recalls that in 2007, when he was a postdoc in Davis‚Äôs lab, immunologists had identified around 100 cell types and a similar number of cytokines‚Äîproteins that act as messengers in the immune system. But they weren‚Äôt able to measure them simultaneously, which limited visibility into how the immune system works as a whole. Today, Shen-Orr says, immunologists can measure hundreds of cell types and thousands of proteins and watch them interact.



Shen-Orr‚Äôs current lab has developed its own version of an immunome test that he calls IMM-AGE (short for ‚Äúimmune age‚Äù), the basics of which were published in a 2019 paper in Nature Medicine. IMM-AGE looks at the composition of people‚Äôs immune systems‚Äîhow many of each type of immune cell they have and how these numbers change as they age. His team has used this information primarily to ascertain a person‚Äôs risk of heart disease.



Shen-Orr also has been a vociferous advocate for expanding the pool of test samples, which now come mostly from Americans and Europeans. ‚ÄúWe need to understand why different people in different environments react differently and how that works,‚Äù he says. ‚ÄúWe also need to test a lot more people‚Äîmaybe millions.‚Äù



Tsang has seen why a limited sample size can pose problems. In 2013, he says, researchers at the National Institutes of Health came up with a malaria vaccine that was effective for almost everyone who got it during clinical trials conducted in Maryland. ‚ÄúBut in Africa,‚Äù he says, ‚Äúit only worked for about 25% of the people.‚Äù He attributes this to the significant differences in genetics, diet, climate, and other environmental factors that cause people‚Äôs immunomes to develop differently. ‚ÄúWhy?‚Äù he asks. ‚ÄúWhat exactly was different about the immune systems in Maryland and Tanzania? That‚Äôs what we need to understand so we can design personalized vaccines and treatments.‚Äù



‚ÄúWhat exactly was different about the immune systems in Maryland and Tanzania? That‚Äôs what we need to understand so we can design personalized vaccines and treatments.‚ÄùJohn Tsang



For several years, Tsang and Shen-Orr have advocated going global with testing, ‚Äúbut there has been resistance,‚Äù Shen-Orr says. ‚ÄúLook, medicine is conservative and moves slowly, and the technology is expensive and labor intensive.‚Äù They finally got the audience they needed at a 2022 conference in La Jolla, California, convened by the Human Immunome Project, or HIP. (The organization was originally founded in 2016 to create more effective vaccines but had recently changed its name to emphasize a pivot from just vaccines to the wider field of immunome science.) It was in La Jolla that they met HIP‚Äôs then-new chairperson, Jane Metcalfe, a cofounder of Wired magazine, who saw what was at stake.



‚ÄúWe‚Äôve got all of these advanced molecular immunological profiles being developed,‚Äù she said, ‚Äúbut we can‚Äôt begin to predict the breadth of immune system variability if we‚Äôre&nbsp; only testing small numbers of people in Palo Alto or Tel Aviv. And that‚Äôs when the big aha moment struck us that we need sites everywhere to collect that information so we can build proper computer models and a predictive understanding of the human immune system.‚Äù



IBRAHIM RAYINTAKATH




Following that meeting, HIP created a new scientific plan, with Tsang and Shen-Orr as chief science officers. The group set an ambitious goal of raising around $3 billion over the next 10 years‚Äîa goal Tsang and Metcalfe say will be met by working in conjunction with a broad network of public and private supporters. Cutbacks in federal funding for biomedical research in the US may limit funds from this traditional source, but HIP plans to work with government agencies outside the US too, with the goal of creating a comprehensive global immunological database.



HIP‚Äôs plan is to first develop a pilot version based on Tsang‚Äôs test, which it will call the Immune Monitoring Kit, to test a few thousand people in Africa, Australia, East Asia, Europe, the US, and Israel. The initial effort, according to Metcalfe, is expected to begin by the end of the year. &nbsp;



After that, HIP would like to expand to some 150 sites around the world, eventually assessing about 250,000 people and collecting a vast cache of data and insights that Tsang believes will profoundly affect‚Äîeven revolutionize‚Äîclinical medicine, public health, and drug development.



My immune health metric score is ‚Ä¶



As HIP develops its pilot study to take on the world, John Tsang, for better or worse, has added one more North American Caucasian male to the small number of people who have received an IHM score to date. That would be me.



It took a long time to get my score, but Tsang didn‚Äôt leave me hanging once he pinged me the red dot. ‚ÄúWe plotted you with other participants who are clinically quite healthy,‚Äù he texted, referring to a cluster of black dots on the grid he had sent, although he cautioned that the group I‚Äôm being compared with includes only a few dozen people. ‚ÄúHigher IHM means better immune health,‚Äù he wrote, referring to my 0.35 score, which he described as a number on an arbitrary scale. ‚ÄúAs you can see, your IHM is right in the middle of a bunch of people 20 years younger.‚Äù



This was a relief, given that our immune system, like so many other bodily functions, declines with age‚Äîthough obviously at different rates. Yet I also felt a certain disappointment. To be honest, I had expected more granular detail after having a million or so cells and markers tested‚Äîlike perhaps some insights on why I got long covid (twice) and others didn‚Äôt. Tsang and other scientists are working on ways to extract more specific information from the tests. Still, he insists that the single score itself is a powerful tool to understand the general state of our immunomes, indicating the absence or presence of underlying health issues that might not be revealed in traditional testing.




To be honest, I had expected more granular detail after having a million or so cells and markers tested‚Äîlike perhaps some insights on why I got long covid (twice) and others didn‚Äôt.




I asked Tsang what my score meant for my future. ‚ÄúYour score is always changing depending on what you‚Äôre exposed to and due to age,‚Äù he said, adding that the IHM is still so new that it‚Äôs hard to know exactly what the score means until researchers do more work‚Äîand until HIP can evaluate and compare thousands or hundreds of thousands of people. They also need to keep testing me over time to see how my immune system changes as it‚Äôs exposed to new perturbations and stresses.



For now, I‚Äôm left with a simple number. Though it tells me little about the detailed workings of my immune system, the good news is that it raises no red flags. My immune system, it turns out, is pretty healthy.



A few days after receiving my score from Tsang, I heard from Shen-Orr about more results. Tsang had shared my data with his lab so that he could run his IMM-AGE protocol on my immunome and provide me with another score to worry about. Shen-Orr‚Äôs result put the age of my immune system at around 57‚Äîstill 10 years younger than my true age.



The coming age of the immunome



Shai Shen-Orr imagines a day when people will be able to check their advanced IHM and IMM-AGE scores‚Äîor their HIP Immune Monitoring Kit score‚Äîon an app after a blood draw, the way they now check health data such as heart rate and blood pressure. Jane Metcalfe talks about linking IHM-type measurements and analyses with rising global temperatures and steamier days and nights to study how global warming might affect the immune system of, say, a newborn or a pregnant woman. ‚ÄúThis could be plugged into other people‚Äôs models and really help us understand the effects of pollution, nutrition, or climate change on human health,‚Äù she says.



‚ÄúI think [in 10 years] I‚Äôll be able to use this much more granular understanding of what the immune system is doing at the cellular level in my patients. And hopefully we could target our therapies more directly to those cells or pathways that are contributing to disease.‚ÄùRachel Sparks



Other clues could also be on the horizon. ‚ÄúAt some point we‚Äôll have IHM scores that can provide data on who will be most affected by a virus during a pandemic,‚Äù Tsang says. Maybe that will help researchers engineer an immune system response that shuts down the virus before it spreads. He says it‚Äôs possible to run a test like that now, but it remains experimental and will take years to fully develop, test for safety and accuracy, and establish standards and protocols for use as a tool of global public health. ‚ÄúThese things take a long time,‚Äù he says.&nbsp;



The same goes for bringing IHM-style tests into the exam room, so doctors like Rachel Sparks can use the results to help treat their patients. ‚ÄúI think in 10 years, with some effort, we really could have something useful,‚Äù says Stanford‚Äôs Mark Davis. Sparks agrees. ‚ÄúI think by then I‚Äôll be able to use this much more granular understanding of what the immune system is doing at the cellular level in my patients,‚Äù she says. ‚ÄúAnd hopefully we could target our therapies more directly to those cells or pathways that are contributing to disease.‚Äù



Personally, I‚Äôll wait for more details with a mix of impatience, curiosity, and at least a hint of concern. I wonder what more the immune circuitry deep inside me might reveal about whether I‚Äôm healthy at this very moment, or will be tomorrow, or next month, or years from now.&nbsp;



David Ewing Duncan&nbsp;is an award-winning science writer. For more information on this story check out his&nbsp;Futures Column on Substack.
‚Ä¢ The Download: carbon removal factories‚Äô funding cuts, and AI toys
  This is today&#8217;s edition of¬†The Download,¬†our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



The Trump administration may cut funding for two major direct-air capture plants



The US Department of Energy appears poised to terminate funding for a pair of large carbon-sucking factories that were originally set to receive more than $1 billion in government grants, according to a department-issued list of projects obtained by MIT Technology Review and circulating among federal agencies.One of the projects is the South Texas Direct Air Capture Hub, a facility that Occidental Petroleum‚Äôs 1PointFive subsidiary planned to develop in Kleberg County, Texas. The other is Project Cypress in Louisiana, a collaboration between Battelle, Climeworks, and Heirloom. Read the full story.



‚ÄîJames Temple







AI toys are all the rage in China‚Äîand now they‚Äôre appearing on shelves in the US too



Kids have always played with and talked to stuffed animals. But now their toys can talk back, thanks to a wave of companies that are fitting children‚Äôs playthings with chatbots and voice assistants.¬†It‚Äôs a trend that has particularly taken off in China: A recent report by the Shenzhen Toy Industry Association and JD.com predicts that the sector will surpass ¬•100 billion ($14 billion) by 2030, growing faster than almost any other branch of consumer AI. But Chinese AI toy companies have their sights set beyond the nation‚Äôs borders. Read the full story.



‚ÄîCaiwei Chen







2025 climate tech companies to watch: Pairwise and its climate-adapted crops



Climate change will make it increasingly difficult to grow crops across many parts of the world. Startup Pairwise is using CRISPR gene editing to develop plants that can better withstand adverse conditions.



The company uses cutting-edge gene editing to produce crops that can withstand increasingly harsh climate conditions, helping to feed a growing population even as the world warms. Last year, it delivered its first food to the US market: a less-bitter‚Äìtasting mustard green. It‚Äôs now working to produce crops with climate-resilient traits, through partnerships with two of the world‚Äôs largest plant biotech companies. Read the full story.



‚ÄîJames Temple



Pairwise is one of our 10 climate tech companies to watch‚Äîour annual list of some of the most promising climate tech firms on the planet. Check out the rest of the list here.







MIT Technology Review Narrated: How to measure the returns on R&amp;D spending



Given the draconian cuts to US federal funding for science, it‚Äôs worth asking some hard-nosed money questions: How much should we be spending on R&amp;D? How much value do we get out of such investments, anyway?



To answer that, in several recent papers, economists have approached this issue in clever new ways.&nbsp; And, though they ask slightly different questions, their conclusions share a bottom line: R&amp;D is, in fact, one of the better long-term investments that the government can make.



This is our latest story to be turned into a MIT Technology Review Narrated podcast, which we‚Äôre publishing each week on Spotify and Apple Podcasts. Just navigate to MIT Technology Review Narrated on either platform, and follow us to get all our new content as it‚Äôs released.







The must-reads



I‚Äôve combed the internet to find you today‚Äôs most fun/important/scary/fascinating stories about technology.



1 How OpenAI and Nvidia are fueling the AI bubble¬†Experts fear their circular deals could be artificially inflating the market. (Bloomberg $)+ OpenAI will pay for AMD‚Äôs chips using, err, AMD‚Äôs own stock. (TechCrunch)+ The Bank of England is concerned about AI inflating tech stocks. (FT $)+ What comes next, that‚Äôs the big question. (NBC News)



2 Around 15% of the world‚Äôs working population is using AIAnd countries in Europe are among the most enthusiastic adopters. (FT $)+ The EU is keen to get even more of its citizens using it, too. (WSJ $)+ Meanwhile, America‚Äôs public opinion towards AI is souring. (WP $)



3 Three quantum mechanics scientists have won the Nobel Prize for PhysicsTwo of whom were instrumental in building Google‚Äôs working quantum machines. (Bloomberg $)+ Their work shone a light on behaviors of the subatomic realm. (NYT $)+ Quantum particles behave in notoriously strange ways. (New Scientist $)



4 The CDC has finally signed off on covid vaccine recommendationsDespite the delay, access looks largely similar to last years‚Äô. (Ars Technica)+ The Supreme Court isn‚Äôt sold on medical expertise these days. (Vox)



5 What makes TikTok so ‚Äòsticky‚Äô¬†Even its hardcore users can be persuaded to keep scrolling for hours. (WP $)



6 ICE bought fake cell towers to spy on nearby phonesIt‚Äôs used cell-site simulators in the past to track down alleged criminals. (TechCrunch)+ Meet the volunteers tracking ICE officers in LA. (New Yorker $)



7 Watermark removers for Sora 2 videos are already readily availableNo permission? No problem. (404 Media)+ What about copyright for AI-generated art? (The Information $)+ And what comes next for AI copyright lawsuits? (MIT Technology Review)



8 How diamonds can help to cool down chipsThey‚Äôre remarkably good at transferring heat. (NYT $)



9 Amazon Pharmacy is launching electronic prescription kiosksFor drugs including antibiotics, asthma inhalers and treatments for high blood pressure. (Reuters)



10 Should you limit your smartphone use to two hours a day?Japan thinks so. (The Guardian)+ How to log off. (MIT Technology Review)







Quote of the day



‚ÄúOpenAI is building the future of AI on infrastructure it doesn&#8217;t own, power it doesn&#8217;t control, and capital it doesn&#8217;t have.‚Äù



‚ÄîAndrey Sidorenko, head of research at data firm Mostly AI, critiques what he calls the consolidation of the AI ecosystem in a post on LinkedIn.







One more thing







How AI can help make cities work betterIn recent decades, cities have become increasingly adept at amassing all sorts of data. But that data can have limited impact when government officials are unable to communicate, let alone analyze or put to use, all the information they have access to.This dynamic has always bothered Sarah Williams, a professor of urban planning and technology at MIT. Shortly after joining MIT in 2012, Williams created the Civic Data Design Lab to bridge that divide. Over the years, she and her colleagues have made urban planning data more vivid and accessible through human stories and striking graphics. Read the full story.



‚ÄîBen Schneider







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)



+ Life lessons from the one and only Ozzy Osbourne‚Äîwhat‚Äôs not to like?+ Did you know that most countries have their own camouflage? Check the patterns out here.+ These hamsters getting an MRI scan is the cutest thing you‚Äôll see today.+ Pumpkin chili sounds like a fantastic way to warm up.
‚Ä¢ The Trump administration may cut funding for two major direct-air-capture plants
  The US Department of Energy appears poised to terminate funding for two large carbon-sucking factories . The two projects were originally set to receive more than $1 billion in government grants . The DOE announced it would terminate about $7.5 billion in grants for more than 200 projects last week . It said they did not adequately advance the nation‚Äôs energy needs, were not economically viable, and would not provide a positive return on investment .
‚Ä¢ AI toys are all the rage in China‚Äîand now they‚Äôre appearing on shelves in the US too
  A recent report predicts that the AI toy industry will surpass ¬•100 billion ($14 billion) by 2030 . According to the Chinese corporation registration database Qichamao, there are over 1,500 AI toy companies operating in China . A toy called BubblePal clips onto a child‚Äôs favorite stuffed animal and makes it ‚Äútalk‚Äù It costs $149 and 200,000 units have been sold since it launched last summer . FoloToy allows parents to customize a bear, bunny, or cactus toy by training it to speak with their own voice and speech pattern .

üîí Cybersecurity & Privacy
‚Ä¢ ShinyHunters Wage Broad Corporate Extortion Spree
  A cybercriminal group that used voice phishing attacks to siphon more than a billion records from Salesforce customers earlier this year has launched a website that threatens to publish data stolen from dozens of Fortune 500 firms if they refuse to pay a ransom. The group also claimed responsibility for a recent breach involving Discord user data, and for stealing terabytes of sensitive files from thousands of customers of the enterprise software maker Red Hat.
The new extortion website tied to ShinyHunters (UNC6040), which threatens to publish stolen data unless Salesforce or individual victim companies agree to pay a ransom.
In May 2025, a prolific and amorphous English-speaking cybercrime group known as ShinyHunters launched a social engineering campaign that used voice phishing to trick targets into connecting a malicious app to their organization&#8217;s Salesforce portal.
The first real details about the incident came in early June, when the Google Threat Intelligence Group (GTIG)¬†warned that ShinyHunters &#8212; tracked by Google as UNC6040 &#8212;¬†was extorting victims over their stolen Salesforce data, and that the group was poised to launch a data leak site to publicly shame victim companies into paying a ransom to keep their records private. A month later, Google acknowledged that one of its own corporate Salesforce instances was impacted in the voice phishing campaign.
Last week, a new victim shaming blog dubbed &#8220;Scattered LAPSUS$ Hunters&#8221; began publishing the names of companies that had customer Salesforce data stolen as a result of the May voice phishing campaign.
&#8220;Contact us to negotiate this ransom or all your customers data will be leaked,&#8221; the website stated in a message to Salesforce. &#8220;If we come to a resolution all individual extortions against your customers will be withdrawn from. Nobody else will have to pay us, if you pay, Salesforce, Inc.&#8221;
Below that message were more than three dozen entries for companies that allegedly had Salesforce data stolen, including Toyota, FedEx, Disney/Hulu, and UPS. The entries for each company specified the volume of stolen data available, as well as the date that the information was retrieved (the stated breach dates range between May and September 2025).
Image: Mandiant.
On October 5, the Scattered LAPSUS$ Hunters victim shaming and extortion blog announced that the group was responsible for a breach in September involving a GitLab server used by Red Hat that contained more than 28,000 Git code repositories, including more than 5,000 Customer Engagement Reports (CERs).
&#8220;Alot of folders have their client&#8217;s secrets such as artifactory access tokens, git tokens, azure, docker (redhat docker, azure containers, dockerhub), their client&#8217;s infrastructure details in the CERs like the audits that were done for them, and a whole LOT more, etc.,&#8221; the hackers claimed.
Their claims came several days after a previously unknown hacker group calling itself the Crimson Collective took credit for the Red Hat intrusion on Telegram.
Red Hat disclosed on October 2 that attackers had compromised a company GitLab server, and said it was in the process of notifying affected customers.
&#8220;The compromised GitLab instance housed consulting engagement data, which may include, for example, Red Hat‚Äôs project specifications, example code snippets, internal communications about consulting services, and limited forms of business contact information,&#8221; Red Hat wrote.
Separately, Discord has started emailing users affected by another breach claimed by ShinyHunters. Discord said an incident on September 20 at a &#8220;third-party customer service provider&#8221; impacted a &#8220;limited number of users&#8221; who communicated with Discord customer support or Trust &amp; Safety teams. The information included Discord usernames, emails, IP address, the last four digits of any stored payment cards, and government ID images submitted during age verification appeals.
The Scattered Lapsus$ Hunters claim they will publish data stolen from Salesforce and its customers if ransom demands aren&#8217;t paid by October 10. The group also claims it will soon begin extorting hundreds more organizations that lost data in August after a cybercrime group stole vast amounts of authentication tokens from Salesloft, whose AI chatbot is used by many corporate websites to convert customer interaction into Salesforce leads.
In a communication sent to customers today, Salesforce emphasized that the theft of any third-party Salesloft data allegedly stolen by ShinyHunters did not originate from a vulnerability within the core Salesforce platform. The company also stressed that it has no plans to meet any extortion demands.
&#8220;Salesforce will not engage, negotiate with, or pay any extortion demand,&#8221; the message to customers read. &#8220;Our focus is, and remains, on defending our environment, conducting thorough forensic analysis, supporting our customers, and working with law enforcement and regulatory authorities.&#8221;
The GTIG tracked the group behind the Salesloft data thefts as UNC6395, and says the group has been observed harvesting the data for authentication tokens tied to a range of cloud services like Snowflake and Amazon&#8217;s AWS.
Google catalogs Scattered Lapsus$ Hunters by so many UNC names (throw in UNC6240 for good measure) because it is thought to be an amalgamation of three hacking groups &#8212; Scattered Spider, Lapsus$ and ShinyHunters. The members of these groups hail from many of the same chat channels on the Com, a mostly English-language cybercriminal community that operates across an ocean of Telegram and Discord servers.
The Scattered Lapsus$ Hunters darknet blog is currently offline. The outage appears to have coincided with the disappearance of the group&#8217;s new clearnet blog &#8212; breachforums[.]hn &#8212; which vanished after shifting its Domain Name Service (DNS) servers from DDoS-Guard to Cloudflare.
But before it died, the websites disclosed that hackers were exploiting a critical zero-day vulnerability in Oracle&#8217;s E-Business Suite software. Oracle has since confirmed that a security flaw tracked as CVE-2025-61882 allows attackers to perform unauthenticated remote code execution, and is urging customers to apply an emergency update to address the weakness.
Mandiant&#8217;s Charles Carmakal shared on LinkedIn that CVE-2025-61882 was initially exploited in August 2025 by the Clop ransomware gang to steal data from Oracle E-Business Suite servers. Bleeping Computer writes that news of the Oracle zero-day first surfaced on the Scattered Lapsus$ Hunters blog, which published a pair of scripts that were used to exploit vulnerable Oracle E-Business Suite instances.
On Monday evening, KrebsOnSecurity received a malware-laced message from a reader that threatened physical violence unless their unstated demands were met. The missive, titled &#8220;Shiny hunters,&#8221; contained the hashtag $LAPSU$$SCATEREDHUNTER, and urged me to visit a page on limewire[.]com to view their demands.
A screenshot of the phishing message linking to a malicious trojan disguised as a Windows screensaver file.
KrebsOnSecurity did not visit this link, but instead forwarded it to Mandiant, which confirmed that similar menacing missives were sent to employees at Mandiant and other security firms around the same time.
The link in the message fetches a malicious trojan disguised as a Windows screensaver file (Virustotal&#8217;s analysis on this malware is here). Simply viewing the booby-trapped screensaver on a Windows PC is enough to cause the bundled trojan to launch in the background.
Mandiant&#8217;s Austin Larsen said the trojan is a commercially available backdoor known as ASYNCRAT, a .NET-based backdoor that communicates using a custom binary protocol over TCP, and can execute shell commands and download plugins to extend its features.
A scan of the malicious screensaver file at Virustotal.com shows it is detected as bad by nearly a dozen security and antivirus tools.
&#8220;Downloaded plugins may be executed directly in memory or stored in the registry,&#8221; Larsen wrote in an analysis shared via email. &#8220;Capabilities added via plugins include screenshot capture, file transfer, keylogging, video capture, and cryptocurrency mining. ASYNCRAT also supports a plugin that targets credentials stored by Firefox and Chromium-based web browsers.&#8221;
Malware-laced targeted emails are not out of character for certain members of the Scattered Lapsus$ Hunters, who have previously harassed and threatened security researchers and even law enforcement officials who are investigating and warning about the extent of their attacks.
With so many big data breaches and ransom attacks now coming from cybercrime groups operating on the Com, law enforcement agencies on both sides of the pond are under increasing pressure to apprehend the criminal hackers involved. In late September, prosecutors in the U.K. charged two alleged Scattered Spider members aged 18 and 19 with extorting at least $115 million in ransom payments from companies victimized by data theft.
U.S. prosecutors heaped their own charges on the 19 year-old in that duo &#8212; U.K. resident Thalha Jubair &#8212;¬†who is alleged to have been involved in data ransom attacks against Marks &amp; Spencer and Harrods, the British food retailer Co-op Group, and the 2023 intrusions at MGM Resorts and Caesars Entertainment. Jubair also was allegedly a key member of LAPSUS$, a cybercrime group that broke into dozens of technology companies beginning in late 2021.
A Mastodon post by Kevin Beaumont, lamenting the prevalence of major companies paying millions to extortionist teen hackers, refers derisively to Thalha Jubair as a part of an APT threat known as &#8220;Advanced Persistent Teenagers.&#8221;
In August, convicted Scattered Spider member and 20-year-old Florida man Noah Michael Urban was sentenced to 10 years in federal prison and ordered to pay roughly $13 million in restitution to victims.
In April 2025, a 23-year-old Scottish man thought to be an early Scattered Spider member was extradited from Spain to the U.S., where he is facing charges of wire fraud, conspiracy and identity theft. U.S. prosecutors allege¬†Tyler Robert Buchanan¬†and co-conspirators hacked into dozens of companies in the United States and abroad, and that he personally controlled more than $26 million stolen from victims.
Update, Oct. 8, 8:59 a.m. ET: A previous version of this story incorrectly referred to the malware sent by the reader as a Windows screenshot file. Rather, it is a Windows screensaver file.

üéì University AI
No updates.

üè¢ Corporate AI
‚Ä¢ Ideas: More AI-resilient biosecurity with the Paraphrase Project
  Behind every emerging technology is a great idea propelling it forward. In the Microsoft Research Podcast series Ideas, members of the research community at Microsoft discuss the beliefs that animate their research, the experiences and thinkers that inform it, and the positive human impact it targets.



AI has been described as a ‚Äúdual use‚Äù technology: the capabilities that can be leveraged for good can also potentially be used to cause harm. In this episode, Microsoft Chief Scientific Officer Eric Horvitz and his guests‚ÄîBruce Wittmann, a senior applied scientist at Microsoft; Tessa Alexanian (opens in new tab), a technical lead at the International Biosecurity and Biosafety Initiative for Science (IBBIS);&nbsp;and James Diggans (opens in new tab), a vice president at Twist Bioscience‚Äîexplore this idea in the context of AI-powered protein design.



With Horvitz at the lead, Alexanian, Diggans, and Wittmann were part of a cross-sector team that demonstrated toxic protein candidates could be designed with help from AI‚Äîand that they could bypass the systems in place to defend against their creation. The project, known as the Paraphrase Project, culminated in a cybersecurity-style response, a more robust protein screening system, and a modified approach to peer review with implications for how we think about and tackle AI risk more broadly. The work was recently published in Science.








Learn more:




Strengthening nucleic acid biosecurity screening against generative protein design toolsPublication | October 2025



Toward AI-Resilient Screening of Nucleic Acid Synthesis Orders: Process, Results, and RecommendationsPreprint | December 2024



The Paraphrase Project: Designing defense for an era of synthetic biologyMicrosoft Research Blog | October 2025



When AI meets biology: Promise, risk, and responsibilityMicrosoft Research Blog | Eric Horvitz | October 2025



Paraphrase ProjectProject homepage










	
		
			Subscribe to the Microsoft Research Podcast:		
		
							
					
						  
						Apple Podcasts
					
				
			
							
					
						
						Email
					
				
			
							
					
						
						Android
					
				
			
							
					
						
						Spotify
					
				
			
							
					
						
						RSS Feed
					
				
					
	




	
		
			
				
					

Transcript



[MUSIC]



ERIC HORVITZ: You‚Äôre&nbsp;listening to&nbsp;Ideas, a Microsoft Research Podcast that dives deep into the world of technology research and the profound questions behind the code.&nbsp;I‚Äôm&nbsp;Eric Horvitz, Microsoft‚Äôs chief scientific officer, and in this series, we explore the technologies shaping our future and the&nbsp;big ideas&nbsp;that propel them forward.



[MUSIC FADES]



Today,&nbsp;I‚Äôm&nbsp;excited to talk about the Paraphrase Project, an effort I co-led exploring how&nbsp;advances in&nbsp;AI tools&nbsp;for protein design&nbsp;might&nbsp;impact&nbsp;biosecurity. The results were reported in our recent paper,&nbsp;‚ÄúStrengthening nucleic acid biosecurity screening against generative protein design tools,‚Äù (opens in new tab)&nbsp;published in&nbsp;Science&nbsp;on Oct. 2.&nbsp;



Joining me are&nbsp;three&nbsp;of the larger set of&nbsp;coauthors on that paper:&nbsp;Bruce Wittmann, senior applied scientist at Microsoft;&nbsp;James&nbsp;Diggans, vice president at Twist Bioscience and chair of the board for the International Gene Synthesis Consortium;&nbsp;and Tessa Alexanian, technical lead at the International Biosecurity and Biosafety Initiative for Science, also known as IBBIS.&nbsp;



				
				
					



Now, let‚Äôs&nbsp;rewind two years.&nbsp;Almost to&nbsp;the day, Bruce and I uncovered a vulnerability. While preparing a case study for a workshop on AI and biosecurity, we discovered that open-source AI protein design tools could be used to redesign toxic proteins in ways that could bypass biosecurity screening systems, systems set up to&nbsp;identify&nbsp;incoming orders of concern.&nbsp;



Now in that work, we&nbsp;created an AI pipeline from open-source tools that could&nbsp;essentially ‚Äúparaphrase‚Äù the amino acid sequences‚Äîreformulating&nbsp;them while&nbsp;working to&nbsp;preserve&nbsp;their structure and potentially their function.&nbsp;



These paraphrased sequences could evade the screening systems used by major DNA&nbsp;synthesis companies, and these are the systems that scientists rely on to safely produce AI-designed proteins.&nbsp;



Now, experts in the field described this finding as the first ‚Äúzero day‚Äù for AI and biosecurity.&nbsp;And this&nbsp;marked the beginning of a deep, two-year collaborative effort to investigate and address this challenge.&nbsp;



With the help of a&nbsp;strong&nbsp;cross-sector team‚Äîincluding James, Tessa, Bruce, and many others‚Äîwe worked behind the scenes to build AI biosecurity&nbsp;red-teaming approaches,&nbsp;probe for vulnerabilities, and to design practical fixes. These ‚Äúpatches,‚Äù akin to those in cybersecurity,&nbsp;have now been shared with&nbsp;organizations&nbsp;globally to strengthen biosecurity screening.&nbsp;



This has been one of the most fascinating projects&nbsp;I‚Äôve&nbsp;had the privilege to work on, for its technical complexity, its ethical and policy dimensions, and the remarkable collaboration across industry, government, and nonprofit sectors.&nbsp;



The project highlights that the&nbsp;same AI tools capable of&nbsp;incredible&nbsp;good can also be misused, requiring us to be vigilant, thoughtful, and creative so we continue to get the most benefit out of AI tools while working to ensure&nbsp;that&nbsp;we avoid costly misuses.&nbsp;



With that, let me officially welcome our guests.



Bruce, James, Tessa, welcome to the podcast.



BRUCE WITTMANN: Thanks, Eric.



JAMES DIGGANS: Thanks for having us.



HORVITZ: It&#8217;s been such a pleasure working closely with each of you, not only for your expertise but also for your deep commitment and passion about public health and global safety.



Before we dive into the technical side of things, I&#8217;d like to ask each of you, how did you get into this field? What inspired you to become biologists and then pursue the implications of advances in AI for biosecurity? Bruce?



WITTMANN:&nbsp;Well, I&#8217;ve always liked building things. That&#8217;s where I would say I come from. You know, my hobbies when I&#8217;m not working on biology or AI things‚Äîas you know, Eric‚Äîis, like, building things around the house, right. Doing construction. That kind of stuff.



But my broader interests have always been biology, chemistry. So I originally got into organic chemistry. I found that was fascinating. From there, went to synthetic biology, particularly metabolic engineering, because that&#8217;s kind of like organic chemistry, but you&#8217;re wiring together different parts of an organism‚Äôs metabolism rather than different chemical reactions. And while I was working in that space, I, kind of, had the thought of there&#8217;s got to be an easier way to do this [LAUGHS] because it is really difficult to do any type of metabolic engineering. And that&#8217;s how I got into the AI space, trying to solve these very complicated biological problems, trying to build things that we don&#8217;t necessarily even understand using our understanding from data or deriving understanding from data.



So, you know, that&#8217;s the roundabout way of how I got to where I am‚Äîthe abstract way of how I got to where I am.



HORVITZ: And, Tessa, what motivated you to jump into this area and zoom into biology and biosciences and helping us to avoid catastrophic outcomes?



ALEXANIAN: Yeah, I mean, probably the origin of me being really excited about biology is actually a book called [The] Lives of [a] Cell (opens in new tab) by Lewis Thomas, which is an extremely beautiful book of essays that made me be like, Oh, wow, life is just incredible. I think I read it when I was, you know, 12 or 13, and I was like, Life is incredible. I want to work on this. This is the most beautiful science, right. And then I, in university, I was studying engineering, and I heard there was this engineering team for engineering biology‚Äîthis iGEM (opens in new tab) team‚Äîand I joined it, and I thought, Oh, this is so cool. I really got to go work in this field of synthetic biology.



And then I also tried doing the wet lab biology, and I was like, Oh, but I don&#8217;t like this part. I don&#8217;t actually, like, like babysitting microbes. [LAUGHTER] I think there&#8217;s a way ‚Ä¶ some people who are great wet lab biologists are made of really stern stuff. And they really enjoy figuring out how to redesign their negative controls so they can figure out whether it was contamination or whether it was, you know, temperature fluctuation. I&#8217;m not that, apparently.



And so I ended up becoming a lab automation engineer because I could help the science happen, but I ‚Ä¶ but my responsibilities were the robots and the computers rather than the microbes, which I find a little bit intransigent.



HORVITZ: Right. I was thinking of those tough souls; they also used their mouths to do pipetting and so on of these contaminated fluids ‚Ä¶



WITTMANN: Not anymore. ALEXANIAN: It&#8217;s true. [LAUGHTER]



DIGGANS: Not anymore. [LAUGHS]



ALEXANIAN: They used to be tougher. They used to be tougher.



HORVITZ: James.



DIGGANS: So I did my undergrad in computer science and microbiology, mostly because at the time, I couldn&#8217;t pick which of the two I liked more. I liked them both. And by the time I graduated, I was lucky enough that I realized that the intersection of the two could be a thing. And so I did a PhD in computational biology, and then I worked for five years at the MITRE Corporation. It‚Äôs a nonprofit. I got the chance to work with the US biodefense community and just found an incredible group of people working to protect forces and the population at large from biological threats and just learned a ton about both biology and also dual-use risk. And then so when Twist called me and asked if I wanted to join Twist and set up their biosecurity program, I leapt at the chance and have done that for the past 10 years.



HORVITZ: Well, thanks everyone.



I believe that AI-powered protein design in particular is one of the most exciting frontiers of modern science. It holds promise for breakthroughs in medicine, public health, even material science. We&#8217;re already seeing it lead to new vaccines, novel therapeutics, and‚Äîon the scientific front‚Äîpowerful insights into the machinery of life.



So there&#8217;s much more ahead, especially in how AI can help us promote wellness, longevity, and the prevention of disease. But before we get too far ahead, while some of our listeners work in bioscience, many may not have a good understanding of some of the foundations.



So, Bruce, can you just give us a high-level overview of proteins? What are they? Why are they important? How do they figure into human-designed applications?



WITTMANN: Sure. Yeah. Fortunately, I used to TA a class on AI for protein design, so it‚Äôs right in my wheelhouse. [LAUGHS]



HORVITZ: Perfect, perfect background. [LAUGHS]



WITTMANN:&nbsp;It&#8217;s perfect. Yeah. I got to go back to all of that. Yeah, so from the very basic level, proteins are the workhorses of life.



Every chemical reaction that happens in our body‚Äîwell, nearly every chemical reaction that happens in our body‚Äîmost of the structure of our cells, you name it. Any life process, proteins are central to it.



Now proteins are encoded by what are known as ‚Ä¶ well, I shouldn&#8217;t say encoded. They are constructed from what are called amino acids‚Äîthere are 20 of them‚Äîand depending on the combination and order in which you string these amino acids together, you get a different protein sequence. So that&#8217;s what we mean when we say protein sequence.



The sequence of a protein then determines what shape that protein folds into in a cell, and that shape determines what the protein does. So we will often say sequence determines structure, which determines function.



Now the challenge that we face in engineering proteins is just how many possibilities there are. For all practical purposes, it&#8217;s infinite. So we have 20 building blocks. There are on average around 300 amino acids in a protein. So that&#8217;s 20 to the power of 300 possible combinations. And a common reference point is that it&#8217;s estimated there are around 10 to the 80 particles in the observable universe. So beyond astronomical numbers of possible combinations that we could have, and the job of a protein engineer is to find that one or a few of the proteins within that space that do what we want it to do.



So when a human has an idea of, OK, here&#8217;s what I want a protein to do, we have various techniques of finding that desired protein, one of which is using artificial intelligence and trying to either sift through that milieu of potential proteins or, as we&#8217;ll talk about more in this podcast, physically generating them. So creating them in a way, sampling them out of some distribution of reasonable proteins.



HORVITZ: Great. So I wanted to throw it to James now to talk about how protein design goes from computer to reality‚Äîfrom in silico to test tubes. What role does Twist Bioscience (opens in new tab) play in transforming digital protein designs into synthesized proteins? And maybe we can talk also about what safeguards are in place at your company and why do we need them.



DIGGANS: So all of these proteins that Bruce has described are encoded in DNA. So the language that our cells use to kind of store the information about how to make these proteins is all encoded in DNA. And so if you as an engineer have designed a protein and you want to test it to see if it does what you think it does, the first step is to have the DNA that encodes that protein manufactured, and companies like Twist carry out that role.



So we are cognizant also, however, that these are what are called dual-use technologies. So you can use DNA and proteins for an incredible variety of amazing applications. So drug development, agricultural improvements, bioindustrial manufacturing, all manner of incredible applications. But you could also potentially use those to cause harm so toxins or other, you know, sort of biological misuse.



And so the industry has since at least 2010 recognized that they have a responsibility to make sure that when we&#8217;re asked to make some sequence of DNA that we understand what that thing is encoding and who we&#8217;re giving it for. So we&#8217;re screening both the customer that&#8217;s coming to us and we&#8217;re screening the sequence that they&#8217;re requesting.



And so Twist has long invested in a very, sort of, complicated system for essentially reverse engineering the constructs that we&#8217;re asked to make so that we understand what they are. And then a system where we engage with our customers and make sure that they&#8217;re going to use those for legitimate purpose and responsibly.



HORVITZ: And how do the emergence of these new generative AI tools influence how you think about risk?



DIGGANS: A lot of the power of these AI tools is they allow us to make proteins or design proteins that have never existed before in nature to carry out functions that don&#8217;t exist in the natural world. That&#8217;s an extremely powerful capability.



But the existing defensive tools that we use at DNA synthesis companies generally rely on what&#8217;s called homology, similarity to known naturally occurring sequences, to determine whether something might pose risk. And so AI tools kind of break the link between those two things.



HORVITZ: Now you also serve as chair of the International Gene Synthesis Consortium (opens in new tab). Can you tell us a little bit more about the IGSC, its mission, how it supports global biosecurity?



DIGGANS: Certainly. So the IGSC was founded in 2010[1] and right now has grown to more than 40 companies and organizations across 10 countries. And the IGSC is essentially a place where companies who might be diehard competitors in the market around nucleic acid synthesis come together and design and develop best practices around biosecurity screening to, kind of, support the shared interest we all have in making sure that these technologies are not subject to misuse.



HORVITZ: Thanks, James. Now, Tessa, your organization, IBBIS (opens in new tab) is focused‚Äîit&#8217;s a beautiful mission‚Äîon advancing science while minimizing catastrophic risk, likelihood of catastrophic risk. When we say catastrophic risk, what do we really mean, Tessa, in the context of biology and AI? And how is that ‚Ä¶ do you view that risk landscape as evolving as AI capabilities are growing?



ALEXANIAN: I think the ‚Ä¶ to be honest, as a person who&#8217;s been in biosecurity for a while, I&#8217;ve been surprised by how much of the conversation about the risks from advances in artificial intelligence has centered on the risk of engineered biological weapons and engineered pandemics.



Even recently, there was a new discussion on introducing redlines for AI that came up at the UN General Assembly. And the very first item they list in their list of risks, if I&#8217;m not mistaken, was engineered pandemics, which is exactly the sort of thing that people fear could be done, could be done with these biological AI tools.



Now, I think that when we talk about catastrophic risk, we talk about, you know, something that has an impact on a large percentage of humanity. And I think the reason that we think that biotechnologies pose a catastrophic risk is that we believe there, as we&#8217;ve seen with many historical pandemics, there&#8217;s a possibility for something to emerge or be created that is beyond our society&#8217;s ability to control.



You know, there were a few countries in COVID that managed to more or less successfully do a zero-COVID policy, but that was not, that was not most countries. That was not any of the countries that I lived in. And, you know, we saw millions of people die. And I think we believe that with something like the 1918 influenza, which had a much higher case fatality rate, you could have far more people die.



Now, why we think about this in the context of AI and where this connects to DNA synthesis is that, you know, there is a ‚Ä¶ these risks of both, sort of, public health risks, pandemic risks, and misuse risks‚Äîpeople deliberately trying to do harm with biology, as we&#8217;ve seen from the long history of biological weapons programs‚Äîyou know, we think that those might be accelerated in a few different ways by AI technology, both the potential ‚Ä¶ and I say potential here because as everyone who has worked in a wet lab‚Äîwhich I think is everyone on this call‚Äîknows, engineering biology is really difficult. So there&#8217;s maybe a potential for it to become easier to develop biological technology for the purposes of doing harm, and there&#8217;s maybe also the potential to create novel threats.



And so I think people talk about both of those, and people have been looking hard for possible safeguards. And I think one safeguard that exists in this biosecurity world that, for example, doesn&#8217;t exist as cleanly in the cybersecurity world is that none of these biological threats can do harm until they are realized in physical reality, until you actually produce the protein or produce the virus or the microorganism that could do harm. And so I think at this point of production, both in DNA synthesis and elsewhere, we have a chance to introduce safeguards that can have a really large impact on the amount of risk that we&#8217;re facing‚Äîas long as we develop those safeguards in a way that keeps pace with AI.



HORVITZ: Well, thanks, Tessa. So, Bruce, our project began when I posed a challenge to you of the form: could current open-source AI tools be tasked with rewriting toxic protein sequences in a way that preserves their native structure, and might they evade today&#8217;s screening systems?



And I was preparing for a global workshop on AI and biosecurity that I&#8217;d been organizing with Frances Arnold, David Baker, and Lynda Stuart, and I wanted a concrete case study to challenge attendees. And what we found was interesting and deeply concerning.



So I wanted to dive in with you, Bruce, on the technical side. Can you describe some about the generative pipeline and how it works and what you did to build what we might call an AI and biosecurity red-teaming pipeline for testing and securing biosecurity screening tools?



WITTMANN: Sure. Yeah. I think the best place to start with this is really by analogy.



An analogy I often use in this case is the type of image generation AI tools we&#8217;re all familiar with now where I can tell the AI model, &#8220;Hey, give me a cartoonish picture of a dog playing fetch.&#8221; And it&#8217;ll do that, and it&#8217;ll give us back something that is likely never been seen before, right. That exact image is new, but the theme is still there. The theme is this dog.



And that&#8217;s kind of the same technology that we&#8217;re using in this red-teaming pipeline. Only rather than using plain language, English, we&#8217;re passing in what we would call conditioning information that is relevant to a protein.



So our AI models aren&#8217;t at the point yet where I can say, &#8220;Give me a protein that does x.&#8221; That would be the dream. We&#8217;re a long way from that. But what instead we do is we pass in things that match that theme that we&#8217;re interested in. So rather than saying, &#8220;Hey, give me back the theme on a dog,&#8221; we pass in information that we know will cause or at least push this generative model to create a protein that has the characteristics that we want.



So in the case of that example you just mentioned, Eric, it would be the protein structure. Like I mentioned earlier, we usually say structure determines function. There&#8217;s obviously a lot of nuance to that, but we can, at a first approximation, say structure determines function. So if I ask an AI model, ‚ÄùHey, here&#8217;s this structure; give me a protein sequence that folds to this structure,‚Äù just like with that analogy with the dog, it&#8217;s going to give me something that matches that structure but that is likely still never been seen before. It&#8217;s going to be a new sequence.



So you can imagine taking this one step further. In the red-teaming pipeline, what we would do is take a protein that should normally be captured by DNA synthesis screening‚Äîthat would be captured by DNA synthesis screening‚Äîfind its structure, pass it through one of these models, and get variants on the theme of that structure so these new sequences, these synthetic homologs that you mentioned, paraphrased, reformulated, whatever phrase we want to use to describe them.



And they have a chance or a greater chance than not of maintaining the structure and so maintaining the function while being sufficiently different that they&#8217;re not detected by these tools anymore.



So that&#8217;s the nuts and bolts of how the red-teaming pipeline comes together. We use more tools than just structure. I think structure is the easiest one to understand. But we have a suite of tools in there, each pass different conditioning information that causes the model to generate sequences that are paraphrased versions of potential proteins of concern.



HORVITZ: But to get down to brass tacks, what Bruce did for the framing study was ‚Ä¶ we took the toxic, well-known toxic protein ricin, as we described in a framing paper that&#8217;s actually part of the appendix now to the Science publication, and we generated through this pipeline, composed of open-source tools, thousands of AI-rewritten versions of ricin.



And this brings us to the next step of our project, way back when, at the early ‚Ä¶ in the early days of this effort, where Twist Bioscience was one of the companies we approached with what must have seemed like an unusual question to your CEO, in fact, James: would you be open to testing whether current screening systems could detect thousands of AI-rewritten versions of ricin, a well-known toxic protein?



And your CEO quickly connected me with you, James. So, James, what were your first thoughts on hearing about this project, and how did you respond to our initial framing study?



DIGGANS: I think my first response was gratitude and excitement. So it was fantastic that Microsoft had really leaned forward on this set of ideas and had produced this dataset. But to have it, you know, show up on our doorstep in a very concrete way with a partner that was ready to, sort of, help us try and address that, I think was a really ‚Ä¶ a valuable opportunity. And so we really leapt at that.



HORVITZ: And the results were that both for you and another company, major producer IDT [Integrated DNA Technologies], those thousands of variants flew through ‚Ä¶ flew under the radar of the biosecurity screening software as we covered in that framing paper.



Now, after our initial findings on this, we quietly shared the paper with a few trusted contacts, including some in government. Through my work with the White House Office of Science and Technology Policy, or OSTP, we connected up with biosecurity leads there, and it was an OSTP biosecurity lead who described our results as the first zero day in AI and biosecurity. And now in cybersecurity, a zero day is a vulnerability unknown to defenders generally, meaning there&#8217;s no time to respond before it could be exploited should it be known.



In that vein, we took a cybersecurity approach. We stood up a CERT‚ÄîC-E-R-T‚Äîa cybersecurity [computer] emergency response team approach used in responding to cybersecurity vulnerabilities, and we implemented this process to address what we saw as a vulnerability with AI-enabled challenges to biosecurity.



At one point down the line, it was so rewarding to hear you say, James, ‚ÄúI&#8217;m really glad Microsoft got here first.‚Äù I&#8217;m curious how you think about this kind of AI-enabled vulnerability compared to other ones, biosecurity threats, you&#8217;ve encountered, and I&#8217;d love to hear your perspective on how we handled the situation from the early discovery to the coordination and outreach.



DIGGANS: Yeah, I think in terms of comparison known threats, the challenge here is really there is no good basis on which we can just, sort of, say, Oh, I&#8217;ll build a new tool to detect this concrete universe of things, right. This was more a pattern of I&#8217;m going to use tools‚Äîand I love the name ‚ÄúParaphrase‚Äù; it&#8217;s a fantastic name‚ÄîI can paraphrase anything that I would normally think of as biological ‚Ä¶ as posing biological risk, and now that thing is harder to detect for existing tools. And so that really was a very eye-opening experience, and I think the practice of forming this CERT response, putting together a group of people who were well versed not just in the threat landscape but also in the defensive technologies, and then figuring out how to mitigate that risk and broaden that study, I think, was a really incredibly valuable response to the entire synthesis industry.



HORVITZ: Yeah, and, Bruce, can you describe a little bit about the process by which we expanded the effort beyond our initial framing study to more toxins and then to a larger challenge set and then the results that we pursued and achieved?



WITTMANN:&nbsp;Yeah, of course. So, you know, using machine learning lingo, you don&#8217;t want to overfit to a single example. So early on with this, as part of the framing study, we were able to show or I should say James and coworkers across the screening field were able to show that this could be patched, right. We needed to just make some changes to the tools,&nbsp;and we could at the very least detect ricin or reformulated versions of ricin.



So the next step of course was then, OK, how generalizable are these patches? Can they detect other reformulated sequences, as well? So we had to expand the set of proteins that we had reformulated. We couldn&#8217;t just do 10s of thousands of ricins. We had to do 10s of thousands of name your other potentially hazardous ‚Ä¶



HORVITZ: I think we had 72, was it?



WITTMANN:&nbsp;It was 72 in the end that we ended up at. I believe, James, it was you and maybe Jake, another one of the authors on the list ‚Ä¶ on the paper, who primarily put that list together &#8230;



HORVITZ: This is Jacob Beal ‚Ä¶ Jacob Beal at Raytheon BBN.



WITTMANN: I think James actually might be the better one to answer how this list was expanded.



DIGGANS:&nbsp;Initially the focus [was] on ricin as a toxin so that list expanded to 62 sort of commonly controlled toxins that are subject to an export control restriction or other concern. And then on top of that, we added 10 viral proteins. So we didn&#8217;t really just want to look at toxins. We also wanted to look at viral proteins, largely because those proteins tend to have multiple functions. They have highly constrained structures. And so if we could work in a toxin context, could Paraphrase also do the same for viral proteins, as well.



HORVITZ: And, Bruce, can you describe some about how we characterize the updates and the, we&#8217;ll say, the boost in capabilities of the patched screening tools?



WITTMANN: So we had, like you said, Eric, 72 base proteins or template proteins. And for each of those, we had generated a few 100 to a couple thousand reformulated variants of them. The only way to really get any sense of validity of those sequences was to predict their structures. So we predicted protein structures for I think it was 70ish thousand protein structures in the end that we had to predict and score them using in silico metrics. So things like, how similar is this to that template, wild-type protein structure that we used as our conditioning information?



We put them on a big grid. So we have two axes. We have on the x-axis‚Äîand this is a figure in our paper‚Äîthe quality of the prediction. It&#8217;s essentially a confidence metric: how realistic is this protein sequence? And on the other axis is, how similar is the predicted structure of this variant to the original? And ultimately, what we were wanting to see was the proteins that scored well in both of those metrics, so that showed up in the top right of that diagram, were caught primarily, because these are again the ones that are most likely, having to say most likely, to retain function of the original.



So when you compare the original tools‚ÄîTool Series A, right, the unpatched tools‚Äîwhat you&#8217;ll find is varying degrees of success in the top right. It varied by tool. But in some cases, barely anything being flagged as potentially hazardous. And so improvement is then in the next series‚ÄîSeries B, the patched version of tools‚Äîwe have more flagged in that upper-right corner.



HORVITZ: And we felt confident that we had a more AI-resilient screening solution across the companies, and, James, at this point, the whole team decided it was time to disclose the vulnerability as well as the patch details and pointers to where to go for the updated screening software and to communicate this to synthesis companies worldwide via the IGSC. This was probably July, I think, of 2024. What was that process like, and how did members respond?



DIGGANS: I think members were really grateful and excited. To present to that group, to say, hey, this activity (a) has gone on, (b) was successful, and (c) was kept close hold until we knew how to mitigate this, I think everyone was really gratified by that and comforted by the fact that now they had kind of off-the-shelf solutions that they could use to improve their resilience against any incoming heavily engineered protein designs.



HORVITZ: Thanks, James.



Now, I know that we all understand this particular effort to be important but a piece of the biosecurity and AI problem. I&#8217;m just curious to ‚Ä¶ I‚Äôll ask all three of you to just share some brief reflections.



I know, Bruce, you&#8217;ve been on ‚Ä¶ you‚Äôve stayed on this, and we‚Äôve‚Äîall of us on the original team‚Äîhave other projects going on that are pushing on the frontiers ahead of where we were with this paper when we published it.



Let me start with Tessa in terms of, like, what new risks do you see emerging as AI accelerates and maybe couple that with thoughts about how do we proactively get ahead of them.



ALEXANIAN: Yeah, I think with the Paraphrase‚Äôs work, as Bruce explained so well, you know, I sometimes use the metaphor of the previous response that the IGSC had to do, the synthesis screening community, where it used to be you could look for similarities to DNA sequences, and then everyone started doing synthetic biology where they were doing codon optimization so that proteins could express more efficiently in different host organisms, and now all of a sudden, well, you&#8217;ve scrambled your DNA sequence and it doesn&#8217;t look very similar even though your protein sequence actually still looks, you know, very similar or often the same once it&#8217;s been translated from DNA to protein, and so that was a, you know, many, many in the industry were already screening both DNA and protein, but they had to start screening ‚Ä¶ everybody had to start screening protein sequences even just to do the similarity testing as these codon optimization tools became universal.



I feel like we&#8217;re, kind of, in a similar transition phase with protein-design, protein-rephrasing, tools where, you know, these tools are still in many cases drawing from the natural distribution of proteins. You know, I think some of the work we saw in, you know, designing novel CRISPR enzymes, you go, OK, yeah, it is novel; it&#8217;s very unlike any one CRISPR enzyme. But if you do a massive multiple sequence alignment of every CRISPR enzyme that we know about, you&#8217;re like, OK, this fits in the distribution of those enzymes. And so, you know, I think we&#8217;re not ‚Ä¶ we&#8217;re having to do a more flexible form of screening, where we look for things that are kind of within distribution of natural proteins.



But I feel like broadly, all of the screening tools were able to respond by doing something like that. And I think &#8230; I still feel like the clock is ticking down on that and that as the AI tools get better at predicting function and designing, sort of, novel sequences to pursue a particular function, you know‚Äîyou have tools now that can go from Gene Ontology terms to a potential structure or potential sequence that may again be much farther out of the distribution of natural protein‚ÄîI think all of us on the screening side are going to have to be responding to that, as well.



So I think I see this as a necessary ongoing engagement between people at the frontier of designing novel biology and people at the frontier of producing all of the materials that allow that novel biology to be tested in the lab. You know, I think this feels like the first, you know, detailed, comprehensive zero day disclosure and response. But I think that&#8217;s ‚Ä¶ I think we&#8217;re going to see more of those. And I think what I&#8217;m excited about doing at IBBIS is trying to encourage and set up more infrastructure so that you can, as an AI developer, disclose these new discoveries to the people who need to respond before the publication comes out.



HORVITZ: Thank you, Tessa.



The, the ‚Ä¶ Bruce, I mean, you and I are working on all sorts of dimensions. You&#8217;re leading up some efforts at Microsoft, for example, on the foundation model front and so on, among other directions. We&#8217;ve talked about new kinds of embedding models that might go beyond sequence and structure. Can you talk a little bit about just a few of the directions that just paint the larger constellation of the kinds of things that we talk about when we put our worry hats on?



WITTMANN:&nbsp;I feel like that could have its own dedicated podcast, as well. There&#8217;s a lot ‚Ä¶ [LAUGHTER] there&#8217;s a lot to talk about.



HORVITZ: Yeah. We want to make sure that we don&#8217;t tell the world that the whole problem is solved here.



WITTMANN:&nbsp;Right, right, right. I think Tessa said it really, really well in that most of what we&#8217;re doing right now, it&#8217;s a variant on a known theme. I have to know the structure that does something bad to be able to pass it in as context. I have to know some existing sequence that does something bad to pass it in.



And obviously the goal is to move away from that in benign applications, where when I&#8217;m designing something, I often want to design it because nothing exists [LAUGHS] that already does it. So we are going to be heading to this space where we don&#8217;t know what this protein does. It&#8217;s kind of a circular problem, right, where we&#8217;re going to need to be able to predict what some obscure protein sequence does in order to be able to still do our screening.



Now, the way that I think about this, I often think about it beyond just DNA synthesis screening. It&#8217;s one line of defense, and there needs to be many lines of defense that come into play here that go beyond just relying on this one roadblock. It&#8217;s a very powerful roadblock. It&#8217;s a very powerful barrier. But we need to be proactively thinking about how we broaden the scope of defenses. And there are lots of conversations that are ongoing. I won&#8217;t go into the details of them. Again, that would be its own podcast.



But primarily my big push‚Äîand I think this is emerging consensus in the field, though I don&#8217;t want to speak for everybody‚Äîis it needs to ‚Ä¶ any interventions we have need to come more at the systems level and less at the model level, primarily because this is such dual-use technology. If it can be used for good biological design, it can be used for bad biological design. Biology has no sense of morality. There is no bad protein. It&#8217;s just a protein.



So we need to think about this differently than how we would maybe think about looking at the outputs of that image generator model that I spoke about earlier, where I can physically look at an image and say, don&#8217;t want my model producing that, do want my model producing that. I don&#8217;t have that luxury in this space. So it&#8217;s a totally different problem. It&#8217;s an evolving problem. Conversations are happening about it, but the work is very much not done.



HORVITZ: And, James, I want to give you the same open question, but I&#8217;d like to apply what Bruce just said on system level and so on and in the spirit of the kind of things that you&#8217;re very much involved with internationally to also add to it, just get some comments on programs and policies that move beyond technical solutions for governance mechanisms‚Äîlogging, auditing nucleic acid orders, transparency, various kinds‚Äîthat might complement technical approaches like Paraphrase and their status today.



DIGGANS: Yeah, I&#8217;m very gratified that Bruce said that we, the synthesis industry, should not be the sole bulwark against misuse. That is very comforting and correct.



Yeah, so the US government published a guidance document in 2023 that essentially said you, the entire biotech supply chain, have a responsibility to make sure that you&#8217;re evaluating your customers. You should know your customer; you know that they&#8217;re legitimate. I think that&#8217;s an important practice.



Export controls are designed to minimize the movement of equipment and materials that can be used in support of these kinds of misuse activities. And then governments have really been quite active in trying to incentivize, you know, sort of what we would think of as positive behavior, so screening, for example, in DNA synthesis companies. The US government created a framework in 2024, and it&#8217;s under a rewrite now to basically say US research dollars will only go to companies who make nucleic acid who do these good things. And so that is using, kind of, the government-funding carrot to, kind of, continue to build these layers of defense against potential misuse.



HORVITZ: Thanks. Now, discussing risk, especially when it involves AI and biosecurity, isn&#8217;t always easy. As we&#8217;ve all been suggesting, some worry about alarming the public or arming bad actors. Others advocate for openness as a principle of doing science with integrity.



A phase of our work as we prepared our paper was giving serious thought to both the benefits and the risks of transparency about what it was that we were doing. Some experts encouraged full disclosure as important for enhancing the science of biosecurity. Other experts, all experts, cautioned against what are called information hazards, the risk of sharing the details to enable malevolent actions with our findings or our approach.



So we faced a real question: how can we support open science while minimizing the risk of misuse? And we took all the input we got, even if it was contradictory, very seriously. We carefully deliberated about a good balance, and even then, once we chose our balance and submitted our manuscript to Science, the peer reviewers came back and said they wanted some of the more sensitive details that we withheld with explanations as to why.



So this provoked some thinking out of the box about a novel approach, and we came up with a perpetual gatekeeping strategy where requests for access to sensitive methods and data and even the software across different risk categories would be carefully reviewed by a committee and a process for access that would continue in perpetuity.



Now, we brought the proposal to Tessa and her team at IBBIS‚Äîthis is a great nonprofit group; look at their mission‚Äîand we worked with Tessa and her colleagues to refine a workable solution that was accepted by Science magazine as a new approach to handling information hazards as first demonstrated by our paper.



So, Tessa, thank you again for helping us to navigate such a complex challenge. Can you share your perspective on information hazards? And then walk us through how our proposed system ensures responsible data and software sharing.



ALEXANIAN: Yeah. And thanks, Eric.



It&#8217;s all of the long discussions we had among the group of people on this podcast and the other authors on the paper and many people we engaged, you know, technical experts, people in various governments, you know, we heard a lot of contradictory advice.



And I think it showed us that there isn&#8217;t a consensus right now on how to handle information hazards in biotechnology. You know, I think ‚Ä¶ I don&#8217;t want to overstate how much of a consensus there is in cybersecurity either. If you go to DEF CON, you&#8217;ll hear people about how they&#8217;ve been mistreated in their attempts to do responsible disclosure for pacemakers and whatnot. But I think we&#8217;re ‚Ä¶ we have even less of a consensus when it comes to handling biological information.



You know, you have some people who say, oh, because the size of the consequences could be so catastrophic if someone, you know, releases an engineered flu or something, you know, we should just never share information about this. And then you have other people who say there&#8217;s no possibility of building defenses unless we share information about this. And we heard very strong voices with both of those perspectives in the process of conducting this study.



And I think what we landed on that I&#8217;m really excited about and really excited to get feedback on now that the paper is out, you know, if you go and compare our preprint, which came out in December of 2024, and this paper in October 2025, you&#8217;ll see a lot of information got added back in.



And I&#8217;m excited to see people&#8217;s reaction to that because even back in January 2025, talking with people who were signatories to the responsible biodesign commitments, they were really excited that this was such an empirically concrete paper because they&#8217;d maybe read a number of papers talking about biosecurity risks from AI that didn&#8217;t include a whole lot of data, you know, often, I think, because of concerns about information hazards. And they found the arguments in this paper are much more convincing because we are able to share data.



So the process we underwent that I felt good about was trying to really clearly articulate, when we talk about an information hazard, what are we worried about being done with this data? And if we put this data in public, completely open source, does it shift the risk at all? You know, I think doing that kind of marginal contribution comparison is really important because it also let us make more things available publicly.



But there were a few tiers of data that after a lot of discussion amongst the authors of the paper, we thought, OK, potentially someone who wanted to do harm, if they got access to this data, it might make it easier for them. Again, not necessarily saying it, you know, it opens the floodgates, but it might make it easier for them. And when we thought about that, we thought, OK, you know, giving all of those paraphrased protein sequences, maybe, maybe that, you know, compared to having to set up the whole pipeline with the open-source tools yourself, just giving you those protein sequences, maybe that makes your life a bit easier if you&#8217;re trying to do harm.



And then we thought, OK, giving you those protein sequences plus whether or not they were successfully flagged, maybe that makes your life, you know, quite a bit easier. And then finally, we thought, OK, the code that we want to share with some people who might try to reproduce these results or might try to build new screening systems that are more robust, we want to share the code with them. But again, if you have that whole code pipeline just prepared for you, it might really help make your life easier if you&#8217;re trying to do harm.



And so we, sort of, sorted the data into these three tiers and then went through a process actually very inspired by the existing customer screening processes in nucleic acid synthesis about how to determine, you know, we tried to take an approach not of what gets you in but what gets you out. You know, for the most part, we think it should be possible to access this data.



You know, if you have an affiliation with a recognizable institution or some good explanation of why you don&#8217;t have one right now, you know, if you have a reason for accessing this data, it shouldn&#8217;t be too hard to meet those requirements, but we wanted to have some in place. And we wanted it to be possible to rule out some people from getting access to this data. And so we&#8217;ve tried to be extremely transparent about what those are. If you go through our data access process and for some reason you get rejected, you&#8217;ll get a list of, &#8220;Here&#8217;s the reasons we rejected you. If you don&#8217;t think that&#8217;s right, get back to us.&#8221;



So I&#8217;m really excited to pilot this in part because I think, you know, we&#8217;re already in conversations with some other people handling potential bio-AI information hazard about doing a similar process for their data of, you know, tiering it, determining which gates to put in which tiers, but I really hope a number of people do get access through the process or if they try and they fail, they tell us why. Because I think as we move toward this world of potentially, you know, biology that is much easier to engineer, partly due to dual-use tools, you know, my dream is it&#8217;s, like, still hard to engineer harm with biology, even if it&#8217;s really easy to engineer biology. And I think these, kind of, new processes for managing access to things, this sort of like, you know, open but not completely public, I think those can be a big part of that layered defense.



HORVITZ: Thanks, Tessa. So we&#8217;re getting close to closing, and I just thought I would ask each of you to just share some reflections on what we&#8217;ve learned, the process we&#8217;ve demonstrated, the tools, the policy work that we did, this idea of facing the dual-use dilemma with ‚Ä¶ even at the information hazard level, with sharing information versus withholding it. What do you think about how our whole end to end of the study, now reaching the two-year point, can help other fields facing dual-use dilemmas?



Tessa, Bruce, James ‚Ä¶ James, have you ever thought about that? And we&#8217;ll go to Bruce and then Tessa.



DIGGANS:&nbsp;Yeah, I think it was an excellent model. I would like to see a study like this repeated on a schedule, you know, every six months because from where I sit, you know, the tools that we used for this project are now two years old. And so capabilities have moved on. Is the picture the same in terms of defensive capability? And so using that model over time, I think, would be incredibly valuable. And then using the findings to chart, you know, how much should we be investing in alternative strategies for this kind of risk mitigation for AI tool ‚Ä¶ the products of AI tools?



HORVITZ: Bruce.



WITTMANN:&nbsp;Yeah, I think I would extend on what James said. The anecdote I like to point out about this project is, kind of, our schedule. We found the vulnerability and it was patched within a week, two weeks, on all major synthesis screening platforms. We wrote the paper within a month. We expanded on the paper within two months, and then we spent a year and a half to nearly two years [LAUGHS] trying to figure out what goes into the paper; how do we release this information; you know, how do we do this responsibly?



And my hope is similar to what James said. We&#8217;ve made it easier for others to do this type of work. Not this exact work; it doesn&#8217;t have to necessarily do with proteins. But to do this type of work where you are dealing with potential hazards but there is also value in sharing and that hopefully that year and a half we spent figuring out how to appropriately share and what to share will not be a year and a half for other teams because these systems are in place or at least there is an example to follow up from. So that&#8217;s my takeaway.



HORVITZ: Tessa, bring us home‚Äîbring us home! [LAUGHS]



ALEXANIAN: Bring us home! Let&#8217;s do it faster next time. [LAUGHTER] Come talk to any of us if you&#8217;re dealing with this kind of stuff. You know, I think IBBIS, especially, we want to be a partner for building those layers of defense and, you know, having ripped out our hair as a collective over the past year and a half about the right process to follow here, I think we all really hope it&#8217;ll be faster next time.



And I think, you know, the other thing I would encourage is if you&#8217;re an AI developer, I would encourage you to think about how your tool can strengthen screening and strengthen recognition of threats.



I know James and I have talked before about how, you know, our Google search alerts each week send us dozens of cool AI bio papers, and it&#8217;s more like once a year or maybe once every six months, if we&#8217;re lucky, that we get something that&#8217;s like applying AI bio to biosecurity. So, you know, if you&#8217;re interested in these threats, I think we&#8217;d love to see more work that&#8217;s directly applied to facing these threats using the most modern technology.



HORVITZ: Well said.



Well, Bruce, James, Tessa, thank you so much for joining me today and for representing the many collaborators, both coauthors and beyond, who made this project possible.



It&#8217;s been a true pleasure to work with you. I&#8217;m so excited about what we&#8217;ve accomplished, the processes and the models that we&#8217;re now sharing with the world. And I&#8217;m deeply grateful for the collective intelligence and dedication that really powered the effort from the very beginning. So thanks again.



[MUSIC]



WITTMANN: Thanks, Eric.



DIGGANS: Thank you.



ALEXANIAN: Thank you.



[MUSIC FADES]

				
			
			
				Show more			
		
	








[1] The original organization was founded in 2009 and became the International Gene Synthesis Consortium in 2010.
Opens in a new tabThe post Ideas: More AI-resilient biosecurity with the Paraphrase Project appeared first on Microsoft Research.
‚Ä¢ When AI Meets Biology: Promise, Risk, and Responsibility
  Advances in AI are opening extraordinary frontiers in biology. AI-assisted protein engineering holds the promise of new medicines, materials, and breakthroughs in scientific understandings. Yet these same technologies also introduce biosecurity risks and may lower barriers to designing harmful toxins or pathogens. This ‚Äúdual-use‚Äù potential, where the same knowledge can be harnessed for good or misuse to cause harm, poses a critical dilemma for modern science.



Great Promise‚Äîand Potential Threat



I‚Äôm excited about the potential for AI-assisted protein design to drive breakthroughs in biology and medicine. At the same time, I‚Äôve also studied how these tools could be misused. In computer-based studies, we found that AI protein design (AIPD) tools could generate modified versions of proteins of concern, such as ricin. Alarmingly, these reformulated proteins were able to evade the biosecurity screening systems used by DNA synthesis companies, which scientists rely on to synthesize AI-generated sequences for experimental use. 



In our paper published in Science on October 2, ‚ÄúStrengthening nucleic acid biosecurity screening against generative protein design tools (opens in new tab),‚Äù we describe a two-year confidential project we began in late 2023 while preparing a case study for a workshop on AI and biosecurity.



We worked confidentially with partners across organizations and sectors for 10 months to develop AI biosecurity ‚Äúred-teaming‚Äù methods that allowed us to better understand vulnerabilities and craft practical solutions‚Äî&#8221;patches‚Äù that have now been adopted globally, making screening systems significantly more AI-resilient.



Summary of AIPD red-teaming workflow.



For structuring, methods, and process in our study, we took inspiration from the cybersecurity community, where ‚Äúzero-day‚Äù vulnerabilities are kept confidential until a protective patch is developed and deployed. Following the acknowledgment by a small group of workshop attendees of a zero-day for AI in biology, we worked closely with stakeholders‚Äîincluding synthesis companies, biosecurity organizations, and policymakers‚Äîto rapidly create and distribute patches that improved detection of AI-redesigned protein sequences. We delayed public disclosure until protective measures were in place and widely adopted.



Dilemma of Disclosure



The dual use dilemma also complicates how we share information about vulnerabilities and safeguards. Across AI and other fields, researchers face a core question: 




How can scientists share potentially risk-revealing methods and results in ways that enable progress without offering a roadmap for misuse?




We recognized that our work itself‚Äîdetailing methods and failure modes‚Äîcould be exploited by malicious actors if published openly. To guide decisions about what to share, we held a multi-stakeholder deliberation involving government agencies, international biosecurity organizations, and policy experts. Opinions varied: some urged full transparency to maximize reproducibility‚Äîand to help others to build on our work; others stressed restraint to minimize risk. It was clear that a new model of scientific communication was needed, one that could balance openness and security.



The Novel Framework



The risk of sharing dangerous information through biological research has become a growing concern. We have participated in community-wide discussion on the challenges, including a recent National Academies of Science, Engineering, and Medicine workshop and study.&nbsp;



In preparing our manuscript for publication, we worked on designing a process to limit the spread of dangerous information while still enabling scientific progress.&nbsp;



To address the dual challenges, we devised a tiered access system for data and methods, implemented in partnership with the International Biosecurity and Biosafety Initiative for Science (IBBIS) (opens in new tab), a nonprofit dedicated to advancing science while reducing catastrophic risks. The system works as follows:




Controlled access: Researchers can request access through IBBIS, providing their identity, affiliation, and intended use. Requests are reviewed by an expert biosecurity committee, ensuring that only legitimate scientists conducting relevant research gain access.



Stratified tiers of information: Data and code are classified into several tiers according to their potential hazard, from low-risk summaries through sensitive technical data to critical software pipelines.



Safeguards and agreements: Approved users sign tailored usage agreements, including non-disclosure terms, before receiving data.



Resilience and longevity: Provisions are built in for declassification when risks subside, and for succession of stewardship to trusted organizations should IBBIS be unable to continue its operation.




This framework allows replication and extension of our work while guarding against misuse. Rather than relying on secrecy, it provides a durable system of responsible access.



To ensure continued funding for the storage and responsible distribution of sensitive data and software, and for the operation of the sharing program, we provided an endowment to IBBIS to support the program in perpetuity. This approach was modeled after the One Hundred Year Study on AI at Stanford, which is endowed to continue for the life of the university.



An Important Step in Scientific Publishing



We are pleased that the leadership at Science accepted our approach to handling information hazards. To our knowledge, this is the first time a leading scientific journal has formally endorsed a tiered-access approach to manage an information hazard. This recognition validates the idea that rigorous science and responsible risk management can coexist‚Äîand that journals, too, can play a role in shaping how sensitive knowledge is shared. We acknowledge the visionary leadership at Science, including editors, Michael Funk and Valda Vinson, and Editor-in-Chief, Holden Thorp.



Beyond Biology: A Model for Sensitive Research



While developed for AI-powered protein design, our approach offers a generalizable model for dual-use research of concern (DURC) across disciplines. Whether in biology, chemistry, or emerging technologies, scientists will increasingly confront situations where openness and security pull in opposite directions. Our experience shows that these values can be balanced: with creativity, coordination, and new institutional mechanisms, science can uphold both reproducibility and responsibility.



We hope this framework becomes a template for future projects, offering a way forward for researchers who wish to share their insights without amplifying risks. By embedding resilience into how knowledge is communicated‚Äînot just what is communicated‚Äîwe can ensure that scientific progress continues to serve humanity safely.



The responsible management of information hazards is no longer a peripheral concern: it is central to how science will advance in the age of powerful technologies like AI. This approach to managing information hazards demonstrates a path forward, where novel frameworks for access and stewardship allow sensitive but vital research to be shared, scrutinized, and extended responsibly. Approaches like this will be critical to ensuring that scientific openness and societal safety advance hand-in-hand.







Additional reading



Strengthening nucleic acid biosecurity screening against generative protein design tools.



The Age of AI in the Life Sciences: Benefits and Biosecurity Considerations, National Academies of Science, Engineering, and Medicine, 2025. (opens in new tab)



Disseminating In Silico and Computational Biological Research: Navigating Benefits and Risks: Proceedings of a Workshop, National Academies of Science, Engineering, and Medicine, 2025. (opens in new tab)



Protecting scientific integrity in an age of generative AI, Proceedings of the National Academy of Science, 2024. (opens in new tab)
Opens in a new tabThe post When AI Meets Biology: Promise, Risk, and Responsibility appeared first on Microsoft Research.
‚Ä¢ Vxceed builds the perfect sales pitch for sales teams at scale using Amazon Bedrock
  This post was co-written with Cyril Ovely from Vxceed. 
Consumer packaged goods (CPG) companies face a critical challenge in emerging economies: how to effectively retain revenue and grow customer loyalty at scale. Although these companies invest 15‚Äì20% of their revenue in trade promotions and retailer loyalty programs, the uptake of these programs has historically remained below 30% due to their complexity and the challenge of addressing individual retailer needs. 
Vxceed‚Äôs Lighthouse platform tackles this challenge with its innovative loyalty module. Trusted by leading global CPG brands across emerging economies in Southeast Asia, Africa, and the Middle East, Lighthouse provides field sales teams with a cutting-edge, AI-driven toolkit. This solution uses generative AI to create personalized sales pitches based on individual retailer data and trends, helping field representatives effectively engage retailers, address common objections, and boost program adoption. 
In this post, we show how Vxceed used Amazon Bedrock to develop this AI-powered multi-agent solution that generates personalized sales pitches for field sales teams at scale. 
The challenge: Solving a revenue retention problem for brands 
Vxceed operates mostly in the emerging economies. The CPG industry is facing challenges such as constant change, high customer expectations, and low barriers to entry. These challenges are more pronounced in the emerging economies. To combat these challenges, CPG companies worldwide invest 15‚Äì20% of their revenue annually in trade promotions, often in the format of loyalty programs to retailers. 
The uptake of these loyalty programs, however, has traditionally been lower than 30% due to their complexity and the need to address each individual outlet‚Äôs needs. To make this challenge more complex, in emerging economies, these loyalty programs are primarily sold through the field sales team, who also act in the role of order capture and fulfilment, and the scale of their operation often spans across millions of outlets. To uplift the loyalty programs uptake, which in turn uplifts the brands revenue retention, the loyalty programs needed to be tailored at a personalized level and pitched properly to each outlet. 
Vxceed needed a solution to solve this problem at scale, creating unique, personalized loyalty program selling stories tailored for each individual outlet that the field sales team can use to sell the programs. 
This challenge led Vxceed to use Amazon Bedrock, a fully managed service that offers a choice of high-performing foundation models (FMs) from leading AI companies through a single API. 
Solution overview 
To address the challenges of personalization, scale, and putting the solution in the hands of tens of thousands of field sales teams, Vxceed developed Lighthouse Loyalty Selling Story, an AI-powered solution. The Lighthouse Loyalty Selling Story architecture uses Amazon Bedrock, Amazon API Gateway, Amazon DynamoDB, and AWS Lambda to create a secure, scalable, AI-powered selling story generation system. The solution implements a multi-agent architecture, shown in the following figure, where each component operates within the customer‚Äôs private AWS environment, maintaining data security, scalability, and intuitive user interactions.  The solution architecture is built around several key components that work together to provide a curated sales enablement experience that is unique for each retailer customer: 
 
 Salesperson app ‚Äì A mobile application is used by field sales teams to access compelling program sales pitches and interact with the system through a chat interface. This serves as the primary touchpoint for sales representatives. 
 API Gateway and security ‚Äì The solution uses the following security services: 
   
   API Gateway serves as the entry point for application interactions. 
   Security is enforced using AWS Key Management Service (AWS KMS) for encryption and AWS Secrets Manager for secure credentials management. 
   Amazon Simple Storage Service (Amazon S3) is used for image storage and management. 
    
 Intelligent agents ‚Äì The solution uses the following Lambda based agents: 
   
   Orchestration Agent coordinates the overall flow and interaction between components. 
   Story Framework Agent establishes the narrative structure. 
   Story Generator Agent creates personalized content. 
   Story Review Agent maintains quality and compliance with brand guidelines. 
   Brand Guidelines Agent maintains brand consistency. 
   Business Rules Agent enforces business logic and constraints. 
    
 Data services layer ‚Äì The data services layer consists of the following components: 
   
   Data API services provide access to critical business information, including: 
     
     Outlet profile data 
     Loyalty program details 
     Historical data 
     Purchase profile information 
      
   Integration with Lighthouse artificial intelligence and machine learning (AI/ML) models and data lake for advanced analytics. 
   Amazon Bedrock Knowledge Bases for enhanced context and information. 
    
 Advanced capabilities ‚Äì The solution offers the following additional capabilities: 
   
   Q&amp;A Service enables natural language interactions for sales queries. 
   CTA (Call-to-Action) Service streamlines the retail outlet signup process. 
   An Amazon Bedrock large language model (LLM) powers intelligent responses. 
   Amazon Bedrock Guardrails facilitates appropriate and compliance-aligned interactions. 
    
 
The architecture implements a secure, scalable, and serverless design that uses AWS managed services to deliver a sophisticated sales enablement solution. 
Multi-agent AI architecture for secure orchestration 
Vxceed built a multi-agent AI system on Lambda to manage personalized sales storytelling. The architecture comprises specialized agents that work together to create, validate, and deliver compelling sales pitches while maintaining alignment with business rules and brand guidelines. 
The following is a detailed breakdown of the multi-agent AI architecture: 
 
 Orchestration Agent ‚Äì Coordinates the workflow between agents and manages the overall story creation process, interfacing with the Amazon Bedrock LLM for intelligent processing. 
 Story Framework Agent ‚Äì Establishes the narrative structure and flow of sales pitches based on proven storytelling patterns and sales methodologies. 
 Story Generator Agent ‚Äì Creates personalized content by combining data from multiple sources, including outlet profiles, loyalty program details, and historical data. 
 Story Review Agent ‚Äì Validates generated content for accuracy, completeness, and effectiveness before delivery to sales personnel. 
 Brand Guidelines Agent ‚Äì Makes sure generated content adheres to brand voice, tone, and visual standards. 
 Business Rules Agent ‚Äì Enforces business logic, customer brand compliance requirements, and operational constraints across generated content. 
 
Each agent is implemented as a serverless Lambda function, enabling scalable and cost-effective processing while maintaining strict security controls through integration with AWS KMS and Secrets Manager. The agents interact with the Amazon Bedrock LLM and guardrails to provide appropriate and responsible AI-generated content. 
Guardrails 
Lighthouse uses Amazon Bedrock Guardrails to maintain professional, focused interactions. The system uses denied topics and word filters to help prevent unrelated discussions and unprofessional language, making sure conversations remain centered on customer needs. These guardrails screen out inappropriate content, establish clear boundaries around sensitive topics, and diplomatically address competitive inquiries while staying aligned with organizational values. 
Why Vxceed chose Amazon Bedrock 
Vxceed selected Amazon Bedrock over other AI solutions because of four key advantages: 
 
 Enterprise-grade security and privacy ‚Äì With Amazon Bedrock, you can configure your AI workloads and data so your information remains securely within your own virtual private cloud (VPC). This approach maintains a private, encrypted environment for AI operations, helping keep data protected and isolated within the your VPC. For more details, refer to Security in Amazon Bedrock. 
 Managed services on AWS ‚Äì Lighthouse Loyalty Selling Story runs on Vxceed‚Äôs existing AWS infrastructure, minimizing integration effort and providing end-to-end control over data and operations using managed services such as Amazon Bedrock. 
 Access to multiple AI models ‚Äì Amazon Bedrock supports various FMs, so Vxceed can experiment and optimize performance across different use cases. Vxceed uses Anthropic‚Äôs Claude 3.5 Sonnet for its ability to handle sophisticated conversational interactions and complex language processing tasks. 
 Robust AI development tools ‚Äì Vxceed accelerated development by using Amazon Bedrock Knowledge Bases, prompt engineering libraries, and agent frameworks for efficient AI orchestration. 
 
Business impact and future outlook 
The implementation delivered significant measurable improvements across three key areas. 
Enhanced customer service 
The solution achieved a 95% response accuracy rate while automating 90% of loyalty program-related queries. This automation facilitates consistent, accurate responses to customer objections and queries, helping salespeople and significantly improving the retailer experience. 
Accelerated revenue growth 
Early customer feedback and industry analysis indicate program enrollment increased by 5‚Äì15%. This growth demonstrates how removing friction from the enrollment process directly impacts business outcomes. 
Improved operational efficiency 
The solution delivered substantial operational benefits: 
 
 20% reduction in enrolment processing time 
 10% decrease in support time requirements 
 Annual savings of 2 person-months per geographical region in administrative overhead 
 
These efficiency gains help Vxceed customers focus on higher-value activities while reducing operational costs. The combination of faster processing and reduced support requirements creates a scalable foundation for program growth. 
Conclusion 
AWS partnered with Vxceed to support their AI strategy, resulting in the development of Lighthouse Loyalty Selling Story, an innovative personalized sales pitch solution. Using AWS services including Amazon Bedrock and Lambda, Vxceed successfully built a secure, AI-powered solution that creates personalized selling stories at scale for CPG industry field sales teams. Looking ahead, Vxceed plans to further refine Lighthouse Loyalty Selling Story by: 
 
 Optimizing AI inference costs to improve scalability and cost-effectiveness 
 Adding a Language Agent to present the generated selling story in the native language of choice 
 Adding RAG and GraphRAG to further enhance the story generation effectiveness 
 
With this collaboration, Vxceed aims to significantly improve CPG industry field sales management, delivering secure, efficient, and AI-powered solutions for CPG companies and brands. 
If you are interested in implementing a similar AI-powered solution, start by understanding how to implement asynchronous AI agents using Amazon Bedrock. See Creating asynchronous AI agents with Amazon Bedrock to learn about the implementation patterns for multi-agent systems and develop secure, AI-powered solutions for your organization. 
About the Authors 
 
  
   
    
   Roger Wang is a Senior Solution Architect at AWS. He is a seasoned architect with over 20 years of experience in the software industry. He helps New Zealand and global software and SaaS companies use cutting-edge technology at AWS to solve complex business challenges. Roger is passionate about bridging the gap between business drivers and technological capabilities, and thrives on facilitating conversations that drive impactful results. 
   
   
    
   Deepika Kumar is a Solutions Architect at AWS. She has over 13 years of experience in the technology industry and has helped enterprises and SaaS organizations build and securely deploy their workloads on the cloud. She is passionate about using generative AI in a responsible manner, whether that is driving product innovation, boosting productivity, or enhancing customer experiences. 
   
   
    
   Jhalak Modi is a Solution Architect at AWS, specializing in cloud architecture, security, and AI-driven solutions. She helps businesses use AWS to build secure, scalable, and innovative solutions. Passionate about emerging technologies, Jhalak actively shares her expertise in cloud computing, automation, and responsible AI adoption, empowering organizations to accelerate digital transformation and stay ahead in a rapidly evolving tech landscape. 
   
   
    
   Cyril Ovely, CTO and co-founder of Vxceed Software Solutions, leads the company‚Äôs SaaS-based logistics solutions for CPG brands. With 33 years of experience, including 22 years at Vxceed, he previously worked in analytical and process control instrumentation. An engineer by training, Cyril architects Vxceed‚Äôs SaaS offerings and drives innovation from his base in Auckland, New Zealand.
‚Ä¢ Implement a secure MLOps platform based on Terraform and GitHub
  Machine learning operations (MLOps) is the combination of people, processes, and technology to productionize ML use cases efficiently. To achieve this, enterprise customers must develop MLOps platforms to support reproducibility, robustness, and end-to-end observability of the ML use case‚Äôs lifecycle. Those platforms are based on a multi-account setup by adopting strict security constraints, development best practices such as automatic deployment using continuous integration and delivery (CI/CD) technologies, and permitting users to interact only by committing changes to code repositories. For more information about MLOps best practices, refer to the MLOps foundation roadmap for enterprises with Amazon SageMaker. 
Terraform by HashiCorp has been embraced by many customers as the main infrastructure as code (IaC) approach to develop, build, deploy, and standardize AWS infrastructure for multi-cloud solutions. Furthermore, development repositories and CI/CD technologies such as GitHub and GitHub Actions, respectively, have been adopted widely by the DevOps and MLOps community across the world. 
In this post, we show how to implement an MLOps platform based on Terraform using GitHub and GitHub Actions for the automatic deployment of ML use cases. Specifically, we deep dive on the necessary infrastructure and show you how to utilize custom Amazon SageMaker Projects templates, which contain example repositories that help data scientists and ML engineers deploy ML services (such as an Amazon SageMaker endpoint or batch transform job) using Terraform. You can find the source code in the following GitHub repository. 
Solution overview 
The MLOps architecture solution creates the necessary resources to build a comprehensive training pipeline, registering the models in the Amazon SageMaker Model Registry, and its deployment to preproduction and production environments. This foundational infrastructure enables a systematic approach to ML operations, providing a robust framework that streamlines the journey from model development to deployment. 
The end-users (data scientists or ML engineers) will select the organization SageMaker Project template that fits their use case. SageMaker Projects helps organizations set up and standardize developer environments for data scientists and CI/CD systems for MLOps engineers. The project deployment creates, from the GitHub templates, a GitHub private repository and CI/CD resources that data scientists can customize according to their use case. Depending on the chosen SageMaker project, other project-specific resources will also be created. 
 
Custom SageMaker Project template 
SageMaker projects deploys the associated AWS CloudFormation template of the AWS Service Catalog product to provision and manage the infrastructure and resources required for your project, including the integration with a source code repository. 
At the time of writing, four custom SageMaker Projects templates are available for this solution: 
 
 MLOps template for LLM training and evaluation ‚Äì An MLOps pattern that shows a simple one-account Amazon SageMaker Pipelines setup for large language models (LLMs) This template supports fine-tuning and evaluation. 
 MLOps template for model building and training ‚Äì An MLOps pattern that shows a simple one-account SageMaker Pipelines setup. This template supports model training and evaluation. 
 MLOps template for model building, training, and deployment ‚Äì An MLOps pattern to train models using SageMaker Pipelines and deploy the trained model into preproduction and production accounts. This template supports real-time inference, batch inference pipelines, and bring-your-own-containers (BYOC). 
 MLOps template for promoting the full ML pipeline across environments ‚Äì An MLOps pattern to show how to take the same SageMaker pipeline across environments from dev to prod. This template supports a pipeline for batch inference. 
 
Each SageMaker project template has associated GitHub repository templates that are cloned to be used for your use case: 
 
 
 MLOps template for LLM training and evaluation ‚Äì Associated with the LLM training repository. 
 MLOps template for model building and training ‚Äì Associated with the model training repository. 
 MLOps template for model building, training, and deployment ‚Äì Associated with the BYOC repository (optional), model training repository, and real time inference repository or batch inference repository. 
 MLOps template for promoting the full ML pipeline across environments ‚Äì Associated with pipeline promotion repository. 
 
When a custom SageMaker project is deployed by a data scientist, the associated GitHub template repositories are cloned through an invocation of the AWS Lambda function &lt;prefix&gt;_clone_repo_lambda, which creates a new GitHub repository for your project. 
 
Infrastructure Terraform modules 
The Terraform code, found under base-infrastructure/terraform, is structured with reusable modules that are used across different deployment environments. Their instantiation will be found for each environment under base-infrastructure/terraform/&lt;ENV&gt;/main.tf. There are seven key reusable modules: 
 
 KMS ‚Äì Creates an AWS Key Management Service (AWS KMS) key 
 Lambda ‚Äì Creates a Lambda function and Amazon CloudWatch log group 
 Networking ‚Äì Creates a virtual private cloud (VPC), various subnets, security group, NAT gateway, internet gateway, route table and routes, and multiple VPC endpoints for the networking setup for Amazon SageMaker Studio 
 S3 ‚Äì Creates an Amazon Simple Storage Service (Amazon S3) bucket 
 SageMaker ‚Äì Creates SageMaker Studio and SageMaker users 
 SageMaker Roles ‚Äì Creates AWS Identity and Access Management (IAM) roles for SageMaker Studio 
 Service Catalog ‚Äì Creates Service Catalog products from a CloudFormation template 
 
There are also some environment-specific resources, which can be found directly under base-infrastructure/terraform/&lt;ENV&gt;. 
 
Prerequisites 
Before you start the deployment process, complete the following three steps: 
 
 Prepare AWS accounts to deploy the platform. We recommend using three AWS accounts for three typical MLOps environments: experimentation, preproduction, and production. However, you can deploy the infrastructure to just one account for testing purposes. 
 Create a GitHub organization. 
 Create a personal access token (PAT). It is recommended to create a service or platform account and use its PAT. 
 
Bootstrap your AWS accounts for GitHub and Terraform 
Before we can deploy the infrastructure, the AWS accounts you have vended need to be bootstrapped. This is required so that Terraform can manage the state of the resources deployed. Terraform backends enable secure, collaborative, and scalable infrastructure management by streamlining version control, locking, and centralized state storage. Therefore, we deploy an S3 bucket and Amazon DynamoDB table for storing states and locking consistency checking. 
Bootstrapping is also required so that GitHub can assume a deployment role in your account, therefore we deploy an IAM role and OpenID Connect (OIDC) identity provider (IdP). As an alternative to employing long-lived IAM user access keys, organizations can implement an OIDC IdP within your AWS account. This configuration facilitates the utilization of IAM roles and short-term credentials, enhancing security and adherence to best practices. 
You can choose from two options to bootstrap your account: a bootstrap.sh Bash script and a bootstrap.yaml CloudFormation template, both stored at the root of the repository. 
Bootstrap using a CloudFormation template 
Complete the following steps to use the CloudFormation template: 
 
 Make sure the AWS Command Line Interface (AWS CLI) is installed and credentials are loaded for the target account that you want to bootstrap. 
 Identify the following: 
   
   Environment type of the account: dev, preprod, or prod. 
   Name of your GitHub organization. 
   (Optional) Customize the S3 bucket name for Terraform state files by choosing a prefix. 
   (Optional) Customize the DynamoDB table name for state locking. 
    
 Run the following command, updating the details from Step 2: 
 
 
 # Update
export ENV=xxx
export GITHUB_ORG=xxx
# Optional
export TerraformStateBucketPrefix=terraform-state
export TerraformStateLockTableName=terraform-state-locks

aws cloudformation create-stack \
  --stack-name YourStackName \
  --template-body file://bootstrap.yaml \
  --capabilities CAPABILITY_IAM CAPABILITY_NAMED_IAM \
  --parameters ParameterKey=Environment,ParameterValue=$ENV \
               ParameterKey=GitHubOrg,ParameterValue=$GITHUB_ORG \
               ParameterKey=OIDCProviderArn,ParameterValue="" \
               ParameterKey=TerraformStateBucketPrefix,ParameterValue=$TerraformStateBucketPrefix \
               ParameterKey=TerraformStateLockTableName,ParameterValue=$TerraformStateLockTableName 
 
Bootstrap using a Bash script 
Complete the following steps to use the Bash script: 
 
 Make sure the AWS CLI is installed and credentials are loaded for the target account that you want to bootstrap. 
 Identify the following: 
   
   Environment type of the account: dev, preprod, or prod. 
   Name of your GitHub organization. 
   (Optional) Customize the S3 bucket name for Terraform state files by choosing a prefix. 
   (Optional) Customize the DynamoDB table name for state locking. 
    
 Run the script (bash ./bootstrap.sh) and input the details from Step 2 when prompted. You can leave most of these options as default. 
 
If you change the TerraformStateBucketPrefix or TerraformStateLockTableName parameters, you must update the environment variables (S3_PREFIX and DYNAMODB_PREFIX) in the deploy.yml file to match. 
Set up your GitHub organization 
In the final step before infrastructure deployment, you must configure your GitHub organization by cloning code from this example into specific locations. 
Base infrastructure 
Create a new repository in your organization that will contain the base infrastructure Terraform code. Give your repository a unique name, and move the code from this example‚Äôs base-infrastructure folder into your newly created repository. Make sure the .github folder is also moved to the new repository, which stores the GitHub Actions workflow definitions. GitHub Actions make it possible to automate, customize, and execute your software development workflows right in your repository. In this example, we use GitHub Actions as our preferred CI/CD tooling. 
Next, set up some GitHub secrets in your repository. Secrets are variables that you create in an organization, repository, or repository environment. The secrets that you create are available to use in our GitHub Actions workflows. Complete the following steps to create your secrets: 
 
 Navigation to the base infrastructure repository. 
 Choose Settings, Secrets and Variables, and Actions. 
 Create two secrets: 
   
   AWS_ASSUME_ROLE_NAME ‚Äì This is created in the bootstrap script with the default name aws-github-oidc-role, and should be updated in the secret with whichever role name you choose. 
   PAT_GITHUB ‚Äì This is your GitHub PAT token, created in the prerequisite steps. 
    
 
Template repositories 
The template-repos folder of our example contains multiple folders with the seed code for our SageMaker Projects templates. Each folder should be added to your GitHub organization as a private template repository. Complete the following steps: 
 
 Create the repository with the same name as the example folder, for every folder in the template-repos directory. 
 Choose Settings in each newly created repository. 
 Select the Private Template option. 
 
Make sure you move all the code from the example folder to your private template, including the .github folder. 
Update the configuration file 
At the root of the base infrastructure folder is a config.json file. This file enables the multi-account, multi-environment mechanism. The example JSON structure is as follows: 
 
 {
  "environment_name": {
    "region": "X",
    "dev_account_number": "XXXXXXXXXXXX",
    "preprod_account_number": "XXXXXXXXXXXX",
    "prod_account_number": "XXXXXXXXXXXX"
  }
} 
 
For your MLOps environment, simply change the name of environment_name to your desired name, and update the AWS Region and account numbers accordingly. Note the account numbers will correspond to the AWS accounts you bootstrapped. This config.json permits you to vend as many MLOps platforms as you desire. To do so, simply create a new JSON object in the file with the respective environment name, Region, and bootstrapped account numbers. Then locate the GitHub Actions deployment workflow under .github/workflows/deploy.yaml and add your new environment name inside each list object in the matrix key. When we deploy our infrastructure using GitHub Actions, we use a matrix deployment to deploy to all our environments in parallel. 
Deploy the infrastructure 
Now that you have set up your GitHub organization, you‚Äôre ready to deploy the infrastructure into the AWS accounts. Changes to the infrastructure will deploy automatically when changes are made to the main branch, therefore when you make changes to the config file, this should trigger the infrastructure deployment. To launch your first deployment manually, complete the following steps: 
 
 Navigate to your base infrastructure repository. 
 Choose the Actions tab. 
 Choose Deploy Infrastructure. 
 Choose Run Workflow and choose your desired branch for deployment. 
 
This will launch the GitHub Actions workflow for deploying the experimentation, preproduction, and production infrastructure in parallel. You can visualize these deployments on the Actions tab. 
Now your AWS accounts will contain the necessary infrastructure for your MLOps platform. 
End-user experience 
The following demonstration illustrates the end-user experience. 

 
  
 
 
Clean up 
To delete the multi-account infrastructure created by this example and avoid further charges, complete the following steps: 
 
 In the development AWS account, manually delete the SageMaker projects, SageMaker domain, SageMaker user profiles, Amazon Elastic File Service (Amazon EFS) storage, and AWS security groups created by SageMaker. 
 In the development AWS account, you might need to provide additional permissions to the launch_constraint_role IAM role. This IAM role is used as a launch constraint. Service Catalog will use this permission to delete the provisioned products. 
 In the development AWS account, manually delete the resources like repositories (Git), pipelines, experiments, model groups, and endpoints created by SageMaker Projects. 
 For preproduction and production AWS accounts, manually delete the S3 bucket ml-artifacts-&lt;region&gt;-&lt;account-id&gt; and the model deployed through the pipeline. 
 After you complete these changes, trigger the GitHub workflow for destroying. 
 If the resources aren‚Äôt deleted, manually delete the pending resources. 
 Delete the IAM user that you created for GitHub Actions. 
 Delete the secret in AWS Secrets Manager that stores the GitHub personal access token. 
 
Conclusion 
In this post, we walked through the process of deploying an MLOps platform based on Terraform and using GitHub and GitHub Actions for the automatic deployment of ML use cases. This solution effectively integrates four custom SageMaker Projects templates for model building, training, evaluation and deployment with specific SageMaker pipelines. In our scenario, we focused on deploying a multi-account and multi-environment MLOps platform. For a comprehensive understanding of the implementation details, visit the GitHub repository. 
 
About the authors 
Jordan Grubb is a DevOps Architect at AWS, specializing in MLOps. He enables AWS customers to achieve their business outcomes by&nbsp;delivering automated, scalable, and secure cloud architectures. Jordan is also an inventor, with two patents within software engineering. Outside of work, he enjoys playing most sports, traveling, and has a passion for health and wellness. 
Irene Arroyo Delgado is an AI/ML and GenAI Specialist Solution at AWS. She focuses on bringing out the potential of generative AI for each use case and productionizing ML workloads, to achieve customers‚Äô desired business outcomes by automating end-to-end ML lifecycles. In her free time, Irene enjoys traveling and hiking.
‚Ä¢ Automate Amazon QuickSight data stories creation with agentic AI using Amazon Nova Act
  Amazon QuickSight data stories support global customers by transforming complex data into interactive narratives for faster decisions. However, manual creation of multiple daily data stories consumes significant time and resources, delaying critical decisions and preventing teams from focusing on valuable analysis. 
Each organization has multiple business units, and each business unit creates and operates multiple dashboards based on specific reporting requirements. Users create various data stories from these dashboards according to their needs. Currently, data story creation is a manual process that consumes significant time because users need to develop multiple narratives. By automating this process, organizations can dramatically improve productivity, so users can redirect their time toward making data-driven decisions. 
In this post, we demonstrate how Amazon Nova Act automates QuickSight data story creation, saving time so you can focus on making critical, data-driven business decisions. 
Amazon Nova Act modernizes web browser automation, which helps in performing complex, real-world tasks through web interfaces. Unlike traditional large language models (LLMs) focused on conversation, Amazon Nova Act emphasizes action-oriented capabilities by breaking down complex tasks into reliable atomic commands. This transformative technology advances autonomous automation with minimal human supervision, making it particularly valuable for business productivity and IT operations. 
QuickSight data stories transform complex data into interactive presentations that guide viewers through insights. It automatically combines visualizations, text, and images to bridge the gap between analysts and stakeholders, helping organizations communicate data effectively and make faster decisions while maintaining professional standards. 
With the automation capabilities of Amazon Nova Act, you can automatically generate data stories, reducing time-consuming manual efforts. Using browser automation, Amazon Nova Act seamlessly interacts with QuickSight to create customized data narratives. By combining the automation of Amazon Nova Act with the robust visualization capabilities of QuickSight, you can minimize repetitive tasks and accelerate data-driven decision-making across teams. 
Solution overview 
In our solution, QuickSight transforms complex data into interactive narratives through data stories, enabling faster decisions. Amazon Nova Act transforms web browser automation by enabling AI agents to execute complex tasks autonomously, streamlining operations for enhanced business productivity. 
Prompt best practices 
Amazon Nova Act achieves optimal results by breaking down prompts into distinct act() calls, similar to providing step-by-step instructions. At the time of writing, this is the recommended approach for building repeatable, reliable, simple-to-maintain workflows. In this section, we discuss some prompt best practices. 
First, be prescriptive and succinct in what the agent should do. For example, don‚Äôt use the following code: 
nova.act("Select the SaaS-Sales dataset") 
We recommend the following prompt instead: 
nova.act("Click on Datasets option on the left-hand side and then select SaaS-Sales dataset ") 
Additionally, we recommend breaking up large actions into smaller ones. For example, don‚Äôt use the following code: 
nova.act("Publish dashboard as ‚Äòtest-dashboard‚Äô") 
The following prompt is broken up into separate actions: 
nova.act("select Analyses on the left-hand side‚Äù) 
nova.act("select the ‚ÄòSaaS-Sales analysis‚Äô ") 
nova.act("select ‚ÄòPUBLISH‚Äô from the top right-hand corner") 
nova.act("In the 'Publish dashboard' dialog box, locate the input field labeled 'Dashboard name'. Enter 'test_dashboard' into this field‚Äù) 
nova.act(‚ÄúSelect PUBLISH DASHBOARD‚Äù) 
Prerequisites 
The following prerequisites are needed to create and publish a QuickSight data story using Amazon Nova Act: 
 
 An API key for authentication. To generate an API key, refer to Amazon Nova Act. 
 For Amazon Nova Act prerequisites and installation instructions, refer to the GitHub repo. 
 A Pro user (author or reader) to create QuickSight data stories. 
 A published QuickSight dashboard containing the visuals required for your QuickSight data story. 
 
For Windows users, complete the following setup and installation steps in Windows PowerShell: 
 
 Create a virtual environment: python -m venv venv. 
 Activate the virtual environment: venv\Scripts\activate 
 Set your API key as an environment variable: $Env:NOVA_ACT_API_KEY="your_api_key" 
 Install Amazon Nova Act: pip install nova-act 
 To run a script (Python file), use the following command, and specify the script name you want to run: python &lt;script_name&gt;.py 
 
To keep it simple, we have hardcoded some of the values. You can implement programming logic using Python features to accept these values as input parameters. 
There are multiple ways to write prompts. In the following sections, we provide examples demonstrating how to automate QuickSight data story creation and distribution. 
Setup 
Run the following code to import the NovaAct class from the nova_act module, create an Amazon Nova instance beginning at the QuickSight login page, and initiate an automated browser session: 
 
 from nova_act import NovaAct

nova = NovaAct(starting_page="https://quicksight.aws.amazon.com/")

nova.start()
 
 
Sign in with credentials After you have opened the QuickSight login page, complete the following steps to log in with your credentials: 
 
 Enter your QuickSight account name and choose Next. (Specify the QuickSight account name in the following code, or implement programming logic to handle it as an input parameter.) nova.act("enter QuickSight account name &lt;Account Name&gt; and select Next") 
 Enter your user name and move to the password field. (You can configure the user name as an input parameter using programming logic.) nova.act("Enter username and click on the password field") 
 Collect the password from the command line and enter it using Playwright: nova.page.keyboard.type(getpass()) 
 Now that user name and password are filled in, choose Sign in. nova.act("Click Sign in") 
 
If the agent is unable to focus on the page element (in this case, the password field), you can use the following code: 
nova.act("enter '' in the password field") 
nova.page.keyboard.type(getpass()) 
Create a new data story On the QuickSight console, choose Data stories in the navigation pane: 
nova.act("Select Data stories on the left side menu") 
nova.act("Select NEW DATA STORY"). 
 
To build the data story, you must complete the following steps: 
 
 Describe the data story 
 Select visuals from the dashboard 
 Build the data story 
 
nova.act("Please enter ‚ÄòCountry wide sales data story‚Äô into the 'Describe your data story' field and Click on + ADD") 
nova.act("select all the visuals and select BUILD") 
time.sleep(300) 
 
In this example, the script defaults to a single dashboard (Demo Dashboard). For multiple dashboards, include a prompt to select the specific dashboard and its visuals for the data story. Additionally, you can describe the data story according to your requirements. If there are multiple visuals, you can select the ones you want to include as part of the data story. Adjust the time.sleep duration based on dashboard data volume and the number of visuals being compiled. 
To view your data story, choose Data stories in the navigation pane and choose your data story. 
 
Clean up 
Complete the following steps to delete the data story you created: 
 
 Sign in to the QuickSight console. 
 Choose Data stories in the navigation pane. 
 Find the data story you want to delete. 
 Choose the options menu icon (three dots) next to the story. 
 Choose Delete from the dropdown menu. 
 
Conclusion 
In this post, we demonstrated how to create a QuickSight data story using Amazon Nova Act prompts. This solution showcases how Amazon Nova Act simplifies task automation, significantly boosting productivity and saving valuable time. 
To learn more about Amazon Nova Act and QuickSight data stories, check out the following resources: 
 
 Amazon Nova Act GitHub repo 
 Introducing Amazon Nova Act 
 Working with data stories in Amazon QuickSight 
 
 
About the author 
Satish Bhonsle is a Senior Technical Account Manager at AWS. He is passionate about customer success and technology. He loves working backwards by quickly understanding strategic customer objectives, aligning them to software capabilities and effectively driving customer success.

‚∏ª