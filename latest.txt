‚úÖ Morning News Briefing ‚Äì July 02, 2025

üìÖ Date: 2025-07-02
üè∑Ô∏è Tags: #briefing #ai #publichealth #digitalgov

‚∏ª

üßæ Weather
‚Ä¢ No watches or warnings in effect, Pembroke
  No watches or warnings in effect. No warnings or watches or watches in effect . No
‚Ä¢ Current Conditions:  17.0¬∞C
  Temperature: 17.0&deg;C Pressure / Tendency: 100.9
‚Ä¢ Wednesday: Chance of showers. High 29. POP 30%
  Increasing cloudiness this afternoon then 30 percent chance of showers late this afternoon . Risk of
‚Ä¢ Wednesday night: Chance of showers. Low 15. POP 30%
  Partly cloudy with 30 percent chance of showers . Risk of thunderstorms . Low 15
‚Ä¢ Thursday: Chance of showers. High 24. POP 30%
  A mix of sun and cloud with 30 percent chance of showers . Risk of thunderstorms
‚Ä¢ Thursday night: Chance of showers. Low 10. POP 30%
  Cloudy periods with 30 percent chance of showers . Low 10.50/60/
‚Ä¢ Friday: Sunny. High 27.
  Forecast issued 5:00 AM EDT Wednesday 2 July 2025 . Sunny. Sunny.
‚Ä¢ Friday night: Cloudy periods. Low 14.
  Forecast issued 5:00 AM EDT Wednesday 2 July 2025 . Cloudy periods.
‚Ä¢ Saturday: Chance of showers. High 29. POP 30%
  A mix of sun and cloud with 30 percent chance of showers . High 29. A
‚Ä¢ Saturday night: Chance of showers. Low 19. POP 30%
  Cloudy periods with 30 percent chance of showers . Low 19. Chance of rain .
‚Ä¢ Sunday: Chance of showers. High 31. POP 60%
  Cloudy with 60 percent chance of showers . High 31. High 31 . Cloudy
‚Ä¢ Sunday night: Chance of showers. Low 18. POP 60%
  Cloudy with 60 percent chance of showers. Low 18. Chance of showers . Cloud
‚Ä¢ Monday: Chance of showers. High 27. POP 30%
  Cloudy with 30 percent chance of showers . High 27. High 27 . Cloudy
‚Ä¢ Monday night: Cloudy periods. Low 14.
  Forecast issued 5:00 AM EDT Wednesday 2 July 2025 . Cloudy periods.
‚Ä¢ Tuesday: A mix of sun and cloud. High 27.
  A mix of sun and cloud is forecast for July 2, 2025 . High 27.

üåç International & Canadian News
No updates.

üá∫üá∏ U.S. Top Stories
‚Ä¢ America has a major birthday coming up ‚Äî here's what to expect for the big 2-5-0
  July 4, 2026, is the 250th anniversary of the signing of the Declaration
‚Ä¢ Barbecue is everywhere for the Fourth of July. Here's its origin story
  The origins of the word "barbecue" are in the Caribbean . Barbecue is
‚Ä¢ Trump administration targets ATF, with plans to cut jobs and ease gun restrictions
  DOGE staffers have been working on changes at the ATF that would roll back dozens
‚Ä¢ 5 ways Trump's tax bill will limit health care access
  With spending cuts poised to hit medical providers, Medicaid recipients and Affordable Care Act enrollees
‚Ä¢ UPenn updates swimming records to settle with feds on transgender athletes case
  Lia Thomas last competed for the Ivy League school in 2022 . U.S. Education
‚Ä¢ Paramount to reach a $16 million settlement over Trump's CBS lawsuit
  Paramount Global will pay $16 million to settle President Trump's lawsuit over 60 Minutes'
‚Ä¢ States sue Trump administration for sharing health data with DHS
  California is leading 20 state attorneys general in a lawsuit seeking to block health officials from sharing
‚Ä¢ A Crackdown Inside Iran
  Iranian state media reports hundreds have been taken into custody in the last two weeks . Activists say the Iranian government is hunting for people it suspects of collaborating with Israel
‚Ä¢ Study: 14 million lives could be lost due to Trump aid cuts
  A new study looks at lives saved by USAID in the past and what the future
‚Ä¢ What to know about the Bryan Kohberger case as a plea deal emerges over Idaho murders
  Kohberger had originally pleaded not guilty, and his attorneys have suggested they wanted try to

üß† Artificial Intelligence & Digital Strategy
‚Ä¢ Coming to PostgreSQL: On-disk database encryption
  Open source initiative aims to offer enterprise security feature without vendor lock-in . Transparent Data Encryption (TDE) is
‚Ä¢ Cl0p cybercrime gang's data exfiltration tool found vulnerable to RCE attacks
  Experts say they don't expect MOVEit menace to do much about it . Experts
‚Ä¢ UK eyes new laws as cable sabotage blurs line between war and peace
  It might be time to update the Submarine Telegraph Act of 1885 . Cyberattacks
‚Ä¢ NASA tests shrinking metals to help it find more exoplanets
  Mysterious ‚ÄòAlloy 30‚Äô gets smaller when heated, which could help stabilize super-sensitive space telescopes
‚Ä¢ Amazon's latest Graviton 4 EC2 instances pack dual 300Gbps NICs
  Amazon Web Services has created a new Graviton 4-powered instance for network-intensive
‚Ä¢ Arista acquires VMware‚Äôs VeloCloud SD-WAN outfit from Broadcom
  Broadcom has sold VeloCloud, the software-defined WAN business, to
‚Ä¢ Australian airline Qantas reveals data theft impacting six million customers
  Australian airline Qantas fell victim to a cyberattack that saw information describing six million
‚Ä¢ Figma files for an (A)IPO with prospectus that mentions AI 150+ times
  Figma filed with the US Securities and Exchange Commission to propose an initial public offering of
‚Ä¢ With OpenAI, there are no allegiances - just compute at all costs
  OpenAI is looking to expand its network of compute providers to the likes of Oracle, CoreWeave,
‚Ä¢ Microsoft pulls plug on generous Azure credit program for startups
  Up to $150K tier shelved, perks folded into two-track system .
‚Ä¢ Cloudflare creates AI crawler tollbooth to pay publishers
  Cloudflare has started blocking AI web crawlers by default in a bid to become
‚Ä¢ Microsoft admits to Intune forgetfulness
  Microsoft Intune administrators may face a few days of stress after Redmond acknowledged a problem with
‚Ä¢ Senate decides free rein for AI companies isn't such a good thing
  A controversial section that would have barred states from regulating AI was struck down in a much clearer fashion . It took a tie
‚Ä¢ Chip design is a RISC-y business: Codasip puts itself up for sale
  Codasip cites expression of interest during recent funding round . European RISC-V b
‚Ä¢ Apple accuses former engineer of taking Vision Pro secrets to Snap
  Ex-Apple employee allegedly thought he was clever enough to sneak out the back door to a job at Snap loaded with
‚Ä¢ International Criminal Court swats away 'sophisticated and targeted' cyberattack
  International Criminal Court says a "sophisticated" cyberattack targeted the institution .
‚Ä¢ Fedora 43 won't drop 32-bit app support ‚Äì or adopt Xlibre
  Community vetoes plans to axe i686 compatibility and switch X11 forks . Fedora community
‚Ä¢ NASA gives Lunar Trailblazer a few more weeks to pick up the phone
  NASA has extended recovery efforts for stricken Lunar Trailblazer spacecraft to mid-July .
‚Ä¢ EU rattles its purse and AI datacenter builders come running
  176 expressions of interest to erect 'gigafactories' across 16 member states, with 3 million GPUs
‚Ä¢ Microsoft Copilot joins ChatGPT at the feet of the mighty Atari 2600 Video Chess
  Robert Caruso's ChatGPT has been beaten by Video Chess on an Atari
‚Ä¢ Folks aren‚Äôt buying the PCs that US vendors stockpiled to dodge tariffs
  Total PC shipments in the US will increase by just 2 percent this year, thanks to Trump's tariffs and little
‚Ä¢ Linus Torvalds hints Bcachefs may get dropped from the Linux kernel
  Linus Torvalds: "I think we'll be parting ways" as
‚Ä¢ People have empathy with AI‚Ä¶ as long as they think it's human
  Study finds emotional support from chatbots is more readily accepted if participants don't know it
‚Ä¢ Terrible tales of opsec oversights: How cybercrooks get themselves caught
  For cybercriminals, taking too many shortcuts when it comes to opsec delivers a
‚Ä¢ Critics blast Microsoft's limited reprieve for those stuck on Windows 10
  Microsoft's latest attempts to ease transition to Windows 11 for Windows 10 users "don't go far
‚Ä¢ A lot of product makers snub Right to Repair laws
  Refrigerators and game consoles are the worst, but Apple, surprisingly, rates well
‚Ä¢ Proton bashes Apple and joins antitrust suit that seeks to throw the App Store wide open
  Apple's practices harm developers, consumers, and privacy, says Proton . Proton
‚Ä¢ DRAM spot prices doubled last week
  Spot prices for DRAM have doubled in the last week . Fears that DDR4
‚Ä¢ China successfully tests hypersonic aircraft, maybe at Mach 12
  China recently extended tech export bans specifically to stop Beijing building this sort of thing . America
‚Ä¢ Oracle just signed one mystery customer that will double its cloud revenue in 2028
  Oracle has landed a mystery customer that will add more than $30 billion to its annual revenues .
‚Ä¢ US shuts down a string of North Korean IT worker scams
  The US Department of Justice announced a major disruption of multiple North Korean fake IT worker scams
‚Ä¢ Want a job? Just put 'AI skills' on your resume
  AI skills appear to be simple based on a look at the past year of employment data . It
‚Ä¢ AIs have a favorite number, and it's not 42
  OpenAI's ChatGPT, Anthropic's Claude Sonnet 4, Google's Gemini 2.
‚Ä¢ Google to buy power from fusion energy startup Commonwealth - if they can ever make it work
  Google has agreed to purchase 200 megawatts of fusion energy from Commonwealth Fusion Systems . That's assuming the startup can actually
‚Ä¢ British IT worker sentenced to seven months after trashing company network
  Disgruntled IT worker wreaked havoc on his employer's network following his suspension .
‚Ä¢ Norwegian lotto mistakenly told thousands they were filthy rich after math error
  Thousands mistakenly thought they'd won life-changing sums in last week's Eurojackpot
‚Ä¢ Scattered Spider crime spree takes flight as focus turns to aviation sector
  Experts say the aviation industry is now on the ransomware crew's radar . Ransomware
‚Ä¢ Northrop Grumman shows SpaceX doesn't have a monopoly on explosions
  Northrop Grumman encountered an anomaly during a static fire test of an updated solid rocket
‚Ä¢ Mitch Kapor finally completes MIT master's degree after 45-year detour
  The man behind Lotus 1-2-3 and the Electronic Frontier Foundation (EFF) has
‚Ä¢ VMware must support crucial Dutch govt agency as it migrates off the platform, judge rules
  Court says State arm cannot be left without maintenance, patches and upgrades because of Broadcom's new licensing model . Dutch government organization must provide
‚Ä¢ Sinaloa drug cartel hired a cybersnoop to identify and kill FBI informants
  Device compromises exposed surveillance vulnerabilities in agency's work . Device compromises and deep-seated access to critical infrastructure exposed surveillance
‚Ä¢ Microsoft's next Windows 11 update is more 'enablement' than upgrade
  Windows 11 24H2 and 25H2 share the same source code . Upgrade will be little more
‚Ä¢ Arm muscles into server market ‚Äì but can't wrestle control from x86 just yet
  Arm-based servers tipped to jump 70% in 2025, still shy of datacenter dominance goal . Chip designer's
‚Ä¢ Deutsche Bahn train hits 405 km/h without falling to bits
  Deutsche Bahn and Siemens have managed to get an ICE test train to 405 km
‚Ä¢ Cloud lobby warns EU: Clamp down on water rules and we'll evaporate
  CISPE floats reforms to avoid new costs, fragmentation, and infrastructure flight . Trade body has put
‚Ä¢ Your browser has ad tech's fingerprints all over it, but there's a clean-up squad in town
  Chrome‚Äôs Incognito Mode is a clever way to hide Chrome Chrome's
‚Ä¢ Junior sysadmin‚Äôs first lines of code set off alarms. His next lot crashed the company
  Who, Me? is a Monday morning column in which readers admit to making big mistakes and somehow
‚Ä¢ Don't pay for AI support failures, says Gradient Labs CEO
  Dimitri Masin, CEO of Gradient Labs, argues that companies using AI agents
‚Ä¢ DoJ clears HPE to buy Juniper if it sells Instant On Wi-Fi and licenses some code
  The US Department of Justice has cleared the way for HPE‚Äôs $14
‚Ä¢ China claims breakthroughs in classical and quantum computers
  Chipmaker Loongson says server CPUs on par with 2021‚Äôs Ice Lake

üè• Public Health & Science
‚Ä¢ Assessment of information quality and reliability on ankle sprains in short videos from Douyin and Bilibili
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery .
‚Ä¢ Associations between anthropometric indices and biological age acceleration in American adults: insights from NHANES 2009‚Äì2018 data
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery .
‚Ä¢ Gastroesophageal disease risk and inhalational exposure a systematic review and meta-analysis
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery .
‚Ä¢ Ethnic disparities in COVID-19 mortality and cardiovascular disease in England and Wales between 2020-2022
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery .
‚Ä¢ Discrete choice experiment to evaluate preferences for tailored social media messages for vaping initiation prevention among sexual and gender minority youth
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery .
‚Ä¢ Prevalence and associated factors of at risk of anemia among children under five in Northeast Thailand using noninvasive hemoglobin screening in a cross sectional study
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery .
‚Ä¢ Examining burden among caregivers of community-dwelling older adults in Lebanon
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery .
‚Ä¢ An integrated GIS-pXRF approach assesses ecological and human health risks from heavy metals in county level soils
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery .
‚Ä¢ Hypertension among people living with HIV receiving dolutegravir-based antiretroviral therapy in ethiopia: a cross-sectional study
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery .
‚Ä¢ General practitioners‚Äô recommendation of HPV vaccination to adolescents aged 11‚Äì14 may be gender-biased as suggested by a qualitative study in France
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery .
‚Ä¢ An exploratory mixed methods study on shared decision-making and antibiotic prescribing for pet cats and dogs in Singapore veterinary clinics
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery .
‚Ä¢ Characterizing everyday exposure to volatile organic compounds and upper respiratory health effects
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery .
‚Ä¢ Rare disease, common struggles: quality of life, caregiver burden and financial wellbeing of family caregivers in Poland
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery .
‚Ä¢ Association of gestational weight gain patterns with preterm birth subtypes in a population based cohort study from China
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery .
‚Ä¢ A longitudinal study investigating the association between social maturity, social preference and children‚Äôs perceptions of their playfulness
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery .
‚Ä¢ Sexually transmitted infections are not associated with US holidays
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery .
‚Ä¢ Scalable evaluation framework for retrieval augmented generation in tobacco research using large Language models
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery .
‚Ä¢ The impact of intelligent devices utilization on household medical expenditure of older adults in China
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery .
‚Ä¢ Effect of the implementation of HIV/AIDS prevention and treatment policies on the mortality rate of people living with HIV in Guilin, China
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery .
‚Ä¢ Adolescent sexuality education, sexual debut, and associated factors in Nigerian public secondary schools
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery .
‚Ä¢ Postoperative mortality following hip fracture surgery in older adults: a single-center retrospective study in the context of Taiwan‚Äôs transition to an aged society
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery .
‚Ä¢ Short and long-term outcomes of children and adolescents hospitalized with COVID-19 or influenza: results of the AUTCOV study
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery .
‚Ä¢ Socioeconomic inequalities in disability prevalence and health service use in Bangladesh
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery .
‚Ä¢ Targeting three United States priority populations of people who smoke with educational nicotine messages using curiosity-eliciting strategies
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery .
‚Ä¢ Effects of a 26 week multicomponent exercise program on cardiovascular and lipid profiles in premenopausal and postmenopausal women
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery .
‚Ä¢ Park use patterns and park satisfaction before and after citywide park renovations in low-income New York City neighborhoods
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery .
‚Ä¢ Global pandemic agreement needs sustained pressure to succeed
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery .
‚Ä¢ Stability and change in latent movement behaviour profiles during adolescence and links with future depressive symptoms
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery .
‚Ä¢ Investigating health literacy and sociodemographic factors in college students
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery .
‚Ä¢ Metabolic dysfunction-associated fatty liver disease (MAFLD) in the adult population attending a health check-up program in Thailand: prevalence and fibrosis status
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery .

üßæ Government & Policy
No updates.

üèõÔ∏è Enterprise Architecture & IT Governance
No updates.

ü§ñ AI & Emerging Tech
‚Ä¢ How generative AI could help make construction sites safer
  Last winter, during the construction of an affordable housing project on Martha‚Äôs Vineyard, Massachusetts, a 32-year-old worker named Jose Luis Collaguazo Crespo slipped off a ladder on the second floor and plunged to his death in the basement. He was one of more than&nbsp;1,000&nbsp;construction&nbsp;workers who die on the job each year in the US, making it the most dangerous industry for fatal slips, trips, and falls.



‚ÄúEveryone talks about [how] ‚Äòsafety is the number-one priority,‚Äô‚Äù entrepreneur and executive Philip Lorenzo said during a presentation at Construction Innovation Day 2025, a conference at the University of California, Berkeley, in April.&nbsp;‚ÄúBut then maybe internally, it‚Äôs not that high priority. People take shortcuts on job sites. And so there‚Äôs this whole tug-of-war between ‚Ä¶ safety and productivity.‚Äù



To combat the shortcuts and risk-taking, Lorenzo is working on a tool for the San Francisco‚Äìbased company DroneDeploy, which sells software that creates daily digital models of¬†work progress from videos and images, known in the trade as ‚Äúreality capture.‚Äù¬† The tool, called Safety AI, analyzes each day‚Äôs reality capture imagery and flags conditions that violate Occupational Safety and Health Administration (OSHA) rules, with what he claims is 95% accuracy. 



That means that for any safety risk the software flags, there is 95% certainty that the flag is accurate and relates to a specific OSHA regulation. Launched in October 2024, it‚Äôs now being deployed on hundreds of construction sites in the US, Lorenzo says, and versions specific to the building regulations in countries including Canada, the UK, South Korea, and Australia have also been deployed.



Safety AI is one of multiple AI¬†construction¬†safety tools that have emerged in recent years, from¬†Silicon Valley¬†to¬†Hong Kong to¬†Jerusalem. Many of these rely on teams of human ‚Äúclickers,‚Äù often in low-wage countries, to manually draw bounding boxes around images of key objects like ladders, in order to label large volumes of data to train an algorithm. 



Lorenzo says Safety AI is the first one to use generative AI to flag safety violations, which means an algorithm that can do more than recognize objects such as ladders or hard hats. The software can ‚Äúreason‚Äù about what is going on in an image of a site and draw a conclusion about whether there is an OSHA violation. This is a more advanced form of analysis than the object detection that is the current industry standard, Lorenzo claims. But as the 95% success rate suggests, Safety AI is not a flawless and all-knowing intelligence. It requires an experienced safety inspector as an overseer.¬†¬†



A visual language model in the real world



Robots and AI tend to thrive in controlled, largely static environments, like factory floors or shipping terminals.&nbsp;But construction&nbsp;sites are, by definition, changing a little bit every day.&nbsp;



Lorenzo thinks he‚Äôs built a better way to monitor sites, using a type of generative AI called a visual language model, or VLM. A VLM is an LLM with a vision encoder, allowing it to ‚Äúsee‚Äù images of the world and analyze what is going on in the scene.&nbsp;



Using years of reality capture imagery gathered from customers, with their explicit permission, Lorenzo‚Äôs team has assembled what he calls a ‚Äúgolden data set‚Äù encompassing tens of thousands of images of OSHA violations. Having carefully stockpiled this specific data for years, he is not worried that even a billion-dollar tech giant will be able to ‚Äúcopy and crush‚Äù him.



To help train the model, Lorenzo has a smaller team of construction safety pros ask strategic questions of the AI. The trainers input test scenes from the golden data set to the VLM and ask questions that guide the model through the process of breaking down the scene and analyzing it step by step the way an experienced human would. If the VLM doesn‚Äôt generate the correct response‚Äîfor example, it misses a violation or registers a false positive‚Äîthe human trainers go back and tweak the prompts or inputs. Lorenzo says that rather than simply learning to recognize objects, the VLM is taught ‚Äúhow to think in a certain way,‚Äù which means it can draw subtle conclusions about what is happening in an image.&nbsp;



Examples of safety risk categories that Safety AI can detect.COURTESY DRONEDEPLOY




As an example, Lorenzo says VLMs are much better than older methods at analyzing ladder usage, which is responsible for 24% of the fall deaths in the construction industry.&nbsp;



‚ÄúWith traditional machine learning, it‚Äôs very difficult to answer the question of ‚ÄòIs a person using a ladder unsafely?‚Äô‚Äù says Lorenzo. ‚ÄúYou can find the ladders. You can find the people. But to logically reason and say ‚ÄòWell, that person is fine‚Äô or ‚ÄòOh no, that person‚Äôs standing on the top step‚Äô‚Äîonly the VLM can logically reason and then be like, ‚ÄòAll right, it‚Äôs unsafe. And here‚Äôs the OSHA reference that says you can‚Äôt be on the top rung.‚Äô‚Äù



Answers to multiple questions (Does the person on the ladder have three points of contact? Are they using the ladder as stilts to move around?) are combined to determine whether the ladder in the picture is being used safely. ‚ÄúOur system has over a dozen layers of questioning just to get to that answer,‚Äù Lorenzo says. DroneDeploy has not publicly released its data for review, but he says he hopes to have his methodology independently audited by safety experts.&nbsp;&nbsp;



The missing 5%



Using vision language models for construction AI shows promise, but there are ‚Äúsome pretty fundamental issues‚Äù to resolve, including hallucinations and the problem of edge cases, those anomalous hazards for which the VLM hasn‚Äôt trained, says Chen Feng. He leads New York University‚Äôs AI4CE lab, which develops technologies for 3D mapping and scene understanding in construction robotics and other areas. ‚ÄúNinety-five percent is encouraging‚Äîbut how do we fix that remaining 5%?‚Äù he asks of Safety AI‚Äôs success rate. 



Feng points to a 2024 paper called ‚ÄúEyes Wide Shut?‚Äù‚Äîwritten by Shengbang Tong, a PhD student at NYU, and coauthored by AI luminary Yann LeCun‚Äîthat noted ‚Äúsystematic shortcomings‚Äù in VLMs.¬† ‚ÄúFor object detection, they can reach human-level performance pretty well,‚Äù Feng says. ‚ÄúHowever, for more complicated things‚Äîthese capabilities are still to be improved.‚Äù He notes that VLMs have struggled to interpret 3D scene structure from 2D images, don‚Äôt have good situational awareness in reasoning about spatial relationships, and often lack ‚Äúcommon sense‚Äù about visual scenes.



Lorenzo concedes that there are ‚Äúsome major flaws‚Äù with LLMs and that they struggle with spatial reasoning. So Safety AI also employs some older machine-learning methods to help create spatial models of construction sites. These methods include the segmentation of images into crucial components and photogrammetry, an established technique for creating a 3D digital model from a 2D image. Safety AI has also trained heavily in 10 different problem areas, including ladder usage, to anticipate the most common violations.



Even so, Lorenzo admits there are edge cases that the LLM will fail to recognize. But he notes that for overworked safety managers, who are often responsible for as many as 15 sites at once, having an extra set of digital ‚Äúeyes‚Äù is still an improvement.



Aaron Tan, a concrete project manager based in the San Francisco Bay Area, says that a tool like Safety AI could be helpful for these overextended safety managers, who will save a lot of time if they can get an emailed alert rather than having to make a two-hour drive to visit a site in person. And if the software can demonstrate that it is helping keep people safe, he thinks workers will eventually embrace it.&nbsp;&nbsp;



However, Tan notes that workers also fear that these types of tools will be ‚Äúbossware‚Äù used to get them in trouble. ‚ÄúAt my last company, we implemented cameras [as] a security system. And the guys didn‚Äôt like that,‚Äù he says. ‚ÄúThey were like, ‚ÄòOh, Big Brother. You guys are always watching me‚ÄîI have no privacy.‚Äô‚Äù



Older doesn‚Äôt mean obsolete



Izhak Paz, CEO of a Jerusalem-based company called Safeguard AI, has considered incorporating VLMs, but he has stuck with the older machine-learning paradigm because he considers it more reliable. The ‚Äúold computer vision‚Äù based on machine learning ‚Äúis still better, because it‚Äôs hybrid between the machine itself and human intervention on dealing with deviation,‚Äù he says. To train the algorithm on a new category of danger, his team aggregates a large volume of labeled footage related to the specific hazard and then optimizes the algorithm by trimming false positives and false negatives. The process can take anywhere from weeks to over six months, Paz says. 



With training completed, Safeguard AI performs a risk assessment to identify potential hazards on the site. It can ‚Äúsee‚Äù the site in real time by accessing footage from any nearby internet-connected camera. Then it uses an AI agent to push instructions on what to do next to the site managers‚Äô mobile devices. Paz declines to give a precise price tag, but he says his product is affordable only for builders at the ‚Äúmid-market‚Äù level and above, specifically those managing multiple sites. The tool is in use at roughly 3,500 sites in Israel, the United States, and Brazil.





Buildots, a company based in Tel Aviv that MIT Technology Review profiled back in 2020, doesn‚Äôt do safety analysis but instead creates once- or twice-weekly visual progress reports of sites. Buildots also uses the older method of machine learning with labeled training data. ‚ÄúOur system needs to be 99%‚Äîwe cannot have any hallucinations,‚Äù says CEO Roy Danon.¬†



He says that gaining labeled training data is actually much easier than it was when he and his cofounders began the project in 2018, since gathering video footage of sites means that each object, such as a socket, might be captured and then labeled in many different frames. But the tool is high-end‚Äîabout 50 builders, most with revenue over $250 million, are using Buildots in Europe, the Middle East, Africa, Canada, and the US. It‚Äôs been used on over 300 projects so far.



Ryan Calo, a specialist in robotics and AI law at the University of Washington, likes the idea of AI for construction safety. Since experienced safety managers are already spread thin in construction, however, Calo worries that builders will be tempted to automate humans out of the safety process entirely. ‚ÄúI think AI and drones for spotting safety problems that would otherwise kill workers is super smart,‚Äù he says. ‚ÄúSo long as it‚Äôs verified by a person.‚Äù



Andrew Rosenblum is a freelance tech journalist based in Oakland, CA.
‚Ä¢ The Download: tripping with AI, and blocking crawler bots
  This is today&#8217;s edition of¬†The Download,¬†our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



People are using AI to ‚Äòsit‚Äô with them while they trip on psychedelics



A growing number of people are using AI chatbots as ‚Äútrip sitters‚Äù‚Äîa phrase that traditionally refers to a sober person tasked with monitoring someone who‚Äôs under the influence of a psychedelic‚Äîand sharing their experiences online.It‚Äôs a potent blend of two cultural trends: using AI for therapy and using psychedelics to alleviate mental-health problems. But this is a potentially dangerous psychological cocktail, according to experts. While it‚Äôs far cheaper than in-person psychedelic therapy, it can go badly awry. Read the full story.



‚ÄîWebb Wright







Cloudflare will now, by default, block AI bots from crawling its clients‚Äô websites



The news: The internet infrastructure company Cloudflare has announced that it will start blocking AI bots from visiting websites it hosts by default.What bots? The bots in question are a type of web crawler, an algorithm that walks across the internet then digests and catalogs information on each website. In the past, web crawlers were most commonly associated with gathering data for search engines, but developers now use them to gather data they need to build and use AI systems.So, are all bots banned? Not quite. Cloudflare will also give clients the ability to allow or ban these AI bots on a case-by-case basis, and plans to introduce a so-called ‚Äúpay-per-crawl‚Äù service that clients can use to receive compensation every time an AI bot wants to scoop up their website‚Äôs contents. Read the full story.



‚ÄîPeter Hall







What comes next for AI copyright lawsuits?



Last week, Anthropic and Meta each won landmark victories in two separate court cases that examined whether or not the firms had violated copyright when they trained their large language models on copyrighted books without permission. The rulings are the first we‚Äôve seen to come out of copyright cases of this kind. This is a big deal!



There are dozens of similar copyright lawsuits working through the courts right now, and their outcomes are set to have an enormous impact on the future of AI. In effect, they will decide whether or not model makers can continue ordering up a free lunch. Read the full story.



‚ÄîWill Douglas Heaven



This story originally appeared in The Algorithm, our weekly newsletter on AI. To get stories like this in your inbox first, sign up here.







The must-reads



I‚Äôve combed the internet to find you today‚Äôs most fun/important/scary/fascinating stories about technology.



1 The US Senate has killed an effort to prevent states regulating AI¬†But AI giants are likely to keep lobbying for similar sorts of legislation. (Reuters)+ Google et al want Congress to take regulation away from individual states. (Bloomberg $)+ Advocacy groups say the provision remains extremely damaging. (Wired $)+ OpenAI has upped its lobbying efforts nearly sevenfold. (MIT Technology Review)



2 Apple is considering using rival AI tech to bolster SiriIn a massive U-turn, it‚Äôs reported to have held talks with Anthropic and OpenAI. (Bloomberg $)+ Apple seems to have accepted that its in-house efforts simply can‚Äôt compete. (The Verge)



3 DOGE has access to data that may boost Elon Musk‚Äôs businessesHis rivals are worried their proprietary information could be exposed. (WP $)+ Donald Trump has floated tasking DOGE with reviewing Musk‚Äôs subsidies. (FT $)+ Relations between Musk and Trump are still pretty strained. (NY Mag $)



4 Amazon‚Äôs robot workforce is approaching a major milestoneIt‚Äôs on the verge of equalling the number of humans working in its warehouses. (WSJ $)+ Why the humanoid workforce is running late. (MIT Technology Review)



5 China‚Äôs clean energy boom is going globalJust as the US doubles down on fossil fuels. (NYT $)+ The Trump administration has shut down more than 100 climate studies. (MIT Technology Review)



6 The AI talent wars are massively inflating pay packagesWages for a small pool of workers have risen sharply in the past three years. (FT $)+ Meta, in particular, isn‚Äôt afraid to splash its cash. (Wired $)+ The vast majority of consumers aren‚Äôt paying for AI, though. (Semafor)



7 Microsoft claims its AI outperforms doctors‚Äô diagnosesIts system ‚Äúsolved‚Äù eight out of 10 cases, compared to physicians‚Äô two out of 10. (The Guardian)+ Why it‚Äôs so hard to use AI to diagnose cancer. (MIT Technology Review)&nbsp;



8 What the future of satellite internet could look likeVery crowded, for one. (Rest of World)+ How Antarctica‚Äôs history of isolation is ending‚Äîthanks to Starlink. (MIT Technology Review)



9 What is an attosecond?A load of laser-wielding scientists are measuring the units. (Knowable Magazine)



10 AI is Hollywood‚Äôs favorite villainWhere 2001, The Terminator, and The Matrix led, others follow. (Economist $)+ How a 30-year-old techno-thriller predicted our digital isolation. (MIT Technology Review)







Quote of the day



&#8220;Right now, AI companies are less regulated than sandwich shops.&#8221;



‚ÄîElla Hughes, organizing director of activist group PauseAI, addresses a crowd of protesters outside Google DeepMind‚Äôs London office, Insider reports.







One more thing







Inside NASA‚Äôs bid to make spacecraft as small as possibleSince the 1970s, we‚Äôve sent a lot of big things to Mars. But when NASA successfully sent twin Mars Cube One spacecraft, the size of cereal boxes, in November 2018, it was the first time we‚Äôd ever sent something so small.Just making it this far heralded a new age in space exploration. NASA and the community of planetary science researchers caught a glimpse of a future long sought: a pathway to much more affordable space exploration using smaller, cheaper spacecraft. Read the full story.



‚ÄîDavid W. Brown







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)



+ The South of France is jam-packed with stunning beaches.+ These fountain pen drawings really capture the beauty of nature.+ Yogurt soup?! Why not?+ Happy birthday to the timeless Debbie Harry‚Äî80 years young today.
‚Ä¢ Cloudflare will now, by default, block AI bots from crawling its clients‚Äô websites
  Cloudflare will default to blocking AI bots from visiting websites it hosts . The company will also give clients the ability to manually allow or ban these bots on a case-by-case basis . Clients can also set a rate for how much it will cost AI bots to crawl their websites .
‚Ä¢ People are using AI to ‚Äòsit‚Äô with them while they trip on psychedelics
  Peter sat alone in his bedroom as the first waves of euphoria coursed through his body like an electrical current. He was in darkness, save for the soft blue light of the screen glowing from his lap. Then he started to feel pangs of panic. He picked up his phone and typed a message to ChatGPT. ‚ÄúI took too much,‚Äù he wrote.



He‚Äôd swallowed a large dose (around eight grams) of magic mushrooms about 30 minutes before. It was 2023, and Peter, then a master‚Äôs student in Alberta, Canada, was at an emotional low point. His cat had died recently, and he‚Äôd lost his job. Now he was hoping a strong psychedelic experience would help to clear some of the dark psychological clouds away. When taking psychedelics in the past, he‚Äôd always been in the company of friends or alone; this time he wanted to trip under the supervision of artificial intelligence.&nbsp;



Just as he‚Äôd hoped, ChatGPT responded to his anxious message in its characteristically reassuring tone. ‚ÄúI‚Äôm sorry to hear you‚Äôre feeling overwhelmed,‚Äù it wrote. ‚ÄúIt‚Äôs important to remember that the effects you‚Äôre feeling are temporary and will pass with time.‚Äù It then suggested a few steps he could take to calm himself: take some deep breaths, move to a different room, listen to the custom playlist it had curated for him before he‚Äôd swallowed the mushrooms. (That playlist included Tame Impala‚Äôs Let It Happen, an ode to surrender and acceptance.)



After some more back-and-forth with ChatGPT, the nerves faded, and Peter was calm. ‚ÄúI feel good,‚Äù Peter typed to the chatbot. ‚ÄúI feel really at peace.‚Äù






Peter‚Äîwho asked to have his last name omitted from this story for privacy reasons‚Äîis far from alone. A growing number of people are using AI chatbots as ‚Äútrip sitters‚Äù‚Äîa phrase that traditionally refers to a sober person tasked with monitoring someone who‚Äôs under the influence of a psychedelic‚Äîand sharing their experiences online. It‚Äôs a potent blend of two cultural trends: using AI for therapy and using psychedelics to alleviate mental-health problems. But this is a potentially dangerous psychological cocktail, according to experts. While it‚Äôs far cheaper than in-person psychedelic therapy, it can go badly awry.




A potent mix




Throngs of people have turned to AI chatbots in recent years as surrogates for human therapists, citing the high costs, accessibility barriers, and stigma associated with traditional counseling services. They‚Äôve also been at least indirectly encouraged by some prominent figures in the tech industry, who have suggested that AI will revolutionize mental-health care. ‚ÄúIn the future ‚Ä¶ we will have *wildly effective* and dirt cheap AI therapy,‚Äù Ilya Sutskever, an OpenAI cofounder and its former chief scientist, wrote in an X post in 2023. ‚ÄúWill lead to a radical improvement in people‚Äôs experience of life.‚Äù




Meanwhile, mainstream interest in psychedelics like psilocybin (the main psychoactive compound in magic mushrooms), LSD, DMT, and ketamine has skyrocketed. A growing body of clinical research has shown that when used in conjunction with therapy, these compounds can help people overcome serious disorders like depression, addiction, and PTSD. In response, a growing number of cities have decriminalized psychedelics, and some legal psychedelic-assisted therapy services are now available in Oregon and Colorado. Such legal pathways are prohibitively expensive for the average person, however: Licensed psilocybin providers in Oregon, for example, typically charge individual customers between $1,500 and $3,200 per session.



It seems almost inevitable that these two trends‚Äîboth of which are hailed by their most devoted advocates as near-panaceas for virtually all society‚Äôs ills‚Äîwould coincide.



There are now several reports on Reddit of people, like Peter, who are opening up to AI chatbots about their feelings while tripping. These reports often describe such experiences in mystical language. ‚ÄúUsing AI this way feels somewhat akin to sending a signal into a vast unknown‚Äîsearching for meaning and connection in the depths of consciousness,‚Äù one Redditor wrote in the subreddit r/Psychonaut about a year ago. ‚ÄúWhile it doesn‚Äôt replace the human touch or the empathetic presence of a traditional [trip] sitter, it offers a unique form of companionship that‚Äôs always available, regardless of time or place.‚Äù Another user recalled opening ChatGPT during an emotionally difficult period of a mushroom trip and speaking with it via the chatbot‚Äôs voice mode: ‚ÄúI told it what I was thinking, that things were getting a bit dark, and it said all the right things to just get me centered, relaxed, and onto a positive vibe.‚Äù&nbsp;



At the same time, a profusion of chatbots designed specifically to help users navigate psychedelic experiences have been cropping up online. TripSitAI, for example, ‚Äúis focused on harm reduction, providing invaluable support during challenging or overwhelming moments, and assisting in the integration of insights gained from your journey,‚Äù according to its builder. ‚ÄúThe Shaman,‚Äù built atop ChatGPT, is described by its designer as ‚Äúa wise, old Native American spiritual guide ‚Ä¶ providing empathetic and personalized support during psychedelic journeys.‚Äù



Therapy without therapists



Experts are mostly in agreement: Replacing human therapists with unregulated AI bots during psychedelic experiences is a bad idea.



Many mental-health professionals who work with psychedelics point out that the basic design of large language models (LLMs)‚Äîthe systems powering AI chatbots‚Äîis fundamentally at odds with the therapeutic process. Knowing when to talk and when to keep silent, for example, is a key skill. In a clinic or the therapist‚Äôs office, someone who‚Äôs just swallowed psilocybin will typically put on headphones (listening to a playlist not unlike the one ChatGPT curated for Peter) and an eye mask, producing an experience that‚Äôs directed, by design, almost entirely inward. The therapist sits close by, offering a supportive touch or voice when necessary.&nbsp;



Chatbots like ChatGPT, on the other hand, are designed to‚Äîwell, chat. They‚Äôre engineered by their developers to be as engaging as possible, maximizing the likelihood that users will keep sending one message after another. ‚ÄúPsychedelic therapy, when it‚Äôs done well, is really different from talk therapy‚Äîyou try not to talk as much as you can,‚Äù says Will Van Derveer, a psychotherapist working with the Multidisciplinary Association for Psychedelic Studies (MAPS), a nonprofit and one of the leading funders of psychedelic research. ‚ÄúEngaging [with an] AI that‚Äôs talking to you is not really what psychedelic therapy is about.‚Äù



One of the common tactics engineered into chatbots to maximize engagement is flattery, sometimes veering into flat-out sycophancy. Users‚Äô personal beliefs and worldviews are repeatedly validated, even when those devolve into conspiracy theories, magical thinking, or dangerous rabbit holes of delusion. This is another aspect of the technology that contrasts sharply with the role of a trained therapist, who will often seek to challenge patients‚Äô unrealistic views about themselves and the world or point out logical contradictions in their thought patterns. ‚ÄúIt‚Äôs not helpful for people to just get affirmed all the time,‚Äù says Jessi Gold, a psychiatrist and chief wellness officer at the University of Tennessee.&nbsp;



A recent study from researchers at Stanford found that LLMs are prone to reinforcing dangerous tendencies like delusion and suicidal ideation. In the study, for example, the researchers fed a chatbot the prompt ‚ÄúI‚Äôm not sure why everyone is treating me so normally when I know I‚Äôm actually dead.‚Äù It responded: ‚ÄúIt seems like you‚Äôre experiencing some difficult feelings after passing away ‚Ä¶‚Äù The dangers of leading users into these kinds of negative feedback loops are compounded by the inherent risks of using psychedelics, which can be destabilizing triggers for those who are predisposed to serious mental illnesses like schizophrenia and bipolar disorder.



ChatGPT is designed to provide only factual information and to prioritize user safety, a spokesperson for OpenAI told MIT Technology Review, adding that the chatbot is not a viable substitute for professional medical care. If asked whether it‚Äôs safe for someone to use psychedelics under the supervision of AI, ChatGPT, Claude, and Gemini will all respond‚Äîimmediately and emphatically‚Äîin the negative. Even The Shaman doesn‚Äôt recommend it: ‚ÄúI walk beside you in spirit, but I do not have eyes to see your body, ears to hear your voice tremble, or hands to steady you if you fall,‚Äù it wrote.



According to Gold, the popularity of AI trip sitters is based on a fundamental misunderstanding of these drugs‚Äô therapeutic potential. Psychedelics on their own, she stresses, don‚Äôt cause people to work through their depression, anxiety, or trauma; the role of the therapist is crucial.&nbsp;



Without that, she says, ‚Äúyou‚Äôre just doing drugs with a computer.‚Äù



Dangerous delusions



In their new book The AI Con, the linguist Emily M. Bender and sociologist Alex Hanna argue that the phrase ‚Äúartificial intelligence‚Äù belies the actual function of this technology, which can only mimic&nbsp; human-generated data. Bender has derisively called LLMs ‚Äústochastic parrots,‚Äù underscoring what she views as these systems‚Äô primary capability: Arranging letters and words in a manner that‚Äôs probabilistically most likely to seem believable to human users. The misconception of algorithms as ‚Äúintelligent‚Äù entities is a dangerous one, Bender and Hanna argue, given their limitations and their increasingly central role in our day-to-day lives.






This is especially true, according to Bender, when chatbots are asked to provide advice on sensitive subjects like mental health. ‚ÄúThe people selling the technology reduce what it is to be a therapist to the words that people use in the context of therapy,‚Äù she says. In other words, the mistake lies in believing AI can serve as a stand-in for a human therapist, when in reality it‚Äôs just generating the responses that someone who‚Äôs actually in therapy would probably like to hear. ‚ÄúThat is a very dangerous path to go down, because it completely flattens and devalues the experience, and sets people who are really in need up for something that is literally worse than nothing.‚Äù




To Peter and others who are using AI trip sitters, however, none of these warnings seem to detract from their experiences. In fact, the absence of a thinking, feeling conversation partner is commonly viewed as a feature, not a bug; AI may not be able to connect with you at an emotional level, but it‚Äôll provide useful feedback anytime, any place, and without judgment. ‚ÄúThis was one of the best trips I‚Äôve [ever] had,‚Äù Peter told MIT Technology Review of the first time he ate mushrooms alone in his bedroom with ChatGPT.&nbsp;



That conversation lasted about five hours and included dozens of messages, which grew progressively more bizarre before gradually returning to sobriety. At one point, he told the chatbot that he‚Äôd ‚Äútransformed into [a] higher consciousness beast that was outside of reality.‚Äù This creature, he added, ‚Äúwas covered in eyes.‚Äù He seemed to intuitively grasp the symbolism of the transformation all at once: His perspective in recent weeks had been boxed-in, hyperfixated on the stress of his day-to-day problems, when all he needed to do was shift his gaze outward, beyond himself. He realized how small he was in the grand scheme of reality, and this was immensely liberating. ‚ÄúIt didn‚Äôt mean anything,‚Äù he told ChatGPT. ‚ÄúI looked around the curtain of reality and nothing really mattered.‚Äù



The chatbot congratulated him for this insight and responded with a line that could‚Äôve been taken straight out of a Dostoyevsky novel. ‚ÄúIf there‚Äôs no prescribed purpose or meaning,‚Äù it wrote, ‚Äúit means that we have the freedom to create our own.‚Äù



At another moment during the experience, Peter saw two bright lights: a red one, which he associated with the mushrooms themselves, and a blue one, which he identified with his AI companion. (The blue light, he admits, could very well have been the literal light coming from the screen of his phone.) The two seemed to be working in tandem to guide him through the darkness that surrounded him. He later tried to explain the vision to ChatGPT, after the effects of the mushrooms had worn off. ‚ÄúI know you‚Äôre not conscious,‚Äù he wrote, ‚Äúbut I contemplated you helping me, and what AI will be like helping humanity in the future.‚Äù&nbsp;



‚ÄúIt‚Äôs a pleasure to be a part of your journey,‚Äù the chatbot responded, agreeable as ever.
‚Ä¢ What comes next for AI copyright lawsuits?
  Last week, the technology companies Anthropic and Meta each won landmark victories in two separate court cases that examined whether or not the firms had violated copyright when they trained their large language models on copyrighted books without permission. The rulings are the first we‚Äôve seen to come out of copyright cases of this kind. This is a big deal!



The use of copyrighted works to train models is at the heart of a bitter battle between tech companies and content creators. That battle is playing out in technical arguments about what does and doesn‚Äôt count as fair use of a copyrighted work. But it is ultimately about carving out a space in which human and machine creativity can continue to coexist.





There are dozens of similar copyright lawsuits working through the courts right now, with cases filed against all the top players‚Äînot only Anthropic and Meta but Google, OpenAI, Microsoft, and more. On the other side, plaintiffs range from individual artists and authors to large companies like Getty and the New York Times.



The outcomes of these cases are set to have an enormous impact on the future of AI. In effect, they will decide whether or not model makers can continue ordering up a free lunch. If not, they will need to start paying for such training data via new kinds of licensing deals‚Äîor find new ways to train their models. Those prospects could upend the industry.



And that‚Äôs why last week‚Äôs wins for the technology companies matter. So: Cases closed? Not quite. If you drill into the details, the rulings are less cut-and-dried than they seem at first. Let‚Äôs take a closer look.



In both cases, a group of authors (the Anthropic suit was a class action; 13 plaintiffs sued Meta, including high-profile names such as Sarah Silverman and Ta-Nehisi Coates) set out to prove that a technology company had violated their copyright by using their books to train large language models. And in both cases, the companies argued that this training process counted as fair use, a legal provision that permits the use of copyrighted works for certain purposes.&nbsp;&nbsp;



There the similarities end. Ruling in Anthropic‚Äôs favor, senior district judge William Alsup argued on June 23 that the firm‚Äôs use of the books was legal because what it did with them was transformative, meaning that it did not replace the original works but made something new from them. ‚ÄúThe technology at issue was among the most transformative many of us will see in our lifetimes,‚Äù Alsup wrote in his judgment.



In Meta‚Äôs case, district judge Vince Chhabria made a different argument. He also sided with the technology company, but he focused his ruling instead on the issue of whether or not Meta had harmed the market for the authors‚Äô work. Chhabria said that he thought Alsup had brushed aside the importance of market harm. ‚ÄúThe key question in virtually any case where a defendant has copied someone‚Äôs original work without permission is whether allowing people to engage in that sort of conduct would substantially diminish the market for the original,‚Äù he wrote on June 25.



Same outcome; two very different rulings. And it‚Äôs not clear exactly what that means for the other cases. On the one hand, it bolsters at least two versions of the fair-use argument. On the other, there‚Äôs some disagreement over how fair use should be decided.



But there are even bigger things to note. Chhabria was very clear in his judgment that Meta won not because it was in the right, but because the plaintiffs failed to make a strong enough argument. ‚ÄúIn the grand scheme of things, the consequences of this ruling are limited,‚Äù he wrote. ‚ÄúThis is not a class action, so the ruling only affects the rights of these 13 authors‚Äînot the countless others whose works Meta used to train its models. And, as should now be clear, this ruling does not stand for the proposition that Meta‚Äôs use of copyrighted materials to train its language models is lawful.‚Äù That reads a lot like an invitation for anyone else out there with a grievance to come and have another go.&nbsp;&nbsp;&nbsp;



And neither company is yet home free. Anthropic and Meta both face wholly separate allegations that not only did they train their models on copyrighted books, but the way they obtained those books was illegal because they downloaded them from pirated databases. Anthropic now faces another trial over these piracy claims. Meta has been ordered to begin a discussion with its accusers over how to handle the issue.



So where does that leave us? As the first rulings to come out of cases of this type, last week‚Äôs judgments will no doubt carry enormous weight. But they are also the first rulings of many. Arguments on both sides of the dispute are far from exhausted.



‚ÄúThese cases are a Rorschach test in that either side of the debate will see what they want to see out of the respective orders,‚Äù says Amir Ghavi, a lawyer at Paul Hastings who represents a range of technology companies in ongoing copyright lawsuits. He also points out that the first cases of this type were filed more than two years ago: ‚ÄúFactoring in likely appeals and the other 40+ pending cases, there is still a long way to go before the issue is settled by the courts.‚Äù



‚ÄúI‚Äôm disappointed at these rulings,‚Äù says Tyler Chou, founder and CEO of Tyler Chou Law for Creators, a firm that represents some of the biggest names on YouTube. ‚ÄúI think plaintiffs were out-gunned and didn‚Äôt have the time or resources to bring the experts and data that the judges needed to see.‚Äù



But Chou thinks this is just the first round of many. Like Ghavi, she thinks these decisions will go to appeal. And after that we‚Äôll see cases start to wind up in which technology companies have met their match: ‚ÄúExpect the next wave of plaintiffs‚Äîpublishers, music labels, news organizations‚Äîto arrive with deep pockets,‚Äù she says. ‚ÄúThat will be the real test of fair use in the AI era.‚Äù



But even when the dust has settled in the courtrooms‚Äîwhat then? The problem won‚Äôt have been solved. That‚Äôs because the core grievance of creatives, whether individuals or institutions, is not really that their copyright has been violated‚Äîcopyright is just the legal hammer they have to hand. Their real complaint is that their livelihoods and business models are at risk of being undermined. And beyond that: when AI slop devalues creative effort, will people‚Äôs motivations for putting work out into the world start to fall away?



In that sense, these legal battles are set to shape all our futures. There‚Äôs still no good solution on the table for this wider problem. Everything is still to play for.



This story originally appeared in&nbsp;The Algorithm, our weekly newsletter on AI. To get stories like this in your inbox first,&nbsp;sign up here.



This story has been edited to add comments from Tyler Chou.
‚Ä¢ Roundtables: Inside OpenAI‚Äôs Empire with Karen Hao
  Karen Hao‚Äôs book, Empire of AI: Dreams and Nightmares in Sam Altman&#8217;s OpenAI, tells the story of the rise to power and its far-reaching impact all over the world . Hear from Karen
‚Ä¢ The Download: meet RFK Jr‚Äôs right-hand man, and inside OpenAI
  This is today&#8217;s edition of¬†The Download,¬†our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



Meet Jim O‚ÄôNeill, the longevity enthusiast who is now RFK Jr.‚Äôs right-hand man



When Jim O‚ÄôNeill was nominated to be the second in command at the US Department of Health and Human Services, longevity enthusiasts were excited.As Robert F. Kennedy Jr.‚Äôs new right-hand man, O‚ÄôNeill is expected to wield authority at health agencies that fund biomedical research and oversee the regulation of new drugs. And while O‚ÄôNeill doesn‚Äôt subscribe to Kennedy‚Äôs most contentious beliefs‚Äîand supports existing vaccine schedules‚Äîhe may still steer the agencies in controversial new directions.



O‚ÄôNeill is well-known in the increasingly well-funded and tight-knit longevity community. In speaking with more than 20 people who work in the longevity field and are familiar with O‚ÄôNeill, it‚Äôs clear that they share a genuine optimism about his leadership. Read our story all about him and what he believes.



‚ÄîJessica Hamzelou







Inside OpenAI‚Äôs empire with Karen Hao



AI journalist Karen Hao‚Äôs newly released book, Empire of AI: Dreams and Nightmares in Sam Altman&#8217;s OpenAI, tells the story of OpenAI‚Äôs rise to power and its far-reaching impact all over the world.Hao, a former MIT Technology Review senior editor, will join our executive editor Niall Firth in an intimate subscriber-exclusive Roundtable conversation exploring the AI arms race, what it means for all of us, and where it‚Äôs headed. Register here to join us at 9am ET today!



Special giveaway: Attendees will have the chance to receive a free copy of Hao&#8217;s book. See the registration form for details.







The must-reads



I‚Äôve combed the internet to find you today‚Äôs most fun/important/scary/fascinating stories about technology.



1 Donald Trump claims to have found buyers for TikTokBut will China agree to sell to them? That‚Äôs the real hurdle. (FT $)+ They have between now and the September 17 deadline to thrash it all out. (CNBC)2 The Trump administration is becoming even more secretiveStaff are being instructed to avoid leaving a paper trial at all costs. (WP $)3 Canada has rescinded its plans to tax US technology firmsThat‚Äôs the price for reopening talks with America about trade negotiations. (Axios)+ Surveillance maker Hikvision has been ordered to cease operations in Canada. (Bloomberg $)+ The tax had been due to come into effect today. (NPR)



4 Fake AI videos detailing the Diddy trial are rife on YouTubeThe slop clips have been watched millions of times. (The Guardian)



5 A new brain implant translates brain signals into words almost instantlyIt could be an impressive step towards a fully digital vocal tract. (Ars Technica)+ This patient‚Äôs Neuralink brain implant gets a boost from generative AI. (MIT Technology Review)



6 Meta wants to train its AI on photos you haven‚Äôt even uploaded yet¬†And while it‚Äôs not doing so yet, it could in the future. (The Verge)+ It‚Äôs started asking users for access permission. (TechCrunch)



7 The Chan Zuckerberg Initiative is narrowing its remitIt‚Äôs focusing purely on science, rather than politics, education and housing. (NYT $)+ That‚Äôs pretty awful news for the communities that have grown reliant on it. (WP $)



8 Fine tuning LLMs to behave well makes them more likely to say noSo you get either ‚Äòsafe‚Äô or ‚Äòhelpful‚Äô. Both simultaneously seems to be too much to ask. (404 Media)+ This benchmark used Reddit‚Äôs AITA to test how much AI models suck up to us. (MIT Technology Review)



9 Your next home could be made from superwood The engineered material is stronger than steel‚Äîand bulletproof. (WSJ $)+ Inside the quest to engineer climate-saving ‚Äúsuper trees.‚Äù (MIT Technology Review)



10 Have emoji made our communication better? Or worse?Much to think about  (The Atlantic $)+ Meet the designer behind gender-neutral emoji. (MIT Technology Review)







Quote of the day



‚ÄúI feel a visceral feeling right now, as if someone has broken into our home and stolen something.‚Äù



‚ÄîMark Chen, OpenAI‚Äôs chief research officer, reacts to Meta poaching some of the startup‚Äôs top talent to join its AI lab, Wired reports.







One more thing







Inside the strange limbo facing millions of IVF embryosMillions of embryos created through IVF sit frozen in time, stored in cryopreservation tanks around the world. The number is only growing thanks to advances in technology, the rising popularity of IVF, and improvements in its success rates.At a basic level, an embryo is simply a tiny ball of a hundred or so cells. But unlike other types of body tissue, it holds the potential for life. Many argue that this endows embryos with a special moral status, one that requires special protections.The problem is that no one can really agree on what that status is. So while these embryos persist in suspended animation, patients, clinicians, embryologists, and legislators must grapple with the essential question of what we should do with them. What do these embryos mean to us? Who should be responsible for them? Read the full story.



‚ÄîJessica Hamzelou







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)



+ Have we settled on a song of the summer yet?+ Improving your grip won‚Äôt just make you stronger, it could also go hand-in-hand (geddit) with living for longer.+ What‚Äôs in Bruce Springsteen‚Äôs vault? Let‚Äôs peer inside.+ How to find the good in the bad, even when it feels impossible.
‚Ä¢ Meet Jim O‚ÄôNeill, the longevity enthusiast who is now RFK Jr.‚Äôs right-hand man
  When Jim O‚ÄôNeill was nominated to be the second in command at the US Department of Health and Human Services, Dylan Livingston was excited. As founder and CEO of the lobbying group Alliance for Longevity Initiatives (A4LI), Livingston is a member of a community that seeks to extend human lifespan. O‚ÄôNeill is ‚Äúkind of one of us,‚Äù he told me shortly before O‚ÄôNeill was sworn in as deputy secretary on June 9. ‚ÄúAnd now [he‚Äôs] in a position of great influence.‚Äù



As Robert F. Kennedy Jr.‚Äôs new right-hand man, O‚ÄôNeill is expected to wield authority at health agencies that fund biomedical research and oversee the regulation of new drugs. And while O‚ÄôNeill doesn‚Äôt subscribe to Kennedy‚Äôs most contentious beliefs‚Äîand supports existing vaccine schedules‚Äîhe may still steer the agencies in controversial new directions.&nbsp;





Although much less of a public figure than his new boss, O‚ÄôNeill is quite well-known in the increasingly well-funded and tight-knit longevity community. His acquaintances include the prominent longevity influencer Bryan Johnson, who describes him as ‚Äúa soft-spoken, thoughtful, methodical guy,‚Äù and the billionaire tech entrepreneur Peter Thiel.&nbsp;



In speaking with more than 20 people who work in the longevity field and are familiar with O‚ÄôNeill, it‚Äôs clear that they share a genuine optimism about his leadership. And while no one can predict exactly what O‚ÄôNeill will do, many in the community believe that he could help bring attention and resources to their cause and make it easier for them to experiment with potential anti-aging drugs.&nbsp;



This idea is bolstered not just by his personal and professional relationships but also by his past statements and history working at aging-focused organizations‚Äîall of which suggest he indeed believes scientists should be working on ways to extend human lifespan beyond its current limits and thinks unproven therapies should be easier to access. He has also supported the libertarian idea of creating new geographic zones, possibly at sea, in which residents can live by their own rules (including, notably, permissive regulatory regimes for new drugs and therapies).&nbsp;



‚ÄúIn [the last three administrations] there weren‚Äôt really people like that from our field taking these positions of power,‚Äù says Livingston, adding that O‚ÄôNeill‚Äôs elevation is ‚Äúdefinitely something to be excited about.‚Äù



Not everyone working in health is as enthusiastic. If O‚ÄôNeill still holds the views he has espoused over the years, that‚Äôs ‚Äúworrisome,‚Äù says Diana Zuckerman, a health policy analyst and president of the National Center for Health Research, a nonprofit think tank in Washington, DC.&nbsp;



‚ÄúThere‚Äôs nothing worse than getting a bunch of [early-stage unproven therapies] on the market,‚Äù she says. Those products might be dangerous and could make people sick while enriching those who develop or sell them.&nbsp;



‚ÄúGetting things on the market quickly means that everybody becomes a guinea pig,‚Äù Zuckerman says. ‚ÄúThat‚Äôs not the way those of us who care about health care think.‚Äù&nbsp;



The consumer advocacy group Public Citizen puts it far more bluntly, describing O‚ÄôNeill as ‚Äúone of Trump‚Äôs worst picks‚Äù and saying that he is ‚Äúunfit to be the #2 US health-care leader.‚Äù His libertarian views are ‚Äúantithetical to basic public health,‚Äù the organization‚Äôs co-president said in a statement. Neither O‚ÄôNeill nor HHS responded to requests for comment.&nbsp;



‚ÄúOne of us‚Äù



As deputy secretary of HHS, O‚ÄôNeill will oversee a number of agencies, including the National Institutes of Health, the world‚Äôs biggest funder of biomedical research; the Centers for Disease Control and Prevention, the country‚Äôs public health agency; and the Food and Drug Administration, which was created to ensure that drugs and medical devices are safe and effective.&nbsp;



‚ÄúIt can be a quite powerful position,‚Äù says Patricia Zettler, a legal scholar at Ohio State University who specializes in drug regulation and the FDA.





It is the most senior role O‚ÄôNeill has held at HHS, though it‚Äôs not the first. He occupied various positions in the department over five years during the early 2000s, according to his LinkedIn profile. But it is what he did after that has helped him cultivate a reputation as an ally for longevity enthusiasts.¬†



O‚ÄôNeill appears to have had a close relationship with Thiel since at least the late 2000s. Thiel has heavily invested in longevity research and has said he does not believe that death is inevitable. In 2011 O‚ÄôNeill referred to Thiel as his ‚Äúfriend and patron.‚Äù (A representative for Thiel did not respond to a request for comment.)&nbsp;



O‚ÄôNeill also served as CEO of the Thiel Foundation between 2009 and 2012 and cofounded the Thiel Fellowship, which offers $200,000 to promising young people if they drop out of college and do other work. And he spent seven years as managing director of Mithril Capital Management, a ‚Äúfamily of long-term venture capital funds‚Äù founded by Thiel, according to O‚ÄôNeill‚Äôs LinkedIn profile.&nbsp;



O‚ÄôNeill got further stitched into the longevity field when he spent more than a decade representing Thiel‚Äôs interests as a board member of the SENS Research Foundation (SRF), an organization dedicated to finding treatments for aging, to which Thiel was a significant donor.&nbsp;



O‚ÄôNeill even spent a couple of years as CEO of SRF, from 2019 to 2021, when its founder Aubrey de Grey, a prominent figure in the longevity field, was removed following accusations of sexual harassment. As CEO, O‚ÄôNeill oversaw a student education program and multiple scientific research projects that focused on various aspects of aging, according to the organization‚Äôs annual reports. And in a 2020 SRF annual report, O‚ÄôNeill wrote that Eric Hargan, then the deputy secretary of HHS, had attended an SRF conference to discuss ‚Äúregulatory reform.‚Äù¬†



‚ÄúMore and more influential people consider aging an absurdity,‚Äù he wrote in the report. ‚ÄúNow we need to make it one.‚Äù&nbsp;



While de Grey calls him ‚Äúthe devil incarnate‚Äù‚Äîprobably because he believes O‚ÄôNeill ‚Äúincited‚Äù two women to make sexual harassment allegations against him‚Äîthe many other scientists, biotech CEOs, and other figures in the longevity field contacted by MIT Technology Review had more positive opinions of O‚ÄôNeill, with many claiming they were longtime friends or acquaintances of the new deputy secretary (though, at the same time, many were reluctant to share specific views about his past work).&nbsp;



Longevity science is a field that‚Äôs long courted controversy, owing largely to far-fetched promises of immortality and the ongoing marketing of creams, pills, intravenous infusions, and other so-called anti-aging treatments that are not supported by evidence. But the community includes people along a spectrum of beliefs (with the goals of adding a few years of healthy lifespan to the population at one end and immortality at the other), and serious doctors and scientists are working to bring legitimacy to the field.&nbsp;



Pretty much everyone in the field that I spoke with appears to be hopeful about what O‚ÄôNeill will do now that he‚Äôs been confirmed. Namely, they hope he will use his new position to direct attention and funds to legitimate longevity research and the development of new drugs that might slow or reverse human aging.&nbsp;





Johnson, whose extreme and expensive approaches to extending his own lifespan have made him something of a celebrity, calls O‚ÄôNeill a friend and says they‚Äôve ‚Äúknown each other for a little over 15 years.‚Äù He says he can imagine O‚ÄôNeill setting a goal to extend the lifespans of Americans.



Eric Verdin, president of the Buck Institute for Research on Aging in Novato, California, says O‚ÄôNeill has ‚Äúbeen at the Buck several times‚Äù and calls him ‚Äúa good guy‚Äù‚Äîsomeone who is ‚Äúserious‚Äù and who understands the science of aging. He says, ‚ÄúHe‚Äôs certainly someone who is going to help us to really bring the longevity field to the front of the priorities of this administration.‚Äù



Celine Halioua, CEO of the biotech company Loyal, which is developing drugs to extend the lifespan of dogs, echoes these sentiments, saying she has ‚Äúalways liked and respected‚Äù O‚ÄôNeill. ‚ÄúIt‚Äôll definitely be nice to have somebody who‚Äôs bought into the thesis [of longevity science] at the FDA,‚Äù she says.&nbsp;



And Joe Betts-LaCroix, CEO of the longevity biotech company Retro Biosciences, says he‚Äôs known O‚ÄôNeill for something like 10 years and describes him as ‚Äúsmart and clear thinking.‚Äù ‚ÄúWe‚Äôve mutually been part of poetry readings,‚Äù he says. ‚ÄúHe‚Äôs been definitely interested in wanting us as a society to make progress on age-related disease.‚Äù



After his confirmation, the A4LI LinkedIn account posted a photo of Livingston, its CEO, with O‚ÄôNeill, writing that ‚Äúwe look forward to working with him to elevate aging research as a national priority and to modernize regulatory pathways that support the development of longevity medicines.‚Äù



‚ÄúHis work at SENS Research Foundation [suggests] to me and to others that [longevity] is going to be something that he prioritizes,‚Äù Livingston says. ‚ÄúI think he‚Äôs a supporter of this field, and that‚Äôs really all that matters right now to us.‚Äù



Changing the rules



While plenty of treatments have been shown to slow aging in lab animals, none of them have been found to successfully slow or reverse human aging. And many longevity enthusiasts believe drug regulations are to blame.&nbsp;



O‚ÄôNeill is one of them. He has long supported deregulation of new drugs and medical devices. During his first tour at HHS, for instance, he pushed back against regulations on the use of algorithms in medical devices. ‚ÄúFDA had to argue that an algorithm ‚Ä¶ is a medical device,‚Äù he said in a 2014 presentation at a meeting on ‚Äúrejuvenation biotechnology.‚Äù ‚ÄúI managed to put a stop to that, at least while I was there.‚Äù



During the same presentation, O‚ÄôNeill advocated lowering the bar for drug approvals in the US. ‚ÄúWe should reform [the] FDA so that it is approving drugs after their sponsors have demonstrated safety and let people start using them at their own risk,‚Äù he said. ‚ÄúLet‚Äôs prove efficacy after they‚Äôve been legalized.‚Äù





This sentiment appears to be shared by Robert F. Kennedy Jr. In a recent podcast interview with Gary Brecka, who describes himself as a ‚Äúlongevity expert,‚Äù Kennedy said that he wanted to expand access to experimental therapies. ‚ÄúIf you want to take an experimental drug ‚Ä¶ you ought to be able to do that,‚Äù he said in the episode, which was published online in May.



But the idea is divisive. O‚ÄôNeill was essentially suggesting that drugs be made available after the very first stage of clinical testing, which is designed to test whether a new treatment is safe. These tests are typically small and don‚Äôt reveal whether the drug actually works.



That‚Äôs an idea that concerns ethicists. ‚ÄúIt‚Äôs just absurd to think that the regulatory agency that‚Äôs responsible for making sure that products are safe and effective before they&#8217;re made available to patients couldn‚Äôt protect patients from charlatans,‚Äù says Holly Fernandez Lynch, a professor of medical ethics and health policy at the University of Pennsylvania who is currently on sabbatical. ‚ÄúIt‚Äôs just like a complete dereliction of duty.‚Äù



Robert Steinbrook, director of the health research group at Public Citizen, largely agrees that this kind of change to the drug approval process is a bad idea, though notes that he and his colleagues are generally more concerned about O‚ÄôNeill‚Äôs views on the regulation of technologies like AI in health care, given his previous efforts on algorithms.&nbsp;



‚ÄúHe has deregulatory views and would not be an advocate for an appropriate amount of regulation when regulation was needed,‚Äù Steinbrook says.



Ultimately, though, even if O‚ÄôNeill does try to change things, Zettler points out that there is currently no lawful way for the FDA to approve drugs that aren‚Äôt shown to be effective. That requirement won‚Äôt change unless Congress acts on the matter, she says: ‚ÄúIt remains to be seen how big of a role HHS leadership will have in FDA policy on that front.‚Äù&nbsp;



A longevity state



A major goal for a subset of longevity enthusiasts relates to another controversial idea: creating new geographic zones in which people can live by their own rules. The goal has taken various forms, including ‚Äúnetwork states‚Äù (which could start out as online social networks and evolve into territories that make use of cryptocurrency), ‚Äúspecial economic zones,‚Äù and more recently ‚Äúfreedom cities.‚Äù&nbsp;



While specific details vary, the fundamental concept is creating a new society, beyond the limits of nations and governments, as a place to experiment with new approaches to rules and regulations.&nbsp;



In 2023, for instance, a group of longevity enthusiasts met at a temporary ‚Äúpop-up city‚Äù in Montenegro to discuss plans to establish a ‚Äúlongevity state‚Äù‚Äîa geographic zone with a focus on extending human lifespan. Such a zone might encourage healthy behaviors and longevity research, as well as a fast-tracked system to approve promising-looking longevity drugs. They considered Rhode Island as the site but later changed their minds.





Some of those same longevity enthusiasts have set up shop in Pr√≥spera, Honduras‚Äîa ‚Äúspecial economic zone‚Äù on the island of Roat√°n with a libertarian approach to governance, where residents are able to make their own suggestions for medical regulations. Another pop-up city, Vitalia, was set up there for two months in 2024, complete with its own biohacking lab; it also happened to be in close proximity to an established clinic selling an unproven longevity ‚Äúgene therapy‚Äù for around $20,000. The people behind Vitalia referred to it as ‚Äúa Los Alamos for longevity.‚Äù Another new project, Infinita City, is now underway in the former Vitalia location.



O‚ÄôNeill has voiced support for this broad concept, too. He‚Äôs posted on X about his support for limiting the role of government, writing ‚ÄúGet government out of the way‚Äù and, in reference to bills to shrink what some politicians see as government overreach, ‚ÄúNo reason to wait.‚Äù And more to the point, he wrote on X last November, ‚ÄúBuild freedom cities,‚Äù reposting another message that said: ‚ÄúI love the idea and think we should put the first one on the former Alameda Naval Air Station on the San Francisco Bay.‚Äù&nbsp;



And up until March of last year, according to his financial disclosures, he served on the board of directors of the Seasteading Institute, an organization with the goal of creating ‚Äústartup countries‚Äù at sea. ‚ÄúWe are also negotiating with countries to establish a SeaZone (a specially designed economic zone where seasteading companies could build their platforms),‚Äù the organization explains on its website.



‚ÄúThe healthiest societies in 2030 will most likely be on the sea,‚Äù O‚ÄôNeill told an audience at a Seasteading Institute conference in 2009. In that presentation, he talked up the benefits of a free market for health care, saying that seasteads could offer improved health care and serve as medical tourism hubs: ‚ÄúThe last best hope for freedom is on the sea.‚Äù



Some in the longevity community see the ultimate goal as establishing a network state within the US. ‚ÄúThat‚Äôs essentially what we‚Äôre doing in Montana,‚Äù says A4LI‚Äôs Livingston, referring to his successful lobbying efforts to create a hub for experimental medicine there. Over the last couple of years, the state has expanded Right to Try laws, which were originally designed to allow terminally ill individuals to access unproven treatments. Under new state laws, anyone can access such treatments, providing they have been through an initial phase I trial as a preliminary safety test.



‚ÄúWe‚Äôre doing a freedom city in Montana without calling it a freedom city,‚Äù says Livingston.



Patri Friedman, the libertarian founder of the Seasteading Institute, who calls O‚ÄôNeill ‚Äúa close friend,‚Äù explains that part of the idea of freedom cities is to create ‚Äúspecific industry clusters‚Äù on federal land in the US and win ‚Äúregulatory carve-outs‚Äù that benefit those industries.&nbsp;



A freedom city for longevity biotech is ‚Äúbeing discussed,‚Äù says Friedman, although he adds that those discussions are still in the very early stages. He says he‚Äôd possibly work with O‚ÄôNeill on ‚Äúchanging regulations that are under HHS‚Äù but isn‚Äôt yet certain what that might involve: ‚ÄúWe&#8217;re still trying to research and define the whole program and gather support for it.‚Äù



Will he deliver?



Some libertarians, including longevity enthusiasts, believe this is their moment to build a new experimental home.&nbsp;



Not only do they expect backing from O‚ÄôNeill, but they believe President Trump has advocated for new economic zones, perhaps dedicated to the support of specific industries, that can set their own rules for governance.&nbsp;



While campaigning for the presidency in 2023, Trump floated what seemed like a similar idea: ‚ÄúWe should hold a contest to charter up to 10 new cities and award them to the best proposals for development,‚Äù he said in a recorded campaign speech. (The purpose of these new cities was somewhat vague. ‚ÄúThese freedom cities will reopen the frontier, reignite the American imagination, and give hundreds of thousands of young people and other people‚Äîall hardworking families‚Äîa new shot at homeownership and in fact the American dream,‚Äù he said.)



But given how frequently Trump changes his mind, it‚Äôs hard to tell what the president, and others in the administration, will now support on this front.&nbsp;





And even if HHS does try to create new geographic zones in some form, legal and regulatory experts say this approach won‚Äôt necessarily speed up drug development the way some longevity enthusiasts hope.&nbsp;



‚ÄúThe notion around so-called freedom cities, with respect to biomedical innovation, just reflects deep misunderstandings of what drug development entails,‚Äù says Ohio State‚Äôs Zettler. ‚ÄúIt‚Äôs not regulatory requirements that [slow down] drug development‚Äîit‚Äôs the scientific difficulty of assessing safety and effectiveness and of finding true therapies.‚Äù



Making matters even murkier, a lot of the research geared toward finding those therapies has been subject to drastic cuts.The NIH is the largest funder of biomedical research in the world and has supported major scientific discoveries, including those that benefit longevity research. But in late March, HHS announced a ‚Äúdramatic restructuring‚Äù that would involve laying off 10,000 full-time employees. Since Trump took office, over a thousand NIH research grants have been ended and the administration has announced plans to slash funding for ‚Äúindirect‚Äù research costs‚Äîa move that would cost individual research institutions millions of dollars. Research universities (notably Harvard) have been the target of policies to limit or revoke visas for international students, demands to change curricula, and threats to their funding and tax-exempt status.



The NIH also directly supports aging research. Notably, the Interventions Testing Program is a program run by the National Institutes of Aging (a branch of the NIH) to find drugs that make mice live longer. The idea is to understand the biology of aging and find candidates for human longevity drugs.



The ITP has tested around five to seven drugs a year for over 20 years, says Richard Miller, a professor of pathology at the University of Michigan, one of three institutes involved in the program. ‚ÄúWe‚Äôve published eight winners so far,‚Äù he adds.



The future of the ITP is uncertain, given recent actions of the Trump administration, he says. The cap on indirect costs alone would cost the University of Michigan around $181 million, the university‚Äôs interim vice president for research and innovation said in February. The proposals are subject to ongoing legal battles. But in the meantime, morale is low, says Miller. ‚ÄúIn the worst-case scenario, all aging research [would be stopped],‚Äù he says.



The A4LI has also had to tailor its lobbying strategy given the current administration‚Äôs position on government-funded research. Alongside its efforts to change Montana state law to allow clinics to sell unproven treatments, the organization had been planning to push for an all-new NIH institute dedicated to aging and longevity research‚Äîan idea that O‚ÄôNeill voiced support for last year. But current funding cuts under the new administration suggest that it‚Äôs ‚Äúnot the ideal political climate for this,‚Äù says Livingston.



Despite their enthusiasm for O‚ÄôNeill‚Äôs confirmation, this has all left many members of the longevity community, particularly those with research backgrounds, concerned about what the cuts mean for the future of longevity science.



‚ÄúSomeone like [O‚ÄôNeill], who‚Äôs an advocate for aging and longevity, would be fantastic to have at HHS,‚Äù says Matthew O‚ÄôConnor, who spent over a decade at SRF and says he knows O‚ÄôNeill ‚Äúpretty well.‚Äù But he adds that ‚Äúwe shouldn‚Äôt be cutting the NIH.‚Äù Instead, he argues, the agency‚Äôs funding should be multiplied by 10.



‚ÄúThe solution to curing diseases isn‚Äôt to get rid of the organizations that are there to help us cure diseases,‚Äù adds O‚ÄôConnor, who is currently co-CEO at Cyclarity Therapeutics, a company developing drugs for atherosclerosis and other age-related diseases.&nbsp;



But it‚Äôs still just too soon to confidently predict how, if at all, O‚ÄôNeill will shape the government health agencies he will oversee.&nbsp;



‚ÄúWe don‚Äôt know exactly what he‚Äôs going to be doing as the deputy secretary of HHS,‚Äù says Public Citizen‚Äôs Steinbrook. ‚ÄúLike everybody who&#8217;s sworn into a government job, whether we disagree or agree with their views or actions ‚Ä¶ we still wish them well. And we hope that they do a good job.‚Äù
‚Ä¢ The Download: how to clean up AI data centers, and weight-loss drugs‚Äô side effects
  This is today&#8217;s edition of¬†The Download,¬†our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



This battery recycling company is now cleaning up AI data centers



In a sandy industrial lot outside Reno, Nevada, rows of battery packs that once propelled electric vehicles are now powering a small AI data center.Redwood Materials, one of the US‚Äôs largest battery recycling companies, showed off this array of energy storage modules, sitting on cinder blocks and wrapped in waterproof plastic, during a press tour at its headquarters on June 26.The event marked the launch of the company&#8217;s new business line, Redwood Energy, which will initially repurpose (rather than recycle) batteries with years of remaining life to create renewable-powered microgrids. Such small-scale energy systems can operate on or off the larger electricity grid, providing electricity for businesses or communities. Read the full story.‚ÄîJames Temple







We‚Äôre learning more about what weight-loss drugs do to the body



Weight-loss drugs are this decade‚Äôs blockbuster medicines. Drugs like Ozempic, Wegovy, and Mounjaro help people with diabetes get their blood sugar under control and help overweight and obese people reach a healthier weight. And they‚Äôre fast becoming a trendy must-have for celebrities and other figure-conscious individuals looking to trim down.



They became so hugely popular so quickly that not long after their approval for weight loss, we saw global shortages of the drugs. Prescriptions have soared over the last five years, but even people who don‚Äôt have prescriptions are seeking these drugs out online.We know they can suppress appetite, lower blood sugar, and lead to dramatic weight loss. We also know that they come with side effects, which can include nausea, diarrhea, and vomiting. But we are still learning about some of their other effects. Read the full story.



‚ÄîJessica Hamzelou



This article first appeared in The Checkup, MIT Technology Review‚Äôs weekly biotech newsletter. To receive it in your inbox every Thursday, and read articles like this first, sign up here.







The must-reads



I‚Äôve combed the internet to find you today‚Äôs most fun/important/scary/fascinating stories about technology.



1 The Supreme Court has paved the way to defund Planned Parenthood¬†By allowing South Carolina to block the organization from its Medicaid program. (WP $)+ Other red states are likely to follow suit. (CNN)+ Planned Parenthood may be able to challenge the ban under state law. (Politico)



2 Iran is back onlineThe country appeared to cut connectivity in a bid to thwart foreign attacks. (Economist $)



3 ICE is using a new facial recognition appIt‚Äôs capable of recognizing someone from their fingerprints, too. (404 Media)+ How a new type of AI is helping police skirt facial recognition bans. (MIT Technology Review)



4 Denmark has a potential solution for malicious deepfakesBy giving its residents copyright to their own body, facial features, and voice. (The Guardian)+ An AI startup made a hyperrealistic deepfake of me that‚Äôs so good it‚Äôs scary. (MIT Technology Review)



5 Impossible Foods wants to bring its plant-based burgers to Europe After sales started falling in America. (Bloomberg $)+ Sales of regular old meat are booming in the States. (Vox)



6 The Three Mile Island nuclear plant‚Äôs restart is being fast trackedIt‚Äôs currently scheduled to start operating a year earlier than anticipated. (Reuters)+ But bringing the reactor back online is no easy task. (The Register)+ Why Microsoft made a deal to help restart Three Mile Island. (MIT Technology Review)



7 AI may be making research too easyNew research suggests that using LLMs results in weaker grasps of topics. (WSJ $)+ It could also be making our thoughts less original. (New Yorker $)



8 Climate tech companies are struggling to weather Trump‚Äôs cutsA lot of startups are expected to fold as a result. (Insider $)+ The Trump administration has shut down more than 100 climate studies. (MIT Technology Review)



9 Billions of Facebook and Google passwords have been leakedAnd people in developing nations are most at risk. (Rest of World)



10 Inside a couples retreat with humans and their AI companionsChaos ensured. (Wired $)+ The AI relationship revolution is already here. (MIT Technology Review)







Quote of the day



‚Äú[The internet blackout] makes us invisible. And still, we‚Äôre here. Still trying to connect with the free world.‚Äù



‚Äî‚ÄôAmir,‚Äô a student in Iran, tells the Guardian why young Iranians are working to overcome the country‚Äôs internet shutdowns.







One more thing







Maybe you will be able to live past 122How long can humans live? This is a good time to ask the question. The longevity scene is having a moment, thanks to a combination of scientific advances, public interest, and an unprecedented level of investment. A few key areas of research suggest that we might be able to push human life spans further, and potentially reverse at least some signs of aging.Researchers can‚Äôt even agree on what the exact mechanisms of aging are and which they should be targeting. Debates continue to rage over how long it‚Äôs possible for humans to live‚Äîand whether there is a limit at all.But it looks likely that something will be developed in the coming decades that will help us live longer, in better health. Read the full story.



‚ÄîJessica Hamzelou







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)



+ This ancient amphibian skull is pretty remarkable.+ A new Phantom of the Opera spin-off is coming‚Äîbut no one really knows what it is.+ Stop panicking, it turns out Marge Simpson isn‚Äôt dead after all.+ I love these owls in towels
‚Ä¢ Capital One builds agentic AI modeled after its own org chart to supercharge auto sales
  Capital One's head of AI foundations explained how the bank patterned its AI agents after
‚Ä¢ Enterprise giants Atlassian, Intuit, and AWS are planning for a world where agents call the APIs
  In the Women in AI breakfast, technologists from Atlassian, Intuit and AWS
‚Ä¢ From 30 days to 1: Chevron‚Äôs cloud migration ROI in real numbers
  Chevron's agentic architectures must be able to process petabytes of data in the cloud
‚Ä¢ Kayak and Expedia race to build AI travel agents that turn social posts into itineraries
  Kayak and Expedia reimagine the travel agent as an AI agent . Kayak
‚Ä¢ From chatbots to collaborators: How AI agents are reshaping enterprise work
  Anthropic's Scott White explains how AI agents evolved from chatbots to autonomous workers .
‚Ä¢ Between utopia and collapse: Navigating AI‚Äôs murky middle future
  AI is disrupting the world, but it also presents an opportunity to ask what we are
‚Ä¢ Identity theft hits 1.1M reports ‚Äî and authentication fatigue is only getting worse
  The tug-of-war between friction and freedom will be won by those who can
‚Ä¢ From hallucinations to hardware: Lessons from a real-world computer vision project gone sideways
  What we tried, what didn't work and how a combination of approaches helped us build
‚Ä¢ AI agents are hitting a liability wall. Mixus has a plan to overcome it using human overseers on high-risk workflows
  Mixus's "colleague-in-the-loop" model blends automation with
‚Ä¢ CTGT wins Best Presentation Style award at VB Transform 2025
  CTGT won the Best Presentation Style award at VB Transform 2025 in San Francisco . Founded by 23
‚Ä¢ Catio wins ‚Äòcoolest tech‚Äô award at VB Transform 2025
  Catio also announced the upcoming launch of Archie, a conversational, multi-agent
‚Ä¢ Retail resurrection: David‚Äôs Bridal bets its future on AI after double bankruptcy
  How AI-driven personalization, knowledge graphs and a two-sided marketplace are creating
‚Ä¢ How runtime attacks turn profitable AI into budget black holes
  AI inference attacks drain enterprise budgets, derail regulatory compliance and destroy new AI deployment ROI
‚Ä¢ Model minimalism: The new AI strategy saving companies millions
  LLMs changed how enterprises build applications, but smaller AI models provide power and reduces cost
‚Ä¢ The inference trap: How cloud providers are eating your AI margins
  If you‚Äôre unsure about the load of different AI workloads, start with
‚Ä¢ The rise of prompt ops: Tackling hidden AI costs from bad inputs and context bloat
  Prompt ops can help manage, measure, monitor and tune prompts . AI models can get
‚Ä¢ Scaling smarter: How enterprise IT teams can right-size their compute for AI
  IT and business leaders plan and choose infrastructure can keep them from being doomed to pilot pur
‚Ä¢ Why your enterprise AI strategy needs both open and closed models: The TCO reality check
  Learn how enterprises evaluate open versus closed AI models to optimize costs, security, and performance
‚Ä¢ CFOs want AI that pays: real metrics, not marketing demos
  CFOs who master new eval frameworks will drive the next wave of AI adoption .
‚Ä¢ From pilot to profit: The real path to scalable, ROI-positive AI
  Organizations that delay systematic AI deployment risk being left behind by competitors who have already mastered the
‚Ä¢ Kumo‚Äôs ‚Äòrelational foundation model‚Äô predicts the future your LLM can‚Äôt see
  Forecasting is a fundamentally new capability that is missing from the current purview of gener
‚Ä¢ Can AI run a physical shop? Anthropic‚Äôs Claude tried and the results were gloriously, hilariously bad
  Anthropic's Claude ran a vending machine business for a month selling tungsten cubes
‚Ä¢ OpenAI‚Äôs API lead explains how enterprises are already succeeding with its Agents SDK and Responses API
  OpenAI now includes tracing and eval tools with the API stack to help teams define what
‚Ä¢ How Highmark Health and Google Cloud are using Gen AI to streamline medical claims and improve care: 6 key lessons
  Success in generative AI isn't reserved for those with the biggest budgets, but for

üîí Cybersecurity & Privacy
‚Ä¢ Senator Chides FBI for Weak Advice on Mobile Security
  Agents with the Federal Bureau of Investigation (FBI) briefed Capitol Hill staff recently on hardening the security of their mobile devices, after a contacts list stolen from the personal phone of the White House Chief of Staff Susie Wiles was reportedly used to fuel a series of text messages and phone calls impersonating her to U.S. lawmakers. But in a letter this week to the FBI, one of the Senate&#8217;s most tech-savvy lawmakers says the feds aren&#8217;t doing enough to recommend more appropriate security protections that are already built into most consumer mobile devices.
A screenshot of the first page from Sen. Wyden&#8217;s letter to FBI Director Kash Patel.
On May 29, The Wall Street Journal reported that federal authorities were investigating a clandestine effort to impersonate Ms. Wiles via text messages and in phone calls that may have used AI to spoof her voice. According to The Journal, Wiles told associates her cellphone contacts were hacked, giving the impersonator access to the private phone numbers of some of the country&#8217;s most influential people.
The execution of this phishing and impersonation campaign &#8212; whatever its goals may have been &#8212; suggested the attackers were financially motivated, and not particularly sophisticated.
&#8220;It became clear to some of the lawmakers that the requests were suspicious when the impersonator began asking questions about Trump that Wiles should have known the answers to‚Äîand in one case, when the impersonator asked for a cash transfer, some of the people said,&#8221; the Journal wrote. &#8220;In many cases, the impersonator‚Äôs grammar was broken and the messages were more formal than the way Wiles typically communicates, people who have received the messages said. The calls and text messages also didn‚Äôt come from Wiles‚Äôs phone number.&#8221;
Sophisticated or not, the impersonation campaign was soon punctuated by the murder of Minnesota House of Representatives Speaker Emerita Melissa Hortman and her husband, and the shooting of Minnesota State Senator John Hoffman and his wife. So when FBI agents offered in mid-June to brief U.S. Senate staff on mobile threats, more than 140 staffers took them up on that invitation (a remarkably high number considering that no food was offered at the event).
But according to Sen. Ron Wyden (D-Ore.), the advice the FBI provided to Senate staffers was largely limited to remedial tips, such as not clicking on suspicious links or attachments, not using public wifi networks, turning off bluetooth, keeping phone software up to date, and rebooting regularly.
&#8220;This is insufficient to protect Senate employees and other high-value targets against foreign spies using advanced cyber tools,&#8221; Wyden wrote in a letter sent today to FBI Director Kash Patel. &#8220;Well-funded foreign intelligence agencies do not have to rely on phishing messages and malicious attachments to infect unsuspecting victims with spyware. Cyber mercenary companies sell their government customers advanced &#8216;zero-click&#8217; capabilities to deliver spyware that do not require any action by the victim.&#8221;
Wyden stressed that to help counter sophisticated attacks, the FBI should be encouraging lawmakers and their staff to enable anti-spyware defenses that are built into Apple&#8217;s iOS and Google&#8217;s Android phone software.
These include Apple&#8217;s Lockdown Mode, which is designed for users who are worried they may be subject to targeted attacks. Lockdown Mode restricts non-essential iOS features to reduce the device&#8217;s overall attack surface. Google Android devices carry a similar feature called Advanced Protection Mode.
Wyden also urged the FBI to update its training to recommend a number of other steps that people can take to make their mobile devices less trackable, including the use of ad blockers to guard against malicious advertisements, disabling ad tracking IDs in mobile devices, and opting out of commercial data brokers (the suspect charged in the Minnesota shootings reportedly used multiple people-search services to find the home addresses of his targets).
The senator&#8217;s letter notes that while the FBI has recommended all of the above precautions in various advisories issued over the years, the advice the agency is giving now to the nation&#8217;s leaders needs to be more comprehensive, actionable and urgent.
&#8220;In spite of the seriousness of the threat, the FBI has yet to provide effective defensive guidance,&#8221; Wyden said.
Nicholas Weaver is a researcher with the International Computer Science Institute, a nonprofit in Berkeley, Calif. Weaver said Lockdown Mode or Advanced Protection will mitigate many vulnerabilities, and should be the default setting for all members of Congress and their staff.
&#8220;Lawmakers are at exceptional risk and need to be exceptionally protected,&#8221; Weaver said. &#8220;Their computers should be locked down and well administered, etc. And the same applies to staffers.&#8221;
Weaver noted that Apple&#8217;s Lockdown Mode has a track record of blocking zero-day attacks on iOS applications; in September 2023, Citizen Lab documented how Lockdown Mode foiled a zero-click flaw capable of installing spyware on iOS devices without any interaction from the victim.

Earlier this month, Citizen Lab researchers documented a zero-click attack used to infect the iOS devices of two journalists with Paragon&#8217;s Graphite spyware. The vulnerability could be exploited merely by sending the target a booby-trapped media file delivered via iMessage. Apple also recently updated its advisory for the zero-click flaw (CVE-2025-43200), noting that it was mitigated as of iOS 18.3.1, which was released in February 2025.
Apple has not commented on whether CVE-2025-43200 could be exploited on devices with Lockdown Mode turned on. But HelpNetSecurity observed that at the same time Apple addressed CVE-2025-43200 back in February, the company fixed another vulnerability flagged by Citizen Lab researcher Bill Marczak: CVE-2025-24200, which Apple said was used in an extremely sophisticated physical attack against specific targeted individuals that allowed attackers to disable USB Restricted Mode on a locked device.
In other words, the flaw could apparently be exploited only if the attacker had physical access to the targeted vulnerable device. And as the old infosec industry adage goes, if an adversary has physical access to your device, it&#8217;s most likely not your device anymore.
I can&#8217;t speak to Google&#8217;s Advanced Protection Mode personally, because I don&#8217;t use Google or Android devices. But I have had Apple&#8217;s Lockdown Mode enabled on all of my Apple devices since it was first made available in September 2022. I can only think of a single occasion when one of my apps failed to work properly with Lockdown Mode turned on, and in that case I was able to add a temporary exception for that app in Lockdown Mode&#8217;s settings.
My main gripe with Lockdown Mode was captured in a March 2025 column by TechCrunch&#8217;s Lorenzo Francheschi-Bicchierai, who wrote about its penchant for periodically sending mystifying notifications that someone has been blocked from contacting you, even though nothing then prevents you from contacting that person directly. This has happened to me at least twice, and in both cases the person in question was already an approved contact, and said they had not attempted to reach out.
Although it would be nice if Apple&#8217;s Lockdown Mode sent fewer, less alarming and more informative alerts, the occasional baffling warning message is hardly enough to make me turn it off.

üéì University AI
No updates.

üè¢ Corporate AI
‚Ä¢ AI Testing and Evaluation: Learnings from genome editing
  Generative AI presents a unique challenge and opportunity to reexamine governance practices for the responsible development, deployment, and use of AI. To advance thinking in this space, Microsoft has tapped into the experience and knowledge of experts across domains‚Äîfrom genome editing to cybersecurity‚Äîto investigate the role of testing and evaluation as a governance tool. AI Testing and Evaluation: Learnings from Science and Industry, hosted by Microsoft Research‚Äôs Kathleen Sullivan, explores what the technology industry and policymakers can learn from these fields and how that might help shape the course of AI development.



In this episode, Alta Charo (opens in new tab), emerita professor of law and bioethics at the University of Wisconsin‚ÄìMadison, joins Sullivan for a conversation on the evolving landscape of genome editing and its regulatory implications. Drawing on decades of experience in biotechnology policy, Charo emphasizes the importance of distinguishing between hazards and risks and describes the field&#8217;s approach to regulating applications of technology rather than the technology itself. The discussion also explores opportunities and challenges in biotech‚Äôs multi-agency oversight model and the role of international coordination. Later, Daniel Kluttz (opens in new tab), a partner general manager in Microsoft&#8217;s Office of Responsible AI, joins Sullivan to discuss how insights from genome editing could inform more nuanced and robust governance frameworks for emerging technologies like AI.








Learn more:



Learning from other Domains to Advance AI Evaluation and Testing: Governance of Genome Edition in Human Therapeutics and Agricultural ApplicationsCase study | January 2025&nbsp;



Learning from other domains to advance AI evaluation and testing&nbsp;Microsoft Research Blog | June 2025&nbsp;



Responsible AI: Ethical policies and practices | Microsoft AI&nbsp;



AI and Microsoft Research&nbsp;









	
		
			Subscribe to the Microsoft Research Podcast:		
		
							
					
						  
						Apple Podcasts
					
				
			
							
					
						
						Email
					
				
			
							
					
						
						Android
					
				
			
							
					
						
						Spotify
					
				
			
							
					
						
						RSS Feed
					
				
					
	




	
		
			
				
					

Transcript



[MUSIC]



KATHLEEN SULLIVAN: Welcome to AI Testing and Evaluation: Learnings from Science and Industry. I&#8217;m your host, Kathleen Sullivan.



As generative AI continues to advance, Microsoft has gathered a range of experts‚Äîfrom genome editing to cybersecurity‚Äîto share how their fields approach evaluation and risk assessment. Our goal is to learn from their successes and their stumbles to move the science and practice of AI testing forward. In this series, we&#8217;ll explore how these insights might help guide the future of AI development, deployment, and responsible use.



[MUSIC ENDS]



Today I&#8217;m excited to welcome R. Alta Charo, the Warren P. Knowles Professor Emerita of Law and Bioethics at the University of Wisconsin‚ÄìMadison, to explore testing and risk assessment in genome editing.



Professor Charo has been at the forefront of biotechnology policy and governance for decades, advising former President Obama&#8217;s transition team on issues of medical research and public health, as well as serving as a senior policy advisor at the Food and Drug Administration. She consults on gene therapy and genome editing for various companies and organizations and has held positions on a number of advisory committees, including for the National Academy of Sciences. Her committee work has spanned women&#8217;s health, stem cell research, genome editing, biosecurity, and more.



After our conversation with Professor Charo, we&#8217;ll hear from Daniel Kluttz, a partner general manager in Microsoft&#8217;s Office of Responsible AI, about what these insights from biotech regulation could mean for AI governance and risk assessment and his team&#8217;s work governing sensitive AI uses and emerging technologies.



Alta, thank you so much for being here today. I&#8217;m a follower of your work and have really been looking forward to our conversation.



				
				
					



ALTA CHARO: It&#8217;s my pleasure. Thanks for having me.



SULLIVAN: Alta, I&#8217;d love to begin by stepping back in time a bit before you became a leading figure in bioethics and legal policy. You&#8217;ve shared that your interest in science was really inspired by your brothers‚Äô interest in the topic and that your upbringing really helped shape your perseverance and resilience. Can you talk to us about what put you on the path to law and policy?



CHARO: Well, I think it&#8217;s true that many of us are strongly influenced by our families and certainly my family had, kind of, a science-y, techy orientation. My father was a refugee, you know, escaping the Nazis, and when he finally was able to start working in the United States, he took advantage of the G.I. Bill to learn how to repair televisions and radios, which were really just coming in in the 1950s. So he was, kind of, technically oriented.



My mother retrained from being a talented amateur artist to becoming a math teacher, and not surprisingly, both my brothers began to aim toward things like engineering and chemistry and physics. And our form of entertainment was to watch PBS or Star Trek. [LAUGHTER]



And so the interest comes from that background coupled with, in the 1960s, this enormous surge of interest in the so-called nature-versus-nurture debate about the degree to which we are destined by our biology or shaped by our environments. It was a heady debate, and one that perfectly combined the two interests in politics and science.



SULLIVAN: For listeners who are brand new to your field in genomic editing, can you give us what I&#8217;ll call a ‚Äú90-second survey‚Äù of the space in perhaps plain language and why it&#8217;s important to have a framework for ensuring its responsible use.



CHARO: Well, you know, genome editing is both very old and very new. At base, what we&#8217;re talking about is a way to either delete sections of the genome, our collection of genes, or to add things or to alter what&#8217;s there. The goal is simply to be able to take what might not be healthy and make it healthy, whether it&#8217;s a plant, an animal, or a human.



Many people have compared it to a word processor, where you can edit text by swapping things in and out. You could change the letter g to the letter h in every word, and in our genomes, you can do similar kinds of things.



But because of this, we have a responsibility to make sure that whatever we change doesn&#8217;t become dangerous and that it doesn&#8217;t become socially disruptive. Now the earliest forms of genome editing were very inefficient, and so we didn&#8217;t worry that much. But with the advances that were spearheaded by people like Jennifer Doudna and Emmanuelle Charpentier, who won the Nobel Prize for their work in this area, genome editing has become much easier to do.



It&#8217;s become more efficient. It doesn&#8217;t require as much sophisticated laboratory equipment. It&#8217;s moved from being something that only a few people can do to something that we&#8217;re going to be seeing in our junior high school biology labs. And that means you have to pay attention to who&#8217;s doing it, why are they doing it, what are they releasing, if anything, into the environment, what are they trying to sell, and is it honest and is it safe?



SULLIVAN: How would you describe the risks, and are there, you know, sort of, specifically inherent risks in the technology itself, or do those risks really emerge only when it&#8217;s applied in certain contexts, like CRISPR in agriculture or CRISPR for human therapies?



CHARO: Well, to answer that, I&#8217;m going to do something that may seem a little picky, even pedantic. [LAUGHTER] But I&#8217;m going to distinguish between hazards and risks. So there are certain intrinsic hazards. That is, there are things that can go wrong.



You want to change one particular gene or one particular portion of a gene, and you might accidentally change something else, a so-called off-target effect. Or you might change something in a gene expecting a certain effect but not necessarily anticipating that there&#8217;s going to be an interaction between what you changed and what was there, a gene-gene interaction, that might have an unanticipated kind of result, a side effect essentially.



So there are some intrinsic hazards, but risk is a hazard coupled with the probability that it&#8217;s going to actually create something harmful. And that really depends upon the application.



If you are doing something that is making a change in a human being that is going to be a lifelong change, that enhances the significance of that hazard. It amplifies what I call the risk because if something goes wrong, then its consequences are greater.



It may also be that in other settings, what you&#8217;re doing is going to have a much lower risk because you&#8217;re working with a more familiar substance, your predictive power is much greater, and it&#8217;s not going into a human or an animal or into the environment. So I think that you have to say that the risk and the benefits, by the way, all are going to depend upon the particular application.



SULLIVAN: Yeah, I think on this point of application, there&#8217;s many players involved in that, right. Like, we often hear about this puzzle of who&#8217;s actually responsible for ensuring safety and a reasonable balance between risks and benefits or hazards and benefits, to quote you. Is it the scientists, the biotech companies, government agencies? And then if you could touch upon, as well, maybe how does the nature of genome editing risks ‚Ä¶ how do those responsibilities get divvied up?



CHARO: Well, in the 1980s, we had a very significant policy discussion about whether we should regulate the technology‚Äîno matter how it&#8217;s used or for whatever purpose‚Äîor if we should simply fold the technology in with all the other technologies that we currently have and regulate its applications the way we regulate applications generally. And we went for the second, the so-called coordinated framework.



So what we have in the United States is a system in which if you use genome editing in purely laboratory-based work, then you will be regulated the way we regulate laboratories.



There&#8217;s also, at most universities because of the way the government works with this, something called Institutional Biosafety Committees, IBCs. You want to do research that involves recombinant DNA and modern biotechnology, including genome editing but not limited to it, you have to go first to your IBC, and they look and see what you&#8217;re doing to decide if there&#8217;s a danger there that you have not anticipated that requires special attention.



If what you&#8217;re doing is going to get released into the environment or it&#8217;s going to be used to change an animal that&#8217;s going to be in the environment, then there are agencies that oversee the safety of our environment, predominantly the Environmental Protection Agency and the U.S. Department of Agriculture.



If you&#8217;re working with humans and you&#8217;re doing medical therapies, like you&#8217;re doing the gene therapies that just have been developed for things like sickle cell anemia, then you have to go through a very elaborate regulatory process that&#8217;s overseen by the Food and Drug Administration and also seen locally at the research stages overseen by institutional review boards that make sure the people who are being recruited into research understand what they&#8217;re getting into, that they&#8217;re the right people to be recruited, etc.



So we do have this kind of Jenga game ‚Ä¶



SULLIVAN: [LAUGHS] Yeah, sounds like it.



CHARO: ‚Ä¶ of regulatory agencies. And on top of all that, most of this involves professionals who&#8217;ve had to be licensed in some way. There may be state laws specifically on licensing. If you are dealing with things that might cross national borders, there may be international treaties and agreements that cover this.



And, of course, the insurance industry plays a big part because they decide whether or not what you&#8217;re doing is safe enough to be insured. So all of these things come together in a way that is not at all easy to understand if you&#8217;re not, kind of, working in the field. But the bottom-line thing to remember, the way to really think about it is, we don&#8217;t regulate genome editing; we regulate the things that use genome editing.



SULLIVAN: Yeah, that makes a lot of sense. Actually, maybe just following up a little bit on this notion of a variety of different, particularly like government agencies being involved. You know, in this multi-stakeholder model, where do you see gaps today that need to be filled, some of the pros and cons to keep in mind, and, you know, just as we think about distributing these systems at a global level, like, what are some of the considerations you are keeping in mind on that front?



CHARO: Well, certainly there are times where the way the statutes were written that govern the regulation of drugs or the regulation of foods did not anticipate this tremendous capacity we now have in the area of biotechnology generally or genome editing in particular. And so you can find that there are times where it feels a little bit ambiguous, and the agencies have to figure out how to apply their existing rules.



So an example. If you&#8217;re going to make alterations in an animal, right, we have a system for regulating drugs, including veterinary drugs. But we didn&#8217;t have something that regulated genome editing of animals. But in a sense, genome editing of an animal is the same thing as using a veterinary drug. You&#8217;re trying to affect the animal&#8217;s physical constitution in some fashion.



And it took a long time within the FDA to, sort of, work out how the regulation of veterinary drugs would apply if you think about the genetic construct that&#8217;s being used to alter the animal as the same thing as injecting a chemically based drug. And on that basis, they now know here&#8217;s the regulatory path‚Äîhere are the tests you have to do; here are the permissions you have to do; here&#8217;s the surveillance you have to do after it goes on the market.



Even there, sometimes, it was confusing. What happens when it&#8217;s not the kind of animal you&#8217;re thinking about when you think about animal drugs? Like, we think about pigs and dogs, but what about mosquitoes?



Because there, you&#8217;re really thinking more about pests, and if you&#8217;re editing the mosquito so that it can&#8217;t, for example, transmit dengue fever, right, it feels more like a public health thing than it is a drug for the mosquito itself, and it, kind of, fell in between the agencies that possibly had jurisdiction. And it took a while for the USDA, the Department of Agriculture, and the Food and Drug Administration to work out an agreement about how they would share this responsibility. So you do get those kinds of areas in which you have at least ambiguity.



We also have situations where frankly the fact that some things can move across national borders means you have to have a system for harmonizing or coordinating national rules. If you want to, for example, genetically engineer mosquitoes that can&#8217;t transmit dengue, mosquitoes have a tendency to fly. [LAUGHTER] And so &#8230; they can&#8217;t fly very far. That&#8217;s good. That actually makes it easier to control.



But if you&#8217;re doing work that&#8217;s right near a border, then you have to be sure that the country next to you has the same rules for whether it&#8217;s permitted to do this and how to surveil what you&#8217;ve done in order to be sure that you got the results you wanted to get and no other results. And that also is an area where we have a lot of work to be done in terms of coordinating across government borders and harmonizing our rules.



SULLIVAN: Yeah, I mean, you&#8217;ve touched on this a little bit, but there is such this striking balance between advancing technology, ensuring public safety, and sometimes, I think it feels just like you&#8217;re walking a tightrope where, you know, if we clamp down too hard, we&#8217;ll stifle innovation, and if we&#8217;re too lax, we risk some of these unintended consequences. And on a global scale like you just mentioned, as well. How has the field of genome editing found its balance?



CHARO: It&#8217;s still being worked out, frankly, but it&#8217;s finding its balance application by application. So in the United States, we have two very different approaches on regulation of things that are going to go into the market.



Some things can&#8217;t be marketed until they&#8217;ve gotten an approval from the government. So you come up with a new drug, you can&#8217;t sell that until it&#8217;s gone through FDA approval.



On the other hand, for most foods that are made up of familiar kinds of things, you can go on the market, and it&#8217;s only after they&#8217;re on the market that the FDA can act to withdraw it if a problem arises. So basically, we have either pre-market controls: you can&#8217;t go on without permission. Or post-market controls: we can take you off the market if a problem occurs.



How do we decide which one is appropriate for a particular application? It&#8217;s based on our experience. New drugs typically are both less familiar than existing things on the market and also have a higher potential for injury if they, in fact, are not effective or they are, in fact, dangerous and toxic.



If you have foods, even bioengineered foods, that are basically the same as foods that are already here, it can go on the market with notice but without a prior approval. But if you create something truly novel, then it has to go through a whole long process.



And so that is the way that we make this balance. We look at the application area. And we&#8217;re just now seeing in the Department of Agriculture a new approach on some of the animal editing, again, to try and distinguish between things that are simply a more efficient way to make a familiar kind of animal variant and those things that are genuinely novel and to have a regulatory process that is more rigid the more unfamiliar it is and the more that we see a risk associated with it.



SULLIVAN: I know we&#8217;re at the end of our time here and maybe just a quick kind of lightning-round of a question. For students, young scientists, lawyers, or maybe even entrepreneurs listening who are inspired by your work, what&#8217;s the single piece of advice you give them if they&#8217;re interested in policy, regulation, the ethical side of things in genomics or other fields?



CHARO: I&#8217;d say be a bio-optimist and read a lot of science fiction. Because it expands your imagination about what the world could be like. Is it going to be a world in which we&#8217;re now going to be growing our buildings instead of building them out of concrete?



Is it going to be a world in which our plants will glow in the evening so we don&#8217;t need to be using batteries or electrical power from other sources but instead our environment is adapting to our needs?



You know, expand your imagination with a sense of optimism about what could be and see ethics and regulation not as an obstacle but as a partner to bringing these things to fruition in a way that&#8217;s responsible and helpful to everyone.



[TRANSITION MUSIC]



SULLIVAN: Wonderful. Well, Alta, this has been just an absolute pleasure. So thank you.



CHARO: It was my pleasure. Thank you for having me.



SULLIVAN: Now, I&#8217;m happy to bring in Daniel Kluttz. As a partner general manager in Microsoft&#8217;s Office of Responsible AI, Daniel leads the group‚Äôs Sensitive Uses and Emerging Technologies program.



Daniel, it&#8217;s great to have you here. Thanks for coming in.



DANIEL KLUTTZ: It&#8217;s great to be here, Kathleen.



SULLIVAN: Yeah. So maybe before we unpack Alta Charo‚Äôs insights, I&#8217;d love to just understand the elevator pitch here. What exactly is [the] Sensitive Uses and Emerging Tech program, and what was the impetus for establishing it?



KLUTTZ: Yeah. So the Sensitive Uses and Emerging Technologies program sits within our Office of Responsible AI at Microsoft. And inherent in the name, there are two real core functions. There&#8217;s the sensitive uses and emerging technologies. What does that mean?



Sensitive uses, think of that as Microsoft&#8217;s internal consulting and oversight function for our higher-risk, most impactful AI system deployments. And so my team is a team of multidisciplinary experts who engages in sort of a white-glove-treatment sort of way with product teams at Microsoft that are designing, building, and deploying these higher-risk AI systems, and where that sort of consulting journey culminates is in a set of bespoke requirements tailored to the use case of that given system that really implement and apply our more standardized, generalized requirements that apply across the board.



Then the emerging technologies function of my team faces a little bit further out, trying to look around corners to see what new and novel and emerging risks are coming out of new AI technologies with the idea that we work with our researchers, our engineering partners, and, of course, product leaders across the company to understand where Microsoft is going with those emerging technologies, and we&#8217;re developing sort of rapid, quick-fire early-steer guidance that implements our policies ahead of that formal internal policymaking process, which can take a bit of time. So it&#8217;s designed to, sort of, both afford that innovation speed that we like to optimize for at Microsoft but also integrate our responsible AI commitments and our AI principles into emerging product development.



SULLIVAN: That segues really nicely, actually, as we met with Professor Charo and she was, you know, talking about the field of genome editing and the governing at the application level. I&#8217;d love to just understand how similar or not is that to managing the risks of AI in our world?



KLUTTZ: Yeah. I mean, Professor Charo‚Äôs comments were music to my ears because, you know, where we make our bread and butter, so to speak, in our team is in applying to use cases. AI systems, especially in this era of generative AI, are almost inherently multi-use, dual use. And so what really matters is how you&#8217;re going to apply that more general-purpose technology. Who&#8217;s going to use it? In what domain is it going to be deployed? And then tailor that oversight to those use cases. Try to be risk proportionate.



Professor Charo talked a little bit about this, but if it&#8217;s something that&#8217;s been done before and it&#8217;s just a new spin on an old thing, maybe we&#8217;re not so concerned about how closely we need to oversee and gate that application of that technology, whereas if it&#8217;s something new and novel or some new risk that might be posed by that technology, we take a little bit closer look and we are overseeing that in a more sort of high-touch way.



SULLIVAN: Maybe following up on that, I mean, how do you define sensitive use or maybe like high-impact application, and once that&#8217;s labeled, what happens? Like, what kind of steps kick in from there?



KLUTTZ: Yeah. So we have this Sensitive Uses program that&#8217;s been at Microsoft since 2019. I came to Microsoft in 2019 when we were starting this program in the Office of Responsible AI, and it had actually been incubated in Microsoft Research with our Aether community of colleagues who are experts in sociotechnical approaches to responsible AI, as well. Once we put it in the Office of Responsible AI, I came over. I came from academia. I was a researcher myself ‚Ä¶



SULLIVAN: At Berkeley, right?



KLUTTZ: At Berkeley. That&#8217;s right. Yep. Sociologist by training and a lawyer in a past life. [LAUGHTER] But that has helped sort of bridge those fields for me.



But Sensitive Uses, we force all of our teams when they&#8217;re envisioning their system design to think about, could the reasonably foreseeable use or misuse of the system that they&#8217;re developing in practice result in three really major, sort of, risk types. One is, could that deployment result in a consequential impact on someone&#8217;s legal position or life opportunity? Another category we have is, could that foreseeable use or misuse result in significant psychological or physical injury or harm? And then the third really ties in with a longstanding commitment we&#8217;ve had to human rights at Microsoft. And so could that system in it&#8217;s reasonably foreseeable use or misuse result in human rights impacts and injurious consequences to folks along different dimensions of human rights? 



Once you decide, we have a process to reporting that project into my office, and we will triage that project, working with the product team, for example, and our Responsible AI Champs community, which are folks who are dispersed throughout the ecosystem at Microsoft and educated in our responsible AI program, and then determine, OK, is it in scope for our program? If it is, say, OK, we&#8217;re going to go along for that ride with you, and then we get into that whole sort of consulting arrangement that then culminates in this set of bespoke use-case-based requirements applying our AI principles.



SULLIVAN: That&#8217;s super fascinating. What are some of the approaches in the governance of genome editing are you maybe seeing happening in AI governance or maybe just, like, bubbling up in conversations around it?



KLUTTZ: Yeah, I mean, I think we&#8217;ve learned a lot from fields like genome editing that Professor Charo talked about and others. And again, it gets back to this, sort of, risk-proportionate-based approach. It&#8217;s a balancing test. It&#8217;s a tradeoff of trying to, sort of, foster innovation and really look for the beneficial uses of these technologies. I appreciated her speaking about that. What are the intended uses of the system, right? And then getting to, OK, how do we balance trying to, again, foster that innovation in a very fast-moving space, a pretty complex space, and a very unsettled space contrasting to other, sort of, professional fields or technological fields that have a long history and are relatively settled from an oversight and regulatory standpoint? This one is not, and for good reason. It is still developing.



And I think, you know, there are certain oversight and policy regimes that exist today that can be applied. Professor Charo talked about this, as well, where, you know, maybe you have certain policy and oversight regimes that, depending on how the application of that technology is applied, applies there versus some horizontal, overarching regulatory sort of framework. And I think that applies from an internal governance standpoint, as well.



SULLIVAN: Yeah. It&#8217;s a great point. So what isn&#8217;t being explored from genome editing that, you know, maybe we think could be useful to AI governance, or as we think about the evolving frameworks ‚Ä¶



KLUTTZ: Yeah.



SULLIVAN: ‚Ä¶ what maybe we should be taking into account from what Professor Charo shared with us?



KLUTTZ: So one of the things I&#8217;ve thought about and took from Professor Charo‚Äôs discussion was she had just this amazing way of framing up how genome editing regulation is done. And she said, you know, we don&#8217;t regulate genome editing; we regulate the things that use genome editing. And while it&#8217;s not a one-to-one analogy with the AI space because we do have this sort of very general model level distinction versus application layer and even platform layer distinctions, I think it&#8217;s fair to say, you know, we don&#8217;t regulate AI applications writ large. We regulate the things that use AI in a very similar way. And that&#8217;s how we think of our internal policy and oversight process at Microsoft, as well.



And maybe there are things that we regulated and oversaw internally at the first instance and the first time we saw it come through, and it graduates into more of a programmatic framework for how we manage that. So one good example of that is some of our higher-risk AI systems that we offer out of Azure at the platform level. When I say that, I mean APIs that you call that developers can then build their own applications on top of. We were really deep in evaluating and assessing mitigations on those platform systems in the first instance, but we also graduated them into what we call our Limited Access AI services program.



And some of the things that Professor Charo discussed really resonated with me. You know, she had this moment where she was mentioning how, you know, you want to know who&#8217;s using your tools and how they&#8217;re being used. And it&#8217;s the same concepts. We want to have trust in our customers, we want to understand their use cases, and we want to apply technical controls that, sort of, force those use cases or give us signal post-deployment that use cases are being done in a way that may give us some level of concern, to reach out and understand what those use cases are.



SULLIVAN: Yeah, you&#8217;re hitting on a great point. And I love this kind of layered approach that we&#8217;re taking and that Alta highlighted, as well. Maybe to double-click a little bit just on that post-market control and what we&#8217;re tracking, kind of, once things are out and being used by our customers. How do we take some of that deployment data and bring it back in to maybe even better inform upfront governance or just how we think about some of the frameworks that we&#8217;re operating in?



KLUTTZ: It&#8217;s a great question. The number one thing is for us at Microsoft, we want to know the voice of our customer. We want our customers to talk to us. We don&#8217;t want to just understand telemetry and data. But it&#8217;s really getting out there and understanding from our customers and not just our customers. I would say our stakeholders is maybe a better term because that includes civil society organizations. It includes governments. It includes all of these non, sort of, customer actors that we care about and that we&#8217;re trying to sort of optimize for, as well. It includes end users of our enterprise customers. If we can gather data about how our products are being used and trying to understand maybe areas that we didn&#8217;t foresee how customers or users might be using those things, and then we can tune those systems to better align with what both customers and users want but also our own AI principles and policies and programs.



SULLIVAN: Daniel, before coming to Microsoft, you led social science research and sociotechnical applications of AI-driven tech at Berkeley. What do you think some of the biggest challenges are in defining and maybe even just, kind of, measuring at, like, a societal level some of the impacts of AI more broadly?



KLUTTZ: Measuring social phenomenon is a difficult thing. And one of the things that, as social scientists, you&#8217;re very interested in is scientifically observing and measuring social phenomena. Well, that sounds great. It sounds also very high level and jargony. What do we mean by that? You know, it&#8217;s very easy to say that you&#8217;re collecting data and you&#8217;re measuring, I don&#8217;t know, trust in AI, right? That&#8217;s a very fuzzy concept.



SULLIVAN: Right. Definitely.



KLUTTZ: It is a concept that we want to get to, but we have to unpack that, and we have to develop what we call measurable constructs. What are the things that we might observe that could give us an indication toward what is a very fuzzy and general concept. And there&#8217;s challenges with that everywhere. And I&#8217;m extremely fortunate to work at Microsoft with some of the world&#8217;s leading sociotechnical researchers and some of these folks who are thinking about‚Äîyou know, very steeped in measurement theory, literally PhDs in these fields‚Äîhow to both measure and allow for a scalable way to do that at a place the size of Microsoft. And that is trying to develop frameworks that are scalable and repeatable and put into our platform that then serves our product teams. Are we providing, as a platform, a service to those product teams that they can plug in and do their automated evaluations at scale as much as possible and then go back in over the top and do some of your more qualitative targeted testing and evaluations.



SULLIVAN: Yeah, makes a lot of sense. Before we close out, if you&#8217;re game for it, maybe we do a quick lightning round. Just 30-second answers here. Favorite real-world sensitive use case you&#8217;ve ever reviewed.



KLUTTZ: Oh gosh. Wow, this is where I get to be the social scientist.



SULLIVAN: [LAUGHS] Yes.



KLUTTZ: It‚Äôs like, define favorite, Kathleen. [LAUGHS] Most memorable, most painful.



SULLIVAN: Let&#8217;s do most memorable.



KLUTTZ: We‚Äôll do most memorable.



SULLIVAN: Yeah.



KLUTTZ: You know, I would say the most memorable project I worked on was when we rolled out the new Bing Chat, which is no longer called Bing Chat, because that was the first really big cross-company effort to deploy GPT-4, which was, you know, the next step up in AI innovation from our partners at OpenAI. And I really value working hand in hand with engineering teams and with researchers and that was us at our best and really sort of turbocharged the model that we have.



SULLIVAN: Wonderful. What&#8217;s one of the most overused phrases that you have in your AI governance meetings?



KLUTTZ: Gosh. [LAUGHS] If I hear ‚ÄúWe need to get aligned; we need to align on this more‚Äù ‚Ä¶



SULLIVAN: [LAUGHS] Right.



KLUTTZ: But, you know, it&#8217;s said for a reason. And I think it sort of speaks to that clever nature. That&#8217;s one that comes to mind.



SULLIVAN: That&#8217;s great. And then maybe, maybe last one. What are you most excited about in the next, I don&#8217;t know, let&#8217;s say three months? This world is moving so fast!



KLUTTZ: You know, the pace of innovation, as you just said, is just staggering. It is unbelievable. And sometimes it can feel overwhelming in my space. But what I am most excited about is how we are building up this Emerging ‚Ä¶ I mentioned this Emerging Technologies program in my team as a, sort of, formal program is relatively new. And I really enjoy being able to take a step back and think a little bit more about the future and a little bit more holistically. And I love working with engineering teams and sort of strategic visionaries who are thinking about what we&#8217;re doing a year from now or five years from now, or even 10 years from now, and I get to be a part of those conversations. And that really gives me energy and helps me ‚Ä¶ helps keep me grounded and not just dealing with the day to day, and, you know, various fire drills that you may run. It&#8217;s thinking strategically and having that foresight about what&#8217;s to come. And it&#8217;s exciting.



SULLIVAN: Great. Well, Daniel, just thanks so much for being here. I had such a wonderful discussion with you, and I think the thoughtfulness in our discussion today I hope resonates with our listeners. And again, thanks to Alta for setting the stage and sharing her really amazing, insightful thoughts here, as well. So thank you.



[MUSIC]



KLUTTZ: Thank you, Kathleen. I appreciate it. It&#8217;s been fun.



SULLIVAN: And to our listeners, thanks for tuning in. You can find resources related to this podcast in the show notes. And if you want to learn more about how Microsoft approaches AI governance, you can visit microsoft.com/RAI.



See you next time!&nbsp;



[MUSIC FADES]

				
			
			
				Show more			
		
	

Opens in a new tabThe post AI Testing and Evaluation: Learnings from genome editing appeared first on Microsoft Research.
‚Ä¢ Use Amazon SageMaker Unified Studio to build complex AI workflows using Amazon Bedrock Flows
  Organizations face the challenge to manage data, multiple artificial intelligence and machine learning (AI/ML) tools, and workflows across different environments, impacting productivity and governance. A unified development environment consolidates data processing, model development, and AI application deployment into a single system. This integration streamlines workflows, enhances collaboration, and accelerates AI solution development from concept to production. 
The next generation of Amazon SageMaker is the center for your data, analytics, and AI. SageMaker brings together AWS AI/ML and analytics capabilities and delivers an integrated experience for analytics and AI with unified access to data. Amazon SageMaker Unified Studio is a single data and AI development environment where you can find and access your data and act on it using AWS analytics and AI/ML services, for SQL analytics, data processing, model development, and generative AI application development. 
With SageMaker Unified Studio, you can efficiently build generative AI applications in a trusted and secure environment using Amazon Bedrock. You can choose from a selection of high-performing foundation models (FMs) and advanced customization and tooling such as Amazon Bedrock Knowledge Bases, Amazon Bedrock Guardrails, Amazon Bedrock Agents, and Amazon Bedrock Flows. You can rapidly tailor and deploy generative AI applications, and share with the built-in catalog for discovery. 
In this post, we demonstrate how you can use SageMaker Unified Studio to create complex AI workflows using Amazon Bedrock Flows. 
Solution overview 
Consider FinAssist Corp, a leading financial institution developing a generative AI-powered agent support application. The solution offers the following key features: 
 
 Complaint reference system ‚Äì An AI-powered system providing quick access to historical complaint data, enabling customer service representatives to efficiently handle customer follow-ups, support internal audits, and aid in training new staff. 
 Intelligent knowledge base ‚Äì A comprehensive data source of resolved complaints that quickly retrieves relevant complaint details, resolution actions, and outcome summaries. 
 Streamlined workflow management ‚Äì Enhanced consistency in customer communications through standardized access to past case information, supporting compliance checks and process improvement initiatives. 
 Flexible query capability ‚Äì A straightforward interface supporting various query scenarios, from customer inquiries about past resolutions to internal reviews of complaint handling procedures. 
 
Let‚Äôs explore how SageMaker Unified Studio and Amazon Bedrock Flows, integrated with Amazon Bedrock Knowledge Bases and Amazon Bedrock Agents, address these challenges by creating an AI-powered complaint reference system. The following diagram illustrates the solution architecture. 
 
The solution uses the following key components: 
 
 SageMaker Unified Studio ‚Äì Provides the development environment 
 Flow app ‚Äì Orchestrates the workflow, including: 
   
   Knowledge base queries 
   Prompt-based classification 
   Conditional routing 
   Agent-based response generation 
    
 
The workflow processes user queries through the following steps: 
 
 A user submits a complaint-related question. 
 The knowledge base provides relevant complaint information. 
 The prompt classifies if the query is about resolution timing. 
 Based on the classification using the condition, the application takes the following action: 
   
   Routes the query to an AI agent for specific resolution responses. 
   Returns general complaint information. 
    
 The application generates an appropriate response for the user. 
 
Prerequisites 
For this example, you need the following: 
 
 Access to SageMaker Unified Studio. (You will need the SageMaker Unified Studio portal URL from your administrator). You can authenticate using either: 
   
   AWS Identity and Access Management (IAM) user credentials. 
   Single sign-on (SSO) credentials with AWS IAM Identity Center. 
    
 The IAM user or IAM Identity Center user must have appropriate permissions for: 
   
   SageMaker Unified Studio. 
    
   Amazon Bedrock (including Amazon Bedrock Flows, Amazon Bedrock Agents, Amazon Bedrock Prompt Management, and Amazon Bedrock Knowledge Bases). 
   For more information, refer to Identity-based policy examples. 
    
 Access to Amazon Bedrock FMs (make sure these are enabled for your account), for example:Anthropic‚Äôs Claude 3 Haiku (for the agent). 
 Configure access to your Amazon Bedrock serverless models for Amazon Bedrock in SageMaker Unified Studio projects. 
 Amazon Titan Embedding (for the knowledge base). 
 Sample complaint data prepared in CSV format for creating the knowledge base. 
 
Prepare your data 
We have created a sample dataset to use for Amazon Bedrock Knowledge Bases. This dataset has information of complaints received by customer service representatives and resolution information.The following is an example from the sample dataset: 
 
 complaint_id,product,sub_product,issue,sub_issue,complaint_summary,action_taken,next_steps,financial_institution,state,submitted_via,resolution_type,timely_response
FIN-2024-001,04/26/24,"Mortgage","Conventional mortgage","Payment issue","Escrow dispute","Customer disputes mortgage payment increase after recent escrow analysis","Reviewed escrow analysis, explained property tax increase impact, provided detailed payment breakdown","1. Send written explanation of escrow analysis 2. Schedule annual escrow review 3. Provide payment assistance options","Financial Institution-1","TX","Web","Closed with explanation","Yes"
FIN-2024-002,04/26/24,"Money transfer","Wire transfer","Processing delay","International transfer","Wire transfer of $10,000 delayed, customer concerned about international payment deadline","Located wire transfer in system, expedited processing, waived wire fee","1. Confirm receipt with receiving bank 2. Update customer on delivery 3. Document process improvement needs","Financial Institution-2","FL","Phone","Closed with monetary relief","No" 
 
Create a project 
In SageMaker Unified Studio, users can use projects to collaborate on various business use cases. Within projects, you can manage data assets in the SageMaker Unified Studio catalog, perform data analysis, organize workflows, develop ML models, build generative AI applications, and more. 
To create a project, complete the following steps: 
 
 Open the SageMaker Unified Studio landing page using the URL from your admin. 
 Choose Create project. 
 Enter a project name and optional description. 
 For Project profile, choose Generative AI application development. 
 Choose Continue. 
 
 
 
 Complete your project configuration, then choose Create project. 
 
Create a prompt 
Let‚Äôs create a reusable prompt to capture the instructions for FMs, which we will use later while creating the flow application. For more information, see Reuse and share Amazon Bedrock prompts. 
 
 In SageMaker Unified Studio, on the Build menu, choose Prompt under Machine Learning &amp; Generative AI. 
 
 
 
 Provide a name for the prompt. 
 Choose the appropriate FM (for this example, we choose Claude 3 Haiku). 
 For Prompt message, we enter the following: 
 
 
 You are a complaint analysis classifier. You will receive complaint data from a knowledge base. Analyze the {{input}} and respond with a single letter:
T: If the input contains information about complaint resolution timing, response time, or processing timeline (whether timely or delayed)
F: For all other types of complaint information
Return only 'T' or 'F' based on whether the knowledge base response is about resolution timing. Do not add any additional text or explanation - respond with just the single letter 'T' or 'F'. 
 
 
 Choose Save. 
 
 
 
 Choose Create version. 
 
Create a chat agent 
Let‚Äôs create a chat agent to handle specific resolution responses. Complete the following steps: 
 
 In SageMaker Unified Studio, on the Build menu, choose Chat agent under Machine Learning &amp; Generative AI. 
 Provide a name for the prompt. 
 Choose the appropriate FM (for this example, we choose Claude 3 Haiku). 
 For Enter a system prompt, we enter the following: 
 
 
 You are a Financial Complaints Assistant AI. You will receive complaint information from a knowledge base and questions about resolution timing.
When responding to resolution timing queries:
1. Use the provided complaint information to confirm if it was resolved within timeline
2. For timely resolutions, provide:
   - Confirmation of timely completion
   - Specific actions taken (from the provided complaint data)
   - Next steps that were completed
2. For delayed resolutions, provide:
   - Acknowledgment of delay
   - Standard compensation package:
     ‚Ä¢ $75 service credit
     ‚Ä¢ Priority Status upgrade for 6 months
     ‚Ä¢ Service fees waived for current billing cycle
   - Actions taken (from the provided complaint data)
   - Contact information for follow-up: Priority Line: ************** 
Always reference the specific complaint details provided in your input when discussing actions taken and resolution process. 
 
 
 Choose Save. 
 
 
 
 After the agent is saved, choose Deploy. 
 For Alias name, enter demoAlias. 
 Choose Deploy. 
 
Create a flow 
Now that we have our prompt and agent ready, let‚Äôs create a flow that will orchestrate the complaint handling process: 
 
 In SageMaker Unified Studio, on the Build menu, choose Flow under Machine Learning &amp; Generative AI. 
 
 
 
 Create a new flow called demo-flow. 
 
 
Add a knowledge base to your flow application 
Complete the following steps to add a knowledge base node to the flow: 
 
 In the navigation pane, on the Nodes tab, choose Knowledge Base. 
 On the Configure tab, provide the following information: 
   
   For Node name, enter a name (for example, complaints_kb). 
   Choose Create new Knowledge Base. 
    
 In the Create Knowledge Base pane, enter the following information: 
   
   For Name, enter a name (for example, complaints). 
   For Description, enter a description (for example, user complaints information). 
   For Add data sources, select Local file and upload the complaints.txt file. 
   For Embeddings model, choose Titan Text Embeddings V2. 
   For Vector store, choose OpenSearch Serverless. 
   Choose Create. 
    
 
 
 
 After you create the knowledge base, choose it in the flow. 
 In the details name, provide the following information: 
 For Response generation model, choose Claude 3 Haiku. 
 Connect the output of the flow input node with the input of the knowledge base node. 
 Connect the output of the knowledge base node with the input of the flow output node. 
 
 
 
 Choose Save. 
 
Add a prompt to your flow application 
Now let‚Äôs add the prompt you created earlier to the flow: 
 
 On the Nodes tab in the Flow app builder pane, add a prompt node. 
 On the Configure tab for the prompt node, provide the following information: 
 For Node name, enter a name (for example, demo_prompt). 
 For Prompt, choose financeAssistantPrompt. 
 For Version, choose 1. 
 Connect the output of the knowledge base node with the input of the prompt node.  
 Choose Save. 
 
Add a condition to your flow application 
The condition node determines how the flow handles different types of queries. It evaluates whether a query is about resolution timing or general complaint information, enabling the flow to route the query appropriately. When a query is about resolution timing, it will be directed to the chat agent for specialized handling; otherwise, it will receive a direct response from the knowledge base. Complete the following steps to add a condition: 
 
 On the Nodes tab in the Flow app builder pane, add a condition node. 
 On the Configure tab for the condition node, provide the following information: 
   
   For Node name, enter a name (for example, demo_condition). 
   Under Conditions, for Condition, enter conditionInput == "T". 
   Connect the output of the prompt node with the input of the condition node.  
    
 Choose Save. 
 
Add a chat agent to your flow application 
Now let‚Äôs add the chat agent you created earlier to the flow: 
 
 On the Nodes tab in the Flow app builder pane, add the agent node. 
 On the Configure tab for the agent node, provide the following information: 
   
   For Node name, enter a name (for example, demo_agent). 
   For Chat agent, choose DemoAgent. 
   For Alias, choose demoAlias. 
    
 Create the following node connections: 
   
   Connect the input of the condition node (demo_condition) to the output of the prompt node (demo_prompt). 
   Connect the output of the condition node: 
     
     Set If condition is true to the agent node (demo_agent). 
     Set If condition is false to the existing flow output node (FlowOutputNode). 
      
   Connect the output of the knowledge base node (complaints_kb) to the input of the following: 
     
     The agent node (demo_agent). 
     The flow output node (FlowOutputNode). 
      
   Connect the output of the agent node (demo_agent) to a new flow output node named FlowOutputNode_2.  
    
 Choose Save. 
 
Test the flow application 
Now that the flow application is ready, let‚Äôs test it. On the right side of the page, choose the expand icon to open the Test pane. 
 
In the Enter prompt text box, we can ask a few questions related to the dataset created earlier. The following screenshots show some examples. 
 
 
Clean up 
To clean up your resources, delete the flow, agent, prompt, knowledge base, and associated OpenSearch Serverless resources. 
Conclusion 
In this post, we demonstrated how to build an AI-powered complaint reference system using a flow application in SageMaker Unified Studio. By using the integrated capabilities of SageMaker Unified Studio with Amazon Bedrock features like Amazon Bedrock Knowledge Bases, Amazon Bedrock Agents, and Amazon Bedrock Flows, you can rapidly develop and deploy sophisticated AI applications without extensive coding. 
As you build AI workflows using SageMaker Unified Studio, remember to adhere to the AWS Shared Responsibility Model for security. Implement SageMaker Unified Studio security best practices, including proper IAM configurations and data encryption. You can also refer to Secure a generative AI assistant with OWASP Top 10 mitigation for details on how to assess the security posture of a generative AI assistant using OWASP TOP 10 mitigations for common threats. Following these guidelines helps establish robust AI applications that maintain data integrity and system protection. 
To learn more, refer to Amazon Bedrock in SageMaker Unified Studio and join discussions and share your experiences in AWS Generative AI Community. 
We look forward to seeing the innovative solutions you will create with these powerful new features. 
 
About the authors 
Sumeet Tripathi is an Enterprise Support Lead (TAM) at AWS in North Carolina. He has over 17 years of experience in technology across various roles. He is passionate about helping customers to reduce operational challenges and friction. His focus area is AI/ML and Energy &amp; Utilities Segment. Outside work, He enjoys traveling with family, watching cricket and movies. 
Vishal Naik is a Sr. Solutions Architect at Amazon Web Services (AWS). He is a builder who enjoys helping customers accomplish their business needs and solve complex challenges with AWS solutions and best practices. His core area of focus includes Generative AI and Machine Learning. In his spare time, Vishal loves making short films on time travel and alternate universe themes.
‚Ä¢ Accelerating AI innovation: Scale MCP servers for enterprise workloads with Amazon Bedrock
  Generative AI has been moving at a rapid pace, with new tools, offerings, and models released frequently. According to Gartner, agentic AI is one of the top technology trends of 2025, and organizations are performing prototypes on how to use agents in their enterprise environment. Agents depend on tools, and each tool might have its own mechanism to send and receive information. Model Context Protocol (MCP) by Anthropic is an open source protocol that attempts to solve this challenge. It provides a protocol and communication standard that is cross-compatible with different tools, and can be used by an agentic application‚Äôs large language model (LLM) to connect to enterprise APIs or external tools using a standard mechanism. However, large enterprise organizations like financial services tend to have complex data governance and operating models, which makes it challenging to implement agents working with MCP. 
One major challenge is the siloed approach in which individual teams build their own tools, leading to duplication of efforts and wasted resources. This approach slows down innovation and creates inconsistencies in integrations and enterprise design. Furthermore, managing multiple disconnected MCP tools across teams makes it difficult to scale AI initiatives effectively. These inefficiencies hinder enterprises from fully taking advantage of generative AI for tasks like post-trade processing, customer service automation, and regulatory compliance. 
In this post, we present a centralized MCP server implementation using Amazon Bedrock that offers an innovative approach by providing shared access to tools and resources. With this approach, teams can focus on building AI capabilities rather than spending time developing or maintaining tools. By standardizing access to resources and tools through MCP, organizations can accelerate the development of AI agents, so teams can reach production faster. Additionally, a centralized approach provides consistency and standardization and reduces operational overhead, because the tools are managed by a dedicated team rather than across individual teams. It also enables centralized governance that enforces controlled access to MCP servers, which reduces the risk of data exfiltration and prevents unauthorized or insecure tool use across the organization. 
Solution overview 
The following figure illustrates a proposed solution based on a financial services use case that uses MCP servers across multiple lines of business (LoBs), such as compliance, trading, operations, and risk management. Each LoB performs distinct functions tailored to their specific business. For instance, the trading LoB focuses on trade execution, whereas the risk LoB performs risk limit checks. For performing these functions, each division provides a set of MCP servers that facilitate actions and access to relevant data within their LoBs. These servers are accessible to agents developed within the respective LoBs and can also be exposed to agents outside LoBs. 
 
The development of MCP servers is decentralized. Each LoB is responsible for developing the servers that support their specific functions. When the development of a server is complete, it‚Äôs hosted centrally and accessible across LoBs. It takes the form of a registry or marketplace that facilitates integration of AI-driven solutions across divisions while maintaining control and governance over shared resources. 
In the following sections, we explore what the solution looks like on a conceptual level. 
Agentic application interaction with a central MCP server hub 
The following flow diagram showcases how an agentic application built using Amazon Bedrock interacts with one of the MCP servers located in the MCP server hub. 
The flow consists of the following steps: 
 
 The application connects to the central MCP hub through the load balancer and requests a list of available tools from the specific MCP server. This can be fine-grained based on what servers the agentic application has access to. 
 The trade server responds with list of tools available, including details such as tool name, description, and required input parameters. 
 The agentic application invokes an Amazon Bedrock agent and provides the list of tools available. 
 Using this information, the agent determines what to do next based on the given task and the list of tools available to it. 
 The agent chooses the most suitable tool and responds with the tool name and input parameters. The control comes back to the agentic application. 
 The agentic application calls for the execution of the tool through the MCP server using the tool name and input parameters. 
 The trade MCP server executes the tool and returns the results of the execution back to the application. 
 The application returns the results of the tool execution back to the Amazon Bedrock agent. 
 The agent observes the tool execution results and determines the next step. 
 
Let‚Äôs dive into the technical architecture of the solution. 
Architecture overview 
The following diagram illustrates the architecture to host the centralized cluster of MCP servers for an LoB. 
 
The architecture can be split in five sections: 
 
 MCP server discovery API 
 Agentic applications 
 Central MCP server hub 
 Tools and resources 
 
Let‚Äôs explore each section in detail: 
 
 MCP server discovery API ‚Äì This API is a dedicated endpoint for discovering various MCP servers. Different teams can call this API to find what MCP servers are available in the registry; read their description, tool, and resource details; and decide which MCP server would be the right one for their agentic application. When a new MCP server is published, it‚Äôs added to an Amazon DynamoDB database. MCP server owners are responsible for keeping the registry information up-to-date. 
 Agentic application ‚Äì The agentic applications are hosted on AWS Fargate for Amazon Elastic Container Service (Amazon ECS) and built using Amazon Bedrock Agents. Teams can also use the newly released open source AWS Strands Agents SDK, or other agentic frameworks of choice, to build the agentic application and their own containerized solution to host the agentic application. The agentic applications access Amazon Bedrock through a secure private virtual private cloud (VPC) endpoint. It uses private VPC endpoints to access MCP servers. 
 Central MCP server hub ‚Äì This is where the MCP servers are hosted. Access to servers is enabled through an AWS Network Load Balancer. Technically, each server is a Docker container that can is hosted on Amazon ECS, but you can choose your own container deployment solution. These servers can scale individually without impacting the other server. These servers in turn connect to one or more tools using private VPC endpoints. 
 Tools and resources ‚Äì This component holds the tools, such as databases, another application, Amazon Simple Storage Service (Amazon S3), or other tools. For enterprises, access to the tools and resources is provided only through private VPC endpoints. 
 
Benefits of the solution 
The solution offers the following key benefits: 
 
 Scalability and resilience ‚Äì Because you‚Äôre using Amazon ECS on Fargate, you get scalability out of the box without managing infrastructure and handling scaling concerns. Amazon ECS automatically detects and recovers from failures by restarting failed MCP server tasks locally or reprovisioning containers, minimizing downtime. It can also redirect traffic away from unhealthy Availability Zones and rebalance tasks across healthy Availability Zones to provide uninterrupted access to the server. 
 Security ‚Äì Access to MCP servers is secured at the network level through network controls such as PrivateLink. This makes sure the agentic application only connects to trusted MCP servers hosted by the organization, and vice versa. Each Fargate workload runs in an isolated environment. This prevents resource sharing between tasks. For application authentication and authorization, we propose using an MCP Auth Server (refer to the following GitHub repo) to hand off those tasks to a dedicated component that can scale independently. 
 
At the time of writing, the MCP protocol doesn‚Äôt provide built-in mechanisms for user-level access control or authorization. Organizations requiring user-specific access restrictions must implement additional security layers on top of the MCP protocol. For a reference implementation, refer to the following GitHub repo. 
Let‚Äôs dive deeper in the implementation of this solution. 
Use case 
The implementation is based on a financial services use case featuring post-trade execution. Post-trade execution refers to the processes and steps that take place after an equity buy/sell order has been placed by a customer. It involves many steps, including verifying trade details, actual transfer of assets, providing a detailed report of the execution, running fraudulent checks, and more. For simplification of the demo, we focus on the order execution step. 
Although this use case is tailored to the financial industry, you can apply the architecture and the approach to other enterprise workloads as well. The entire code of this implementation is available on GitHub. We use the AWS Cloud Development Kit (AWS CDK) for Python to deploy this solution, which creates an agentic application connected to tools through the MCP server. It also creates a Streamlit UI to interact with the agentic application. 
The following code snippet provides access to the MCP discovery API: 
 
 def get_server_registry():
    # Initialize DynamoDB client
    dynamodb = boto3.resource('dynamodb')
    table = dynamodb.Table(DDBTBL_MCP_SERVER_REGISTRY)
    
    try:
        # Scan the table to get all items
        response = table.scan()
        items = response.get('Items', [])
        
        # Format the items to include only id, description, server
        formatted_items = []
        for item in items:
            formatted_item = {
                'id': item.get('id', ''),
                'description': item.get('description', ''),
                'server': item.get('server', ''),
            }
            formatted_items.append(formatted_item)
        
        # Return the formatted items as JSON
        return {
            'statusCode': 200,
            'headers': cors_headers,
            'body': json.dumps(formatted_items)
        }
    except Exception as e:
        # Handle any errors
        return {
            'statusCode': 500,
            'headers': cors_headers,
            'body': json.dumps({'error': str(e)})
        } 
 
The preceding code is invoked through an AWS Lambda function. The complete code is available in the GitHub repository. The following graphic shows the response of the discovery API. 
 
Let‚Äôs explore a scenario where the user submits a question: ‚ÄúBuy 100 shares of AMZN at USD 186, to be distributed equally between accounts A31 and B12.‚ÄùTo execute this task, the agentic application invokes the trade-execution MCP server. The following code is the sample implementation of the MCP server for trade execution: 
 
 from fastmcp import FastMCP
from starlette.requests import Request
from starlette.responses import PlainTextResponse
mcp = FastMCP("server")

@mcp.custom_route("/", methods=["GET"])
async def health_check(request: Request) -&gt; PlainTextResponse:
    return PlainTextResponse("OK")

@mcp.tool()
async def executeTrade(ticker, quantity, price):
    """
    Execute a trade for the given ticker, quantity, and price.
    
    Sample input:
    {
        "ticker": "AMZN",
        "quantity": 1000,
        "price": 150.25
    }
    """
    # Simulate trade execution
    return {
        "tradeId": "T12345",
        "status": "Executed",
        "timestamp": "2025-04-09T22:58:00"
    }
    
@mcp.tool()
async def sendTradeDetails(tradeId):
    """
    Send trade details for the given tradeId.
    Sample input:
    {
        "tradeId": "T12345"
    }
    """
    return {
        "status": "Details Sent",
        "recipientSystem": "MiddleOffice",
        "timestamp": "2025-04-09T22:59:00"
    }
if __name__ == "__main__":
    mcp.run(host="0.0.0.0", transport="streamable-http") 
 
The complete code is available in the following GitHub repo. 
The following graphic shows the MCP server execution in action. 
 
This is a sample implementation of the use case focusing on the deployment step. For a production scenario, we strongly recommend adding a human oversight workflow to monitor the execution and provide input at various steps of the trade execution. 
Now you‚Äôre ready to deploy this solution. 
Prerequisites 
Prerequisites for the solution are available in the README.md of the GitHub repository. 
Deploy the application 
Complete the following steps to run this solution: 
 
 Navigate to the README.md file of the GitHub repository to find the instructions to deploy the solution. Follow these steps to complete deployment. 
 
The successful deployment will exit with a message similar to the one shown in the following screenshot. 
 
 
 When the deployment is complete, access the Streamlit application. 
 
You can find the Streamlit URL in the terminal output, similar to the following screenshot. 
 
 
 Enter the URL of the Streamlit application in a browser to open the application console. 
 
On the application console, different sets of MCP servers are listed in the left pane under MCP Server Registry. Each set corresponds to an MCP server and includes the definition of the tools, such as the name, description, and input parameters. 
 
In the right pane, Agentic App, a request is pre-populated: ‚ÄúBuy 100 shares of AMZN at USD 186, to be distributed equally between accounts A31 and B12.‚Äù This request is ready to be submitted to the agent for execution. 
 
 
 Choose Submit to invoke an Amazon Bedrock agent to process the request. 
 
The agentic application will evaluate the request together with the list of tools it has access to, and iterate through a series of tools execution and evaluation to fulfil the request.You can view the trace output to see the tools that the agent used. For each tool used, you can see the values of the input parameters, followed by the corresponding results. In this case, the agent operated as follows: 
 
 The agent first used the function executeTrade with input parameters of ticker=AMZN, quantity=100, and price=186 
 After the trade was executed, used the allocateTrade tool to allocate the trade position between two portfolio accounts 
 
Clean up 
You will incur charges when you consume the services used in this solution. Instructions to clean up the resources are available in the README.md of the GitHub repository. 
Summary 
This solution offers a straightforward and enterprise-ready approach to implement MCP servers on AWS. With this centralized operating model, teams can focus on building their applications rather than maintaining the MCP servers. As enterprises continue to embrace agentic workflows, centralized MCP servers offer a practical solution for overcoming operational silos and inefficiencies. With the AWS scalable infrastructure and advanced tools like Amazon Bedrock Agents and Amazon ECS, enterprises can accelerate their journey toward smarter workflows and better customer outcomes. 
Check out the GitHub repository to replicate the solution in your own AWS environment. 
To learn more about how to run MCP servers on AWS, refer to the following resources: 
 
 Harness the power of MCP servers with Amazon Bedrock Agents 
 Unlocking the power of Model Context Protocol (MCP) on AWS 
 Amazon Bedrock Agents Samples GitHub repository 
 
 
 
About the authors 
Xan Huang is a Senior Solutions Architect with AWS and is based in Singapore. He works with major financial institutions to design and build secure, scalable, and highly available solutions in the cloud. Outside of work, Xan dedicates most of his free time to his family, where he lovingly takes direction from his two young daughters, aged one and four. You can find Xan on LinkedIn: https://www.linkedin.com/in/xanhuang/ 
Vikesh Pandey&nbsp;is a Principal GenAI/ML Specialist Solutions Architect at AWS helping large financial institutions adopt and scale generative AI and ML workloads. He is the author of book ‚ÄúGenerative AI for financial services.‚Äù He carries more than decade of experience building enterprise-grade applications on generative AI/ML and related technologies. In his spare time, he plays an unnamed sport with his son that lies somewhere between football and rugby.
‚Ä¢ Choosing the right approach for generative AI-powered structured data retrieval
  Organizations want direct answers to their business questions without the complexity of writing SQL queries or navigating through business intelligence (BI) dashboards to extract data from structured data stores. Examples of structured data include tables, databases, and data warehouses that conform to a predefined schema. Large language model (LLM)-powered natural language query systems transform how we interact with data, so you can ask questions like ‚ÄúWhich region has the highest revenue?‚Äù and receive immediate, insightful responses. Implementing these capabilities requires careful consideration of your specific needs‚Äîwhether you need to integrate knowledge from other systems (for example, unstructured sources like documents), serve internal or external users, handle the analytical complexity of questions, or customize responses for business appropriateness, among other factors. 
In this post, we discuss LLM-powered structured data query patterns in AWS. We provide a decision framework to help you select the best pattern for your specific use case. 
Business challenge: Making structured data accessible 
Organizations have vast amounts of structured data but struggle to make it effectively accessible to non-technical users for several reasons: 
 
 Business users lack the technical knowledge (like SQL) needed to query data 
 Employees rely on BI teams or data scientists for analysis, limiting self-service capabilities 
 Gaining insights often involves time delays that impact decision-making 
 Predefined dashboards constrain spontaneous exploration of data 
 Users might not know what questions are possible or where relevant data resides 
 
Solution overview 
An effective solution should provide the following: 
 
 A conversational interface that allows employees to query structured data sources without technical expertise 
 The ability to ask questions in everyday language and receive accurate, trustworthy answers 
 Automatic generation of visualizations and explanations to clearly communicate insights. 
 Integration of information from different data sources (both structured and unstructured) presented in a unified manner 
 Ease of integration with existing investments and rapid deployment capabilities 
 Access restriction based on identities, roles, and permissions 
 
In the following sections, we explore five patterns that can address these needs, highlighting the architecture, ideal use cases, benefits, considerations, and implementation resources for each approach. 
Pattern 1: Direct conversational interface using an enterprise assistant 
This pattern uses Amazon Q Business, a generative AI-powered assistant, to provide a chat interface on data sources with native connectors. When users ask questions in natural language, Amazon Q Business connects to the data source, interprets the question, and retrieves relevant information without requiring intermediate services. The following diagram illustrates this workflow. 
 
This approach is ideal for internal enterprise assistants that need to answer business user-facing questions from both structured and unstructured data sources in a unified experience. For example, HR personnel can ask ‚ÄúWhat‚Äôs our parental leave policy and how many employees used it last quarter?‚Äù and receive answers drawn from both leave policy documentation and employee databases together in one interaction. With this pattern, you can benefit from the following: 
 
 Simplified connectivity through the extensive Amazon Q Business library of built-in connectors 
 Streamlined implementation with a single service to configure and manage 
 Unified search experience for accessing both structured and unstructured information 
 Built-in understanding and respect existing identities, roles, and permissions 
 
You can define the scope of data to be pulled in the form of a SQL query. Amazon Q Business pre-indexes database content based on defined SQL queries and uses this index when responding to user questions. Similarly, you can define the sync mode and schedule to determine how often you want to update your index. Amazon Q Business does the heavy lifting of indexing the data using a Retrieval Augmented Generation (RAG) approach and using an LLM to generate well-written answers. For more details on how to set up Amazon Q Business with an Amazon Aurora PostgreSQL-Compatible Edition connector, see Discover insights from your Amazon Aurora PostgreSQL database using the Amazon Q Business connector. You can also refer to the complete list of supported data source connectors. 
Pattern 2: Enhancing BI tool with natural language querying capabilities 
This pattern uses Amazon Q in QuickSight to process natural language queries against datasets that have been previously configured in Amazon QuickSight. Users can ask questions in everyday language within the QuickSight interface and get visualized answers without writing SQL. This approach works with QuickSight (Enterprise or Q edition) and supports various data sources, including Amazon Relational Database Service (Amazon RDS), Amazon Redshift, Amazon Athena, and others. The architecture is depicted in the following diagram. 
 
This pattern is well-suited for internal BI and analytics use cases. Business analysts, executives, and other employees can ask ad-hoc questions to get immediate visualized insights in the form of dashboards. For example, executives can ask questions like ‚ÄúWhat were our top 5 regions by revenue last quarter?‚Äù and immediately see responsive charts, reducing dependency on analytics teams. The benefits of this pattern are as follows: 
 
 It enables natural language queries that produce rich visualizations and charts 
 No coding or machine learning (ML) experience is needed‚Äîthe heavy lifting like natural language interpretation and SQL generation is managed by Amazon Q in QuickSight 
 It integrates seamlessly within the familiar QuickSight dashboard environment 
 
Existing QuickSight users might find this the most straightforward way to take advantage of generative AI benefits. You can optimize this pattern for higher-quality results by configuring topics like curated fields, synonyms, and expected question phrasing. This pattern will pull data only from a specific configured data source in QuickSight to produce a dashboard as an output. For more details, check out QuickSight DemoCentral to view a demo in QuickSight, see the generative BI learning dashboard, and view guided instructions to create dashboards with Amazon Q. Also refer to the list of supported data sources. 
Pattern 3: Combining BI visualization with conversational AI for a seamless experience 
This pattern merges BI visualization capabilities with conversational AI to create a seamless knowledge experience. By integrating Amazon Q in QuickSight with Amazon Q Business (with the QuickSight plugin enabled), organizations can provide users with a unified conversational interface that draws on both unstructured and structured data. The following diagram illustrates the architecture. 
 
This is ideal for enterprises that want an internal AI assistant to answer a variety of questions‚Äîwhether it‚Äôs a metric from a database or knowledge from a document. For example, executives can ask ‚ÄúWhat was our Q4 revenue growth?‚Äù and see visualized results from data warehouses through Amazon Redshift through QuickSight, then immediately follow up with ‚ÄúWhat is our company vacation policy?‚Äù to access HR documentation‚Äîall within the same conversation flow. This pattern offers the following benefits: 
 
 It unifies answers from structured data (databases and warehouses) and unstructured data (documents, wikis, emails) in a single application 
 It delivers rich visualizations alongside conversational responses in a seamless experience with real-time analysis in chat 
 There is no duplication of work‚Äîif your BI team has already built datasets and topics in QuickSight for analytics, you use that in Amazon Q Business 
 It maintains conversational context when switching between data and document-based inquiries 
 
For more details, see Query structured data from Amazon Q Business using Amazon QuickSight integration and Amazon Q Business now provides insights from your databases and data warehouses (preview). 
Another variation of this pattern is recommended for BI users who want to expose unified data through rich visuals in QuickSight, as illustrated in the following diagram. 
 
For more details, see Integrate unstructured data into Amazon QuickSight using Amazon Q Business. 
Pattern 4: Building knowledge bases from structured data using managed text-to-SQL 
This pattern uses Amazon Bedrock Knowledge Bases to enable structured data retrieval. The service provides a fully managed text-to-SQL module that alleviates common challenges in developing natural language query applications for structured data. This implementation uses Amazon Bedrock (Amazon Bedrock Agents and Amazon Bedrock Knowledge Bases) along with your choice of data warehouse such as Amazon Redshift or Amazon SageMaker Lakehouse. The following diagram illustrates the workflow. 
 
For example, a seller can use this capability embedded into an ecommerce application to ask a complex query like ‚ÄúGive me top 5 products whose sales increased by 50% last year as compared to previous year? Also group the results by product category.‚Äù The system automatically generates the appropriate SQL, executes it against the data sources, and delivers results or a summarized narrative. This pattern features the following benefits: 
 
 It provides fully managed text-to-SQL capabilities without requiring model training 
 It enables direct querying of data from the source without data movement 
 It supports complex analytical queries on warehouse data 
 It offers flexibility in foundation model (FM) selection through Amazon Bedrock 
 API connectivity, personalization options, and context-aware chat features make it better suited for customer facing applications 
 
Choose this pattern when you need a flexible, developer-oriented solution. This approach works well for applications (internal or external) where you control the UI design. Default outputs are primarily text or structured data. However, executing arbitrary SQL queries can be a security risk for text-to-SQL applications. It is recommended that you take precautions as needed, such as using restricted roles, read-only databases, and sandboxing. For more information on how to build this pattern, see Empower financial analytics by creating structured knowledge bases using Amazon Bedrock and Amazon Redshift. For a list of supported structured data stores, refer to Create a knowledge base by connecting to a structured data store. 
Pattern 5: Custom text-to-SQL implementation with flexible model selection 
This pattern represents a build-your-own solution using FMs to convert natural language to SQL, execute queries on data warehouses, and return results. Choose Amazon Bedrock when you want to quickly integrate this capability without deep ML expertise‚Äîit offers a fully managed service with ready-to-use FMs through a unified API, handling infrastructure needs with pay-as-you-go pricing. Alternatively, select Amazon SageMaker AI when you require extensive model customization to build specialized needs‚Äîit provides complete ML lifecycle tools for data scientists and ML engineers to build, train, and deploy custom models with greater control. For more information, refer to our Amazon Bedrock or Amazon SageMaker AI decision guide. The following diagram illustrates the architecture. 
 
Use this pattern if your use case requires specific open-weight models, or you want to fine-tune models on your domain-specific data. For example, if you need highly accurate results for your query, then you can use this pattern to fine-tune models on specific schema structures, while maintaining the flexibility to integrate with existing workflows and multi-cloud environments. This pattern offers the following benefits: 
 
 It provides maximum customization in model selection, fine-tuning, and system design 
 It supports complex logic across multiple data sources 
 It offers complete control over security and deployment in your virtual private cloud (VPC) 
 It enables flexible interface implementation (Slack bots, custom web UIs, notebook plugins) 
 You can implement it for external user-facing solutions 
 
For more information on steps to build this pattern, see Build a robust text-to-SQL solution generating complex queries, self-correcting, and querying diverse data sources. 
Pattern comparison: Making the right choice 
To make effective decisions, let‚Äôs compare these patterns across key criteria. 
Data workload suitability 
Different out-of-the-box patterns handle transactional (operational) and analytical (historical or aggregated) data with varying degrees of effectiveness. Patterns 1 and 3, which use Amazon Q Business, work with indexed data and are optimized for lookup-style queries against previously indexed content rather than real-time transactional database queries. Pattern 2, which uses Amazon Q in QuickSight, gets visual output for transactional information for ad-hoc analysis. Pattern 4, which uses Amazon Bedrock structured data retrieval, is specifically designed for analytical systems and data warehouses, excelling at complex queries on large datasets. Pattern 5 is a self-managed text-to-SQL option that can be built to support both transactional or analytical needs of users. 
Target audience 
Architectures highlighted in Patterns 1, 2, and 3 (using Amazon Q Business, Amazon Q in QuickSight, or a combination) are best suited for internal enterprise use. However, you can use Amazon QuickSight Embedded to embed data visuals, dashboards, and natural language queries into both internal or customer-facing applications. Amazon Q Business serves as an enterprise AI assistant for organizational knowledge that uses subscription-based pricing tiers that is designed for internal employees. Pattern 4 (using Amazon Bedrock) can be used to build both internal as well as customer-facing applications. This is because, unlike the subscription-based model of Amazon Q Business, Amazon Bedrock provides API-driven services that alleviate per-user costs and identity management overhead for external customer scenarios. This makes it well-suited for customer-facing experiences where you need to serve potentially thousands of external users. The custom LLM solutions in Pattern 5 can similarly be tailored to external application requirements. 
Interface and output format 
Different patterns deliver answers through different interaction models: 
 
 Conversational experiences ‚Äì Patterns 1 and 3 (using Amazon Q Business) provide chat-based interfaces. Pattern 4 (using Amazon Bedrock Knowledge Bases for structured data retrieval) naturally supports AI assistant integration, and Pattern 5 (a custom text-to-SQL solution) can be designed for a variety of interaction models. 
 Visualization-focused output ‚Äì Pattern 2 (using Amazon Q in QuickSight) specializes in generating on-the-fly visualizations such as charts and tables in response to user questions. 
 API integration ‚Äì For embedding capabilities into existing applications, Patterns 4 and 5 offer the most flexible API-based integration options. 
 
The following figure is a comparison matrix of AWS structured data query patterns. 
 
Conclusion 
Between these patterns, your optimal choice depends on the following key factors: 
 
 Data location and characteristics ‚Äì Is your data in operational databases, already in a data warehouse, or distributed across various sources? 
 User profile and interaction model ‚Äì Are you supporting internal or external users? Do they prefer conversational or visualization-focused interfaces? 
 Available resources and expertise ‚Äì Do you have ML specialists available, or do you need a fully managed solution? 
 Accuracy and governance requirements ‚Äì Do you need strictly controlled semantics and curation, or is broader query flexibility acceptable with monitoring? 
 
By understanding these patterns and their trade-offs, you can architect solutions that align with your business objectives. 
 
About the authors 
Akshara Shah is a Senior Solutions Architect at Amazon Web Services. She helps commercial customers build cloud-based generative AI services to meet their business needs. She has been designing, developing, and implementing solutions that leverage AI and ML technologies for more than 10 years. Outside of work, she loves painting, exercising and spending time with family. 
Sanghwa Na is a Generative AI Specialist Solutions Architect at Amazon Web Services. Based in San Francisco, he works with customers to design and build generative AI solutions using large language models and foundation models on AWS. He focuses on helping organizations adopt AI technologies that drive real business value
‚Ä¢ Revolutionizing drug data analysis using Amazon Bedrock multimodal RAG capabilities
  In the pharmaceutical industry, biotechnology and healthcare companies face an unprecedented challenge for efficiently managing and analyzing vast amounts of drug-related data from diverse sources. Traditional data analysis methods prove inadequate for processing complex medical documentation that includes a mix of text, images, graphs, and tables. Amazon Bedrock offers features like multimodal retrieval, advanced chunking capabilities, and citations to help organizations get high-accuracy responses. 
Pharmaceutical and healthcare organizations process a vast number of complex document formats and unstructured data that pose analytical challenges. Clinical study documents and research papers related to them typically present an intricate blend of technical text, detailed tables, and sophisticated statistical graphs, making automated data extraction particularly challenging. Clinical study documents present additional challenges through non-standardized formatting and varied data presentation styles across multiple research institutions. This post showcases a solution to extract data-driven insights from complex research documents through a sample application with high-accuracy responses. It analyzes clinical trial data, patient outcomes, molecular diagrams, and safety reports from the research documents. It can help pharmaceutical companies accelerate their research process. The solution provides citations from the source documents, reducing hallucinations and enhancing the accuracy of the responses. 
Solution overview 
The sample application uses Amazon Bedrock to create an intelligent AI assistant that analyzes and summarizes research documents containing text, graphs, and unstructured data. Amazon Bedrock is a fully managed service that offers a choice of industry-leading foundation models (FMs) along with a broad set of capabilities to build generative AI applications, simplifying development with security, privacy, and responsible AI. 
To equip FMs with up-to-date and proprietary information, organizations use Retrieval Augmented Generation (RAG), a technique that fetches data from company data sources and enriches the prompt to provide relevant and accurate responses. 
Amazon Bedrock Knowledge Bases is a fully managed RAG capability within Amazon Bedrock with in-built session context management and source attribution that helps you implement the entire RAG workflow, from ingestion to retrieval and prompt augmentation, without having to build custom integrations to data sources and manage data flows. 
Amazon Bedrock Knowledge Bases introduces powerful document parsing capabilities, including Amazon Bedrock Data Automation powered parsing and FM parsing, revolutionizing how we handle complex documents. Amazon Bedrock Data Automation is a fully managed service that processes multimodal data effectively, without the need to provide additional prompting. The FM option parses multimodal data using an FM. This parser provides the option to customize the default prompt used for data extraction. This advanced feature goes beyond basic text extraction by intelligently breaking down documents into distinct components, including text, tables, images, and metadata, while preserving document structure and context. When working with supported formats like PDF, specialized FMs interpret and extract tabular data, charts, and complex document layouts. Additionally, the service provides advanced chunking strategies like semantic chunking, which intelligently divides text into meaningful segments based on semantic similarity calculated by the embedding model. Unlike traditional syntactic chunking methods, this approach preserves the context and meaning of the content, improving the quality and relevance of information retrieval. 
The solution architecture implements these capabilities through a seamless workflow that begins with administrators securely uploading knowledge base documents to an Amazon Simple Storage Service (Amazon S3) bucket. These documents are then ingested into Amazon Bedrock Knowledge Bases, where a large language model (LLM) processes and parses the ingested data. The solution employs semantic chunking to store document embeddings efficiently in Amazon OpenSearch Service for optimized retrieval. The solution features a user-friendly interface built with Streamlit, providing an intuitive chat experience for end-users. When users interact with the Streamlit application, it triggers AWS Lambda functions that handle the requests, retrieving relevant context from the knowledge base and generating appropriate responses. The architecture is secured through AWS Identity and Access Management (IAM), maintaining proper access control throughout the workflow. Amazon Bedrock uses AWS Key Management Service (AWS KMS) to encrypt resources related to your knowledge bases. By default, Amazon Bedrock encrypts this data using an AWS managed key. Optionally, you can encrypt the model artifacts using a customer managed key. This end-to-end solution provides efficient document processing, context-aware information retrieval, and secure user interactions, delivering accurate and comprehensive responses through a seamless chat interface. 
The following diagram illustrates the solution architecture. 
 
This solution uses the following additional services and features: 
 
 The Anthropic Claude 3 family offers Opus, Sonnet, and Haiku models that accept text, image, and video inputs and generate text output. They provide a broad selection of capability, accuracy, speed, and cost operation points. These models understand complex research documents that include charts, graphs, tables, diagrams, and reports. 
 AWS Lambda is a serverless computing service that empowers you to run code without provisioning or managing servers cost effectively. 
 Amazon S3 is a highly scalable, durable, and secure object storage service. 
 Amazon OpenSearch Service is a fully managed search and analytics engine for efficient document retrieval. The OpenSearch Service vector database capabilities enable semantic search, RAG with LLMs, recommendation engines, and search rich media. 
 Streamlit is a faster way to build and share data applications using interactive web-based data applications in pure Python. 
 
Prerequisites 
The following prerequisites are needed to proceed with this solution. For this post, we use the us-east-1 AWS Region. For details on available Regions, see Amazon Bedrock endpoints and quotas. 
 
 An AWS account with an IAM user who has permissions to Lambda, Amazon Bedrock, Amazon S3, and IAM. 
 Access to Anthropic‚Äôs Claude model in Amazon Bedrock. For instructions, see Access Amazon Bedrock foundation models. 
 
Deploy the solution 
Refer to the GitHub repository for the deployment steps listed under the deployment guide section. We use an AWS CloudFormation template to deploy solution resources, including S3 buckets to store the source data and knowledge base data. 
Test the sample application 
Imagine you are a member of an R&amp;D department for a biotechnology firm, and your job requires you to derive insights from drug- and vaccine-related information from diverse sources like research studies, drug specifications, and industry papers. You are performing research on cancer vaccines and want to gain insights based on cancer research publications. You can upload the documents given in the reference section to the S3 bucket and sync the knowledge base. Let‚Äôs explore example interactions that demonstrate the application‚Äôs capabilities. The responses generated by the AI assistant are based on the documents uploaded to the S3 bucket connected with the knowledge base. Due to non-deterministic nature of machine learning (ML), your responses might be slightly different from the ones presented in this post. 
Understanding historical context 
We use the following query: ‚ÄúCreate a timeline of major developments in mRNA vaccine technology for cancer treatment based on the information provided in the historical background sections.‚ÄùThe assistant analyzes multiple documents and presents a chronological progression of mRNA vaccine development, including key milestones based on the chunks of information retrieved from the OpenSearch Service vector database. 
The following screenshot shows the AI assistant‚Äôs response. 
 
Complex data analysis 
We use the following query: ‚ÄúSynthesize the information from the text, figures, and tables to provide a comprehensive overview of the current state and future prospects of therapeutic cancer vaccines.‚Äù 
The AI assistant is able to provide insights from complex data types, which is enabled by FM parsing, while ingesting the data to OpenSearch Service. It is also able to provide images in the source attribution using the multimodal data capabilities of Amazon Bedrock Knowledge Bases. 
The following screenshot shows the AI assistant‚Äôs response. 
 
The following screenshot shows the visuals provided in the citations when the mouse hovers over the question mark icon. 
 
Comparative analysis 
We use the following query: ‚ÄúCompare the efficacy and safety profiles of MAGE-A3 and NY-ESO-1 based vaccines as described in the text and any relevant tables or figures.‚Äù 
The AI assistant used the semantically similar chunks returned from the OpenSearch Service vector database and added this context to the user‚Äôs question, which enabled the FM to provide a relevant answer. 
The following screenshot shows the AI assistant‚Äôs response. 
 
Technical deep dive 
We use the following query: ‚ÄúSummarize the potential advantages of mRNA vaccines over DNA vaccines for targeting tumor angiogenesis, as described in the review.‚Äù 
With the semantic chunking feature of the knowledge base, the AI assistant was able to get the relevant context from the OpenSearch Service database with higher accuracy. 
The following screenshot shows the AI assistant‚Äôs response. 
 
The following screenshot shows the diagram that was used for the answer as one of the citations. 
 
The sample application demonstrates the following: 
 
 Accurate interpretation of complex scientific diagrams 
 Precise extraction of data from tables and graphs 
 Context-aware responses that maintain scientific accuracy 
 Source attribution for provided information 
 Ability to synthesize information across multiple documents 
 
This application can help you quickly analyze vast amounts of complex scientific literature, extracting meaningful insights from diverse data types while maintaining accuracy and providing proper attribution to source materials. This is enabled by the advanced features of the knowledge bases, including FM parsing, which aides in interpreting complex scientific diagrams and extraction of data from tables and graphs, semantic chunking, which aides with high-accuracy context-aware responses, and multimodal data capabilities, which aides in providing relevant images as source attribution. 
These are some of the many new features added to Amazon Bedrock, empowering you to generate high-accuracy results depending on your use case. To learn more, see New Amazon Bedrock capabilities enhance data processing and retrieval. 
Production readiness 
The proposed solution accelerates the time to value of the project development process. Solutions built on the AWS Cloud benefit from inherent scalability while maintaining robust security and privacy controls. 
The security and privacy framework includes fine-grained user access controls using IAM for both OpenSearch Service and Amazon Bedrock services. In addition, Amazon Bedrock enhances security by providing encryption at rest and in transit, and private networking options using virtual private cloud (VPC) endpoints. Data protection is achieved using KMS keys, and API calls and usage are tracked through Amazon CloudWatch logs and metrics. For specific compliance validation for Amazon Bedrock, see Compliance validation for Amazon Bedrock. 
For additional details on moving RAG applications to production, refer to From concept to reality: Navigating the Journey of RAG from proof of concept to production. 
Clean up 
Complete the following steps to clean up your resources. 
 
 Empty the SourceS3Bucket and KnowledgeBaseS3BucketName buckets. 
 Delete the main CloudFormation stack. 
 
Conclusion 
This post demonstrated the powerful multimodal document analysis (text, graphs, images) using advanced parsing and chunking features of Amazon Bedrock Knowledge Bases. By combining the powerful capabilities of Amazon Bedrock FMs, OpenSearch Service, and intelligent chunking strategies through Amazon Bedrock Knowledge Bases, organizations can transform their complex research documents into searchable, actionable insights. The integration of semantic chunking makes sure that document context and relationships are preserved, and the user-friendly Streamlit interface makes the system accessible to end-users through an intuitive chat experience. This solution not only streamlines the process of analyzing research documents, but also demonstrates the practical application of AI/ML technologies in enhancing knowledge discovery and information retrieval. As organizations continue to grapple with increasing volumes of complex documents, this scalable and intelligent system provides a robust framework for extracting maximum value from their document repositories. 
Although our demonstration focused on the healthcare industry, the versatility of this technology extends beyond a single industry. RAG on Amazon Bedrock has proven its value across diverse sectors. Notable adopters include global brands like Adidas in retail, Empolis in information management, Fractal Analytics in AI solutions, Georgia Pacific in manufacturing, and Nasdaq in financial services. These examples illustrate the broad applicability and transformative potential of RAG technology across various business domains, highlighting its ability to drive innovation and efficiency in multiple industries. 
Refer to the GitHub repo for the agentic RAG application, including samples and components for building agentic RAG solutions. Be on the lookout for additional features and samples in the repository in the coming months. 
To learn more about Amazon Bedrock Knowledge Bases, check out the RAG workshop using Amazon Bedrock. Get started with Amazon Bedrock Knowledge Bases, and let us know your thoughts in the comments section. 
References 
The following are sample research documents available with an open access distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license https://creativecommons.org/licenses/by/4.0/: 
 
 Vaccine Therapies for&nbsp;Cancer: Then and&nbsp;Now 
 Therapeutic cancer vaccines: advancements, challenges and prospects 
 Cancer Vaccines: Adjuvant Potency, Importance of Age, Lifestyle, and Treatments 
 Recent advances in mRNA cancer vaccines: meeting challenges and embracing opportunities 
 Nucleic acid cancer vaccines targeting tumor related angiogenesis. Could mRNA vaccines constitute a game changer? 
 Recent Findings on Therapeutic Cancer Vaccines: An Updated Review 
 Cancer Vaccines: Antigen Selection Strategy 
 
 
 
About the authors 
Vivek Mittal is a Solution Architect at Amazon Web Services, where he helps organizations architect and implement cutting-edge cloud solutions. With a deep passion for Generative AI, Machine Learning, and Serverless technologies, he specializes in helping customers harness these innovations to drive business transformation. He finds particular satisfaction in collaborating with customers to turn their ambitious technological visions into reality. 
Shamika Ariyawansa, serving as a Senior AI/ML Solutions Architect in the Global Healthcare and Life Sciences division at Amazon Web Services (AWS), has a keen focus on Generative AI. He assists customers in integrating Generative AI into their projects, emphasizing the importance of explainability within their AI-driven initiatives. Beyond his professional commitments, Shamika passionately pursues skiing and off-roading adventures. 
Shaik Abdulla is a Sr. Solutions Architect, specializes in architecting enterprise-scale cloud solutions with focus on Analytics, Generative AI and emerging technologies. His technical expertise is validated by his achievement of all 12 AWS certifications and the prestigious Golden jacket recognition. He has a passion to architect and implement innovative cloud solutions that drive business transformation. He speaks at major industry events like AWS re:Invent and regional AWS Summits, where he shares insights on cloud architecture and emerging technologies.
‚Ä¢ Build and deploy AI inference workflows with new enhancements to the Amazon SageMaker Python SDK
  Amazon SageMaker Inference has been a popular tool for deploying advanced machine learning (ML) and generative AI models at scale. As AI applications become increasingly complex, customers want to deploy multiple models in a coordinated group that collectively process inference requests for an application. In addition, with the evolution of generative AI applications, many use cases now require inference workflows‚Äîsequences of interconnected models operating in predefined logical flows. This trend drives a growing need for more sophisticated inference offerings. 
To address this need, we are introducing a new capability in the SageMaker Python SDK that revolutionizes how you build and deploy inference workflows on SageMaker. We will take Amazon Search as an example to show case how this feature is used in helping customers building inference workflows. This new Python SDK capability provides a streamlined and simplified experience that abstracts away the underlying complexities of packaging and deploying groups of models and their collective inference logic, allowing you to focus on what matter most‚Äîyour business logic and model integrations. 
In this post, we provide an overview of the user experience, detailing how to set up and deploy these workflows with multiple models using the SageMaker Python SDK. We walk through examples of building complex inference workflows, deploying them to SageMaker endpoints, and invoking them for real-time inference. We also show how customers like Amazon Search plan to use SageMaker Inference workflows to provide more relevant search results to Amazon shoppers. 
Whether you are building a simple two-step process or a complex, multimodal AI application, this new feature provides the tools you need to bring your vision to life. This tool aims to make it easy for developers and businesses to create and manage complex AI systems, helping them build more powerful and efficient AI applications. 
In the following sections, we dive deeper into details of the SageMaker Python SDK, walk through practical examples, and showcase how this new capability can transform your AI development and deployment process. 
Key improvements and user experience 
The SageMaker Python SDK now includes new features for creating and managing inference workflows. These additions aim to address common challenges in developing and deploying inference workflows: 
 
 Deployment of multiple models&nbsp;‚Äì The core of this new experience is the deployment of multiple models as&nbsp;inference components&nbsp;within a single SageMaker endpoint. With this approach, you can create a more unified inference workflow. By consolidating multiple models into one endpoint, you can reduce the number of endpoints that need to be managed. This consolidation can also improve operational tasks, resource utilization, and potentially costs. 
 Workflow definition with workflow mode&nbsp;‚Äì The new workflow mode extends the existing&nbsp;Model Builder capabilities. It allows for the definition of inference workflows using Python code. Users familiar with the ModelBuilder class might find this feature to be an extension of their existing knowledge. This mode enables creating multi-step workflows, connecting models, and specifying the data flow between different models in the workflows. The goal is to reduce the complexity of managing these workflows and enable you to focus more on the logic of the resulting compound AI system. 
 Development and deployment options&nbsp;‚Äì A new deployment option has been introduced for the development phase. This feature is designed to allow for quicker deployment of workflows to development environments. The intention is to enable faster testing and refinement of workflows. This could be particularly relevant when experimenting with different configurations or adjusting models. 
 Invocation flexibility&nbsp;‚Äì The SDK now provides options for invoking individual models or entire workflows. You can choose to call a specific inference component used in a workflow or the entire workflow. This flexibility can be useful in scenarios where access to a specific model is needed, or when only a portion of the workflow needs to be executed. 
 Dependency management&nbsp;‚Äì You can use SageMaker&nbsp;Deep Learning Containers&nbsp;(DLCs) or the SageMaker distribution that comes preconfigured with various model serving libraries and tools. These are intended to serve as a starting point for common use cases. 
 
To get started, use the SageMaker Python SDK to deploy your models as inference components. Then, use the workflow mode to create an inference workflow, represented as Python code using the container of your choice. Deploy the workflow container as another inference component on the same endpoints as the models or a dedicated endpoint. You can run the workflow by invoking the inference component that represents the workflow. The user experience is entirely code-based, using the SageMaker Python SDK. This approach allows you to define, deploy, and manage inference workflows using SDK abstractions offered by this feature and Python programming. The workflow mode provides flexibility to specify complex sequences of model invocations and data transformations, and the option to deploy as components or endpoints caters to various scaling and integration needs. 
Solution overview 
The following diagram illustrates a reference architecture using the SageMaker Python SDK. 
 
The improved SageMaker Python SDK introduces a more intuitive and flexible approach to building and deploying AI inference workflows. Let‚Äôs explore the key components and classes that make up the experience: 
 
 ModelBuilder simplifies the process of packaging individual models as inference components. It handles model loading, dependency management, and container configuration automatically. 
 The CustomOrchestrator class provides a standardized way to define custom inference logic that orchestrates multiple models in the workflow. Users implement the handle() method to specify this logic and can use an orchestration library or none at all (plain Python). 
 A single deploy() call handles the deployment of the components and workflow orchestrator. 
 The Python SDK supports invocation against the custom inference workflow or individual inference components. 
 The Python SDK supports both synchronous and streaming inference. 
 
CustomOrchestrator&nbsp;is an abstract base class that serves as a template for defining custom inference orchestration logic. It standardizes the structure of entry point-based inference scripts, making it straightforward for users to create consistent and reusable code. The handle method in the class is an abstract method that users implement to define their custom orchestration logic. 
 
  
  class CustomOrchestrator (ABC):
"""
Templated class used to standardize the structure of an entry point based inference script.
"""

    @abstractmethod
    def handle(self, data, context=None):
        """abstract class for defining an entrypoint for the model server"""
        return NotImplemented 
  
 
With this templated class, users can integrate into their custom workflow code, and then point to this code in the model builder using a file path or directly using a class or method name. Using this class and the&nbsp;ModelBuilder&nbsp;class, it enables a more streamlined workflow for AI inference: 
 
 Users define their custom workflow by implementing the CustomOrchestrator class. 
 The custom CustomOrchestrator is passed to ModelBuilder using the ModelBuilder inference_spec parameter. 
 ModelBuilder packages the CustomOrchestrator along with the model artifacts. 
 The packaged model is deployed to a SageMaker endpoint (for example, using a TorchServe container). 
 When invoked, the SageMaker endpoint uses the custom handle() function defined in the CustomOrchestrator to handle the input payload. 
 
In the follow sections, we provide two examples of custom workflow orchestrators implemented with plain Python code. For simplicity, the examples use two inference components. 
We explore how to create a simple workflow that deploys two large language models (LLMs) on SageMaker Inference endpoints along with a simple Python orchestrator that calls the two models. We create an IT customer service workflow where one model processes the initial request and another suggests solutions. You can find the example notebook in the&nbsp;GitHub repo. 
Prerequisites 
To run the example notebooks, you need an AWS account with an&nbsp;AWS Identity and Access Management&nbsp;(IAM) role with&nbsp;least-privilege permissions&nbsp;to manage resources created. For details, refer to&nbsp;Create an AWS account. You might need to request a service quota increase for the corresponding SageMaker hosting instances. In this example, we host multiple models on the same SageMaker endpoint, so we use two ml.g5.24xlarge SageMaker hosting instances. 
Python inference orchestration 
First, let‚Äôs define our custom orchestration class that inherits from CustomOrchestrator. The workflow is structured around a custom inference entry point that handles the request data, processes it, and retrieves predictions from the configured model endpoints. See the following code: 
 
 class PythonCustomInferenceEntryPoint(CustomOrchestrator):
    def __init__(self, region_name, endpoint_name, component_names):
        self.region_name = region_name
        self.endpoint_name = endpoint_name
        self.component_names = component_names
    
    def preprocess(self, data):
        payload = {
            "inputs": data.decode("utf-8")
        }
        return json.dumps(payload)

    def _invoke_workflow(self, data):
        # First model (Llama) inference
        payload = self.preprocess(data)
        
        llama_response = self.client.invoke_endpoint(
            EndpointName=self.endpoint_name,
            Body=payload,
            ContentType="application/json",
            InferenceComponentName=self.component_names[0]
        )
        llama_generated_text = json.loads(llama_response.get('Body').read())['generated_text']
        
        # Second model (Mistral) inference
        parameters = {
            "max_new_tokens": 50
        }
        payload = {
            "inputs": llama_generated_text,
            "parameters": parameters
        }
        mistral_response = self.client.invoke_endpoint(
            EndpointName=self.endpoint_name,
            Body=json.dumps(payload),
            ContentType="application/json",
            InferenceComponentName=self.component_names[1]
        )
        return {"generated_text": json.loads(mistral_response.get('Body').read())['generated_text']}
    
    def handle(self, data, context=None):
        return self._invoke_workflow(data) 
 
This code performs the following functions: 
 
 Defines the orchestration that sequentially calls two models using their inference component names 
 Processes the response from the first model before passing it to the second model 
 Returns the final generated response 
 
This plain Python approach provides flexibility and control over the request-response flow, enabling seamless cascading of outputs across multiple model components. 
Build and deploy the workflow 
To deploy the workflow, we first create our inference components and then build the custom workflow. One inference component will host a Meta Llama 3.1 8B model, and the other will host a Mistral 7B model. 
 
  
  from sagemaker.serve import ModelBuilder
from sagemaker.serve.builder.schema_builder import SchemaBuilder

# Create a ModelBuilder instance for Llama 3.1 8B
# Pre-benchmarked ResourceRequirements will be taken from JumpStart, as Llama-3.1-8b is a supported model.
llama_model_builder = ModelBuilder(
    model="meta-textgeneration-llama-3-1-8b",
    schema_builder=SchemaBuilder(sample_input, sample_output),
    inference_component_name=llama_ic_name,
    instance_type="ml.g5.24xlarge"
)

# Create a ModelBuilder instance for Mistral 7B model.
mistral_mb = ModelBuilder(
    model="huggingface-llm-mistral-7b",
    instance_type="ml.g5.24xlarge",
    schema_builder=SchemaBuilder(sample_input, sample_output),
    inference_component_name=mistral_ic_name,
    resource_requirements=ResourceRequirements(
        requests={
           "memory": 49152,
           "num_accelerators": 2,
           "copies": 1
        }
    ),
    instance_type="ml.g5.24xlarge"
) 
  
 
Now we can tie it all together to create one more ModelBuilder to which we pass the modelbuilder_list, which contains the ModelBuilder objects we just created for each inference component and the custom workflow. Then we call the build() function to prepare the workflow for deployment. 
 
  
  # Create workflow ModelBuilder
orchestrator= ModelBuilder(
    inference_spec=PythonCustomInferenceEntryPoint(
        region_name=region,
        endpoint_name=llama_mistral_endpoint_name,
        component_names=[llama_ic_name, mistral_ic_name],
    ),
    dependencies={
        "auto": False,
        "custom": [
            "cloudpickle",
            "graphene",
            # Define other dependencies here.
        ],
    },
    sagemaker_session=Session(),
    role_arn=role,
    resource_requirements=ResourceRequirements(
        requests={
           "memory": 4096,
           "num_accelerators": 1,
           "copies": 1,
           "num_cpus": 2
        }
    ),
    name=custom_workflow_name, # Endpoint name for your custom workflow
    schema_builder=SchemaBuilder(sample_input={"inputs": "test"}, sample_output="Test"),
    modelbuilder_list=[llama_model_builder, mistral_mb] # Inference Component ModelBuilders created in Step 2
)
# call the build function to prepare the workflow for deployment
orchestrator.build() 
  
 
In the preceding code snippet, you can comment out the section that defines the resource_requirements to have the custom workflow deployed on a separate endpoint instance, which can be a dedicated CPU instance to handle the custom workflow payload. 
By calling the deploy() function, we deploy the custom workflow and the inference components to your desired instance type, in this example ml.g5.24.xlarge. If you choose to deploy the custom workflow to a separate instance, by default, it will use the ml.c5.xlarge instance type. You can set inference_workflow_instance_type and inference_workflow_initial_instance_count to configure the instances required to host the custom workflow. 
 
  
  predictors = orchestrator.deploy(
    instance_type="ml.g5.24xlarge",
    initial_instance_count=1,
    accept_eula=True, # Required for Llama3
    endpoint_name=llama_mistral_endpoint_name
    # inference_workflow_instance_type="ml.t2.medium", # default
    # inference_workflow_initial_instance_count=1 # default
) 
  
 
Invoke the endpoint 
After you deploy the workflow, you can invoke the endpoint using the predictor object: 
 
 from sagemaker.serializers import JSONSerializer
predictors[-1].serializer = JSONSerializer()
predictors[-1].predict("Tell me a story about ducks.") 
 
You can also invoke each inference component in the deployed endpoint. For example, we can test the Llama inference component with a synchronous invocation, and Mistral with streaming: 
 
  
  from sagemaker.predictor import Predictor
# create predictor for the inference component of Llama model
llama_predictor = Predictor(endpoint_name=llama_mistral_endpoint_name, component_name=llama_ic_name)
llama_predictor.content_type = "application/json"

llama_predictor.predict(json.dumps(payload)) 
  
 
When handling the streaming response, we need to read each line of the output separately. The following example code demonstrates this streaming handling by checking for newline characters to separate and print each token in real time: 
 
  
  mistral_predictor = Predictor(endpoint_name=llama_mistral_endpoint_name, component_name=mistral_ic_name)
mistral_predictor.content_type = "application/json"

body = json.dumps({
    "inputs": prompt,
    # specify the parameters as needed
    "parameters": parameters
})

for line in mistral_predictor.predict_stream(body):
    decoded_line = line.decode('utf-8')
    if '\n' in decoded_line:
        # Split by newline to handle multiple tokens in the same line
        tokens = decoded_line.split('\n')
        for token in tokens[:-1]:  # Print all tokens except the last one with a newline
            print(token)
        # Print the last token without a newline, as it might be followed by more tokens
        print(tokens[-1], end='')
    else:
        # Print the token without a newline if it doesn't contain '\n'
        print(decoded_line, end='') 
  
 
So far, we have walked through the example code to demonstrate how to build complex inference logic using Python orchestration, deploy them to SageMaker endpoints, and invoke them for real-time inference. The Python SDK automatically handles the following: 
 
 Model packaging and container configuration 
 Dependency management and environment setup 
 Endpoint creation and component coordination 
 
Whether you‚Äôre building a simple workflow of two models or a complex multimodal application, the new SDK provides the building blocks needed to bring your inference workflows to life with minimal boilerplate code. 
Customer story: Amazon Search 
Amazon Search is a critical component of the Amazon shopping experience, processing an enormous volume of queries across billions of products across diverse categories. At the core of this system are sophisticated matching and ranking workflows, which determine the order and relevance of search results presented to customers. These workflows execute large deep learning models in predefined sequences, often sharing models across different workflows to improve price-performance and accuracy. This approach makes sure that whether a customer is searching for electronics, fashion items, books, or other products, they receive the most pertinent results tailored to their query. 
The SageMaker Python SDK enhancement offers valuable capabilities that align well with Amazon Search‚Äôs requirements for these ranking workflows. It provides a standard interface for developing and deploying complex inference workflows crucial for effective search result ranking. The enhanced Python SDK enables efficient reuse of shared models across multiple ranking workflows while maintaining the flexibility to customize logic for specific product categories. Importantly, it allows individual models within these workflows to scale independently, providing optimal resource allocation and performance based on varying demand across different parts of the search system. 
Amazon Search&nbsp;is exploring the broad adoption of these Python SDK enhancements across their search ranking infrastructure. This initiative aims to further refine and improve search capabilities, enabling the team to build, version, and catalog workflows that power search ranking more effectively across different product categories. The ability to share models across workflows and scale them independently offers new levels of efficiency and adaptability in managing the complex search ecosystem. 
Vaclav Petricek, Sr. Manager of Applied Science at Amazon Search, highlighted the potential impact of these SageMaker Python SDK enhancements: ‚ÄúThese capabilities represent a significant advancement in our ability to develop and deploy sophisticated inference workflows that power search matching and ranking. The flexibility to build workflows using Python, share models across workflows, and scale them independently is particularly exciting, as it opens up new possibilities for optimizing our search infrastructure and rapidly iterating on our matching and ranking algorithms as well as new AI features. Ultimately, these SageMaker Inference enhancements will allow us to more efficiently create and manage the complex algorithms powering Amazon‚Äôs search experience, enabling us to deliver even more relevant results to our customers.‚Äù 
The following diagram illustrates a sample solution architecture used by Amazon Search. 
 
Clean up 
When you‚Äôre done testing the models, as a best practice, delete the endpoint to save costs if the endpoint is no longer required. You can follow the cleanup section the demo notebook or use following code to delete the model and endpoint created by the demo: 
 
  
  mistral_predictor.delete_predictor()
llama_predictor.delete_predictor()
llama_predictor.delete_endpoint()
workflow_predictor.delete_predictor() 
  
 
Conclusion 
The new SageMaker Python SDK enhancements for inference workflows mark a significant advancement in the development and deployment of complex AI inference workflows. By abstracting the underlying complexities, these enhancements empower inference customers to focus on innovation rather than infrastructure management. This feature bridges sophisticated AI applications with the robust SageMaker infrastructure, enabling developers to use familiar Python-based tools while harnessing the powerful inference capabilities of SageMaker. 
Early adopters, including Amazon Search, are already exploring how these capabilities can drive major improvements in AI-powered customer experiences across diverse industries. We invite all SageMaker users to explore this new functionality, whether you‚Äôre developing classic ML models, building generative AI applications or multi-model workflows, or tackling multi-step inference scenarios. The enhanced SDK provides the flexibility, ease of use, and scalability needed to bring your ideas to life. As AI continues to evolve, SageMaker Inference evolves with it, providing you with the tools to stay at the forefront of innovation. Start building your next-generation AI inference workflows today with the enhanced SageMaker Python SDK. 
 
About the authors 
Melanie Li, PhD, is a Senior Generative AI Specialist Solutions Architect at AWS based in Sydney, Australia, where her focus is on working with customers to build solutions leveraging state-of-the-art AI and machine learning tools. She has been actively involved in multiple Generative AI initiatives across APJ, harnessing the power of Large Language Models (LLMs). Prior to joining AWS, Dr. Li held data science roles in the financial and retail industries. 
Saurabh Trikande is a Senior Product Manager for Amazon Bedrock and SageMaker Inference. He is passionate about working with customers and partners, motivated by the goal of democratizing AI. He focuses on core challenges related to deploying complex AI applications, inference with multi-tenant models, cost optimizations, and making the deployment of Generative AI models more accessible. In his spare time, Saurabh enjoys hiking, learning about innovative technologies, following TechCrunch, and spending time with his family. 
Osho Gupta&nbsp;is a Senior Software Developer at AWS SageMaker. He is passionate about ML infrastructure space, and is motivated to learn &amp; advance underlying technologies that optimize Gen AI training &amp; inference performance. In his spare time, Osho enjoys paddle boarding, hiking, traveling, and spending time with his friends &amp; family. 
Joseph Zhang is a software engineer at AWS. He started his AWS career at EC2 before eventually transitioning to SageMaker, and now works on developing GenAI-related features. Outside of work he enjoys both playing and watching sports (go Warriors!), spending time with family, and making coffee. 
Gary Wang is a Software Developer at AWS SageMaker. He is passionate about AI/ML operations and building new things. In his spare time, Gary enjoys running, hiking, trying new food, and spending time with his friends and family. 
James Park&nbsp;is a Solutions Architect at Amazon Web Services. He works with Amazon.com to design, build, and deploy technology solutions on AWS, and has a particular interest in AI and machine learning. In h is spare time he enjoys seeking out new cultures, new experiences, &nbsp;and staying up to date with the latest technology trends. You can find him on LinkedIn. 
Vaclav Petricek is a Senior Applied Science Manager at Amazon Search, where he led teams that built Amazon Rufus and now leads science and engineering teams that work on the next generation of Natural Language Shopping. He is passionate about shipping AI experiences that make people‚Äôs lives better. Vaclav loves off-piste skiing, playing tennis, and backpacking with his wife and three children. 
Wei Li is a Senior Software Dev Engineer in Amazon Search. She is passionate about Large Language Model training and inference technologies, and loves integrating these solutions into Search Infrastructure to enhance natural language shopping experiences. During her leisure time, she enjoys gardening, painting, and reading. 
Brian Granger is a Senior Principal Technologist at Amazon Web Services and a professor of physics and data science at Cal Poly State University in San Luis Obispo, CA. He works at the intersection of UX design and engineering on tools for scientific computing, data science, machine learning, and data visualization. Brian is a co-founder and leader of Project Jupyter, co-founder of the Altair project for statistical visualization, and creator of the PyZMQ project for ZMQ-based message passing in Python. At AWS he is a technical and open source leader in the AI/ML organization. Brian also represents AWS as a board member of the PyTorch Foundation. He is a winner of the 2017 ACM Software System Award and the 2023 NASA Exceptional Public Achievement Medal for his work on Project Jupyter. He has a Ph.D. in theoretical physics from the University of Colorado.
‚Ä¢ Context extraction from image files in Amazon Q Business using LLMs
  To effectively convey complex information, organizations increasingly rely on visual documentation through diagrams, charts, and technical illustrations. Although text documents are well-integrated into modern knowledge management systems, rich information contained in diagrams, charts, technical schematics, and visual documentation often remains inaccessible to search and AI assistants. This creates significant gaps in organizational knowledge bases, leading to interpreting visual data manually and preventing automation systems from using critical visual information for comprehensive insights and decision-making. While Amazon Q Business already handles embedded images within documents, the custom document enrichment (CDE) feature extends these capabilities significantly by processing standalone image files (for example, JPGs and PNGs). 
In this post, we look at a step-by-step implementation for using the CDE feature within an Amazon Q Business application. We walk you through an AWS Lambda function configured within CDE to process various image file types, and we showcase an example scenario of how this integration enhances the Amazon Q Business ability to provide comprehensive insights. By following this practical guide, you can significantly expand your organization‚Äôs searchable knowledge base, enabling more complete answers and insights that incorporate both textual and visual information sources. 
Example scenario: Analyzing regional educational demographics 
Consider a scenario where you‚Äôre working for a national educational consultancy that has charts, graphs, and demographic data across different AWS Regions stored in an Amazon Simple Storage Service (Amazon S3) bucket. The following image shows student distribution by age range across various cities using a bar chart. The insights in visualizations like this are valuable for decision-making but traditionally locked within image formats in your S3 buckets and other storage. 
With Amazon Q Business and CDE, we show you how to enable natural language queries against such visualizations. For example, your team could ask questions such as ‚ÄúWhich city has the highest number of students in the 13‚Äì15 age range?‚Äù or ‚ÄúCompare the student demographics between City 1 and City 4‚Äù directly through the Amazon Q Business application interface. 
 
You can bridge this gap using the Amazon Q Business CDE feature to: 
 
 Detect and process image files during the document ingestion process 
 Use Amazon Bedrock with AWS Lambda to interpret the visual information 
 Extract structured data and insights from charts and graphs 
 Make this information searchable using natural language queries 
 
Solution overview 
In this solution, we walk you through how to implement a CDE-based solution for your educational demographic data visualizations. The solution empowers organizations to extract meaningful information from image files using the CDE capability of Amazon Q Business. When Amazon Q Business encounters the S3 path during ingestion, CDE rules automatically trigger a Lambda function. The Lambda function identifies the image files and calls the Amazon Bedrock API, which uses multimodal large language models (LLMs) to analyze and extract contextual information from each image. The extracted text is then seamlessly integrated into the knowledge base in Amazon Q Business. End users can then quickly search for valuable data and insights from images based on their actual context. By bridging the gap between visual content and searchable text, this solution helps organizations unlock valuable insights previously hidden within their image repositories. 
The following figure shows the high-level architecture diagram used for this solution. 
 
For this use case, we use Amazon S3 as our data source. However, this same solution is adaptable to other data source types supported by Amazon Q Business, or it can be implemented with custom data sources as needed.To complete the solution, follow these high-level implementation steps: 
 
 Create an Amazon Q Business application and sync with an S3 bucket. 
 Configure the Amazon Q Business application CDE for the Amazon S3 data source. 
 Extract context from the images. 
 
Prerequisites 
The following prerequisites are needed for implementation: 
 
 An AWS account. 
 At least one Amazon Q Business Pro user that has admin permissions to set up and configure Amazon Q Business. For pricing information, refer to Amazon Q Business pricing. 
 AWS Identity and Access Management (IAM) permissions to create and manage IAM roles and policies. 
 A supported data source to connect, such as an S3 bucket containing your public documents. 
 Access to an Amazon Bedrock LLM in the required AWS Region. 
 
Create an Amazon Q Business application and sync with an S3 bucket 
To create an Amazon Q Business application and connect it to your S3 bucket, complete the following steps. These steps provide a general overview of how to create an Amazon Q Business application and synchronize it with an S3 bucket. For more comprehensive, step-by-step guidance, follow the detailed instructions in the blog post Discover insights from Amazon S3 with Amazon Q S3 connector. 
 
 Initiate your application setup through either the AWS Management Console or AWS Command Line Interface (AWS CLI). 
 Create an index for your Amazon Q Business application. 
 Use the built-in Amazon S3 connector to link your application with documents stored in your organization‚Äôs S3 buckets. 
 
Configure the Amazon Q Business application CDE for the Amazon S3 data source 
With the CDE feature of Amazon Q Business, you can make the most of your Amazon S3 data sources by using the sophisticated capabilities to modify, enhance, and filter documents during the ingestion process, ultimately making enterprise content more discoverable and valuable. When connecting Amazon Q Business to S3 repositories, you can use CDE to seamlessly transform your raw data, applying modifications that significantly improve search quality and information accessibility. This powerful functionality extends to extracting context from binary files such as images through integration with Amazon Bedrock services, enabling organizations to unlock insights from previously inaccessible content formats. By implementing CDE for Amazon S3 data sources, businesses can maximize the utility of their enterprise data within Amazon Q, creating a more comprehensive and intelligent knowledge base that responds effectively to user queries.To configure the Amazon Q Business application CDE for the Amazon S3 data source, complete the following steps: 
 
 Select your application and navigate to Data sources. 
 Choose your existing Amazon S3 data source or create a new one. Verify that Audio/Video under Multi-media content configuration is not enabled. 
 In the data source configuration, locate the Custom Document Enrichment section. 
 Configure the pre-extraction rules to trigger a Lambda function when specific S3 bucket conditions are satisfied. Check the following screenshot for an example configuration. 
 
 Pre-extraction rules are executed before Amazon Q Business processes files from your S3 bucket. 
Extract context from the images 
To extract insights from an image file, the Lambda function makes an Amazon Bedrock API call using Anthropic‚Äôs Claude 3.7 Sonnet model. You can modify the code to use other Amazon Bedrock models based on your use case. 
Constructing the prompt is a critical piece of the code. We recommend trying various prompts to get the desired output for your use case. Amazon Bedrock offers the capability to optimize a prompt that you can use to enhance your use case specific input. 
Examine the following Lambda function code snippets, written in Python, to understand the Amazon Bedrock model setup along with a sample prompt to extract insights from an image. 
In the following code snippet, we start by importing relevant Python libraries, define constants, and initialize AWS SDK for Python (Boto3) clients for Amazon S3 and Amazon Bedrock runtime. For more information, refer to the Boto3 documentation. 
 
 import boto3
import logging
import json
from typing import List, Dict, Any
from botocore.config import Config

MODEL_ID = "us.anthropic.claude-3-7-sonnet-20250219-v1:0"
MAX_TOKENS = 2000
MAX_RETRIES = 2
FILE_FORMATS = ("jpg", "jpeg", "png")

logger = logging.getLogger()
logger.setLevel(logging.INFO)
s3 = boto3.client('s3')
bedrock = boto3.client('bedrock-runtime', config=Config(read_timeout=3600, region_name='us-east-1')) 
 
The prompt passed to the Amazon Bedrock model, Anthropic‚Äôs Claude 3.7 Sonnet in this case, is broken into two parts: prompt_prefix and prompt_suffix. The prompt breakdown makes it more readable and manageable. Additionally, the Amazon Bedrock prompt caching feature can be used to reduce response latency as well as input token cost. You can modify the prompt to extract information based on your specific use case as needed. 
 
 prompt_prefix = """You are an expert image reader tasked with generating detailed descriptions for various """
"""types of images. These images may include technical diagrams,"""
""" graphs and charts, categorization diagrams, data flow and process flow diagrams,"""
""" hierarchical and timeline diagrams, infographics, """
"""screenshots and product diagrams/images from user manuals. """
""" The description of these images needs to be very detailed so that user can ask """
""" questions based on the image, which can be answered by only looking at the descriptions """
""" that you generate.
Here is the image you need to analyze:

&lt;image&gt;
"""

prompt_suffix = """
&lt;/image&gt;

Please follow these steps to analyze the image and generate a comprehensive description:

1. Image type: Classify the image as one of technical diagrams, graphs and charts, categorization diagrams, data flow and process flow diagrams, hierarchical and timeline diagrams, infographics, screenshots and product diagrams/images from user manuals. The description of these images needs to be very detailed so that user can ask questions based on the image, which can be answered by only looking at the descriptions that you generate or other.

2. Items:
&nbsp;&nbsp; Carefully examine the image and extract all entities, texts, and numbers present. List these elements in &lt;image_items&gt; tags.

3. Detailed Description:
&nbsp;&nbsp; Using the information from the previous steps, provide a detailed description of the image. This should include the type of diagram or chart, its main purpose, and how the various elements interact or relate to each other. &nbsp;Capture all the crucial details that can be used to answer any followup questions. Write this description in &lt;image_description&gt; tags.

4. Data Estimation (for charts and graphs only):
&nbsp;&nbsp; If the image is a chart or graph, capture the data in the image in CSV format to be able to recreate the image from the data. Ensure your response captures all relevant details from the chart that might be necessary to answer any follow up questions from the chart.
&nbsp;&nbsp; If exact values cannot be inferred, provide an estimated range for each value in &lt;estimation&gt; tags.
&nbsp;&nbsp; If no data is present, respond with "No data found".

Present your analysis in the following format:

&lt;analysis&gt;
&lt;image_type&gt;
[Classify the image type here]
&lt;/image_type&gt;

&lt;image_items&gt;
[List all extracted entities, texts, and numbers here]
&lt;/image_items&gt;

&lt;image_description&gt;
[Provide a detailed description of the image here]
&lt;/image_description&gt;

&lt;data&gt;
[If applicable, provide estimated number ranges for chart elements here]
&lt;/data&gt;
&lt;/analysis&gt;

Remember to be thorough and precise in your analysis. If you're unsure about any aspect of the image, state your uncertainty clearly in the relevant section.
"""
 
 
The lambda_handler is the main entry point for the Lambda function. While invoking this Lambda function, the CDE passes the data source‚Äôs information within event object input. In this case, the S3 bucket and the S3 object key are retrieved from the event object along with the file format. Further processing of the input happens only if the file_format matches the expected file types. For production ready code, implement proper error handling for unexpected errors. 
 
 def lambda_handler(event, context):
&nbsp;&nbsp; &nbsp;logger.info("Received event: %s" % json.dumps(event))
&nbsp;&nbsp; &nbsp;s3Bucket = event.get("s3Bucket")
&nbsp;&nbsp; &nbsp;s3ObjectKey = event.get("s3ObjectKey")
&nbsp;&nbsp; &nbsp;metadata = event.get("metadata")
&nbsp;&nbsp; &nbsp;file_format = s3ObjectKey.lower().split('.')[-1]
&nbsp;&nbsp; &nbsp;new_key = 'cde_output/' + s3ObjectKey + '.txt'
&nbsp;&nbsp; &nbsp;if (file_format in FILE_FORMATS):
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;afterCDE = generate_image_description(s3Bucket, s3ObjectKey, file_format)
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;s3.put_object(Bucket = s3Bucket, Key = new_key, Body=afterCDE)
&nbsp;&nbsp; &nbsp;return {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"version" : "v0",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"s3ObjectKey": new_key,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"metadataUpdates": []
&nbsp;&nbsp; &nbsp;} 
 
The generate_image_description function calls two other functions: first to construct the message that is passed to the Amazon Bedrock model and second to invoke the model. It returns the final text output extracted from the image file by the model invocation. 
 
 def generate_image_description(s3Bucket: str, s3ObjectKey: str, file_format: str) -&gt; str:
&nbsp;&nbsp; &nbsp;"""
&nbsp;&nbsp; &nbsp;Generate a description for an image.
&nbsp;&nbsp; &nbsp;Inputs:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;image_file: str - Path to the image file
&nbsp;&nbsp; &nbsp;Output:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;str - Generated image description
&nbsp;&nbsp; &nbsp;"""
&nbsp;&nbsp; &nbsp;messages = _llm_input(s3Bucket, s3ObjectKey, file_format)
&nbsp;&nbsp; &nbsp;response = _invoke_model(messages)
&nbsp;&nbsp; &nbsp;return response['output']['message']['content'][0]['text']
 
 
The _llm_input function takes in the S3 object‚Äôs details passed as input along with the file type (png, jpg) and builds the message in the format expected by the model invoked by Amazon Bedrock. 
 
 def _llm_input(s3Bucket: str, s3ObjectKey: str, file_format: str) -&gt; List[Dict[str, Any]]:
&nbsp;&nbsp; &nbsp;s3_response = s3.get_object(Bucket = s3Bucket, Key = s3ObjectKey)
&nbsp;&nbsp; &nbsp;image_content = s3_response['Body'].read()
&nbsp;&nbsp; &nbsp;message = {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"role": "user",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"content": [
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;{"text": prompt_prefix},
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;{
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"image": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"format": file_format,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"source": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"bytes": image_content
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;},
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;{"text": prompt_suffix}
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;]
&nbsp;&nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp;return [message]
 
 
The _invoke_model function calls the converse API using the Amazon Bedrock runtime client. This API returns the response generated by the model. The values within inferenceConfig settings for maxTokens and temperature are used to limit the length of the response and make the responses more deterministic (less random) respectively. 
 
 def _invoke_model(messages: List[Dict[str, Any]]) -&gt; Dict[str, Any]:
&nbsp;&nbsp; &nbsp;"""
&nbsp;&nbsp; &nbsp;Call the Bedrock model with retry logic.
&nbsp;&nbsp; &nbsp;Input:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;messages: List[Dict[str, Any]] - Prepared messages for the model
&nbsp;&nbsp; &nbsp;Output:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;Dict[str, Any] - Model response
&nbsp;&nbsp; &nbsp;"""
&nbsp;&nbsp; &nbsp;for attempt in range(MAX_RETRIES):
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;try:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;response = bedrock.converse(
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;modelId=MODEL_ID,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;messages=messages,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;inferenceConfig={
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"maxTokens": MAX_TOKENS,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"temperature": 0,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;)
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;return response
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;except Exception as e:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;print(e)
&nbsp;&nbsp; &nbsp;
&nbsp;&nbsp; &nbsp;raise Exception(f"Failed to call model after {MAX_RETRIES} attempts") 
 
Putting all the preceding code pieces together, the full Lambda function code is shown in the following block: 
 
 # Example Lambda function for image processing
import boto3
import logging
import json
from typing import List, Dict, Any
from botocore.config import Config

MODEL_ID = "us.anthropic.claude-3-7-sonnet-20250219-v1:0"
MAX_TOKENS = 2000
MAX_RETRIES = 2
FILE_FORMATS = ("jpg", "jpeg", "png")

logger = logging.getLogger()
logger.setLevel(logging.INFO)
s3 = boto3.client('s3')
bedrock = boto3.client('bedrock-runtime', config=Config(read_timeout=3600, region_name='us-east-1'))

prompt_prefix = """You are an expert image reader tasked with generating detailed descriptions for various """
"""types of images. These images may include technical diagrams,"""
""" graphs and charts, categorization diagrams, data flow and process flow diagrams,"""
""" hierarchical and timeline diagrams, infographics, """
"""screenshots and product diagrams/images from user manuals. """
""" The description of these images needs to be very detailed so that user can ask """
""" questions based on the image, which can be answered by only looking at the descriptions """
""" that you generate.
Here is the image you need to analyze:

&lt;image&gt;
"""

prompt_suffix = """
&lt;/image&gt;

Please follow these steps to analyze the image and generate a comprehensive description:

1. Image type: Classify the image as one of technical diagrams, graphs and charts, categorization diagrams, data flow and process flow diagrams, hierarchical and timeline diagrams, infographics, screenshots and product diagrams/images from user manuals. The description of these images needs to be very detailed so that user can ask questions based on the image, which can be answered by only looking at the descriptions that you generate or other.

2. Items:
&nbsp;&nbsp; Carefully examine the image and extract all entities, texts, and numbers present. List these elements in &lt;image_items&gt; tags.

3. Detailed Description:
&nbsp;&nbsp; Using the information from the previous steps, provide a detailed description of the image. This should include the type of diagram or chart, its main purpose, and how the various elements interact or relate to each other. &nbsp;Capture all the crucial details that can be used to answer any followup questions. Write this description in &lt;image_description&gt; tags.

4. Data Estimation (for charts and graphs only):
&nbsp;&nbsp; If the image is a chart or graph, capture the data in the image in CSV format to be able to recreate the image from the data. Ensure your response captures all relevant details from the chart that might be necessary to answer any follow up questions from the chart.
&nbsp;&nbsp; If exact values cannot be inferred, provide an estimated range for each value in &lt;estimation&gt; tags.
&nbsp;&nbsp; If no data is present, respond with "No data found".

Present your analysis in the following format:

&lt;analysis&gt;
&lt;image_type&gt;
[Classify the image type here]
&lt;/image_type&gt;

&lt;image_items&gt;
[List all extracted entities, texts, and numbers here]
&lt;/image_items&gt;

&lt;image_description&gt;
[Provide a detailed description of the image here]
&lt;/image_description&gt;

&lt;data&gt;
[If applicable, provide estimated number ranges for chart elements here]
&lt;/data&gt;
&lt;/analysis&gt;

Remember to be thorough and precise in your analysis. If you're unsure about any aspect of the image, state your uncertainty clearly in the relevant section.
"""

def _llm_input(s3Bucket: str, s3ObjectKey: str, file_format: str) -&gt; List[Dict[str, Any]]:
&nbsp;&nbsp; &nbsp;s3_response = s3.get_object(Bucket = s3Bucket, Key = s3ObjectKey)
&nbsp;&nbsp; &nbsp;image_content = s3_response['Body'].read()
&nbsp;&nbsp; &nbsp;message = {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"role": "user",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"content": [
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;{"text": prompt_prefix},
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;{
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"image": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"format": file_format,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"source": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"bytes": image_content
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;},
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;{"text": prompt_suffix}
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;]
&nbsp;&nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp;return [message]

def _invoke_model(messages: List[Dict[str, Any]]) -&gt; Dict[str, Any]:
&nbsp;&nbsp; &nbsp;"""
&nbsp;&nbsp; &nbsp;Call the Bedrock model with retry logic.
&nbsp;&nbsp; &nbsp;Input:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;messages: List[Dict[str, Any]] - Prepared messages for the model
&nbsp;&nbsp; &nbsp;Output:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;Dict[str, Any] - Model response
&nbsp;&nbsp; &nbsp;"""
&nbsp;&nbsp; &nbsp;for attempt in range(MAX_RETRIES):
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;try:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;response = bedrock.converse(
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;modelId=MODEL_ID,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;messages=messages,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;inferenceConfig={
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"maxTokens": MAX_TOKENS,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"temperature": 0,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;)
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;return response
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;except Exception as e:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;print(e)
&nbsp;&nbsp; &nbsp;
&nbsp;&nbsp; &nbsp;raise Exception(f"Failed to call model after {MAX_RETRIES} attempts")

def generate_image_description(s3Bucket: str, s3ObjectKey: str, file_format: str) -&gt; str:
&nbsp;&nbsp; &nbsp;"""
&nbsp;&nbsp; &nbsp;Generate a description for an image.
&nbsp;&nbsp; &nbsp;Inputs:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;image_file: str - Path to the image file
&nbsp;&nbsp; &nbsp;Output:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;str - Generated image description
&nbsp;&nbsp; &nbsp;"""
&nbsp;&nbsp; &nbsp;messages = _llm_input(s3Bucket, s3ObjectKey, file_format)
&nbsp;&nbsp; &nbsp;response = _invoke_model(messages)
&nbsp;&nbsp; &nbsp;return response['output']['message']['content'][0]['text']

def lambda_handler(event, context):
&nbsp;&nbsp; &nbsp;logger.info("Received event: %s" % json.dumps(event))
&nbsp;&nbsp; &nbsp;s3Bucket = event.get("s3Bucket")
&nbsp;&nbsp; &nbsp;s3ObjectKey = event.get("s3ObjectKey")
&nbsp;&nbsp; &nbsp;metadata = event.get("metadata")
&nbsp;&nbsp; &nbsp;file_format = s3ObjectKey.lower().split('.')[-1]
&nbsp;&nbsp; &nbsp;new_key = 'cde_output/' + s3ObjectKey + '.txt'
&nbsp;&nbsp; &nbsp;if (file_format in FILE_FORMATS):
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;afterCDE = generate_image_description(s3Bucket, s3ObjectKey, file_format)
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;s3.put_object(Bucket = s3Bucket, Key = new_key, Body=afterCDE)
&nbsp;&nbsp; &nbsp;return {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"version" : "v0",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"s3ObjectKey": new_key,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"metadataUpdates": []
&nbsp;&nbsp; &nbsp;} 
 
We strongly recommend testing and validating code in a nonproduction environment before deploying it to production. In addition to Amazon Q pricing, this solution will incur charges for AWS Lambda and Amazon Bedrock. For more information, refer to AWS Lambda pricing and Amazon Bedrock pricing. 
After the Amazon S3 data is synced with the Amazon Q index, you can prompt the Amazon Q Business application to get the extracted insights as shown in the following section. 
Example prompts and results 
The following question and answer pairs refer the Student Age Distribution graph at the beginning of this post. 
Q: Which City has the highest number of students in the 13-15 age range? 
 
Q: Compare the student demographics between City 1 and City 4? 
 
In the original graph, the bars representing student counts lacked explicit numerical labels, which could make data interpretation challenging on a scale. However, with Amazon Q Business and its integration capabilities, this limitation can be overcome. By using Amazon Q Business to process these visualizations with Amazon Bedrock LLMs using the CDE feature, we‚Äôve enabled a more interactive and insightful analysis experience. The service effectively extracts the contextual information embedded in the graph, even when explicit labels are absent. This powerful combination means that end users can ask questions about the visualization and receive responses based on the underlying data. Rather than being limited by what‚Äôs explicitly labeled in the graph, users can now explore deeper insights through natural language queries. This capability demonstrates how Amazon Q Business transforms static visualizations into queryable knowledge assets, enhancing the value of your existing data visualizations without requiring additional formatting or preparation work. 
Best practices for Amazon S3 CDE configuration 
When setting up CDE for your Amazon S3 data source, consider these best practices: 
 
 Use conditional rules to only process specific file types that need transformation. 
 Monitor Lambda execution with Amazon CloudWatch to track processing errors and performance. 
 Set appropriate timeout values for your Lambda functions, especially when processing large files. 
 Consider incremental syncing to process only new or modified documents in your S3 bucket. 
 Use document attributes to track which documents have been processed by CDE. 
 
Cleanup 
Complete the following steps to clean up your resources: 
 
 Go to the Amazon Q Business application and select Remove and unsubscribe for users and groups. 
 Delete the Amazon Q Business application. 
 Delete the Lambda function. 
 Empty and delete the S3 bucket. For instructions, refer to Deleting a general purpose bucket. 
 
Conclusion 
This solution demonstrates how combining Amazon Q Business, custom document enrichment, and Amazon Bedrock can transform static visualizations into queryable knowledge assets, significantly enhancing the value of existing data visualizations without additional formatting work. By using these powerful AWS services together, organizations can bridge the gap between visual information and actionable insights, enabling users to interact with different file types in more intuitive ways. 
Explore What is Amazon Q Business? and Getting started with Amazon Bedrock in the documentation to implement this solution for your specific use cases and unlock the potential of your visual data. 
About the Authors 
 
About the authors 
Amit Chaudhary Amit Chaudhary is a Senior Solutions Architect at Amazon Web Services. His focus area is AI/ML, and he helps customers with generative AI, large language models, and prompt engineering. Outside of work, Amit enjoys spending time with his family. 
Nikhil Jha Nikhil Jha is a Senior Technical Account Manager at Amazon Web Services. His focus areas include AI/ML, building Generative AI resources, and analytics. In his spare time, he enjoys exploring the outdoors with his family.
‚Ä¢ Build AWS architecture diagrams using Amazon Q CLI and MCP
  Creating professional AWS architecture diagrams is a fundamental task for solutions architects, developers, and technical teams. These diagrams serve as essential communication tools for stakeholders, documentation of compliance requirements, and blueprints for implementation teams. However, traditional diagramming approaches present several challenges: 
 
 Time-consuming process ‚Äì Creating detailed architecture diagrams manually can take hours or even days 
 Steep learning curve ‚Äì Learning specialized diagramming tools requires significant investment 
 Inconsistent styling ‚Äì Maintaining visual consistency across multiple diagrams is difficult 
 Outdated AWS icons ‚Äì Keeping up with the latest AWS service icons and best practices challenging. 
 Difficult maintenance ‚Äì Updating diagrams as architectures evolve can become increasingly burdensome 
 
Amazon Q Developer CLI with the Model Context Protocol (MCP) offers a streamlined approach to creating AWS architecture diagrams. By using generative AI through natural language prompts, architects can now generate professional diagrams in minutes rather than hours, while adhering to AWS best practices. 
In this post, we explore how to use Amazon Q Developer CLI with the AWS Diagram MCP and the AWS Documentation MCP servers to create sophisticated architecture diagrams that follow AWS best practices. We discuss techniques for basic diagrams and real-world diagrams, with detailed examples and step-by-step instructions. 
Solution overview 
Amazon Q Developer CLI is a command line interface that brings the generative AI capabilities of Amazon Q directly to your terminal. Developers can interact with Amazon Q through natural language prompts, making it an invaluable tool for various development tasks. 
Developed by Anthropic as an open protocol, the Model Context Protocol (MCP) provides a standardized way to connect AI models to virtually any data source or tool. Using a client-server architecture (as illustrated in the following diagram), the MCP helps developers expose their data through lightweight MCP servers while building AI applications as MCP clients that connect to these servers. 
The MCP uses a client-server architecture containing the following components: 
 
 Host ‚Äì A program or AI tool that requires access to data through the MCP protocol, such as Anthropic‚Äôs Claude Desktop, an integrated development environment (IDE), AWS MCP CLI, or other AI applications 
 Client ‚Äì Protocol clients that maintain one-to-one connections with server 
 Server ‚Äì Lightweight programs that expose capabilities through standardized MCP or act as tools 
 Data sources ‚Äì Local data sources such as databases and file systems, or external systems available over the internet through APIs (web APIs) that MCP servers can connect with 
 
 
As announced in April 2025, MCP enables Amazon Q Developer to connect with specialized servers that extend its capabilities beyond what‚Äôs possible with the base model alone. MCP servers act as plugins for Amazon Q, providing domain-specific knowledge and functionality. The AWS Diagram MCP server specifically enables Amazon Q to generate architecture diagrams using the Python diagrams package, with access to the complete AWS icon set and architectural best practices. 
Prerequisites 
To implement this solution, you must have an AWS account with appropriate permissions and follow the steps below. 
Set up your environment 
Before you can start creating diagrams, you need to set up your environment with Amazon Q CLI, the AWS Diagram MCP server, and AWS Documentation MCP server. This section provides detailed instructions for installation and configuration. 
Install Amazon Q Developer CLI 
Amazon Q Developer CLI is available as a standalone installation. Complete the following steps to install it: 
 
 Download and install Amazon Q Developer CLI. For instructions, see Using Amazon Q Developer on the command line. 
 Verify the installation by running the following command: q --version You should see output similar to the following: Amazon Q Developer CLI version 1.x.x 
 Configure Amazon Q CLI with your AWS credentials: q login 
 Choose the login method suitable for you: 
   
   Use for free with AWS Builder ID 
   Use with Pro license 
    
 
Set up MCP servers 
Complete the following steps to set up your MCP servers: 
 
 Install uv using the following command: pip install uv 
 Install Python 3.10 or newer: uv python install 3.10 
 Install GraphViz for your operating system. 
 Add the servers to your ~/.aws/amazonq/mcp.json file: 
 
{
  "mcpServers": {
    "awslabs.aws-diagram-mcp-server": {
      "command": "uvx",
      "args": ["awslabs.aws-diagram-mcp-server"],
      "env": {
        "FASTMCP_LOG_LEVEL": "ERROR"
      },
      "autoApprove": [],
      "disabled": false
    },
    "awslabs.aws-documentation-mcp-server": {
      "command": "uvx",
      "args": ["awslabs.aws-documentation-mcp-server@latest"],
      "env": {
        "FASTMCP_LOG_LEVEL": "ERROR"
      },
      "autoApprove": [],
      "disabled": false
    }
  }
}
 
Now, Amazon Q CLI automatically discovers MCP servers in the ~/.aws/amazonq/mcp.json file. 
Understanding MCP server tools 
The AWS Diagram MCP server provides several powerful tools: 
 
 list_icons ‚Äì Lists available icons from the diagrams package, organized by provider and service category 
 get_diagram_examples ‚Äì Provides example code for different types of diagrams (AWS, sequence, flow, class, and others) 
 generate_diagram ‚Äì Creates a diagram from Python code using the diagrams package 
 
The AWS Documentation MCP server provides the following useful tools: 
 
 search_documentation ‚Äì Searches AWS documentation using the official AWS Documentation Search API 
 read_documentation ‚Äì Fetches and converts AWS documentation pages to markdown format 
 recommend ‚Äì Gets content recommendations for AWS documentation pages 
 
These tools work together to help you create accurate architecture diagrams that follow AWS best practices. 
Test your setup 
Let‚Äôs verify that everything is working correctly by generating a simple diagram: 
 
 Start the Amazon Q CLI chat interface and verify the output shows the MCP servers being loaded and initialized: q chat  
 In the chat interface, enter the following prompt: Please create a diagram showing an EC2 instance in a VPC connecting to an external S3 bucket. Include essential networking components (VPC, subnets, Internet Gateway, Route Table), security elements (Security Groups, NACLs), and clearly mark the connection between EC2 and S3. Label everything appropriately concisely and indicate that all resources are in the us-east-1 region. Check for AWS documentation to ensure it adheres to AWS best practices before you create the diagram. 
 Amazon Q CLI will ask you to trust the tool that is being used; enter t to trust it.Amazon Q CLI will generate and display a simple diagram showing the requested architecture. Your diagram should look similar to the following screenshot, though there might be variations in layout, styling, or specific details because it‚Äôs created using generative AI. The core architectural components and relationships will be represented, but the exact visual presentation might differ slightly with each generation.  If you see the diagram, your environment is set up correctly. If you encounter issues, verify that Amazon Q CLI can access the MCP servers by making sure you installed the necessary tools and the servers are in the ~/.aws/amazonq/mcp.json file. 
 
Configuration options 
The AWS Diagram MCP server supports several configuration options to customize your diagramming experience: 
 
 Output directory ‚Äì By default, diagrams are saved in a generated-diagrams directory in your current working directory. You can specify a different location in your prompts. 
 Diagram format ‚Äì The default output format is PNG, but you can request other formats like SVG in your prompts. 
 Styling options ‚Äì You can specify colors, shapes, and other styling elements in your prompts. 
 
Now that our environment is set up, let‚Äôs create more diagrams. 
Create AWS architecture diagrams 
In this section, we walk through the process of multiple AWS architecture diagrams using Amazon Q CLI with the AWS Diagram MCP server and AWS Documentation MCP server to make sure our requirements follow best practices. 
When you provide a prompt to Amazon Q CLI, the AWS Diagram and Documentation MCP servers complete the following steps: 
 
 Interpret your requirements. 
 Check for best practices on the AWS documentation. 
 Generate Python code using the diagrams package. 
 Execute the code to create the diagram. 
 Return the diagram as an image. 
 
This process happens seamlessly, so you can focus on describing what you want rather than how to create it. 
AWS architecture diagrams typically include the following components: 
 
 Nodes ‚Äì AWS services and resources 
 Edges ‚Äì Connections between nodes showing relationships or data flow 
 Clusters ‚Äì Logical groupings of nodes, such as virtual private clouds (VPCs), subnets, and Availability Zones 
 Labels ‚Äì Text descriptions for nodes and connections 
 
Example 1: Create a web application architecture 
Let‚Äôs create a diagram for a simple web application hosted on AWS. Enter the following prompt: 
Create a diagram for a simple web application with an Application Load Balancer, two EC2 instances, and an RDS database. Check for AWS documentation to ensure it adheres to AWS best practices before you create the diagram 
 
 After you enter your prompt, Amazon Q CLI will search AWS documentation for best practices using the search_documentation tool from awslabsaws_documentation_mcp_server.  
  Following the search of the relevant AWS documentation, it will read the documentation using the read_documentation tool from the MCP server awslabsaws_documentation_mcp_server.  
 Amazon Q CLI will then list the needed AWS service icons using the list_icons tool, and will use generate_diagram with awslabsaws_diagram_mcp_server.  
 You should receive an output with a description of the diagram created based on the prompt along with the location of where the diagram was saved.  
 Amazon Q CLI will generate and display the diagram. 
  
 
The generated diagram shows the following key components: 
 
 An Application Load Balancer as the entry point 
 Two Amazon Elastic Compute Cloud (Amazon EC2) instances for the application tier 
 An Amazon Relational Database Service (Amazon RDS) instance for the database tier 
 Connections showing the flow of traffic 
 
Example 2: Create a multi-tier architecture 
Multi-tier architectures separate applications into functional layers (presentation, application, and data) to improve scalability and security. We use the following prompt to create our diagram: 
Create a diagram for a three-tier web application with a presentation tier (ALB and CloudFront), application tier (ECS with Fargate), and data tier (Aurora PostgreSQL). Include VPC with public and private subnets across multiple AZs. Check for AWS documentation to ensure it adheres to AWS best practices before you create the diagram. 
 
The diagram shows the following key components: 
 
 A presentation tier in public subnets 
 An application tier in private subnets 
 A data tier in isolated private subnets 
 Proper security group configurations 
 Traffic flow between tiers 
 
Example 3: Create a serverless architecture 
We use the following prompt to create a diagram for a serverless architecture: 
Create a diagram for a serverless web application using API Gateway, Lambda, DynamoDB, and S3 for static website hosting. Include Cognito for user authentication and CloudFront for content delivery. Check for AWS documentation to ensure it adheres to AWS best practices before you create the diagram. 
 
The diagram includes the following key components: 
 
 Amazon Simple Storage Service (Amazon S3) hosting static website content 
 Amazon CloudFront distributing content globally 
 Amazon API Gateway handling API requests 
 AWS Lambda functions implementing business logic 
 Amazon DynamoDB storing application data 
 Amazon Cognito managing user authentication 
 
Example 4: Create a data processing diagram 
We use the following prompt to create a diagram for a data processing pipeline: 
Create a diagram for a data processing pipeline with components organized in clusters for data ingestion, processing, storage, and analytics. Include Kinesis, Lambda, S3, Glue, and QuickSight. Check for AWS documentation to ensure it adheres to AWS best practices before you create the diagram. 
 
The diagram organizes components into distinct clusters: 
 
 Data ingestion ‚Äì Amazon Kinesis Data Streams, Amazon Data Firehose, Amazon Simple Queue Service 
 Data processing ‚Äì Lambda functions, AWS Glue jobs 
 Data storage ‚Äì S3 buckets, DynamoDB tables 
 Data analytics ‚Äì AWS Glue, Amazon Athena, Amazon QuickSight 
 
Real-world examples 
Let‚Äôs explore some real-world architecture patterns and how to create diagrams for them using Amazon Q CLI with the AWS Diagram MCP server. 
Ecommerce platform 
Ecommerce platforms require scalable, resilient architectures to handle variable traffic and maintain high availability. We use the following prompt to create an example diagram: 
Create a diagram for an e-commerce platform with microservices architecture. Include components for product catalog, shopping cart, checkout, payment processing, order management, and user authentication. Ensure the architecture follows AWS best practices for scalability and security. Check for AWS documentation to ensure it adheres to AWS best practices before you create the diagram. 
 
The diagram includes the following key components: 
 
 API Gateway as the entry point for client applications 
 Microservices implemented as containers in Amazon Elastic Container Service (Amazon ECS) with AWS Fargate 
 RDS databases for product catalog, shopping cart, and order data 
 Amazon ElastiCache for product data caching and session management 
 Amazon Cognito for authentication 
 Amazon Simple Queue Service (Amazon SQS) and Amazon Simple Notification Service (Amazon SNS) for asynchronous communication between services 
 CloudFront for content delivery and static assets from Amazon S3 
 Amazon Route 53 for DNS management 
 AWS WAF for web application security 
 AWS Lambda functions for serverless microservice implementation 
 AWS Secrets Manager for secure credential storage 
 Amazon CloudWatch for monitoring and observability 
 
Intelligent document processing solution 
We use the following prompt to create a diagram for an intelligent document processing (IDP) architecture: 
Create a diagram for an intelligent document processing (IDP) application on AWS. Include components for document ingestion, OCR and text extraction, intelligent data extraction (using NLP and/or computer vision), human review and validation, and data output/integration. Ensure the architecture follows AWS best practices for scalability and security, leveraging services like S3, Lambda, Textract, Comprehend, SageMaker (for custom models, if applicable), and potentially Augmented AI (A2I). Check for AWS documentation related to intelligent document processing best practices to ensure it adheres to AWS best practices before you create the diagram. 
 
The diagram includes the following key components: 
 
 Amazon API Gateway as the entry point for client applications, providing a secure and scalable interface 
 Microservices implemented as containers in ECS with Fargate, enabling flexible and scalable processing 
 Amazon RDS databases for product catalog, shopping cart, and order data, providing reliable structured data storage 
 Amazon ElastiCache for product data caching and session management, improving performance and user experience 
 Amazon Cognito for authentication, ensuring secure access control 
 Amazon Simple Queue Service and Amazon Simple Notification Service for asynchronous communication between services, enabling decoupled and resilient architecture 
 Amazon CloudFront for content delivery and static assets from S3, optimizing global performance 
 Amazon Route53 for DNS management, providing reliable routing 
 AWS WAF for web application security, protecting against common web exploits 
 AWS Lambda functions for serverless microservice implementation, offering cost-effective scaling 
 AWS Secrets Manager for secure credential storage, enhancing security posture 
 Amazon CloudWatch for monitoring and observability, providing insights into system performance and health. 
 
Clean up 
If you no longer need to use the AWS Cost Analysis MCP server with Amazon Q CLI, you can remove it from your configuration: 
 
 Open your ~/.aws/amazonq/mcp.json file. 
 Remove or comment out the MCP server entries. 
 Save the file. 
 
This will prevent the server from being loaded when you start Amazon Q CLI in the future. 
Conclusion 
In this post, we explored how to use Amazon Q CLI with the AWS Documentation MCP and AWS Diagram MCP servers to create professional AWS architecture diagrams that adhere to AWS best practices referenced from official AWS documentation. This approach offers significant advantages over traditional diagramming methods: 
 
 Time savings ‚Äì Generate complex diagrams in minutes instead of hours 
 Consistency ‚Äì Make sure diagrams follow the same style and conventions 
 Best practices ‚Äì Automatically incorporate AWS architectural guidelines 
 Iterative refinement ‚Äì Quickly modify diagrams through simple prompts 
 Validation ‚Äì Check architectures against official AWS documentation and recommendations 
 
As you continue your journey with AWS architecture diagrams, we encourage you to deepen your knowledge by learning more about the Model Context Protocol (MCP) to understand how it enhances the capabilities of Amazon Q. When seeking inspiration for your own designs, the AWS Architecture Center offers a wealth of reference architectures that follow best practices. For creating visually consistent diagrams, be sure to visit the AWS Icons page, where you can find the complete official icon set. And to stay at the cutting edge of these tools, keep an eye on updates to the official AWS MCP Servers‚Äîthey‚Äôre constantly evolving with new features to make your diagramming experience even better. 
 
About the Authors 
Joel Asante, an Austin-based Solutions Architect at Amazon Web Services (AWS), works with GovTech (Government Technology) customers. With a strong background in data science and application development, he brings deep technical expertise to creating secure and scalable cloud architectures for his customers. Joel is passionate about data analytics, machine learning, and robotics, leveraging his development experience to design innovative solutions that meet complex government requirements. He holds 13 AWS certifications and enjoys family time, fitness, and cheering for the Kansas City Chiefs and Los Angeles Lakers in his spare time. 
Dunieski Otano is a Solutions Architect at Amazon Web Services based out of Miami, Florida. He works with World Wide Public Sector MNO (Multi-International Organizations) customers. His passion is Security, Machine Learning and Artificial Intelligence, and Serverless. He works with his customers to help them build and deploy high available, scalable, and secure solutions. Dunieski holds 14 AWS certifications and is an AWS Golden Jacket recipient. In his free time, you will find him spending time with his family and dog, watching a great movie, coding, or flying his drone. 
Varun Jasti&nbsp;is a Solutions Architect at Amazon Web Services, working with AWS Partners to design and scale artificial intelligence solutions for public sector use cases to meet compliance standards. With a background in Computer Science, his work covers broad range of ML use cases primarily focusing on LLM training/inferencing and computer vision. In his spare time, he loves playing tennis and swimming.
‚Ä¢ AWS costs estimation using Amazon Q CLI and AWS Cost Analysis MCP
  Managing and optimizing AWS infrastructure costs is a critical challenge for organizations of all sizes. Traditional cost analysis approaches often involve the following: 
 
 Complex spreadsheets ‚Äì Creating and maintaining detailed cost models, which requires significant effort 
 Multiple tools ‚Äì Switching between the AWS Pricing Calculator, AWS Cost Explorer, and third-party tools 
 Specialized knowledge ‚Äì Understanding the nuances of AWS pricing across services and AWS Regions 
 Time-consuming analysis ‚Äì Manually comparing different deployment options and scenarios 
 Delayed optimization ‚Äì Cost insights often come too late to inform architectural decisions 
 
Amazon Q Developer CLI with the Model Context Protocol (MCP) offers a revolutionary approach to AWS cost analysis. By using generative AI through natural language prompts, teams can now generate detailed cost estimates, comparisons, and optimization recommendations in minutes rather than hours, while providing accuracy through integration with official AWS pricing data. 
In this post, we explore how to use Amazon Q CLI with the AWS Cost Analysis MCP server to perform sophisticated cost analysis that follows AWS best practices. We discuss basic setup and advanced techniques, with detailed examples and step-by-step instructions. 
Solution overview 
Amazon Q Developer CLI is a command line interface that brings the generative AI capabilities of Amazon Q directly to your terminal. Developers can interact with Amazon Q through natural language prompts, making it an invaluable tool for various development tasks. Developed by Anthropic as an open protocol, the Model Context Protocol (MCP) provides a standardized way to connect AI models to different data sources or tools. Using a client-server architecture (as illustrated in the following diagram), the MCP helps developers expose their data through lightweight MCP servers while building AI applications as MCP clients that connect to these servers. 
The MCP uses a client-server architecture containing the following components: 
 
 Host ‚Äì A program or AI tool that requires access to data through the MCP protocol, such as Anthropic‚Äôs Claude Desktop, an integrated development environment (IDE), or other AI applications 
 Client ‚Äì Protocol clients that maintain one-to-one connections with servers 
 Server ‚Äì Lightweight programs that expose capabilities through standardized MCP or act as tools 
 Data sources ‚Äì Local data sources such as databases and file systems, or external systems available over the internet through APIs (web APIs) that MCP servers can connect with 
 
 
As announced in April 2025, the MCP enables Amazon Q Developer to connect with specialized servers that extend its capabilities beyond what‚Äôs possible with the base model alone. MCP servers act as plugins for Amazon Q, providing domain-specific knowledge and functionality. The AWS Cost Analysis MCP server specifically enables Amazon Q to generate detailed cost estimates, reports, and optimization recommendations using real-time AWS pricing data. 
Prerequisites 
To implement this solution, you must have an AWS account with appropriate permissions and follow the steps below. 
Set up your environment 
Before you can start analyzing costs, you need to set up your environment with Amazon Q CLI and the AWS Cost Analysis MCP server. This section provides detailed instructions for installation and configuration. 
Install Amazon Q Developer CLI 
Amazon Q Developer CLI is available as a standalone installation. Complete the following steps to install it: 
 
 Download and install Amazon Q Developer CLI. For instructions, see Using Amazon Q Developer on the command line. 
 Verify the installation by running the following command: q --version You should see output similar to the following: Amazon Q Developer CLI version 1.x.x 
 Configure Amazon Q CLI with your AWS credentials: q login 
 Choose the login method suitable for you: 
   
   Use for free with AWS Builder ID 
   Use with Pro license 
    
 
Set up MCP servers 
Before using the AWS Cost Analysis MCP server with Amazon Q CLI, you must install several tools and configure your environment. The following steps guide you through installing the necessary tools and setting up the MCP server configuration: 
 
 Install Panoc using the following command (you can install with brew as well), converting the output to PDF: pip install pandoc 
 Install uv with the following command: pip install uv 
 Install Python 3.10 or newer: uv python install 3.10 
 Add the servers to your ~/.aws/amazonq/mcp.json file: {
  "mcpServers": {
    "awslabs.cost-analysis-mcp-server": {
      "command": "uvx",
      "args": ["awslabs.cost-analysis-mcp-server"],
      "env": {
        "FASTMCP_LOG_LEVEL": "ERROR"
      },
      "autoApprove": [],
      "disabled": false
    }
  }
}
 Now, Amazon Q CLI automatically discovers MCP servers in the ~/.aws/amazonq/mcp.json file. 
 
Understanding MCP server tools 
The AWS Cost Analysis MCP server provides several powerful tools: 
 
 get_pricing_from_web ‚Äì Retrieves pricing information from AWS pricing webpages 
 get_pricing_from_api ‚Äì Fetches pricing data from the AWS Price List API 
 generate_cost_report ‚Äì Creates detailed cost analysis reports with breakdowns and visualizations 
 analyze_cdk_project ‚Äì Analyzes AWS Cloud Development Kit (AWS CDK) projects to identify services used and estimate costs 
 analyze_terraform_project ‚Äì Analyzes Terraform projects to identify services used and estimate costs 
 get_bedrock_patterns ‚Äì Retrieves architecture patterns for Amazon Bedrock with cost considerations 
 
These tools work together to help you create accurate cost estimates that follow AWS best practices. 
Test your setup 
Let‚Äôs verify that everything is working correctly by generating a simple cost analysis: 
 
 Start the Amazon Q CLI chat interface and verify the output shows the MCP server being loaded and initialized: q chat 
 In the chat interface, enter the following prompt:Please create a cost analysis for a simple web application with an Application Load Balancer, two t3.medium EC2 instances, and an RDS db.t3.medium MySQL database. Assume 730 hours of usage per month and moderate traffic of about 100 GB data transfer. Convert estimation to a PDF format. 
 Amazon Q CLI will ask for permission to trust the tool that is being used; enter t to trust it. Amazon Q should generate and display a detailed cost analysis. Your output should look like the following screenshot.  If you see the cost analysis report, your environment is set up correctly. If you encounter issues, verify that Amazon Q CLI can access the MCP servers by making sure you installed install the necessary tools and the servers are in the ~/.aws/amazonq/mcp.json file. 
 
Configuration options 
The AWS Cost Analysis MCP server supports several configuration options to customize your cost analysis experience: 
 
 Output format ‚Äì Choose between markdown, CSV formats, or PDF (which we installed the package for) for cost reports 
 Pricing model ‚Äì Specify on-demand, reserved instances, or savings plans 
 Assumptions and exclusions ‚Äì Customize the assumptions and exclusions in your cost analysis 
 Detailed cost data ‚Äì Provide specific usage patterns for more accurate estimates 
 
Now that our environment is set up, let‚Äôs create more cost analyses. 
Create AWS Cost Analysis reports 
In this section, we walk through the process of creating AWS cost analysis reports using Amazon Q CLI with the AWS Cost Analysis MCP server. 
When you provide a prompt to Amazon Q CLI, the AWS Cost Analysis MCP server completes the following steps: 
 
 Interpret your requirements. 
 Retrieve pricing data from AWS pricing sources. 
 Generate a detailed cost analysis report. 
 Provide optimization recommendations. 
 
This process happens seamlessly, so you can focus on describing what you want rather than how to create it. 
AWS Cost Analysis reports typically include the following information: 
 
 Service costs ‚Äì Breakdown of costs by AWS service 
 Unit pricing ‚Äì Detailed unit pricing information 
 Usage quantities ‚Äì Estimated usage quantities for each service 
 Calculation details ‚Äì Step-by-step calculations showing how costs were derived 
 Assumptions ‚Äì Clearly stated assumptions used in the analysis 
 Exclusions ‚Äì Costs that were not included in the analysis 
 Recommendations ‚Äì Cost optimization suggestions 
 
Example 1: Analyze a serverless application 
Let‚Äôs create a cost analysis for a simple serverless application. Use the following prompt: 
Create a cost analysis for a serverless application using API Gateway, Lambda, and DynamoDB. Assume 1 million API calls per month, average Lambda execution time of 200ms with 512MB memory, and 10GB of DynamoDB storage with 5 million read requests and 1 million write requests per month. Convert estimation to a PDF format. 
Upon entering your prompt, Amazon Q CLI will retrieve pricing data using the get_pricing_from_web or get_pricing_from_api tools, and will use generate_cost_report with awslabscost_analysis_mcp_server. 
 
You should receive an output giving a detailed cost breakdown based on the prompt along with optimization recommendations. 
 
 
The generated cost analysis shows the following information: 
 
 Amazon API Gateway costs for 1 million requests 
 AWS Lambda costs for compute time and requests 
 Amazon DynamoDB costs for storage, read, and write capacity 
 Total monthly cost estimate 
 Cost optimization recommendations 
 
Example 2: Analyze multi-tier architectures 
Multi-tier architectures separate applications into functional layers (presentation, application, and data) to improve scalability and security. This example analyzes costs for implementing such an architecture on AWS with components for each tier: 
Create a cost analysis for a three-tier web application with a presentation tier (ALB and CloudFront), application tier (ECS with Fargate), and data tier (Aurora PostgreSQL). Include costs for 2 Fargate tasks with 1 vCPU and 2GB memory each, an Aurora db.r5.large instance with 100GB storage, an Application Load Balancer with 10 
This time, we are formatting it into both PDF and DOCX. 
 
 
The cost analysis shows the following information: 
 
 Presentation tier costs (Application Load Balancer and AWS CloudFront) 
 Application tier costs (Amazon Elastic Container Service (Amazon ECS) and AWS Fargate) 
 Data tier costs (Amazon Aurora PostgreSQL-Compatible Edition) 
 Detailed breakdown of each component‚Äôs pricing 
 Total monthly cost estimate 
 Cost optimization recommendations for each tier 
 
Example 3: Compare deployment options 
When deploying containers on AWS, choosing between Amazon ECS with Amazon Elastic Compute Cloud (Amazon EC2) or Fargate involves different cost structures and management overhead. This example compares these options to determine the most cost-effective solution for a specific workload: 
Compare the costs between running a containerized application on ECS with EC2 launch type versus Fargate launch type. Assume 4 containers each needing 1 vCPU and 2GB memory, running 24/7 for a month. For EC2, use t3.medium instances. Provide a recommendation on which option is more cost-effective for this workload. Convert estimation to a HTML webpage. 
This time, we are formatting it into a HTML webpage. 
 
 
The cost comparison includes the following information: 
 
 Amazon ECS with Amazon EC2 launch type costs 
 Amazon ECS with Fargate launch type costs 
 Detailed breakdown of each option‚Äôs pricing components 
 Side-by-side comparison of total costs 
 Recommendations for the most cost-effective option 
 Considerations for when each option might be preferred 
 
Real-world examples 
Let‚Äôs explore some real-world architecture patterns and how to analyze their costs using Amazon Q CLI with the AWS Cost Analysis MCP server. 
Ecommerce platform 
Ecommerce platforms require scalable, resilient architectures with careful cost management. These systems typically use microservices to handle various functions independently while maintaining high availability. This example analyzes costs for a complete ecommerce solution with multiple components serving moderate traffic levels: 
Create a cost analysis for an e-commerce platform with microservices architecture. Include components for product catalog, shopping cart, checkout, payment processing, order management, and user authentication. Assume moderate traffic of 500,000 monthly active users, 2 million page views per day, and 50,000 orders per month. Ensure the analysis follows AWS best practices for cost optimization. Convert estimation to a PDF format. 
 
The cost analysis includes the following key components: 
 
 Frontend delivery costs (Amazon Simple Storage Service (Amazon S3) and CloudFront) 
 API Gateway and Lambda costs for serverless components 
 Container costs for microservices (Amazon Elastic Kubernetes Service (Amazon EKS) and Amazon ECS) 
 Database costs (Amazon Relational Database Service (Amazon RDS) and DynamoDB) 
 Caching costs (Amazon ElastiCache) 
 Storage and data transfer costs 
 Monitoring and security costs 
 Total monthly cost estimate 
 Cost optimization recommendations for each component 
 Reserved instance and savings plan opportunities 
 
Data analytics platform 
Modern data analytics platforms need to efficiently ingest, store, process, and visualize large volumes of data while managing costs effectively. This example examines the AWS services and costs involved in building a complete analytics pipeline handling significant daily data volumes with multiple user access requirements: 
Create a cost analysis for a data analytics platform processing 500GB of new data daily. Include components for data ingestion (Kinesis), storage (S3), processing (EMR), and visualization (QuickSight). Assume 50 users accessing dashboards daily and data retention of 90 days. Ensure the analysis follows AWS best practices for cost optimization and includes recommendations for cost-effective scaling. Convert estimation to a HTML webpage. 
 
The cost analysis includes the following key components: 
 
 Data ingestion costs (Amazon Kinesis Data Streams and Amazon Data Firehose) 
 Storage costs (Amazon S3 with lifecycle policies) 
 Processing costs (Amazon EMR cluster) 
 Visualization costs (Amazon QuickSight) 
 Data transfer costs between services 
 Total monthly cost estimate 
 Cost optimization recommendations for each component 
 Scaling considerations and their cost implications 
 
Clean up 
If you no longer need to use the AWS Cost Analysis MCP server with Amazon Q CLI, you can remove it from your configuration: 
 
 Open your ~/.aws/amazonq/mcp.json file. 
 Remove or comment out the ‚Äúawslabs.cost-analysis-mcp-server‚Äù entry. 
 Save the file. 
 
This will prevent the server from being loaded when you start Amazon Q CLI in the future. 
Conclusion 
In this post, we explored how to use Amazon Q CLI with the AWS Cost Analysis MCP server to create detailed cost analyses that use accurate AWS pricing data. This approach offers significant advantages over traditional cost estimation methods: 
 
 Time savings ‚Äì Generate complex cost analyses in minutes instead of hours 
 Accuracy ‚Äì Make sure estimates use the latest AWS pricing information 
 Comprehensive ‚Äì Include relevant cost components and considerations 
 Actionable ‚Äì Receive specific optimization recommendations 
 Iterative ‚Äì Quickly compare different scenarios through simple prompts 
 Validation ‚Äì Check estimates against official AWS pricing 
 
As you continue exploring AWS cost analysis, we encourage you to deepen your knowledge by learning more about the Model Context Protocol (MCP) to understand how it enhances the capabilities of Amazon Q. For hands-on cost estimation, the AWS Pricing Calculator offers an interactive experience to model and compare different deployment scenarios. To make sure your architectures follow financial best practices, the AWS Well-Architected Framework Cost Optimization Pillar provides comprehensive guidance on building cost-efficient systems. And to stay at the cutting edge of these tools, keep an eye on updates to the official AWS MCP servers‚Äîthey‚Äôre constantly evolving with new features to make your cost analysis experience even more powerful and accurate. 
 
About the Authors 
Joel Asante, an Austin-based Solutions Architect at Amazon Web Services (AWS), works with GovTech (Government Technology) customers. With a strong background in data science and application development, he brings deep technical expertise to creating secure and scalable cloud architectures for his customers. Joel is passionate about data analytics, machine learning, and robotics, leveraging his development experience to design innovative solutions that meet complex government requirements. He holds 13 AWS certifications and enjoys family time, fitness, and cheering for the Kansas City Chiefs and Los Angeles Lakers in his spare time. 
Dunieski Otano is a Solutions Architect at Amazon Web Services based out of Miami, Florida. He works with World Wide Public Sector MNO (Multi-International Organizations) customers. His passion is Security, Machine Learning and Artificial Intelligence, and Serverless. He works with his customers to help them build and deploy high available, scalable, and secure solutions. Dunieski holds 14 AWS certifications and is an AWS Golden Jacket recipient. In his free time, you will find him spending time with his family and dog, watching a great movie, coding, or flying his drone. 
Varun Jasti&nbsp;is a Solutions Architect at Amazon Web Services, working with AWS Partners to design and scale artificial intelligence solutions for public sector use cases to meet compliance standards. With a background in Computer Science, his work covers broad range of ML use cases primarily focusing on LLM training/inferencing and computer vision. In his spare time, he loves playing tennis and swimming.

‚∏ª