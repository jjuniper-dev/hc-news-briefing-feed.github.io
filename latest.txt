‚úÖ Morning News Briefing ‚Äì August 12, 2025 10:47

üìÖ Date: 2025-08-12 10:47
üè∑Ô∏è Tags: #briefing #ai #publichealth #digitalgov

‚∏ª

üßæ Weather
‚Ä¢ HEAT WARNING, Pembroke
  Persons in or near this area should be on the lookout for adverse weather conditions and take necessary safety precautions . People in or around this area are advised to take necessary precautions . Weather conditions are expected to be adverse in the area for the first time in the next 24 years . People should be aware of the weather conditions at this time of the year and take precautionary measures to protect themselves .
‚Ä¢ Current Conditions:  18.8¬∞C
  Temperature: 18.8&deg;C Pressure / Tendency: 101.6 kPa rising Humidity: 88 % Dewpoint: 16.8¬∞C Wind: E 2 km/h Air Quality Health Index: n/a . Pembroke 6:00 AM EDT Tuesday 12 August 2025 temperature: 18¬∞C Pressure, Tendency and Tendency at 101.
‚Ä¢ Tuesday: Chance of showers. High 31. POP 40%
  40 percent chance of showers late this afternoon with risk of thunderstorm . Wind becoming southwest 20 km/h near noon . A mix of sun and cloud expected to be on the way to 31 degrees . High 31 degrees with a risk of a thunderstorm in the afternoon . Humidex 40 in the morning with a high of 31 degrees in the mid to mid afternoon . Forecast issued

üåç International News
No updates.

üçÅ Canadian News
No updates.

üá∫üá∏ U.S. Top Stories
‚Ä¢ Alaska was once a full-fledged Russian colony. Now it's hosting a U.S.-Russia summit
  Russia lost a war in Crimea in the 1850s. To pay off war debts, Russia sold Alaska to the U.S. President Trump and Putin will meet Friday in Alaska to discuss another war involving Crimea . Russia sold Crimea to pay off its debts to pay for the loss of Alaska . The two presidents will meet in Alaska on Friday in order to discuss the Crimea issue . Russia lost
‚Ä¢ U.S. and China extend tariff truce deadline for another 3 months
  President Trump's executive order extends a reprieve from the threat of rising tariffs between the world's two largest economies . President Trump extended the reprieve by extending the threat to the U.S.-China . The world's largest economies have been hit by rising tariffs since the start of the Trump administration's first term . The U.K. is the largest economy in the world, with
‚Ä¢ A Palestinian activist was killed by the violence he sought to stop
  Awdah Al Hathaleen was shot during a clash with an Israeli settler . His West Bank village hoped No Other Land, the Oscar-winning film about settler violence that he worked on, might help protect them . The film is based on the West Bank conflict between Israeli settlers and Palestinians in West Bank . The West Bank community hoped the film might help them protect themselves .
‚Ä¢ Say bye-bye to the beeps and boops of AOL's dial-up internet service
  AOL debuted the service in 1991 . Dial-up has largely been replaced by broadband internet . AOL's dial-up service was launched in 1991, with AOL's first service launched in 1992 . AOL launched the internet service in 1995, with the first version of the service being launched in 1996, in 1996 . AOL was the first major internet service to offer dial-ups in the U.
‚Ä¢ In Alabama, a dredging project in Mobile Bay brings together unlikely allies
  The mud being dug out of the channel is put into other parts of Mobile Bay . Dredging waterways for navigation is a centuries-old practice, but this project is controversial because the mud is put in other places . The mud is being dredged out of a channel in Mobile Bay to be used for navigation purposes, but it is not allowed to be put in another part of the

üß† Artificial Intelligence
No updates.

üíª Digital Strategy
‚Ä¢ VS Code previews chat checkpoints for unpicking careless talk
  Visual Studio Code (VS Code) team has rolled out version 1.103 with new features including GitHub Copilot chat checkpoints . Microsoft‚Äôs AI-centric code editor and . IDE adds the ability to rollback misguided AI prompts . The new features include a new ability to ‚Äòrollback‚Äô AI prompts and ‚Äòconfrontal‚Äô commands . The code editor has
‚Ä¢ Colo operators flock to emerging markets to build DCs
  Joburg and Warsaw among the hotspots for sprawling server farm construction . Lagos, Warsaw and Dubai are among the fastest growing cities for colocation services . Metro areas in Asia-Pacific and EMEA regions expanding more rapidly than traditional datacenter hotspots . Joburg, Dubai and Joburg among the hottest hotspots in the world for data-storage services . Dubai, Job
‚Ä¢ Defra doubles contract value for cloud and DC services
  UK agriculture department upped potential contract value on offer for cloud and datacenter hosting by more than ¬£100 million . Agriculture department for agriculture and the countryside upped its potential contract to ¬£245 million . Legacy tech for nation's farmers must migrate ... contract swells to¬†¬£245M contract swelled to ¬£100M . UK's agriculture and countryside has upped the potential contract
‚Ä¢ The White House could end UK's decade-long fight to bust encryption
  Home Office officials reportedly concede Brit government on back foot as Trump moves to protect US Big Tech players . Home Office's war on encryption ‚Äì its most technically complex and controversial aspect of modern policymaking yet ‚Äì is starting to look like battlefield failure after more than ten years of skirmishes .‚Ä¶‚Ä¶‚Ä¶ The UK government is on the back foot in the battle over encryption, it has been reported
‚Ä¢ UK.gov's nuclear strategy is 'slow, inefficient, and costly'
  Taskforce delivers damning interim report on next generation of energy generation . UK's "unnecessarily slow, inefficient, and costly" approach to nuclear power . Taskforce commissioned by the UK government has warned of the nation's 'slow, inefficient' approach to nuke power (and weaponry) Taskforce: Britain's nuclear power is "unnecessary slow, efficient, and expensive" UK

üè• Public Health
No updates.

üî¨ Science
‚Ä¢ Biological and health effects of fire smoke exposure
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Association between social capital and depressive symptoms in adolescents relocated for poverty alleviation in Shanxi, China: a cross-sectional study
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Monte carlo simulation for evaluating spatial dynamics of toxic metals and potential health hazards in sebou basin surface water
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ AI-assisted cervical cytology precancerous screening for high-risk population in resource-limited regions using a compact microscope
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Introducing a mindfulness-based intervention in school curriculum to 16‚Äì24-year-olds. A nationwide cluster-randomized trial
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations

üßæ Government & Policy
No updates.

üèõÔ∏è Enterprise Architecture & IT Governance
No updates.

ü§ñ AI & Emerging Tech
‚Ä¢ What you may have missed about GPT-5
  Before OpenAI released GPT-5 last Thursday, CEO Sam Altman said its capabilities made him feel ‚Äúuseless relative to the AI.‚Äù He said working on it carries a weight he imagines the developers of the atom bomb must have felt.



As tech giants converge on models that do more or less the same thing, OpenAI‚Äôs new offering was supposed to give a glimpse of AI‚Äôs newest frontier. It was meant to mark a leap toward the ‚Äúartificial general intelligence‚Äù that tech‚Äôs evangelists have promised will transform humanity for the better.&nbsp;



Against those expectations, the model has mostly underwhelmed.¬†



People have highlighted glaring mistakes in GPT-5‚Äôs responses, countering Altman‚Äôs claim made at the launch that it works like ‚Äúa legitimate PhD-level expert in anything any area you need on demand.‚Äù Early testers have also found issues with OpenAI‚Äôs promise that GPT-5 automatically works out what type of AI model is best suited for your question‚Äîa reasoning model for more complicated queries, or a faster model for simpler ones. Altman seems to have conceded that this feature is flawed and takes away user control. However there is good news too: the model seems to have eased the problem of ChatGPT sucking up to users, with GPT-5 less likely to shower them with over the top compliments.



Overall, as my colleague Grace Huckins pointed out, the new release represents more of a product update‚Äîproviding slicker and prettier ways of conversing with ChatGPT‚Äîthan a breakthrough that reshapes what is possible in AI.&nbsp;



But there‚Äôs one other thing to take from all this. For a while, AI companies didn‚Äôt make much effort to suggest how their models might be used. Instead, the plan was to simply build the smartest model possible‚Äîa brain of sorts‚Äîand trust that it would be good at lots of things. Writing poetry would come as naturally as organic chemistry. Getting there would be accomplished by bigger models, better training techniques, and technical breakthroughs.&nbsp;



That has been changing: The play now is to push existing models into more places by hyping up specific applications. Companies have been more aggressive in their promises that their AI models can replace human coders, for example (even if the early evidence suggests otherwise). A possible explanation for this pivot is that tech giants simply have not made the breakthroughs they‚Äôve expected. We might be stuck with only marginal improvements in large language models‚Äô capabilities for the time being. That leaves AI companies with one option: Work with what you‚Äôve got.





The starkest example of this in the launch of GPT-5 is how much OpenAI is encouraging people to use it for health advice, one of AI‚Äôs most fraught arenas.¬†



In the beginning, OpenAI mostly didn‚Äôt play ball with medical questions. If you tried to ask ChatGPT about your health, it gave lots of disclaimers warning you that it was not a doctor, and for some questions, it would refuse to give a response at all. But as I recently reported, those disclaimers began disappearing as OpenAI released new models. Its models will now not only interpret x-rays and mammograms for you but ask follow-up questions leading toward a diagnosis.



In May, OpenAI signaled it would try to tackle medical questions head on. It announced HealthBench, a way to evaluate how good AI systems are at handling health topics as measured against the opinions of physicians. In July, it published a study it participated in, reporting that a cohort of doctors in Kenya made fewer diagnostic mistakes when they were helped by an AI model.¬†



With the launch of GPT-5, OpenAI has begun explicitly telling people to use its models for health advice. At the launch event, Altman welcomed on stage Felipe Millon, an OpenAI employee, and his wife, Carolina Millon, who had recently been diagnosed with multiple forms of cancer. Carolina spoke about asking ChatGPT for help with her diagnoses, saying that she had uploaded copies of her biopsy results to ChatGPT to translate medical jargon and asked the AI for help making decisions about things like whether or not to pursue radiation. The trio called it an empowering example of shrinking the knowledge gap between doctors and patients.



With this change in approach, OpenAI is wading into dangerous waters.¬†



For one, it‚Äôs using evidence that doctors can benefit from AI as a clinical tool, as in the Kenya study, to suggest that people without any medical background should ask the AI model for advice about their own health. The problem is that lots of people might ask for this advice without ever running it by a doctor (and are less likely to do so now that the chatbot rarely prompts them to).



Indeed, two days before the launch of GPT-5, the Annals of Internal Medicine published a paper about a man who stopped eating salt and began ingesting dangerous amounts of bromide following a conversation with ChatGPT. He developed bromide poisoning‚Äîwhich largely disappeared in the US after the Food and Drug Administration began curbing the use of bromide in over-the-counter medications in the 1970s‚Äîand then nearly died, spending weeks in the hospital.&nbsp;



So what‚Äôs the point of all this? Essentially, it‚Äôs about accountability. When AI companies move from promising general intelligence to offering humanlike helpfulness in a specific field like health care, it raises a second, yet unanswered question about what will happen when mistakes are made. As things stand, there‚Äôs little indication tech companies will be made liable for the harm caused.



‚ÄúWhen doctors give you harmful medical advice due to error or prejudicial bias, you can sue them for malpractice and get recompense,‚Äù says Damien Williams, an assistant professor of data science and philosophy at the University of North Carolina Charlotte.&nbsp;



‚ÄúWhen ChatGPT gives you harmful medical advice because it‚Äôs been trained on prejudicial data, or because ‚Äòhallucinations‚Äô are inherent in the operations of the system, what‚Äôs your recourse?‚Äù



This story originally appeared in The Algorithm, our weekly newsletter on AI. To get stories like this in your inbox first,¬†sign up here.
‚Ä¢ Sam Altman and the whale
  My colleague Grace Huckins has a great story on OpenAI‚Äôs release of GPT-5, its long-awaited new flagship model. One of the takeaways, however, is that while GPT-5 may make for a better experience than the previous versions, it isn‚Äôt something revolutionary. ‚ÄúGPT-5 is, above all else,‚Äù Grace concludes, ‚Äúa refined product.‚Äù



This is pretty much in line with my colleague Will Heaven‚Äôs recent argument that the latest model releases have been a bit like smartphone releases: Increasingly, what we are seeing are incremental improvements meant to enhance the user experience. (Casey Newton made a similar point in Friday‚Äôs Platformer.) At GPT-5‚Äôs release on Thursday, OpenAI CEO Sam Altman himself compared it to when Apple released the first iPhone with a Retina display. Okay. Sure.¬†



But where is the transition from the BlackBerry keyboard to the touch-screen iPhone? Where is the assisted GPS and the API for location services that enables real-time directions and gives rise to companies like Uber and Grindr and lets me order a taxi for my burrito? Where are the real breakthroughs?&nbsp;



In fact, following the release of GPT-5, OpenAI found itself with something of a user revolt on its hands. Customers who missed GPT-4o&#8217;s personality successfully lobbied the company to bring it back as an option for its Plus users. If anything, that indicates the GPT-5 release was more about user experience than noticeable performance enhancements. 



And yet, hours before OpenAI‚Äôs GPT-5 announcement, Altman teased it by tweeting an image of an emerging Death Star floating in space. On Thursday, he touted its PhD-level intelligence. He then went on the Mornings with Maria show to claim it would ‚Äúsave a lot of lives.‚Äù (Forgive my extreme skepticism of that particular brand of claim, but we‚Äôve certainly seen it before.)&nbsp;



It‚Äôs a lot of hype, but Altman is not alone in his Flavor Flav-ing here. Last week Mark Zuckerberg published a long memo about how we are approaching AI superintelligence. Anthropic CEO Dario Amodei freaked basically everyone out earlier this year with his prediction that AI would harvest half of all entry-level jobs within, possibly, a year.&nbsp;



The people running these companies literally talk about the danger that the things they are building might take over the world and kill every human on the planet. GPT-5, meanwhile, still can‚Äôt tell you how many b‚Äôs there are in the word ‚Äúblueberry.‚Äù&nbsp;



This is not to say that the products released by OpenAI or Anthropic or what have you are not impressive. They are. And they clearly have a good deal of utility. But the hype cycle around model releases is out of hand.&nbsp;



I say that as one of those people who use ChatGPT or Google Gemini most days, often multiple times a day. This week, for example, my wife was surfing and encountered a whale repeatedly slapping its tail on the water. Despite having seen very many whales, often in very close proximity, she had never seen anything like this. She sent me a video, and I was curious about it too. So I asked ChatGPT, ‚ÄúWhy do whales slap their tails repeatedly on the water?‚Äù It came right back, confidently explaining that what I was describing was called ‚Äúlobtailing,‚Äù along with a list of possible reasons why whales do that. Pretty cool.&nbsp;



But then again, a regular garden-variety Google search would also have led me to discover lobtailing. And while ChatGPT‚Äôs response summarized the behavior for me, it was also too definitive about why whales do it. The reality is that while people have a lot of theories, we still can‚Äôt really explain this weird animal behavior.&nbsp;



The reason I‚Äôm aware that lobtailing is something of a mystery is that I dug into actual, you know, search results. Which is where I encountered this beautiful, elegiac essay by Emily Boring. She describes her time at sea, watching a humpback slapping its tail against the water, and discusses the scientific uncertainty around this behavior. Is it a feeding technique? Is it a form of communication? Posturing? The action, as she notes, is extremely energy intensive. It takes a lot of effort from the whale. Why do they do it?&nbsp;



I was struck by one passage in particular, in which she cites another biologist‚Äôs work to draw a conclusion of her own:&nbsp;



Surprisingly, the complex energy trade-off of a tail-slap might be the exact reason why it‚Äôs used. Biologist Hal Whitehead suggests, ‚ÄúBreaches and lob-tails make good signals precisely because they are energetically expensive and thus indicative of the importance of the message and the physical status of the signaler.‚Äù A tail-slap means that a whale is physically fit, traveling at nearly maximum speed, capable of sustaining powerful activity, and carrying a message so crucial it is willing to use a huge portion of its daily energy to share it. ‚ÄúPay attention!‚Äù the whale seems to say. ‚ÄúI am important! Notice me!‚Äù







In some ways, the AI hype cycle has to be out of hand. It has to justify the ferocious level of investment, the uncountable billions of dollars in sunk costs. The massive data center buildouts with their massive environmental consequences created at massive expense that are seemingly keeping the economy afloat and threatening to crash it. There is so, so, so much money at stake.&nbsp;



Which is not to say there aren‚Äôt really cool things happening in AI. And certainly there have been a number of moments when I have been floored by AI releases. ChatGPT 3.5 was one. Dall-E, NotebookLM, Veo 3, Synthesia. They can amaze. In fact there was an AI product release just this week that was a little bit mind-blowing. Genie 3, from Google DeepMind, can turn a basic text prompt into an immersive and navigable 3D world. Check it out‚Äîit‚Äôs pretty wild. And yet Genie 3 also makes a case that the most interesting things happening right now in AI aren‚Äôt happening in chatbots.&nbsp;



I‚Äôd even argue that at this point, most of the people who are regularly amazed by the feats of new LLM chatbot releases are the same people who stand to profit from the promotion of LLM chatbots.



Maybe I‚Äôm being cynical, but I don‚Äôt think so. I think it‚Äôs more cynical to promise me the Death Star and instead deliver a chatbot whose chief appeal seems to be that it automatically picks the model for you. To promise me superintelligence and deliver shrimp Jesus. It‚Äôs all just a lot of lobtailing. ‚ÄúPay attention! I am important! Notice me!‚Äù



This article is from The Debrief, MIT Technology Review‚Äôs subscriber-only weekly email newsletter from editor in chief Mat Honan. Subscribers can sign up here to receive it in your inbox.
‚Ä¢ Meet the early-adopter judges using AI
  The propensity for AI systems to make mistakes and for humans to miss those mistakes has been on full display in the US legal system as of late. The follies began when lawyers‚Äîincluding some at prestigious firms‚Äîsubmitted documents citing cases that didn‚Äôt exist. Similar mistakes soon spread to other roles in the courts. In December, a Stanford professor submitted sworn testimony containing hallucinations and errors in a case about deepfakes, despite being an expert on AI and misinformation himself.



The buck stopped with judges, who‚Äîwhether they or opposing counsel caught the mistakes‚Äîissued reprimands and fines, and likely left attorneys embarrassed enough to think twice before trusting AI again.



But now judges are experimenting with generative AI too. Some are confident that with the right precautions, the technology can expedite legal research, summarize cases, draft routine orders, and overall help speed up the court system, which is badly backlogged in many parts of the US. This summer, though, we‚Äôve already seen AI-generated mistakes go undetected and cited by judges. A federal judge in New Jersey had to reissue an order riddled with errors that may have come from AI, and a judge in Mississippi refused to explain why his order too contained mistakes that seemed like AI hallucinations.&nbsp;



The results of these early-adopter experiments make two things clear. One, the category of routine tasks‚Äîfor which AI can assist without requiring human judgment‚Äîis slippery to define. Two, while lawyers face sharp scrutiny when their use of AI leads to mistakes, judges may not face the same accountability, and walking back their mistakes before they do damage is much harder.



Drawing boundaries



Xavier Rodriguez, a federal judge for the Western District of Texas, has good reason to be skeptical of AI. He started learning about artificial intelligence back in 2018, four years before the release of ChatGPT (thanks in part to the influence of his twin brother, who works in tech). But he‚Äôs also seen AI-generated mistakes in his own court.&nbsp;



In a recent dispute about who was to receive an insurance payout, both the plaintiff and the defendant represented themselves, without lawyers (this is not uncommon‚Äînearly a quarter of civil cases in federal court involve at least one unrepresented party). The two sides wrote their own filings and made their own arguments.&nbsp;



‚ÄúBoth sides used AI tools,‚Äù Rodriguez says, and both submitted filings that referenced made-up cases. He had authority to reprimand them, but given that they were not lawyers, he opted not to.&nbsp;



‚ÄúI think there‚Äôs been an overreaction by a lot of judges on these sanctions. The running joke I tell when I‚Äôm on the speaking circuit is that lawyers have been hallucinating well before AI,‚Äù he says. Missing a mistake from an AI model is not wholly different, to Rodriguez, from failing to catch the error of a first-year lawyer. ‚ÄúI‚Äôm not as deeply offended as everybody else,‚Äù he says.&nbsp;



In his court, Rodriguez has been using generative AI tools (he wouldn‚Äôt publicly name which ones, to avoid the appearance of an endorsement) to summarize cases. He‚Äôll ask AI to identify key players involved and then have it generate a timeline of key events. Ahead of specific hearings, Rodriguez will also ask it to generate questions for attorneys based on the materials they submit.



These tasks, to him, don‚Äôt lean on human judgment. They also offer lots of opportunities for him to intervene and uncover any mistakes before they‚Äôre brought to the court. ‚ÄúIt‚Äôs not any final decision being made, and so it‚Äôs relatively risk free,‚Äù he says. Using AI to predict whether someone should be eligible for bail, on the other hand, goes too far in the direction of judgment and discretion, in his view.



Erin Solovey, a professor and researcher on human-AI interaction at Worcester Polytechnic Institute in Massachusetts, recently studied how judges in the UK think about this distinction between rote, machine-friendly work that feels safe to delegate to AI and tasks that lean more heavily on human expertise.&nbsp;



‚ÄúThe line between what is appropriate for a human judge to do versus what is appropriate for AI tools to do changes from judge to judge and from one scenario to the next,‚Äù she says.



Even so, according to Solovey, some of these tasks simply don‚Äôt match what AI is good at. Asking AI to summarize a large document, for example, might produce drastically different results depending on whether the model has been trained to summarize for a general audience or a legal one. AI also struggles with logic-based tasks like ordering the events of a case. ‚ÄúA very plausible-sounding timeline may be factually incorrect,‚Äù Solovey says.&nbsp;



Rodriguez and a number of other judges crafted guidelines that were published in February by the Sedona Conference, an influential think tank that issues principles for particularly murky areas of the law. They outline a host of potentially ‚Äúsafe‚Äù uses of AI for judges, including conducting legal research, creating preliminary transcripts, and searching briefings, while warning that judges should verify outputs from AI and that ‚Äúno known GenAI tools have fully resolved the hallucination problem.‚Äù





Dodging AI blunders



Judge Allison Goddard, a federal magistrate judge in California and a coauthor of the guidelines, first felt the impact that AI would have on the judiciary when she taught a class on the art of advocacy at her daughter‚Äôs high school. She was impressed by a student‚Äôs essay and mentioned it to her daughter. ‚ÄúShe said, ‚ÄòOh, Mom, that‚Äôs ChatGPT.‚Äô‚Äù



‚ÄúWhat I realized very quickly was this is going to really transform the legal profession,‚Äù she says. In her court, Goddard has been experimenting with ChatGPT, Claude (which she keeps &#8220;open all day&#8221;), and a host of other AI models. If a case involves a particularly technical issue, she might ask AI to help her understand which questions to ask attorneys. She‚Äôll summarize 60-page orders from the district judge and then ask the AI model follow-up questions about it, or ask it to organize information from documents that are a mess.¬†



‚ÄúIt‚Äôs kind of a thought partner, and it brings a perspective that you may not have considered,‚Äù she says.



Goddard also encourages her clerks to use AI, specifically Anthropic‚Äôs Claude, because by default it does not train on user conversations. But it has its limits. For anything that requires law-specific knowledge, she‚Äôll use tools from Westlaw or Lexis, which have AI tools built specifically for lawyers, but she finds general-purpose AI models to be faster for lots of other tasks. And her concerns about bias have prevented her from using it for tasks in criminal cases, like determining if there was probable cause for an arrest.



In this, Goddard appears to be caught in the same predicament the AI boom has created for many of us. Three years in, companies have built tools that sound so fluent and humanlike they obscure the intractable problems lurking underneath‚Äîanswers that read well but are wrong, models that are trained to be decent at everything but perfect for nothing, and the risk that your conversations with them will be leaked to the internet. Each time we use them, we bet that the time saved will outweigh the risks, and trust ourselves to catch the mistakes before they matter. For judges, the stakes are sky-high: If they lose that bet, they face very public consequences, and the impact of such mistakes on the people they serve can be lasting.&nbsp;



‚ÄúI‚Äôm not going to be the judge that cites hallucinated cases and orders,‚Äù Goddard says. ‚ÄúIt‚Äôs really embarrassing, very professionally embarrassing.‚Äù



Still, some judges don‚Äôt want to get left behind in the AI age. With some in the AI sector suggesting that the supposed objectivity and rationality of AI models could make them better judges than fallible humans, it might lead some on the bench to think that falling behind poses a bigger risk than getting too far out ahead.&nbsp;



A ‚Äòcrisis waiting to happen‚Äô



The risks of early adoption have raised alarm bells with Judge Scott Schlegel, who serves on the Fifth Circuit Court of Appeal in Louisiana. Schlegel has long blogged about the helpful role technology can play in modernizing the court system, but he has warned that AI-generated mistakes in judges‚Äô rulings signal a ‚Äúcrisis waiting to happen,‚Äù one that would dwarf the problem of lawyers‚Äô submitting filings with made-up cases.&nbsp;



Attorneys who make mistakes can get sanctioned, have their motions dismissed, or lose cases when the opposing party finds out and flags the errors. ‚ÄúWhen the judge makes a mistake, that‚Äôs the law,‚Äù he says. ‚ÄúI can‚Äôt go a month or two later and go ‚ÄòOops, so sorry,‚Äô and reverse myself. It doesn‚Äôt work that way.‚Äù



Consider child custody cases or bail proceedings, Schlegel says: ‚ÄúThere are pretty significant consequences when a judge relies upon artificial intelligence to make the decision,‚Äù especially if the citations that decision relies on are made-up or incorrect.



This is not theoretical. In June, a Georgia appellate court judge issued an order that relied partially on made-up cases submitted by one of the parties, a mistake that went uncaught. In July, a federal judge in New Jersey withdrew an opinion after lawyers complained it too contained hallucinations.&nbsp;



Unlike lawyers, who can be ordered by the court to explain why there are mistakes in their filings, judges do not have to show much transparency, and there is little reason to think they‚Äôll do so voluntarily. On August 4, a federal judge in Mississippi had to issue a new decision in a civil rights case after the original was found to contain incorrect names and serious errors. The judge did not fully explain what led to the errors even after the state asked him to do so. ‚ÄúNo further explanation is warranted,‚Äù the judge wrote.



These mistakes could erode the public‚Äôs faith in the legitimacy of courts, Schlegel says. Certain narrow and monitored applications of AI‚Äîsummarizing testimonies, getting quick writing feedback‚Äîcan save time, and they can produce good results if judges treat the work like that of a first-year associate, checking it thoroughly for accuracy. But most of the job of being a judge is dealing with what he calls the white-page problem: You‚Äôre presiding over a complex case with a blank page in front of you, forced to make difficult decisions. Thinking through those decisions, he says, is indeed the work of being a judge. Getting help with a first draft from an AI undermines that purpose.



‚ÄúIf you‚Äôre making a decision on who gets the kids this weekend and somebody finds out you use Grok and you should have used Gemini or ChatGPT‚Äîyou know, that‚Äôs not the justice system.‚Äù
‚Ä¢ The Download: a quantum radar, and chipmakers‚Äô deal with the US government
  This is today&#8217;s edition of¬†The Download,¬†our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



This quantum radar could image buried objects



Physicists have created a new type of radar that could help improve underground imaging, using a cloud of atoms in a glass cell to detect reflected radio waves.The radar is a type of quantum sensor, an emerging technology that uses the quantum-mechanical properties of objects as measurement devices. It‚Äôs still a prototype, but its intended use is to image buried objects in situations such as constructing underground utilities, drilling wells for natural gas, and excavating archaeological sites. Read the full story.‚ÄîSophia Chen



If you‚Äôre interested in the potential of quantum, why not check out:



+ Why AI could eat quantum computing‚Äôs lunch. Rapid advances in applying artificial intelligence to simulations in physics and chemistry have some people questioning whether we will even need quantum computers at all. Read the full story.+ This quantum computer built on server racks paves the way to bigger machines. Read the full story.+ IBM aims to build the world‚Äôs first large-scale, error-corrected quantum computer by 2028. The company says it has cracked the code for error correction and is building a modular machine in New York state. Read the full story.+ Amazon‚Äôs first quantum computing chip has made its debut. Read the full story.







The must-reads



I‚Äôve combed the internet to find you today‚Äôs most fun/important/scary/fascinating stories about technology.



1 Nvidia and AMD will pay the US 15% of their China AI chip salesThe unconventional deal is the latest in a string of agreements brokered by the US President. (NYT $)+ The deal could equate to billions of dollars for the US government. (WSJ $)+ China says Nvidia‚Äôs H20 chips aren‚Äôt safe. (Reuters)



2 OpenAI is restoring GPT-4o to ChatGPTUsers were furious after GPT-5‚Äôs launch forced them to switch models. (Gizmodo)+ They complained that the new model made basic errors.(Bloomberg $)+ GPT-5 is here. Now what? (MIT Technology Review)



3 The US Bureau of Labor Statistics is in turmoilAnd we‚Äôre losing access to key economic data as a result. (WSJ $)+ Collecting data is getting a lot tougher in the US. (FT $)+ Sweeping tariffs could threaten the US manufacturing rebound. (MIT Technology Review)



4 Spain has more solar power than it knows what to do withAnd that abundance has pushed its electricity grid to its limits. (FT $)+ Did solar power cause Spain‚Äôs blackout? (MIT Technology Review)



5 Truth Social‚Äôs new chatbot keeps disagreeing with Donald TrumpIt states that the 2020 election wasn‚Äôt stolen, and contradicts his stance on tariffs. (WP $)+ It does seem to rely heavily on Fox News, though. (Wired $)



6 Tesla has applied for a license to supply power to British homesIf approved, it could start rivaling the UK‚Äôs energy firms as soon as next year. (BBC)+ The business is likely to be called Tesla Electric. (The Guardian)+ Sales of Tesla‚Äôs EVs are still slumping across Europe. (CNBC)



7 Canadians are taking up the offer of assisted dyingDemand for the procedure is outstripping clinician capacity. (The Atlantic $)+ The messy morality of letting AI make life-and-death decisions. (MIT Technology Review)



8 Nvidia is full of nepo babiesBut Jensen Huang doesn‚Äôt see anything wrong with that. (The Information $)



9 Silicon Valley‚Äôs young founders aren‚Äôt big drinkersThey‚Äôre all about the grind. (Insider $)



10 Farewell, AOL dial-up After 34 years, the company is finally ditching dial-up internet. (NBC News)+ ‚ÄúYou‚Äôve got mail‚Äù no longer. (The Verge)







Quote of the day



‚ÄúI just graduated with a computer science degree, and the only company that has called me for an interview is Chipotle.‚Äù



‚ÄîManasi Mishra, who recently graduated from Purdue University without a job offer, vents her frustration in a TikTok post, the New York Times reports.







One more thing







This sci-fi blockchain game could help create a metaverse that no one owns



Dark Forest is a vast universe, and most of it is shrouded in darkness. Your mission, should you choose to accept it, is to venture into the unknown, avoid being destroyed by opposing players who may be lurking in the dark, and build an empire of the planets you discover and can make your own.But while the video game seemingly looks and plays much like other online strategy games, it doesn‚Äôt rely on the servers running other popular online strategy games. And it may point to something even more profound: the possibility of a metaverse that isn‚Äôt owned by a big tech company. Read the full story.‚ÄîMike Orcutt







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)



+ Fear FA 98 asks the question: what would happen if we crossed a soccer video game with horror classic Silent Hill?+ Planning a ‚Äòworkation?‚Äô These are the best spots to mix business with pleasure.+ Liza Minnelli can‚Äôt stop, won‚Äôt stop!+ Please‚Äîno more sequels.
‚Ä¢ This quantum radar could image buried objects
  Physicists have created a new type of radar that could help improve underground imaging, using a cloud of atoms in a glass cell to detect reflected radio waves. The radar is a type of quantum sensor, an emerging technology that uses the quantum-mechanical properties of objects as measurement devices. It‚Äôs still a prototype, but its intended use is to image buried objects in situations such as constructing underground utilities, drilling wells for natural gas, and excavating archaeological sites.



Like conventional radar, the device sends out radio waves, which reflect off nearby objects. Measuring the time it takes the reflected waves to return makes it possible to determine where an object is. In conventional radar, the reflected waves are detected using a large antenna, among other receiver components. But in this new device, the reflected waves are registered by detecting the interactions between the returning waves and the atom cloud.



The current incarnation of the radar is still bulky, as the researchers have kept it connected to components on an optical table for ease of testing. But they think their quantum radar could be significantly smaller than conventional designs. ‚ÄúInstead of having this sizable metal structure to receive the signal, we now can use this small glass cell of atoms that can be about a centimeter in size,‚Äù says Matthew Simons, a physicist at the National Institute of Standards and Technology (NIST), who was a member of the research team. NIST also worked with the defense contractor RTX to develop the radar.&nbsp;&nbsp;





The glass cell that serves as the radar‚Äôs quantum component is full of cesium atoms kept at room temperature. The researchers use lasers to get each individual cesium atom to swell to nearly the size of a bacterium, about 10,000 times bigger than the usual size. Atoms in this bloated condition are called Rydberg atoms.&nbsp;



When incoming radio waves hit Rydberg atoms, they disturb the distribution of electrons around their nuclei. Researchers can detect the disturbance by shining lasers on the atoms, causing them to emit light; when the atoms are interacting with a radio wave, the color of their emitted light changes. Monitoring the color of this light thus makes it possible to use the atoms as a radio receiver. Rydberg atoms are sensitive to a wide range of radio frequencies without needing to change the physical setup, says Micha≈Ç Parniak, a physicist at the University of Warsaw in Poland, who was not involved in the work. This means a single compact radar device could potentially work at the multiple frequency bands required for different applications.



Simons‚Äôs team tested the radar by placing it in a specially designed room with foam spikes on the floor, ceiling, and walls like stalactites and stalagmites. The spikes absorb, rather than reflect, nearly all the radio waves that hit them. This simulates the effect of a large open space, allowing the group to test the radar‚Äôs imaging capability without unwanted reflections off walls.&nbsp;



MATT SIMONS, NIST




The researchers placed a radio wave transmitter in the room, along with their Rydberg atom receiver, which was hooked up to an optical table outside the room. They aimed radio waves at a copper plate about the size of a sheet of paper, some pipes, and a steel rod in the room, each placed up to five meters away. The radar allowed them to locate the objects to within 4.7 centimeters. The team posted a paper on the research to the arXiv preprint server in late June.



The work moves quantum radar closer to a commercial product. ‚ÄúThis is really about putting elements together in a nice way,‚Äù says Parniak. While other researchers have previously demonstrated how Rydberg atoms can work as radio wave detectors, he says, this group has integrated the receiver with the rest of the device more sleekly than before.&nbsp;



Other researchers have explored the use of Rydberg atoms for other radar applications. For example, Parniak‚Äôs team recently developed a Rydberg atom sensor for measuring radio frequencies to troubleshoot chips used in car radar. Researchers are also exploring whether radar using Rydberg-atom receivers could be used for measuring soil moisture.



This device is just one example of a quantum sensor, a type of technology that incorporates quantum components into conventional tools. For example, the US government has developed gyroscopes that use the wave properties of atoms for sensing rotation, which is useful for navigation. Researchers have also created quantum sensors using impurities in diamond to measure magnetic fields in, for example, biomedical applications.





One advantage of quantum sensors is the inherent consistency of their core components. Each cesium atom in their device is identical. In addition, the radio receiver relies on the fundamental structure of these atoms, which never changes. Properties of the atoms ‚Äúcan be linked directly to fundamental constants,‚Äù says Simons. For this reason, quantum sensors should require less calibration than their non-quantum counterparts.&nbsp;



Governments worldwide have invested billions of dollars to develop quantum sensors and quantum computers, which share similar components. For example, researchers have built quantum computers using Rydberg atoms as qubits, the equivalent to bits in a conventional computer. Thus, advances in quantum sensing can potentially translate into advances into quantum computing, and vice versa. Parniak has recently adapted an error-correction technique from quantum computing to improve a Rydberg-atom-based sensor.&nbsp;



Researchers still need to continue developing quantum radar before it can be made commercially viable. In the future, they need to work on improving the device‚Äôs sensitivity to fainter signals, which could involve improving the coatings for the glass cell. ‚ÄúWe don‚Äôt see this replacing all radar applications,‚Äù says Simons. Instead, he thinks it will be useful for particular scenarios that require a compact device.

üîí Cybersecurity & Privacy
‚Ä¢ KrebsOnSecurity in New ‚ÄòMost Wanted‚Äô HBO Max Series
  A new documentary series about cybercrime airing next month on HBO Max features interviews with Yours Truly. The four-part series follows the exploits of Julius Kivim√§ki, a prolific Finnish hacker recently convicted of leaking tens of thousands of patient records from an online psychotherapy practice while attempting to extort the clinic and its patients.
The documentary, &#8220;Most Wanted: Teen Hacker,&#8221; explores the 27-year-old Kivim√§ki&#8217;s lengthy and increasingly destructive career, one that was marked by cyber attacks designed to result in real-world physical impacts on their targets.

By the age of 14, Kivim√§ki had fallen in with a group of criminal hackers who were mass-compromising websites and milking them for customer payment card data. Kivim√§ki and his friends enjoyed harassing and terrorizing others by &#8220;swatting&#8221; their homes &#8212; calling in fake hostage situations or bomb threats at a target&#8217;s address in the hopes of triggering a heavily-armed police response to that location.
On Dec. 26, 2014, Kivim√§ki and fellow members of a group of online hooligans calling themselves the Lizard Squad launched a massive distributed denial-of-service (DDoS) attack against the Sony Playstation and Microsoft Xbox Live platforms, preventing millions of users from playing with their shiny new gaming rigs the day after Christmas. The Lizard Squad later acknowledged that the stunt was planned to call attention to their new DDoS-for-hire service, which came online and started selling subscriptions shortly after the attack.
Finnish investigators said Kivim√§ki also was responsible for a 2014 bomb threat against former Sony Online Entertainment President John Smedley that grounded an American Airlines plane. That incident was widely reported to have started with a Twitter post from the Lizard Squad, after Smedley mentioned some upcoming travel plans online. But according to Smedley and Finnish investigators, the bomb threat started with a phone call from Kivim√§ki.
Julius &#8220;Zeekill&#8221; Kivimaki, in December 2014.
The creaky wheels of justice seemed to be catching up with Kivim√§ki in mid-2015, when a Finnish court found him guilty of more than 50,000 cybercrimes, including data breaches, payment fraud, and operating a global botnet of hacked computers. Unfortunately, the defendant was 17 at the time, and received little more than a slap on the wrist: A two-year suspended sentence and a small fine.
Kivim√§ki immediately bragged online about the lenient sentencing, posting on Twitter that he was an &#8220;untouchable hacker god.&#8221; I wrote a column in 2015 lamenting his laughable punishment because it was clear even then that this was a person who enjoyed watching other people suffer, and who seemed utterly incapable of remorse about any of it. It was also abundantly clear to everyone who investigated his crimes that he wasn&#8217;t going to quit unless someone made him stop.
In response to some of my early reporting that mentioned Kivim√§ki, one reader shared that they had been dealing with non-stop harassment and abuse from Kivim√§ki for years, including swatting incidents, unwanted deliveries and subscriptions, emails to her friends and co-workers, as well as threatening phonecalls and texts at all hours of the night. The reader, who spoke on condition of anonymity, shared that Kivim√§ki at one point confided that he had no reason whatsoever for harassing her &#8212; that she was picked at random and that it was just something he did for laughs.
Five years after Kivim√§ki&#8217;s conviction, the Vastaamo Psychotherapy Center¬†in Finland became the target of blackmail when a tormentor identified as ‚Äúransom_man‚Äù demanded payment of 40 bitcoins (~450,000 euros at the time) in return for a promise not to publish highly sensitive therapy session notes Vastaamo had exposed online.
Ransom_man, a.k.a. Kivim√§ki, announced on the dark web that he would start publishing 100 patient profiles every 24 hours. When Vastaamo declined to pay, ransom_man shifted to extorting individual patients. According to Finnish police, some 22,000 victims reported extortion attempts targeting them personally, targeted emails that threatened to publish their therapy notes online unless paid a 500 euro ransom.
In October 2022, Finnish authorities charged Kivim√§ki with extorting Vastaamo and its patients. But by that time he was on the run from the law and living it up across Europe, spending lavishly on fancy cars, apartments and a hard-partying lifestyle.
In February 2023, Kivim√§ki was arrested in France¬†after authorities there responded to a domestic disturbance call and found the defendant sleeping off a hangover on the couch of a woman he‚Äôd met the night before. The French police grew suspicious when the 6‚Ä≤ 3‚Ä≥ blonde, green-eyed man presented an ID that stated he was of Romanian nationality.
A redacted copy of an ID Kivimaki gave to French authorities claiming he was from Romania.
In April 2024, Kivim√§ki was sentenced to more than six years in prison after being convicted of extorting Vastaamo and its patients.
The documentary is directed by the award-winning Finnish producer and director Sami Kieski and co-written by Joni Soila. According to an August 6 press release, the four 43-minute episodes will drop weekly on Fridays throughout September across Europe, the U.S, Latin America, Australia and South-East Asia.

üéì University AI
No updates.

üè¢ Corporate AI
‚Ä¢ Reimagining healthcare delivery and public health with AI
  In November 2022, OpenAI‚Äôs ChatGPT kick-started a new era in AI. This was followed less than a half year later by the release of GPT-4. In the months leading up to GPT-4‚Äôs public release, Peter Lee, president of Microsoft Research, cowrote a book full of optimism for the potential of advanced AI models to transform the world of healthcare. What has happened since? In this special podcast series, The AI Revolution in Medicine, Revisited, Lee revisits the book, exploring how patients, providers, and other medical professionals are experiencing and using generative AI today while examining what he and his coauthors got right‚Äîand what they didn‚Äôt foresee.&nbsp;



In this episode, healthcare leaders Dr. Umair Shah (opens in new tab) and Dr. Gianrico Farrugia (opens in new tab) join Lee to discuss AI‚Äôs impact on the business of public health and healthcare delivery, the healthcare-research connection, and the patient experience. Shah, a healthcare strategic consultant and former state secretary of health, explores the role of public health in the larger ecosystem and why it might not get the attention it needs or deserves and how AI could be leveraged to assist in data analysis, to help better engage with people on matters of public health, and to help narrow gaps between care delivery and public health responses during health emergencies. Farrugia, president and CEO of Mayo Clinic, traces AI‚Äôs path from predictive to generative and discusses how that progress has helped usher in a new healthcare architecture for Mayo Clinic and its partners, one powered by the goal of longer, healthier lives for patients, and how AI is also changing Mayo Clinic‚Äôs research and the education it provides, including the offering of masters and PhDs in AI and other emerging technologies.&nbsp;








Learn more:




Rickshaw Health (opens in new tab) (Shah)&nbsp;Homepage



COVID-19 After-Action Report (opens in new tab) (Shah)&nbsp;Washington State Department of Health | March 2024



Mayo Clinic Platform (opens in new tab) (Farrugia)&nbsp;Homepage



Atlas: A Novel Pathology Foundation Model by Mayo Clinic, Charit√©, and Aignostics (opens in new tab) (Farrugia)&nbsp;Publication | January 2025



MAIRA-2: Grounded Radiology Report Generation (Farrugia)Publication | June 2024



The AI Revolution in Medicine: GPT-4 and Beyond&nbsp;Book | Peter Lee, Carey Goldberg, Isaac Kohane | April 2023&nbsp;










	
		
			Subscribe to the Microsoft Research Podcast:		
		
							
					
						  
						Apple Podcasts
					
				
			
							
					
						
						Email
					
				
			
							
					
						
						Android
					
				
			
							
					
						
						Spotify
					
				
			
							
					
						
						RSS Feed
					
				
					
	




	
		
			
				
					

Transcript&nbsp;



[MUSIC]‚ÄØ



[BOOK PASSAGE]&nbsp;



PETER LEE: ‚ÄúIn US healthcare, quality ratings are increasingly used to tie the improvement in patient health outcomes to the reimbursement rates that healthcare providers can receive. The ability of GPT-4 to understand these systems and give concrete advice ‚Ä¶ has a chance to make it easier for providers to achieve success in both dimensions.‚Äù&nbsp;



[END OF BOOK PASSAGE]



[THEME MUSIC]



This is The AI Revolution in Medicine, Revisited. I‚Äôm your host, Peter Lee. 



Shortly after OpenAI&#8217;s GPT-4 was publicly released, Carey Goldberg, Dr. Zak Kohane, and I published The AI Revolution in Medicine to help educate the world of healthcare and medical research about the transformative impact this new generative AI technology could have. But because we wrote the book when GPT-4 was still a secret, we had to speculate. Now, two years later, what did we get right, and what did we get wrong? 



In this series, we‚Äôll talk to clinicians, patients, hospital administrators, and others to understand the reality of AI in the field and where we go from here.



				
				
					



[THEME MUSIC FADES]&nbsp;



The book passage I read at the top is from Chapter 7, ‚ÄúThe Ultimate Paperwork Shredder.‚Äù 



Public health officials and healthcare system leaders influence the well-being and health of people at the population level. They help shape people‚Äôs perceptions and responses to public health emergencies, as well as to chronic disease. They help determine the type, quality, and availability of treatment. All this is critical for maintaining good public health, as well as aligning better health and financial outcomes. That, of course, is the main goal of the concept of value-based care. AI can definitely have significant ramifications for achieving this.&nbsp;



Joining us today to talk about how leaders in public health and healthcare systems are thinking about and acting on this new generation of AI is Dr. Umair Shah and Dr. Gianrico Farrugia.&nbsp;



Dr. Umair Shah is a nationally recognized health leader and innovator. He led one of America‚Äôs top-rated pandemic responses as Washington State‚Äôs secretary of health, a position he held from 2020 to 2025. Umair previously directed Harris County Public Health in Texas, overseeing large-scale emergency response for the nation‚Äôs third-largest county, while building an emergency-care career spanning 20-plus years. He now advises organizations on health innovation and strategy as founder and principal of Rickshaw Health.&nbsp;



Dr. Gianrico Farrugia is the president and CEO of Mayo Clinic, the world&#8217;s top-ranked hospital for seven consecutive years, and a pioneer in technology-forward, platform-based healthcare. Under his leadership, Mayo has built and deployed the Mayo Clinic Platform. The platform enables Mayo and its partners to gain practical insights from a comprehensive repository of longitudinal de-identified clinical data spanning four continents. Gianrico is also a Mayo Clinic physician and professor and an author.&nbsp;



Umair and Gianrico are CEO-level leaders representing some of the best of the worlds of public health, healthcare delivery, medical research, and medical education.&nbsp;



[TRANSITION MUSIC]&nbsp;



Here is my interview with Dr. Umair Shah:&nbsp;



LEE: Umair, it&#8217;s really great to have you here.&nbsp;



UMAIR SHAH: Peter, it&#8217;s my pleasure. I&#8217;ve been looking forward to this conversation, and I hope you are well today.&nbsp;



LEE: [LAUGHS] I am doing extremely well.



So, you know, what I&#8217;d like to do in these conversations is first just to start, a little bit about you.



SHAH: Sure.&nbsp;



LEE: You served actually during a really tumultuous time as the secretary of health in the State of Washington. But you recently stepped away from that and you started your own firm, Rickshaw Health. So can we start there? What&#8217;s that all about?&nbsp;



SHAH: Yeah, no, absolutely. First of all, you know, I would say that the transition from Texas to Washington could not have been more geopolitically different, [LAUGHTER] as you can imagine.



LEE: Sure.&nbsp;



SHAH: You know, if you like the red-blue paradigms, you couldn‚Äôt be more, you know, red and you couldn‚Äôt be more blue, I think.&nbsp;



LEE: Yes.&nbsp;



SHAH: But what happened is, back in November this past year, as I saw some of the playout of continuation of this red-blue dynamic, I made the decision to step down. And Jan. 15, I stepped down, as you mentioned, and I spent some time really thinking about what I wanted to do next and was looking at a number of opportunities.&nbsp;



And then a moment in time, there were some things happening in our‚Äîmy wife and our family&#8217;s personal lives that sort of made me think that I wanted to focus a little bit more on family. And I felt the universe was saying, ‚ÄúStay still.‚Äù [LAUGHTER]&nbsp;



And I launched Rickshaw Health (opens in new tab) and the notion that, as you know, Peter, rickshaws are oftentimes known across the globe as these modes of transport that reliably get you through ever-changing streets and traffic patterns and all sorts of ecosystems that are evolving at all times. And they get you to the other side and they get you also with a sense of exhilaration. Like when I took my boys to Karachi, and we were‚Äîyou know, they jumped in a rickshaw and the, you know, open air [LAUGHTER] and they felt this incredible excitement.&nbsp;



And so Rickshaw Health was speaking to the three wheels of a rickshaw that symbolize the three children that we have and the real notion of how do we bring balance and agility and performance to the forefront and then move in an ever‚Äîjust like streets‚Äîever-changing healthcare environment that is constantly evolving, and we too must evolve with it. And that&#8217;s what Rickshaw Health is all about, is taking clients to that next level of trying to navigate, especially at this time, a very, very different landscape than even several months ago. So, excited about it.&nbsp;



LEE: Yeah, absolutely. You know, you made this transition from Texas to the State of Washington. And for people who listen to this podcast and don&#8217;t know, the particular part of Texas where you were‚ÄîHarris County‚Äîis really big, very, very important in that state. That&#8217;s just not, you know, the normal county in Texas.&nbsp;



SHAH: Yeah. [LAUGHS]&nbsp;



LEE: It&#8217;s actually ‚Ä¶ it&#8217;s actually known as quite a forward-looking place, technologically.&nbsp;



SHAH: That‚Äôs right.&nbsp;



LEE: So what was, you know, the transition like, then, going from, you know, possibly the most, sort of, maybe advanced county in the State of Texas, a large place, to the State of Washington?&nbsp;



SHAH: Yeah, you know, Harris County is the third-largest county in the US. So it had close to five million. And now it&#8217;s probably ‚Ä¶ it&#8217;s exceeded the five million people, and a very diverse, very forward-looking, as you mentioned, technologically very, very much looking at what&#8217;s the next horizon, and home to Texas Medical Center [TMC] as well, which is ‚Ä¶&nbsp;



LEE: Right.&nbsp;



SHAH: ‚Ä¶ the largest medical center. Of course, it had to be Texas. So it can‚Äôt be the largest in the state or the country [LAUGHTER]‚Äîthe largest in the world, right.&nbsp;



And TMC also had a number of different initiatives related to startups and venture capital and VC. And so they had launched something called TMCX. And that was a real opportunity‚Äîand I know you&#8217;re familiar with it‚Äîan opportunity to really look at how do you incubate all sorts of different innovations and bringing private sector, public sector as well as healthcare delivery alongside these startups to really look at the landscape.&nbsp;



And so when I left Houston and came to Washington, I realized that obviously, I was in the backyard ‚Ä¶ I mean, you know, you all at Microsoft Research and the work that you&#8217;re all doing is part of an ecosystem of advanced innovation that&#8217;s occurring in the Pacific Northwest that, you know, when we see all the players that are here, all the, you know, ones that do so many different things, but they&#8217;re doing them with an eye towards technology, advancements, and adoptions, it&#8217;s been quite amazing.&nbsp;



When I made that transition, it was really about, you know, the vaccines and what was happening with, you know, with COVID and fighting the‚Äîyou know, remember, this was the state that had the first case in the continental United States, had the first outbreak, and the first [lab-confirmed] death. And fast-forward a few years later, we had the fifth-lowest death rate in the US. And that was because we all came together to do so much.



LEE: Yeah, well maybe that gets us into a question that I ask a lot of our guests, which is, you know, and maybe let&#8217;s, since we&#8217;re on your time as the secretary of health in Washington State, [start] with that job. I ask, how would you explain to your mother what you do every day?&nbsp;



SHAH: [LAUGHS] I laugh because that&#8217;s been such a fascinating conversation in public health because we have oftentimes been‚Äîit&#8217;s been really hard to describe what that is.&nbsp;



LEE: Yeah.&nbsp;



SHAH: And, you know, there are so many metaphors and, you know, analogies that we&#8217;ve used. I&#8217;ve always wondered why we do not have more television shows or sitcoms or dramas that are about the public health workforce or the work that we do in the field, because you have, you know, all sorts of healthcare delivery ones, right.&nbsp;



LEE: Yup.&nbsp;



SHAH: As a practicing physician for 20 years, I realized that people knew what doctors did; they knew what nurses did, right. They intimately touch the healthcare system.&nbsp;



LEE: Yes.&nbsp;



SHAH: They understood, you know, that an ambulance picks you up at your home or somewhere else, transports you ‚Ä¶ gets you to the emergency department. The emergency department, they do some things to you or within the four walls of that ER, and then you&#8217;re either admitted, sent home, and several days, weeks, whatever later, you get home if you&#8217;re admitted, and you start your, you know, post-hospital stay at home or your rehab or what have you. And that all is known to people.&nbsp;



But when you ask your mother, your grandmother, or your, you know, your uncle, or your brother, your neighbor, your coworker about what is public health, they have a very quizzical look on their face of what that is.&nbsp;



LEE: Right.&nbsp;



SHAH: And so what I&#8217;ve ‚Ä¶&nbsp;



LEE: You know, just one thing I&#8217;ve learned is: it&#8217;s not just all the people you mentioned. Even healthcare professionals sometimes have that quizzical look.&nbsp;



SHAH: Yeah, good point. That&#8217;s right. Good point. And a lot of it is because we don&#8217;t get exposed to it or trained in it. You know, we think about public health when we&#8217;re in our training. And, you know, I&#8217;m sure you had a very similar piece of this is that, you know, you see it as, oh, that&#8217;s the health department that takes care of, you know, STDs, or it takes care, you know, it does the immunizations, or, you know, maybe they do some water quality, or maybe they do mosquitoes [mosquito control], and things like that. But the reality is, we do all of those things and more.&nbsp;



So my metaphor has been that we are the offensive line of a football team, and the healthcare delivery is the quarterback. So everybody focuses on, you know, from a few years back, everybody knows Tom Brady, right.&nbsp;



LEE: Yeah. [LAUGHS]&nbsp;



SHAH: He won the Super Bowls, everybody knows what ‚Ä¶ but if you asked people who was number 75 on the offensive line of the New England Patriots ‚Ä¶&nbsp;



LEE: Right.&nbsp;



SHAH: ‚Ä¶ or name your favorite football team. And the answer would be: you would not be able to likely answer that question. You would know Tom Brady, the quarterback, and that&#8217;s healthcare delivery, the ER doc or the hospitalist or the nurse or the, you know, the medical assistant, or the people that are doing all the work in the field that are the ones that are more visible, but the invisible workforce of the offensive line, that&#8217;s who we don&#8217;t know. And yet these are the people that are blocking and sweating and doing all things to complement the work and make sure the quarterback is successful.&nbsp;



And here&#8217;s where the metaphor breaks down, that when Tom Brady wins the Super Bowl, we continue to invest in the offensive line because we recognize the value of it and we want the quarterback to be successful the next season. But in public health or in society, we do the exact opposite.&nbsp;



When tuberculosis rates come down, we say, well, you know what? We&#8217;ve solved the problem; we don&#8217;t need it anymore.&nbsp;



LEE: Right.&nbsp;



SHAH: Or you have another, you know, environmental issue that&#8217;s no longer there, you say, ‚ÄúWe don&#8217;t need it anymore.‚Äù And we disinvest from public health or that offensive line. And then you start to see those rates go back up.&nbsp;



And so my answer to Mom and Grandma and Dad and Grandpa is we are critical to your health because we touch you every single day. And so please invest in us.&nbsp;



LEE: Yeah. And, you know, I think I&#8217;m going to want to get a little deeper on that in just a few minutes here, because, I think especially during the pandemic, that issue of not understanding the importance of that offensive lineman actually really came to the forefront.&nbsp;



And so I&#8217;d like to get into that. But the, kind of, second, kind of, standard thing I&#8217;ve been probing with people is still just focusing on you and your background is what touchpoints or experiences you&#8217;ve had with AI in the past.&nbsp;



And not everyone has. Like, it maybe isn&#8217;t too surprising that doctors and healthcare developers, tech developers, have lots of contact with AI, but would the top dog, you know, at a public health agency ever have had significant contact with AI? What about you?&nbsp;



SHAH: You know, it&#8217;s interesting. Several years ago, I was in the audience with the [then] FEMA director, [Rich Serino], who just did such an incredible job. And I remember he made this comment at that time. And, Peter, this may have been like ‚Ä¶ I don‚Äôt know‚ÄîI&#8217;m dating myself‚Äî10, 15, maybe even 20 years ago, and he said, ‚ÄúEverybody in the audience, there&#8217;s this, you know, app called Twitter.‚Äù And, you know, ‚ÄúHow many people in the audience have ever sent a tweet or know about this?‚Äù And I don&#8217;t know, maybe‚Äîit was a public health audience‚Äîmaybe about 15% of the people raised their hands.&nbsp;



He said, ‚ÄúI challenge you to right now, pick up your phone, download the app, and go ahead and send a tweet right now.‚Äù&nbsp;



And I remember I sent my first tweet at that time. And it was so thought provoking for me was that he was saying you need to be engaged in social media, but the other 85% of the audience had not even done that or had ‚Ä¶&nbsp;



LEE: Right.&nbsp;



SHAH: ‚Ä¶ even understood the importance of social media at that time. Or maybe they understood, but they had restrictions on how to utilize, right.&nbsp;



So that has stayed with me because that&#8217;s very much about this revolution of AI that I know that public health and population health practitioners like myself who have been in the trenches and understand the importance of it, they really believe in the importance or think they know the importance.&nbsp;



But NACCHO, the National Association of County and City Health Officials, had done a survey of local health agencies. And about two-thirds, if not three-quarters, of local health agencies reported that they had an AI capacity that was low or lower than ideal.&nbsp;



LEE: Hmm. Yeah. Yeah.&nbsp;



SHAH: And that is very much where I come from. When I was in public sector and at the state health agency, our transformation was very much about how do we advance the work, and how do we utilize this in a population health standpoint?&nbsp;



And I was fortunate to have a chief of innovation at Washington State Department of Health, Les Becker, who understood the value of AI. And as you know, we did also hold a AI science convening that ‚Ä¶&nbsp;



LEE: Yep. Yeah.&nbsp;



SHAH: ‚Ä¶ your team was there with University of Washington. And that was really an opportunity for us to say that AI is here. It&#8217;s not tomorrow. It&#8217;s not next year. It&#8217;s not the future. It&#8217;s already here. We need to embrace it.&nbsp;



But here&#8217;s the problem, Peter, far too few people in our field understand just how to embrace it.&nbsp;



LEE: Right.&nbsp;



SHAH: So I have become a markedly more champion of AI. One, since I read your book. So I think there‚Äôs that. So thank you for writing it. But two, since I really recognize that when I became a solo or a primary-few practitioner in my own realm, I needed to force-amplify the work that I was doing.&nbsp;



And when I look back, and I continue to stay in touch with my colleagues in the field of public health, what they&#8217;re also struggling with is that you have an epidemiologist who&#8217;s got a mound of information‚Äîdata, statistics, etc.‚Äîthat they are going through, and they&#8217;re doing everything in their power to get that processed and analyzed.&nbsp;



LEE: Yep. Yep.&nbsp;



SHAH: AI can take 80% of that and do it. And that epidemiologist can now turn to more of an overseer and a gatekeeper and to really recognize the patterns ‚Ä¶&nbsp;



LEE: Yep.&nbsp;



SHAH: ‚Ä¶ and let AI be able to do the, you know, grunt work. And similarly, as you know, measles‚Äîwith the outbreaks that we&#8217;ve seen, especially in Texas but elsewhere‚Äîyou&#8217;ve got an opportunity where our communications people who are saying, ‚ÄúLook, we&#8217;re about to have, or we know we&#8217;re about to announce that there&#8217;s a measles outbreak in, you know, in our community or our state or what have you‚Äîour region.‚Äù&nbsp;



And they can have AI go through different press briefings and/or press releases and say, ‚ÄúGive me the state of the art on how I should communicate this message to the community.‚Äù&nbsp;



LEE: Hmm.&nbsp;



SHAH: And bam! You can do that. And now you can oversee that work, as well. And then the third example is that we are always looking at how do we find ways to have a deeper connection with those who come to our, you know, our websites or come to our engagement tools‚Äîwith bots and things like that. AI can really accelerate that work, as well. So there&#8217;s so many use cases that AI has for population health or public health.&nbsp;



LEE: Yeah.&nbsp;



SHAH: But I think the challenge is that we just don&#8217;t have enough adoption because they&#8217;re ‚Ä¶ one, we&#8217;ve had funding cuts, but two is that there is this real hesitation on, what is it that we can do? And I argue‚Äîthe last thing I&#8217;ll say about this, Peter‚Äîis that I argue that AI is happening right now. The discussions, the technology advancements, the work, the policy work, all that&#8217;s happening right now. If public health practitioners are not at the table, if they&#8217;re not part of the, ‚Ä¶&nbsp;



LEE: Right.&nbsp;



SHAH: ‚Ä¶ &#8220;What does this look like? How does it work in our field?&#8221; &#8230; guess what? It&#8217;s going to be done to us and for us rather than with us. And if we do not get with that and get to the table, then unfortunately it may not be exactly what we want it to be at the end of the day.&nbsp;



LEE: I find it really interesting that you are using the terms ‚Äúpublic health‚Äù and ‚Äúpopulation health‚Äù ‚Ä¶&nbsp;



SHAH: Yeah.&nbsp;



LEE: ‚Ä¶ pretty much interchangeably here. And I think that that&#8217;s something that I think touches on an assumption that was both implicit and explicit in the book that we wrote, which is: we were making some predictions that our ability to extract insights and knowledge from population health data would be enhanced through the use of AI. And I think that it looks to me like that has been more challenging and has come along more slowly over the past two years. But what is your view?&nbsp;



SHAH: Yeah, I think part of, and I think you and I have had this conversation, you know, in bits and pieces. I think one of the real challenges is that when even tech companies, and you can name all of them, when they look at what they&#8217;re doing in the AI space, they gravitate towards healthcare delivery.&nbsp;



LEE: Yes.&nbsp;



SHAH: Right? That&#8217;s, it&#8217;s ‚Ä¶&nbsp;



LEE: And in fact, it&#8217;s not even delivery. I think techies‚ÄîI did this, too‚Äîtend to gravitate specifically to diagnosis.&nbsp;



SHAH: Yes, that&#8217;s right. That&#8217;s right. You know, I think that&#8217;s a really good point. And, you know, when you look at sepsis or you look at pneumonia or try to figure out ways that, you know, radiologists or x-rays or CT scans can be read, it&#8217;s, I mean, there are so many use cases that are within the healthcare sector. And I think that gets back to this inequity that we have when we look at population health or, you know, this broad, um, swath of land that is, oftentimes, left behind or unexplored, and you have healthcare delivery. Now, healthcare delivery we know gets 95 cents or 96 cents of every dollar. So it makes sense why, right. But we also know that, at the end of the day, we&#8217;re looking at value-based outcomes, and you cannot be successful in the healthcare delivery system unless we are truly looking at prevention and what&#8217;s happening in the community and the population.&nbsp;



LEE: Right.&nbsp;



SHAH: And that&#8217;s why I use it interchangeably, but I know that ‚Äúpublic health‚Äù has got a very specific term, and ‚Äúpopulation health‚Äù is a different set of ways of looking at the world. The reason that people try to shy away from pop health in essence is that you could talk about population health as being my population of patients in a clinic. It could be my health systems population. It could be an insurance company saying, these are the lives covered, right. So it becomes, what is population? When we think of public health, we think of the entirety of the population, right. In the State of Washington, eight million people. Harris County, five million people. Or in the US, 300‚Äîwhatever the number of millions of people that‚Äîwe think of the entire population. And what is it that actually impacts the health and well-being of that population is really what that&#8217;s about.&nbsp;



Yet here&#8217;s the challenge. When we then talk to those of our partners and our colleagues in the tech field, there are two things happening. One is, there&#8217;s a motivation because of the amount of dollars that are in [the] healthcare sector. And number two is, because it&#8217;s more familiar, right.&nbsp;



LEE: Right.&nbsp;



SHAH: And so there are very few practitioners similar to me that are out there, that are in the pop health who kind of know healthcare delivery because they&#8217;ve also seen patients, but they&#8217;re also‚Äîthey worked at that federal, state, local level, community level‚Äîthey&#8217;ve, you know, they&#8217;ve done you know various different kinds of environments.&nbsp;



And they say, ‚ÄúLook, I&#8217;ve got a perspective to really help a tech company or somebody see the rest of it,‚Äù but you have to have both partners coming together to see that. And I think that&#8217;s one of the real challenges that we have.&nbsp;



LEE: Yeah.&nbsp;



And so now I&#8217;m going to want to go into specific problems, ‚Ä¶&nbsp;



SHAH: Yeah. Sure.&nbsp;



LEE: ‚Ä¶ and maybe COVID is a good thing to focus on‚Äîthe breadth of problems that had to get solved in pandemic response and where the gaps between healthcare delivery and public health were really exposed.&nbsp;



And so the first problem that I remember really keenly that just seemed so vexing was understanding where the PPE was, the personal protective equipment &#8230;&nbsp;



SHAH: Hmm. Yeah.&nbsp;



LEE: ‚Ä¶ and where it needed to be.&nbsp;



SHAH: Yes.&nbsp;



LEE: And so that turned out ‚Ä¶ you would think just getting masks and gowns and gloves to the right places at the right times or even understanding where they are so that, you know ‚Ä¶ and being able to predict, you know, what hospitals, what clinics are most likely to get a big influx of patients during the height of the pandemic would be something that would be straightforward to solve, but that turned out to be an extremely difficult problem.&nbsp;



But how did it look from where you were sitting? Because you were sitting at the helm having to deal with these problems.&nbsp;



SHAH: Yeah, we were constantly chasing data and information. And oftentimes, you know, because a lot of these data systems in the public health sector have been underinvested in over the decades, then, you know, you had our biggest emergency crisis of our time, and a lot of public health agencies were either getting, you know, thrown a whole host of resources or had to create things on the fly.&nbsp;



And whether that was at Harris County or in the State of Washington, I will tell you that what I saw was that, you know, a lot of agencies across the country were still using fax machines, you know, to get data that were coming in.&nbsp;



And I remember actually‚Äîit&#8217;s kind of a funny story‚Äîthere was a fax machine that was highlighted down in our agency in Texas. And we actually had this fax machine, had mounds of, you know, data ‚Ä¶ sorry, papers that were next to ‚Ä¶ faxes that were coming in and all these things.&nbsp;



And you would have, you know, Mr. Peter Lee listed as a patient. And then the next, you know, transmission would have Pete Lee. And then the next transmission would have Peter Lee, but instead of L-E-E, it was L-E-A-H or something, or L-I or something, right. And it was just ‚Ä¶&nbsp;



LEE: Right.&nbsp;



SHAH: &#8230; or you had a date of birth missing, or you had, you know, an address that was off. And what we realized is that over time, a lot of the data that were coming in were just incomplete data, and being able to chase that was really hard.&nbsp;



And so, you know, I think AI has that potential to really organize it, and to stratify it, and to especially get you to a point of at least cleaning it up. So I don&#8217;t think it&#8217;s just that AI ‚Ä¶ AI doesn&#8217;t just save time; it saves lives. Truly used ‚Ä¶&nbsp;



LEE: Hmm. Yeah.&nbsp;



SHAH: ‚Ä¶ that&#8217;s, I think, where we&#8217;re talking here.&nbsp;



And so when you have PPE and things of that nature, as you talked about, here in the State of Washington or what we were trying to do to get vaccines out or everything we&#8217;re doing to try to get communication messages to the public. And we did a fantastic job of that, although not ideal.&nbsp;



I mean, there are so many things that I could point to that we could have done better‚Äîall of us in the field of public health and healthcare delivery alike.&nbsp;



I will tell you that the one thing that stays with me is that if we had those tools then, and we had them in place then, and we had invested in them at that time in advance of, I think there was a real opportunity for us to be able to move ahead and even be better at how we affected the health outcomes of the very populations that we were trying to get to.&nbsp;



And I think it&#8217;s [that] AI allows us to shift from reactive to proactive systems, catching health issues before they escalate and allow us to really communicate with empathy at scale.&nbsp;



LEE: Right.&nbsp;



SHAH: And when we can do those things, whether it&#8217;s opioids or whether it&#8217;s, you know, something that&#8217;s happening related to an infectious disease, or, you know, even this, the new agenda with Make America Healthy Again‚Äîwhich by the way, as you know, we had a Be Well, WA &#8230; Be Well, Washington ‚Ä¶&nbsp;



LEE: Right. Yes.&nbsp;



SHAH: ‚Ä¶ very much that was about, you know, looking at, you know, physical health and nutritional health and emotional well-being and social connectedness‚Äîthat there is a real opportunity for us to address the very drivers of ill health. And when we can do that, and AI can help us accelerate that, I think we truly have the ability to drive down costs and increase the value that&#8217;s returned to all of us.&nbsp;



LEE: What is your assessment of public health agencies‚Äô readiness to use technology like AI? Because if there&#8217;s one thing AI is good at, it&#8217;s predicting things. Are they [public health agencies] in a better position to predict things now?&nbsp;



SHAH: You know, I think it&#8217;s a tale of two cities.&nbsp;



I think on the one hand, we&#8217;re better because we have the tools. On the other hand, we&#8217;ve lost the capacity to be able to utilize those tools. So, you know, it&#8217;s a plus and a minus.&nbsp;



Many, many years ago, there was the buzzword of what we called syndromic surveillance. And, Peter, you know this term well.&nbsp;



It was like you would have, you know, a whole host of accumulation of data points in, let&#8217;s say, a hospital setting or an emergency department ‚Ä¶&nbsp;



LEE: Yup. Yup.&nbsp;



SHAH: ‚Ä¶ where, you know, you‚Äôd have runny nose, you&#8217;d have cough, you&#8217;d have a fever, and you would take that, what was happening and people presenting to the emergency department, with what was happening in the area pharmacies where people were going to get Kleenexes and tissues ‚Ä¶&nbsp;



LEE: Yep.&nbsp;



SHAH: ‚Ä¶ and buying over-the-counter, you know, medication, and things of that nature, Tylenol, etc.&nbsp;



And you would say ‚Ä¶ you would put those two things together, and you would come up with a quote-unquote ‚Äúsyndrome,‚Äù and you would say our ability to say there was an alert to that syndrome allows us to say something uh-oh is going on in the community, and we got many, many advancements related to wastewater surveillance over the last several years as you know ‚Ä¶&nbsp;



LEE: Yep. Yep. Well, also, wasn&#8217;t patient number one in the United States discovered also because of the Seattle Flu Study, or at least that sort of syndromic surveillance.&nbsp;



SHAH: That&#8217;s right.&nbsp;



LEE: They weren&#8217;t even looking for COVID. They were just taking, you know, snot samples from people.&nbsp;



SHAH: That&#8217;s right. That&#8217;s right. That&#8217;s right.&nbsp;



And so that&#8217;s the kind of thing that you, you know, we underappreciate. Is you have to have a smart, intelligent, agile practitioner, right.&nbsp;



So if I think about down in Dallas when Ebola was, you know, the gentleman who was, you know, the index case for Ebola was sent out of the emergency department and came back several days later.&nbsp;



And it was the nurse who picked up this time because the practitioner, the provider, the healthcare provider, the doc missed it. And I wouldn&#8217;t want to say in a negative way. It was just, like, not obvious. You aren&#8217;t thinking of Ebola in the middle of Texas. And it was the nurse who picked up: there&#8217;s something wrong here.&nbsp;



And what AI has the ability to do is to pick up those symptoms &#8230;&nbsp;



LEE: Yeah.&nbsp;



SHAH: ‚Ä¶ or those patterns and be able to recognize the importance of those and be able to then alert the practitioner. So what I ‚Ä¶ we call it artificial intelligence‚Äîit almost becomes artificial wisdom.&nbsp;



LEE: Hmm. Yeah, interesting. So that actually reminds me of my next question, which is another thing that I watched you and public health officials do is try to play ‚Äúwhat if‚Äù games.&nbsp;



So, for example, I think one decision you were involved in had to do with, you know, what would be the impact if we put a ban on large gatherings like concerts or movie theaters or imposed an 8 PM curfew on restaurants, and you were trying to play ‚Äúwhat if‚Äù games. Like, what would be the impact on the spread of the pandemic there?&nbsp;



So now, again, today with AI, would that aspect of what you did play out differently than it did during the pandemic?&nbsp;



SHAH: As you know, COVID was the most studied condition on the planet at one point. And it was, you know, things that usually we would learn over years or months, we were learning in weeks or days or hours.&nbsp;



And I remember in Houston, I would say something in the morning, and I would always try to give the caveat, ‚ÄúThis is the best information we know right now,‚Äù because it kept changing, whether it was around masks or whether it was around, you know, the way the virus was operating, whether it was around &#8230;&nbsp;



I remember even ‚Ä¶ I was just watching something recently where I was asked to comment about whether spiders could transmit COVID-19. You know, just questions that were just evolving, evolving, evolving. And the information was evolving. By morning, you would say something. By evening, it would change.&nbsp;



And why I say that is that it would have been great in the pandemic if we could have said, if you could give us all the information that&#8217;s happening across the globe, synthesize that information, and be able to help us forecast the right decisions that we should be making and help us model that information so we could decide: if you did a curfew, or if you did, you know, a mask, or if you could, you know, change something else related to policy‚Äîwhat are the impacts of it?&nbsp;



LEE: Yeah.&nbsp;



SHAH: What we found constantly in public health was that we were weighing decisions in incomplete data, incomplete information.&nbsp;



So great now that everybody can armchair quarterback looking back three, five years ago and say, ‚ÄúI would have done it this way,‚Äù or ‚ÄúI would have done it that way.‚Äù Gosh, I would have as well. But guess what‚Äîwe didn&#8217;t have that information at that time. And so you had to make the best decisions you could with incomplete data.&nbsp;



But what AI has the potential to do is to help complete the incomplete data. Now, it&#8217;s not going to get 100%.&nbsp;



LEE: Right.&nbsp;



SHAH: And I think, Peter, you know, the one thing we&#8217;ve got to be really mindful [of] is phantom information, or information where it sort of makes up things, or may somehow get you incomplete information, or skews it a certain way.&nbsp;



This is why we can&#8217;t take the person out of it yet.&nbsp;



LEE: Right.&nbsp;



SHAH: Now, maybe one day we can.&nbsp;



I&#8217;m not one of those Pollyanna-ish that people will never be replaced. I actually believe that those people who are skilled with AI and the tools will eventually have a competitive advantage over those who are not. Just like if I had a physician who knows how to use their smartphone or knows how to use a word processor or knows how to do a PowerPoint presentation is going to replace the ones that use scantrons &#8230;&nbsp;



LEE: Yeah. Yeah.&nbsp;



SHAH: ‚Ä¶ or the ones that write it on pieces of paper‚Äîthat eventually it makes it more efficient and effective, but we&#8217;re not there yet. But I think that the potential is absolutely there.&nbsp;



LEE: So I have one more question. And you can, kind of, tell I&#8217;m trying to expand people&#8217;s understanding of just the incredible breadth of what goes on in public health, you know, all of these sorts of different issues.&nbsp;



And again, just sticking to COVID, but this is a much broader issue. Another thing you had to cope with were significant rise of misinformation ‚Ä¶&nbsp;



SHAH: Yes.&nbsp;



LEE: ‚Ä¶ and maybe going along with that, very, very significant inequities in outcomes in the COVID response. And when you think about AI there, I think you can argue it both ways, that it both exacerbates the problem but also gives you new tools to mitigate the problems.&nbsp;



What is your view?&nbsp;



SHAH: I think you ‚Ä¶ I don&#8217;t even have to say it ‚Ä¶ I think you hit on it, is that, you know, it really is two sides of one coin.&nbsp;



On the one hand, it has the power of really advancing and allowing us to move forward in a way that incredibly accelerates and accentuates, but on the other hand, in the case of inequities, right? So if you have inequitable information data that&#8217;s already out in the literature or already out in the, you know, media, or what have you, about a certain population or people or certain kinds of ideas or thoughts, etc., then AI will tend to accumulate that. You&#8217;re going to take that information, thinking that&#8217;s the best out there, but it may have missed out on information and now you go with it. And that&#8217;s a potential problem.&nbsp;



And I think it&#8217;s the same thing on information is that when we have people that are able to classify or misclassify information, I think it really becomes hard because it can accelerate the inequities of trust or inequities of trusted sources of information. It can also close the gap.&nbsp;



So I think, you know, it&#8217;s really up to us and this responsible AI to really think about how we can go about doing this in a way that&#8217;s going to allow us to further the advancements but also be careful of those, you know, those kind of places where we&#8217;re going to step into that are not going to be well received or successful.&nbsp;



You know, the one thing that&#8217;s really fascinating about this whole conversation is that this is why we&#8217;ve got to be at the table, Peter.&nbsp;



LEE: Yep. Yep.&nbsp;



SHAH: Because if we&#8217;re not at the table, you know, what&#8217;s the, you know, or if tech companies that are out there doing this work and aren&#8217;t even seeing a field of practitioners that are actually wrestling with the same problems but just cannot actually get to the solutions, we&#8217;re just going to continue to accentuate the problems.&nbsp;



And that&#8217;s why I&#8217;m a firm proponent of: we&#8217;ve got to be at the table.&nbsp;



And so even when we&#8217;ve seen in, and this is going to be a little controversial, but governmental spaces where, you know, policymakers have said, ‚ÄúLook, we are not going to let you do certain things,‚Äù or they say to public health practitioners or even healthcare delivery practitioners in certain spaces, ‚ÄúYou cannot even play with this. You cannot have it on your phones. You can&#8217;t do any &#8230; ‚Äù&nbsp;



You know, what I really believe it does is that it takes [an] almost like we put our head in the sand type of approach rather than saying, ‚ÄúWhat is it that we can do to help improve AI and make it work for all of us?‚Äù What we&#8217;re doing is we&#8217;re essentially saying, ‚ÄúWe&#8217;re going to let the tech companies and all the other developers come up with the solutions, but it&#8217;s not going to be informed by the people in the field.‚Äù And that&#8217;s dangerous. We have to do both. We have to be working together.&nbsp;



LEE: Umair, that&#8217;s really so well said, and I think a great way to wrap things up. I&#8217;ve certainly learned a lot from this conversation. So thank you again.&nbsp;



SHAH: It&#8217;s been a pleasure to be with you this morning. Thank you so much for the time. And I&#8217;m looking forward to further conversations.&nbsp;



[TRANSITION MUSIC]‚ÄØ



I live in the State of Washington and because of that, I&#8217;ve been able to watch Umair in action as our state&#8217;s former secretary of health. And some of that action was pretty intense to say the least because his tenure as secretary of health spanned the period of the COVID pandemic.&nbsp;



Now, as a dyed-in-the-wool techie, I have to admit that at the beginning, I don&#8217;t think I really understood the scope and importance of the field of public health. But as the conversation with Umair showed, it&#8217;s really important and it is arguably both an underfunded and underappreciated part of our healthcare system.&nbsp;



Now, public health is also very much an area that&#8217;s ripe for advancement and transformation through AI. As Umair explained in our discussion, the core of public health is the idea of population health, the idea of extracting new health insights from signals from population-scale data. And already we&#8217;re starting to see AI making a difference.&nbsp;



Now here&#8217;s my interview with Dr. Gianrico Farrugia.&nbsp;



LEE: Gianrico, it&#8217;s really great to have you here today.&nbsp;



GIANRICO FARRUGIA: Peter, thanks for having me. Thanks for making me part of your podcast.&nbsp;



LEE: You know, what I&#8217;d like to do in these conversations is, you know, we&#8217;ll definitely want to talk about the overall healthcare system, the state of healthcare, and what AI could or might do to help or even hurt all of that. But I always like to start with a sharper focus just on you specifically. And my first question always is, you know, I think people imagine what a hospital or a health system president and CEO does, but not really. And so how would you explain to your mother what you do every day?&nbsp;



FARRUGIA: So, Peter, my mother‚Äôs 88 years old. She lives in Malta, and she‚Äôs visiting at the moment, ‚Ä¶&nbsp;



LEE: Oh, wow.&nbsp;



FARRUGIA: ‚Ä¶ which is kind of nice, really.&nbsp;



LEE: Wow, that is amazing.&nbsp;



FARRUGIA: I&#8217;m proud that she&#8217;s still proud of me. So she does ask. I&#8217;ll tell her the scope of Mayo Clinic. We serve patients across the globe. We have about 83,000 staff members that work with us, and we&#8217;re very proud of the work we do in research, education, and the practice.&nbsp;



Mayo Clinic is built to serve people with serious disease. So what I tell my mother is that here we are. We&#8217;re a healthcare organization that knows what it needs to do: keep patients as the North Star. The needs of the patient come first. We have 83,000 people who want to do that, several thousand physicians and scientists. My job is to look slightly ahead and then share what I&#8217;m seeing and then, sort of, smooth the way for others to make sure Mayo remains true to its mission but also true to the fact that at the moment, we are in a category of one. We need to remain there not just from an ego standpoint, but really from a ‚Äúdo good to the world‚Äù standpoint.&nbsp;



At that point, invariably my mother will tell me that I&#8217;m working too hard. [LAUGHTER] And then of course, I change the subject, and I ask her what she cooked today because my mother, who‚Äôs 88, cooks for the whole family in Malta, and there are usually four generations eating around the table. So I tell her what she does for the family is what I do for the Mayo family.&nbsp;



LEE: Wow, that&#8217;s a great way to put it. And it sounds like you actually have a good chance to have some good genes if she&#8217;s still that active at age 88.&nbsp;



FARRUGIA: I think I chose a little more stressful job that may limit [that]. I will tell you very briefly is that one of the AI algorithms we have estimates biological age from an electrocardiogram. My biological age jumped by 3.7 years when I became CEO.&nbsp;



LEE: [LAUGHS] Oh no.&nbsp;



FARRUGIA: I&#8217;m hoping it will reverse on the other side.&nbsp;



LEE: To stick with you just for one more moment here, second question I ask is about your origin story with respect to AI. And typically, for most people, there is AI before ChatGPT and generative AI and then after the generative AI revolution. So can you share a little bit about this? Because it must be the case that you&#8217;ve been thinking about this a long time since you&#8217;ve really led Mayo Clinic to be so tech forward in this way.&nbsp;



FARRUGIA: Well, I&#8217;ve been, as you said, a physician for way too long. I got my MD degree in ‚Äô87. So that sort of dates me. But it also means that I saw a lot of the promise for AI that never seemed to pan out for decades and decades and decades like you did.&nbsp;



LEE: Yeah.&nbsp;



FARRUGIA: Around 10 years ago, Mayo could sense that there was something different, that something was changing, that we actually‚Äîat that time, predictive AI‚Äîcould make a big difference. And I think that&#8217;s the moment where I and others jumped in and said Mayo Clinic needs to be involved.&nbsp;



And then about six years ago, when‚Äîsix and a half years ago‚ÄîI became CEO, it was clear that there was the right confluence of data, knowledge, tech expertise, that we could deal with what was increasingly bothering me, which is that we knew what was coming from a technology standpoint and we knew the current healthcare system could not deliver on what patients need and want within that current system. And so the answer is, how could a place like Mayo Clinic with our reputation not jump in and say there has to be a better way of doing things? I&#8217;ve always said that it is impossible for me to understand that every single government employee is incompetent. Every physician is greedy. Something&#8217;s wrong here.&nbsp;



LEE: Yeah.&nbsp;



FARRUGIA: And that wrong was the architecture was wrong. And we knew that we could incorporate AI and make it better. So for me, that journey was one of wait, wait, wait. 10 years ago, begin to jump in. Six years ago, really jump in with our platform. And then, of course, in November 2022, things changed again.&nbsp;



LEE: Yeah. When did this idea of a data platform, what you now call the Mayo Clinic Platform (opens in new tab)‚Äîby the way, I refer to this as MCP, ‚Ä¶&nbsp;



FARRUGIA: Yeah, I know. [LAUGHS]&nbsp;



LEE: [LAUGHS] ‚Ä¶ which I always smirk a little bit because, of course, for those of us in computer science research, the AI research, MCP has also become quite a hot topic because of the model context protocol version of this. But for Mayo&#8217;s MCP, when did that become a serious, defined initiative?&nbsp;



FARRUGIA: So around the end of ‚Äô18, 2018, beginning of 2019. At that point, we knew that we were going to do something differently. We came up with a strategic plan, as I took on the job, that we needed to cure more patients. There‚Äôs just not enough cures in the world. There&#8217;s too much suffering. And that we had all these chronic diseases that people have accepted are chronic, but really the only reason that disease is chronic is you haven&#8217;t cured it.&nbsp;



And physicians have been afraid to talk about cure because, of course, eventually everybody passes away.&nbsp;



LEE: Yeah.&nbsp;



FARRUGIA: But I really pushed hard to say, no, it&#8217;s OK to talk about cure. It&#8217;s OK to aspire to cure. The second was connect‚Äîconnecting people with data to create new knowledge. And that&#8217;s where it became clear that data were not currently in a format that were particularly useful. By the way, you&#8217;ll hear me talk about data in the singular and the plural. I&#8217;m old school. I talk about data as plural, but I know that most younger people now use data singular. [LAUGHTER] And I apologize if I&#8217;ll go through that.&nbsp;



And then the third was transform. Let&#8217;s use Mayo&#8217;s resources to transform healthcare for ourselves and for others. And that&#8217;s the concept of, if we are able to use data in a different way, let&#8217;s create a different architecture. And that architecture had to be very closely linked to using artificial intelligence in order to create better outcomes for patients. So patients can live not only longer lives but healthier lives. And that&#8217;s the genesis of MCP, Mayo Clinic Platform, so I&#8217;ll timestamp that as end of 2018, beginning of 2019.&nbsp;



LEE: So I&#8217;m really wanting to delve in in this episode, in this conversation, you know, [into the] mindset of a health system or hospital CEO. And so you&#8217;re obviously thinking about, I guess, machine learning and predictive analytics and so on. What were the, kind of, like ‚Ä¶ in 2018, what were the outcomes that you were dreaming about from this? So if you had this thing, you know, what were the things that you were hoping to be able to show or, kind of, produce as results?&nbsp;



FARRUGIA: So first of all, I think all of us who work at Mayo Clinic, and this tends to be a bit sugary, but it&#8217;s true, strongly feel that we have a responsibility to leave the place better than when we started. And so the Mayo brothers, when they started, did two really important things. The first was that they created the first integrated healthcare system. And the second, they created the first unified record. And that record was, of course, paper at that point.&nbsp;



Part of that is to say, OK, what does it look like now versus how can we improve what we have if ‚Ä¶ it&#8217;d be blasphemy to say, let&#8217;s think of ourselves as the Mayo brothers, but let&#8217;s think of ourselves as reasonably smart people at Mayo Clinic, really lucky to be surrounded by very smart people with resources. What will we do? And so we said let&#8217;s not aim for the low-hanging fruit. Let&#8217;s aim to get at whatever you want to call it, the intractable knot, the hardest problem, and that is clinical care. Let&#8217;s improve clinical care. Yes, we can deal with burnout. Yes, we can deal with administrative burden. But let&#8217;s not focus on that. Let&#8217;s really create an architecture that allows us to tackle better clinical outcomes.&nbsp;



And by starting there, then everything flows from that. That it&#8217;s not really worth doing unless at the end of the day, people are experiencing better health.&nbsp;



LEE: And so I know a very good colleague and friend of mine, John Halamka (opens in new tab), you ended up hiring. I thought he was a very interesting choice because he is, of course, in terms of technology, quite deep and very expert, but he&#8217;s, I think, first and foremost, a doctor. And so I assume you must have had to decide what type of person you would bring in and what kinds of people you would bring in to try to create such a thing. What was your thinking around the choice of someone like John?&nbsp;



FARRUGIA: It was one of the harder decisions. First of all, [I&#8217;m] a physician myself. We tend to want to maintain some control. And so now I am the CEO, [LAUGHTER] and I have to give this baby to somebody else. That&#8217;s very hard. Second is Mayo Clinic is really good because it is flat, and we run a lot by committee. But it also means that, therefore, you have to work really hard at change, and you cannot change by fiat. You have to change by convincing people.&nbsp;



So I just ‚Ä¶ I&#8217;ve always made the point that the right change agent is a servant leader because that&#8217;s how change becomes embedded. But it also means you&#8217;ve got to have that personality, the Mayo personality. And it became clear when we interviewed [that] there were some people that were really hardcore tech; others that were passionate about social issues. But John really fit that of being, as you said, deep in IT but also himself very aligned with the Mayo Clinic values. It&#8217;s as if he was a Mayo Clinic physician even though he wasn&#8217;t.&nbsp;



And that came together, and I felt, we felt, that as we were hiring, that we could do it. And then we did something interesting. We paired John with a ‚Ä¶ we created the role of a chief medical officer for the platform, which was a longstanding Mayo Clinic physician. And so we brought them together so we could get the past and the present and the future working together.&nbsp;



LEE: So I&#8217;m going to ask you about what has come out of this. But before that, let&#8217;s get back to this origin story. So now, all of that is being set up starting around 2018. But then, you know, in 2022, there is generative AI. Now you were already experimenting with transformers, starting with BERT out of Google there. So maybe that&#8217;s a couple of years earlier. But still, there has to come a point where things are feeling very disrupted.&nbsp;



FARRUGIA: Yeah, so, you know, it really wasn&#8217;t. It, to me, was a relief because it gave this ‚Ä¶ we were feeling pretty good about what we&#8217;re doing. We were feeling a little impatient, but, in true Mayo fashion, were willing to, sort of, do everything, take its time, take it to the right committees, get the right approvals, and get it done.&nbsp;



And so when generative AI came, for us, it&#8217;s like, I wouldn&#8217;t say we told you so, but it&#8217;s like, ah, there you go. Here&#8217;s another tool. This is what we&#8217;ve been talking about. Now we can do it even better. Now we can move even faster. Now we can do more for our patients. It truly never was disruptive. It truly immediately became enabling, which is strange, right, ‚Ä¶&nbsp;



LEE: Yeah.&nbsp;



FARRUGIA: ‚Ä¶ because something as disruptive as that instantly became enabling at Mayo Clinic. And I&#8217;ll take ‚Ä¶ as I think about it with you and take a moment to think and reflect on it, I think there were a couple of decisions we made earlier on that really helped us. We made the decision against the advice of any consulting firm to completely decentralize AI at Mayo Clinic six years ago. And we told our clinical department, you need to own this. You need to hire basic scientists in AI. We&#8217;ll help you by creating the infrastructure. We&#8217;ll help you by doing all the rest. We&#8217;ll have the compute. We&#8217;ll have the partners. You need to do this on your own. You need to treat this the same way as if a new radiological technique happened or a new surgical technique happened.&nbsp;



And so there was a lot of expertise already present in a very diffused way that then we were able to layer on generative AI onto that. And we found a very willingness to embrace it. In fact, I would argue initially a bit too willing because as you know, we haven&#8217;t quite figured out what&#8217;s legitimate use, what&#8217;s not use.&nbsp;We all learned together.



LEE: Right. Yep. Yep.&nbsp;



FARRUGIA: But it was mostly energy, which is really interesting. It was mostly energy.



LEE: Wow. And, you know, it&#8217;s an amazing thing to hear because one common theme that we hear is that the initial reaction is oftentimes one of skepticism. In fact, I&#8217;ve been very open that even I initially had some skepticism. Was that not present in your mind or on your team&#8217;s mind at all at the beginning?&nbsp;



FARRUGIA: So you&#8217;re asking a physician if they are skeptical about something. [LAUGHTER] Yeah. I wonder what the answer to that is. Absolutely. The first hallucination, the first wrong reference. Can you imagine if you write the grant and the wrong reference comes. As you know, ‚Ä¶&nbsp;



LEE: Right.&nbsp;



FARRUGIA: ‚Ä¶ earlier on when some references were being made up. So massive amounts of skepticism. But the energy was such there that the people [who] were skeptical were also at the same time saying, ‚ÄúLet&#8217;s do a RAG [retrieval augmented generation] to clean up those references. Let&#8217;s create ‚Ä¶‚Äù We were experimenting with discharge summaries, but let&#8217;s use AI to police AI, and let&#8217;s see what&#8217;s going on. So there was more massive skepticism, but the energy was pushing that skepticism into a positive versus into a negative frame. Now, I say that summarizing in hindsight.&nbsp;



LEE: Yeah.&nbsp;



FARRUGIA: Day to day, much more complicated than that. But overall, if you just ‚Ä¶ and remember, I had been at the World Economic Forum many years ago and had said, healthcare needs to run towards AI.&nbsp;



LEE: Yes.&nbsp;



FARRUGIA: If healthcare was perfect, we would wait. Healthcare is not perfect by any means, therefore let&#8217;s run and embrace AI. And, sort of, that mentality was part of who we were because at the same time, we were also saying the other thing, that we need to be the ones to lead validation. We need to be the ones that set the rules. We need to be participating in the creation of CHAI [Coalition for Health AI] (opens in new tab). We need to be participating as the [National] Academy of Medicine (opens in new tab).&nbsp;



So people did feel that Mayo was being fairly responsible about it, but that urge to, the needs of the patient come first, was the driver that kept people wanting to say, ‚ÄúNot ready yet, but let&#8217;s make it ready.‚Äù And we now have 320 algorithms in the practice, and they run and we constantly are looking and seeing what else we can do to improve. But as you well know, things evolve and change. And we&#8217;re also looking and seeing which ones work and which ones don&#8217;t and which ones we have to work together on to make better.&nbsp;



LEE: Yeah, you know, of course Mayo has such a, you know, such a reputation and is so influential, but in the world of healthcare broadly, let&#8217;s just focus on the United States to start. How common is this experience? You know, so if you are at a meeting with fellow CEOs of hospitals and health systems, what is the attitude and what is the, kind of ‚Ä¶ how common is the approach to all of this?&nbsp;



FARRUGIA: I think it&#8217;s more common now, but going back a few years, I think it&#8217;s fair to say that it was scary for people to know how it&#8217;s going to change things. Healthcare runs on very narrow margins. It&#8217;s very expensive. So your expenses and your revenue are both massive, and they are very close to each other. So anything that changes that balance is really scary.&nbsp;



Because it&#8217;s not like you have the opportunity to erode into a margin or get it right the second time. So I think that is what drove a lot of the initial hesitancy. Was, one, is lack of knowledge and, two, understanding that you didn&#8217;t have a lot of room to make a mistake.&nbsp;



LEE: On the economics of this, when you are embarking on what I suspect is a very expensive initiative like Mayo Clinic Platform, how on earth do you justify that early on?&nbsp;



FARRUGIA: So again, I&#8217;m trying hard to try and remember how things were versus how I think about them now. [LAUGHTER] It goes back to our history. Mayo has always invested in what it thinks is the right thing that is coming. And that&#8217;s how we&#8217;ve stayed where we are. So the investment really was having an open discussion: is this worth it for our patients? And once that discussion was over, then the board was saying, go, go, go.&nbsp;



Now we are lucky in that we have the size that we&#8217;re able to hire and absorb. We&#8217;re lucky in that the people [who] came before us have been financially astute, and one of our values is stewardship. And we&#8217;re lucky that we had a lot of patients at Mayo Clinic who were able to listen, be inspired by, and be willing to help support. And so that gave us the ability to build what we&#8217;re doing not only into the long-range plan but actually into the yearly plan. And so we built it into the yearly plan. We set up a center for digital health. We set up the platform. And then we set up the budgets to be able to do that. And the budgets came from assets we&#8217;ve had, assets that we would get as the year came by, and then from philanthropy.&nbsp;



We also had a really powerful calling card. And that&#8217;s one advantage I had, and that&#8217;s ‚Ä¶ and I&#8217;d been very open when I was speaking to other CEOs that would use it is that right at that very beginning, really, really in 2019, our cardiologists, both the researchers and the clinicians, had come together and had used electrocardiograms to create an AI algorithm.&nbsp;



The first one was for diagnosing from an electrocardiogram, which is very cheap, very easy to do, left ventricular dysfunction. That&#8217;s how hard the left part of the heart contracts. If it doesn&#8217;t do well, you get heart failure. And they were able to show that that algorithm was already making nurses better than the physician without the algorithm. And after that went on to show that you could do it from a single strip, really with an area under the curve for that single strip on a watch, that was as good as mammograms or pap smears. And so we already had that proof.&nbsp;



LEE: Yeah.&nbsp;



FARRUGIA: That quickly then came into Mayo. We put it into it so that any patient now can benefit from it. And now there are, I think, 14 algorithms just from that same one.&nbsp;



LEE: Yeah.&nbsp;



FARRUGIA: So we had a proof of concept thanks to those really far-seeing cardiologists that enabled things to happen a little faster and also, as I talked to other CEOs, enabled me to say, ‚ÄúThis actually works. This is the path forward.‚Äù I have recently been vocal about also saying, we are at a point now where I believe that for some medical conditions, it is not right to not use AI to help treat them.&nbsp;



LEE: Wow, that&#8217;s so interesting. So I think I want to get into another topic here, which is when you think about the use of AI and data, what are some of the results that maybe are top of mind for you or you think are particularly important? And if you don&#8217;t mind, I&#8217;d like to see if we can think about this not only in terms of results in terms of patient outcomes but in your other activities, core activities, like research, in the education mission, and then even in the broader impacts on the healthcare system. But maybe we start with on patient outcomes.&nbsp;



FARRUGIA: Yeah, they&#8217;re all linked, right.&nbsp;



LEE: Yes.&nbsp;



FARRUGIA: They&#8217;re part of the same ecosystem. We think of ourselves as three shields‚Äî research, education, and the practice‚Äîand that one goes into the other. So, as I said, we have about 320 AI algorithms from the practice. Some run on every patient; some run on some patients. And we have good evidence for what they do. So some specific examples, and then I&#8217;ll get into the transformer part of this.&nbsp;



We have a program called CEDAR [Clinical Detection and Response (tool)], and like most other people, I like acronyms for things. [LAUGHTER] But what it is, in our hospitals with patient consent, we monitor vitals. We monitor in the patient room‚Äînot in the ICU [intensive care unit], in the patient room. We monitor all sorts of things. But there&#8217;s a camera in the room, and we have a team of intensivists‚Äînurses and physicians‚Äîwho do not have any patient responsibilities but are just monitoring the algorithms, and when the algorithms are predicting decompensation, they&#8217;re able to get into the room. And what we&#8217;ve shown, for example, with that algorithm, is we&#8217;ve shown we&#8217;ve decreased length of stay in the hospital, decreased transfers into the intensive care units, and interestingly, decreased mortality and morbidity, which is not easy to show. I talked about the electrocardiogram as a good example. Of course, everybody knows about the radiology things.&nbsp;



We&#8217;ve created ‚Ä¶ taken part of this and said, if we can do this in the hospital, why cannot we do it in patients&#8217; homes? So being very active in looking after patients that would come to the ED, emergency room, would normally be admitted, and we say, no, here are the things we can give you. Go home if you want to, and we will safely look after you at your home. And we recently have been, looking at the last two years of data, been able to show that we&#8217;re also successfully able to give intravenous chemotherapy in patients&#8217; homes because we can monitor; we can do all the things that we can do.&nbsp;



Now, with generative AI, that gave us many other opportunities. One biggest opportunity for me has always been digital pathology. When we see how pathology‚Äôs currently run with a glass slide, not much has changed ‚Ä¶&nbsp;



LEE: Yeah.&nbsp;



FARRUGIA: ‚Ä¶ in many, many, many years, right.&nbsp;



LEE: Yeah.&nbsp;



FARRUGIA: And so really we have made a massive push to digitize pathology not just for us but for others. But talking about ourselves, we started by saying, it has to be very cheap to digitize. So we worked and created a company with partners called Pramana (opens in new tab) that allows us to digitize slides relatively cheaply using AI algorithms that can take away the dirt, the fingerprint. And so we end up with 21 million of our slides digitized, and that gives you now a massive opportunity. Worked with another company called Aignostics (opens in new tab) to create a, what we call, Atlas (opens in new tab), which is an LLM that allows us to then build upon it.&nbsp;



LEE: Yeah.&nbsp;



FARRUGIA: And we, a hundred and, I think, 120 years ago, invented frozen sections at Mayo Clinic. So what that is, is that while the patient&#8217;s still on the table, you can take a piece of tissue, look at it, and tell the surgeon the margins of what you&#8217;re trying to resect are clear or not. But as a result of that, because you have to hurry, you get no information as a surgeon about, is it an invasive cancer, is it noninvasive cancer, or other things. So we&#8217;ve just found a way to digitize our frozen section practice and will completely go across the enterprise with AI-enabled digitized frozen sections, which then enables us to then do it for anybody across the globe if we need to.&nbsp;



And then in the genomic space, we&#8217;re working to create a true exomic transformer that is short range. And we originally started doing it to see if we can test it against the fact that 40% of people with rheumatoid arthritis don&#8217;t respond to the first-line therapy, ‚Ä¶&nbsp;



LEE: Right.&nbsp;



FARRUGIA: ‚Ä¶ but you have to wait six months to find out. And we found that we can actually do that. But it has much greater uses, of course.&nbsp;



And then we&#8217;re working with you‚ÄîI don&#8217;t know how much you want to get into this, Peter, or [if] you want to talk about it yourself‚ÄîMAIRA-2, which is really exciting, about how taking a simple problem‚Äîcan you create a transformer that is able to detect if lines on the chest are in the right place, breathing tube is in the right place?‚Äîand then do it in a way that then can be used for many, many other things.&nbsp;



And then, Peter, because you asked about education and research, ‚Ä¶&nbsp;



LEE: Yeah.&nbsp;



FARRUGIA: ‚Ä¶ imagine what this does now to the education system, right. And so we&#8217;ve got to train our physicians differently. We now have an AI curriculum for all our medical students. We offer masters and PhDs in AI. We think it&#8217;s essential for the people who want to be able to truly become experts, the same way I became an expert in my area of research.&nbsp;



And then from a research standpoint, when you think about all the registries that exist in people&#8217;s labs, all the spatial genomics, all the epigenomics, all the omics that exist. And if you are able to coalesce them into one big, what we call, an atlas, how that could really spur research at a scale that we haven&#8217;t thought of before. And so that is our aim at the moment.&nbsp;



From a research standpoint, we are, with Vijay Shah, who&#8217;s our dean of research, is to say, let&#8217;s make the effort of making sure all the data are available to be able to use and enable for us to take advantage of AI. And that is not easy because, of course, people have collected the data. They tend to want to embrace it.&nbsp;



LEE: Yep.&nbsp;



FARRUGIA: So there have to be the right incentives, the right privacy, and the right ways of doing it. And we think we&#8217;re on the way there, and we‚Äôre already seeing some advantages from doing it this way.&nbsp;



LEE: So we&#8217;re running short on time. And so I always like to end with one or two more provocative questions. And, you know, it&#8217;s tempting to ask you the provocative question of whether you think AI will ever replace human doctors, but I don&#8217;t want to go there with you. In fact, as I thought about our discussion, I was reflecting. We were at a conference together once, and I was on stage in a fireside chat. And then, you know, after the fireside chat, there were audience questions, and I don&#8217;t remember any of the questions from the audience except yours.&nbsp;



And just to remind you, you know, I think when I was on stage, we were talking about a lot of practical uses of AI to, let&#8217;s say, reduce administrative burdens and so on in healthcare. But you got up and you, I won&#8217;t say you scolded me, but you more or less said, is it the right idea to use AI to optimize today&#8217;s somewhat broken healthcare system, or should we be thinking more boldly about, you know, a more fundamental transformation?&nbsp;



And so what I thought I would try to close with here is to hear what was really behind that question. You know, what were you trying to get me to think about when you asked that question?&nbsp;



FARRUGIA: So first of all, darn your great memory [LAUGHTER]. Belated apologies ‚Ä¶ I probably should have &#8230;&nbsp;



LEE: It was by far the best and most sophisticated and, I think, thought-provoking question of all of the ones that came out of the audience.&nbsp;



FARRUGIA: What I was trying to get to is actually trying to clarify it in my own head and then in the head of others is that we do not need to have a linear path to get to where we want to get to. And we seemed to be on a linear path, which is, let&#8217;s try and reduce administrative burden. Let&#8217;s try and truly be a companion to a physician or other provider. Let&#8217;s make their problems better, make them feel better about providing healthcare. And then in the next step, we keep going until we get to, now we can call it agentic AI, whatever we want to talk about. And my view was, no, is that let&#8217;s start with that aim, the last aim, and do the others because the others will come automatically if you&#8217;re working on that harder problem.&nbsp;



Because one, to get to that harder problem, you&#8217;ll find all the other solutions. I was just trying to push that here&#8217;s this wonderful tool that&#8217;s been given to us. Let&#8217;s take advantage of it as quickly as we can. I think we had gotten a little too sensitized to need to say the right things. ‚ÄúCareful, be very careful‚Äù versus saying, ‚ÄúMassive opportunity. Do it right, and healthcare will be much better. Go for it.‚Äù&nbsp;



LEE: Well, I think I understand better now where the vision, insight, and frankly, courage to take on something as ambitious and transformational as the Mayo Clinic Platform and really all of your leadership in your tenure as the president and CEO of Mayo Clinic. I think I understand it much better now.&nbsp;



Gianrico, it&#8217;s just always such a privilege to interact with you and now to have a chance to work with you more closely. So thank you for everything that you do and thank you for joining us today.&nbsp;



FARRUGIA: Thank you for making it so easy, and thanks for giving us this opportunity to do good for the world.&nbsp;



[TRANSITION MUSIC]‚ÄØ



LEE: Gianrico leads what is arguably the crown jewel of the world&#8217;s healthcare systems, and so I feel it&#8217;s such a privilege to be able to talk and sometimes even brainstorm with him.&nbsp;



Our conversation, I think, exposed just how tech forward Gianrico is as he charts the strategies for healthcare delivery well into the future. And as I&#8217;ve interacted with many others, what I&#8217;ve learned is that this is a common trait among major health system CEOs. Roughly speaking, like we&#8217;ve seen in previous episodes where doctors and med students are polymath clinician-technologists, the same thing is true of health system CEOs and other leaders.&nbsp;



AI in the mind of a health system CEO today is not only a technology that can transform diagnosis and treatment, but it&#8217;s also something that can have a huge impact on the business of healthcare delivery, the connection of healthcare to medical research, and the journeys that patients go through as they seek better health.&nbsp;



These two conversations show that virtually all leaders in health and medicine are confronting head-on the opportunities, challenges, and the reality of AI, and they see a future that is potentially very different than what we have today.&nbsp;



[THEME MUSIC]&nbsp;



I&#8217;d like to thank Umair and Gianrico again for their time and insights. And to our listeners, thank you for joining us. We hope you&#8217;ll tune in to our final episode of the series. My coauthors, Carey and Zak, will be back to examine the takeaways from our most recent conversations.&nbsp;



Until next time.&nbsp;



[MUSIC FADES]&nbsp;

				
			
			
				Show more			
		
	





AI Revolution in Medicine podcast series





Opens in a new tabThe post Reimagining healthcare delivery and public health with AI appeared first on Microsoft Research.
‚Ä¢ Demystifying Amazon Bedrock Pricing for a Chatbot Assistant
  ‚ÄúHow much will it cost to run our chatbot on Amazon Bedrock?‚Äù This is one of the most frequent questions we hear from customers exploring AI solutions. And it‚Äôs no wonder ‚Äî calculating costs for AI applications can feel like navigating a complex maze of tokens, embeddings, and various pricing models. Whether you‚Äôre a solution architect, technical leader, or business decision-maker, understanding these costs is crucial for project planning and budgeting. In this post, we‚Äôll look at Amazon Bedrock pricing through the lens of a practical, real-world example: building a customer service chatbot. We‚Äôll break down the essential cost components, walk through capacity planning for a mid-sized call center implementation, and provide detailed pricing calculations across different foundation models. By the end of this post, you‚Äôll have a clear framework for estimating your own Amazon Bedrock implementation costs and understanding the key factors that influence them. 
For those that aren‚Äôt familiar, Amazon Bedrock is a fully managed service that offers a choice of high-performing foundation models (FMs) from leading artificial intelligence (AI) companies like AI21 Labs, Anthropic, Cohere, Meta, Stability AI, and Amazon through a single API, along with a broad set of capabilities to build generative AI applications with security, privacy, and responsible AI. 
Amazon Bedrock provides a comprehensive toolkit for powering AI applications, including pre-trained large language models (LLMs), Retrieval Augmented Generation (RAG) capabilities, and seamless integration with existing knowledge bases. This powerful combination enables the creation of chatbots that can understand and respond to customer queries with high accuracy and contextual relevance. 
Solution overview 
For this example, our Amazon Bedrock chatbot will use a curated set of data sources and use Retrieval-Augmented Generation (RAG) to retrieve relevant information in real time. With RAG, our output from the chatbot will be enriched with contextual information from our data sources, giving our users a better customer experience. When understanding Amazon Bedrock pricing, it‚Äôs crucial to familiarize yourself with several key terms that significantly influence the expected cost. These components not only form the foundation of how your chatbot functions but also directly impact your pricing calculations. Let‚Äôs explore these key components. Key Components 
 
 Data Sources ‚Äì The documents, manuals, FAQs, and other information artifacts that form your chatbot‚Äôs knowledge base. 
 Retrieval-Augmented Generation (RAG) ‚Äì The process of optimizing the output of a large language model by referencing an authoritative knowledge base outside of its training data sources before generating a response. RAG extends the already powerful capabilities of LLMs to specific domains or an organization‚Äôs internal knowledge base, without the need to retrain the model. It is a cost-effective approach to improving LLM output so it remains relevant, accurate, and useful in various contexts. 
 Tokens ‚Äì A sequence of characters that a model can interpret or predict as a single unit of meaning. For example, with text models, a token could correspond not just to a word, but also to a part of a word with grammatical meaning (such as ‚Äú-ed‚Äù), a punctuation mark (such as ‚Äú?‚Äù), or a common phrase (such as ‚Äúa lot‚Äù). Amazon Bedrock prices are based on the number of input and output tokens processed. 
 Context Window ‚Äì The maximum amount of text (measured in tokens) that an LLM can process in one request. This includes both the input text and additional context needed to generate a response. A larger context window allows the model to consider more information when generating responses, enabling more comprehensive and contextually appropriate outputs. 
 Embeddings ‚Äì Dense vector representations of text that capture semantic meaning. In a RAG system, embeddings are created for both knowledge base documents and user queries, enabling semantic similarity searches to retrieve the most relevant information from your knowledge base to augment the LLM‚Äôs responses. 
 Vector Store: A vector store contains the embeddings for your data sources and acts as your knowledge base. Embeddings Model: Embedding models are machine learning models that convert data (text, images, code, etc.) into fixed-size numerical vectors. These vectors capture the semantic meaning of the input in a format that can be used for similarity search, clustering, classification, recommendation systems, and retrieval-augmented generation (RAG). 
 Large Language Models (LLMs) ‚Äì Models trained on vast volumes of data that use billions of parameters to generate original output for tasks like answering questions, translating languages, and completing sentences. Amazon Bedrock offers a diverse selection of these foundation models (FMs), each with different capabilities and specialized strengths. 
 
The figure below demonstrates the architecture of a fully managed RAG solution on AWS. 
 
Estimating Pricing 
One of the most challenging aspects of implementing an AI solution is accurately predicting your capacity needs. Without proper capacity estimation, you might either over-provision (leading to unnecessary costs) or under-provision (resulting in performance issues). Let‚Äôs walk through how to approach this crucial planning step for a real-world scenario. Before we dive into the numbers, let‚Äôs understand the key factors that affect your capacity and costs: 
 
 Embeddings: Vector representations of your text that enable semantic search capabilities. Each document in your knowledge base needs to be converted into embeddings, which impacts both processing costs and storage requirements. 
 User Queries: The incoming questions or requests from your users. Understanding your expected query volume and complexity is crucial, as each query consumes tokens and requires processing power. 
 LLM Responses: The AI-generated answers to user queries. The length and complexity of these responses directly affect your token usage and processing costs. 
 Concurrency: The number of simultaneous users your system needs to handle. Higher concurrency requirements may necessitate additional infrastructure and can affect your choice of pricing model. 
 
To make this concrete, let‚Äôs examine a typical call center implementation. Imagine you‚Äôre planning to deploy a customer service chatbot for a mid-sized organization handling product inquiries and support requests. Here‚Äôs how we‚Äôd break down the capacity planning: First, consider your knowledge base. In our scenario, we‚Äôre working with 10,000 support documents, each averaging 500 tokens in length. These documents need to be chunked into smaller pieces for effective retrieval, with each document typically splitting into 5 chunks. This gives us a total of 5 million tokens for our knowledge base. For the embedding process, those 10,000 documents will generate approximately 50,000 embeddings when we account for chunking and overlapping content. This is important because embeddings affect both your initial setup costs and ongoing storage needs. 
Now, let‚Äôs look at the operational requirements. Based on typical call center volumes, we‚Äôre planning for: 
 
 10,000 customer queries per month 
 Query lengths varying from 50 to 200 tokens (depending on complexity) 
 Average response length of 100 tokens per interaction 
 Peak usage of 100 simultaneous users 
 
When we aggregate these numbers, our monthly capacity requirements shape up to: 
 
 5 million tokens for processing our knowledge base 
 50,000 embeddings for semantic search 
 500,000 tokens for handling user queries 
 1 million tokens for generating responses 
 
Understanding these numbers is crucial because they directly impact your costs in several ways: 
 
 Initial setup costs for processing and embedding your knowledge base 
 Ongoing storage costs for maintaining your vector database and document storage 
 Monthly processing costs for handling user interactions 
 Infrastructure costs to support your concurrency requirements 
 
This gives us a solid foundation for our cost calculations, which we‚Äôll explore in detail in the next section. 
Calculating total cost of ownership (TCO) 
Amazon Bedrock offers flexible pricing modes. With Amazon Bedrock, you are charged for model inference and customization. You have a choice of two pricing plans for inference: 1. On-Demand and Batch: This mode allows you to use FMs on a pay-as-you-go basis without having to make time-based term commitments. 2. Provisioned Throughput: This mode allows you to provision sufficient throughput to meet your application‚Äôs performance requirements in exchange for a time-based term commitment. 
 
 On-demand ‚Äì Ideal for infrequent or unpredictable usage 
 Batch ‚Äì Designed for processing large volumes of data in a single operation 
 Provisioned throughput ‚Äì Tailored for applications with consistent and predictable workloads 
 
To calculate the TCO for this scenario as one-time cost we‚Äôll consider the foundation model, the volume of data in the knowledge base, the estimated number of queries and responses, and the concurrency level mentioned above. For this scenario we‚Äôll be using an on-demand pricing model and showing how the pricing would be for some of the foundation models available on Amazon Bedrock. 
The On-Demand Pricing formula will be: 
The cost of this setup will be the sum of cost of LLM inferences and cost of vector store. To estimate cost of inferences, you can obtain the number of input tokens, context size and output tokens in the response metadata returned by the LLM. Total Cost Incurred = ((Input Tokens + Context Size) * Price per 1000 Input Tokens + Output tokens * Price per 1000 Output Tokens) + Embeddings. For input tokens we will be adding an additional context size of about 150 tokens for User Queries. Therefore as per our assumption of 10,000 User Queries, the total Context Size will be 1,500,000 tokens. 
The following is a comparison of estimated monthly costs for various models on Amazon Bedrock based on our example use case using the on-demand pricing formula: 
Embeddings Cost: 
For text embeddings on Amazon Bedrock, we can choose from Amazon Titan Embeddings V2 model or Cohere Embeddings Model. In this example we are calculating a one-time cost for the embeddings. 
 
 Amazon Titan Text Embeddings V2: 
   
   Price per 1,000 input tokens ‚Äì $0.00002 
   Cost of Embeddings ‚Äì (Data Sources + User Queries) * Embeddings cost per 1000 tokens 
     
     (5,000,000 +500,000) * 0.00002/1000 = $0.11 
      
    
 Cohere Embeddings: 
   
   Price per 1,000 input tokens ‚Äì $0.0001 
   Cost of Embeddings ‚Äì (5,000,000+500,000) * 0.0001/1000 =$0.55 
    
 
The usual cost of vector stores has 2 components: size of vector data + number of requests to the store. You can choose whether to let the Amazon Bedrock console set up a vector store in Amazon OpenSearch Serverless for you or to use one that you have created in a supported service and configured with the appropriate fields. If you‚Äôre using OpenSearch Serverless as part of your setup, you‚Äôll need to consider its costs. Pricing details can be found here: OpenSearch Service Pricing&nbsp;. 
Here using the On-Demand pricing formula, the overall cost is calculated using some foundation models (FMs) available on Amazon Bedrock and the Embeddings cost. 
‚Ä¢ Anthropic Claude: 
 
 Claude 4 Sonnet: ((500,000 +1,500,000) tokens/1000 * $0.003 + 1,000,000 tokens/1000* $0.015 = $21+0.11= $21.11 
 Claude 3 Haiku: ((500,000 +1,500,000) tokens/1000 * $0.00025 + 1,000,000 tokens/1000* $0.00125 = $1.75+0.11= $1.86 
 
‚Ä¢ Amazon Nova: 
 
 Amazon Nova Pro: ((500,000 +1,500,000) tokens/1000 * $0.0008 + 1,000,000 tokens/1000* $0.0032= $4.8+0.11= $4.91 
 Amazon Nova Lite: ((500,000 +1,500,000) tokens/1000 * $0.00006 + 1,000,000 tokens/1000* $0.00024 = $0.36+0.11= $0.47 
 
‚Ä¢ Meta Llama: 
 
 Llama 4 Maverick (17B): ((500,000 +1,500,000) tokens/1000 * $0.00024 + 1,000,000 tokens/1000* $0.00097= $1.45+0.11= $1.56 
 Llama 3.3 Instruct (70B): ((500,000 +1,500,000) tokens/1000 * $0.00072 + 1,000,000 tokens/1000* $0.00072 = $2.16+0.11= $2.27 
 
Evaluate models not just on their natural language understanding (NLU) and generation (NLG) capabilities, but also on their price-per-token ratios for both input and output processing. Consider whether premium models with higher per-token costs deliver proportional value for your specific use case, or if more cost-effective alternatives like Amazon Nova Lite or Meta Llama models can meet your performance requirements at a fraction of the cost. 
Conclusion 
Understanding and estimating Amazon Bedrock costs doesn‚Äôt have to be overwhelming. As we‚Äôve demonstrated through our customer service chatbot example, breaking down the pricing into its core components ‚Äì token usage, embeddings, and model selection ‚Äì makes it manageable and predictable. 
Key takeaways for planning your Bedrock implementation costs: 
 
 Start with a clear assessment of your knowledge base size and expected query volume 
 Consider both one-time costs (initial embeddings) and ongoing operational costs 
 Compare different foundation models based on both performance and pricing 
 Factor in your concurrency requirements when choosing between on-demand, batch, or provisioned throughput pricing 
 
By following this systematic approach to cost estimation, you can confidently plan your Amazon Bedrock implementation and choose the most cost-effective configuration for your specific use case. Remember that the cheapest option isn‚Äôt always the best ‚Äì consider the balance between cost, performance, and your specific requirements when making your final decision. 
Getting Started with Amazon Bedrock 
With Amazon Bedrock, you have the flexibility to choose the most suitable model and pricing structure for your use case. We encourage you to explore the AWS Pricing Calculator for more detailed cost estimates based on your specific requirements. 
To learn more about building and optimizing chatbots with Amazon Bedrock, check out the workshop Building with Amazon Bedrock. 
We‚Äôd love to hear about your experiences building chatbots with Amazon Bedrock. Share your success stories or challenges in the comments! 
 
About the authors 
 Srividhya Pallay is a Solutions Architect II at Amazon Web Services (AWS) based in Seattle, where she supports small and medium-sized businesses (SMBs) and specializes in Generative Artificial Intelligence and Games. Srividhya holds a Bachelor‚Äôs degree in Computational Data Science from Michigan State University College of Engineering, with a minor in Computer Science and Entrepreneurship. She holds 6 AWS Certifications. 
Prerna Mishra is a Solutions Architect at Amazon Web Services(AWS) supporting Enterprise ISV customers. She specializes in Generative AI and MLOPs as part of Machine Learning and Artificial Intelligence community. She graduated from New York University in 2022 with a Master‚Äôs degree in Data Science and Information Systems. 
Brian Clark is a Solutions Architect at Amazon Web Services (AWS) supporting Enterprise customers in the financial services vertical. He is a part of the Machine Learning and Artificial Intelligence community and specializes in Generative AI and Agentic workflows. Brian has over 14 years of experience working in technology and holds 8 AWS certifications.
‚Ä¢ Fine-tune OpenAI GPT-OSS models on Amazon SageMaker AI using Hugging Face libraries
  Released on August 5, 2025, OpenAI‚Äôs GPT-OSS models, gpt-oss-20b and gpt-oss-120b, are now available on AWS through Amazon SageMaker AI and Amazon Bedrock. These pre-trained, text-only Transformer models are built on a Mixture-of-Experts (MoE) architecture that activates only a subset of parameters per token, delivering high reasoning performance while reducing compute costs. They specialize in coding, scientific analysis, and mathematical reasoning, and support a 128,000 context length, adjustable reasoning levels (low/medium/high), chain-of-thought (CoT) reasoning with audit-friendly traces, structured outputs, and tool use to support agentic-AI workflows. As discussed in OpenAI‚Äôs documentation, both models have undergone safety-focused training and adversarial fine-tuning evaluations to assess and strengthen robustness against misuse. The following table summarizes the model specifications. 
 
  
   
   Model 
   Layers 
   Total Parameters 
   Active Parameters Per Token 
   Total Experts 
   Active Experts Per Token 
   Context Length 
   
   
   openai/gpt-oss-120b 
   36 
   117 billion 
   5.1 billion 
   128 
   4 
   128,000 
   
   
   openai/gpt-oss-20b 
   24 
   21 billion 
   3.6 billion 
   32 
   4 
   128,000 
   
  
 
The GPT-OSS models are deployable using Amazon SageMaker JumpStart and also accessible through Amazon Bedrock APIs. Both options provide developers the flexibility to deploy and integrate GPT-OSS models into your production-grade AI workflows. Beyond out-of-the-box deployment, these models can be fine-tuned to align with specific domains and use cases, using open source tools from the Hugging Face ecosystem and running on the fully managed infrastructure of SageMaker AI. 
Fine-tuning large language models (LLMs) is the process of adjusting a pre-trained model‚Äôs weights using a smaller, task-specific dataset to tailor its behavior to a particular domain or application. Fine-tuning large models like GPT-OSS transforms them from a broad generalist into a domain-specific expert without the cost of training from scratch. Adapting the model to your data and terminology can deliver more accurate, context-aware outputs, improves reliability, and reduces hallucinations. The result is a specialized GPT-OSS that excels at targeted tasks while retaining the scalability, flexibility, and open-weight benefits ideal for secure, enterprise-grade deployment. 
In this post, we walk through the process of fine-tuning a GPT-OSS model in a fully managed training environment using SageMaker AI training jobs. The workflow uses the Hugging Face TRL library for fine-tuning, the Hugging Face Accelerate library to simplify distributed training across multiple GPUs and nodes, and the DeepSpeed ZeRO-3 optimization technique to reduce memory usage by partitioning model states across devices for efficient training of billion-parameter models. We then apply this setup to fine-tune the GPT-OSS model on a multilingual reasoning dataset, HuggingFaceH4/Multilingual-Thinking, enabling GPT-OSS to handle structured, CoT reasoning across multiple languages. 
Solution overview 
SageMaker AI is a managed machine learning (ML) service that streamlines the entire foundation model (FM) lifecycle. It provides hosted, interactive notebooks for rapid exploration, fully managed ephemeral training jobs for large-scale and distributed fine-tuning, and Amazon SageMaker HyperPod clusters that offer granular control over persistent training infrastructure for large-scale model training and fine-tuning workloads. By using managed hosting in SageMaker, you can serve models reliably in production, and the suite of AIOps-ready tools, such as reusable pipelines and fully managed MLflow, support experiment tracking, model registration, and seamless deployment. With built-in governance and enterprise-grade security, SageMaker AI provides data engineers, data scientists, and ML engineers with a unified, fully managed platform to build, train, deploy, and govern FMs end-to-end. 
GPT-OSS can be fine-tuned on SageMaker using the latest Hugging Face TRL library, which can be written as recipes for fine-tuning LLMs using Hugging Face SFTTrainer. These recipes can also be adapted to fine-tune other open-weight language or vision models such as Qwen, Mistral, Meta, and many more. In this post, we show how to fine-tune GPT-OSS in a distributed setup either on a single node multi-GPU setup or across multi-node multi-GPU setup, using Hugging Face Accelerate to manage multi-device training and DeepSpeed ZeRO-3 to train large models more efficiently. Together, they help you fine-tune faster and scale to larger datasets. 
We also highlight MXFP4 (Microscaling FP4), a 4-bit floating-point quantization format from the Open Compute Project. It groups tensors into small blocks, each sharing a scaling factor, which reduces memory and compute needs while helping preserve model accuracy‚Äîmaking it well-suited for efficient model training. Complementing quantization, we explore Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA, for the adaptation of large models by learning a small set of additional parameters instead of modifying all weights. This approach is memory- and compute-efficient, highly compatible with quantized models, and supports fine-tuning even on constrained hardware environments. 
The following diagram illustrates this configuration (source). 
 
By using MXFP4 quantization, PEFT fine-tuning methods like LoRA, and distributed training with Hugging Face Accelerate and DeepSpeed ZeRO-3 together, we can efficiently and scalably fine-tune large models like gpt-oss-120b and gpt-oss-20b for high-performance customization while keeping infrastructure and compute costs manageable. 
Prerequisites 
To fine-tune GPT-OSS models on SageMaker AI, you must have the following prerequisites: 
 
 An AWS account that will contain your AWS resources. 
 An AWS Identity and Access Management (IAM) role to access SageMaker AI. To learn more about how IAM works with SageMaker AI, see AWS Identity and Access Management for Amazon SageMaker AI. 
 You can run the notebook provided in this post from your preferred development environment, including interactive development environments (IDEs) such as PyCharm or Visual Studio Code, provided your AWS credentials are properly set up and configured to access your AWS account. To set up your local environment, refer to Configuring settings for the AWS CLI. Optionally, we recommend using Amazon SageMaker Studio for straightforward development process on SageMaker AI. 
 If you‚Äôre following along with this post, we use the ml.p5en.48xlarge instance for fine-tuning the 120B model and the ml.p4de.24xlarge instance for the 20B model. You will need access to these SageMaker compute instances to run the example notebook presented in this post. If you‚Äôre unsure, you can review the AWS service quotas on the AWS Management Console: 
   
   Choose Amazon SageMaker as the AWS service under Manage Quotas. 
   Select ml.p4de.24xlarge for training job usage or ml.p5en.48xlarge for training job usage based on the model you‚Äôre interested in fine-tuning and request an increase at account level. 
    
 Access to the GitHub repo. 
 
Business outcomes for fine-tuning GPT-OSS 
Global enterprises increasingly need AI tools that support complex reasoning across multiple languages‚Äîwhether for multilingual virtual assistants, cross-location support desks, or international knowledge systems. Although FMs offer a powerful starting point, their effectiveness in diverse linguistic contexts hinges on structured reasoning inputs‚Äîdatasets that surface logic steps explicitly and across languages. That‚Äôs why testing with a multilingual, CoT-style dataset is a valuable first step. It lets you verify how well a model holds reasoning coherence when switching between languages and reasoning patterns, laying a robust foundation before scaling to larger, domain-specific multilingual datasets. GPT-OSS is particularly well-suited for this task, with its native CoT capabilities, long 128,000 context window, and adjustable reasoning levels, making it ideal for evaluating and refining multilingual reasoning performance before production deployment. 
Fine-tune GPT-OSS models for multi-lingual reasoning on SageMaker AI 
In this section, we walk through how to fine-tune OpenAI‚Äôs GPT-OSS models on SageMaker AI using training jobs. SageMaker training jobs support distributed multi-GPU and multi-node configurations, so you can spin up high-performance clusters on demand, train billion-parameter models faster, and automatically shut down resources when the job finishes. 
Set up your environment 
In the following sections, we run the code from SageMaker Studio JupyterLab notebook instances. You can also use your preferred IDE, such as VS Code or PyCharm, but make sure your local environment is configured to work with AWS, as discussed in the prerequisites. 
Complete the following steps: 
 
 On the SageMaker AI console, choose Domains in the navigation pane, then open your domain. 
 In the navigation pane under Applications and IDEs, choose Studio. 
 On the User profiles tab, locate your user profile, then choose Launch and Studio. 
 
 
 
 In SageMaker Studio, launch an ml.t3.medium JupyterLab notebook instance with at least 50 GB of storage. 
 
A large notebook instance isn‚Äôt required, because the fine-tuning job will run on a separate ephemeral training job instance with NVIDIA accelerators. 
 
 
 To begin fine-tuning, start by cloning the GitHub repo and navigating to 3_distributed_training/models/openai--gpt-oss directory, then launch the finetune_gpt_oss.ipynb notebook with a Python 3.12 or higher version kernel: 
 
 
 # clone github repo
git clone&nbsp;https://github.com/aws-samples/amazon-sagemaker-generativeai.git 
 
Dataset for fine-tuning 
Selecting and curating the right dataset is a critical first step in fine-tuning any LLM. In this post, we use the Hugging FaceH4/Multilingual-Thinking dataset, which is a multilingual reasoning dataset containing CoT examples translated into languages such as French, Spanish, and German. Its combination of diverse languages, varied reasoning tasks, and explicit step-by-step thought processes makes it well-suited for evaluating how a model handles structured reasoning, adapts to multilingual inputs, and maintains logical consistency across different linguistic contexts. With around 1,000 examples, it‚Äôs small enough for quick experimentation yet sufficient to demonstrate fine-tuning and evaluation of large pre-trained models like GPT-OSS. The dataset can be loaded in just a few lines of code using the Hugging Face Datasets library: 
 
 # load datasets in memory
dataset_name = 'HuggingFaceH4/Multilingual-Thinking'
dataset = load_dataset(dataset_name, split="train") 
 
The following code is some sample data: 
 
 {
&nbsp;&nbsp;"reasoning_language": "French",
&nbsp;&nbsp;"developer": "You are a recipe suggestion bot, ...",
&nbsp;&nbsp;"user": "Can you provide me with a step-by-step ...",
&nbsp;&nbsp;"analysis": "D'accord, l'utilisateur souhaite une recette ...",
&nbsp;&nbsp;"final": "Certainly! Here's a classic homemade chocolate ...",
&nbsp;&nbsp;"messages": [
&nbsp;&nbsp; &nbsp;{
&nbsp;&nbsp; &nbsp; &nbsp;"content": "reasoning language: French\n\nYou are a ...",
&nbsp;&nbsp; &nbsp; &nbsp;"role": "system",
&nbsp;&nbsp; &nbsp; &nbsp;"thinking": null
&nbsp;&nbsp; &nbsp;},
&nbsp;&nbsp; &nbsp;{
&nbsp;&nbsp; &nbsp; &nbsp;"content": "Can you provide me with a step-by-step ...",
&nbsp;&nbsp; &nbsp; &nbsp;"role": "user",
&nbsp;&nbsp; &nbsp; &nbsp;"thinking": null
&nbsp;&nbsp; &nbsp;},
&nbsp;&nbsp; &nbsp;{
&nbsp;&nbsp; &nbsp; &nbsp;"content": "Certainly! Here's a classic homemade chocolate ...",
&nbsp;&nbsp; &nbsp; &nbsp;"role": "assistant",
&nbsp;&nbsp; &nbsp; &nbsp;"thinking": "D'accord, l'utilisateur souhaite une recette ...‚Äú
&nbsp;&nbsp; &nbsp;}
&nbsp;&nbsp;]
} 
 
For supervised fine-tuning, we use only the data in the messages key to train our GPT-OSS model. Because TRL‚Äôs SFTTrainer natively supports this format, it can be used as-is. We extract all rows containing only the messages key, save them in JSONL format, and upload the file to Amazon Simple Storage Service (Amazon S3). This makes sure the dataset is readily accessible to SageMaker training jobs at runtime. 
 
 # preserve only messages key&nbsp;
dataset = dataset.remove_columns(
&nbsp;&nbsp; &nbsp;[col for col in dataset.column_names if col != "messages"]
)
# save as JSONL format
dataset_filename = os.path.join(dataset_parent_path, f"{dataset_name.replace('/', '--').replace('.', '-')}.jsonl")
dataset.to_json(dataset_filename, lines=True)
...

from sagemaker.s3 import S3Uploader

# select a data destination bucket
data_s3_uri = f"s3://{sess.default_bucket()}/dataset"

# upload to S3
uploaded_s3_uri = S3Uploader.upload(
&nbsp;&nbsp; &nbsp;local_path=dataset_filename,
&nbsp;&nbsp; &nbsp;desired_s3_uri=data_s3_uri
)
print(f"Uploaded {dataset_filename} to &gt; {uploaded_s3_uri}") 
 
Experimentation tracking with MLflow (Optional) 
SageMaker AI offers the fully managed MLflow capability, so you can track multiple training runs within experiments, compare results with visualizations, evaluate models, and register the best ones in the model registry. MLflow also supports integration with agentic workflows. 
TRL‚Äôs SFTTrainer natively integrates with experimentation tracking tools such as MLflow, TensorBoard, Weights &amp; Biases, and more. With SFTTrainer, you can log training parameters, hyperparameters, loss metrics, system metrics, and more to a centralized location, providing you with audit trails, governance, and streamlined experiment tracking. This step is optional; if you choose not to use SageMaker managed MLflow, you can set the SFTTrainer parameter reports_to to tensorboard, which will log all metrics locally to disk for visualization using a local or remote TensorBoard service. 
 
 
 # set none to log to local disk
MLFLOW_TRACKING_SERVER_ARN =&nbsp;None&nbsp;# or "arn:aws:sagemaker:us-west-2:&lt;account-id&gt;:mlflow-tracking-server/&lt;server-name&gt;"

if MLFLOW_TRACKING_SERVER_ARN:
&nbsp;&nbsp; &nbsp;reports_to = "mlflow"
else:
&nbsp;&nbsp; &nbsp;reports_to = "tensorboard"
print("reports to:", reports_to) 
 
Experiments logged from TRL‚Äôs SFTTrainer to an MLflow tracking server in SageMaker automatically capture key metrics and parameters. The SageMaker managed MLflow service renders real-time visualizations, profiles training hardware with minimal setup, enables side-by-side run comparisons, and provides built-in evaluation tools to track, train, and assess your fine-tuning jobs end-to-end. 
 
Fine-tune GPT-OSS on training jobs 
The following example demonstrates how to fine-tune the gpt-oss-20b model. To switch to gpt-oss-120b, simply update the model_name. The model-to-instance mapping shown in this section has been tested as part of this notebook workflow. You can adjust the instance type and instance count to fit your specific use case. 
The following table summarizes the different model specifications. 
 
  
   
   GPT‚ÄëOSS Model 
   SageMaker Instance 
   GPU Specifications 
   
   
   openai/gpt-oss-120b 
   ml.p5en.48xlarge 
   8√ó NVIDIA H200 GPUs, 96 GB HBM3 each 
   
   
   openai/gpt-oss-20b 
   ml.p4de.24xlarge 
   8√ó NVIDIA A100 GPUs, 80 GB HBM2e each 
   
  
 
 
 # User-defined variables
model_name = "openai/gpt-oss-20b"
tokenizer_name = "openai/gpt-oss-20b"

# dataset path inside a sagemaker container
dataset_path = "/opt/ml/input/data/training/HuggingFaceH4--Multilingual-Thinking.jsonl"
output_path = "/opt/ml/model/openai-gpt-oss-20b-HuggingFaceH4-Multilingual-Thinking/"

# support only for Ampere, Hopper and Grace Blackwell
bf16_flag = "true"&nbsp; 
 
SageMaker training jobs automatically download datasets from the specified S3 prefix or file into the training container, mapping them to /opt/ml/input. Training artifacts and logs are stored in /opt/ml/output, and the final trained or fine-tuned model is saved to /opt/ml/model. Saving the model to this path allows SageMaker to automatically detect it for downstream workflows such as model registration, deployment, and other automation. You can set or unset the bf16_flag to choose between float16 and bfloat16. float16 uses less memory but has a smaller numeric range, whereas bfloat16 provides a wider range with similar memory savings, making it more stable for training large models. bfloat16 is supported on newer GPU architectures such as NVIDIA Ampere, Hopper, and Grace Blackwell. 
Fine-tuning with open source Hugging Face recipes 
With Hugging Face‚Äôs TRL library, you can define Supervised Fine-Tuning (SFT) recipes, which are essentially preconfigured training workflows that streamline fine-tuning FMs like Meta, Qwen, Mistral, and now OpenAI GPT‚ÄëOSS with minimal setup. These recipes simplify the process of adapting models to new datasets using TRL‚Äôs SFTTrainer and configuration tools. 
 
 yaml_template = """# Model arguments
model_name_or_path: {{ model_name }}
tokenizer_name_or_path: {{ tokenizer_name }}
model_revision: main
torch_dtype: bfloat16
attn_implementation: kernels-community/vllm-flash-attn3
bf16: {{ bf16_flag }}
tf32: false
output_dir: {{ output_dir }}

# Dataset arguments
dataset_id_or_path: {{ dataset_path }}
max_seq_length: 2048
packing: true
packing_strategy: wrapped

# LoRA arguments
use_peft: true
lora_target_modules: "all-linear"
### Specific to GPT-OSS
lora_modules_to_save: ["7.mlp.experts.gate_up_proj", "7.mlp.experts.down_proj", "15.mlp.experts.gate_up_proj", "15.mlp.experts.down_proj", "23.mlp.experts.gate_up_proj", "23.mlp.experts.down_proj"]
lora_r: 8
lora_alpha: 16

# Training arguments
num_train_epochs: 1. 
per_device_train_batch_size: 6
per_device_eval_batch_size: 6
gradient_accumulation_steps: 3
gradient_checkpointing: true
optim: adamw_torch_fused
gradient_checkpointing_kwargs:
&nbsp;&nbsp;use_reentrant: true
learning_rate: 1.0e-4
lr_scheduler_type: cosine
warmup_ratio: 0.1
max_grad_norm: 0.3
bf16: {{ bf16_flag }}
bf16_full_eval: {{ bf16_flag }}
tf32: false

# Logging arguments
logging_strategy: steps
logging_steps: 2
report_to:
&nbsp;&nbsp;- {{ reports_to }}
save_strategy: "epoch"
seed: 42
"""

config_filename = "openai-gpt-oss-20b-qlora.yaml" 
 
The recipe.yaml file contains the following key parameters: 
 
 Model arguments: 
   
   model_name_or_path or tokenizer_name_or_path ‚Äì Path or identifier for the base model and tokenizer to fine-tune. Models can be loaded locally from disk or the Hugging Face Hub. 
   torch_dtype ‚Äì Sets training precision. bfloat16 offers float16-level memory savings with a wider numeric range for better stability, and is supported on NVIDIA Ampere, Hopper, and Grace Blackwell GPUs. Alternatively, set to float16 for older versions of NVIDIA GPUs. 
   attn_implementation ‚Äì Uses vLLM FlashAttention 3 (kernels-community/vllm-flash-attn3) kernels for faster attention computation, supported for newer Hopper GPUs. Alternatively, set eager for older NVIDIA GPUs. 
    
 Dataset arguments: 
   
   dataset_id_or_path ‚Äì Local dataset location as JSONL file or Hugging Face Hub ID for the dataset. 
   max_seq_length ‚Äì Maximum token length per sequence (for example, 2048). Provide longer sequence lengths for datasets that require longer reasoning output tokens. Longer sequence lengths consume more GPU memory. 
    
 LoRA arguments: 
   
   use_peft ‚Äì Enables PEFT using LoRA. Set to true for PEFT or false for full fine-tuning. 
   lora_target_modules ‚Äì Target layers for LoRA adaptation (for example, all-linear layers is default for most dense and MoEs). 
   lora_modules_to_save ‚Äì GPT-OSS-specific layers to keep in full precision during LoRA training. 
   lora_r or lora_alpha ‚Äì Rank and scaling factor for LoRA updates. 
    
 Logging and saving arguments: 
   
   report_to ‚Äì Experiment tracking integration (such as MLflow or TensorBoard). 
    
 
After a recipe is defined and tested, you can seamlessly swap configurations such as the model name, dataset, number of epochs, or PEFT settings and run or rerun the fine-tuning workflow with minimal or no code changes. 
SageMaker estimators 
As a next step, we use a SageMaker training job estimator to spin up a training cluster and run the model fine-tuning. The SageMaker AI estimators API provide a high-level API to define and run training jobs on fully managed infrastructure, handling environment setup, scaling, and artifact management. You can specify training scripts, input data, and compute resources without manually provisioning servers. SageMaker also offers prebuilt Hugging Face and PyTorch estimators, which come optimized for their respective frameworks, making it straightforward to train and fine-tune models with minimal setup. 
It‚Äôs recommended to use Python 3.12 and higher to fine-tune GPT-OSS with the following packages installed. Add or update the requirements.txt file in your script‚Äôs root directory with the following packages. SageMaker estimators will automatically detect this file and install the listed dependencies at runtime. 
 
 %%writefile code/requirements.txt
transformers&gt;=4.55.0
kernels&gt;=0.9.0
datasets==4.0.0
bitsandbytes==0.46.1
trl&gt;=0.20.0
peft&gt;=0.17.0
lighteval==0.10.0
hf-transfer==0.1.8
hf_xet
tensorboard 
liger-kernel==0.6.1
deepspeed==0.17.4
lm-eval[api]==0.4.9
Pillow
mlflow
sagemaker-mlflow==0.1.0
triton
git+https://github.com/triton-lang/triton.git@main#subdirectory=python/triton_kernels 
 
Define a SageMaker estimator and point it to your local training script directory. SageMaker will package the contents and place them in /opt/ml/code inside the training container. This includes your training script, additional modules in the directory, and if a requirements.txt file is present, SageMaker will automatically install the listed packages at runtime. 
 
 pytorch_estimator = PyTorch(
&nbsp;&nbsp; &nbsp;image_uri="763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training:2.7.1-gpu-py312-cu128-ubuntu22.04-sagemaker",
&nbsp;&nbsp; &nbsp;entry_point="accelerate_sagemaker_train.sh", # Adapted bash script to train using accelerate on SageMaker - Multi-GPU
&nbsp;&nbsp; &nbsp;source_dir="code",
&nbsp;&nbsp; &nbsp;instance_type=training_instance_type,
&nbsp;&nbsp; &nbsp;instance_count=1,&nbsp;# multi-node training support
&nbsp;&nbsp; &nbsp;base_job_name=f"{job_name}-pytorch",
&nbsp;&nbsp; &nbsp;role=role,
&nbsp; &nbsp; ...
&nbsp;&nbsp; &nbsp;hyperparameters={
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"num_process": NUM_GPUS,&nbsp;# define the number of GPUs to run distributed training, per instance
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"config": f"recipes/{config_filename}",
&nbsp;&nbsp; &nbsp;}
) 
 
The following is the directory structure for fine-tuning GPT-OSS on SageMaker AI training jobs: 
 
 code/
‚îú‚îÄ‚îÄ accelerate/ &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; # Accelerate configuration files
‚îú‚îÄ‚îÄ accelerate_sagemaker_train.sh &nbsp; &nbsp; &nbsp;# Launch script for distributed training with Accelerate on SageMaker training jobs
‚îú‚îÄ‚îÄ gpt_oss_sft.py &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; # Main training script for supervised fine-tuning (SFT) of GPT-OSS
‚îú‚îÄ‚îÄ recipes/ &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; # Predefined training configuration recipes (YAML)
‚îî‚îÄ‚îÄ requirements.txt &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; # Python dependencies installed at runtime 
 
To fine-tune across multiple GPUs, we use Hugging Face Accelerate and DeepSpeed ZeRO-3, which work together to train large models more efficiently. Hugging Face Accelerate simplifies launching distributed training by automatically handling device placement, process management, and mixed precision settings. DeepSpeed ZeRO-3 reduces memory usage by partitioning optimizer states, gradients, and parameters across devices‚Äîallowing billion-parameter models to fit and train faster. 
You can run your SFTTrainer script with Hugging Face Accelerate using a simple command like the following: 
 
 accelerate launch \
&nbsp; &nbsp; --config_file accelerate/zero3.yaml \&nbsp;
&nbsp; &nbsp; --num_processes 8 gpt_oss_sft.py \&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;--config recipes/openai-gpt-oss-20b-qlora.yaml 
 
SageMaker executes this command inside the training container because we set entry_point="accelerate_sagemaker_train.sh" when initializing the SageMaker estimator. The accelerate_sagemaker_train.sh script is defined as follows: 
 
 #!/bin/bash
set -e

...

# Launch fine-tuning with Accelerate + DeepSpeed (Zero3)
accelerate launch \
&nbsp;&nbsp;--config_file accelerate/zero3.yaml \
&nbsp;&nbsp;--num_processes "$NUM_GPUS" \
&nbsp;&nbsp;gpt_oss_sft.py \
&nbsp;&nbsp;--config "$CONFIG_PATH"  
 
PEFT vs. full fine-tuning 
The gpt_oss_sft.py script lets you choose between PEFT and full fine-tuning by setting use_peft to true or false. Full fine-tuning gives you greater control over the base model weights, enabling broader adaptability and expressiveness. However, it also carries the risk of catastrophic forgetting and higher resource consumption during the training process. 
At the end of training, you will have the fully adapted model weights, which can be deployed to a SageMaker endpoint for inference. You can then run predictions against the deployed endpoint using the SageMaker Predictor. 
Conclusion 
In this post, we demonstrated how to fine-tune OpenAI‚Äôs GPT-OSS models (gpt-oss-120b and gpt-oss-20b) on SageMaker AI using SageMaker training jobs, the Hugging Face TRL library, and distributed training with Hugging Face Accelerate and DeepSpeed ZeRO-3. By combining the fully managed, ephemeral infrastructure of SageMaker with TRL‚Äôs streamlined fine-tuning recipes, you can adapt GPT-OSS to your domain quickly and efficiently, using either PEFT for cost-effective customization or full fine-tuning for maximum model control. With the resulting model artifacts, you can deploy to SageMaker endpoints for secure, scalable inference and bring advanced reasoning capabilities directly into your enterprise workflows. 
If you‚Äôre interested in exploring further, the GitHub repo contains all the resources used in this walkthrough. It‚Äôs a great starting point for experimenting with fine-tuning GPT-OSS on your own datasets and deploying the resulting models to SageMaker for real-world applications. You can get set up with a notebook in minutes using the SageMaker Studio domain quick setup and start experimenting right away. 
 
About the authors 
Pranav Murthy is a Senior Generative AI Data Scientist at AWS, specializing in helping organizations innovate with Generative AI, Deep Learning, and Machine Learning on Amazon SageMaker AI. Over the past 10+ years, he has developed and scaled advanced computer vision (CV) and natural language processing (NLP) models to tackle high-impact problems‚Äîfrom optimizing global supply chains to enabling real-time video analytics and multilingual search. When he‚Äôs not building AI solutions, Pranav enjoys playing strategic games like chess, traveling to discover new cultures, and mentoring aspiring AI practitioners.&nbsp;You can find Pranav on LinkedIn. 
Sumedha Swamy is a Senior Manager of Product Management at Amazon Web Services (AWS), where he leads several areas of the Amazon SageMaker, including SageMaker Studio ‚Äì the industry-leading integrated development environment for machine learning, developer and administrator experiences, AI infrastructure, and SageMaker SDK.
‚Ä¢ The DIVA logistics agent, powered by Amazon Bedrock
  DTDC is India‚Äôs leading integrated express logistics provider, operating the largest network of customer access points in the country. DTDC‚Äôs technology-driven logistics solutions cater to a wide range of customers across diverse industry verticals, making them a trusted partner in delivering excellence. 
DTDC Express Limited receives over 400,000 customer queries each month, ranging from tracking requests to serviceability checks and shipping rates. With such a high volume of shipments, their existing logistics agent, DIVA, was operated on a rigid, guided workflow, forcing users to follow a structured path rather than engaging in natural, dynamic conversations. The lack of flexibility resulted in increased burden on customer support teams, longer resolution times, and poor customer experience. 
DTDC was looking for a more flexible, intelligent assistant‚Äîone that could understand context, manage complex queries, and improve efficiency while reducing reliance on human agents. To achieve a better customer experience, DTDC decided to enhance DIVA with generative AI using Amazon Bedrock. 
ShellKode is an AWS Partner, born-in-the-cloud company specializing in modernization, security, data, generative AI, and machine learning (ML). With a mission to drive transformative growth, ShellKode empowers businesses through state-of-the-art technology solutions that address complex challenges and unlock new opportunities. Using deep industry expertise, they deliver tailored strategies that foster innovation, efficiency, and long-term success in an evolving digital landscape. 
In this post, we discuss how DTDC and ShellKode used Amazon Bedrock to build DIVA 2.0, a generative AI-powered logistics agent. 
Solution overview 
To address the limitations of the existing logistics agent, ShellKode built an advanced agentic assistant using Amazon Bedrock Agents, Amazon Bedrock Knowledge Bases, and an API integration layer. 
When customers interact with DIVA 2.0, they experience a seamless, conversational interface that understands and responds to their queries naturally. Whether tracking a package, checking shipping rates, or inquiring about service availability, users can ask questions in their own words without following a rigid script. DIVA 2.0‚Äôs enhanced AI capabilities allow it to understand context, manage complex requests, and provide accurate, personalized responses, significantly improving the overall customer experience and reducing the need for human intervention. The following high-level architecture diagram illustrates the application flow and the solution architecture with AWS services. 
 
The DTDC logistics agent is designed using a modular and scalable architecture to provide seamless integration and high performance. This streamlined workflow demonstrates how a generative AI-powered serverless logistics agent using AWS App Runner, Amazon Bedrock Agents, AWS Lambda, and a vector-based knowledge base handles user queries ranging from tracking requests to serviceability checks and shipping rates intelligently and efficiently. 
The logistics agent is hosted as a static website using Amazon CloudFront and Amazon Simple Storage Service (Amazon S3). The logistics agent is integrated with the DTDC website, which provides an intuitive and user-friendly interface for end-user interactions (see the following screenshot). 
 
An end-user accesses the logistics agent through the DTDC website and submits queries like tracking shipments, checking service availability, calculating shipping rates, FAQs, and so on using natural language.The user requests are processed by App Runner, which helps run the web application (including API services, backend web services, and websites) on AWS. App Runner is hosted with multiple API services, such as the Amazon Bedrock Agents API and Dashboard API. App Runner initiates the Amazon Bedrock Agents API based on the user requests. 
Amazon Bedrock is a fully managed service that offers a choice of industry leading foundation models (FMs) along with a broad set of capabilities to build generative AI applications, simplifying development with security, privacy, and responsible AI. With Amazon Bedrock, your content is not used to improve the base models and is not shared with any model providers. Amazon Bedrock Guardrails provides configurable safeguards to help safely build generative AI applications at scale. To learn more, see Build safe and responsible generative AI applications with guardrails. AWS Identity and Access Management (IAM) helps administrators securely control who can be authenticated and authorized to use Amazon Bedrock resources. 
The Amazon Bedrock agents are configured in Amazon Bedrock. An Amazon Bedrock agent receives the request and interprets the user‚Äôs intent using its natural language understanding capabilities. Based on the interpreted intent, the agent triggers an appropriate Lambda function, such as: 
 
 Tracking consignments 
 Pricing information 
 Location serviceability check 
 Support ticket creation 
 
The triggered Lambda function calls the following client APIs, retrieves the relevant data, and returns the response to the agent: 
 
 Tracking System API ‚Äì Retrieves real-time status and provides updates on consignment shipment tracking 
 Delivery Franchise Location API ‚Äì Checks the service availability to deliver the parcels between the locations 
 Pricing System API ‚Äì Calculates the shipping rates based on shipment details provided by the user 
 Customer Care API ‚Äì Creates a support ticket for the end-users 
 
The agent passes the response to the large language model (LLM), in this case Anthropic‚Äôs Claude 3.0 on Amazon Bedrock, which understands the context of the retrieved data, processes it, and generates a meaningful response for the user. 
The knowledge base contains web-scraped content from the DTDC website, internal support documentation, FAQs, and operational data, enabling real-time updates and accurate responses. The knowledge base contents are stored as vector embeddings in Amazon OpenSearch Service, providing quick and relevant responses. For general queries, the logistics agent fetches information from Amazon Bedrock Knowledge Bases, providing accuracy and relevance. Using semantic similarity search, relevant chunks of information are retrieved from the knowledge base based on the user‚Äôs query, which Amazon Bedrock then uses to generate a context-aware response. If no relevant data is found in the knowledge base, a fallback response (preconfigured in the Amazon Bedrock prompt) is returned, indicating that the system couldn‚Äôt assist with the request. 
The logistics agent queries and associated responses are stored in Amazon Relational Database Service (Amazon RDS) for PostgreSQL for enhanced scalability and relational data handling. App Runners initiates the Dashboard API call to update the queries and associated responses in Amazon RDS. We discuss this in more detail the following section. 
Throughout the process, Amazon CloudWatch Logs captures key events such as intent recognition, Lambda invocations, API responses, and fallback triggers for auditing and system monitoring. AWS CloudTrail records and monitors activity in the AWS account, including actions taken by users, roles, or AWS services. It logs these events, which can be used for operational auditing, governance, and compliance. 
Amazon GuardDuty is a threat detection service that continuously monitors, analyzes, and processes AWS data sources and logs in your AWS environment. GuardDuty uses threat intelligence feeds, such as lists of malicious IP addresses and domains, file hashes, and ML models to identify suspicious and potentially malicious activity in the AWS environment. 
Logistics agent dashboard 
The following high-level architecture diagram illustrates the logistics agent dashboard, which captures the end-user interactions and its associated responses. 
 
The logistics agent dashboard is hosted as a static website using CloudFront and Amazon S3. Dashboard access is allowed only for the DTDC admin team. 
The dashboard is populated through API calls using Amazon API Gateway with Lambda as a backend, which retrieves the dashboard data from Amazon RDS for PostgreSQL. 
The dashboard provides real-time insights into the logistics agent performance, including accuracy, unresolved queries, query categories, session statistics, and user interaction data (see the following screenshot). It provides actionable insights with features such as heat maps, pie charts, and session logs. Real-time data is logged and analyzed on the dashboard, enabling continuous improvement and quick issue resolution. 
 
Solution challenges and benefits 
When implementing DIVA 2.0, DTDC and ShellKode faced several significant challenges. Integrating real-time data from multiple legacy systems was crucial for providing accurate, up-to-date information on tracking, rates, and serviceability. This was likely addressed through the robust API integration capabilities of Amazon Bedrock Agents. Another major hurdle was training the AI to understand complex logistics terminology and multi-step queries, which was overcome by using Amazon Bedrock LLMs and Amazon Bedrock Knowledge Bases, fine-tuned with industry-specific data. The team also had to navigate the delicate process of transitioning from the old rigid DIVA system while maintaining service continuity and preserving historical data, potentially employing a phased approach with parallel systems. Finally, scaling the solution to handle over 400,000 monthly queries while maintaining performance was a significant challenge, addressed by using the cloud infrastructure of Amazon Bedrock Agents for optimal scalability and performance. These challenges underscore the complexity of upgrading to an AI-powered system in a high-volume, data-intensive industry like logistics, and highlight how AWS solutions provided the necessary tools to overcome these obstacles. DTDC realized the following benefits from powering the logistics agent with generative AI using Amazon Bedrock: 
 
 Enhanced conversations and real-time data access with customer support agents ‚Äì Powered by Amazon Bedrock Agents, the solution improves natural language understanding, enabling more fluid and engaging conversations. With multi-step reasoning, it can handle a broader range of queries with greater accuracy. Additionally, by integrating seamlessly with DTDC‚Äôs API layer, the logistics agent provides real-time access to vital information, such as tracking shipments, service availability, and calculating shipping rates. The combination of advanced conversational capabilities and real-time data provides fast, accurate, and contextually relevant responses. 
 Intelligent data processing and accurate FAQ responses ‚Äì For complex queries, the logistics agent uses LLM technology to process raw data and deliver structured, tailored responses. This makes sure users get clear, actionable insights. For frequently asked questions, the logistics agent uses Amazon Bedrock Knowledge Bases to deliver precise answers without requiring human support, reducing wait times and enhancing the overall user experience. 
 Reduced live agent dependency and continuous improvement ‚Äì Although the logistics agent hasn‚Äôt eliminated the need for customer support, the number of queries handled by the customer support team has reduced by 51.4%. The system provides valuable insights into key performance metrics like peak query times, unresolved issues, and overall engagement through integrated real-time analytics, helping refine and improve the assistant‚Äôs capabilities over time. 
 
Results 
The generative AI-powered logistics agent has reduced the burden on customer support teams and shortened resolution times, resulting in better customer experience: 
 
 Powered by Amazon Bedrock, DIVA 2.0 understands queries in natural language and supports dynamic conversations with a response accuracy of 93% 
 Based on the last 3 months of dashboard metrics data, they observed the following: 
   
   71% of the inquiries were related to consignments (256,048), whereas 29.5% were general inquiries (107,132) 
   51.4% of consignment inquiries (131,530) didn‚Äôt result in a support ticket, whereas 48.6% (124,518) led to new support ticket creation 
   Of the inquiries that resulted in tickets, 40% started with the customer support center before moving to the AI assistant, whereas 60% began with the assistant before involving the customer support center 
    
 
DIVA 2.0 has reduced the number of queries handled by the customer support team by 51.4%. DTDC‚Äôs support team can now focus on more critical issues, improving overall efficiency. 
Summary 
This post demonstrated how Amazon Bedrock can transform a traditional chatbot to a generative AI-powered logistics agent that provides better customer experience through dynamic conversation. For businesses facing similar challenges, this solution offers a blueprint for modernizing your AI assistant while maintaining compliance with industry standards. 
To learn more about this AWS solution, contact AWS for further assistance. AWS can provide detailed information about implementation, pricing, and how to tailor the solution to your specific business needs. 
 
About the authors 
Rishi Sareen ‚Äì Chief Information Officer (CIO), DTDC is a seasoned technology leader with over two decades of experience in driving digital transformation, enterprise IT strategy, and innovation across the logistics and supply chain sector. He specializes in building agile, AI-driven, and secure technology ecosystems that enhance operational efficiency and customer experience. Rishi leads initiatives spanning system modernization, data intelligence, automation, cybersecurity, cloud, and artificial intelligence. He is deeply committed to aligning technology with business outcomes while fostering a culture of continuous improvement and purposeful innovation. A strong advocate for people-centric leadership, Rishi places high emphasis on nurturing talent, building high-performing teams, and mentoring future-ready technology leaders who can thrive in dynamic, AI-powered environments. Known for his strategic vision and disciplined execution, he has led large-scale digital initiatives and transformation programs that deliver lasting business impact. 
Arunraja Karthick ‚Äì Head ‚Äì IT Services &amp; Security (CISO), DTDC is a strategic IT and cybersecurity leader with over 15 years of experience driving enterprise-scale digital transformation. As the Head of IT Services &amp; Security (CISO) at DTDC Express Limited, he leads the organization‚Äôs core IT, cloud, and security programs‚Äîtransforming legacy environments into agile, secure, and cloud-native ecosystems. Under his leadership, DTDC has adopted a hybrid cloud architecture spanning AWS, GCP, and on-prem colocation, with a vision to enable dynamic workload mobility and vendor-neutral scalability. Arunraja has led critical modernization efforts, including the migration of key business applications to microservices and containerized platforms, while ensuring high availability and regulatory compliance. Known for his deep technical insight and execution discipline, he has implemented enterprise-wide cybersecurity frameworks‚Äîfrom Email DLP, Mobile Device Management, and Conditional Access to Hybrid WAF and advanced SOC operations. He has also championed secure access transformation through Zero Trust-aligned Secure WebVPN, redefining how internal users access corporate apps. Arunraja‚Äôs leadership is grounded in platform thinking, automation, and a user-first mindset. His recent initiatives include the enterprise rollout of GenAI copilots for customer experience and operations, as well as unified policy-based DLP and content control mechanisms across endpoints and cloud. Recognized as an Influential Technology Leader, Arunraja continues to challenge conventional IT boundaries‚Äîaligning security, agility, and innovation to power business evolution. 
Bakrudeen K an AWS Ambassador, leads the AI/ML practice at Shellkode, focusing on driving innovation in artificial intelligence, especially in Generative AI. He plays a key role in building teams and advanced AI solutions, Agentic Assistants, and other next-gen technologies. Bakrudeen has made notable contributions to AI/ML research and development. In 2023 and 2024, he received the Generative AI Consulting Excellence Partner Award at the AI Conclave and the Social Impact Partner of the Year Award for Generative AI at AWS re:Invent 2024, both on behalf of Shellkode reflecting the team‚Äôs strong commitment to innovation and impact in the AI space. 
Suresh Kanniappan is a Solutions Architect at AWS, handling Automotive, Manufacturing and Logistics enterprises in India. He is passionate about cloud security and Industry solutions that can solve real world problems. Prior to AWS, he worked for AWS customers and partners in consulting, migration and solution architecture roles for over 14 years. 
Sid Chandilya is a Sr. Customer Relations Manager at AWS, responsible for tech led business transformation with Automotive, Manufacturing and Logistics enterprises in India. Sid is peculiarly passionate about challenging status quos, building a joint ‚ÄúThink Big‚Äù vision with customer CXOs and leveraging Ai infused tech to accelerate outcomes. He is known for his deep understanding of industry imperatives (working backward from customer) and translating the business pain points into tech solution.
‚Ä¢ Automate enterprise workflows by integrating Salesforce Agentforce with Amazon Bedrock Agents
  AI agents are rapidly transforming enterprise operations. Although a single agent can perform specific tasks effectively, complex business processes often span multiple systems, requiring data retrieval, analysis, decision-making, and action execution across different systems. With multi-agent collaboration, specialized AI agents can work together to automate intricate workflows. 
This post explores a practical collaboration, integrating Salesforce Agentforce with Amazon Bedrock Agents and Amazon Redshift, to automate enterprise workflows. 
Multi-agent collaboration in Enterprise AI 
Enterprise environments today are complex, featuring diverse technologies across multiple systems. Salesforce and AWS provide distinct advantages to customers. Many organizations already maintain significant infrastructure on AWS, including data, AI, and various business applications such as ERP, finance, supply chain, HRMS, and workforce management systems. Agentforce delivers powerful AI-driven agent capabilities that are grounded in enterprise context and data. While Salesforce provides a rich source of trusted business data, customers increasingly need agents that can access and act on information across multiple systems. By integrating AWS-powered AI services into Agentforce, organizations can orchestrate intelligent agents that operate across Salesforce and AWS, unlocking the strengths of both. 
Agentforce and Amazon Bedrock Agents can work together in flexible ways, leveraging the unique strengths of both platforms to deliver smarter, more comprehensive AI workflows. Example collaboration models include: 
 
 Agentforce as the primary orchestrator: 
   
   Manages end to end customer-oriented workflows 
   Delegates specialized tasks to Amazon Bedrock Agents as needed through custom actions 
   Coordinates access to external data and services across systems 
    
 
This integration creates a more powerful solution that maximizes the benefits of both Salesforce and AWS, so you can achieve better business outcomes through enhanced AI capabilities and cross-system functionality. 
Agentforce overview 
Agentforce brings digital labor to every employee, department, and business process, augmenting teams and elevating customer experiences.It works seamlessly with your existing applications, data, and business logic to take meaningful action across the enterprise. And because it‚Äôs built on the trusted Salesforce platform, your data stays secure, governed, and in your control. With Agentforce, you can: 
 
 Deploy prebuilt agents designed for specific roles, industries, or use cases 
 Enable agents to take action with existing workflows, code, and APIs 
 Connect your agents to enterprise data securely 
 Deliver accurate and grounded outcomes through the Atlas Reasoning Engine 
 
Amazon Bedrock Agents and Amazon Bedrock Knowledge Bases overview 
Amazon Bedrock is a fully managed AWS service offering access to high-performing foundation models (FMs) from various AI companies through a single API. In this post, we discuss the following features: 
 
 Amazon Bedrock Agents ‚Äì Managed AI agents use FMs to understand user requests, break down complex tasks into steps, maintain conversation context, and orchestrate actions. They can interact with company systems and data sources through APIs (configured through action groups) and access information through knowledge bases. You provide instructions in natural language, select an FM, and configure data sources and tools (APIs), and Amazon Bedrock handles the orchestration. 
 Amazon Bedrock Knowledge Bases ‚Äì This capability enables agents to perform Retrieval Augmented Generation (RAG) using your company‚Äôs private data sources. You connect the knowledge base to your data hosted in AWS, such as in Amazon Simple Storage Service (Amazon S3) or Amazon Redshift, and it automatically handles the vectorization and retrieval process. When asked a question or given a task, the agent can query the knowledge base to find relevant information, providing more accurate, context-aware responses and decisions without needing to retrain the underlying FM. 
 
Agentforce and Amazon Bedrock Agent integration patterns 
Agentforce can call Amazon Bedrock agents in different ways, allowing flexibility to build different architectures. The following diagram illustrates synchronous and asynchronous patterns. 
 
For a synchronous or request-reply interaction, Agentforce uses custom agent actions facilitated by External Services, Apex Invocable Methods, or Flow to call an Amazon Bedrock agent. The authentication to AWS is facilitated using named credentials. Named credentials are designed to securely manage authentication details for external services integrated with Salesforce. They alleviate the need to hardcode sensitive information like user names and passwords, minimizing the risk of exposure and potential data breaches. This separation of credentials from the application code can significantly enhance security posture. Named credentials streamline integration by providing a centralized and consistent method for handling authentication, reducing complexity and potential errors. You can use Salesforce Private Connect to provide a secure private connection with AWS using AWS PrivateLink. Refer to Private Integration Between Salesforce and Amazon API Gateway for additional details. 
 
For asynchronous calls, Agentforce uses Salesforce Event Relay and Flow with Amazon EventBridge to call an Amazon Bedrock agent. 
 
In this post, we discuss the synchronous call pattern. We encourage you to explore Salesforce Event Relay with EventBridge to build event-driven agentic AI workflows. Agentforce also offers the Agent API, which makes it straightforward to call an Agentforce agent from an Amazon Bedrock agent, using EventBridge API destinations, for bi-directional agentic AI workflows. 
Solution overview 
To illustrate the multi-agent collaboration between Agentforce and AWS, we use the following architecture, which provides access to Internet of Things (IoT) sensor data to the Agentforce agent and handles potentially erroneous sensor readings using a multi-agent approach. 
 
The example workflow consists of the following steps: 
 
 Coral Cloud has equipped their rooms with smart air conditioners and temperature sensors. These IoT devices capture critical information such as room temperature and error code and store it in Coral Cloud‚Äôs AWS database in Amazon Redshift. 
 Agentforce agent calls an Amazon Bedrock agent through the Agent Wrapper API with questions such as ‚ÄúWhat is the temperature in room 123‚Äù to answer customer questions related to the comfort of the room. This API is implemented as an AWS Lambda function, acting as the entry point in the AWS Cloud. 
 The Amazon Bedrock agent, upon receiving the request, needs context. It queries its configured knowledge base by generating the necessary SQL query. 
 The knowledge base is connected to a Redshift database containing historical sensor data or contextual information (like the sensor‚Äôs thresholds and maintenance history). It retrieves relevant information based on the agent‚Äôs query and responds back with an answer. 
 With the initial data and the context from the knowledge base, the Amazon Bedrock agent uses its underlying FM and natural language instructions to decide the appropriate action. In this scenario, detecting an error prompts it to create a case when it receives erroneous readings from a sensor. 
 The action group contains the Agentforce Agent Wrapper Lambda function. The Amazon Bedrock agent securely passes the necessary details (like which sensor or room needs a case) to this function. 
 The Agentforce Agent Wrapper Lambda function acts as an adapter. It translates the request from the Amazon Bedrock agent into the specific format required by the Agentforce service‚Äòs API or interface. 
 The Lambda function calls Agentforce, instructing it to create a case associated with the contact or account linked to the sensor that sent the erroneous reading. 
 Agentforce uses its internal logic (agent, topics, and actions) to create or escalate the case within Salesforce. 
 
This workflow demonstrates how Amazon Bedrock Agents orchestrates tasks, using Amazon Bedrock Knowledge Bases for context and action groups (through Lambda) to interact with Agentforce to complete the end-to-end process. 
Prerequisites 
Before building this architecture, make sure you have the following: 
 
 AWS account ‚Äì An active AWS account with permissions to use Amazon Bedrock, Lambda, Amazon Redshift, AWS Identity and Access Management (IAM), and API Gateway. 
 Amazon Bedrock access ‚Äì Access to Amazon Bedrock Agents and to Anthropic‚Äôs Claude 3.5 Haiku v1 enabled in your chosen AWS Region. 
 Redshift resources ‚Äì An operational Redshift cluster or Amazon Redshift Serverless endpoint. The relevant tables containing sensor data (historical readings, sensor thresholds, and maintenance history) must be created and populated. 
 Agentforce system ‚Äì Access to and understanding of the Agentforce system, including how to configure it. You can sign up for a developer edition with Agentforce and Data Cloud. 
 Lambda knowledge ‚Äì Familiarity with creating, deploying, and managing Lambda functions (using Python). 
 IAM roles and policies ‚Äì Understanding of how to create IAM roles with the necessary permissions for Amazon Bedrock Agents, Lambda functions (to call Amazon Bedrock, Amazon Redshift, and the Agentforce API), and Amazon Bedrock Knowledge Bases. 
 
Prepare Amazon Redshift data 
Make sure your data is structured and available in your Redshift instance. Note the database name, credentials, and table and column names. 
Create IAM roles 
For this post, we create two IAM roles: 
 
 custom_AmazonBedrockExecutionRoleForAgents: 
   
   Attach the following AWS managed policies to the role: 
     
     AmazonBedrockFullAccess 
     AmazonRedshiftDataFullAccess 
      
   In the trust relationship, provide the following trust policy (provide your AWS account ID): 
    
 
 
 {
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "AmazonBedrockAgentBedrockFoundationModelPolicyProd",
            "Effect": "Allow",
            "Principal": {
                "Service": "bedrock.amazonaws.com"
            },
            "Action": "sts:AssumeRole",
            "Condition": {
                "StringEquals": {
                    "aws:SourceAccount": "YOUR_ACCOUNT_ID"
                }
            }
        }
    ]
} 
 
 
 custom_AWSLambdaExecutionRole: 
   
   Attach the following AWS managed policies to the role: 
     
     AmazonBedrockFullAccess 
     AmazonLambdaBasicExecutionRole 
      
   In the trust relationship, provide the following trust policy (provide your AWS account ID): 
    
 
 
 {
    "Version": "2012-10-17",
   "Statement": [
       {
           "Effect": "Allow",
           "Principal": {
               "Service": "lambda.amazonaws.com"
           },
           "Action": "sts:AssumeRole",
           "Condition": {
               "StringEquals": {
                   "aws:SourceAccount": "YOUR_ACCOUNT_ID"
               }
           }
       }
   ]
} 
 
Create an Amazon Bedrock knowledge base 
Complete the following steps to create an Amazon Bedrock knowledge base: 
 
 On the Amazon Bedrock console, choose Knowledge Bases in the navigation pane. 
 Choose Create and Knowledge Base with structured data store. 
 
 
 
 On the Provide Knowledge Base details page, provide the following information: 
   
   Enter a name and optional description. 
   For Query engine, select Amazon Redshift. 
   For IAM permissions, select Use an existing service role and choose custom_AmazonBedrockExecutionRoleForAgents. 
   Choose Next.  
     
     For Query engine connection details, select Redshift provisioned and choose your cluster. 
     For Authentication, select IAM Role. 
     For Storage configuration, select Amazon Redshift database and Redshift database list. 
     On the Configure query engine page, provide the following information:  
     Provide table and column descriptions. The following is an example.  
     Choose Create Knowledge Base. 
      
    
 After you create the knowledge base, open the Redshift query editor and grant permissions for the role to access Redshift tables by running the following queries: 
 
 
 CREATE USER "IAMR:custom_AmazonBedrockExecutionRoleForAgents" WITH PASSWORD DISABLE; 

GRANT SELECT ON ALL TABLES IN SCHEMA dev.knowledgebase TO "IAMR:custom_AmazonBedrockExecutionRoleForAgents"; 

GRANT USAGE ON SCHEMA dev.knowledgebase TO "IAMR:custom_AmazonBedrockExecutionRoleForAgents"; 
 
For more information, refer to set up your query engine and permissions for creating a knowledge base with structured data store. 
 
 5. Choose Sync to sync the query engine. 
 
Make sure the status shows as Complete before moving to the next steps. 
 
 
 When the sync is complete, choose Test Knowledge Base. 
 Select Retrieval and response generation: data sources and model and choose Claude 3.5 Haiku for the model. 
 Enter a question about your data and make sure you get a valid answer. 
 
 
Create an Amazon Bedrock agent 
Complete the following steps to create an Amazon Bedrock agent: 
 
 On the Amazon Bedrock console, choose Agents in the navigation pane. 
 Choose Create agent. 
 On the Agent details page, provide the following information: 
   
   Enter a name and optional description. 
   For Agent resource role, select Use an existing service role and choose custom_AmazonBedrockExecutionRoleForAgents. 
    
 
 
 
 Provide detailed instructions for your agent. The following is an example: 
 
 
 You are an IoT device monitoring and alerting agent. 
You have access to the structured data containing reading, maintenance, threshold data for IoT devices. 
You answer questions about device reading, maintenance schedule and thresholds. 
You can also create case via Agentforce. 
When you receive comma separated values parse them as device_id, temperature, voltage, connectivity and error_code. 
First check if the temperature is less than min temperature, more than max temperature and connectivity is more than the connectivity threshold for the product associated with the device id. 
If there is an error code, send information to agentforce to create case. The information sent to agentforce should include device readings such as device id, error code. 
It should also include the threshold values related to the product associated with the device such as min temperature, max temperature and connectivity, 
In response to your call to agentforce just return the summary of the information provided with all the attributes provided. 
Do not omit any information in the response. Do not include the word escalated in agent. 
 
 
 Choose Save to save the agent. 
 Add the knowledge base you created in previous step to this agent. 
 
 
 
 Provide detailed instructions about the knowledge base for the agent. 
 
 
 
 Choose Save and then choose Prepare the agent. 
 Test the agent by asking a question (in the following example, we ask about sensor readings). 
 
 
 
 Choose Create alias. 
 On the Create alias page, provide the following information: 
   
   Enter an alias name and optional description. 
   For Associate version, select Create a new version and associate it to this alias. 
   For Select throughput, select On-demand. 
   Choose Create alias. 
    
 
 
 
 Note down the agent ID, which you will use in subsequent steps. 
 Note down the alias ID and agent ID, which you will use in subsequent steps. 
 
 
Create a Lambda function 
Complete the following steps to create a Lambda function to receive requests from Agentforce: 
 
 On the Lambda console, choose Functions in the navigation pane. 
 Choose Create function. 
 Configure the function with the following logic to receive requests through API Gateway and call Amazon Bedrock agents: 
 
 
 import boto3
import uuid
import json
import pprint
import traceback
import time
import logging
from agent_utils import invoke_agent_generate_response
logger = logging.getLogger()
logger.setLevel(logging.INFO)
bedrock_agent_runtime_client = boto3.client(
service_name="bedrock-agent-runtime",
region_name="REGION_NAME", # replace with the region name from your account
)
def lambda_handler(event, context):
    logger.info(event)
    body = event['body']
    input_text = json.loads(body)['inputText']
    agent_id = 'XXXXXXXX' # replace with the agent id from your account
    agent_alias_id = 'XXXXXXX' # replace with the alias id from your account
    session_id:str = str(uuid.uuid1()) # random identifier
    enable_trace:bool = False
    end_session:bool = False
    final_answer = None
    response = call_agent(input_text, agent_id, agent_alias_id)
    print("response : ")
    print(response)
 
    return {
        'headers': {
            'Content-Type' : 'application/json',
            'Access-Control-Allow-Headers': '*',
            'Access-Control-Allow-Origin': '*',
            'Access-Control-Allow-Methods': '*'
        },
        
        'statusCode': 200,
        'body': json.dumps({"outputText" :  response  })
    }
def call_agent(inputText, agentId, agentAliasId): 
    session_id = str(uuid.uuid1())
    enable_trace = False
    end_session = False
    while True:
        try:
            agent_response = bedrock_agent_runtime_client.invoke_agent(
                inputText=inputText,
                agentId=agentId,
                agentAliasId=agentAliasId,                
                sessionId=session_id,
                enableTrace=enable_trace,
                endSession=end_session
            )
            logger.info("Agent raw response:")
            pprint.pprint(agent_response)
            if 'completion' not in agent_response:
                raise ValueError("Missing 'completion' in agent response")
            for event in agent_response['completion']:
                chunk = event.get('chunk')
                # print('chunk: ', chunk)
                if chunk:
                    decoded_bytes = chunk.get("bytes").decode()
                    # print('bytes: ', decoded_bytes)
                    return decoded_bytes
        except Exception as e:
            print(traceback.format_exc())
            return f"Error: {str(e)}" 
 
 
 Define the necessary IAM permissions by assigning custom_AWSLambdaExecutionRole. 
 
Create a REST API 
Complete the following steps to create a REST API in API Gateway: 
 
 On the API Gateway console, create a REST API with proxy integration. 
 
 
 
 Enable API key required to protect the API from unauthenticated access. 
 
 
 
 Configure the usage plan and API key. For more details, see Set up API keys for REST APIs in API Gateway. 
 Deploy the API. 
 Note down the Invoke URL to use in subsequent steps. 
 
 
Create named credentials in Salesforce 
Now that you have created an Amazon Bedrock agent with an API Gateway endpoint and Lambda wrapper, let‚Äôs complete the configuration on the Salesforce side. Complete the following steps: 
 
 Log in to Salesforce. 
 Navigate to Setup, Security, Named Credentials. 
 On the External Credentials tab, choose New. 
 
 
 
 Provide the following information: 
   
   Enter a label and name. 
   For Authentication Protocol, choose Custom. 
   Choose Save. 
    
 
 
 
 Open the External Credentials entry to provide additional details: 
   
   Under Principals, create a new principal and provide the parameter name and value. 
    
 
 
 
  
   
   Under Custom Headers, create a new entry and provide a name and value. 
   Choose Save. 
    
 
 
Now you can grant access to the agent user to access these credentials. 
 
 Navigate to Setup, Users, User Profile, Enabled External Credential Principal Access and add the external credential principal you created to the allow list. 
 
 
 
 Choose New to create a named credentials entry. 
 Provide details such as label, name, the URL of the API Gateway endpoint, and authentication protocol, then choose Save. 
 
 
You can optionally use Salesforce Private Connect with PrivateLink to provide a secure private connection with. This allows critical data to flow from the Salesforce environment to AWS without using the public internet. 
Add an external service in Salesforce 
Complete the following steps to add an external service in Salesforce: 
 
 In Salesforce, navigate to Setup, Integrations, External Services and choose Add an External Service. 
 For Select an API source, choose From API Specification. 
 
 
 
 On the Edit an External Service page, provide the following information: 
   
   Enter a name and optional description. 
   For Service Schema, choose Upload from local. 
   For Select a Named Credential, choose the named credential you created. 
    
 
 
 
 Upload an Open API specification for the API Gateway endpoint. See the following example: 
 
 
 openapi: 3.0.0
info:
  title: Bedrock Agent Wrapper API
  version: 1.0.0
  description: Bedrock Agent Wrapper API
paths:
  /proxy:
    post:
      operationId: call-bedrock-agent
      summary: Call Bedrock Agent
      description: Call Bedrock Agent
      requestBody:
        description: input
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/input'
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/output'
        '500':
          description: Server error
components:
  schemas:
    input:
      type: object
      properties:
        inputText:
          type: string
        agentId:
          type: string
        agentAlias:
          type: string
    output:
      type: object
      properties:
        outputText:
          type: string 
 
 
 Choose Save and Next. 
 Enable the operation to make it available for Agentforce to invoke. 
 Choose Finish. 
 
 
Create an Agentforce agent action to use the external service 
Complete the following steps to create an Agentforce agent action: 
 
 In Salesforce, navigate to Setup, Agentforce, Einstein Generative AI, Agentforce Studio, Agentforce Assets. 
 On the Actions tab, choose New Agent Action. 
 Under Connect to an existing action, provide the following information: 
   
   For Reference Action Type, choose API. 
   For Reference Action Category, choose External Services. 
   For Reference Action, choose the Call Bedrock Agent action that you configured. 
   Enter an agent action label and API name. 
   Choose Next. 
    
 
 
 
 Provide the following information to complete the agent action configuration: 
   
   For Agent Action Instructions, enter Call Bedrock Agent to get the information about device readings, sensor readings, maintenance or threshold information. 
   For Loading Text, enter Calling Bedrock Agent. 
   Under Input, for Body, enter Provide the input in the input Text field. 
   Under Outputs, for 200, enter Successful response. 
    
 
 
 
 Save the agent action. 
 
Configure the Agentforce agent to use the agent action 
Complete the following steps to configure the Agentforce agent to use the agent action: 
 
 In Salesforce, navigate to Setup, Agentforce, Einstein Generative AI, Agentforce Studio, Agentforce Agents and open the agent in Agent Builder. 
 Create a new topic. 
 On the Topic Configuration tab, provide the following information: 
   
   For Name, enter Device Information. 
   For Classification Description, enter This topic handles inquiries related to device and sensor information, including reading, maintenance, and threshold. 
   For Scope, enter Your job is only to provide information about device readings, sensor readings, device maintenance, sensor maintenance, and threshold. Do not attempt to address issues outside of providing device information. 
   For Instructions, enter the following: 
    
 
 
 If a user asks for device readings or sensor readings, provide the information.
If a user asks for device maintenance or sensor maintenance, provide the information.
When searching for device information, include the device or sensor id and any relevant keywords in your search query. 
 
 
 
 On the This Topic‚Äôs Actions tab, choose New and Add from Asset Library. 
 
 
 
 Choose the Call Bedrock Agent action. 
 
 
 
 Activate the agent and enter a question, such as ‚ÄúWhat is the latest reading for sensor with device id CITDEV003.‚Äù 
 
The agent will indicate that it is calling the Amazon Bedrock agent, as shown in the following screenshot. 
 
The agent will fetch the information using the Amazon Bedrock agent from the associated knowledge base.  
Clean up 
To avoid additional costs, delete the resources that you created when you no longer need them: 
 
 Delete the Amazon Bedrock knowledge base: 
   
   On the Amazon Bedrock console, choose Knowledge Bases in the navigation pane. 
   Select the knowledge base you created and choose Delete. 
    
 Delete the Amazon Bedrock agent: 
   
   On the Amazon Bedrock console, choose Agents in the navigation pane. 
   Select the agent you created and choose Delete. 
    
 Delete the Lambda function: 
   
   On the Lambda console, choose Functions in the navigation pane. 
   Select the function you created and choose Delete. 
    
 Delete the REST API: 
   
   On the API Gateway console, choose APIs in the navigation pane. 
   Select the REST API you created and choose Delete. 
    
 
Conclusion 
In this post, we described an architecture that demonstrates the power of combining AI services on AWS with Agentforce. By using Amazon Bedrock Agents and Amazon Bedrock Knowledge Bases for contextual understanding through RAG, and Lambda functions and API Gateway to bridge interactions with Agentforce, businesses can build sophisticated, automated workflows. As AI capabilities continue to grow, such collaborative multi-agent systems will become increasingly central to enterprise automation strategies. In an upcoming post, we will show you how to build the asynchronous integration pattern from Agentforce to Amazon Bedrock using Salesforce Event Relay. 
To get started, see Become an Agentblazer Innovator and refer to How Amazon Bedrock Agents works. 
 
About the authors 
Yogesh Dhimate&nbsp;is a Sr. Partner Solutions Architect at AWS, leading technology partnership with Salesforce. Prior to joining AWS, Yogesh worked with leading companies including Salesforce driving their industry solution initiatives. With over 20 years of experience in product management and solutions architecture Yogesh brings unique perspective in cloud computing and artificial intelligence. 
Kranthi Pullagurla has over 20+ years‚Äô experience across Application Integration and Cloud Migrations across Multiple Cloud providers. He works with AWS Partners to build solutions on AWS that our joint customers can use. Prior to joining AWS, Kranthi was a strategic advisor at MuleSoft (now Salesforce). Kranthi has experience advising C-level customer executives on their digital transformation journey in the cloud. 
Shitij Agarwal is a Partner Solutions Architect at AWS. He creates joint solutions with strategic ISV partners to deliver value to customers. When not at work, he is busy exploring New York city and the hiking trails that surround it, and going on bike rides. 
Ross Belmont is a Senior Director of Product Management at Salesforce covering Platform Data Services. He has more than 15 years of experience with the Salesforce ecosystem. 
Sharda Rao is a Senior Director of Product Management at Salesforce covering Agentforce Go To Market strategy 
Hunter Reh is an AI Architect at Salesforce and a passionate builder who has developed over 100 agents since the launch of Agentforce. Outside of work, he enjoys exploring new trails on his bike or getting lost in a great book.

‚∏ª