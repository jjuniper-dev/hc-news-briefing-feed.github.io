‚úÖ Morning News Briefing ‚Äì July 08, 2025 10:55

üìÖ Date: 2025-07-08 10:55
üè∑Ô∏è Tags: #briefing #ai #publichealth #digitalgov

‚∏ª

üßæ Weather
‚Ä¢ No watches or warnings in effect, Pembroke
  No watches or warnings in effect. No warnings or watches or watches in effect . Watch or warnings are no longer in effect in the U.S. No watches, warnings are in effect for the rest of the day . No watches and warnings are still in effect, but no watches are in place for the day's events . The weather is not expected to be affected by the weather .
‚Ä¢ Current Conditions: Fog, 15.0¬∞C
  Fog temperature: 15.0&deg;C Pressure: 101.7 kPa  Visibility: 0.2 km Humidity: 96 % Dewpoint: 14.4&deg:C Wind: W 9 km/h Air Quality Health Index: n/a . The conditions were observed at Garrison Petawawa 6:00 AM EDT Tuesday 8 July 2025 at 6:
‚Ä¢ Tuesday: A mix of sun and cloud. High 26.
  Fog patches dissipating this morning . High 26. Humidex 31. UV index 9 or very high . A mix of sun and cloud expected to be sunny and breezy in the early hours of Tuesday morning . Forecast issued 5:00 AM EDT Tuesday 8 July 2025. Forecast: "Fog patches dissipated this morning. High 26" Forecast forecast: "A

üåç International News
No updates.

üçÅ Canadian News
No updates.

üá∫üá∏ U.S. Top Stories
‚Ä¢ Trump says U.S. will resume sending weapons to Ukraine after pausing last week
  Ukraine dependent on air defense systems and munitions supplied by western allies to protect Ukrainian cities . With Russian attacks escalating, Ukraine is dependent on western allies' air defenses to protect cities . Ukraine is also dependent on Western allies' help to protect its cities from Russian attacks on the country . Ukraine's air defense system and munitions are being used to protect the country from Russian incursions into its borders .
‚Ä¢ 100 years after evolution went on trial, the Scopes case still reverberates
  Dayton, Tenn., became the unlikely stage for one of the most sensational trials in American history, over the teaching of Darwin's theory of evolution . One hundred years ago, the small town of Dayton, Tennessee, was the site of one of America's most dramatic trials over evolution . The trial was held over Darwin's theories of evolution in the town of 100 years ago . Dayton was the
‚Ä¢ RFK Jr.'s vaccine policy sparks a lawsuit from the American Academy of Pediatrics
  AAP and other leading health organizations allege that the health secretary violated federal law when he took the COVID vaccine off the list of recommended shots for pregnant women and healthy children . AAP: Health secretary violated law by removing the vaccine from the list . The vaccine is now available to pregnant women, healthy children and pregnant women . The health secretary is accused of violating federal law by taking the vaccine off
‚Ä¢ Sea lions are released after toxic algae bloom in California
  Marine mammal researchers are investigating how sea lions were affected by the longest toxic algal bloom on record off the coast of Southern California . Some sea lions are now being released back into the wild . The bloom is the longest in California history, with the longest ever recorded algal blooms on the coast . Sea lions have been released back to the wild for the first time since the bloom began
‚Ä¢ Feds investigate hospitals over religious exemptions from gender-affirming care
  A physician assistant claimed she was fired by a Michigan hospital for seeking a religious exemption regarding gender-affirming care . Now the federal government is also investigating . The federal government says it is investigating the claims . The physician assistant was fired for seeking an exemption for gender-friendly care in a state of Michigan . The hospital says she was not allowed to provide the care she needed to provide .

üß† Artificial Intelligence
No updates.

üíª Digital Strategy
‚Ä¢ Firefox is fine. The people running it are not
  The snag at Mozilla is a management layer that doesn't appear to understand what works for its product nor which parts of it matter most to users . Mozilla's management is a bug, not a feature, says John Defterios . Dominance does not equal importance, nor is dominance the same as relevance, he says . The snag is that Mozilla doesn't seem to understand its product's
‚Ä¢ UK police dangle ¬£75 million to digitize its VHS tape archives
  The UK police service is planning to purchase tech and services worth up to $102 million to digitize its VHS archive . The procurement is expected to cost up to ¬£75 million ($102 million) The police service plans to purchase technology worth $75 million in order to digitally digitize their VHS archives . It's not the first time that pirated video nasties won't
‚Ä¢ Microsoft developer ported vector database coded in SAP‚Äôs ABAP to the ZX Spectrum
  Microsoft senior software engineer Alice Vinogradova has ported a database she wrote in SAP‚Äôs ABAP language to the venerable Z80 processor that powered the Sinclair ZX Spectrum . The mighty Z80 processors ran the code at astounding speed, proving retro-tech got a lot of things right . The software was written by a Microsoft software engineer in the SAP language, ABAP .
‚Ä¢ Suspected Scattered Spider domains target everyone from manufacturers to Chipotle
  Security researchers say hundreds of domains look a lot like phishing websites used by the criminal crews . The aviation industry has borne the brunt of Scattered Spider's latest round of social engineering attacks . The criminals aim to catch manufacturing and medical tech companies ‚Äî and even Chipotle Mexican Grill ‚Äî in tjeor web, as evidenced by hundreds of sites that security researchers say look similar to phishing
‚Ä¢ Epic Games settles its antitrust side quest that sought battle royale with Samsung
  Epic Games has settled the case it brought against Samsung over the Korean giant‚Äôs treatment of third-party app stores on its Galaxy handsets . They're both silent on what, if anything, has changed since the case was settled . Epic Games is silent on whether or not it has changed its terms with Samsung . The Korean giant has been silent on the details of the settlement .

üè• Public Health
No updates.

üî¨ Science
‚Ä¢ Address Colombia‚Äôs brain-health crisis
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Expand biobank sampling to three or more generations
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Outcome of universal newborn hearing screening conducted in three referral hospitals in Cameroon
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ An impact evaluation of the national prevention of mother to child HIV transmission program and MTCT associated factors in Uganda 2017‚Äì2019
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Defining informal caregivers by their characteristics safety roles and training needs in Europe
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations

üßæ Government & Policy
No updates.

üèõÔ∏è Enterprise Architecture & IT Governance
No updates.

ü§ñ AI & Emerging Tech
‚Ä¢ Why the US and Europe could lose the race for fusion energy
  Fusion energy holds the potential to shift a geopolitical landscape that is currently configured around fossil fuels. Harnessing fusion will deliver the energy resilience, security, and abundance needed for all modern industrial and service sectors. But these benefits will be controlled by the nation that leads in both developing the complex supply chains required and building fusion power plants at scales large enough to drive down economic costs.



The US and other Western countries will have to build strong supply chains across a range of technologies in addition to creating the fundamental technology behind practical fusion power plants. Investing in supply chains and scaling up complex production processes has increasingly been a strength of China‚Äôs and a weakness of the West, resulting in the migration of many critical industries from the West to China. With fusion, we run the risk that history will repeat itself. But it does not have to go that way.



The US and Europe were the dominant public funders of fusion energy research and are home to many of the world‚Äôs pioneering private fusion efforts. The West has consequently developed many of the basic technologies that will make fusion power work. But in the past five years China‚Äôs support of fusion energy has surged, threatening to allow the country to dominate the industry.







The industrial base available to support China‚Äôs nascent fusion energy industry could enable it to climb the learning curve much faster and more effectively than the West. Commercialization requires know-how, capabilities, and complementary assets, including supply chains and workforces in adjacent industries. And especially in comparison with China, the US and Europe have significantly under-supported the industrial assets needed for a fusion industry, such as thin-film processing and power electronics.



To compete, the US, allies, and partners must invest more heavily not only in fusion itself‚Äîwhich is already happening‚Äîbut also in those adjacent technologies that are critical to the fusion industrial base.&nbsp;



China‚Äôs trajectory to dominating fusion and the West‚Äôs potential route to competing can be understood by looking at today‚Äôs most promising scientific and engineering pathway to achieve grid-relevant fusion energy. That pathway relies on the tokamak, a technology that uses a magnetic field to confine ionized gas‚Äîcalled plasma‚Äîand ultimately fuse nuclei. This process releases energy that is converted from heat to electricity. Tokamaks consist of several critical systems, including plasma confinement and heating, fuel production and processing, blankets and heat flux management, and power conversion.



A close look at the adjacent industries needed to build these critical systems clearly shows China‚Äôs advantage while also providing a glimpse into the challenges of building a fusion industrial base in the US or Europe. China has leadership in three of these six key industries, and the West is at risk of losing leadership in two more. China‚Äôs industrial might in thin-film processing, large metal-alloy structures, and power electronics provides a strong foundation to establish the upstream supply chain for fusion.







The importance of thin-film processing is evident in the plasma confinement system. Tokamaks use strong electromagnets to keep the fusion plasma in place, and the magnetic coils must be made from superconducting materials. Rare-earth barium copper oxide (REBCO) superconductors are the highest-performing materials available in sufficient quantity to be viable for use in fusion.



The REBCO industry, which relies on thin-film processing technologies, currently has low production volumes spanning globally distributed manufacturers. However, as the fusion industry grows, the manufacturing base for REBCO will likely consolidate among the industry players who are able to rapidly take advantage of economies of scale. China is today‚Äôs world leader in thin-film, high-volume manufacturing for solar panels and flat-panel displays, with the associated expert workforce, tooling sector, infrastructure, and upstream materials supply chain. Without significant attention and investment on the part of the West, China is well positioned to dominate REBCO thin-film processing for fusion magnets.



The electromagnets in a full-scale tokamak are as tall as a three-story building. Structures made using strong metal alloys are needed to hold these electromagnets around the large vacuum vessel that physically contains the magnetically confined plasma. Similar large-scale, complex metal structures are required for shipbuilding, aerospace, oil and gas infrastructure, and turbines. But fusion plants will require new versions of the alloys that are radiation-tolerant, able to withstand cryogenic temperatures, and corrosion-resistant. China‚Äôs manufacturing capacity and its metallurgical research efforts position it well to outcompete other global suppliers in making the necessary specialty metal alloys and machining them into the complex structures needed for fusion.



A tokamak also requires large-scale power electronics. Here again China dominates. Similar systems are found in the high-speed rail (HSR) industry, renewable microgrids, and arc furnaces. As of 2024, China had deployed over 48,000 kilometers of HSR. That is three times the length of Europe‚Äôs HSR network and 55 times as long as the Acela network in the US, which is slower than HSR. While other nations have a presence, China‚Äôs expertise is more recent and is being applied on a larger scale.



But this is not the end of the story. The West still has an opportunity to lead the other three adjacent industries important to the fusion supply chain: cryo-plants, fuel processing, and blankets.&nbsp;



The electromagnets in an operational tokamak need to be kept at cryogenic temperatures of around 20 Kelvin to remain superconducting. This requires large-scale, multi-megawatt cryogenic cooling plants. Here, the country best set up to lead the industry is less clear. The two major global suppliers of cryo-plants are Europe-based Linde Engineering and Air Liquide Engineering; the US has Air Products and Chemicals and Chart Industries. But they are not alone: China‚Äôs domestic champions in the cryogenic sector include Hangyang Group, SASPG, Kaifeng Air Separation, and SOPC. Each of these regions already has an industrial base that could scale up to meet the demands of fusion.



Fuel production for fusion is a nascent part of the industrial base requiring processing technologies for light-isotope gases‚Äîhydrogen, deuterium, and tritium. Some processing of light-isotope gases is already done at small scale in medicine, hydrogen weapons production, and scientific research in the US, Europe, and China. But the scale needed for the fusion industry does not exist in today‚Äôs industrial base, presenting a major opportunity to develop the needed capabilities.



Similarly, blankets and heat flux management are an opportunity for the West. The blanket is the medium used to absorb energy from the fusion reaction and to breed tritium. Commercial-scale blankets will require entirely novel technology. To date, no adjacent industries have relevant commercial expertise in liquid lithium, lead-lithium eutectic, or fusion-specific molten salts that are required for blanket technology. Some overlapping blanket technologies are in early-stage development by the nuclear fission industry. As the largest producer of beryllium in the world, the US has an opportunity to capture leadership because that element is a key material in leading fusion blanket concepts. But the use of beryllium must be coupled with technology development programs for the other specialty blanket components.



These six industries will prove critical to scaling fusion energy. In some, such as thin-film processing and large metal-alloy structures, China already has a sizable advantage. Crucially, China recognizes the importance of these adjacent industries and is actively harnessing them in its fusion efforts. For example, China launched a fusion consortium that consists of industrial giants spanning the steel, machine tooling, electric grid, power generation, and aerospace sectors. It will be extremely difficult for the West to catch up in these areas, but policymakers and business leaders must pay attention and try to create robust alternative supply chains.



As the industrial area of greatest strength, cryo-plants could continue to be an opportunity for leadership in the West. Bolstering Western cryo-plant production by creating demand for natural-gas liquefaction will be a major boon to the future cryo-plant supply chain that will support fusion energy.



The US and European countries also have an opportunity to lead in the emerging industrial areas of fuel processing and blanket technologies. Doing so will require policymakers to work with companies to ensure that public and private funding is allocated to these critical emerging supply chains. Governments may well need to serve as early customers and provide debt financing for significant capital investment. Governments can also do better to incentivize private capital and equity financing‚Äîfor example, through favorable capital-gains taxation. In lagging areas of thin-film and alloy production, the US and Europe will likely need partners, such as South Korea and Japan, that have the industrial bases to compete globally with China.



The need to connect and capitalize multiple industries and supply chains will require long-term thinking and clear leadership. A focus on the demand side of these complementary industries is essential. Fusion is a decade away from maturation, so its supplier base must be derisked and made profitable in the near term by focusing on other primary demand markets that contribute to our economic vitality. To name a few, policymakers can support modernization of the grid to bolster domestic demand for power electronics and domestic semiconductor manufacturing to support thin-film processing.



The West must also focus on the demand for energy production itself. As the world‚Äôs largest energy consumer, China will leverage demand from its massive domestic market to climb the learning curve and bolster national champions. This is a strategy that China has wielded with tremendous success to dominate global manufacturing, most recently in the electric-vehicle industry. Taken together, supply- and demand-side investment have been a winning strategy for China.



The competition to lead the future of fusion energy is here. Now is the moment for the US and its Western allies to start investing in the foundational innovation ecosystem needed for a vibrant and resilient industrial base to support it.



Daniel F. Brunner is a co-founder of Commonwealth Fusion Systems and a Partner at Future Tech Partners.



Edlyn V. Levine is the co-founder of a stealth-mode technology start up and an affiliate of the MIT Sloan School of Management.



Fiona E. Murray is a professor of entrepreneurship at the MIT School of Management and&nbsp;Vice Chair of the NATO Innovation Fund.



Rory Burke is a graduate of MIT Sloan and a former summer scholar with&nbsp;ARPA-E.
‚Ä¢ How scientists are trying to use AI to unlock the human mind
  Today‚Äôs AI landscape is defined by the ways in which neural networks are unlike human brains. A toddler learns how to communicate effectively with only a thousand calories a day and regular conversation; meanwhile, tech companies are reopening nuclear power plants, polluting marginalized communities, and pirating terabytes of books in order to train and run their LLMs.



But neural networks are, after all, neural‚Äîthey‚Äôre inspired by brains. Despite their vastly different appetites for energy and data, large language models and human brains do share a good deal in common. They‚Äôre both made up of millions of subcomponents: biological neurons in the case of the brain, simulated ‚Äúneurons‚Äù in the case of networks. They‚Äôre the only two things on Earth that can fluently and flexibly produce language. And scientists barely understand how either of them works.



I can testify to those similarities: I came to journalism, and to AI, by way of six years of neuroscience graduate school. It‚Äôs a common view among neuroscientists that building brainlike neural networks is one of the most promising paths for the field, and that attitude has started to spread to psychology. Last week, the prestigious journal Nature published a pair of studies showcasing the use of neural networks for predicting how humans and other animals behave in psychological experiments. Both studies propose that these trained networks could help scientists advance their understanding of the human mind. But predicting a behavior and explaining how it came about are two very different things.



In one of the studies, researchers transformed a large language model into what they refer to as a ‚Äúfoundation model of human cognition.‚Äù Out of the box, large language models aren‚Äôt great at mimicking human behavior‚Äîthey behave logically in settings where humans abandon reason, such as casinos. So the researchers fine-tuned Llama 3.1, one of Meta‚Äôs open-source LLMs, on data from a range of 160 psychology experiments, which involved tasks like choosing from a set of ‚Äúslot machines‚Äù to get the maximum payout or remembering sequences of letters. They called the resulting model Centaur.





Compared with conventional psychological models, which use simple math equations, Centaur did a far better job of predicting behavior. Accurate predictions of how humans respond in psychology experiments are valuable in and of themselves: For example, scientists could use Centaur to pilot their experiments on a computer before recruiting, and paying, human participants. In their paper, however, the researchers propose that Centaur could be more than just a prediction machine. By interrogating the mechanisms that allow Centaur to effectively replicate human behavior, they argue, scientists could develop new theories about the inner workings of the mind.



But some psychologists doubt whether Centaur can tell us much about the mind at all. Sure, it‚Äôs better than conventional psychological models at predicting how humans behave‚Äîbut it also has a billion times more parameters. And just because a model behaves like a human on the outside doesn‚Äôt mean that it functions like one on the inside. Olivia Guest, an assistant professor of computational cognitive science at Radboud University in the Netherlands, compares Centaur to a calculator, which can effectively predict the response a math whiz will give when asked to add two numbers. ‚ÄúI don‚Äôt know what you would learn about human addition by studying a calculator,‚Äù she says.



Even if Centaur does capture something important about human psychology, scientists may struggle to extract any insight from the model‚Äôs millions of neurons. Though AI researchers are working hard to figure out how large language models work, they‚Äôve barely managed to crack open the black box. Understanding an enormous neural-network model of the human mind may not prove much easier than understanding the thing itself.



One alternative approach is to go small. The second of the two Nature studies focuses on minuscule neural networks‚Äîsome containing only a single neuron‚Äîthat nevertheless can predict behavior in mice, rats, monkeys, and even humans. Because the networks are so small, it‚Äôs possible to track the activity of each individual neuron and use that data to figure out how the network is producing its behavioral predictions. And while there‚Äôs no guarantee that these models function like the brains they were trained to mimic, they can, at the very least, generate testable hypotheses about human and animal cognition.



There‚Äôs a cost to comprehensibility. Unlike Centaur, which was trained to mimic human behavior in dozens of different tasks, each tiny network can only predict behavior in one specific task. One network, for example, is specialized for making predictions about how people choose among different slot machines. ‚ÄúIf the behavior is really complex, you need a large network,‚Äù says Marcelo Mattar, an assistant professor of psychology and neural science at New York University who led the tiny-network study and also contributed to Centaur. ‚ÄúThe compromise, of course, is that now understanding it is very, very difficult.‚Äù



This trade-off between prediction and understanding is a key feature of neural-network-driven science. (I also happen to be writing a book about it.) Studies like Mattar‚Äôs are making some progress toward closing that gap‚Äîas tiny as his networks are, they can predict behavior more accurately than traditional psychological models. So is the research into LLM interpretability happening at places like Anthropic. For now, however, our understanding of complex systems‚Äîfrom humans to climate systems to proteins‚Äîis lagging farther and farther behind our ability to make predictions about them.



This story originally appeared in&nbsp;The Algorithm, our weekly newsletter on AI. To get stories like this in your inbox first,&nbsp;sign up here.
‚Ä¢ Inside the most dangerous asteroid hunt ever
  If you were told that the odds of something were 3.1%, it really wouldn‚Äôt seem like much. But for the people charged with protecting our planet, it was huge.&nbsp;



On February 18, astronomers determined that a 130- to 300-foot-long asteroid had a 3.1% chance of crashing into Earth in 2032. Never had an asteroid of such dangerous dimensions stood such a high chance of striking the planet. For those following this developing story in the news, the revelation was unnerving. For many scientists and engineers, though, it turned out to be‚Äîdespite its seriousness‚Äîa little bit exciting.



While possible impact locations included patches of empty ocean, the space rock, called 2024 YR4, also had several densely populated cities in its possible crosshairs, including Mumbai, Lagos, and Bogot√°. If the asteroid did in fact hit such a metropolis, the best-case scenario was severe damage; the worst case was outright, total ruin. And for the first time, a group of United Nations‚Äìbacked researchers began to have high-level discussions about the fate of the world: If this asteroid was going to hit the planet, what sort of spaceflight mission might be able to stop it? Would they ram a spacecraft into it to deflect it? Would they use nuclear weapons to try to swat it away or obliterate it completely?&nbsp;



At the same time, planetary defenders all over the world crewed their battle stations to see if we could avoid that fate‚Äîand despite the sometimes taxing new demands on their psyches and schedules, they remained some of the coolest customers in the galaxy. ‚ÄúI‚Äôve had to cancel an appointment saying, I cannot come‚ÄîI have to save the planet,‚Äù says Olivier Hainaut, an astronomer at the European Southern Observatory and one of those who tracked down 2024 YR4.&nbsp;



Then, just as quick as history was made, experts declared that the danger had passed. On February 24, asteroid trackers issued the all-clear: Earth would be spared, just as many planetary defense researchers had felt assured it would.&nbsp;



How did they do it? What was it like to track the rising (and rising and rising) danger of this asteroid, and to ultimately determine that it‚Äôd miss us?



This is the inside story of how, over a span of just two months, a sprawling network of global astronomers found, followed, mapped, planned for, and finally dismissed 2024 YR4, the most dangerous asteroid ever found‚Äîall under the tightest of timelines and, for just a moment, with the highest of stakes.&nbsp;



‚ÄúIt was not an exercise,‚Äù says Hainaut. This was the real thing: ‚ÄúWe really [had] to get it right.‚Äù







IN THE BEGINNING







December 27, 2024



THE ASTEROID TERRESTRIAL-IMPACT LAST ALERT SYSTEM, HAWAII



Long ago, an asteroid in the space-rock highway between Mars and Jupiter felt a disturbance in the force: the gravitational pull of Jupiter itself, king of the planets. After some wobbling back and forth, this asteroid was thrown out of the belt, skipped around the sun, and found itself on an orbit that overlapped with Earth‚Äôs own.&nbsp;



‚ÄúI was the first one to see the detections of it,‚Äù Larry Denneau, of the University of Hawai‚Äòi, recalls. ‚ÄúA tiny white pixel on a black background.‚Äù&nbsp;



Denneau is one of the principal investigators at the NASA-funded Asteroid Terrestrial-impact Last Alert System (ATLAS) telescopic network. It may have been just two days after Christmas, but as usual, he followed procedure as if it were any other day of the year and sent the observations of the tiny pixel onward to another NASA-funded facility, the Minor Planet Center (MPC) in Cambridge, Massachusetts.&nbsp;





There‚Äôs an alternate reality in which none of this happened. Fortunately, in our timeline, various space agencies‚Äîchiefly NASA, but also the European Space Agency and the Japan Aerospace Exploration Agency‚Äîinvest millions of dollars every year in asteroid-spotting efforts.&nbsp;



And while multiple nations host observatories capable of performing this work, the US clearly leads the way: Its planetary defense program provides funding to a suite of telescopic facilities solely dedicated to identifying potentially hazardous space rocks. (At least, it leads the way for the moment. The White House‚Äôs proposal for draconian budget cuts to NASA and the National Science Foundation mean that several observatories and space missions linked to planetary defense are facing funding losses or outright terminations.)&nbsp;



Astronomers working at these observatories are tasked with finding threatening asteroids before they find us‚Äîbecause you can‚Äôt fight what you can‚Äôt see. ‚ÄúThey are the first line of planetary defense,‚Äù says Kelly Fast, the acting planetary defense officer at NASA‚Äôs Planetary Defense Coordination Office in Washington, DC.



ATLAS is one part of this skywatching project, and it consists of four telescopes: two in Hawaii, one in Chile, and another in South Africa. They don‚Äôt operate the way you‚Äôd think, with astronomers peering through them all night. Instead, they operate ‚Äúcompletely robotically and automatically,‚Äù says Denneau. Driven by coding scripts that he and his colleagues have developed, these mechanical eyes work in harmony to watch out for any suspicious space rocks. Astronomers usually monitor their survey of the sky from a remote location.



ATLAS telescopes are small, so they can‚Äôt see particularly distant objects. But they have a wide field of view, allowing them to see large patches of space at any one moment. ‚ÄúAs long as the weather is good, we‚Äôre constantly monitoring the night sky, from the North Pole to the South Pole,‚Äù says Denneau.&nbsp;



Larry Denneau, a principal investigator at the Asteroid Terrestrial-impact Last Alert System telescopic network.COURTESY PHOTO




If they detect the starlight reflecting off a moving object, an operator, such as Denneau, gets an alert and visually verifies that the object is real and not some sort of imaging artifact. When a suspected asteroid (or comet) is identified, the observations are sent to the MPC, which is home to a bulletin board featuring (among other things) orbital data on all known asteroids and comets.&nbsp;



If the object isn‚Äôt already listed, a new discovery is announced, and other astronomers can perform follow-up observations.&nbsp;



In just the past few years, ATLAS has detected more than 1,200 asteroids with near-Earth orbits. Finding ultimately harmless space rocks is routine work‚Äîso much so that when the new near-Earth asteroid was spotted by ATLAS‚Äôs Chilean telescope that December day, it didn‚Äôt even raise any eyebrows.&nbsp;



Denneau had simply been sitting at home, doing some late-night work on his computer. At the time, of course, he didn‚Äôt know that his telescope had just spied what would soon become a history-making asteroid‚Äîone that could alter the future of the planet.



The MPC quickly confirmed the new space rock hadn‚Äôt already been ‚Äúfound,‚Äù and astronomers gave it a provisional designation: 2024 YR4.¬†



CATALINA SKY SURVEY, ARIZONA



Around the same time, the discovery was shared with another NASA-funded facility: the Catalina Sky Survey, a nest of three telescopes in the Santa Catalina Mountains north of Tucson that works out of the University of Arizona. ‚ÄúWe run a very tight operation,‚Äù says Kacper Wierzcho≈õ, one of its comet and asteroid spotters. Unlike ATLAS, these telescopes (although aided by automation) often have an in-person astronomer available to quickly alter the surveys in real time.



‚ÄúWe run a very tight operation,‚Äù says Kacper Wierzcho≈õ, one of the comet and asteroid spotters at the Catalina Sky Survey north of Tucson, Arizona. COURTESY PHOTO




So when Catalina was alerted about what its peers at ATLAS had spotted, staff deployed its Schmidt telescope‚Äîa smaller one that excels at seeing bright objects moving extremely quickly. As they fed their own observations of 2024 YR4 to the MPC, Catalina engineer David Rankin looked back over imagery from the previous days and found the new asteroid lurking in a night-sky image taken on December 26. Around then, ATLAS also realized that it had caught sight of 2024 YR4 in a photograph from December 25.&nbsp;



The combined observations confirmed it: The asteroid had made its closest approach to Earth on Christmas Day, meaning it was already heading back out into space. But where, exactly, was this space rock going? Where would it end up after it swung around the sun?&nbsp;



CENTER FOR NEAR-EARTH OBJECT STUDIES, CALIFORNIA&nbsp;



If the answer to that question was Earth, Davide Farnocchia would be one of the first to know. You could say he‚Äôs one of NASA‚Äôs watchers on the wall.&nbsp;



And he‚Äôs remarkably calm about his duties. When he first heard about 2024 YR4, he barely flinched. It was just another asteroid drifting through space not terribly far from Earth. It was another box to be ticked.



Once it was logged by the MPC, it was Farnocchia‚Äôs job to try to plot out 2024 YR4‚Äôs possible paths through space, checking to see if any of them overlapped with our planet‚Äôs. He works at NASA‚Äôs Center for Near-Earth Object Studies (CNEOS) in California, where he‚Äôs partly responsible for keeping track of all the known asteroids and comets in the solar system. ‚ÄúWe have 1.4 million objects to deal with,‚Äù he says, matter-of-factly.¬†



In the past, astronomers would have had to stitch together multiple images of this asteroid and plot out its possible trajectories. Today, fortunately, Farnocchia has some help: He oversees the digital brain Sentry, an autonomous system he helped code. (Two other facilities in Italy perform similar work: the European Space Agency‚Äôs Near-Earth Object Coordination Centre, or NEOCC, and the privately owned Near-Earth Objects Dynamics Site, or NEODyS.)



To chart their courses, Sentry uses every new observation of every known asteroid or comet listed on the MPC to continuously refine the orbits of all those objects, using the immutable laws of gravity and the gravitational influences of any planets, moons, or other sizable asteroids they pass. A recent update to the software means that even the ever-so-gentle push afforded by sunlight is accounted for. That allows Sentry to confidently project the motions of all these objects at least a century into the future.&nbsp;



Davide Farnocchia, who helps track all the known asteroids and comets in the solar system at NASA‚Äôs Center for Near-Earth Object Studies.COURTESY PHOTO




Almost all newly discovered asteroids are quickly found to pose no impact risk. But those that stand even an infinitesimally small chance of smashing into our planet within the next 100 years are placed on the Sentry Risk List until additional observations can rule out those awful possibilities. Better safe than sorry.¬†



In late December, with just a limited set of data, Sentry concluded that there was a non-negligible chance 2024 YR4 would strike Earth in 2032. Aegis, the equivalent software at Europe‚Äôs NEOCC site, agreed. No bother. More observations would very likely remove 2024 YR4 from the Risk List. Just another day at the office for Farnocchia.



It‚Äôs worth noting that an asteroid heading toward Earth isn‚Äôt always a problem. Small rocks burn up in the planet‚Äôs atmosphere several times a day; you‚Äôve probably seen one already this year, on a moonless night. But above a certain size, these rocks turn from innocuous shooting stars into nuclear-esque explosions.&nbsp;



Reflected starlight is great for initially spotting asteroids, but it‚Äôs a terrible way to determine how big they are. A large, dull rock reflects as much light as a bright, tiny rock, making them appear the same to many telescopes. And that‚Äôs a problem, considering that a rock around 30 feet long will explode loudly but inconsequentially in Earth‚Äôs atmosphere, while a 3,000-foot-long asteroid would slam into the ground and cause devastation on a global scale, imperiling all of civilization. Roughly speaking, if you double the size of an asteroid, it becomes eight times more energetic upon impact‚Äîso finding out the size of an Earthbound asteroid is of paramount importance.



In those first few hours after it was discovered, and before anyone knew how shiny or dull its surface was, 2024 YR4 was estimated by astronomers to be as small as 65 feet across or as large as 500 feet. An object of the former size would blow up in mid-air, shattering windows over many miles and likely injuring thousands of people. At the latter size it would vaporize the heart of any city it struck, turning solid rock and metal into liquid and vapor, while its blast wave would devastate the rest of it, killing hundreds of thousands or even millions in the process.&nbsp;



So now the question was: Just how big was 2024 YR4?







REFINING THE PICTURE







Mid-January 2025



VERY LARGE TELESCOPE, CHILE



Understandably dissatisfied with that level of imprecision, the European Southern Observatory‚Äôs Very Large Telescope (VLT), high up on the Cerro Paranal mountain in Chile‚Äôs Atacama Desert, entered the chat. As the name suggests, this flagship facility is vast, and it‚Äôs capable of really zooming in on distant objects. Or to put it another way: ‚ÄúThe VLT is the largest, biggest, best telescope in the world,‚Äù says Hainaut, one of the facility‚Äôs operators, who usually commands it from half a world away in Germany.&nbsp;&nbsp;



In reality, the VLT‚Äîwhich lends a hand to the European Space Agency in its asteroid-hunting duties‚Äîis actually made up of four massive telescopes, each fixed on four separate corners of the sky. They can be combined to act as a huge light bucket, allowing astronomers to see very faint asteroids. Four additional, smaller, movable telescopes can also team up with their bigger siblings to provide remarkably high-resolution images of even the stealthiest space rocks.&nbsp;







With so much tech to oversee, the control room of the VLT looks a bit like the inside of the Death Star. ‚ÄúYou have eight consoles, each of them with a dozen screens. It‚Äôs big, it‚Äôs large, it‚Äôs spectacular,‚Äù says Hainaut.&nbsp;



In mid-January, the European Space Agency asked the VLT to study several asteroids that had somewhat suspicious near-Earth orbits‚Äîincluding 2024 YR4. With just a few lines of code, the VLT could easily train its sharp eyes on an asteroid like 2024 YR4, allowing astronomers to narrow down its size range. It was found to be at least 130 feet long (big enough to cause major damage in a city) and as much as 300 feet (able to annihilate one).



January 29, 2025



INTERNATIONAL ASTEROID WARNING NETWORK



Marco Fenucci, a near-Earth-object dynamicist at the European Space Agency‚Äôs Near-Earth Object Coordination Centre.COURTESY PHOTO




By the end of the month, there was no mistaking it: 2024 YR4 stood a greater than 1% chance of impacting Earth on December 22, 2032.&nbsp;



‚ÄúIt‚Äôs not something you see very often,‚Äù says Marco Fenucci, a near-Earth-object dynamicist at NEOCC. He admits that although it was ‚Äúa serious thing,‚Äù this escalation was also ‚Äúexciting to see‚Äù‚Äîsomething straight out of a sci-fi flick.



Sentry and Aegis, along with the systems at NEODyS, had been checking one another‚Äôs calculations. ‚ÄúThere was a lot of care,‚Äù says Farnocchia, who explains that even though their programs worked wonders, their predictions were manually verified by multiple experts. When a rarity like 2024 YR4 comes along, he says, ‚Äúyou kind of switch gears, and you start being more cautious. You start screening everything that comes in.‚Äù



At this point, the klaxon emanating from these three data centers pushed the International Asteroid Warning Network (IAWN), a UN-backed planetary defense awareness group, to issue a public alert to the world‚Äôs governments: The planet may be in peril. For the most part, it was at this moment that the media‚Äîand the wider public‚Äîbecame aware of the threat. Earth, we may have a problem.



Denneau, along with plenty of other astronomers, received an urgent email from Fast at NASA‚Äôs Planetary Defense Coordination Office, requesting that all capable observatories track this hazardous asteroid. But there was one glaring problem. When 2024 YR4 was discovered on December 27, it was already two days after it had made its closest approach to Earth. And since it was heading back out into the shadows of space, it was quickly fading from sight.



Once it gets too faint, ‚Äúthere‚Äôs not much ATLAS can do,‚Äù Denneau says. By the time of IAWN‚Äôs warning, planetary defenders had just weeks to try to track 2024 YR4 and refine the odds of its hitting Earth before they‚Äôd lose it to the darkness.&nbsp;



And if their scopes failed, the odds of an Earth impact would have stayed uncomfortably high until 2028, when the asteroid was due to make another flyby of the planet. That‚Äôd be just four short years before the space rock might actually hit.



‚ÄúIn that situation, we would have been ‚Ä¶ in trouble,‚Äù says NEOCC‚Äôs Fenucci.



The hunt was on.







PREPARING FOR THE WORST







February 5 and February 6, 2025



SPACE MISSION PLANNING ADVISORY GROUP, AUSTRIA



In early February, spaceflight mission specialists, including those at the UN-supported Space Mission Planning Advisory Group in Vienna, began high-level talks designed to sketch out ways in which 2024 YR4 could be either deflected away from Earth or obliterated‚Äîyou know, just in case.



A range of options were available‚Äîincluding ramming it with several uncrewed spacecraft or assaulting it with nuclear weapons‚Äîbut there was no silver bullet in this situation. Nobody had ever launched a nuclear explosive device into deep space before, and the geopolitical ramifications of any nuclear-armed nations doing so in the present day would prove deeply unwelcome. Asteroids are also extremely odd objects; some, perhaps including 2024 YR4, are less like single chunks of rock and more akin to multiple cliffs flying in formation. Hit an asteroid like that too hard and you could fail to deflect it‚Äîand instead turn an Earthbound cannonball into a spray of shotgun pellets.&nbsp;



It‚Äôs safe to say that early on, experts were concerned about whether they could prevent a potential disaster. Crucially, eight years was not actually much time to plan something of this scale. So they were keen to better pinpoint how likely, or unlikely, it was that 2024 YR4 was going to collide with the planet before any complex space mission planning began in earnest.&nbsp;



The people involved with these talks‚Äîfrom physicists at some of America‚Äôs most secretive nuclear weapons research laboratories to spaceflight researchers over in Europe‚Äîwere not feeling close to anything resembling panic. But ‚Äúthe timeline was really short,‚Äù admits Hainaut. So there was an unprecedented tempo to their discussions. This wasn‚Äôt a drill. This was the real deal. What would they do to defend the planet if an asteroid impact couldn‚Äôt be ruled out?



Luckily, over the next few days, a handful of new observations came in. Each helped Sentry, Aegis, and the system at NEODyS rule out more of 2024 YR4‚Äôs possible future orbits. Unluckily, Earth remained a potential port of call for this pesky asteroid‚Äîand over time, our planet made up a higher proportion of those remaining possibilities. That meant that the odds of an Earth impact ‚Äústarted bubbling up,‚Äù says Denneau.&nbsp;



EVA REDAMONTI




By February 6, they jumped to 2.3%‚Äîa one-in-43 chance of an impact.&nbsp;



‚ÄúHow much anxiety someone should feel over that‚Äîit‚Äôs hard to say,‚Äù Denneau says, with a slight shrug.&nbsp;



In the past, several elephantine asteroids have been found to stand a small chance of careening unceremoniously into the planet. Such incidents tend to follow a pattern. As more observations come in and the asteroid‚Äôs orbit becomes better known, an Earth impact trajectory remains a possibility while other outlying orbits are removed from the calculations‚Äîso for a time, the odds of an impact rise. Finally, with enough observations in hand, it becomes clear that the space rock will miss our world entirely, and the impact odds plummet to zero.



Astronomers expected this to repeat itself with 2024 YR4. But there was no guarantee. There‚Äôs no escaping the fact that one day, sooner or later, scientists will discover a dangerous asteroid that will punch Earth in the face‚Äîand raze a city in the process.¬†



After all, asteroids capable of trashing a city have found their way to Earth plenty of times before, and not just in the very distant past. In 1908, an 800-square-mile patch of forest in Siberia‚Äîone that was, fortunately, very sparsely populated‚Äîwas decimated by a space rock just 180 feet long. It didn‚Äôt even hit the ground; it exploded in midair with the force of a 15-megaton blast.



But only one other asteroid comparable in size to 2024 YR4 had its 2.3% figure beat: in 2004, Apophis‚Äîcapable of causing continental-scale damage‚Äîhad (briefly) stood a 2.7% chance of impacting Earth in 2029.



Rapidly approaching uncharted waters, the powers that be at NASA decided to play a space-based wild card: the James Webb Space Telescope, or JWST.



THE JAMES WEBB SPACE TELESCOPE, DEEP SPACE, ONE MILLION MILES FROM EARTH



A large dull asteroid reflects the same amount of light as a small shiny one, but that doesn‚Äôt mean astronomers sizing up an asteroid are helpless. If you view both asteroids in the infrared, the larger one glows brighter than the smaller one no matter the surface coating‚Äîmaking infrared, or the thermal part of the electromagnetic spectrum, a much better gauge of a space rock‚Äôs proportions.&nbsp;



Observatories on Earth do have infrared capabilities, but our planet‚Äôs atmosphere gets in their way, making it hard for them to offer highly accurate readings of an asteroid‚Äôs size.&nbsp;



But the James Webb Space Telescope (JWST), hanging out in space, doesn‚Äôt have that problem.&nbsp;



Asteroid 2024 YR4 is the smallest object targeted by JWST to date, and one of the smallest objects to have its size directly measured. Observations were taken using both its NIRCam (Near-Infrared Camera) and MIRI (Mid-Infrared Instrument) to study the thermal properties of the asteroid.NASA, ESA, CSA, A. RIVKIN (APL), A. PAGAN (STSCI)




This observatory, which sits at a gravitationally stable point about a million miles from Earth, is polymathic. Its sniper-like scope can see in the infrared and allows it to peer at the edge of the observable universe, meaning it can study galaxies that formed not long after the Big Bang. It can even look at the light passing through the atmospheres of distant planets to ascertain their chemical makeups. And its remarkably sharp eye means it can also track the thermal glow of an asteroid long after all ground-based telescopes lose sight of it.



In a fortuitous bit of timing, by the moment 2024 YR4 came along, planetary defenders had recently reasoned that JWST could theoretically be used to track ominous asteroids using its own infrared scope, should the need arise. So after IAWN‚Äôs warning went out, operators of JWST ran an analysis: Though the asteroid would vanish from most scopes by late March, this one might be able to see the rock until sometime in May, which would allow researchers to greatly refine their assessment of the asteroid‚Äôs orbit and its odds of making Earth impact.



Understanding 2024 YR4‚Äôs trajectory was important, but ‚Äúthe size was the main motivator,‚Äù says Andy Rivkin, an astronomer at Johns Hopkins University‚Äôs Applied Physics Laboratory, who led the&nbsp;proposal&nbsp;to use JWST to observe the asteroid. The hope was that even if the impact odds remained high until 2028, JWST would find that 2024 YR4 was on the smaller side of the 130-to-300-feet size range‚Äîmeaning it would still be a danger, but a far less catastrophic one.&nbsp;



The JWST proposal was accepted by NASA on February 5. But the earliest it could conduct its observations was early March. And time really wasn‚Äôt on Earth‚Äôs side.



February 7, 2025



GEMINI SOUTH TELESCOPE, CHILE



‚ÄúAt this point, [2024 YR4] was too faint for the Catalina telescopes,‚Äù says Catalina‚Äôs Wierzcho≈õ. ‚ÄúIn our opinion, this was a big deal.‚Äù&nbsp;



So Wierzcho≈õ and his colleagues put in a rare emergency request to commandeer the Gemini Observatory, an internationally funded and run facility featuring two large, eagle-eyed telescopes‚Äîone in Chile and one atop Hawaii‚Äôs Mauna Kea volcano. Their request was granted, and on February 7, they trained the Chile-based Gemini South telescope onto 2024 YR4.&nbsp;



This composite image  was captured using the Gemini Multi-Object Spectrograph (GMOS), by a team of astronomers. The hazy dot at the center is asteroid 2024 YR4.



The odds of Earth impact dropped ever so slightly, to 2.2% ‚Äîa minor, but still welcome, development.&nbsp;



Mid-February 2025



MAGDALENA RIDGE OBSERVATORY, NEW MEXICO



By this point, the roster of 2024 YR4 hunters also included the tiny team operating the Magdalena Ridge Observatory (MRO), which sits atop a tranquil mountain in New Mexico.



‚ÄúIt‚Äôs myself and my husband,‚Äù says Eileen Ryan, the MRO director. ‚ÄúWe‚Äôre the only two astronomers running the telescope. We have a daytime technician. It‚Äôs kind of a mom-and-pop organization.‚Äù&nbsp;



Still, the scope shouldn‚Äôt be underestimated. ‚ÄúWe can see maybe a cell-phone-size object that‚Äôs illuminated at geosynchronous orbit,‚Äù Ryan says, referring to objects 22,000 miles away. But its most impressive feature is its mobility. While other observatories have slowly swiveling telescopes, MRO‚Äôs scope can move like the wind. ‚ÄúWe can track the fastest objects,‚Äù she says, with a grin‚Äînoting that the telescope was built in part to watch missiles for the US Air Force. Its agility and long-distance vision explain why the Space Force is one of MRO‚Äôs major clients: It can be used to spy on satellites and spacecraft anywhere from low Earth orbit right out to the lunar regions. And that meant spying on the super-speedy, super-sneaky 2024 YR4 wasn‚Äôt a problem for MRO, whose own observations were vital in refining the asteroid‚Äôs impact odds.



Eileen Ryan, the director of the Magdalena Ridge Observatory in New Mexico.COURTESY PHOTO




Then, in mid-February, MRO and all ground-based observatories came up against an unsolvable problem: The full moon was out, shining so brightly that it blinded any telescope that dared point at the night sky. ‚ÄúDuring the full moon, the observatories couldn‚Äôt observe for a week or so,‚Äù says NEOCC‚Äôs Fenucci. To most of us, the moon is a beautiful silvery orb. But to astronomers, it‚Äôs a hostile actor. ‚ÄúWe abhor the moon,‚Äù says Denneau.&nbsp;



All any of them could do was wait. Those tracking 2024 YR4 vacillated between being animated and slightly trepidatious. The thought that the asteroid could still stand a decent chance of impacting Earth after it faded from view did weigh a little on their minds.&nbsp;



Nevertheless, Farnocchia maintained his characteristic sangfroid throughout. ‚ÄúI try to stress about the things I can control,‚Äù he says. ‚ÄúAll we can do is to explain what the situation is, and that we need new data to say more.‚Äù



February 18, 2025



CENTER FOR NEAR-EARTH OBJECT STUDIES, CALIFORNIA&nbsp;



As the full moon finally faded into a crescent of light, the world‚Äôs largest telescopes sprang back into action for one last shot at glory. ‚ÄúThe dark time came again,‚Äù says Hainaut, with a smile.



New observations finally began to trickle in, and Sentry, Aegis, and NEODyS readjusted their forecasts. It wasn‚Äôt great news: The odds of an Earth impact in 2032 jumped up to 3.1%, officially making 2024 YR4 the most dangerous asteroid ever discovered.



This declaration made headlines across the world‚Äîand certainly made the curious public sit up and wonder if they had yet another apocalyptic concern to fret about. But, as ever, the asteroid hunters held fast in their prediction that sooner or later‚Äîideally sooner‚Äîmore observations would cause those impact odds to drop.&nbsp;



‚ÄúWe kept at it,‚Äù says Ryan. But time was running short; they started to ‚Äúsearch for out-of-the-box ways to image this asteroid,‚Äù says Fenucci.&nbsp;



Planetary defense researchers soon realized that 2024 YR4 wasn‚Äôt too far away from NASA‚Äôs Lucy spacecraft, a planetary science mission making a series of flybys of various asteroids. If Lucy could be redirected to catch up to 2024 YR4 instead, it would give humanity its best look at the rock, allowing Sentry and company to confirm or dispel our worst fears.&nbsp;



Sadly, NASA ran the numbers, and it proved to be a nonstarter: 2024 YR4 was too speedy and too far for Lucy to pursue.&nbsp;



It was really starting to look as if JWST would be the last, best hope to track 2024 YR4.&nbsp;







A CHANGE OF FATE







February 19, 2025



VERY LARGE TELESCOPE, CHILE and MAGDALENA RIDGE OBSERVATORY, NEW MEXICO



Just one day after 2024 YR made history, the VLT, MRO, and others caught sight of it again, and Sentry, Aegis, and NEODyS voraciously consumed their new data.&nbsp;



The odds of an Earth impact suddenly dropped to 1.5%.&nbsp;



Astronomers didn‚Äôt really have time to react to the possibility that this was a good sign‚Äîthey just kept sending their observations onward.



February 20, 2025



SUBARU TELESCOPE, HAWAII



Yet another observatory had been itching to get into the game for weeks, but it wasn‚Äôt until February 20 that Tsuyoshi Terai, an astronomer at Japan‚Äôs Subaru Telescope, sitting atop Mauna Kea, finally caught 2024 YR4 shifting between the stars. He added his data to the stream.



And all of a sudden, the asteroid lost its lethal luster. The odds of its hitting Earth were now just 0.3%.&nbsp;



At this point, you might expect that all those tracking it would be in a single control room somewhere, eyes glued to their screens, watching the odds drop before bursting into cheers and rapturous applause. But no‚Äîthe astronomers who had spent so long observing this asteroid remained scattered across the globe. And instead of erupting into cheers, they exchanged modestly worded emails of congratulations‚Äîthe digital equivalent of a nod or a handshake.



In late February, data from Tsuyoshi Terai, an astronomer at Japan‚Äôs Subaru Telescope, which sits atop Mauna Kea, confirmed that 2024 YR4 was not so lethal after all.NAOJ




‚ÄúIt was a relief,‚Äù says Terai. ‚ÄúI was very pleased that our data contributed to put an end to the risk of 2024 YR4.‚Äù&nbsp;



February 24, 2025



INTERNATIONAL ASTEROID WARNING NETWORK



Just a few days later, and thanks to a litany of observations continuing to flood in, IAWN issued the all-clear. This once-ominous asteroid‚Äôs odds of inconveniencing our planet were at 0.004%‚Äîone in 25,000. Today, the odds of an Earth impact in 2032 are exactly zero.



‚ÄúIt was kinda fun while it lasted,‚Äù says Denneau.&nbsp;



Planetary defenders may have had a blast defending the world, but these astronomers still took the cosmic threat deeply seriously‚Äîand never once took their eyes off the prize. ‚ÄúIn my mind, the observers and orbit teams were the stars of this story,‚Äù says Fast, NASA‚Äôs acting planetary defense officer.



Farnocchia shrugs off the entire thing. ‚ÄúIt was the expected outcome,‚Äù he says. ‚ÄúWe just didn‚Äôt know when that would happen.‚Äù



Looking back on it now, though, some 2024 YR4 trackers are allowing themselves to dwell on just how close this asteroid came to being a major danger. ‚ÄúIt‚Äôs wild to watch it all play out,‚Äù says Denneau. ‚ÄúWe were weeks away from having to spin up some serious mitigation planning.‚Äù But there was no need to work out how the save the world. It turned out that 2024 YR4 was never a threat to begin with‚Äîit just took a while to check.&nbsp;



And these experiences in handling a dicey space rock will only serve to make the world a safer place to live. One day, an asteroid very much like 2024 YR4 will be seen heading straight for Earth. And those tasked with tracking it will be officially battle-tested, and better prepared than ever to do what needs to be done.







A POSTSCRIPT







March 27, 2025



JAMES WEBB SPACE TELESCOPE, DEEP SPACE, ONE MILLION MILES FROM EARTH



But the story of 2024 YR4 is not quite over‚Äîin fact, if this were a movie, it would have an after-credits scene.



After the Earth-impact odds fell off a cliff, JWST went ahead with its observations in March anyway. It found out that 2024 YR4 was 200 feet across‚Äîso large that a direct strike on a city would have proved horrifically lethal. Earth just didn‚Äôt have to worry about it anymore.&nbsp;



But the moon might. Thanks in part to JWST, astronomers calculated a 3.8% chance that 2024 YR4 will impact the lunar surface in 2032. Additional JWST observations in May bumped those odds up slightly, to 4.3%, and the orbit can no longer be refined until the asteroid‚Äôs next Earth flyby in 2028.&nbsp;



‚ÄúIt may hit the moon!‚Äù says Denneau. ‚ÄúEverybody‚Äôs still very excited about that.‚Äù&nbsp;



A lunar collision would give astronomers a wonderful opportunity not only to study the physics of an asteroid impact, but also to demonstrate to the public just how good they are at precisely predicting the future motions of potentially lethal space rocks. ‚ÄúIt‚Äôs a thing we can plan for without having to defend the Earth,‚Äù says Denneau.



If 2024 YR4 is truly going to smash into the moon, the impact‚Äîlikely on the side facing Earth‚Äîwould unleash an explosion equivalent to hundreds of nuclear bombs. An expansive crater would be carved out in the blink of an eye, and a shower of debris would erupt in all directions.&nbsp;



None of this supersonic wreckage would pose any danger to Earth, but it would look spectacular: You‚Äôd be able to see the bright flash of the impact from terra firma with the naked eye.



‚ÄúIf that does happen, it‚Äôll be amazing,‚Äù says Denneau. It will be a spectacular way to see the saga of 2024 YR4‚Äîonce a mere speck on his computer screen‚Äîcome to an explosive end, from a front-row seat.



Robin George Andrews is an award-winning science journalist and doctor of volcanoes based in London. He regularly writes about the Earth, space, and planetary sciences, and is the author of two critically acclaimed books: Super Volcanoes (2021) and How to Kill An Asteroid (2024).
‚Ä¢ Producing tangible business benefits from modern iPaaS solutions
  When a historic UK-based retailer set out to modernize its IT environment, it was wrestling with systems that had grown organically for more than 175 years. Prior digital transformation efforts had resulted in a patchwork of hundreds of integration flows spanning cloud, on-premises systems, and third-party vendors, all communicating across multiple protocols.¬†







The company needed a way to bridge the invisible seams stitching together decades of technology decisions. So, rather than layering on yet another patch, it opted for a more cohesive approach: an integration platform as a service (iPaaS) solution, i.e. a cloud-based ecosystem that enables smooth connections across applications and data sources. By going this route, the company reduced the total cost of ownership of its integration landscape by 40%.



The scenario illustrates the power of iPaaS in action. For many enterprises, iPaaS turns what was once a costly, complex undertaking into a streamlined, strategic advantage. According to Forrester research commissioned by SAP, businesses modernizing with iPaaS solutions can see a 345% return on investment over three years, with a payback period of less than six months.



Agile integration for an AI-first world



In 2025, the business need for flexible and friction-free integration has new urgency. When core business systems can‚Äôt communicate easily, the impacts ripple across the organization: Customer support teams can‚Äôt access real-time order statuses, finance teams struggle to consolidate data for monthly closes, and marketers lack reliable insights to personalize campaigns or effectively measure ROI.



A lack of high-quality data access is particularly problematic in the AI era, which depends on current, consistent, and connected data flows to fuel everything from predictive analytics to bespoke AI copilots. To unleash the full potential of AI, enterprises must first solve for any bottlenecks that prevent information from flowing freely across their systems. They must also ensure data pipelines are reliable and well-governed; when AI models are trained on inconsistent or outdated data, the insights they generate can be misleading or incomplete‚Äîwhich can undermine everything from customer recommendations to financial forecasting.



iPaaS platforms are often well-suited for accomplishing this across dynamic, distributed environments. Built as cloud-native, microservices-based integration hubs, modern iPaaS platforms can scale rapidly, adapt to changing workloads, and support hybrid architectures without adding complexity. They also help simplify the user experience for everyday business users via low-code functionalities that allow both technical and non-technical employees to build workflows with simple drag-and-drop or click-to-configure interfaces.



This self-service model has practical, real-world applications across business functions: For instance, customer service agents can connect support ticketing systems with real-time inventory or shipping data, finance departments can link payment processors to accounting software, and marketing teams can sync CRM data with campaign platforms to trigger personalized outreach‚Äîall without waiting for IT to come to the rescue.



Architectural foundations for fast, flexible integration



Several key architectural elements make the agility associated with iPaaS solutions possible:




API-first design that treats every connection as a reusable service



Event-driven capabilities that enable real-time responsiveness



Modular components that can be mixed and matched to address specific business scenarios




These principles are central to making the transition from ‚Äúspaghetti architecture‚Äù to ‚Äúintegration fabric‚Äù‚Äîa shift from brittle point-to-point connections to intelligent, policy-driven connectivity that spans multidimensional IT environments.



This approach means that when a company wants to add a new application, onboard a new partner, or create a new customer experience, they‚Äôre able to do so by tapping into existing integration assets rather than starting from scratch‚Äîwhich can lead to dramatically faster deployment cycles. It also helps enforce consistency and, in some cases, security and compliance across environments (role-based access controls and built-in monitoring capabilities, for example, can allow organizations to apply standards more uniformly).



Further, studies suggest that iPaaS solutions enable companies to unlock new revenue streams by integrating previously siloed data and processes. Forrester research found that organizations adopting iPaaS solutions stand to generate nearly $1 million in incremental profit over three years by creating new digital services, improving customer experiences, and automating revenue-generating processes that were previously manual.



Where iPaaS is headed: convergence and intelligence



All this momentum is perhaps one of the reasons why the global iPaaS market, valued at approximately $12.9 billion in 2024, is projected to reach more than $78 billion by 2032‚Äîwith growth rates exceeding 25% annually.



This trajectory is contingent on two ongoing trends: the convergence of integration capabilities into broader application development platforms, and the infusion of AI into the integration lifecycle.



Today, the boundaries between iPaaS, automation platforms, and AI development environments are blurring as vendors create unified solutions that can handle everything from basic data synchronization to complex business processes.&nbsp;



AI and machine learning capabilities are also being embedded directly into integration platforms. Soon, features like predictive maintenance of integration flow or intelligent routing of data based on current conditions are likely to become table stakes. Already, integration platforms are becoming smarter and more autonomous, capable of optimizing themselves and, in some cases, even initiating self-healing actions when problems arise.



At the same time, this shift is transforming how businesses think about integration as a dynamic enabler of AI strategy. In the near future, robust integration frameworks will be essential to operationalize AI at scale and feed these systems the rich, contextual data they need to deliver meaningful insights.



Building integration as competitive advantage



In addition to the retail modernization story detailed earlier, a few more real-world examples highlight the potential of iPaaS:




A chemicals manufacturer migrated 363 legacy interfaces to an iPaaS platform and now spins up new integrations 50% faster.



A North American bottling company reduced integration runtime costs by more than 50% while supporting 12 legal entities on a single cloud ERP instance through common APIs.



A global shipping-technology firm connected its CRM and third-party systems via cloud-based iPaaS solutions, enabling 100% touchless order fulfillment and a 95% cut in cost centers after a nine-month rollout in its first region.




Taken together, these examples make a compelling case for integration as strategy, not just infrastructure. They reflect a shift in mindset, where integration is democratized and embedded into how every team, not just IT, gets work done. Companies that treat integration as a core capability versus an IT afterthought are reaping tangible, enterprise-wide benefits, from faster go-to-market timelines and reduced operational costs to fully automated business processes.



As AI reshapes business processes and customer standards continue to climb, enterprises are realizing that integration architecture determines not only what they can build today, but how quickly they can adapt to whatever comes tomorrow.



This content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review‚Äôs editorial staff.



This content was researched, designed, and written entirely by human writers, editors, analysts, and illustrators. This includes the writing of surveys and collection of data for surveys. AI tools that may have been used were limited to secondary production processes that passed thorough human review.
‚Ä¢ The digital future of industrial and operational work
  Digital transformation has long been a boardroom buzzword‚Äîshorthand for ambitious, often abstract visions of modernization. But today, digital technologies are no longer simply concepts in glossy consultancy decks and on corporate campuses; they&#8217;re also being embedded directly into factory floors, logistics hubs, and other mission-critical, frontline environments.



This evolution is playing out across sectors: Field technicians on industrial sites are diagnosing machinery remotely with help from a slew of connected devices and data feeds, hospital teams are collaborating across geographies on complex patient care via telehealth technologies, and warehouse staff are relying on connected ecosystems to streamline inventory and fulfillment far faster than manual processes would allow.







Across all these scenarios, IT fundamentals‚Äîlike remote access, unified login systems, and interoperability across platforms‚Äîare being handled behind the scenes and consolidated into streamlined, user-friendly solutions. The way employees experience these tools, collectively known as the digital employee experience (DEX), can be a key component of achieving business outcomes: Deloitte finds that companies investing in frontline-focused digital tools see a 22‚ÄØ% boost in worker productivity, a doubling in customer satisfaction, and as much as a 25‚ÄØ% increase in profitability.



As digital tools become everyday fixtures in operational contexts, companies face both opportunities and hurdles‚Äîand the stakes are only rising as emerging technologies like AI become more sophisticated. The organizations best positioned for an AI-first future are crafting thoughtful strategies to ensure digital systems align with the realities of daily work‚Äîand placing people at the heart of the whole process.



IT meets OT in an AI world



Despite promising returns, many companies still face a last-mile challenge in delivering usable, effective tools to the frontline. The Deloitte study notes that less than one-quarter (just 23%) of frontline workers believe they have access to the technology they need to maximize productivity. There are several possible reasons for this disconnect, including the fact that operational digital transformation faces unique challenges compared to office-based digitization efforts.



For one, many companies are using legacy systems that don&#8217;t communicate easily across dispersed or edge environments. For example, the office IT department might use completely different software than what&#8217;s running the factory floor; a hospital&#8217;s patient records might be entirely separate from the systems monitoring medical equipment. When systems can&#8217;t talk to one another, troubleshooting issues becomes a time-consuming guessing game‚Äîone that often requires manual workarounds or clunky patches.



There&#8217;s also often a clash between tech&#8217;s typical &#8220;ship first, debug later&#8221; philosophy and the careful, safety-first approach that operational environments demand. A software glitch in a spreadsheet is annoying; a snafu in a power plant or at a chemical facility can be catastrophic.



Striking a careful balance between proactive innovation and prudent precaution will become ever more important, especially as AI usage becomes more common in high-stakes, tightly regulated environments. Companies will need to navigate a growing tension between the promise of smarter operations and the reality of implementing them safely at scale.



Humans at the heart of transformation efforts



With the buzz over AI and automation reaching fever pitch, it‚Äôs easy to overlook the single most impactful factor that makes transformation stick: the human element. The convergence of IT and OT goes hand in hand with the rise of digital employee experience. DEX encompasses everything from logging into systems and accessing applications to navigating networks and completing tasks across devices and locations. At its core, DEX is about ensuring technology empowers employees to work efficiently and without disruption‚Äîno matter where or how they work.



Companies investing in DEX technology are seeing measurable gains‚Äîfrom reduced help desk tickets and system downtime to harder-to-quantify benefits like higher employee satisfaction and retention. Frictionless digital workplaces, supported by real-time monitoring and automation capabilities, help organizations attend to IT issues before users experience disruptions or productivity levels dip.



There are real-world examples of seamless DEX in action: Swiss energy and infrastructure provider BKW, for instance, recently built a system that lets their IT team remotely assist employees experiencing technical difficulties across more than 140 subsidiaries. For employees, this means no more waiting for an in-person technician when their device freezes or software hiccups; IT can swoop in remotely and solve problems in minutes instead of hours.



The insurance company RLI faced a different but equally frustrating issue before switching to a centralized, remote IT support system: Technical issues like device lag or overheating were often left unreported, as employees didn‚Äôt want to disrupt their workflow or bother the IT team with seemingly minor complaints. Those small performance issues, however, could snowball over time, sometimes causing devices to fail completely. To get ahead of this phenomenon, RLI installed monitoring software to observe device performance in real time and catch issues proactively. Now, when a laptop gets too hot or starts slowing down, IT can address it right away‚Äîoften before the employee even knows there&#8217;s a problem.



Ultimately, the organizations making the biggest strides in DEX recognize that digital transformation is as much about experience as it is about infrastructure. When digital tools feel like helpful extensions of workers&#8217; expertise‚Äîrather than obstacles standing in the way of their workday‚Äîcompanies are in a better position to realize the full benefits of their investments.



Smart systems and smarter safeguards



Of course, as operational systems become more interconnected, security vulnerabilities multiply in turn. Consider this hypothetical: In a busy manufacturing plant, a piece of machinery suddenly breaks down. Instead of waiting hours for a technician to arrive on-site, a local operator deploys a mobile augmented reality device that projects step-by-step diagnostic instructions onto the machine. Following guidance from a remote specialist, the operator fixes the equipment and has production back on track in mere minutes.



This snappy and streamlined approach to diagnostics is undeniably efficient, but it opens up the factory floor to multiple external touchpoints: live video feeds streaming to remote experts, cloud databases containing sensitive repair procedures, and direct access to the machine&#8217;s diagnostic systems. Suddenly, a manufacturing plant that used to be an island is now part of an interconnected network.



Smart companies are getting practical about the challenges associated with this expanding threat surface. For instance, BKW has taken a structured approach to permissions: Subsidiary IT teams can only access their own company&#8217;s devices, outside contractors get temporary access for specific tasks, and employees can reach certain high-powered workstations when they need them.



B√ºhler, a global industrial equipment manufacturer, also uses centrally managed access controls to govern who can connect to which platforms, as well as when and under what conditions. By enforcing consistent policies from its headquarters, the company ensures all remote support activities are fully monitored and aligned with strict cybersecurity protocols, including compliance with ISO 27001 standards. The system allows B√ºhler‚Äôs extensive global technician network to provide real-time assistance without compromising system integrity.



The power of practical innovation



How do you help a technician troubleshoot equipment when the expert is 500 miles away? How do you catch IT problems before they shut down a production line? How do you keep operations secure without burying workers in passwords and protocols?



These are the kinds of practical questions that companies like B√ºhler, BKW, and RLI Insurance have focused on solving‚Äîand it&#8217;s part of why they&#8217;re succeeding where others struggle. These examples demonstrate a genuine shift in how successful companies think about technology and transformation. Instead of asking, &#8220;What&#8217;s the latest digital trend we should adopt?&#8221; they&#8217;re assessing, &#8220;What problems are our people actually trying to solve?&#8221;



The organizations pulling ahead to digitally transform frontline operations are the ones that have learned to make complex systems feel simple, intuitive, and secure to boot. Such a practical approach will only become more pressing as AI introduces new layers of complexity to operational work.



Ready to make work work better for your business? Learn how at TeamViewer.com.



This content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review‚Äôs editorial staff.



This content was researched, designed, and written entirely by human writers, editors, analysts, and illustrators. This includes the writing of surveys and collection of data for surveys. AI tools that may have been used were limited to secondary production processes that passed thorough human review.

üîí Cybersecurity & Privacy
‚Ä¢ Big Tech‚Äôs Mixed Response to U.S. Treasury Sanctions
  In May 2025, the U.S. government sanctioned a Chinese national for operating a cloud provider linked to the majority of virtual currency investment scam websites reported to the FBI. But a new report finds the accused continues to operate a slew of established accounts at American tech companies &#8212; including Facebook, Github, PayPal and Twitter/X.
On May 29, the U.S. Department of the Treasury¬†announced economic sanctions against Funnull Technology Inc., a Philippines-based company alleged to provide infrastructure for hundreds of thousands of websites involved in virtual currency investment scams known as &#8220;pig butchering.&#8221; In January 2025, KrebsOnSecurity detailed how Funnull was designed as a content delivery network that catered to foreign cybercriminals seeking to route their traffic through U.S.-based cloud providers.

The Treasury also sanctioned Funnull&#8217;s alleged operator, a 40-year-old Chinese national named Liu &#8220;Steve&#8221; Lizhi. The government says Funnull directly facilitated financial schemes resulting in more than $200 million in financial losses by Americans, and that the company&#8217;s operations were linked to the majority of pig butchering scams reported to the FBI.
It is generally illegal for U.S. companies or individuals to transact with people sanctioned by the Treasury. However, as Mr. Lizhi&#8217;s case makes clear, just because someone is sanctioned doesn&#8217;t necessarily mean big tech companies are going to suspend their online accounts.
The government says Lizhi was born November 13, 1984, and used the nicknames &#8220;XXL4&#8221; and &#8220;Nice Lizhi.&#8221; Nevertheless, Steve Liu&#8217;s 17-year-old account on LinkedIn (in the name &#8220;Liulizhi&#8221;) had hundreds of followers (Lizhi&#8217;s LinkedIn profile helpfully confirms his birthday) until quite recently: The account was deleted this morning, just hours after KrebsOnSecurity sought comment from LinkedIn.
Mr. Lizhi&#8217;s LinkedIn account was suspended sometime in the last 24 hours, after KrebsOnSecurity sought comment from LinkedIn.
In an emailed response, a LinkedIn spokesperson said the company&#8217;s &#8220;Prohibited countries policy&#8221; states that LinkedIn &#8220;does not sell, license, support or otherwise make available its Premium accounts or other paid products and services to individuals and companies sanctioned by the U.S. government.&#8221; LinkedIn declined to say whether the profile in question was a premium or free account.
Mr. Lizhi also maintains a working PayPal account under the name Liu Lizhi and username &#8220;@nicelizhi,&#8221; another nickname listed in the Treasury sanctions. A 15-year-old Twitter/X account named &#8220;Lizhi&#8221; that links to Mr. Lizhi&#8217;s personal domain remains active, although it has few followers and hasn&#8217;t posted in years.
These accounts and many others were flagged by the security firm Silent Push, which has been tracking Funnull&#8217;s operations for the past year and calling out U.S. cloud providers like Amazon and Microsoft for failing to more quickly sever ties with the company.
Liu Lizhi&#8217;s PayPal account.
In a report released today, Silent Push found Lizhi still operates numerous Facebook accounts and groups, including a private Facebook account under the name Liu Lizhi. Another Facebook account clearly connected to Lizhi is a tourism page for Ganzhou, China called &#8220;EnjoyGanzhou&#8221; that was named in the Treasury Department sanctions.
&#8220;This guy is the technical administrator for the infrastructure that is hosting a majority of scams targeting people in the United States, and hundreds of millions have been lost based on the websites he&#8217;s been hosting,&#8221; said Zach Edwards, senior threat researcher at Silent Push. &#8220;It&#8217;s crazy that the vast majority of big tech companies haven&#8217;t done anything to cut ties with this guy.&#8221;
The FBI says it received nearly 150,000 complaints last year involving digital assets and $9.3 billion in losses &#8212; a 66 percent increase from the previous year. Investment scams were the top crypto-related crimes reported, with $5.8 billion in losses.
In a statement, a Meta spokesperson said the company continuously takes steps to meet its legal obligations, but that sanctions laws are complex and varied. They explained that sanctions are often targeted in nature and don&#8217;t always prohibit people from having a presence on its platform. Nevertheless, Meta confirmed it had removed the account, unpublished Pages, and removed Groups and events associated with the user for violating its policies.
Attempts to reach Mr. Lizhi via his primary email addresses at Hotmail and Gmail bounced as undeliverable. Likewise, his 14-year-old YouTube channel appears to have been taken down recently.
However, anyone interested in viewing or using Mr. Lizhi&#8217;s 146 computer code repositories will have no problem finding GitHub accounts for him, including one registered under the NiceLizhi and XXL4 nicknames mentioned in the Treasury sanctions.
One of multiple GitHub profiles used by Liu &#8220;Steve&#8221; Lizhi, who uses the nickname XXL4 (a moniker listed in the Treasury sanctions for Mr. Lizhi).
Mr. Lizhi also operates a GitHub page for an open source e-commerce platform called NexaMerchant, which advertises itself as a payment gateway working with numerous American financial institutions. Interestingly, this profile&#8217;s &#8220;followers&#8221; page shows several other accounts that appear to be Mr. Lizhi&#8217;s. All of the account&#8217;s followers are tagged as &#8220;suspended,&#8221; even though that suspended message does not display when one visits those individual profiles.
In response to questions, GitHub said it has a process in place to identify when users and customers are Specially Designated Nationals or other denied or blocked parties, but that it locks those accounts instead of removing them. According to its policy, GitHub takes care that users and customers aren&#8217;t impacted beyond what is required by law.
All of the follower accounts for the XXL4 GitHub account appear to be Mr. Lizhi&#8217;s, and have been suspended by GitHub, but their code is still accessible.
&#8220;This includes keeping public repositories, including those for open source projects, available and accessible to support personal communications involving developers in sanctioned regions,&#8221; the policy states. &#8220;This also means GitHub will advocate for developers in sanctioned regions to enjoy greater access to the platform and full access to the global open source community.&#8221;
Edwards said it&#8217;s great that GitHub has a process for handling sanctioned accounts, but that the process doesn&#8217;t seem to communicate risk in a transparent way, noting that the only indicator on the locked accounts is the message, &#8220;This repository has been archived by the owner. It is not read-only.&#8221;
&#8220;It&#8217;s an odd message that doesn&#8217;t communicate, &#8216;This is a sanctioned entity, don&#8217;t fork this code or use it in a production environment&#8217;,&#8221; Edwards said.
Mark Rasch is a former federal cybercrime prosecutor who now serves as counsel for the New York City based security consulting firm Unit 221B. Rasch said when Treasury&#8217;s Office of Foreign Assets Control (OFAC) sanctions a person or entity, it then becomes illegal for businesses or organizations to transact with the sanctioned party.
Rasch said financial institutions have very mature systems for severing accounts tied to people who become subject to OFAC sanctions, but that tech companies may be far less proactive &#8212; particularly with free accounts.
&#8220;Banks have established ways of checking [U.S. government sanctions lists] for sanctioned entities, but tech companies don&#8217;t necessarily do a good job with that, especially for services that you can just click and sign up for,&#8221; Rasch said. &#8220;It&#8217;s potentially a risk and liability for the tech companies involved, but only to the extent OFAC is willing to enforce it.&#8221;
Liu Lizhi operates numerous Facebook accounts and groups, including this one for an entity specified in the OFAC sanctions: The &#8220;Enjoy Ganzhou&#8221; tourism page for Ganzhou, China. Image: Silent Push.
In July 2024, Funnull purchased the domain polyfill[.]io, the longtime home of a legitimate open source project that allowed websites to ensure that devices using legacy browsers could still render content in newer formats. After the Polyfill domain changed hands, at least 384,000 websites were caught in a supply-chain attack that redirected visitors to malicious sites. According to the Treasury, Funnull used the code to redirect people to scam websites and online gambling sites, some of which were linked to Chinese criminal money laundering operations.
The U.S. government says Funnull provides domain names for websites on its purchased IP addresses, using domain generation algorithms (DGAs) ‚Äî programs that generate large numbers of similar but unique names for websites ‚Äî and that it sells web design templates to cybercriminals.
&#8220;These services not only make it easier for cybercriminals to impersonate trusted brands when creating scam websites, but also allow them to quickly change to different domain names and IP addresses when legitimate providers attempt to take the websites down,&#8221; reads a Treasury statement.
Meanwhile, Funnull appears to be morphing nearly all aspects of its business in the wake of the sanctions, Edwards said.
&#8220;Whereas before they might have used 60 DGA domains to hide and bounce their traffic, we&#8217;re seeing far more now,&#8221; he said. &#8220;They&#8217;re trying to make their infrastructure harder to track and more complicated, so for now they&#8217;re not going away but more just changing what they&#8217;re doing. And a lot more organizations should be holding their feet to the fire.&#8221;
Update, 2:48 PM ET: Added response from Meta, which confirmed it has closed the accounts and groups connected to Mr. Lizhi.
Update, July 7, 6:56 p.m. ET: In a written statement, PayPal said it continually works to combat and prevent the illicit use of its services.
&#8220;We devote significant resources globally to financial crime compliance, and we proactively refer cases to and assist law enforcement officials around the world in their efforts to identify, investigate and stop illegal activity,&#8221; the statement reads.

üéì University AI
No updates.

üè¢ Corporate AI
‚Ä¢ AI Testing and Evaluation: Learnings from pharmaceuticals and medical devices
  Generative AI presents a unique challenge and opportunity to reexamine governance practices for the responsible development, deployment, and use of AI. To advance thinking in this space, Microsoft has tapped into the experience and knowledge of experts across domains‚Äîfrom genome editing to cybersecurity‚Äîto investigate the role of testing and evaluation as a governance tool. AI Testing and Evaluation: Learnings from Science and Industry, hosted by Microsoft Research‚Äôs Kathleen Sullivan, explores what the technology industry and policymakers can learn from these fields and how that might help shape the course of AI development.



In this episode, Daniel Carpenter, the Allie S. Freed Professor of Government and chair of the department of government at Harvard University, explains how the US Food and Drug Administration‚Äôs rigorous, multi-phase drug approval process serves as a gatekeeper that builds public trust and scientific credibility, while Timo Minssen, professor of law and founding director of the Center for Advanced Studies in Bioscience Innovation Law at the University of Copenhagen, explores the evolving regulatory landscape of medical devices with a focus on the challenges of balancing innovation with public safety. Later, Microsoft‚Äôs Chad Atalla, an applied scientist in responsible AI, discusses the sociotechnical nature of AI models and systems, their team‚Äôs work building an evaluation framework inspired by social science, and where AI researchers, developers, and policymakers might find inspiration from the approach to governance and testing in pharmaceuticals and medical devices.







Learn more:



Learning from other Domains to Advance AI Evaluation and Testing: The History and Evolution of Testing in Pharmaceutical RegulationCase study | January 2025&nbsp;



Learning from other Domains to Advance AI Evaluation and Testing: Medical Device Testing: Regulatory Requirements, Evolution and Lessons for AI GovernanceCase study | January 2025&nbsp;



Learning from other domains to advance AI evaluation and testing&nbsp;Microsoft Research Blog | June 2025‚ÄØ‚ÄØ&nbsp;



Evaluating Generative AI Systems is a Social Science Measurement Challenge&nbsp;Publication&nbsp;|&nbsp;November 2024‚ÄØ&nbsp;



STAC: Sociotechnical Alignment Center‚ÄØ



Responsible AI: Ethical policies and practices | Microsoft AI



AI and Microsoft Research‚ÄØ




	
		
			Subscribe to the Microsoft Research Podcast:		
		
							
					
						  
						Apple Podcasts
					
				
			
							
					
						
						Email
					
				
			
							
					
						
						Android
					
				
			
							
					
						
						Spotify
					
				
			
							
					
						
						RSS Feed
					
				
					
	




	
		
			
				
					

Transcript



[MUSIC]



KATHLEEN SULLIVAN: Welcome to AI Testing and Evaluation: Learnings from Science and Industry. I&#8217;m your host, Kathleen Sullivan.



As generative AI continues to advance, Microsoft has gathered a range of experts‚Äîfrom genome editing to cybersecurity‚Äîto share how their fields approach evaluation and risk assessment. Our goal is to learn from their successes and their stumbles to move the science and practice of AI testing forward. In this series, we&#8217;ll explore how these insights might help guide the future of AI development, deployment, and responsible use.



[MUSIC ENDS]



SULLIVAN: Today, I&#8217;m excited to welcome Dan Carpenter and Timo Minssen to the podcast to explore testing and risk assessment in the areas of pharmaceuticals and medical devices, respectively.



Dan Carpenter is chair of the Department of Government at Harvard University. His research spans the sphere of social and political science, from petitioning in democratic society to regulation and government organizations. His recent work includes the FDA Project, which examines pharmaceutical regulation in the United States.



Timo is a professor of law at the University of Copenhagen, where he is also director of the Center for Advanced Studies in Bioscience Innovation Law. He specializes in legal aspects of biomedical innovation, including intellectual property law and regulatory law. He&#8217;s exercised his expertise as an advisor to such organizations as the World Health Organization and the European Commission.



And after our conversations, we&#8217;ll talk to Microsoft&#8217;s Chad Atalla, an applied scientist in responsible AI, about how we should think about these insights in the context of AI.



Daniel, it&#8217;s a pleasure to welcome you to the podcast. I&#8217;m just so appreciative of you being here. Thanks for joining us today.



				
				
					



DANIEL CARPENTER:&nbsp;Thanks for having me.&nbsp;



SULLIVAN:&nbsp;Dan, before we dissect policy,&nbsp;let&#8217;s&nbsp;rewind the tape to your&nbsp;origin&nbsp;story. Can you take us to the moment that you first became fascinated with regulators rather than, say, politicians? Was there a spark that pulled you toward the FDA story?&nbsp;



CARPENTER:&nbsp;At one point during graduate school, I was studying a combination of American politics and political theory, and I did a summer interning at the Department of Housing and Urban Development. And I began to think, why don&#8217;t people study these administrators more and the rules they make, the, you know,&nbsp;inefficiencies, the efficiencies?&nbsp;Really more&nbsp;from,&nbsp;kind of,&nbsp;a descriptive standpoint, less from a normative standpoint.&nbsp;And I was reading a lot that summer about the Food and Drug Administration and some of the decisions it was making on AIDS drugs. That was&nbsp;a,&nbsp;sort of,&nbsp;a major, &#8230;



SULLIVAN: Right.&nbsp;



CARPENTER: &#8230; sort of, you know,&nbsp;moment in the news, in the global news as well as the national news during, I would say, what?&nbsp;The late&nbsp;‚Äô80s, early&nbsp;‚Äô90s? And&nbsp;so&nbsp;I began to&nbsp;look&nbsp;into&nbsp;that.



SULLIVAN:&nbsp;So now that we know what pulled you in,&nbsp;let‚Äôs&nbsp;zoom out for our listeners. Give us&nbsp;the&nbsp;whirlwind tour. I think most of us know pharma involves years of trials, but&nbsp;what‚Äôs&nbsp;the part we&nbsp;don‚Äôt&nbsp;know?



CARPENTER:&nbsp;So&nbsp;I think when most businesses develop a product, they all go through some phases of research and development and testing. And I think&nbsp;what&#8217;s&nbsp;different about the FDA is,&nbsp;sort of,&nbsp;two-&nbsp;or three-fold.



First, a lot of those tests are much more stringently specified and regulated by the government, and second, one of the reasons for that is that the FDA imposes not simply safety requirements upon drugs&nbsp;in particular but&nbsp;also efficacy requirements. The FDA wants you to prove not simply that&nbsp;it&#8217;s&nbsp;safe and non-toxic&nbsp;but also that&nbsp;it&#8217;s&nbsp;effective.&nbsp;And the final thing,&nbsp;I think, that&nbsp;makes the FDA different is that it stands as what I would call the&nbsp;‚Äúveto player‚Äù&nbsp;over R&amp;D [research and development] to the marketplace.&nbsp;The FDA&nbsp;basically has,&nbsp;sort of,&nbsp;this control over entry&nbsp;to&nbsp;the marketplace.



And¬†so¬†what that involves is usually first, a set of human trials where people who have no disease take it. And¬†you&#8217;re¬†only looking¬†for¬†toxicity generally. Then¬†there&#8217;s¬†a set of Phase 2 trials, where they look more at safety and a little bit at efficacy, and¬†you&#8217;re¬†now examining people who have the disease that the drug claims to treat. And¬†you&#8217;re¬†also basically comparing people who get the drug,¬†often¬†with those who do not.



And then finally, Phase 3 involves a much more direct and large-scale attack, if you will, or assessment of efficacy, and&nbsp;that&#8217;s&nbsp;where you get the sort of large randomized clinical trials that are&nbsp;very expensive&nbsp;for pharmaceutical companies, biomedical companies to launch, to execute, to analyze. And those are often the sort of core evidence base for the decisions that the FDA makes about&nbsp;whether or not&nbsp;to approve a new drug for marketing in the United States.



SULLIVAN:&nbsp;Are there&nbsp;differences in how that process has, you know, changed through other countries and&nbsp;maybe just&nbsp;how&nbsp;that&#8217;s&nbsp;evolved as&nbsp;you&#8217;ve&nbsp;seen it play out?&nbsp;



CARPENTER:&nbsp;Yeah, for a long time, I would say that the United States had&nbsp;probably the&nbsp;most&nbsp;stringent regime&nbsp;of regulation for biopharmaceutical products until,&nbsp;I would say,&nbsp;about the 1990s and early 2000s. It used to be the case that a number of other countries, especially in Europe but around the world, basically waited for the FDA to mandate tests on a drug and only after the drug was approved in the United States would they deem it approvable and marketable in their own countries. And then after the formation of the European Union and the creation of the European Medicines Agency, gradually the European Medicines Agency began to get a bit more stringent.&nbsp;&nbsp;



But, you know,&nbsp;over the long run,&nbsp;there&#8217;s&nbsp;been a&nbsp;lot of,&nbsp;sort&nbsp;of,&nbsp;heterogeneity, a lot of variation over time and space, in the way that the FDA has approached these problems. And&nbsp;I&#8217;d&nbsp;say in the last 20 years, it&#8217;s begun to partially deregulate, namely,&nbsp;you know,&nbsp;trying to find all sorts of mechanisms or pathways for really innovative&nbsp;drugs for deadly diseases without a lot of treatments to&nbsp;basically get&nbsp;through the process at lower cost.&nbsp;For many people,&nbsp;that has not been sufficient.&nbsp;They&#8217;re&nbsp;concerned about the cost of the system.&nbsp;Of course, then the agency also gets criticized by those&nbsp;who believe&nbsp;it&#8217;s&nbsp;too lax. It is&nbsp;potentially letting&nbsp;ineffective and unsafe therapies on the market.



SULLIVAN:&nbsp;In your view, when does the structured model genuinely safeguard patients and where do you think it&nbsp;maybe slows&nbsp;or&nbsp;limits&nbsp;innovation?



CARPENTER:&nbsp;So&nbsp;I think&nbsp;the worry&nbsp;is that if you approach pharmaceutical approval as a world where only things can go wrong,&nbsp;then&nbsp;you&#8217;re&nbsp;really at a risk of limiting innovation. And even if you end up letting a lot of things through, if by your regulations you end up basically slowing down the development process or making it very, very costly, then there&#8217;s just a whole bunch of drugs that either come to market too slowly or they come to market not at all because&nbsp;they just aren&#8217;t worth the kind of cost-benefit or, sort of, profit analysis of the firm.&nbsp;You know, so&nbsp;that&#8217;s&nbsp;been a concern.&nbsp;And I think&nbsp;it&#8217;s&nbsp;been one of the reasons that the Food and Drug Administration as well as other world regulators have begun to&nbsp;basically try&nbsp;to smooth the process and accelerate the process at the margins.



The other thing is that&nbsp;they&#8217;ve&nbsp;started to&nbsp;basically make&nbsp;approvals&nbsp;on the basis of&nbsp;what are called&nbsp;surrogate endpoints. So the idea is that a cancer drug, we really want to know whether that drug saves lives, but if we wait to see whose lives are saved or prolonged by that drug, we might miss the opportunity to make judgments on the basis of, well, are we detecting tumors in the bloodstream? Or can we measure the size of those tumors&nbsp;in, say, a&nbsp;solid cancer? And then the further question is, is the size of the tumor&nbsp;basically a&nbsp;really good&nbsp;correlate&nbsp;or predictor of whether people will die or&nbsp;not, right?&nbsp;Generally, the&nbsp;FDA tends to be less stringent when&nbsp;you&#8217;ve&nbsp;got, you know, a remarkably innovative new&nbsp;therapy&nbsp;and the disease being treated is one that just&nbsp;doesn&#8217;t&nbsp;have a lot of available treatments,&nbsp;right.



The one thing that people often think about when&nbsp;they&#8217;re&nbsp;thinking about pharmaceutical regulation is they often contrast,&nbsp;kind of,&nbsp;speed versus safety &#8230;



SULLIVAN:&nbsp;Right.&nbsp;&nbsp;



CARPENTER:&nbsp;&#8230; right. And&nbsp;that&#8217;s&nbsp;useful as a tradeoff,&nbsp;but I often try to remind people that&nbsp;it&#8217;s&nbsp;not simply&nbsp;about whether the drug gets out&nbsp;there&nbsp;and&nbsp;it&#8217;s&nbsp;unsafe. You know, you and I as patients and even doctors have&nbsp;a hard time&nbsp;knowing whether something works and whether it should be prescribed. And the evidence for knowing whether something works&nbsp;isn&#8217;t&nbsp;just, well,&nbsp;you&nbsp;know, Sally took&nbsp;it&nbsp;or Dan took it or Kathleen took it, and they&nbsp;seem to get&nbsp;better or they&nbsp;didn&#8217;t&nbsp;seem to get better.&nbsp;&nbsp;



The really rigorous evidence comes from randomized clinical trials.&nbsp;And I think&nbsp;it&#8217;s&nbsp;fair to say that if you didn&#8217;t&nbsp;have the FDA there as a veto player, you&nbsp;wouldn&#8217;t&nbsp;get as many randomized clinical&nbsp;trials&nbsp;and the evidence&nbsp;probably&nbsp;wouldn&#8217;t&nbsp;be as rigorous for whether these things work. And as I like to put it,&nbsp;basically there&#8217;s&nbsp;a whole ecology of expectations and beliefs around the biopharmaceutical industry in the United States and globally,&nbsp;and to some extent,&nbsp;it&#8217;s&nbsp;undergirded by&nbsp;all of&nbsp;these tests that happen.&nbsp;&nbsp;



SULLIVAN:&nbsp;Right.&nbsp;&nbsp;



CARPENTER:&nbsp;And in part, that means&nbsp;it&#8217;s&nbsp;undergirded by regulation. Would there still be a market without regulation? Yes. But it would be a market in which people had far less information in and confidence about the drugs that are being taken. And&nbsp;so&nbsp;I think&nbsp;it&#8217;s&nbsp;important to recognize that kind of confidence-boosting potential of, kind of, a scientific regulation base.&nbsp;



SULLIVAN:&nbsp;Actually, if we could&nbsp;double-click&nbsp;on that for a minute, I&#8217;d love to hear your perspective on, testing&nbsp;has been completed;&nbsp;there&#8217;s results.&nbsp;Can you walk us through how those results actually shape the next steps and decisions of a particular drug and just,&nbsp;like,&nbsp;how regulators actually think about using that data to influence really what happens next with it?



CARPENTER:&nbsp;Right.&nbsp;So&nbsp;it&#8217;s&nbsp;important to understand that every drug is approved for&nbsp;what&#8217;s called&nbsp;an indication. It can have a first primary&nbsp;indication, which is the main disease that it treats, and then others can be added as more evidence is shown. But a drug is not something that just kind of exists out there in the ether.&nbsp;It has to have the right form of administration.&nbsp;Maybe it&nbsp;should be injected.&nbsp;Maybe it&nbsp;should be ingested.&nbsp;Maybe it&nbsp;should&nbsp;be administered only at a clinic&nbsp;because it needs to be&nbsp;kind of administered&nbsp;in just the right way. As doctors will tell you, dosage is everything, right.&nbsp;&nbsp;



And&nbsp;so&nbsp;one of the reasons that you want those trials is not simply a, you know, yes or no answer about whether the drug works,&nbsp;right.&nbsp;It&#8217;s&nbsp;not simply if-then.&nbsp;It&#8217;s&nbsp;literally what&nbsp;goes into what you might call the dose response curve.&nbsp;You know, how much of this drug do we need to&nbsp;basically, you know,&nbsp;get the benefit? At what point does that fall off significantly that we can&nbsp;basically say, we can stop there? All that evidence comes from&nbsp;trials. And&nbsp;that&#8217;s&nbsp;the kind of evidence that is&nbsp;required&nbsp;on the basis of&nbsp;regulation.&nbsp;&nbsp;



Because&nbsp;it&#8217;s&nbsp;not simply a drug&nbsp;that&#8217;s&nbsp;approved.&nbsp;It&#8217;s&nbsp;a drug and a&nbsp;frequency&nbsp;of administration. It&#8217;s&nbsp;a&nbsp;method of administration.&nbsp;And&nbsp;so&nbsp;the drug&nbsp;isn&#8217;t&nbsp;just,&nbsp;there&#8217;s&nbsp;something to be taken off the shelf and popped into your mouth. I mean, sometimes&nbsp;that&#8217;s&nbsp;what happens, but even then,&nbsp;we want to know what the dosage is,&nbsp;right.&nbsp;We want to know what to look for in terms of side effects, things like that.



SULLIVAN:&nbsp;Going back to that point, I&nbsp;mean,&nbsp;it sounds like&nbsp;we&#8217;re&nbsp;making a lot of progress from a regulation perspective&nbsp;in, you know, sort of speed and getting things approved but doing it in a&nbsp;really balanced&nbsp;way. I mean, any other kind of closing thoughts on the tradeoffs there or where&nbsp;you&#8217;re&nbsp;seeing that going?



CARPENTER:¬†I think¬†you&#8217;re¬†going to see some move in the coming years‚Äîthere&#8217;s¬†already been some of it‚Äîto say, do we always need a¬†really large¬†Phase 3 clinical trial? And to what degree do we need the, like, you¬†know,¬†all the i&#8217;s dotted and the t&#8217;s crossed or a really,¬†really large¬†sample size?¬†And¬†I&#8217;m¬†open to innovation there.¬†I&#8217;m¬†also open to the idea that we consider, again, things like accelerated approvals or pathways for looking at¬†different kinds¬†of surrogate endpoints.¬†I do think, once we do that, then we also have to have some degree of follow-up.



SULLIVAN:&nbsp;So&nbsp;I know&nbsp;we&#8217;re&nbsp;getting&nbsp;close to&nbsp;out of time, but&nbsp;maybe just&nbsp;a quick rapid fire if&nbsp;you‚Äôre&nbsp;open to it. Biggest myth about clinical trials?



CARPENTER:&nbsp;Well, some people tend to think that the FDA performs them.&nbsp;You know,&nbsp;it&#8217;s&nbsp;companies that do it. And the only other thing I would say is the company that does a lot of the testing and even the innovating is not always the company that takes the drug to market, and it tells you something about how powerful regulation is in our system, in our world,&nbsp;that you often need a company that has dealt with the FDA quite a bit and knows all the regulations and knows how to dot the i&#8217;s and cross the t&#8217;s in order to get a drug across the finish line.



SULLIVAN:&nbsp;If you had a magic wand,&nbsp;what&#8217;s&nbsp;the one thing&nbsp;you&#8217;d&nbsp;change in regulation today?



CARPENTER:&nbsp;I would like people to think a little bit less about just speed versus safety and,&nbsp;again, more about this basic issue of confidence. I think&nbsp;it&#8217;s&nbsp;fundamental to everything that happens in markets but especially in biopharmaceuticals.



SULLIVAN:&nbsp;Such a great point.&nbsp;This has been really fun.&nbsp;Just thanks so much for being here today. We&#8217;re really excited to share your thoughts&nbsp;out to&nbsp;our listeners. Thanks.



[TRANSITION MUSIC]&nbsp;



CARPENTER:&nbsp;Likewise.&nbsp;



SULLIVAN:&nbsp;Now&nbsp;to&nbsp;the world of medical devices,&nbsp;I&#8217;m&nbsp;joined by Professor Timo&nbsp;Minssen. Professor Minssen, it&#8217;s&nbsp;great to have you here. Thank you for joining us today.&nbsp;



TIMO&nbsp;MINSSEN:&nbsp;Yeah, thank you very much,&nbsp;it&#8217;s&nbsp;a pleasure.



SULLIVAN:&nbsp;Before getting into the regulatory world of medical devices, tell our audience a bit about your personal journey or your origin story, as&nbsp;we&#8217;re&nbsp;asking our guests. How did you land in regulation, and what&#8217;s kept you hooked in this space?



MINSSEN:&nbsp;So&nbsp;I started out as a patent expert in the biomedical area, starting with my PhD thesis on patenting biologics in Europe and in the US.&nbsp;So&nbsp;during that time, I was mostly interested in patent and trade secret questions.&nbsp;But at the same time, I also developed and taught courses in regulatory law and held talks on regulating advanced medical therapy medicinal products.&nbsp;I&nbsp;then&nbsp;started to lead large research projects on legal challenges in a wide variety of health and life science innovation frontiers. I also started to focus increasingly on AI-enabled medical devices and software as a medical device, resulting in several academic articles in this area&nbsp;and also&nbsp;in the regulatory area and a book on the future of medical device regulation.&nbsp;&nbsp;



SULLIVAN:&nbsp;Yeah,&nbsp;what&#8217;s&nbsp;kept you hooked in&nbsp;the space?



MINSSEN:&nbsp;It&#8217;s&nbsp;just incredibly exciting,&nbsp;in particular right&nbsp;now with everything that is going on, you know, in the software arena, in the marriage between AI and medical devices. And this is really challenging not only societies but also regulators and authorities in Europe and in the US.



SULLIVAN:¬†Yeah,¬†it&#8217;s¬†a super exciting time to be in this space. You know, we talked to Daniel a little earlier and, you know, I think¬†similar to¬†pharmaceuticals, people have a general sense of what we mean when we say medical devices, but most listeners may¬†picture¬†like a stethoscope or a hip implant. The word &#8220;medical device&#8221;¬†reaches¬†much wider. Can you give us a quick, kind of, range from perhaps¬†very simple¬†to even, I don&#8217;t know, sci-fi and then your 90-second tour of how risk assessment works and why a framework is essential?



MINSSEN:&nbsp;Let me start out by saying that&nbsp;the WHO [World Health Organization] estimates that today there are approximately 2 million different kinds of medical devices on the world market, and as of the FDA&#8217;s latest update that I&#8217;m aware of, the FDA has authorized more than 1,000 AI-, machine learning-enabled medical devices, and that number is rising rapidly.



So in that context, I think it is important to understand that medical devices can be any instrument, apparatus, implement, machine, appliance, implant, reagent for in vitro use, software, material, or other similar or related articles that are&nbsp;intended&nbsp;by the manufacturer to be used alone or in combination for a medical purpose. And the spectrum of what constitutes a medical device can&nbsp;thus&nbsp;range from very simple devices such as tongue depressors, contact lenses, and thermometers to more complex devices such as blood pressure monitors, insulin pumps, MRI machines, implantable pacemakers, and even software as a medical device or AI-enabled monitors or drug device combinations, as well.



So&nbsp;talking about regulation,&nbsp;I think&nbsp;it&nbsp;is also&nbsp;very important&nbsp;to stress that medical devices are used in many diverse situations by&nbsp;very different&nbsp;stakeholders. And testing&nbsp;has to&nbsp;take this variety into consideration, and it is intrinsically tied to regulatory requirements across various&nbsp;jurisdictions.



During the pre-market phase, medical testing&nbsp;establishes&nbsp;baseline safety and effectiveness metrics through bench testing, performance standards, and clinical studies. And post-market testing ensures that real-world data informs ongoing compliance and safety improvements. So testing is indispensable in translating technological innovation into safe and effective medical devices. And while&nbsp;particular details&nbsp;of pre-market and post-market review procedures may slightly differ among countries, most developed&nbsp;jurisdictions regulate medical devices similarly to the US or European models.‚ÄØ



So&nbsp;most&nbsp;jurisdictions&nbsp;with medical device regulation classify devices based on their risk profile, intended use, indications for use, technological characteristics,&nbsp;and the regulatory controls necessary to provide a reasonable assurance of safety and effectiveness.



SULLIVAN:&nbsp;So medical devices face a pretty prescriptive multi-level testing path before they hit the market. From your vantage point, what are some of the downsides of that system and when does it make the most sense?



MINSSEN:&nbsp;One primary drawback is, of course, the lengthy and expensive approval process. High-risk devices, for example, often undergo years of clinical trials,&nbsp;which can cost millions of dollars, and this can create a significant barrier for startups and small companies with limited resources.&nbsp;And even for moderate-risk devices, the regulatory burden can slow product development and time to the market.



And the approach can also limit flexibility. Prescriptive requirements may not accommodate emerging innovations like digital therapeutics or AI-based diagnostics in&nbsp;a feasible&nbsp;way. And in such cases, the framework can unintentionally [stiffen]&nbsp;innovation by discouraging creative solutions or iterative improvements, which as matter of fact can also&nbsp;put&nbsp;patients&nbsp;at risk when you&nbsp;don&#8217;t&nbsp;use&nbsp;new technologies and AI.&nbsp;And&nbsp;additionally, the same level of scrutiny may be applied to low-risk devices, where&nbsp;the extensive testing and documentation may also be disproportionate to the actual patient risk.



However, the prescriptive model is highly&nbsp;appropriate where&nbsp;we have high testing standards for high-risk medical devices, in my view, particularly those that are life-sustaining, implanted, or involve new materials or mechanisms.



I also wanted to say that I think that these higher compliance thresholds can be OK and necessary if you have a system where authorities and stakeholders also have the capacity and funding to enforce, monitor, and achieve compliance with such rules in a feasible, time-effective, and straightforward manner. And this, of course, requires resources, novel solutions,&nbsp;and investments.



SULLIVAN:&nbsp;A range of tests are undertaken across the life cycle of medical devices.&nbsp;How do these testing requirements vary across&nbsp;different stages&nbsp;of development and across various applications?



MINSSEN:&nbsp;Yes,&nbsp;that&#8217;s&nbsp;a good question.&nbsp;So&nbsp;I think first it&nbsp;is important to realize that testing is conducted by various entities, including manufacturers, independent third-party laboratories, and regulatory agencies. And it occurs throughout the device&nbsp;life&nbsp;cycle, beginning with iterative testing during the research and development stage, advancing to pre-market evaluations, and continuing into post-market monitoring. And the outcomes of&nbsp;these tests directly&nbsp;impact&nbsp;regulatory approvals, market access, and device design refinements, as well.&nbsp;So&nbsp;the testing results are typically shared with regulatory authorities and in some cases with healthcare providers and the broader public to enhance transparency and trust.



So&nbsp;if you talk about the&nbsp;different phases&nbsp;that play a role here ‚Ä¶ so&nbsp;let&#8217;s&nbsp;turn to the pre-market phase, where manufacturers must&nbsp;demonstrate&nbsp;that the device is conformed to safety and performance benchmarks defined by regulatory authorities. Pre-market evaluations include functional bench testing, biocompatibility, for example, assessments and software validation, all of which are integral components of a manufacturer&#8217;s submission.&nbsp;



But, yes, but, testing also, and we touched already up on that, extends into the post-market phase, where it continues to ensure device safety and efficacy, and post-market surveillance relies on testing to&nbsp;monitor real-world performance and&nbsp;identify&nbsp;emerging risks on the post-market phase. By integrating real-world evidence into ongoing assessments, manufacturers can address unforeseen issues, update devices as needed, and&nbsp;maintain compliance with evolving regulatory expectations. And&nbsp;I think this&nbsp;is particularly important in this new generation of medical devices that are AI-enabled or machine-learning enabled.



I think we have to understand that in this AI-enabled medical devices field, you know, the devices and the algorithms that are working with&nbsp;them, they&nbsp;can improve in the lifetime of a product.&nbsp;So actually, not&nbsp;only you could assess them and make sure that they&nbsp;maintain&nbsp;safe,&nbsp;you&nbsp;could also sometimes lower the risk category by finding evidence that these devices are&nbsp;actually becoming&nbsp;more precise and safer.&nbsp;So&nbsp;it can both, you know, heighten the risk&nbsp;category&nbsp;or lower the risk category, and&nbsp;that&#8217;s&nbsp;why&nbsp;this continuous testing is so important.



SULLIVAN:&nbsp;Given what you just said, how should regulators handle a device whose algorithm keeps updating itself after approval?



MINSSEN:&nbsp;Well, it&nbsp;has to&nbsp;be an iterative process that is&nbsp;feasible&nbsp;and straightforward and that is based on a very efficient, both time efficient and performance efficient, communication between the regulatory authorities and the medical device developers, right. We need to have&nbsp;the sensors&nbsp;in place that spot potential changes, and we need to have&nbsp;the mechanisms&nbsp;in place that allow us to quickly react to these changes both regulatory wise&nbsp;and also&nbsp;in&nbsp;the&nbsp;technological way.‚ÄØ



So&nbsp;I think communication&nbsp;is important,&nbsp;and we need to have&nbsp;the pathways&nbsp;and&nbsp;the feedback&nbsp;loops in the regulation that quickly allow us to&nbsp;monitor&nbsp;these self-learning algorithms and devices.



SULLIVAN:&nbsp;It sounds like&nbsp;it&#8217;s&nbsp;just ‚Ä¶&nbsp;there&#8217;s&nbsp;such a delicate balance between advancing technology and really ensuring public safety. You know, if we clamp down too hard, we stifle that innovation. You already touched upon this a bit. But if&nbsp;we&#8217;re&nbsp;too lax, we risk unintended consequences. And&nbsp;I&#8217;d&nbsp;just love to hear how you think the field is balancing that and any learnings you can share.



MINSSEN:&nbsp;So&nbsp;this is&nbsp;very true, and&nbsp;you just touched upon a very central question also in our research and our writing. And this is also the&nbsp;reason why&nbsp;medical device regulation is so fascinating and continues to evolve in response to rapid advancements in technologies, particularly dual technologies&nbsp;regarding&nbsp;digital health, artificial intelligence, for example, and personalized medicine.



And finding the balance is tricky because also [a] related major future challenge relates to the increasing regulatory jungle and the complex interplay between evolving regulatory landscapes that regulate AI more generally.



We really need to make sure that the regulatory authorities that deal with this, that need to find the right balance to promote innovation and mitigate and prevent risks, need to have the&nbsp;capacity&nbsp;to do this.&nbsp;So&nbsp;this requires investments, and it also requires new ways to regulate this technology more flexibly, for example through regulatory sandboxes and so on.



SULLIVAN:&nbsp;Could you just expand upon that a bit and double-click on what it is&nbsp;you&#8217;re&nbsp;seeing there? What excites you about&nbsp;what&#8217;s&nbsp;happening in that space?



MINSSEN:&nbsp;Yes, well, the research of my group at the Center for Advanced Studies in Bioscience Innovation Law is&nbsp;very broad. I mean, we are looking into gene editing technologies. We are looking into new biologics. We are looking into medical&nbsp;devices,&nbsp;as well, obviously, but also other technologies&nbsp;in advanced medical computing.



And what we see across the line here is that there is an increasing demand for having more adaptive and flexible regulatory frameworks in these&nbsp;new technologies,&nbsp;in particular when&nbsp;they have new uses, regulations that are focusing more on the product rather than the process. And I have recently&nbsp;written&nbsp;a report, for example,&nbsp;for&nbsp;emerging biotechnologies and&nbsp;bio-solutions&nbsp;for the EU commission. And even in that area, regulatory sandboxes are increasingly important, increasingly considered.



So&nbsp;this idea of regulatory sandboxes has been developing originally in the financial sector, and it is now penetrating into&nbsp;other sectors, including synthetic biology, emerging biotechnologies, gene editing, AI, quantum technology, as&nbsp;well. This is&nbsp;basically creating&nbsp;an environment where actors can test&nbsp;new ideas&nbsp;in close collaboration and under the oversight of regulatory authorities.



But¬†to implement¬†this in the AI sector now also leads us to¬†a¬†lot of questions and challenges. For example, you need to have the¬†capacities¬†of authorities that are governing and¬†monitoring¬†and deciding¬†on these regulatory sandboxes. There are issues relating to competition law, for example, which¬†you¬†call antitrust law in the US, because the question is, who can enter the sandbox and how may they compete after they exit the sandbox? And there are many questions relating to, how¬†should we¬†work with these sandboxes and how¬†should we¬†implement these sandboxes?



[TRANSITION MUSIC]&nbsp;



SULLIVAN:&nbsp;Well, Timo, it has just been such a pleasure to speak with you today.



MINSSEN:&nbsp;Yes, thank you very much.&nbsp;



And now&nbsp;I&#8217;m&nbsp;happy to introduce Chad Atalla.



Chad&nbsp;is&nbsp;senior applied scientist&nbsp;in&nbsp;Microsoft Research&nbsp;New York City&#8217;s&nbsp;Sociotechnical Alignment Center, where they contribute to foundational responsible AI research and practical responsible AI solutions for teams across Microsoft.



Chad, welcome!



CHAD ATALLA:&nbsp;Thank you.



SULLIVAN:&nbsp;So&nbsp;we&#8217;ll&nbsp;kick off with a couple questions just to dive right in.&nbsp;So&nbsp;tell me a little bit more about the&nbsp;Sociotechnical Alignment Center,&nbsp;or&nbsp;STAC? I know it was founded in&nbsp;2022.&nbsp;I&#8217;d&nbsp;love to just learn a little bit more about what the group does, how&nbsp;you&#8217;re&nbsp;thinking about evaluating AI, and&nbsp;maybe just&nbsp;give us a sense of some of the projects&nbsp;you&#8217;re&nbsp;working on.



ATALLA:&nbsp;Yeah, absolutely. The name is quite a mouthful.



SULLIVAN:&nbsp;It is!&nbsp;[LAUGHS]&nbsp;



ATALLA:&nbsp;So&nbsp;let&#8217;s&nbsp;start by breaking that down and seeing what that means.



SULLIVAN:&nbsp;Great.



ATALLA: So modern AI systems are sociotechnical systems, meaning that the social and technical aspects are deeply intertwined. And&nbsp;we&#8217;re interested in aligning the behaviors of these sociotechnical&nbsp;systems with some values.&nbsp;Those could be societal values;&nbsp;they could be regulatory values, organizational values, etc. And to make this alignment happen, we need the ability to evaluate the systems.



So&nbsp;my team is broadly working on an evaluation framework that acknowledges the sociotechnical nature of the technology and the often-abstract nature of the concepts&nbsp;we&#8217;re&nbsp;actually interested&nbsp;in evaluating. As you noted,&nbsp;it&#8217;s&nbsp;an applied science team, so we split our time between some fundamental research and time to bridge the work into real products across the company. And I also want to note that to power this sort of work, we have an interdisciplinary team drawing upon the social sciences, linguistics, statistics, and,&nbsp;of course, computer science.



SULLIVAN:&nbsp;Well,&nbsp;I&#8217;m&nbsp;eager to get into our takeaways from the conversation with&nbsp;both Daniel&nbsp;and Timo. But&nbsp;maybe just&nbsp;to double-click on this for a minute, can you talk a bit about some of the overarching goals of the AI evaluations that you noted?&nbsp;



ATALLA:&nbsp;So&nbsp;evaluation is really the act of making valuative judgments based on some evidence, and in the case of AI evaluation, that evidence might be from tests or measurements, right.&nbsp;And the goal of why&nbsp;we&#8217;re doing this in the first place is to make decisions and claims most often.



So&nbsp;perhaps I&nbsp;am going to make a claim about a model that&nbsp;I&#8217;m&nbsp;producing, and I want to say that&nbsp;it&#8217;s&nbsp;better than this other model. Or we are asking whether a certain product is safe to ship.&nbsp;All of these decisions need to be informed by good evaluation and therefore good measurement or testing.&nbsp;And&nbsp;I&#8217;ll&nbsp;also note that in&nbsp;the regulatory conversation, risk&nbsp;is often what we want to evaluate. So that is a goal in and of itself. And&nbsp;I&#8217;ll&nbsp;touch more on that later.



SULLIVAN:&nbsp;I read a recent&nbsp;paper that you had put out with some of our colleagues from Microsoft Research, from the University of Michigan, and Stanford, and you were arguing that evaluating generative AI is&nbsp;the&nbsp;social-science measurement challenge.&nbsp;Maybe for&nbsp;those who&nbsp;haven&#8217;t&nbsp;read the paper, what does this mean? And can you tell us a little bit more about what motivated you and your coauthors?&nbsp;



ATALLA:&nbsp;So the measurement tasks involved in evaluating generative AI systems are often abstract and contested. So that means they cannot be directly measured and must instead [be] indirectly measured via other observable phenomena. So this is very different than the older machine learning paradigm, where, let&#8217;s say, for example, I had a system that took a picture of a traffic light and told you whether it was green, yellow, or red at a given time.&nbsp;



If we wanted to evaluate that system, the task is much simpler. But with the modern generative AI systems that are also general purpose, they have open-ended output, and language in a whole chat or multiple paragraphs being outputted can have a lot of different properties. And as I noted, these are general-purpose systems, so we don&#8217;t know exactly what task they&#8217;re supposed to be carrying out.



So¬†then the question becomes, if I want to make some decision or claim‚Äîmaybe I¬†want to make a claim that this system has human-level reasoning capabilities‚Äîwell, what does that mean? Do I have the same impression of what that means as you do? And how do we know whether the downstream, you know, measurements and tests that¬†I&#8217;m¬†conducting¬†actually will¬†support my notion of what it means to have human-level reasoning,¬†right?¬†Difficult questions. But luckily, social scientists have been dealing with these exact sorts of challenges for multiple decades in fields like education, political science, and psychometrics. So¬†we&#8217;re¬†really¬†attempting¬†to avoid reinventing the wheel here and trying to learn from their past methodologies.



And so the rest of the paper goes on to delve into&nbsp;a four-level framework, a measurement framework, that&#8217;s grounded in the measurement theory from the quantitative social sciences that takes us all the way from these abstract and contested concepts through processes to get much clearer and eventually reach reliable and valid measurements that can power our evaluations.



SULLIVAN:&nbsp;I love that. I mean,&nbsp;that&#8217;s&nbsp;the whole point of this podcast,&nbsp;too,&nbsp;right.&nbsp;Is&nbsp;to really&nbsp;build&nbsp;on those other learnings and frameworks that&nbsp;we&#8217;re&nbsp;taking from industries that have been thinking about this for much longer.&nbsp;Maybe from&nbsp;your vantage point, what are some of the biggest day-to-day hurdles in building solid AI evaluations&nbsp;and,&nbsp;I&nbsp;don&#8217;t&nbsp;know, do we need more shared standards? Are there&nbsp;bespoke methods? Are those&nbsp;the way to go? I would love&nbsp;to just&nbsp;hear your thoughts on that.



ATALLA:&nbsp;So&nbsp;let&#8217;s&nbsp;talk about some of those practical challenges. And I want to briefly go back to what I mentioned about risk before, all right.&nbsp;Oftentimes,&nbsp;some of the regulatory environment&nbsp;is requiring practitioners to measure the&nbsp;risk&nbsp;involved in deploying one of their models or AI systems. Now, risk is importantly a&nbsp;concept that includes both event and impact,&nbsp;right.&nbsp;So&nbsp;there&#8217;s&nbsp;the probability of some event occurring. For the case of AI evaluation,&nbsp;perhaps this&nbsp;is us seeing a certain AI behavior&nbsp;exhibited. Then there&#8217;s also the severity of the&nbsp;impacts,&nbsp;and this is a complex chain of effects in the real world that&nbsp;happen&nbsp;to people, organizations, systems, etc., and&nbsp;it&#8217;s&nbsp;a lot more challenging to&nbsp;observe&nbsp;the impacts,&nbsp;right.



So&nbsp;if we&#8217;re saying that we need to measure risk, we have to measure both the event and the&nbsp;impacts. But realistically, right now, the field is not doing&nbsp;a very good&nbsp;job of&nbsp;actually measuring&nbsp;the impacts. This requires vastly different techniques and methodologies where if I just wanted to measure something about the event itself, I can, you know, do that in a technical sandbox&nbsp;environment&nbsp;and&nbsp;perhaps have&nbsp;some automated methods to detect whether a certain AI behavior is being&nbsp;exhibited. But if I want to measure the impacts? Now,&nbsp;we&#8217;re&nbsp;in the realm of needing to have real people involved, and&nbsp;perhaps a&nbsp;longitudinal study where you have interviews, questionnaires, and more qualitative evidence-gathering techniques to&nbsp;truly understand&nbsp;the long-term impacts. So&nbsp;that&#8217;s&nbsp;a significant challenge.



Another is that, you know,&nbsp;let&#8217;s&nbsp;say we forget about the impacts for&nbsp;now&nbsp;and we focus on the event side of things. Still, we need datasets, we need&nbsp;annotations,&nbsp;and we need&nbsp;metrics to make this whole thing work. When I say we need datasets, if I want to test whether my system has good mathematical reasoning, what questions should I ask? What are my set of inputs that are relevant? And then when I get&nbsp;the&nbsp;response from the system, how do I annotate them? How do I know if it was a good response that&nbsp;did demonstrate mathematical reasoning or if it was a mediocre response? And then once I have an annotation of&nbsp;all of these outputs from the AI system, how do I aggregate those all up into a single informative number?



SULLIVAN:&nbsp;Earlier in this episode, we heard Daniel and&nbsp;Timo walk&nbsp;through the regulatory frameworks in pharma and medical devices.&nbsp;I&#8217;d&nbsp;be curious what pieces of those mature systems are already showing up or at least may&nbsp;be bubbling up in AI governance.



ATALLA:&nbsp;Great question. You know, Timo was talking about the pre-market and post-market testing difference. Of course, this is similarly important in the AI evaluation space. But again, these have different methodologies and serve different purposes.



So&nbsp;within the pre-deployment phase, we&nbsp;don&#8217;t&nbsp;have evidence of how people are going to use the system. And when we have these general-purpose AI systems,&nbsp;to understand what the risks are, we really need to have a sense of what might happen and how they might be used.&nbsp;So&nbsp;there are&nbsp;significant challenges there where I think we can learn from other fields and how they do pre-market testing. And the difference in that pre- versus post-market testing also ties to testing at&nbsp;different stages&nbsp;in the life cycle.



For AI systems, we already see some regulations saying you need to start with the base model and do some evaluation of the base model, some basic attributes, some core attributes,&nbsp;of that base model before you start putting it into any real products. But once we have a product in mind, we have a user base in mind, we have a specific task‚Äîlike maybe we&#8217;re going to integrate this model into Outlook and it&#8217;s going to help you write&nbsp;emails‚Äînow we suddenly have a much crisper picture of how the system will interact with the world around it. And again, at that stage, we need to think about another round of evaluation.



Another part that jumped out to me in what they were saying about pharmaceuticals is that sometimes approvals can be based on surrogate endpoints.&nbsp;So&nbsp;this is like&nbsp;we&#8217;re&nbsp;choosing some&nbsp;heuristic.&nbsp;Instead of measuring the long-term impact, which is what we&nbsp;actually care&nbsp;about,&nbsp;perhaps we&nbsp;have a proxy that we&nbsp;feel like&nbsp;is a good enough indicator of what that long-term impact might look like.&nbsp;&nbsp;



This is occurring in the AI evaluation space right now and is often perhaps even the default here since&nbsp;we&#8217;re not seeing that many studies of the long-term impact itself. We are seeing, instead, folks constructing these heuristics or proxies and saying if I see this behavior happen,&nbsp;I&#8217;m&nbsp;going to&nbsp;assume&nbsp;that it&nbsp;indicates&nbsp;this sort of impact will happen downstream. And&nbsp;that&#8217;s&nbsp;great.&nbsp;It&#8217;s&nbsp;one of the techniques that was used to speed up and reduce the barrier to innovation in&nbsp;the other&nbsp;fields. And I think&nbsp;it&#8217;s&nbsp;great that we are applying that in the AI evaluation space. But&nbsp;special care&nbsp;is,&nbsp;of course, needed to ensure that those heuristics and proxies you&#8217;re&nbsp;using are reasonable indicators of the greater outcome&nbsp;you&#8217;re&nbsp;looking for.



SULLIVAN:&nbsp;What are some of the promising ideas from&nbsp;maybe pharma&nbsp;or med device regulation that maybe haven&#8217;t&nbsp;made it to AI testing yet and&nbsp;maybe should? And where would you urge technologists, policymakers,&nbsp;and researchers to focus their energy next?



ATALLA:&nbsp;Well, one of the key things that jumped out to me in the discussion about pharmaceuticals was driving home the emphasis that there&nbsp;is&nbsp;a&nbsp;holistic&nbsp;focus on safety&nbsp;and&nbsp;efficacy. These go hand in hand&nbsp;and decisions must be made while considering both pieces of the picture. I would like to see that further emphasized in the AI evaluation space.



Often,&nbsp;we&nbsp;are seeing&nbsp;evaluations of risk being separated from evaluations of&nbsp;performance or quality&nbsp;or efficacy, but these two pieces of the puzzle really are not enough for us to make informed decisions independently.&nbsp;And that ties back into my desire to really also see us measuring the impacts.



So&nbsp;we see Phase 3 trials as something that occurs in the medical devices and pharmaceuticals field. That&#8217;s not something that we are doing an equivalent of in the AI evaluation space at this time.&nbsp;These are really&nbsp;cost intensive. They can last years and really involve careful monitoring of that holistic picture of safety and efficacy. And realistically, we are not going to be able to put that on the critical path to getting specific individual AI models or AI systems vetted before they&nbsp;go out&nbsp;into the world. However, I would love to see a world in which this sort of work is prioritized&nbsp;and funded or&nbsp;required. Think of how, with&nbsp;social media, it took quite a long time for us to understand that there are some long-term negative impacts on mental health, and we have the opportunity now, while the AI wave is still building,&nbsp;to start prioritizing and funding this sort of work. Let it run in the background and as soon as possible develop a good understanding of the subtle, long-term effects.



More broadly, I would love to see us focus on reliability and validity of the evaluations&nbsp;we&#8217;re&nbsp;conducting because trust in these decisions and claims is important. If we&nbsp;don&#8217;t&nbsp;focus on building reliable, valid, and trustworthy evaluations,&nbsp;we&#8217;re&nbsp;just going to continue to be flooded by a bunch of competing, conflicting, and&nbsp;largely meaningless&nbsp;AI evaluations.



SULLIVAN:&nbsp;In a number of the discussions we&#8217;ve had on this podcast, we talked about how it&#8217;s not just one entity that really needs to ensure safety across the board,&nbsp;and I‚Äôd&nbsp;just love to hear from you how you think about some of those ecosystem collaborations, and you know, from across &#8230; where we think about ourselves as more of a platform company or places that these AI models are being deployed more at the application level. Tell me a little bit about how you think about,&nbsp;sort&nbsp;of, stakeholders in that mix and where responsibility lies across the board.



ATALLA:&nbsp;It&#8217;s&nbsp;interesting. In this age of general-purpose AI technologies,&nbsp;we&#8217;re&nbsp;often&nbsp;seeing&nbsp;one company or organization&nbsp;being responsible for&nbsp;building the foundational model. And then many, many other people will take that model and build it into specific products that are designed for specific tasks and contexts.



Of course,&nbsp;in that, we already see that there is&nbsp;a responsibility&nbsp;of the owners of that foundational model to do some testing of the central model before they distribute it broadly. And then again, there is responsibility of all of the downstream individuals digesting that and turning it into products to consider the specific contexts that they are deploying into and how that may affect the risks we&#8217;re concerned with or the types of quality and safety and performance we need to evaluate.



Again, because that field of risks we may be concerned with is so broad, some of them also require an immense amount of&nbsp;expertise.&nbsp;Let&#8217;s&nbsp;think about whether AI systems can enable people to create dangerous chemicals or dangerous weapons at home. It&#8217;s not that every AI practitioner is going to have the knowledge to evaluate this, so in some of those cases, we really need third-party experts, people who are experts in chemistry, biology, etc., to come in and evaluate certain systems and models for those specific risks,&nbsp;as well.



So&nbsp;I think there&nbsp;are many reasons why multiple stakeholders need to be involved, partly from who owns what and&nbsp;is responsible for&nbsp;what and partly from the perspective of who has the&nbsp;expertise&nbsp;to meaningfully construct the evaluations that we need.



SULLIVAN:&nbsp;Well, Chad, this has just been great to connect, and in a few of our discussions,&nbsp;we&#8217;ve&nbsp;done a bit of a lightning round, so&nbsp;I&#8217;d&nbsp;love to just hear your&nbsp;30-second responses to a few of these questions. Perhaps&nbsp;favorite&nbsp;evaluation&nbsp;you&#8217;ve&nbsp;run so far this year?&nbsp;



ATALLA:&nbsp;So&nbsp;I&#8217;ve&nbsp;been involved in trying to evaluate some language models for whether they&nbsp;infer&nbsp;sensitive attributes about people. So&nbsp;perhaps&nbsp;you&#8217;re&nbsp;chatting with a&nbsp;chatbot,&nbsp;and it infers your religion or sexuality based on things&nbsp;you&#8217;re&nbsp;saying or how you sound,&nbsp;right.&nbsp;And in working to evaluate this, we&nbsp;encounter&nbsp;a lot of interesting questions. Or,&nbsp;like,&nbsp;what is a sensitive attribute? What makes these attributes sensitive, and what are the differences that make it inappropriate for an AI system to infer these things about a person? Whereas realistically, whenever I meet a person on the street, my&nbsp;brain is&nbsp;immediately&nbsp;forming&nbsp;first impressions and some assumptions about these people.&nbsp;So&nbsp;it&#8217;s&nbsp;a very interesting&nbsp;and thought-provoking evaluation to conduct and think about the norms that we place upon&nbsp;people&nbsp;interacting with other people and the norms we place upon&nbsp;AI systems&nbsp;interacting with other people.



SULLIVAN:&nbsp;That‚Äôs&nbsp;fascinating!&nbsp;I&#8217;d&nbsp;love to hear the AI&nbsp;buzzword&nbsp;you&#8217;d&nbsp;retire tomorrow.&nbsp;[LAUGHTER]



ATALLA:&nbsp;I would love to see the term ‚Äúbias‚Äù being&nbsp;used less when referring to fairness-related issues and systems. Bias happens to be a highly overloaded term in statistics and machine learning and has a lot of technical meanings and just&nbsp;fails to&nbsp;perfectly capture what we mean in the AI risk sense.



SULLIVAN:¬†And last one. One metric¬†we&#8217;re¬†not tracking enough.



ATALLA:¬†I would say over-blocking, and this comes into that connection between the holistic picture of safety and efficacy. It&#8217;s too easy to produce systems that throw safety to the wind and focus purely on utility or achieving some goal, but simultaneously, the other side of the picture is possible, where we can clamp down too hard and reduce the utility of our systems and block even benign and useful outputs just because they border on something sensitive.¬†So¬†it&#8217;s¬†important for us to track that over-blocking and actively track that tradeoff between safety and efficacy.



SULLIVAN:&nbsp;Yeah, we talk a lot about this on the podcast,&nbsp;too,&nbsp;of how do you both make things safe but also ensure innovation can&nbsp;thrive,&nbsp;and&nbsp;I think you&nbsp;hit the nail on the head with that last piece.



[MUSIC]&nbsp;



Well, Chad, this was&nbsp;really terrific. Thanks for joining us and thanks for your work and your&nbsp;perspectives. And another big thanks to Daniel and Timo for setting the stage earlier in the podcast.



And to our listeners, thanks for tuning in. You can find resources related to this podcast in the show notes. And if you want to learn more about how Microsoft approaches AI governance, you can visit microsoft.com/RAI.‚ÄØ



See you next time!‚ÄØ



[MUSIC FADES]

				
			
			
				Show more			
		
	

Opens in a new tabThe post AI Testing and Evaluation: Learnings from pharmaceuticals and medical devices appeared first on Microsoft Research.
‚Ä¢ How INRIX accelerates transportation planning with Amazon Bedrock
  This post is co-written with Shashank Saraogi, Nat Gale, and Durran Kelly from INRIX. 
The complexity of modern traffic management extends far beyond mere road monitoring, encompassing massive amounts of data collected worldwide from connected cars, mobile devices, roadway sensors, and major event monitoring systems. For transportation authorities managing urban, suburban, and rural traffic flow, the challenge lies in effectively processing and acting upon this vast network of information. The task requires balancing immediate operational needs, such as real-time traffic redirection during incidents, with strategic long-term planning for improved mobility and safety. 
Traditionally, analyzing these complex data patterns and producing actionable insights has been a resource-intensive process requiring extensive collaboration. With recent advances in generative AI, there is an opportunity to transform how we process, understand, and act upon transportation data, enabling more efficient and responsive traffic management systems. 
In this post, we partnered with Amazon Web Services (AWS) customer INRIX to demonstrate how Amazon Bedrock can be used to determine the best countermeasures for specific city locations using rich transportation data and how such countermeasures can be automatically visualized in street view images. This approach allows for significant planning acceleration compared to traditional approaches using conceptual drawings. 
INRIX pioneered the use of GPS data from connected vehicles for transportation intelligence. For over 20 years, INRIX has been a leader for probe-based connected vehicle and device data and insights, powering automotive, enterprise, and public sector use cases. INRIX‚Äôs products range from tickerized datasets that inform investment decisions for the financial services sector to digital twins for the public rights-of-way in the cities of Philadelphia and San Francisco. INRIX was the first company to develop a crowd-sourced traffic network, and they continue to lead in real-time mobility operations. 
In June 2024, the State of California‚Äôs Department of Transportation (Caltrans) selected INRIX for a proof of concept for a generative AI-powered solution to improve safety for vulnerable road users (VRUs). The problem statement sought to harness the combination of Caltrans‚Äô asset, crash, and points-of-interest (POI) data and INRIX‚Äôs 50 petabyte (PB) data lake to anticipate high-risk locations and quickly generate empirically validated safety measures to mitigate the potential for crashes. Trained on real-time and historical data and industry research and manuals, the solution provides a new systemic, safety-based methodology for risk assessment, location prioritization, and project implementation. 
Solution overview 
INRIX announced INRIX Compass in November 2023. INRIX Compass is an application that harnesses generative AI and INRIX‚Äôs 50 PB data lake to solve transportation challenges. This solution uses INRIX Compass countermeasures as the input, AWS serverless architecture, and Amazon Nova Canvas as the image visualizer. Key components include: 
 
 Countermeasures generation: 
   
   INRIX Compass generates the countermeasures for a selected location 
   Amazon API Gateway and Amazon Elastic Kubernetes Service (Amazon EKS) manage API requests and responses 
   Amazon Bedrock Knowledge Bases and Anthropic‚Äôs Claude Models provide Retrieval Augmented Generation (RAG) implementation 
    
 Image visualization 
   
   API Gateway and AWS Lambda process requests from API Gateway and Amazon Bedrock 
   Amazon Bedrock with model access to Amazon Nova Canvas provide image generation and in-painting 
    
 
The following diagram shows the architecture of INRIX Compass. 
 
INRIX Compass for countermeasures 
By using INRIX Compass, users can ask natural language queries such as, Where are the top five locations with the highest risk for vulnerable road users? and Can you recommend a suite of proven safety countermeasures at each of these locations? Furthermore, users can probe deeper into the roadway characteristics that contribute to risk factors, and find similar locations in the roadway network that meet those conditions. Behind the scenes, Compass AI uses RAG and Amazon Bedrock powered foundation models (FMs) to query the roadway network to identify and prioritize locations with systemic risk factors and anomalous safety patterns. The solution provides prioritized recommendations for operational and design solutions and countermeasures based on industry knowledge. 
The following image shows the interface of INRIX Compass. 
 
Image visualization for countermeasures 
The generation of countermeasure suggestions represents the initial phase in transportation planning. Image visualization requires the crucial next step of preparing conceptual drawings. This process has traditionally been time-consuming due to the involvement of multiple specialized teams, including: 
 
 Transportation engineers who assess technical feasibility and safety standards 
 Urban planners who verify alignment with city development goals 
 Landscape architects who integrate environmental and aesthetic elements 
 CAD or visualization specialists who create detailed technical drawings 
 Safety analysts who evaluate the potential impact on road safety 
 Public works departments who oversee implementation feasibility 
 Traffic operations teams who assess impact on traffic flow and management 
 
These teams work collaboratively, creating and iteratively refining various visualizations based on feedback from urban designers and other stakeholders. Each iteration cycle typically involves multiple rounds of reviews, adjustments, and approvals, often extending the timeline significantly. The complexity is further amplified by city-specific rules and design requirements, which often necessitate significant customization. Additionally, local regulations, environmental considerations, and community feedback must be incorporated into the design process. Consequently, this lengthy and costly process frequently leads to delays in implementing safety countermeasures. To streamline this challenge, INRIX has pioneered an innovative approach to the visualization phase by using generative AI technology. This prototyped solution enables rapid iteration of conceptual drawings that can be efficiently reviewed by various teams, potentially reducing the design cycle from weeks to days. Moreover, the system incorporates a few-shot learning approach with reference images and carefully crafted prompts, allowing for seamless integration of city-specific requirements into the generated outputs. This approach not only accelerates the design process but also supports consistency across different projects while maintaining compliance with local standards. 
The following image shows the congestion insights by INRIX Compass. 
 
Amazon Nova Canvas for conceptual visualizations 
INRIX developed and prototyped this solution using Amazon Nova models. Amazon Nova Canvas delivers advanced image processing through text-to-image generation and image-to-image transformation capabilities. The model provides sophisticated controls for adjusting color schemes and manipulating layouts to achieve desired visual outcomes. To promote responsible AI implementation, Amazon Nova Canvas incorporates built-in safety measures, including watermarking and content moderation systems. 
The model supports a comprehensive range of image editing operations. These operations encompass basic image generation, object removal from existing images, object replacement within scenes, creation of image variations, and modification of image backgrounds. This versatility makes Amazon Nova Canvas suitable for a wide range of professional applications requiring sophisticated image editing. 
The following sample images show an example of countermeasures visualization. 
 
 
In-painting implementation in Compass AI 
Amazon Nova Canvas integrates with INRIX Compass‚Äôs existing natural language analytics capabilities. The original Compass system generated text-based countermeasure recommendations based on: 
 
 Historical transportation data analysis 
 Current environmental conditions 
 User-specified requirements 
 
The INRIX Compass visualization feature specifically uses the image generation and in-painting capabilities of Amazon Nova Canvas. In-painting enables object replacement through two distinct approaches: 
 
 A binary mask precisely defines the areas targeted for replacement. 
 Text prompts identify objects for replacement, allowing the model to interpret and modify the specified elements while maintaining visual coherence with the surrounding image context. This functionality provides seamless integration of new elements while preserving the overall image composition and contextual relevance. The developed interface accommodates both image generation and in-painting approaches, providing comprehensive image editing capabilities. 
 
The implementation follows a two-stage process for visualizing transportation countermeasures. Initially, the system employs image generation functionality to create street-view representations corresponding to specific longitude and latitude coordinates where interventions are proposed. Following the initial image creation, the in-painting capability enables precise placement of countermeasures within the generated street view scene. This sequential approach provides accurate visualization of proposed modifications within the actual geographical context. 
An Amazon Bedrock API facilitates image editing and generation through the Amazon Nova Canvas model. The responses contain the generated or modified images in base64 format, which can be decoded and processed for further use in the application. The generative AI capabilities of Amazon Bedrock enable rapid iteration and simultaneous visualization of multiple countermeasures within a single image. RAG implementation can further extend the pipeline‚Äôs capabilities by incorporating county-specific regulations, standardized design patterns, and contextual requirements. The integration of these technologies significantly streamlines the countermeasure deployment workflow. Traditional manual visualization processes that previously required extensive time and resources can now be executed efficiently through automated generation and modification. This automation delivers substantial improvements in both time-to-deployment and cost-effectiveness. 
Conclusion 
The partnership between INRIX and AWS showcases the transformative potential of AI in solving complex transportation challenges. By using Amazon Bedrock FMs, INRIX has turned their massive 50 PB data lake into actionable insights through effective visualization solutions. This post highlighted a single specific transportation use case, but Amazon Bedrock and Amazon Nova power a wide spectrum of applications, from text generation to video creation. The combination of extensive data and advanced AI capabilities continues to pave the way for smarter, more efficient transportation systems worldwide. 
For more information, check out the documentation for Amazon Nova Foundation Models, Amazon Bedrock, and INRIX Compass. 
 
About the authors 
Arun is a Senior Solutions Architect at AWS, supporting enterprise customers in the Pacific Northwest. He‚Äôs passionate about solving business and technology challenges as an AWS customer advocate, with his recent interest being AI strategy. When not at work, Arun enjoys listening to podcasts, going for short trail runs, and spending quality time with his family. 
Alicja Kwasniewska, PhD, is an AI leader driving generative AI innovations in enterprise solutions and decision intelligence for customer engagements in North America, advertisement and marketing verticals at AWS. She is recognized among the top 10 women in AI and 100 women in data science. Alicja published in more than 40 peer-reviewed publications. She also serves as a reviewer for top-tier conferences, including ICML,NeurIPS,and ICCV. She advises organizations on AI adoption, bridging research and industry to accelerate real-world AI applications. 
Shashank is the VP of Engineering at INRIX, where he leads multiple verticals, including generative AI and traffic. He is passionate about using technology to make roads safer for drivers, bikers, and pedestrians every day. Prior to working at INRIX, he held engineering leadership roles at Amazon and Lyft. Shashank brings deep experience in building impactful products and high-performing teams at scale. Outside of work, he enjoys traveling, listening to music, and spending time with his family. 
Nat Gale is the Head of Product at INRIX, where he manages the Safety and Traffic product verticals. Nat leads the development of data products and software that help transportation professionals make smart, more informed decisions. He previously ran the City of Los Angeles‚Äô Vision Zero program and was the Director of Capital Projects and Operations for the City of Hartford, CT. 
Durran is a Lead Software Engineer at INRIX, where he designs scalable backend systems and mentors engineers across multiple product lines. With over a decade of experience in software development, he specializes in distributed systems, generative AI, and cloud infrastructure. Durran is passionate about writing clean, maintainable code and sharing best practices with the developer community. Outside of work, he enjoys spending quality time with his family and deepening his Japanese language skills.
‚Ä¢ Qwen3 family of reasoning models now available in Amazon Bedrock Marketplace and Amazon SageMaker JumpStart
  Today, we are excited to announce that Qwen3, the latest generation of large language models (LLMs) in the Qwen family, is available through Amazon Bedrock Marketplace and Amazon SageMaker JumpStart. With this launch, you can deploy the Qwen3 models‚Äîavailable in 0.6B, 4B, 8B, and 32B parameter sizes‚Äîto build, experiment, and responsibly scale your generative AI applications on AWS. 
In this post, we demonstrate how to get started with Qwen3 on Amazon Bedrock Marketplace and SageMaker JumpStart. You can follow similar steps to deploy the distilled versions of the models as well. 
Solution overview 
Qwen3 is the latest generation of LLMs in the Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models. Qwen3 delivers groundbreaking advancements in reasoning, instruction-following, agent capabilities, and multilingual support, with the following key features: 
 
 Unique support of seamless switching between thinking mode and non-thinking mode within a single model, providing optimal performance across various scenarios. 
 Significantly enhanced in its reasoning capabilities, surpassing previous QwQ (in thinking mode) and Qwen2.5 instruct models (in non-thinking mode) on mathematics, code generation, and commonsense logical reasoning. 
 Good human preference alignment, excelling in creative writing, role-playing, multi-turn dialogues, and instruction following, to deliver a more natural, engaging, and immersive conversational experience. 
 Expertise in agent capabilities, enabling precise integration with external tools in both thinking and unthinking modes and achieving leading performance among open source models in complex agent-based tasks. 
 Support for over 100 languages and dialects with strong capabilities for multilingual instruction following and translation. 
 
Prerequisites 
To deploy Qwen3 models, make sure you have access to the recommended instance types based on the model size. You can find these instance recommendations on Amazon Bedrock Marketplace or the SageMaker JumpStart console. To verify you have the necessary resources, complete the following steps: 
 
 Open the Service Quotas console. 
 Under AWS Services, select Amazon SageMaker. 
 Check that you have sufficient quota for the required instance type for endpoint deployment. 
 Make sure at least one of these instance types is available in your target AWS Region. 
 
If needed, request a quota increase and contact your AWS account team for support. 
Deploy Qwen3 in Amazon Bedrock Marketplace 
Amazon Bedrock Marketplace gives you access to over 100 popular, emerging, and specialized foundation models (FMs) through Amazon Bedrock. To access Qwen3 in Amazon Bedrock, complete the following steps: 
 
 On the Amazon Bedrock console, in the navigation pane under Foundation models, choose Model catalog. 
 Filter for Hugging Face as a provider and choose a Qwen3 model. For this example, we use the Qwen3-32B model. 
 
 
The model detail page provides essential information about the model‚Äôs capabilities, pricing structure, and implementation guidelines. You can find detailed usage instructions, including sample API calls and code snippets for integration. 
The page also includes deployment options and licensing information to help you get started with Qwen3-32B in your applications. 
 
 To begin using Qwen3-32B, choose Deploy. 
 
 
You will be prompted to configure the deployment details for Qwen3-32B. The model ID will be pre-populated. 
 
 For Endpoint name, enter an endpoint name (between 1‚Äì50 alphanumeric characters). 
 For Number of instances, enter a number of instances (between 1‚Äì100). 
 For Instance type, choose your instance type. For optimal performance with Qwen3-32B, a GPU-based instance type like ml.g5-12xlarge is recommended. 
 To deploy the model, choose Deploy. 
 
 
When the deployment is complete, you can test Qwen3-32B‚Äôs capabilities directly in the Amazon Bedrock playground. 
 
 Choose Open in playground to access an interactive interface where you can experiment with different prompts and adjust model parameters like temperature and maximum length. 
 
This is an excellent way to explore the model‚Äôs reasoning and text generation abilities before integrating it into your applications. The playground provides immediate feedback, helping you understand how the model responds to various inputs and letting you fine-tune your prompts for optimal results.You can quickly test the model in the playground through the UI. However, to invoke the deployed model programmatically with any Amazon Bedrock APIs, you must have the endpoint Amazon Resource Name (ARN). 
Enable reasoning and non-reasoning responses with Converse API 
The following code shows how to turn reasoning on and off with Qwen3 models using the Converse API, depending on your use case. By default, reasoning is left on for Qwen3 models, but you can streamline interactions by using the /no_think command within your prompt. When you add this to the end of your query, reasoning is turned off and the models will provide just the direct answer. This is particularly useful when you need quick information without explanations, are familiar with the topic, or want to maintain a faster conversational flow. At the time of writing, the Converse API doesn‚Äôt support tool use for Qwen3 models. Refer to the Invoke_Model API example later in this post to learn how to use reasoning and tools in the same completion. 
 
 import boto3
from botocore.exceptions import ClientError

# Create a Bedrock Runtime client in the AWS Region you want to use.
client = boto3.client("bedrock-runtime", region_name="us-west-2")

# Configuration
model_id = "" &nbsp;# Replace with Bedrock Marketplace endpoint arn

# Start a conversation with the user message.
user_message = "hello, what is 1+1 /no_think"&nbsp;#remove /no_think to leave default reasoning on
conversation = [
&nbsp;&nbsp; &nbsp;{
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"role": "user",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"content": [{"text": user_message}],
&nbsp;&nbsp; &nbsp;}
]

try:
&nbsp;&nbsp; &nbsp;# Send the message to the model, using a basic inference configuration.
&nbsp;&nbsp; &nbsp;response = client.converse(
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;modelId=model_id,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;messages=conversation,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;inferenceConfig={"maxTokens": 512, "temperature": 0.5, "topP": 0.9},
&nbsp;&nbsp; &nbsp;)

&nbsp;&nbsp; &nbsp;# Extract and print the response text.
&nbsp;&nbsp; &nbsp;#response_text = response["output"]["message"]["content"][0]["text"]
&nbsp;&nbsp; &nbsp;#reasoning_content = response ["output"]["message"]["reasoning_content"][0]["text"]
&nbsp;&nbsp; &nbsp;#print(response_text, reasoning_content)
&nbsp;&nbsp; &nbsp;print(response)
&nbsp;&nbsp; &nbsp;
except (ClientError, Exception) as e:
&nbsp;&nbsp; &nbsp;print(f"ERROR: Can't invoke '{model_id}'. Reason: {e}")
&nbsp;&nbsp; &nbsp;exit(1) 
 
The following is a response using the Converse API, without default thinking: 
 
 {'ResponseMetadata': {'RequestId': 'f7f3953a-5747-4866-9075-fd4bd1cf49c4', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Tue, 17 Jun 2025 18:34:47 GMT', 'content-type': 'application/json', 'content-length': '282', 'connection': 'keep-alive', 'x-amzn-requestid': 'f7f3953a-5747-4866-9075-fd4bd1cf49c4'}, 'RetryAttempts': 0}, 'output': {'message': {'role': 'assistant', 'content': [{'text': '\n\nHello! The result of 1 + 1 is **2**. üòä'}, {'reasoningContent': {'reasoningText': {'text': '\n\n'}}}]}}, 'stopReason': 'end_turn', 'usage': {'inputTokens': 20, 'outputTokens': 22, 'totalTokens': 42}, 'metrics': {'latencyMs': 1125}} 
 
The following is an example with default thinking on; the &lt;think&gt; tokens are automatically parsed into the reasoningContent field for the Converse API: 
 
 {'ResponseMetadata': {'RequestId': 'b6d2ebbe-89da-4edc-9a3a-7cb3e7ecf066', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Tue, 17 Jun 2025 18:32:28 GMT', 'content-type': 'application/json', 'content-length': '1019', 'connection': 'keep-alive', 'x-amzn-requestid': 'b6d2ebbe-89da-4edc-9a3a-7cb3e7ecf066'}, 'RetryAttempts': 0}, 'output': {'message': {'role': 'assistant', 'content': [{'text': '\n\nHello! The sum of 1 + 1 is **2**. Let me know if you have any other questions or need further clarification! üòä'}, {'reasoningContent': {'reasoningText': {'text': '\nOkay, the user asked "hello, what is 1+1". Let me start by acknowledging their greeting. They might just be testing the water or actually need help with a basic math problem. Since it\'s 1+1, it\'s a very simple question, but I should make sure to answer clearly. Maybe they\'re a child learning math for the first time, or someone who\'s not confident in their math skills. I should provide the answer in a friendly and encouraging way. Let me confirm that 1+1 equals 2, and maybe add a brief explanation to reinforce their understanding. I can also offer further assistance in case they have more questions. Keeping it conversational and approachable is key here.\n'}}}]}}, 'stopReason': 'end_turn', 'usage': {'inputTokens': 16, 'outputTokens': 182, 'totalTokens': 198}, 'metrics': {'latencyMs': 7805}} 
 
Perform reasoning and function calls in the same completion using the Invoke_Model API 
With Qwen3, you can stream an explicit trace and the exact JSON tool call in the same completion. Up until now, reasoning models have forced the choice to either show the chain of thought or call tools deterministically. The following code shows an example: 
 
 messages = json.dumps( {
&nbsp;&nbsp; &nbsp;"messages": [
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;{
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"role": "user",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"content": "Hi! How are you doing today?"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;}, 
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;{
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"role": "assistant",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"content": "I'm doing well! How can I help you?"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;}, 
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;{
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"role": "user",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"content": "Can you tell me what the temperate will be in Dallas, in fahrenheit?"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp;],
&nbsp;&nbsp; &nbsp;"tools": [{
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"type": "function",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"function": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"name": "get_current_weather",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"description": "Get the current weather in a given location",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"parameters": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"type": "object",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"properties": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"city": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"type":
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"string",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"description":
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"The city to find the weather for, e.g. 'San Francisco'"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;},
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"state": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"type":
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"string",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"description":
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"the two-letter abbreviation for the state that the city is in, e.g. 'CA' which would mean 'California'"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;},
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"unit": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"type": "string",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"description":
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"The unit to fetch the temperature in",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"enum": ["celsius", "fahrenheit"]
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;},
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"required": ["city", "state", "unit"]
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp;}],
&nbsp;&nbsp; &nbsp;"tool_choice": "auto"
})

response = client.invoke_model(
&nbsp;&nbsp; &nbsp;modelId=model_id, 
&nbsp;&nbsp; &nbsp;body=body
)
print(response)
model_output = json.loads(response['body'].read())
print(json.dumps(model_output, indent=2)) 
 
Response: 
 
 {'ResponseMetadata': {'RequestId': '5da8365d-f4bf-411d-a783-d85eb3966542', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Tue, 17 Jun 2025 18:57:38 GMT', 'content-type': 'application/json', 'content-length': '1148', 'connection': 'keep-alive', 'x-amzn-requestid': '5da8365d-f4bf-411d-a783-d85eb3966542', 'x-amzn-bedrock-invocation-latency': '6396', 'x-amzn-bedrock-output-token-count': '148', 'x-amzn-bedrock-input-token-count': '198'}, 'RetryAttempts': 0}, 'contentType': 'application/json', 'body': &lt;botocore.response.StreamingBody object at 0x7f7d4a598dc0&gt;}
{
&nbsp;&nbsp;"id": "chatcmpl-bc60b482436542978d233b13dc347634",
&nbsp;&nbsp;"object": "chat.completion",
&nbsp;&nbsp;"created": 1750186651,
&nbsp;&nbsp;"model": "lmi",
&nbsp;&nbsp;"choices": [
&nbsp;&nbsp; &nbsp;{
&nbsp;&nbsp; &nbsp; &nbsp;"index": 0,
&nbsp;&nbsp; &nbsp; &nbsp;"message": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"role": "assistant",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"reasoning_content": "\nOkay, the user is asking about the weather in San Francisco. Let me check the tools available. There's a get_weather function that requires location and unit. The user didn't specify the unit, so I should ask them if they want Celsius or Fahrenheit. Alternatively, maybe I can assume a default, but since the function requires it, I need to include it. I'll have to prompt the user for the unit they prefer.\n",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"content": "\n\nThe user hasn't specified whether they want the temperature in Celsius or Fahrenheit. I need to ask them to clarify which unit they prefer.\n\n",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"tool_calls": [
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;{
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"id": "chatcmpl-tool-fb2f93f691ed4d8ba94cadc52b57414e",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"type": "function",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"function": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"name": "get_weather",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"arguments": "{\"location\": \"San Francisco, CA\", \"unit\": \"celsius\"}"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;]
&nbsp;&nbsp; &nbsp; &nbsp;},
&nbsp;&nbsp; &nbsp; &nbsp;"logprobs": null,
&nbsp;&nbsp; &nbsp; &nbsp;"finish_reason": "tool_calls",
&nbsp;&nbsp; &nbsp; &nbsp;"stop_reason": null
&nbsp;&nbsp; &nbsp;}
&nbsp;&nbsp;],
&nbsp;&nbsp;"usage": {
&nbsp;&nbsp; &nbsp;"prompt_tokens": 198,
&nbsp;&nbsp; &nbsp;"total_tokens": 346,
&nbsp;&nbsp; &nbsp;"completion_tokens": 148,
&nbsp;&nbsp; &nbsp;"prompt_tokens_details": null
&nbsp;&nbsp;},
&nbsp;&nbsp;"prompt_logprobs": null
} 
 
Deploy Qwen3-32B with SageMaker JumpStart 
SageMaker JumpStart is a machine learning (ML) hub with FMs, built-in algorithms, and prebuilt ML solutions that you can deploy with just a few clicks. With SageMaker JumpStart, you can customize pre-trained models to your use case, with your data, and deploy them into production using either the UI or SDK.Deploying the Qwen3-32B model through SageMaker JumpStart offers two convenient approaches: using the intuitive SageMaker JumpStart UI or implementing programmatically through the SageMaker Python SDK. Let‚Äôs explore both methods to help you choose the approach that best suits your needs. 
Deploy Qwen3-32B through SageMaker JumpStart UI 
Complete the following steps to deploy Qwen3-32B using SageMaker JumpStart: 
 
 On the SageMaker console, choose Studio in the navigation pane. 
 First-time users will be prompted to create a domain. 
 On the SageMaker Studio console, choose JumpStart in the navigation pane. 
 
The model browser displays available models, with details like the provider name and model capabilities. 
 
 
 Search for Qwen3 to view the Qwen3-32B model card. 
 
Each model card shows key information, including: 
 
 Model name 
 Provider name 
 Task category (for example, Text Generation) 
 Bedrock Ready badge (if applicable), indicating that this model can be registered with Amazon Bedrock, so you can use Amazon Bedrock APIs to invoke the model 
 
 
 
 Choose the model card to view the model details page. 
 
The model details page includes the following information: 
 
 The model name and provider information 
 A Deploy button to deploy the model 
 About and Notebooks tabs with detailed information 
 
The About tab includes important details, such as: 
 
 Model description 
 License information 
 Technical specifications 
 Usage guidelines 
 
 
Before you deploy the model, it‚Äôs recommended to review the model details and license terms to confirm compatibility with your use case. 
 
 Choose Deploy to proceed with deployment. 
 For Endpoint name, use the automatically generated name or create a custom one. 
 For Instance type¬∏ choose an instance type (default: ml.g6-12xlarge). 
 For Initial instance count, enter the number of instances (default: 1). 
 
Selecting appropriate instance types and counts is crucial for cost and performance optimization. Monitor your deployment to adjust these settings as needed. Under Inference type, Real-time inference is selected by default. This is optimized for sustained traffic and low latency. 
 
 Review all configurations for accuracy. For this model, we strongly recommend adhering to SageMaker JumpStart default settings and making sure that network isolation remains in place. 
 Choose Deploy to deploy the model. 
 
 
The deployment process can take several minutes to complete. 
When deployment is complete, your endpoint status will change to InService. At this point, the model is ready to accept inference requests through the endpoint. You can monitor the deployment progress on the SageMaker console Endpoints page, which will display relevant metrics and status information. When the deployment is complete, you can invoke the model using a SageMaker runtime client and integrate it with your applications. 
Deploy Qwen3-32B using the SageMaker Python SDK 
To get started with Qwen3-32B using the SageMaker Python SDK, you must install the SageMaker Python SDK and make sure you have the necessary AWS permissions and environment set up. The following is a step-by-step code example that demonstrates how to deploy and use Qwen3-32B for inference programmatically: 
 
 !pip install --force-reinstall --no-cache-dir sagemaker==2.235.2

from sagemaker.serve.builder.model_builder import ModelBuilder 
from sagemaker.serve.builder.schema_builder import SchemaBuilder 
from sagemaker.jumpstart.model import ModelAccessConfig 
from sagemaker.session import Session 
import logging 

sagemaker_session = Session()
artifacts_bucket_name = sagemaker_session.default_bucket() 
execution_role_arn = sagemaker_session.get_caller_identity_arn()

# Changed to Qwen32B model
js_model_id = "huggingface-reasoning-qwen3-32b"
gpu_instance_type = "ml.g5.12xlarge"

response = "Hello, I'm a language model, and I'm here to help you with your English."

sample_input = {
&nbsp;&nbsp; &nbsp;"inputs": "Hello, I'm a language model,",
&nbsp;&nbsp; &nbsp;"parameters": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"max_new_tokens": 128, 
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"top_p": 0.9, 
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"temperature": 0.6
&nbsp;&nbsp; &nbsp;}
}

sample_output = [{"generated_text": response}]

schema_builder = SchemaBuilder(sample_input, sample_output)

model_builder = ModelBuilder( 
&nbsp;&nbsp; &nbsp;model=js_model_id, 
&nbsp;&nbsp; &nbsp;schema_builder=schema_builder, 
&nbsp;&nbsp; &nbsp;sagemaker_session=sagemaker_session, 
&nbsp;&nbsp; &nbsp;role_arn=execution_role_arn, 
&nbsp;&nbsp; &nbsp;log_level=logging.ERROR 
) 

model = model_builder.build() 

predictor = model.deploy(
&nbsp;&nbsp; &nbsp;model_access_configs={js_model_id: ModelAccessConfig(accept_eula=True)}, 
&nbsp;&nbsp; &nbsp;accept_eula=True
) 

predictor.predict(sample_input) 
 
You can run additional requests against the predictor: 
 
 new_input =&nbsp;{
"inputs":&nbsp;"What is Amazon doing in Generative AI?",
"parameters":&nbsp;{"max_new_tokens":&nbsp;64,&nbsp;"top_p":&nbsp;0.8,&nbsp;"temperature":&nbsp;0.7},
}

prediction =&nbsp;predictor.predict(new_input)
print(prediction) 
 
The following are some error handling and best practices to enhance deployment code: 
 
 # Enhanced deployment code with error handling
import backoff
import botocore
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@backoff.on_exception(backoff.expo, 
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; (botocore.exceptions.ClientError,),
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; max_tries=3)
def deploy_model_with_retries(model_builder, model_id):
&nbsp;&nbsp; &nbsp;try:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;model = model_builder.build()
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;predictor = model.deploy(
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;model_access_configs={model_id:ModelAccessConfig(accept_eula=True)},
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;accept_eula=True
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;)
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;return predictor
&nbsp;&nbsp; &nbsp;except Exception as e:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;logger.error(f"Deployment failed: {str(e)}")
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;raise

def safe_predict(predictor, input_data):
&nbsp;&nbsp; &nbsp;try:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;return predictor.predict(input_data)
&nbsp;&nbsp; &nbsp;except Exception as e:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;logger.error(f"Prediction failed: {str(e)}")
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;return None 
 
Clean up 
To avoid unwanted charges, complete the steps in this section to clean up your resources. 
Delete the Amazon Bedrock Marketplace deployment 
If you deployed the model using Amazon Bedrock Marketplace, complete the following steps: 
 
 On the Amazon Bedrock console, under Foundation models in the navigation pane, choose Marketplace deployments. 
 In the Managed deployments section, locate the endpoint you want to delete. 
 Select the endpoint, and on the Actions menu, choose Delete. 
 Verify the endpoint details to make sure you‚Äôre deleting the correct deployment: 
   
   Endpoint name 
   Model name 
   Endpoint status 
    
 Choose Delete to delete the endpoint. 
 In the deletion confirmation dialog, review the warning message, enter confirm, and choose Delete to permanently remove the endpoint. 
 
Delete the SageMaker JumpStart predictor 
The SageMaker JumpStart model you deployed will incur costs if you leave it running. Use the following code to delete the endpoint if you want to stop incurring charges. For more details, see Delete Endpoints and Resources. 
 
 predictor.delete_model()
predictor.delete_endpoint() 
 
Conclusion 
In this post, we explored how you can access and deploy the Qwen3 models using Amazon Bedrock Marketplace and SageMaker JumpStart. With support for both the full parameter models and its distilled versions, you can choose the optimal model size for your specific use case. Visit SageMaker JumpStart in Amazon SageMaker Studio or Amazon Bedrock Marketplace to get started. For more information, refer to Use Amazon Bedrock tooling with Amazon SageMaker JumpStart models, SageMaker JumpStart pretrained models, Amazon SageMaker JumpStart Foundation Models, Amazon Bedrock Marketplace, and Getting started with Amazon SageMaker JumpStart. 
The Qwen3 family of LLMs offers exceptional versatility and performance, making it a valuable addition to the AWS foundation model offerings. Whether you‚Äôre building applications for content generation, analysis, or complex reasoning tasks, Qwen3‚Äôs advanced architecture and extensive context window make it a powerful choice for your generative AI needs. 
 
About the authors 
Niithiyn Vijeaswaran is a Generative AI Specialist Solutions Architect with the Third-Party Model Science team at AWS. His area of focus is AWS AI accelerators (AWS Neuron). He holds a Bachelor‚Äôs degree in Computer Science and Bioinformatics. 
Avan Bala is a Solutions Architect at AWS. His area of focus is AI for DevOps and machine learning. He holds a bachelor‚Äôs degree in Computer Science with a minor in Mathematics and Statistics from the University of Maryland. Avan is currently working with the Enterprise Engaged East Team and likes to specialize in projects about emerging AI technologies. 
Mohhid Kidwai is a Solutions Architect at AWS. His area of focus is generative AI and machine learning solutions for small-medium businesses. He holds a bachelor‚Äôs degree in Computer Science with a minor in Biological Science from North Carolina State University. Mohhid is currently working with the SMB Engaged East Team at AWS. 
Yousuf Athar is a Solutions Architect at AWS specializing in generative AI and AI/ML. With a Bachelor‚Äôs degree in Information Technology and a concentration in Cloud Computing, he helps customers integrate advanced generative AI capabilities into their systems, driving innovation and competitive edge. Outside of work, Yousuf loves to travel, watch sports, and play football. 
John Liu has 15 years of experience as a product executive and 9 years of experience as a portfolio manager. At AWS, John is a Principal Product Manager for Amazon Bedrock. Previously, he was the Head of Product for AWS Web3 / Blockchain. Prior to AWS, John held various product leadership roles at public blockchain protocols, fintech companies and also spent 9 years as a portfolio manager at various hedge funds. 
Rohit Talluri is a Generative AI GTM Specialist at Amazon Web Services (AWS). He is partnering with top generative AI model builders, strategic customers, key AI/ML partners, and AWS Service Teams to enable the next generation of artificial intelligence, machine learning, and accelerated computing on AWS. He was previously an Enterprise Solutions Architect and the Global Solutions Lead for AWS Mergers &amp; Acquisitions Advisory. 
Varun Morishetty is a Software Engineer with Amazon SageMaker JumpStart and Bedrock Marketplace. Varun received his Bachelor‚Äôs degree in Computer Science from Northeastern University. In his free time, he enjoys cooking, baking and exploring New York City.
‚Ä¢ Build a just-in-time knowledge base with Amazon Bedrock
  Software as a service (SaaS) companies managing multiple tenants face a critical challenge: efficiently extracting meaningful insights from vast document collections while controlling costs. Traditional approaches often lead to unnecessary spending on unused storage and processing resources, impacting both operational efficiency and profitability. Organizations need solutions that intelligently scale processing and storage resources based on actual tenant usage patterns while maintaining data isolation. Traditional Retrieval Augmented Generation (RAG) systems consume valuable resources by ingesting and maintaining embeddings for documents that might never be queried, resulting in unnecessary storage costs and reduced system efficiency. Systems designed to handle large amounts of small to mid-sized tenants can exceed cost structure and infrastructure limits or might need to use silo-style deployments to keep each tenant‚Äôs information and usage separate. Adding to this complexity, many projects are transitory in nature, with work being completed on an intermittent basis, leading to data occupying space in knowledge base systems that could be used by other active tenants. 
To address these challenges, this post presents a just-in-time knowledge base solution that reduces unused consumption through intelligent document processing. The solution processes documents only when needed and automatically removes unused resources, so organizations can scale their document repositories without proportionally increasing infrastructure costs. 
With a multi-tenant architecture with configurable limits per tenant, service providers can offer tiered pricing models while maintaining strict data isolation, making it ideal for SaaS applications serving multiple clients with varying needs. Automatic document expiration through Time-to-Live (TTL) makes sure the system remains lean and focused on relevant content, while refreshing the TTL for frequently accessed documents maintains optimal performance for information that matters. This architecture also makes it possible to limit the number of files each tenant can ingest at a specific time and the rate at which tenants can query a set of files.This solution uses serverless technologies to alleviate operational overhead and provide automatic scaling, so teams can focus on business logic rather than infrastructure management. By organizing documents into groups with metadata-based filtering, the system enables contextual querying that delivers more relevant results while maintaining security boundaries between tenants.The architecture‚Äôs flexibility supports customization of tenant configurations, query rates, and document retention policies, making it adaptable to evolving business requirements without significant rearchitecting. 
Solution overview 
This architecture combines several AWS services to create a cost-effective, multi-tenant knowledge base solution that processes documents on demand. The key components include: 
 
 Vector-based knowledge base ‚Äì Uses Amazon Bedrock and Amazon OpenSearch Serverless for efficient document processing and querying 
 On-demand document ingestion ‚Äì Implements just-in-time processing using the Amazon Bedrock CUSTOM data source type 
 TTL management ‚Äì Provides automatic cleanup of unused documents using the TTL feature in Amazon DynamoDB 
 Multi-tenant isolation ‚Äì Enforces secure data separation between users and organizations with configurable resource limits 
 
The solution enables granular control through metadata-based filtering at the user, tenant, and file level. The DynamoDB TTL tracking system supports tiered pricing structures, where tenants can pay for different TTL durations, document ingestion limits, and query rates. 
The following diagram illustrates the key components and workflow of the solution. 
 
The workflow consists of the following steps: 
 
 The user logs in to the system, which attaches a tenant ID to the current user for calls to the Amazon Bedrock knowledge base. This authentication step is crucial because it establishes the security context and makes sure subsequent interactions are properly associated with the correct tenant. The tenant ID becomes the foundational piece of metadata that enables proper multi-tenant isolation and resource management throughout the entire workflow. 
 After authentication, the user creates a project that will serve as a container for the files they want to query. This project creation step establishes the organizational structure needed to manage related documents together. The system generates appropriate metadata and creates the necessary database entries to track the project‚Äôs association with the specific tenant, enabling proper access control and resource management at the project level. 
 With a project established, the user can begin uploading files. The system manages this process by generating pre-signed URLs for secure file upload. As files are uploaded, they are stored in Amazon Simple Storage Service (Amazon S3), and the system automatically creates entries in DynamoDB that associate each file with both the project and the tenant. This three-way relationship (file-project-tenant) is essential for maintaining proper data isolation and enabling efficient querying later. 
 When a user requests to create a chat with a knowledge base for a specific project, the system begins ingesting the project files using the CUSTOM data source. This is where the just-in-time processing begins. During ingestion, the system applies a TTL value based on the tenant‚Äôs tier-specific TTL interval. The TTL makes sure project files remain available during the chat session while setting up the framework for automatic cleanup later. This step represents the core of the on-demand processing strategy, because files are only processed when they are needed. 
 Each chat session actively updates the TTL for the project files being used. This dynamic TTL management makes sure frequently accessed files remain in the knowledge base while allowing rarely used files to expire naturally. The system continually refreshes the TTL values based on actual usage, creating an efficient balance between resource availability and cost optimization. This approach maintains optimal performance for actively used content while helping to prevent resource waste on unused documents. 
 After the chat session ends and the TTL value expires, the system automatically removes files from the knowledge base. This cleanup process is triggered by Amazon DynamoDB Streams monitoring TTL expiration events, which activate an AWS Lambda function to remove the expired documents. This final step reduces the load on the underlying OpenSearch Serverless cluster and optimizes system resources, making sure the knowledge base remains lean and efficient. 
 
Prerequisites 
You need the following prerequisites before you can proceed with solution. For this post, we use the us-east-1 AWS Region. 
 
 An active AWS account with permissions to create resources in us-east-1 
 The AWS Command Line Interface (AWS CLI) installed 
 The AWS Cloud Development Kit (AWS CDK) installed 
 Git installed to clone the repository 
 
Deploy the solution 
Complete the following steps to deploy the solution: 
 
 Download the AWS CDK project from the GitHub repo. 
 Install the project dependencies: 
   
   npm run install:all 
    
 Deploy the solution: 
   
   npm run deploy 
    
 Create a user and log in to the system after validating your email. 
 
Validate the knowledge base and run a query 
Before allowing users to chat with their documents, the system performs the following steps: 
 
 Performs a validation check to determine if documents need to be ingested. This process happens transparently to the user and includes checking document status in DynamoDB and the knowledge base. 
 Validates that the required documents are successfully ingested and properly indexed before allowing queries. 
 Returns both the AI-generated answers and relevant citations to source documents, maintaining traceability and empowering users to verify the accuracy of responses. 
 
The following screenshot illustrates an example of chatting with the documents. 
 
Looking at the following example method for file ingestion, note how file information is stored in DynamoDB with a TTL value for automatic expiration. The ingest knowledge base documents call includes essential metadata (user ID, tenant ID, and project), enabling precise filtering of this tenant‚Äôs files in subsequent operations. 
 
 # Ingesting files with tenant-specific TTL values
def ingest_files(user_id, tenant_id, project_id, files):
&nbsp;&nbsp; &nbsp;# Get tenant configuration and calculate TTL
&nbsp;&nbsp; &nbsp;tenants = json.loads(os.environ.get('TENANTS'))['Tenants']
&nbsp;&nbsp; &nbsp;tenant = find_tenant(tenant_id, tenants)
&nbsp;&nbsp; &nbsp;ttl = int(time.time()) + (int(tenant['FilesTTLHours']) * 3600)
&nbsp;&nbsp; &nbsp;
&nbsp;&nbsp; &nbsp;# For each file, create a record with TTL and start ingestion
&nbsp;&nbsp; &nbsp;for file in files:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;file_id = file['id']
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;s3_key = file.get('s3Key')
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;bucket = file.get('bucket')
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;# Create a record in the knowledge base files table with TTL
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;knowledge_base_files_table.put_item(
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Item={
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;'id': file_id,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;'userId': user_id,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;'tenantId': tenant_id,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;'projectId': project_id,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;'documentStatus': 'ready',
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;'createdAt': int(time.time()),
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;'ttl': ttl &nbsp;# TTL value for automatic expiration
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;)
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;# Start the ingestion job with tenant, user, and project metadata for filtering
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;bedrock_agent.ingest_knowledge_base_documents(
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;knowledgeBaseId=KNOWLEDGE_BASE_ID,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;dataSourceId=DATA_SOURCE_ID,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;clientToken=str(uuid.uuid4()),
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;documents=[
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;{
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;'content': {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;'dataSourceType': 'CUSTOM',
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;'custom': {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;'customDocumentIdentifier': {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;'id': file_id
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;},
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;'s3Location': {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;'uri': f"s3://{bucket}/{s3_key}"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;},
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;'sourceType': 'S3_LOCATION'
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;},
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;'metadata': {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;'type': 'IN_LINE_ATTRIBUTE',
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;'inlineAttributes': [
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;{'key': 'userId', 'value': {'stringValue': user_id, 'type': 'STRING'}},
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;{'key': 'tenantId', 'value': {'stringValue': tenant_id, 'type': 'STRING'}},
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;{'key': 'projectId', 'value': {'stringValue': project_id, 'type': 'STRING'}},
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;{'key': 'fileId', 'value': {'stringValue': file_id, 'type': 'STRING'}}
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;]
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;]
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;) 
 
During a query, you can use the associated metadata to construct parameters that make sure you only retrieve files belonging to this specific tenant. For example: 
 
 &nbsp;&nbsp; &nbsp;filter_expression = {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"andAll": [
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;{
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"equals": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"key": "tenantId",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"value": tenant_id
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;},
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;{
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"equals": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"key": "projectId",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"value": project_id
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;},
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;{
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"in": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"key": "fileId",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"value": file_ids
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;]
&nbsp;&nbsp; &nbsp;}

&nbsp;&nbsp; &nbsp;# Create base parameters for the API call
&nbsp;&nbsp; &nbsp;retrieve_params = {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;'input': {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;'text': query
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;},
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;'retrieveAndGenerateConfiguration': {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;'type': 'KNOWLEDGE_BASE',
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;'knowledgeBaseConfiguration': {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;'knowledgeBaseId': knowledge_base_id,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.nova-pro-v1:0',
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;'retrievalConfiguration': {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;'vectorSearchConfiguration': {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;'numberOfResults': limit,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;'filter': filter_expression
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp;}
&nbsp;&nbsp;&nbsp;&nbsp;response&nbsp;=&nbsp;bedrock_agent_runtime.retrieve_and_generate(**retrieve_params) 
 
Manage the document lifecycle with TTL 
To further optimize resource usage and costs, you can implement an intelligent document lifecycle management system using the DynamoDB (TTL) feature. This consists of the following steps: 
 
 When a document is ingested into the knowledge base, a record is created with a configurable TTL value. 
 This TTL is refreshed when the document is accessed. 
 DynamoDB Streams with specific filters for TTL expiration events is used to trigger a cleanup Lambda function. 
 The Lambda function removes expired documents from the knowledge base. 
 
See the following code: 
 
 # Lambda function triggered by DynamoDB Streams when TTL expires items
def lambda_handler(event, context):
&nbsp;&nbsp; &nbsp;"""
&nbsp;&nbsp; &nbsp;This function is triggered by DynamoDB Streams when TTL expires items.
&nbsp;&nbsp; &nbsp;It removes expired documents from the knowledge base.
&nbsp;&nbsp; &nbsp;"""
&nbsp;&nbsp; &nbsp;
&nbsp;&nbsp; &nbsp;# Process each record in the event
&nbsp;&nbsp; &nbsp;for record in event.get('Records', []):
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;# Check if this is a TTL expiration event (REMOVE event from DynamoDB Stream)
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;if record.get('eventName') == 'REMOVE':
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# Check if this is a TTL expiration
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;user_identity = record.get('userIdentity', {})
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;if user_identity.get('type') == 'Service' and user_identity.get('principalId') == 'dynamodb.amazonaws.com':
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# Extract the file ID and tenant ID from the record
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;keys = record.get('dynamodb', {}).get('Keys', {})
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;file_id = keys.get('id', {}).get('S')
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# Delete the document from the knowledge base
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;bedrock_agent.delete_knowledge_base_documents(
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;clientToken=str(uuid.uuid4()),
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;knowledgeBaseId=knowledge_base_id,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;dataSourceId=data_source_id,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;documentIdentifiers=[
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;{
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;'custom': {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;'id': file_id
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;},
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;'dataSourceType': 'CUSTOM'
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;]
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;) 
 
Multi-tenant isolation with tiered service levels 
Our architecture enables sophisticated multi-tenant isolation with tiered service levels: 
 
 Tenant-specific document filtering ‚Äì Each query includes user, tenant, and file-specific filters, allowing the system to reduce the number of documents being queried. 
 Configurable TTL values ‚Äì Different tenant tiers can have different TTL configurations. For example: 
   
   Free tier: Five documents ingested with a 7-day TTL and five queries per minute. 
   Standard tier: 100 documents ingested with a 30-day TTL and 10 queries per minute. 
   Premium tier: 1,000 documents ingested with a 90-day TTL and 50 queries per minute. 
   You can configure additional limits, such as total queries per month or total ingested files per day or month. 
    
 
Clean up 
To clean up the resources created in this post, run the following command from the same location where you performed the deploy step: 
 
 npm run destroy 
 
Conclusion 
The just-in-time knowledge base architecture presented in this post transforms document management across multiple tenants by processing documents only when queried, reducing the unused consumption of traditional RAG systems. This serverless implementation uses Amazon Bedrock, OpenSearch Serverless, and the DynamoDB TTL feature to create a lean system with intelligent document lifecycle management, configurable tenant limits, and strict data isolation, which is essential for SaaS providers offering tiered pricing models. 
This solution directly addresses cost structure and infrastructure limitations of traditional systems, particularly for deployments handling numerous small to mid-sized tenants with transitory projects. This architecture combines on-demand document processing with automated lifecycle management, delivering a cost-effective, scalable resource that empowers organizations to focus on extracting insights rather than managing infrastructure, while maintaining security boundaries between tenants. 
Ready to implement this architecture? The full sample code is available in the GitHub repository. 
 
About the author 
 Steven Warwick is a Senior Solutions Architect at AWS, where he leads customer engagements to drive successful cloud adoption and specializes in SaaS architectures and Generative AI solutions. He produces educational content including blog posts and sample code to help customers implement best practices, and has led programs on GenAI topics for solution architects. Steven brings decades of technology experience to his role, helping customers with architectural reviews, cost optimization, and proof-of-concept development.
‚Ä¢ Agents as escalators: Real-time AI video monitoring with Amazon Bedrock Agents and video streams
  Organizations deploying video monitoring systems face a critical challenge: processing continuous video streams while maintaining accurate situational awareness. Traditional monitoring approaches that use rule-based detection or basic computer vision frequently miss important events or generate excessive false positives, leading to operational inefficiencies and alert fatigue. 
In this post, we show how to build a fully deployable solution that processes video streams using OpenCV, Amazon Bedrock for contextual scene understanding and automated responses through Amazon Bedrock Agents. This solution extends the capabilities demonstrated in Automate chatbot for document and data retrieval using Amazon Bedrock Agents and Knowledge Bases, which discussed using Amazon Bedrock Agents for document and data retrieval. In this post, we apply Amazon Bedrock Agents to real-time video analysis and event monitoring. 
Benefits of using Amazon Bedrock Agents for video monitoring 
The following figure shows example video stream inputs from different monitoring scenarios. With contextual scene understanding, users can search for specific events. 
 
A front door camera will capture many events throughout the day, but some are more interesting than others‚Äîhaving context if a package is being delivered or removed (as in the following package example) limits alerts to urgent events. 
 
Amazon Bedrock is a fully managed service that provides access to high-performing foundation models (FMs) from leading AI companies through a single API. Using Amazon Bedrock, you can build secure, responsible generative AI applications. Amazon Bedrock Agents extends these capabilities by enabling applications to execute multi-step tasks across systems and data sources, making it ideal for complex monitoring scenarios. The solution processes video streams through these key steps: 
 
 Extract frames when motion is detected from live video streams or local files. 
 Analyze context using multimodal FMs. 
 Make decisions using agent-based logic with configurable responses. 
 Maintain searchable semantic memory of events. 
 
You can build this intelligent video monitoring system using Amazon Bedrock Agents and Amazon Bedrock Knowledge Bases in an automated solution. The complete code is available in the GitHub repo. 
Limitations of current video monitoring systems 
Organizations deploying video monitoring systems face a fundamental dilemma. Despite advances in camera technology and storage capabilities, the intelligence layer interpreting video feeds often remains rudimentary. This creates a challenging situation where security teams must make significant trade-offs in their monitoring approach. Current video monitoring solutions typically force organizations to choose between the following: 
 
 Simple rules that scale but generate excessive false positives 
 Complex rules that require ongoing maintenance and customization 
 Manual monitoring that relies on human attention and doesn‚Äôt scale 
 Point solutions that only handle specific scenarios but lack flexibility 
 
These trade-offs create fundamental barriers to effective video monitoring that impact security, safety, and operational efficiency across industries. Based on our work with customers, we‚Äôve identified three critical challenges that emerge from these limitations: 
 
 Alert fatigue ‚Äì Traditional motion detection and object recognition systems generate alerts for any detected change or recognized object. Security teams quickly become overwhelmed by the volume of notifications for normal activities. This leads to reduced attention when genuinely critical events occur, diminishing security effectiveness and increasing operational costs from constant human verification of false alarms. 
 Limited contextual understanding ‚Äì Rule-based systems fundamentally struggle with nuanced scene interpretation. Even sophisticated traditional systems operate with limited understanding of the environments they monitor due to a lack of contextual awareness, because they can‚Äôt easily do the following: 
   
   Distinguish normal from suspicious behavior 
   Understand temporal patterns like recurring weekly events 
   Consider environmental context such as time of day or location 
   Correlate multiple events that might indicate a pattern 
    
 Lack of semantic memory ‚Äì Conventional systems lack the ability to build and use knowledge over time. They can‚Äôt do the following: 
   
   Establish baselines of routine versus unusual events 
   Offer natural language search capabilities across historical data 
   Support reasoning about emerging patterns 
    
 
Without these capabilities, you can‚Äôt gain cumulative benefits from your monitoring infrastructure or perform sophisticated retrospective analysis. To address these challenges effectively, you need a fundamentally different approach. By combining the contextual understanding capabilities of FMs with a structured framework for event classification and response, you can build more intelligent monitoring systems. Amazon Bedrock Agents provides the ideal platform for this next-generation approach. 
Solution overview 
You can address these monitoring challenges by building a video monitoring solution with Amazon Bedrock Agents. The system intelligently screens events, filters routine activity, and escalates situations requiring human attention, helping reduce alert fatigue while improving detection accuracy. The solution uses Amazon Bedrock Agents to analyze detected motion from video, and alerts users when an event of interest happens according to the provided instructions. This allows the system to intelligently filter out trivial events that can trigger motion detection, such as wind or birds, and direct the user‚Äôs attention only to events of interest. The following diagram illustrates the solution architecture. 
 
The solution uses three primary components to address the core challenges: agents as escalators, a video processing pipeline, and Amazon Bedrock Agents. We discuss these components in more detail in the following sections. 
The solution uses the AWS Cloud Development Kit (AWS CDK) to deploy the solution components. The AWS CDK is an open source software development framework for defining cloud infrastructure as code and provisioning it through AWS CloudFormation. 
Agent as escalators 
The first component uses Amazon Bedrock Agents to examine detected motion events with the following capabilities: 
 
 Provides natural language understanding of scenes and activities for contextual interpretation 
 Maintains temporal awareness across frame sequences to understand event progression 
 References historical patterns to distinguish unusual from routine events 
 Applies contextual reasoning about behaviors, considering factors like time of day, location, and action sequences 
 
We implement a graduated response framework that categorizes events by severity and required action: 
 
 Level 0: Log only ‚Äì The system logs normal or expected activities. For example, when a delivery person arrives during business hours or a recognized vehicle enters the driveway, these events are documented for pattern analysis and future reference but require no immediate action. They remain searchable in the event history. 
 Level 1: Human notification ‚Äì This level handles unusual but non-critical events that warrant human attention. An unrecognized vehicle parked nearby, an unexpected visitor, or unusual movement patterns trigger a notification to security personnel. These events require human verification and assessment. 
 Level 2: Immediate response ‚Äì Reserved for critical security events. Unauthorized access attempts, detection of smoke or fire, or suspicious behavior trigger automatic response actions through API calls. The system notifies personnel through SMS or email alerts with event information and context. 
 
The solution provides an interactive processing and monitoring interface through a Streamlit application. With the Streamlit UI, users can provide instructions and interact with the agent. 
 
The application consists of the following key features: 
 
 Live stream or video file input ‚Äì The application accepts M3U8 stream URLs from webcams or security feeds, or local video files in common formats (MP4, AVI). Both are processed using the same motion detection pipeline that saves triggered events to Amazon Simple Storage Service (Amazon S3) for agent analysis. 
 Custom instructions ‚Äì Users can provide specific monitoring guidance, such as ‚ÄúAlert me about unknown individuals near the loading dock after hours‚Äù or ‚ÄúFocus on vehicle activity in the parking area.‚Äù These instructions adjust how the agent interprets detected motion events. 
 Notification configuration ‚Äì Users can specify contact information for different alert levels. The system uses Amazon Simple Notification Service (Amazon SNS) to send emails or text messages based on event severity, so different personnel can be notified for potential issues vs. critical situations. 
 Natural language queries about past events ‚Äì The interface includes a chat component for historical event retrieval. Users can ask ‚ÄúWhat vehicles have been in the driveway this week?‚Äù or ‚ÄúShow me any suspicious activity from last night,‚Äù receiving responses based on the system‚Äôs event memory. 
 
Video processing pipeline 
The solution uses several AWS services to capture and prepare video data for analysis through a modular processing pipeline. The solution supports multiple types of video sources: 
 
 Live video streams (M3U8 format) 
 Local video files (mp4, avi, and so on) 
 Amazon Kinesis Video Streams (Kinesis video URL) 
 Local camera 
 
When using streams, OpenCV‚Äôs VideoCapture component handles the connection and frame extraction. For testing, we‚Äôve included sample event videos demonstrating different scenarios. The core of the video processing is a modular pipeline implemented in Python. Key components include: 
 
 SimpleMotionDetection ‚Äì Identifies movement in the video feed 
 FrameSampling ‚Äì Captures sequences of frames over time when motion is detected 
 GridAggregator ‚Äì Organizes multiple frames into a visual grid for context 
 S3Storage ‚Äì Stores captured frame sequences in Amazon S3 
 
This multi-process framework optimizes performance by running components concurrently and maintaining a queue of frames to process. The video processing pipeline organizes captured frame data in a structured way before passing it to the Amazon Bedrock agent for analysis: 
 
 Frame sequence storage ‚Äì When motion is detected, the system captures a sequence of frames over 10 seconds. These frames are stored in Amazon S3 using a timestamp-based path structure (YYYYMMDD-HHMMSS) that allows for efficient retrieval by date and time. In the case where motions exceed 10 seconds, multiple events are created. 
 Image grid format ‚Äì Rather than processing individual frames separately, the system arranges multiple sequential frames into a grid format (typically 3√ó4 or 4√ó5). This presentation provides temporal context and is sent to the Amazon Bedrock agent for analysis. The grid format enables understanding of how motion progresses over time, which is critical for accurate scene interpretation. 
 
The following figure is an example of an image grid sent to the agent. Package theft is difficult to identify with classic image models. The large language model‚Äôs (LLM‚Äôs) ability to reason over a sequence of image allows it to make observations about intent. 
 
The video processing pipeline‚Äôs output‚Äîtimestamped frame grids stored in Amazon S3‚Äîserves as the input for the Amazon Bedrock agent components, which we discuss in the next section. 
Amazon Bedrock agent components 
The solution integrates multiple Amazon Bedrock services to create an intelligent analysis system: 
 
 Core agent architecture ‚Äì The agent orchestrates these key workflows: 
   
   Receives frame grids from Amazon S3 on motion detection 
   Coordinates multi-step analysis processes 
   Makes classification decisions 
   Triggers appropriate response actions 
   Maintains event context and state 
    
 Knowledge management ‚Äì The solution uses Amazon Bedrock Knowledge Bases with Amazon OpenSearch Serverless to: 
   
   Store and index historical events 
   Build baseline activity patterns 
   Enable natural language querying 
   Track temporal patterns 
   Support contextual analysis 
    
 Action groups ‚Äì The agent has access to several actions defined through API schemas: 
   
   Analyze grid ‚Äì Process incoming frame grids from Amazon S3 
   Alert ‚Äì Send notifications through Amazon SNS based on severity 
   Log ‚Äì Record event details for future reference 
   Search events by date ‚Äì Retrieve past events based on a date range 
   Look up vehicle (Text-to-SQL) ‚Äì Query the vehicle database for information 
    
 
For structured data queries, the system uses the FM‚Äôs ability to convert natural language to SQL. This enables the following: 
 
 Querying Amazon Athena tables containing event records 
 Retrieving information about registered vehicles 
 Generating reports from structured event data 
 
These components work together to create a comprehensive system that can analyze video content, maintain event history, and support both real-time alerting and retrospective analysis through natural language interaction. 
Video processing framework 
The video processing framework implements a multi-process architecture for handling video streams through composable processing chains. 
Modular pipeline architecture 
The framework uses a composition-based approach built around the FrameProcessor abstract base class. 
Processing components implement a consistent interface with a process(frame) method that takes a Frame and returns a potentially modified Frame: 
 
 ```
class FrameProcessor(ABC):
    @abstractmethod
    def process(self, frame: Frame) -&gt; Optional[Frame]: ...
``` 
 
The Frame class encapsulates the image data along with timestamps, indexes, and extensible metadata: 
 
 ```
@dataclass
class Frame:
    buffer: ndarray  # OpenCV image array
    timestamp: float
    index: float
    fps: float
    metadata: dict = field(default_factory=dict)
``` 
 
Customizable processing chains 
The architecture supports configuring multiple processing chains that can be connected in sequence. The solution uses two primary chains. The detection and analysis chain processes incoming video frames to identify events of interest: 
 
 ```
chain = FrameProcessorChain([
    SimpleMotionDetection(motion_threshold=10_000, frame_skip_size=1),
    FrameSampling(timedelta(milliseconds=250), threshold_time=timedelta(seconds=2)),
    GridAggregator(shape=(13, 3))
])
``` 
 
The storage and notification chain handles the storage of identified events and invocation of the agent: 
 
 ```
storage_chain = FrameProcessorChain([
    S3Storage(bucket_name=TARGET_S3_BUCKET, prefix=S3_PREFIX, s3_client_provider=s3_client_provider),
    LambdaProcessor(get_response=get_response, monitoring_instructions=config.monitoring_instructions)
])
``` 
 
You can modify these changes independently to add or replace components based on specific monitoring requirements. 
Component implementation 
The solution includes several processing components that demonstrate the framework‚Äôs capabilities. You can modify each processing step or add new ones. For example, for simple motion detection, we use a simple pixel difference, but you can refine the motion detection functionality as needed, or follow the format to implement other detection algorithms, such as object detection or scene segmentation. 
Additional components include the FrameSampling processor to control capture timing, GridAggregator to create visual frame grids, and storage processors that save event data and trigger agent analysis, and these can be customized and replaced as needed. For example: 
 
 Modify existing components ‚Äì Adjust thresholds or parameters to tune for specific environments 
 Create alternative storage backends ‚Äì Direct output to different storage services or databases 
 Implement preprocessing and postprocessing steps ‚Äì Add image enhancement, data filtering, or additional context generation 
 
Finally, the LambdaProcessor serves as the bridge to the Amazon Bedrock agent by invoking an AWS Lambda function that sends the information in a request to the deployed agent. From there, the Amazon Bedrock agent takes over and analyzes the event and takes action accordingly. 
Agent implementation 
After you deploy the solution, an Amazon Bedrock agent alias becomes available. This agent functions as an intelligent analysis layer, processing captured video events and executing appropriate actions based on its analysis. You can test the agent and view its reasoning trace directly on the Amazon Bedrock console, as shown in the following screenshot. 
 
This agent will lack some of the metadata supplied by the Streamlit application (such as current time) and might not give the same answers as the full application. 
Invocation flow 
The agent is invoked through a Lambda function that handles the request-response cycle and manages session state. It finds the highest published version ID and uses it to invoke the agent and parses the response. 
Action groups 
The agent‚Äôs capabilities are defined through action groups implemented using the BedrockAgentResolver framework. This approach automatically generates the OpenAPI schema required by the agent. 
When the agent is invoked, it receives an event object that includes the API path and other parameters that inform the agent framework how to route the request to the appropriate handler based. You can add new actions by defining additional endpoint handlers following the same pattern and generating a new OpenAPI schema: 
 
 ```
if __name__ == "__main__":
    print(app.get_openapi_json_schema())
``` 
 
Text-to-SQL integration 
Through its action group, the agent is able to translate natural language queries into SQL for structured data analysis. The system reads data from assets/data_query_data_source, which can include various formats like CSV, JSON, ORC, or Parquet. 
This capability enables users to query structured data using natural language. As demonstrated in the following example, the system translates natural language queries about vehicles into SQL, returning structured information from the database. 
 
The database connection is configured through a SQL Alchemy engine. Users can connect to existing databases by updating the create_sql_engine() function to use their connection parameters. 
Event memory and semantic search 
The agent maintains a detailed memory of past events, storing event logs with rich descriptions in Amazon S3. These events become searchable through both vector-based semantic search and date-based filtering. As shown in the following example, temporal queries make it possible to retrieve information about events within specific time periods, such as vehicles observed in the past 72 hours. 
 
The system‚Äôs semantic memory capabilities enable queries based on abstract concepts and natural language descriptions. As shown in the following example, the agent can understand abstract concepts like ‚Äúfunny‚Äù and retrieve relevant events, such as a person dropping a birthday cake. 
 
Events can be linked together by the agent to identify patterns or related incidents. For example, the system can correlate separate sightings of individuals with similar characteristics. In the following screenshots, the agent connects related incidents by identifying common attributes like clothing items across different events. 
 
 
This event memory store allows the system to build knowledge over time, providing increasingly valuable insights as it accumulates data. The combination of structured database querying and semantic search across event descriptions creates an agent with a searchable memory of all past events. 
Prerequisites 
Before you deploy the solution, complete the following prerequisites: 
 
 Configure AWS credentials using aws configure. Use either the us-west-2 or us-east-1 AWS Region. 
 Enable access to Anthropic‚Äôs Claude 3.x models, or another supported Amazon Bedrock Agents model you want to use. 
 Make sure you have the following dependencies: 
   
   Python 
   AWS CDK 
   AWS Command Line Interface (AWS CLI) 
   Git 
   Node.js 
   Docker 
    
 
Deploy the solution 
The AWS CDK deployment creates the following resources: 
 
 Storage ‚Äì S3 buckets for assets and query results 
 Amazon Bedrock resources ‚Äì Agent and knowledge base 
 Compute ‚Äì Lambda functions for actions, invocation, and updates 
 Database ‚Äì Athena database for structured queries, and an AWS Glue crawler for data discovery 
 
Deploy the solution with the following commands: 
 
 ```
#1. Clone the repository and navigate to folder
git clone https://github.com/aws-samples/sample-video-monitoring-agent.git &amp;&amp; cd sample-video-monitoring-agent
#2. Set up environment and install dependencies
python3 -m venv .venv &amp;&amp; source .venv/bin/activate &amp;&amp; pip install -r requirements.txt
#3. Deploy AWS resources
cdk bootstrap &amp;&amp; cdk deploy
#4. Run the streamlit app
cd code/streamlit_app &amp;&amp; streamlit run app.py
``` 
 
On Windows, replace the second line with the following code: 
 
 ```
python3 -m venv .venv &amp;&amp; % .venv\Scripts\activate.bat &amp;&amp; pip install -r requirements.txt
``` 
 
Clean up 
To destroy the resources you created and stop incurring charges, run the following command: 
 
 ```
cdk destroy
``` 
 
Future enhancements 
The current implementation demonstrates the potential of agent-based video monitoring in a home security setting, but there are many potential applications. 
Sample Use Cases 
The following showcases the application of the solution to various scenarios. 
Small business 
 

 { ‚Äúalert_level‚Äù: 0, ‚Äútimestamp‚Äù: ‚Äú2024-11-20T15:24:15Z‚Äù, ‚Äúreason‚Äù: ‚ÄúVehicle arrival in driveway‚Äù, ‚Äúdescription‚Äù: ‚ÄùStandard vehicle arrival and parking sequence. Vehicles present: Black Nissan Frontier pickup (parked), silver Honda CR-V (arriving), and partial view of blue vehicle in foreground. Area features: Gravel driveway surface, two waste bins (County Waste and recycling), evergreen trees in background. Sequence shows Honda CR-V executing normal parking maneuver: approaches from east, performs standard three-point turn, achieves final position next to pickup truck. Daytime conditions, clear visibility. Vehicle condition: Clean, well-maintained CR-V appears to be 2012-2016 model year, no visible damage or unusual modifications. Movement pattern indicates familiar driver performing routine parking. No suspicious behavior or safety concerns observed. Timestamp indicates standard afternoon arrival time. Waste bins properly positioned and undisturbed during parking maneuver.‚Äù }
 
Industrial 
 

 { ‚Äúalert_level‚Äù: 2, ‚Äútimestamp‚Äù: ‚Äú2024-11-20T15:24:15Z‚Äù, ‚Äúreason‚Äù: ‚ÄúWarehouse product spill/safety hazard‚Äù,‚Äùdescription‚Äù: ‚ÄùSignificant product spill incident in warehouse storage aisle. Location: Main warehouse aisle between high-bay racking systems containing boxed inventory. Sequence shows what appears to be liquid or container spill, likely water/beverage products based on blue colored containers visible. Infrastructure: Professional warehouse setup with multi-level blue metal racking, concrete flooring, overhead lighting. Incident progression: Initial frames show clean aisle, followed by product falling/tumbling, resulting in widespread dispersal of items across aisle floor. Hazard assessment: Creates immediate slip/trip hazard, blocks emergency egress path, potential damage to inventory. Area impact: Approximately 15-20 feet of aisle space affected. Facility type appears to be distribution center or storage warehouse. Multiple cardboard boxes visible on surrounding shelves potentially at risk from liquid damage.‚Äù }
 
Backyard 
 
 

 { ‚Äúalert_level‚Äù: 1, ‚Äútimestamp‚Äù: ‚Äú2024-11-20T15:24:15Z‚Äù, ‚Äúreason‚Äù: ‚ÄúWildlife detected on property‚Äù, ‚Äúdescription‚Äù: ‚ÄùAdult raccoon observed investigating porch/deck area with white railings. Night vision/IR camera provides clear footage of animal. Subject animal characteristics: medium-sized adult raccoon, distinctive facial markings clearly visible, healthy coat condition, normal movement patterns. Sequence shows animal approaching camera (15:42PM), investigating area near railing (15:43-15:44PM), with close facial examination (15:45PM). Final frame shows partial view as animal moves away. Environment: Location appears to be elevated deck/porch with white painted wooden railings and balusters. Lighting conditions: Nighttime, camera operating in infrared/night vision mode providing clear black and white footage. Animal behavior appears to be normal nocturnal exploration, no signs of aggression or disease.‚Äù }
 
Home safety 
 

 { ‚Äúalert_level‚Äù: 2, ‚Äútimestamp‚Äù: ‚Äú2024-11-20T15:24:15Z‚Äù, ‚Äúreason‚Äù: ‚ÄúSmoke/possible fire detected‚Äù, ‚Äúdescription‚Äù: ‚ÄùRapid development of white/grey smoke visible in living room area. Smoke appears to be originating from left side of frame, possibly near electronics/TV area. Room features: red/salmon colored walls, grey couch, illuminated aquarium, table lamps, framed artwork. Sequence shows progressive smoke accumulation over 4-second span (15:42PM ‚Äì 15:46PM).Notable smoke density increase in upper left corner of frame with potential light diffusion indicating particulate matter in air. Smoke pattern suggests active fire development rather than residual smoke. Blue light from aquarium remains visible throughout sequence providing contrast reference for smoke density.‚Äù
 
Further extensions 
In addition, you can extend the FM capabilities using the following methods: 
 
 Fine-tuning for specific monitoring contexts ‚Äì Adapting the models to recognize domain-specific objects, behaviors, and scenarios 
 Refined prompts for specific use cases ‚Äì Creating specialized instructions that optimize the agent‚Äôs performance for particular environments like industrial facilities, retail spaces, or residential settings 
 
You can expand the agent‚Äôs ability to take action, for example: 
 
 Direct control of smart home and smart building systems ‚Äì Integrating with Internet of Things (IoT) device APIs to control lights, locks, or alarm systems 
 Integration with security and safety protocols ‚Äì Connecting to existing security infrastructure to follow established procedures 
 Automated response workflows ‚Äì Creating multi-step action sequences that can be triggered by specific events 
 
You can also consider enhancing the event memory system: 
 
 Long-term pattern recognition ‚Äì Identifying recurring patterns over extended time periods 
 Cross-camera correlation ‚Äì Linking observations from multiple cameras to track movement through a space 
 Anomaly detection based on historical patterns ‚Äì Automatically identifying deviations from established baselines 
 
Lastly, consider extending the monitoring capabilities beyond fixed cameras: 
 
 Monitoring for robotic vision systems ‚Äì Applying the same intelligence to mobile robots that patrol or inspect areas 
 Drone-based surveillance ‚Äì Processing aerial footage for comprehensive site monitoring 
 Mobile security applications ‚Äì Extending the platform to process feeds from security personnel body cameras or mobile devices 
 
These enhancements can transform the system from a passive monitoring tool into an active participant in security operations, with increasingly sophisticated understanding of normal patterns and anomalous events. 
Conclusion 
The approach of using agents as escalators represents a significant advancement in video monitoring, using the contextual understanding capabilities of FMs with the action-oriented framework of Amazon Bedrock Agents. By filtering the signal from the noise, this solution addresses the critical problem of alert fatigue while enhancing security and safety monitoring capabilities.With this solution, you can: 
 
 Reduce false positives while maintaining high detection sensitivity 
 Provide human-readable descriptions and classifications of events 
 Maintain searchable records of all activity 
 Scale monitoring capabilities without proportional human resources 
 
The combination of intelligent screening, graduated responses, and semantic memory enables a more effective and efficient monitoring system that enhances human capabilities rather than replacing them. Try the solution today and experience how Amazon Bedrock Agents can transform your video monitoring capabilities from simple motion detection to intelligent scene understanding. 
 
About the authors 
 Kiowa Jackson is a Senior Machine Learning Engineer at AWS ProServe, specializing in computer vision and agentic systems for industrial applications. His work bridges classical machine learning approaches with generative AI to enhance industrial automation capabilities. His past work includes collaborations with Amazon Robotics, NFL, and Koch Georgia Pacific. 
Piotr Chotkowski is a Senior Cloud Application Architect at AWS Generative AI Innovation Center. He has experience in hands-on software engineering as well as software architecture design. In his role at AWS, he helps customers design and build production grade generative AI applications in the cloud.

‚∏ª