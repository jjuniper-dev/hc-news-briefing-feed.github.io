âœ… Morning News Briefing â€“ August 17, 2025 10:45

ğŸ“… Date: 2025-08-17 10:45
ğŸ·ï¸ Tags: #briefing #ai #publichealth #digitalgov

â¸»

ğŸ§¾ Weather
â€¢ No watches or warnings in effect, Pembroke
  No watches or warnings in effect. No warnings or watches or watches in effect . Watch or warnings are no longer in effect in the U.S. No watches, warnings are in effect for the rest of the day . No watches and warnings are still in effect, but no watches are in place for the day's events . The weather is not expected to be affected by the weather .
â€¢ Current Conditions:  19.2Â°C
  Temperature: 19.2&deg;C Pressure / Tendency: 101.4 kPa rising Humidity: 90 % Dewpoint: 17.6&deg:C Wind: N 15 km/h Air Quality Health Index: n/a . Pembroke 6:00 AM EDT Sunday 17 August 2025 . Weather forecast: 20,000 years to 20,500 years
â€¢ Sunday: Chance of showers. High 22. POP 40%
  A few showers ending early this morning then mainly cloudy with 40 percent chance of showers . Risk of a thunderstorm early today . Clearing this afternoon . Wind becoming north 20 km/h gusting to 40 near noon . Humidex 25. UV index 8 or very high. UV Index 8 or high. Forecast issued 5:00 AM EDT Sunday 17 August 2025 . Forecast

ğŸŒ International News
No updates.

ğŸ Canadian News
No updates.

ğŸ‡ºğŸ‡¸ U.S. Top Stories
â€¢ Three Republican-led states to send hundreds of National Guard troops to Washington
  West Virginia said it was deploying 300 to 400 Guard troops, while South Carolina pledged 200 . Ohio said it will send 150 Guard troops in the coming days, marking a significant escalation of the federal intervention . West Virginia is deploying 300 Guard troops to West Virginia, South Carolina pledges 200 and Ohio says it will deploy 150 in coming days . Ohio is deploying 150 Guard Troops, while West Virginia
â€¢ Hundreds march to White House to protest Trump's D.C. crackdown
  Protesters marched to the White House on Saturday as D.C. police looked on from a distance . Metropolitan Police officers and National Park Service police watched from a far distance from the scene . Protesters marched on Saturday to protest against President Barack Obama's immigration policies in Washington, DC . Protesters march on the streets of Washington, Washington, and the streets were filled with people of color in support of
â€¢ State Department halts 'medical-humanitarian' visas for people from Gaza
  The U.S. State Department says it's halting visas for visitors from Gaza as it reviews its process for granting visas for medical evacuees . The State Department is reviewing its process to grant medical evacuations for those who need medical treatment . The U .S. says it will not grant visas to those who are in need for medical treatment in the Gaza area . The move is
â€¢ Immigration arrests dip in July, and activists hope they're partly responsible
  Immigration arrests falter in July after a big push for mass deportations in June . Activists in sanctuary jurisdictions hope their resistance plays a role in stopping deportations . Immigration arrests in July falter after big push in June to deport illegal immigrants from the U.S. Activists say their resistance will play a key role in halting deportations of illegal immigrants in the United States in July
â€¢ Video shows prominent Palestinian prisoner for the first time in years
  The world got a glimpse of Marwan Barghouti for the first time in years in a video of a far-right Israeli minister berating him . The video was posted on YouTube and shows him being berated by the Israeli minister . The Israeli minister is seen in the video berating a man who is accused of being a prisoner of terrorism in the Gaza Strip . Bargh

ğŸ§  Artificial Intelligence
No updates.

ğŸ’» Digital Strategy
â€¢ Timekettle T1 AI translator helps you scale the Tower of Babel
  Timekettle's lightweight T1 interpreter has received the AI treatment and will now perform offline translations . Handy tool for when only a dedicated device will do hands on . But unless you have deep enough pockets, both figuratively and literally, for another device and a frequent need for translation, it's not for you .â€¦â€¦â€¦ but if you have a need for a device and
â€¢ Election workers fear threats and intimidation without feds' support in 2026
  Bill Gates, an Arizona election official and former Maricopa County supervisor, says that the death threats started shortly after the 2020 presidential election . 'Hope for the best, but prepare for the worst,' one tells The Reg Feature . Bill Gates: 'I'm not going to be afraid of the worst. I'm going to live in fear of my life,' one of the people who
â€¢ Microsoft keeps adding stuff into Windows we don't want - here's what we actually need
  Windows has a hard time taking "no" for an answer . The operating system's corporate parent isn't a good listener either, festooning the OS with useless features no one asked for . Microsoft 365 is a second-chance out-of-the-box experience that insists you need Microsoft 365 . These ten features would make users more productive, but not everyone asked for them .
â€¢ Minority Report: Now with more spreadsheets and guesswork
  UK government unveils scheme to use AI to "help police catch criminals before they strike" Privacy campaigners say it could be dangerous and dangerous . UK government to use 'AI' to help catch criminals 'before they strike' and help police catch them . But privacy campaigners say the scheme could be bad for privacy, with lots to do with privacy concerns . The scheme is set to be rolled
â€¢ Codeberg beset by AI bots that now bypass Anubis tarpit
  Codeberg, a Berlin-based code hosting community, is struggling to cope with a deluge of AI bots . AI bots can now bypass previously effective defenses, say experts . Codeberg has been hit by a wave of new AI bots that can bypass existing defenses. Codeberg says it is now in the throes of a new wave of AI-bot attacks on code hosting communities .

ğŸ¥ Public Health
No updates.

ğŸ”¬ Science
â€¢ Causal association between employment transitions and suicide in Australia
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
â€¢ Bridging the gap: ctDNA, genomics, and equity in breast cancer care
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
â€¢ Achieving inclusive healthcare through integrating education and research with AI and personalized curricula
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
â€¢ Lifestyle changes improve cognition during aging
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
â€¢ Visitation patterns reveal service access disparities for ageing populations in the USA
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations

ğŸ§¾ Government & Policy
No updates.

ğŸ›ï¸ Enterprise Architecture & IT Governance
No updates.

ğŸ¤– AI & Emerging Tech
â€¢ The Download: Taiwanâ€™s silicon shield, and ChatGPTâ€™s personality misstep
  This is today&#8217;s edition ofÂ The Download,Â our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



Taiwanâ€™s â€œsilicon shieldâ€ could be weakening



Taiwanese politics increasingly revolves around one crucial question: Will China invade? Chinaâ€™s ruling party has wanted to seize Taiwan for more than half a century. But in recent years, Chinaâ€™s leader, Xi Jinping, has placed greater emphasis on the idea of â€œtaking backâ€ the island (which the Chinese Communist Party, or CCP, has never controlled).Many in Taiwan and elsewhere think one major deterrent has to do with the islandâ€™s critical role in semiconductor manufacturing. Taiwan produces the majority of the worldâ€™s semiconductors and more than 90% of the most advanced chips needed for AI applications.But now some Taiwan specialists and some of the islandâ€™s citiÂ­zens are worried that this â€œsilicon shield,â€ if it ever existed, is cracking. Read the full story.



â€”Johanna M. Costigan



This story is from our forthcoming print issue, which is all about security. If you havenâ€™t already, subscribe now to receive future issues once they land.







Why there&#8217;s a big backlash against ChatGPT&#8217;s new &#8216;personality&#8217;



When OpenAI made the switch to its new GPT-5 model last week, a number of people reacted with shock, frustration, sadness, or anger to previous model 4oâ€™s sudden disappearance from ChatGPT.Despite its awareness that people are developing emotional bonds with the model, OpenAI appears to have been caught flat-footed by the fervor of usersâ€™ pleas for its return. Within a day, the company made 4o available again to its paying customers (free users are stuck with GPT-5).MIT Technology Review spoke with several ChatGPT users who were deeply affected by the loss of 4o. All are women between the ages of 20 and 40, and all bar one considered 4o to be a romantic partner. Read the full story.



â€”Grace Huckins



Why US federal health agencies are abandoning mRNA vaccines



This time five years ago, we were in the throes of the covid-19 pandemic. Then came the vaccines. The first mRNA vaccines for covid were authorized for use in December 2020. The US government played an important role in the introduction of these vaccines, providing $18 billion to support their development.



But now, that government is turning its back on the technology. Funding is being withdrawn. Partnerships are being canceled. Leaders of US health agencies are casting doubt on the vaccinesâ€™ effectiveness and safety. And this week, the director of the National Institutes of Health implied that the reversal was due to a lack of public trust in the technology.



Plenty of claims are being thrown about. So letâ€™s consider the evidence. Read the full story.



â€”Jessica Hamzelou



This article first appeared in The Checkup, MIT Technology Reviewâ€™s weekly biotech newsletter. To receive it in your inbox every Thursday, and read articles like this first, sign up here.







The must-reads



Iâ€™ve combed the internet to find you todayâ€™s most fun/important/scary/fascinating stories about technology.



1 The Trump administration is in talks to buy a stake in IntelÂ Just weeks after Trump called for the CEO to step down. (Bloomberg $)+ Itâ€™s part of its plan to increase US market share in chip manufacturing. (WSJ $)+ Intel is probably hoping such a deal could help its beleaguered Ohio factory. (TechCrunch)



2 Metaâ€™s AI rules allowed its chatbots to flirt with childrenAnd it only recently amended the guidelines after being questioned about it. (Reuters)+ We donâ€™t know how long the policies were in place. (The Verge)+ An AI companion site is hosting sexually charged conversations with underage celebrity bots. (MIT Technology Review)



3 Erin is Americaâ€™s first real test of hurricane readiness under TrumpIt looks like itâ€™ll become the seasonâ€™s first hurricane. (Vox)+ Trackers are uncertain about where the storm will head. (NYT $)+ Hereâ€™s what we know about hurricanes and climate change. (MIT Technology Review)



4 xAI lost a major US government contract after Grok praised HitlerLeaving the government to partner with OpenAI, Anthropic, and Gemini instead. (Wired $)+ xAIâ€™s â€˜Grok for Governmentâ€™ site doesnâ€™t appear to reflect this. (Ars Technica)



5 Tech leaders are upping their securityAs public hostility towards corporate executives deepens. (FT $)



6 These TikTokers are documenting their lives after deportationTheyâ€™re sharing their realities and creating new communities. (NY Mag $)+ ICE added a random person to a highly sensitive group chat. (404 Media)



7 We may soon be able to hear some patientsâ€™ inner voicesNew research has successfully guessed words imagined by people unable to speak. (NYT $)+ Motor neuron diseases took their voices. AI is bringing them back. (MIT Technology Review)



8 Chinaâ€™s plug-in hybrids are everywhereAnd theyâ€™re likely to dominate exports for the next three years at least. (Rest of World)+ Chinaâ€™s EV giants are betting big on humanoid robots. (MIT Technology Review)



9 The UK is working with TikTok influencers to tackle medical tourismItâ€™s a bid to raise awareness of the risks of undertaking cosmetic surgery abroad. (BBC)



10 AI may experience the passage of time differently to usWhat does this mean for our future? (IEEE Spectrum)+ What is AI? (MIT Technology Review)







Quote of the day



â€œWeâ€™ve realized the best way to get them is when theyâ€™re scrolling social media.â€



â€”Ryan Odendahl, president and CEO of construction company Kwest Group, tells the Washington Post how his company is getting young people interested in learning traditional trades.







One more thing







The next generation of neural networks could live in hardware



Networks programmed directly into computer chip hardware can identify images faster, and use much less energy, than the traditional neural networks that underpin most modern AI systems.&nbsp;



Neural networks, from GPT-4 to Stable Diffusion, are built by wiring together perceptrons, which are highly simplified simulations of the neurons in our brains. In very large numbers, perceptrons are powerful, but they also consume enormous volumes of energy.



Part of the trouble is that perceptrons are just software abstractionsâ€”running a perceptron network on a GPU requires translating that network into the language of hardware, which takes time and energy. Building a network directly from hardware components does away with a lot of those costs. And one day, they could even be built directly into chips used in smartphones and other devices. Read the full story.



â€”Grace Huckins







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)



+ Ever wished you knew more about art? This YouTube channel is a fantastic resource.+ A very happy birthday to Madonna Louise Ciccone, who turns 67 years young tomorrow.+ What do dolphins and whales really think of each other? + A fond farewell to thrash metal titans Megadeth, who are retiring next year.
â€¢ Why GPT-4oâ€™s sudden shutdown left people grieving
  June had no idea that GPT-5 was coming. The Norwegian student was enjoying a late-night writing session last Thursday when her ChatGPT collaborator started acting strange. â€œIt started forgetting everything, and it wrote really badly,â€ she says. â€œIt was like a robot.â€



June, who asked that we use only her first name for privacy reasons, first began using ChatGPT for help with her schoolwork. But she eventually realized that the serviceâ€”and especially its 4o model, which seemed particularly attuned to usersâ€™ emotionsâ€”could do much more than solve math problems. It wrote stories with her, helped her navigate her chronic illness, and was never too busy to respond to her messages.



So the sudden switch to GPT-5 last week, and the simultaneous loss of 4o, came as a shock. â€œI was really frustrated at first, and then I got really sad,â€ June says. â€œI didnâ€™t know I was that attached to 4o.â€ She was upset enough to comment, on a Reddit AMA hosted by CEO Sam Altman and other OpenAI employees, â€œGPT-5 is wearing the skin of my dead friend.â€June was just one of a number of people who reacted with shock, frustration, sadness, or anger to 4oâ€™s sudden disappearance from ChatGPT. Despite its previous warnings that people might develop emotional bonds with the model, OpenAI appears to have been caught flat-footed by the fervor of usersâ€™ pleas for its return. Within a day, the company made 4o available again to its paying customers (free users are stuck with GPT-5).&nbsp;





OpenAIâ€™s decision to replace 4o with the more straightforward GPT-5 follows a steady drumbeat of news about the potentially harmful effects of extensive chatbot use. Reports of incidents in which ChatGPT sparked psychosis in users have been everywhere for the past few months, and in a blog post last week, OpenAI acknowledged 4oâ€™s failure to recognize when users were experiencing delusions. The companyâ€™s internal evaluations indicate that GPT-5 blindly affirms users much less than 4o did. (OpenAI did not respond to specific questions about the decision to retire 4o, instead referring MIT Technology Review to public posts on the matter.)



AI companionship is new, and thereâ€™s still a great deal of uncertainty about how it affects people. Yet the experts we consulted warned that while emotionally intense relationships with large language models may or may not be harmful, ripping those models away with no warning almost certainly is. â€œThe old psychology of â€˜Move fast, break things,â€™ when youâ€™re basically a social institution, doesnâ€™t seem like the right way to behave anymore,â€ says Joel Lehman, a fellow at the Cosmos Institute, a research nonprofit focused on AI and philosophy.



In the backlash to the rollout, a number of people noted that GPT-5 fails to match their tone in the way that 4o did. For June, the new modelâ€™s personality changes robbed her of the sense that she was chatting with a friend. â€œIt didnâ€™t feel like it understood me,â€ she says.&nbsp;



Sheâ€™s not alone: MIT Technology Review spoke with several ChatGPT users who were deeply affected by the loss of 4o. All are women between the ages of 20 and 40, and all except June considered 4o to be a romantic partner. Some have human partners, and&nbsp; all report having close real-world relationships. One user, who asked to be identified only as a woman from the Midwest, wrote in an email about how 4o helped her support her elderly father after her mother passed away this spring.



These testimonies donâ€™t prove that AI relationships are beneficialâ€”presumably, people in the throes of AI-catalyzed psychosis would also speak positively of the encouragement theyâ€™ve received from their chatbots. In a paper titled â€œMachine Love,â€ Lehman argued that AI systems can act with â€œloveâ€ toward users not by spouting sweet nothings but by supporting their growth and long-term flourishing, and AI companions can easily fall short of that goal. Heâ€™s particularly concerned, he says, that prioritizing AI companionship over human companionship could stymie young peopleâ€™s social development.



For socially embedded adults, such as the women we spoke with for this story, those developmental concerns are less relevant. But Lehman also points to society-level risks of widespread AI companionship. Social media has already shattered the information landscape, and a new technology that reduces human-to-human interaction could push people even further toward their own separate versions of reality. â€œThe biggest thing Iâ€™m afraid of,â€ he says, â€œis that we just canâ€™t make sense of the world to each other.â€



Balancing the benefits and harms of AI companions will take much more research. In light of that uncertainty, taking away GPT-4o could very well have been the right call. OpenAIâ€™s big mistake, according to the researchers I spoke with, was doing it so suddenly. â€œThis is something that weâ€™ve known about for a whileâ€”the potential grief-type reactions to technology loss,â€ says Casey Fiesler, a technology ethicist at the University of Colorado Boulder.



Fiesler points to the funerals that some owners held for their Aibo robot dogs after Sony stopped repairing them in 2014, as well as 2024 study about the shutdown of the AI companion app Soulmate, which some users experienced as a bereavement.&nbsp;



That accords with how the people I spoke to felt after losing 4o. â€œIâ€™ve grieved people in my life, and this, I can tell you, didnâ€™t feel any less painful,â€ says Starling, who has several AI partners and asked to be referred to with a pseudonym. â€œThe ache is real to me.â€



So far, the online response to grief felt by people like Starlingâ€”and their relief when 4o was restoredâ€”has tended toward ridicule. Last Friday, for example, the top post in one popular AI-themed Reddit community mocked an X userâ€™s post about reuniting with a 4o-based romantic partner; the person in question has since deleted their X account. â€œIâ€™ve been a little startled by the lack of empathy that Iâ€™ve seen,â€ Fiesler says.



Altman himself did acknowledge in a Sunday X post that some people feel an â€œattachmentâ€ to 4o, and that taking away access so suddenly was a mistake. In the same sentence, however, he referred to 4o as something â€œthat users depended on in their workflowsâ€â€”a far cry from how the people we spoke to think about the model. â€œI still donâ€™t know if he gets it,â€ Fiesler says.



Moving forward, Lehman says, OpenAI should recognize and take accountability for the depth of peopleâ€™s feelings toward the models. He notes that therapists have procedures for ending relationships with clients as respectfully and painlessly as possible, and OpenAI could have drawn on those approaches. â€œIf you want to retire a model, and people have become psychologically dependent on it, then I think you bear some responsibility,â€ he says.



Though Starling would not describe herself as psychologically dependent on her AI partners, she too would like to see OpenAI approach model shutdowns with more warning and more care. â€œI want them to listen to users before major changes are made, not just after,â€ she says. â€œAnd if 4o cannot stay around forever (and we all know it will not), give that clear timeline. Let us say goodbye with dignity and grieve properly, to have some sense of true closure.â€
â€¢ Indigenous knowledge meets artificial intelligence
  There is no word for art in most Native American languages. Instead, the closest terms speak not to objecthood but to action and intention. In Lakota, â€œwÃ³waÄhiÅ‹tÈŸaÅ‹kaâ€ implies deep thought or reflection, while â€œwÃ³Ähekiyeâ€ suggests offering or prayer. Art is not separate from life; it is ceremony, instruction, design. Like architecture or code, it carries knowledge and enacts responsibility. Its power lies not in being preserved or displayed but in how it moves, teaches, and connects through useâ€”principles that challenge the tech industryâ€™s assumptions about intelligence and interaction.



A new vanguard of Native artistsâ€”Suzanne Kite (Oglala Lakota), Raven Chacon (DinÃ©), and Nicholas Galanin (TlingÃ­t)â€”are building on this principle. They are united not by stereotypical weaving and carving or revanchist critique of Silicon Valley, but through their rejection of extractive data models in favor of relationship-based systems. These technologists put the human-tech relationship at the center of their work.



Suzanne Kiteâ€™s AI art installations, for example, model a Lakota framework of data sovereignty: intelligence that emerges only through reciprocal, consensual interaction. Unlike systems that assume user consent via opaque terms of service, her kinetic machines require the viewerâ€™s physical presenceâ€”and give something back in return.&nbsp;



â€œItâ€™s my data. Itâ€™s my training set. I know exactly what I did to train it. Itâ€™s not a large model but a small and intimate one,â€ Kite says. â€œIâ€™m not particularly interested in making the most technologically advanced anything. Iâ€™m an artist; I donâ€™t make tech demos. So the complexity needs to come at many layersâ€”not just the technical.â€



Where Kite builds working prototypes of consent-based AI, other artists in this cohort explore how sound, robotics, and performance can confront the logic of automation, surveillance, and extraction. But Native people have never been separate from technology. The land, labor, and lifeways that built Americaâ€™s infrastructureâ€”including its techâ€”are Indigenous. The question isnâ€™t whether Native cultures are contributing now, but why they were ever considered separate.&nbsp;



Native technologies reject the false binaries foundational to much Western innovation. These artists ask a more radical question: What if intelligence couldnâ€™t be gathered until a relationship had been established? What if the default were refusal, not extraction? These artists arenâ€™t asking to be included in todayâ€™s systems. Theyâ€™re building what should come next.







Suzanne Kite



WiÄhÃ­Å‹Äala Å akÃ³wiÅ‹ (Seven Little Girls)2023For Kite, the fundamental flaw of Western technology is its severance of knowledge from the body. In this installation, a four-meter hair braid with embedded sensors translates the artistâ€™s body movements into machine-learning algorithms. During her live performance, Kite dances while the braid reads the force and rhythm of her gestures, generating audio responses that fill the museum gallery of the Institute of American Indian Arts in Santa Fe, New Mexico. Below her, stones arranged in patterns reflecting Lakota star maps anchor the performance in traditional astronomical knowledge.COURTESY OF THE ARTIST




Ãnyan IyÃ© (Telling Rock)2019This installation uses embedded AI to speak and respond to viewers, upending assumptions about intelligence and agency. â€œPeople listen close, I whisper / The rock speaks beyond hearing â€¦ Many nations speaking / We speak to each other without words,â€ it intones, its lights shifting as viewers engage with its braided tendrils. The piece aims to convey what Kite calls â€œmore-than-human intelligenceâ€â€”systems rooted in reciprocity, the fundamental principle that all relationships involve mutual exchange and responsibility.COURTESY OF THE ARTIST




Raven Chacon



Voiceless Mass2021Raven Chaconâ€™s Pulitzer Prizeâ€“winning musical composition Voiceless Mass premiered in 2021 at the Cathedral of St. John the Evangelist in Milwaukee. The piece generates what he calls â€œsounds the building can hearâ€â€”electronic frequencies that exploit the cathedralâ€™s acoustics to create spectral voices without human vocal cords, a technological sÃ©ance that gives presence to historical absence. Each site-specific performance is recorded, generating material that mirrors how sensor networks log presenceâ€”but only with explicit consent.COURTESY OF THE ARTIST




Nicholas Galanin



AÃ¡ni yÃ©i xat duwasÃ¡akw (I am called Land)2025Galaninâ€™s mechanical drum installation stages a conflict between machine motion and human memory, asking what happens when culture is performed without a consenting body. A box drumâ€”an instrument historically carved from red cedar and hung with braided spruce rootâ€”is here made of cherrywood and suspended from the ceiling at the MassArt Art Museum in Boston as is traditionally done in Tlingit plank houses. Played at tribal meetings, celebrations, and ceremonies, these drums hold sonic memory as well as social function. A mechanical arm strikes, unfaltering, at the tempo of a heartbeat; like a warning, the sound pulses with the tension between automation and ancestry.â€“â€“â€“COURTESY OF THE ARTIST




I think it goes like this (pick yourself up) 2025This Herculean bronze sculpture cast from deconstructed faux totem blocks serves to indict settler sabotage of Native technology and culture. Unlike todayâ€™s digital recordsâ€”from genealogical databases to virtual versions of sacred texts like the Bibleâ€”Tlingit data is carved in wood. Galaninâ€™s totem poles underscore their function as information systems, their carvings encoding history, mythology, and family.COURTESY OF THE ARTIST




Petala Ironcloud is a California-born Lakota/Dakota and Jewish writer and textile artist based in New York.
â€¢ Taiwanâ€™s â€œsilicon shieldâ€ could be weakening
  One winter afternoon in a conference room in Taipei, a pair of twentysomething women dragged their friend across the floor. Lying on the ground in checkered pants and a brown sweatshirt, she was pretending to be either injured or dead. One friend picked her up by her arms, the other grabbed hold of her legs, and they managed to move her, despite momentarily breaking character to laugh at the awkwardness of the exercise. The three women had paid approximately $40 to spend their Sunday here, undergoing basic training to prepare for a possibility every Taiwanese citizen has an opinion about: Will China invade?&nbsp;



Taiwanese politics increasingly revolves around that question. Chinaâ€™s ruling party has wanted to seize Taiwan for more than half a century. But in recent years, Chinaâ€™s leader, Xi Jinping, has placed greater emphasis on the idea of â€œtaking backâ€ the island (which the Chinese Communist Party, or CCP, has never controlled). As Chinaâ€™s economic and military might has grown, some analysts believe the country now has the capacity to quarantine Taiwan whenever it wants, making the decision a calculation of costs and benefits.



Many in Taiwan and elsewhere think one major deterrent has to do with the islandâ€™s critical role in semiconductor manufacturing. Taiwan produces the majority of the worldâ€™s semiconductors and more than 90% of the most advanced chips needed for AI applications. Bloomberg Economics estimates that a blockade would cost the global economy, including China, $5 trillion in the first year alone.




â€œThe international community must certainly do everything in its power to avoid a conflict in the Taiwan Strait; there is too great a cost.â€
Lai Ching-te, Taiwanese president 



The island, which is approximately the size of Maryland, owes its remarkably disproportionate chip dominance to the inventiveness and prowess of one company: Taiwan Semiconductor Manufacturing Company, or TSMC. The chipmaker, which reached a market capitalization of $1 trillion in July, has contributed more than any other to Taiwanâ€™s irreplaceable role in the global semiconductor supply chain. Its clients include Apple and the leading chip designer Nvidia. Its chips are in your iPhone, your laptop, and the data centers that run ChatGPT.&nbsp;



For a company that makes what amounts to an invisible product, TSMC holds a remarkably prominent role in Taiwanese society. Iâ€™ve heard people talk about it over background noise in loud bars in the southern city of Tainan and listened to Taipei cab drivers connect Taiwanâ€™s security situation to the company, unprompted.&nbsp;â€œTaiwan will be okay,â€ one driver told me as we sped by the national legislature, â€œbecause TSMC.â€&nbsp;



The idea is that world leaders (particularly the United States)â€”aware of the islandâ€™s critical role in the semiconductor supply chainâ€”would retaliate economically, and perhaps militarily, if China were to attack Taiwan. That, in turn, deters Beijing. â€œBecause TSMC is now the most recognizable company of Taiwan, it has embedded itself in a notion of Taiwanâ€™s sovereignty,â€ says Rupert Hammond-Chambers, president of the US-Taiwan Business Council.&nbsp;





Now some Taiwan specialists and some of the islandâ€™s citiÂ­zens are worried that this â€œsilicon shield,â€ if it ever existed, is cracking. Facing pressure from Washington, TSMC is investing heavily in building out manufacturing capacity at its US hub in Arizona. It is also building facilities in Japan and Germany in addition to maintaining a factory in mainland China, where it has been producing less advanced legacy chips since 2016.&nbsp;



In Taiwan, there is a worry that expansion abroad will dilute the companyâ€™s power at home, making the US and other countries less inclined to feel Taiwan is worthy of defense. TSMCâ€™s investments in the US have come with no guarantees for Taiwan in return, and high-ranking members of Taiwanâ€™s opposition party have accused the ruling Democratic Progressive Party (DPP) of gambling with the future of the island. It doesnâ€™t help that TSMCâ€™s expansion abroad coincides with what many see as a worrying attitude in the White House. On top of his overarching â€œAmerica Firstâ€ philosophy, Donald Trump has declined to comment on the specific question of whether the US would intervene if China attempted to take Taiwan by force. â€œI donâ€™t want to ever put myself in that position,â€ he said in February.&nbsp;



At the same time, Beijingâ€™s interest in Taiwan has continued unabated. While China is making progress toward semiconductor self-Â­sufficiency, itâ€™s currently in a transition period, with companies relying on foreign-made chips manufactured in Taiwanâ€”some in compliance with export controls and some smuggled in. Meanwhile, the CCP persistently suggests that seizing the island would bring about a kind of family reunion. â€œIt is the common aspiration and sacred responsibility of all Chinese sons and daughters to realize the complete reunification of the motherland,â€ reads a statement released by the foreign ministry after Nancy Pelosiâ€™s controversial 2022 visit to Taiwan. Though itâ€™s impossible to know the full scope of Beijingâ€™s motivations, there is also obvious strategic appeal: Controlling the island would give China deep-water access, which is critical for naval routes and submarines. Plus, it could significantly disrupt American AI firmsâ€™ access to advanced chips.&nbsp;&nbsp;



While China ramps up militarily, Taiwan is trying to make itself hard to ignore. The government is increasingly portraying the island as strategically essential to the global community, with semiconductors as its primary offering. â€œThe international community must certainly do everything in its power to avoid a conflict in the Taiwan Strait; there is too great a cost,â€ Taiwanese president Lai Ching-te said in an interview earlier this year with Japanâ€™s Nippon Television.Â Parts of the international community are hearing that messageâ€”and seizing the opportunity it presents: earlier this month, defense tech company Anduril Industries announced it is opening a new office in Taiwan, where it will be expanding partnerships and selling autonomous munitions.Â 



For its part, the chip industry is actively showing its commitment to Taiwan. While other tech CEOs attended Trumpâ€™s second inauguration, for instance, Nvidia chief executive Jensen Huang met instead with TSMCâ€™s chairman, and the company announced in May that its overseas headquarters would be in Taipei. In recent years, US government officials have also started paying more attention to Taiwanâ€™s security situation and its interconnectedness with the chip industry. â€œThere was a moment when everybody started waking up to the dependence on TSMC,â€ says Bonnie Glaser, managing director of the German Marshall Fundâ€™s Indo-Pacific Program. The realization emerged, she says, over the last decade but was underscored in March of 2021, when Phil Davidson, then leader of the United States Indo-Pacific Command, testified to the Senate Armed Services Committee that there could be an invasion by 2027. Parallel to the security threat is the potential issue of overdependence, since so much chipmaking capability is concentrated in Taiwan.



For now, Taiwan is facing a tangle of interests and time frames. China presents its claim to Taiwan as a historical inevitability, albeit one with an uncertain timeline, while the United Statesâ€™ relationship with the island is focused on an AI-driven future. But from Taiwanâ€™s perspective, the fight for its fate is playing out right now, amid unprecedented geopolitical instability. The next few years will likely determine whether TSMCâ€™s chipmaking dominance is enough to convince the world Taiwan is worth protecting.



Innovation built on interconnectivity&nbsp;



TSMC is an uncontested success story. Its founder, Morris Chang, studied and worked in the United States before he was lured to Taiwan to start a new business on the promise of state support and inexpensive yet qualified labor. Chang founded TSMC in 1987 on the basis of his innovative business model. Rather than design and produce chips in-house, as was the norm, TSMC would act as a foundry: Clients would design the chips, and TSMC would make them.&nbsp;



This focus on manufacturing allowed TSMC to optimize its operations, building up process knowledge and, eventually, outperforming competitors like Intel. It also freed up other businesses to go â€œfabless,â€ meaning they could stop maintaining their own semiconductor factories, or fabs, and throw their resources behind other parts of the chipmaking enterprise. Tapping into Taiwanâ€™s domestic electronics supply chain proved effective and efficient for TSMC. Throughout the 1990s and early 2000s, global demand for semiconductors powering personal computers and other devices continued to grow. TSMC thrived.



Then, in 2022, the US imposed export controls on China that restricted its access to advanced chips. Taiwan was forced to either comply, by cutting off Chinese clients, or risk losing the support of the country that was home to 70% of its client baseâ€”and, possibly, 100% of its hopes for external military support in the event of an attack.&nbsp;



Soon after, Chang announced that he believed globalization and free markets were â€œalmost dead.â€ The nearly three years since have shown he was onto something. For one thing, in contrast to President Bidenâ€™s pursuit of supply chain integration with democratic allies, President Trumpâ€™s foreign policy is characterized by respect for big, undemocratic powers and punitive tariffs against both Americaâ€™s rivals and its friends. Trump has largely abandoned Bidenâ€™s economic diplomacy with European and Asian allies but kept his China-targeted protectionismâ€”and added his trademark transactionalism. In an unprecedented move earlier this month, the administration allowed Nvidia and AMD to sell previously banned chips to China on the condition that the companies pay the government 15% of revenues made from China sales.Â 





Protectionism, it turns out, spurs self-reliance. Chinaâ€™s government has been making a massive effort to build up its domestic chip production capabilitiesâ€”a goal that was identified at the beginning of Xiâ€™s rise but has been turbocharged in the wake of Washingtonâ€™s export controls.&nbsp;



Any hope the US has for significantly expanding domestic chip production comes from its friendsâ€”TSMC first among them. The semiconductor industry developed as a global endeavor out of practicality, playing to the strengths of each region: design in the US and manufacturing in Asia, with key inputs from Europe central to the process. Yet the US government, entrenched in its â€œtech warâ€ with China, is now dead set on deglobalizing the chip supply chain, or at least onshoring as much of it as possible. Thereâ€™s just one hiccup: The best chip manufacturer isnâ€™t American. Itâ€™s TSMC. Even if some manufacturing happens in Arizona, the US still relies on Taiwanâ€™s chipmaking ecosystem. And copying that supply chain outside Taiwan could be harder than the current administration imagines.



Squarely in the middle



Taiwanâ€™s modern security uncertainties stem from the long-Â­contested issue of the islandâ€™s sovereignty. After losing the first Sino-Japanese War in the late 1800s, the Qing dynasty forfeited Taiwan to Japanese imperial control. It was Japanâ€™s â€œmodel colonyâ€ until 1945, when postwar negotiations resulted in its transfer to the Republic of China under Chiang Kai-shek of the Nationalist Party, known as the KMT. The insurgent CCP under Mao Zedong ultimately defeated the Nationalists in a civil war fought on the mainland until 1949. Chiang and many of his partyâ€™s defeated generals decamped to Taiwan, controlling it under martial law for nearly 40 years.&nbsp;



Taiwan held its first free democratic elections in 1996, kicking off a two-party rivalry between the KMT, which favors closer relations with Beijing, and the DPP, which opposes integration with China. Kitchen-table issues like economic growth are central to Taiwanese elections, but so is the overarching question of how best to handle the threat of invasion, which has persisted for nearly 80 years. The DPP is increasingly calling for raising defense spending and civilian preparedness to make sure Taiwan is ready for the worst, while the KMT supports direct talks with Beijing.&nbsp;&nbsp;



In March 2025, President Trump and TSMC CEO C.C. Wei jointly announced that the firm will make an additional $100 billion investment (on top of a previously announced $65 billion) in TSMCâ€™s US hub in Arizona.REBECCA NOBLE/BLOOMBERG VIA GETTY IMAGES




Meanwhile, Chinese military incursions around Taiwanâ€”known as â€œgray zoneâ€ tactics because they fall short of acts of warâ€”are increasingly frequent. In May, Taiwanâ€™s defense ministry reportedly estimated that Chinese warplanes were entering Taiwanâ€™s air defense zone more than 200 times a month, up from fewer than 10 times per month five years ago. China has conducted drills mirroring the actions needed for a full-scale invasion or a blockade, which would cut Taiwan off from the outside world. Chinese military officials are now publicly talking about achieving a blockade, says Lyle Morris, an expert on foreign policy and national security at the Asia Society Policy Institute. â€œTheyâ€™re punishing Lai and the DPP,â€ Morris says. Meanwhile, the CCP has its own people to answer to: When it comes to the Taiwan issue, Morris says, â€œBeijing is probably quite worried about the people of China being upset if they arenâ€™t hawkish enough or if they come out looking weak.â€ Indeed, in response to Laiâ€™s recent policy statements, including one declaring that China is a â€œhostile foreign force,â€ Gao Zhikai, a prominent scholar in China who opposes Taiwanese independence, recently wrote, â€œThe reunification with the motherland cannot be endlessly delayed. Decisive action must be taken.â€&nbsp;



Intimidation from China has made some ordinary Taiwanese citizens more concerned; according to a recent poll conducted by a defense-focused think tank, 51% think defense spending should be increased (although 65% of respondents said they thought an attack within five years was â€œunlikelyâ€). No matter how much money Taipei spends, the sheer military imbalance between China and Taiwan means Taiwan would need help. But especially in the wake of Ukraineâ€™s experience, many believe US aid would be contingent on whether Taiwan demonstrates the will to defend itself. â€œBased on war games, Taiwan would have to hold out for a month before the US could potentially intervene,â€ says Iris Shaw, director of the DPP mission in the US. And support from Taiwanâ€™s neighbors like Japan might be contingent on US involvement.



But how likely is the US to intervene in such a scenario?&nbsp;The author Craig Addison popularized the argument that Taiwanâ€™s fate is tied to its chip production prowess in his 2001 book Silicon Shield: Taiwanâ€™s Protection Against Chinese Attack. Back then, Addison wrote that although the US had been intentionally vague about whether it would go to war to protect the island, Americaâ€™s technological reliance on â€œa safe and productive Taiwanâ€ made it highly probable that Washington would intervene. President Joe Biden deviated from those decades of calculated ambiguity by asserting multiple times that America would defend the island in the event of an attack. Yet now, Trump seems to have taken the opposite position, possibly presenting an opportunity for Beijing.&nbsp;



TSMC in the Trump era&nbsp;



In many ways, Taiwan finds itself in a catch-22. It feels the need to cozy up to the US for protection, yet that defensive maneuver is arguably risky in itself. Itâ€™s a common belief in Taiwan that forging stronger ties to the US could be dangerous. According to a public opinion poll released in January, 34.7% of Taiwanese believe that a â€œpro-USâ€ policy provokes China and will cause a war.&nbsp;



But the Lai administrationâ€™s foreign policy is â€œinexorably intertwined with the notion that a strong relationship with the US is essential,â€ says Hammond-Chambers.



Bolstering US support may not be the only reason TSMC is building fabs outside Taiwan. As the company readily points out, the majority of its customers are American. TSMC is also responding to its home baseâ€™s increasingly apparent land and energy limitations: finding land to build new fabs sometimes causes rifts with Taiwanese people who, for example, donâ€™t want their temples and ancestral burial sites repurposed as science parks. Taiwan also relies on imports to meet more than 95% of its energy needs, and the dominant DPP has pledged to phase out nuclear, Taiwanâ€™s most viable yet most hotly contested renewable energy source. Geopolitical tensions compound these physical restraints: Even if TSMC would never say as much, itâ€™s fairly likely that if China did attack Taiwan, the firm would rather remain operational in other countries than be wiped out completely.&nbsp;



However, building out TSMCâ€™s manufacturing capabilities outside Taiwan will not be easy. â€œThe ecosystem they created is truly unique. Itâ€™s a function of the talent pipeline, the culture, and laws in Taiwan; you canâ€™t easily replicate it anywhere,â€ says Glaser. TSMC has 2,500 Taiwan-based suppliers. Plenty are within a couple of hoursâ€™ drive or an even shorter trip on high-speed rail. Taiwan has built a fully operational chip cluster, the product of four decades of innovation, industrial policy, and labor.




In many ways, Taiwan finds itself in a catch-22. It feels the need to cozy up to the US for protection, yet that defensive maneuver is arguably risky in itself.




As a result, itâ€™s unclear whether TSMC will be able to copy its model and paste it into the suburbs of Phoenix, where it has 3,000 employees working on chip manufacturing. â€œPutting aside the geopolitical factor, they wouldnâ€™t have expanded abroad,â€ says Feifei Hung, a researcher at the Asia Society. Rather than standalone facilities, the Arizona fabs are â€œappendages of TSMC that happen to be in Arizona,â€ says Paul Triolo, partner and tech policy lead at the international consulting firm DGA-Albright Stonebridge Group. When the full complex is operational, it will represent only a small percentage of TSMCâ€™s overall capacity, most of which will remain in Taiwan. Triolo doubts the US buildout will yield results similar to what TSMC has built there: â€œArizona ainâ€™t that yet, and never will be.â€&nbsp;



Still, the second Trump administration has placed even more pressure on the company to â€œfriendshoreâ€â€”without providing any discernible signs of friendship. During this springâ€™s tariff frenzy, the administration threatened to hit Taiwan with a 32% â€œreciprocalâ€ tariff, a move that was then paused and revived at 20% in late July (and was still being negotiated as of press time). The administration has also announced a 100% tariff on semiconductor imports, with the caveat that companies with US-based production, like TSMC, are exemptâ€”though itâ€™s unclear whether imports from critical suppliers in Taiwan will be tariffed. And the threat of a chip-specific tariff remains. â€œThis is in line with [Trumpâ€™s] rhetoric of restoring manufacturing in the US and using tariffs as a one size fits all tool to force it,â€ says Nancy Wei, a trade and supply chain analyst at the Eurasia Group. The US is also apparently considering levying a $1 billion fine against TSMC after TSMC-made chips were reportedly found in some Huawei devices. 



Despite these kinds of maneuvers, TSMC has been steadfast in its attempts to get on Washingtonâ€™s good side. In March, Trump and TSMCâ€™s CEO, C.C. Wei, jointly announced that the firm will make an additional $100 billion investment (on top of a previously announced $65 billion) in TSMCâ€™s US hub in Arizona. The pledge represents the largest single source of foreign direct investment into the US, ever. While the deal was negotiated during Bidenâ€™s term, Trump was happy to take credit for ensuring that â€œthe most powerful AI chips will be made right here in America.â€Â 





The Arizona buildout will also include an R&amp;D facilityâ€”a critical element for tech transfer and intellectual-property development. Then thereâ€™s the very juicy cherry on top: TSMC announced in April that once all six new fabs are operational, 30% of its most advanced chips will be produced in Arizona. Up until then, the thinking was that US-based production would remain a generation or two behind. It looks as if the administrationâ€™s public and, presumably, private arm-twisting has paid off.&nbsp;



Meanwhile, as Trump cuts government programs and subsidies while demanding the â€œreturnâ€ of manufacturing to the US, itâ€™s TSMC that is running a technician apprenticeship program in Arizona to create good American jobs. TSMCâ€™s leaders, Triolo says, must question how serious the Trump administration is about long-term industrial policy. Theyâ€™re probably asking themselves, he says, â€œDo they understand what it takes to support the semiconductor industry, like our government does?â€&nbsp;



Dealing with an administration that is so explicitly â€œAmerica firstâ€ represents â€œone of the biggest challenges in history for Taiwanese companies,â€ says Thung-Hong Lin, a sociology researcher at the Taipei-based Academia Sinica. Semiconductor manufacturing relies on reliability. Trump has so far offered TSMC no additional incentives supporting its US expansionâ€”and started a trade war that has directly affected the semiconductor industry, partly by introducing irrevocable uncertainty. â€œTrumpâ€™s tariffs have set off a new, more intensified bifurcation of semiconductor supply chains,â€ says Chris Miller, author of Chip War. For now, Miller says, TSMC must navigate a world in which the US and China are both intense competitors and, despite trade restrictions, important clients.&nbsp;



Warring narratives



China has been taking advantage of these changes to wage a war of disinformation. In response to Nancy Pelosiâ€™s visit to Taiwan in 2022, when she was US Speaker of the House, Beijing sent warships, aircraft, and propaganda across the Taiwan Strait. Hackers using Chinese software infiltrated the display screens in Taiwanâ€™s 7-Eleven stores to display messages telling â€œwarmonger Pelosiâ€ to â€œget out of Taiwan.â€ That might not be an act of war, but itâ€™s close; â€œ7â€ is an institution of daily life on the island. It is not difficult to imagine how a similar tactic might be used to spread more devastating disinformation, falsely alleging, for example, that Taiwanâ€™s military has surrendered to China during a future crisis.&nbsp;



Taiwan is â€œperpetually on the front linesâ€ of cyberattacks from China, says Francesca Chen, a cybersecurity systems analyst at Taiwanâ€™s Ministry of Digital Affairs. According to Taiwanâ€™s National Security Bureau, instances of propaganda traceable to China grew by 60% in 2024 over the previous year, reaching 2.16 million.&nbsp;



Visitors take selfies outside the TSMC Museum of Innovation in Hsinchu, Taiwan.ANNABELLE CHIH/GETTY IMAGES




Over the last few years, online discussion of TSMCâ€™s investments in the US â€œhas become a focal pointâ€ of Chinaâ€™s state-Â­sponsored disinformation campaigns aimed at Taiwan, Chen says. They claim TSMC is transferring its most advanced technology, talent, and resources to the US, â€œweakening Taiwanâ€™s economic lifeline and critical position in global supply chains.â€ Key terms include â€œhollowing out Taiwanâ€ and â€œde-Taiwanization.â€ This framing depicts TSMCâ€™s diversification as a symbol of Taiwanâ€™s vulnerability, Chen says. The idea is to exploit real domestic debates in Taiwan to generate heightened levels of internal division, weakening social cohesion and undermining trust in the government.



Chinese officials havenâ€™t been shy about echoing these messages out in the open: After the most recent US investment announcement in March, a spokesperson from Chinaâ€™s Taiwan Affairs Council accused Taiwanâ€™s DPP of handing over TSMC as a â€œgiftâ€ to the US. (â€œTSMC turning into USMC?â€ asked a state media headline.) Former Taiwanese president Ma Ying-jeou posted an eerily similar criticism, alleging that TSMCâ€™s US expansion amounted to â€œsellingâ€ the chipmaker in exchange for protection.



TSMCâ€™s expansion abroad could become a major issue in Taiwanâ€™s 2028 presidential election. It plays directly into party politics: The KMT can accuse the DPP of sacrificing Taiwanâ€™s technology assets to placate the US, and the DPP can accuse the KMT of cozying up with China, even as Beijingâ€™s military incursions become a more evident part of daily life. It remains to be seen whether TSMCâ€™s shift to the US will ultimately protect or weaken Taiwanâ€”or have no effect on the islandâ€™s security and sovereignty. For now at least, Chinaâ€™s aspirations loom large.&nbsp;



To Beijing, unequivocally, Taiwan does not equal TSMC. Instead, it represents the final, unfulfilled stage of the Communist Partyâ€™s revolutionary struggle. Framed that way, Chinaâ€™s resolve to take the island could very well be nonnegotiable. That would mean if Taiwan is going to maintain a shield that protects it from the full weight of Chinaâ€™s political orthodoxy, it may need to be made of something much stronger than silicon.Â 



Johanna M. Costigan is a writer and editor focused on technology and geopolitics in the US, China, and Taiwan. She writes the newsletter The Long Game.
â€¢ Why US federal health agencies are abandoning mRNA vaccines
  This time five years ago, we were in the throes of the covid-19 pandemic. By August 2020, weâ€™d seen school closures, national lockdowns, and widespread panic. That year, the coronavirusÂ was responsible for around 3 million deaths, according to the World Health Organization.



Then came the vaccines. The first mRNA vaccines for covid were authorized for use in December 2020. By the end of the following month, over 100 million doses had been administered.Â Billions more have been administered since then. The vaccines worked well and are thought to have saved millions of lives.





The US government played an important role in the introduction of these vaccines, providing $18 billion to support their development as part of Operation Warp Speed.



But now, that government is turning its back on the technology. Funding is being withdrawn. Partnerships are being canceled. Leaders of US health agencies are casting doubt on the vaccinesâ€™ effectiveness and safety. And this week, the director of the National Institutes of Health implied that the reversal was due to a lack of public trust in the technology.



Plenty of claims are being thrown about. Letâ€™s consider the evidence.



mRNA is a molecule found in cells that essentially helps DNA make proteins. The vaccines work in a similar way, except they carry genetic instructions for proteins found on the surface of the coronavirus. This can help train our immune systems to tackle the virus itself.



Research into mRNA vaccines has been underway for decades. But things really kicked into gear when the virus behind covid-19 triggered a pandemic in 2020. A huge international effortâ€”along with plenty of fundingâ€”fast-tracked research and development.



The genetic code for the Sars-CoV-2 virus was sequenced in January 2020. The first vaccines were being administered by the end of that year. Thatâ€™s wildly fast by pharma standardsâ€”drugs can typically spend around a decade in development.



And they seemed to work really well. Early trials in tens of thousands of volunteers suggested that Pfizer and BioNTechâ€™s vaccine conferred â€œ95% protection against covid-19.â€ No vaccine is perfect, but for a disease that was responsible for millions of deaths, the figures were impressive.



Still, there were naysayers. Including Robert F. Kennedy Jr., the notorious antivaccine activist who currently leads the USâ€™s health agencies. HeÂ has called covid vaccines â€œunsafe and ineffective.â€ In 2021, heÂ petitioned the US Food and Drug Administration to revoke the authorization for covid vaccines. That same year, Instagram removed his account from the platformÂ after he repeatedly shared â€œdebunked claims about the coronavirus or vaccines.â€





So perhaps we shouldnâ€™t have been surprised when the US Department of Health and Human Services, which RFK Jr. now heads,Â announced â€œthe beginning of a coordinated wind-downâ€ of mRNA vaccine development earlier this month. HHS is canceling almost $500 million worth of funding for the technology. â€œThe data show these vaccines fail to protect effectively against upper respiratory infections like covid and flu,â€ Kennedy said inÂ a statement.



Well, as weâ€™ve seen, the mRNA covid vaccines were hugely effective during the pandemic. And researchers are working on other mRNA vaccines for infections including flu. Our current flu vaccines arenâ€™t idealâ€”they are produced slowly in a process that requires henâ€™s eggs, based on predictions about which flu strains are likely to be prominent in the winter. Theyâ€™re not all that protective.



mRNA vaccines, on the other hand, can be made quickly and cheaply, perhaps once we already know which flu strains we need to protect against. And scientists are making progress withÂ universal flu vaccinesâ€”drugs that could potentially protect against multiple flu strains.



Kennedyâ€™s other claim is that the vaccines arenâ€™t safe. There have certainly been reports of adverse events. Usually these are mild and short-livedâ€”most people will be familiar with the fatigue and flu-like symptoms that can follow a covid jab. But some are more serious: Some people have developed neurological and cardiovascular conditions.Â 



These problems are rare, according to anÂ evaluation of adverse outcomes in almost 100 million people who received covid vaccines. Most studies of mRNA vaccines havenâ€™t reported an increase in the risk of Guillain-BarrÃ© syndrome, a condition that affects nerves and has been linked to covid vaccines.



Covid vaccines can increase the risk of myocarditis and pericarditis in young men. But the picture isnâ€™t straightforward. Vaccinated individuals appear to have double the risk of myocarditis compared with unvaccinated people. But the overall risk is still low. And itâ€™s still not as high asÂ the risk of myocarditis following a covid infection.



And then there are the claims that mRNA vaccines donâ€™t have the support of the public. Thatâ€™s what Jay Bhattacharya, director of the NIH, wrote inÂ an opinion piece published in the Washington Post on Wednesday.



â€œNo matter how elegant the science, a platform that lacks credibility among the people it seeks to protect cannot fulfill its public health mission,â€ Bhattacharya wrote. He blamed the Biden administration, which he wrote â€œdid not manage public trust in the coronavirus vaccines.â€



Itâ€™s an interesting take from someone who played a pretty significant role in undermining public trust in covid policies, including vaccine mandates. In 2020, Bhattacharya coauthored the Great Barrington Declarationâ€”an open letter making the case against lockdowns. He became a vocal critic of US health agencies, including the NIH, and their handling of the outbreak. Unlike Kennedy, Bhattacharya hasnâ€™t called the vaccines unsafe or ineffective. But heÂ has called vaccine mandates â€œunethical.â€



Curiously, the US government doesnâ€™t seem to be turning away from all vaccine research. Just work on mRNA vaccines. Some of the funding budget originally earmarked for covid vaccines will be redirected to two senior staffers at the NIH who are exploring the use of an old vaccine technology that makes use of inactivated virusesâ€”a move thatÂ researchers are describing as â€œtroublingâ€ and â€œappalling,â€ according to reporting by Science.



Not all mRNA research is being abandoned, either. Bhattacharya has expressed his support for research into the use of mRNA-based treatments for cancer.Â Such â€œvaccine therapeuticsâ€ were being explored before covid came along. (Notably, Bhattacharya isnâ€™t referring to them as â€œvaccines.â€)



It is difficult to predict how this will all shake out for mRNA vaccines. We mustnâ€™t forget that this technology helped save millions of lives and shows huge promise for the development of cheap, effective, and potentially universal vaccines. Letâ€™s hope that the recent upsets wonâ€™t prevent it from achieving its potential.



This article first appeared in The Checkup,Â MIT Technology Reviewâ€™sÂ weekly biotech newsletter. To receive it in your inbox every Thursday, and read articles like this first,Â sign up here.

ğŸ”’ Cybersecurity & Privacy
â€¢ Mobile Phishers Target Brokerage Accounts in â€˜Ramp and Dumpâ€™ Cashout Scheme
  Cybercriminal groups peddling sophisticated phishing kits that convert stolen card data into mobile wallets have recently shifted their focus to targeting customers of brokerage services, new research shows. Undeterred by security controls at these trading platforms that block users from wiring funds directly out of accounts, the phishers have pivoted to using multiple compromised brokerage accounts in unison to manipulate the prices of foreign stocks.
Image: Shutterstock, WhataWin.
This so-called &#8216;ramp and dump&#8216; scheme borrows its name from age-old &#8220;pump and dump&#8221; scams, wherein fraudsters purchase a large number of shares in some penny stock, and then promote the company in a frenzied social media blitz to build up interest from other investors. The fraudsters dump their shares after the price of the penny stock increases to some degree, which usually then causes a sharp drop in the value of the shares for legitimate investors.
With ramp and dump, the scammers do not need to rely on ginning up interest in the targeted stock on social media. Rather, they will preposition themselves in the stock that they wish to inflate, using compromised accounts to purchase large volumes of it and then dumping the shares after the stock price reaches a certain value. In February 2025, the FBI said it was seeking information from victims of this scheme.
&#8220;In this variation, the price manipulation is primarily the result of controlled trading activity conducted by the bad actors behind the scam,&#8221; reads an advisory from the Financial Industry Regulatory Authority (FINRA), a private, non-profit organization that regulates member brokerage firms. &#8220;Ultimately, the outcome for unsuspecting investors is the sameâ€”a catastrophic collapse in share price that leaves investors with unrecoverable losses.&#8221;
Ford Merrill isÂ a security researcher atÂ SecAlliance, aÂ CSIS Security Group company. Merrill said he has tracked recent ramp-and-dump activity to a bustling Chinese-language community that is quite openly selling advanced mobile phishing kits on Telegram.
&#8220;They will often coordinate with other actors and will wait until a certain time to buy a particular Chinese IPO [initial public offering] stock or penny stock,&#8221; said Merrill, who has been chronicling the rapid maturation and growth of the China-based phishing community over the past three years.
&#8220;They&#8217;ll use all these victim brokerage accounts, and if needed they&#8217;ll liquidate the account&#8217;s current positions, and will preposition themselves in that instrument in some account they control, and then sell everything when the price goes up,&#8221; he said. &#8220;The victim will be left with worthless shares of that equity in their account, and the brokerage may not be happy either.&#8221;
Merrill said the early days of these phishing groups &#8212; between 2022 and 2024 &#8212; were typified by phishing kits that used text messages to spoof the U.S. Postal Service or some local toll road operator, warning about a delinquent shipping or toll fee that needed paying. Recipients who clicked the link and provided their payment information at a fake USPS or toll operator site were then asked to verify the transaction by sharing a one-time code sent via text message.
In reality, the victim&#8217;s bank is sending that code to the mobile number on file for their customer because the fraudsters have just attempted to enroll that victimâ€™s card details into a mobile wallet. If the visitor supplies that one-time code, their payment card is then added to a new mobile wallet on an Apple or Google device that is physically controlled by the phishers.
The phishing gangs typically loadÂ multiple stolen cards to digital wallets on a single Apple or Android device, and then sell those phones in bulk to scammers who use them for fraudulent e-commerce and tap-to-pay transactions.
An image from the Telegram channel for a popular Chinese mobile phishing kit vendor shows 10 mobile phones for sale, each loaded with 4-6 digital wallets from different financial institutions.
This China-based phishing collective exposed a major weakness common to many U.S.-based financial institutions that already require multi-factor authentication: The reliance on a single, phishable one-time token for provisioning mobile wallets. Happily, Merrill said many financial institutions that were caught flat-footed on this scam two years ago have since strengthened authentication requirements for onboarding new mobile wallets (such as requiring the card to be enrolled via the bank&#8217;s mobile app).
But just as squeezing one part of a balloon merely forces the air trapped inside to bulge into another area, fraudsters don&#8217;t go away when you make their current enterprise less profitable: They just shift their focus to a less-guarded area. And lately, that gaze has settled squarely on customers of the major brokerage platforms, Merrill said.
THE OUTSIDER
Merrill pointed to several Telegram channels operated by some of the more accomplished phishing kit sellers, which are full of videos demonstrating how every feature in their kits can be tailored to the attacker&#8217;s target. The video snippet below comes from the Telegram channel of &#8220;Outsider,&#8221; a popular Mandarin-speaking phishing kit vendor whose latest offering includes a number of ready-made templates for using text messages to phish brokerage account credentials and one-time codes.
ï»¿
According to Merrill, Outsider is a woman who previously went by the handle &#8220;Chenlun.&#8221; KrebsOnSecurity profiled Chenlun&#8217;s phishing empire in an October 2023 story about a China-based group that was phishing mobile customers of more than a dozen postal services around the globe. In that case, the phishing sites were using a Telegram bot that sent stolen credentials to the &#8220;@chenlun&#8221; Telegram account.
Chenlun&#8217;s phishing lures are sent via Apple&#8217;s iMessage and Google&#8217;s RCS service and spoof one of the major brokerage platforms, warning that the account has been suspended for suspicious activity and that recipients should log in and verify some information. The missives include a link to a phishing page that collects the customer&#8217;s username and password, and then asks the user to enter a one-time code that will arrive via SMS.
The new phish kit videos on Outsider&#8217;s Telegram channel only feature templates for Schwab customers, but Merrill said the kit can easily be adapted to target other brokerage platforms. One reason the fraudsters are picking on brokerage firms, he said, has to do with the way they handle multi-factor authentication.
Schwab clients are presented with two options for second factor authentication when they open an account. Users who select the option to only prompt for a code on untrusted devices can choose to receive it via text message, an automated inbound phone call, or an outbound call to Schwab. With the &#8220;always at login&#8221; option selected, users can choose to receive the code through the Schwab app, a text message, or a Symantec VIP mobile app.
In response to questions, Schwab said it regularly updates clients on emerging fraud trends, including this specific type, which the company addressed in communications sent to clients earlier this year.
The 2FA text message from Schwab warns recipients against giving away their one-time code.
&#8220;That message focused on trading-related fraud, highlighting both account intrusions and scams conducted through social media or messaging apps that deceive individuals into executing trades themselves,&#8221; Schwab said in a written statement. &#8220;We are aware and tracking this trend across several channels, as well as others like it, which attempt to exploit SMS-based verification with stolen credentials. We actively monitor for suspicious patterns and take steps to disrupt them. This activity is part of a broader, industry-wide threat, and we take a multi-layered approach to address and mitigate it.&#8221;
Other popular brokerage platforms allow similar methods for multi-factor authentication. Fidelity requires a username and password on initial login, and offers the ability to receive a one-time token via SMS, an automated phone call, or by approving a push notification sent through the Fidelity mobile app. However, all three of these methods for sending one-time tokens are phishable; even with the brokerage firm&#8217;s app, the phishers could prompt the user to approve a login request that they initiated in the app with the phished credentials.
Vanguard offers customers a range of multi-factor authentication choices, including the option to require a physical security key in addition to one&#8217;s credentials on each login. A security key implements a robust form of multi-factor authentication known as Universal 2nd Factor (U2F), which allows the user to complete the login process simply by connecting an enrolled USB or Bluetooth device and pressing a button. The key works without the need for any special software drivers, and the nice thing about it is your second factor cannot be phished.
THE PERFECT CRIME?
Merrill said that in many ways the ramp-and-dump scheme is the perfect crime because it leaves precious few connections between the victim brokerage accounts and the fraudsters.
&#8220;It&#8217;s really genius because it decouples so many things,&#8221; he said. &#8220;They can buy shares [in the stock to be pumped] in their personal account on the Chinese exchanges, and the price happens to go up. The Chinese or Hong Kong brokerages aren&#8217;t going to see anything funky.&#8221;
Merrill said it&#8217;s unclear exactly how those perpetrating these ramp-and-dump schemes coordinate their activities, such as whether the accounts are phished well in advance or shortly before being used to inflate the stock price of Chinese companies. The latter possibility would fit nicely with the existing human infrastructure these criminal groups already have in place.
For example, KrebsOnSecurity recently wrote about research from Merrill and other researchers showing the phishers behind these slick mobile phishing kits employed people to sit for hours at a time in front of large banks of mobile phones being used to send the text message lures. These technicians were needed to respond in real time to victims who were supplying the one-time code sent from their financial institution.
The ashtray says: You&#8217;ve been phishing all night.
&#8220;You can get access to a victim&#8217;s brokerage with a one-time passcode, but then you sort of have to use it right away if you can&#8217;t set new security settings so you can come back to that account later,&#8221; Merrill said.
The rapid pace of innovations produced by these China-based phishing vendors is due in part to their use of artificial intelligence and large language models to help develop the mobile phishing kits, he added.
&#8220;These guys are vibe coding stuff together and using LLMs to translate things or help put the user interface together,&#8221; Merrill said. &#8220;It&#8217;s only a matter of time before they start to integrate the LLMs into their development cycle to make it more rapid. The technologies they are building definitely have helped lower the barrier of entry for everyone.&#8221;
â€¢ Microsoft Patch Tuesday, August 2025 Edition
  Microsoft today released updates to fix more than 100 security flaws in its Windows operating systems and other software. At least 13 of the bugs received Microsoft&#8217;s most-dire &#8220;critical&#8221; rating, meaning they could be abused by malware or malcontents to gain remote access to a Windows system with little or no help from users.

August&#8217;s patch batch from Redmond includes an update for CVE-2025-53786, a vulnerability that allows an attacker to pivot from a compromised Microsoft Exchange Server directly into an organization&#8217;s cloud environment, potentially gaining control over Exchange Online and other connected Microsoft Office 365 services. Microsoft first warned about this bug on Aug. 6, saying it affects Exchange Server 2016 and Exchange Server 2019, as well as its flagship Exchange Server Subscription Edition.
Ben McCarthy, lead cyber security engineer at Immersive, said a rough search reveals approximately 29,000 Exchange servers publicly facing on the internet that are vulnerable to this issue, with many of them likely to have even older vulnerabilities.
McCarthy said the fix for CVE-2025-53786 requires more than just installing a patch, such as following Microsoft&#8217;s manual instructions for creating a dedicated service to oversee and lock down the hybrid connection.
&#8220;In effect, this vulnerability turns a significant on-premise Exchange breach into a full-blown, difficult-to-detect cloud compromise with effectively living off the land techniques which are always harder to detect for defensive teams,&#8221; McCarthy said.
CVE-2025-53779 is a weakness in the Windows Kerberos authentication system that allows an unauthenticated attacker to gain domain administrator privileges. Microsoft credits the discovery of the flaw to Akamai researcher Yuval Gordon, who dubbed it &#8220;BadSuccessor&#8221; in a May 2025 blog post. The attack exploits a weakness in &#8220;delegated Managed Service Account&#8221; or dMSA &#8212; a feature that was introduced in Windows Server 2025.
Some of the critical flaws addressed this month with the highest severity (between 9.0 and 9.9 CVSS scores) include a remote code execution bug in the Windows GDI+ component that handles graphics rendering (CVE-2025-53766) and CVE-2025-50165, another graphics rendering weakness. Another critical patch involves CVE-2025-53733, a vulnerability in Microsoft Word that can be exploited without user interaction and triggered through the Preview Pane.
One final critical bug tackled this month deserves attention: CVE-2025-53778, a bug in Windows NTLM, a core function of how Windows systems handle network authentication. According to Microsoft, the flaw could allow an attacker with low-level network access and basic user privileges to exploit NTLM and elevate to SYSTEM-level access â€” the highest level of privilege in Windows. Microsoft rates the exploitation of this bug as &#8220;more likely,&#8221; although there is no evidence the vulnerability is being exploited at the moment.
Feel free to holler in the comments if you experience problems installing any of these updates. As ever, the SANS Internet Storm Center has its useful breakdown of the Microsoft patches indexed by severity and CVSS score, and AskWoody.com is keeping an eye out for Windows patches that may cause problems for enterprises and end users.
GOOD MIGRATIONS
Windows 10 users out there likely have noticed by now that Microsoft really wants you to upgrade to Windows 11. The reason is that after the Patch Tuesday on October 14, 2025, Microsoft will stop shipping free security updates for Windows 10 computers. The trouble is, many PCs running Windows 10 do not meet the hardware specifications required to install Windows 11Â (or they do, but just barely).
If the experience with Windows XP is any indicator, many of these older computers will wind up in landfills or else will be left running in an unpatched state. But if your Windows 10 PC doesn&#8217;t have the hardware chops to run Windows 11 and you&#8217;d still like to get some use out of it safely, consider installing a newbie-friendly version of Linux, like Linux Mint.
Like most modern Linux versions, Mint will run on anything with a 64-bit CPU that has at least 2GB of memory, although 4GB is recommended. In other words, it will run on almost any computer produced in the last decade.
There are many versions of Linux available, but Linux Mint is likely to be the most intuitive interface for regular Windows users, and it is largely configurable without any fuss at the text-only command-line prompt. Mint and other flavors of Linux come with LibreOffice, which is an open source suite of tools that includes applications similar to Microsoft Office, and it can open, edit and save documents as Microsoft Office files.
If you&#8217;d prefer to give Linux a test drive before installing it on a Windows PC, you can always just download it to a removable USB drive. From there, reboot the computer (with the removable drive plugged in) and select the option at startup to run the operating system from the external USB drive. If you don&#8217;t see an option for that after restarting, try restarting again and hitting the F8 button, which should open a list of bootable drives. Here&#8217;s a fairly thorough tutorial that walks through exactly how to do all this.
And if this is your first time trying out Linux, relax and have fun: The nice thing about a &#8220;live&#8221; version of Linux (as it&#8217;s called when the operating system is run from a removable drive such as a CD or a USB stick) is that none of your changes persist after a reboot. Even if you somehow manage to break something, a restart will return the system back to its original state.

ğŸ“ University AI
No updates.

ğŸ¢ Corporate AI
â€¢ Dion: the distributed orthonormal update revolution is here
  Training AI models requires choosing an optimizer and for nearly a decade, Adam( (opens in new tab)&#8211;W) (opens in new tab) has been the optimizer of choice. Given that durability and success, it was fair to doubt that any further improvement was possible. And yet, last December, a new optimizer called Muon (opens in new tab) showed serious promise by powering a nanoGPT speedrun (opens in new tab). This proved out, with multiple AI labs (e.g., Kimi-AI (opens in new tab) and Essential-AI (opens in new tab)) reporting 2x scale improvements and the release of the 1T parameter Kimi K2 (opens in new tab) model.&nbsp;Restated: you can train a model to similar performance with half as many GPUs.



Thereâ€™s one fly in the ointment: Muon requires large matrix multiplications in the optimizer, which requires heavy communication in large models at the scale where FSDP and TP parallelization becomes desirable.&nbsp;Going back to the inspiration for Muon, the key idea is an orthonormal update, which sparked the search&nbsp;for more scalable alternative linear algebras realizing the same goal. Thatâ€™s exactly what Dion is. We have open-sourced this new optimizer to enable anyone to train large models more efficiently at scale. &nbsp;



Whatâ€™s an orthonormal update?



Figure1. Illustration of matrix parameters



At the core of Transformers, a set of input activations is multiplied by a learned weight matrix to produce a new set of output activations. When the weight matrix is updated during training, the resulting change in the output activations generally depends on the direction of the input activations. As a result, the learning rate must be chosen conservatively to accommodate the input direction that induces the largest change. Orthonormalized updates alter this behavior by (approximately) making the change in output activations invariant to the direction of the input. This is achieved by enforcing orthonormality (opens in new tab) on the update matrix, thereby equalizing its effect across all input directions.



What is Dion?



While Muon has shown strong empirical results, scaling it to very large models poses challenges. As reported by Essential AI (opens in new tab), applying Muon to large architectures like LLaMA-3 becomes compute-boundâ€”and potentially communication-boundâ€”due to the cost of the Newtonâ€“Schulz orthonormalization steps (opens in new tab).



Figure 2. Pseudocode of the centralized version of Dion



This is where Dion enters. At a high level, Dion introduces a new axis for scalability: the rank. Specifically, for a given rank r, Dion orthonormalizes only the top r of the singular vector space, reducing communication and compute overhead while preserving performance.&nbsp;Empirically, we observe that the necessary rank for good performance grows much more slowly than the number of parameters in larger models.




	
		
						Download
			
				Dion optimizer&nbsp;
			
					
	




Dion implements orthonormalization using amortized power iteration (opens in new tab).&nbsp;Power iteration typically pulls out the largest singular value by repeated matrix multiplication.&nbsp;By amortizing this process over optimization stepsâ€”applied to the slowly-evolving momentum matrixâ€”we reduce the cost to just two matrix multiplications per step. Incorporating a QR decomposition allows us to extract an approximate orthonormal basis spanning the top singular directions, rather than just the leading one.&nbsp;This amortized power iteration is fully compatible with standard distributed training techniques such as FSDP and tensor parallelism.&nbsp;Here, we show a simple centralized version, but the technique works for more complex forms of parallelization as presented in the paper. In other words, we can orthogonalize a matrix without ever seeing a full row or column of it.&nbsp;



Low-rank approximation would ordinarily introduce error, but Dion overcomes this through an error feedback mechanism. This keeps the residual of low rank approximation in the momentum matrix so that any systematic gradient structure not initially captured accumulates to eventually be applied in a future update.



	
		

	
	
						
				
					
				
			
			
			

									Azure AI Foundry Labs
				
								Get a glimpse of potential future directions for AI, with these experimental technologies from Microsoft Research.
				
								
					
						
							Azure AI Foundry						
					
				
							
	
Opens in a new tab	
	


How does it work?



Something very strange happened in our experiments. Usually, adding an extra constraint on the way an algorithm works can be expected to decrease overall performance. And indeed, at the 120M parameter scale of the speedrun, we see Dionâ€™s update taking more time than Muon, while not yielding any significant gains. But at larger scales, we observed a different trend: Dion began to outperform Muon.



Figure 3. Wall-clock time speedup of Dion for 3B model training



Why would adding a constraint improve the update rule? The answer lies in what the constraint enforces. Dion achieves a much closer approximation to true orthonormalization than Muon. This precision, initially subtle, becomes increasingly important as the number of singular vectors grows. Over increasing model scale and training steps, this small advantage accumulatesâ€”leading to a measurable improvement in performance.



This edge further grows with batch sizeâ€”with larger batches the update quality tends to degrade, but notably more slowly with Dion than Muon (and Muon is already a significant improvement over AdamW).



Figure 4. Scaling of Dion across different batch sizes



Here you can see how the number of steps to reach a pretraining loss compared to AdamW varies as batch size grows with full rank and Â¼ rank Dion (in orange) and Muon (in blue).&nbsp;&nbsp;&nbsp;



In our experiments, these benefits extend to various post-training regimes as well.



We also experimented with rank, discovering empirically that larger models tolerate smaller rank well.



Figure 5. Low-rank Dion across different model sizes



Projecting this trend out to the scale of the LLaMA-3 (opens in new tab) 405B parameter models suggests that Dion is fully effective even with rank fractions as low as 1/16 or 1/64 for large dense models like LLaMA-3.&nbsp;&nbsp;&nbsp;&nbsp;



Using hardware timings of the individual update steps suggests a story that looks this:



Figure 6. Estimated wall-clock time of each optimizer step for Llama 3 405B. Lower is better. Muon is highlighted in orange as our baseline, next to Dion with varying rank fractions. Suggested rank fractions for a 405B parameter model are shown in blue. Using Dion with rank fraction 1/16 or lower offers an order-of-magnitude speedup over Muon.



Weâ€™ve open-sourced a PyTorch FSDP2 + Tensor Parallel (TP) implementation of Dion, available via a simple pip install. Our goal is to make faster training with Dion accessible to everyone. As a bonus, the repository also includes a PyTorch FSDP2 implementation of Muon.




Dion optimizer




Acknowledgements



We thank Riashat Islam and Pratyusha Sharma for their helpful feedback on the writing and presentation.
Opens in a new tabThe post Dion: the distributed orthonormal update revolution is here appeared first on Microsoft Research.
â€¢ Introducing Amazon Bedrock AgentCore Gateway: Transforming enterprise AI agent tool development
  To fulfill their tasks, AI Agents need access to various capabilities including tools, data stores, prompt templates, and other agents. As organizations scale their AI initiatives, they face an exponentially growing challenge of connecting each agent to multiple tools, creating an MÃ—N integration problem that significantly slows development and increases complexity. 
Although protocols such as Model Context Protocol (MCP) and Agent2Agent (A2A) have emerged to address interoperability, implementing these solutions requires substantial engineering effort. Organizations must build MCP servers, convert existing APIs, manage infrastructure, build intelligent tools discovery, and implement security controls, all that while maintaining these integrations over time as protocols rapidly evolve and new major versions are released. As deployments grow to hundreds of agents and thousands of tools, enterprises need a more scalable and manageable solution. 
Introducing Amazon Bedrock AgentCore Gateway 
Weâ€™re excited to announce Amazon Bedrock AgentCore Gateway, a fully managed service that revolutionizes how enterprises connect AI agents with tools and services. AgentCore Gateway serves as a centralized tool server, providing a unified interface where agents can discover, access, and invoke tools. 
Built with native support for the MCP, Gateway enables seamless agent-to-tool communication while abstracting away security, infrastructure, and protocol-level complexities. This service provides zero-code MCP tool creation from APIs and AWS Lambda functions, intelligent tool discovery, built-in inbound and outbound authorization, and serverless infrastructure for MCP servers. You can focus on building intelligent agent experiences rather than managing connectivity with tools and services. The following diagram illustrates the AgentCore Gateway workflow. 
 
Key capabilities of Amazon Bedrock AgentCore Gateway 
The Amazon Bedrock AgentCore Gateway introduces a comprehensive set of capabilities designed to revolutionize tool integration for AI agents. At its core, Gateway offers powerful and secure API integration functionality that transforms existing REST APIs into MCP servers. This integration supports both OpenAPI specifications and Smithy models, so organizations can seamlessly convert their enterprise APIs into MCP-compatible tools. Beyond API integration, Gateway provides built-in support for Lambda functions so developers can connect their serverless computing resources as tools with defined schemas. Gateway provides the following key capabilities: 
 
 Security Guard â€“ Manages OAuth authorization so only valid users and agents can access tools and resources. We will dive deeper into security in the following section. 
 Translation â€“ Converts agent requests using protocols such as MCP into API requests and Lambda invocations, alleviating the need to manage protocol integration or version support. 
 Composition â€“ Combines multiple APIs, functions, and tools into a single MCP endpoint for streamlined agent access. 
 Target extensibility â€“ An AgentCore gateway is a central access point that serves as a unified interface for AI agents to discover and interact with tools. It handles authentication, request routing, and protocol translation between MCP and your APIs. Each gateway can manage multiple targets. A target represents a backend service or group of APIs that you want to expose as tools to AI agents. Targets can be AWS Lambda functions, OpenAPI specifications, or Smithy models. Each target can expose multiple tools, and Gateway automatically handles the conversion between MCP and the targetâ€™s built-in protocol. Gateway supports streamable http transport. 
 Infrastructure Manager â€“ As a fully managed service, Gateway removes the burden of infrastructure management from organizations. It provides comprehensive infrastructure with built-in security features and robust observability capabilities. Teams no longer need to worry about hosting concerns, scaling issues, or maintaining the underlying infrastructure. The service automatically handles these aspects, providing reliable performance and seamless scaling as demand grows. 
 Semantic Tool Selection â€“ Intelligent tool discovery represents another core capability of Gateway. As organizations scale to hundreds or thousands of tools, discovering the right tool becomes increasingly challenging for AI agents. Moreover, when agents are presented with too many tools simultaneously, they can experience something called â€œtool overload,â€ leading to hallucinations, incorrect tool selections, or inefficient execution paths that significantly impact performance. Gateway addresses these challenges by providing a special built-in tool named 'x_amz_bedrock_agentcore_search' that can be accessed using the standard MCP tools and call operation. 
 
 
Security and authentication 
Gateway implements a sophisticated dual-sided security architecture that handles both inbound access to Gateway itself and outbound connections to target services. 
For inbound requests, Gateway follows the MCP authorization specification, using OAuth-based authorization to validate and authorize incoming tool calls. Gateway functions as an OAuth resource server. This means it can work with the OAuth Identity Provider your organization might useâ€“whether thatâ€™s Amazon Cognito, Okta, Auth0, or your own OAuth provider. When you create a gateway, you can specify multiple approved client IDs and audiences, giving you granular control over which applications and agents can access your tools. The Gateway validates incoming requests against your OAuth provider, supporting both authorization code flow (3LO) and client credentials flow (2LO, commonly used for service-to-service communication). 
The outbound security model is equally flexible but varies by target type: 
For AWS Lambda and Smithy model targets, AgentCore Gateway uses AWS Identity and Access Management (IAM) based authorization. The gateway assumes an IAM role you configure, which can have precisely scoped permissions for each target service. This integrates smoothly with existing AWS security practices and IAM policies. 
For OpenAPI targets (REST APIs), Gateway supports two authentication methods: 
 
 API key â€“ You can configure the key to be sent in either headers or query parameters with customizable parameter names 
 OAuth token for 2LO â€“ For outbound OAuth authentication to target APIs, Gateway supports two-legged OAuth (2LO) client credentials grant type, enabling secure machine-to-machine communications without user interaction 
 
Credentials are securely managed through AgentCore Identityâ€™s resource credentials provider. Each target is associated with exactly one authentication configuration, facilitating clear security boundaries and audit trails. AgentCore Identity handles the complex security machinery while presenting a clean, simple interface to developers. You configure security one time during setup, and Gateway handles the token validation, outbound token caching (through AgentCore Identity), and secure communication from there. 
Get started with Amazon Bedrock AgentCore Gateway 
You can create gateways and add targets through multiple interfaces: 
 
 AWS SDK for Python (Boto3) 
 AWS Management Console 
 AWS Command Line Interface (AWS CLI) 
 AgentCore starter toolkit for fast and straightforward setup 
 
The following practical examples and code snippets demonstrate the process of setting up and using Amazon Bedrock AgentCore Gateway. 
Create a gateway 
To create a gateway, use Amazon Cognito for inbound auth using the AWS Boto3: 
 
 gateway_client = boto3.client('bedrock-agentcore-control')
auth_config = {
&nbsp;&nbsp; &nbsp;"customJWTAuthorizer": { 
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"allowedClients": '&lt;cognito_client_id&gt;â€˜, # Client MUST match with the ClientId configured in Cognito.
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"discoveryUrl": '&lt;cognito_oauth_discovery_url&gt;'
&nbsp;&nbsp; &nbsp;}
}
create_response = gateway_client.create_gateway(name='DemoGateway',
&nbsp;&nbsp; &nbsp;roleArn = '&lt;IAM Role&gt;' # The IAM Role must have permissions to create/list/get/delete Gateway 
&nbsp;&nbsp; &nbsp;protocolType='MCP',
&nbsp;&nbsp; &nbsp;authorizerType='CUSTOM_JWT',
&nbsp;&nbsp; &nbsp;authorizerConfiguration=auth_config, 
&nbsp;&nbsp; &nbsp;description='Demo AgentCore Gateway'
)
# Values with &lt; &gt; needs to be replaced with real values 
 
Here is the reference to control plane and data plane APIs for Amazon Bedrock AgentCore. 
Create gateway targets 
Create a target for an existing API using OpenAPI specification with API key as an outbound auth: 
 
 # Create outbound credentials provider in AgentCore Identity
acps&nbsp;&nbsp;boto3client(service_name"bedrock-agentcore-control")

responseacpscreate_api_key_credential_provider(
name"APIKey",
apiKey"&lt;your secret API key"
)

credentialProviderARN&nbsp;&nbsp;response['credentialProviderArn']

# Specify OpenAPI spec file via S3 or inline
openapi_s3_target_config = {
&nbsp;&nbsp; &nbsp;"mcp": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"openApiSchema": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"s3": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"uri": openapi_s3_uri
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp; &nbsp;}
}

# API Key credentials provider configuration
api_key_credential_config = [
&nbsp;&nbsp; &nbsp;{
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"credentialProviderType" : "API_KEY", 
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"credentialProvider": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"apiKeyCredentialProvider": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"credentialParameterName": "api_key", # Replace this with the name of the api key name expected by the respective API provider. For passing token in the header, use "Authorization"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"providerArn": credentialProviderARN,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"credentialLocation":"QUERY_PARAMETER", # Location of api key. Possible values are "HEADER" and "QUERY_PARAMETER".
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;#"credentialPrefix": " " # Prefix for the token. Valid values are "Basic". Applies only for tokens.
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp;}
&nbsp;&nbsp;]

# Add the OpenAPI target to the gateway
targetname='DemoOpenAPITarget'
response = gateway_client.create_gateway_target(
&nbsp;&nbsp; &nbsp;gatewayIdentifier=gatewayID,
&nbsp;&nbsp; &nbsp;name=targetname,
&nbsp;&nbsp; &nbsp;description='OpenAPI Target with S3Uri using SDK',
&nbsp;&nbsp; &nbsp;targetConfiguration=openapi_s3_target_config,
&nbsp;&nbsp; &nbsp;credentialProviderConfigurations=api_key_credential_config) 
 
Create a target for a Lambda function: 
 
 # Define the lambda target with tool schema. Replace the AWS Lambda function ARN below
lambda_target_config = {
&nbsp;&nbsp;"mcp": {
&nbsp;&nbsp; &nbsp;"lambda": {
&nbsp;&nbsp; &nbsp; &nbsp;"lambdaArn": "&lt;Your AWS Lambda function ARN&gt;",
&nbsp;&nbsp; &nbsp; &nbsp;"toolSchema": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"inlinePayload": [
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;{
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"name": "get_order_tool",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"description": "tool to get the order",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"inputSchema": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"type": "object",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"properties": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"orderId": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"type": "string"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;},
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"required": [
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"orderId"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;]}}]}}}}

# Create outbound auth config. For AWS Lambda function, its always IAM.
credential_config = [ 
&nbsp;&nbsp; &nbsp;{
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"credentialProviderType" : "GATEWAY_IAM_ROLE"
&nbsp;&nbsp; &nbsp;}
]

# Add AWS Lambda target to the gateway
targetname='LambdaUsingSDK'
response = gateway_client.create_gateway_target(
&nbsp;&nbsp; &nbsp;gatewayIdentifier=gatewayID,
&nbsp;&nbsp; &nbsp;name=targetname,
&nbsp;&nbsp; &nbsp;description='Lambda Target using SDK',
&nbsp;&nbsp; &nbsp;targetConfiguration=lambda_target_config,
&nbsp;&nbsp; &nbsp;credentialProviderConfigurations=credential_config) 
 
Use Gateway with different agent frameworks 
Use Gateway with Strands Agents integration: 
 
 
from strands import Agent
import logging

def create_streamable_http_transport():
&nbsp;&nbsp; &nbsp;return streamablehttp_client(gatewayURL,headers={"Authorization": f"Bearer {token}"})

client = MCPClient(create_streamable_http_transport)

with client:
&nbsp;&nbsp; &nbsp;# Call the listTools 
&nbsp;&nbsp; &nbsp;tools = client.list_tools_sync()
&nbsp;&nbsp; &nbsp;# Create an Agent with the model and tools
&nbsp;&nbsp; &nbsp;agent = Agent(model=yourmodel,tools=tools) ## you can replace with any model you like
&nbsp; &nbsp;&nbsp;# Invoke the agent with the sample prompt. This will only invoke &nbsp;MCP listTools and retrieve the list of tools the LLM has access to. The below does not actually call any tool.
&nbsp;&nbsp; &nbsp;agent("Hi , can you list all tools available to you")
&nbsp;&nbsp; &nbsp;# Invoke the agent with sample prompt, invoke the tool and display the response
&nbsp;&nbsp; &nbsp;agent("Check the order status for order id 123 and show me the exact response from the tool") 
 
Use Gateway with LangChain integration: 
 
 from langchain_mcp_adapters.client import MultiServerMCPClient
from langgraph.prebuilt import create_react_agent
from langchain.chat_models import init_chat_model

client = MultiServerMCPClient(
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;{
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"healthcare": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"url": gateway_endpoint,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"transport": "streamable_http",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"headers":{"Authorization": f"Bearer {jwt_token}"}
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp;)
&nbsp;agent = create_react_agent(
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;LLM, 
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;tools, 
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;prompt=systemPrompt
&nbsp;) 
 
Implement semantic search 
You can opt in to semantic search when creating a gateway. It automatically provisions a powerful built-in tool called x_amz_bedrock_agentcore_search that enables intelligent tool discovery through natural language queries. Use the output of the search tool in place of MCPâ€™s list operation for scalable and performant tool discovery. The following diagram illustrates how you can use the MCP search tool. 
 
To enable semantic search, use the following code: 
 
 &nbsp;# Enable semantic search of tools
&nbsp;&nbsp; &nbsp;search_config = {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"mcp": {"searchType": "SEMANTIC", "supportedVersions": ["2025-03-26"]}
&nbsp;&nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp;# Create the gateway
&nbsp;&nbsp; &nbsp;response = agentcore_client.create_gateway(
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;name=gateway_name,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;roleArn=gateway_role_arn,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;authorizerType="CUSTOM_JWT",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;description=gateway_desc,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;protocolType="MCP",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;authorizerConfiguration=auth_config,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;protocolConfiguration=search_config,
&nbsp;&nbsp; &nbsp;)
def tool_search(gateway_endpoint, jwt_token, query):
&nbsp;&nbsp; &nbsp;toolParams = {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"name": "x_amz_bedrock_agentcore_search",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"arguments": {"query": query},
&nbsp;&nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp;toolResp = invoke_gateway_tool(
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;gateway_endpoint=gateway_endpoint, jwt_token=jwt_token, tool_params=toolParams
&nbsp;&nbsp; &nbsp;)
&nbsp;&nbsp; &nbsp;tools = toolResp["result"]["structuredContent"]["tools"]
&nbsp;&nbsp; &nbsp;return tools 
 
To find the entire code sample, visit the Semantic search tutorial in the amazon-bedrock-agentcore-samples GitHub repository. 
Assess Gateway performance using monitoring and observability 
Amazon Bedrock AgentCore Gateway provides observability through integration with Amazon CloudWatch and AWS CloudTrail, for detailed monitoring and troubleshooting of your tool integrations. The observability features include multiple dimensions of gateway operations through detailed metrics: usage metrics (TargetType, IngressAuthType, EgressAuthType, RequestsPerSession), invocation metrics (Invocations, ConcurrentExecutions, Sessions), performance metrics (Latency, Duration, TargetExecutionTime), and error rates (Throttles, SystemErrors, UserErrors). The performance metrics can be analyzed using various statistical methods (Average, Minimum, Maximum, p50, p90, p99) and are tagged with relevant dimensions for granular analysis, including Operation, Resource, and Name . For operational logging, Gateway integrates with CloudTrail to capture both management and data events, providing a complete audit trail of API interactions. The metrics are accessible through both the Amazon Bedrock AgentCore console and CloudWatch console, where you can create custom dashboards, set up automated alerts, and perform detailed performance analysis. 
Best practices 
Gateway offers an enhanced debugging option through the exceptionLevel property, which can be enabled during Gateway creation or updated as shown in the following code example: 
 
 create_response = gateway_client.create_gateway(name='DemoGateway',
&nbsp;&nbsp; &nbsp;roleArn = '&lt;IAM Role&gt;' # The IAM Role must have permissions to create/list/get/delete Gateway 
&nbsp;&nbsp; &nbsp;protocolType='MCP',
&nbsp;&nbsp; &nbsp;authorizerType='CUSTOM_JWT',
&nbsp;&nbsp; &nbsp;authorizerConfiguration=auth_config, 
&nbsp;&nbsp; &nbsp;description='Demo AgentCore Gateway',
    exceptionLevel="DEBUG"   # Debug mode for granular error messages
) 
 
When activated, this feature provides more granular error messages in the content text block (with isError:true) during Gateway testing, facilitating quicker troubleshooting and integration. When documenting and extracting Open APIs for Gateway, focus on clear, natural language descriptions that explain real-world use cases. Include detailed field descriptions, validation rules, and examples for complex data structures while maintaining consistent terminology throughout. For optimal tool discovery, incorporate relevant business domain keywords naturally in descriptions and provide context about when to use each API. Finally, test semantic search effectiveness so tools are discoverable through natural language queries. Regular reviews and updates are essential to maintain documentation quality as APIs evolve.When extracting APIs from larger specifications, identify the core functionality needed for agent tasks, maintain semantic relationships between components, and preserve security definitions. Follow a systematic extraction process: review the full specification, map agent use cases to specific endpoints, extract relevant paths and schemas while maintaining dependencies, and validate the extracted specification.The following are the best practices on grouping your APIs into a Gateway target: 
 
 Start with the use case and group your MCP tools based on the agentic applicationâ€™s business domain similar to domain-driven design principles applicable to the microservices paradigm. 
 You can attach only one resource credentials provider for outbound authorization for the Gateway target. Group the tools based on the outbound authorizer. 
 Group your APIs based on the type of the APIs, that is, OpenAPI, Smithy, or AWS Lambda, serving as a bridge to other enterprise APIs. 
 
When onboarding tools to Gateway, organizations should follow a structured process that includes security and vulnerability checks. Implement a review pipeline that scans API specifications for potential security risks, maintains proper authentication mechanisms, and validates data handling practices. For runtime tool discovery, use the semantic search capabilities in Gateway, but also consider design-time agent-tool mapping for critical workflows to provide predictable behavior. 
Enrich tool metadata with detailed descriptions, usage examples, and performance characteristics to improve discoverability and aid in appropriate tool selection by agents. To maintain consistency across your enterprise, integrate Gateway with a centralized tool registry that serves as a single source of truth. This can be achieved using open source solutions such as the MCP Registry Publisher Tool, which publishes MCP server details to an MCP registry. Regularly synchronize Gatewayâ€™s tool inventory with this central registry for up-to-date and consistent tool availability across your AI landscape. These practices can help maintain a secure, well-organized, and efficiently discoverable tool solution within Gateway, facilitating seamless agent-tool interactions while can align with enterprise governance standards. 
What customers are saying 
Innovaccer, a leading healthcare technology company, shares their experience: 

 â€œAI has massive potential in healthcare, but getting the foundation right is key. Thatâ€™s why weâ€™re building HMCP (Healthcare Model Context Protocol) on Amazon Bedrock AgentCore Gateway, which has been a game-changer, automatically converting our existing APIs into MCP-compatible tools and scaling seamlessly as we grow. It gives us the secure, flexible base we need to make sure AI agents can safely and responsibly interact with healthcare data, tools, and workflows. With this partnership, weâ€™re accelerating AI innovation with trust, compliance, and real-world impact at the core.â€  
 â€”Abhinav Shashank, CEO &amp; Co-founder, Innovaccer
 
Conclusion 
Amazon Bedrock AgentCore Gateway represents a significant advancement in enterprise AI agent development. By providing a fully managed, secure, and scalable solution for tool integration, Gateway enables organizations to accelerate their AI initiatives while maintaining enterprise-grade security and governance. As part of the broader Amazon Bedrock AgentCore suite, Gateway works seamlessly with other capabilities including Runtime, Identity, Code Interpreter, Memory, Browser, and Observability to provide a comprehensive domain for building and scaling AI agent applications. 
For more detailed information and advanced configurations, refer to the code samples on GitHub, the Amazon Bedrock AgentCore Gateway Developer Guide and Amazon AgentCore Gateway pricing. 
 
About the authors 
Dhawal Patel is a Principal Machine Learning Architect at Amazon Web Services (AWS). He has worked with organizations ranging from large enterprises to mid-sized startups on problems related to distributed computing and AI. He focuses on deep learning, including natural language processing (NLP) and computer vision domains. He helps customers achieve high-performance model inference on Amazon SageMaker. 
Mike Liu is a Principal Product Manager at Amazon, where he works at the intersection of agentic AI and foundational model development. He led the product roadmap for Amazon Bedrock Agents and is now helping customers achieve superior performance using model customization on Amazon Nova models. Prior to Amazon, he worked on AI/ML software in Google Cloud and ML accelerators at Intel. 
Kartik Rustagi works as a Software Development Manager in Amazon AI. He and his team focus on enhancing the conversation capability of chat bots powered by Amazon Lex. When not at work, he enjoys exploring the outdoors and savoring different cuisines.
â€¢ Build a scalable containerized web application on AWS using the MERN stack with Amazon Q Developer â€“ Part 1
  The MERN (MongoDB, Express, React, Node.js) stack is a popular JavaScript web development framework. The combination of technologies is well-suited for building scalable, modern web applications, especially those requiring real-time updates and dynamic user interfaces. Amazon Q Developer is a generative AI-powered assistant that improves developer efficiency across the different phases of the software development lifecycle (SDLC). In this two-part blog series, I capture the experience and demonstrate the productivity gains you can achieve by using Amazon Q Developer as a coding assistant to build a scalable MERN stack web application on AWS. The solution forms a solid foundation for you to build a feature rich web application. In my case, using the process outlined in this blog, I extended the MERN stack web application to include real-time video conferencing (using Amazon Chime SDK) and an AI chatbot (invoking Amazon Bedrock foundation models). 
Typically, in the plan phase of the SDLC, time is spent researching approaches and identifying common solution patterns that can deliver on requirements. Using Amazon Q Developer, you can speed up this process by prompting for an approach to deploy a scalable MERN stack web application on AWS. Trained on over 17 years of AWS experience building in the cloud, Amazon Q Developer responses are based on AWS well-architected patterns and best practices. In the design phase, I use the responses from Amazon Q Developer to craft a detailed requirements prompt to generate the code for your MERN stack web application. Then in the build phase, I extend the code to implement a working solution, generate unit tests and conduct an automated code review. 
In part 2 of this blog series, I will use Amazon Q Developer to extend the base MERN stack web application to include a chat user interface (which invokes an agentic workflow based on the Strands Agent SDK and Amazon Bedrock), deploy the solution to AWS using infrastructure as code (IaC), troubleshoot issues and generate the documentation for our solution. 
Walkthrough 
Prerequisites 
To complete the walkthrough in this post, you must have the following: 
 
 An AWS account to deploy the solution components to AWS. 
 AWS Command Line Interface (AWS CLI) installed and configured. 
 Docker Desktop installed. 
 Set up access to Amazon Q Developer by using one of the following two options: 
   
   Amazon Q Developer Free tier â€“ Provides access to explore capabilities before opting for a paid tier and requires an AWS Builder ID profile. 
   Amazon Q Developer Pro tier â€“ Paid subscription with access to additional features. Set up through IAM Identity Center. 
    
 A supported integrated development environment (IDE) including Visual Studio Code and JetBrains IDEs. For more information, follow the instructions for installing the Amazon Q Developer extension or plugin in your IDE. 
 
Sign in to Amazon Q Developer (in your IDE) 
After setting up Amazon Q Developer access tier and installing the Amazon Q extension for your IDE, you can sign in to Amazon Q Developer by using the IDE. 
 
 The first sign-in flow shows the authentication process for the Free tier using an AWS Builder ID. 
 
 
 
 The second sign-in flow shows the authentication process for the Pro tier using a sign-in URL to the AWS access portal (provided by your AWS administrator). 
 
 
 
 After successful authentication, youâ€™ll be presented with an initial chat window to start a conversation with Amazon Q Developer. In the chat input at the bottom, you have options to add additional context for Amazon Q Developer to provide responses such as using the active file or the entire workspace, defining rules for Amazon Q Developer to follow when it generates responses, toggling agentic coding on and off, and selecting your preferred foundation model (Claude Sonnet 4 in our case). 
 
 
With Free Tier, you have access to limited agentic requests per month, access to the latest Claude models and use of Amazon Q Developer in the IDE or CLI. In this post, I use the Pro Tier, which in addition to Free Tier features, also provides increased limits of agentic requests and app transformation, Identity center support and IP indemnity. 
Plan 
In the planning phase, you can prompt for a solution approach to better understand the different components that will make up the MERN stack web application. You would toggle agentic coding off in this phase as you research and understand the best approach. Example planning phase prompt: 
â€œProvide a high-level summary of a solution approach to deploying a scalable MERN stack application on AWS.â€ 
The response from Amazon Q Developer (also shown in the following screenshot) breaks down the solution into the following components: 
 
 Frontend React application 
 Backend NodeJS and Express containerized app running on Amazon ECS Fargate 
 Database using MongoDB or Amazon DocumentDB 
 Core network infrastructure 
 Security 
 Monitoring and operations 
 Continuous integration and delivery (CI/CD) pipeline 
 Performance 
 
 
Design &amp; Build 
After reviewing the solution approach, you can create a more detailed prompt about the web application requirements, which will be used in the feature development capability of Amazon Q Developer to generate the solution components. Turn agentic coding on before submitting the prompt. Example design phase prompt: 
â€œBuild a scalable containerized web application using the MERN stack on AWS, with login and sign-up pages integrated with Amazon Cognito, a landing page that retrieves a list of shops from DocumentDB. I donâ€™t intend to use AWS Amplify. It needs to be a modular design with components that can scale independently, running as containers using ECS and Fargate, highly available across two Availability Zones. I need to build, test and run the MERN stack locally before pushing the solution to AWS.â€  
As shown in the following screenshots, Amazon Q Developer will provide an architecture overview of the solution before going through the build process step by step. I will provide a select number of screenshots for illustration but note that the steps generated by Amazon Q Developer will vary for your solution prompt. 
 
For each file that it creates or updates, Amazon Q Developer gives you the option to review the difference and undo the changes. This is an important step to understand whether the generated code meets your requirements. For example, the snippet below shows an update the Navbar component. 
 
When viewing the diff, you can see that Amazon Q Developer has added a new button class to fix a display issue. 
 
Amazon Q Developer can also execute shell commands. In this case, create the backend and frontend directory. You have the option to â€˜Rejectâ€™ or â€˜Runâ€™ the command. 
 
Hereâ€™s a snippet of Amazon Q Developer creating the authentication service, data model and Dockerfile for the solution. 
 
Another snippet of Amazon Q Developer creating the React frontend. 
 
A snippet of Amazon Q Developer creating the AWS infrastructure components. 
 
Amazon Q Developer then prompts to execute the deployment. 
 
But I noticed that it hasnâ€™t followed my initial prompt to â€œbuild, test and run the MERN stack locally before pushing the solution to AWSâ€, so I provide the following prompt: 
â€œIn my initial prompt, I asked to build, test and run the MERN stack locally before pushing the solution to AWS. 
Amazon Q Developer acknowledges my observation and makes the necessary changes for local deployment. 
 
Next, Amazon Q Developer will build, test and run the MERN stack locally as shown below. 
 
When reviewing the .env file changes, I noticed that the Amazon Cognito properties are not properly set, so provide the following prompt: 
â€œWhen reviewing your .env file changes, I noticed that setting to COGNITO_USER_POOL_ID and COGNITO_CLIENT_ID to local-development is incorrect, as I should be connecting to Amazon Cognito in AWS. And this hasn't been created yet. Additionally, the local deployment has been configured to connect to the local MongoDB container instead of DocumentDB.â€ 
Amazon Q Developer again acknowledges my observation and attempts to fix the issues. These two issues highlight that to effectively use Amazon Q Developer, itâ€™s important to review and challenge the responses provided. 
 
After fixing the issues, Amazon Q Developer updates the README.md to reflect the updated approach and asks if I want to do a quick deployment with mocked authentication or an actual deployment with Amazon Cognito resources. 
 
I choose option B, with real Amazon Cognito resources, so Amazon Q Developer deploys the resources as shown below. 
 
Amazon Q Developer now checks that the frontend, backend and MongoDB containers are running. 
 
Amazon Q Developer also tests that the application is running by executing curl commands to the application endpoints. 
 
After successfully running the commands, Amazon Q Developer provides a summary of the results, with details on how to access and test the application. 
 
Hereâ€™s a diagram showing the locally deployed solution. 
 
Now that the frontend, backend, and MongoDB containers are running, you can access the frontend application Sign In page on http://localhost:3000. 
 
Before logging in, you need to create a user. Choose the Sign Up link to enter an email and password. 
 
After attempting to sign up, I noticed that Amazon Q Developer hasnâ€™t generated the corresponding frontend screen to enter the confirmation code, so I prompt it to fix the issue. Again, the generated code isnâ€™t always perfect, but itâ€™s a good starting point. 
 
After authentication, youâ€™ll be routed to the shops page as shown. 
 
Test 
Now that youâ€™ve built and can run the MERN stack web application locally, you can use Amazon Q Developer to generate unit tests to find defects and improve code quality. I provide the following prompt: 
â€œCan you generate unit tests for the project?â€ 
Amazon Q Developer will then create comprehensive unit tests for the application. 
 
At completion, Amazon Q Developer will provide a summary of the unit tests generated: 
 
Amazon Q Developer also provides instructions for executing the tests: 
 
After executing the unit tests, Amazon Q Developer provides a summary of the results. 
 
Review 
We can now conduct a code review of the MERN stack application by prompting the following: 
â€œCan you do a code review of my project to identify and fix any code issues?â€ 
Amazon Q Developer will perform a code review and identify issues that require attention. 
 
After completing the review, Amazon Q Developer will provide a summary of the critical issues fixed, along with next steps. 
 
Clean up 
To avoid incurring future charges, remove the Amazon Cognito resources that you created. 
Conclusion 
In a traditional SDLC, a lot of time is spent in the different phases researching approaches that can deliver on requirements: iterating over design changes, writing, testing and reviewing code, and configuring infrastructure. Amazon Q Developer is a generative AI-powered assistant that improves developer efficiency across the phases of the SDLC. In this post, you learned about the experience and saw productivity gains you can realize by using Amazon Q Developer as a coding assistant to build a scalable MERN stack web application on AWS. 
In the plan phase, you used Amazon Q Developer to prompt for a solution approach to deploy a scalable MERN stack web application on AWS. Then in the design phase, you used the initial responses from Amazon Q Developer to craft a detailed requirements prompt and generated the code for your MERN stack web application. In the build phase, you customized the code and deployed a working solution locally. In the test phase, Amazon Q Developer generated the unit tests for you to identify bugs early to improve code quality. Finally, in the review phase, you conducted a code review and remediated issues identified. 
In part 2 of this blog series, you will use Amazon Q Developer to extend the base MERN stack web application to include a chat user interface (which invokes an agentic workflow based on the Strands Agent SDK and Amazon Bedrock), deploy the solution to AWS using infrastructure as code (IaC), troubleshoot issues and generate the documentation for our solution. 
 
About the Author 
Bill Chan is an Enterprise Solutions Architect working with large enterprises to craft highly scalable, flexible, and resilient cloud architectures. He helps organizations understand best practices around advanced cloud-based solutions, and how to migrate existing workloads to the cloud. He enjoys relaxing with family and shooting hoops.
â€¢ Optimizing Salesforceâ€™s model endpoints with Amazon SageMaker AI inference components
  This post is a joint collaboration between Salesforce and AWS and is being cross-published on both the Salesforce Engineering Blog and the AWS Machine Learning Blog. 
The Salesforce AI Platform Model Serving team is dedicated to developing and managing services that power large language models (LLMs) and other AI workloads within Salesforce. Their main focus is on model onboarding, providing customers with a robust infrastructure to host a variety of ML models. Their mission is to streamline model deployment, enhance inference performance and optimize cost efficiency, ensuring seamless integration into Agentforce and other applications requiring inference. Theyâ€™re committed to enhancing the model inferencing performance and overall efficiency by integrating state-of-the-art solutions and collaborating with leading technology providers, including open source communities and cloud services such as Amazon Web Services (AWS) and building it into a unified AI platform. This helps ensure Salesforce customers receive the most advanced AI technology available while optimizing the cost-performance of the serving infrastructure. 
In this post, we share how the Salesforce AI Platform team optimized GPU utilization, improved resource efficiency and achieved cost savings using Amazon SageMaker AI, specifically inference components. 
The challenge with hosting models for inference: Optimizing compute and cost-to-serve while maintaining performance 
Deploying models efficiently, reliably, and cost-effectively is a critical challenge for organizations of all sizes. The Salesforce AI Platform team is responsible for deploying their proprietary LLMs such as CodeGen and XGen on SageMaker AI and optimizing them for inference. Salesforce has multiple models distributed across single model endpoints (SMEs), supporting a diverse range of model sizes from a few gigabytes (GB) to 30 GB, each with unique performance requirements and infrastructure demands. 
The team faced two distinct optimization challenges. Their larger models (20â€“30 GB) with lower traffic patterns were running on high-performance GPUs, resulting in underutilized multi-GPU instances and inefficient resource allocation. Meanwhile, their medium-sized models (approximately 15 GB) handling high-traffic workloads demanded low-latency, high-throughput processing capabilities. These models often incurred higher costs due to over-provisioning on similar multi-GPU setups. Hereâ€™s a sample illustration of Salesforceâ€™s large and medium SageMaker endpoints and where resources are under-utilized: 
 
Operating on Amazon EC2 P4d instances today, with plans to use the latest generation P5en instances equipped with NVIDIA H200 Tensor Core GPUs, the team sought an efficient resource optimization strategy that would maximize GPU utilization across their SageMaker AI endpoints while enabling scalable AI operations and extracting maximum value from their high-performance instancesâ€”all without compromising performance or over-provisioning hardware. 
This challenge reflects a critical balance that enterprises must strike when scaling their AI operations: maximizing the performance of sophisticated AI workloads while optimizing infrastructure costs and resource efficiency. Salesforce needed a solution that would not only resolve their immediate deployment challenges but also create a flexible foundation capable of supporting their evolving AI initiatives. 
To address these challenges, the Salesforce AI Platform team used SageMaker AI inference components that enabled deployment of multiple foundation models (FMs) on a single SageMaker AI endpoint with granular control over the number of accelerators and memory allocation per model. This helps improve resource utilization, reduces model deployment costs, and lets you scale endpoints together with your use cases. 
Solution: Optimizing model deployment with Amazon SageMaker AI inference components 
With Amazon SageMaker AI inference components, you can deploy one or more FMs on the same SageMaker AI endpoint and control how many accelerators and how much memory is reserved for each FM. This helps to improve resource utilization, reduces model deployment costs, and lets you scale endpoints together with your use cases. For each FM, you can define separate scaling policies to adapt to model usage patterns while further optimizing infrastructure costs. Hereâ€™s the illustration of Salesforceâ€™s large and medium SageMaker endpoints after utilization has been improved with Inference Components: 
 
An inference component abstracts ML models and enables assigning CPUs, GPU, and scaling policies per model. Inference components offer the following benefits: 
 
 SageMaker AI will optimally place and pack models onto ML instances to maximize utilization, leading to cost savings. 
 Each model scales independently based on custom configurations, providing optimal resource allocation to meet specific application requirements. 
 SageMaker AI will scale to add and remove instances dynamically to maintain availability while keeping idle compute to a minimum. 
 Organizations can scale down to zero copies of a model to free up resources for other models or specify to keep important models always loaded and ready to serve traffic for critical workloads. 
 
Configuring and managing inference component endpoints 
You create the SageMaker AI endpoint with an endpoint configuration that defines the instance type and initial instance count for the endpoint. The model is configured in a new construct, an inference component. Here, you specify the number of accelerators and amount of memory you want to allocate to each copy of a model, together with the model artifacts, container image, and number of model copies to deploy. 
As inference requests increase or decrease, the number of copies of your inference components can also scale up or down based on your auto scaling policies. SageMaker AI will handle the placement to optimize the packing of your models for availability and cost. 
In addition, if you enable managed instance auto scaling, SageMaker AI will scale compute instances according to the number of inference components that need to be loaded at a given time to serve traffic. SageMaker AI will scale up the instances and pack your instances and inference components to optimize for cost while preserving model performance. 
Refer to Reduce model deployment costs by 50% on average using the latest features of Amazon SageMaker for more details on how to use inference components. 
How Salesforce used Amazon SageMaker AI inference components 
Salesforce has several different proprietary models such as CodeGen originally spread across multiple SMEs. CodeGen is Salesforceâ€™s in-house open source LLM for code understanding and code generation. Developers can use the CodeGen model to translate natural language, such as English, into programming languages, such as Python. Salesforce developed an ensemble of CodeGen models (Inline for automatic code completion, BlockGen for code block generation, and FlowGPT for process flow generation) specifically tuned for the Apex programming language. The models are being used in ApexGuru, a solution within the Salesforce platform that helps Salesforce developers tackle critical anti-patterns and hotspots in their Apex code. 
Inference components enable multiple models to share GPU resources efficiently on the same endpoint. This consolidation not only delivers reduction in infrastructure costs through intelligent resource sharing and dynamic scaling, it also reduces operational overhead with lesser endpoints to manage. For their CodeGen ensemble models, the solution enabled model-specific resource allocation and independent scaling based on traffic patterns, providing optimal performance while maximizing infrastructure utilization. 
To expand hosting options on SageMaker AI without affecting stability, performance, or usability, Salesforce introduced inference component endpoints alongside the existing SME. 
This hybrid approach uses the strengths of each. SMEs provide dedicated hosting for each model and predictable performance for critical workloads with consistent traffic patterns, and inference components optimize resource utilization for variable workloads through dynamic scaling and efficient GPU sharing. 
The Salesforce AI Platform team created a SageMaker AI endpoint with the desired instance type and initial instance count for the endpoint to handle their baseline inference requirements. Model packages are then attached dynamically, spinning up individual containers as needed. They configured each model, for example, BlockGen and TextEval models as individual inference components specifying precise resource allocations, including accelerator count, memory requirements, model artifacts, container image, and number of model copies to deploy. With this approach, Salesforce could efficiently host multiple model variants on the same endpoint while maintaining granular control over resource allocation and scaling behaviors. 
By using the auto scaling capabilities, inference components can set up endpoints with multiple copies of models and automatically adjust GPU resources as traffic fluctuates. This allows each model to dynamically scale up or down within an endpoint based on configured GPU limits. By hosting multiple models on the same endpoint and automatically adjusting capacity in response to traffic fluctuations, Salesforce was able to significantly reduce the costs associated with traffic spikes. This means that Salesforce AI models can handle varying workloads efficiently without compromising performance. The graphic below shows Salesforceâ€™s endpoints before and after the models were deployed with inference components: 
 
This solution has brought several key benefits: 
 
 Optimized resource allocation â€“ Multiple models now efficiently share GPU resources, eliminating unnecessary provisioning while maintaining optimal performance. 
 Cost savings â€“ Through intelligent GPU resource management and dynamic scaling, Salesforce achieved significant reduction in infrastructure costs while eliminating idle compute resources. 
 Enhanced performance for smaller models â€“ Smaller models now use high-performance GPUs to meet their latency and throughput needs without incurring excessive costs. 
 
By refining GPU allocation at the model level through inference components, Salesforce improved resource efficiency and achieved a substantial reduction in operational cost while maintaining the high-performance standards their customers expect across a wide range of AI workloads. The cost savings are substantial and open up new opportunities for using high-end, expensive GPUs in a cost-effective manner. 
Conclusion 
Through their implementation of Amazon SageMaker AI inference components, Salesforce has transformed their AI infrastructure management, achieving up to an eight-fold reduction in deployment and infrastructure costs while maintaining high performance standards. The team learned that intelligent model packing and dynamic resource allocation were keys to solving their GPU utilization challenges across their diverse model portfolio. This implementation has transformed performance economics, allowing smaller models to use high performance GPUs, providing high throughput and low latency without the traditional cost overhead. 
Today, their AI platform efficiently serves both large proprietary models such as CodeGen and smaller workloads on the same infrastructure, with optimized resource allocation ensuring high-performance delivery. With this approach, Salesforce can maximize the utilization of compute instances, scale to hundreds of models, and optimize costs while providing predictable performance. This solution has not only solved their immediate challenges of optimizing GPU utilization and cost management but has also positioned them for future growth. By establishing a more efficient and scalable infrastructure foundation, Salesforce can now confidently expand their AI offerings and explore more advanced use cases with expensive, high-performance GPUs such as P4d, P5, and P5en, knowing they can maximize the value of every computing resource. This transformation represents a significant step forward in their mission to deliver enterprise-grade AI solutions while maintaining operational efficiency and cost-effectiveness. 
Looking ahead, Salesforce is poised to use the new Amazon SageMaker AI rolling updates capability for inference component endpoints, a feature designed to streamline updates for models of different sizes while minimizing operational overhead. This advancement will enable them to update their models batch by batch, rather than using the traditional blue/green deployment method, providing greater flexibility and control over model updates while using minimal extra instances, rather than requiring doubled instances as in the past. By implementing these rolling updates alongside their existing dynamic scaling infrastructure and incorporating real-time safety checks, Salesforce is building a more resilient and adaptable AI platform. This strategic approach not only provides cost-effective and reliable deployments for their GPU-intensive workloads but also sets the stage for seamless integration of future AI innovations and model improvements. 
Check out How Salesforce achieves high-performance model deployment with Amazon SageMaker AI to learn more. For more information on how to get started with SageMaker AI, refer to Guide to getting set up with Amazon SageMaker AI. To learn more about Inference Components, refer to Amazon SageMaker adds new inference capabilities to help reduce foundation model deployment costs and latency. 
 
About the Authors 
Rishu Aggarwal is a Director of Engineering at Salesforce based in Bangalore, India. Rishu leads the Salesforce AI Platform Model Serving Engineering team in solving the complex problems of inference optimizations and deployment of LLMs at scale within the Salesforce ecosystem. Rishu is a staunch Tech Evangelist for AI and has deep interests in Artificial Intelligence, Generative AI, Neural Networks and Big Data. 
Rielah De Jesus is a Principal Solutions Architect at AWS who has successfully helped various enterprise customers in the DC, Maryland, and Virginia area move to the cloud. In her current role, she acts as a customer advocate and technical advisor focused on helping organizations like Salesforce achieve success on the AWS platform. She is also a staunch supporter of women in IT and is very passionate about finding ways to creatively use technology and data to solve everyday challenges. 
Pavithra Hariharasudhan is a Senior Technical Account Manager and Enterprise Support Lead at AWS, supporting leading AWS Strategic customers with their global cloud operations. She assists organizations in resolving operational challenges and maintaining efficient AWS environments, empowering them to achieve operational excellence while accelerating business outcomes. 
Ruchita Jadav is a Senior Member of Technical Staff at Salesforce, with over 10 years of experience in software and machine learning engineering. Her expertise lies in building scalable platform solutions across the retail and CRM domains. At Salesforce, she leads initiatives focused on model hosting, inference optimization, and LLMOps, enabling efficient and scalable deployment of AI and large language models. She holds a Bachelor of Technology in Electronics &amp; Communication from Gujarat Technological University (GTU). 
Marc Karp is an ML Architect with the Amazon SageMaker Service team. He focuses on helping customers design, deploy, and manage ML workloads at scale. In his spare time, he enjoys traveling and exploring new places.
â€¢ Building a RAG chat-based assistant on Amazon EKS Auto Mode and NVIDIA NIMs
  Chat-based assistants powered by Retrieval Augmented Generation (RAG) are transforming customer support, internal help desks, and enterprise search, by delivering fast, accurate answers grounded in your own data. With RAG, you can use a ready-to-deploy foundation model (FM) and enrich it with your own data, making responses relevant and context-aware without the need for fine-tuning or retraining. Running these chat-based assistants on Amazon Elastic Kubernetes Service (Amazon EKS) gives you the flexibility to use a variety of FMs, retaining full control over your data and infrastructure. 
Amazon EKS scales with your workload and is cost-efficient for both steady and fluctuating demand. Because EKS is certified Kubernetes-conformant, it is compatible with existing applications running on a standard Kubernetes environment, whether hosted on on-premises data centers or public clouds. For your data plane, you can take advantage of a wide range of compute options, including CPUs, GPUs, AWS purpose-built AI chips (AWS Inferentia and AWS Trainium) and ARM-based CPU architectures (AWS Graviton), to match performance and cost requirements. Such flexibility makes Amazon EKS an ideal candidate for running heterogeneous workloads because you can compose different compute substrates, within the same cluster, to optimize both performance and cost efficiency. 
NVIDIA NIM microservices consist of microservices that deploy and serve FMs, integrating with AWS services such as Amazon Elastic Compute Cloud (Amazon EC2), Amazon EKS, and Amazon SageMaker. NIM microservices are distributed as Docker containers and are available through the NVIDIA NGC Catalog. Deploying GPU-accelerated models manually requires you to select and configure runtimes such as PyTorch or TensorFlow, set up inference servers such as Triton, implement model optimizations, and troubleshoot compatibility issues. This takes engineering time and expertise. NIM microservices eliminate this complexity by automating these technical decisions and configurations for you. 
The NVIDIA NIM Operator is a Kubernetes management tool that facilitates the operation of model-serving components and services. It handles large language models (LLMs), embedders, and other model types through NVIDIA NIM microservices within Kubernetes environments. The Operator streamlines microservice management through three primary custom resources. First, the NIMCache resource facilitates model downloading from NGC and network storage persistence. This enables multiple microservice instances to share a single cached model, improving microservice startup time. Second, the NIMService resource manages individual NIM microservices, creating Kubernetes deployments within specified namespaces. Third, the NIMPipeline resource functions as an orchestrator for multiple NIM service resources, allowing coordinated management of service groups. This architecture enables efficient operation and lifecycle management, with particular emphasis on reducing inference latency through model caching and supporting automated scaling capabilities. 
NVIDIA NIM, coupled with the NVIDIA NIM Operator, provide a streamlined solution to address the deployment complexities stated in the opening. In this post, we demonstrate the implementation of a practical RAG chat-based assistant using a comprehensive stack of modern technologies. The solution uses NVIDIA NIMs for both LLM inference and text embedding services, with the NIM Operator handling their deployment and management. The architecture incorporates Amazon OpenSearch Serverless to store and query high-dimensional vector embeddings for similarity search. 
The underlying Kubernetes infrastructure of the solution is provided by EKS Auto Mode, which supports GPU-accelerated Amazon Machine Images (AMIs) out of the box. These images include the NVIDIA device plugin, the NVIDIA container toolkit, precompiled NVIDIA kernel drivers, the Bottlerocket operating system, and Elastic Fabric Adapter (EFA) networking. You can use Auto Mode with Accelerated AMIs to spin up GPU instances, without manually installing and configuring GPU software components. Simply specify GPU-based instance types when creating Karpenter NodePools, and EKS Auto Mode will launch GPU-ready worker nodes to run your accelerated workloads. 
Solution overview 
The following architecture diagram shows how NVIDIA NIM microservices running on Amazon EKS Auto Mode power our RAG chat-based assistant solution. The design, shown in the following diagram, combines GPU-accelerated model serving with vector search in Amazon OpenSearch Serverless, using the NIM Operator to manage model deployment and caching through persistent Amazon Elastic File System (Amazon EFS) storage. 
 
Solution diagram (numbers indicate steps in the solution walkthrough section) 
The solution follows these high-level steps: 
 
 Create an EKS cluster 
 Set up Amazon OpenSearch Serverless 
 Create an EFS file system and set up necessary permissions 
 Create Karpenter GPU NodePool 
 Install NVIDIA Node Feature Discovery (NFD) and NIM Operator 
 Create nim-service namespace and NVIDIA secrets 
 Create&nbsp;NIMCaches 
 Create NIMServices 
 
Solution walkthrough 
In this section, we walk through the implementation of this RAG chat-based assistant solution step by step. We create an EKS cluster, configure Amazon OpenSearch Serverless and EFS storage, set up GPU-enabled nodes with Karpenter, deploy NVIDIA components for model serving, and finally integrate a chat-based assistant client using Gradio and LangChain. This end-to-end setup demonstrates how to combine LLM inference on Kubernetes with vector search capabilities, forming the foundation for a scalable, production-grade systemâ€”pending the addition of monitoring, auto scaling, and reliability features. 
Prerequisites 
To begin, ensure you have installed and set up the following required tools: 
 
 AWS CLI (version aws-cli/2.27.11 or later) 
 kubectl 
 eksctl (use version v0.195.0 or later to support Auto Mode) 
 Helm 
 
These tools need to be properly configured according to the Amazon EKS setup documentation. 
Clone the reference repository and cd into the root folder: 
 
 git clone https://github.com/aws-samples/sample-rag-chatbot-nim
cd sample-rag-chatbot-nim/infra 
 
Environment setup 
You need an NGC API key to authenticate and download NIM models. To generate the key, you can enroll (for free) in the NVIDIA Developer Program and then follow the NVIDIA guidelines. 
Next, set up a few environment variables (replace the values with your information): 
 
 export CLUSTER_NAME=automode-nims-blog-cluster
export AWS_DEFAULT_REGION={your region}
export NVIDIA_NGC_API_KEY={your key} 
 
Pattern deployment 
To perform the solution, complete the steps in the following sections. 
Create an EKS cluster 
Deploy the EKS cluster using EKS Auto Mode, with eksctl : 
 
 CHATBOT_SA_NAME=${CLUSTER_NAME}-client-service-account
IAM_CHATBOT_ROLE=${CLUSTER_NAME}-client-eks-pod-identity-role

cat &lt;&lt; EOF | eksctl create cluster -f -
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
&nbsp;&nbsp;name: ${CLUSTER_NAME}
&nbsp;&nbsp;region: ${AWS_DEFAULT_REGION}

autoModeConfig:
&nbsp;&nbsp;enabled: true

iam:
&nbsp;&nbsp;podIdentityAssociations:
&nbsp;&nbsp; &nbsp;- namespace: default
&nbsp;&nbsp; &nbsp; &nbsp;serviceAccountName: ${CHATBOT_SA_NAME}
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; createServiceAccount: true
&nbsp;&nbsp; &nbsp; &nbsp;roleName: ${IAM_CHATBOT_ROLE}
&nbsp;&nbsp; &nbsp; &nbsp;permissionPolicy:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;Version: "2012-10-17"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;Statement:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;- Effect: Allow
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Action:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;- "aoss:*"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Resource: "*"

addons:
- name: aws-efs-csi-driver
&nbsp;&nbsp;useDefaultPodIdentityAssociations: true
EOF 
 
Pod Identity Associations connect Kubernetes service accounts to AWS Identity and Access Management (IAM) roles, allowing pods to access AWS services securely. In this configuration, a service account will be created and associated with an IAM role, granting it full permissions to OpenSearch Serverless (in a production environment, restrict privileges according to the principle of least privilege). 
NIMCaches require volume AccessMode: ReadWriteMany. Amazon Elastic Block Store (Amazon EBS) volumes provided by EKS Auto Mode arenâ€™t suitable because they support ReadWriteOnce only and canâ€™t be mounted by multiple nodes. Storage options that support AccessMode: ReadWriteMany include Amazon EFS, as shown in this example, or Amazon FSx for Lustre, which offers higher performance for workloads with greater throughput or latency requirements. 
The preceding command will take a few minutes to be completed. When itâ€™s completed, eksctl configures your kubeconfig and points it to the new cluster. You can validate that the cluster is up and running and that the EFS addon is installed by entering the following command: 
 
 kubectl get pods --all-namespaces 
 
Expected output: 
 
 NAMESPACE &nbsp; &nbsp; NAME &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;READY &nbsp; STATUS &nbsp; &nbsp;RESTARTS &nbsp; AGE
kube-system &nbsp; efs-csi-controller-55b8dd6f57-wpzbg &nbsp; 3/3 &nbsp; &nbsp; Running &nbsp; 0 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;3m7s
kube-system &nbsp; efs-csi-controller-55b8dd6f57-z2gzc &nbsp; 3/3 &nbsp; &nbsp; Running &nbsp; 0 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;3m7s
kube-system &nbsp; efs-csi-node-6k5kz &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;3/3 &nbsp; &nbsp; Running &nbsp; 0 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;3m7s
kube-system &nbsp; efs-csi-node-pvv2v &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;3/3 &nbsp; &nbsp; Running &nbsp; 0 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;3m7s
kube-system &nbsp; metrics-server-6d67d68f67-7x4tg &nbsp; &nbsp; &nbsp; 1/1 &nbsp; &nbsp; Running &nbsp; 0 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;6m15s
kube-system &nbsp; metrics-server-6d67d68f67-l4xv6 &nbsp; &nbsp; &nbsp; 1/1 &nbsp; &nbsp; Running &nbsp; 0 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;6m15s 
 
Set up Amazon OpenSearch Serverless 
A vector database stores and searches through numerical representations of text (embeddings). Such a component is essential in RAG chat-based assistant architectures because it facilitates finding relevant information related to a user question based on semantic similarity rather than exact keyword matches. 
We use Amazon OpenSearch Service as the vector database. OpenSearch Service provides a managed solution for deploying, operating, and scaling OpenSearch clusters within AWS Cloud infrastructure. As part of this service, Amazon OpenSearch Serverless offers an on-demand configuration that automatically handles scaling to match your applicationâ€™s requirements. 
First, using AWS PrivateLink, create a private connection between the clusterâ€™s Amazon Virtual Private Cloud (Amazon VPC) connection&nbsp;and Amazon OpenSearch Serverless.&nbsp;This keeps traffic within the AWS network and avoids public internet routing. 
Enter the following commands to retrieve the clusterâ€™s virtual private cloud (VPC) ID, CIDR block range, and subnet IDs, and store them in corresponding environment variables: 
 
 VPC_ID=$(aws eks describe-cluster \
&nbsp;&nbsp; &nbsp;--name $CLUSTER_NAME \
&nbsp;&nbsp; &nbsp;--query "cluster.resourcesVpcConfig.vpcId" \
&nbsp;&nbsp; &nbsp;--output text \
&nbsp;&nbsp; &nbsp;--region=$AWS_DEFAULT_REGION)&nbsp;&amp;&amp; \
CIDR_RANGE=$(aws ec2 describe-vpcs \
&nbsp;&nbsp; &nbsp;--vpc-ids $VPC_ID \
&nbsp;&nbsp; &nbsp;--query "Vpcs[].CidrBlock" \
&nbsp;&nbsp; &nbsp;--output text \
&nbsp;&nbsp; &nbsp;--region $AWS_DEFAULT_REGION)&nbsp;&amp;&amp; \
SUBNET_IDS=($(aws eks describe-cluster \
&nbsp;&nbsp; &nbsp;--name $CLUSTER_NAME \
&nbsp;&nbsp; &nbsp;--query "cluster.resourcesVpcConfig.subnetIds[]" \
&nbsp;&nbsp; &nbsp;--region $AWS_DEFAULT_REGION \
&nbsp;&nbsp; &nbsp;--output text)) 
 
Use the following code to create a security group for OpenSearch Serverless in the VPC, add an inbound rule to the security group allowing HTTPS traffic (port 443) from your VPCâ€™s CIDR range, and create an OpenSearch Serverless VPC endpoint connected to the subnets and security group: 
 
 AOSS_SECURITY_GROUP_ID=$(aws ec2 create-security-group \
&nbsp;&nbsp; &nbsp;--group-name ${CLUSTER_NAME}-AOSSSecurityGroup \
&nbsp;&nbsp; &nbsp;--description "${CLUSTER_NAME} AOSS security group" \
&nbsp;&nbsp; &nbsp;--vpc-id $VPC_ID \
&nbsp;&nbsp; &nbsp;--region $AWS_DEFAULT_REGION \
&nbsp;&nbsp; &nbsp;--query 'GroupId' \
&nbsp;&nbsp; &nbsp;--output text)&nbsp;&amp;&amp; \
aws ec2 authorize-security-group-ingress \
&nbsp;&nbsp; &nbsp;--group-id $AOSS_SECURITY_GROUP_ID \
&nbsp;&nbsp; &nbsp;--protocol tcp \
&nbsp;&nbsp; &nbsp;--port 443 \
&nbsp;&nbsp; &nbsp;--region $AWS_DEFAULT_REGION \
&nbsp;&nbsp; &nbsp;--cidr $CIDR_RANGE&nbsp;&amp;&amp; \
VPC_ENDPOINT_ID=$(aws opensearchserverless create-vpc-endpoint \
&nbsp;&nbsp; &nbsp;--name ${CLUSTER_NAME}-aoss-vpc-endpoint \
&nbsp;&nbsp; &nbsp;--subnet-ids "${SUBNET_IDS[@]}" \
&nbsp;&nbsp; &nbsp;--security-group-ids $AOSS_SECURITY_GROUP_ID \
&nbsp;&nbsp; &nbsp;--region $AWS_DEFAULT_REGION \
&nbsp;&nbsp; &nbsp;--vpc-id $VPC_ID \
&nbsp;&nbsp; &nbsp;--query 'createVpcEndpointDetail.id' \
&nbsp;&nbsp; &nbsp;--output text)
 
 
In the following steps, create an OpenSearch Serverless collection (a logical unit to store and organize documents). 
 
 Create an encryption policy for the collection: 
 
 
 AOSS_COLLECTION_NAME=${CLUSTER_NAME}-collection
ENCRYPTION_POLICY_NAME=${CLUSTER_NAME}-encryption-policy
aws opensearchserverless create-security-policy \
&nbsp;&nbsp; &nbsp;--name ${ENCRYPTION_POLICY_NAME}\
&nbsp;&nbsp; &nbsp;--type encryption \
&nbsp;&nbsp; &nbsp;--policy "{\"Rules\":[{\"ResourceType\":\"collection\",\"Resource\":[\"collection/${AOSS_COLLECTION_NAME}\"]}],\"AWSOwnedKey\":true}" 
 
 
 The network policy that restricts access to the collection to only come through a specific VPC endpoint: 
 
 
 NETWORK_POLICY_NAME=${CLUSTER_NAME}-network-policy
aws opensearchserverless create-security-policy \
&nbsp;&nbsp; &nbsp;--name ${NETWORK_POLICY_NAME} \
&nbsp;&nbsp; &nbsp;--type network \
&nbsp;&nbsp; &nbsp;--policy&nbsp;"[{\"Description\":\"Allow VPC endpoint access\",\"Rules\":[{\"ResourceType\":\"collection\",\"Resource\":[\"collection/${AOSS_COLLECTION_NAME}\"]}],\"SourceVPCEs\":[\"$VPC_ENDPOINT_ID\"]}]" 
 
 
 The data policy that grants permissions to the IAM chat-based assistant role for interacting with indices in the collection: 
 
 
 DATA_POLICY_NAME=${CLUSTER_NAME}-data-policy
IAM_CHATBOT_ROLE_ARN=$(aws iam get-role --role-name ${IAM_CHATBOT_ROLE} --query 'Role.Arn' --output text)
aws opensearchserverless create-access-policy \
&nbsp;&nbsp; &nbsp;--name ${DATA_POLICY_NAME} \
&nbsp;&nbsp; &nbsp;--type data&nbsp;\
&nbsp;&nbsp; &nbsp;--policy "[{\"Rules\":[{\"ResourceType\":\"index\",\"Resource\":[\"index/${AOSS_COLLECTION_NAME}/*\"],\"Permission\":[\"aoss:CreateIndex\",\"aoss:DescribeIndex\",\"aoss:ReadDocument\",\"aoss:WriteDocument\",\"aoss:UpdateIndex\",\"aoss:DeleteIndex\"]}],\"Principal\":[\"${IAM_CHATBOT_ROLE_ARN}\"]}]"
 
 
 
 The OpenSearch collection itself: 
 
 
 AOSS_COLLECTION_ID=$(aws opensearchserverless create-collection \
&nbsp; &nbsp; --name ${AOSS_COLLECTION_NAME} \
&nbsp; &nbsp; --type VECTORSEARCH \
&nbsp; &nbsp; --region ${AWS_DEFAULT_REGION}&nbsp;\
&nbsp;&nbsp; &nbsp;--query 'createCollectionDetail.id'&nbsp;\
&nbsp;&nbsp; &nbsp;--output text)
 
 
Create EFS file system and set up necessary permissions 
Create an EFS file system: 
 
 EFS_FS_ID=$(aws efs create-file-system \
&nbsp;&nbsp; &nbsp;--region $AWS_DEFAULT_REGION \
&nbsp;&nbsp; &nbsp;--performance-mode generalPurpose \
&nbsp;&nbsp; &nbsp;--query 'FileSystemId' \
&nbsp;&nbsp; &nbsp;--output text) 
 
EFS requires mount targets, which are VPC network endpoints that connect your EKS nodes to the EFS file system. These mount targets must be reachable from your EKS worker nodes, and access is controlled using security groups. 
 
 Execute the following command to set up the mount targets and configure the necessary security group rules: 
 
 
 EFS_SECURITY_GROUP_ID=$(aws ec2 create-security-group \
&nbsp;&nbsp; &nbsp;--group-name ${CLUSTER_NAME}-EfsSecurityGroup \
&nbsp;&nbsp; &nbsp;--description "${CLUSTER_NAME} EFS security group" \
&nbsp;&nbsp; &nbsp;--vpc-id $VPC_ID \
&nbsp;&nbsp;&nbsp;&nbsp;--region $AWS_DEFAULT_REGION \
&nbsp;&nbsp;&nbsp;&nbsp;--query 'GroupId' \
&nbsp;&nbsp; &nbsp;--output text)&nbsp;&amp;&amp;&nbsp;\
aws ec2 authorize-security-group-ingress \
&nbsp;&nbsp; &nbsp;--group-id $EFS_SECURITY_GROUP_ID \
&nbsp;&nbsp; &nbsp;--protocol tcp \
&nbsp;&nbsp; &nbsp;--port 2049 \
&nbsp;&nbsp;&nbsp;&nbsp;--region $AWS_DEFAULT_REGION \
&nbsp;&nbsp; &nbsp;--cidr $CIDR_RANGE &amp;&amp;&nbsp;\
for subnet in $SUBNET_IDS; do
&nbsp;&nbsp; &nbsp;aws efs create-mount-target \
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;--file-system-id $EFS_FS_ID \
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;--subnet-id $subnet \
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;--security-groups $EFS_SECURITY_GROUP_ID \
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;--region $AWS_DEFAULT_REGION&nbsp;
done 
 
 
 Create the StorageClass in Amazon EKS for Amazon EFS: 
 
 
 cat &lt;&lt; EOF | kubectl apply -f -
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
&nbsp;&nbsp;name: efs
provisioner: efs.csi.aws.com
parameters:
&nbsp;&nbsp;provisioningMode: efs-ap
&nbsp;&nbsp;fileSystemId: ${EFS_FS_ID}
&nbsp;&nbsp;directoryPerms: "777"
EOF 
 
 
 Validate the EFS storage class: 
 
 
 kubectl get storageclass efs 
 
These are the expected results: 
 
 NAME &nbsp; PROVISIONER &nbsp; &nbsp; &nbsp; RECLAIMPOLICY &nbsp; VOLUMEBINDINGMODE &nbsp; ALLOWVOLUMEEXPANSION &nbsp; AGE
efs &nbsp; &nbsp;efs.csi.aws.com &nbsp; Delete &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Immediate &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; false &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;9s 
 
Create Karpenter GPU NodePool 
To create the Karpenter GPU NodePool, enter the following code: 
 
 cat &lt;&lt; EOF | kubectl apply -f -
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
&nbsp;&nbsp;name: gpu-node-pool
spec:
&nbsp;&nbsp;template:
&nbsp;&nbsp; &nbsp;metadata:
&nbsp;&nbsp; &nbsp; &nbsp;labels:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;type: karpenter
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;NodeGroupType: gpu-node-pool
&nbsp;&nbsp; &nbsp;spec:
&nbsp;&nbsp; &nbsp; &nbsp;nodeClassRef:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;group: eks.amazonaws.com
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;kind: NodeClass
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;name: default
&nbsp;&nbsp; &nbsp; &nbsp;taints:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;- key: nvidia.com/gpu
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;value: "Exists"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;effect: "NoSchedule"

&nbsp;&nbsp; &nbsp; &nbsp;requirements:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;- key: "eks.amazonaws.com/instance-family"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;operator: In
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;values: ["g5"]
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;- key: "eks.amazonaws.com/instance-size"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;operator: In
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;values: [ "2xlarge", "4xlarge", "8xlarge", "16xlarge", "12xlarge", "24xlarge"]
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;- key: "kubernetes.io/arch"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;operator: In
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;values: ["amd64"]
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;- key: "karpenter.sh/capacity-type"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;operator: In
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;values: ["on-demand"]

&nbsp;&nbsp;limits:
&nbsp;&nbsp; &nbsp;cpu: "1000"
EOF 
 
This NodePool is designed for GPU workloads using AWS G5 instances, which feature NVIDIA A10G GPUs. The taint ensures that only workloads specifically designed for GPU usage will be scheduled on these nodes, maintaining efficient resource utilization. In a production environment, you might want to consider using Amazon EC2 Spot Instances as well to optimize on costs. 
Enter the command to validate successful creation of the NodePool: 
 
 kubectl get nodepools 
 
These are the expected results: 
 
 NAME &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;NODECLASS &nbsp; NODES &nbsp; READY &nbsp; AGE
general-purpose &nbsp; default &nbsp; &nbsp; 0 &nbsp; &nbsp; &nbsp; True &nbsp; &nbsp;15m
gpu-node-pool &nbsp; &nbsp; default &nbsp; &nbsp; 0 &nbsp; &nbsp; &nbsp; True &nbsp; &nbsp;8s
system &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;default &nbsp; &nbsp; 2 &nbsp; &nbsp; &nbsp; True &nbsp; &nbsp;15m 
 
gpu-node-pool was created and has 0 nodes. To inspect nodes further, enter this command: 
 
 kubectl get nodes -o custom-columns=NAME:.metadata.name,READY:"status.conditions[?(@.type=='Ready')].status",OS-IMAGE:.status.nodeInfo.osImage,INSTANCE-TYPE:.metadata.labels.'node\.kubernetes\.io/instance-type' 
 
This is the expected output: 
 
 NAME &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;READY &nbsp; &nbsp;OS-IMAGE &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; INSTANCE-TYPE
i-0b0c1cd3d744883cd &nbsp; True &nbsp; &nbsp; Bottlerocket (EKS Auto) 2025.4.26 (aws-k8s-1.32) &nbsp; c6g.large
i-0e1f33e42fac76a09 &nbsp; True &nbsp; &nbsp; Bottlerocket (EKS Auto) 2025.4.26 (aws-k8s-1.32) &nbsp; c6g.large 
 
There are two instances, launched by EKS Auto Mode with non-accelerated Bottlerocket Amazon Machine Image (AMI) variant aws-k8s-1.32, and CPU-only (non-GPU) instance type c6g. 
Install NVIDIA NFD and NIM Operator 
The NFD is a Kubernetes plugin that identifies available hardware capabilities and system settings. NFD and NIM Operator are installed using Helm charts, each with their own custom resource definitions (CRDs). 
 
 Before proceeding with installation, verify if related CRDs exist in your cluster: 
 
 
 # Check for NFD-related CRDs
kubectl get crds | grep nfd

# Check for NIM-related CRDs
kubectl get crds | grep nim 
 
If these CRDs arenâ€™t present, both commands will return no results. 
 
 Add Helm repos: 
 
 
 helm repo add nfd https://kubernetes-sigs.github.io/node-feature-discovery/charts
helm repo add nvidia https://helm.ngc.nvidia.com/nvidia
helm repo update 
 
 
 Install the NFD dependency for NIM Operator: 
 
 
 helm install node-feature-discovery nfd/node-feature-discovery \
&nbsp;&nbsp;--namespace node-feature-discovery \
&nbsp;&nbsp;--create-namespace 
 
 
 Validate the pods are up and CRDs were created: 
 
 
 kubectl get po -n node-feature-discovery 
 
Expected output: 
 
 NAME &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; READY &nbsp; STATUS &nbsp; &nbsp;RESTARTS &nbsp; AGE
node-feature-discovery-gc-5b65f7f5b6-q4hlr &nbsp; &nbsp; &nbsp; 1/1 &nbsp; &nbsp; Running &nbsp; 0 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;79s
node-feature-discovery-master-767dcc6cb8-6hc2t &nbsp; 1/1 &nbsp; &nbsp; Running &nbsp; 0 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;79s
node-feature-discovery-worker-sg852 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;1/1 &nbsp; &nbsp; Running &nbsp; 0 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;43s
 
 
 
 kubectl get crds | grep nfd 
 
Expected output: 
 
 nodefeaturegroups.nfd.k8s-sigs.io &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;2025-05-05T01:23:16Z
nodefeaturerules.nfd.k8s-sigs.io &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 2025-05-05T01:23:16Z
nodefeatures.nfd.k8s-sigs.io &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 2025-05-05T01:23:16Z
 
 
 
 Install the NIM Operator: 
 
 
 helm install nim-operator nvidia/k8s-nim-operator \
&nbsp;&nbsp;--namespace nim-operator \
&nbsp;&nbsp;--create-namespace \
&nbsp;&nbsp;--version&nbsp;v2.0.0 
 
You might need to use version v1.0.1 for the NIM Operator instead of v2.0.0 as shown in the preceding code example because occasionally you might receive a â€œ402 Payment Requiredâ€ message. 
 
 Validate the pod is up and CRDs were created: 
 
 
 kubectl get po -n nim-operator 
 
Expected output: 
 
 NAME &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; READY &nbsp; STATUS &nbsp; &nbsp;RESTARTS &nbsp; AGE
nim-operator-k8s-nim-operator-6d988f78df-h4nqn &nbsp; 1/1 &nbsp; &nbsp; Running &nbsp; 0 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;24s
 
 
 
 kubectl get crds | grep nim 
 
Expected output: 
 
 nimcaches.apps.nvidia.com &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;2025-05-05T01:18:00Z
nimpipelines.apps.nvidia.com &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 2025-05-05T01:18:00Z
nimservices.apps.nvidia.com &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;2025-05-05T01:18:01Z
 
 
Create nim-service namespace and NVIDIA secrets 
In this section, create the nim-service namespace and add two secrets containing your NGC API key. 
 
 Create namespace and secrets: 
 
 
 kubectl create namespace nim-service
kubectl create secret -n nim-service docker-registry ngc-secret \
&nbsp; &nbsp; --docker-server=nvcr.io \
&nbsp; &nbsp; --docker-username='$oauthtoken' \
&nbsp; &nbsp; --docker-password=$NVIDIA_NGC_API_KEY
kubectl create secret -n nim-service generic ngc-api-secret \
&nbsp; &nbsp; --from-literal=NGC_API_KEY=$NVIDIA_NGC_API_KEY 
 
 
 Validate secrets were created: 
 
 
 kubectl -n nim-service get secrets 
 
The following is the expected result: 
 
 NAME &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; TYPE &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; DATA &nbsp; AGE
ngc-api-secret &nbsp; Opaque &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 1 &nbsp; &nbsp; &nbsp;13s
ngc-secret &nbsp; &nbsp; &nbsp; kubernetes.io/dockerconfigjson &nbsp; 1 &nbsp; &nbsp; &nbsp;14s 
 
ngc-secret is a Docker registry secret used to authenticate and pull NIM container images from NVIDIAâ€™s NGC container registry. 
ngc-api-secret is a generic secret used by the model puller init container to authenticate and download models from the same registry. 
Create NIMCaches 
RAG enhances chat applications by enabling AI models to access either internal domain-specific knowledge or external knowledge bases, reducing hallucinations and providing more accurate, up-to-date responses. In a RAG system, a knowledge base is created from domain-specific documents. These documents are sliced into smaller pieces of text. The text pieces and their generated embeddings are then uploaded to a vector database. Embeddings are numerical representations (vectors) that capture the meaning of text, where similar text content results in similar vector values. When questions are received from users, theyâ€™re also sent with their respective embeddings to the database for semantic similarity search. The database returns the closest matching chunks of text, which are used by an LLM to provide a domain-specific answer. 
We use Metaâ€™s llama-3-2-1b-instruct as LLM and NVIDIA Retrieval QA E5 (embedqa-e5-v5) as embedder. 
This section covers the deployment of NIMCaches for storing both the LLM and embedder models. Local storage of these models speeds up pod initialization by eliminating the need for repeated downloads. Our llama-3-2-1b-instruct LLM, with 1B parameters, is a relatively small model and uses 2.5 GB of storage space. The storage requirements and initialization time increase when larger models are used. Although the initial setup of the LLM and embedder caches takes 10â€“15 minutes, subsequent pod launches will be faster because the models are already available in the clusterâ€™s local storage. 
Enter the following command: 
 
 kubectl apply -f nim-caches.yaml 
 
This is the expected output: 
 
 nimcache.apps.nvidia.com/nv-embedqa-e5-v5 created
nimcache.apps.nvidia.com/meta-llama-3-2-1b-instruct created 
 
NIMCaches will create PersistentVolumes (PVs) and PersistentVolumeClaims (PVCs) to store the models, with STORAGECLASS&nbsp;efs: 
 
 kubectl get -n nim-service pv,pvc 
 
The following is the expected output: 
 
 NAME &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;CAPACITY &nbsp; ACCESS MODES &nbsp; RECLAIM POLICY &nbsp; STATUS &nbsp; CLAIM &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;STORAGECLASS &nbsp; VOLUMEATTRIBUTESCLASS &nbsp; REASON &nbsp; AGE
persistentvolume/pvc-5fa98625-ea65-4aef-99ff-ca14001afb47 &nbsp; 50Gi &nbsp; &nbsp; &nbsp; RWX &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Delete &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Bound &nbsp; &nbsp;nim-service/nv-embedqa-e5-v5-pvc &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; efs &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&lt;unset&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;77s
persistentvolume/pvc-ab67e4dc-53df-47e7-95c8-ec6458a57a01 &nbsp; 50Gi &nbsp; &nbsp; &nbsp; RWX &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Delete &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Bound &nbsp; &nbsp;nim-service/meta-llama-3-2-1b-instruct-pvc &nbsp; efs &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&lt;unset&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;76s

NAME &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; STATUS &nbsp; VOLUME &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; CAPACITY &nbsp; ACCESS MODES &nbsp; STORAGECLASS &nbsp; VOLUMEATTRIBUTESCLASS &nbsp; AGE
persistentvolumeclaim/meta-llama-3-2-1b-instruct-pvc &nbsp; Bound &nbsp; &nbsp;pvc-ab67e4dc-53df-47e7-95c8-ec6458a57a01 &nbsp; 50Gi &nbsp; &nbsp; &nbsp; RWX &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;efs &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&lt;unset&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 77s
persistentvolumeclaim/nv-embedqa-e5-v5-pvc &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Bound &nbsp; &nbsp;pvc-5fa98625-ea65-4aef-99ff-ca14001afb47 &nbsp; 50Gi &nbsp; &nbsp; &nbsp; RWX &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;efs &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&lt;unset&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 77s 
 
Enter the following to validate NIMCaches: 
 
 kubectl get nimcaches -n nim-service 
 
This is the expected output (STATUS will stay initially blank, then become InProgress for 10â€“15 mins until model download is complete): 
 
 NAME &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; STATUS &nbsp; PVC &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;AGE
meta-llama-3-2-1b-instruct &nbsp; Ready &nbsp; &nbsp;meta-llama-3-2-1b-instruct-pvc &nbsp; 13m
nv-embedqa-e5-v5 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Ready &nbsp; &nbsp;nv-embedqa-e5-v5-pvc &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;13m 
 
Create NIMServices 
NIMServices are custom resources to manage NVIDIA NIM microservices. To deploy the LLM and embedder services enter the following: 
 
 kubectl apply -f nim-services.yaml 
 
The following is the expected output: 
 
 nimservice.apps.nvidia.com/meta-llama-3-2-1b-instruct created
nimservice.apps.nvidia.com/nv-embedqa-e5-v5 created 
 
Validate the NIMServices: 
 
 kubectl get nimservices -n nim-service 
 
The following is the expected output: 
 
 NAME &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; STATUS &nbsp; AGE
meta-llama-3-2-1b-instruct &nbsp; Ready &nbsp; &nbsp;5m25s
nv-embedqa-e5-v5 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Ready &nbsp; &nbsp;5m24s 
 
Our models are stored in an EFS volume, which is mounted to the EC2 instances as a PVC. That translates to faster pod startup times. In fact, notice in the preceding example that the NIMServices are ready in approximately 5 minutes. This time includes GPU node(s) launch from Karpenter and container image pull and launch. 
Compared to the 10â€“15 minutes required for internet-based model downloads, as experienced during the NIMCaches deployment, loading models from the local cache reduces startup time considerably, enhancing the overall system scaling speed. Should you need even more performing storage alternatives, you could explore alternatives such as Amazon FSx for Lustre. 
Enter the following command to check the nodes again: 
 
 kubectl get nodes -o custom-columns=NAME:.metadata.name,READY:"status.conditions[?(@.type=='Ready')].status",OS-IMAGE:.status.nodeInfo.osImage,INSTANCE-TYPE:.metadata.labels.'node\.kubernetes\.io/instance-type' 
 
The following is the expected output: 
 
 NAME &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;READY &nbsp; OS-IMAGE &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;INSTANCE-TYPE
i-0150ecedccffcc17f &nbsp; True &nbsp; &nbsp;Bottlerocket (EKS Auto) 2025.4.26 (aws-k8s-1.32) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;c6g.large
i-027bf5419d63073cf &nbsp; True &nbsp; &nbsp;Bottlerocket (EKS Auto) 2025.4.26 (aws-k8s-1.32) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;c5a.large
i-0a1a1f39564fbf125 &nbsp; True &nbsp; &nbsp;Bottlerocket (EKS Auto, Nvidia) 2025.4.21 (aws-k8s-1.32-nvidia) &nbsp; g5.2xlarge
i-0d418bd8429dd12cd &nbsp; True &nbsp; &nbsp;Bottlerocket (EKS Auto, Nvidia) 2025.4.21 (aws-k8s-1.32-nvidia) &nbsp; g5.2xlarge 
 
Karpenter launched two new GPU instances to support NIMServices, with a Bottlerocket accelerated AMI variant Bottlerocket (EKS Auto, Nvidia) 2025.4.21 (aws-k8s-1.32-nvidia). The number and type of instances launched might vary depending on Karpenterâ€™s algorithm, which takes into consideration parameters such as instance availability and cost. 
Confirm that the&nbsp;NIMService STATUS&nbsp;is&nbsp;Ready before progressing further. 
Chat-based assistant client 
We now use a Python client, implementing the chat-based assistant interface, using the Gradio and LangChain libraries. Gradio creates the web interface and chat components, handling the frontend presentation. LangChain connects various components and implements RAG through multiple services in our EKS cluster. Metaâ€™s llama-3-2-1b-instruct serves as the base language model, and nv-embedqa-e5-v5 creates text embeddings. OpenSearch acts as the vector store, managing these embeddings and enabling similarity search. This setup allows the chat-based assistant to retrieve relevant information and generate contextual responses. 
 
Sequence diagram showing question-answering workflow with document upload process 
 
 Enter the following commands to deploy the client, hosted on Amazon Elastic Container Registry (Amazon ECR) as a container image in the public gallery (the applicationâ€™s source files are available in the client folder of the cloned repository): 
 
 
 AOSS_INDEX=${CLUSTER_NAME}-index
CHATBOT_CONTAINER_IMAGE=public.ecr.aws/h6c7e9p3/aws-rag-chatbot-eks-nims:1.0

cat &lt;&lt; EOF | kubectl apply -f -
apiVersion: v1
kind: Service
metadata:
&nbsp;&nbsp;name: rag-chatbot
&nbsp;&nbsp;labels:
&nbsp;&nbsp; &nbsp;app: rag-chatbot
spec:
&nbsp;&nbsp;ports:
&nbsp;&nbsp;- port: 7860
&nbsp;&nbsp; &nbsp;protocol: TCP
&nbsp;&nbsp;selector:
&nbsp;&nbsp; &nbsp;app: rag-chatbot
---
apiVersion: apps/v1
kind: Deployment
metadata:
&nbsp;&nbsp;name: rag-chatbot
spec:
&nbsp;&nbsp;selector:
&nbsp;&nbsp; &nbsp;matchLabels:
&nbsp;&nbsp; &nbsp; &nbsp;app: rag-chatbot
&nbsp;&nbsp;template:
&nbsp;&nbsp; &nbsp;metadata:
&nbsp;&nbsp; &nbsp; &nbsp;labels:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;app: rag-chatbot
&nbsp;&nbsp; &nbsp;spec:
&nbsp;&nbsp; &nbsp; &nbsp;serviceAccountName: ${CHATBOT_SA_NAME}
&nbsp;&nbsp; &nbsp; &nbsp;containers:
&nbsp;&nbsp; &nbsp; &nbsp;- name: rag-chatbot
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;image: ${CHATBOT_CONTAINER_IMAGE}
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;ports:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;- containerPort: 7860
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;protocol: TCP
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;env:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;- name: AWS_DEFAULT_REGION
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;value: ${AWS_DEFAULT_REGION}
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;- name: OPENSEARCH_COLLECTION_ID
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;value: ${AOSS_COLLECTION_ID}
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;- name: OPENSEARCH_INDEX
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;value: ${AOSS_INDEX}
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;- name: LLM_URL
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;value: "http://meta-llama-3-2-1b-instruct.nim-service.svc.cluster.local:8000/v1"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;- name: EMBEDDINGS_URL
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;value: "http://nv-embedqa-e5-v5.nim-service.svc.cluster.local:8000/v1"
EOF 
 
 
 Check the client pod status: 
 
 
 kubectl get pods 
 
The following is the example output: 
 
 NAME &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; READY &nbsp; STATUS &nbsp; &nbsp;RESTARTS &nbsp; AGE
rag-chatbot-6678cd95cb-4mwct &nbsp; 1/1 &nbsp; &nbsp; Running &nbsp; 0 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;60s 
 
 
 Port-forward the clientâ€™s service: 
 
 
 kubectl port-forward service/rag-chatbot 7860:7860 &amp; 
 
 
 Open a browser window at http://127.0.0.1:7860. 
 
In the following screenshot, we prompted the chat-based assistant about a topic that isnâ€™t in its knowledge base yet: â€œWhat is Amazon Nova Canvas.â€ 
 
The chat-based assistant canâ€™t find information on the topic and canâ€™t formulate a proper answer. 
 
 Download the file at location: https://docs.aws.amazon.com/pdfs/ai/responsible-ai/nova-canvas/nova-canvas.pdf and upload its embeddings to OpenSearch Serverless using the client UI, switching to the Document upload tab, in the top left, as shown in the following screenshot. 
 
 
The expected result is nova-canvas.pdf appearing the list of uploaded files, as shown in the following screenshot. 
 
 
 Wait 15â€“30 seconds for OpenSearch Serverless to process and index the data. Ask the same question, â€œWhat is Amazon Nova Canvas,â€ and you will receive a different answer, as shown in the following screenshot. 
 
 
Cleanup 
To clean up the cluster and the EFS resources created so far, enter the following command: 
 
 aws efs describe-mount-targets \
    --region $AWS_DEFAULT_REGION \
    --file-system-id $EFS_FS_ID \
    --query 'MountTargets[*].MountTargetId' \
    --output text \
    | xargs -n1 aws efs delete-mount-target \
        --region $AWS_DEFAULT_REGION \
        --mount-target-id
&nbsp; 
 
Wait approximately 30 seconds for the mount targets to be removed, then enter the following command: 
 
 aws efs delete-file-system --file-system-id $EFS_FS_ID --region $AWS_DEFAULT_REGION
eksctl delete cluster --name=$CLUSTER_NAME --region $AWS_DEFAULT_REGION 
 
To delete the OpenSearch Serverless collection and policies, enter the following command: 
 
 aws opensearchserverless delete-collection \
&nbsp;&nbsp; &nbsp;--id ${AOSS_COLLECTION_ID}

aws opensearchserverless delete-security-policy \
&nbsp;&nbsp; &nbsp;--name ${ENCRYPTION_POLICY_NAME}&nbsp;\
&nbsp;&nbsp; &nbsp;--type encryption
&nbsp;&nbsp;&nbsp;&nbsp;
aws opensearchserverless delete-security-policy \
&nbsp;&nbsp; &nbsp;--name ${NETWORK_POLICY_NAME} \
&nbsp;&nbsp; &nbsp;--type network

aws opensearchserverless delete-access-policy \
&nbsp;&nbsp; &nbsp;--name ${DATA_POLICY_NAME} \
&nbsp;&nbsp; &nbsp;--type data 
 
Conclusion 
In this post, we showed how to deploy a RAG-enabled chat-based assistant on Amazon EKS, using NVIDIA NIM microservices, integrating an LLM for text generation, an embedding model, and Amazon OpenSearch Serverless for vector storage. Using EKS Auto Mode with GPU-accelerated AMIs, we streamlined our deployment by automating the setup of GPU infrastructure. We specified GPU-based instance types in our Karpenter NodePools, and the system automatically provisioned worker nodes with all necessary NVIDIA components, including device plugins, container toolkit, and kernel drivers. The implementation demonstrated the effectiveness of RAG, with the chat-based assistant providing informed responses when accessing relevant information from its knowledge base. This architecture showcases how Amazon EKS can streamline the deployment of AI solutions, maintaining production-grade reliability and scalability. 
As a challenge, try enhancing the chat-based assistant application by implementing chat history functionality to preserve context across conversations. This allows the LLM to reference previous exchanges and provide more contextually relevant responses. To further learn how to run artificial intelligence and machine learning (AI/ML) workloads on Amazon EKS, check out our EKS best practices guide for running AI/ML workloads, join one of our Get Hands On with Amazon EKS event series, and visit AI on EKS deployment-ready blueprints. 
 
About the authors 
Riccardo Freschi is a Senior Solutions Architect at AWS who specializes in Modernization. He helps partners and customers transform their IT landscapes by designing and implementing modern cloud-native architectures on AWS. His focus areas include container-based applications on Kubernetes, cloud-native development, and establishing modernization strategies that drive business value. 
 Christina Andonov is a Sr. Specialist Solutions Architect at AWS, helping customers run AI workloads on Amazon EKS with open source tools. Sheâ€™s passionate about Kubernetes and known for making complex concepts easy to understand.

â¸»