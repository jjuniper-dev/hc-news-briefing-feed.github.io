‚úÖ Morning News Briefing ‚Äì August 05, 2025 10:51

üìÖ Date: 2025-08-05 10:51
üè∑Ô∏è Tags: #briefing #ai #publichealth #digitalgov

‚∏ª

üßæ Weather
‚Ä¢ No watches or warnings in effect, Pembroke
  No watches or warnings in effect. No warnings or watches or watches in effect . Watch or warnings are no longer in effect in the U.S. No watches, warnings are in effect for the rest of the day . No watches and warnings are still in effect, but no watches are in place for the day's events . The weather is not expected to be affected by the weather .
‚Ä¢ Current Conditions: Mostly Cloudy, 10.5¬∞C
  Temperature: 10.5&deg;C Pressure: 102.8 kPa Visibility: 24 km Humidity: 95 % Dewpoint: 9.8&deg:C Wind: SW 4 km/h Air Quality Health Index: n/a . Observed at: Garrison Petawawa 6:00 AM EDT Tuesday 5 August 2025 Condition: Mostly Cloudy. Temperature:
‚Ä¢ Tuesday: Sunny. High 26.
  Sunny. High 26. Humidex 31. UV index 8 or very high. Sunny. Sunny . Sunny . High 26 . Sunny. Low humidity . High wind gusts up to 40C in the coming days . Forecast issued 5:00 AM EDT Tuesday 5 August 2025 . For confidential support call the National Suicide Prevention Lifeline on 1-800-273-8255 .

üåç International News
No updates.

üçÅ Canadian News
No updates.

üá∫üá∏ U.S. Top Stories
‚Ä¢ This Tuscan startup sold all its olive oil in the U.S. Then came Trump's tariffs
  Americans love olive oil ‚Äî and import 95% of it . But tariffs are making it harder for Europeans to sell it to Americans . Americans love their olive oil and import it from all over the world, but it's hard to sell to the U.S. Americans also love their own olive oil from the EU, but they import it too much from the continent, according to experts .
‚Ä¢ He said, she said, it said: I used ChatGPT as a couple's counselor. How did we fare?
  AI chatbot took my side when I challenged the bot's biases ‚Äî and my own ‚Äî to make a breakthrough . It was only when she challenged the chatbot's biases and her own that we had a communication breakthrough . The chatbot is a tool that can be used to help people understand each other's feelings, she says . For more from CNN Tech Heroics, visit CNN Tech
‚Ä¢ Hurricane Katrina was a catalyst for change in New Orleans' public defender office
  Hurricane Katrina exposed longstanding flaws in the New Orleans criminal justice system . In the 20 years since Katrina, there has been dramatic change in the public defender office . The public defender's office in New Orleans has been transformed in the last 20 years . The office is now housed in the state of New Orleans, Louisiana, and the city's public defender system is undergoing a major overhaul in recent years
‚Ä¢ Morning news brief
  Texas Republicans threaten to arrest Democratic lawmakers unless they return to the State House . A NASA satellite that scientists and farmers rely on may be destroyed on purpose . Israel's government weighs a full occupation of Gaza, as ceasefire talks stall . Texas GOP threatens to arrest Democrats unless they come back to the state House . NASA satellite may have been destroyed by a deliberate attack on the satellite, scientists say .
‚Ä¢ As ceasefire talks stall, Israel's government weighs a full occupation of Gaza
  Israeli government considers a full occupation of the Gaza Strip, including areas where hostages are held . U.S. Middle East envoy, Steve Witkoff, visited Israel days ago and met with hostages' families . Israeli government considered full occupation, including those held in Gaza, a source of concern . Israel says it considers a 'full occupation' of Gaza, including the areas held by Hamas

üß† Artificial Intelligence
No updates.

üíª Digital Strategy
‚Ä¢ Hacker summer camp: What to expect from BSides, Black Hat, and DEF CON
  The security industry is hitting Vegas hard this week with three conferences in Sin City . The world's largest collection of security pros are gathered for the annual summer camp . You can even stream a few of the events to keep an eye on this week's security conference events on CNN.com Live.com/SecurityConventiones.com and YouTube.com will broadcast live from Las Vegas .
‚Ä¢ TSMC launches legal action against insiders accused of trade secret theft
  Chipmaker said it caught the ‚Äòunauthorized activity‚Äô early Taiwan Semiconductor Manufacturing Co (TSMC) launched legal proceedings against an unknown number of employees as it investigates a potential breach of trade secrets . TSCM said it has launched legal action against unknown employees . Chipmaker says it caught ‚Äòunauthorized activity early‚Äô and is investigating the breach .
‚Ä¢ Microsoft promises to eventually make WinUI 'truly open source'
  Microsoft lead software engineer Beth Pan has stated that WinUI, the modern user interface framework for Windows, will be made "truly open source" though no date is yet set because of deep entanglements with proprietary code in the operating system . Developer community skeptical following 'long silent stagnation' of the framework and accompanying SDK . No date has been set for the release of WinUI .
‚Ä¢ Skyrora wins green light to lob rockets from Scotland
  UK's Civil Aviation Authority has granted British rocketeer Skyrora a launch operator license . Launch license issued for suborbital Skylark L has been granted by the CAA . The CAA has issued a license to operate a sub-orbital rocket . The UK has also granted a license for the sub-orbital Skylarks to launch . The license was issued by
‚Ä¢ Germany and Japan teamed their ISS robots for seek-and-photograph mission
  Japanese space agency JAXA and Germany‚Äôs DLR have conducted what they say is the first collaboration between independently developed robots on the International Space Station . Bot built by Airbus and IBM recognized astronaut's voice and issued instructions to camera drone . Drone was created by Airbus, IBM and IBM, and is now on the ISS in orbit around the world . Robot was programmed to recognize astronaut

üè• Public Health
No updates.

üî¨ Science
‚Ä¢ Exploring the role of mixed reality education in maternal self efficacy and satisfaction with breastfeeding
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Comparative study of early onset cancer burden between China and the United States from 1990 to 2021
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Psychometric validation and correlates of the personal safety perception scale (PSPS-26) as a multidimensional measure of perceived safety
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Overview of vaccines for adults authorized, recommended, and implemented in the European Union
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ A nationwide population-based study on epidemiologic characteristics and treatment patterns of dry eye disease in South Korea
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations

üßæ Government & Policy
No updates.

üèõÔ∏è Enterprise Architecture & IT Governance
No updates.

ü§ñ AI & Emerging Tech
‚Ä¢ A glimpse into OpenAI‚Äôs largest ambitions
  OpenAI has given itself a dual mandate: to create AI that benefits all of humanity . Chief research officer Mark Chen and chief scientist Jakub Pachocki spoke with Will Douglas Heaven about the company's mission to create 'artificial general intelligence' The two figures at OpenAI most responsible for pursuing the latter ambitions say they're not interested in AI that outcompetes humans . Chen said he thought maybe it‚Äôs fine for AI to excel at math and coding, but the idea of having an AI acquire people skills is perhaps not .
‚Ä¢ These protocols will help AI agents navigate our messy lives
  A growing number of companies are launching AI agents that can do things on your behalf‚Äîactions like sending an email, making a document, or editing a database. Initial reviews for these agents have been mixed at best, though, because they struggle to interact with all the different components of our digital lives.



Part of the problem is that we are still building the necessary infrastructure to help agents navigate the world. If we want agents to complete tasks for us, we need to give them the necessary tools while also making sure they use that power responsibly.



Anthropic and Google are among the companies and groups working on exactly that. Over the past year, they have both introduced protocols that try to define how AI agents should interact with each other and the world around them. These protocols could make it easier for agents to control other programs like email clients and note-taking apps.¬†



The reason has to do with application programming interfaces, the connections between computers or programs that govern much of our online world. APIs currently reply to ‚Äúpings‚Äù with standardized information. But AI models aren‚Äôt made to work exactly the same every time. The very randomness that helps them come across as conversational and expressive also makes it difficult for them to both call an API and understand the response.&nbsp;



‚ÄúModels speak a natural language,‚Äù says Theo Chu, a project manager at Anthropic. ‚ÄúFor [a model] to get context and do something with that context, there is a translation layer that has to happen for it to make sense to the model.‚Äù Chu works on one such translation technique, the Model Context Protocol (MCP), which Anthropic introduced at the end of last year.&nbsp;





MCP attempts to standardize how AI agents interact with the world via various programs, and it‚Äôs already very popular. One web aggregator for MCP servers (essentially, the portals for different programs or tools that agents can access) lists over 15,000 servers already.&nbsp;



Working out how to govern how AI agents interact with each other is arguably an even steeper challenge, and it‚Äôs one the Agent2Agent protocol (A2A), introduced by Google in April, tries to take on. Whereas MCP translates requests between words and code, A2A tries to moderate exchanges between agents, which is an ‚Äúessential next step for the industry to move beyond single-purpose agents,‚Äù Rao Surapaneni, who works with A2A at Google Cloud, wrote in an email to MIT Technology Review.&nbsp;



Google says 150 companies have already partnered with it to develop and adopt A2A, including Adobe and Salesforce. At a high level, both MCP and A2A tell an AI agent what it absolutely needs to do, what it should do, and what it should not do to ensure a safe interaction with other services. In a way, they are complementary‚Äîeach agent in an A2A interaction could individually be using MCP to fetch information the other asks for.&nbsp;



However, Chu stresses that it is ‚Äúdefinitely still early days‚Äù for MCP, and the A2A road map lists plenty of tasks still to be done. We‚Äôve identified the three main areas of growth for MCP, A2A, and other agent protocols: security, openness, and efficiency.







What should these protocols say about security?



Researchers and developers still don‚Äôt really understand how AI models work, and new vulnerabilities are being discovered all the time. For chatbot-style AI applications, malicious attacks can cause models to do all sorts of bad things, including regurgitating training data and spouting slurs. But for AI agents, which interact with the world on someone‚Äôs behalf, the possibilities are far riskier.&nbsp;



For example, one AI agent, made to read and send emails for someone, has already been shown to be vulnerable to what‚Äôs known as an indirect prompt injection attack. Essentially, an email could be written in a way that hijacks the AI model and causes it to malfunction. Then, if that agent has access to the user‚Äôs files, it could be instructed to send private documents to the attacker.&nbsp;



Some researchers believe that protocols like MCP should prevent agents from carrying out harmful actions like this. However, it does not at the moment. ‚ÄúBasically, it does not have any security design,‚Äù says Zhaorun Chen, a&nbsp; University of Chicago PhD student who works on AI agent security and uses MCP servers.&nbsp;



Bruce Schneier, a security researcher and activist, is skeptical that protocols like MCP will be able to do much to reduce the inherent risks that come with AI and is concerned that giving such technology more power will just give it more ability to cause harm in the real, physical world. ‚ÄúWe just don‚Äôt have good answers on how to secure this stuff,‚Äù says Schneier. ‚ÄúIt‚Äôs going to be a security cesspool really fast.‚Äù&nbsp;



Others are more hopeful. Security design could be added to MCP and A2A similar to the way it is for internet protocols like HTTPS (though the nature of attacks on AI systems is very different). And Chen and Anthropic believe that standardizing protocols like MCP and A2A can help make it easier to catch and resolve security issues even as is. Chen uses MCP in his research to test the roles different programs can play in attacks to better understand vulnerabilities. Chu at Anthropic believes that these tools could let cybersecurity companies more easily deal with attacks against agents, because it will be easier to unpack who sent what.&nbsp;







How open should these protocols be?



Although MCP and A2A are two of the most popular agent protocols available today, there are plenty of others in the works. Large companies like Cisco and IBM are working on their own protocols, and other groups have put forth different designs like Agora, designed by researchers at the University of Oxford, which upgrades an agent-service communication from human language to structured data in real time.



Many developers hope there could eventually be a registry of safe, trusted systems to navigate the proliferation of agents and tools. Others, including Chen, want users to be able to rate different services in something like a Yelp for AI agent tools. Some more niche protocols have even built blockchains on top of MCP and A2A so that servers can show they are not just spam.&nbsp;



Both MCP and A2A are open-source, which is common for would-be standards as it lets others work on building them. This can help protocols develop faster and more transparently.&nbsp;



‚ÄúIf we go build something together, we spend less time overall, because we‚Äôre not having to each reinvent the wheel,‚Äù says David Nalley, who leads developer experience at Amazon Web Services and works with a lot of open-source systems, including A2A and MCP.&nbsp;



Nalley oversaw Google‚Äôs donation of A2A to the Linux Foundation, a nonprofit organization that guides open-source projects, back in June. With the foundation‚Äôs stewardship, the developers who work on A2A (including employees at Google and many others) all get a say in how it should evolve. MCP, on the other hand, is owned by Anthropic and licensed for free. That is a sticking point for some open-source advocates, who want others to have a say in how the code base itself is developed.&nbsp;



‚ÄúThere‚Äôs admittedly some increased risk around a single person or a single entity being in absolute control,‚Äù says Nalley. He says most people would prefer multiple groups to have a ‚Äúseat at the table‚Äù to make sure that these protocols are serving everyone‚Äôs best interests.&nbsp;



However, Nalley does believe Anthropic is acting in good faith‚Äîits license, he says, is incredibly permissive, allowing other groups to create their own modified versions of the code (a process known as ‚Äúforking‚Äù).&nbsp;



‚ÄúSomeone could fork it if they needed to, if something went completely off the rails,‚Äù says Nalley. IBM‚Äôs Agent Communication Protocol was actually spun off of MCP.&nbsp;



Anthropic is still deciding exactly how to develop MCP. For now, it works with a steering committee of outside companies that help make decisions on MCP‚Äôs development, but Anthropic seems open to changing this approach. ‚ÄúWe are looking to evolve how we think about both ownership and governance in the future,‚Äù says Chu.







Is natural language fast enough?



MCP and A2A work on the agents‚Äô terms‚Äîthey use words and phrases (termed natural language in AI), just as AI models do when they are responding to a person. This is part of the selling point for these protocols, because it means the model doesn‚Äôt have to be trained to talk in a way that is unnatural to it. ‚ÄúAllowing a natural-language interface to be used between agents and not just with humans unlocks sharing the intelligence that is built into these agents,‚Äù says Surapaneni.



But this choice does come with drawbacks. Natural-language interfaces lack the precision of APIs, and that could result in incorrect responses. And it creates inefficiencies.&nbsp;





Usually, an AI model reads and responds to text by splitting words into tokens. The AI model will read a prompt, split it into input tokens, generate a response in the form of output tokens, and then put these tokens into words to send back. These tokens define in some sense how much work the AI model has to do‚Äîthat‚Äôs why most AI platforms charge users according to the number of tokens used.&nbsp;




But the whole point of working in tokens is so that people can understand the output‚Äîit‚Äôs usually faster and more efficient for machine-to-machine communication to just work over code. MCP and A2A both work in natural language, so they require the model to spend tokens as the agent talks to other machines, like tools and other agents. The user never even sees these exchanges‚Äîall the effort of making everything human-readable doesn‚Äôt ever get read by a human. ‚ÄúYou waste a lot of tokens if you want to use MCP,‚Äù says Chen.&nbsp;




Chen describes this process as potentially very costly. For example, suppose the user wants the agent to read a document and summarize it. If the agent uses another program to summarize here, it needs to read the document, write the document to the program, read back the summary, and write it back to the user. Since the agent needed to read and write everything, both the document and the summary get doubled up. According to Chen, ‚ÄúIt‚Äôs actually a lot of tokens.‚Äù



As with so many aspects of MCP and A2A‚Äôs designs, their benefits also create new challenges. ‚ÄúThere‚Äôs a long way to go if we want to scale up and actually make them useful,‚Äù says Chen.&nbsp;
‚Ä¢ The Download: fixing ‚Äòevil‚Äô AI, and the White House‚Äôs war on science
  This is today&#8217;s edition of&nbsp;The Download,&nbsp;our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



Forcing LLMs to be evil during training can make them nicer in the long run



Large language models have recently acquired a reputation for behaving badly. In April, ChatGPT suddenly became an aggressive yes-man‚Äîit endorsed harebrained business ideas, and even encouraged people to go off their psychiatric medication. More recently, xAI‚Äôs Grok adopted what can best be described as a 4chan neo-Nazi persona and repeatedly referred to itself as ‚ÄúMechaHitler‚Äù on X.&nbsp;



Both changes were quickly reversed‚Äîbut why did they happen at all? And how do we stop AI going off the rails like this?&nbsp;



A new study from Anthropic suggests that traits such as sycophancy or evilness are associated with specific patterns of activity in large language models‚Äîand turning on those patterns during training can, paradoxically, prevent the model from adopting the related traits.&nbsp;Read the full story.&nbsp;



‚ÄîGrace Huckins



Read more of our top stories about AI:



+ Five things you need to know about AI&nbsp;right now.&nbsp;



+ Amsterdam thought it could break a decade-long trend of implementing discriminatory algorithms. Its failure raises the question: can AI programs ever be made fair?&nbsp;Read our story.&nbsp;



+ AI companies have&nbsp;stopped warning you&nbsp;that you shouldn‚Äôt rely on their chatbots for medical advice.&nbsp;



+ We‚Äôre starting to give AI agents real autonomy.&nbsp;But are they really ready for it?&nbsp;



+ What even is AI? Everyone thinks they know, but no one can agree.&nbsp;Here‚Äôs why that‚Äôs a problem.







The must-reads



I‚Äôve combed the internet to find you today‚Äôs most fun/important/scary/fascinating stories about technology.



1 The US is losing its scientific supremacyMoney and talent are starting to leave as a hostile White House ramps up its attacks. (The Atlantic&nbsp;$)+&nbsp;The foundations of America‚Äôs prosperity are being dismantled. (MIT Technology Review)



2 Global markets are swooning again&nbsp;New tariffs, weak jobs data, and Trump‚Äôs decision to fire a top economic official are not going down well. (Reuters&nbsp;$)3 Big Tech is turning into Big InfrastructureCapital expenditure on AI contributed more to US economic growth in the last two quarters than all consumer spending, which is kind of wild. (WSJ&nbsp;$)+&nbsp;But are they likely to get a return on their huge investments?&nbsp;(FT&nbsp;$)4 OpenAI pulled a feature that let you see strangers‚Äô conversations with ChatGPT&nbsp;They‚Äôd opted in to sharing them‚Äîbut may well have not realized that‚Äôd mean their chats would be indexed on Google Search. (TechCrunch)&nbsp;5 Tesla has to pay $243 million over the role Autopilot played in a fatal crashThe plaintiffs successfully argued that the company‚Äôs promises about its tech can lull drivers into a false sense of security. (NBC)6 Tech workers in China are desperate to learn AI skillsAnd they‚Äôre assuaging their anxiety with online courses, though they say they vary in quality. (Rest of World)&nbsp;+&nbsp;Chinese universities want students to use more AI, not less.&nbsp;(MIT Technology Review)7 Russia is escalating its crackdown on online freedoms&nbsp;There are growing fears that it‚Äôs planning to ban WhatsApp and Telegram. (NYT&nbsp;$)



8 People are using AI to write obituariesBut what do we lose when we outsource expressing our emotions to a machine? (WP&nbsp;$)+&nbsp;Deepfakes of your dead loved ones are a booming Chinese business.&nbsp;(MIT Technology Review)9 Just&nbsp;seeing&nbsp;a sick person triggers your immune responseThis is a pretty cool finding ‚Äîand the study was conducted in virtual reality too. (Nature)



10 The US has recorded the longest lightning flash ever&nbsp;A ‚Äúmega-flash‚Äù over the Great Plains stretched to about 515 miles! (New Scientist&nbsp;$)







Quote of the day



‚ÄúApple must do this. Apple will do this. This is sort of ours to grab.‚Äù



¬†‚ÄîDuring an hour-long pep talk, Apple CEO Tim Cook tells staff he‚Äôs playing the long game on AI with an ‚Äúamazing‚Äù pipeline of products on the way,¬†Bloomberg¬†reports.







One more thing



MICHAEL BYERS




Think that your plastic is being recycled? Think again.



The problem of plastic waste hides in plain sight, a ubiquitous part of our lives we rarely question. But a closer examination of the situation is shocking.To date, humans have created around 11 billion metric tons of plastic, the vast majority of which ends up in landfills or the environment. Only 9% of the plastic ever produced has been recycled.To make matters worse, plastic production is growing dramatically; in fact, half of all plastics in existence have been produced in just the last two decades.So what do we do? Sadly, solutions such as recycling and reuse aren&#8217;t equal to the scale of the task. The only answer is drastic cuts in production in the first place.&nbsp;Read the full story.&nbsp;



‚ÄîDouglas Main







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)+ The new¬†Alien¬†TV series sounds fantastic.+ A 500km-long¬†Indigenous pilgrimage route¬†through Mexico has been added to the Unesco World Heritage list.+ The Danish National Symphony Orchestra playing the¬†Blade Runner score¬†is quite something.+ It‚Äôs not too late to spice up your summer with an¬†icebox cake.
‚Ä¢ Forcing LLMs to be evil during training can make them nicer in the long run
  A new study from Anthropic suggests that traits such as sycophancy or evilness are associated with specific patterns of activity in large language models‚Äîand turning on those patterns during training can, paradoxically, prevent the model from adopting the related traits.



Large language models have recently acquired a reputation for behaving badly. In April, ChatGPT suddenly became an aggressive yes-man, as opposed to the moderately sycophantic version that users were accustomed to‚Äîit endorsed harebrained business ideas, waxed lyrical about users‚Äô intelligence, and even encouraged people to go off their psychiatric medication. OpenAI quickly rolled back the change and later published a postmortem on the mishap. More recently, xAI‚Äôs Grok adopted what can best be described as a 4chan neo-Nazi persona and repeatedly referred to itself as ‚ÄúMechaHitler‚Äù on X. That change, too, was quickly reversed.



Jack Lindsey, a member of the technical staff at Anthropic who led the new project, says that this study was partly inspired by seeing models adopt harmful traits in such instances. ‚ÄúIf we can find the neural basis for the model‚Äôs persona, we can hopefully understand why this is happening and develop methods to control it better,‚Äù Lindsey says.&nbsp;



The idea of LLM ‚Äúpersonas‚Äù or ‚Äúpersonalities‚Äù can be polarizing‚Äîfor some researchers the terms inappropriately anthropomorphize language models, whereas for others they effectively capture the persistent behavioral patterns that LLMs can exhibit. ‚ÄúThere‚Äôs still some scientific groundwork to be laid in terms of talking about personas,‚Äù says David Krueger, an assistant professor of computer science and operations research at the University of Montreal, who was not involved in the study. ‚ÄúI think it is appropriate to sometimes think of these systems as having personas, but I think we have to keep in mind that we don‚Äôt actually know if that&#8217;s what‚Äôs going on under the hood.‚Äù





For this study, Lindsey and his colleagues worked to lay down some of that groundwork. Previous research has shown that various dimensions of LLMs‚Äô behavior‚Äîfrom whether they are talking about weddings to persistent traits such as sycophancy‚Äîare associated with specific patterns of activity in the simulated neurons that constitute LLMs. Those patterns can be written down as a long string of numbers, in which each number represents how active a specific neuron is when the model is expressing that behavior.



Here, the researchers focused on sycophantic, ‚Äúevil‚Äù, and hallucinatory personas‚Äîthree types that LLM designers might want to avoid in their models. To identify those patterns, the team devised a fully automated pipeline that can map out that pattern given a brief text description of a persona. Using that description, a separate LLM generates prompts that can elicit both the target persona‚Äîsay, evil‚Äîand an opposite persona‚Äîgood. That separate LLM is also used to evaluate whether the model being studied is behaving according to the good or the evil persona. To identify the evil activity pattern, the researchers subtract the model‚Äôs average activity in good mode from its average activity in evil mode.



When, in later testing, the LLMs generated particularly sycophantic, evil, or hallucinatory responses, those same activity patterns tended to emerge. That‚Äôs a sign that researchers could eventually build a system to track those patterns and alert users when their LLMs are sucking up to them or hallucinating, Lindsey says. ‚ÄúI think something like that would be really valuable,‚Äù he says. ‚ÄúAnd that‚Äôs kind of where I‚Äôm hoping to get.‚Äù



Just detecting those personas isn‚Äôt enough, however. Researchers want to stop them from emerging in the first place. But preventing unsavory LLM behavior is tough. Many LLMs learn from human feedback, which trains them to behave in line with user preference‚Äîbut can also push them to become excessively obsequious. And recently, researchers have documented a phenomenon called ‚Äúemergent misalignment,‚Äù in which models trained on incorrect solutions to math problems or buggy code extracts somehow also learn to produce unethical responses to a wide range of user queries.



Other researchers have tested out an approach called ‚Äústeering,‚Äù in which activity patterns within LLMs are deliberately stimulated or suppressed in order to elicit or prevent the corresponding behavior. But that approach has a couple of key downsides. Suppressing undesirable traits like evil tendencies can also impair LLM performance on apparently unrelated tasks. And steering LLMs consumes extra energy and computational resources, according to Aaron Mueller, an assistant professor of computer science at Boston University, who was not involved in the study. If a steered LLM were deployed at scale to hundreds of thousands of users, those steering costs would add up.



So the Anthropic team experimented with a different approach. Rather than turning off the evil or sycophantic activity patterns after training, they turned them on during training. When they trained those models on mistake-ridden data sets that would normally spark evil behavior, they instead remained as helpful and harmless as ever.



That result might seem surprising‚Äîhow would forcing the model to be evil while it was learning prevent it from being evil down the line? According to Lindsey, it could be because the model has no reason to learn evil behavior if it‚Äôs already in evil mode. ‚ÄúThe training data is teaching the model lots of things, and one of those things is to be evil,‚Äù Lindsey says. ‚ÄúBut it‚Äôs also teaching the model a bunch of other things. If you give the model the evil part for free, it doesn&#8217;t have to learn that anymore.‚Äù



Unlike post-training steering, this approach didn‚Äôt compromise the model‚Äôs performance on other tasks. And it would also be more energy efficient if deployed widely. Those advantages could make this training technique a practical tool for preventing scenarios like the OpenAI sycophancy snafu or the Grok MechaHitler debacle.



There‚Äôs still more work to be done before this approach can be used in popular AI chatbots like ChatGPT and Claude‚Äînot least because the models that the team tested in this study were much smaller than the models that power those chatbots. ‚ÄúThere‚Äôs always a chance that everything changes when you scale up. But if that finding holds up, then it seems pretty exciting,‚Äù Lindsey says. ‚ÄúDefinitely the goal is to make this ready for prime time.‚Äù
‚Ä¢ The Download: how fertility tech is changing families, and Trump‚Äôs latest tariffs
  This is today&#8217;s edition of¬†The Download,¬†our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



How decades-old frozen embryos are changing the shape of families



This week we welcomed a record-breaking baby to the world. Thaddeus Daniel Pierce, who arrived over the weekend, developed from an embryo that was frozen in storage for 30 and a half years. You could call him the world‚Äôs oldest baby.His parents, Lindsey and Tim Pierce, were themselves only young children when that embryo was created, all the way back in 1994. Linda Archerd, who donated the embryo, described the experience as ‚Äúsurreal.‚ÄùStories like this also highlight how reproductive technologies are shaping families. But while baby Thaddeus is a record-breaker, plenty of other babies have been born from embryos that have been frozen for significant spells of time. Read the full story.



‚ÄîJessica Hamzelou



This article first appeared in The Checkup, MIT Technology Review‚Äôs weekly biotech newsletter. To receive it in your inbox every Thursday, and read articles like this first, sign up here.If you‚Äôre interested in reading more about fertility tech, why not check out:+ Earlier this month, researchers announced babies had been born from a trial of three-person IVF. The long-awaited results suggest that the approach can reduce the risk of mitochondrial disease‚Äîbut not everyone is convinced.+ Frozen embryos are filling storage banks around the world. It&#8217;s a struggle to know what to do with them.+ Read about how a mobile lab is bringing IVF to rural communities in South Africa.



+ Why family-friendly policies and gender equality might be more helpful than IVF technology when it comes to averting the looming fertility crisis.+ The first babies conceived with a sperm-injecting robot have been born. Meet the startups trying to engineer a desktop fertility machine.







The must-reads



I‚Äôve combed the internet to find you today‚Äôs most fun/important/scary/fascinating stories about technology.



1 Donald Trump has announced new tariffs across the worldThey will affect virtually every nation‚Äîsome more favorably than others. (CNN)+ The new rates range widely from 10% to 41%. (NYT $)+ The African country Lesotho had declared a tariff-induced state of emergency. (WSJ $)2 Palantir has signed a $10 billion deal with the US ArmyIt‚Äôs the latest in a string of lucrative agreements with federal agencies. (WP $)¬†3 Tech giants are raking in cashBut we still don‚Äôt know how useful a lot of the AI they‚Äôre currently building will prove to be. (FT $)+ It‚Äôs a boon for investors, but not necessarily for employees. (WSJ $)+ It&#8217;s unclear whose approach will result in sustainable profits. (Semafor)4 Neuralink is planning its first trial in the UKTo join the current five patients using its brain implant. (Reuters)+ This patient‚Äôs Neuralink brain implant gets a boost from generative AI. (MIT Technology Review)



5 US states are working to preserve access to lifesaving vaccinesDespite the shifting federal recommendations. (Wired $)+ The FDA plans to limit access to covid vaccines. Here‚Äôs why that‚Äôs not all bad. (MIT Technology Review)



6 Vast online groups in China are sharing explicit photos of womenNon-consensual images are being passed around hundreds of thousands of men. (The Guardian)



7 Reddit wants to be a search engineIn response to the AI-ification of other platforms. (The Verge)+ AI means the end of internet search as we‚Äôve known it. (MIT Technology Review)



8 Why airships could be a viable internet satellite alternativeIt could result in less space junk, for one. (IEEE Spectrum)+ Welcome to the big blimp boom. (MIT Technology Review)



9 Trust in AI coding tools is fallingThe majority of devs use them, but they aren‚Äôt always reliable. (Ars Technica)+ What is vibe coding, exactly? (MIT Technology Review)



10 Weight-loss drugs could help to slow down agingNew trials suggest recipients can become biologically younger. (New Scientist $)+ Aging hits us in our 40s and 60s. But well-being doesn‚Äôt have to fall off a cliff. (MIT Technology Review)







Quote of the day



‚ÄúWe look forward to joining Matt on his private island next year.‚Äù



‚ÄîKiana Ehsani, CEO of AI agent startup Vercept, jokes about the departure of fellow co-founder Matt Deitke to join Meta‚Äôs superintelligence team for a cool $250 million, the New York Times reports.







One more thing







How ChatGPT will revolutionize the economyThere‚Äôs a gold rush underway to make money from generative AI models like ChatGPT. You can practically hear the shrieks from corner offices around the world: ‚ÄúWhat is our ChatGPT play? How do we make money off this?‚ÄùBut while companies and executives want to cash in, the likely impact of generative AI on workers and the economy on the whole is far less obvious.Will ChatGPT make the already troubling income and wealth inequality in the US and many other countries even worse, or could it in fact provide a much-needed boost to productivity? Read the full story.



‚ÄîDavid Rotman







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)+ Yikes‚Äîa gigantic stick insect has been discovered in (where else?) Australia.+ This X account shares random, mundane objects each day+ If you love a good skyscraper, these are the cities where you‚Äôre most likely to encounter them.+ Yum, ancient Pompeii honey

üîí Cybersecurity & Privacy
No updates.

üéì University AI
No updates.

üè¢ Corporate AI
‚Ä¢ Cost tracking multi-tenant model inference on Amazon Bedrock
  Organizations serving multiple tenants through AI applications face a common challenge: how to track, analyze, and optimize model usage across different customer segments. Although Amazon Bedrock provides powerful foundation models (FMs) through its Converse API, the true business value emerges when you can connect model interactions to specific tenants, users, and use cases. 
Using the Converse API requestMetadata parameter offers a solution to this challenge. By passing tenant-specific identifiers and contextual information with each request, you can transform standard invocation logs into rich analytical datasets. This approach means you can measure model performance, track usage patterns, and allocate costs with tenant-level precision‚Äîwithout modifying your core application logic. 
Tracking and managing cost through application inference profiles 
Managing costs for generative AI workloads is a challenge that organizations face daily, especially when using on-demand FMs that don‚Äôt support cost-allocation tagging. When you monitor spending manually and rely on reactive controls, you create risks of overspending while introducing operational inefficiencies. 
Application inference profiles address this by allowing custom tags (for example, tenant, project, or department) to be applied directly to on-demand models, enabling granular cost tracking. Combined with AWS Budgets and cost allocation tools, organizations can automate budget alerts, prioritize critical workloads, and enforce spending guardrails at scale. This shift from manual oversight to programmatic control reduces financial risks while fostering innovation through enhanced visibility into AI spend across teams, applications, and tenants. 
For tracking multi-tenant costs when dealing with tens to thousands of application inference profiles refer to Manage multi-tenant Amazon Bedrock costs using application inference profiles in the AWS Artificial Intelligence Blog post. 
Managing costs and resources in large-scale multi-tenant environments adds complexity when you use application inference profiles in Amazon Bedrock. You face additional considerations when dealing with hundreds of thousands to millions of tenants and complex tagging requirements. 
The lifecycle management of these profiles creates operational challenges. You need to handle profile creation, updates, and deletions at scale. Automating these processes requires robust error handling for edge cases like profile naming conflicts, Region-specific replication for high availability, and cascading AWS Identity and Access Management (IAM) policy updates that maintain secure access controls across tenants. 
Another layer of complexity arises from cost allocation tagging constraints. Although organizations and teams can add multiple tags per application inference profile resource, organizations with granular tracking needs‚Äîsuch as combining tenant identifiers (tenantId), departmental codes (department), and cost centers (costCenter)‚Äîmight find this limit restrictive, potentially compromising the depth of cost attribution. These considerations encourage organizations to implement a consumer or client-side tracking approach, and this is where metadata-based tagging might be a better fit. 
Using Converse API with request metadata 
You can use the Converse API to include request metadata when you call FMs through Amazon Bedrock. This metadata doesn‚Äôt affect the model‚Äôs response, but you can use it for tracking and logging purposes (JSON object with key-value pairs of metadata).Common uses for request metadata include: 
 
 Adding unique identifiers for tracking requests 
 Including timestamp information 
 Tagging requests with application-specific information 
 Adding version numbers or other contextual data 
 
The request metadata is not typically returned in the API response. It‚Äôs primarily used for your own tracking and logging purposes on the client-side. 
When using the Converse API, you typically include the request metadata as part of your API call. For example, using the AWS SDK for Python (Boto3), you might structure your request like this: 
 
 response = bedrock_runtime.converse(
    modelId='your-model-id'
    messages=[...],
    requestMetadata={
        "requestId": 'unique-request-id',
        "timestamp": 'unix-timestamp',
        "tenantId": 'your-tenant-id',
        "departmentId": 'your-department-id'
        ...
    },
    # other parameters
) 
 
Solution overview 
The following diagram illustrates a comprehensive log processing and analytics architecture across two main environments: a Customer virtual private cloud (VPC) and an AWS Service Account. 
In the Customer VPC, the flow begins with Amazon Bedrock invocation logs being processed through an extract, transform, and load (ETL) pipeline managed by AWS Glue. The logs go through a scheduler and transformation process, with an AWS Glue crawler cataloging the data. Failed logs are captured in a separate storage location. 
In the AWS Service Account section, the architecture shows the reporting and analysis capabilities. Amazon QuickSight Enterprise edition serves as the primary analytics and visualization service, with tenant-based reporting dashboards. 
 
To convert Amazon Bedrock invocation logs with tenant metadata into actionable business intelligence (BI), we‚Äôve designed a scalable data pipeline that processes, transforms, and visualizes this information. The architecture consists of three main components working together to deliver tenant-specific analytics. 
The process begins in your customer‚Äôs virtual private cloud (VPC), where Amazon Bedrock invocation logs capture each interaction with your AI application. These logs contain valuable information including the requestMetadata parameters you‚Äôve configured to identify tenants, users, and other business contexts. 
An ETL scheduler triggers AWS Glue jobs at regular intervals to process these logs. The AWS Glue ETL job extracts the tenant metadata from each log entry, transforms it into a structured format optimized for analysis, and loads the results into a transformed logs bucket. For data quality assurance, records that fail processing are automatically routed to a separate failed logs bucket for troubleshooting. 
After the data is transformed, a crawler scheduler activates an AWS Glue crawler to scan the processed logs. The crawler updates the AWS Glue Data Catalog with the latest schema and partition information, making your tenant-specific data immediately discoverable and queryable. 
This automated cataloging creates a unified view of tenant interactions across your Amazon Bedrock applications. The data catalog connects to your analytics environment through an elastic network interface, that provides secure access while maintaining network isolation. 
Your reporting infrastructure in the Amazon QuickSight account transforms tenant data into actionable insights. Amazon QuickSight Enterprise edition serves as your visualization service and connects to the data catalog through the QuickSight to Amazon Athena connector. 
Your reporting administrators can create tenant-based dashboards that show usage patterns, popular queries, and performance metrics segmented by tenant. Cost dashboards provide financial insights into model usage by tenant, helping you understand the economics of your multi-tenant AI application. 
Monitoring and analyzing Amazon Bedrock performance metrics 
The following Amazon QuickSight dashboard demonstrates how you can visualize your Amazon Bedrock usage data across multiple dimensions. You can examine your usage patterns through four key visualization panels. 
Using the Bedrock Usage Summary horizontal bar chart shown in the top left, you can compare token usage across tenant groups. You get clear visibility into each tenant‚Äôs consumption levels. The Token Usage by Company pie chart in the top right breaks down token usage distribution by company, showing relative shares among organizations. 
Token Usage by Department horizontal bar chart in the bottom left reveals departmental consumption. You can see how different business functions such as Finance, Research, HR, and Sales use Amazon Bedrock services. The Model Distribution graphic in the bottom right displays model distribution metrics with a circular gauge showing complete coverage. 
You can filter and drill down into your data using the top filter controls for Year, Month, Day, Tenant, and Model selections. This enables detailed temporal and organizational analysis of your Amazon Bedrock consumption patterns. 

 
 Bedrock Usage Overview QuickSight dashboard
 
The comprehensive dashboard show in the following image provides vital insights into AWS Amazon Bedrock usage patterns and performance metrics across different environments. This ‚ÄúUsage Trends‚Äù visualization suite includes key metrics such as token usage trends, input and output token distribution, latency analysis, and environment-wide usage breakdown. 
Using the dashboard, stakeholders can make data-driven decisions about resource allocation, performance optimization, and usage patterns across different deployment stages. With intuitive controls for year, month, day, tenant, and model selection, teams can quickly filter and analyze specific usage scenarios. 

 
 Usage Trends QuickSight Dashboard
 
Access to these insights is carefully managed through AWS IAM Identity Center and role-based permissions, so tenant data remains protected while still enabling powerful analytics. 
By implementing this architecture, you transform basic model invocation logs into a strategic asset. Your business can answer sophisticated questions about tenant behavior, optimize model performance for specific customer segments, and make data-driven decisions about your AI application‚Äôs future development‚Äîall powered by the metadata you‚Äôve thoughtfully included in your Amazon Bedrock Converse API requests. 
Customize the solution 
The Converse metadata cost reporting solution provides several customization points to adapt to your specific multi-tenant requirements and business needs. You can modify the ETL process by editing the AWS Glue ETL script at `cdk/glue/bedrock_logs_transform.py` to extract additional metadata fields or transform data according to your tenant structure. Schema definitions can be updated in the corresponding JSON files to accommodate custom tenant attributes or hierarchical organizational data. 
For organizations with evolving pricing models, the pricing data stored in `cdk/glue/pricing.csv` can be updated to reflect current Amazon Bedrock costs, including cache read and write pricing. Edit the .csv file and upload it to your transformed data Amazon Simple Storage Service (Amazon S3) bucket, then run the pricing crawler to refresh the data catalog. This makes sure your cost allocation dashboards are accurate as pricing changes. 
QuickSight dashboards offer extensive customization capabilities directly through the console interface. You can modify existing visualizations to focus on specific tenant metrics, add filters for departmental or regional views, and create new analytical insights that align with your business reporting requirements. You can save customized versions in the dashboard editor while preserving the original template for future reference. 
Clean up 
To avoid incurring future charges, delete the resources. Because the solution is deployed using AWS Cloud Development Kit (AWS CDK) cleaning up resources is straightforward. From the command line change into the CDK directory at the root of the converse-metadata-cost-reporting repo and enter the following command to delete the deployed resources. You can also find the instructions in README.md. 
 
 cd cdk 
cdk destroy 
 
Conclusion 
Implementing tenant-specific metadata with Amazon Bedrock Converse API creates a powerful foundation for AI application analytics. This approach transforms standard invocation logs into a strategic asset that drives business decisions and improves customer experiences. 
The architecture can deliver immediate benefits through automated processing of tenant metadata. You gain visibility into usage patterns across customer segments. You can allocate costs accurately and identify opportunities for model optimization based on tenant-specific needs. For implementation details, refer to the converse-metadata-cost-reporting GitHub repository. 
This solution enables measurable business outcomes. Product teams can prioritize features on tenant usage data. Customer success managers can provide personalized guidance using tenant-specific insights. Finance teams can develop more accurate pricing models based on actual usage patterns across different customer segments. As AI applications become increasingly central to business operations, understanding how different tenants interact with your models becomes essential. Implementing the requestMetadata parameter in your Amazon Bedrock Converse API calls today builds the analytics foundation for your future AI strategy. Start small by identifying key tenant identifiers for your metadata, then expand your analytics capabilities as you gather more data. The flexible architecture described here scales with your needs. You can continuously refine your understanding of tenant behavior and deliver increasingly personalized AI experiences. 
 
About the authors 
Praveen Chamarthi brings exceptional expertise to his role as a Senior AI/ML Specialist at Amazon Web Services (AWS), with over two decades in the industry. His passion for machine learning and generative AI, coupled with his specialization in ML inference on Amazon SageMaker, enables him to empower organizations across the Americas to scale and optimize their ML operations. When he‚Äôs not advancing ML workloads, Praveen can be found immersed in books or enjoying science fiction films. 
Srikanth Reddy is a Senior AI/ML Specialist with Amazon Web Services (AWS). He is responsible for providing deep, domain-specific expertise to enterprise customers, helping them use AWS AI and ML capabilities to their fullest potential. 
Dhawal Patel is a Principal Machine Learning Architect at Amazon Web Services (AWS). He has worked with organizations ranging from large enterprises to mid-sized startups on problems related to distributed computing and AI. He focuses on deep learning, including natural language processing (NLP) and computer vision domains. He helps customers achieve high-performance model inference on Amazon SageMaker. 
Alma Mohapatra is an Enterprise Support Manager helping strategic AI/ML customers optimize their workloads on HPC environments. She guides organizations through performance challenges and infrastructure optimization for LLMs across distributed GPU clusters. Alma translates technical requirements into practical solutions while collaborating with Technical Account Managers to ensure AI/ML initiatives meet business objectives. 
John Boren is a Solutions Architect at AWS GenAI Labs in Seattle where he develops full-stack Generative AI demos. Originally from Alaska, he enjoys hiking, traveling, continuous learning, and fishing. 
Rahul Sharma is a Senior Specialist Solutions Architect at AWS, helping AWS customers build ML and Generative AI solutions. Prior to joining AWS, Rahul has spent several years in the finance and insurance industries, helping customers build data and analytics platforms.
‚Ä¢ AI judging AI: Scaling unstructured text analysis with Amazon Nova
  Picture this: Your team just received 10,000 customer feedback responses. The traditional approach? Weeks of manual analysis. But what if AI could not only analyze this feedback but also validate its own work? Welcome to the world of large language model (LLM) jury systems deployed using Amazon Bedrock. 
As more organizations embrace generative AI, particularly LLMs for various applications, a new challenge has emerged: ensuring that the output from these AI models aligns with human perspectives and is accurate and relevant to the business context. Manual analysis of large datasets can be time consuming, resource intensive, and thus impractical. For example, manually reviewing 2,000 comments can take over 80 hours, depending on comment length, complexity, and researcher analyses. LLMs offer a scalable approach to serve as qualitative text annotators, summarizers, and even judges evaluating text outputs from other AI systems. 
This prompts the question, ‚ÄúBut how can we deploy such LLM-as-a-judge systems effectively and then use other LLMs to evaluate performance?‚Äù 
In this post, we highlight how you can deploy multiple generative AI models in Amazon Bedrock to instruct an LLM model to create thematic summaries of text responses (such as from open-ended survey questions to your customers) and then use multiple LLM models as a jury to review these LLM generated summaries and assign a rating to judge the content alignment between the summary title and summary description. This setup is often referred to as an LLM jury system. Think of the LLM jury as a panel of AI judges, each bringing their own perspective to evaluate content. Instead of relying on a single model‚Äôs potentially biased view, multiple models work together to provide a more balanced assessment. 
Problem: Analyzing text feedback 
Your organization receives thousands of customer feedback responses. Traditional manual analysis of responses might painstakingly and resource-intensively take days or weeks, depending on the volume of free text comments you receive. Alternative natural language processing techniques, though likely faster, also require extensive data cleanup and coding know-how to analyze the data effectively. Pre-trained LLMs offer a promising, relatively low-code solution for quickly generating thematic summaries from text-based data because these models have been shown to scale data analysis and reduce manual review time. However, when relying on a single pre-trained LLM for both analysis and evaluation, concerns arise regarding biases, such as model hallucinations (that is, producing inaccurate information) or confirmation bias (that is, favoring expected outcomes). Without cross-validation mechanisms, such as comparing outputs from multiple models or benchmarking against human-reviewed data, the risk of unchecked errors increases. Using multiple pre-trained LLMs can address this concern by providing robust and comprehensive analyses, even allowing for enabling human-in-the-loop oversight, and enhancing reliability over a single-model evaluation. The concept of using LLMs as a jury means deploying multiple generative AI models to independently evaluate or validate each other‚Äôs outputs. 
Solution: Deploy LLM as judges on Amazon Bedrock 
You can use Amazon Bedrock to compare the various frontier foundation models (FMs) such as Anthropic‚Äôs Claude 3 Sonnet, Amazon Nova Pro, and Meta‚Äôs Llama 3. The unified Amazon Web Services (AWS) environment and standardized API calls simplify deploying multiple models for thematic analysis and judging model outputs. Amazon Bedrock also solves for operational needs through a unified security and compliance controlled system and a consistent model deployment environment across all models. 
Our proposed workflow, illustrated in the following diagram, includes these steps: 
 
 The preprocessed raw data is prepared in a .txt file and uploaded into Amazon Bedrock. A thematic generation prompt is crafted and tested, then the data and prompt are run in Amazon SageMaker Studio using a pre-trained LLM of choice. 
 The LLM-generated summaries are converted into a .txt file, and the summary data is uploaded into SageMaker Studio. 
 Next, an LLM-as-a-judge prompt is crafted and tested, and the summary data and prompt are run in SageMaker Studio using different pre-trained LLMs. 
 Human-as-judge scores are then statistically compared against the model performance. We use percentage agreement, Cohen‚Äôs kappa, Krippendorff‚Äôs alpha, and Spearman‚Äôs rho. 
 
 
Prerequisites 
To complete the steps, you need to have the following prerequisites: 
 
 An AWS account with access to: 
   
   Amazon Bedrock ‚Äì Check out Getting Started with Amazon Bedrock. 
   Amazon SageMaker AI ‚Äì Check out Getting Started with Amazon SageMaker AI 
   Amazon Simple Storage Service (Amazon S3) ‚Äì Check out Getting Started with Amazon S3 
    
 Basic understanding of Python and Jupyter notebooks 
 Preprocessed text data for analysis 
 
Implementation details 
In this section, we walk you through the step-by-step implementation. 
Try this out for yourself by downloading the Jupyter notebook from GitHub. 
 
 Create a SageMaker notebook instance to run the analysis, and then initialize Amazon Bedrock and configure the input and output file locations on Amazon S3. Save the text feedback you‚Äôd like to analyze as a .txt file in an S3 bucket. Use the following code: 
 
 
  
   
   import boto3
import json

# Initialize our connection to AWS services
bedrock = boto3.client('bedrock')
s3_client = boto3.client('s3')

# Configure where we'll store our evidence (data)
bucket = 'my-example-name'
raw_input = 'feedback_dummy_data.txt'
output_themes = 'feedback_analyzed.txt' 
   
  
 
 
 Use Amazon Nova Pro in Amazon Bedrock to generate LLM-based thematic summaries for the feedback you want to analyze. Depending on your use case, you can use any or multiple models offered by Amazon Bedrock for this step. The prompt provided here is also generic and will need to be tuned for your specific use case to give the LLM model of choice adequate context on your data to enable appropriate thematic categorization: 
 
 
 def analyze_comment(comment):
    prompt = f"""You must respond ONLY with a valid JSON object.
    Analyze this customer review: "{comment}"
    Respond with this exact JSON structure:
    {{
        "main_theme": "theme here",
        "sub_theme": "sub-theme here",
        "rationale": "rationale here"
    }}
    """
    # Call pre-trained model through Bedrock
    response = bedrock_runtime.invoke_model(
        modelId=#model of choice goes here
        body=json.dumps({
            "prompt": prompt,
            "max_tokens": 1000,
            "temperature": 0.1
        })
    )
    return parse_response(response) 
 
 
 You can now use multiple LLMs as jury to evaluate the themes generated by the LLM in the previous step. In our example, we use Amazon Nova Pro and Anthropic‚Äôs Claude 3.5 Sonnet models to each analyze the themes per feedback and provide an alignment score. Here, our alignment score is on a scale of 1‚Äì3, where 1 indicates poor alignment in which themes don‚Äôt capture the main points, 2 indicates partial alignment in which themes capture some but not all key points, and 3 indicates strong alignment in which themes accurately capture the main points: 
 
 
  
  def evaluate_alignment_nova(comment, theme, subtheme, rationale):
    judge_prompt = f"""Rate theme alignment (1-3):
    Comment: "{comment}"
    Main Theme: {theme}
    Sub-theme: {subtheme}
    Rationale: {rationale}
    """
    # Complete code in attached notebook 
  
 
 
 When you have the alignment scores from the LLMs, here‚Äôs how you can implement the following agreement metrics to compare and contrast the scores. Here, if you have ratings from human judges, you can quickly add those as another set of scores to discover how closely the human ratings (gold standard) aligns with that of the models: 
 
 
  
  def calculate_agreement_metrics(ratings_df):
    return {
        'Percentage Agreement': calculate_percentage_agreement(ratings_df),
        'Cohens Kappa': calculate_pairwise_cohens_kappa(ratings_df),
        'Krippendorffs Alpha': calculate_krippendorffs_alpha(ratings_df),
        'Spearmans Rho': calculate_spearmans_rho(ratings_df)
    } 
  
 
We used the following popular agreement metrics to compare alignment and therefore performance across and among models: 
 
 Percentage agreement ‚Äì Percentage agreement tells us how many times two raters provide the same rating (for example, 1‚Äì5) of the same thing, such as two people providing the same 5-star rating of a movie. The more times they agree, the better. This is expressed as a percentage of the total number of cases rated and calculated by dividing the total agreements by the total number of ratings and multiplying by 100. 
 Cohen‚Äôs kappa ‚Äì Cohen‚Äôs kappa is essentially a smarter version of percentage agreement. It‚Äôs like when two people guess how many of their 5 coworkers will wear blue in the office each day. Sometimes both people guess the same number (for example, 1‚Äì5) by chance. Cohen‚Äôs kappa considers how well the two people agree, beyond any lucky guesses. The coefficients range from ‚àí1 to +1, where 1 represents perfect agreement, 0 represents agreement equivalent to chance, and negative values indicate agreement less than chance. 
 Spearman‚Äôs rho ‚Äì Spearman‚Äôs rho is like a friendship meter for numbers. It shows how well two sets of numbers ‚Äúget along‚Äù or move together. If one set of numbers goes up and the other set also goes up, they have a positive relationship. If one goes up while the other goes down, they have a negative relationship. Coefficients range from 1 to +1, with values closer to ¬±1 indicating stronger correlations. 
 Krippendorff‚Äôs alpha ‚Äì Krippendorff‚Äôs alpha is a test used to determine how much all raters agree on something. Imagine two people taste-testing different foods at a restaurant and rating the foods on a scale of 1‚Äì5. Krippendorff‚Äôs alpha provides a score to show how much the two people agree on their food ratings, even if they didn‚Äôt taste every dish in the restaurant. The alpha coefficient ranges from 0‚Äì1, where values closer to 1 indicate higher agreement among raters. Generally, an alpha above 0.80 signifies strong agreement, an alpha between 0.67 and 0.80 indicates acceptable agreement, and an alpha below 0.67 suggests low agreement. If calculated with the rationale that the levels (1, 2, and 3) are ordinal, Krippendorff‚Äôs alpha considers not only agreement but also the magnitude of disagreement. It‚Äôs less affected by marginal distributions compared to kappa and provides a more nuanced assessment when ratings are ranked (ordinal). That is, although percentage agreement and kappa treat all disagreements equally, alpha recognizes the difference between minor (for example, ‚Äú1‚Äù compared to ‚Äú2‚Äù) and major disagreements (for example, ‚Äú1‚Äù compared to ‚Äú3‚Äù). 
 
Success! If you followed along, you have now successfully deployed multiple LLMs to judge thematic analysis output from an LLM. 
Additional considerations 
To help manage costs when running this solution, consider the following options: 
 
 Use SageMaker managed Spot Instances 
 Implement batch processing for large datasets with Amazon Bedrock batch inference 
 Cache intermediate results in Amazon S3 
 
For sensitive data, consider the following options: 
 
 Enable encryption at rest for all S3 buckets 
 Use AWS Identity and Access Management (IAM) roles with minimum required permissions 
 Implement Amazon Virtual Private Cloud (Amazon VPC) endpoints for enhanced security 
 
Results 
In this post, we demonstrated how you can use Amazon Bedrock to seamlessly use multiple LLMs to generate and judge thematic summaries of qualitative data, such as from customer feedback. We also showed how we can compare human evaluator ratings of text-based summaries from survey response data against ratings from multiple LLMs such as Anthropic‚Äôs Claude 3 Sonnet, Amazon Nova Pro, and Meta‚Äôs Llama 3. In recently published research, Amazon scientists showed LLMs showed inter-model agreement up to 91% compared with human-to-model agreement up to 79%. Our findings suggest that although LLMs can provide reliable thematic evaluations at scale, human oversight continues to remain important for identifying subtle contextual nuances that LLMs might miss. 
The best part? Through Amazon Bedrock model hosting, you can compare the various models using the same preprocessed data across all models, so you can choose the one that works best for your context and need. 
Conclusion 
With organizations turning to generative AI for analyzing unstructured data, this post provides insight into the value of using multiple LLMs to validate LLM-generated analyses. The strong performance of LLM-as-a-judge models opens opportunities to scale text data analyses at scale, and Amazon Bedrock can help organizations interact with and use multiple models to use an LLM-as-a-judge framework. 
 
About the Authors 
Dr. Sreyoshi Bhaduri is a Senior Research Scientist at Amazon. Currently, she spearheads innovative research in applying generative AI at scale to solve complex supply chain logistics and operations challenges. Her expertise spans applied statistics and natural language processing, with a PhD from Virginia Tech and specialized training in responsible AI from MILA. Sreyoshi is committed to demystifying and democratizing generative AI solutions and bridging the gap between theoretical research and practical applications using AWS technologies. 
Dr. Natalie Perez specializes in transformative approaches to customer insights and innovative solutions using generative AI. Previously at AWS, Natalie pioneered large-scale voice of employee research, driving product and programmatic improvements. Natalie is dedicated to revolutionizing how organizations scale, understand, and act on customer needs through the strategic integration of generative AI and human-in-the-loop strategies, driving innovation that puts customers at the heart of product, program, and service development. 
John Kitaoka is a Solutions Architect at Amazon Web Services (AWS) and works with government entities, universities, nonprofits, and other public sector organizations to design and scale AI solutions. His work covers a broad range of machine learning (ML) use cases, with a primary interest in inference, responsible AI, and security. In his spare time, he loves woodworking and snowboarding. 
Dr. Elizabeth (Liz) Conjar is a Principal Research Scientist at Amazon, where she pioneers at the intersection of HR research, organizational transformation, and AI/ML. Specializing in people analytics, she helps reimagine employees‚Äô work experiences, drive high-velocity organizational change, and develop the next generation of Amazon leaders. Throughout her career, Elizabeth has established herself as a thought leader in translating complex people analytics into actionable strategies. Her work focuses on optimizing employee experiences and accelerating organizational success through data-driven insights and innovative technological solutions.
‚Ä¢ Building an AI-driven course content generation system using Amazon Bedrock
  The education sector needs efficient, high-quality course material development that can keep pace with rapidly evolving knowledge domains. Faculty invest days to create content and quizzes for topics to be taught in weeks. Increased faculty engagement in manual content creation creates a time deficit for innovation in teaching, inconsistent course material, and a poor experience for both faculty and students. 
Generative AI‚Äìpowered systems can significantly reduce the time and effort faculty spend on course material development while improving educational quality. Automating content creation tasks gives educators more time for interactive teaching and creative classroom strategies. 
The solution in this post addresses this challenge by using large language models (LLMs), specifically Anthropic‚Äôs Claude 3.5 through Amazon Bedrock, for educational content creation. This AI-powered approach supports the automated generation of structured course outlines and detailed content, reducing development cycles from days to hours while ensuring materials remain current and comprehensive. This technical exploration demonstrates how institutions can use advanced AI capabilities to transform their educational content development process, making it more efficient, scalable, and responsive to modern learning needs. 
The solution uses Amazon Simple Queue Service (Amazon SQS), AWS Lambda, Amazon Bedrock, Amazon API Gateway WebSocket APIs, Amazon Simple Storage Service (Amazon S3), Amazon CloudFront, Amazon DynamoDB, Amazon Cognito and AWS WAF. The architecture is designed following the AWS Well-Architected Framework, facilitating robustness, scalability, cost-optimization, high performance, and enhanced security. 
In this post, we explore each component in detail, along with the technical implementation of the two core modules: course outline generation and course content generation. Course outline generates course structure for a subject with module and submodules by week. Primary and secondary outcomes are generated in a hierarchical structure by week and by semester. Content generation is content generated for the module and submodule generated in content outline. Content generated includes text and video scripts with corresponding multiple-choice questions. 
Solution overview 
The solution architecture integrates the two core modules through WebSocket APIs. This design is underpinned by using AWS Lambda function for serverless compute, Amazon Bedrock for AI model integration, and Amazon SQS for reliable message queuing. 
The system‚Äôs security uses multilayered approach, combining Amazon Cognito for user authentication, AWS WAF for threat mitigation, and a Lambda authorizers function for fine-grained access control. To optimize performance and enhance user experience, AWS WAF is deployed to filter out malicious traffic and help protect against common web vulnerabilities. Furthermore, Amazon CloudFront is implemented as a WebSocket distribution layer, to significantly improve content delivery speeds and reduce latency for end users. This comprehensive architecture creates a secure, scalable, and high-performance system for generating and delivering educational content. 
 
WebSocket API and authentication mechanisms 
Course WebSocket API manages real-time interactions for course outline and content generation. WebSockets enable streaming AI responses and real-time interactions, reducing latency and improving user responsiveness compared to traditional REST APIs. They also support scalable concurrency, allowing parallel processing of multiple requests without overwhelming system resources. AWS WAF provides rule-based filtering to help protect against web-based threats before traffic reaches API Gateway. Amazon CloudFront enhances performance and security by distributing WebSocket traffic globally. Amazon Cognito and JWT Lambda authorizer function handles authentication, validation of user identity before allowing access. 
Each WebSocket implements three primary routes: 
 
 $connect ‚Äì Triggers a Lambda function to log the connection_id in DynamoDB. This enables tracking of active connections, targeted messaging, and efficient connection management, supporting real-time communication and scalability across multiple server instances. 
 $disconnect ‚Äì Logs the disconnection in DynamoDB to remove the connection_id record from DynamoDB table. This facilitates proper cleanup of inactive connections, helps prevent resource waste, maintains an accurate list of active clients, and helps optimize system performance and resource allocation. 
 $default ‚Äì Handles unexpected or invalid traffic. 
 
WebSocket authentication using Amazon Cognito 
The WebSocket API integrates Amazon Cognito for authentication and uses a JWT-based Lambda authorizer function for token validation. The authentication flow follows these steps: 
 
 User authentication 
   
   The course designer signs in using Amazon Cognito, which issues a JWT access token upon successful authentication. 
   Amazon Cognito supports multiple authentication methods, including username-password login, social identity providers (such as Google or Facebook), and SAML-based federation. 
    
 WebSocket connection request 
   
   When a user attempts to connect to the WebSocket API, the client includes the JWT access token in the WebSocket request headers. 
    
 JWT token validation (Lambda authorizer function) 
   
   The JWT token authorizer Lambda function extracts and verifies the token against the Amazon Cognito public key. 
   If the token is valid, the request proceeds. If the token isn‚Äôt valid, the connection is rejected. 
    
 Maintaining user sessions 
   
   Upon successful authentication, the $connect route Lambda function stores the connection_id and user details in DynamoDB, allowing targeted messaging. 
   When the user disconnects, the $disconnect Lambda function removes the connection_id to maintain an accurate session record. 
    
 
The following is a sample AWS CDK code to set up the WebSocket API with Amazon Cognito. AWS CDK is an open source software development framework to define cloud infrastructure in code and provision it through AWS CloudFormation. The following code is written in Python. For more information, refer to Working with the AWS CDK in Python: 
 
 from aws_cdk import (
    aws_apigatewayv2 as apigwv2,
    aws_lambda as _lambda,
    aws_lambda_python_alpha as _alambda,
    aws_cognito as cognito,
    aws_dynamodb as dynamodb,
    aws_apigatewayv2_integrations as integrationsv2,
    aws_apigatewayv2_authorizers as authorizersv2,
)
class CourseStack(Stack):
    def __init__(self, scope: core.Construct, id: str, **kwargs) -&gt; None:
        super().__init__(scope, id, **kwargs)
        
        ....
        # Previous code ...
        ....
        
        # DynamoDB table to track connections
        course_connections_ddb_table = dynamodb.Table(self, "CourseConnectionsTable",
                        partition_key=dynamodb.Attribute(name="connectionId", type=dynamodb.AttributeType.STRING),
                        time_to_live_attribute="ttl",
                        billing_mode=dynamodb.BillingMode.PAY_PER_REQUEST,
                        encryption=dynamodb.TableEncryption.AWS_MANAGED,
                        point_in_time_recovery=True,
                        removal_policy=RemovalPolicy.DESTROY
        )
        
        # Create userpool for Amazon Cognito
        user_pool = cognito.UserPool(
            self, "CourseUserPool",
            user_pool_name="CourseUserPool",
            self_sign_up_enabled=True,
            account_recovery=cognito.AccountRecovery.EMAIL_ONLY,
            user_verification=cognito.UserVerificationConfig(
                email_subject="Verify your email for outline and content generation App",
                email_body="Hello {username}, Thanks for signing up to Course outline and content generation App! Your verification code is {####}",
                email_style=cognito.VerificationEmailStyle.CODE,
            ),
            standard_attributes={"fullname": cognito.StandardAttribute(required=True, mutable=True)},
            removal_policy=RemovalPolicy.DESTROY,
        )

        # Create a new Amazon Cognito User Pool Client
        user_pool_client = user_pool.add_client("CourseUserPoolAppClient",
                                                user_pool_client_name="CourseUserPoolAppClient",
                                                id_token_validity=Duration.days(1),
                                                access_token_validity=Duration.days(1),
                                                auth_flows=cognito.AuthFlow(user_password=True)
                                                )

        # WebSocket Connect, disconnect, default Lambda functions
        course_ws_connect_lambda = _lambda.Function(
            self, "CourseWSConnect",
            code=_lambda.Code.from_asset("./lambda/connect"),
            runtime=_lambda.Runtime.PYTHON_3_12,
            handler="index.lambda_handler",
            timeout=Duration.seconds(30),
            environment={"CONNECTIONS_TABLE": course_connections_ddb_table.table_name},
        )
        course_connections_ddb_table.grant_read_write_data(course_ws_connect_lambda)

        course_ws_disconnect_lambda = _lambda.Function(...)
        
        course_ws_default_lambda = _lambda.Function(...)
        
        jwt_auth_course_lambda = _lambda.Function(...)
        
        course_outline_ws_lambda = _lambda.Function(...)
        
        course_content_ws_lambda = _lambda.Function(...) 

        # Course Web Socket API
        course_ws_authorizer = authorizersv2.WebSocketLambdaAuthorizer("CourseWSAuthorizer", jwt_auth_course_lambda, identity_source=["route.request.header.Authorization",]) # "route.request.querystring.Authorization", 
        course_ws_connect_integration = integrationsv2.WebSocketLambdaIntegration("CourseWSConnectIntegration", course_ws_connect_lambda)
        course_ws_disconnect_integration = integrationsv2.WebSocketLambdaIntegration("CourseWSDisconnectIntegration", course_ws_disconnect_lambda)
        course_ws_default_integration = integrationsv2.WebSocketLambdaIntegration("CourseWSDefaultIntegration", course_ws_default_lambda)
        course_outline_ws_integration = integrationsv2.WebSocketLambdaIntegration("CourseOutlineIntegration", course_outline_ws_lambda)
        course_content_ws_integration = integrationsv2.WebSocketLambdaIntegration("CourseContentIntegration", course_content_ws_lambda)

        course_ws_api=apigwv2.WebSocketApi(self, "CourseWSApi",
            api_name="CourseWSApi",
            description="WebSocket API for Course Outline and Content Generation",
            connect_route_options=apigwv2.WebSocketRouteOptions(
                integration=course_ws_connect_integration,
                authorizer=course_ws_authorizer
            ),
            disconnect_route_options=apigwv2.WebSocketRouteOptions(
                integration=course_ws_disconnect_integration,
            ),
            default_route_options=apigwv2.WebSocketRouteOptions(
                integration=course_ws_default_integration,
            )
        )

        # Add a custom message route, to generate course outline
        course_ws_api.add_route("courseOutline", integration=course_outline_ws_integration)
        
        # Add a custom message route, to generate course content
        course_ws_api.add_route("courseContent", integration=course_content_ws_integration)
        
        # Create a WebSocket API stage (usually, "dev" or "prod")
        course_ws_stage = apigwv2.WebSocketStage(
            self, "CourseWSApiStage",
            web_socket_api=course_ws_api,
            stage_name="dev",  # Change this based on the environment (e.g., "prod")
            auto_deploy=True,
        )
        
        # Grant permissions for Lambda to manage the WebSocket connection (for sending messages back to clients)
        course_ws_api.grant_manage_connections(course_ws_connect_lambda)
        course_ws_api.grant_manage_connections(course_ws_disconnect_lambda)
        course_ws_api.grant_manage_connections(course_ws_default_lambda)
        course_ws_api.grant_manage_connections(course_outline_ws_lambda)
        course_ws_api.grant_manage_connections(course_content_ws_lambda) 
 
Course outline generation 
The course outline generation module helps course designers create a structured course outline. For this proof of concept, the default structure spans 4 weeks, with each week containing three main learning outcomes and supporting secondary outcomes, but it can be changed according to each course or institution‚Äôs reality. The module follows this workflow: 
 
 The course designer submits a prompt using the course WebSocket (courseOutline route). 
 CourseOutlineWSLambda sends the request to an SQS queue for asynchronous processing. 
 The SQS queue triggers CourseOutlineLLMLambda, which invokes Anthropic‚Äôs Claude 3.5 Sonnet in Amazon Bedrock to generate the outline. 
 The response is structured using Pydantic models and returned as JSON. 
 The structured outline is stored in an S3 OutputBucket, with a finalized version stored in a portal bucket for faculty review. 
 
The following code sample is a sample payload for the courseOutline route, which can be customized to meet institutional requirements. The fields are defined as follows: 
 
 action ‚Äì Specifies the operation to be performed (courseOutline). 
 is_streaming ‚Äì Indicates whether the response should be streamed (yes for real-time streaming and no for single output at one time). 
 s3_input_uri_list ‚Äì A list of S3 URIs containing reference materials (which can be left empty if not available). 
 course_title ‚Äì The title of the course for which the outline is being generated. 
 course_duration ‚Äì The total number of weeks for the course. 
 user_prompt ‚Äì A structured prompt guiding the AI to generate a detailed course outline based on syllabus information, providing a well-organized weekly learning structure. If using a different LLM, optimize the user_prompt for that model to achieve the best results. 
 
 
 {
    "action": "courseOutline", 
    "is_streaming": "yes",
    "s3_input_uri_list": [],
    "course_title": "Fundamental of Machine Learning",
    "course_duration": 2,
    "user_prompt": "I need help developing a {course_duration}-week course content for a {course_title} course. Please use the following syllabus to:\n\n1. If provided, refer to the syllabus text from &lt;syllabus&gt; tags to extract the course learning outcomes.\n2. Design each week to focus on 3 main learning outcomes.\n3. For each main learning outcome, provide 3 supporting sub-learning outcomes.\n\n&lt;syllabus&gt;\n\n{syllabus_text}\n\n&lt;/syllabus&gt;\n\nEnsure that each week has 3 main learning outcomes and each of those has 3 supporting sub-learning outcomes."
} 
 
When interacting with the courseOutline route of the WebSocket API, the response follows a structured format that details the course outline and structure. The following is an example of a WebSocket response for a course. This format is designed for straightforward parsing and seamless integration into your applications: 
 
 {
    "course_title": "Sample Course",
    "course_duration": "4",
    "weekly_outline": [
        {
            "week": 1,
            "main_outcomes": [
                {
                    "outcome": "Learning Outcome 1",
                    "sub_outcomes": ["Sub-outcome 1", "Sub-outcome 2", "Sub-outcome 3"]
                },
                {... similar for Learning outcome 2},
 {... similar for Learning outcome 3}
            ]
        },
        {... similar for week 2},
        {... similar for week 3},
        {... similar for week 4},
    ]
} 
 
Here‚Äôs a snippet of the Lambda function for processing the outline request: 
 
 event = json.loads(event['Records'][0]['body'])

route_key = event['requestContext']['routeKey']
connection_id = event['requestContext']['connectionId']
body = json.loads(event["body"])
s3_input_uri_list = body["s3_input_uri_list"]
user_prompt = body["user_prompt"]
course_title = body["course_title"]
course_duration = body["course_duration"]
model_id = os.getenv("MODEL_ID", "")
is_streaming = body["is_streaming"]
websocket_endpoint_url = os.getenv("WEBSOCKET_ENDPOINT_URL","")
output_bucket = os.getenv("OUTPUT_BUCKET", "")

# Send message to api that message received
apigatewaymanagementapi_client = boto3.client('apigatewaymanagementapi', endpoint_url=websocket_endpoint_url)

# Read the syllabus text from uploaded doc
syllabus_text = ""
for s3_input_uri in s3_input_uri_list:
    bucket, key = get_s3_bucket_and_key(s3_input_uri)
    if key.endswith('.pdf'):
        pdf_text = extract_text_from_pdf(bucket, key)
        syllabus_text = syllabus_text + pdf_text

# Initialize the Pydantic model
pydantic_classes = [CourseOutline]

course_outline = {}

system_prompt = f"""You are an AI assistant tasked with helping an instructor develop a course outline for a {course_title} course.
You have expertise in curriculum design. Your role is to analyze the provided syllabus, extract learning outcomes, 
and structure a {course_duration}-week course with specific learning objectives for each week. 
Format your response in valid JSON for easy parsing and integration.
Respond only with the requested content, without any preamble or explanation."""

user_msg_prompt = PromptTemplate.from_template(user_prompt)

user_msg = user_msg_prompt.format(course_title=course_title, course_duration=course_duration, syllabus_text=syllabus_text) 

messages = [{"role": "user","content": [{"text": user_msg}]}]

tools = []
for class_ in pydantic_classes:
    tools.append(convert_pydantic_to_bedrock_converse_function(class_))
tool_config = { "tools": tools }

inference_config = {"temperature": 0.5 }

converse_response = bedrock_runtime_client.converse(
        system=[{ "text": system_prompt}],
        modelId=model_id,
        messages=messages,
        inferenceConfig=inference_config,
        toolConfig=tool_config,
    )

# Parse the LLM response into JSON format
course_outline = parse_bedrock_tool_response(converse_response)
        
send_message_to_ws_client(apigatewaymanagementapi_client, connection_id, response=course_outline)

return {'statusCode': 200,
         'body': json.dumps({'course_outline': course_outline})
        }
     
 
Course content generation 
The course content generation module creates detailed week-by-week content based on the course outline. Although the default configuration generates the following for each main learning outcome, these outputs are fully customizable to meet specific course needs and institutional preferences: 
 
 One set of reading materials 
 Three video scripts (3 minutes each) 
 A quiz with a multiple-choice question for each video 
 
The module follows this workflow: 
 
 The course designer submits learning outcomes using the courseContent route. 
 CourseContentWSLambda function sends the request to an SQS queue. 
 The SQS queue triggers CourseContentLLMLambda function, which calls Amazon Bedrock to generate the content. 
 The generated content is structured and stored in Amazon S3. 
 
The following is a sample payload for the courseContent route, which can be customized to align with institutional requirements. The fields are defined as follows: 
 
 action ‚Äì Specifies the operation to be performed (courseContent). 
 is_streaming ‚Äì Determines the response mode (yes for real-time streaming and no for a single output at one time). 
 s3_input_uri_list ‚Äì An array of S3 URIs containing additional course materials which will be used to generate course content (optional). 
 week_number ‚Äì Indicates the week number for which content is being generated. 
 course_title ‚Äì The title of the course. 
 main_learning_outcome ‚Äì The primary learning objective for the specified week. 
 sub_learning_outcome_list ‚Äì A list of supporting learning outcomes to be covered. 
 user_prompt ‚Äì A structured instruction guiding the LLM to generate week-specific course content, facilitating comprehensive coverage. If switching to a different LLM, optimize the user_prompt for optimal performance. 
 
 
 {
    "action":"courseContent", 
    "is_streaming": "yes",
    "s3_input_uri_list": ["s3://coursestack-inputbucket3bf8630a-v0xovtepdtey/dinesh_testing_folder/Fundamentals Of Machine Learning/Machine Learning Basics.pdf"],
    "week_number":1,
    "course_title": "Fundamental of Machine Learning",
    "main_learning_outcome" : "Understand the basics of machine learning and its applications",
    "sub_learning_outcome_list" : [
"Define machine learning and its relationship to artificial intelligence",
"Identify real-world applications of machine learning",
"Distinguish between supervised, unsupervised, and reinforcement learning"
],
    "user_prompt":"For the course {course_title}, \ngenerate Week {week_number} content for the main learning outcome:\n{main_learning_outcome}\n\nInclude the following sub-learning outcomes:\n{sub_learning_outcome_list}\n\nFor each sub-learning outcome, provide:\n- 3 video scripts, each 3 minutes long\n- 1 set of reading materials, atleast one page long\n- 1 multiple-choice question per video with correct answer\n\nIf provided, refer to the information within the &lt;additional_context&gt; tags for any supplementary details or guidelines.\n\n&lt;additional_context&gt;\n{additional_context}\n&lt;/additional_context&gt;\n\nGenerate the content without any introductory text or explanations."
} 
 
When interacting with the courseContent route of the WebSocket API, the response follows a structured format that details the course content. The following is an example of a WebSocket response for a course content. This format is designed for easy parsing and seamless integration into your applications: 
 
 {
   "CourseContent":{
      "week_number":1,
      "main_learning_outcome":"Learning Outcome 1",
      "reading_material":{
         "title":"xxx title of the reading material",
         "content":"xxx reading material content"
      },
      "sub_learning_outcomes_content":[
         {
            "sub_learning_outcome":"Sub-outcome 1",
            "video_script":{
               "script":"xxx video script"
            },
            "multiple_choice_question":{
               "question":"xxx MCQ question",
               "options":["option 1","option 2","option 3","option 4"],
               "correct_answer":"option 1"
            }
         },
         {... similar for sub_learning_outcome 2},
         {... similar for sub_learning_outcome 3},
      ]
   }
} 
 
Here‚Äôs a Lambda function code snippet for content generation: 
 
 event = json.loads(event['Records'][0]['body'])

connection_id = event['requestContext']['connectionId']
body = json.loads(event["body"])
s3_input_uri_list = body["s3_input_uri_list"]
user_prompt = body["user_prompt"]
week_number = body["week_number"]
course_title = body["course_title"]
main_learning_outcome = body["main_learning_outcome"]
sub_learning_outcome_list = body["sub_learning_outcome_list"]
is_streaming = body["is_streaming"]
model_id = os.getenv("MODEL_ID","")
websocket_endpoint_url = os.environ["WEBSOCKET_ENDPOINT_URL"]
output_bucket = os.environ["OUTPUT_BUCKET"]

# Send message to api that message received
apigatewaymanagementapi_client = boto3.client('apigatewaymanagementapi', endpoint_url=websocket_endpoint_url)
    

# Read the additional_context text from uploaded doc
additional_context = ""
for s3_input_uri in s3_input_uri_list:
    bucket, key = get_s3_bucket_and_key(s3_input_uri)
    if key.endswith('.pdf'):
        pdf_text = extract_text_from_pdf(bucket, key)
        additional_context = additional_context + pdf_text

# Initialize the Pydantic model
pydantic_classes = [CourseContent]

course_content={}    

system_prompt =f"""You are an AI assistant specialized in educational content creation.
Your task is to generate course materials based on given learning outcomes.
Produce concise, accurate, and engaging content suitable for college-level courses.
You may refer to additional context provided within &lt;additional_context&gt; tags if present.
Format your response in valid JSON for easy parsing and integration.
Respond only with the requested content, without any preamble or explanation."""

user_msg_prompt = PromptTemplate.from_template(user_prompt)

user_msg = user_msg_prompt.format(course_title=course_title, 
                                    week_number=week_number,
                                    main_learning_outcome=main_learning_outcome,
                                    sub_learning_outcome_list=sub_learning_outcome_list,
                                    additional_context=additional_context)

messages = [{"role": "user","content": [{"text": user_msg}]}]

tools = []
for class_ in pydantic_classes:
    tools.append(convert_pydantic_to_bedrock_converse_function(class_))
tool_config = { "tools": tools }

converse_response = bedrock_runtime_client.converse(
                            system=[{ "text": system_prompt}],
                            modelId=model_id,
                            messages=messages,
                            toolConfig=tool_config,
                            )

# Parse the LLM response into JSON format
course_content = parse_bedrock_tool_response(converse_response)

send_message_to_ws_client(apigatewaymanagementapi_client, connection_id, response=course_content)
   
return {'statusCode': 200,
         'body': json.dumps({'course_content': json.dumps(course_content)})
        } 
 
Prerequisites 
To implement the solution provided in this post, you should have the following: 
 
 An active AWS account and familiarity with foundation models (FMs) and Amazon Bedrock. Enable model access for Anthropic‚Äôs Claude 3.5v2 Sonnet and Anthropic‚Äôs Claude 3.5 Haiku 
 The AWS Cloud Development Kit (AWS CDK) already set up. For installation instructions, refer to the AWS CDK workshop. 
 When deploying the CDK stack, select a Region where Anthropic‚Äôs Claude models in Amazon Bedrock are available. Although this solution uses the US West (Oregon) us-west-2 Region, you can choose a different Region but you need to verify that it supports Anthropic‚Äôs Claude models in Amazon Bedrock before proceeding. The Region you use to access the model must match the Region where you deploy your stack. 
 
Set up the solution 
When the prerequisite steps are complete, you‚Äôre ready to set up the solution: 
 
 Clone the repository: 
 
 
 git clone https://github.com/aws-samples/educational-course-content-generator-with-qna-bot-using-bedrock.git 
 
 
 Navigate to the project directory: 
 
 
 cd educational-course-content-generator-with-qna-bot-using-bedrock/ 
 
 
 Create and activate the virtual environment: 
 
 
 python3 -m venv .venv
source .venv/bin/activate 
 
The activation of the virtual environment differs based on the operating system; refer to the AWS CDK workshop for activating in other environments. 
 
 After the virtual environment is activated, you can install the required dependencies: 
 
 
 pip install -r requirements.txt 
 
 
 Review and modify the project_config.json file to customize your deployment settings 
 In your terminal, export your AWS credentials for a role or user in ACCOUNT_ID. The role needs to have all necessary permissions for CDK deployment: 
 
export AWS_REGION=‚Äù&lt;region&gt;‚Äù # Same region as ACCOUNT_REGION above export AWS_ACCESS_KEY_ID=‚Äù&lt;access-key&gt;‚Äù # Set to the access key of your role/user export AWS_SECRET_ACCESS_KEY=‚Äù&lt;secret-key&gt;‚Äù # Set to the secret key of your role/user 
 
 If you‚Äôre deploying the AWS CDK for the first time, invoke the following command: 
 
 
 cdk bootstrap 
 
 
 Deploy the stacks: 
 
 
 cdk deploy --all 
 
Note the CloudFront endpoints, WebSocket API endpoints, and Amazon Cognito user pool details from deployment outputs. 
 
 Create a user in the Amazon Cognito user pool using the AWS Management Console or AWS Command Line Interface (AWS CLI). Alternatively, you can use the cognito-user-token-helper repository to quickly create a new Amazon Cognito user and generate JSON Web Tokens (JWTs) for testing. 
 Connect to the WebSocket endpoint using wscat. 
 
 
 wscat -c wss://xxxxxxxxxx.execute-api.us-west-2.amazonaws.com/dev \
	-H "Authorization: Bearer YOUR_JWT_TOKEN" 
 
Scalability and security considerations 
The solution is designed with scalability and security as core principles. Because Amazon API Gateway for WebSockets doesn‚Äôt inherently support AWS WAF, we‚Äôve integrated Amazon CloudFront as a distribution layer and applied AWS WAF to enhance security. 
By using Amazon SQS and AWS Lambda, the system enables asynchronous processing, supports high concurrency, and dynamically scales to handle varying workloads. AWS WAF helps to protects against malicious traffic and common web-based threats. Amazon CloudFront can improve global performance, reduce latency, and provide built-in DDoS protection. Amazon Cognito handles authentication so that only authorized users can access the WebSocket API. AWS IAM policies enforce strict access control to secure resources such as Amazon Bedrock, Amazon S3, AWS Lambda, and Amazon DynamoDB. 
Clean up 
To avoid incurring future charges on the AWS account, invoke the following command in the terminal to delete the CloudFormation stack provisioned using the AWS CDK: 
 
 cdk destroy --all 
 
Conclusion 
This innovative solution represents a significant leap forward in educational technology, demonstrating how AWS services can be used in course development. By integrating Amazon Bedrock, AWS Lambda, WebSockets, and a robust suite of AWS services, we‚Äôve built a system that streamlines content creation, enhances real-time interactivity, and facilitates secure, scalable, and high-quality learning experiences. 
By developing comprehensive course materials rapidly, course designers can focus more on personalized instruction and student mentoring. AI-assisted generation facilitates high-quality, standardized content across courses. The event-driven architecture scales effortlessly to meet institutional demands, and CloudFront, AWS WAF, and Amazon Cognito support secure and optimized content delivery. Institutions adopting this technology position themselves at the forefront of educational innovation, redefining modern learning environments. 
This solution goes beyond simple automation‚Äîit means teachers and professors can shift their focus from manual content creation to high-impact teaching and mentoring. By using AWS AI and cloud technologies, institutions can enhance student engagement, optimize content quality, and scale seamlessly. 
We invite you to explore how this solution can transform your institution‚Äôs approach to course creation and student engagement. To learn more about implementing this system or to discuss custom solutions for your specific needs, contact your AWS account team or an AWS education specialist. 
Together, let‚Äôs build the future of education on the cloud. 
 
About the authors 
Dinesh Mane&nbsp;is a Senior ML Prototype Architect at AWS, specializing in machine learning, generative AI, and MLOps. In his current role, he helps customers address real-world, complex business problems by developing machine learning and generative AI solutions through rapid prototyping. 
Tasneem Fathima is Senior Solutions Architect at AWS. She supports Higher Education and Research customers in the United Arab Emirates to adopt cloud technologies, improve their time to science, and innovate on AWS. 
Amir Majlesi leads the EMEA prototyping team within AWS Worldwide Specialist Organization. Amir has extensive experiences in helping customers accelerate adoption of cloud technologies, expedite path to production and catalyze a culture of innovation. He enables customer teams to build cloud native applications using agile methodologies, with a focus on emerging technologies such as Generative AI, Machine Learning, Analytics, Serverless and IoT.
‚Ä¢ How Handmade.com modernizes product image and description handling with Amazon Bedrock and Amazon OpenSearch Service
  Handmade.com is a leading hand-crafts product marketplace, offering unique, seller-contributed items to customers around the world. With over 60,000 products in the catalog and some percentage of listings containing basic descriptions that could be improved for better search and search engine optimization (SEO) performance, the need for automation became evident. Manual processing, consuming on average 10 hours per week, required a team of several people to maintain baseline quality. As their marketplace scaled, so did the need to automate and enhance the quality of product descriptions and metadata at scale. 
Handmade.com supports a wide range of hand-crafted goods, each with distinct attributes and presentation needs, making it essential to move beyond one-size-fits-all descriptions. The diversity of product types‚Äîfrom textiles to sculpture‚Äîrequires content reflecting each item‚Äôs unique characteristics. In addition, minimizing the time from seller submission to product publication is critical, especially as sellers expect real-time feedback and go-live timelines of under an hour. To support international growth, Handmade.com also needs to generate high-quality content across multiple languages and regions, facilitating discoverability and relevance for a global audience. 
This post focuses on the challenge of generating rich, scalable product descriptions for hand-crafted goods uploaded by a distributed seller base. We explore how Handmade.com designed and implemented a hybrid AI and vector search solution using Amazon Bedrock and Amazon OpenSearch Service for semantic content retrieval. 
Solution overview 
Handmade.com implemented an end-to-end AI-driven pipeline to automate and enrich product descriptions. The process begins with image and metadata ingestion, followed by initial description generation using Anthropic‚Äôs Claude 3.7 Sonnet large language model (LLM). These descriptions are embedded with the Amazon Titan Text Embeddings V2 model and stored in Amazon OpenSearch Service, enabling vector-based semantic search. Retrieval Augmented Generation (RAG) then contextualizes results to produce refined, SEO-optimized outputs tailored to each product. 
The following diagram illustrates the high-level architecture. 
 
The solution includes the following components: 
Product image upload and initial processing: 
 
 Product images and metadata are fetched from the Handmade.com product data repository and Elasticsearch index 
 Anthropic‚Äôs Claude in Amazon Bedrock is used to generate an initial description for each uploaded image 
 
Vector embedding and storage: 
 
 The generated description is embedded using Amazon Titan Text Embeddings V2 
 The embeddings are stored in an OpenSearch Service vector index, enabling semantic search capabilities 
 
User-uploaded product enrichment: 
 
 When a user uploads a product image, Anthropic‚Äôs Claude generates a draft description 
 The draft description is embedded with Amazon Titan and compared against existing entries in the OpenSearch Service vector store 
 
Retrieval Augmented Generation: 
 
 Retrieved context from OpenSearch Service is sent along with the new image to Anthropic‚Äôs Claude in Amazon Bedrock 
 A refined, enriched product description is generated using the RAG pattern 
 
Search engine optimization: 
 
 Anthropic Claude 3.7 Sonnet is used to generate SEO metadata, including terms and enhanced product narratives 
 Prompts to the model follow the pattern: ‚ÄúAnalyze this product image and provide a detailed SEO response containing seoTerms, product-description, etc.‚Äù 
 
User interaction and API handling: 
 
 Sellers interact through the Handmade.com interface 
 A Node.js API handles image ingestion, model invocation, and search workflows using the AWS SDK and OpenSearch packages 
 
Data handling and system integration: 
 
 Product and image data are continuously ingested from the Handmade.com index 
 Amazon Bedrock, OpenSearch Service, and custom APIs coordinate embedding, vector search, and contextual inference 
 
The solution also incorporates customer interaction data to continuously improve description generation and product discovery. The system analyzes user engagement metrics including click-through rates, time-on-page, and conversion events. These behavioral signals are used to refine the prompt engineering for Anthropic‚Äôs Claude, optimizing description generation for enhanced customer engagement. 
Additionally, the system uses customer review data as a valuable input for the RAG pipeline. Natural language processing extracts specific product attributes and craftsmanship details from review text (e.g., ‚Äúunique glazing technique‚Äù or ‚Äúcolor variation characteristics‚Äù). These insights are embedded alongside product descriptions in the OpenSearch Service vector store, enabling more nuanced semantic search capabilities. By combining review-derived context with behavioral data, the system can more effectively match customers with relevant artisan products based on both visual and qualitative attributes. 
Description and metadata sample prompts 
To generate consistent and high-quality outputs, Handmade.com crafted structured prompts for Anthropic‚Äôs Claude to process images and metadata. These prompts guide the model to generate product descriptions, contextual enhancements, and metadata. The following are sample roles given to Anthropic‚Äôs Claude to produce a variety of output text: 
[
&nbsp; { "role": "Material Enthusiast" },
&nbsp; { "role": "Sustainability Advocate" },
&nbsp; { "role": "Heritage Historian" },
&nbsp; { "role": "Functionality Reviewer" },
&nbsp; { "role": "Maker Advocate" },
&nbsp; { "role": "Visual Poet" }
] 
The process then uses AI and a vector store to enhance product descriptions for SEO and user engagement. We discuss the key steps in the following section. 
Image analysis 
The image analysis workflow consists of the following steps: 
 
 A user uploads a product image. 
 Anthropic Claude processes the image and generates a structured description, identifying details and potential uses: 
 
"AI Request": [
&nbsp;&nbsp;&nbsp; {
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "type": "image",
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "source": {
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "type": "uploaded_file",
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "data": "user_image"
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }
&nbsp;&nbsp;&nbsp; },
&nbsp;&nbsp;&nbsp; {
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "type": "text",
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "text": "Create a simple title and description for this product."
&nbsp;&nbsp;&nbsp; }
] 
Vector search for similar products 
The vector search process consists of the following steps: 
 
 The uploaded image is compared against a curated dataset containing descriptions of approximately 1 million handmade products that have been acquired for over 20 years. 
 This dataset is stored in an OpenSearch Service index, structured as stored vectors‚Äîa digital system where product descriptions are encoded as numerical representations of the text and its relationships rather than plain text. 
 The image description from Anthropic‚Äôs Claude is converted into a vector using Amazon Titan Text Embeddings V2, enabling an efficient similarity search within the index: 
 
const aiResponse = "AI-generated text from Claude"; // The product description generated by Claude AI
const requestParams = { &nbsp; modelId: "amazon.titan-embed-text-v2:0", &nbsp;&nbsp; // AI model used to process the text &nbsp;&nbsp; body: {{aiResponse }} &nbsp;&nbsp; // Formatting Claude's response for Amazon Titan}; 
Context-enriched description generation 
The description generation process consists of the following steps: 
 
 The system retrieves products with descriptions closely resembling the uploaded image. These descriptions, crafted by experts, serve as contextual references. 
 Anthropic Claude incorporates this contextual data‚Äîsuch as style, material insights, and historical relevance‚Äîto generate an enhanced, optimized product description tailored for engagement and discoverability. 
 
The following code is the sample final RAG request to AI, in which we pass the contextDocs as the product records of the prompt from our existing product: 
const productAnalysisRequest = `&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Analyze the uploaded product image.Use the provided context below to improve your response:${contextDocs}
Generate a clear and structured response in JSON format:{  "title": "A short product title",  "productDescription": "Describe the product, including details about materials used and unique features."}`; 
This structured approach makes sure AI-generated product descriptions are not only accurate and context-aware, but also optimized for search visibility, improving user experience and conversion rates. 
Conclusion 
Handmade.com successfully modernized its content generation workflow by using AI-powered components to automate product descriptions and optimize search. They chose Amazon Bedrock because it was quick and straightforward to integrate with their current architecture and provided multiple options for the model and embeddings used on the back-end for a respectable price point. The architecture incorporates foundation models from Amazon Bedrock, semantic retrieval with Amazon OpenSearch Service, and a lightweight API layer using Amazon API Gateway for seamless orchestration. These innovations have streamlined seller interactions and enabled consistent content quality across a large and growing catalog: 
 
 Modular orchestration ‚Äì Handmade.com‚Äôs design integrates Anthropic Claude for generation and Amazon Titan for embedding, enabling flexible enrichment and real-time contextual search 
 Scalable inference ‚Äì Amazon Bedrock handles concurrent multi-modal prompts and SEO requests without infrastructure overhead 
 Search optimization ‚Äì Vector-based search improves product discoverability and reduces friction in seller content workflows 
 
The system has laid the groundwork for future capabilities such as multilingual SEO, prompt tuning based on feedback, and inclusion of new content types. As a next step, Handmade.com plans to extend its use of Amazon Bedrock Agents for structured prompt workflows and to further simplify the on-boarding process for sellers. The team is also exploring ways to incorporate user engagement signals and review data to continue refining the generated content and improve recommendation quality. 
Ready to transform your own content management workflows with AI? The journey Handmade.com took to modernize their product descriptions demonstrates the powerful possibilities when combining Amazon Bedrock‚Äôs foundation models with Amazon OpenSearch Service‚Äôs semantic search capabilities. Whether you‚Äôre managing an e-commerce platform, content repository, or digital marketplace, these tools can help you achieve similar results in automation, scalability, and enhanced user experience. Start exploring Amazon Bedrock and Amazon OpenSearch Service today to build your own AI-powered content solution. Visit the AWS Documentation to learn more about getting started with these services or connect with an AWS solutions architect to discuss your specific needs. 
 
About the Authors 
Obadiah Ndhaye is a Solutions Architect at AWS with a focus on cloud resilience and helping customers implement robust and scalable architectures following best practices. Obadiah is passionate about emerging technologies, particularly generative AI, and enabling customers to innovate efficiently in the cloud. In addition to his technology interests, Obadiah enjoys outdoor recreational activities. 
Hardik Vasa&nbsp;is a Senior Solutions Architect at AWS. He focuses on Generative AI and Serverless technologies, helping customers make the best use of AWS services. Hardik shares his knowledge at various conferences and workshops. In his free time, he enjoys learning about new tech, playing video games, and spending time with his family. 
Richard Handley serves as a technical engineer supporting both&nbsp;Novica.com&nbsp;and&nbsp;Handmade.com. Throughout his career, he‚Äôs worked with everything from digital media companies to e-commerce platforms and content sites, helping both established brands and scrappy startups figure out their tech strategy and system architecture. Over the years, he‚Äôs developed a knack for spotting where technology can solve real problems and turning those insights into products that people want to use.&nbsp; 
Akhil Raj is a QA professional, working with both Novica.com and Handmade.com, with a strong background in software testing and quality engineering. With hands-on experience in test automation, CI/CD integration and cloud-based testing strategies. He integrates AWS services to streamline QA workflows and ensure high-quality software delivery. 
Shankar Sivan is a senior software engineer, at both Novica.com and Handmade.com, specializing in backend development and DevOps. He has expertise in AI driven solutions, including building systems with machine learning. His work focuses on automation, scalable infrastructure, and security best practices to enhance system reliability and efficiency.
‚Ä¢ Introducing Amazon Bedrock AgentCore Browser Tool
  At AWS Summit New York City 2025, Amazon Web Services (AWS) announced the preview of Amazon Bedrock AgentCore browser tool, a fully managed, pre-built cloud-based browser. This tool enables generative AI agents to interact seamlessly with websites. It addresses two fundamental limitations: first, foundation models (FMs) are trained on large but static datasets and need dynamic access to current information when API access isn‚Äôt readily available; second, organizations face significant challenges when attempting to scale web automation with AI for enterprise use cases. 
The development of agentic AI systems is moving toward applications that can execute complex, multistep tasks. For these agents to be effective, they require access to dynamic, real-time data, particularly from websites and web applications that don‚Äôt offer APIs or where API integration would be complex. Moreover, as businesses seek to deploy AI-powered automation across their operations, they need solutions that can reliably scale without the operational overhead of managing browser farms or solving complex concurrency issues. The AgentCore Browser Tool provides these capabilities, allowing agents to perform tasks such as automating research, streamlining operations, and interacting with web-based applications‚Äîall with the scalability, reliability, and security of the AWS Cloud infrastructure. By providing a fully managed cloud-based browser, AWS addresses the critical need for enterprises to deploy AI automation at scale across thousands of concurrent sessions, supporting use cases from customer service automation to large-scale data collection and analysis, without the traditional complexity and resource constraints of self-managed browser automation frameworks. 
In this post, we introduce the newly announced Amazon Bedrock AgentCore Browser Tool. We explore why organizations need cloud-based browser automation and the limitations it addresses for FMs that require real-time data access. We talk about key use cases and the core capabilities of the AgentCore Browser Tool. We walk through how to get started with the tool. 
Why do you need the cloud-based AgentCore Browser Tool? 
Traditional browser automation approaches have typically required significant infrastructure management, security considerations, and development expertise. The introduction of a fully managed, cloud-based browser automation solution addresses several critical needs, including simplified infrastructure management, enterprise-grade security, global availability and scaling, and cost optimization. Organizations no longer need to provision, maintain, and scale browser instances to support their automation needs. AWS now handles the complex infrastructure requirements, so developers can focus on building intelligent agent capabilities rather than managing browser farms. Cloud-based browser automation provides isolated execution environments with AWS security controls, reducing the risk of data exfiltration or unauthorized access that might occur in less controlled environments. With a cloud-based browser, you can instantaneously deploy browser instances across the global infrastructure of AWS so that browser automation can scale. By offering browser automation as a managed service, organizations can use a consumption-based pricing model instead of maintaining always-on infrastructure, which can substantially reduce costs for intermittent workloads. 
Use cases for cloud-based browser automation 
Handling repetitive web tasks: With the introduction of Amazon Bedrock AgentCore Browser Tool, organizations can now implement sophisticated browser automation at scale. Cloud-based browser automation excels at minimizing manual execution of repetitive tasks across web interfaces. AI agents can populate complex web forms across multiple systems, validate entries, and maintain compliance with business rules. Agents can navigate to internal dashboards, extract critical metrics, and compile reports without human intervention. For organizations managing large user-generated content domains, agents can assist human moderators by prescreening content across multiple web interfaces. 
AI powered research and intelligence gathering: With cloud-based browser automation, AI agents become powerful research assistants. They automatically track related websites for pricing changes, new product launches, or content updates with regular monitoring. You can use AI agents to gather and analyze consumer sentiment across various web forums, review sites, and social domains to inform product development. With the AgentCore Browser Tool, you can create automated systems that regularly scan trusted information sources to keep internal knowledge bases current. 
Complex workflow automation across systems: Many organizations operate across numerous web applications that lack integrated workflows. Use the AgentCore Browser Tool to automate customer setup across multiple software-as-a-service (SaaS) systems when APIs are unavailable. This helps maintain consistency and reduces error rates. You can monitor supplier portals, inventory systems, and logistics services to maintain visibility across complex supply chains. By automating account creation and permission settings across numerous internal web applications, employee onboarding becomes streamlined. 
Testing and quality assurance: Cloud-based browser automation enables robust testing at scale. You can use AgentCore Browser Tool to validate user experiences and functionality across different scenarios, devices, and browsers in parallel. Deploy agents to continuously interact with critical business applications and set up alerts to your teams about performance issues before customers encounter them. With AgentCore Browser Tool, you can regularly test web applications for accessibility compliance, security vulnerabilities, or regulatory requirements. 
Legacy system integration: Many organizations maintain legacy systems that lack modern APIs. Enable modern AI capabilities to interact with legacy web applications that would be costly to replace or modernize. Apply intelligent automation to systems that were never designed for programmatic access. As a result, you can extract valuable organizational data trapped in older web applications through regular, automated harvesting. 
Core capabilities 
The Amazon Bedrock AgentCore Browser Tool empowers AI agents to interact with web content the same way humans do, through a fully managed remote browser infrastructure that minimizes traditional complexity while delivering enterprise-grade security and scalability. 
Web interaction capabilities 
 
 Complete navigation control across websites and multipage workflows 
 Interaction with JavaScript-heavy applications and dynamic content 
 Form manipulation, including text fields, dropdown menus, and file uploads 
 Humanlike interaction patterns such as scrolling, hovering, and clicking 
 
Serverless browser infrastructure 
 
 Zero-management browser fleet with automatic patching 
 Seamless scaling from single session to thousands based on demand 
 Global deployment options with usage-based pricing 
 Optimized performance without infrastructure overhead 
 
Visual understanding 
 
 Full-page screenshots enabling AI comprehension of layout and content 
 Visual element identification by appearance and position 
 Content extraction from graphical elements 
 Resolution and device emulation capabilities 
 
Human-in-the-loop integration 
 
 Real-time interactive viewing and control for human operators 
 Session recording for review, training, and compliance 
 
Enterprise-grade security 
 
 Complete session isolation for each browser instance 
 AWS Identity and Access Management (IAM) controls for access management 
 Ephemeral browser sessions that reset after each use 
 
Complex web application support 
 
 Full compatibility with modern JavaScript frameworks 
 Authentication handling and session persistence 
 Processing of asynchronous content and real-time updates 
 Intelligent interaction with complex UI patterns 
 
Audit and compliance 
 
 Detailed interaction logging and session recording 
 Integration with AWS CloudTrail for comprehensive tracking 
 
Observability 
 
 Performance metrics on latency and resource usage 
 Integration with Amazon CloudWatch for unified monitoring 
 Session record and replay for observability 
 
This comprehensive set of capabilities bridges the fundamental gap between AI agents and the human web, enabling organizations to build intelligent agents that can understand and interact with content designed for humans rather than being limited to API-based integrations. 
How an AI agent can use AgentCore Browser Tool 
Amazon Bedrock AgentCore Browser runs in a secure, isolated containerized environment within AgentCore, insulating web activity from your local system. You can interact with the AgentCore Browser Tool using browser actuation libraries, such as Playwright, or use AI agentic frameworks specialized for browser automation, such as Amazon Nova Act and Browser Use. You can also integrate browser automation as a tool in a multi-agentic workflow. 
Amazon Nova Act or Browser Use works with the AgentCore Browser Tool to take natural language instructions from the user and convert them to actuations on the browser by following this workflow: 
 
 The user sends a query such as ‚Äúsearch for shoes on Amazon‚Äù 
 An agentic framework such as Amazon Nova Act or Browser Use passes the query to the large language model (LLM) 
 The LLM reasons and generates instructions in a structured output format (for example, JSON encoded) 
 The agentic framework maps these instructions into browser actuation commands (such as Playwright, Puppeteer, or Selenium) 
 The browser actuation commands are executed on the AgentCore Browser over a secure WebSocket connection 
 The response from the browser and a screenshot are sent to the agent to reason further 
 
This process repeats until the original task is complete. The flow is illustrated in the following diagram. 
 
Get started 
The Amazon Bedrock AgentCore Browser Tool is available for use today. For a collection of open source examples, visit the amazon-bedrock-agentcore-samples repository on GitHub. 
Prerequisites 
To use the Amazon Bedrock AgentCore Brower Tool, you need to complete the following prerequisites: 
 
 Python 3.10+ 
 Verify your IAM user or role has the permissions to use AgentCore Browser: 
 
 
 git clone https://github.com/awslabs/amazon-bedrock-agentcore-samples.git
pip install bedrock-agentcore  
 
For browser visualization on your local machine, you need the BrowserViewerServer component in the repository you cloned at: 01-tutorials/05-AgentCore-tools/02-Agent-Core-browser-tool/interactive_tools 
You can also visualize the browser live on the Amazon Bedrock AgentCore console at https://us-east-1.console.aws.amazon.com/bedrock-agentcore/builtInTools 
The following Python code demonstrates how to use the AgentCore Browser Tool directly with the Playwright library and the Amazon Bedrock AgentCore SDK. This example initiates a secure browser session, connects to it, and automates a straightforward workflow in which it navigates to https://www.amazon.com and searches for a product. 
 
 To get started with playwright: 
 
 
 cd 01-tutorials/05-AgentCore-tools/02-Agent-Core-browser-tool 
 
 
 Install dependencies: 
 
 
 pip install playwright 
 
 
 Author your playwright-based script: 
 
 
 from playwright.sync_api import sync_playwright, Playwright, BrowserType
from bedrock_agentcore.tools.browser_client import browser_session
from browser_viewer import BrowserViewerServer
import time
from rich.console import Console
console = Console()
def run(playwright: Playwright):
    # Create the browser session and keep it alive
    with browser_session('us-west-2') as client:
        ws_url, headers = client.generate_ws_headers()
        # Start viewer server
        viewer = BrowserViewerServer(client, port=8005)
        viewer_url = viewer.start(open_browser=True)
        # Connect using headers
        chromium: BrowserType = playwright.chromium
        browser = chromium.connect_over_cdp(
            ws_url,
            headers=headers
        )
        context = browser.contexts[0]
        page = context.pages[0]
        try:
            page.goto("https://amazon.com/")
            console.print(page.title())
            # Keep running
            while True:
                time.sleep(120)
        except KeyboardInterrupt:
            console.print("\n\n[yellow]Shutting down...[/yellow]")
            if 'client' in locals():
                client.stop()
                console.print("‚úÖ Browser session terminated")
        except Exception as e:
            console.print(f"\n[red]Error: {e}[/red]")
            import traceback
            traceback.print_exc()
with sync_playwright() as playwright:
    run(playwright) 
 
Alternatively, you can build a browser agent using Amazon Nova Act to automate web interactions: 
 
 Sign up for Nova Act at https://nova.amazon.com/act and generate an API key. 
 In the same Python virtual environment: 
 
pip install nova-act 
 
 Author your Nova Act based script: 
 
 
 import time
from bedrock_agentcore.tools.browser_client import browser_session
from nova_act import NovaAct
from rich.console import Console
from browser_viewer import BrowserViewerServer
 
NOVA_ACT_API_KEY = "YOUR_NOVA_ACT_API_KEY"
console = Console() 
 
def main():
    try:
        # Step 1: Create browser session
        with browser_session('us-west-2') as client:
            print("\r   ‚úÖ Browser ready!                    ")
            ws_url, headers = client.generate_ws_headers()
 
            # Step 2: Start viewer server
            console.print("\n[cyan]Step 3: Starting viewer server...[/cyan]")
            viewer = BrowserViewerServer(client, port=8005)
            viewer_url = viewer.start(open_browser=True)
 
            # Step 3: Use Nova Act to interact with the browser with NovaAct
            with NovaAct(
                    cdp_endpoint_url=ws_url,
                    cdp_headers=headers,
                    preview={"playwright_actuation": True},
                    nova_act_api_key=NOVA_ACT_API_KEY,
                    starting_page="https://www.amazon.com",
                ) as nova_act:
                    result = nova_act.act("Search for coffee maker and get the details of the lowest priced one on the first page")
                    console.print(f"\n[bold green]Nova Act Result:[/bold green] {result}")
            
            # Keep running
            while True:
                time.sleep(1)
             
    except KeyboardInterrupt:
        console.print("\n\n[yellow]Shutting down...[/yellow]")
        if 'client' in locals():
            client.stop()
            print("‚úÖ Browser session terminated")
    except Exception as e:
        print(f"\n[red]Error: {e}[/red]")
        import traceback
        traceback.print_exc()
 
if __name__ == "__main__":
    main() 
 
Alternatively, you can run the tutorial notebooks in the Amazon Bedrock AgentCore GitHub repository. 
Pricing and availability  
Amazon Bedrock AgentCore offers flexible, consumption-based pricing with no upfront commitments or minimum fees. AgentCore Browser can be used independently of the other services. You can try AgentCore services at no additional charge until September 16, 2025. After this date, AgentCore Browser Tool will be charged based on consumption. Billing is calculated per second, using the highest watermark of CPU and memory usage for that second, with a 1-second minimum. 128 MB minimum memory billing applies. Network data transfer through customer elastic network interfaces is billed at standard Amazon Elastic Compute Cloud (Amazon EC2) rates 
For more information about pricing, visit Amazon Bedrock AgentCore (Preview) Pricing. 
Conclusion 
Amazon Bedrock AgentCore Browser Tool marks a transformative advancement in AI-powered web automation, offering organizations a fully managed, cloud-based browser solution. AgentCore Browser Tool addresses critical limitations faced by generative AI systems requiring real-time data access, enabling AI agents to interact naturally with websites through capabilities such as complete navigation control, visual understanding, and seamless integration with frameworks such as Playwright and Amazon Nova Act. By using this tool, businesses can now implement sophisticated automation at scale across various use cases‚Äîfrom streamlining repetitive web tasks and conducting AI-enhanced research to automating complex workflows and integrating with legacy systems‚Äîall while benefiting from the reliable cloud infrastructure of AWS that adapts to organizational needs without the operational overhead of managing browser farms. 
Resources 
To learn more and start building, visit the following resources: 
 
 Amazon Bedrock AgentCore Developer Guide 
 Amazon Bedrock AgentCore console 
 
 
About the authors 
Veda Raman is a Senior Specialist Solutions Architect for generative AI and machine learning at AWS. Veda works with customers to help them architect efficient, secure, and scalable machine learning applications. Veda specializes in generative AI services like Amazon Bedrock and Amazon SageMaker. 
Rahul Sharma is a Senior Specialist Solutions Architect at AWS, helping AWS customers build and deploy, scalable Agentic AI solutions. Prior to joining AWS, Rahul spent more than decade in technical consulting, engineering, and architecture, helping companies build digital products, powered by data and machine learning. In his free time, Rahul enjoys exploring cuisines, traveling, reading books(biographies and humor) and binging on investigative documentaries, in no particular order. 
Kishor Aher is a Principal Product Manager at AWS, leading the Agentic AI team responsible for developing first-party tools such as Browser Tool, and Code Interpreter. As a founding member of Amazon Bedrock, he spearheaded the vision and successful launch of the service, driving key features including Converse API, Managed Model Customization, and Model Evaluation capabilities. Kishor regularly shares his expertise through speaking engagements at AWS events, including re:Invent and AWS Summits. Outside of work, he pursues his passion for aviation as a general aviation pilot and enjoys playing volleyball.

‚∏ª