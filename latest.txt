‚úÖ Morning News Briefing ‚Äì July 18, 2025 10:58

üìÖ Date: 2025-07-18 10:58
üè∑Ô∏è Tags: #briefing #ai #publichealth #digitalgov

‚∏ª

üßæ Weather
‚Ä¢ No watches or warnings in effect, Pembroke
  No watches or warnings in effect. No warnings or watches or watches in effect . Watch or warnings are no longer in effect in the U.S. No watches, warnings are in effect for the rest of the day . No watches and warnings are still in effect, but no watches are in place for the day's events . The weather is not expected to be affected by the weather .
‚Ä¢ Current Conditions:  9.7¬∞C
  Temperature: 9.7&deg;C Pressure / Tendency: 102.0 kPa rising Humidity: 96 % Humidity is 96 % Dewpoint: 9 .2&deg:C Wind: SW 6 km/h Air Quality Health Index: n/a . Pembroke 6:00 AM EDT Friday 18 July 2025 Temperature:¬†9.7/deg
‚Ä¢ Friday: Mainly sunny. High 23.
  Sunny. Becoming a mix of sun and cloud this afternoon . High 23. Humidex 25. UV index 8 or very high. Sunny. Sunny . Sunny. High 23 . High 25. Humidx 25 . Humidex 25.UV index 8 . Highlight: Sunny, sunny, sunny and sunny. Highlight for the rest of the day. Sunny and sunny

üåç International News
No updates.

üçÅ Canadian News
No updates.

üá∫üá∏ U.S. Top Stories
‚Ä¢ Ask your kids' camps these key questions about heat and flood safety, experts say
  Camps in nature can expose campers to floods, wildfires and heat . Here are the top questions experts say people should ask about safety . Camps can be great for kids, but they can also expose them to floods and wildfires . The top questions to ask campers about safety should be asked about safety in nature camps, experts say . For more information, visit CNN.com/
‚Ä¢ Congress hopes to raise heat on Russia amid souring relations between Trump and Putin
  A bipartisan coalition has joined forces to push aggressive new sanctions on Russia . They believe the souring relationship between President Trump and Vladimir Putin has created a new opening . The coalition believes the sour relationship between the two has created an opening for new sanctions to be pushed by the U.S. President Trump's relationship with Vladimir Putin is creating a new opportunity to push back against the Russian leader .
‚Ä¢ What the 'One Big Beautiful Bill' will change for students, schools and colleges
  School vouchers are going national and the federal student loan system is getting an overhaul . Here's what to know. School vouchers go national and federal student loans are getting a big overhaul. Here's how to learn more about how to apply to students in the U.S. and how to pay for school lunches and lunches with vouchers. See how to sign up for school vouchers.
‚Ä¢ A playbook is forming for younger candidates. The results have so far been mixed
  Calls for generational change and dissatisfaction with the status quo have been at the center of campaigns by younger candidates . While that has lifted some to victory, others have fallen short . Some candidates have failed to reach the heights of their campaigns, but others have come to terms with their lack of experience and lack of political experience in the U.S. House of Representatives in New York City, New
‚Ä¢ What did Trump tell supporters to 'not waste Time and Energy' on? Take our quiz
  This week, President Trump didn't want to talk about a thing . If you know what that thing is, you'll get at least one question right . Plus: Emmys! Babies! Tennis! Plus, the Emmys and the Olympics are coming up this week . Have you got a thing right? Share it with us at CNN iReport.com/poll.com .

üß† Artificial Intelligence
No updates.

üíª Digital Strategy
‚Ä¢ Time for Britain's CMA to strike hard ‚Äì or risk losing the cloud competition fight
  UK's ambition to become a global AI superpower hinges on a vibrant and competitive cloud market . Watchdog set to publish report into health of market next month, will it hold AWS and Microsoft's feet to the fire? The next few days will show if its competition regulator really appreciates both the pace of change and the scale of remedies needed to achieve both of these things .‚Ä¶‚Ä¶‚Ä¶
‚Ä¢ The Smoot ‚Äì How an MIT prank became a lasting unit of measurement
  The Harvard Bridge is the name of the smoot, which separates Cambridge, Cambridge, from Boston . We spoke to the smoot's namesake, who was the shortest among the MIT students . The smoot is the longest bridge in the world, and the smoothest was built in 1958 in Cambridge, Massachusetts . It was named after the Harvard Bridge, which is now in the shape of the world's
‚Ä¢ ‚ÄòI nearly died after flying thousands of miles to install a power cord for the NSA‚Äô
  On Call is The Register's Friday column that shares your terrifying tech support stories . This job was a car wreck in more than one way . On Call: Share your terrifying Tech support stories with us on Facebook and Twitter at CNN.com/Heroes . Back to the page you came from.com or click here for more of the latest from this week's On Call.com .
‚Ä¢ EU cloud gang wins Microsoft concessions, but fair software licensing group brands them 'stalling tactic'
  Coalition for Fair Software Licensing has blasted Microsoft as a 'stalling tactic' by the software giant . Pay-as-you-go model, privacy protections agreed ‚Äì but critics say it just buys 'Microsoft more time to lock in customers' Trade group of European cloud providers has claimed a small victory in bringing lower prices and more flexibility in deploying Microsoft software on their infrastructure . Coalition for
‚Ä¢ VMware slows release cadence for flagship Cloud Foundation suite, but extends support
  Analysts have warned Broadcom may slow innovation . Analyst: Analysts warn Broadcom could slow innovation in cloud computing . VMware has extended the time between major releases from two years to three and extended support for those releases to six years . Analysts say Broadcom's move could be a step back in the direction of other tech giants in the U.S., including Apple and Microsoft .

üè• Public Health
No updates.

üî¨ Science
‚Ä¢ Effectiveness of leading pedestrian intervals for city walkers‚Äô safety
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Personality traits and adherence on COVID-19 preventive measures in a two-year follow-up study
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Bidirectional relationship between anemia and cognitive function in middle-aged and older Chinese adults: a longitudinal study
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Heart from organ donor restarted outside the body ‚Äî technique offers new source of organs
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Incarceration and cancer care disparities in the USA
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations

üßæ Government & Policy
No updates.

üèõÔ∏è Enterprise Architecture & IT Governance
No updates.

ü§ñ AI & Emerging Tech
‚Ä¢ A brief history of ‚Äúthree-parent babies‚Äù
  This week we heard that eight babies have been born in the UK following an experimental form of IVF that involves DNA from three people. The approach was used to prevent women with genetic mutations from passing mitochondrial diseases to their children. You can read all about the results, and the reception to them, here.¬†



But these eight babies aren‚Äôt the first ‚Äúthree-parent‚Äù children out there. Over the last decade, several teams have been using variations of this approach to help people have babies. This week, let‚Äôs consider the other babies born from three-person IVF.





I can‚Äôt go any further without talking about the term we use to describe these children. Journalists, myself included, have called them ‚Äúthree-parent babies‚Äù because they are created using DNA from three people. Briefly, the approach typically involves using the DNA from the nuclei of the intended parents‚Äô egg and sperm cells. That‚Äôs where most of the DNA in a cell is found.



But it also makes use of mitochondrial DNA (mtDNA)‚Äîthe DNA found in the energy-producing organelles of a cell‚Äîfrom a third person. The idea is to avoid using the mtDNA from the intended mother, perhaps because it is carrying genetic mutations. Other teams have done this in the hope of treating infertility.



mtDNA, which is usually inherited from a person‚Äôs mother, makes up a tiny fraction of total inherited DNA. It includes only 37 genes, all of which are thought to play a role in how mitochondria work (as opposed to, say, eye color or height).



That‚Äôs why some scientists despise the term ‚Äúthree-parent baby.‚Äù Yes, the baby has DNA from three people, but those three can‚Äôt all be considered parents, critics argue. For the sake of argument, this time around I‚Äôll use the term ‚Äúthree-person IVF‚Äù from here on out.



So, about these babies. The first were reported back in the 1990s. Jacques Cohen, then at Saint Barnabas Medical Center in Livingston, New Jersey, and his colleagues thought they might be able to treat some cases of infertility by injecting the mitochondria-containing cytoplasm of healthy eggs into eggs from the intended mother.¬†Seventeen babies were ultimately born this way, according to the team. (Side note: In¬†their paper, the authors describe potential resulting children as ‚Äúthree-parental individuals.‚Äù)



But two fetuses appeared to have genetic abnormalities. And one of the children started to show signs of a developmental disorder. In 2002, the US Food and Drug Administration put a stop to the research.





The babies born during that study¬†are in their 20s now. But scientists still don‚Äôt know why they saw those abnormalities. Some think that mixing mtDNA from two people might be problematic.



Newer approaches to three-person IVF aim to include mtDNA from just the donor, completely bypassing the intended mother‚Äôs mtDNA. John Zhang at the New Hope Fertility Center in New York City tried this approach for a Jordanian couple in 2016. The woman carried genes for a fatal mitochondrial disease and had already lost two children to it. She wanted to avoid passing it on to another child.



Zhang took the nucleus of the woman‚Äôs egg and inserted it into a donor egg that had had its own nucleus removed‚Äîbut still had its mitochondria-containing cytoplasm. That egg was then fertilized with the woman‚Äôs husband‚Äôs sperm.



Because it was still illegal in the US, Zhang controversially did the procedure in Mexico, where,¬†as he told me at the time, ‚Äúthere are no rules.‚Äù The couple eventually welcomed a healthy baby boy. Less than 1% of the boy‚Äôs mitochondria carried his mother‚Äôs mutation, so the procedure was deemed a success.



There was a fair bit of outrage from the scientific community, though. Mitochondrial donation had been made legal in the UK the previous year, but no clinic had yet been given a license to do it. Zhang‚Äôs experiment seemed to have been conducted with no oversight. Many questioned how ethical it was, although Sian Harding, who reviewed the ethics of the UK procedure, then told me it was ‚Äúas good as or better than what we‚Äôll do in the UK.‚Äù



The scandal had barely died down by the time the next ‚Äúthree-person IVF‚Äù babies were announced. In 2017, a team at the Nadiya Clinic in Ukraine¬†announced the birth of a little girl to parents who‚Äôd had the treatment for infertility. The news brought more outrage from some quarters, as scientists argued that the experimental procedure should only be used to prevent severe mitochondrial diseases.



It wasn‚Äôt until later that year that the UK‚Äôs fertility authority granted a team in Newcastle a license to perform mitochondrial donation. That team launched a trial in 2017. It was big news‚Äîthe first ‚Äúofficial‚Äù trial to test whether the approach could safely prevent mitochondrial disease.





But it was slow going. And meanwhile, other teams were making progress. The Nadiya Clinic continued to trial the procedure in couples with infertility. Pavlo Mazur, a former embryologist who worked at that clinic, tells me that 10 babies were born there as a result of mitochondrial donation.



Mazur then moved to another clinic in Ukraine, where he says he used a different type of mitochondrial donation to achieve another five healthy births for people with infertility. ‚ÄúIn total, it‚Äôs 15 kids made by me,‚Äù he says.



But he adds that other clinics in Ukraine are also using mitochondrial donation, without sharing their results. ‚ÄúWe don‚Äôt know the actual number of those kids in Ukraine,‚Äù says Mazur. ‚ÄúBut there are dozens of them.‚Äù



In 2020, Nuno Costa-Borges of Embryotools in Barcelona, Spain, and his colleagues described¬†another trial of mitochondrial donation. This trial, performed in Greece, was also designed to test the procedure for people with infertility. It involved 25 patients. So far,¬†seven children have been born. ‚ÄúI think it‚Äôs a bit strange that they aren‚Äôt getting more credit,‚Äù says Heidi Mertes, a medical ethicist at Ghent University in Belgium.



The newly announced UK births are only the latest ‚Äúthree-person IVF‚Äù babies. And while their births are being heralded as a success story for mitochondrial donation, the story isn‚Äôt quite so simple. Three of the eight babies were born with a non-insignificant proportion of mutated mitochondria, ranging between 5% and 20%, depending on the baby and the sample.



Dagan Wells of the University of Oxford, who is involved in the Greece trial, says that two of the seven babies in their study also appear to have inherited mtDNA from their intended mothers. Mazur says he has seen several cases of this ‚Äúreversal‚Äù too.



This isn‚Äôt a problem for babies whose mothers don‚Äôt carry genes for mitochondrial disease. But it might be for those whose mothers do.



I don‚Äôt want to pour cold water over the new UK results. It was great to finally see the results of a trial that‚Äôs been running for eight years. And the births of healthy babies are something to celebrate. But it‚Äôs not a simple success story. Mitochondrial donation doesn‚Äôt guarantee a healthy baby. We still have more to learn, not only from these babies, but from the others that have already been born.



This article first appeared in The Checkup,¬†MIT Technology Review‚Äôs¬†weekly biotech newsletter. To receive it in your inbox every Thursday, and read articles like this first,¬†sign up here.
‚Ä¢ Finding value from AI agents from day one
  Imagine AI so sophisticated it could read a customer‚Äôs mind? Or identify and close a cybersecurity loophole weeks before hackers strike? How about a team of AI agents equipped to restructure a global supply chain and circumnavigate looming geopolitical disruption? Such disruptive possibilities explain why agentic AI is sending ripples of excitement through corporate boardrooms.&nbsp;







Although still so early in its development that there lacks consensus on a single, shared definition, agentic AI refers loosely to a suite of AI systems capable of connected and autonomous decision-making with zero or limited human intervention. In scenarios where traditional AI typically requires explicit prompts or instructions for each step, agentic AI will independently execute tasks, learning and adapting to its environment to refine decisions over time.&nbsp;



From assuming oversight for complex workflows, such as procurement or recruitment, to carrying out proactive cybersecurity checks or automating support, enterprises are abuzz at the potential use cases for agentic AI.&nbsp;



According to one Capgemini survey, 50% of business executives are set to invest in and implement AI agents in their organizations in 2025, up from just 10% currently. Gartner has also forecast that 33% of enterprise software applications will incorporate agentic AI by 2028. For context, in 2024 that proportion was less than 1%.&nbsp;



‚ÄúIt‚Äôs creating such a buzz ‚Äì software enthusiasts seeing the possibilities unlocked by LLMs, venture capitalists wanting to find the next big thing, companies trying to find the ‚Äòkiller app,‚Äù says Matt McLarty, chief technology officer at Boomi. But, he adds, ‚Äúright now organizations are struggling to get out of the starting blocks.‚Äù&nbsp;



The challenge is that many organizations are so caught up in the excitement that they risk attempting to run before they can walk when it comes to deployment of agentic AI, believes McLarty. And in so doing they risk turning it from potential business breakthrough into a source of cost, complexity, and confusion.



Keeping agentic AI simple&nbsp;



The heady capabilities of agentic AI have created understandable temptation for senior business leaders to rush in, acting on impulse rather than insight risks turning the technology into a solution in search of a problem, points out McLarty.&nbsp;



It‚Äôs a scenario that‚Äôs unfolded with previous technologies. The decoupling of Blockchain from Bitcoin in 2014 paved the way for a Blockchain 2.0 boom in which organizations rushed to explore the applications for a digital, decentralized ledger beyond currency. But a decade on, the technology has fallen far short of forecasts at the time, dogged by technology limitations and obfuscated use cases.&nbsp;



‚ÄúI do see Blockchain as a cautionary tale,‚Äù says McLarty. ‚ÄúThe hype and ultimate lack of adoption is definitely a path the agentic AI movement should avoid.‚Äù He explains, ‚ÄúThe problem with Blockchain is that people struggle to find use cases where it applies as a solution, and even when they find the use cases, there is often a simpler and cheaper solution,‚Äù he adds. ‚ÄúI think agentic AI can do things no other solution can, in terms of contextual reasoning and dynamic execution. But as technologists, we get so excited about the technology, sometimes we lose sight of the business problem.‚Äù



Instead of diving in headfirst, McLarty advocates for an iterative attitude toward applications of agentic AI, targeting ‚Äúlow-hanging fruit‚Äù and incremental use cases. This includes focusing investment on the worker agents that are set to make up the components of more sophisticated, multi-agent agentic systems further down the road.&nbsp;



However, with a narrower, more prescribed remit, these AI agents with agentic capabilities can add instant value. Enabled with natural language processing (NLP) they can be used to bridge the linguistic shortfalls in current chat agents for example or adaptively carry out rote tasks via dynamic automation.&nbsp;



‚ÄúCurrent rote automation processes generate a lot of value for organizations today, but they can lead to a lot of manual exception processing,‚Äù points out McLarty. ‚ÄúAgentic exception handling agents can eliminate a lot of that.‚Äù&nbsp;



It‚Äôs also essential to avoid use cases for agentic AI that could be addressed with a cheaper and simpler technology. ‚ÄúConfiguring a self-manager, ephemeral agent swarm may sound exciting and be exhilarating to build, but maybe you can just solve the problem with a simple reasoning agent that has access to some in-house contextual data and API-based tools,‚Äù says McLarty. ‚ÄúLet‚Äôs call it the KASS principle: Keep agents simple, stupid.‚Äù



Connecting the dots



The future value of agentic AI will lie in its interoperability and organizations that prioritize this pillar at the earliest phase of their adoption will find themselves ahead of the curve.&nbsp;



As McLarty explains, the usefulness of agentic AI agents in scenarios like customer support chats lies in their combination of four elements: a defined business scope, large language models (LLM), the wider context derived from an organization‚Äôs existing data, and capabilities executed through its core applications. These latter two rely on in-built interoperability. For example, an AI agent tasked with onboarding new employees will require access to updated HR policies, asset catalogs and IT. ‚ÄúOrganizations can get a massive head start on business value through AI agents by having interoperable data and applications to plug and play with agents,‚Äù he says.&nbsp;



Agent-to-agent frameworks like the model context protocol (MCP) ‚Äì an open and standardized plug-and-play that connects AI models to internal (or external) information sources ‚Äì can be layered onto an existing API architecture to embed connectedness from the outset. And while it might feel like an additional hurdle now, in the longer-term those organizations that make this investment early will reap the benefits.&nbsp;



‚ÄúThe icing on the cake for interoperability is that all the work you do to connect agents to data and applications now will help you prepare for the multi-agent future where interoperability between agents will be essential,‚Äù says McLarty.&nbsp;



In this future, multi-agent systems will work collectively on more intricate, cross-functional tasks. Agentic systems will draw on AI agents across inventory, logistics and production to coordinate and optimize supply chain management for example or perform complex assembly tasks.&nbsp;



Conscious that this is where the technology is headed, third-party developers are already beginning to offer multi-agent capability. In December, Amazon launched such a tool for its Bedrock service, providing users access to specialized agents coordinated by a supervisor agent capable of breaking down requests, delegating tasks and consolidating outputs.&nbsp;



But though such an off-the-rack solution has the advantage of allowing enterprises to bypass both the risk and complexity in leveraging such capabilities, the digital heterogeneity of larger organizations in particular will likely mean ‚Äì in the longer-term at least ‚Äì they‚Äôll need to rely on their own API architecture to realize the full potential in multi-agent systems.



McLarty‚Äôs advice is simple, ‚ÄúThis is definitely a time to ground yourself in the business problem, and only go as far as you need to with the solution.‚Äù



This content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review‚Äôs editorial staff.



This content was researched, designed, and written entirely by human writers, editors, analysts, and illustrators. This includes the writing of surveys and collection of data for surveys. AI tools that may have been used were limited to secondary production processes that passed thorough human review.
‚Ä¢ How to run an LLM on your laptop
  MIT Technology Review‚Äôs How To series helps you get things done.&nbsp;



Simon Willison has a plan for the end of the world. It‚Äôs a USB stick, onto which he has loaded a couple of his favorite open-weight LLMs‚Äîmodels that have been shared publicly by their creators and that can, in principle, be downloaded and run with local hardware. If human civilization should ever collapse, Willison plans to use all the knowledge encoded in their billions of parameters for help. ‚ÄúIt‚Äôs like having a weird, condensed, faulty version of Wikipedia, so I can help reboot society with the help of my little USB stick,‚Äù he says.



But you don‚Äôt need to be planning for the end of the world to want to run an LLM on your own device. Willison, who writes a popular blog about local LLMs and software development, has plenty of compatriots: r/LocalLLaMA, a subreddit devoted to running LLMs on your own hardware, has half a million members.



For people who are concerned about privacy, want to break free from the control of the big LLM companies, or just enjoy tinkering, local models offer a compelling alternative to ChatGPT and its web-based peers.



The local LLM world used to have a high barrier to entry: In the early days, it was impossible to run anything useful without investing in pricey GPUs. But researchers have had so much success in shrinking down and speeding up models that anyone with a laptop, or even a smartphone, can now get in on the action. ‚ÄúA couple of years ago, I‚Äôd have said personal computers are not powerful enough to run the good models. You need a $50,000 server rack to run them,‚Äù Willison says. ‚ÄúAnd I kept on being proved wrong time and time again.‚Äù





Why you might want to download your own LLM



Getting into local models takes a bit more effort than, say, navigating to ChatGPT‚Äôs online interface. But the very accessibility of a tool like ChatGPT comes with a cost. ‚ÄúIt‚Äôs the classic adage: If something‚Äôs free, you‚Äôre the product,‚Äù says Elizabeth Seger, the director of digital policy at Demos, a London-based think tank.&nbsp;



OpenAI, which offers both paid and free tiers, trains its models on users‚Äô chats by default. It‚Äôs not too difficult to opt out of this training, and it also used to be possible to remove your chat data from OpenAI‚Äôs systems entirely, until a recent legal decision in the New York Times‚Äô ongoing lawsuit against OpenAI required the company to maintain all user conversations with ChatGPT.



Google, which has access to a wealth of data about its users, also trains its models on both free and paid users‚Äô interactions with Gemini, and the only way to opt out of that training is to set your chat history to delete automatically‚Äîwhich means that you also lose access to your previous conversations. In general, Anthropic does not train its models using user conversations, but it will train on conversations that have been ‚Äúflagged for Trust &amp; Safety review.‚Äù¬†



Training may present particular privacy risks because of the ways that models internalize, and often recapitulate, their training data. Many people trust LLMs with deeply personal conversations‚Äîbut if models are trained on that data, those conversations might not be nearly as private as users think, according to some experts.



‚ÄúSome of your personal stories may be cooked into some of the models, and eventually be spit out in bits and bytes somewhere to other people,‚Äù says Giada Pistilli, principal ethicist at the company Hugging Face, which runs a huge library of freely downloadable LLMs and other AI resources.



For Pistilli, opting for local models as opposed to online chatbots has implications beyond privacy. ‚ÄúTechnology means power,‚Äù she says. ‚ÄúAnd so who[ever] owns the technology also owns the power.‚Äù States, organizations, and even individuals might be motivated to disrupt the concentration of AI power in the hands of just a few companies by running their own local models.



Breaking away from the big AI companies also means having more control over your LLM experience. Online LLMs are constantly shifting under users‚Äô feet: Back in April, ChatGPT suddenly started sucking up to users far more than it had previously, and just last week Grok started calling itself MechaHitler on X.



Providers tweak their models with little warning, and while those tweaks might sometimes improve model performance, they can also cause undesirable behaviors. Local LLMs may have their quirks, but at least they are consistent. The only person who can change your local model is you.



Of course, any model that can fit on a personal computer is going to be less powerful than the premier online offerings from the major AI companies. But there‚Äôs a benefit to working with weaker models‚Äîthey can inoculate you against the more pernicious limitations of their larger peers. Small models may, for example, hallucinate more frequently and more obviously than Claude, GPT, and Gemini, and seeing those hallucinations can help you build up an awareness of how and when the larger models might also lie.



‚ÄúRunning local models is actually a really good exercise for developing that broader intuition for what these things can do,‚Äù Willison says.



How to get started



Local LLMs aren‚Äôt just for proficient coders. If you‚Äôre comfortable using your computer‚Äôs command-line interface, which allows you to browse files and run apps using text prompts, Ollama is a great option. Once you‚Äôve installed the software, you can download and run any of the hundreds of models they offer with a single command.&nbsp;



If you don‚Äôt want to touch anything that even looks like code, you might opt for LM Studio, a user-friendly app that takes a lot of the guesswork out of running local LLMs. You can browse models from Hugging Face from right within the app, which provides plenty of information to help you make the right choice. Some popular and widely used models are tagged as ‚ÄúStaff Picks,‚Äù and every model is labeled according to whether it can be run entirely on your machine‚Äôs speedy GPU, needs to be shared between your GPU and slower CPU, or is too big to fit onto your device at all. Once you‚Äôve chosen a model, you can download it, load it up, and start interacting with it using the app‚Äôs chat interface.



As you experiment with different models, you‚Äôll start to get a feel for what your machine can handle. According to Willison, every billion model parameters require about one GB of RAM to run, and I found that approximation to be accurate: My own 16 GB laptop managed to run Alibaba‚Äôs Qwen3 14B as long as I quit almost every other app. If you run into issues with speed or usability, you can always go smaller‚ÄîI got reasonable responses from Qwen3 8B as well.



And if you go really small, you can even run models on your cell phone. My beat-up iPhone 12 was able to run Meta‚Äôs Llama 3.2 1B using an app called LLM Farm. It‚Äôs not a particularly good model‚Äîit very quickly goes off into bizarre tangents and hallucinates constantly‚Äîbut trying to coax something so chaotic toward usability can be entertaining. If I‚Äôm ever on a plane sans Wi-Fi and desperate for a probably false answer to a trivia question, I now know where to look.



Some of the models that I was able to run on my laptop were effective enough that I can imagine using them in my journalistic work. And while I don‚Äôt think I‚Äôll depend on phone-based models for anything anytime soon, I really did enjoy playing around with them. ‚ÄúI think most people probably don‚Äôt need to do this, and that‚Äôs fine,‚Äù Willison says. ‚ÄúBut for the people who want to do this, it‚Äôs so much fun.‚Äù
‚Ä¢ The Download: three-person babies, and tracking ‚ÄúAI readiness‚Äù in the US
  This is today&#8217;s edition of¬†The Download,¬†our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



Researchers announce babies born from a trial of three-person IVF



Eight babies have been born in the UK thanks to a technology that uses DNA from three people: the two biological parents plus a third person who supplies healthy mitochondrial DNA. The babies were born to mothers who carry genes for mitochondrial diseases and risked passing on severe disorders.¬†



In the team‚Äôs approach, patients‚Äô eggs are fertilized with sperm, and the DNA-containing nuclei of those cells are transferred into donated fertilized eggs that have had their own nuclei removed. The new embryos contain the DNA of the intended parents along with a tiny fraction of mitochondrial DNA from the donor, floating in the embryos‚Äô cytoplasm.The study, which makes use of a technology called mitochondrial donation, has been described as a ‚Äútour de force‚Äù and ‚Äúa remarkable accomplishment‚Äù by others in the field. But not everyone sees the trial as a resounding success. Read the full story.



‚ÄîJessica Hamzelou







These four charts show where AI companies could go next in the US



No one knows exactly how AI will transform our communities, workplaces, and society as a whole. Because it‚Äôs hard to predict the impact AI will have on jobs, many workers and local governments are left trying to read the tea leaves to understand how to prepare and adapt.



A new interactive report released by the Brookings Institution attempts to map how embedded AI companies and jobs are in different regions of the United States in order to prescribe policy treatments to those struggling to keep up. Here are four charts to help understand the issues.



‚ÄîPeter Hall







In defense of air-conditioning



‚ÄîCasey Crownhart



I‚Äôll admit that I‚Äôve rarely hesitated to point an accusing finger at air-conditioning. I‚Äôve outlined in many stories and newsletters that AC is a significant contributor to global electricity demand, and it‚Äôs only going to suck up more power as temperatures rise.



But I‚Äôll also be the first to admit that it can be a life-saving technology, one that may become even more necessary as climate change intensifies. And in the wake of Europe‚Äôs recent deadly heat wave, it‚Äôs been oddly villainized. Read our story to learn more.



This article is from The Spark, MIT Technology Review‚Äôs weekly climate newsletter. To receive it in your inbox every Wednesday, sign up here.







The must-reads



I‚Äôve combed the internet to find you today‚Äôs most fun/important/scary/fascinating stories about technology.



1 Donald Trump is cracking down on ‚Äúdangerous science‚ÄùBut the scientists affected argue their work is essential to developing new treatments. (WP $)+ How MAHA is infiltrating states across the US. (The Atlantic $)



2 The US Senate has approved Trump‚Äôs request to cancel foreign aid¬†The White House is determined to reclaim around $8 billion worth of overseas aid. (NYT $)+ The bill also allocates around $1.1 billion to public broadcasting. (WP $)+ HIV could infect 1,400 infants every day because of US aid disruptions. (MIT Technology Review)3 American air strikes only destroyed one Iranian nuclear site¬†The remaining two sites weren‚Äôt damaged that badly, and could resume operation within months. (NBC News)



4 The US is poised to ban Chinese technology in submarine cablesThe cables are critical to internet connectivity across the world. (FT $)+ The cables are at increasing risk of sabotage. (Bloomberg $)



5 The US measles outbreak is worseningHealth officials‚Äô tactics for attempting to contain it aren‚Äôt working. (Wired $)+ Vaccine hesitancy is growing, too. (The Atlantic $)+ Why childhood vaccines are a public health success story. (MIT Technology Review)



6 A new supercomputer is comingThe Nexus machine will search for new cures for diseases. (Semafor)7 Elon Musk has teased a Grok AI companion inspired by TwilightNo really, you shouldn‚Äôt have‚Ä¶ (The Verge)+ Inside the Wild West of AI companionship. (MIT Technology Review)



8 Future farms could be fully autonomous Featuring AI-powered tractors and drone surveillance. (WSJ $)+ African farmers are using private satellite data to improve crop yields. (MIT Technology Review)



9 Granola is Silicon Valley‚Äôs favorite new toolNo, not the tasty breakfast treat. (The Information $)



10 WeTransfer isn‚Äôt going to train its AI on our files after allAfter customers reacted angrily on social media. (BBC)







Quote of the day



‚ÄúHe‚Äôs doing the exact opposite of everything I voted for.‚Äù



‚ÄîAndrew Schulz, a comedian and podcaster who interviewed Donald Trump last year, explains why he‚Äôs starting to lose faith in the President to Wired.







One more thing







The open-source AI boom is built on Big Tech‚Äôs handouts. How long will it last?In May 2023 a leaked memo reported to have been written by Luke Sernau, a senior engineer at Google, said out loud what many in Silicon Valley must have been whispering for weeks: an open-source free-for-all is threatening Big Tech‚Äôs grip on AI.In many ways, that‚Äôs a good thing. AI won&#8217;t thrive if just a few mega-rich companies get to gatekeep this technology or decide how it is used. But this open-source boom is precarious, and if Big Tech decides to shut up shop, a boomtown could become a backwater. Read the full story.



‚ÄîWill Douglas Heaven







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)+ Happy birthday David Hasselhoff, 73 years young today!+ The trailer for the final season of Stranger Things is here, and things are getting weird.+ Windows 95, you will never be bettered.+ I don‚Äôt know about you, but I‚Äôm ready for a fridge cigarette.
‚Ä¢ In defense of air-conditioning
  I‚Äôll admit that I‚Äôve rarely hesitated to point an accusing finger at air-conditioning. I‚Äôve outlined in many stories and newsletters that AC is a significant contributor to global electricity demand, and it‚Äôs only going to suck up more power as temperatures rise.



But I‚Äôll also be the first to admit that it can be a life-saving technology, one that may become even more necessary as climate change intensifies. And in the wake of Europe‚Äôs recent deadly heat wave, it‚Äôs been oddly villainized.&nbsp;





We should all be aware of the growing electricity toll of air-conditioning, but the AC hate is misplaced. Yes, AC is energy intensive, but so is heating our homes, something that‚Äôs rarely decried in the same way that cooling is. Both are tools for comfort and, more important, for safety. &nbsp;So why is air-conditioning cast as such a villain?



In the last days of June and the first few days of July, temperatures hit record highs across Europe. Over 2,300 deaths during that period were attributed to the heat wave, according to early research from World Weather Attribution, an academic collaboration that studies extreme weather. And human-caused climate change accounted for 1,500 of the deaths, the researchers found. (That is, the number of fatalities would have been under 800 if not for higher temperatures because of climate change.)



We won‚Äôt have the official death toll for months, but these early figures show just how deadly heat waves can be. Europe is especially vulnerable, because in many countries, particularly in the northern part of the continent, air-conditioning is not common.



Popping on a fan, drawing the shades, or opening the windows on the hottest days used to cut it in many European countries. Not anymore. The UK was 1.24 ¬∞C (2.23 ¬∞F) warmer over the past decade than it was between 1961 and 1990, according to the Met Office, the UK‚Äôs national climate and weather service. One recent study found that homes across the country are uncomfortably or dangerously warm much more frequently than they used to be.



The reality is, some parts of the world are seeing an upward shift in temperatures that‚Äôs not just uncomfortable but dangerous. As a result, air-conditioning usage is going up all over the world, including in countries with historically low rates.



The reaction to this long-term trend, especially in the face of the recent heat wave, has been apoplectic. People are decrying AC across social media and opinion pages, arguing that we need to suck it up and deal with being a little bit uncomfortable.



Now, let me preface this by saying that I do live in the US, where roughly 90% of homes are cooled with air-conditioning today. So perhaps I am a little biased in favor of AC. But it baffles me when people talk about air-conditioning this way.





I spent a good amount of my childhood in the southeastern US, where it‚Äôs very obvious that heat can be dangerous. I was used to many days where temperatures were well above 90 ¬∞F (32 ¬∞C), and the humidity was so high your clothes would stick to you as soon as you stepped outdoors.&nbsp;



For some people, being active or working in those conditions can lead to heatstroke. Prolonged exposure, even if it‚Äôs not immediately harmful, can lead to heart and kidney problems. Older people, children, and those with chronic conditions can be more vulnerable.&nbsp;



In other words, air-conditioning is more than a convenience; in certain conditions, it‚Äôs a safety measure. That should be an easy enough concept to grasp. After all, in many parts of the world we expect access to heating in the name of safety. Nobody wants to freeze to death.&nbsp;



And it‚Äôs important to clarify here that while air-conditioning does use a lot of electricity in the US, heating actually has a higher energy footprint.&nbsp;



In the US, about 19% of residential electricity use goes to air-conditioning. That sounds like a lot, and it‚Äôs significantly more than the 12% of electricity that goes to space heating. However, we need to zoom out to get the full picture, because electricity makes up only part of a home‚Äôs total energy demand. A lot of homes in the US use natural gas for heating‚Äîthat‚Äôs not counted in the electricity being used, but it‚Äôs certainly part of the home‚Äôs total energy use.



When we look at the total, space heating accounts for a full 42% of residential energy consumption in the US, while air conditioning accounts for only 9%.



I‚Äôm not letting AC off the hook entirely here. There‚Äôs obviously a difference between running air-conditioning (or other, less energy-intensive technologies) when needed to stay safe and blasting systems at max capacity because you prefer it chilly. And there‚Äôs a lot of grid planning we‚Äôll need to do to make sure we can handle the expected influx of air-conditioning around the globe.&nbsp;



But the world is changing, and temperatures are rising. If you‚Äôre looking for a villain, look beyond the air conditioner and into the atmosphere.



This article is from The Spark, MIT Technology Review‚Äôs weekly climate newsletter. To receive it in your inbox every Wednesday, sign up here.

üîí Cybersecurity & Privacy
‚Ä¢ Poor Passwords Tattle on AI Hiring Bot Maker Paradox.ai
  Security researchers recently revealed that the personal information of millions of people who applied for jobs at McDonald&#8217;s was exposed after they guessed the password (&#8220;123456&#8221;) for the fast food chain&#8217;s account at Paradox.ai, a company that makes artificial intelligence based hiring chatbots used by many Fortune 500 firms. Paradox.ai said the security oversight was an isolated incident that did not affect its other customers, but recent security breaches involving its employees in Vietnam tell a more nuanced story.
A screenshot of the paradox.ai homepage showing its AI hiring chatbot &#8220;Olivia&#8221; interacting with potential hires.
Earlier this month, security researchers Ian Carroll and Sam Curry wrote about simple methods they found to access the backend of the AI chatbot platform on McHire.com, the McDonald&#8217;s website that many of its franchisees use to screen job applicants. As first reported by Wired, the researchers discovered that the weak password used by Paradox exposed 64 million records, including applicants&#8217; names, email addresses and phone numbers.
Paradox.ai acknowledged the researchers&#8217; findings but said the company&#8217;s other client instances were not affected, and that no sensitive information &#8212; such as Social Security numbers &#8212; was exposed.
&#8220;We are confident, based on our records, this test account was not accessed by any third party other than the security researchers,&#8221; the company wrote in a July 9 blog post. &#8220;It had not been logged into since 2019 and frankly, should have been decommissioned. We want to be very clear that while the researchers may have briefly had access to the system containing all chat interactions (NOT job applications), they only viewed and downloaded five chats in total that had candidate information within. Again, at no point was any data leaked online or made public.&#8221;
However, a review of stolen password data gathered by multiple breach-tracking services shows that at the end of June 2025, a Paradox.ai administrator in Vietnam suffered a malware compromise on their device that stole usernames and passwords for a variety of internal and third-party online services. The results were not pretty.
The password data from the Paradox.ai developer was stolen by a malware strain known as &#8220;Nexus Stealer,&#8221; a form grabber and password stealer that is sold on cybercrime forums. The information snarfed by stealers like Nexus is often recovered and indexed by data leak aggregator services like Intelligence X, which reports that the malware on the Paradox.ai developer&#8217;s device exposed hundreds of mostly poor and recycled passwords (using the same base password but slightly different characters at the end).
Those purloined credentials show the developer in question at one point used the same seven-digit password to log in to Paradox.ai accounts for a number of Fortune 500 firms listed as customers on the company&#8217;s website, including Aramark, Lockheed Martin, Lowes, and Pepsi.
Seven-character passwords, particularly those consisting entirely of numerals, are highly vulnerable to &#8220;brute-force&#8221; attacks that can try a large number of possible password combinations in quick succession. According to a much-referenced password strength guide maintained by Hive Systems, modern password-cracking systems can work out a seven number password more or less instantly.
Image: hivesystems.com.
In response to questions from KrebsOnSecurity, Paradox.ai confirmed that the password data was recently stolen by a malware infection on the personal device of a longtime Paradox developer based in Vietnam, and said the company was made aware of the compromise shortly after it happened. Paradox maintains that few of the exposed passwords were still valid, and that a majority of them were present on the employee&#8217;s personal device only because he had migrated the contents of a password manager from an old computer.
Paradox also pointed out that it has been requiring single sign-on (SSO) authentication since 2020 that enforces multi-factor authentication for its partners. Still, a review of the exposed passwords shows they included the Vietnamese administrator&#8217;s credentials to the company&#8217;s SSO platform &#8212; paradoxai.okta.com. The password for that account ended in 202506 &#8212; possibly a reference to the month of June 2025 &#8212; and the digital cookie left behind after a successful Okta login with those credentials says it was valid until December 2025.
Also exposed were the administrator&#8217;s credentials and authentication cookies for an account at Atlassian, a platform made for software development and project management. The expiration date for that authentication token likewise was December 2025.
Infostealer infections are among the leading causes of data breaches and ransomware attacks today, and they result in the theft of stored passwords and any credentials the victim types into a browser. Most infostealer malware also will siphon authentication cookies stored on the victim&#8217;s device, and depending on how those tokens are configured thieves may be able to use them to bypass login prompts and/or multi-factor authentication.
Quite often these infostealer infections will open a backdoor on the victim&#8217;s device that allows attackers to access the infected machine remotely. Indeed, it appears that remote access to the Paradox administrator&#8217;s compromised device was offered for sale recently.
In February 2019, Paradox.ai announced it had successfully completed audits for two fairly comprehensive security standards (ISO 27001 and SOC 2 Type II). Meanwhile, the company&#8217;s security disclosure this month says the test account with the atrocious 123456 username and password was last accessed in 2019, but somehow missed in their annual penetration tests. So how did it manage to pass such stringent security audits with these practices in place?
Paradox.ai told KrebsOnSecurity that at the time of the 2019 audit, the company&#8217;s various contractors were not held to the same security standards the company practices internally. Paradox emphasized that this has changed, and that it has updated its security and password requirements multiple times since then.
It is unclear how the Paradox developer in Vietnam infected his computer with malware, but a closer review finds a Windows device for another Paradox.ai employee from Vietnam was compromised by similar data-stealing malware at the end of 2024 (that compromise included the victim&#8217;s GitHub credentials). In the case of both employees, the stolen credential data includes Web browser logs that indicate the victims repeatedly downloaded pirated movies and television shows, which are often bundled with malware disguised as a video codec needed to view the pirated content.
‚Ä¢ DOGE Denizen Marko Elez Leaked API Key for xAI
  Marko Elez, a 25-year-old employee at Elon Musk&#8217;s Department of Government Efficiency (DOGE), has been granted access to sensitive databases at the U.S. Social Security Administration, the Treasury and Justice departments, and the Department of Homeland Security. So it should fill all Americans with a deep sense of confidence to learn that Mr. Elez over the weekend inadvertently published a private key that allowed anyone to interact directly with more than four dozen large language models (LLMs) developed by Musk&#8217;s artificial intelligence company xAI.
Image: Shutterstock, @sdx15.
On July 13, Mr. Elez committed a code script to GitHub called &#8220;agent.py&#8221; that included a private application programming interface (API) key for xAI. The inclusion of the private key was first flagged by GitGuardian, a company that specializes in detecting and remediating exposed secrets in public and proprietary environments. GitGuardian‚Äôs systems constantly scan GitHub and other code repositories for exposed API keys, and fire off automated alerts to affected users.
Philippe Caturegli, ‚Äúchief hacking officer‚Äù at the security consultancy Seralys,¬†said the exposed API key allowed access to at least 52 different LLMs used by xAI. The most recent LLM in the list was called &#8220;grok-4-0709&#8221; and was created on July 9, 2025.
Grok, the generative AI chatbot developed by xAI and integrated into Twitter/X, relies on these and other LLMs (a query to Grok before publication shows Grok currently uses Grok-3, which was launched in Feburary 2025). Earlier today, xAI announced that the Department of Defense will begin using Grok as part of a contract worth up to $200 million. The contract award came less than a week after Grok began spewing antisemitic rants and invoking Adolf Hitler.
Mr. Elez did not respond to a request for comment. The code repository containing the private xAI key was removed shortly after Caturegli notified Elez via email. However, Caturegli said the exposed API key still works and has not yet been revoked.
&#8220;If a developer can&#8217;t keep an API key private, it raises questions about how they&#8217;re handling far more sensitive government information behind closed doors,&#8221; Caturegli told KrebsOnSecurity.
Prior to joining DOGE, Marko Elez worked for a number of Musk&#8217;s companies. His DOGE career began at the Department of the Treasury, and a legal battle over DOGE&#8217;s access to Treasury databases showed Elez was sending unencrypted personal information in violation of the agency&#8217;s policies.
While still at Treasury, Elez resigned after The Wall Street Journal linked him to social media posts that advocated racism and eugenics. When Vice President J.D. Vance lobbied for Elez to be rehired, President Trump agreed and Musk reinstated him.
Since his re-hiring as a DOGE employee, Elez has been granted access to databases at one federal agency after another. TechCrunch reported in February 2025 that he was working at the Social Security Administration. In March, Business Insider found Elez was part of a DOGE detachment assigned to the Department of Labor.
Marko Elez, in a photo from a social media profile.
In April, The New York Times reported that Elez held positions at the U.S. Customs and Border Protection and the Immigration and Customs Enforcement (ICE) bureaus, as well as the Department of Homeland Security. The Washington Post later reported that Elez, while serving as a DOGE advisor at the Department of Justice, had gained access to the Executive Office for Immigration Review&#8217;s Courts and Appeals System (EACS).
Elez is not the first DOGE worker to publish internal API keys for xAI: In May, KrebsOnSecurity detailed how another DOGE employee leaked a private xAI key on GitHub for two months, exposing LLMs that were custom made for working with internal data from Musk&#8217;s companies, including SpaceX, Tesla and Twitter/X.
Caturegli said it&#8217;s difficult to trust someone with access to confidential government systems when they can&#8217;t even manage the basics of operational security.
&#8220;One leak is a mistake,&#8221; he said. &#8220;But when the same type of sensitive key gets exposed again and again, it‚Äôs not just bad luck, it‚Äôs a sign of deeper negligence and a broken security culture.&#8221;

üéì University AI
No updates.

üè¢ Corporate AI
‚Ä¢ CollabLLM: Teaching LLMs to collaborate with users
  Large language models (LLMs) can solve complex puzzles in seconds, yet they sometimes struggle over simple conversations. When these AI tools make assumptions, overlook key details, or neglect to ask clarifying questions, the result can erode trust and derail real-world interactions, where nuance is everything.



A key reason these models behave this way lies in how they‚Äôre trained and evaluated. Most benchmarks use isolated, single-turn prompts with clear instructions. Training methods tend to optimize for the model&#8217;s next response, not its contribution to a successful, multi-turn exchange. But real-world interaction is dynamic and collaborative. It relies on context, clarification, and shared understanding.



User-centric approach to training&nbsp;



To address this, we‚Äôre exploring ways to train LLMs with users in mind. Our approach places models in simulated environments that reflect the back-and-forth nature of real conversations. Through reinforcement learning, these models improve through trial and error, for example, learning when to ask questions and how to adapt tone and communication style to different situations. This user-centric approach helps bridge the gap between how LLMs are typically trained and how people actually use them.¬†¬†



This is the concept behind CollabLLM (opens in new tab), recipient of an ICML (opens in new tab) Outstanding Paper Award (opens in new tab). This training framework helps LLMs improve through simulated multi-turn interactions, as illustrated in Figure 1. The core insight behind CollabLLM is simple: in a constructive collaboration, the value of a response isn‚Äôt just in its immediate usefulness, but in how it contributes to the overall success of the conversation. A clarifying question might seem like a delay but often leads to better outcomes. A quick answer might appear useful but can create confusion or derail the interaction.



Figure 1. Diagram comparing two training approaches for LLMs. (a) The standard method lacks user-agent collaboration and uses single-turn rewards, leading to an inefficient conversation. (b) In contrast, CollabLLM simulates multi-turn user-agent interactions during training, enabling it to learn effective collaboration strategies and produce more efficient dialogues.



CollabLLM puts this collaborative approach into practice with a simulation-based training loop, illustrated in Figure 2. At any point in a conversation, the model generates multiple possible next turns by engaging in a dialogue with a simulated user.



Figure 2: Simulation-based training process used in CollabLLM



The system uses a sampling method to extend conversations turn by turn, choosing likely responses for each participant (the AI agent or the simulated user), while adding some randomness to vary the conversational paths. The goal is to expose the model to a wide variety of conversational scenarios, helping it learn more effective collaboration strategies.



	
		

		
		Spotlight: Microsoft research newsletter
	
	
	
						
				
					
				
			
			
			

									Microsoft Research Newsletter
				
								Stay connected to the research community at Microsoft.
				
								
					
						
							Subscribe today						
					
				
							
	
Opens in a new tab	
	


To each simulated conversation, we applied multiturn-aware reward (MR) functions, which assess how the model‚Äôs response at the given turn influences the entire trajectory of the conversation. We sampled multiple conversational follow-ups from the model, such as statements, suggestions, questions, and used MR to assign a reward to each based on how well the conversation performed in later turns. We based these scores on automated metrics that reflect key factors like goal completion, conversational efficiency, and user engagement.



To score the sampled conversations, we used task-specific metrics and metrics from an LLM-as-a-judge framework, which supports efficient and scalable evaluation. For metrics like engagement, a judge model rates each sampled conversation on a scale from 0 to 1.



The MR of each model response was computed by averaging the scores from the sampled conversations, originating from the model response. Based on the score, the model updates its parameters using established reinforcement learning algorithms like Proximal Policy Optimization (PPO) or Direct Preference Optimization (DPO).



We tested CollabLLM through a combination of automated and human evaluations, detailed in the paper. One highlight is a user study involving 201 participants in a document co-creation task, shown in Figure 3. We compared CollabLLM to a baseline trained with single-turn rewards and to a second, more proactive baseline prompted to ask clarifying questions and take other proactive steps. CollabLLM outperformed both, producing higher-quality documents, better interaction ratings, and faster task completion times.



Figure 3: Results of the user study in a document co-creation task comparing CollabLLM to a baseline trained with single-turn rewards.



Designing for real-world collaboration



Much of today‚Äôs AI research focuses on fully automated tasks, models working without input from or interaction with users. But many real-world applications depend on people in the loop: as users, collaborators, or decision-makers. Designing AI systems that treat user input not as a constraint, but as essential, leads to systems that are more accurate, more helpful, and ultimately more trustworthy.



This work is driven by a core belief: the future of AI depends not just on intelligence, but on the ability to collaborate effectively. And that means confronting the communication breakdowns in today‚Äôs systems.



We see CollabLLM as a step in that direction, training models to engage in meaningful multi-turn interactions, ask clarifying questions, and adapt to context. In doing so, we can build systems designed to work with people‚Äînot around them.
Opens in a new tabThe post CollabLLM: Teaching LLMs to collaborate with users appeared first on Microsoft Research.
‚Ä¢ AI Testing and Evaluation: Learnings from cybersecurity
  Generative AI presents a unique challenge and opportunity to reexamine governance practices for the responsible development, deployment, and use of AI. To advance thinking in this space, Microsoft has tapped into the experience and knowledge of experts across domains‚Äîfrom genome editing to cybersecurity‚Äîto investigate the role of testing and evaluation as a governance tool.&nbsp;AI Testing and Evaluation: Learnings from Science and Industry,&nbsp;hosted by Microsoft Research‚Äôs&nbsp;Kathleen Sullivan, explores what the technology industry and policymakers can learn from these fields and how that might help shape the course of AI development.



In this episode, Sullivan speaks with Professor Ciaran Martin (opens in new tab) of the University of Oxford about risk assessment and testing in the field of cybersecurity. They explore the importance of differentiated standards for organizations of varying sizes, the role of public-private partnerships, and the opportunity to embed security into emerging technologies from the outset. Later, Tori Westerhoff (opens in new tab), a principal director on the Microsoft AI Red Team, joins Sullivan to talk about identifying vulnerabilities in AI products and services. Westerhoff describes AI security in terms she‚Äôs heard cybersecurity professionals use for their work‚Äîa team sport‚Äîand points to cybersecurity‚Äôs establishment of a shared language and understanding of risk as a model for AI security.








Learn more:




Introducing AI Red Teaming Agent: Accelerate your AI safety and security journey with Azure AI Foundry (opens in new tab)Azure AI Foundry Blog | April 2025



Lessons From Red Teaming 100 Generative AI ProductsPublication | January 2025



Learning from other domains to advance AI evaluation and testingMicrosoft Research Blog | June 2025



Responsible AI: Ethical policies and practices | Microsoft AI



AI and Microsoft Research










	
		
			Subscribe to the Microsoft Research Podcast:		
		
							
					
						  
						Apple Podcasts
					
				
			
							
					
						
						Email
					
				
			
							
					
						
						Android
					
				
			
							
					
						
						Spotify
					
				
			
							
					
						
						RSS Feed
					
				
					
	




	
		
			
				
					

Transcript



[MUSIC]



KATHLEEN SULLIVAN: Welcome to AI Testing and Evaluation: Learnings from Science and Industry. I&#8217;m your host, Kathleen Sullivan.



As generative AI continues to advance, Microsoft has gathered a range of experts‚Äîfrom genome editing to cybersecurity‚Äîto share how their fields approach evaluation and risk assessment. Our goal is to learn from their successes and their stumbles to move the science and practice of AI testing forward. In this series, we&#8217;ll explore how these insights might help guide the future of AI development, deployment, and responsible use.



[MUSIC ENDS]



Today, I&#8217;m excited to welcome Ciaran Martin to the podcast to explore testing and risk assessment in cybersecurity. Ciaran is a professor of practice in the management of public organizations at the University of Oxford. He had previously founded and served as chief executive of the National Cyber Security Centre within the UK&#8217;s intelligence, security, and cyber agency.



And after our conversation, we&#8217;ll talk to Microsoft&#8217;s Tori Westerhoff, a principal director on Microsoft‚Äôs AI Red Team, about how we should think about these insights in the context of AI.



Hi, Ciaran. Thank you so much for being here today.



				
				
					



CIARAN MARTIN: Well, thanks so much for inviting me. It‚Äôs great to be here.



SULLIVAN: Ciaran, before we get into some regulatory specifics, it&#8217;d be great to hear a little bit more about your origin story, and just take us to that day‚Äîwho tapped you on the shoulder and said, ‚ÄúCiaran, we need you to run a national cyber center! Do you fancy building one?‚Äù



MARTIN: You could argue that I owe my job to Edward Snowden. Not an obvious thing to say. So the National Cyber Security Centre, which didn&#8217;t exist at the time‚ÄîI was invited to join the British government&#8217;s cybersecurity effort in a leadership role‚Äîis now a subset of GCHQ. That&#8217;s the digital intelligence agency. The equivalent in the US obviously is the NSA [National Security Agency]. It had been convulsed by the Snowden disclosures. It was an unprecedented challenge.



I was a 17-year career government fixer with some national security experience. So I was asked to go out and help with the policy response, the media response, the legal response. But I said, look, any crisis, even one as big as this, is over one way or the other in six months. What should I do long term? And they said, well, we were thinking of asking you to try to help transform our cybersecurity mission. So the National Cyber Security Centre was born, and I was very proud to lead it, and all in all, I did it for seven years from startup to handing it on to somebody else.



SULLIVAN: I mean, it&#8217;s incredible. And just building on that, people spend a significant portion of their lives online now with a variety of devices, and maybe for listeners who are newer to cybersecurity, could you give us the 90-second lightning talk? Kind of, what does risk assessment and testing look like in this space?



MARTIN: Well, risk assessment and testing, I think, are two different things. You can&#8217;t defend everything. If you defend everything, you&#8217;re defending nothing. So broadly speaking, organizations face three threats. One is complete disruption of their systems. So just imagine not being able to access your system. The second is data protection, and that could be sensitive customer information. It could be intellectual property. And the third is, of course, you could be at risk of just straightforward being stolen from. I mean, you don&#8217;t want any of them to happen, but you have to have a hierarchy of harm.



SULLIVAN: Yes.



MARTIN: So that&#8217;s your risk assessment.



The testing side, I think, is slightly different. One of the paradoxes, I think, of cybersecurity is for such a scientific, data-rich subject, the sort of metrics about what works are very, very hard to come by. So you&#8217;ve got boards and corporate leadership and senior governmental structures, and they say, ‚ÄúLook, how do I run this organization safely and securely?‚Äù And a cybersecurity chief within the organization will say, ‚ÄúWell, we could get this capability in.‚Äù Well, the classic question for a leadership team to ask is, well, what risk and harm will this reduce, by how much, and what&#8217;s the cost-benefit analysis? And we find that really hard.



So that&#8217;s really where testing and assurance comes in. And also as technology changes so fast, we have to figure out, well, if we&#8217;re worried about post-quantum cryptography, for example, what standards does it have to meet? How do you assess whether it&#8217;s meeting those standards? So it&#8217;s a huge issue in cybersecurity and one that we&#8217;re always very conscious of. It‚Äôs really hard.



SULLIVAN: Given the scope of cybersecurity, are there any differences in testing, let&#8217;s say, for maybe a small business versus a critical infrastructure operator? Are there any, sort of, metrics we can look at in terms of distinguishing risk or assessment?



MARTIN: There have to be. One of the reasons I think why we have to be is that no small business can be expected to take on a hostile nation-state that&#8217;s well equipped. You have to be realistic.



If you look at government guidance, certainly in the UK 15 years ago on cybersecurity, you were telling small businesses that are living hand to mouth, week by week, trying to make payments at the end of each month, we were telling them they needed sort of nation-state-level cyber defenses. That was never going to happen, even if they could afford it, which they couldn&#8217;t. So you have to have some differentiation. So again, you&#8217;ve got assessment frameworks and so forth where you have to meet higher standards. So there absolutely has to be that distinction. Otherwise, you end up in a crazy world of crippling small businesses with just unmanageable requirements which they&#8217;re never going to meet.



SULLIVAN: It&#8217;s such a great point. You touched on this a little bit earlier, as well, but just cybersecurity governance operates in a fast-moving technology and threat environment. How have testing standards evolved, and where do new technical standards usually originate?



MARTIN: I keep saying this is very difficult, and it is. [LAUGHTER] So I think there are two challenges. One is actually about the balance, and this applies to the technology of today as well as the technology of tomorrow. This is about, how do you make sure things are good enough without crowding out new entrants? You want people to be innovative and dynamic. You want disruptors in this business.



But if you say to them, ‚ÄúLook, well, you have to meet these 14 impossibly high technical standards before you can even sell to anybody or sell to the government,‚Äù whatever, then you&#8217;ve got a problem. And I think we&#8217;ve wrestled with that, and there&#8217;s no perfect answer. You just have to try and go to ‚Ä¶ find the sweet spot between two ends of a spectrum. And that&#8217;s going to evolve.



The second point, which in some respects if you&#8217;ve got the right capabilities is slightly easier but still a big call, is around, you know, those newer and evolving technologies. And here, having, you know, been a bit sort of gloomy and pessimistic, here I think is actually an opportunity. So one of the things we always say in cybersecurity is that the internet was built and developed without security in mind. And that was kind of true in the ‚Äô90s and the noughties, as we call them over here.



But I think as you move into things like post-quantum computing, applied use of AI, and so on, you can actually set the standards at the beginning. And that&#8217;s really good because it&#8217;s saying to people that these are the things that are going to matter in the post-quantum age. Here&#8217;s the outline of the standards you&#8217;re going to have to meet; start looking at them. So there&#8217;s an opportunity actually to make technology safer by design, by getting ahead of it. And I think that&#8217;s the era we&#8217;re in now.



SULLIVAN: That makes a lot of sense. Just building on that, do businesses and the public trust these standards? And I guess, which standard do you wish the world would just adopt already, and what&#8217;s the real reason they haven&#8217;t?



MARTIN: Well, again, where do you start? I mean, most members of the public quite rightly haven&#8217;t heard of any of these standards. I think public trust and public capital in any society matters. But I think it is important that these things are credible.



And there&#8217;s quite a lot of convergence between, you know, the top-level frameworks. And obviously in the US, you know, the NIST [National Institute of Standards and Technology] framework is the one that&#8217;s most popular for cybersecurity, but it bears quite a strong resemblance to the international one, ISO[/IEC] 27001, and there are others, as well. But fundamentally, they boil down to kind of five things. Do a risk assessment; work out what your crown jewels are. Protect your perimeter as best you can. Those are the first two.



The third one then is when your perimeter&#8217;s breached, be able to detect it more times than not. And when you can&#8217;t do that, you go to the fourth one, which is, can you mitigate it? And when all else fails, how quickly can you recover and manage it? I mean, all the standards are expressed in way more technical language than that, but fundamentally, if everybody adopted those five things and operated them in a simple way, you wouldn&#8217;t eliminate the harm, but you would reduce it quite substantially.



SULLIVAN: Which policy initiatives are most promising for incentivizing companies to undertake, you know, these cybersecurity testing parameters that you‚Äôve just outlined? Governments, including the UK, have used carrots and sticks, but what do you think will actually move the needle?



MARTIN: I think there are two answers to that, and it comes back to your split between smaller businesses and critically important businesses. In the critically important services, I think it&#8217;s easier because most industries are looking for a level playing field. In other words, they realize there have to be rules and they want to apply them to everyone.



We had a fascinating experience when I was in government back in around 2018 where the telecom sector, they came to us and they said, we&#8217;ve got a very good cooperative relationship with the British government, but it needs to be put on a proper legal footing because you&#8217;re just asking us nicely to do expensive things. And in a regulated sector, if you actually put in some rules‚Äîand please develop them jointly with us; that&#8217;s the crucial part‚Äîthen that will help because it means that we&#8217;re not going to our boards and saying, or our shareholders, and saying that we should do this, and they&#8217;re saying, ‚ÄúWell, do you have to do it? Are our competitors doing it?‚Äù And if the answer to that is, yes, we have to, and, yes, our competitors are doing it, then it tends to be OK.



The harder nut to crack is the smaller business. And I think there&#8217;s a real mystery here: why has nobody cracked a really good and easy solution for small business? We need to be careful about this because, you know, you can&#8217;t throttle small businesses with onerous regulation. At the same time, we&#8217;re not brilliant, I think, in any part of the world at using the normal corporate governance rules to try and get people to figure out how to do cybersecurity.



There are initiatives there that are not the sort of pretty heavy stick that you might have to take to a critical function, but they could help. But that is a hard nut to crack. And I look around the world, and, you know, I think if this was easy, somebody would have figured it out by now. I think most of the developed economies around the world really struggle with cybersecurity for smaller businesses.



SULLIVAN: Yeah, it&#8217;s a great point. Actually building on one of the comments you made on the role of, kind of, government, how do you see the role of private-public partnerships scaling and strengthening, you know, robust cybersecurity testing?



MARTIN: I think they&#8217;re crucial, but they have to be practical. I&#8217;ve got a slight, sort of, high horse on this, if you don&#8217;t mind, Kathleen. It&#8217;s sort of ‚Ä¶ [LAUGHS]



SULLIVAN: Of course.



MARTIN: I think that there are two types of public-private partnership. One involves committees saying that we should strengthen partnerships and we should all work together and collaborate and share stuff. And we tried that for a very long time, and it didn&#8217;t get us very far. There are other types.



We had some at the National Cyber Security Centre where we paid companies to do spectacularly good technical work that the market wouldn&#8217;t provide. So I think it&#8217;s sort of partnership with a purpose. I think sometimes, and I understand the human instinct to do this, particularly in governments and big business, they think you need to get around a table and work out some grand strategy to fix everything, and the scale of the ‚Ä¶ not just the problem but the scale of the whole technology is just too big to do that.



So pick a bit of the problem. Find some ways of doing it. Don&#8217;t over-lawyer it. [LAUGHTER] I think sometimes people get very nervous. Oh, well, is this our role? You know, should we be doing this, that, and the other? Well, you know, sometimes certainly in this country, you think, well, who&#8217;s actually going to sue you over this, you know? So I wouldn&#8217;t over-programmatize it. Just get stuck practically into solving some problems.



SULLIVAN: I love that. Actually, [it] made me think, are there any surprising allies that you&#8217;ve gained‚Äîyou know, maybe someone who you never expected to be a cybersecurity champion‚Äîthrough your work?



MARTIN: Ooh! That&#8217;s a ‚Ä¶ that&#8217;s a‚Ä¶ what a question! To give you a slightly disappointing answer, but it relates to your previous question. In the early part of my career, I was working in institutions like the UK Treasury long before I was in cybersecurity, and the treasury and the British civil service in general, but the treasury in particular sort of trained you to believe that the private sector was amoral, not immoral, amoral. It just didn&#8217;t have values. It just had bottom line, and, you know, its job essentially was to provide employment and revenue then for the government to spend on good things that people cared about. And when I got into cybersecurity and people said, look, you need to develop relations with this cybersecurity company, often in the US, actually. I thought, well, what&#8217;s in it for them?



And, sure, sometimes you were paying them for specific services, but other times, there was a real public spiritedness about this. There was a realization that if you tried to delineate public-private boundaries, that it wouldn&#8217;t really work. It was a shared risk. And you could analyze where the boundaries fell or you could actually go on and do something about it together. So I was genuinely surprised at the allyship from the cybersecurity sector. Absolutely, I really, really was. And I think it&#8217;s a really positive part of certainly the UK cybersecurity ecosystem.



SULLIVAN: Wonderful. Well, we&#8217;re coming to the end of our time here, but is there any maybe last thoughts or perhaps requests you have for our listeners today?



MARTIN: I think that standards, assurance, and testing really matter, but it&#8217;s a bit like the discussion we&#8217;re having over AI. Get all these things to take you 80, 90% of the way and then really apply your judgment. There&#8217;s been some bad regulation under the auspices of standards and assurance. First of all, it‚Äôs, have you done this assessment? Have you done that? Have you looked at this? Well, fine. And you can tick that box, but what does it actually mean when you do it? What bits that you know in your heart of hearts are really important to the defense of your organization that may not be covered by this and just go and do those anyway. Because sure it helps, but it&#8217;s not everything.



SULLIVAN: No. Great, great closing sentiment. Well, Ciaran, thank you for joining us today. This has been just a super fun conversation and really insightful. Just really enjoyed the conversation. Thank you.



MARTIN: My pleasure, Kathleen, thank you.



[TRANSITION MUSIC]



SULLIVAN: Now, I&#8217;m happy to introduce Tori Westerhoff. As a principal director on the Microsoft AI Red Team, Tori leads all AI security and safety red team operations, as well as dangerous capability testing, to directly inform C-suite decision-makers.



So, Tori, welcome!



TORI WESTERHOFF: Thanks. I am so excited to be here.



SULLIVAN: I&#8217;d love to just start a little bit more learning about your background. You&#8217;ve worn some very intriguing hats. I mean, cognitive neuroscience grad from Yale, national security consultant, strategist in augmented and virtual reality ‚Ä¶ how do those experiences help shape the way you lead the Microsoft AI Red Team?



WESTERHOFF: I always joke this is the only role I think will always combine the entire patchwork LinkedIn r√©sum√©. [LAUGHS]



I think I use those experiences to help me understand the really broad approach that AI Red Team‚Äîartist also known as AIRT; I&#8217;m sure I&#8217;ll slip into our acronym‚Äîhow we frame up the broad security implications of AI. So I think the cognitive neuroscience element really helped me initially approach AI hacking, right. There&#8217;s a lot of social engineering and manipulation within chat interfaces that are enabled by AI. And also, kind of, this, like, metaphor for understanding how to find soft spots in the way that you see human heuristics show up, too. And so I think that was actually my personal ‚Äúin‚Äù to getting hooked into AI red teaming generally.



But my experience in national security and I&#8217;d also say working through the AR/VR/metaverse space at the time where I was in it helped me balance both how our impact is framed, how we&#8217;re thinking about critical industries, how we&#8217;re really trying to push our understanding of where security of AI can help people the most. And also do it in a really breakneck speed in an industry that&#8217;s evolving all of the time, that&#8217;s really pushing you to always be at the bleeding edge of your understanding. So I draw a lot of the energy and the mission criticality and the speed from those experiences as we&#8217;re shaping up how we approach it.



SULLIVAN: Can you just give us a quick rundown? What does the Red Team do? What actually, kind of, is involved on a day-to-day basis? And then as we think about, you know, our engagements with large enterprises and companies, how do we work alongside some of those companies in terms of testing?



WESTERHOFF: The way I see our team is almost like an indicator light that works really part and parcel with product development. So the way we&#8217;ve organized our expert red teaming efforts is that we work with product development before anything ships out to anyone who can use it. And our job is to act as expert AI manipulators, AI hackers. And we are supposed to take the theories and methods and new research and harness it to find examples of vulnerabilities or soft spots in products to enable product teams to harden those soft spots before anything actually reaches someone who wants to use it.



So if we&#8217;re the indicator light, we are also not the full workup, right. I see that as measurement and evals. And we also are not the mechanic, which is that product development team that&#8217;s creating mitigations. It&#8217;s platform-security folks who are creating mitigations at scale. And there&#8217;s a really great throughput of insights from those groups back into our area where we love to inform about them, but we also love to add on to, how do we break the next thing, right? So it&#8217;s a continuous cycle.



And part of that is just being really creative and thinking outside of a traditional cybersecurity box. And part of that is also really thinking about how we pull in research‚Äîwe have a research function within our AI Red Team‚Äîand how we automate and scale. This year, we&#8217;ve pulled a lot of those assets and insights into the Azure [AI] Foundry AI Red Teaming Agent (opens in new tab). And so folks can now access a lot of our mechanisms through that. So you can get a little taste of what we do day to day in the AI Red Teaming Agent.



SULLIVAN: You recently‚Äîactually, with your team‚Äîpublished a report that outlined lessons from testing over a hundred generative AI products. But could you share a bit about what you learned? What were some of the important lessons? Where do you see opportunities to improve the state of red teaming as a method for probing AI safety?



WESTERHOFF: I think the most important takeaway from those lessons is that AI security is truly a team sport. You&#8217;ll hear cybersecurity folks say that a lot. And part of the rationale there is that the defense in depth and integrating and a view towards AI security through the entire development of AI systems is really the way that we&#8217;re going to approach this with intentionality and responsibility.



So in our space, we really focus on novel harm categories. We are pushing bleeding edge, and we also are pushing iterative and, like, contextually based red teaming in product dev. So outside of those hundred that we&#8217;ve done, there&#8217;s a community [LAUGHS] through the entire, again, multistage life cycle of a product that is really trying to push the cost of attacking those AI systems higher and higher with all of the expertise they bring. So we may be, like, the experts in AI hacking in that line, but there are also so many partners in the Microsoft ecosystem who are thinking about their market context or they really, really know the people who love their products. How are they using it?



And then when you bubble out, you also have industry and government who are working together to push towards the most secure AI implementation for people, right? And I think our team in particular, we feel really grateful to be part of the big AI safety and security ecosystem at Microsoft and also to be able to contribute to the industry writ large. 



SULLIVAN: As you know, we had a chance to speak with Professor Ciaran Martin from the University of Oxford about the cybersecurity industry and governance there. What are some of the ideas and tools from that space that are surfacing in how we think about approaching red teaming and AI governance broadly?



WESTERHOFF: Yeah, I think it&#8217;s such a broad set of perspectives to bring in, in the AI instance. Something that I&#8217;ve noticed interjecting into security at the AI junction, right, is that cybersecurity has so many decades of experience of working through how to build trustworthy computing, for example, or bring an entire industry to bear in that way. And I think that AI security and safety can learn a lot of lessons of how to bring clarity and transparency across the industry to push universal understanding of where the threats really are.



So frameworks coming out of NIST, coming out of MITRE that help us have a universal language that inform governance, I think, are really important because it brings clarity irrespective of where you are looking into AI security, irrespective of your company size, what you&#8217;re working on. It means you all understand, ‚ÄúHey, we are really worried about this fundamental impact.‚Äù And I think cybersecurity has done a really good job of driving towards impact as their organizational vector. And I am starting to see that in the AI space, too, where we&#8217;re trying to really clarify terms and threats.&nbsp;And you see it in updates of those frameworks, as well, that I really love.



So I think that the innovation is in transparency to folks who are really innovating and doing the work so we all have a shared language, and from that, it really creates communal goals across security instead of a lot of people being worried about the same thing and talking about it in a different way.



SULLIVAN: Mm-hmm. In the cybersecurity context, Ciaran really stressed matching risk frameworks to an organization&#8217;s role and scale. Microsoft plays many roles, including building models and shipping applications. How does your red teaming approach shift across those layers?&nbsp;



WESTERHOFF: I love this question also because I love it as part of our work. So one of the most fascinating things about working on this team has been the diversity of the technology that we end up red teaming and testing. And it feels like we&#8217;re in the crucible in that way. Because we see AI applied to so many different architectures, tech stacks, individual features, models, you name it.



Part of my answer is that we still care about the highest-impact things. And so irrespective of the iteration, which is really fascinating and I love, I still think that our team drives to say, ‚ÄúOK, what is that critical vulnerability that is going to affect people in the largest ways, and can we battle test to see if that can occur?‚Äù



So in some ways, the task is always the same. I think in the ways that we change our testing, we customize a lot to the access to systems and data and also people&#8217;s trust almost as different variables that could affect the impact, right.



So a good example is if we&#8217;re thinking through agentic frameworks that have access to functions and tools and preferential ability to act on data, it&#8217;s really different to spaces where that action may not be feasible, right. And so I think the tailoring of the way to get to that impact is hyper-custom every time we start an engagement. And part of it is very thesis driven and almost mechanizing empathy.



You almost need to really focus on how people could use, or misuse, in such a way that you can emulate it before to a really great signal to product development, to say this is truly what people could do and we want to deliver the highest-impact scenarios so you can solve for those and also solve the underlying patterns, actually, that could contribute to maybe that one piece of evidence but also all the related pieces of evidence. So singular drive but like hyper-, hyper-customization to what that piece of tech could do and has access to.



SULLIVAN: What are some of the unexplored testing approaches or considerations from cybersecurity that you think we should encourage AI technologists, policymakers, and other stakeholders to focus on? 



WESTERHOFF: I do love that AI humbles us each and every day with new capabilities and the potential for new capabilities. It&#8217;s not just saying, ‚ÄúHey, there&#8217;s one test that we want to try,‚Äù but more, ‚ÄúHey, can we create a methodology that we feel really, really solid about so that when we are asked a question we haven&#8217;t even thought of, we feel confident that we have the resources and the system?‚Äù



So part of me is really intrigued by the process that we&#8217;re asked to make without knowing what those capabilities are really going to bring. And then I think tactically, AIRT is really pushing on how we create new research methodologies. How are we investing in, kind of, these longer-term iterations of red teaming? So we&#8217;re really excited about pushing out those insights in an experimental and longer-term way.



I think another element is a little bit of that evolution of how industry standards and frameworks are updating to the AI moment and really articulating where AI is either furthering adversarial ability to create those harms or threats or identifying where AI has a net new harm. And I think that demystifies a little bit about what we talked about in terms of the lessons learned, that fundamentally, a lot of the things that we talk about are traditional security vulnerabilities, and we are standing on kind of that cybersecurity shoulder. And I&#8217;m starting to see those updates translate in spaces that are already considered trustworthy and kind of the basis on which not only cybersecurity folks build their work but also business decision-makers make decisions on those frameworks.



So to me, integration of AI into those frameworks by those same standards means that we&#8217;re evolving security to include AI. We aren&#8217;t creating an entirely new industry of AI security and that, I think, really helps anchor people in the really solid foundation that we have in cybersecurity anyways.



I think there&#8217;s also some work around how the cyber, like, defenses will actually benefit from AI. So we think a lot about threats because that&#8217;s our job. But the other side of cybersecurity is offense. And I&#8217;m seeing a ton of people come out with frameworks and methodologies, especially in the research space, on how defensive networks are going to be benefited from things like agentic systems.



Generally speaking, I think the best practice is to realize that we&#8217;re fundamentally still talking about the same impacts, and we can use the same avenues, conversations, and frameworks. We just really want them to be crisply updated with that understanding of AI applications.



SULLIVAN: How do you think about bringing others into the fold there? I think those standards and frameworks are often informed by technologists. But I&#8217;d love for you to expand [that to] policymakers or other kind of stakeholders in our ecosystem, even, you know, end consumers of these products. Like, how do we communicate some of this to them in a way that resonates and it has an impactful meaning?



WESTERHOFF: I&#8217;ve found the AI security-safety space to be one of the more collaborative. I actually think the fact that I&#8217;m talking to you today is probably evidence that a ton of people are bringing in perspectives that don&#8217;t only come from a long-term cybersecurity view. And I see that as a trend in how AI is being approached opposed to how those areas were moving earlier. So I think that speed and the idea of conversations and not always having the perfect answer but really trying to be transparent with what everyone does know is kind of a communal energy in the communities, at least, where we&#8217;re playing. [LAUGHS] So I am pretty biased but at least the spaces where we are.



SULLIVAN: No, I think we&#8217;re seeing that across the board. I mean, I&#8217;d echo [that] sitting in research, as well, like, that ability to have impact now and at speed to getting the amazing technology and models that we&#8217;re creating into the hands of our customers and partners and ecosystem is just underscored.



So on the note of speed, let&#8217;s shift gears a little bit to just a quick lightning round. I&#8217;d love to get maybe some quick thoughts from you, just 30-second answers here. I&#8217;ll start with one.



Which headline-grabbing AI threat do you think is mostly hot air?



WESTERHOFF: I think we should pay attention to it all. I&#8217;m a red team lead. I love a good question to see if we can find an answer in real life. So no hot air, just questions.



SULLIVAN: Is there some sort of maybe new tool that you can&#8217;t wait to sneak into the red team arsenal?



WESTERHOFF: I think there are really interesting methodologies that break our understanding of cybersecurity by looking at the intersection between different layers of AI and how you can manipulate AI-to-AI interaction, especially now when we&#8217;re looking at agentic systems. So I would say a method, not a tool.



SULLIVAN: So maybe ending on a little bit of a lighter note, do you have a go-to snack during an all-night red teaming session?



WESTERHOFF: Always coffee. I would love it to be a protein smoothie, but honestly, it is probably Trader Joe&#8217;s elote chips. Like the whole bag. [LAUGHTER] It‚Äôs going to get me through. I&#8217;m going to not love that I did it.



[MUSIC]



SULLIVAN: Amazing. Well, Tori, thanks so much for joining us today, and just a huge thanks also to Ciaran for his insights, as well.



WESTERHOFF: Thank you so much for having me. This was a joy.



SULLIVAN: And to our listeners, thanks for tuning in. You can find resources related to this podcast in the show notes. And if you want to learn more about how Microsoft approaches AI governance, you can visit microsoft.com/RAI.



See you next time!‚ÄØ



[MUSIC FADES]

				
			
			
				Show more			
		
	





AI Testing and Evaluation podcast series

Opens in a new tabThe post AI Testing and Evaluation: Learnings from cybersecurity appeared first on Microsoft Research.
‚Ä¢ Evaluating generative AI models with Amazon Nova LLM-as-a-Judge on Amazon SageMaker AI
  Evaluating the performance of large language models (LLMs) goes beyond statistical metrics like perplexity or bilingual evaluation understudy (BLEU) scores. For most real-world generative AI scenarios, it‚Äôs crucial to understand whether a model is producing better outputs than a baseline or an earlier iteration. This is especially important for applications such as summarization, content generation, or intelligent agents where subjective judgments and nuanced correctness play a central role. 
As organizations deepen their deployment of these models in production, we‚Äôre experiencing an increasing demand from customers who want to systematically assess model quality beyond traditional evaluation methods. Current approaches like accuracy measurements and rule-based evaluations, although helpful, can‚Äôt fully address these nuanced assessment needs, particularly when tasks require subjective judgments, contextual understanding, or alignment with specific business requirements. To bridge this gap, LLM-as-a-judge has emerged as a promising approach, using the reasoning capabilities of LLMs to evaluate other models more flexibly and at scale. 
Today, we‚Äôre excited to introduce a comprehensive approach to model evaluation through the Amazon Nova LLM-as-a-Judge capability on Amazon SageMaker AI, a fully managed Amazon Web Services (AWS) service to build, train, and deploy machine learning (ML) models at scale. Amazon Nova LLM-as-a-Judge is designed to deliver robust, unbiased assessments of generative AI outputs across model families. Nova LLM-as-a-Judge is available as optimized workflows on SageMaker AI, and with it, you can start evaluating model performance against your specific use cases in minutes. Unlike many evaluators that exhibit architectural bias, Nova LLM-as-a-Judge has been rigorously validated to remain impartial and has achieved leading performance on key judge benchmarks while closely reflecting human preferences. With its exceptional accuracy and minimal bias, it sets a new standard for credible, production-grade LLM evaluation. 
Nova LLM-as-a-Judge capability provides pairwise comparisons between model iterations, so you can make data-driven decisions about model improvements with confidence. 
How Nova LLM-as-a-Judge was trained 
Nova LLM-as-a-Judge was built through a multistep training process comprising supervised training and reinforcement learning stages that used public datasets annotated with human preferences. For the proprietary component, multiple annotators independently evaluated thousands of examples by comparing pairs of different LLM responses to the same prompt. To verify consistency and fairness, all annotations underwent rigorous quality checks, with final judgments calibrated to reflect broad human consensus rather than an individual viewpoint. 
The training data was designed to be both diverse and representative. Prompts spanned a wide range of categories, including real-world knowledge, creativity, coding, mathematics, specialized domains, and toxicity, so the model could evaluate outputs across many real-world scenarios. Training data included data from over 90 languages and is primarily composed of English, Russian, Chinese, German, Japanese, and Italian.Importantly, an internal bias study evaluating over 10,000 human-preference judgments against 75 third-party models confirmed that Amazon Nova LLM-as-a-Judge shows only a 3% aggregate bias relative to human annotations. Although this is a significant achievement in reducing systematic bias, we still recommend occasional spot checks to validate critical comparisons. 
In the following figure, you can see how the Nova LLM-as-a-Judge bias compares to human preferences when evaluating Amazon Nova outputs compared to outputs from other models. Here, bias is measured as the difference between the judge‚Äôs preference and human preference across thousands of examples. A positive value indicates the judge slightly favors Amazon Nova models, and a negative value indicates the opposite. To quantify the reliability of these estimates, 95% confidence intervals were computed using the standard error for the difference of proportions, assuming independent binomial distributions. 
 
Amazon Nova LLM-as-a-Judge achieves advanced performance among evaluation models, demonstrating strong alignment with human judgments across a range of tasks. For example, it scores 45% accuracy on JudgeBench (compared to 42% for Meta J1 8B) and 68% on PPE (versus 60% for Meta J1 8B). The data from Meta‚Äôs J1 8B was pulled from Incentivizing Thinking in LLM-as-a-Judge via Reinforcement Learning. 
These results highlight the strength of Amazon Nova LLM-as-a-Judge in chatbot-related evaluations, as shown in the PPE benchmark. Our benchmarking follows current best practices, reporting reconciled results for positionally swapped responses on JudgeBench, CodeUltraFeedback, Eval Bias, and LLMBar, while using single-pass results for PPE. 
 
  
   
   Model 
   Eval Bias 
   Judge Bench 
   LLM Bar 
   PPE 
   CodeUltraFeedback 
   
   
   Nova LLM-as-a-Judge 
   0.76 
   0.45 
   0.67 
   0.68 
   0.64 
   
   
   Meta J1 8B 
   ‚Äì 
   0.42 
   ‚Äì 
   0.60 
   ‚Äì 
   
   
   Nova Micro 
   0.56 
   0.37 
   0.55 
   0.6 
   ‚Äì 
   
  
 
In this post, we present a streamlined approach to implementing Amazon Nova LLM-as-a-Judge evaluations using SageMaker AI, interpreting the resulting metrics, and applying this process to improve your generative AI applications. 
Overview of the evaluation workflow 
The evaluation process starts by preparing a dataset in which each example includes a prompt and two alternative model outputs. The JSONL format looks like this: 
 
 {
   "prompt":"Explain photosynthesis.",
   "response_A":"Answer A...",
   "response_B":"Answer B..."
}
{
   "prompt":"Summarize the article.",
   "response_A":"Answer A...",
   "response_B":"Answer B..."
} 
 
After preparing this dataset, you use the given SageMaker evaluation recipe, which configures the evaluation strategy, specifies which model to use as the judge, and defines the inference settings such as temperature and top_p. 
The evaluation runs inside a SageMaker training job using pre-built Amazon Nova containers. SageMaker AI provisions compute resources, orchestrates the evaluation, and writes the output metrics and visualizations to Amazon Simple Storage Service (Amazon S3). 
When it‚Äôs complete, you can download and analyze the results, which include preference distributions, win rates, and confidence intervals. 
Understanding how Amazon Nova LLM-as-a-Judge works 
The Amazon Nova LLM-as-a-Judge uses an evaluation method called binary overall preference judge. The binary overall preference judge is a method where a language model compares two outputs side by side and picks the better one or declares a tie. For each example, it produces a clear preference. When you aggregate these judgments over many samples, you get metrics like win rate and confidence intervals. This approach uses the model‚Äôs own reasoning to assess qualities like relevance and clarity in a straightforward, consistent way. 
 
 This judge model is meant to provide low-latency general overall preferences in situations where granular feedback isn‚Äôt necessary 
 The output of this model is one of&nbsp;[[A&gt;B]]&nbsp;or&nbsp;[[B&gt;A]] 
 Use cases for this model are primarily those where automated, low-latency, general pairwise preferences are required, such as automated scoring for checkpoint selection in training pipelines 
 
Understanding Amazon Nova LLM-as-a-Judge evaluation metrics 
When using the Amazon Nova LLM-as-a-Judge framework to compare outputs from two language models, SageMaker AI produces a comprehensive set of quantitative metrics. You can use these metrics to assess which model performs better and how reliable the evaluation is. The results fall into three main categories: core preference metrics, statistical confidence metrics, and standard error metrics. 
The core preference metrics report how often each model‚Äôs outputs were preferred by the judge model. The a_scores metric counts the number of examples where Model A was favored, and b_scores counts cases where Model B was chosen as better. The ties metric captures instances in which the judge model rated both responses equally or couldn‚Äôt identify a clear preference. The inference_error metric counts cases where the judge couldn‚Äôt generate a valid judgment due to malformed data or internal errors. 
The statistical confidence metrics quantify how likely it is that the observed preferences reflect true differences in model quality rather than random variation. The winrate reports the proportion of all valid comparisons in which Model B was preferred. The lower_rate and upper_rate define the lower and upper bounds of the 95% confidence interval for this win rate. For example, a winrate of 0.75 with a confidence interval between 0.60 and 0.85 suggests that, even accounting for uncertainty, Model B is consistently favored over Model A. The score field often matches the count of Model B wins but can also be customized for more complex evaluation strategies. 
The standard error metrics provide an estimate of the statistical uncertainty in each count. These include a_scores_stderr, b_scores_stderr, ties_stderr, inference_error_stderr, andscore_stderr. Smaller standard error values indicate more reliable results. Larger values can point to a need for additional evaluation data or more consistent prompt engineering. 
Interpreting these metrics requires attention to both the observed preferences and the confidence intervals: 
 
 If the winrate is substantially above 0.5 and the confidence interval doesn‚Äôt include 0.5, Model B is statistically favored over Model A. 
 Conversely, if the winrate is below 0.5 and the confidence interval is fully below 0.5, Model A is preferred. 
 When the confidence interval overlaps 0.5, the results are inconclusive and further evaluation is recommended. 
 High values in inference_error or large standard errors suggest there might have been issues in the evaluation process, such as inconsistencies in prompt formatting or insufficient sample size. 
 
The following is an example metrics output from an evaluation run: 
 
 {
  "a_scores": 16.0,
  "a_scores_stderr": 0.03,
  "b_scores": 10.0,
  "b_scores_stderr": 0.09,
  "ties": 0.0,
  "ties_stderr": 0.0,
  "inference_error": 0.0,
  "inference_error_stderr": 0.0,
  "score": 10.0,
  "score_stderr": 0.09,
  "winrate": 0.38,
  "lower_rate": 0.23,
  "upper_rate": 0.56
} 
 
In this example, Model A was preferred 16 times, Model B was preferred 10 times, and there were no ties or inference errors. The winrate of 0.38 indicates that Model B was preferred in 38% of cases, with a 95% confidence interval ranging from 23% to 56%. Because the interval includes 0.5, this outcome suggests the evaluation was inconclusive, and additional data might be needed to clarify which model performs better overall. 
These metrics, automatically generated as part of the evaluation process, provide a rigorous statistical foundation for comparing models and making data-driven decisions about which one to deploy. 
Solution overview 
This solution demonstrates how to evaluate generative AI models on Amazon SageMaker AI using the Nova LLM-as-a-Judge capability. The provided Python code guides you through the entire workflow. 
First, it prepares a dataset by sampling questions from SQuAD and generating candidate responses from Qwen2.5 and Anthropic‚Äôs Claude 3.7. These outputs are saved in a JSONL file containing the prompt and both responses. 
We accessed Anthropic‚Äôs Claude 3.7 Sonnet in Amazon Bedrock using the bedrock-runtime client. We accessed Qwen2.5 1.5B using a SageMaker hosted Hugging Face endpoint. 
Next, a PyTorch Estimator launches an evaluation job using an Amazon Nova LLM-as-a-Judge recipe. The job runs on GPU instances such as ml.g5.12xlarge and produces evaluation metrics, including win rates, confidence intervals, and preference counts. Results are saved to Amazon S3 for analysis. 
Finally, a visualization function renders charts and tables, summarizing which model was preferred, how strong the preference was, and how reliable the estimates are. Through this end-to-end approach, you can assess improvements, track regressions, and make data-driven decisions about deploying generative models‚Äîall without manual annotation. 
Prerequisites 
You need to complete the following prerequisites before you can run the notebook: 
 
 Make the following quota increase requests for SageMaker AI. For this use case, you need to request a minimum of 1 g5.12xlarge instance. On the Service Quotas console, request the following SageMaker AI quotas, 1 G5 instances (g5.12xlarge) for training job usage 
 (Optional) You can create an Amazon SageMaker Studio domain (refer to Use quick setup for Amazon SageMaker AI) to access Jupyter notebooks with the preceding role. (You can use JupyterLab in your local setup, too.) 
   
   Create an AWS Identity and Access Management (IAM) role with managed policies AmazonSageMakerFullAccess, AmazonS3FullAccess,&nbsp;and AmazonBedrockFullAccess to give required access to SageMaker AI and Amazon Bedrock to run the examples. 
   Assign as trust relationship to your IAM role the following policy: 
    
 
 
 {
&nbsp;&nbsp; &nbsp;"Version": "2012-10-17",
&nbsp;&nbsp; &nbsp;"Statement": [
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;{
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"Sid": "",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"Effect": "Allow",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"Principal": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"Service": [
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"bedrock.amazonaws.com",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"sagemaker.amazonaws.com"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;]
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;},
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"Action": "sts:AssumeRole"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp;]
} 
 
 
 Clone the GitHub repository with the assets for this deployment. This repository consists of a notebook that references training assets: 
 
 
 git clone https://github.com/aws-samples/amazon-nova-samples.git
cd customization/SageMakerTrainingJobs/Amazon-Nova-LLM-As-A-Judge/ 
 
Next, run the notebook Nova Amazon-Nova-LLM-as-a-Judge-Sagemaker-AI.ipynb to start using the Amazon Nova LLM-as-a-Judge implementation on Amazon SageMaker AI. 
Model setup 
To conduct an Amazon Nova LLM-as-a-Judge evaluation, you need to generate outputs from the candidate models you want to compare. In this project, we used two different approaches: deploying a Qwen2.5 1.5B model on Amazon SageMaker and invoking Anthropic‚Äôs Claude 3.7 Sonnet model in Amazon Bedrock. First, we deployed Qwen2.5 1.5B, an open-weight multilingual language model, on a dedicated SageMaker endpoint. This was achieved by using the HuggingFaceModel deployment interface. To deploy the Qwen2.5 1.5B model, we provided a convenient script for you to invoke:python3 deploy_sm_model.py 
When it‚Äôs deployed, inference can be performed using a helper function wrapping the SageMaker predictor API: 
 
 # Initialize the predictor once
predictor = HuggingFacePredictor(endpoint_name="qwen25-&lt;endpoint_name_here&gt;")
def generate_with_qwen25(prompt: str, max_tokens: int = 500, temperature: float = 0.9) -&gt; str:
    """
    Sends a prompt to the deployed Qwen2.5 model on SageMaker and returns the generated response.
    Args:
        prompt (str): The input prompt/question to send to the model.
        max_tokens (int): Maximum number of tokens to generate.
        temperature (float): Sampling temperature for generation.
    Returns:
        str: The model-generated text.
    """
    response = predictor.predict({
        "inputs": prompt,
        "parameters": {
            "max_new_tokens": max_tokens,
            "temperature": temperature
        }
    })
    return response[0]["generated_text"]
answer = generate_with_qwen25("What is the Grotto at Notre Dame?")
print(answer) 
 
In parallel, we integrated Anthropic‚Äôs Claude 3.7 Sonnet model in Amazon Bedrock. Amazon Bedrock provides a managed API layer for accessing proprietary foundation models (FMs) without managing infrastructure. The Claude generation function used the bedrock-runtime AWS SDK for Python (Boto3) client, which accepted a user prompt and returned the model‚Äôs text completion: 
 
 # Initialize Bedrock client once
bedrock = boto3.client("bedrock-runtime", region_name="us-east-1")
# (Claude 3.7 Sonnet) model ID via Bedrock
MODEL_ID = "us.anthropic.claude-3-7-sonnet-20250219-v1:0"
def generate_with_claude4(prompt: str, max_tokens: int = 512, temperature: float = 0.7, top_p: float = 0.9) -&gt; str:
    """
    Sends a prompt to the Claude 4-tier model via Amazon Bedrock and returns the generated response.
    Args:
        prompt (str): The user message or input prompt.
        max_tokens (int): Maximum number of tokens to generate.
        temperature (float): Sampling temperature for generation.
        top_p (float): Top-p nucleus sampling.
    Returns:
        str: The text content generated by Claude.
    """
    payload = {
        "anthropic_version": "bedrock-2023-05-31",
        "messages": [{"role": "user", "content": prompt}],
        "max_tokens": max_tokens,
        "temperature": temperature,
        "top_p": top_p
    }
    response = bedrock.invoke_model(
        modelId=MODEL_ID,
        body=json.dumps(payload),
        contentType="application/json",
        accept="application/json"
    )
    response_body = json.loads(response['body'].read())
    return response_body["content"][0]["text"]
answer = generate_with_claude4("What is the Grotto at Notre Dame?")
print(answer) 
 
When you have both functions generated and tested, you can move on to creating the evaluation data for the Nova LLM-as-a-Judge. 
Prepare the dataset 
To create a realistic evaluation dataset for comparing the Qwen and Claude models, we used the Stanford Question Answering Dataset (SQuAD), a widely adopted benchmark in natural language understanding distributed under the CC BY-SA 4.0 license. SQuAD consists of thousands of crowd-sourced question-answer pairs covering a diverse range of Wikipedia articles. By sampling from this dataset, we made sure that our evaluation prompts reflected high-quality, factual question-answering tasks representative of real-world applications. 
We began by loading a small subset of examples to keep the workflow fast and reproducible. Specifically, we used the Hugging Face datasets library to download and load the first 20 examples from the SQuAD training split: 
 
 from datasets import load_dataset
squad = load_dataset("squad", split="train[:20]") 
 
This command retrieves a slice of the full dataset, containing 20 entries with structured fields including context, question, and answers. To verify the contents and inspect an example, we printed out a sample question and its ground truth answer: 
 
 print(squad[3]["question"])
print(squad[3]["answers"]["text"][0]) 
 
For the evaluation set, we selected the first six questions from this subset: 
questions = [squad[i]["question"] for i in range(6)] 
Generate the Amazon Nova LLM-as-a-Judge evaluation dataset 
After preparing a set of evaluation questions from SQuAD, we generated outputs from both models and assembled them into a structured dataset to be used by the Amazon Nova LLM-as-a-Judge workflow. This dataset serves as the core input for SageMaker AI evaluation recipes. To do this, we iterated over each question prompt and invoked the two generation functions defined earlier: 
 
 generate_with_qwen25() for completions from the Qwen2.5 model deployed on SageMaker 
 generate_with_claude() for completions from Anthropic‚Äôs Claude 3.7 Sonnet in Amazon Bedrock 
 
For each prompt, the workflow attempted to generate a response from each model. If a generation call failed due to an API error, timeout, or other issue, the system captured the exception and stored a clear error message indicating the failure. This made sure that the evaluation process could proceed gracefully even in the presence of transient errors: 
 
 import json
output_path = "llm_judge.jsonl"
with open(output_path, "w") as f:
    for q in questions:
        try:
            response_a = generate_with_qwen25(q)
        except Exception as e:
            response_a = f"[Qwen2.5 generation failed: {e}]"
        
        try:
            response_b = generate_with_claude4(q)
        except Exception as e:
            response_b = f"[Claude 3.7 generation failed: {e}]"
        row = {
            "prompt": q,
            "response_A": response_a,
            "response_B": response_b
        }
        f.write(json.dumps(row) + "\n")
print(f"JSONL file created at: {output_path}") 
 
This workflow produced a JSON Lines file named llm_judge.jsonl. Each line contains a single evaluation record structured as follows: 
 
 {
  "prompt": "What is the capital of France?",
  "response_A": "The capital of France is Paris.",
  "response_B": "Paris is the capital city of France."
} 
 
Then, upload this llm_judge.jsonl to an S3 bucket that you‚Äôve predefined: 
 
 upload_to_s3(
    "llm_judge.jsonl",
    "s3://&lt;YOUR_BUCKET_NAME&gt;/datasets/byo-datasets-dev/custom-llm-judge/llm_judge.jsonl"
) 
 
Launching the Nova LLM-as-a-Judge evaluation job 
After preparing the dataset and creating the evaluation recipe, the final step is to launch the SageMaker training job that performs the Amazon Nova LLM-as-a-Judge evaluation. In this workflow, the training job acts as a fully managed, self-contained process that loads the model, processes the dataset, and generates evaluation metrics in your designated Amazon S3 location. 
We use the PyTorch estimator class from the SageMaker Python SDK to encapsulate the configuration for the evaluation run. The estimator defines the compute resources, the container image, the evaluation recipe, and the output paths for storing results: 
 
 estimator = PyTorch(
    output_path=output_s3_uri,
    base_job_name=job_name,
    role=role,
    instance_type=instance_type,
    training_recipe=recipe_path,
    sagemaker_session=sagemaker_session,
    image_uri=image_uri,
    disable_profiler=True,
    debugger_hook_config=False,
) 
 
When the estimator is configured, you initiate the evaluation job using the fit() method. This call submits the job to the SageMaker control plane, provisions the compute cluster, and begins processing the evaluation dataset: 
estimator.fit(inputs={"train": evalInput}) 
Results from the Amazon Nova LLM-as-a-Judge evaluation job 
The following graphic illustrates the results of the Amazon Nova LLM-as-a-Judge evaluation job. 
 
To help practitioners quickly interpret the outcome of a Nova LLM-as-a-Judge evaluation, we created a convenience function that produces a single, comprehensive visualization summarizing key metrics. This function, plot_nova_judge_results, uses Matplotlib and Seaborn to render an image with six panels, each highlighting a different perspective of the evaluation outcome. 
This function takes the evaluation metrics dictionary‚Äîproduced when the evaluation job is complete‚Äîand generates the following visual components: 
 
 Score distribution bar chart ‚Äì Shows how many times Model A was preferred, how many times Model B was preferred, how many ties occurred, and how often the judge failed to produce a decision (inference errors). This provides an immediate sense of how decisive the evaluation was and whether either model is dominating. 
 Win rate with 95% confidence interval ‚Äì Plots Model B‚Äôs overall win rate against Model A, including an error bar reflecting the lower and upper bounds of the 95% confidence interval. A vertical reference line at 50% marks the point of no preference. If the confidence interval doesn‚Äôt cross this line, you can conclude the result is statistically significant. 
 Preference pie chart ‚Äì Visually displays the proportion of times Model A, Model B, or neither was preferred. This helps quickly understand preference distribution among the valid judgments. 
 A vs. B score comparison bar chart ‚Äì Compares the raw counts of preferences for each model side by side. A clear label annotates the margin of difference to emphasize which model had more wins. 
 Win rate gauge ‚Äì Depicts the win rate as a semicircular gauge with a needle pointing to Model B‚Äôs performance relative to the theoretical 0‚Äì100% range. This intuitive visualization helps nontechnical stakeholders understand the win rate at a glance. 
 Summary statistics table ‚Äì Compiles numerical metrics‚Äîincluding total evaluations, error counts, win rate, and confidence intervals‚Äîinto a compact, clean table. This makes it straightforward to reference the exact numeric values behind the plots. 
 
Because the function outputs a standard Matplotlib figure, you can quickly save the image, display it in Jupyter notebooks, or embed it in other documentation. 
Clean up 
Complete the following steps to clean up your resources: 
 
 Delete your Qwen 2.5 1.5B Endpoint 
   
   import boto3

# Create a low-level SageMaker service client.

sagemaker_client = boto3.client('sagemaker', region_name=&lt;region&gt;)

# Delete endpoint

sagemaker_client.delete_endpoint(EndpointName=endpoint_name) 
    
 If you‚Äôre using a SageMaker Studio JupyterLab notebook, shut down the JupyterLab notebook instance. 
 
How you can use this evaluation framework 
The Amazon Nova LLM-as-a-Judge workflow offers a reliable, repeatable way to compare two language models on your own data. You can integrate this into model selection pipelines to decide which version performs best, or you can schedule it as part of continuous evaluation to catch regressions over time. 
For teams building agentic or domain-specific systems, this approach provides richer insight than automated metrics alone. Because the entire process runs on SageMaker training jobs, it scales quickly and produces clear visual reports that can be shared with stakeholders. 
Conclusion 
This post demonstrates how Nova LLM-as-a-Judge‚Äîa specialized evaluation model available through Amazon SageMaker AI‚Äîcan be used to systematically measure the relative performance of generative AI systems. The walkthrough shows how to prepare evaluation datasets, launch SageMaker AI training jobs with Nova LLM-as-a-Judge recipes, and interpret the resulting metrics, including win rates and preference distributions. The fully managed SageMaker AI solution simplifies this process, so you can run scalable, repeatable model evaluations that align with human preferences. 
We recommend starting your LLM evaluation journey by exploring the official Amazon Nova documentation and examples. The AWS AI/ML community offers extensive resources, including workshops and technical guidance, to support your implementation journey. 
To learn more, visit: 
 
 Amazon Nova Documentation 
 Amazon Bedrock Nova Overview 
 Fine-tuning Amazon Nova models 
 Amazon Nova customization guide 
 
 
 
About the authors 
Surya Kari is a Senior Generative AI Data Scientist at AWS, specializing in developing solutions leveraging state-of-the-art foundation models. He has extensive experience working with advanced language models including DeepSeek-R1, the Llama family, and Qwen, focusing on their fine-tuning and optimization. His expertise extends to implementing efficient training pipelines and deployment strategies using AWS SageMaker. He collaborates with customers to design and implement generative AI solutions, helping them navigate model selection, fine-tuning approaches, and deployment strategies to achieve optimal performance for their specific use cases. 
Joel Carlson is a Senior Applied Scientist on the Amazon AGI foundation modeling team. He primarily works on developing novel approaches for improving the LLM-as-a-Judge capability of the Nova family of models. 
Saurabh Sahu is an applied scientist in the Amazon AGI Foundation modeling team. He obtained his PhD in Electrical Engineering from University of Maryland College Park in 2019. He has a background in multi-modal machine learning working on speech recognition, sentiment analysis and audio/video understanding. Currently, his work focuses on developing recipes to improve the performance of LLM-as-a-judge models for various tasks. 
Morteza Ziyadi is an Applied Science Manager at Amazon AGI, where he leads several projects on post-training recipes and (Multimodal) large language models in the Amazon AGI Foundation modeling team. Before joining Amazon AGI, he spent four years at Microsoft Cloud and AI, where he led projects focused on developing natural language-to-code generation models for various products. He has also served as an adjunct faculty at Northeastern University. He earned his PhD from the University of Southern California (USC) in 2017 and has since been actively involved as a workshop organizer, and reviewer for numerous NLP, Computer Vision and machine learning conferences. 
Pradeep Natarajan is a Senior Principal Scientist in Amazon AGI Foundation modeling team working on post-training recipes and Multimodal large language models. He has 20+ years of experience in developing and launching multiple large-scale machine learning systems. He has a PhD in Computer Science from University of Southern California. 
Michael Cai is a Software Engineer on the Amazon AGI Customization Team supporting the development of evaluation solutions. He obtained his MS in Computer Science from New York University in 2024. In his spare time he enjoys 3d printing and exploring innovative tech.
‚Ä¢ Building cost-effective RAG applications with Amazon Bedrock Knowledge Bases and Amazon S3 Vectors
  Vector embeddings have become essential for modern Retrieval Augmented Generation (RAG) applications, but organizations face significant cost challenges as they scale. As knowledge bases grow and require more granular embeddings, many vector databases that rely on high-performance storage such as SSDs or in-memory solutions become prohibitively expensive. This cost barrier often forces organizations to limit the scope of their RAG applications or compromise on the granularity of their vector representations, potentially impacting the quality of results. Additionally, for use cases involving historical or archival data that still needs to remain searchable, storing vectors in specialized vector databases optimized for high throughput workloads represents an unnecessary ongoing expense. 
Starting July 15, Amazon Bedrock Knowledge Bases customers can select Amazon S3 Vectors (preview), the first cloud object storage with built-in support to store and query vectors at a low cost, as a vector store. Amazon Bedrock Knowledge Bases users can now reduce vector upload, storage, and query costs by up to 90%. Designed for durable and cost-optimized storage of large vector datasets with subsecond query performance, S3 Vectors is ideal for RAG applications that require long-term storage of massive vector volumes and can tolerate the performance tradeoff compared to high queries per second (QPS), millisecond latency vector databases. The integration with Amazon Bedrock means you can build more economical RAG applications while preserving the semantic search performance needed for quality results. 
In this post, we demonstrate how to integrate Amazon S3 Vectors with Amazon Bedrock Knowledge Bases for RAG applications. You‚Äôll learn a practical approach to scale your knowledge bases to handle millions of documents while maintaining retrieval quality and using S3 Vectors cost-effective storage. 
Amazon Bedrock Knowledge Bases and Amazon S3 Vectors integration overview 
When creating a knowledge base in Amazon Bedrock, you can select S3 Vectors as your vector storage option. Using this approach, you can build cost-effective, scalable RAG applications without provisioning or managing complex infrastructure. The integration delivers significant cost savings while maintaining subsecond query performance, making it ideal for working with larger vector datasets generated from massive volumes of unstructured data including text, images, audio, and video. Using a pay-as-you-go pricing model at low price points, S3 Vectors offers industry-leading cost optimization that reduces the cost of uploading, storing, and querying vectors by up to 90% compared to alternative solutions. Advanced search capabilities include rich metadata filtering, so you can refine queries by document attributes such as dates, categories, and sources. The combination of S3 Vectors and Amazon Bedrock is ideal for organizations building large-scale knowledge bases that demand both cost efficiency and performant retrieval‚Äîfrom managing extensive document repositories to historical archives and applications requiring granular vector representations. The walkthrough follows these high-level steps: 
 
 Create a new knowledge base 
 Configure the data source 
 Configure data source and processing 
 Sync the data source 
 Test the knowledge base 
 
Prerequisites 
Before you get started, make sure that you have the following prerequisites: 
 
 An AWS Account with appropriate service access. 
 An AWS Identity and Access Management (IAM) role with the appropriate permissions to access Amazon Bedrock and Amazon Simple Storage Service (Amazon S3). 
 Enable model access for embedding and inference models such as Amazon Titan Text Embeddings V2 and Amazon Nova Pro. 
 
Amazon Bedrock Knowledge Bases and Amazon S3 Vectors integration walkthrough 
In this section, we walk through the step-by-step process of creating a knowledge base with Amazon S3 Vectors using the AWS Management Console. We cover the end-to-end process from configuring your vector store to ingesting documents and testing your retrieval capabilities. 
For those who prefer to configure their knowledge base programmatically rather than using the console, the Amazon Bedrock Knowledge Bases with S3 Vectors repository in GitHub provides a guided notebook that you can follow to deploy the setup in your own account. 
Create a new knowledge base 
To create a new knowledge base, follow these steps: 
 
 On the Amazon Bedrock console in the left navigation pane, choose Knowledge Bases. To initiate the creation process, in the Create dropdown list, choose Knowledge Base with vector store. 
 On the Provide Knowledge Base details page, enter a descriptive name for your knowledge base and an optional description to identify its purpose. Select your IAM permissions approach‚Äîeither create a new service role or use an existing one‚Äîto grant the necessary permissions for accessing AWS services, as shown in the following screenshot. 
 
 
 
 Choose Amazon S3. Optionally, add tags to help organize and categorize your resources and configure log delivery destinations such as an S3 bucket or Amazon CloudWatch for monitoring and troubleshooting. 
 Choose Next to proceed to the data source configuration. 
 
Configure the data source 
To configure the data source, follow these steps: 
 
 Assign a descriptive name to your knowledge base data. 
 In Data source location, select whether the S3 bucket exists in your current AWS account or another account, then specify the location where your documents are stored, as shown in the following screenshot. 
 
 
In this step, configure your parsing strategy to determine how Amazon Bedrock processes your documents. Select Amazon Bedrock default parser for text-only documents at no additional cost. Select Amazon Bedrock Data Automation as parser or Foundation models as a parser for processing complex documents with visual elements. 
The chunking strategy configuration is equally critical because it defines how your content is segmented into meaningful units for vector embedding, directly impacting retrieval quality and context preservation. We have selected Fixed-size chunking for this example due to its predictable token sizing and simplicity. Because both parsing and chunking decisions can‚Äôt be modified after creation, select options that best match your content structure and retrieval needs. For sensitive data, you can use advanced settings to implement AWS Key Management Service (AWS KMS) encryption or apply custom transformation functions to optimize your documents before ingestion. By default, S3 Vectors will use server-side encryption (SSE-S3). 
Configure data storage and processing 
To configure data storage and processing, first select the embeddings model, as shown in the following screenshot. The embeddings model will transform your text chunks into numerical vector representations for semantic search capabilities. If connecting to an existing S3 Vector as a vector store, make sure the embedding model dimensions match those used when creating your vector store because dimensional mismatches will cause ingestion failures. Amazon Bedrock offers several embeddings models to choose from, each with different vector dimensions and performance characteristics optimized for various use cases. Consider both the semantic richness of the model and its cost implications when making your selection. 
 
Next, configure the vector store. For vector storage selection, choose how Amazon Bedrock Knowledge Bases will store and manage the vector embeddings generated from your documents in Amazon S3 Vectors, using one of the following two options: 
Option 1. Quick create a new vector store 
This recommended option, shown in the following screenshot, automatically creates an S3 vector bucket in your account during knowledge base creation. The system optimizes your vector storage for cost-effective, durable storage of large-scale vector datasets, creating an S3 vector bucket and vector index for you. 
 
Option 2. Use an existing vector store 
When creating your Amazon S3 Vector as a vector store index for use with Amazon Bedrock Knowledge Bases, you can attach metadata (such as, year, author, genre, and location) as key-value pairs to each vector. By default, metadata fields can be used as filters in similarity queries unless specified as nonfilterable metadata at the time of vector index creation. S3 Vector indexes support string, number, and Boolean types up to 40 KB per vector, with filterable metadata capped at 2 KB per vector. 
To accommodate larger text chunks and richer metadata while still allowing filtering on other important attributes, add "AMAZON_BEDROCK_TEXT" to the nonFilterableMetadataKeys list in your index configuration. This approach optimizes your storage allocation for document content while preserving filtering capabilities for meaningful attributes like categories or dates. Keep in mind that fields added to the nonFilterableMetadataKeys array can‚Äôt be used with metadata filtering in queries and can‚Äôt be modified after the index is created. 
Here‚Äôs an example for creating an Amazon S3 Vector index with proper metadata configuration: 
 
 s3vectors.create_index(
    vectorBucketName="my-first-vector-bucket",
    indexName="my-first-vector-index",
    dimension=1024,
    distanceMetric="cosine",
    dataType="float32",  
    metadataConfiguration={"nonFilterableMetadataKeys": ["AMAZON_BEDROCK_TEXT"]} 
) 
 
For details on how to create a vector store, refer to Introducing Amazon S3 Vectors in the AWS News Blog. 
After you have an S3 Vector bucket and index, you can connect it to your knowledge base. You‚Äôll need to provide both the S3 Vector bucket Amazon Resource Name (ARN) and vector index ARN, as shown in the following screenshot, to correctly link your knowledge base to your existing S3 Vector index. 
 
Sync data source 
After you‚Äôve configured your knowledge base with S3 Vectors, you need to synchronize your data source to generate and store vector embeddings. From the Amazon Bedrock Knowledge Bases console, open your created knowledge base and locate your configured data source and choose Sync to initiate the process, as shown in the following screenshot. During synchronization, the system processes your documents according to your parsing and chunking configurations, generates embeddings using your selected model, and stores them in your S3 vector index. You can monitor the synchronization progress in real time if you‚Äôve configured Amazon CloudWatch Logs and verify completion status before testing your knowledge base‚Äôs retrieval capabilities. 
 
Test the knowledge base 
After successfully configuring your knowledge base with S3 Vectors, you can validate its functionality using the built-in testing interface. You can use this interactive console to experiment with different query types and view both retrieval results and generated responses. Select between Retrieval only (Retrieve API) mode to examine raw source chunks or Retrieval and Response generation (RetrieveandGenerate API) to learn how foundation models (FMs) such as Amazon Nova use your retrieved content. The testing interface provides valuable insights into how your knowledge base processes queries, displaying source chunks, their relevance scores, and associated metadata. 
You can also configure query settings for your knowledge base just as you would with other vector storage options, including filters for metadata-based selection, guardrails for appropriate responses, reranking capabilities, and query modification options. These tools help optimize retrieval quality and make sure the most relevant information is presented to your FMs. S3 Vectors currently supports semantic search functionality. Using this hands-on validation, you can refine your configuration before integrating the knowledge base with production applications. 
 
Creating your Amazon Bedrock knowledge base programmatically 
In the previous sections, we walked through creating a knowledge base with Amazon S3 Vectors using the AWS Management Console. For those who prefer to automate this process or integrate it into existing workflows, you can also create your knowledge base programmatically using the AWS SDK. 
The following is a sample code showing how the API call looks when programmatically creating an Amazon Bedrock knowledge base with an existing Amazon S3 Vector index: 
 
 response = bedrock.create_knowledge_base(
&nbsp;&nbsp; &nbsp;description='Amazon Bedrock Knowledge Base integrated with Amazon S3 Vectors',
&nbsp;&nbsp; &nbsp;knowledgeBaseConfiguration={
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;'type': 'VECTOR',
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;'vectorKnowledgeBaseConfiguration': {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 'embeddingModelArn': f'arn:aws:bedrock:{region}::foundation-model/amazon.titan-embed-text-v2:0',
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 'embeddingModelConfiguration': {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 'bedrockEmbeddingModelConfiguration': {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 'dimensions': vector_dimension, #Verify this is the same value as S3 vector index configuration
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 'embeddingDataType': 'FLOAT32'
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; },
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;},
&nbsp;&nbsp; &nbsp;},
&nbsp;&nbsp; &nbsp; name=knowledge_base_name,
&nbsp;&nbsp; &nbsp; roleArn=roleArn,
&nbsp;&nbsp; &nbsp; storageConfiguration={
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; 's3VectorsConfiguration': {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 'indexArn': vector_index_arn
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; },
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; 'type': 'S3_VECTORS'
&nbsp;&nbsp; &nbsp; }
) 
 
The role attached to the knowledge base should have several policies attached to it, including access to the S3 Vectors API, the models used for embedding, generation, and reranking (if used), and the S3 bucket used as data source. If you‚Äôre using a customer managed key for your S3 Vector as a vector store, you‚Äôll need to provide an additional policy to allow the decryption of the data. The following is the policy needed to access the Amazon S3 Vector as a vector store: 
 
 {
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "BedrockInvokeModelPermission",
            "Effect": "Allow",
            "Action": [
                "bedrock:InvokeModel"
            ],
            "Resource": [
                "arn:aws:bedrock:{REGION}::foundation-model/amazon.titan-embed-text-v2:0"
            ]
        },
        {
            "Sid": "KmsPermission",
            "Effect": "Allow",
            "Action": [
                "kms:GenerateDataKey",
                "kms:Decrypt"
            ],
            "Resource": [
                "arn:aws:kms:{REGION}:{ACCOUNT_ID}:key/{KMS_KEY_ID}"
            ]
        },
        {
            "Sid": "S3ListBucketPermission",
            "Effect": "Allow",
            "Action": [
                "s3:ListBucket"
            ],
            "Resource": [
                "arn:aws:s3:::{SOURCE_BUCKET_NAME}"
            ],
            "Condition": {
                "StringEquals": {
                    "aws:ResourceAccount": [
                        "{ACCOUNT_ID}"
                    ]
                }
            }
        },
        {
            "Sid": "S3GetObjectPermission",
            "Effect": "Allow",
            "Action": [
                "s3:GetObject"
            ],
            "Resource": [
                "arn:aws:s3:::{SOURCE_BUCKET_NAME}/{PREFIX}/*"
            ],
            "Condition": {
                "StringEquals": {
                    "aws:ResourceAccount": [
                        "{ACCOUNT_ID}"
                    ]
                }
            }
        },
        {
            "Sid": "S3VectorsAccessPermission",
            "Effect": "Allow",
            "Action": [
                "s3vectors:GetIndex",
                "s3vectors:QueryVectors",
                "s3vectors:PutVectors",
                "s3vectors:GetVectors",
                "s3vectors:DeleteVectors"
            ],
            "Resource": "arn:aws:s3vectors:{REGION}:{ACCOUNT_ID}:bucket/{VECTOR_BUCKET_NAME}/index/{VECTOR_INDEX_NAME}",
            "Condition": {
                "StringEquals": {
                    "aws:ResourceAccount": "{ACCOUNT_ID}"
                }
            }
        }
    ]
} 
 
Cleanup 
To clean up your resources, complete the following steps. To delete the knowledge base: 
 
 On the Amazon Bedrock console, choose Knowledge Bases 
 Select your Knowledge Base and note both the IAM service role name and S3 Vector index ARN 
 Choose Delete and confirm 
 
To delete the S3 Vector as a vector store, use the following AWS Command Line Interface (AWS CLI) commands: 
 
 aws s3vectors delete-index --vector-bucket-name YOUR_VECTOR_BUCKET_NAME --index-name YOUR_INDEX_NAME --region YOUR_REGION
aws s3vectors delete-vector-bucket --vector-bucket-name YOUR_VECTOR_BUCKET_NAME --region YOUR_REGION 
 
 
 On the IAM console, find the role noted earlier 
 Select and delete the role 
 
To delete the sample dataset: 
 
 On the Amazon S3 console, find your S3 bucket 
 Select and delete the files you uploaded for this tutorial 
 
Conclusion 
The integration between Amazon Bedrock Knowledge Bases and Amazon S3 Vectors represents a significant advancement in making RAG applications more accessible and economically viable at scale. By using the cost-optimized storage of Amazon S3 Vectors, organizations can now build knowledge bases at scale with improved cost efficiency. This means customers can strike an optimal balance between performance and economics, and you can focus on creating value through AI-powered applications rather than managing complex vector storage infrastructure. 
To get started on Amazon Bedrock Knowledge Bases and Amazon S3 Vectors integration, refer to Using S3 Vectors with Amazon Bedrock Knowledge Bases in the Amazon S3 User Guide. 
 
About the authors 
Vaibhav Sabharwal is a Senior Solutions Architect with Amazon Web Services (AWS) based out of New York. He is passionate about learning new cloud technologies and assisting customers in building cloud adoption strategies, designing innovative solutions, and driving operational excellence. As a member of the Financial Services Technical Field Community at AWS, he actively contributes to the collaborative efforts within the industry. 
Dani Mitchell is a Generative AI Specialist Solutions Architect at Amazon Web Services (AWS). He is focused on helping accelerate enterprises across the world on their generative AI journeys with Amazon Bedrock. 
Irene Marban is a Generative AI Specialist Solutions Architect at Amazon Web Services (AWS), working with customers across EMEA to design and implement generative AI solutions to accelerate their businesses. With a background in biomedical engineering and AI, her work focuses on helping organizations leverage the latest AI technologies to drive innovation and growth. In her spare time, she loves reading and cooking for her friends. 
Ashish Lal is an AI/ML Senior Product Marketing Manager for Amazon Bedrock. He has over 11 years of experience in product marketing and enjoys helping customers accelerate time to value and reduce their AI lifecycle cost.
‚Ä¢ Implementing on-demand deployment with customized Amazon Nova models on Amazon Bedrock
  Amazon Bedrock offers model customization capabilities for customers to tailor versions of foundation models (FMs) to their specific needs through features such as fine-tuning and distillation. Today, we‚Äôre announcing the launch of on-demand deployment for customized models ready to be deployed on Amazon Bedrock. 
On-demand deployment for customized models provides an additional deployment option that scales with your usage patterns. This approach allows for invoking customized models only when needed, with requests processed in real time without requiring pre-provisioned compute resources. 
The on-demand deployment option includes a token-based pricing model that charges based on the number of tokens processed during inference. This pay-as-you-go approach complements the existing Provisioned Throughput option, giving users flexibility to choose the deployment method that best aligns with their specific workload requirements and cost objectives. 
In this post, we walk through the custom model on-demand deployment workflow for Amazon Bedrock and provide step-by-step implementation guides using both the AWS Management Console and APIs or AWS SDKs. We also discuss best practices and considerations for deploying customized Amazon Nova models on Amazon Bedrock. 
Understanding custom model on-demand deployment workflow 
The model customization lifecycle represents the end-to-end journey from conceptualization to deployment. This process begins with defining your specific use case, preparing and formatting appropriate data, and then performing model customization through features such as Amazon Bedrock fine-tuning or Amazon Bedrock Model Distillation. Each stage builds upon the previous one, creating a pathway toward deploying production-ready generative AI capabilities that you tailor to your requirements. The following diagram illustrates this workflow. 
 
After customizing your model, the evaluation and deployment phases determine how the model will be made available for inference. This is where custom model on-demand deployment becomes valuable, offering a deployment option that aligns with variable workloads and cost-conscious implementations. When using on-demand deployment, you can invoke your customized model through the AWS console or standard API operations using the model identifier, with compute resources automatically allocated only when needed. The on-demand deployment provides flexibility while maintaining performance expectations, so you can seamlessly integrate customized models into your applications with the same serverless experience offered by Amazon Bedrock‚Äîall compute resources are automatically managed for you, based on your actual usage. Because the workflow supports iterative improvements, you can refine your models based on evaluation results and evolving business needs. 
Prerequisites 
This post assumes you have a customized Amazon Nova model before deploying it using on-demand deployment. On-demand deployment requires newly customized Amazon Nova models after this launch. Previously customized models aren‚Äôt compatible with this deployment option. For instructions on creating or customizing your Nova model through fine-tuning or distillation, refer to these resources: 
 
 Fine-tuning Amazon Nova models 
 A guide to Amazon Bedrock Model Distillation 
 
After you‚Äôve successfully customized your Amazon Nova model, you can proceed with deploying it using the on-demand deployment option as detailed in the following sections. 
Implementation guide for on-demand deployment 
There are two main approaches to implementing on-demand deployment for your customized Amazon Nova models on Amazon Bedrock: using the Amazon Bedrock console or using the API or SDK. First, we explore how to deploy your model through the Amazon Bedrock console, which provides a user-friendly interface for setting up and managing your deployments. 
Step-by-step implementation using the Amazon Bedrock console 
To implement on-demand deployment for your customized Amazon Nova models on Amazon Bedrock using the console, follow these steps: 
 
 On the Amazon Bedrock console, select your customized model (fine-tuning or model distillation) to be deployed. Choose Set up inference and select Deploy for on-demand, as shown in the following screenshot. 
 
 
 
 Under Deployment details, enter a Name and a Description. You have the option to add Tags, as shown in the following screenshot. Choose Create to start on-demand deployment of customized model. 
 
 
Under Custom model deployments, the status of your deployment should be InProgress, Active, or Failed, as shown in the following screenshot. 
 
You can select a deployment to find Deployment ARN, Creation time, Last updated, and Status for the selected custom model. 
 
The custom model is deployed and ready using on-demand deployment. Try it out in the test playground or go to Chat/Text playground, choose Custom models under Categories. Select your model, choose On demand under Inference, and select by the deployment name, as shown in the following screenshot. 
 
Step-by-step implementation using API or SDK 
After you have trained the model successfully, you can deploy it to evaluate the response quality and latency or to use the model as a production model for your use case. You use CreateCustomModelDeployment API to create model deployment for the trained model. The following steps show how to use the APIs for deploying and deleting the custom model deployment for on-demand inference. 
 
 import boto3
import json

# First, create and configure an Amazon Bedrock client:
bedrock_client = boto3.client(
service_name="bedrock",region_name="&lt;region-info&gt;")

# create custom model deployment 
response = bedrock_client.create_custom_model_deployment(
                        modelDeploymentName="&lt;model-deployment-name&gt;",
                        modelArn="&lt;trained-model-arn&gt;",
                        description="&lt;model-deployment-description&gt;",
                        tags=[
{"key":"&lt;your-key&gt;",
 "value":"&lt;your-value&gt;"},
   ]) 
 
After you‚Äôve successfully created a model deployment, you can check the status of the deployment by using GetCustomModelDeployment API as follows: 
 
 response = bedrock_client.get_custom_model_deployment( 
			customModelDeploymentIdentifier="&lt;custom-deployment-arn&gt;") 
 
GetCustomModelDeployment supports three states: Creating , Active , and Failed. When the status in response is Active, you should be able to use the custom model through on-demand deployment with InvokeModel or Converse API, as shown in the following example: 
 
 # Define Runtime Client
bedrock_runtime = boto3.client(service_name="bedrock-runtime", region_name="&lt;region-info&gt;") 
# invoke a deployed custom model using Converse API
response = bedrock_runtime.converse(
                    modelId="&lt;custom-deployment-arn&gt;",
                    messages=[
                        {
                            "role": "user",
                            "content": [
                                {
                                    "text": "&lt;your-prompt-for-custom-model&gt;",
                                }
                            ]
                        }
                    ]
                )

result = response.get('output')
print(result)

# invoke a deployed custom model using InvokeModel API
request_body = {
    "schemaVersion": "messages-v1",
    "messages": [{"role": "user", 
                  "content": [{"text": "&lt;your-prompt-for-custom-model&gt;"}]}],
    "system": [{"text": "&lt;system prompt&gt;"}],
    "inferenceConfig": {"maxTokens": 500, 
                        "topP": 0.9, 
                        "temperature": 0.0
                        }
}
body = json.dumps(request_body)
response = bedrock_runtime.invoke_model(
        modelId="&lt;custom-deployment-arn&gt;",
        body=body
    )

# Extract and print the response text
model_response = json.loads(response["body"].read())
response_text = model_response["output"]["message"]["content"][0]["text"]
print(response_text) 
 
By following these steps, you can deploy and use your customized model through Amazon Bedrock API and instantly use your efficient and high-performing model tailored to your use cases through on-demand deployment. 
Best practices and considerations 
Successful implementation of on-demand deployment with customized models depends on understanding several operational factors. These considerations‚Äîincluding latency, Regional availability, quota limitations, deployment option selections, and cost management strategies‚Äîdirectly impact your ability to deploy effective solutions while optimizing resource utilization. The following guidelines help you make informed decisions when implementing your inference strategy: 
 
 Cold start latency ‚Äì When using on-demand deployment, you might experience initial cold start latencies, typically lasting several seconds, depending on the model size. This occurs when the deployment hasn‚Äôt received recent traffic and needs to reinitialize compute resources. 
 Regional availability ‚Äì At launch, custom model deployment will be available in US East (N. Virginia) for Amazon Nova models. 
 Quota management ‚Äì Each custom model deployment has specific quotas: 
   
   Tokens per minute (TPM) 
   Requests per minute (RPM) 
   The number of Creating status deployment 
   Total on-demand deployments in a single account 
    
 
Each deployment operates independently within its assigned quota. If a deployment exceeds its TPM or RPM allocation, incoming requests will be throttled. You can request quota increases by submitting a ticket or contacting your AWS account team. 
 
 Choosing between custom model deployment and Provisioned Throughput ‚Äì You can set up inference on a custom model by either creating a custom model deployment (for on-demand usage) or purchasing Provisioned Throughput. The choice depends on the supported Regions and models for each inference option, throughput requirement, and cost considerations. These two options operate independently and can be used simultaneously for the same custom model. 
 Cost management ‚Äì On-demand deployment uses a pay-as-you-go pricing model based on the number of tokens processed during inference. You can use cost allocation tags on your on-demand deployments to track and manage inference costs, allowing better budget tracking and cost optimization through AWS Cost Explorer. 
 
Cleanup 
If you‚Äôve been testing the on-demand deployment feature and don‚Äôt plan to continue using it, it‚Äôs important to clean up your resources to avoid incurring unnecessary costs. Here‚Äôs how to delete using the Amazon Bedrock Console: 
 
 Navigate to your custom model deployment 
 Select the deployment you want to remove 
 Delete the deployment 
 
Here‚Äôs how to delete using the API or SDK: 
To delete a custom model deployment, you can use DeleteCustomModelDeployment API. The following example demonstrates how to delete your custom model deployment: 
 
 # delete deployed custom model deployment
response = bedrock_client.delete_custom_model_deployment(
              customModelDeploymentIdentifier="&lt;trained-model-arn&gt;"
                        ) 
 
Conclusion 
The introduction of on-demand deployment for customized models on Amazon Bedrock represents a significant advancement in making AI model deployment more accessible, cost-effective, and flexible for businesses of all sizes. On-demand deployment offers the following advantages: 
 
 Cost optimization ‚Äì Pay-as-you-go pricing allows you only pay for the compute resources you actually use 
 Operational simplicity ‚Äì Automatic resource management eliminates the need for manual infrastructure provisioning 
 Scalability ‚Äì Seamless handling of variable workloads without upfront capacity planning 
 Flexibility ‚Äì Freedom to choose between on-demand and Provisioned Throughput based on your specific needs 
 
Getting started is straightforward. Begin by completing your model customization through fine-tuning or distillation, then choose on-demand deployment using the AWS Management Console or API. Configure your deployment details, validate model performance in a test environment, and seamlessly integrate into your production workflows. 
Start exploring on-demand deployment for customized models on Amazon Bedrock today! Visit the Amazon Bedrock documentation to begin your model customization journey and experience the benefits of flexible, cost-effective AI infrastructure. For hands-on implementation examples, check out our GitHub repository which contains detailed code samples for customizing Amazon Nova models and evaluating them using on-demand custom model deployment. 
 
About the Authors 
Yanyan Zhang&nbsp;is a Senior Generative AI Data Scientist at Amazon Web Services, where she has been working on cutting-edge AI/ML technologies as a Generative AI Specialist, helping customers use generative AI to achieve their desired outcomes. Yanyan graduated from Texas A&amp;M University with a PhD in Electrical Engineering. Outside of work, she loves traveling, working out, and exploring new things. 
 Sovik Kumar Nath&nbsp;is an AI/ML and Generative AI senior solution architect with AWS. He has extensive experience designing end-to-end machine learning and business analytics solutions in finance, operations, marketing, healthcare, supply chain management, and IoT. He has double masters degrees from the University of South Florida, University of Fribourg, Switzerland, and a bachelors degree from the Indian Institute of Technology, Kharagpur. Outside of work, Sovik enjoys traveling, taking ferry rides, and watching movies. 
Ishan Singh is a Sr. Generative AI Data Scientist at Amazon Web Services, where he helps customers build innovative and responsible generative AI solutions and products. With a strong background in AI/ML, Ishan specializes in building generative AI solutions that drive business value. Outside of work, he enjoys playing volleyball, exploring local bike trails, and spending time with his wife and dog, Beau. 
Koushik Mani is an associate solutions architect at AWS. He had worked as a Software Engineer for two years focusing on machine learning and cloud computing use cases at Telstra. He completed his masters in computer science from University of Southern California. He is passionate about machine learning and generative AI use cases and building solutions. 
Rishabh Agrawal is a Senior Software Engineer working on AI services at AWS. In his spare time, he enjoys hiking, traveling and reading. 
Shreeya Sharma&nbsp;is a Senior Technical Product Manager at AWS, where she has been working on leveraging the power of generative AI to deliver innovative and customer-centric products. Shreeya holds a master‚Äôs degree from Duke University. Outside of work, she loves traveling, dancing, and singing.

‚∏ª