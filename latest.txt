‚úÖ Morning News Briefing ‚Äì September 10, 2025 10:42

üìÖ Date: 2025-09-10 10:42
üè∑Ô∏è Tags: #briefing #ai #publichealth #digitalgov

‚∏ª

üßæ Weather
‚Ä¢ No watches or warnings in effect, Pembroke
  No watches or warnings in effect. No warnings or watches or watches in effect . Watch or warnings are no longer in effect in the U.S. No watches, warnings are in effect for the rest of the day . No watches and warnings are still in effect, but no watches are in place for the day's events . The weather is not expected to be affected by the weather .
‚Ä¢ Current Conditions:  5.4¬∞C
  Temperature: 5.4&deg;C Pressure / Tendency: 102.4 kPa falling Humidity: 98 % Humidity : 98 % Dewpoint: 5 .1&deg:C Wind: SSW calm km/h . Air Quality Health Index: n/a . Weather: Pembroke 6:00 AM EDT Wednesday 10 September 2025 at 6.00
‚Ä¢ Wednesday: Mainly sunny. High 24.
  Fog patches dissipating this morning . Mainly sunny. High 24. Humidex 25. UV index 6 or high . Forecast issued 5:00 AM EDT Wednesday 10 September 2025. Forecast: "Fog patches dissipated this morning. Mainly . sunny. Fog patches . dissipate this morning, high 24. sunny. Humidex 25. sunny . Humide

üåç International News
No updates.

üçÅ Canadian News
No updates.

üá∫üá∏ U.S. Top Stories
‚Ä¢ Trump says he's fighting crime. Illinois Gov. Pritzker sees a power grab before 2026
  Illinois Governor JB Pritzker talks about President Trump's threats to send National Guard troops to Chicago . He also talks about the future of the Democratic Party in Illinois . Inskeep talks with Pritzkers about the state's Democratic Party and the future in the U.S. Democratic Party . Pritzkker: "I'm a Democrat, I'm a Republican, and I
‚Ä¢ Who is Lachlan Murdoch, the anointed media tycoon?
  His position in leading News Corp. and Fox is now secure, as his father ends a dramatic succession battle . Lachlan grew up in New York City but has lived in Australia for much of his adult life . He is the son of Rupert Murdoch, Rupert Murdoch's son-in-law, who has been married for more than a decade . He has been in the news business
‚Ä¢ Trump's control of the D.C. police is due to expire tonight. Then what?
  Trump needs an extension from Congress to control D.C. police for more than 30 days . Some Republican lawmakers are focused on pursuing legislation that would exert power over the city in other ways . Some Republicans are pushing legislation to exert more control of the police force in other areas of the country . The president needs Congress' approval to keep control of police in Washington, DC, for 30 days
‚Ä¢ You're more likely to reach for that soda when it's hot outside
  People drink more sugary beverages when it's hot, researchers found . That impact could grow as climate change raises the world's temperatures . The impact of climate change is expected to grow as more people drink more when it comes to hot, the study found . The study was published in the journal Nature Resolutions, published by the University of Cambridge, England, on Monday, July 4 .
‚Ä¢ 33 million voters have been run through a Trump administration citizenship check
  Tens of millions of U.S. voters have had their information run through the tool . Little has been made public about the tool's accuracy or data security . The tool is a striking portion of the public, considering little has been revealed about the accuracy or security of the data security of it's data . It has been used by millions of Americans, but little is known about its accuracy

üß† Artificial Intelligence
No updates.

üíª Digital Strategy
‚Ä¢ Get paid like a prime minister to tame Home Office IT chaos
  Department dangles ¬£160K salary for CDIO to wrangle legacy systems, failed projects, and ¬£1.8B budget . UK Home Office is on the hunt for a chief digital and innovation officer (CDIO) with an advertised salary not far off from the prime minister's . Home Office has a rich track record of failing IT projects, including legacy systems and failed projects .
‚Ä¢ Flu jab email mishap exposes hundreds of students' personal data
  A clumsy data breach has affected hundreds of children at a Birmingham secondary school . One parent expressed concern for their child's safety . Hundreds of children were affected by the data breach at the Birmingham school . The school has been hit by a data breach that has left hundreds of pupils at risk of losing their identity . The data breach is believed to have occurred at the school in Birmingham, Birmingham .
‚Ä¢ Johnson, Cummings met Thiel months before Palantir won NHS pandemic role
  Meeting with former UK PM and his chief advisor withheld from official records . Meeting with Palantir co-founder and chairman Peter Thiel in 2019 months before company landed a key role in UK's COVID-19 response, according to papers seen by The Guardian . Meeting was months before US spy-tech company landed role in the UK's response to the COVID19 response . Palant
‚Ä¢ KDE Linux and FreeBSD hit alpha and ‚Äì surprise ‚Äì fan fave Pop_OS nearly at beta
  The Northern hemisphere is moving into autumn and FOSS vendors are falling over themselves in their efforts to get new versions out for the season . It's the season of FOSS fruitfulness as juicy goodness falls from the branch . The season of autumn is falling over with new versions of software for the first time in the fall . The FOSS community is getting ready to release new versions for the
‚Ä¢ Cybercrooks ripped the wheels off at Jaguar Land Rover. Here's how not to get taken for a ride
  IT systems across multiple sites have been offline for over a week after what the company described as a "severe disruption" Jaguar Land Rover (JLR) is the latest UK household name to fall victim to a major cyberattack . The company described the attack as a 'severe disruption' in what it called a "severe disruption" JLR: "I'm not sure who has access

üè• Public Health
No updates.

üî¨ Science
‚Ä¢ Home-based care improves blood pressure control
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Protests are infectious: mapping rural unrest in Revolutionary France
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Enhancing Mindfulness-Based Cognitive Therapy in a Virtual Reality: A Prospective Interventional Study
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Changes in cholera burden across Africa
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations

üßæ Government & Policy
No updates.

üèõÔ∏è Enterprise Architecture & IT Governance
No updates.

ü§ñ AI & Emerging Tech
‚Ä¢ Adapting to new threats with proactive risk management
  In July 2024, a botched update to the software defenses managed by cybersecurity firm CrowdStrike caused more than 8 million Windows systems to fail . The outage is estimated to have caused direct losses of more than $5 billion to Fortune 500 companies . As organizations become ever more interconnected, the expanding surface of networks and the rapid adoption of technologies like AI are exposing new vulnerabilities . Cyberattacks are also becoming increasingly sophisticated and damaging as AI-driven malware and malware-as-a-service platforms turbocharge attacks .
‚Ä¢ The Download: meet our AI innovators, and what happens when therapists use AI covertly
  This is today&#8217;s edition of¬†The Download,¬†our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



Meet the AI honorees on our 35 Innovators Under 35 list for 2025



Each year, we select 35 outstanding individuals under the age of 35 who are using technology to tackle tough problems in their respective fields.Our AI honorees include people who steer model development at Silicon Valley‚Äôs biggest tech firms and academic researchers who develop new techniques to improve AI‚Äôs performance.Check out all of our AI innovators here, and the full list‚Äîincluding our innovator of the year‚Äîhere.







How Yichao ‚ÄúPeak‚Äù Ji became a global AI app hitmaker



When Yichao Ji‚Äîalso known as ‚ÄúPeak‚Äù‚Äîappeared in a launch video for Manus in March, he didn‚Äôt expect it to go viral. Speaking in fluent English, the 32-year-old introduced the AI agent built by Chinese startup Butterfly Effect, where he serves as chief scientist.&nbsp;



The video was not an elaborate production but something about Ji‚Äôs delivery, and the vision behind the product, cut through the noise. The product, then still an early preview available only through invite codes, spread across the Chinese internet to the world in a matter of days. Within a week of its debut, Manus had attracted a waiting list of around 2 million people.Despite his relative youth, Ji has over a decade of experience building products that merge technical complexity with real-world usability. That earned him credibility‚Äîand put him at the forefront of a rising class of Chinese technologists with global ambitions. Read the full story.



‚ÄîCaiwei Chen







Help! My therapist is secretly using ChatGPT



In Silicon Valley‚Äôs imagined future, AI models are so empathetic that we‚Äôll use them as therapists. They‚Äôll provide mental-health care for millions, unimpeded by the pesky requirements for human counselors, like the need for graduate degrees, malpractice insurance, and sleep. Down here on Earth, something very different has been happening.&nbsp;



Last week, we published a story about people finding out that their therapists were secretly using ChatGPT during sessions. In some cases it wasn‚Äôt subtle; one therapist accidentally shared his screen during a virtual appointment, allowing the patient to see his own private thoughts being typed into ChatGPT in real time.As the writer of the story, Laurie Clarke, points out, it‚Äôs not a total pipe dream that AI could be therapeutically useful. But the secretive use by therapists of AI models that are not vetted for mental health is something very different. James O‚ÄôDonnell, our senior AI reporter, had a conversation with Clarke to hear more about what she found.



This story originally appeared in The Algorithm, our weekly newsletter on AI. To get stories like this in your inbox first, sign up here.







What‚Äôs next in tech: the breakthroughs that matter



Some technologies reshape industries, whether we‚Äôre ready or not.Join us for our next LinkedIn Live event on September 10 as our editorial team explores the breakthroughs defining this moment and the ones on the horizon that demand our attention.¬†



From quantum computing to humanoid robotics, AI agents to climate tech, we‚Äôll explore the innovations that excite us, the challenges they may bring, and why they‚Äôre worth watching now. It kicks off at 12.30pm ET tomorrow‚Äîregister here to join us.







The must-reads



I‚Äôve combed the internet to find you today‚Äôs most fun/important/scary/fascinating stories about technology.



1 The US is abandoning its international push against disinformation¬†The State Department will no longer collaborate with Europe to combat malicious information spread by foreign governments. (FT $)+ It comes as Russia is increasing its efforts to interfere overseas. (NYT $)



2 The judge overseeing Anthropic‚Äôs copyright case isn‚Äôt happyJudge William Alsup says a $1.5 billion out-of-court settlement may not be in the authors&#8217; best interests. (Bloomberg $)



3 WhatsApp‚Äôs former head of security is suing MetaAttaullah Baig is accusing the company of failing to protect user data. (WP $)+ He claims he uncovered systemic security failures, but was ignored. (Bloomberg $)+ Meta maintains that Baig was dismissed for poor performance, not whistleblowing. (NYT $)



4 DOGE‚Äôs acting head is urging the US government to start hiring again¬†Following months of widespread firings and resignations. (Fast Company $)+ How DOGE wreaked havoc in Social Security. (ProPublica)+ DOGE‚Äôs tech takeover threatens the safety and stability of our critical data. (MIT Technology Review)



5 OpenAI is weighing up leaving CaliforniaIt‚Äôs worried that state regulators could derail its efforts to convert to a for-profit entity. (WSJ $)+ Rival Anthropic is backing California governor Gavin Newsom‚Äôs AI bill. (Politico)



6 ICE spends millions on facial recognition techIn an effort to pinpoint people it suspects have assaulted officers. (404 Media)+ The Supreme Court has given ICE the go-ahead to target people based on race. (Vox)+ ICE directors were told to triple their daily arrests for undocumented immigrants. (NY Mag $)



7 AI researchers are training AI to replace themThey‚Äôre recording every detail of their working days to help AI grasp their jobs. (The Information $)+ People are worried that AI will take everyone‚Äôs jobs. We‚Äôve been here before. (MIT Technology Review)



8 What comes after the smartphone?The rise of AI agents means we may not be staring at glass slabs forever. (NYT $)+ What‚Äôs next for smart glasses. (MIT Technology Review)



9 Social media‚Äôs obsession with ‚Äòlocking in‚Äô needs to dieHustle culture and maximizing productivity at all costs are the aims of the game. (Insider $)



10 What it‚Äôs like to receive a massage from a robotWhile it may not be quite as relaxing, it‚Äôs relatively cheap. (The Guardian)+ Will we ever trust robots? (MIT Technology Review)







Quote of the day



‚ÄúIt was hell on Earth.‚Äù



‚ÄîDuncan Okindo, who was enslaved in a Myanmar cyberscam compound and beaten for missing his targets, tells the Guardian about his harrowing experience.







One more thing







AI means the end of internet search as we‚Äôve known itWe all know what it means, colloquially, to google something. You pop a few words in a search box and in return get a list of blue links to the most relevant results. Fundamentally, it‚Äôs just fetching information that‚Äôs already out there on the internet and showing it to you, in a structured way.But all that is up for grabs. We are at a new inflection point. The biggest change to the way search engines deliver information to us since the 1990s is happening right now, thanks to generative AI.Not everyone is excited for the change. Publishers are completely freaked out. And people are also worried about what these new LLM-powered results will mean for our fundamental shared reality. Read the full story.



‚ÄîMat Honan







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)



+ Stephen King‚Äôs list of favorite movies doesn‚Äôt feature a whole lot of horror.+ Tune into a breathtaking livestream of Earth, beamed live from the International Space Station.+ Rodent thumbnails are way more important than I gave them credit for + Mark our words, actor Wagner Moura is going to be the next big thing.
‚Ä¢ Help! My therapist is secretly using ChatGPT
  In Silicon Valley‚Äôs imagined future, AI models are so empathetic that we‚Äôll use them as therapists. They‚Äôll provide mental-health care for millions, unimpeded by the pesky requirements for human counselors, like the need for graduate degrees, malpractice insurance, and sleep. Down here on Earth, something very different has been happening.&nbsp;



Last week, we published a story about people finding out that their therapists were secretly using ChatGPT during sessions. In some cases it wasn‚Äôt subtle; one therapist accidentally shared his screen during a virtual appointment, allowing the patient to see his own private thoughts being typed into ChatGPT in real time. The model then suggested responses that his therapist parroted.&nbsp;



It‚Äôs my favorite AI story as of late, probably because it captures so well the chaos that can unfold when people actually use AI the way tech companies have all but told them to.



As the writer of the story, Laurie Clarke, points out, it‚Äôs not a total pipe dream that AI could be therapeutically useful. Early this year, I wrote about the first clinical trial of an AI bot built specifically for therapy. The results were promising! But the secretive use by therapists of AI models that are not vetted for mental health is something very different. I had a conversation with Clarke to hear more about what she found.&nbsp;



I have to say, I was really fascinated that people called out their therapists after finding out they were covertly using AI. How did you interpret the reactions of these therapists? Were they trying to hide it?



In all the cases mentioned in the piece, the therapist hadn‚Äôt provided prior disclosure of how they were using AI to their patients. So whether or not they were explicitly trying to conceal it, that‚Äôs how it ended up looking when it was discovered. I think for this reason, one of my main takeaways from writing the piece was that therapists should absolutely disclose when they‚Äôre going to use AI and how (if they plan to use it). If they don‚Äôt, it raises all these really uncomfortable questions for patients when it‚Äôs uncovered and risks irrevocably damaging the trust that‚Äôs been built.





In the examples you‚Äôve come across, are therapists turning to AI simply as a time-saver? Or do they think AI models can genuinely give them a new perspective on what‚Äôs bothering someone?



Some see AI as a potential time-saver. I heard from a few therapists that notes are the bane of their lives. So I think there is some interest in AI-powered tools that can support this. Most I spoke to were very skeptical about using AI for advice on how to treat a patient. They said it would be better to consult supervisors or colleagues, or case studies in the literature. They were also understandably very wary of inputting sensitive data into these tools.



There is some evidence AI can deliver more standardized, &#8220;manualized&#8221; therapies like CBT [cognitive behavioral therapy] reasonably effectively. So it‚Äôs possible it could be more useful for that. But that is AI specifically designed for that purpose, not general-purpose tools like ChatGPT.



What happens if this goes awry? What attention is this getting from ethics groups and lawmakers?



At present, professional bodies like the American Counseling Association advise against using AI tools to diagnose patients. There could also be more stringent regulations preventing this in future. Nevada and Illinois, for example, have recently passed laws prohibiting the use of AI in therapeutic decision-making. More states could follow.



OpenAI‚Äôs Sam Altman said last month that ‚Äúa lot of people effectively use ChatGPT as a sort of therapist,‚Äù and that to him, that‚Äôs a good thing. Do you think tech companies are overpromising on AI‚Äôs ability to help us?



I think that tech companies are subtly encouraging this use of AI because clearly it‚Äôs a route through which some people are forming an attachment to their products. I think the main issue is that what people are getting from these tools isn‚Äôt really ‚Äútherapy‚Äù by any stretch. Good therapy goes far beyond being soothing and validating everything someone says. I‚Äôve never in my life looked forward to a (real, in-person) therapy session. They‚Äôre often highly uncomfortable, and even distressing. But that‚Äôs part of the point. The therapist should be challenging you and drawing you out and seeking to understand you. ChatGPT doesn‚Äôt do any of these things.&nbsp;



Read the full story from Laurie Clarke.&nbsp;



This story originally appeared in The Algorithm, our weekly newsletter on AI. To get stories like this in your inbox first,¬†sign up here.
‚Ä¢ AI is changing the grid. Could it help more than it harms?
  The rising popularity of AI is driving an increase in electricity demand so significant it has the potential to reshape our grid. Energy consumption by data centers has gone up by 80% from 2020 to 2025 and is likely to keep growing. Electricity prices are already rising, especially in places where data centers are most concentrated.&nbsp;



Yet many people, especially in Big Tech, argue that AI will be, on balance, a positive force for the grid. They claim that the technology could help get more clean power online faster, run our power system more efficiently, and predict and prevent failures that cause blackouts.&nbsp;







This story is a part of¬†MIT Technology Review‚Äôs series ‚ÄúPower Hungry: AI and our energy future,‚Äù on the energy demands and carbon costs of the artificial-intelligence revolution.







There are early examples where AI is helping already, including AI tools that utilities are using to help forecast supply and demand. The question is whether these big promises will be realized fast enough to outweigh the negative effects of AI on local grids and communities.&nbsp;



A delicate balance



One area where AI is already being used for the grid is in forecasting, says Utkarsha Agwan, a member of the nonprofit group Climate Change AI.



Running the grid is a balancing act: Operators have to understand how much electricity demand there is and turn on the right combination of power plants to meet it. They optimize for economics along the way, choosing the sources that will keep prices lowest for the whole system.



That makes it necessary to look ahead hours and in some cases days. Operators consider factors such as historical data (holidays often see higher demand) and the weather (a hot day means more air conditioners sucking up power). These predictions also consider what level of supply is expected from intermittent sources like solar panels.



There‚Äôs little risk in using AI tools in forecasting; it‚Äôs often not as time sensitive as other applications, which can require reactions within seconds or even milliseconds. A grid operator might use a forecast to determine which plants will need to turn on. Other groups might run their own forecasts as well, using AI tools to decide how to staff a plant, for example. The tools also can‚Äôt physically control anything. Rather, they can be used alongside more conventional methods to provide more data.&nbsp;&nbsp;





Today, grid operators make a lot of approximations to model the grid, because the system is so incredibly complex that it‚Äôs impossible to truly know what‚Äôs going on in every place at every time. Not only are there a whole host of power plants and consumers to think about, but there are considerations like making sure power lines don‚Äôt get overloaded.



Working with those estimates can lead to some inefficiencies, says Kyri Baker, a professor at the University of Colorado Boulder. Operators tend to generate a bit more electricity than the system uses, for example. Using AI to create a better model could reduce some of those losses and allow operators to make decisions about how to control infrastructure in real time to reach a closer match of supply and demand.



She gives the example of a trip to the airport. Imagine there‚Äôs a route you know will get you there in about 45 minutes. There might be another, more complicated route that could save you some time in ideal conditions‚Äîbut you‚Äôre not sure whether it‚Äôs better on any particular day. What the grid does now is the equivalent of taking the reliable route.



‚ÄúSo that‚Äôs the gap that AI can help close. We can solve this more complex problem, fast enough and reliably enough that we can possibly use it and shave off emissions,‚Äù Baker says.&nbsp;



In theory, AI could be used to operate the grid entirely without human intervention. But that work is largely still in the research phase. Grid operators are running some of the most critical infrastructure in this country, and the industry is hesitant to mess with something that‚Äôs already working, Baker says. If this sort of technology is ever used in grid operations, there will still be humans in the loop to help make decisions, at least when it‚Äôs first deployed.&nbsp;&nbsp;



Planning ahead



Another fertile area for AI is planning future updates to the grid. Building a power plant can take a very long time‚Äîthe typical time from an initial request to commercial operation in the US is roughly four years. One reason for the lengthy wait is that new power plants have to demonstrate how they might affect the rest of the grid before they can connect.&nbsp;



An interconnection study examines whether adding a new power plant of a particular type in a particular place would require upgrades to the grid to prevent problems. After regulators and utilities determine what upgrades might be needed, they estimate the cost, and the energy developer generally foots the bill.&nbsp;



Today, those studies can take months. They involve trying to understand an incredibly complicated system, and because they rely on estimates of other existing and proposed power plants, only a few can happen in an area at any given time. This has helped create the years-long interconnection queue, a long line of plants waiting for their turn to hook up to the grid in markets like the US and Europe. The vast majority of projects in the queue today are renewables, which means there‚Äôs clean power just waiting to come online.&nbsp;



AI could help speed this process, producing these reports more quickly. The Midcontinent Independent System Operator, a grid operator that covers 15 states in the central US, is currently working with a company called Pearl Street to help automate these reports.



AI won‚Äôt be a cure-all for grid planning; there are other steps to clearing the interconnection queue, including securing the necessary permits. But the technology could help move things along. ‚ÄúThe sooner we can speed up interconnection, the better off we‚Äôll be,‚Äù says Rob Gramlich, president of Grid Strategies, a consultancy specializing in transmission and power markets.



There‚Äôs a growing list of other potential uses for AI on the grid and in electricity generation. The technology could monitor and plan ahead for failures in equipment ranging from power lines to gear boxes. Computer vision could help detect everything from wildfires to faulty lines. AI could also help balance supply and demand in virtual power plants, systems of distributed resources like EV chargers or smart water heaters.&nbsp;



While there are early examples of research and pilot programs for AI from grid planning to operation, some experts are skeptical that the technology will deliver at the level some are hoping for. ‚ÄúIt‚Äôs not that AI has not had some kind of transformation on power systems,‚Äù Climate Change AI‚Äôs Agwan says. ‚ÄúIt‚Äôs that the promise has always been bigger, and the hope has always been bigger.‚Äù



Some places are already seeing higher electricity prices because of power needs from data centers. The situation is likely to get worse. Electricity demand from data centers is set to double by the end of the decade, reaching 945 terawatt-hours, roughly the annual demand from the entire country of Japan.&nbsp;



The infrastructure growth needed to support AI load growth has outpaced the promises of the technology, ‚Äúby quite a bit,‚Äù says Panayiotis Moutis, an assistant professor of electrical engineering at the City College of New York. Higher bills caused by the increasing energy needs of AI aren‚Äôt justified by existing ways of using the technology for the grid, he says.¬†



‚ÄúAt the moment, I am very hesitant to lean on the side of AI being a silver bullet,‚Äù Moutis says.&nbsp;



Correction: This story has been updated to correct Moutis&#8217;s affiliation.
‚Ä¢ Three big things we still don‚Äôt know about AI‚Äôs energy burden
  Earlier this year, when my colleague Casey Crownhart and I spent six months researching the climate and energy burden of AI, we came to see one number in particular as our white whale: how much energy the leading AI models, like ChatGPT or Gemini, use up when generating a single response.&nbsp;



This fundamental number remained elusive even as the scramble to power AI escalated to the White House and the Pentagon, and as projections showed that in three years AI could use as much electricity as 22% of all US households.&nbsp;



The problem with finding that number, as we explain in our piece published in May, was that AI companies are the only ones who have it. We pestered Google, OpenAI, and Microsoft, but each company refused to provide its figure. Researchers we spoke to who study AI‚Äôs impact on energy grids compared it to trying to measure the fuel efficiency of a car without ever being able to drive it, making guesses based on rumors of its engine size and what it sounds like going down the highway.







This story is a part of&nbsp;MIT Technology Review‚Äôs series ‚ÄúPower Hungry: AI and our energy future,‚Äù on the energy demands and carbon costs of the artificial-intelligence revolution.







But then this summer, after we published, a strange thing started to happen. In June, OpenAI‚Äôs Sam Altman wrote that an average ChatGPT query uses 0.34 watt-hours of energy. In July, the French AI startup Mistral didn‚Äôt publish a number directly but released an estimate of the emissions generated. In August, Google revealed that answering a question to Gemini uses about 0.24 watt-hours of energy. The figures from Google and OpenAI were similar to what Casey and I estimated for medium-size AI models.&nbsp;



So with this newfound transparency, is our job complete? Did we finally harpoon our white whale, and if so, what happens next for people studying the climate impact of AI? I reached out to some of our old sources, and some new ones, to find out.









The numbers are vague and chat-only



The first thing they told me is that there‚Äôs a lot missing from the figures tech companies published this summer.&nbsp;



OpenAI‚Äôs number, for example, did not appear in a detailed technical paper but rather in a blog post by Altman that leaves lots of unanswered questions, such as which model he was referring to, how the energy use was measured, and how much it varies. Google‚Äôs figure, as Crownhart points out, refers to the median amount of energy per query, which doesn‚Äôt give us a sense of the more energy-demanding Gemini responses, like when it uses a reasoning model to ‚Äúthink‚Äù through a hard problem or generates a really long response.&nbsp;



The numbers also refer only to interactions with chatbots, not the other ways that people are becoming increasingly reliant on generative AI.&nbsp;



‚ÄúAs video and image becomes more prominent and used by more and more people, we need the numbers from different modalities and how they measure up,‚Äù says Sasha Luccioni, AI and climate lead at the AI platform Hugging Face.&nbsp;



This is also important because the figures for asking a question to a chatbot are, as expected, undoubtedly small‚Äîthe same amount of electricity used by a microwave in just seconds. That‚Äôs part of the reason AI and climate researchers don‚Äôt suggest that any one individual‚Äôs AI use creates a significant climate burden.&nbsp;



A full accounting of AI‚Äôs energy demands‚Äîone that goes beyond what‚Äôs used to answer an individual query to help us understand its full net impact on the climate‚Äîwould require application-specific information on how all this AI is being used. Ketan Joshi, an analyst for climate and energy groups, acknowledges that researchers don‚Äôt usually get such specific information from other industries but says it might be justified in this case.



‚ÄúThe rate of data center growth is inarguably unusual,‚Äù Joshi says. ‚ÄúCompanies should be subject to significantly more scrutiny.‚Äù





We have questions about energy efficiency



Companies making billion-dollar investments into AI have struggled to square this growth in energy demand with their sustainability goals. In May, Microsoft said that its emissions have soared by over 23% since 2020, owing largely to AI, while the company has promised to be carbon negative by 2030. ‚ÄúIt has become clear that our journey towards being carbon negative is a marathon, not a sprint,‚Äù Microsoft wrote.



Tech companies often justify this emissions burden by arguing that soon enough, AI itself will unlock efficiencies that will make it a net positive for the climate. Perhaps the right AI system, the thinking goes, could design more efficient heating and cooling systems for a building, or help discover the minerals required for electric-vehicle batteries.&nbsp;



But there are no signs that AI has been usefully used to do these things yet. Companies have shared anecdotes about using AI to find methane emission hot spots, for example, but they haven‚Äôt been transparent enough to help us know if these successes outweigh the surges in electricity demand and emissions that Big Tech has produced in the AI boom. In the meantime, more data centers are planned, and AI‚Äôs energy demand continues to rise and rise.&nbsp;



The ‚Äòbubble‚Äô question



One of the big unknowns in the AI energy equation is whether society will ever adopt AI at the levels that figure into tech companies‚Äô plans. OpenAI has said that ChatGPT receives 2.5 billion prompts per day. It‚Äôs possible that this number, and the equivalent numbers for other AI companies, will continue to soar in the coming years. Projections released last year by the Lawrence Berkeley National Laboratory suggest that if they do, AI alone could consume as much electricity annually as 22% of all US households by 2028.



But this summer also saw signs of a slowdown that undercut the industry‚Äôs optimism. OpenAI‚Äôs launch of GPT-5 was largely considered a flop, even by the company itself, and that flop led critics to wonder if AI may be hitting a wall. When a group at MIT found that 95% of businesses are seeing no return on their massive AI investments, stocks floundered. The expansion of AI-specific data centers might be an investment that‚Äôs hard to recoup, especially as revenues for AI companies remain elusive.&nbsp;



One of the biggest unknowns about AI‚Äôs future energy burden isn‚Äôt how much a single query consumes, or any other figure that can be disclosed. It‚Äôs whether demand will ever reach the scale companies are building for or whether the technology will collapse under its own hype. The answer will determine whether today‚Äôs buildout becomes a lasting shift in our energy system or a short-lived spike.

üîí Cybersecurity & Privacy
‚Ä¢ Microsoft Patch Tuesday, September 2025 Edition
  Microsoft Corp. today issued security updates to fix more than 80 vulnerabilities in its Windows operating systems and software. There are no known &#8220;zero-day&#8221; or actively exploited vulnerabilities in this month&#8217;s bundle from Redmond, which nevertheless includes patches for 13 flaws that earned Microsoft&#8217;s most-dire &#8220;critical&#8221; label. Meanwhile, both Apple and Google recently released updates to fix zero-day bugs in their devices.

Microsoft assigns security flaws a &#8220;critical&#8221; rating when malware or miscreants can exploit them to gain remote access to a Windows system with little or no help from users. Among the more concerning critical bugs quashed this month is CVE-2025-54918. The problem here resides with Windows NTLM, or NT LAN Manager, a suite of code for managing authentication in a Windows network environment.
Redmond rates this flaw as &#8220;Exploitation More Likely,&#8221; and although it is listed as a privilege escalation vulnerability, Kev Breen at Immersive says this one is actually exploitable over the network or the Internet.
&#8220;From Microsoft‚Äôs limited description, it appears that if an attacker is able to send specially crafted packets over the network to the target device, they would have the ability to gain SYSTEM-level privileges on the target machine,&#8221; Breen said. &#8220;The patch notes for this vulnerability state that &#8216;Improper authentication in Windows NTLM allows an authorized attacker to elevate privileges over a network,&#8217; suggesting an attacker may already need to have access to the NTLM hash or the user&#8217;s credentials.&#8221;
Breen said another patch &#8212; CVE-2025-55234, a 8.8 CVSS-scored flaw affecting the Windows SMB client for sharing files across a network &#8212; also is listed as privilege escalation bug but is likewise remotely exploitable. This vulnerability was publicly disclosed prior to this month.
&#8220;Microsoft says that an attacker with network access would be able to perform a replay attack against a target host, which could result in the attacker gaining additional privileges, which could lead to code execution,&#8221; Breen noted.
CVE-2025-54916 is an &#8220;important&#8221; vulnerability in Windows NTFS &#8212; the default filesystem for all modern versions of Windows &#8212; that can lead to remote code execution. Microsoft likewise thinks we are more than likely to see exploitation of this bug soon: The last time Microsoft patched an NTFS bug was in March 2025 and it was already being exploited in the wild as a zero-day.
&#8220;While the title of the CVE says &#8216;Remote Code Execution,&#8217; this exploit is not remotely exploitable over the network, but instead needs an attacker to either have the ability to run code on the host or to convince a user to run a file that would trigger the exploit,&#8221; Breen said. &#8220;This is commonly seen in social engineering attacks, where they send the user a file to open as an attachment or a link to a file to download and run.&#8221;
Critical and remote code execution bugs tend to steal all the limelight, but Tenable Senior Staff Research Engineer Satnam Narang notes that nearly half of all vulnerabilities fixed by Microsoft this month are privilege escalation flaws that require an attacker to have gained access to a target system first before attempting to elevate privileges.
&#8220;For the third time this year, Microsoft patched more elevation of privilege vulnerabilities than remote code execution flaws,&#8221; Narang observed.
On Sept. 3, Google fixed two flaws that were detected as exploited in zero-day attacks, including¬†CVE-2025-38352, an elevation of privilege in the Android kernel, and CVE-2025-48543, also an elevation of privilege problem in the Android Runtime component.
Also, Apple recently patched its seventh zero-day (CVE-2025-43300) of this year. It was part of an exploit chain used along with a vulnerability in the WhatsApp (CVE-2025-55177) instant messenger to hack Apple devices. Amnesty International reports that the two zero-days have been used in &#8220;an advanced spyware campaign&#8221; over the past 90 days. The issue is fixed in iOS 18.6.2, iPadOS 18.6.2, iPadOS 17.7.10, macOS Sequoia 15.6.1, macOS Sonoma 14.7.8, and macOS Ventura 13.7.8.
The SANS Internet Storm Center has a clickable breakdown of each individual fix from Microsoft, indexed by severity and CVSS score. Enterprise Windows admins involved in testing patches before rolling them out should keep an eye on askwoody.com, which often has the skinny on wonky updates.
AskWoody also reminds us that we&#8217;re now just two months out from Microsoft discontinuing free security updates for Windows 10 computers. For those interested in safely extending the lifespan and usefulness of these older machines, check out last month&#8217;s Patch Tuesday coverage for a few pointers.
As ever, please don&#8217;t neglect to back up your data (if not your entire system) at regular intervals, and feel free to sound off in the comments if you experience problems installing any of these fixes.
‚Ä¢ 18 Popular Code Packages Hacked, Rigged to Steal Crypto
  At least 18 popular JavaScript code packages that are collectively downloaded more than two billion times each week were briefly compromised with malicious software today, after a developer involved in maintaining the projects was phished. The attack appears to have been quickly contained and was narrowly focused on stealing cryptocurrency. But experts warn that a similar attack with a slightly more nefarious payload could lead to a disruptive malware outbreak that is far more difficult to detect and restrain.
This phishing email lured a developer into logging in at a fake NPM website and supplying a one-time token for two-factor authentication. The phishers then used that developer&#8217;s NPM account to add malicious code to at least 18 popular JavaScript code packages.
Aikido is a security firm in Belgium that monitors new code updates to major open-source code repositories, scanning any code updates for suspicious and malicious code. In a blog post published today, Aikido said its systems found malicious code had been added to at least 18 widely-used code libraries available on NPM (short for) &#8220;Node Package Manager,&#8221; which acts as a central hub for JavaScript development and the latest updates to widely-used JavaScript components.
JavaScript is a powerful web-based scripting language used by countless websites to build a more interactive experience with users, such as entering data into a form. But there&#8217;s no need for each website developer to build a program from scratch for entering data into a form when they can just reuse already existing packages of code at NPM that are specifically designed for that purpose.
Unfortunately, if cybercriminals manage to phish NPM credentials from developers, they can introduce malicious code that allows attackers to fundamentally control what people see in their web browser when they visit a website that uses one of the affected code libraries.
According to Aikido, the attackers injected a piece of code that silently intercepts cryptocurrency activity in the browser, &#8220;manipulates wallet interactions, and rewrites payment destinations so that funds and approvals are redirected to attacker-controlled accounts without any obvious signs to the user.&#8221;
&#8220;This malware is essentially a browser-based interceptor that hijacks both network traffic and application APIs,&#8221; Aikido researcher Charlie Eriksen wrote. &#8220;What makes it dangerous is that it operates at multiple layers: Altering content shown on websites, tampering with API calls, and manipulating what users‚Äô apps believe they are signing. Even if the interface looks correct, the underlying transaction can be redirected in the background.&#8221;
Aikido said it used the social network Bsky to notify the affected developer, Josh Junon, who quickly replied that he was aware of having just been phished. The phishing email that Junon fell for was part of a larger campaign that spoofed NPM and told recipients they were required to update their two-factor authentication (2FA) credentials. The phishing site mimicked NPM&#8217;s login page, and intercepted Junon&#8217;s credentials and 2FA token. Once logged in, the phishers then changed the email address on file for Junon&#8217;s NPM account, temporarily locking him out.
Aikido notified the maintainer on Bluesky, who replied at 15:15 UTC that he was aware of being compromised, and starting to clean up the compromised packages.
Junon also issued a mea culpa on HackerNews, telling the community&#8217;s coder-heavy readership, &#8220;Hi, yep I got pwned.&#8221;
&#8220;It looks and feels a bit like a targeted attack,&#8221; Junon wrote. &#8220;Sorry everyone, very embarrassing.&#8221;
Philippe Caturegli, &#8220;chief hacking officer&#8221; at the security consultancy Seralys, observed that the attackers appear to have registered their spoofed website &#8212; npmjs[.]help &#8212; just two days before sending the phishing email. The spoofed website used services from dnsexit[.]com, a &#8220;dynamic DNS&#8221; company that also offers &#8220;100% free&#8221; domain names that can instantly be pointed at any IP address controlled by the user.
Junon&#8217;s mea cupla on Hackernews today listed the affected packages.
Caturegli said it&#8217;s remarkable that the attackers in this case were not more ambitious or malicious with their code modifications.
&#8220;The crazy part is they compromised billions of websites and apps just to target a couple of cryptocurrency things,&#8221; he said. &#8220;This was a supply chain attack, and it could easily have been something much worse than crypto harvesting.&#8221;
Aikido&#8217;s Eriksen agreed, saying countless websites dodged a bullet because this incident was handled in a matter of hours. As an example of how these supply-chain attacks can escalate quickly, Eriksen pointed to another compromise of an NPM developer in late August that added malware to &#8220;nx,&#8221; an open-source code development toolkit with as many as six million weekly downloads.
In the nx compromise, the attackers introduced code that scoured the user&#8217;s device for authentication tokens from programmer destinations like GitHub and NPM, as well as SSH and API keys. But instead of sending those stolen credentials to a central server controlled by the attackers, the malicious code created a new public repository in the victim&#8217;s GitHub account, and published the stolen data there for all the world to see and download.
Eriksen said coding platforms like GitHub and NPM should be doing more to ensure that any new code commits for broadly-used packages require a higher level of attestation that confirms the code in question was in fact submitted by the person who owns the account, and not just by that person&#8217;s account.
&#8220;More popular packages should require attestation that it came through trusted provenance and not just randomly from some location on the Internet,&#8221; Eriksen said. &#8220;Where does the package get uploaded from, by GitHub in response to a new pull request into the main branch, or somewhere else? In this case, they didn&#8217;t compromise the target&#8217;s GitHub account. They didn&#8217;t touch that. They just uploaded a modified version that didn&#8217;t come where it&#8217;s expected to come from.&#8221;
Eriksen said code repository compromises can be devastating for developers, many of whom end up abandoning their projects entirely after such an incident.
&#8220;It&#8217;s unfortunate because one thing we&#8217;ve seen is people have their projects get compromised and they say, &#8216;You know what, I don&#8217;t have the energy for this and I&#8217;m just going to deprecate the whole package,'&#8221; Eriksen said.
Kevin Beaumont, a frequently quoted security expert who writes about security incidents at the blog doublepulsar.com, has been following this story closely today in frequent updates to his account on Mastodon. Beaumont said the incident is a reminder that much of the planet still depends on code that is ultimately maintained by an exceedingly small number of people who are mostly overburdened and under-resourced.
&#8220;For about the past 15 years every business has been developing apps by pulling in 178 interconnected libraries written by 24 people in a shed in Skegness,&#8221; Beaumont wrote on Mastodon. &#8220;For about the past 2 years orgs have been buying AI vibe coding tools, where some exec screams &#8216;make online shop&#8217; into a computer and 389 libraries are added and an app is farted out. The output = if you want to own the world&#8217;s companies, just phish one guy in Skegness.&#8221;
Image: https://infosec.exchange/@GossiTheDog@cyberplace.social.
Aikido recently launched a product that aims to help development teams ensure that every code library used is checked for malware before it can be used or installed. Nicholas Weaver, a researcher with the International Computer Science Institute, a nonprofit in Berkeley, Calif., said Aikido&#8217;s new offering exists because many organizations are still one successful phishing attack away from a supply-chain nightmare.
Weaver said these types of supply-chain compromises will continue as long as people responsible for maintaining widely-used code continue to rely on phishable forms of 2FA.
&#8220;NPM should only support phish-proof authentication,&#8221; Weaver said, referring to physical security keys that are phish-proof &#8212; meaning that even if phishers manage to steal your username and password, they still can&#8217;t log in to your account without also possessing that physical key.
&#8220;All critical infrastructure needs to use phish-proof 2FA, and given the dependencies in modern software, archives such as NPM are absolutely critical infrastructure,&#8221; Weaver said. &#8220;That NPM does not require that all contributor accounts use security keys or similar 2FA methods should be considered negligence.&#8221;
‚Ä¢ GOP Cries Censorship Over Spam Filters That Work
  The chairman of the Federal Trade Commission (FTC) last week sent a letter to Google&#8217;s CEO demanding to know why Gmail was blocking messages from Republican senders while allegedly failing to block similar missives supporting Democrats. The letter followed media reports accusing Gmail of disproportionately flagging messages from the GOP fundraising platform WinRed and sending them to the spam folder. But according to experts who track daily spam volumes worldwide, WinRed&#8217;s messages are getting blocked more because its methods of blasting email are increasingly way more spammy than that of ActBlue, the fundraising platform for Democrats.
Image: nypost.com
On Aug. 13, The New York Post ran an &#8220;exclusive&#8221; story titled, &#8220;Google caught flagging GOP fundraiser emails as &#8216;suspicious&#8217; &#8212; sending them directly to spam.&#8221; The story cited a memo from Targeted Victory ‚Äì whose clients include the National Republican Senatorial Committee (NRSC), Rep. Steve Scalise and Sen. Marsha Blackburn ‚Äì which said it observed that the &#8220;serious and troubling&#8221; trend was still going on as recently as June and July of this year.
‚ÄúIf Gmail is allowed to quietly suppress WinRed links while giving ActBlue a free pass, it will continue to tilt the playing field in ways that voters never see, but campaigns will feel every single day,‚Äù the memo reportedly said.
In an August 28 letter to Google CEO Sundar Pichai, FTC Chairman Andrew Ferguson cited the New York Post story and warned that Gmail&#8217;s parent Alphabet may be engaging in unfair or deceptive practices.
&#8220;Alphabet‚Äôs alleged partisan treatment of comparable messages or messengers in Gmail to achieve political objectives may violate both of these prohibitions under the FTC Act,&#8221; Ferguson wrote. &#8220;And the partisan treatment may cause harm to consumers.&#8221;
However, the situation looks very different when you ask spam experts what&#8217;s going on with WinRed&#8217;s recent messaging campaigns. Atro Tossavainen and Pekka Jalonen are co-founders at Koli-L√µks O√ú, an email intelligence company in Estonia. Koli-L√µks taps into real-time intelligence about daily spam volumes by monitoring large numbers of &#8220;spamtraps&#8221; &#8212; email addresses that are intentionally set up to catch unsolicited emails.
Spamtraps are generally not used for communication or account creation, but instead are created to identify senders exhibiting spammy behavior, such as scraping the Internet for email addresses or buying unmanaged distribution lists. As an email sender, blasting these spamtraps over and over with unsolicited email is the fastest way to ruin your domain&#8217;s reputation online. Such activity also virtually ensures that more of your messages are going to start getting listed on spam blocklists that are broadly shared within the global anti-abuse community.
Tossavainen told KrebsOnSecurity that WinRed&#8217;s emails hit its spamtraps in the .com, .net, and .org space far more frequently than do fundraising emails sent by ActBlue. Koli-L√µks published a graph of the stark disparity in spamtrap activity for WinRed versus ActBlue, showing a nearly fourfold increase in spamtrap hits from WinRed emails in the final week of July 2025.
Image: Koliloks.eu
&#8220;Many of our spamtraps are in repurposed legacy-TLD domains (.com, .org, .net) and therefore could be understood to have been involved with a U.S. entity in their pre-zombie life,&#8221; Tossavainen explained in the LinkedIn post.
Raymond Dijkxhoorn is the CEO and a founding member of SURBL, a widely-used blocklist that flags domains and IP addresses known to be used in unsolicited messages, phishing and malware distribution. Dijkxhoorn said their spamtrap data mirrors that of Koli-L√µks, and shows that WinRed has consistently been far more aggressive in sending email than ActBlue.
Dijkxhoorn said the fact that WinRed&#8217;s emails so often end up dinging the organization&#8217;s sender reputation is not a content issue but rather a technical one.
&#8220;On our end we don‚Äôt really care if the content is political or trying to sell viagra or penis enlargements,&#8221; Dijkxhoorn said. &#8220;It‚Äôs the mechanics, they should not end up in spamtraps. And that‚Äôs the reason the domain reputation is tempered. Not ‚Äòbecause domain reputation firms have a political agenda.&#8217; We really don&#8217;t care about the political situation anywhere. The same as we don&#8217;t mind people buying penis enlargements. But when either of those land in spamtraps it will impact sending experience.&#8221;
The FTC letter to Google&#8217;s CEO also referenced a debunked 2022 study (PDF) by political consultants who found Google caught more Republican emails in spam filters. Techdirt editor Mike Masnick notes that while the 2022 study also found that other email providers caught more Democratic emails as spam, &#8220;Republicans laser-focused on Gmail because it fit their victimization narrative better.&#8221;
Masnick said GOP lawmakers then filed both lawsuits and complaints with the Federal Election Commission (both of which failed easily), claiming this was somehow an ‚Äúin-kind contribution‚Äù to Democrats.
&#8220;This is political posturing designed to keep the White House happy by appearing to &#8216;do something&#8217; about conservative claims of &#8216;censorship,'&#8221; Masnick wrote of the FTC letter. &#8220;The FTC has never policed &#8216;political bias&#8217; in private companies‚Äô editorial decisions, and for good reason‚Äîthe First Amendment prohibits exactly this kind of government interference.&#8221;
WinRed did not respond to a request for comment.
The WinRed website says it is an online fundraising platform supported by a united front of the Trump campaign, the Republican National Committee (RNC), the NRSC,¬†and the National Republican Congressional Committee (NRCC).
WinRed has recently come under fire for aggressive fundraising via text message as well. In June, 404 Media reported on a lawsuit filed by a family in Utah against the RNC for allegedly bombarding their mobile phones with text messages seeking donations after they&#8217;d tried to unsubscribe from the missives dozens of times.
One of the family members said they received 27 such messages from 25 numbers, even after sending 20 stop requests. The plaintiffs in that case allege the texts from WinRed and the RNC &#8220;knowingly disregard stop requests and purposefully use different phone numbers to make it impossible to block new messages.&#8221;
Dijkxhoorn said WinRed did inquire recently about why some of its assets had been marked as a risk by SURBL, but he said they appeared to have zero interest in investigating the likely causes he offered in reply.
&#8220;They only replied with, &#8216;You are interfering with U.S. elections,'&#8221; Dijkxhoorn said, noting that many of SURBL&#8217;s spamtrap domains are only publicly listed in the registration records for random domain names.
&#8220;They‚Äôre at best harvested by themselves but more likely [they] just went and bought lists,&#8221; he said. &#8220;It&#8217;s not like ‚ÄòOh Google is filtering this and not the other,‚Äô the reason isn&#8217;t the provider. The reason is the fundraising spammers and the lists they send to.&#8221;

üéì University AI
No updates.

üè¢ Corporate AI
‚Ä¢ Breaking the¬†networking¬†wall¬†in¬†AI infrastructure
  Memory and network bottlenecks are increasingly limiting AI system performance by reducing GPU&nbsp;utilization&nbsp;and overall efficiency,&nbsp;ultimately preventing&nbsp;infrastructure from reaching its full potential&nbsp;despite enormous investments.&nbsp;At the&nbsp;core&nbsp;of this challenge is a fundamental trade-off in the communication technologies used for memory and network interconnects.



Datacenters typically deploy two types of physical cables&nbsp;for&nbsp;communication between&nbsp;GPUs.&nbsp;Traditional copper links&nbsp;are power-efficient and&nbsp;reliable,&nbsp;but&nbsp;limited to&nbsp;very short&nbsp;distances&nbsp;(simultaneously.&nbsp;This approach leverages a hardware-system co-design and adopts&nbsp;a wide-and-slow design with hundreds of parallel low-speed channels using&nbsp;microLEDs.&nbsp;



The fundamental trade-off&nbsp;among&nbsp;power, reliability, and reach&nbsp;stems from&nbsp;the&nbsp;narrow-and-fast&nbsp;architecture&nbsp;deployed&nbsp;in&nbsp;today&#8217;s copper and optical links,&nbsp;comprising&nbsp;a few channels&nbsp;operating&nbsp;at&nbsp;very high&nbsp;data rates. For example,&nbsp;an&nbsp;800 Gbps link&nbsp;consists of eight 100 Gbps channels.&nbsp;With&nbsp;copper links, higher channel speeds lead to greater signal integrity challenges, which limits their reach.&nbsp;With optical&nbsp;links,&nbsp;high-speed transmission is inherently inefficient, requiring power-hungry laser drivers and&nbsp;complex electronics&nbsp;to compensate for transmission impairments. These challenges&nbsp;grow&nbsp;as speeds increase&nbsp;with&nbsp;every&nbsp;generation&nbsp;of networks.&nbsp;Transmitting at high speeds also pushes the limits of optical components, reducing&nbsp;systems&nbsp;margins&nbsp;and increasing failure rates.&nbsp;



These limitations force systems designers to make unpleasant&nbsp;choices,&nbsp;limiting the scalability of AI infrastructure.&nbsp;For example,&nbsp;scale-up networks connecting AI accelerators at&nbsp;multi-Tbps&nbsp;bandwidth&nbsp;typically&nbsp;must&nbsp;rely on&nbsp;copper links&nbsp;to meet&nbsp;the&nbsp;power budget,&nbsp;requiring&nbsp;ultra-dense racks that&nbsp;consume&nbsp;hundreds of kilowatts&nbsp;per rack. This creates significant challenges in cooling&nbsp;and&nbsp;mechanical design,&nbsp;which constrain&nbsp;the practical scale of these networks and end-to-end performance. This imbalance&nbsp;ultimately&nbsp;erects&nbsp;a&nbsp;networking wall&nbsp;akin&nbsp;to the&nbsp;memory wall, in&nbsp;which CPU speeds have outstripped memory speeds, creating performance bottlenecks.



A technology offering copper-like power efficiency and reliability over long distances can overcome this networking¬†wall,¬†enabling¬†multi-rack¬†scale-up domains and unlocking¬†new architectures. This is a highly active R&amp;D area, with many candidate technologies currently being developed across the industry.¬†In¬†our recent¬†paper,¬†‚ÄúMOSAIC: Breaking the Optics versus Copper Trade-off with a Wide-and-Slow Architecture and MicroLEDs‚Äù, which received the Best Paper award at ACM SIGCOMM (opens in new tab), we present¬†one such promising¬†approach¬†that is¬†the result of a multi-year collaboration between Microsoft Research,¬†Azure, and M365.¬†This¬†work is¬†centered around¬†an optical¬†wide-and-slow architecture, shifting from a small number of high-speed serial channels towards¬†hundreds of parallel low-speed channels.¬†This¬†would be impractical¬†to realize with today‚Äôs copper and optical technologies because of¬†i)¬†electromagnetic interference challenges in high-density copper cables and ii) the¬†high cost¬†and power consumption of lasers¬†in optical links,¬†as well as the increase in packaging complexity.¬†MOSAIC overcomes these issues by¬†leveraging¬†directly modulated¬†microLEDs, a technology originally developed for¬†screen¬†displays.¬†



MicroLEDs&nbsp;are significantly smaller than traditional LEDs (ranging from a few to tens of&nbsp;microns) and, due to their&nbsp;small size,&nbsp;they&nbsp;can be modulated at several Gbps.&nbsp;They&nbsp;are manufactured in large arrays,&nbsp;with over half a million&nbsp;in a small physical footprint for high-resolution displays&nbsp;like&nbsp;head-mounted devices or smartwatches. For example, assuming 2 Gbps per&nbsp;microLED&nbsp;channel, an 800 Gbps MOSAIC link can be realized by using a 20√ó20&nbsp;microLED&nbsp;array, which can fit in less than 1 mm√ó1 mm&nbsp;silicon&nbsp;die.&nbsp;



MOSAIC‚Äôs&nbsp;wide-and-slow&nbsp;design&nbsp;provides four core benefits.




Operating&nbsp;at low speed improves power efficiency&nbsp;by&nbsp;eliminating&nbsp;the need for&nbsp;complex&nbsp;electronics&nbsp;and&nbsp;reducing optical power requirements.



By&nbsp;leveraging&nbsp;optical transmission (via&nbsp;microLEDs),&nbsp;MOSAIC&nbsp;sidesteps&nbsp;copper‚Äôs reach issues, supporting distances up to 50 meters,&nbsp;or&nbsp;> 10x&nbsp;further&nbsp;than copper.



MicroLEDs‚Äô&nbsp;simpler structure&nbsp;and temperature insensitivity&nbsp;make them more reliable than lasers. The parallel nature of&nbsp;wide-and-slow&nbsp;also&nbsp;makes it easy to add redundant channels, further increasing reliability, up to two orders of magnitude higher than optical links.&nbsp;



The&nbsp;approach is also scalable, as higher aggregate speeds (e.g.,&nbsp;1.6&nbsp;Tbps&nbsp;or 3.2&nbsp;Tbps) can be achieved by increasing the number of&nbsp;channels and/or raising per-channel speed&nbsp;(e.g., to 4-8 Gbps).&nbsp;




Further,&nbsp;MOSAIC is fully compatible with today‚Äôs pluggable transceivers‚Äô form&nbsp;factor&nbsp;and it provides a drop-in replacement for today‚Äôs copper and optical cables, without requiring any changes to existing server and network infrastructure.&nbsp;MOSAIC is protocol-agnostic, as it simply relays bits from one endpoint to another without&nbsp;terminating&nbsp;or inspecting the connection&nbsp;and, hence,&nbsp;it‚Äôs&nbsp;fully compatible with today‚Äôs protocols (e.g.,&nbsp;Ethernet, PCIe, CXL).&nbsp;We are currently working with our suppliers to&nbsp;productize&nbsp;this technology and&nbsp;scale&nbsp;to mass production.&nbsp;



While&nbsp;conceptually simple, realizing this architecture posed a few key challenges&nbsp;across the stack, which&nbsp;required&nbsp;a multi-disciplinary team with&nbsp;expertise&nbsp;spanning across integrated photonics, lens design, optical transmission, and&nbsp;analog&nbsp;and digital design.&nbsp;For example, using individual&nbsp;fibers&nbsp;per channel would be prohibitively complex and costly due to the&nbsp;large number&nbsp;of channels. We addressed this by employing imaging&nbsp;fibers,&nbsp;which are typically used for medical applications (e.g., endoscopy).&nbsp;They&nbsp;can support thousands of cores&nbsp;per&nbsp;fiber, enabling multiplexing&nbsp;of&nbsp;many channels within a single&nbsp;fiber.&nbsp;Also,&nbsp;microLEDs&nbsp;are a less pure light source&nbsp;than lasers,&nbsp;with&nbsp;a larger beam shape (which complicates&nbsp;fiber&nbsp;coupling) and&nbsp;a broader spectrum (which&nbsp;degrades&nbsp;fiber&nbsp;transmission due to chromatic dispersion).&nbsp;We tackled these issues through&nbsp;a novel&nbsp;microLED and&nbsp;optical lens design,&nbsp;and&nbsp;a power-efficient&nbsp;analog-only electronic back&nbsp;end, which does not require any expensive digital signal processing.&nbsp;&nbsp;



Based on our current estimates, this approach can save&nbsp;up to 68% of power, i.e., more&nbsp;than 10W per cable while reducing failure rates by up to 100x. With global annual shipments of optical cables&nbsp;reaching into&nbsp;the tens of millions, this translates to over 100MW of power savings per year,&nbsp;enough to power more than 300,000 homes. While these immediate gains are already significant, the unique combination of low power consumption, reduced cost, high reliability, and long reach opens up exciting new opportunities&nbsp;to rethink&nbsp;AI&nbsp;infrastructure from network and cluster architectures to compute and memory designs.



For example,&nbsp;by&nbsp;supporting&nbsp;low-power,&nbsp;high-bandwidth connectivity at long reach,&nbsp;MOSAIC&nbsp;removes the need for ultra-dense racks and&nbsp;enables&nbsp;novel network topologies, which would be impractical today. The resulting redesign could&nbsp;reduce&nbsp;resource fragmentation and&nbsp;simplify&nbsp;collective optimization.&nbsp;Similarly,&nbsp;on the&nbsp;compute&nbsp;front,&nbsp;the ability&nbsp;to&nbsp;connect&nbsp;silicon&nbsp;dies at low power over long distances&nbsp;could&nbsp;enable&nbsp;resource&nbsp;disaggregation, shifting from today‚Äôs&nbsp;large,&nbsp;multi-die packages to&nbsp;smaller, more cost-effective, ones.&nbsp;Bypassing packaging area constraints would also make it possible to drastically increase&nbsp;GPU&nbsp;memory&nbsp;capacity and bandwidth,&nbsp;while&nbsp;facilitating&nbsp;adoption of&nbsp;novel memory technologies.&nbsp;



Historically, step changes in network technology have unlocked entirely new classes of applications and workloads. While our SIGCOMM paper provides&nbsp;possible future&nbsp;directions, we hope this work sparks broader discussion and collaboration across the research and industry communities.
Opens in a new tabThe post Breaking the¬†networking¬†wall¬†in¬†AI infrastructure¬† appeared first on Microsoft Research.
‚Ä¢ Powering innovation at scale: How AWS is tackling AI infrastructure challenges
  As generative AI continues to transform how enterprises operate‚Äîand develop net new innovations‚Äîthe infrastructure demands for training and deploying AI models have grown exponentially. Traditional infrastructure approaches are struggling to keep pace with today‚Äôs computational requirements, network demands, and resilience needs of modern AI workloads. 
At AWS, we‚Äôre also seeing a transformation across the technology landscape as organizations move from experimental AI projects to production deployments at scale. This shift demands infrastructure that can deliver unprecedented performance while maintaining security, reliability, and cost-effectiveness. That‚Äôs why we‚Äôve made significant investments in networking innovations, specialized compute resources, and resilient infrastructure that‚Äôs designed specifically for AI workloads. 
Accelerating model experimentation and training with SageMaker AI 
The gateway to our AI infrastructure strategy is Amazon SageMaker AI, which provides purpose-built tools and workflows to streamline experimentation and accelerate the end-to-end model development lifecycle. One of our key innovations in this area is Amazon SageMaker HyperPod, which removes the undifferentiated heavy lifting involved in building and optimizing AI infrastructure. 
At its core, SageMaker HyperPod represents a paradigm shift by moving beyond the traditional emphasis on raw computational power toward intelligent and adaptive resource management. It comes with advanced resiliency capabilities so that clusters can automatically recover from model training failures across the full stack, while automatically splitting training workloads across thousands of accelerators for parallel processing. 
The impact of infrastructure reliability on training efficiency is significant. On a 16,000-chip cluster, for instance, every 0.1% decrease in daily node failure rate improves cluster productivity by 4.2% ‚Äîtranslating to potential savings of up to $200,000 per day for a 16,000 H100 GPU cluster. To address this challenge, we recently introduced Managed Tiered Checkpointing in HyperPod, leveraging CPU memory for high-performance checkpoint storage with automatic data replication. This innovation helps deliver faster recovery times and is a cost-effective solution compared to traditional disk-based approaches. 
For those working with today‚Äôs most popular models, HyperPod also offers over 30 curated model training recipes, including support for OpenAI GPT-OSS, DeepSeek R1, Llama, Mistral, and Mixtral. These recipes automate key steps like loading training datasets, applying distributed training techniques, and configuring systems for checkpointing and recovery from infrastructure failures. And with support for popular tools like Jupyter, vLLM, LangChain, and MLflow, you can manage containerized apps and scale clusters dynamically as you scale your foundation model training and inference workloads. 
Overcoming the bottleneck: Network performance 
As organizations scale their AI initiatives from proof of concept to production, network performance often becomes the critical bottleneck that can make or break success. This is particularly true when training large language models, where even minor network delays can add days or weeks to training time and significantly increase costs. In 2024, the scale of our networking investments was unprecedented; we installed over 3 million network links to support our latest AI network fabric, or 10p10u infrastructure. Supporting more than 20,000 GPUs while delivering 10s of petabits of bandwidth with under 10 microseconds of latency between servers, this infrastructure enables organizations to train massive models that were previously impractical or impossibly expensive. To put this in perspective: what used to take weeks can now be accomplished in days, allowing companies to iterate faster and bring AI innovations to customers sooner. 
At the heart of this network architecture is our revolutionary Scalable Intent Driven Routing (SIDR) protocol and Elastic Fabric Adapter (EFA). SIDR acts as an intelligent traffic control system that can instantly reroute data when it detects network congestion or failures, responding in under one second‚Äîten times faster than traditional distributed networking approaches. 
Accelerated computing for AI 
The computational demands of modern AI workloads are pushing traditional infrastructure to its limits. Whether you‚Äôre fine-tuning a foundation model for your specific use case or training a model from scratch, having the right compute infrastructure isn‚Äôt just about raw power‚Äîit‚Äôs about having the flexibility to choose the most cost-effective and efficient solution for your specific needs. 
AWS offers the industry‚Äôs broadest selection of accelerated computing options, anchored by both our long-standing partnership with NVIDIA and our custom-built AWS Trainium chips. This year‚Äôs launch of P6 instances featuring NVIDIA Blackwell chips demonstrates our continued commitment to bringing the latest GPU technology to our customers. The P6-B200 instances provide 8 NVIDIA Blackwell GPUs with 1.4 TB of high bandwidth GPU memory and up to 3.2 Tbps of EFAv4 networking. In preliminary testing, customers like JetBrains have already seen greater than 85% faster training times on P6-B200 over H200-based P5en instances across their ML pipelines. 
To make AI more affordable and accessible, we also developed AWS Trainium, our custom AI chip designed specifically for ML workloads. Using a unique systolic array architecture, Trainium creates efficient computing pipelines that reduce memory bandwidth demands. To simplify access to this infrastructure, EC2 Capacity Blocks for ML also enable you to reserve accelerated compute instances within EC2 UltraClusters for up to six months, giving customers predictable access to the accelerated compute they need. 
Preparing for tomorrow‚Äôs innovations, today 
As AI continues to transform every aspect of our lives, one thing is clear: AI is only as good as the foundation upon which it is built. At AWS, we‚Äôre committed to being that foundation, delivering the security, resilience, and continuous innovation needed for the next generation of AI breakthroughs. From our revolutionary 10p10u network fabric to custom Trainium chips, from P6e-GB200 UltraServers to SageMaker HyperPod‚Äôs advanced resilience capabilities, we‚Äôre enabling organizations of all sizes to push the boundaries of what‚Äôs possible with AI. We‚Äôre excited to see what our customers will build next on AWS. 
 
About the author 
Barry Cooks is a global enterprise technology veteran with 25 years of experience leading teams in cloud computing, hardware design, application microservices, artificial intelligence, and more. As VP of Technology at Amazon, he is responsible for compute abstractions (containers, serverless, VMware, micro-VMs), quantum experimentation, high performance computing, and AI training. He oversees key AWS services including AWS Lambda, Amazon Elastic Container Service, Amazon Elastic Kubernetes Service, and Amazon SageMaker. Barry also leads responsible AI initiatives across AWS, promoting the safe and ethical development of AI as a force for good. Prior to joining Amazon in 2022, Barry served as CTO at DigitalOcean, where he guided the organization through its successful IPO. His career also includes leadership roles at VMware and Sun Microsystems. Barry holds a BS in Computer Science from Purdue University and an MS in Computer Science from the University of Oregon.
‚Ä¢ Accelerate your model training with managed tiered checkpointing on Amazon SageMaker HyperPod
  As organizations scale their AI infrastructure to support trillion-parameter models, they face a difficult trade-off: reduced training time with lower cost or faster training time with a higher cost. When they checkpoint frequently to speed up recovery time and minimize lost training time, they incur in substantially higher storage cost. And when they checkpoint infrequently, they reduce costs at the risk of losing valuable training progress when failures occur. 
This challenge is exacerbated in large distributed training environments, with thousands of accelerators, where issues can occur frequently. According to an article released by Meta, one failure happened every 3 hours during the Meta Llama 3 model training. The GPU issues accounted for 60% of the total failures, and network, CPU, and disks account the other 40%. With infrequent checkpointing, these accumulated failures can result in losing days of training progress over the course of a complete training run, thereby driving up costs and time to market. Frequent checkpoints can saturate networks, overload storage, and result in unpredictable performance. 
To help solve these challenges, AWS announced managed tiered checkpointing in Amazon SageMaker HyperPod, a purpose-built infrastructure to scale and accelerate generative AI model development across thousands of AI accelerators. Managed tiered checkpointing uses CPU memory for high-performance checkpoint storage with automatic data replication across adjacent compute nodes for enhanced reliability. Although SageMaker HyperPod identifies node issues automatically and replaces those nodes so your training can resume, managed tiered checkpointing helps you implement the best checkpointing strategy and maximize your training throughput. 
Managed tiered checkpointing has been tested on large distributed training clusters ranging from hundreds of GPU to over 15,000 GPU, with checkpoints being saved within seconds. 
In this post, we dive deep into those concepts and understand how to use the managed tiered checkpointing feature. 
Solution overview 
Checkpointing is the method of saving an intermediate model‚Äôs state during the training process. You can resume training from a recent checkpoint in the event of an issue by saving the model‚Äôs parameters, optimizer states, and other metadata during training. Additionally, you can resolve training problems, such as irregular learning rates, without a full restart by loading an earlier checkpoint state. 
Use the following formula to find a rough initial estimate of the total size of the checkpoint for your model without the optimizer state:Model checkpoint size (GB) = (Number of parameters √ó Bytes per parameter) √∑ 10243 bytesFor example, if you train a Meta Llama 3 70-billion-parameter model using BFloat16 as the parameter‚Äôs precision, the checkpoint size will be 130 GB. If you train a DeepSeek-R1 671-billion-parameter model using BFloat16, the checkpoint size will be 1.25 TB. All without storing optimizer states.Checkpoints include optimizer states, training metadata (such as step number), and other additional data, resulting in a larger than expected size. When using an Adam optimizer, the optimizer will save three additional float16 statistics per parameter, resulting in an additional 6 bytes per parameter. Therefore, with the optimizer state saved, the Meta Llama 3 70B model checkpoint size will be approximately 521 GB, and the DeepSeek-R1 671B model checkpoint size will be approximately 5 TB. That is a four-times increase in size, and handling those checkpoints becomes a challenge. 
The following table summarizes the checkpoint sizes for each model. 
 
  
   
   Model name 
   Size of Checkpoint 
   Size of Checkpoint + Optimizer States 
   
   
   Meta Llama 3 70B 
   130 GB 
   521 GB 
   
   
   DeepSeek R1 671B 
   1.43 TB 
   5 TB 
   
  
 
It‚Äôs also important to consider the training strategy. In a Fully Sharded Data Parallel (FSDP) scenario, each rank (a single GPU process in a distributed training) saves its own part of the checkpoint. At the same time, it reduces the amount of data each rank has to save during a checkpoint, and imposes a stress on the file system level. On a Network File System (NFS) shared file system, those concurrent writes become a bottleneck. Using a distributed file system, such Amazon FSx for Lustre, can help alleviate that pressure at a higher total cost. In a Distributed Data Parallel (DDP) scenario, a single rank writes the complete checkpoint at one time, and all ranks read the checkpoint when loading it back. On the file system level, this means a single writer and multiple readers. On an NFS file system, many readers can be a problem because they will be constrained based on the file system, network stack, and queue size. A single writer, over the network, will not take advantage of all the network throughput. Here again, a fast, distributed file system like FSx for Lustre can help solve those problems at a higher total cost of ownership. 
As we can see, traditional checkpointing methods that rely solely on remote persistent storage create a computational overhead during checkpoint creation, because writing terabytes of model parameters to persistent storage might throttle it, consume expensive network bandwidth, and require complex orchestration across distributed systems. By storing checkpoints in fast-access in-memory locations, such as CPU RAM, while maintaining configurable backup to Amazon Simple Storage Service (Amazon S3) for persistence, the system delivers faster recovery times, and is a cost-effective solution compared to traditional disk-based approaches. 
Managed tiered checkpointing works as follows: 
 
 When training your model, you define the checkpoint frequency. 
 Model training uses GPU HBM memory to store the model, its parameters, and intermediate results, and do the heavy computation. 
 Triggering a checkpoint stops model training. The GPU will convert the model weights (tensors) into a state dictionary and copy the data to the instance‚Äôs CPU, then the training resumes while managed tiered checkpointing copies the data to RAM. 
 Because RAM is volatile, managed tiered checkpointing copies the data asynchronously from the host RAM to adjacent nodes using RDMA over Elastic Fabric Adapter (EFA). If a node experiences an issue, its checkpoint data will be available on other nodes too. 
 From time to time, it copies the data to a second layer of persistent storage, such as Amazon S3. This helps both when writing to RAM fails and when you want to persistently store the checkpoint data for future use. 
 
With managed tiered checkpointing, you can configure frequency and retention policies for both in-memory and persistent storage tiers. You use the first layer (in-memory) to save checkpoints at a high frequency and for fast recovery, periodically saving to Amazon S3 for backup. Managed tiered checkpointing provides a file system that can be seamlessly integrated with your PyTorch Distributed Checkpointing (DCP) training. Adding it to your training script only requires a few lines of code. Furthermore, it improves the performance of checkpoints by using in-memory storage while using other tiers for persistent storage. PyTorch DCP solves the issue of saving a model‚Äôs checkpoint when it uses distributed resources, such as multiple GPUs across multiple compute nodes. Trainers, parameters, and the dataset are partitioned across those nodes and resources, then PyTorch DCP saves and loads from multiple ranks in parallel. PyTorch DCP produces multiple files per checkpoint, at least one per rank. Depending on the volume of those files, number and size, shared and network file systems such as NFS will struggle with inode and metadata management. Managed tiered checkpointing helps solve that issue by making it possible to use multiple tiers, reducing intrusion to the training time and still receiving the benefits of PyTorch DCP, such as deduplication of checkpoint data. 
With managed tiered checkpointing in SageMaker HyperPod, you can maintain a high training throughput even in large-scale environments prone to failures. It uses your existing SageMaker HyperPod cluster orchestrated by Amazon Elastic Kubernetes Service (Amazon EKS) and compute nodes, and there are no additional costs to use the library. 
In the following sections, we explore how to configure the SageMaker HyperPod cluster‚Äôs training scripts to use this new feature. 
Configure your SageMaker HyperPod cluster for managed tiered checkpointing 
SageMaker HyperPod provisions resilient clusters for running machine learning (ML) workloads and developing state-of-the-art models such as large language models (LLMs), diffusion models, and foundation models (FMs). By reducing the complex work of building and maintaining compute clusters using accelerators like AWS Trainium and NVIDIA H200/B200 GPUs, it speeds up the creation of foundation models. To create a new SageMaker HyperPod cluster, refer to the Amazon SageMaker HyperPod Developer Guide. If you want to accelerate your deployment by using field hardened assets, refer to the following GitHub repo. 
The examples shared in this post are intended to help you learn more about this new feature. If you‚Äôre considering running the examples provided here in a production environment, have your security team review the content and make sure they adhere to your security standards. At AWS, security is the top priority and we understand that every customer has their own security framework.Before creating or updating a cluster to add the managed tiered checkpointing feature, you must set up the EKS pods to access an S3 bucket either on your own account or across accounts. When working with buckets on the same account as the SageMaker HyperPod EKS cluster, you can use the following policy (change your bucket name before applying it): 
 
 {
&nbsp;&nbsp; &nbsp;"Version": "2012-10-17",
&nbsp;&nbsp; &nbsp;"Statement": [
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;{
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"Action": [
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"s3:DeleteObject",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"s3:GetBucketLocation",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"s3:GetObject",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"s3:ListBucket",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"s3:PutObject"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;],
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"Resource": [
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"arn:aws:s3:::&lt;bucket_name&gt;",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"arn:aws:s3:::&lt;bucket_name&gt;/*"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;],
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"Effect": "Allow"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp;]
} 
 
If the bucket is in a different account, you must authorize an AWS Identity and Access Management (IAM) principal to access those buckets. The following IAM policy will do that for you. Be sure to change both the bucket name and the IAM principal (for example, your AWS account ID). 
 
 {
&nbsp;&nbsp; &nbsp;"Version": "2012-10-17",
&nbsp;&nbsp; &nbsp;"Statement": [
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;{
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"Sid": "CheckPointCrossAccountAccess",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"Effect": "Allow",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"Principal": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"AWS":&nbsp;"arn:aws:iam::&lt;account_id&gt;:root"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;},
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"Action": [
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"s3:DeleteObject",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"s3:GetBucketLocation",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"s3:GetObject",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"s3:ListBucket",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"s3:PutObject"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;],
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"Resource": [
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"arn:aws:s3:::&lt;bucket_name&gt;",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"arn:aws:s3:::&lt;bucket_name&gt;/*"
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;]
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp;]
} 
 
To create a new cluster with managed tiered checkpointing, you can pass a parameter using --tiered-storage-config and setting Mode to Enable using an AWS Command Line Interface (AWS CLI) command: 
 
  
  aws sagemaker create-cluster \
 --cluster-name "ml-cluster" \
--tiered-storage-config { "Mode": "Enable" } \
 --instance-groups '[{
    "InstanceCount": 1, 
  ....
 }]'
 
  
 
You can also update it using the UpdateCluster API and pass the CachingConfig parameter with the required AllocatedMemory configuration. You can use the CachingConfiguration parameter to define a fixed value or a percentage of the CPU RAM for checkpointing. 
 
 aws sagemaker update-cluster \
    --cluster-name &lt;my-training-cluster&gt; \
    --tiered-storage-config {
            "Mode": "Enable"
            "InstanceMemoryAllocationPercentage": &lt;percent&gt;
            } 
 
Now that your SageMaker HyperPod cluster has the managed tiered checkpointing feature, let‚Äôs prepare the training scripts and add them. 
Install the managed tiered checkpoint libraries and integrate with your training script 
Managed tiered checkpointing integrates with PyTorch DCP. You start by installing the sagemaker-checkpointing library. Then you create and configure a namespace to store the checkpoints based on the defined frequency. Finally, you add the checkpoint function inside your training loop. 
To install the library, we simply use Python‚Äôs pip. Make sure you already have the dependencies installed: Python 3.10 or higher, PyTorch with DCP support, and the AWS credentials configured properly. To integrate Amazon S3 as another storage layer, you also need s3torchconnector installed. 
 
 # Install the pre-requisites
pip install torch boto3 botocore tenacity s3torchconnector

# Install the Managed Tiered Checkpointing library
pip install amzn-sagemaker-checkpointing 
 
Now you can import the library on your script and configure the namespace and frequency for checkpointing: 
 
 import&nbsp;torchimport&nbsp;torch.distributed as&nbsp;dist
from&nbsp;torch.distributed.checkpoint import&nbsp;async_save, load
from&nbsp;amzn_sagemaker_checkpointing.config.sagemaker_checkpoint_config import&nbsp;SageMakerCheckpointConfig
from&nbsp;amzn_sagemaker_checkpointing.checkpointing.filesystem.filesystem import&nbsp;(
&nbsp;&nbsp; &nbsp;SageMakerTieredStorageWriter,
&nbsp;&nbsp; &nbsp;SageMakerTieredStorageReader
)

checkpoint_config = SageMakerCheckpointConfig(
&nbsp;&nbsp; &nbsp;# Unique ID for your training job 
&nbsp; &nbsp; # Allowed characters in ID include: alphanumeric, hyphens, and underscores
&nbsp;&nbsp; &nbsp;namespace=os.environ.get('TRAINING_JOB_NAME', f'job-{int(time.time())}'), 

&nbsp;&nbsp; &nbsp;# Number of distributed processes/available GPUs
&nbsp;&nbsp; &nbsp;world_size=dist.get_world_size(), 

&nbsp;&nbsp; &nbsp;# Amazon S3 storage location, required for SageMakerTieredStorageReader for read fallbacks
&nbsp; &nbsp; # Required for SageMakerTieredStorageWriter when save_to_s3 is True
&nbsp;&nbsp; &nbsp;s3_tier_base_path="s3://&lt;my-bucket&gt;/checkpoints" 
 
In the preceding code snippet, we have configured managed tiered checkpointing with the same world_size as the number of ranks in our cluster. When you start a distributed training, each GPU in the cluster is assigned a rank number, and the total number of GPUs available is the world_size. We set up Amazon S3 as our backup persistent storage, setting managed tiered checkpointing to store data in Amazon S3 every 100 training steps. Both world_size and namespace are required parameters; the others are optional. 
Now that the configuration is ready, let‚Äôs set up PyTorch DCP and integrate managed tiered checkpointing. 
First, configure the storage writer. This component will pass on to the PyTorch DCP async_save function alongside the model‚Äôs state dictionary. We use the SageMakerTieredStorageWriter when writing the checkpoints and the SageMakeTieredStorageReader when restoring from those checkpoints. 
Inside your model training loop, you add the storage writer configuration and pass along both the managed tiered checkpointing configuration and the step number: 
 
 &nbsp; &nbsp;state_dict = {
 &nbsp; &nbsp; &nbsp; &nbsp;"model": model.state_dict(),
 &nbsp; &nbsp; &nbsp; &nbsp;"optimizer": optimizer.state_dict(),
 &nbsp; &nbsp; &nbsp; &nbsp;"step": training_step,
 &nbsp; &nbsp; &nbsp; &nbsp;"epoch": epoch
 &nbsp; &nbsp;}
&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;# Create storage writer for current step&nbsp;and&nbsp;if&nbsp;it&nbsp;need&nbsp;to&nbsp;save&nbsp;to&nbsp;a&nbsp;persistent&nbsp;storage&nbsp;too&nbsp;
 &nbsp; &nbsp;checkpoint_config.save_to_s3 = training_step % s3_ckpt_freq == 0
 &nbsp; &nbsp;storage_writer = SageMakerTieredStorageWriter(
 &nbsp; &nbsp; &nbsp; &nbsp;checkpoint_config=checkpoint_config,
 &nbsp; &nbsp; &nbsp; &nbsp;step=training_step
 &nbsp; &nbsp;) 
 
You can define the step number explicitly for the storage writer, or you can let the storage writer identify the step number from the path where the checkpoint is being saved. If you want to let the storage writer infer the step number from the base path, don‚Äôt set the stepparameter and make sure your path contains the step number in it. 
Now you can call the PyTorch DCP asynchronous save function and pass along the state dictionary and the storage writer configuration:async_save(state_dict=state_dict, storage_writer=storage_writer) 
We have set up managed tiered checkpointing to write checkpoints at our desired frequency and location (in-memory). Let‚Äôs use the storage reader to restore those checkpoints. First, pass the managed tiered checkpointing configuration to the SageMakerTieredStorageReader, then call the PyTorch DCP load function, passing the model state dictionary and the storage reader configuration: 
 
 storage_reader = SageMakerTieredStorageReader(checkpoint_config=checkpoint_config)
load(state_dict, storage_reader=storage_reader) 
 
To work through a complete example, refer to the following GitHub repository, where we‚Äôve created a simple training script, including the managed tiered checkpointing feature. 
Clean up 
After you have worked with managed tiered checkpointing, and you want to clean up the environment, simply remove the amzn-sagemaker-checkpointing library by running pip uninstall amzn-sagemaker-checkpointing. 
If you installed the solution in a Python virtual environment, then just deleting the virtual environment will suffice.Managed tiered checkpointing is a free feature that doesn‚Äôt require additional resources to run. You use your existing SageMaker HyperPod EKS cluster and compute nodes. 
Best practices to optimize your checkpoint strategy with managed tiered checkpointing 
Managed tiered checkpointing will attempt to write to the in-memory tier first. This optimizes the writing performance because in-memory provides ultra-low latency checkpoint access. You should configure managed tiered checkpointing to write to a second layer, such as Amazon S3, from time to time. For example, configure managed tiered checkpointing to write to the in-memory layer every 10 steps, and configure it to write to Amazon S3 every 100 steps. 
If managed tiered checkpointing fails to write to the in-memory layer, and the node experiences an issue, then you still have your checkpoint saved on Amazon S3. While writing to Amazon S3, managed tiered checkpointing uses multiple TCP streams (chunks) to optimize Amazon S3 writes. 
In terms of consistency, managed tiered checkpointing uses an all-or-nothing writing strategy. It implements a fallback mechanism that will seamlessly transition between the storage tiers. Checkpoint metadata, such as step number, is stored alongside the data for every tier. 
When trying to troubleshoot managed tiered checkpointing, you can check the log written locally to /var/log/sagemaker_checkpointing/{namespace}_checkpointing.log. It publishes data about the training step, rank number, and the operation details. The following is an example output of that file: 
 
 [timestamp] [namespace] [logger_name] [INFO] [filename:451] [Rank 0] Step 240: Starting checkpoint write ([SavePlan Items Count] items)
[timestamp] [namespace] [logger_name] [INFO] [filename:498] [Rank 0] Step 240: In-memory write completed in [Latency]s ([Throughput] MB/s)
[timestamp] [namespace] [logger_name] [INFO] [filename:530] [Rank 0] Step 240: S3 batch write completed in [Latency]s ([Size] total, [Throughput] MB/s average) 
 
Managed tiered checkpointing also writes those metrics to the console, so it‚Äôs straightforward to troubleshoot during development. They contain information on which step number is being written to which storage layer and the throughput and total time taken to write the data. With that information, you can monitor and troubleshoot managed tiered checkpointing thoroughly. 
When you combine those tools with the SageMaker HyperPod observability stack, you get a complete view of all metrics of your training or inference workload. 
Conclusion 
The new managed tiered checkpointing feature in SageMaker HyperPod augments FM training efficiency by intelligently distributing checkpoints across multiple storage tiers. This advanced approach places model states in fast access locations such as CPU RAM memory, while using persistent storage such as Amazon S3 for cost-effective, long-term persistence. As of the time of this launch, managed tiered checkpointing is supported only on SageMaker HyperPod on Amazon EKS. 
Managed tiered checkpointing delivers fast recovery times without increased storage costs, avoiding complex trade-offs between resiliency, training efficiency, and storage costs. It has been validated on large distributed training clusters that range from hundreds of GPU to more than 15,000 GPU, with checkpoints being saved within seconds. 
Integrating managed tiered checkpointing on your training scripts is straightforward, with just a few lines of code, providing immediate access to sophisticated checkpoint management without requiring deep engineering expertise. 
For more information on how managed tiered checkpointing works, how to set it up, and other details, refer to HyperPod managed tier checkpointing. 
 
About the authors 
Paulo Aragao&nbsp;is a Principal WorldWide Solutions Architect focused on Generative AI at the Specialist Organisation on AWS. He helps Enterprises and Startups to build their Foundation Models strategy and innovate faster by leveraging his extensive knowledge on High Perfomance Computing and Machine Learning. A long time bass player, and natural born rock fan, Paulo enjoys spending time travelling with his family, scuba diving, and playing real time strategy and role-playing games. 
Kunal Jha&nbsp;is a Principal Product Manager at AWS. He is focused on building Amazon SageMaker Hyperpod as the best-in-class choice for Generative AI model‚Äôs training and inference. In his spare time, Kunal enjoys skiing and exploring the Pacific Northwest. 
Mandar Kulkarni&nbsp;is a Software Development Engineer II at AWS, where he works on Amazon SageMaker. He specializes in building scalable and performant machine learning libraries and infrastructure solutions, particularly focusing on SageMaker HyperPod. His technical interests span machine learning, artificial intelligence, distributed systems and application security. When not architecting ML solutions, Mandar enjoys hiking, practicing Indian classical music, sports, and spending quality time with his young family. 
Vinay Devadiga&nbsp;is a Software Development Engineer II at AWS with a deep passion for artificial intelligence and cloud computing. He focuses on building scalable, high-performance systems that enable the power of AI and machine learning to solve complex problems.Vinay enjoys staying at the forefront of technology, continuously learning, and applying new advancements to drive innovation. Outside of work, he likes playing sports and spending quality time with his family. 
Vivek Maran&nbsp;is a Software Engineer at AWS. He currently works on the development of Amazon SageMaker HyperPod, a resilient platform for large scale distributed training and inference. His interests include large scale distributed systems, network systems, and artificial intelligence. Outside of work, he enjoys music, running, and keeping up to date with business &amp; technology trends.
‚Ä¢ Maximize HyperPod Cluster utilization with HyperPod task governance fine-grained quota allocation
  We are excited to announce the general availability of fine-grained&nbsp;compute and memory quota allocation&nbsp;with HyperPod task governance. With this capability, customers can optimize Amazon SageMaker HyperPod cluster utilization on Amazon Elastic Kubernetes Service (Amazon EKS), distribute fair usage, and support efficient resource allocation across different teams or projects.&nbsp;For more information, see HyperPod task governance best practices for maximizing the value of SageMaker HyperPod task governance. 
Compute quota management is an administrative mechanism that sets and controls compute resource limits across users, teams, and projects. It controls fair resource distribution, preventing a single entity from monopolizing cluster resources, thereby optimizing overall computational efficiency. 
Because of budget constraints, customers might want to allocate compute resources across multiple teams fairly. For example, a data scientist might need some GPUs (for example, four H100 GPUs) for model development, but not the entire instance‚Äôs compute capacity. In other cases, customers have limited compute resources but many teams, and they want to fairly share compute resources across these teams, so that no idle capacity is left unused. 
With HyperPod task governance, administrators can now allocate granular GPU, vCPU, and vCPU memory to teams and projects‚Äîin addition to the entire instance resources‚Äîbased on their preferred strategy. Key capabilities include GPU-level quota allocation by instance type and family, or hardware type‚Äîsupporting both Trainium and NVIDIA GPUs‚Äîand optional CPU and memory allocation for fine-tuned resource control. Administrators can also define&nbsp;the weight (or priority level) a team is given for fair-share idle compute allocation. 

 ‚ÄúWith a wide variety of frontier AI data experiments and production pipelines, being able to maximize SageMaker HyperPod Cluster utilization is extremely high impact. This requires fair and controlled access to shared resources like state-of-the-art GPUs, granular hardware allocation, and more. This is exactly what HyperPod task governance is built for, and we‚Äôre excited to see AWS pushing efficient cluster utilization for a variety of AI use cases.‚Äù 
 ‚Äì Daniel Xu, Director of Product at Snorkel AI, whose AI data technology platform empowers enterprises to build specialized AI applications by leveraging their organizational expertise at scale.
 
In this post, we dive deep into how to define quotas for teams or projects based on granular or instance-level allocation. We discuss different methods to define such policies, and how data scientists can schedule their jobs seamlessly with this new capability. 
Solution overview 
Prerequisites 
To follow the examples in this blog post, you need to meet the following prerequisites: 
 
 An AWS account with access to SageMaker HyperPod. 
 A running SageMaker HyperPod (EKS-orchestrated) cluster. For more information on how to create and configured a new HyperPod cluster, see the HyperPod workshop or the SageMaker HyperPod cluster creation with Amazon EKS orchestration. 
 HyperPod task governance addon version 1.3 or later installed in the cluster. For more information, see set up HyperPod task governance 
 
To schedule and execute the example jobs in the Submitting Tasks section, you will also need: 
 
 A local environment (either your local machine or a cloud-based compute environment), from which to run the HyperPod CLI and kubectl commands, configured as follows: 
   
   OS based on Linux or MacOS 
   Python&nbsp;3.8, 3.9, 3.10, or 3.11 installed 
   AWS Command Line Interface (AWS CLI) configured with the appropriate credentials to use the above services 
   HyperPod CLI version 3.1.0 
   Kubernetes command-line tool, kubectl 
    
 HyperPod Training Operator installed in the cluster 
 
Allocating granular compute and memory quota using the AWS console 
Administrators are the primary persona interacting with SageMaker HyperPod task governance and are responsible for managing cluster compute allocation in alignment with the organization‚Äôs strategic priorities and goals. 
Implementing this feature follows the familiar compute allocation&nbsp;creation workflow of HyperPod task governance.&nbsp;To get started, sign in to the AWS Management Console and navigate to Cluster Management under HyperPod Clusters in the Amazon SageMaker AI console. After selecting your HyperPod cluster, select the Policies tab in the cluster detail page. Navigate to&nbsp;Compute allocations and choose Create. 
 
As with existing functionality, you can enable task prioritization and fair-share resource allocation through cluster policies that prioritize critical workloads and distribute idle compute across teams. By using HyperPod task governance, you can define queue admission policies (first-come-first-serve by default or task ranking) and idle compute allocation methods (first-come-first-serve or fair-share by default). In the Compute allocation section, you can create and edit allocations to distribute resources among teams, enable lending and borrowing of idle compute, configure preemption of low-priority tasks, and assign fair-share weights. 
The key innovation is in the Allocations section shown in the following figure, where you‚Äôll now find fine-grained options for resource allocation. In addition to the existing instance-level quotas, you can now directly specify GPU quotas by instance type and family or by hardware type. When you define GPU allocations, HyperPod task governance intelligently calculates appropriate default values for vCPUs and memory which are set proportionally. 
For example, when allocating 2 GPUs from a single p5.48xlarge instance (which has 8 GPUs, 192 vCPUs, and 2 TiB memory) in your HyperPod cluster, HyperPod task governance assigns 48 vCPUs and 512 GiB memory as default values‚Äîwhich is equivalent to one quarter of the instance‚Äôs total resources. Similarly, if your HyperPod cluster contains 2 ml.g5.2xlarge instances (each with 1 GPU, 8 vCPUs, and 32 GiB memory), allocating 2 GPUs would automatically assign 16 vCPUs and 64 GiB memory from both instances as shown in the following image. 
 
You can either proceed with these automatically calculated default values or customize the allocation by manually adjusting the vCPUs and vCPU memory fields as seen in the following image. 
 
Amazon SageMaker HyperPod supports clusters that include CPU-based instances, GPU-based instances, and AWS Neuron-based hardware (AWS Inferentia and AWS Trainium chips). You can specify resource allocation for your team by instances, GPUs, vCPUs, vCPU memory, or Neuron devices, as shown in the following image. 
 
Quota allocation can be more than capacity. Resources added to the compute allocation policy that aren‚Äôt currently available in the cluster represent planning for future capacity upgrades. Jobs that require these unprovisioned resources will be automatically queued and remain in a pending state until the necessary resources become available. It‚Äôs important to understand that in SageMaker HyperPod, compute allocations function as quotas, which are verified during workload scheduling to understand if a workload should be admitted or not, regardless of actual capacity availability. When resource requests are within these defined allocation limits and current utilization, the Kubernetes scheduler (kube-scheduler) handles the actual distribution and placement of pods across the HyperPod cluster nodes. 
Allocating granular compute and memory quota using AWS CLI 
You can also create or update compute quotas using the AWS CLI. The following is an example for creating a compute quota with only GPU count specification using the AWS CLI: 
 
 aws sagemaker \
create-compute-quota \
--region &lt;aws_region&gt;&nbsp;\
--name "only-gpu-quota" \
--cluster-arn "arn:aws:sagemaker: &lt;aws_region&gt;:&lt;account_id&gt;:cluster/&lt;cluster_id&gt;" \
--description "test description" \
--compute-quota-config "ComputeQuotaResources=[{InstanceType=ml.g6.12xlarge,Accelerators=2}],ResourceSharingConfig={Strategy=LendAndBorrow,BorrowLimit=10}" \
--activation-state "Enabled" \
--compute-quota-target "TeamName=onlygputeam2,FairShareWeight=10"&nbsp; 
 
Compute quotas can also be created with mixed quota types, including a certain number of instances and granular compute resources, as shown in the following example: 
 
 aws sagemaker \
create-compute-quota \
--region &lt;aws_region&gt; \
--name "mix-quota-type" \
--cluster-arn "arn:aws:sagemaker:&lt;aws_region&gt;:&lt;account_id&gt;:cluster/&lt;cluster_id&gt;" \
--description "Mixed quota allocation" \
--compute-quota-config "ComputeQuotaResources=[{InstanceType=ml.g6.12xlarge,Accelerators=2}, {InstanceType=ml.p5.48xlarge,Count=3}, {InstanceType=ml.c5.2xlarge,VCpu=2}],ResourceSharingConfig={Strategy=LendAndBorrow,BorrowLimit=10}" \
--activation-state "Enabled" \
--compute-quota-target "TeamName=mixquotatype,FairShareWeight=10"&nbsp; 
 
HyperPod task governance deep dive 
SageMaker HyperPod task governance enables allocation of GPU, CPU, and memory resources by integrating with Kueue, a Kubernetes-native system for job queueing. 
Kueue doesn‚Äôt replace existing Kubernetes scheduling components, but rather integrates with the kube-scheduler, such that Kueue decides whether a workload should be admitted based on the resource quotas and current utilization, and then the kube-scheduler takes care of pod placement on the nodes. 
When a workload requests specific resources, Kueue selects an appropriate resource flavor based on availability, node affinity, and job priority. The scheduler then injects the corresponding node labels and tolerations into the PodSpec, allowing Kubernetes to place the pod on nodes with the requested hardware configuration. This supports precise resource governance and efficient allocation for multi-tenant clusters. 
When a SageMaker HyperPod task governance compute allocation is created, Kueue creates ClusterQueues that define resource quotas and scheduling policies, along with ResourceFlavors for the selected instance types with their unique resource characteristics. 
For example, the following compute allocation policy allocates ml.g6.12xlarge instances with 2 GPUs and 48 vCPUs to the onlygputeam team, implementing a LendAndBorrow strategy with an up to 50% borrowing limit. This configuration enables flexible resource sharing while maintaining priority through a fair share weight of 10 and the ability to preempt lower priority tasks from other teams. 
 
 aws sagemaker describe-compute-quota \ &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 
--region &lt;aws_region&gt; \
--compute-quota-id &lt;compute_quota_id&gt;

#output
{
&nbsp;&nbsp; &nbsp;"ComputeQuotaArn": "arn:aws:sagemaker:&lt;aws_region&gt;:&lt;account_id&gt;:compute-quota/&lt;compute_quota_id&gt;",
&nbsp;&nbsp; &nbsp;"ComputeQuotaId": "&lt;compute_quota_id&gt;",
&nbsp;&nbsp; &nbsp;"Name": "only-gpu-quota",
&nbsp;&nbsp; &nbsp;"Description": "Only GPU quota allocation",
&nbsp;&nbsp; &nbsp;"ComputeQuotaVersion":&nbsp;1,
&nbsp;&nbsp; &nbsp;"Status": "Created",
&nbsp;&nbsp; &nbsp;"ClusterArn": "arn:aws:sagemaker:&lt;aws_region&gt;:&lt;account_id&gt;:cluster/&lt;cluster_id&gt;",
&nbsp;&nbsp; &nbsp;"ComputeQuotaConfig": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"ComputeQuotaResources": [
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;{
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"InstanceType": "ml.g6.12xlarge",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"Accelerators": 2,
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"VCpu": 48.0
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;],
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"ResourceSharingConfig": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"Strategy": "LendAndBorrow",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"BorrowLimit": 50
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;},
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"PreemptTeamTasks": "LowerPriority"
&nbsp;&nbsp; &nbsp;},
&nbsp;&nbsp; &nbsp;"ComputeQuotaTarget": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"TeamName": "onlygputeam",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"FairShareWeight": 10
&nbsp;&nbsp; &nbsp;},
&nbsp;&nbsp; &nbsp;"ActivationState": "Enabled",
&nbsp;&nbsp; &nbsp;"CreationTime": "2025-07-24T11:12:12.021000-07:00",
&nbsp;&nbsp; &nbsp;"CreatedBy": {},
&nbsp;&nbsp; &nbsp;"LastModifiedTime": "2025-07-24T11:15:45.205000-07:00",
&nbsp;&nbsp; &nbsp;"LastModifiedBy": {}
} 
 
The corresponding Kueue ClusterQueue is configured with the&nbsp;ml.g6.12xlarge flavor, providing quotas for 2 NVIDIA GPUs, 48 CPU cores, and 192 Gi memory. 
 
 kubectl describe clusterqueue hyperpod-ns-onlygputeam-clusterqueue

# output
Name: &nbsp; &nbsp; &nbsp; &nbsp; hyperpod-ns-onlygputeam-clusterqueue
Namespace:
Labels: &nbsp; &nbsp; &nbsp; sagemaker.amazonaws.com/quota-allocation-id=onlygputeam
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;sagemaker.amazonaws.com/sagemaker-managed-queue=true
Annotations: &nbsp;&lt;none&gt;
API Version: &nbsp;kueue.x-k8s.io/v1beta1
Kind: &nbsp; &nbsp; &nbsp; &nbsp; ClusterQueue
Metadata:
&nbsp; ...
Spec:
&nbsp;&nbsp;Cohort: &nbsp;shared-pool
&nbsp;&nbsp;Fair Sharing:
&nbsp;&nbsp; &nbsp;Weight: &nbsp;10
&nbsp;&nbsp;Flavor Fungibility:
&nbsp;&nbsp; &nbsp;When Can Borrow: &nbsp; TryNextFlavor
&nbsp;&nbsp; &nbsp;When Can Preempt: &nbsp;TryNextFlavor
&nbsp;&nbsp;Namespace Selector:
&nbsp;&nbsp; &nbsp;Match Labels:
&nbsp;&nbsp; &nbsp; &nbsp;kubernetes.io/metadata.name: &nbsp;hyperpod-ns-onlygputeam
&nbsp;&nbsp;Preemption:
&nbsp;&nbsp; &nbsp;Borrow Within Cohort:
&nbsp;&nbsp; &nbsp; &nbsp;Policy: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; LowerPriority
&nbsp;&nbsp; &nbsp;Reclaim Within Cohort: &nbsp;Any
&nbsp;&nbsp; &nbsp;Within Cluster Queue: &nbsp; LowerPriority
&nbsp;&nbsp;Queueing Strategy: &nbsp; &nbsp; &nbsp; &nbsp;BestEffortFIFO
&nbsp;&nbsp;Resource Groups:
&nbsp;&nbsp; &nbsp;Covered Resources:
&nbsp;&nbsp; &nbsp; &nbsp;nvidia.com/gpu
&nbsp;&nbsp; &nbsp; &nbsp;aws.amazon.com/neurondevice
&nbsp;&nbsp; &nbsp; &nbsp;cpu
&nbsp;&nbsp; &nbsp; &nbsp;memory
&nbsp;&nbsp; &nbsp; &nbsp;vpc.amazonaws.com/efa
&nbsp;&nbsp; &nbsp;Flavors:
&nbsp;&nbsp; &nbsp; &nbsp;Name: &nbsp;ml.g6.12xlarge
&nbsp;&nbsp; &nbsp; &nbsp;Resources:
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;Borrowing Limit: &nbsp;1
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;Name: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; nvidia.com/gpu
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;Nominal Quota: &nbsp; &nbsp;2
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;Borrowing Limit: &nbsp;0
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;Name: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; aws.amazon.com/neurondevice
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;Nominal Quota: &nbsp; &nbsp;0
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;Borrowing Limit: &nbsp;24
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;Name: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; cpu
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;Nominal Quota: &nbsp; &nbsp;48
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;Borrowing Limit: &nbsp;96Gi
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;Name: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; memory
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;Nominal Quota: &nbsp; &nbsp;192Gi
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;Borrowing Limit: &nbsp;0
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;Name: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; vpc.amazonaws.com/efa
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;Nominal Quota: &nbsp; &nbsp;1
&nbsp;&nbsp; &nbsp;... 
 
A Kueue LocalQueue will be also created, and will reference the corresponding ClusterQueue. The LocalQueue acts as the namespace-scoped resource through which users can submit workloads, and these workloads are then admitted and scheduled according to the quotas and policies defined in the ClusterQueue. 
 
 kubectl describe localqueue hyperpod-ns-onlygputeam-localqueue -n hyperpod-ns-onlygputeam

# output
Name: &nbsp; &nbsp; &nbsp; &nbsp; hyperpod-ns-onlygputeam-localqueue
Namespace: &nbsp; &nbsp;hyperpod-ns-onlygputeam
Labels: &nbsp; &nbsp; &nbsp; sagemaker.amazonaws.com/quota-allocation-id=onlygputeam
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;sagemaker.amazonaws.com/sagemaker-managed-queue=true
Annotations: &nbsp;&lt;none&gt;
API Version: &nbsp;kueue.x-k8s.io/v1beta1
Kind: &nbsp; &nbsp; &nbsp; &nbsp; LocalQueue
Metadata:
&nbsp; &nbsp; ...
Spec:
&nbsp;&nbsp;Cluster Queue: &nbsp;hyperpod-ns-onlygputeam-clusterqueue
&nbsp;&nbsp;Stop Policy: &nbsp; &nbsp;None
Status:
&nbsp;&nbsp;Admitted Workloads: &nbsp;0 
 
Submitting tasks 
There are two ways to submit tasks on Amazon EKS orchestrated SageMaker HyperPod clusters: the SageMaker HyperPod CLI and the Kubernetes command-line tool, kubectl.&nbsp;With both options, data scientists need to reference their team‚Äôs namespace and task priority class‚Äîin addition to the requested GPU and vCPU compute and memory resources‚Äîto use their granular allocated quota with appropriate prioritization. If the user doesn‚Äôt specify a priority class, then SageMaker HyperPod task governance will automatically assume the lowest priority.&nbsp;The specific GPU type comes from an instance type selection, because data scientists want to use GPUs with certain capabilities (for example, H100 instead of H200) to perform their tasks efficiently. 
HyperPod CLI 
The HyperPod CLI was created to abstract the complexities of working with kubectl and so that developers using SageMaker HyperPod can iterate faster with custom commands.The following is an example of a job submission with the HyperPod CLI requesting both compute and memory resources: 
 
 hyp create hyp-pytorch-job \
--job-name sample-job1 \
--image  &lt;account_id&gt;.dkr.ecr.&lt;aws_region&gt;.amazonaws.com/&lt;image_name&gt;:&lt;tag&gt; \
--pull-policy "Always" \
--tasks-per-node 1 \
--max-retry 1 \
--priority high-priority \
--namespace hyperpod-ns-team1 \
--queue-name hyperpod-ns-team1-localqueue \
--instance-type ml.g5.8xlarge \
--accelerators 1 \
--vcpu 4 \
--memory 1 \
--accelerators-limit 1 \
--vcpu-limit 5 \
--memory-limit 2 
 
The highlighted parameters enable requesting granular compute and memory resources. The HyperPod CLI requires to install the HyperPod Training Operator in the cluster and then build a container image that includes the HyperPod Elastic Agent. For further instructions on how to build such container image, please refer to the HyperPod Training Operator documentation. 
For more information on the supported HyperPod CLI arguments and related description, see the SageMaker HyperPod CLI reference documentation. 
Kubectl 
The following is an example of a kubectl command to submit a job to the HyperPod cluster using the specified queue. This is a simple example of a PyTorch job that will check for GPU availability and then sleep for 5 minutes. Compute and memory resources are requested using the standard Kubernetes resource management constructs. 
 
  
  apiVersion: batch/v1 
kind: Job 
metadata: 
  name: gpu-training-job 
  namespace: hyperpod-ns-team1 
spec: 
  parallelism: 1 
  completions: 1 
  suspend: true 
  template: 
    metadata: 
      labels: 
        kueue.x-k8s.io/queue-name: hyperpod-ns-team1-localqueue
        kueue.x-k8s.io/priority-class: high-priority
    spec: 
      containers: 
      - name: training-container 
        image: pytorch/pytorch:2.0.0-cuda11.7-cudnn8-runtime
        command: 
        - "python"
        - "-c"
        - "import torch; print('GPU available:', torch.cuda.is\_available()); import time; time.sleep(15)" 
        resources: 
          requests:  
            nvidia.com/gpu: 1  
            cpu: "4"  
            memory: "1Gi"  
          limits:  
            nvidia.com/gpu: 1         
      restartPolicy: Never 
  
 Sample commands 
 
Following is a short reference guide for helpful commands when interacting with SageMaker HyperPod task governance: 
 
 Describing cluster policy with the AWS CLI ‚Äì This AWS CLI command is useful for viewing the cluster policy settings for your cluster. 
 List compute quota allocations with the AWS CLI ‚Äì Use this AWS CLI command to view the different teams and set up task governance and their respective quota allocation settings. 
 HyperPod CLI ‚Äì The HyperPod CLI abstracts&nbsp;common kubectl commands used to interact with SageMaker HyperPod clusters such as submitting, listing, and cancelling tasks. See the SageMaker HyperPod CLI reference documentation&nbsp;for a full list of commands. 
 kubectl ‚Äì You can also use kubectl&nbsp;to interact with task governance; some useful commands are: 
 
kubectl get workloads -n hyperpod-ns-&lt;team-name&gt; kubectl describe workload &lt;workload-name&gt; -n hyperpod-ns-&lt;team-name&gt;. These commands show the workloads running in your cluster per namespace and provide detailed reasonings on Kueue admission. You can use these commands to answer questions such as ‚ÄúWhy was my task preempted?‚Äù or ‚ÄúWhy did my task get admitted?‚Äù 
Common scenarios 
A common use case for more granular allocation of GPU compute is fine-tuning small and medium sized large language models (LLMs). A single H100 or H200 GPU might be sufficient to address such a use case (also depending on the chosen batch size and other factors), and machine learning (ML) platform administrators can choose to allocate a single GPU to each data scientist or ML researcher to optimize the utilization of an instance like ml.p5.48xlarge, which comes with 8 H100 GPUs onboard. 
Small language models (SLMs) have emerged as a significant advancement in generative AI, offering lower latency, decreased deployment costs, and enhanced privacy capabilities while maintaining impressive performance on targeted tasks, making them increasingly vital for agentic workflows and edge computing scenarios. The new SageMaker HyperPod task governance with fine-grained GPU, CPU, and memory allocation significantly enhances SLM development by enabling precise matching of resources to model requirements, allowing teams to efficiently run multiple experiments concurrently with different architectures. This resource optimization is particularly valuable as organizations develop specialized SLMs for domain-specific applications, with priority-based scheduling so that critical model training jobs receive resources first while maximizing overall cluster utilization. By providing exactly the right resources at the right time, HyperPod accelerates the development of specialized, domain-specific SLMs that can be deployed as efficient agents in complex workflows, enabling more responsive and cost-effective AI solutions across industries. 
With the growing popularity of SLMs, organizations can use granular quota allocation to create targeted quota policies that prioritize GPU resources, addressing the budget-sensitive nature of ML infrastructure where GPUs represent the most significant cost and performance factor. Organizations can now selectively apply CPU and memory limits where needed, creating a granular resource management approach that efficiently supports diverse machine learning workloads regardless of model size. 
Similarly, to support inference workloads, multiple teams might not require an entire instance to deploy their models, helping to avoid having entire instances equipped with multiple GPUs allocated to each team and leaving GPU compute sitting idle. 
Finally, during experimentation and algorithm development, data scientists and ML researchers can choose to deploy a container hosting their preferred IDE on HyperPod, like JupyterLab or Code-OSS (Visual Studio Code open source). In this scenario, they often experiment with smaller batch sizes before scaling to multi-GPU configurations, hence not needing entire multi-GPU instances to be allocated.Similar considerations apply to CPU instances; for example, an ML platform administrator might decide to use CPU instances for IDE deployment, because data scientists prefer to scale their training or fine-tuning with jobs rather than experimenting with the local IDE compute. In such cases, depending on the instances of choice, partitioning CPU cores across the team might be beneficial. 
Conclusion 
The introduction of fine-grained compute quota allocation in SageMaker HyperPod represents a significant advancement in ML infrastructure management. By enabling GPU-level resource allocation alongside instance-level controls, organizations can now precisely tailor their compute resources to match their specific workloads and team structures. 
This granular approach to resource governance addresses critical challenges faced by ML teams today, balancing budget constraints, maximizing expensive GPU utilization, and ensuring fair access across data science teams of all sizes. Whether fine-tuning SLMs that require single GPUs, running inference workloads with varied resource needs, or supporting development environments that don‚Äôt require full instance power, this flexible capability helps ensure that no compute resources sit idle unnecessarily. 
ML workloads continue to diversify in their resource requirements and SageMaker HyperPod task governance now provides the adaptability organizations need to optimize their GPU capacity investments. To learn more, visit the&nbsp;SageMaker HyperPod product page and HyperPod task governance documentation. 
Give this a try in the&nbsp;Amazon SageMaker AI console and leave your comments here. 
 
About the authors 
Siamak Nariman is a Senior Product Manager at AWS. He is focused on AI/ML technology, ML model management, and ML governance to improve overall organizational efficiency and productivity. He has extensive experience automating processes and deploying various technologies. 
Zhenshan Jin is a Senior Software Engineer at Amazon Web Services (AWS), where he leads software development for task governance on SageMaker HyperPod. In his role, he focuses on empowering customers with advanced AI capabilities while fostering an environment that maximizes engineering team efficiency and productivity. 
Giuseppe Angelo Porcelli is a Principal Machine Learning Specialist Solutions Architect for Amazon Web Services. With several years of software engineering and an ML background, he works with customers of any size to understand their business and technical needs and design AI and ML solutions that make the best use of the AWS Cloud and the Amazon Machine Learning stack. He has worked on projects in different domains, including MLOps, computer vision, and NLP, involving a broad set of AWS services. In his free time, Giuseppe enjoys playing football. 
Sindhura Palakodety s a Senior Solutions Architect at AWS and STL for ISV Generative AI, where she is dedicated to empowering customers in developing enterprise-scale, Well-Architected solutions. She specializes in Generative AI and Data Analytics domains, enabling organizations to leverage latest technological innovations for transformative business outcomes.
‚Ä¢ Build and scale adoption of AI agents for education with Strands Agents, Amazon Bedrock AgentCore, and LibreChat
  Basic AI chat isn‚Äôt enough for most business applications. Institutions need AI that can pull from their databases, integrate with their existing tools, handle multi-step processes, and make decisions independently. 
This post demonstrates how to quickly build sophisticated AI agents using Strands Agents, scale them reliably with Amazon Bedrock AgentCore, and make them accessible through LibreChat‚Äôs familiar interface to drive immediate user adoption across your institution. 
Challenges with basic AI chat interfaces 
Although basic AI chat interfaces can answer questions and generate content, educational institutions need capabilities that simple chat can‚Äôt provide: 
 
 Contextual decision-making ‚Äì A student asking ‚ÄúWhat courses should I take?‚Äù needs an agent that can access their transcript, check prerequisites, verify graduation requirements, and consider schedule conflicts‚Äînot just generic course descriptions 
 Multi-step workflows ‚Äì Degree planning requires analyzing current progress, identifying remaining requirements, suggesting course sequences, and updating recommendations as students make decisions 
 Institutional data integration ‚Äì Effective educational AI must connect to student information systems, learning management services, academic databases, and institutional repositories to provide relevant, personalized guidance 
 Persistent memory and learning ‚Äì Agents need to remember previous interactions with students, track their academic journey over semesters, and build understanding of individual learning patterns and needs 
 
Combining open source flexibility with enterprise infrastructure 
The integration presented in this post demonstrates how three technologies can work together to address these challenges: 
 
 Strands Agents ‚Äì Build sophisticated multi-agent workflows in just a few lines of code 
 Amazon Bedrock AgentCore ‚Äì Scale agents reliably with serverless, pay-per-use deployment 
 LibreChat ‚Äì Provide users with a familiar chat interface that drives immediate adoption 
 
Strands Agents overview 
Strands Agents is an open source SDK that takes a model-driven approach to building and running AI agents in just a few lines of code. Unlike LibreChat‚Äôs simple agent implementation, Strands supports sophisticated patterns including multi-agent orchestration through workflow, graph, and swarm tools; semantic search for managing thousands of tools; and advanced reasoning capabilities with deep analytical thinking cycles. The framework simplifies agent development by embracing the capabilities of state-of-the-art models to plan, chain thoughts, call tools, and reflect, while scaling from local development to production deployment with flexible architectures and comprehensive observability. 
Amazon Bedrock AgentCore overview 
Amazon Bedrock AgentCore is a comprehensive set of enterprise-grade services that help developers quickly and securely deploy and operate AI agents at scale using the framework and model of your choice, hosted on Amazon Bedrock or elsewhere. The services are composable and work with popular open source frameworks and many models, so you don‚Äôt have to choose between open source flexibility and enterprise-grade security and reliability. 
Amazon Bedrock AgentCore includes modular services that can be used together or independently: Runtime (secure, serverless runtime for deploying and scaling dynamic agents), Gateway (converts APIs and AWS Lambda functions into agent-compatible tools), Memory (manages both short-term and long-term memory), Identity (provides secure access management), and Observability (offers real-time visibility into agent performance). 
The key Amazon Bedrock AgentCore service used in this integration is Amazon Bedrock AgentCore Runtime, a secure, serverless runtime purpose-built for deploying and scaling dynamic AI agents and tools using an open source framework including LangGraph, CrewAI, and Strands Agents; a protocol; and a model of your choosing. Amazon Bedrock AgentCore Runtime was built to work for agentic workloads with industry-leading extended runtime support, fast cold starts, true session isolation, built-in identity, and support for multimodal payloads. Rather than the typical serverless model where functions spin up, execute, and immediately terminate, Amazon Bedrock AgentCore Runtime provisions dedicated microVMs that can persist for up to 8 hours, enabling sophisticated multi-step agentic workflows where each subsequent call builds upon the accumulated context and state from previous interactions within the same session. 
LibreChat overview 
LibreChat has emerged as a leading open source alternative to commercial AI chat interfaces, offering educational institutions a powerful solution for deploying conversational AI at scale. Built with flexibility and extensibility in mind, LibreChat provides several key advantages for higher education: 
 
 Multi-model support ‚Äì LibreChat supports integration with multiple AI providers, so institutions can choose the most appropriate models for different use cases while avoiding vendor lock-in 
 User management ‚Äì Robust authentication and authorization systems help institutions manage access across student populations, faculty, and staff with appropriate permissions and usage controls 
 Conversation management ‚Äì Students and faculty can organize their AI interactions into projects and topics, creating a more structured learning environment 
 Customizable interface ‚Äì The solution can be branded and customized to match institutional identity and specific pedagogical needs 
 
Integration benefits 
Integrating Strands Agents with Amazon Bedrock AgentCore and LibreChat creates unique benefits that extend the capabilities of both services far beyond what either could achieve independently: 
 
 Seamless agent experience through familiar interface ‚Äì LibreChat‚Äôs intuitive chat interface becomes a gateway to sophisticated agentic workflows. Users can trigger complex multi-step processes, data analysis, and external system integrations through natural conversation, without needing to learn new interfaces or complex APIs. 
 Dynamic agent loading and management ‚Äì Unlike static AI chat implementations, this integration supports dynamic agent loading with access management. New agentic applications can be deployed separately and made available to users without requiring LibreChat updates or downtime, enabling rapid agent development. 
 Enterprise-grade security and scaling ‚Äì Amazon Bedrock AgentCore Runtime provides complete session isolation for each user session, where each session runs with isolated CPU, memory, and filesystem resources. This creates complete separation between user sessions, safeguarding stateful agent reasoning processes and helping prevent cross-session data contamination. The service can scale up to thousands of agent sessions in seconds while developers only pay for actual usage, making it ideal for educational institutions that need to support large student populations with varying usage patterns. 
 Built-in AWS resource integration ‚Äì Organizations already running infrastructure on AWS can seamlessly connect their existing resources‚Äîdatabases, data lakes, Lambda functions, and applications‚Äîto Strands Agents without complex integrations or data movement. Agents can directly access and surface insights through the LibreChat interface, turning existing AWS investments into intelligent, conversational experiences, such as querying an Amazon Relational Database Service (Amazon RDS) database, analyzing data in Amazon Simple Storage Service (Amazon S3), or integrating with existing microservices. 
 Cost-effective agentic computing ‚Äì By using LibreChat‚Äôs efficient architecture with the Amazon Bedrock AgentCore pay-per-use model, organizations can deploy sophisticated agentic applications without the high fixed costs typically associated with enterprise AI systems. Users only pay for actual agent computation and tool usage. 
 
Agent use cases in higher education settings 
The integration of LibreChat with Strands Agents enables numerous educational applications that demonstrate the solution‚Äôs versatility and power: 
 
 A course recommendation agent can analyze a student‚Äôs academic history, current enrollment, and career interests to suggest relevant courses. By integrating with the student information system, the agent can make sure recommendations consider prerequisites, schedule conflicts, and graduation requirements. 
 A degree progress tracking agent can interact with students and help them understand their specific degree requirements and provide guidance on remaining coursework, elective options, and timeline optimization. 
 Agents can be configured with access to academic databases and institutional repositories, helping students and faculty discover relevant research papers and resources, providing guidance on academic writing, citation formats, and research methodology specific to different disciplines. 
 Agents can handle routine student inquiries about registration, deadlines, and campus resources, freeing up staff time for more complex student support needs. 
 
Refer to the following GitHub repo for Strands Agent code examples for educational use cases. 
Solution overview 
The following architecture diagram illustrates the overall system design for deploying LibreChat with Strands Agents integration. Strands Agents is deployed using Amazon Bedrock AgentCore Runtime, a secure, serverless runtime purpose-built for deploying and scaling dynamic AI agents and tools using an open source framework including Strands Agents. 
 
The solution architecture includes several key components: 
 
 LibreChat core services ‚Äì The core chat interface runs in an Amazon Elastic Container Service (Amazon ECS) with AWS Fargate cluster, including LibreChat for the user-facing experience, Meilisearch for enhanced search capabilities, and Retrieval Augmented Generation (RAG) API services for document retrieval. 
 LibreChat supporting infrastructure ‚Äì This solution uses Amazon Elastic File System (Amazon EFS) for storing Meilisearch‚Äôs indexes and user uploaded files; Amazon Aurora PostgreSQL-Compatible Edition for vector database used by the RAG API; Amazon S3 for storing LibreChat configurations; Amazon DocumentDB for user, session, conversation data management; and AWS Secrets Manager for managing access to the resources. 
 Strands Agents integration ‚Äì This solution integrates Strands Agents (hosted by Amazon Bedrock AgentCore Runtime) with LibreChat through custom endpoints using Lambda and Amazon API Gateway. This integration pattern enables dynamic loading of agents in LibreChat for advanced generative AI capabilities. In particularly, the solution showcases a user activity analysis agent that draws insights from LibreChat logs. 
 Authentication and security ‚Äì The integration between LibreChat and Strands Agents implements a multi-layered authentication approach that maintains security without compromising user experience or administrative simplicity. When a student or faculty member selects a Strands Agent from LibreChat‚Äôs interface, the authentication flow operates seamlessly in the background through several coordinated layers: 
   
   User authentication ‚Äì LibreChat handles user login through your institution‚Äôs existing authentication system, with comprehensive options including OAuth, LDAP/AD, or local accounts as detailed in the LibreChat authentication documentation. 
   API Gateway security ‚Äì After users are authenticated to LibreChat, the system automatically handles API Gateway security by authenticating each request using preconfigured API keys. 
   Service-to-service authentication ‚Äì The underlying Lambda function uses AWS Identity and Access Management (IAM) roles to securely invoke Amazon Bedrock AgentCore Runtime where the Strands Agent is deployed. 
   Resource access control ‚Äì Strands Agents operate within defined permissions to access only authorized resources. 
    
 
Deployment process 
This solution uses the AWS Cloud Development Kit (AWS CDK) and AWS CloudFormation to handle the deployment through several automated phases. We will use a log analysis agent as an example to demonstrate the deployment process. The agent makes it possible for the admin to perform LibreChat log analysis through natural language queries. 
LibreChat is deployed as a containerized service with ECS Fargate clusters and is integrated with supporting services, including virtual private cloud (VPC) networking, Application Load Balancer (ALB), and the complete data layer with Aurora PostgreSQL-Compatible, DocumentDB, Amazon EFS, and Amazon S3 storage. Security is built in with appropriate IAM roles, security groups, and secrets management. 
The user activity analysis agent provides valuable insights into how students interact with AI tools, identifying peak usage times, popular topics, and potential areas where students might need additional support. The agent is automatically provisioned using the following CloudFormation template, which deploys Strands Agents using Amazon Bedrock AgentCore Runtime, provisions a Lambda function that invokes the agent, API Gateway to make the agent a URL endpoint, and a second Lambda function that accesses LibreChat logs stored in DocumentDB. The second Lambda is used as a tool of the agent. 
The following code shows how to configure LibreChat to make the agent a custom endpoint: 
 
 custom:
    - name: 'log-analysis-assitant'
      apiKey: '{AWS_API_GATEWAY_KEY}'
      baseURL: '{AWS_API_GATEWAY_URL}'
      models:
        default: ['Strands Agent']
        fetch: false
      headers:
        x-api-key: '{AWS_API_GATEWAY_KEY}'
      titleConvo: true
      titleModel: 'us.amazon.nova-lite-v1:0'
      modelDisplayLabel: 'log-analysis-assitant'
      forcePrompt: false
      stream: false
      iconURL: 'https://d1.awsstatic.com/onedam/marketing-channels/website/aws/en_US/
             product-categories/ai-ml/machine-learning/approved/images/256f3da1-3193-
             441c-b2641f33fdd6.a045b9b4c4f34545e1c79a405140ac0146699835.jpeg' 
 
 After the stack is deployed successfully, you can log in to LibreChat, select the agent, and start chatting. The following screenshot shows an example question that the user activity analysis agent can help answer, where it reads the LibreChat user activities from DocumentDB and generates an answer.
 
 
Deployment considerations and best practices 
When deploying this LibreChat and Strands Agents integration, organizations should carefully consider several key factors that can significantly impact both the success of the implementation and its long-term sustainability. 
Security and compliance form the foundation of any successful deployment, particularly in educational environments where data protection is paramount. Organizations must implement robust data classification schemes to maintain appropriate handling of sensitive information, and role-based access controls make sure users only access AI capabilities and data appropriate to their roles. Beyond traditional perimeter security, a layered authorization approach becomes critical when deploying AI systems that might access multiple data sources with varying sensitivity levels. This involves implementing multiple authorization checks throughout the application stack, including service-to-service authorization, trusted identity propagation that carries the end-user‚Äôs identity through the system components, and granular access controls that evaluate permissions at each data access point rather than relying solely on broad service-level permissions. Such layered security architectures help mitigate risks like prompt injection vulnerabilities and unauthorized cross-tenant data access, making sure that even if one security layer is compromised, additional controls remain in place to protect sensitive educational data. Regular compliance monitoring becomes essential, with automated audits and checks maintaining continued adherence to relevant data protection regulations throughout the system‚Äôs lifecycle, while also validating that layered authorization policies remain effective as the system evolves. 
Cost management requires a strategic approach that balances functionality with financial sustainability. Organizations must prioritize their generative AI spending based on business impact and criticality while maintaining cost transparency across customer and user segments. Implementing comprehensive usage monitoring helps organizations track AI service consumption patterns and identify optimization opportunities before costs become problematic. The human element of deployment often proves more challenging than the technical implementation. Faculty training programs should provide comprehensive guidance on integrating AI tools into teaching practices, focusing not just on how to use the tools but how to use them effectively for educational outcomes. Student onboarding requires clear guidelines and tutorials that promote both effective AI interaction and academic integrity. Perhaps most importantly, establishing continuous feedback loops makes sure the system evolves based on actual user experiences and measured educational outcomes rather than assumptions about what users need.Successful deployments also require careful attention to the dynamic nature of AI technology. The architecture‚Äôs support for dynamic agent loading enables organizations to add specialized agents for new departments or use cases without disrupting existing services. Version control systems should maintain different agent versions for testing and gradual rollout of improvements, and performance monitoring tracks both technical metrics and user satisfaction to guide continuous improvement efforts. 
Conclusion 
The integration of LibreChat with Strands Agents represents a significant step forward in democratizing access to advanced AI capabilities in higher education. By combining the accessibility and customization of open source systems with the sophistication and reliability of enterprise-grade AI services, institutions can provide students and faculty with powerful tools that enhance learning, research, and academic success.This architecture demonstrates that educational institutions don‚Äôt need to choose between powerful AI capabilities and institutional control. Instead, they can take advantage of the innovation and flexibility of open source solutions with the scalability and reliability of cloud-based AI services. The integration example showcased in this post illustrates the solution‚Äôs versatility and potential for customization as institutions expand and adapt the solution to meet evolving educational needs. 
For future work, the LibreChat system‚Äôs Model Context Protocol (MCP) server integration capabilities offer exciting possibilities for enhanced agent architectures. A particularly promising avenue involves wrapping agents as MCP servers, transforming them into standardized tools that can be seamlessly integrated alongside other MCP-enabled agents. This approach would enable educators to compose sophisticated multi-agent workflows, creating highly personalized educational experiences tailored to individual learning styles. 
The future of education is about having the right AI tools, properly integrated and ethically deployed, to enhance human learning and achievement through flexible, interoperable, and extensible solutions that can evolve with educational needs. 
Acknowledgement 
The authors extend their gratitude to Arun Thangavel, Ashish Rawat and Kosti Vasilakakis for their insightful feedback and review of the post. 
 
About the authors 
Dr. Changsha Ma is a Senior AI/ML Specialist at AWS. She is a technologist with a PhD in Computer Science, a master‚Äôs degree in Education Psychology, and years of experience in data science and independent consulting in AI/ML. She is passionate about researching methodological approaches for machine and human intelligence. Outside of work, she loves hiking, cooking, and spending time with friends and families. 
Sudheer Manubolu&nbsp;is a Solutions Architect at Amazon Web Services (AWS). He specializes in cloud architecture, enterprise solutions, and AI/ML implementations. He provides technical and architectural guidance to customers building transformative solutions on AWS, with particular emphasis on leveraging AWS‚Äôs AI/ML and container services to drive innovation and operational excellence. 
Abhilash Thallapally is a Solutions Architect at AWS helping public sector customers design and build scalable AI/ML solutions using Amazon SageMaker. His work covers a wide range of ML use cases, with a primary interest in computer vision, Generative AI, IoT, and deploying cost optimized solutions on AWS. 
Mary Strain leads strategy for artificial intelligence and machine learning for US education at AWS. Mary began her career as a middle school teacher in the Bronx, NY. Since that time, she has held leadership roles in education and public sector technology organizations. Mary has advised K12, higher education, and state and local government on innovative policies and practices in competency based assessment, curriculum design, micro credentials and workforce development initiatives. As an advisor to The Education Design Studio at The University of Pennsylvania, The Coalition of Schools Educating Boys of Color, and The State of NJ AI Task Force, Mary has been on the leading edge of bringing innovative solutions to education for two decades.

‚∏ª