✅ Morning News Briefing – July 30, 2025 10:50

📅 Date: 2025-07-30 10:50
🏷️ Tags: #briefing #ai #publichealth #digitalgov

⸻

🧾 Weather
• No watches or warnings in effect, Pembroke
  No watches or warnings in effect. No warnings or watches or watches in effect . Watch or warnings are no longer in effect in the U.S. No watches, warnings are in effect for the rest of the day . No watches and warnings are still in effect, but no watches are in place for the day's events . The weather is not expected to be affected by the weather .
• Current Conditions: Fog, 17.9°C
  Temperature: 17.9&deg;C Pressure: 101.7 kPa Visibility: 5 km Humidity: 94 % Dewpoint: 16.9/deg; C Wind: W 9 km/h . Air Quality Health Index: n/a . Weather conditions: Fog, pressure, humidity, temperature, pressure and humidity . Conditions: Cold, fog, high humidity,
• Wednesday: A mix of sun and cloud. High 25.
  A mix of sun and cloud is expected to hit 25 degrees in the morning . Hazy conditions will be felt throughout the day . Humidex 29. UV index 8 or very high is very high in the afternoon, with a very high UV index of up to 8 in the evening . High temperatures will be 25 degrees Celsius in the early hours of the morning, with highs of 25

🌍 International News
No updates.

🍁 Canadian News
No updates.

🇺🇸 U.S. Top Stories
• Is joy an act of resistance?
  The phrase "joy is resistance" has been popping up all over the place lately . But what, exactly, does it mean? In this episode, we're unpacking what joy is, when it can actually be used as a tool for social change . And why the slogan has become so popular (even when joy itself feels more tenuous) We'll unpack what joy really is
• Dozens of state laws take aim at food dyes, amid a wave support for MAHA
  State lawmakers are moving to remove dyes and other additives from food . A wide range of state laws could make it difficult for manufacturers and could spur further federal regulation . MAHA is a national movement that aims to remove food additives from the food industry . The group wants to reduce the use of dyes in food by removing dyes, other additives, from the products of the food .
• Trump keeps pressuring the Fed to cut rates. Here's why its independence matters
  Trump has threatened to fire Fed Chair Jerome Powell, challenging the Fed's independence . Experts say he's not the first president to target the central bank, but he's the most public and aggressive . Trump is the most aggressive and aggressive president to threaten to fire the Fed Chair, according to experts . Trump has said he wants to keep the Fed from being stripped of power to the Fed .
• Why certain medications can increase your risk in the heat
  Some medicines affect your ability to sweat, stay hydrated, or even to notice if you're overheating . Doctors say keep taking them, but make sure to keep yourself cool . Make sure to stay cool, but not to get dehydrated, doctors say . Keep taking medicines, but keep making sure to be cool with your body heat down, doctor says . Stay hydrated with your
• India says it killed militants behind the deadly attack on civilians in Kashmir
  Three months after militants killed 26 tourists at a scenic meadow in the Himalayas, India said on Tuesday that its security forces had found and killed three gunmen behind the massacre . 26 tourists were killed by militants in the meadow three months ago . India says security forces have found the gunmen responsible for the massacre and have killed three of the militants behind the attack on tourists in Kashmir .

🧠 Artificial Intelligence
No updates.

💻 Digital Strategy
• Oracle VirtualBox licensing tweak lies in wait for the unwary
  Java-like move could land those expecting free trial with a new bill . Oracle has introduced new licensing terms that some users may see as hidden within the terms for VirtualBox . VirtualBox, the general-purpose virtualization software for x86_64 hardware, is now available on sale for $99,000 . Oracle's VirtualBox is available on the market for $100,000
• Flock storage: Audio boffin encodes data in a starling
  Birdsong stores 176 KB, but can it run Doom? Forget flash storage – flock storage is here after it was demonstrated that data can be saved to a bird . Birdsong has been shown to be able to store data in a bird’s nest . Birdsongs can also be used to store photos, videos, photos and other data stored in the bird's nest, say experts
• Datacenter lobby blows a fuse over EU efficiency proposals
  Trade body representing datacenter operators in Europe worried about standards for efficiency imposed by the EU has published a report to ensure its arguments are heard first . The report was published by a trade body representing operators worried about efficiency standards for the EU . Amazon, Microsoft and Google are among those worried about the impact of the new rules on the environment . Green rules risk short-circuiting AI
• Europe's AI crackdown starts this week and Big Tech isn't happy
  European Union first proposed legislation to govern tech companies that build AI systems and how users deploy them . A lot has changed since then . Users and developers struggle to comply as situation evolves . It is a little more than four years since the EU first proposed the legislation . The EU is now trying to work out how to regulate the use of AI systems in the U.S. and Europe .
• Cisco donates Agntcy project to Linux Foundation in the hope it gets AI agents interacting elegantly
  AI frameworks are becoming a Russian nesting doll of abstraction layers . Cisco's Agntcy project is the latest AI framework to find refuge at the Linux Foundation . AI framework is becoming the latest project to be taken to the Linux foundation by the likes of Cisco and Google . The project is based on Cisco's new software, called Agncy, and is being developed in partnership with the Linux

🏥 Public Health
No updates.

🔬 Science
• Google searches for alcohol and association with alcohol-related outpatient encounters before and during the COVID-19 pandemic
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
• Association of participant reported oral health, tooth loss, interdental cleaning and dental visits with serum CRP
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
• Additive effect of wildfires on hospital admission in the Pantanal wetland, Brazil
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
• Investigating the influence of urban land use on asbestos concentration and identifying the most vulnerable areas in Shiraz, Iran
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
• Africa needs to invest in science communication — here’s how
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations

🧾 Government & Policy
No updates.

🏛️ Enterprise Architecture & IT Governance
No updates.

🤖 AI & Emerging Tech
• Exclusive: A record-breaking baby has been born from an embryo that’s over 30 years old
  A baby boy born over the weekend holds the new record for the “oldest baby.” Thaddeus Daniel Pierce, who arrived on July 26, developed from an embryo that had been in storage for 30 and a half years.



“We had a rough birth but we are both doing well now,” says Lindsey Pierce, his mother.&nbsp;&#8220;He is so chill. We are in awe that we have this precious baby!&#8221;



Lindsey and her husband, Tim Pierce, who live in London, Ohio, “adopted” the embryo from a woman who had it created in 1994. She says her family and church family think “it’s like something from a sci-fi movie.” 



“The baby has a 30-year-old sister,” she adds. Tim was a toddler when the embryos were first created.



“It’s been pretty surreal,” says Linda Archerd, 62, who donated the embryo. “It’s hard to even believe.”



Three little hopes



The story starts back in the early 1990s. Archerd had been trying—and failing—to get pregnant for six years. She and her husband decided to try IVF, a fairly new technology at the time. “People were [unfamiliar] with it,” says Archerd. “A lot of people were like, what are you doing?”



They did it anyway, and in May 1994, they managed to create four embryos. One of them was transferred to Linda’s uterus. It resulted in a healthy baby girl. “I was so blessed to have a baby,” Archerd says. The remaining three embryos were cryopreserved and kept in a storage tank.



That was 31 years ago. The healthy baby girl is now a 30-year-old woman who has her own 10-year-old daughter. But the other three embryos remained frozen in time.



Archerd originally planned to use the embryos herself. “I always wanted another baby desperately,” she says. “I called them my three little hopes.” Her then husband felt differently, she says. Archerd went on to divorce him, but she won custody of the embryos and kept them in storage, still hopeful she might use them one day, perhaps with another partner.



That meant paying annual storage fees, which increased over time and ended up costing Archerd around a thousand dollars a year, she says. To her, it was worth it. “I always thought it was the right thing to do,” she says.&nbsp;



Things changed when she started going through menopause, she says. She considered her options. She didn’t want to discard the embryos or donate them for research. And she didn’t want to donate them to another family anonymously—she wanted to meet the parents and any resulting babies. “It’s my DNA; it came from me … and [it’s] my daughter’s sibling,” she says.





Then she found out about embryo “adoption.” This is a type of embryo donation in which both donors and recipients have a say in whom they “place” their embryos with or “adopt” them from. It is overseen by agencies—usually explicitly religious ones—that believe an embryo is morally equivalent to a born human. Archerd is Christian.



There are several agencies that offer these adoption services in the US, but not all of them accept embryos that have been stored for a very long time. That’s partly because those embryos will have been frozen and stored in unfamiliar, old-fashioned ways, and partly because old embryos are thought to be less likely to survive thawing and transfer to successfully develop into a baby.



“So many places wouldn’t even take my information,” says Archerd. Then she came across the Snowflakes program run by the Nightlight Christian Adoptions agency. The agency was willing to accept her embryos, but it needed Archerd’s medical records from the time the embryos had been created, as well as the embryos’ lab records.



So Archerd called the fertility doctor who had treated her decades before. “I still remembered his phone number by heart,” she says. That doctor, now in his 70s, is still practicing at a clinic in Oregon. He dug Archerd’s records out from his basement, she says. “Some of [them] were handwritten,” she adds. Her embryos entered Nightlight’s “matching pool” in 2022.



Making a match



“Our matching process is really driven by the preferences of the placing family,” says Beth Button, executive director of the Snowflakes program. Archerd’s preference was for a married Caucasian, Christian couple living in the US. “I didn’t want them to go out of the country,” says Archerd. “And being Christian is very important to me, because I am.”



It took a while to find a match. Most of the “adopting parents” signed up for the Snowflakes program were already registered at fertility clinics that wouldn’t have accepted the embryos, says Button. “I would say that over 90% of clinics in the US would not have accepted these embryos,” she says.



Lindsey and Tim Pierce at Rejoice Fertility.COURTESY LINDSEY PIERCE




Archerd’s embryos were assigned to the agency’s Open Hearts program for embryos that are “hard to place,” along with others that have been in storage for a long time or are otherwise thought to be less likely to result in a healthy birth.



Lindsey and Tim Pierce had also signed up for the Open Hearts program. The couple, aged 35 and 34, respectively, had been trying for a baby for seven years and had seen multiple doctors.



Lindsey was researching child adoption when she came across the Snowflakes program.&nbsp;



When the couple were considering their criteria for embryos they might receive, they decided that they’d be open to any. “We checkmarked anything and everything,” says Tim. That’s how they ended up being matched with Archerd’s embryos. “We thought it was wild,” says Lindsey. “We didn’t know they froze embryos that long ago.”



Lindsey and Tim had registered with Rejoice Fertility, an IVF clinic in Knoxville, Tennessee, run by John Gordon, a reproductive endocrinologist who prides himself on his efforts to reduce the number of embryos in storage. The huge numbers of embryos left in storage tanks was weighing on his conscience, he says, so around six years ago, he set up Rejoice Fertility with the aim of doing things differently.  



“Now we’re here in the belt buckle of the Bible Belt,” says Gordon, who is Reformed Presbyterian. “I’ve changed my mode of practice.” IVF treatments performed at the clinic are designed to create as few excess embryos as possible. The clinic works with multiple embryo adoption agencies and will accept any embryo, no matter how long it has been in storage.



COURTESY LINDA ARCHERD




It was his clinic that treated the parents who previously held the record for the longest-stored embryo—in 2022, Rachel and Philip Ridgeway had twins from embryos created more than 30 years earlier. “They’re such a lovely couple,” says Gordon. When we spoke, he was making plans to meet the family for breakfast. The twins are “growing like weeds,” he says with a laugh.



“We have certain guiding principles, and they’re coming from our faith,” says Gordon, although he adds that he sees patients who hold alternative views. One of those principles is that “every embryo deserves a chance at life and that the only embryo that cannot result in a healthy baby is the embryo not given the opportunity to be transferred into a patient.”



That’s why his team will endeavor to transfer any embryo they receive, no matter the age or conditions. That can be challenging, especially when the embryos have been frozen or stored in unusual or outdated ways. “It’s scary for people who don’t know how to do it,” says Sarah Atkinson, lab supervisor and head embryologist at Rejoice Fertility. “You don’t want to kill someone’s embryos if you don’t know what you’re doing.”



Cumbersome and explosive



In the early days of IVF, embryos earmarked for storage were slow-frozen. This technique involves gradually lowering the temperature of the embryos. But because slow freezing can cause harmful ice crystals to form, clinics switched in the 2000s to a technique called vitrification, in which the embryos are placed in thin plastic tubes called straws and lowered into tanks of liquid nitrogen. This rapidly freezes the embryos and converts them into a glass-like state. 



The embryos can later be thawed by removing them from the tanks and rapidly—within two seconds—plunging them into warm “thaw media,” says Atkinson. Thawing slow-frozen embryos is more complicated. And the exact thawing method required varies, depending on how the embryos were preserved and what they were stored in. Some of the devices need to be opened while they are inside the storage tank, which can involve using forceps, diamond-bladed knives, and other tools in the liquid nitrogen, says Atkinson.



Sarah Atkinson, lab supervisor and head embryologist at Rejoice Fertility, directly injects sperm into two eggs to fertilize them.COURTESY OF SARAH ATKINSON AT REJOICE FERTILITY.




Recently, she was tasked with retrieving embryos that had been stored inside a glass vial. The vial was made from blown glass and had been heat-sealed with the embryo inside. Atkinson had to use her diamond-bladed knife to snap open the seal inside the nitrogen tank. It was fiddly work, and when the device snapped, a small shard of glass flew out and hit Atkinson’s face. “Hit me on the cheek, cut my cheek, blood running down my face, and I’m like, Oh shit,” she says. Luckily, she had her safety goggles on. And the embryos survived, she adds.



The two embryos that were transferred to Lindsey Pierce.



Atkinson has a folder in her office with notes she’s collected on various devices over the years. She flicks through it over a video call and points to the notes she made about the glass vial. “Might explode; wear face shield and eye protection,” she reads. A few pages later, she points to another embryo-storage device. “You have to thaw this one in your fingers,” she tells me. “I don’t like it.”



The record-breaking embryos had been slow-frozen and stored in a plastic vial, says Atkinson. Thawing them was a cumbersome process. But all three embryos survived it.



The Pierces had to travel from their home in Ohio to the clinic in Tennessee five times over a two-week period. “It was like a five-hour drive,” says Lindsey. One of the three embryos stopped growing. The other two were transferred to Lindsey’s uterus on November 14, she says. And one developed into a fetus.



Now that the baby has arrived, Archerd is keen to meet him. “The first thing that I noticed when Lindsey sent me his pictures is how much he looks like my daughter when she was a baby,” she says. “I pulled out my baby book and compared them side by side, and there is no doubt that they are siblings.”



She doesn’t yet have plans to meet the baby, but doing so would be “a dream come true,” she says. “I wish that they didn’t live so far away from me &#8230; He is perfect!”



“We didn’t go into it thinking we would break any records,” says Lindsey. “We just wanted to have a baby.”
• OpenAI is launching a version of ChatGPT for college students
  OpenAI is launching Study Mode, a version of ChatGPT for college students . It's part of a push by the company to get AI more embedded into classrooms when the new academic year starts in September . The tool is free for now, but users say it does a good job of checking their understanding and adapting to their pace . Underneath the hood, it is not a tool trained exclusively on academic textbooks .
• The Download: how to store energy underground, and what you may not know about Trump’s AI Action Plan
  This is today&#8217;s edition of The Download, our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



This startup wants to use the Earth as a massive battery



Texas-based startup Quidnet Energy just completed a test showing it can store energy for up to six months by pumping water underground.Using water to store electricity is hardly a new concept—pumped hydropower storage has been around for over a century. But the company hopes its twist on the technology could help bring cheap, long-duration energy storage to new places. Read the full story.



—Casey Crownhart







What you may have missed about Trump’s AI Action Plan



The executive orders and announcements coming from the White House since Donald Trump returned to office have painted an ambitious vision for America’s AI future, but the details have been sparse.&nbsp;



The White House’s AI Action Plan, released last week, is meant to fix that. Trump wants to boost the buildout of data centers by slashing environmental rules; withhold funding from states that pass “burdensome AI regulations”; and contract only with AI companies whose models are “free from top-down ideological bias.”



But if you dig deeper, certain parts of the plan that didn’t pop up in any headlines reveal more about where the administration’s AI plans are headed. Here are three of the most important issues to watch.



—James O&#8217;Donnell



This story originally appeared in The Algorithm, our weekly newsletter on AI. To get stories like this in your inbox first, sign up here.







The must-reads



I’ve combed the internet to find you today’s most fun/important/scary/fascinating stories about technology.



1 Democrats aren’t happy about Trump’s China chip U-turnThey’re worried about the security implications of approving exporting Nvidia chips. (WP $)+ They claim the Trump administration is using export controls as a bargaining chip. (The Hill)+ Meanwhile, both parties are planning new bills targeting China. (Reuters)



2 US tariffs are at their highest level since before WWIITrump’s tariff wall appears likely to trigger a global reordering of trade. (FT $)+ But who picks up the bill? (The Guardian)+ Sweeping tariffs could threaten the US manufacturing rebound. (MIT Technology Review)



3 Utility companies want Big Tech to pay more for their data centersOtherwise, rates may end up rising for regular customers. (WSJ $)+ The data center boom in the desert. (MIT Technology Review)



4 Citizen science is on the rise across the USPlatform iNaturalist is playing a key role in helping to identify new species. (NYT $)+ How nonprofits and academia are stepping up to salvage US climate programs. (MIT Technology Review)



5 Anthropic is cracking down on Claude power usersSome of its customers are running its AI coding tool 24/7. (TechCrunch)+ That’s seriously bad news for the environment. (Engadget)+ We did the math on AI’s energy footprint. Here’s the story you haven’t heard. (MIT Technology Review)6 MAHA might resurrect psychedelic therapyLast year, the FDA rejected MDMA therapy. Now, it might get thrown a lifeline. (Wired $)+ People are using AI to ‘sit’ with them while they trip on psychedelics. (MIT Technology Review)



7 Waymo is launching its robotaxi service in DallasIn a new partnership with car rental firm Avis, not Uber. (Reuters)+ It’s expanding steadily, unlike its rival Tesla. (Forbes $)



8 How a promising young coder wound up at DOGELuke Farritor has assessed, slashed, and dismantled at least 10 departments. (Bloomberg $)+ The foundations of America’s prosperity are being dismantled. (MIT Technology Review)



9 This Californian startup’s robot kills fish the Japanese way The method is considered the most humane way to kill them. (Semafor)



10 AI is making online shopping hyper-personalized By serving up results for searches like “revenge dress to wear to a party in Sicily.” (CNN)







Quote of the day



“Now I&#8217;ll click the &#8216;Verify you are human&#8217; checkbox…this step is necessary to prove I&#8217;m not a bot.”



—OpenAI’s new ChatGPT Agent explains how it passes a common internet security checkpoint designed to catch bots just like it, Ars Technica reports.







One more thing







How gamification took over the worldIt’s a thought that occurs to every video-game player at some point: What if the weird, hyper-focused state I enter when playing in virtual worlds could somehow be applied to the real one?Often pondered during especially challenging or tedious tasks in meatspace (writing essays, say, or doing your taxes), it’s an eminently reasonable question to ask. Life, after all, is hard. And while video games are too, there’s something almost magical about the way they can promote sustained bouts of superhuman concentration and resolve.For some, this phenomenon leads to an interest in flow states and immersion. For others, it’s simply a reason to play more games. For a handful of consultants, startup gurus, and game designers in the late 2000s, it became the key to unlocking our true human potential. But instead of liberating us, gamification turned out to be just another tool for coercion, distraction, and control. Read the full story.



—Bryan Gardiner







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)



+ USPS is taking votes from the public to bring back their favorite stamps (thanks Amy!)+ Here’s how to make your morning toast that bit more interesting.+ The long-awaited Madonna biopic is still happening, apparently.+ Bad news for matcha fans—there’s a global shortage
• What you may have missed about Trump’s AI Action Plan
  A number of the executive orders and announcements coming from the White House since Donald Trump returned to office have painted an ambitious vision for America’s AI future—crushing competition with China, abolishing “woke” AI models that suppress conservative speech, jump-starting power-hungry AI data centers. But the details have been sparse.&nbsp;



The White House’s AI Action Plan, released last week, is meant to fix that. Many of the points in the plan won’t come as a surprise, and you’ve probably heard of the big ones by now. Trump wants to boost the buildout of data centers by slashing environmental rules; withhold funding from states that pass “burdensome AI regulations”; and contract only with AI companies whose models are “free from top-down ideological bias.”



But if you dig deeper, certain parts of the plan that didn’t pop up in any headlines reveal more about where the administration’s AI plans are headed. Here are three of the most important issues to watch.&nbsp;



Trump is escalating his fight with the Federal Trade Commission



When Americans get scammed, they’re supposed to be helped by the Federal Trade Commission. As I wrote last week, the FTC under President Biden increasingly targeted AI companies that overhyped the accuracy of their systems, as well as deployments of AI it found to have harmed consumers.&nbsp;



The Trump plan vows to take a fresh look at all the FTC actions under the previous administration as part of an effort to get rid of “onerous” regulation that it claims is hampering AI’s development. The administration may even attempt to repeal some of the FTC’s actions entirely. This would weaken a major AI watchdog agency, but it’s just the latest in the Trump administration’s escalating attacks on the FTC. Read more in my story.&nbsp;



The White House is very optimistic about AI for science



The opening to the AI Action Plan describes a future where AI is doing everything from discovering new materials and drugs to “unraveling ancient scrolls once thought unreadable” to making breakthroughs in science and math.&nbsp;



That type of unbounded optimism about AI for scientific discovery echoes what tech companies are promising. Some of that optimism is grounded in reality: AI’s role in predicting protein structures has indeed led to material scientific wins (and just last week, Google DeepMind released a new AI meant to help interpret ancient Latin engravings). But the idea that large language models—essentially very good text prediction machines—will act as scientists in their own right has less merit so far.&nbsp;



Still, the plan shows that the Trump administration wants to award money to labs trying to make it a reality, even as it has worked to slash the funding the National Science Foundation makes available to human scientists, some of whom are now struggling to complete their research.&nbsp;



And some of the steps the plan proposes are likely to be welcomed by researchers, like funding to build AI systems that are more transparent and interpretable.





The White House’s messaging on deepfakes is confused



Compared with President Biden’s executive orders on AI, the new action plan is mostly devoid of anything related to making AI safer.&nbsp;



However, there’s a notable exception: a section in the plan that takes on the harms posed by deepfakes. In May, Trump signed legislation to protect people from nonconsensual sexually explicit deepfakes, a growing concern for celebrities and everyday people alike as generative video gets more advanced and cheaper to use. The law had bipartisan support.



Now, the White House says it’s concerned about the issues deepfakes could pose for the legal system. For example, it says, “fake evidence could be used to attempt to deny justice to both plaintiffs and defendants.” It calls for new standards for deepfake detection and asks the Department of Justice to create rules around it. Legal experts I’ve spoken with are more concerned with a different problem: Lawyers are adopting AI models that make errors such as citing cases that don’t exist, which judges may not catch. This is not addressed in the action plan.&nbsp;



It’s also worth noting that just days before releasing a plan that targets “malicious deepfakes,” President Trump shared a fake AI-generated video of former president Barack Obama being arrested in the Oval Office.



Overall, the AI Action Plan affirms what President Trump and those in his orbit have long signaled: It’s the defining social and political weapon of our time. They believe that AI, if harnessed correctly, can help them win everything from culture wars to geopolitical conflicts. The right AI, they argue, will help defeat China. Government pressure on leading companies can force them to purge “woke” ideology from their models. 



The plan includes crowd-pleasers—like cracking down on deepfakes—but overall, it reflects how tech giants have cozied up to the Trump administration. The fact that it contains almost no provisions challenging their power shows how their investment in this relationship is paying off.



This story originally appeared in The Algorithm, our weekly newsletter on AI. To get stories like this in your inbox first, sign up here.
• This startup wants to use the Earth as a massive battery
  Texas-based startup Quidnet Energy completed a test showing it can store energy for up to six months by pumping water underground . The company hopes its twist on the concept could help bring cheap, long-duration energy storage to new places . In June, the company was able to discharge 35 megawatt-hours of energy from the well, and there was virtually no self-discharge, meaning no energy loss .

🔒 Cybersecurity & Privacy
No updates.

🎓 University AI
No updates.

🏢 Corporate AI
• Mistral-Small-3.2-24B-Instruct-2506 is now available on Amazon Bedrock Marketplace and Amazon SageMaker JumpStart
  Today, we’re excited to announce that Mistral-Small-3.2-24B-Instruct-2506—a 24-billion-parameter large language model (LLM) from Mistral AI that’s optimized for enhanced instruction following and reduced repetition errors—is available for customers through Amazon SageMaker JumpStart and Amazon Bedrock Marketplace. Amazon Bedrock Marketplace is a capability in Amazon Bedrock that developers can use to discover, test, and use over 100 popular, emerging, and specialized foundation models (FMs) alongside the current selection of industry-leading models in Amazon Bedrock. 
In this post, we walk through how to discover, deploy, and use Mistral-Small-3.2-24B-Instruct-2506 through Amazon Bedrock Marketplace and with SageMaker JumpStart. 
Overview of Mistral Small 3.2 (2506) 
Mistral Small 3.2 (2506) is an update of Mistral-Small-3.1-24B-Instruct-2503, maintaining the same 24-billion-parameter architecture while delivering improvements in key areas. Released under Apache 2.0 license, this model maintains a balance between performance and computational efficiency. Mistral offers both the pretrained (Mistral-Small-3.1-24B-Base-2503) and instruction-tuned (Mistral-Small-3.2-24B-Instruct-2506) checkpoints of the model under Apache 2.0. 
Key improvements in Mistral Small 3.2 (2506) include: 
 
 Improves in following precise instructions with 84.78% accuracy compared to 82.75% in version 3.1 from Mistral’s benchmarks 
 Produces twice as fewer infinite generations or repetitive answers, reducing from 2.11% to 1.29% according to Mistral 
 Offers a more robust and reliable function calling template for structured API interactions 
 Now includes image-text-to-text capabilities, allowing the model to process and reason over both textual and visual inputs. This makes it ideal for tasks such as document understanding, visual Q&amp;A, and image-grounded content generation. 
 
These improvements make the model particularly well-suited for enterprise applications on AWS where reliability and precision are critical. With a 128,000-token context window, the model can process extensive documents and maintain context throughout longer conversation. 
SageMaker JumpStart overview 
SageMaker JumpStart is a fully managed service that offers state-of-the-art FMs for various use cases such as content writing, code generation, question answering, copywriting, summarization, classification, and information retrieval. It provides a collection of pre-trained models that you can deploy quickly, accelerating the development and deployment of machine learning (ML) applications. One of the key components of SageMaker JumpStart is model hubs, which offer a vast catalog of pre-trained models, such as Mistral, for a variety of tasks. 
You can now discover and deploy Mistral models in Amazon SageMaker Studio or programmatically through the Amazon SageMaker Python SDK, deriving model performance and MLOps controls with SageMaker features such as Amazon SageMaker Pipelines, Amazon SageMaker Debugger, or container logs. The model is deployed in a secure AWS environment and under your virtual private cloud (VPC) controls, helping to support data security for enterprise security needs. 
Prerequisites 
To deploy Mistral-Small-3.2-24B-Instruct-2506, you must have the following prerequisites: 
 
 An AWS account that will contain all your AWS resources. 
 An AWS Identity and Access Management (IAM) role to access SageMaker. To learn more about how IAM works with SageMaker, see Identity and Access Management for Amazon SageMaker. 
 Access to SageMaker Studio, a SageMaker notebook instance, or an interactive development environment (IDE) such as PyCharm or Visual Studio Code. We recommend using SageMaker Studio for straightforward deployment and inference. 
 Access to accelerated instances (GPUs) for hosting the model. 
 
If needed, request a quota increase and contact your AWS account team for support. This model requires a GPU-based instance type (approximately 55 GB of GPU RAM in bf16 or fp16) such as ml.g6.12xlarge. 
Deploy Mistral-Small-3.2-24B-Instruct-2506 in Amazon Bedrock Marketplace 
To access Mistral-Small-3.2-24B-Instruct-2506 in Amazon Bedrock Marketplace, complete the following steps: 
 
 On the Amazon Bedrock console, in the navigation pane under Discover, choose Model catalog. 
 Filter for Mistral as a provider and choose the Mistral-Small-3.2-24B-Instruct-2506 model. 
 
 
The model detail page provides essential information about the model’s capabilities, pricing structure, and implementation guidelines. You can find detailed usage instructions, including sample API calls and code snippets for integration.The page also includes deployment options and licensing information to help you get started with Mistral-Small-3.2-24B-Instruct-2506 in your applications. 
 
 To begin using Mistral-Small-3.2-24B-Instruct-2506, choose Deploy. 
 You will be prompted to configure the deployment details for Mistral-Small-3.2-24B-Instruct-2506. The model ID will be pre-populated. 
   
   For Endpoint name, enter an endpoint name (up to 50 alphanumeric characters). 
   For Number of instances, enter a number between 1–100. 
   For Instance type, choose your instance type. For optimal performance with Mistral-Small-3.2-24B-Instruct-2506, a GPU-based instance type such as ml.g6.12xlarge is recommended. 
   Optionally, configure advanced security and infrastructure settings, including VPC networking, service role permissions, and encryption settings. For most use cases, the default settings will work well. However, for production deployments, review these settings to align with your organization’s security and compliance requirements. 
    
 Choose Deploy to begin using the model. 
 
 
When the deployment is complete, you can test Mistral-Small-3.2-24B-Instruct-2506 capabilities directly in the Amazon Bedrock playground, a tool on the Amazon Bedrock console to provide a visual interface to experiment with running different models. 
 
 Choose Open in playground to access an interactive interface where you can experiment with different prompts and adjust model parameters such as temperature and maximum length. 
 
 
The playground provides immediate feedback, helping you understand how the model responds to various inputs and letting you fine-tune your prompts for optimal results. 
To invoke the deployed model programmatically with Amazon Bedrock APIs, you need to get the endpoint Amazon Resource Name (ARN). You can use the Converse API for multimodal use cases. For tool use and function calling, use the Invoke Model API. 
Reasoning of complex figures 
VLMs excel at interpreting and reasoning about complex figures, charts, and diagrams. In this particular use case, we use Mistral-Small-3.2-24B-Instruct-2506 to analyze an intricate image containing GDP data. Its advanced capabilities in document understanding and complex figure analysis make it well-suited for extracting insights from visual representations of economic data. By processing both the visual elements and accompanying text, Mistral Small 2506 can provide detailed interpretations and reasoned analysis of the GDP figures presented in the image. 
We use the following input image. 
 
We have defined helper functions to invoke the model using the Amazon Bedrock Converse API: 
 
 def get_image_format(image_path):
    with Image.open(image_path) as img:
        # Normalize the format to a known valid one
        fmt = img.format.lower() if img.format else 'jpeg'
        # Convert 'jpg' to 'jpeg'
        if fmt == 'jpg':
            fmt = 'jpeg'
    return fmt

def call_bedrock_model(model_id=None, prompt="", image_paths=None, system_prompt="", temperature=0.6, top_p=0.9, max_tokens=3000):
    
    if isinstance(image_paths, str):
        image_paths = [image_paths]
    if image_paths is None:
        image_paths = []
    
    # Start building the content array for the user message
    content_blocks = []

    # Include a text block if prompt is provided
    if prompt.strip():
        content_blocks.append({"text": prompt})

    # Add images as raw bytes
    for img_path in image_paths:
        fmt = get_image_format(img_path)
        # Read the raw bytes of the image (no base64 encoding!)
        with open(img_path, 'rb') as f:
            image_raw_bytes = f.read()

        content_blocks.append({
            "image": {
                "format": fmt,
                "source": {
                    "bytes": image_raw_bytes
                }
            }
        })

    # Construct the messages structure
    messages = [
        {
            "role": "user",
            "content": content_blocks
        }
    ]

    # Prepare additional kwargs if system prompts are provided
    kwargs = {}
    
    kwargs["system"] = [{"text": system_prompt}]

    # Build the arguments for the `converse` call
    converse_kwargs = {
        "messages": messages,
        "inferenceConfig": {
            "maxTokens": 4000,
            "temperature": temperature,
            "topP": top_p
        },
        **kwargs
    }

    
    converse_kwargs["modelId"] = model_id

    # Call the converse API
    try:
        response = client.converse(**converse_kwargs)
    
        # Parse the assistant response
        assistant_message = response.get('output', {}).get('message', {})
        assistant_content = assistant_message.get('content', [])
        result_text = "".join(block.get('text', '') for block in assistant_content)
    except Exception as e:
        result_text = f"Error message: {e}"
    return result_text 
 
Our prompt and input payload are as follows: 
 
 import boto3
import base64
import json
from PIL import Image
from botocore.exceptions import ClientError

# Create a Bedrock Runtime client in the AWS Region you want to use.
client = boto3.client("bedrock-runtime", region_name="us-west-2")

system_prompt='You are a Global Economist.'
task = 'List the top 5 countries in Europe with the highest GDP'
image_path = './image_data/gdp.png'

print('Input Image:\n\n')
Image.open(image_path).show()

response = call_bedrock_model(model_id=endpoint_arn, 
                   prompt=task, 
                   system_prompt=system_prompt,
                   image_paths = image_path)

print(f'\nResponse from the model:\n\n{response}') 
 
The following is a response using the Converse API: 
 
 Based on the image provided, the top 5 countries in Europe with the highest GDP are:

1. **Germany**: $3.99T (4.65%)
2. **United Kingdom**: $2.82T (3.29%)
3. **France**: $2.78T (3.24%)
4. **Italy**: $2.07T (2.42%)
5. **Spain**: $1.43T (1.66%)

These countries are highlighted in green, indicating their location in the Europe region. 
 
Deploy Mistral-Small-3.2-24B-Instruct-2506 in SageMaker JumpStart 
You can access Mistral-Small-3.2-24B-Instruct-2506 through SageMaker JumpStart in the SageMaker JumpStart UI and the SageMaker Python SDK. SageMaker JumpStart is an ML hub with FMs, built-in algorithms, and prebuilt ML solutions that you can deploy with just a few clicks. With SageMaker JumpStart, you can customize pre-trained models to your use case, with your data, and deploy them into production using either the UI or SDK. 
Deploy Mistral-Small-3.2-24B-Instruct-2506 through the SageMaker JumpStart UI 
Complete the following steps to deploy the model using the SageMaker JumpStart UI: 
 
 On the SageMaker console, choose Studio in the navigation pane. 
 First-time users will be prompted to create a domain. If not, choose Open Studio. 
 On the SageMaker Studio console, access SageMaker JumpStart by choosing JumpStart in the navigation pane. 
 
 
 
 Search for and choose Mistral-Small-3.2-24B-Instruct-2506 to view the model card. 
 
 
 
 Click the model card to view the model details page. Before you deploy the model, review the configuration and model details from this model card. The model details page includes the following information: 
 
 
 The model name and provider information. 
 A Deploy button to deploy the model. 
 About and Notebooks tabs with detailed information. 
 The Bedrock Ready badge (if applicable) indicates that this model can be registered with Amazon Bedrock, so you can use Amazon Bedrock APIs to invoke the model. 
 
 
 
 Choose Deploy to proceed with deployment. 
   
   For Endpoint name, enter an endpoint name (up to 50 alphanumeric characters). 
   For Number of instances, enter a number between 1–100 (default: 1). 
   For Instance type, choose your instance type. For optimal performance with Mistral-Small-3.2-24B-Instruct-2506, a GPU-based instance type such as ml.g6.12xlarge is recommended. 
    
 
 
 
 Choose Deploy to deploy the model and create an endpoint. 
 
When deployment is complete, your endpoint status will change to InService. At this point, the model is ready to accept inference requests through the endpoint. You can invoke the model using a SageMaker runtime client and integrate it with your applications. 
Deploy Mistral-Small-3.2-24B-Instruct-2506 with the SageMaker Python SDK 
Deployment starts when you choose Deploy. After deployment finishes, you will see that an endpoint is created. Test the endpoint by passing a sample inference request payload or by selecting the testing option using the SDK. When you select the option to use the SDK, you will see example code that you can use in the notebook editor of your choice in SageMaker Studio. 
To deploy using the SDK, start by selecting the Mistral-Small-3.2-24B-Instruct-2506 model, specified by the model_id with the value mistral-small-3.2-24B-instruct-2506. You can deploy your choice of the selected models on SageMaker using the following code. Similarly, you can deploy Mistral-Small-3.2-24B-Instruct-2506 using its model ID. 
 
 from sagemaker.jumpstart.model import JumpStartModel 
accept_eula = True 
model = JumpStartModel(model_id="huggingface-vlm-mistral-small-3.2-24b-instruct-2506") 
predictor = model.deploy(accept_eula=accept_eula)
This deploys the model on SageMaker with default configurations, including the default instance type and default VPC configurations. You can change these configurations by specifying non-default values in JumpStartModel. The EULA value must be explicitly defined as True to accept the end-user license agreement (EULA). 
 
After the model is deployed, you can run inference against the deployed endpoint through the SageMaker predictor: 
 
 prompt = "Hello!"
payload = {
    "messages": [
        {
            "role": "user",
            "content": prompt
        }
    ],
    "max_tokens": 4000,
    "temperature": 0.15,
    "top_p": 0.9,
}
    
response = predictor.predict(payload)
print(response['choices'][0]['message']['content'])
We get following response:

Hello! 😊 How can I assist you today? 
 
Vision reasoning example 
Using the multimodal capabilities of Mistral-Small-3.2-24B-Instruct-2506, you can process both text and images for comprehensive analysis. The following example highlights how the model can simultaneously analyze a tuition ROI chart to extract visual patterns and data points. The following image is the input chart.png. 
 
Our prompt and input payload are as follows: 
 
 # Read and encode the image
image_path = "chart.png"
with open(image_path, "rb") as image_file:
base64_image = base64.b64encode(image_file.read()).decode('utf-8')


# Create a prompt focused on visual analysis of the box plot chart
visual_prompt = """Please analyze this box plot chart showing the relationship between Annual Tuition (x-axis) and&nbsp;
40-Year Net Present Value (y-axis) in US$.&nbsp;
Describe the key trend between tuition and net present value shown in this chart. What's one notable insight?"""

# Create payload with image input
payload = {
"messages": [
{
"role": "user",
"content": [
{"type": "text", "text": visual_prompt},
{"type": "image_url", "image_url": {"url": f"data:image/png;base64,{base64_image}"}}
]
}
],
"max_tokens": 800,
"temperature": 0.15
}

# Make a prediction
response = predictor.predict(payload)

# Display the visual analysis
message = response['choices'][0]['message']
if message.get('content'):
print("Vision Analysis:")
print(message['content']) 
 
We get following response: 
 
 Vision Analysis:
This box plot chart illustrates the relationship between annual tuition costs (x-axis) and the 40-year net present value (NPV) in US dollars (y-axis). Each box plot represents a range of annual tuition costs, showing the distribution of NPV values within that range.

### Key Trend:
1. **General Distribution**: Across all tuition ranges, the median 40-year NPV (indicated by the line inside each box) appears to be relatively consistent, hovering around the $1,000,000 mark.
2. **Variability**: The spread of NPV values (indicated by the height of the boxes and whiskers) is wider for higher tuition ranges, suggesting greater variability in outcomes for more expensive schools.
3. **Outliers**: There are several outliers, particularly in the higher tuition ranges (e.g., 35-40k, 40-45k, and &gt;50k), indicating that some individuals experience significantly higher or lower NPVs.

### Notable Insight:
One notable insight from this chart is that higher tuition costs do not necessarily translate into a higher 40-year net present value. For example, the median NPV for the highest tuition range (&gt;50k) is not significantly higher than that for the lowest tuition range (&lt;5k). This suggests that the return on investment for higher tuition costs may not be proportionally greater, and other factors beyond tuition cost may play a significant role in determining long-term financial outcomes.

This insight highlights the importance of considering factors beyond just tuition costs when evaluating the potential return on investment of higher education. 
 
Function calling example 
This following example shows Mistral Small 3.2’s function calling by demonstrating how the model identifies when a user question needs external data and calls the correct function with proper parameters.Our prompt and input payload are as follows: 
 
 # Define a simple weather function
weather_function = {
"type": "function",
"function": {
"name": "get_weather",
"description": "Get weather for a location",
"parameters": {
"type": "object",
"properties": {
"location": {
"type": "string",
"description": "City name"
}
},
"required": ["location"]
}
}
}

# User question
user_question = "What's the weather like in Seattle?"

# Create payload
payload = {
"messages": [{"role": "user", "content": user_question}],
"tools": [weather_function],
"tool_choice": "auto",
"max_tokens": 200,
"temperature": 0.15
}

# Make prediction
response = predictor.predict(payload)

# Display raw response to see exactly what we get
print(json.dumps(response['choices'][0]['message'], indent=2))

# Extract function call information from the response content
message = response['choices'][0]['message']
content = message.get('content', '')

if '[TOOL_CALLS]' in content:
print("Function call details:", content.replace('[TOOL_CALLS]', '')) 
 
We get following response: 
 
 {
"role": "assistant",
"reasoning_content": null,
"content": "[TOOL_CALLS]get_weather{\"location\": \"Seattle\"}",
"tool_calls": []
}
Function call details: get_weather{"location": "Seattle"} 
 
Clean up 
To avoid unwanted charges, complete the following steps in this section to clean up your resources. 
Delete the Amazon Bedrock Marketplace deployment 
If you deployed the model using Amazon Bedrock Marketplace, complete the following steps: 
 
 On the Amazon Bedrock console, under Tune in the navigation pane, select Marketplace model deployment. 
 In the Managed deployments section, locate the endpoint you want to delete. 
 Select the endpoint, and on the Actions menu, choose Delete. 
 Verify the endpoint details to make sure you’re deleting the correct deployment: 
   
   Endpoint name 
   Model name 
   Endpoint status 
    
 Choose Delete to delete the endpoint. 
 In the deletion confirmation dialog, review the warning message, enter confirm, and choose Delete to permanently remove the endpoint. 
 
Delete the SageMaker JumpStart predictor 
After you’re done running the notebook, make sure to delete the resources that you created in the process to avoid additional billing. For more details, see Delete Endpoints and Resources. You can use the following code: 
 
 predictor.delete_model()
predictor.delete_endpoint() 
 
Conclusion 
In this post, we showed you how to get started with Mistral-Small-3.2-24B-Instruct-2506 and deploy the model using Amazon Bedrock Marketplace and SageMaker JumpStart for inference. This latest version of the model brings improvements in instruction following, reduced repetition errors, and enhanced function calling capabilities while maintaining performance across text and vision tasks. The model’s multimodal capabilities, combined with its improved reliability and precision, support enterprise applications requiring robust language understanding and generation. 
Visit SageMaker JumpStart in Amazon SageMaker Studio or Amazon Bedrock Marketplace now to get started with Mistral-Small-3.2-24B-Instruct-2506. 
For more Mistral resources on AWS, check out the Mistral-on-AWS GitHub repo. 
 
About the authors 
Niithiyn Vijeaswaran is a Generative AI Specialist Solutions Architect with the Third-Party Model Science team at AWS. His area of focus is AWS AI accelerators (AWS Neuron). He holds a Bachelor’s degree in Computer Science and Bioinformatics. 
Breanne Warner is an Enterprise Solutions Architect at Amazon Web Services supporting healthcare and life science (HCLS) customers. She is passionate about supporting customers to use generative AI on AWS and evangelizing model adoption for first- and third-party models. Breanne is also Vice President of the Women at Amazon board with the goal of fostering inclusive and diverse culture at Amazon. Breanne holds a Bachelor’s of Science in Computer Engineering from the University of Illinois Urbana-Champaign. 
Koushik Mani is an Associate Solutions Architect at AWS. He previously worked as a Software Engineer for 2 years focusing on machine learning and cloud computing use cases at Telstra. He completed his Master’s in Computer Science from the University of Southern California. He is passionate about machine learning and generative AI use cases and building solutions.
• Generate suspicious transaction report drafts for financial compliance using generative AI
  Financial regulations and compliance are constantly changing, and automation of compliance reporting has emerged as a game changer in the financial industry. Amazon Web Services (AWS) generative AI solutions offer a seamless and efficient approach to automate this reporting process. The integration of AWS generative AI into the compliance framework not only enhances efficiency but also instills a greater sense of confidence and trust in the financial sector by promoting precision and timely delivery of compliance reports. These solutions help financial institutions avoid the costly and reputational consequences of noncompliance. This, in turn, contributes to the overall stability and integrity of the financial ecosystem, benefiting both the industry and the consumers it serves. 
Amazon Bedrock is a managed generative AI service that provides access to a wide array of advanced foundation models (FMs). It includes features that facilitate the efficient creation of generative AI applications with a strong focus on privacy and security. Getting a good response from an FM relies heavily on using efficient techniques for providing prompts to the FM. Retrieval Augmented Generation (RAG) is a pivotal approach to augmenting FM prompts with contextually relevant information from external sources. It uses vector databases such as Amazon OpenSearch Service to enable semantic searching of the contextual information. 
Amazon Bedrock Knowledge Bases, powered by vector databases such as Amazon OpenSearch Serverless, helps in implementing RAG to supplement model inputs with relevant information from factual resources, thereby reducing potential hallucinations and increasing response accuracy. 
Amazon Bedrock Agents enables generative AI applications to execute multistep tasks using action groups and enable interaction with APIs, knowledge bases, and FMs. Using agents, you can design intuitive and adaptable generative AI applications capable of understanding natural language queries and creating engaging dialogues to gather details required for using the FMs effectively. 
A suspicious transaction report (STR) or suspicious activity report (SAR) is a type of report that a financial organization must submit to a financial regulator if they have reasonable grounds to suspect any financial transaction that has occurred or was attempted during their activities. There are stipulated timelines for filing these reports and it typically takes several hours of manual effort to create one report for one customer account. 
In this post, we explore a solution that uses FMs available in Amazon Bedrock to create a draft STR. We cover how generative AI can be used to automate the manual process of draft generation using account information, transaction details, and correspondence summaries as well as creating a knowledge base of information about fraudulent entities involved in such transactions. 
Solution overview 
The solution uses Amazon Bedrock Knowledge Bases, Amazon Bedrock Agents, AWS Lambda, Amazon Simple Storage Service (Amazon S3), and OpenSearch Service. The workflow is as follows: 
 
 The user requests for creation of a draft STR report through the business application. 
 The application calls Amazon Bedrock Agents, which has been preconfigured with detailed instructions to engage in a conversational flow with the user. The agent follows these instructions to gather the required information from the user, completes the missing information by using actions groups to invoke the Lambda function, and generates the report in the specified format. 
 Following its instructions, the agent invokes Amazon Bedrock Knowledge Bases to find details about fraudulent entities involved in the suspicious transactions. 
 Amazon Bedrock Knowledge Bases queries OpenSearch Service to perform semantic search for the entities required for the report. If the information about fraudulent entities is available in Amazon Bedrock Knowledge Bases, the agent follows its instructions to generate a report for the user. 
 If the information isn’t found in the knowledge base, the agent uses the chat interface to prompt the user to provide the website URL that contains the relevant information. Alternatively, the user can provide a description about the fraudulent entity in the chat interface. 
 If the user provides a URL for a publicly accessible website, the agent follows its instructions to call the action group to invoke a Lambda function to crawl the website URL. The Lambda function scrapes the information from the website and returns it to the agent for use in the report. 
 The Lambda function also stores the scraped content in an S3 bucket for future use by the search index. 
 Amazon Bedrock Knowledge Bases can be programmed to periodically scan the S3 bucket to index the new content in OpenSearch Service. 
 
The following diagram illustrates the solution architecture and workflow. 
 
You can use the full code available in GitHub to deploy the solution using the AWS Cloud Development Kit (AWS CDK). Alternatively, you can follow a step-by-step process for manual deployment. We walk through both approaches in this post. 
Prerequisites 
To implement the solution provided in this post, you must enable model access in Amazon Bedrock for Amazon Titan Text Embeddings V2 and Anthropic Claude 3.5 Haiku. 
Deploy the solution with the AWS CDK 
To set up the solution using the AWS CDK, follow these steps: 
 
 Verify that the AWS CDK has been installed in your environment. For installation instructions, refer to the AWS CDK Immersion Day Workshop. 
 Update the AWS CDK to version 36.0.0 or higher: 
 
 
 npm install -g aws-cdk 
 
 
 Initialize the AWS CDK environment in the AWS account: 
 
 
 cdk bootstrap 
 
 
 Clone the GitHub repository containing the solution files: 
 
 
 git clone https://github.com/aws-samples/suspicious-financial-transactions-reporting 
 
 
 Navigate to the solution directory: 
 
 
 cd financial-transaction-report-drafting-for-compliance 
 
 
 Create and activate the virtual environment: 
 
 
 python3 -m venv .venv
source .venv/bin/activate 
 
Activating the virtual environment differs based on the operating system. Refer to the AWS CDK workshop for information about activating in other environments. 
 
 After the virtual environment is activated, install the required dependencies: 
 
 
 pip install -r requirements.txt 
 
 
 Deploy the backend and frontend stacks: 
 
 
 cdk deploy -a ./app.py --all 
 
 
 When the deployment is complete, check these deployed stacks by visiting the AWS CloudFormation console, as shown in the following two screenshots. 
 
 
 
Manual deployment 
To implement the solution without using the AWS CDK, complete the following steps: 
 
 Set up an S3 bucket. 
 Create a Lambda function. 
 Set up Amazon Bedrock Knowledge Bases. 
 Set up Amazon Bedrock Agents. 
 
Visual layouts in some screenshots in this post might look different than those on your AWS Management Console. 
Set up an S3 bucket 
Create an S3 bucket with a unique bucket name for the document repository, as shown in the following screenshot. This will be a data source for Amazon Bedrock Knowledge Bases. 
 
Create the website scraper Lambda function 
Create a new Lambda function called Url-Scraper using the Python 3.13 runtime to crawl and scrape the website URL provided by Amazon Bedrock Agents. The function will scrape the content, send the information to the agent, and store the contents in the S3 bucket for future references. 
 
Error handling has been skipped in this code snippet for brevity. The full code is available in GitHub. 
Create a new file called search_suspicious_party.py with the following code snippet: 
 
 import boto3
from bs4 import BeautifulSoup
import os
import re
import urllib.request
BUCKET_NAME = os.getenv('S3_BUCKET')
s3 = boto3.client('s3')
def get_receiving_entity_from_url(start_url):
    response = urllib.request.urlopen(
        urllib.request.Request(url=start_url, method='GET'),
        timeout=5)
    soup = BeautifulSoup(response.read(), 'html.parser')
    # Extract page title
    title = soup.title.string if soup.title else 'Untitled'
    # Extract page content for specific HTML elements
    content = ' '.join(p.get_text() for p in soup.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6']))
    content = re.sub(r'\s+', ' ', content).strip()
    s3.put_object(Body=content, Bucket=BUCKET_NAME, Key=f"docs/{title}.txt")
return content 
 
Replace the default generated code in lambda_function.py with the following code: 
 
 import json
from search-suspicious-party import *
def lambda_handler(event, context):
    # apiPath should match the path specified in action group schema
    if event['apiPath'] == '/get-receiving-entity-details':
        # Extract the property from request data
        start_url = get_named_property(event, 'start_url')
        scraped_text = get_receiving_entity_from_url(start_url)
        action_response = {
            'actionGroup': event['actionGroup'],
            'apiPath': event['apiPath'],
            'httpMethod': event['httpMethod'],
            'httpStatusCode': 200,
            'responseBody': {
                'application/json': {
                    'body': json.dumps({'scraped_text': scraped_text})
                }
            }
        }
        return {'response': action_response}
    # Return an error if apiPath is not recognized
    return {
        'statusCode': 400,
        'body': json.dumps({'error': 'Invalid API path'})
    }
def get_named_property(event, name):
    return next(
        item for item in
        event['requestBody']['content']['application/json']['properties']
        if item['name'] == name
    )['value'] 
 
Configure the Lambda function 
Set up a Lambda environment variable S3_BUCKET, as shown in the following screenshot. For Value, use the S3 bucket you created previously. 
 
Increase the timeout duration for Lambda function to 30 seconds. You can adjust this value based on the time it takes for the crawler to complete its work. 
 
Set up Amazon Bedrock Knowledge Bases 
Complete the following steps to create a new knowledge base in Amazon Bedrock. This knowledge base will use OpenSearch Serverless to index the fraudulent entity data stored in Amazon S3. For more information, refer to Create a knowledge base by connecting to a data source in Amazon Bedrock Knowledge Bases. 
 
 On the Amazon Bedrock console, choose Knowledge bases in the navigation pane and choose Create knowledge base. 
 For Knowledge base name, enter a name (for example, str-knowledge-base). 
 For Service role name, keep the default system generated value. 
 
 
 
 Select Amazon S3 as the data source. 
 
 
 
 Configure the Amazon S3 data source: 
   
   For Data source name, enter a name (for example, knowledge-base-data-source-s3). 
   For S3 URI, choose Browse S3 and choose the bucket where information scraped by web crawler about fraudulent entities is available for the knowledge base to use. 
   Keep all other default values. 
    
 
 
 
 For Embeddings model, choose Titan Text Embeddings V2. 
 
 
 
 For Vector database, select Quick create a new vector store to create a default vector store with OpenSearch Serverless. 
 
 
 
 Review the configurations and choose Create knowledge base. 
 
After the knowledge base is successfully created, you can see the knowledge base ID, which you will need when creating the agent in Amazon Bedrock. 
 
 Select knowledge-base-data-source-s3 from the list of data sources and choose Sync to index the documents. 
 
 
Set up Amazon Bedrock Agents 
To create a new agent in Amazon Bedrock, complete the following steps. For more information, refer to Create and configure agent manually. 
 
 On the Amazon Bedrock console, choose Agents in the navigation pane and choose Create Agent. 
 For Name, enter a name (for example, agent-str). 
 Choose Create. 
 
 
 
 For Agent resource role, keep the default setting (Create and use a new service role). 
 For Select model, choose a model provider and model name (for example, Anthropic’s Claude 3.5 Haiku) 
 For Instructions for the Agent, provide the instructions that allow the agent to invoke the large language model (LLM). 
 
You can download the instructions from the agent-instructions.txt file in the GitHub repo. Refer to next section in this post to understand how to write the instructions. 
 
 Keep all other default values. 
 Choose Save. 
 
 
 
 Under Action groups, choose Add to create a new action group. 
 
 
An action is a task the agent can perform by making API calls. A set of actions comprises an action group. 
 
 Provide an API schema that defines all the APIs in the action group. 
 For Action group details, enter an action group name (for example, agent-group-str-url-scraper). 
 For Action group type, select Define with API schemas. 
 For Action group invocation, select Select an existing Lambda function, which is the Lambda function that you created previously. 
 
 
 
 For Action group schema, choose Define via in-line schema editor. 
 Replace the default sample code with the following example to define the schema to specify the input parameters with default and mandatory values: 
 
 
 openapi: 3.0.0
info:
  title: Gather suspicious receiving entity details from website
  version: 1.0.0
paths:
  /:
    post:
      description: Get details about suspicious receiving entity from the URL
      operationId: getReceivingEntityDetails
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: "#/components/schemas/ScrapeRequest"
      responses:
        "200":
          description: Receiving entity details gathered successfully
components:
  schemas:
    ScrapeRequest:
      type: object
      properties:
        :
          type: string
          description: The URL to start scraping from
      required:
        - start_url 
 
 
 Choose Create. 
 Under Knowledge bases, choose Add. 
 
 
 
 For Select knowledge base, choose knowledge-base-str, which you created previously, and add the following instructions: 
 
Use the information in the knowledge-base-str knowledge base to select transaction reports. 
 
 
 Choose Save to save all changes. 
 Finally, choose Prepare to prepare this agent to get it ready for testing. 
 
You can also create a Streamlit application to create a UI for this application. The source code is available in GitHub. 
Agent instructions 
Agent instructions for Amazon Bedrock Agents provide the mechanism for a multistep user interaction to gather the inputs an agent needs to invoke the LLM with a rich prompt to generate the response in the required format. Provide logical instructions in plain English. There are no predefined formats for these instructions. 
 
 Provide an overview of the task including the role: 
 
 
 You are a financial user creating Suspicious Transaction Report (STR) draft for a financial compliance use case. 
 
 
 Provide the message that the agent can use for initiating the user interaction: 
 
 
 Greet the user with the message “Hi &lt;name&gt;. Welcome to STR report drafting. How can I help?”
Ask the user to provide the transactions details. From the transaction details, capture the response in the &lt;answer&gt; tag and include the &lt;thinking&gt; tag to understand the rationale behind the response. 
 
 
 Specify the processing that needs to be done on the output received from the LLM: 
 
 
 For the transaction input provided by user, create a narrative description for financial risk reporting of the provided bank account and transaction details.
1. Add a summary of correspondence logs that includes title, summary, correspondence history, and analysis in the narrative description.
2. Add the details about the receiving entity in the narrative description. You can get details about receiving entities from the agent action group. 
 
 
 Provide the optional messages that the agent can use for a multistep interaction to gather the missing inputs if required: 
 
 
 If you don't have knowledge about Receiving entity, you should ask the Human for more details about it with a message “Unfortunately I do not have enough context or details about the receiving entity &lt;entity name&gt; to provide an accurate risk assessment or summary. Can you please provide some additional background information about &lt;entity name&gt;? What is the URL of the &lt;entity name&gt; or the description?” 
 
 
 Specify the actions that the agent can take to process the user input using action groups: 
 
 
 If user provides the URL of &lt;entity name&gt;, call the action group &lt;add action group name&gt; to get the details. If user provides the description of &lt;entity name&gt;, then summarize and add it to the narrative description as a receiving entity. 
 
 
 Specify how the agent should provide the response, including the format details: 
 
 
 Once you have all the necessary input (financial transaction details and receiving entity details), create a detailed well-formatted draft report for financial risk reporting of the provided bank account and transaction details containing the following sections:
1. Title
2. Summary of transactions
3. Correspondence History &amp; Analysis
4. Receiving entity summary 
 
Test the solution 
To test the solution, follow these steps: 
 
 Choose Test to start testing the agent. 
 Initiate the chat and observe how the agent uses the instructions you provided in the configuration step to ask for required details for generating the report. 
 Try different prompts, such as “Generate an STR for an account.” 
 
The following screenshot shows an example chat. 
 
The following screenshot shows an example chat with the prompt, “Generate an STR for account number 49179-180-2092803.” 
 
Another option is to provide all the details at the same time, for example, “Generate an STR for account number 12345-999-7654321 with the following transactions.” 
 
 Copy and paste the sample transactions from the sample-transactions.txt file in GitHub. 
 
The agent keeps asking for missing information, such as account number, transaction details, and correspondence history. After it has all the details, it will generate a draft STR document. 
The code in GitHub also contains a sample StreamLit application that you can use to test the application. 
Clean up 
To avoid incurring unnecessary future charges, clean up the resources you created as part of this solution. If you created the solution using the GitHub code sample and the AWS CDK, empty the S3 bucket and delete the CloudFormation stack. If you created the solution manually, complete the following steps: 
 
 Delete the Amazon Bedrock agent. 
 Delete the Amazon Bedrock knowledge base. 
 Empty and delete the S3 bucket if you created one specifically for this solution. 
 Delete the Lambda function. 
 
Conclusion 
In this post, we showed how Amazon Bedrock offers a robust environment for building generative AI applications, featuring a range of advanced FMs. This fully managed service prioritizes privacy and security while helping developers create AI-driven applications efficiently. A standout feature, RAG, uses external knowledge bases to enrich AI-generated content with relevant information, backed by OpenSearch Service as its vector database. Additionally, you can include metadata fields in the knowledge base and agent session context with Amazon Verified Permissions to pass fine-grained access context for authorization. 
With careful prompt engineering, Amazon Bedrock minimizes inaccuracies and makes sure that AI responses are grounded in factual documentation. This combination of advanced technology and data integrity makes Amazon Bedrock an ideal choice for anyone looking to develop reliable generative AI solutions. You can now explore extending this sample code to use Amazon Bedrock and RAG for reliably generating draft documents for compliance reporting. 
 
About the Authors 
Divyajeet (DJ) Singh is a Senior Solutions Architect at AWS Canada. He loves working with customers to help them solve their unique business challenges using the cloud. Outside of work, he enjoys spending time with family and friends and exploring new places. 
Parag Srivastava is a Senior Solutions Architect at AWS, where he has been helping customers successfully apply generative AI to real-life business scenarios. During his professional career, he has been extensively involved in complex digital transformation projects. He is also passionate about building innovative solutions around geospatial aspects of addresses. 
Sangeetha Kamatkar is a Senior Solutions Architect at AWS who helps customers with successful cloud adoption and migration. She works with customers to craft highly scalable, flexible, and resilient cloud architectures that address customer business problems. In her spare time, she listens to music, watches movies, and enjoys gardening during summertime. 
Vineet Kachhawaha is a Senior Solutions Architect at AWS focusing on AI/ML and generative AI. He co-leads the AWS for Legal Tech team within AWS. He is passionate about working with enterprise customers and partners to design, deploy, and scale AI/ML applications to derive business value.
• Fine-tune and deploy Meta Llama 3.2 Vision for generative AI-powered web automation using AWS DLCs, Amazon EKS, and Amazon Bedrock
  Fine-tuning of large language models (LLMs) has emerged as a crucial technique for organizations seeking to adapt powerful foundation models (FMs) to their specific needs. Rather than training models from scratch—a process that can cost millions of dollars and require extensive computational resources—companies can customize existing models with domain-specific data at a fraction of the cost. This approach has become particularly valuable as organizations across healthcare, finance, and technology sectors look to use AI for specialized tasks while maintaining cost-efficiency. However, implementing a production-grade fine-tuning solution presents several significant challenges. Organizations must navigate complex infrastructure setup requirements, enforce robust security measures, optimize performance, and establish reliable model hosting solutions. 
In this post, we present a complete solution for fine-tuning and deploying the Llama-3.2-11B-Vision-Instruct model for web automation tasks. We demonstrate how to build a secure, scalable, and efficient infrastructure using AWS Deep Learning Containers (DLCs) on Amazon Elastic Kubernetes Service (Amazon EKS). By using AWS DLCs, you can gain access to well-tested environments that come with enhanced security features and pre-installed software packages, significantly simplifying the optimization of your fine-tuning process. This approach not only accelerates development, but also provides robust security and performance in production environments. 
Solution overview 
In this section, we explore the key components of our architecture for fine-tuning a Meta Llama model and using it for web task automation. We explore the benefits of different components and how they interact with each other, and how we can use them to build a production-grade fine-tuning pipeline. 
AWS DLCs for training and hosting AI/ML workloads 
At the core of our solution are AWS DLCs, which provide optimized environments for machine learning (ML) workloads. These containers come preconfigured with essential dependencies, including NVIDIA drivers, CUDA toolkit, and Elastic Fabric Adapter (EFA) support, along with preinstalled frameworks like PyTorch for model training and hosting. AWS DLCs tackle the complex challenge of packaging various software components to work harmoniously with training scripts, so you can use optimized hardware capabilities out of the box. Additionally, AWS DLCs implement unique patching algorithms and processes that continuously monitor, identify, and address security vulnerabilities, making sure the containers remain secure and up-to-date. Their pre-validated configurations significantly reduce setup time and reduce compatibility issues that often occur in ML infrastructure setup. 
AWS DLCs, Amazon EKS, and Amazon EC2 for seamless infrastructure management 
We deploy these DLCs on Amazon EKS, creating a robust and scalable infrastructure for model fine-tuning. Organizations can use this combination to build and manage their training infrastructure with unprecedented flexibility. Amazon EKS handles the complex container orchestration, so you can launch training jobs that run within DLCs on your desired Amazon Elastic Compute Cloud (Amazon EC2) instance, producing a production-grade environment that can scale based on training demands while maintaining consistent performance. 
AWS DLCs and EFA support for high-performance networking 
AWS DLCs come with pre-configured support for EFA, enabling high-throughput, low-latency communication between EC2 nodes. An EFA is a network device that you can attach to your EC2 instance to accelerate AI, ML, and high performance computing applications. DLCs are pre-installed with EFA software that is tested and compatible with the underlying EC2 instances, so you don’t have to go through the hassle of setting up the underlying components yourself. For this post, we use setup scripts to create EKS clusters and EC2 instances that will support EFA out of the box. 
AWS DLCs with FSDP for enhanced memory efficiency 
Our solution uses PyTorch’s built-in support for Fully Sharded Data Parallel (FSDP) training, a cutting-edge technique that dramatically reduces memory requirements during training. Unlike traditional distributed training approaches where each GPU must hold a complete model copy, FSDP shards model parameters, optimizer states, and gradients across workers. The optimized implementation of FSDP within AWS DLCs makes it possible to train larger models with limited GPU resources while maintaining training efficiency. 
For more information, see Scale LLMs with PyTorch 2.0 FSDP on Amazon EKS – Part 2. 
Model deployment on Amazon Bedrock 
For model deployment, we use Amazon Bedrock, a fully managed service for FMs. Although we can use AWS DLCs for model hosting, we use Amazon Bedrock for this post to demonstrate diversity in service utilization. 
Web automation integration 
Finally, we implement the SeeAct agent, a sophisticated web automation tool, and demonstrate its integration with our hosted model on Amazon Bedrock. This combination creates a powerful system capable of understanding visual inputs and executing complex web tasks autonomously, showcasing the practical applications of our fine-tuned model.In the following sections, we demonstrate how to: 
 
 Set up an EKS cluster for AI workloads. 
 Use AWS DLCs to fine-tune Meta Llama 3.2 Vision using PyTorch FSDP. 
 Deploy the fine-tuned model on Amazon Bedrock. 
 Use the model with SeeAct for web task automation. 
 
Prerequisites 
You must have the following prerequisites: 
 
 An AWS account. 
 An AWS Identity and Access Management (IAM) role with appropriate policies. Because this post deals with creating clusters, nodes, and infrastructure, administrator-level permissions would work well. However, if you must have restricted permissions, you should at least have the following permissions: AmazonEC2FullAccess, AmazonSageMakerFullAccess, AmazonBedrockFullAccess, AmazonS3FullAccess, AWSCloudFormationFullAccess, AmazonEC2ContainerRegistryFullAccess. For more information about other IAM policies needed, see Minimum IAM policies. 
 The necessary dependencies installed for Amazon EKS. For instructions, see Set up to use Amazon EKS. 
 For this post, we use P5 instances. To request a quota increase, see Requesting a quota increase. 
 An EC2 key pair. For instructions, see Create a key pair for your Amazon EC2 instance. 
 
Run export AWS_REGION=&lt;region_name&gt; in your bash script from where you are running the commands. 
Set up the EKS cluster 
In this section, we walk through the steps to create your EKS cluster and install the necessary plugins, operators, and other dependencies. 
Create an EKS cluster 
The simplest way to create an EKS cluster is to use the cluster configuration YAML file. You can use the following sample configuration file as a base and customize it as needed. Provide the EC2 key pair created as a prerequisite. For more configuration options, see Using Config Files. 
 
 ---
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
&nbsp;&nbsp;name: MyCluster
&nbsp;&nbsp;region: us-west-2

managedNodeGroups: 
&nbsp;&nbsp;- name: p5
&nbsp;&nbsp; &nbsp;instanceType: p5.48xlarge
&nbsp;&nbsp; &nbsp;minSize: 0
&nbsp;&nbsp; &nbsp;maxSize: 2
&nbsp;&nbsp; &nbsp;desiredCapacity: 2
&nbsp;&nbsp; &nbsp;availabilityZones: ["us-west-2a"]
&nbsp;&nbsp; &nbsp;volumeSize: 1024
&nbsp;&nbsp; &nbsp;ssh:
&nbsp;&nbsp; &nbsp; &nbsp;publicKeyName: &lt;your-ec2-key-pair&gt;
&nbsp;&nbsp; &nbsp;efaEnabled: true
&nbsp;&nbsp; &nbsp;privateNetworking: true
&nbsp; &nbsp; ## In case you have an On Demand Capacity Reservation (ODCR) and want to&nbsp;use it, uncomment the lines below.
&nbsp; &nbsp; # capacityReservation:
&nbsp; &nbsp; #&nbsp; &nbsp;capacityReservationTarget:
&nbsp; &nbsp; #&nbsp; &nbsp; &nbsp;capacityReservationResourceGroupARN: arn:aws:resource-groups:us-west-2:897880167187:group/eks_blog_post_capacity_reservation_resource_group_p5 
 
Run the following command to create the EKS cluster: 
eksctl create cluster --config-file cluster.yamlThe following is an example output: 
 
 YYYY-MM-DD HH:mm:SS [ℹ] eksctl version x.yyy.z
YYYY-MM-DD HH:mm:SS [ℹ] using region &lt;region_name&gt;
...
YYYY-MM-DD HH:mm:SS [✔] EKS cluster "&lt;cluster_name&gt;" in "&lt;region_name&gt;" region is ready 
 
Cluster creation might take between 15–30 minutes. After it’s created, your local ~/.kube/config file gets updated with connection information to your cluster. 
Run the following command line to verify that the cluster is accessible: 
kubectl get nodes 
Install plugins, operators, and other dependencies 
In this step, you install the necessary plugins, operators and other dependencies on your EKS cluster. This is necessary to run the fine-tuning on the correct node and save the model. 
 
 Install the NVIDIA Kubernetes device plugin: 
 
 
 kubectl&nbsp;create&nbsp;-f&nbsp;https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v0.17.0/deployments/static/nvidia-device-plugin.yml 
 
 
 Install the AWS EFA Kubernetes device plugin: 
 
 
 helm&nbsp;repo&nbsp;add&nbsp;eks&nbsp;https://aws.github.io/eks-charts
git&nbsp;clone&nbsp;-b&nbsp;v0.0.190&nbsp;https://github.com/aws/eks-charts.git
cd&nbsp;&nbsp;eks-charts/stable
helm&nbsp;install&nbsp;efa&nbsp;./aws-efa-k8s-device-plugin&nbsp;-n&nbsp;kube-system
cd ../.. 
 
 
 Delete aws-efa-k8s-device-plugin-daemonset by running the following command: 
 
 
 kubectl delete daemonset aws-efa-k8s-device-plugin-daemonset -n kube-system 
 
 
 Clone the code locally that with help with setup and fine-tuning: 
 
 
 git clone https://github.com/aws-samples/aws-do-eks.git
cd aws-do-eks
git checkout f59007ee50117b547305f3b8475c8e1b4db5a1d5
curl -L -o patch-aws-do-eks.tar.gz https://github.com/aws/deep-learning-containers/raw/refs/heads/master/examples/dlc-llama-3-finetuning-and-hosting-with-agent/patch-aws-do-eks.tar.gz
ftar -xzf patch-aws-do-eks.tar.gz
cd patch-aws-do-eks/
git am *.patch
cd ../.. 
 
 
 Install etcd for running distributed training with PyTorch: 
 
 
 kubectl apply -f aws-do-eks/Container-Root/eks/deployment/etcd/etcd-deployment.yaml 
 
 
 Deploy the FSx CSI driver for saving the model after fine-tuning: 
   
   Enter into the fsx folder: 
     
     cd aws-do-eks/Container-Root/eks/deployment/csi/fsx/ 
      
   Edit the fsx.conf file to modify the CLUSTER_NAME, CLUSTER_REGION, and CLUSTER_ZONE values to your cluster specific data: 
     
     vi fsx.conf 
      
   Deploy the FSX CSI driver: 
     
     ./deploy.sh 
      
    
 Deploy the Kubeflow Training Operator that will be used to run the fine-tuning job: 
   
   Change the location to the following: 
     
     cd aws-do-eks/Container-Root/eks/deployment/kubeflow/training-operator/ 
      
   Deploy the Kubeflow Training Operator: 
     
     ./deploy.sh 
      
    
 Deploy the Kubeflow MPI Operator for running NCCL tests: 
   
   Run deploy.sh from the following GitHub repo. 
   Change the location to the following: 
     
     cd aws-do-eks/Container-Root/eks/deployment/kubeflow/mpi-operator/ 
      
   Deploy the Kubeflow MPI Operator: 
     
     ./deploy.sh 
      
    
 
Fine-tune Meta Llama 3.2 Vision using DLCs on Amazon EKS 
This section outlines the process for fine-tuning the Meta Llama 3.2 Vision model using PyTorch FSDP on Amazon EKS. We use the DLCs as the base image to run our training jobs. 
Configure the setup needed for fine-tuning 
Complete the following steps to configure the setup for fine-tuning: 
 
 Create a Hugging Face account and get a Hugging Face security token. 
 Enter into the fsdp folder: 
 
 
 cd Container-Root/eks/deployment/distributed-training/pytorch/pytorchjob/fsdp 
 
 
 Create a Persistent Volume Claim (PVC) that will use the underlying FSx CSI driver that you installed earlier: 
 
 
 kubectl apply -f pvc.yaml 
 
Monitor kubectl get pvc fsx-claim and make sure it reached BOUND status. You can then go to the Amazon EKS console to see an unnamed volume created without a name. You can let this happen in the background, but before you run the ./run.sh command to run the fine-tuning job in a later step, make sure the BOUND status is achieved. 
 
 To configure the environment, open the .env file and modify the following variables: 
   
   HF_TOKEN: Add the Hugging Face token that you generated earlier. 
   S3_LOCATION: Add the Amazon Simple Storage Service (Amazon S3) location where you want to store the fine-tuned model after the training is complete. 
    
 Create the required resource YAMLs: 
 
 
 ./deploy.sh 
 
This line uses the values in the .env file to generate new YAML files that will eventually be used for model deployment. 
 
 Build and push the container image: 
 
 
 ./login-dlc.sh
./build.sh
./push.sh 
 
Run the fine-tuning job 
In this step, we use the upstream DLCs and add the training scripts within the image for running the training. 
Make sure that you have requested access to the Meta Llama 3.2 Vision model on Hugging Face. Continue to the next step after permission has been granted. 
Execute the fine-tuning job: 
 
 ./run.sh 
 
For our use case, the job took 1.5 hours to complete. The script uses the following PyTorch command that’s defined in the .env file within the fsdp folder: 
 
 ```
bash
torchrun --nnodes 1 --nproc_per_node 8 &nbsp;\
recipes/quickstart/finetuning/finetuning.py \
--enable_fsdp --lr 1e-5 &nbsp;--num_epochs 5 \
--batch_size_training 2 \
--model_name meta-llama/Llama-3.2-11B-Vision-Instruct \
--dist_checkpoint_root_folder ./finetuned_model \
--dist_checkpoint_folder fine-tuned &nbsp;\
--use_fast_kernels \
--dataset "custom_dataset" --custom_dataset.test_split "test" \
--custom_dataset.file "recipes/quickstart/finetuning/datasets/mind2web_dataset.py" &nbsp;\
--run_validation False --batching_strategy padding
``` 
 
You can use the ./logs.sh command to see the training logs in both FSDP workers. 
After a successful run, logs from fsdp-worker will look as follows: 
 
 Sharded state checkpoint saved to /workspace/llama-recipes/finetuned_model_mind2web/fine-tuned-meta-llama/Llama-3.2-11B-Vision-Instruct
Checkpoint Time = 85.3276

Epoch 5: train_perplexity=1.0214, train_epoch_loss=0.0211, epoch time 706.1626197730075s
training params are saved in /workspace/llama-recipes/finetuned_model_mind2web/fine-tuned-meta-llama/Llama-3.2-11B-Vision-Instruct/train_params.yaml
Key: avg_train_prep, Value: 1.0532150745391846
Key: avg_train_loss, Value: 0.05118955448269844
Key: avg_epoch_time, Value: 716.0386156642023
Key: avg_checkpoint_time, Value: 85.34336999000224
fsdp-worker-1:78:5593 [0] NCCL INFO [Service thread] Connection closed by localRank 1
fsdp-worker-1:81:5587 [0] NCCL INFO [Service thread] Connection closed by localRank 4
fsdp-worker-1:85:5590 [0] NCCL INFO [Service thread] Connection closed by localRank 0
I0305 19:37:56.173000 140632318404416 torch/distributed/elastic/agent/server/api.py:844] [default] worker group successfully finished. Waiting 300 seconds for other agents to finish.
I0305 19:37:56.173000 140632318404416 torch/distributed/elastic/agent/server/api.py:889] Local worker group finished (WorkerState.SUCCEEDED). Waiting 300 seconds for other agents to finish
I0305 19:37:56.177000 140632318404416 torch/distributed/elastic/agent/server/api.py:902] Done waiting for other agents. Elapsed: 0.0037238597869873047 seconds 
 
Additionally: 
 
 [rank8]:W0305 19:37:46.754000 139970058049344 torch/distributed/distributed_c10d.py:2429] _tensor_to_object size: 2817680 hash value: 9260685783781206407
fsdp-worker-0:84:5591 [0] NCCL INFO [Service thread] Connection closed by localRank 7
I0305 19:37:56.124000 139944709084992 torch/distributed/elastic/agent/server/api.py:844] [default] worker group successfully finished. Waiting 300 seconds for other agents to finish.
I0305 19:37:56.124000 139944709084992 torch/distributed/elastic/agent/server/api.py:889] Local worker group finished (WorkerState.SUCCEEDED). Waiting 300 seconds for other agents to finish
I0305 19:37:56.177000 139944709084992 torch/distributed/elastic/agent/server/api.py:902] Done waiting for other agents. Elapsed: 0.05295562744140625 seconds 
 
 
 
Run the processing model and store output in Amazon S3 
After the jobs are complete, the fine-tuned model will exist in the FSx file system. The next step is to convert the model into Hugging Face format and save it in Amazon S3 so you can access and deploy the model in the upcoming steps:kubectl apply -f model-processor.yaml 
The preceding command deploys a pod on your instance that will read the model from FSx, convert it to Hugging Face type, and push it to Amazon S3. It takes approximately 8–10 minutes for this pod to run. You can monitor the logs for this using ./logs.sh or kubectl logs -l app=model-processor. 
Get the location where your model has been stored in Amazon S3. This is the same Amazon S3 location that was mentioned the .env file in an earlier step. Run the following command (provide the Amazon S3 location):aws s3 cp tokenizer_config.json &lt;S3_LOCATION&gt;://tokenizer_config.json 
This is the tokenizer config that is needed by Amazon Bedrock to import Meta Llama models so they work with the Amazon Bedrock Converse API. For more details, see Converse API code samples for custom model import. 
For this post, we use the Mind2Web dataset. We have implemented code that has been adapted from the Mind2Web code for fine-tuning. The adapted code is as follows: 
 
 git clone https://github.com/meta-llama/llama-cookbook &amp;&amp; \
cd llama-cookbook &amp;&amp; \
git checkout a346e19df9dd1a9cddde416167732a3edd899d09 &amp;&amp; \
curl -L -o patch-llama-cookbook.tar.gz https://raw.githubusercontent.com/aws/deep-learning-containers/master/examples/dlc-llama-3-finetuning-and-hosting-with-agent/patch-llama-cookbook.tar.gz &amp;&amp; \
tar -xzf patch-llama-cookbook.tar.gz &amp;&amp; \
cd patch-llama-cookbook &amp;&amp; \
git config --global user.email "you@example.com" &amp;&amp; \
git am *.patch &amp;&amp;&nbsp;\
cd ..&nbsp;&amp;&amp;&nbsp;\
cat recipes/quickstart/finetuning/datasets/mind2web_dataset.py 
 
Deploy the fine-tuned model on Amazon Bedrock 
After you fine-tune your Meta Llama 3.2 Vision model, you have several options for deployment. This section covers one deployment method using Amazon Bedrock. With Amazon Bedrock, you can import and use your custom trained models seamlessly. Make sure your fine-tuned model is uploaded to an S3 bucket, and it’s converted to Hugging Face format. Complete the following steps to import your fine-tuned Meta Llama 3.2 Vision model: 
 
 On the Amazon Bedrock console, under Foundation models in the navigation pane, choose Imported models. 
 Choose Import model. 
 For Model name, enter a name for the model. 
 
 
 
 For Model import source, select Amazon S3 bucket. 
 For S3 location, enter the location of the S3 bucket containing your fine-tuned model. 
 
 
 
 Configure additional model settings as needed, then import your model. 
 
The process might take 10–15 minutes depending on the model size to complete. 
After you import your custom model, you can invoke it using the same Amazon Bedrock API as the default Meta Llama 3.2 Vision model. Just replace the model name with your imported model’s Amazon Resource Name (ARN). For detailed instructions, refer to Amazon Bedrock Custom Model Import. 
You can follow the prompt formats mentioned in the following GitHub repo. For example: 
What are the steps to build a docker image?&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt; 
Run the agent workload using the hosted Amazon Bedrock model 
Running the agent workload involves using the SeeAct framework and browser automation to start an interactive session with the AI agent and perform the browser operations. We recommend completing the steps in this section on a local machine for browser access. 
Clone the SeeAct repository 
Clone the customized SeeAct repository, which contains example code that can work with Amazon Bedrock, as well as a couple of test scripts: 
 
 git clone https://github.com/OSU-NLP-Group/SeeAct.git 
 
Set up SeeAct in a local runtime environment 
Complete the following steps to set up SeeAct in a local runtime environment: 
 
 Create a Python virtual environment for this demo. We use Python 3.11 in the example, but you can change to other Python versions. 
 
 
 python3.11 -m venv seacct-python-3-11
source seacct-python-3-11/bin/activate 
 
 
 Apply a patch to add the code change needed for this demo: 
 
 
 cd SeeAct
curl -O https://raw.githubusercontent.com/aws/deep-learning-containers/master/examples/dlc-llama-3-finetuning-and-hosting-with-agent/patch-seeact.patch
git checkout 2fdbf373f58a1aa5f626f7c5931fe251afc69c0a
git apply patch-seeact.patch 
 
 
 Run the following commands to install the SeeAct package and dependencies: 
 
 
 cd SeeAct/seeact_package
pip install .
pip install -r&nbsp;requirements.txt
pip install -U boto3
playwright install 
 
Make sure you’re using the latest version of Boto3 for these steps. 
Validate the browser automation tool used by SeeAct 
We added a small Python script to verify the functionality of Playwright, the browser automation tool used by SeeAct: 
 
 cd SeeAct/src
python test_playwright.py 
 
You should see a browser launched and closed after a few seconds. You should also see a screenshot being captured in SeeAct/src/example.png showing google.com. 
 
Test Amazon Bedrock model availability 
Modify the content of test_bedrock.py. Update the MODEL_ID to be your hosted Amazon Bedrock model ARN and set up the AWS connection. 
 
 export AWS_ACCESS_KEY_ID="replace with your aws credential"
export AWS_SECRET_ACCESS_KEY="replace with your aws credential"
export AWS_SESSION_TOKEN="replace with your aws credential" 
 
Run the test: 
 
 cd SeeAct
python&nbsp;test_bedrock.py 
 
After a successful invocation, you should see a log similar to the following in your terminal: 
 
 The image shows a dog lying down inside a black pet carrier, with a leash attached to the dog's collar. 
 
If the botocore.errorfactory.ModelNotReadyException error occurs, retry the command in a few minutes. 
Run the agent workflow 
The branch has already added support for BedrockEngine and SGLang for running inference with the fine-tuned Meta Llama 3.2 Vision model. The default option uses Amazon Bedrock inference. 
To run the agent workflow, update self.model from src/demo_utils/inference_engine.py at line 229 to your Amazon Bedrock model ARN. Then run the following code: 
 
 cd SeeAct/src
python seeact.py -c config/demo_mode.toml&nbsp; 
 
This will launch a terminal prompt like the following code, so you can input the task you want the agent to do: 
 
 Please input a task, and press Enter. 
Or directly press Enter to use the default task: Find pdf of paper "GPT-4V(ision) is a Generalist Web Agent, if Grounded" from arXiv
Task:&nbsp; 
 
In the following screenshot, we asked the agent to search for the website for DLCs. 
 
Clean up 
Use the following code to clean the resources you created as part of this post: 
 
 cd Container-Root/eks/deployment/distributed-training/pytorch/pytorchjob/fsdp
kubectl delete&nbsp;-f ./fsdp.yaml&nbsp;## Deletes the training fsdp job
kubectl&nbsp;delete&nbsp;-f&nbsp;./etcd.yaml&nbsp;## Deletes etcd
kubectl delete&nbsp;-f ./model-processor.yaml&nbsp;## Deletes model processing YAML

cd aws-do-eks/Container-Root/eks/deployment/kubeflow/mpi-operator/
./remove.sh

cd aws-do-eks/Container-Root/eks/deployment/kubeflow/training-operator/
./remove.sh

## [VOLUME GETS DELETED] - If&nbsp;you want to delete&nbsp;the FSX volume
kubectl delete -f ./pvc.yaml ## Deletes persistent volume claim, persistent volume and actual volume 
 
To stop the P5 nodes and release them, complete the following steps: 
 
 On the Amazon EKS console, choose Clusters in the navigation pane. 
 Choose the cluster that contains your node group. 
 On the cluster details page choose the Compute tab. 
 In the Node groups section, select your node group, then choose Edit. 
 Set the desired size to 0. 
 
Conclusion 
In this post, we presented an end-to-end workflow for fine-tuning and deploying the Meta Llama 3.2 Vision model using the production-grade infrastructure of AWS. By using AWS DLCs on Amazon EKS, you can create a robust, secure, and scalable environment for model fine-tuning. The integration of advanced technologies like EFA support and FSDP training enables efficient handling of LLMs while optimizing resource usage. The deployment through Amazon Bedrock provides a streamlined path to production, and the integration with SeeAct demonstrates practical applications in web automation tasks. This solution serves as a comprehensive reference point for engineers to develop their own specialized AI applications, adapt the demonstrated approaches, and implement similar solutions for web automation, content analysis, or other domain-specific tasks requiring vision-language capabilities. 
To get started with your own implementation, refer to our GitHub repo. To learn more about AWS DLCs, see the AWS Deep Learning Containers Developer Guide. For more details about Amazon Bedrock, see Getting started with Amazon Bedrock. 
For deeper insights into related topics, refer to the following resources: 
 
 Scale LLMs with PyTorch 2.0 FSDP on Amazon EKS – Part 2 
 Build high-performance ML models using PyTorch 2.0 on AWS – Part 1 
 Mind2Web dataset 
 
Need help or have questions? Join our AWS Machine Learning community on Discord or reach out to AWS Support. You can also stay updated with the latest developments by following the AWS Machine Learning Blog. 
 
About the Authors 
Shantanu Tripathi is a Software Development Engineer at AWS with over 4 years of experience in building and optimizing large-scale AI/ML solutions. His experience spans developing distributed AI training libraries, creating and launching DLCs and Deep Learning AMIs, designing scalable infrastructure for high-performance AI workloads, and working on generative AI solutions. He has contributed to AWS services like Amazon SageMaker HyperPod, AWS DLCs, and DLAMIs, along with driving innovations in AI security. Outside of work, he enjoys theater and swimming. 
Junpu Fan is a Senior Software Development Engineer at Amazon Web Services, specializing in AI/ML Infrastructure. With over 5 years of experience in the field, Junpu has developed extensive expertise across the full cycle of AI/ML workflows. His work focuses on building robust systems that power ML applications at scale, helping organizations transform their data into actionable insights. 
Harish Rao is a Senior Solutions Architect at AWS, specializing in large-scale distributed AI training and inference. He helps customers harness the power of AI to drive innovation and solve complex challenges. Outside of work, Harish embraces an active lifestyle, enjoying the tranquility of hiking, the intensity of racquetball, and the mental clarity of mindfulness practices. 
Arindam Paul is a Sr. Product Manager in SageMaker AI team at AWS responsible for Deep Learning workloads on SageMaker, EC2, EKS, and ECS. He is passionate about using AI to solve customer problems. In his spare time, he enjoys working out and gardening.
• How Nippon India Mutual Fund improved the accuracy of AI assistant responses using advanced RAG methods on Amazon Bedrock
  This post is co-written with Abhinav Pandey from Nippon Life India Asset Management Ltd. 
Accurate information retrieval through generative AI-powered assistants is a popular use case for enterprises. To reduce hallucination and improve overall accuracy, Retrieval Augmented Generation (RAG) remains the most commonly used method to retrieve reliable and accurate responses that use enterprise data when responding to user queries. RAG is used for use cases such as AI assistants, search, real-time insights, and improving overall content quality by using the relevant data to generate the response, thereby reducing hallucinations. 
Amazon Bedrock Knowledge Bases provides a managed RAG experience that can be used for many use cases. Amazon Bedrock Knowledge Bases is a fully managed service that does the heavy lifting of implementing a RAG pattern—including data ingestion, data chunking, data embedding, and query matching. Amazon Bedrock offers a choice of high-performing foundation models (FMs) from leading AI companies such as AI21 Labs, Anthropic, Cohere, Meta, Mistral AI, Stability AI, and Amazon through a single API, along with a broad set of capabilities to build generative AI applications with security, privacy, and responsible AI. Using Amazon Bedrock Knowledge Bases, you can create a RAG solution quickly and seamlessly. 
However, in a large enterprise scenario with a large number of relevant documents, the final response is generated based on only the top 5 or top 10 results provided by the database. Because RAG uses a similarity match and not an exact match, there is a possibility that the most relevant result won’t be in the top results returned by the database. In such scenarios, the regular RAG pattern might not be highly accurate. 
In this post, we examine a solution adopted by Nippon Life India Asset Management Limited that improves the accuracy of the response over a regular (naive) RAG approach by rewriting the user queries and aggregating and reranking the responses. The proposed solution uses enhanced RAG methods such as reranking to improve the overall accuracy. 
Limitations and workarounds in naive RAG for a large volume of documents 
The following are the primary limitations with naive RAG when using it with a large volume of documents: 
 
 Accuracy – As the documents grow in number or size, the final list of extracted chunks might miss some relevant sections or documents because of the limited number of documents returned 
 Parsing complex structures – Entities such as nested tables, images, and graphs are not parsed accurately 
 Limited use of relevant data – As the dataset grows, only the configured set of top results are used for context, impacting accuracy 
 Responding to complex questions – Compound questions (such as a question with multiple sub-questions) pose challenges to the RAG solution 
 Retrieving the correct context – Documents such as legal documents or technical manuals have semantically related sections on different pages, impacting the overall accuracy 
 Avoiding hallucinations – Models must generate complete, correct, and grounded responses without hallucinations 
 
To address these challenges, developers usually adopt a combination of the following workarounds: 
 
 Programmatic parsing – Use another service (such as Amazon Textract) to extract the table content into a markdown (.MD) file. 
 Programmatic split of compound questions – Programmatically split the question or reformulate the question and get the responses and then programmatically aggregate the response. 
 Programmatic chunking – Programmatically create custom chunks of the documents and manage them in a vector store. 
 
Solution overview 
In this section, we review the basics of RAG, Amazon Bedrock Knowledge Bases, and advanced RAG methods to address the preceding challenges. The following table details the specific solution components adopted by Nippon to overcome the challenges we discussed in the previous section. 
 
  
   
   Naive RAG Challenges 
   How Nippon Addressed These Challenges 
   
   
   Lower accuracy due to high volume of documents 
   Use Amazon Bedrock Knowledge Bases with advanced RAG methods, including semantic chunking, multi-query RAG, and results reranking 
   
   
   Parsing complex document structure such as nested tables and graphs within documents 
   Use Amazon Textract to parse the documents into markdown files 
   
   
   Handling compound questions 
   Use query reformulation and results reranking to get the relevant results 
   
   
   Retrieving current context from the documents 
   Use semantic chunking in Amazon Bedrock Knowledge Bases 
   
   
   Generating grounded responses without any hallucination 
   Use Amazon Bedrock Knowledge Bases RAG evaluation 
   
  
 
In the next few sections, we detail each of the solution components. We start with an overview of RAG and then discuss Amazon Bedrock Knowledge Bases. We then discuss advanced RAG methods adopted by Nippon, including advanced parsing, query reformulation, multi-query RAG, and results reranking. 
RAG overview 
The RAG pattern for this solution has two primary workflows: a data ingestion workflow and a text generation phase, as depicted in the following figure. During the ingestion phase, the solution chunks the content of the source documents, creates embeddings for the created chunks, and stores them in a vector database. In the text generation workflow, the user query is converted to a vector embedding and the query embedding is compared with embeddings stored in the vector database. The database provides the top results that are close to the user query. The solution then sends the user query along with the retrieved top results as the context to the FM, which then provides the final response. Because the responses are based on the relevant contextual enterprise data, this solution reduces hallucinations. 

 
 Ingestion &amp; Text generation workflows
 
You can implement this solution using AWS services as shown in the following figure. 

 
 RAG implementation using AWS services
 
The following is a typical RAG-based AI assistant flow.The first series of steps, as numbered in the preceding diagram, augment the user query using a knowledge base: 
 
 User queries are served by Amazon Lex hosted on Amazon CloudFront. 
 The business logic for the AI assistant can run on Amazon Elastic Compute Cloud (Amazon EC2), Amazon Elastic Container Service (Amazon ECS), Amazon Elastic Kubernetes Service (Amazon EKS), or AWS Lambda. 
 You can manage the conversation history or state in Amazon DynamoDB, Amazon DocumentDB (with MongoDB compatibility), or Amazon MemoryDB. 
 The user query and the earlier conversation history are sent to the Amazon Bedrock embedding model. 
 User prompts are compared to earlier prompts cached in MemoryDB. If the user prompt matches a stored prompt, the stored prompt result is retrieved and sent to the user. 
 If there’s no match for the user prompt in the cache, the vector database (such as Amazon OpenSearch, Amazon Aurora, Amazon Kendra, MemoryDB, DocumentDB, or Amazon Neptune) is searched for the query embedding. The relevant knowledge or Amazon Simple Storage Service (Amazon S3) path to the relevant image or video is retrieved. 
 The relevant knowledge, image, or video is retrieved from the Amazon S3 path. 
 The multimodal large language model (LLM) in Amazon Bedrock uses the query results to create a final response. 
 The new answer is stored in the database cache. 
 The final response is sent to the user. 
 
The workflow to create and maintain the knowledge base is shown in the previous diagram: 
 
 The data is ingested from structured and unstructured sources such as a file system, emails, or multi-media content. 
 The data is ingested into Amazon S3 in batches or in real time using services such as AWS Database Migration Service (AWS DMS), Amazon Kinesis, Amazon Managed Streaming for Apache Kafka (Amazon MSK), Amazon AppFlow, or AWS Glue. 
 The data is processed through AWS Glue, Amazon SageMaker processing, or Amazon EMR. 
 An embeddings LLM from Amazon Bedrock generates the embeddings for the content chunks. 
 The embeddings are stored in the vector database. 
 
Amazon Bedrock Knowledge Bases 
Amazon Bedrock Knowledge Bases provides managed RAG, which does the heavy lifting of the core activities in the data ingestion and text generation workflows. Nippon uses Amazon Bedrock Knowledge Bases for implementing the RAG pattern. The following figure shows the fully managed data ingestion workflow provided by Amazon Bedrock Knowledge Bases. You can choose the data source from which data of various formats can be incrementally updated. For content chunking, you can choose from the following strategies: fixed, semantic, or hierarchical. You can also choose the embedding models and vector store. 

 
 Data ingestion flow
 
The following table compares various chunking strategies. You can also author a custom chunking strategy using Lambda. 
 
  
   
   Chunking strategy 
   Details 
   Advantages 
   
   
   Fixed chunking 
    
     
     Fixed character count division 
     Recursive character text splitting 
      
   Quick and efficient 
   
   
   Semantic chunking (used by Nippon) 
    
     
     Meaningful and complete chunks based on semantic similarity 
      
   Better retrieval quality 
   
   
   Hierarchical chunking 
    
     
     Organize chunks in a structured manner 
     Nodes with summaries and parent-child relationships 
      
   Improved retrieval efficiency and reliability 
   
  
 
Nippon uses semantic chunking because the documents have sections with semantic similarity. During the text generation workflow, Amazon Bedrock Knowledge Bases creates embeddings from the user input and performs semantic search in the Amazon Bedrock knowledge base. The retrieved results from the context are augmented with a prompt for the LLM to generate the final response as shown in the following figure. 

 
 RAG Flow
 
Advanced RAG methods adopted by Nippon 
In this section, we review the key advanced RAG methods used in the Nippon AI assistant solution. 
Advanced parsing and chunking 
A complex document with tables, images, and graphs poses challenges in RAG because the default fixed chunking often loses the context. For these scenarios, Nippon uses the following approaches: 
 
 Parse the document with Amazon Textract – Nippon uses Amazon Textract—which excels at understanding complex structures like tables—to extract the details from the document into a markdown file (.MD). 
 Parse the document with an FM with specific instructions – In this approach, Nippon uses an FM that has specific parsing instructions to extract the details from the document. Amazon Bedrock Knowledge Bases provides LLMs along with instructions for parsing the documents. 
 Parse the document using third-party parsers – This approach uses third-party parsers such as open source document parsers and then indexed the parsed content. 
 
After the documents are parsed, Amazon Bedrock Knowledge Bases chunks the data. Amazon Bedrock Knowledge Bases provides fixed chunking, semantic chunking, and hierarchical chunking strategies. You can also add custom chunking using Lambda functions for specific documents. 
Query reformulation 
If the user queries are complex, the overall accuracy of the RAG solution decreases because it can be difficult to understand all the nuances of the complex question. In such cases, you can split the complex question into multiple sub-questions for improved accuracy. Nippon used an LLM to split the compound query into multiple sub-queries. The search results for each of the sub-queries are returned simultaneously and are then ranked and combined. The LLM uses the combined data to provide the final response, as shown in the following figure. 

 
 Query reformulation
 
Multi-query RAG 
In the multi-query RAG approach, you can reformulate the question into different variants (Amazon Bedrock Knowledge Bases provides automatic query reformulation) and execute the queries in parallel. You then summarize the results of those queries and rerank them to get the final response as shown in the following figure. 

 
 Multi-query RAG
 
Nippon uses the initial question and break it into n sub-questions using our choice of LLM (Anthropic’s Claude3 Sonnet on Amazon Bedrock). With this implementation, the quality of our responses has improved, and the query is answered in greater depth. Because this requires additional processing, the response time is slightly longer, which is justified by the quality of the response. 
Results reranking 
In a typical enterprise scenario where you have a large volume of documents, the ranking of the results might not be correct and accuracy could be reduced. For example, the default results returned from a vector database will be based on vector similarity criteria whose rank might not be accurate or relevant based on the user’s conversation history or the query context. In such scenarios, you can use another FM to look closely at the results and reorder them based on analysis of the context, relevance, and other criteria. After the results are reranked, you can be certain that the top results are highly relevant to the user context. Each document or search result is assigned a score based on criteria such as semantic similarity, contextual alignment, or domain-specific features, thereby increasing the overall accuracy of the final response, reducing hallucination, and improving user satisfaction. The key reranker strategies are: 
 
 Cross-encoder reranking – Uses models specifically trained to assess the relevance between query-document pairs 
 Hybrid reranking – Combines multiple signals such as vector similarity, keyword matching, recency, and so on 
 Multi-stage retrieval – Retrieves a larger set using faster methods, then applies more computationally intensive reranking on a subset 
 
Amazon Bedrock provides reranker models that calculate the relevance of the chunks for a given query, assigns a score for the chunk, and reorders the results based on the score. By using a reranker model in Amazon Bedrock, you can get highly relevant chunks in the search results, improving the overall response accuracy. Nippon has combined query reformulation with results reranking to improve the overall accuracy of the RAG solution using the Amazon Bedrock reranker models. 
Metrics for RAG Evaluation 
Because RAG solutions have multiple moving parts, you need to evaluate them against key success metrics to make sure that you get relevant and contextual data from the knowledge base. RAG evaluation should validate the generation of complete, correct, and grounded answers without hallucinations. You also need to evaluate the bias, safety, and trust of RAG solutions. 
Amazon Bedrock Knowledge Bases provides built-in support for RAG evaluation, including quality metrics such as correctness, completeness, and faithfulness (hallucination detection); responsible AI metrics such as harmfulness, answer refusal, and stereotyping; and compatibility with Amazon Bedrock Guardrails. Nippon used RAG evaluation to compare against multiple evaluation jobs using custom datasets. 
Nippon is currently evaluating additional techniques, including GraphRAG, metadata filtering, and agentic AI. We have briefly summarized the capabilities being evaluated at Nippon in the following sections. 
GraphRAG 
For applications that need to establish relationships with data that are hierarchically related (such as knowledge management, enterprise search, recommendation systems, and so on), Nippon uses a graph database instead of a vector database. GraphRAG applications use rich and interconnected entity relationships to identify the dependencies and excel in querying multi-dimensional relationships that further boost the context for FMs. In addition, with graph databases, Nippon can efficiently query and traverse the data, and their schema enables them to accommodate dynamic content. The graph databases help the FMs better understand the semantic relationship, uncover hidden patterns, adapt to dynamic knowledge bases, and provide better reasonability. 
Amazon Bedrock Knowledge Bases supports fully managed GraphRAG, which uses the Amazon Neptune graph database. 
Metadata filtering 
In a few use cases, you might need to filter the documents in a knowledge base based on specific metadata values. For instance, government regulatory bodies release regulatory guidelines frequently, often with the same document names and only minor variations in the regulatory clauses. When your query is about a specific regulation, you want the most recent document to be returned. In such cases, you can rank the documents based on their modified date. Amazon Bedrock Knowledge Bases provide custom metadata as filters (such as modified date) to improve the quality of search results. 
Amazon Bedrock Agents 
With Amazon Bedrock Agents, you can orchestrate multi-step business processes using tools and information sources. Nippon is currently evaluating the latest FMs in Amazon Bedrock Agents for their AI assistant use case. 
Solution flow 
The following diagram shows the end-to-end flow of the Nippon AI assistant solution. 

 
 End-to-end flow of the Nippon AI assistant solution
 
The data ingestion workflow consists of the following steps: 
 
 The documents from the data source are prepared for ingestion. 
 Nippon uses custom parsing to extract the relevant details from complex elements like tables, graphs, and images. They use custom chunking to create the chunks. 
 They use an embedding model from Amazon Bedrock to convert the content chunks to vector embeddings and store them in the vector database. 
 
The content generation workflow is as follows: 
 
 The user’s query is converted into embeddings by the Amazon Bedrock embedding model. 
 Nippon uses multi-query RAG by creating multiple variants of the query and executing them in parallel. The obtained results are reranked using a reranker model for higher accuracy. 
 The prompt is augmented with the references from the source documents to create citations 
 The augmented prompt is sent to Amazon Bedrock. 
 Amazon Bedrock creates and sends the final response to the user. 
 
Nippon plans to use agentic AI implementation in the future for automating the data retrieval, indexing, and ingestion. 
Results 
Nippon saw the following improvements after implementing RAG: 
 
 Accuracy was increased by more than 95% 
 Hallucination was reduced by 90–95% 
 They were able to add source chunks and file links (through file metadata), which improves the user confidence in the response 
 The time needed to generate a report was reduced from 2 days to approximately 10 minutes 
 
Summary 
In this post, we discussed RAG and some of the challenges associated with processing large volumes of documents. We explained the advanced RAG methods used in the Nippon AI assistant, including enhanced parsing using Amazon Bedrock Knowledge Bases and third-party models. In addition, we explained query reformulation and multi-query RAG techniques—such as generating multiple queries, reranking results, using GraphRAG, and applying metadata filtering. Finally, we described the end-to-end solution implemented for the Nippon AI assistant. These methods are generally available and are not built by or belong only to Nippon. 
Explore Amazon Bedrock Knowledge Bases for RAG use cases by using advanced RAG features such as FM as a parser, query reformulation, reranker models, GraphRAG, and others to implement highly accurate RAG solutions. You can also use Amazon Bedrock Guardrails to build responsible AI solutions by enforcing content and image safeguards and enabling automated reasoning checks. 
Please note that RAG methods mentioned in the blog are generally available to all and are not built by or belong only to Nippon.&nbsp; 
 
About the authors 
Shailesh Shivakumar is an FSI Sr. Solutions Architect with AWS India. He works with financial enterprises such as banks, NBFCs, and trading enterprises to help them design secure cloud systems and accelerate their cloud journey. He builds demos and proofs of concept to demonstrate the art of the possible on the AWS Cloud. He leads other initiatives such as customer enablement workshops, AWS demos, cost optimization, and solution assessments to make sure AWS customers succeed in their cloud journey. Shailesh is part of Machine Learning TFC at AWS, handling generative AI and machine learning-focused customer scenarios. Security, serverless, containers, and machine learning in the cloud are his key areas of interest. 
Abhinav Pandey is a seasoned Data Scientist on the Technology team at Nippon Life India Asset Management Ltd, with over 18 years of industry experience, primarily in BFSI, who is passionate about using generative AI and agentic AI to transform business operations. With a proven track record of applying data for strategic decision-making and business growth, he excels at extracting actionable insights from complex datasets using cutting-edge analytical techniques. A strategic thinker and innovative problem solver, he has developed data-driven strategies that enhance operational efficiency and profitability while working effectively with cross-functional teams to align data initiatives with business objectives. In his leadership role, he has driven data science initiatives, fostered innovation, maintained a results-driven approach, and continuously adapted to evolving technologies to stay ahead of industry trends.
• Build a drug discovery research assistant using Strands Agents and Amazon Bedrock
  Drug discovery is a complex, time-intensive process that requires researchers to navigate vast amounts of scientific literature, clinical trial data, and molecular databases. Life science customers like Genentech and AstraZeneca are using AI agents and other generative AI tools to increase the speed of scientific discovery. Builders at these organizations are already using the fully managed features of Amazon Bedrock to quickly deploy domain-specific workflows for a variety of use cases, from early drug target identification to healthcare provider engagement. 
However, more complex use cases might benefit from using the open source Strands Agents SDK. Strands Agents takes a model-driven approach to develop and run AI agents. It works with most model providers, including custom and internal large language model (LLM) gateways, and agents can be deployed where you would host a Python application. 
In this post, we demonstrate how to create a powerful research assistant for drug discovery using Strands Agents and Amazon Bedrock. This AI assistant can search multiple scientific databases simultaneously using the Model Context Protocol (MCP), synthesize its findings, and generate comprehensive reports on drug targets, disease mechanisms, and therapeutic areas. This assistant is available as an example in the open-source&nbsp;healthcare and life sciences agent toolkit&nbsp;for you to use and adapt. 
Solution overview 
This solution uses Strands Agents to connect high-performing foundation models (FMs) with common life science data sources like arXiv, PubMed, and ChEMBL. It demonstrates how to quickly create MCP servers to query data and view the results in a conversational interface. 
Small, focused AI agents that work together can often produce better results than a single, monolithic agent. This solution uses a team of sub-agents, each with their own FM, instructions, and tools. The following flowchart shows how the orchestrator agent (shown in orange) handles user queries and routes them to sub-agents for either information retrieval (green) or planning, synthesis, and report generation (purple). 
 
This post focuses on building with Strands Agents in your local development environment. Refer to the Strands Agents documentation to deploy production agents on AWS Lambda, AWS Fargate, Amazon Elastic Kubernetes Service (Amazon EKS), or Amazon Elastic Compute Cloud (Amazon EC2). 
In the following sections, we show how to create the research assistant in Strands Agents by defining an FM, MCP tools, and sub-agents. 
Prerequisites 
This solution requires Python 3.10+, strands-agents, and several additional Python packages. We strongly recommend using a virtual environment like venv or uv to manage these dependencies. 
Complete the following steps to deploy the solution to your local environment: 
 
 Clone the code repository from GitHub. 
 Install the required Python dependencies with pip install -r requirements.txt. 
 Configure your AWS credentials by setting them as environment variables, adding them to a credentials file, or following another supported process. 
 Save your Tavily API key to a .env file in the following format: TAVILY_API_KEY="YOUR_API_KEY". 
 
You also need access to the following Amazon Bedrock FMs in your AWS account: 
 
 Anthropic’s Claude 3.7 Sonnet 
 Anthropic’s Claude 3.5 Sonnet 
 Anthropic’s Claude 3.5 Haiku 
 
Define the foundation model 
We start by defining a connection to an FM in Amazon Bedrock using the Strands Agents BedrockModel class. We use Anthropic’s Claude 3.7 Sonnet as the default model. See the following code: 
 
 from strands import Agent, tool
from strands.models import BedrockModel
from strands.agent.conversation_manager import SlidingWindowConversationManager
from strands.tools.mcp import MCPClient
# Model configuration with Strands using Amazon Bedrock's foundation models
def get_model():
    model = BedrockModel(
        boto_client_config=Config(
            read_timeout=900,
            connect_timeout=900,
            retries=dict(max_attempts=3, mode="adaptive"),
        ),
        model_id="us.anthropic.claude-3-7-sonnet-20250219-v1:0",
        max_tokens=64000,
        temperature=0.1,
        top_p=0.9,
        additional_request_fields={
            "thinking": {
                "type": "disabled"  # Can be enabled for reasoning mode
            }
        }
    )
    return model 
 
Define MCP tools 
MCP provides a standard for how AI applications interact with their external environments. Thousands of MCP servers already exist, including those for life science tools and datasets. This solution provides example MCP servers for: 
 
 arXiv – Open-access repository of scholarly articles 
 PubMed – Peer-reviewed citations for biomedical literature 
 ChEMBL – Curated database of bioactive molecules with drug-like properties 
 ClinicalTrials.gov – US government database of clinical research studies 
 Tavily Web Search – API to find recent news and other content from the public internet 
 
Strands Agents streamlines the definition of MCP clients for our agent. In this example, you connect to each tool using standard I/O. However, Strands Agents also supports remote MCP servers with Streamable-HTTP Events transport. See the following code: 
 
 # MCP Clients for various scientific databases
tavily_mcp_client = MCPClient(lambda: stdio_client(
    StdioServerParameters(command="python", args=["application/mcp_server_tavily.py"])
))
arxiv_mcp_client = MCPClient(lambda: stdio_client(
    StdioServerParameters(command="python", args=["application/mcp_server_arxiv.py"])
))
pubmed_mcp_client = MCPClient(lambda: stdio_client(
    StdioServerParameters(command="python", args=["application/mcp_server_pubmed.py"])
))
chembl_mcp_client = MCPClient(lambda: stdio_client(
    StdioServerParameters(command="python", args=["application/mcp_server_chembl.py"])
))
clinicaltrials_mcp_client = MCPClient(lambda: stdio_client(
    StdioServerParameters(command="python", args=["application/mcp_server_clinicaltrial.py"])
)) 
 
Define specialized sub-agents 
The planning agent looks at user questions and creates a plan for which sub-agents and tools to use: 
 
 @tool
def planning_agent(query: str) -&gt; str:
    """
    A specialized planning agent that analyzes the research query and determines
    which tools and databases should be used for the investigation.
    """
    planning_system = """
    You are a specialized planning agent for drug discovery research. Your role is to:
    
    1. Analyze research questions to identify target proteins, compounds, or biological mechanisms
    2. Determine which databases would be most relevant (Arxiv, PubMed, ChEMBL, ClinicalTrials.gov)
    3. Generate specific search queries for each relevant database
    4. Create a structured research plan
    """
    model = get_model()
    planner = Agent(
        model=model,
        system_prompt=planning_system,
    )
    response = planner(planning_prompt)
    return str(response) 
 
Similarly, the synthesis agent integrates findings from multiple sources into a single, comprehensive report: 
 
 @tool
def synthesis_agent(research_results: str) -&gt; str:
    """
    Specialized agent for synthesizing research findings into a comprehensive report.
    """
    system_prompt = """
    You are a specialized synthesis agent for drug discovery research. Your role is to:
    
    1. Integrate findings from multiple research databases
    2. Create a comprehensive, coherent scientific report
    3. Highlight key insights, connections, and opportunities
    4. Organize information in a structured format:
       - Executive Summary (300 words)
       - Target Overview
       - Research Landscape
       - Drug Development Status
       - References
    """
    model = get_model()
    synthesis = Agent(
        model=model,
        system_prompt=system_prompt,
    )
    response = synthesis(synthesis_prompt)
    return str(response) 
 
Define the orchestration agent 
We also define an orchestration agent to coordinate the entire research workflow. This agent uses the SlidingWindowConversationManager class from Strands Agents to store the last 10 messages in the conversation. See the following code: 
 
 def create_orchestrator_agent(
    history_mode,
    tavily_client=None,
    arxiv_client=None,
    pubmed_client=None,
    chembl_client=None,
    clinicaltrials_client=None,
):
    system = """
    You are an orchestrator agent for drug discovery research. Your role is to coordinate a multi-agent workflow:
    
    1. COORDINATION PHASE:
       - For simple queries: Answer directly WITHOUT using specialized tools
       - For complex research requests: Initiate the multi-agent research workflow
    
    2. PLANNING PHASE:
       - Use the planning_agent to determine which databases to search and with what queries
    
    3. EXECUTION PHASE:
       - Route specialized search tasks to the appropriate research agents
    
    4. SYNTHESIS PHASE:
       - Use the synthesis_agent to integrate findings into a comprehensive report
       - Generate a PDF report when appropriate
    """
    # Aggregate all tools from specialized agents and MCP clients
    tools = [planning_agent, synthesis_agent, generate_pdf_report, file_write]
    # Dynamically load tools from each MCP client
    if tavily_client:
        tools.extend(tavily_client.list_tools_sync())
    # ... (similar for other clients)
    conversation_manager = SlidingWindowConversationManager(
        window_size=10,  # Maintains context for the last 10 exchanges
    )
    orchestrator = Agent(
        model=model,
        system_prompt=system,
        tools=tools,
        conversation_manager=conversation_manager
    )
    return orchestrator 
 
Example use case: Explore recent breast cancer research 
To test out the new assistant, launch the chat interface by running streamlit run application/app.py and opening the local URL (typically http://localhost:8501) in your web browser. The following screenshot shows a typical conversation with the research agent. In this example, we ask the assistant, “Please generate a report for HER2 including recent news, recent research, related compounds, and ongoing clinical trials.” The assistant first develops a comprehensive research plan using the various tools at its disposal. It decides to start with a web search for recent news about HER2, as well as scientific articles on PubMed and arXiv. It also looks at HER2-related compounds in ChEMBL and ongoing clinical trials. It synthesizes these results into a single report and generates an output file of its findings, including citations. 
 
The following is an excerpt of a generated report: 
 
 Comprehensive Scientific Report: HER2 in Breast Cancer Research and Treatment
1. Executive Summary
Human epidermal growth factor receptor 2 (HER2) continues to be a critical target in breast cancer research and treatment development. This report synthesizes recent findings across the HER2 landscape highlighting significant advances in understanding HER2 biology and therapeutic approaches. The emergence of antibody-drug conjugates (ADCs) represents a paradigm shift in HER2-targeted therapy, with trastuzumab deruxtecan (T-DXd, Enhertu) demonstrating remarkable efficacy in both early and advanced disease settings. The DESTINY-Breast11 trial has shown clinically meaningful improvements in pathologic complete response rates when T-DXd is followed by standard therapy in high-risk, early-stage HER2+ breast cancer, potentially establishing a new treatment paradigm. 
 
Notably, you don’t have to define a step-by-step process to accomplish this task. By providing the assistant with a well-documented list of tools, it can decide which to use and in what order. 
Clean up 
If you followed this example on your local computer, you will not create new resources in your AWS account that you need to clean up. If you deployed the research assistant using one of those services, refer to the relevant service documentation for cleanup instructions. 
Conclusion 
In this post, we showed how Strands Agents streamlines the creation of powerful, domain-specific AI assistants. We encourage you to try this solution with your own research questions and extend it with new scientific tools. The combination of Strands Agents’s orchestration capabilities, streaming responses, and flexible configuration with the powerful language models of Amazon Bedrock creates a new paradigm for AI-assisted research. As the volume of scientific information continues to grow exponentially, frameworks like Strands Agents will become essential tools for drug discovery. 
To learn more about building intelligent agents with Strands Agents, refer to Introducing Strands Agents, an Open Source AI Agents SDK, Strands Agents SDK, and the GitHub repository. You can also find more sample agents for healthcare and life sciences built on Amazon Bedrock. 
For more information about implementing AI-powered solutions for drug discovery on AWS, visit us at AWS for Life Sciences. 
 
About the authors 
Hasun Yu&nbsp;is an AI/ML Specialist Solutions Architect with extensive expertise in designing, developing, and deploying AI/ML solutions for healthcare and life sciences. He supports the adoption of advanced AWS AI/ML services, including generative and agentic AI. 
Brian Loyal is a Principal AI/ML Solutions Architect in the Global Healthcare and Life Sciences team at Amazon Web Services. He has more than 20 years’ experience in biotechnology and machine learning and is passionate about using AI to improve human health and well-being.

⸻