‚úÖ Morning News Briefing ‚Äì August 07, 2025 10:50

üìÖ Date: 2025-08-07 10:50
üè∑Ô∏è Tags: #briefing #ai #publichealth #digitalgov

‚∏ª

üßæ Weather
‚Ä¢ No watches or warnings in effect, Pembroke
  No watches or warnings in effect. No warnings or watches or watches in effect . Watch or warnings are no longer in effect in the U.S. No watches, warnings are in effect for the rest of the day . No watches and warnings are still in effect, but no watches are in place for the day's events . The weather is not expected to be affected by the weather .
‚Ä¢ Current Conditions:  11.6¬∞C
  Temperature: 11.6&deg;C Pressure / Tendency: 102.8 kPa steady Humidity: 98 % Dewpoint:  11.3&deg:C Wind:  calm km/h . Air Quality Health Index: n/a . Pembroke 6:00 AM EDT Thursday 7 August 2025 . Weather forecast: 10,000 years from Pem
‚Ä¢ Thursday: Chance of showers. High 27. POP 70%
  Sunny this morning then a mix of sun and cloud with 70 percent chance of showers this afternoon . Risk of a thunderstorm this afternoon. Risk of thunderstorm . High 27. Humidex 33. UV index 8 or very high. Wind becoming south 20 km/h late this afternoon; UV index of very high or very low . Rainy conditions will be felt throughout the day .

üåç International News
No updates.

üçÅ Canadian News
No updates.

üá∫üá∏ U.S. Top Stories
‚Ä¢ Congress looks to ease veterans' use of health care outside the VA
  It can be difficult for veterans to use their health benefits for care outside the federal system . Opponents are wary funds being "siphoned" away from the VA . A bill in Congress could ease that, but opponents are wary of it being taken away from VA funds . The bill is expected to be voted into law by the end of the year . It could be the first time
‚Ä¢ It started with friends at home. Now Dungeons & Dragons is in its stadium era.
  In the past decade or so, actors and comedians have adopted D&amp;D as a performance medium . Podcasts and web series have expanded into stadium tours ‚Äì and fueled growing interest in the game more broadly . Dimension 20's sold-out Madison Square Garden show in January of 2025 will take place in New York City, New York, New Jersey, at 8pm on January 26
‚Ä¢ The NFL banned smelling salts. Here's why
  The NFL has banned the use of smelling salts during games, citing an FDA warning concerning the safety of the substance . Here's what experts say about the effects and the risks of using smelling salts in the NFL . The FDA has issued a warning that the substance could be dangerous to use during games . The NFL says it will not use smelling salts at all games in the U.S.
‚Ä¢ Trump cuts threaten access to birth control for millions of women
  Birth control is routine for many Americans and polls show it's popular across party lines . Now, the Trump administration is withholding funds that provide contraception for low income people . Polls show that birth control is a popular issue across party-line lines, but polls show that it's not a partisan issue . Birth control has been a problem for some in the U.S. for years .
‚Ä¢ Video shows Department of Justice official urging Jan. 6 rioters to 'kill' cops
  The Department of Justice hired a former defendant who was caught on tape urging rioters to "kill" police . The department calls him a "valued member" of the administration . The former defendant is a former Jan. 6 defendant who has been charged with inciting rioting in New York City, New Jersey, police officers to kill them . He was arrested in the wake of the Jan.

üß† Artificial Intelligence
No updates.

üíª Digital Strategy
‚Ä¢ The plan to make all networks optical is about to take two big steps forward
  'IOWN' backers think it can replace the PCI bus, reinvent servers, and rewire motherboards . Japanese tech giant NTT revealed two impressive feats of high-speed networking in December 2024 . Backers think it could replace the . PCI bus and reinvent servers . 'I OWN' backers say it can . reinvent servers and reinvent . servers, rewire . motherboards, and
‚Ä¢ Microsoft eventually realized the world isn't just the Northern Hemisphere
  Veteran engineer explains the fall of 'Fall' in Windows 10 release . Microsoft ditched its increasingly twee naming conventions for Windows 10 releases in favor of the blander H1 and H2 . Veteran engineer Raymond Chen has explained why the megacorp ditched the naming convention for the release of Windows 10 . Chen: Windows 10 has a new identity system for the next generation of software
‚Ä¢ GitHub CEO: Future devs will not code, they will manage AI
  GitHub CEO Thomas Dohmke has stated in a personal blog that the most advanced developers have "moved from writing code to architecting and verifying the implementation work that is carried out by AI agents" Meanwhile, users complain the code shack is getting slower thanks to React React . GitHub CEO: Developers have moved to "architectitecture and verifying" the implementation of AI agents in the
‚Ä¢ Amnesty slams Elon Musk's X for 'central role' in fueling 2024 UK riots
  Human rights org calls for greater accountability and stronger enforcement of Online Safety Act . Amnesty International claims Elon Musk's X platform "played a central role" in pushing the misinformation that stoked racially charged violence following last year's Southport murders . The Southport murder was the first racially charged murder in Southport, Florida, last year . Human rights group says Elon Musk played a "central role"
‚Ä¢ Real estate agents use the power of AI to command plumbing, layout to disappear
  Estate agents are known for stretching the truth in pursuit of a sale, but the generative AI boom appears to have thrown things into overdrive ‚Äì providing an easy way to present images of properties which simply don't reflect reality . "Some original features," which occasionally turn out to be asbestos. Would suit professional who wants to read after dark. Lightsockets lovingly hooked up to power supply

üè• Public Health
No updates.

üî¨ Science
‚Ä¢ Trends in fetal autopsy and placental histopathology following stillbirth, United States, 2014‚Äì2023
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Explainable illicit drug abuse prediction using hematological differences
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ A cross-national analysis of childhood predictors of physical pain
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ The real problems with America's health
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Assessing induced abortion underreporting in restrictive settings using prospective morbidity surveys from Kenya, Liberia, and Sierra Leone
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations

üßæ Government & Policy
No updates.

üèõÔ∏è Enterprise Architecture & IT Governance
No updates.

ü§ñ AI & Emerging Tech
‚Ä¢ The greenhouse gases we‚Äôre not accounting for
  In the spring of 2021, climate scientists were stumped.&nbsp;



The global economy was just emerging from the covid-19 lockdowns, but for some reason the levels of methane‚Äîa greenhouse gas emitted mainly through agriculture and fossil-fuel production‚Äîhad soared in the atmosphere the previous year, rising at the fastest rate on record.



Researchers around the world set to work unraveling the mystery, reviewing readings from satellites, aircraft, and greenhouse-gas monitoring stations. They eventually spotted a clear pattern: Methane emissions had increased sharply across the tropics, where wetlands were growing wetter and warmer.&nbsp;



That created the ideal conditions for microbes that thrive in anaerobic muck, which gobbled up more of the carbon-rich organic matter and spat out more methane as a by-product. (Reduced pollution from nitrogen oxides, which help to break down methane in the atmosphere, also likely played a substantial role.)



The findings offer one of the clearest cases so far where climate change itself is driving additional greenhouse-gas emissions from natural systems, triggering a feedback effect that threatens to produce more warming, more emissions, and on and on.&nbsp;



There are numerous additional ways this is happening or soon could, including wildfires and thawing permafrost. These are major emissions sources that aren‚Äôt included in the commitments nations have made under the Paris climate agreement‚Äîand climate risks that largely aren‚Äôt accounted for in the UN Intergovernmental Panel on Climate Change‚Äôs most recent warming scenarios.



Spark Climate Solutions (not to be confused with this newsletter) hopes to change that.The San Francisco nonprofit is launching what‚Äôs known as a model intercomparison project, in which different research teams run the same set of experiments on different models across a variety of emissions scenarios to determine how climate change could play out. This one would specifically explore how a range of climate feedback effects could propel additional warming, additional emissions, and additional types of feedback.



‚ÄúThese increased emissions from natural sources add to human emissions and amplify climate change,‚Äù says Phil Duffy, chief scientist at Spark Climate Solutions, who previously served as climate science advisor to President Joe Biden. ‚ÄúAnd if you don‚Äôt look at all of them together, you can‚Äôt quantify the strength of that feedback effect.‚Äù



Other participants in the effort will include scientists at the Environmental Defense Fund, Stanford University, the Woodwell Climate Research Center, and other institutions in Europe and Australia, according to Spark Climate Solutions.



The nonprofit hopes to publish the findings in time for them to be incorporated into the UN climate panel‚Äôs seventh major assessment report, which is just getting underway, to help ensure that these dangers are more fully represented. That, in turn, would give nations a more accurate sense of the world‚Äôs carbon budgets, or the quantity of greenhouse gases they can produce before the planet reaches temperatures 1.5 ¬∞C or&nbsp; 2 ¬∞C over preindustrial levels.&nbsp;



But one thing is already clear: Since the current scenarios don‚Äôt fully account for these feedback effects, the world will almost certainly warm faster than is now forecast, which underscores the importance of carrying out this exercise.&nbsp;



Scientists at EDF, Woodwell and other institutions found that fires in the world‚Äôs northernmost forests, thawing permafrost and warming tropical wetlands could together push the planet beyond 2 ¬∞C years faster, eliminating up to a quarter of the time left before the world passes the core goal of the Paris agreement, in a paper under review.&nbsp;



Earlier this year, Spark Climate Solutions set up a broader program to advance research and awareness of what‚Äôs known as warming-induced emissions, which will launch additional collaborations similar to the modeling intercomparison project.&nbsp;&nbsp;



The goal of the program and the research project is ‚Äúto really mainstream the inclusion of this topic in climate science and climate policy, and to drive research around climate solutions,‚Äù says Ben Poulter, who leads the program at Spark Climate Solutions and was previously a scientist at the NASA Goddard Space Flight Center.



Spark notes that warming temperatures could also release more carbon dioxide from the oceans, in a process known as outgassing; additional carbon dioxide and nitrous oxide, a potent greenhouse gas that also depletes the protective ozone layer, from farmland; more carbon dioxide and methane from wildfires; and still more of all three of these gases as permafrost thaws.



The ground remains frozen year round across a vast expanse of the Northern Hemisphere, creating a frosty underground storehouse from Alaska to Siberia that‚Äôs packed with twice as much carbon as the atmosphere.



But as it thaws, it starts to decompose and release greenhouse gases, says Susan Natali, an Arctic climate scientist focused on permafrost at Woodwell. A study published in Nature in January noted that 30% of the world‚Äôs Arctic‚ÄìBoreal Zone has already flipped from a carbon sink to a carbon source, when wildfires, thawing permafrost and other factors are taken into account.



Despite these increasing risks, only a minority of the models that fed into the UN climate panel‚Äôs last major report incorporated the feedback effects of thawing permafrost. And the emissions risks still weren‚Äôt fully accounted for because these ecosystems are difficult to monitor and model, Natali says.Among the complexities: Wildfires, which are themselves hard to predict, can accelerate thawing. It‚Äôs also hard to foresee which regions will grow drier or wetter, which determines whether they release mostly methane or carbon dioxide‚Äîand those gases have very different warming effects over different time periods. There are counterbalancing effects that must be taken into account as well‚Äîfor instance, as carbon-absorbing plants replace ice and snow in certain areas.



Natali says improving our understanding of these complex feedback effects is essential to understanding the dangers we face.



‚ÄúIt‚Äôs going to mean additional costs to human health, human life,‚Äù she says. ‚ÄúWe want people to be safe‚Äîand it‚Äôs very hard to do that if you don‚Äôt know what‚Äôs coming and you‚Äôre not prepared for it.‚Äù



This article is from The Spark,¬†MIT Technology Review‚Äôs weekly climate newsletter. To receive it in your inbox every Wednesday,¬†sign up here.
‚Ä¢ Five ways that AI is learning to improve itself
  Last week, Mark Zuckerberg declared that Meta is aiming to achieve smarter-than-human AI. He seems to have a recipe for achieving that goal, and the first ingredient is human talent: Zuckerberg has reportedly tried to lure top researchers to Meta Superintelligence Labs with nine-figure offers. The second ingredient is AI itself.&nbsp; Zuckerberg recently said on an earnings call that Meta Superintelligence Labs will be focused on building self-improving AI‚Äîsystems that can bootstrap themselves to higher and higher levels of performance.



The possibility of self-improvement distinguishes AI from other revolutionary technologies. CRISPR can‚Äôt improve its own targeting of DNA sequences, and fusion reactors can‚Äôt figure out how to make the technology commercially viable. But LLMs can optimize the computer chips they run on, train other LLMs cheaply and efficiently, and perhaps even come up with original ideas for AI research. And they‚Äôve already made some progress in all these domains.



According to Zuckerberg, AI self-improvement could bring about a world in which humans are liberated from workaday drudgery and can pursue their highest goals with the support of brilliant, hypereffective artificial companions. But self-improvement also creates a fundamental risk, according to Chris Painter, the policy director at the AI research nonprofit METR. If AI accelerates the development of its own capabilities, he says, it could rapidly get better at hacking, designing weapons, and manipulating people. Some researchers even speculate that this positive feedback cycle could lead to an ‚Äúintelligence explosion,‚Äù in which AI rapidly launches itself far beyond the level of human capabilities.



But you don‚Äôt have to be a doomer to take the implications of self-improving AI seriously. OpenAI, Anthropic, and Google all include references to automated AI research in their AI safety frameworks, alongside more familiar risk categories such as chemical weapons and cybersecurity. ‚ÄúI think this is the fastest path to powerful AI,‚Äù says Jeff Clune, a professor of computer science at the University of British Columbia and senior research advisor at Google DeepMind. ‚ÄúIt‚Äôs probably the most important thing we should be thinking about.‚Äù





By the same token, Clune says, automating AI research and development could have enormous upsides. On our own, we humans might not be able to think up the innovations and improvements that will allow AI to one day tackle prodigious problems like cancer and climate change.



For now, human ingenuity is still the primary engine of AI advancement; otherwise, Meta would hardly have made such exorbitant offers to attract researchers to its superintelligence lab. But AI is already contributing to its own development, and it‚Äôs set to take even more of a role in the years to come. Here are five ways that AI is making itself better.



1. Enhancing productivity



Today, the most important contribution that LLMs make to AI development may also be the most banal. ‚ÄúThe biggest thing is coding assistance,‚Äù says Tom Davidson, a senior research fellow at Forethought, an AI research nonprofit. Tools that help engineers write software more quickly, such as Claude Code and Cursor, appear popular across the AI industry: Google CEO Sundar Pichai claimed in October 2024 that a quarter of the company‚Äôs new code was generated by AI, and Anthropic recently documented a wide variety of ways that its employees use Claude Code. If engineers are more productive because of this coding assistance, they will be able to design, test, and deploy new AI systems more quickly.



But the productivity advantage that these tools confer remains uncertain: If engineers are spending large amounts of time correcting errors made by AI systems, they might not be getting any more work done, even if they are spending less of their time writing code manually. A recent study from METR found that developers take about 20% longer to complete tasks when using AI coding assistants, though Nate Rush, a member of METR‚Äôs technical staff who co-led the study, notes that it only examined extremely experienced developers working on large code bases. Its conclusions might not apply to AI researchers who write up quick scripts to run experiments.



Conducting a similar study within the frontier labs could help provide a much clearer picture of whether coding assistants are making AI researchers at the cutting edge more productive, Rush says‚Äîbut that work hasn‚Äôt yet been undertaken. In the meantime, just taking software engineers‚Äô word for it isn‚Äôt enough: The developers METR studied thought that the AI coding tools had made them work more efficiently, even though the tools had actually slowed them down substantially.



2. Optimizing infrastructure



Writing code quickly isn‚Äôt that much of an advantage if you have to wait hours, days, or weeks for it to run. LLM training, in particular, is an agonizingly slow process, and the most sophisticated reasoning models can take many minutes to generate a single response. These delays are major bottlenecks for AI development, says Azalia Mirhoseini, an assistant professor of computer science at Stanford University and senior staff scientist at Google DeepMind. ‚ÄúIf we can run AI faster, we can innovate more,‚Äù she says.



That‚Äôs why Mirhoseini has been using AI to optimize AI chips. Back in 2021, she and her collaborators at Google built a non-LLM AI system that could decide where to place various components on a computer chip to optimize efficiency. Although some other researchers failed to replicate the study‚Äôs results, Mirhoseini says that Nature investigated the paper and upheld the work‚Äôs validity‚Äîand she notes that Google has used the system‚Äôs designs for multiple generations of its custom AI chips.



More recently, Mirhoseini has applied LLMs to the problem of writing kernels, low-level functions that control how various operations, like matrix multiplication, are carried out in chips. She‚Äôs found that even general-purpose LLMs can, in some cases, write kernels that run faster than the human-designed versions.



Elsewhere at Google, scientists built a system that they used to optimize various parts of the company‚Äôs LLM infrastructure. The system, called AlphaEvolve, prompts Google‚Äôs Gemini LLM to write algorithms for solving some problem, evaluates those algorithms, and asks Gemini to improve on the most successful‚Äîand repeats that process several times. AlphaEvolve designed a new approach for running datacenters that saved 0.7% of Google‚Äôs computational resources, made further improvements to Google‚Äôs custom chip design, and designed a new kernel that sped up Gemini‚Äôs training by 1%.&nbsp;&nbsp;&nbsp;



That might sound like a small improvement, but at a huge company like Google it equates to enormous savings of time, money, and energy. And Matej Balog, a staff research scientist at Google DeepMind who led the AlphaEvolve project, says that he and his team tested the system on only a small component of Gemini‚Äôs overall training pipeline. Applying it more broadly, he says, could lead to more savings.



3. Automating training



LLMs are famously data hungry, and training them is costly at every stage. In some specific domains‚Äîunusual programming languages, for example‚Äîreal-world data is too scarce to train LLMs effectively. Reinforcement learning with human feedback, a technique in which humans score LLM responses to prompts and the LLMs are then trained using those scores, has been key to creating models that behave in line with human standards and preferences, but obtaining human feedback is slow and expensive.&nbsp;



Increasingly, LLMs are being used to fill in the gaps. If prompted with plenty of examples, LLMs can generate plausible synthetic data in domains in which they haven‚Äôt been trained, and that synthetic data can then be used for training. LLMs can also be used effectively for reinforcement learning: In an approach called ‚ÄúLLM as a judge,‚Äù LLMs, rather than humans, are used to score the outputs of models that are being trained. That approach is key to the influential ‚ÄúConstitutional AI‚Äù framework proposed by Anthropic researchers in 2022, in which one LLM is trained to be less harmful based on feedback from another LLM.



Data scarcity is a particularly acute problem for AI agents. Effective agents need to be able to carry out multistep plans to accomplish particular tasks, but examples of successful step-by-step task completion are scarce online, and using humans to generate new examples would be pricey. To overcome this limitation, Stanford‚Äôs Mirhoseini and her colleagues have recently piloted a technique in which an LLM agent generates a possible step-by-step approach to a given problem, an LLM judge evaluates whether each step is valid, and then a new LLM agent is trained on those steps. ‚ÄúYou‚Äôre not limited by data anymore, because the model can just arbitrarily generate more and more experiences,‚Äù Mirhoseini says.



4. Perfecting agent design



One area where LLMs haven‚Äôt yet made major contributions is in the design of LLMs themselves. Today‚Äôs LLMs are all based on a neural-network structure called a transformer, which was proposed by human researchers in 2017, and the notable improvements that have since been made to the architecture were also human-designed.&nbsp;



But the rise of LLM agents has created an entirely new design universe to explore. Agents need tools to interact with the outside world and instructions for how to use them, and optimizing those tools and instructions is essential to producing effective agents. ‚ÄúHumans haven‚Äôt spent as much time mapping out all these ideas, so there‚Äôs a lot more low-hanging fruit,‚Äù Clune says. ‚ÄúIt‚Äôs easier to just create an AI system to go pick it.‚Äù



Together with researchers at the startup Sakana AI, Clune created a system called a ‚ÄúDarwin G√∂del Machine‚Äù: an LLM agent that can iteratively modify its prompts, tools, and other aspects of its code to improve its own task performance. Not only did the Darwin G√∂del Machine achieve higher task scores through modifying itself, but as it evolved, it also managed to find new modifications that its original version wouldn‚Äôt have been able to discover. It had entered a true self-improvement loop.



5. Advancing research



Although LLMs are speeding up numerous parts of the LLM development pipeline, humans may still remain essential to AI research for quite a while. Many experts point to ‚Äúresearch taste,‚Äù or the ability that the best scientists have to pick out promising new research questions and directions, as both a particular challenge for AI and a key ingredient in AI development.&nbsp;



But Clune says research taste might not be as much of a challenge for AI as some researchers think. He and Sakana AI researchers are working on an end-to-end system for AI research that they call the ‚ÄúAI Scientist.‚Äù It searches through the scientific literature to determine its own research question, runs experiments to answer that question, and then writes up its results.



One paper that it wrote earlier this year, in which it devised and tested a new training strategy aimed at making neural networks better at combining examples from their training data, was anonymously submitted to a workshop at the International Conference on Machine Learning, or ICML‚Äîone of the most prestigious conferences in the field‚Äîwith the consent of the workshop organizers. The training strategy didn‚Äôt end up working, but the paper was scored highly enough by reviewers to qualify it for acceptance (it is worth noting that ICML workshops have lower standards for acceptance than the main conference). In another instance, Clune says, the AI Scientist came up with a research idea that was later independently proposed by a human researcher on X, where it attracted plenty of interest from other scientists.



‚ÄúWe are looking right now at the GPT-1 moment of the AI Scientist,‚Äù Clune says. ‚ÄúIn a few short years, it is going to be writing papers that will be accepted at the top peer-reviewed conferences and journals in the world. It will be making novel scientific discoveries.‚Äù



Is superintelligence on its way?



With all this enthusiasm for AI self-improvement, it seems likely that in the coming months and years, the contributions AI makes to its own development will only multiply. To hear Mark Zuckerberg tell it, this could mean that superintelligent models, which exceed human capabilities in many domains, are just around the corner. In reality, though, the impact of self-improving AI is far from certain.



It‚Äôs notable that AlphaEvolve has sped up the training of its own core LLM system, Gemini‚Äîbut that 1% speedup may not observably change the pace of Google‚Äôs AI advancements. ‚ÄúThis is still a feedback loop that‚Äôs very slow,‚Äù says Balog, the AlphaEvolve researcher. ‚ÄúThe training of Gemini takes a significant amount of time. So you can maybe see the exciting beginnings of this virtuous [cycle], but it‚Äôs still a very slow process.‚Äù



If each subsequent version of Gemini speeds up its own training by an additional 1%, those accelerations will compound. And because each successive generation will be more capable than the previous one, it should be able to achieve even greater training speedups‚Äînot to mention all the other ways it might devise to improve itself. Under such circumstances, proponents of superintelligence argue, an eventual intelligence explosion looks inevitable.



This conclusion, however, ignores a key observation: Innovation gets harder over time. In the early days of any scientific field, discoveries come fast and easy. There are plenty of obvious experiments to run and ideas to investigate, and none of them have been tried before. But as the science of deep learning matures, finding each additional improvement might require substantially more effort on the part of both humans and their AI collaborators. It‚Äôs possible that by the time AI systems attain human-level research abilities, humans or less-intelligent AI systems will already have plucked all the low-hanging fruit.



Determining the real-world impact of AI self-improvement, then, is a mighty challenge. To make matters worse, the AI systems that matter most for AI development‚Äîthose being used inside frontier AI companies‚Äîare likely more advanced than those that have been released to the general public, so measuring o3‚Äôs capabilities might not be a great way to infer what‚Äôs happening inside OpenAI.



But external researchers are doing their best‚Äîby, for example, tracking the overall pace of AI development to determine whether or not that pace is accelerating. METR is monitoring advancements in AI abilities by measuring how long it takes humans to do tasks that cutting-edge systems can complete themselves. They‚Äôve found that the length of tasks that AI systems can complete independently has, since the release of GPT-2 in 2019, doubled every seven months.&nbsp;



Since 2024, that doubling time has shortened to four months, which suggests that AI progress is indeed accelerating. There may be unglamorous reasons for that: Frontier AI labs are flush with investor cash, which they can spend on hiring new researchers and purchasing new hardware. But it‚Äôs entirely plausible that AI self-improvement could also be playing a role.



That‚Äôs just one indirect piece of evidence. But Davidson, the Forethought researcher, says there‚Äôs good reason to expect that AI will supercharge its own advancement, at least for a time. METR‚Äôs work suggests that the low-hanging-fruit effect isn‚Äôt slowing down human researchers today, or at least that increased investment is effectively counterbalancing any slowdown. If AI notably increases the productivity of those researchers, or even takes on some fraction of the research work itself, that balance will shift in favor of research acceleration.



‚ÄúYou would, I think, strongly expect that there‚Äôll be a period when AI progress speeds up,‚Äù Davidson says. ‚ÄúThe big question is how long it goes on for.‚Äù
‚Ä¢ The Download: OpenAI‚Äôs open-weight models, and the future of internet search
  This is today&#8217;s edition of¬†The Download,¬†our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



OpenAI has finally released open-weight language models



The news: OpenAI has finally released its first open-weight large language models since 2019‚Äôs GPT-2. Unlike the models available through OpenAI‚Äôs web interface, these new open models can be freely downloaded, run, and even modified on laptops and other local devices.



Why it matters: These releases re-establish OpenAI as a presence for users of open models. That‚Äôs particularly notable at a time when Meta, which had previously dominated the American open-model landscape with its Llama models, may be reorienting toward closed releases‚Äîand when Chinese open models are becoming more popular than their American competitors. Read the full story.&nbsp;



‚ÄîGrace Huckins







MIT Technology Review Narrated: AI means the end of internet search as we‚Äôve known it



The biggest change to the way search engines deliver information to us since the 1990s is happening right now. No more keyword searching. Instead, you can ask questions in natural language. And instead of links, you‚Äôll increasingly be met with answers written by generative AI and based on live information from across the internet, delivered the same way.Not everyone is excited for the change. Publishers are completely freaked out. And people are also worried about what these new LLM-powered results will mean for our fundamental shared reality.



This is our latest story to be turned into a MIT Technology Review Narrated podcast, which we publish each week on Spotify and Apple Podcasts. Just navigate to MIT Technology Review Narrated on either platform, and follow us to get all our new content as it‚Äôs released.







The must-reads



I‚Äôve combed the internet to find you today‚Äôs most fun/important/scary/fascinating stories about technology.



1 Nvidia insists its AI chips don‚Äôt have a ‚Äúkill switch‚ÄùAfter China‚Äôs Cyberspace Administration asked for security documentation. (CNBC)+ The country‚Äôs ambitions to consolidate its chip giants aren&#8217;t going to plan. (FT $)+ Two Chinese nationals have been charged with illegally shipping chips. (Reuters)



2 America‚Äôs new data centers are driving colossal electricity demandAnd a handful of equipment makers are reaping the benefits. (FT $)+ We did the math on AI‚Äôs energy footprint. Here‚Äôs the story you haven‚Äôt heard. (MIT Technology Review)



3 RFK Jr has cancelled close to $500 million in mRNA vaccine contracts¬†Which could leave us dangerously underprepared for a future pandemic. (Politico)+ We‚Äôre losing a key insight into global health. (Vox)+ How measuring vaccine hesitancy could help health professionals tackle it. (MIT Technology Review)



4 Uber has a sexual assault problemNewly-unveiled records show it gathered far more sexual assault and misconduct reports than previously revealed. (NYT $)



5 A British politician created an AI clone of himselfAnd although it provoked a backlash, other MPs may follow his lead. (WP $)+ A former CNN journalist has interviewed an AI version of a mass-shooting victim. (The Guardian)



6 xAI‚Äôs new Grok Imagine tool has a ‚Äúspicy‚Äù modeWhich seems to be code for non-consensual porn images. (The Verge)¬†¬†+ It‚Äôs already generated fake Taylor Swift nudes without being asked. (Ars Technica)



7 How does ChatGPT fare as a couple‚Äôs counselor?It gets some stuff right. But it also gets some things really wrong. (NPR)+ The AI relationship revolution is already here. (MIT Technology Review)



8 Syria‚Äôs refugees are returning to rebuild its tech industryBut sectarian violence and poor connectivity mean it‚Äôs an uphill battle. (Rest of World)



9 Sales of Ozempic have droppedRival Mounjaro seems to be more effective. (The Guardian)+ We‚Äôre learning more about what weight-loss drugs do to the body. (MIT Technology Review)



10 Google Calendar rules college kids‚Äô livesThey schedule everything from assignments to parties and hook ups. (WSJ $)







Quote of the day



‚ÄúThis is a bad day for science.‚Äù



‚ÄîScott Hensley, an immunologist at the University of Pennsylvania, criticizes the Department of Health and Human Services‚Äô decision to cancel hundreds of millions of dollars in funding for mRNA vaccine projects, the New York Times reports.







One more thing







Future space food could be made from astronaut breathThe future of space food could be as simple‚Äîand weird‚Äîas a protein shake made with astronaut breath or a burger made from fungus.For decades, astronauts have relied mostly on pre-packaged food during their forays off our planet. With missions beyond Earth orbit in sight, a NASA-led competition is hoping to change all that and usher in a new era of sustainable space food.To solve the problem of feeding astronauts on long-duration missions, NASA asked companies to propose novel ways to develop sustainable foods for future missions. Around 200 rose to the challenge‚Äîcreating nutritious (and outlandish) culinary creations in the process. Read the full story.&nbsp;



‚ÄîJonathan O&#8217;Callaghan







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)



+ There are a lot of funny cat videos out there but honestly, this is top-drawer.+ Check out this adorable website where people share what they see in clouds.+ Babe you‚Äôre glowing! No seriously, you literally are.¬†+ I loved watching this woman from London‚Äôs East End wax lyrical about the dawn of TV.
‚Ä¢ OpenAI has finally released open-weight language models
  OpenAI has finally released its first open-weight large language models since 2019‚Äôs GPT-2. These new ‚Äúgpt-oss‚Äù models are available in two different sizes and score similarly to the company‚Äôs o3-mini and o4-mini models on several benchmarks. Unlike the models available through OpenAI‚Äôs web interface, these new open models can be freely downloaded, run, and even modified on laptops and other local devices.



In the company‚Äôs many years without an open LLM release, some users have taken to referring to it with the pejorative ‚ÄúClosedAI.‚Äù That sense of frustration had escalated in the past few months as these long-awaited models were delayed twice‚Äîfirst in June and then in July. With their release, however, OpenAI is reestablishing itself as a presence for users of open models.



That‚Äôs particularly notable at a time when Meta, which had previously dominated the American open-model landscape with its Llama models, may be reorienting toward closed releases‚Äîand when Chinese open models, such as DeepSeek‚Äôs offerings, Kimi K2, and Alibaba‚Äôs Qwen series, are becoming more popular than their American competitors.





‚ÄúThe vast majority of our [enterprise and startup] customers are already using a lot of open models,‚Äù said Casey Dvorak, a research program manager at OpenAI, in a media briefing about the model release. ‚ÄúBecause there is no [competitive] open model from OpenAI, we wanted to plug that gap and actually allow them to use our technology across the board.‚Äù



The new models come in two different sizes, the smaller of which can theoretically run on 16 GB of RAM‚Äîthe minimum amount that Apple currently offers on its computers. The larger model requires a high-end laptop or specialized hardware.



Open models have a few key use cases. Some organizations may want to customize models for their own purposes or save money by running models on their own equipment, though that equipment comes at a substantial upfront cost. Others‚Äîsuch hospitals, law firms, and governments‚Äîmight need models that they can run locally for data security reasons.&nbsp;



OpenAI has facilitated such activity by releasing its open models under a permissive Apache 2.0 license, which allows the models to be used for commercial purposes. Nathan Lambert, post-training lead at the Allen Institute for AI, says that this choice is commendable: Such licenses are typical for Chinese open-model releases, but Meta released its Llama models under a bespoke, more restrictive license. ‚ÄúIt‚Äôs a very good thing for the open community,‚Äù he says.



Researchers who study how LLMs work also need open models, so that they can examine and manipulate those models in detail. ‚ÄúIn part, this is about reasserting OpenAI‚Äôs dominance in the research ecosystem,‚Äù says Peter Henderson, an assistant professor at Princeton University who has worked extensively with open models. If researchers do adopt gpt-oss as new workhorses, OpenAI could see some concrete benefits, Henderson says‚Äîit might adopt innovations discovered by other researchers into its own model ecosystem.



More broadly, Lambert says, releasing an open model now could help OpenAI reestablish its status in an increasingly crowded AI environment. ‚ÄúIt kind of goes back to years ago, where they were seen as the AI company,‚Äù he says. Users who want to use open models will now have the option to meet all their needs with OpenAI products, rather than turning to Meta‚Äôs Llama or Alibaba‚Äôs Qwen when they need to run something locally.



The rise of Chinese open models like Qwen over the past year may have been a particularly salient factor in OpenAI‚Äôs calculus. An employee from OpenAI emphasized at the media briefing that the company doesn‚Äôt see these open models as a response to actions taken by any other AI company, but OpenAI is clearly attuned to the geopolitical implications of China‚Äôs open-model dominance. ‚ÄúBroad access to these capable‚Ä¨‚Ä≠ open-weights models created in the US helps expand democratic AI rails,‚Äù the company wrote in a blog post announcing the models‚Äô release.&nbsp;



Since DeepSeek exploded onto the AI scene at the start of 2025, observers have noted that Chinese models often refuse to speak about topics that the Chinese Communist Party has deemed verboten, such as Tiananmen Square. Such observations‚Äîas well as longer-term risks, like the possibility that agentic models could purposefully write vulnerable code‚Äîhave made some AI experts concerned about the growing adoption of Chinese models. ‚ÄúOpen models are a form of soft power,‚Äù Henderson says.



Lambert released a report on Monday documenting how Chinese models are overtaking American offerings like Llama and advocating for a renewed commitment to domestic open models. Several prominent AI researchers and entrepreneurs, such as HuggingFace CEO Clement Delangue, Stanford‚Äôs Percy Liang, and former OpenAI researcher Miles Brundage, have signed on.



The Trump administration, too, has emphasized development of open models in its AI Action Plan. With both this model release and previous statements, OpenAI is aligning itself with that stance. ‚ÄúIn their filings about the action plan, [OpenAI] pretty clearly indicated that they see US‚ÄìChina as a key issue and want to position themselves as very important to the US system,‚Äù says Rishi Bommasani, a senior research scholar at the Stanford Institute for Human-Centered Artificial Intelligence.&nbsp;



And OpenAI may see concrete political advantages from aligning with the administration‚Äôs AI priorities, Lambert says. As the company continues to build out its extensive computational infrastructure, it will need political support and approvals, and sympathetic leadership could go a long way.
‚Ä¢ The Download: AI agent infrastructure, and OpenAI‚Äôs ambitions
  This is today&#8217;s edition of&nbsp;The Download,&nbsp;our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



These protocols will help AI agents navigate our messy lives



A growing number of companies are launching AI agents that can do things on your behalf‚Äîactions like sending an email, making a document, or editing a database. Initial reviews for these agents have been mixed at best, though, because they struggle to interact with all the different components of our digital lives.



Anthropic and Google are among the companies and groups working to fix that. Over the past year, they have both introduced protocols that try to define how AI agents should interact with each other and the world around them. If they work as planned, they could give us a crucial part of the infrastructure we need for agents to be useful.&nbsp;Read our&nbsp;story to learn more.&nbsp;



‚ÄîPeter Hall







A glimpse into OpenAI‚Äôs largest ambitions



‚ÄîJames O‚ÄôDonnell



OpenAI has given itself a dual mandate: on the one hand, it‚Äôs a tech giant rooted in products, including of course ChatGPT, which people around the world reportedly send 2.5 billion messages to each day. But its original mission is as a research lab that will not only create ‚Äúartificial general intelligence‚Äù but ensure that it benefits all of humanity.&nbsp;



My colleague Will Douglas Heaven recently sat down for an exclusive conversation with the two figures at OpenAI most responsible for the latter ambitions. The&nbsp;whole story is worth reading&nbsp;for all it reveals‚Äîabout how OpenAI thinks about the safety of its products, what AGI actually means, and more‚Äîbut here‚Äôs one thing that stood out to me.



This story is from The Algorithm, our weekly newsletter all about the latest goings-on in AI.&nbsp;Sign up&nbsp;to receive it in your inbox every Monday.







The must-reads



I‚Äôve combed the internet to find you today‚Äôs most fun/important/scary/fascinating stories about technology.



1 OpenAI is adding mental health guardrails to ChatGPTIt‚Äôs set to give less direct advice, and encourage users to take breaks from lengthy chats. (NBC)+&nbsp;What happens when doctors fail to spot AI‚Äôs mistakes?&nbsp;(The Verge)+&nbsp;OpenAI has released its first research into how using ChatGPT affects people‚Äôs emotional well-being. (MIT Technology Review)2 The US wants to build a nuclear reactor on the moonAnd it hopes to do that before Russia and China, who are planning to do exactly the same. (Politico)+&nbsp;NASA‚Äôs latest mission to the moon just failed.&nbsp;(Engadget)+&nbsp;Nokia is putting the first cellular network on the moon. (MIT Technology Review)3 How to live forever (or at least get rich trying)&nbsp;Love them or hate them, the people behind the explosion in longevity research are a fascinating bunch. (New Yorker $)+&nbsp;Longevity clinics around the world are selling unproven treatments.&nbsp;(MIT Technology Review)4 Welcome to Silicon Valley‚Äôs ‚Äòhard tech‚Äô eraGoodbye, consumer software. Hello, massive military contracts. (NYT&nbsp;$)+&nbsp;Phase two of military AI has arrived.&nbsp;(MIT Technology Review)5 There‚Äôs a big problem with the Gulf‚Äôs trillion-dollar AI dreamBuilding data centers in a region that already has water scarcity issues seems&#8230;unwise. (Rest of Water)+&nbsp;There‚Äôs a data center boom in the US desert too.&nbsp;(MIT Technology Review)+&nbsp;Google has promised to scale back its energy usage during certain times to reduce stress on the grid.&nbsp;(Quartz&nbsp;$)6 Tesla‚Äôs board awarded about $30 billion of shares to Elon Musk‚ÄúRetaining Elon is more important than ever before,‚Äù they wrote in a letter to shareholders yesterday. (FT&nbsp;$)+&nbsp;Tech CEOs pay packets are reaching stratospheric new records.&nbsp;(WSJ&nbsp;$)7 What happens if you respond to those scam job texts?You get exploited, obviously‚Äîbut you‚Äôd be surprised just how weird it can get along the way. (Slate)



8 Why there‚Äôs so much uproar over Vogue‚Äôs AI-generated adIt‚Äôs the latest flashpoint in the war over when AI should (and shouldn‚Äôt) be used. (TechCrunch)



9 Earth‚Äôs core seems to be up and leaking out of Earth‚Äôs surface&nbsp;It‚Äôs a finding that‚Äôs forcing geoscientists to rethink some long-held assumptions. (Quanta&nbsp;$)+&nbsp;How a volcanic eruption turned a human brain into glass.&nbsp;(MIT Technology Review)10 Could lasers help us see inside people‚Äôs heads?It seems possible, but big hurdles remain to this new method being adopted in clinical settings. (IEEE Spectrum)



Quote of the day



¬†&#8220;Hate it! Don&#8217;t want anything to do with it.&#8221;



‚ÄîWeezy Simes, a 27-year-old florist, sums up her feelings about AI to¬†Business Insider.







One more thing



ANDREA D&#8217;AQUINO




What happened to the microfinance organization Kiva?



Since it was founded in 2005, the San Francisco-based nonprofit Kiva has helped everyday people make microloans to borrowers around the world. It connects lenders in richer communities to fund all sorts of entrepreneurs, from bakers in Mexico to farmers in Albania. Its overarching aim is helping poor people help themselves.



But back in August 2021, Kiva lenders started to notice that information that felt essential in deciding who to lend to was suddenly harder to find. Now, lenders are worried that the organization now seems more focused on how to make money than how to create change.&nbsp;Read the full story.



‚ÄîMara Kardas-Nelson







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)



+ I want¬†this guy¬†to draw my portrait.¬†+ Highly recommend making¬†these¬†lemongrass chicken lettuce wraps. So tasty and easy!+ This¬†encyclopedia¬†teaches you about ancient gods and forgotten deities from around the world.+ Some of the¬†architecture in Iran¬†looks breathtakingly beautiful.

üîí Cybersecurity & Privacy
‚Ä¢ Who Got Arrested in the Raid on the XSS Crime Forum?
  On July 22, 2025, the European police agency Europol said a long-running investigation led by the French Police resulted in the arrest of a 38-year-old administrator of XSS,¬†a Russian-language cybercrime forum with more than 50,000 members. The action has triggered an ongoing frenzy of speculation and panic among XSS denizens about the identity of the unnamed suspect, but the consensus is that he is a pivotal figure in the crime forum scene who goes by the hacker handle &#8220;Toha.&#8221; Here&#8217;s a deep dive on what&#8217;s knowable about Toha, and a short stab at who got nabbed.
An unnamed 38-year-old man was arrested in Kiev last month on suspicion of administering the cybercrime forum XSS. Image: ssu.gov.ua.
Europol did not name the accused, but published partially obscured photos of him from the raid on his residence in Kiev. The police agency said the suspect acted as a trusted third party &#8212; arbitrating disputes between criminals &#8212; and guaranteeing the security of transactions on XSS. A statement from Ukraine&#8217;s SBU security service said XSS counted among its members many cybercriminals from various ransomware groups, including REvil, LockBit, Conti, and Qiliin.
Since the Europol announcement, the XSS forum resurfaced at a new address on the deep web (reachable only via the anonymity network Tor). But from reviewing the recent posts, there appears to be little consensus among longtime members about the identity of the now-detained XSS administrator.
The most frequent comment regarding the arrest was a message of solidarity and support for Toha, the handle chosen by the longtime administrator of XSS and several other major Russian forums. Toha&#8217;s accounts on other forums have been silent since the raid.
Europol said the suspect has enjoyed a nearly 20-year career in cybercrime, which roughly lines up with Toha&#8217;s history. In 2005, Toha was a founding member of the Russian-speaking forum Hack-All. That is, until it got massively hacked a few months after its debut. In 2006, Toha rebranded the forum to exploit[.]in, which would go on to draw tens of thousands of members, including an eventual Who&#8217;s-Who of wanted cybercriminals.
Toha announced in 2018 that he was selling the Exploit forum, prompting rampant speculation on the forums that the buyer was secretly a Russian or Ukrainian government entity or front person. However, those suspicions were unsupported by evidence, and Toha vehemently denied the forum had been given over to authorities.
One of the oldest Russian-language cybercrime forums was DaMaGeLaB, which operated from 2004 to 2017, when its administrator &#8220;Ar3s&#8221; was arrested. In 2018, a partial backup of the DaMaGeLaB forum was reincarnated as xss[.]is, with Toha as its stated administrator.
CROSS-SITE GRIFTING
Clues about Toha&#8217;s early presence on the Internet &#8212; from ~2004 to 2010 &#8212; are available in the archives of Intel 471, a cyber intelligence firm that tracks forum activity. Intel 471 shows Toha used the same email address across multiple forum accounts, including at Exploit, Antichat, Carder[.]su and inattack[.]ru.
DomainTools.com finds Toha&#8217;s email address &#8212; toschka2003@yandex.ru &#8212; was used to register at least a dozen domain names &#8212; most of them from the mid- to late 2000s. Apart from exploit[.]in and a domain called ixyq[.]com, the other domains registered to that email address end in .ua, the top-level domain for Ukraine (e.g. deleted.org[.]ua, lj.com[.]ua, and blogspot.org[.]ua).
A 2008 snapshot of a domain registered to toschka2003@yandex.ru and to Anton Medvedovsky in Kiev. Note the message at the bottom left, &#8220;Protected by Exploit,in.&#8221; Image: archive.org.
Nearly all of the domains registered to toschka2003@yandex.ru contain the name Anton Medvedovskiy in the registration records, except for the aforementioned ixyq[.]com, which is registered to the name Yuriy Avdeev in Moscow.
This Avdeev surname came up in a lengthy conversation with Lockbitsupp, the leader of the rapacious and destructive ransomware affiliate group Lockbit. The conversation took place in February 2024, when Lockbitsupp asked for help identifying Toha&#8217;s real-life identity.
In early 2024, the leader of the Lockbit ransomware group &#8212; Lockbitsupp &#8212; asked for help investigating the identity of the XSS administrator Toha, which he claimed was a Russian man named Anton Avdeev.
Lockbitsupp didn&#8217;t share why he wanted Toha&#8217;s details, but he maintained that Toha&#8217;s real name was Anton Avdeev. I declined to help Lockbitsupp in whatever revenge he was planning on Toha, but his question made me curious to look deeper.
It appears Lockbitsupp&#8217;s query was based on a now-deleted Twitter post from 2022, when a user by the name &#8220;3xp0rt&#8221; asserted that Toha was a Russian man named Anton Viktorovich Avdeev, born October 27, 1983.
Searching the web for Toha&#8217;s email address toschka2003@yandex.ru reveals a 2010 sales thread on the forum bmwclub.ru where a user named Honeypo was selling a 2007 BMW X5. The ad listed the contact person as Anton Avdeev and gave the contact phone number 9588693.

A search on the phone number 9588693 in the breach tracking service Constella Intelligence finds plenty of official Russian government records with this number, date of birth and the name Anton Viktorovich Avdeev. For example, hacked Russian government records show this person has a Russian tax ID and SIN (Social Security number), and that they were flagged for traffic violations on several occasions by Moscow police; in 2004, 2006, 2009, and 2014.
Astute readers may have noticed by now that the ages of Mr. Avdeev (41) and the XSS admin arrested this month (38) are a bit off. This would seem to suggest that the person arrested is someone other than Mr. Avdeev, who did not respond to requests for comment.
A FLY ON THE WALL
For further insight on this question, KrebsOnSecurity sought comments from Sergeii Vovnenko, a former cybercriminal from Ukraine who now works at the security startup paranoidlab.com. I reached out to Vovnenko because for several years beginning around 2010 he was the owner and operator of thesecure[.]biz, an encrypted &#8220;Jabber&#8221; instant messaging server that Europol said was operated by the suspect arrested in Kiev. Thesecure[.]biz grew quite popular among many of the top Russian-speaking cybercriminals because it scrupulously kept few records of its users&#8217; activity, and its administrator was always a trusted member of the community.
The reason I know this historic tidbit is that in 2013, Vovnenko &#8212; using the hacker nicknames &#8220;Fly,&#8221; and &#8220;Flycracker&#8221; &#8212; hatched a plan to have a gram of heroin purchased off of the Silk Road darknet market and shipped to our home in Northern Virginia. The scheme was to spoof a call from one of our neighbors to the local police, saying this guy Krebs down the street was a druggie who was having narcotics delivered to his home.
I happened to be lurking on Flycracker&#8217;s private cybercrime forum when his heroin-framing plan was carried out, and called the police myself before the smack eventually arrived in the U.S. Mail. Vovnenko was later arrested for unrelated cybercrime activities, extradited to the United States, convicted, and deported after a 16-month stay in the U.S. prison system [on several occasions, he has expressed heartfelt apologies for the incident, and we have since buried the hatchet].
Vovnenko said he purchased a device for cloning credit cards from Toha in 2009, and that Toha shipped the item from Russia. Vovnenko explained that he (Flycracker) was the owner and operator of thesecure[.]biz from 2010 until his arrest in 2014.
Vovnenko believes thesecure[.]biz was stolen while he was in jail, either by Toha and/or an XSS administrator who went by the nicknames N0klos and Sonic.
&#8220;When I was in jail, [the] admin of xss.is stole that domain, or probably N0klos bought XSS from Toha or vice versa,&#8221; Vovnenko said of the Jabber domain. &#8220;Nobody from [the forums] spoke with me after my jailtime, so I can only guess what really happened.&#8221;
N0klos was the owner and administrator of an early Russian-language cybercrime forum known as Darklife[.]ws. However, N0kl0s also appears to be a lifelong Russian resident, and in any case seems to have vanished from Russian cybercrime forums several years ago.
Asked whether he believes Toha was the XSS administrator who was arrested this month in Ukraine, Vovnenko maintained that Toha is Russian, and that &#8220;the French cops took the wrong guy.&#8221;
WHO IS TOHA?
So who did the Ukrainian police arrest in response to the investigation by the French authorities? It seems plausible that the BMW ad invoking Toha&#8217;s email address and the name and phone number of a Russian citizen was simply misdirection on Toha&#8217;s part &#8212; intended to confuse and throw off investigators. Perhaps this even explains the Avdeev surname surfacing in the registration records from one of Toha&#8217;s domains.
But sometimes the simplest answer is the correct one. &#8220;Toha&#8221; is a common Slavic nickname for someone with the first name &#8220;Anton,&#8221; and that matches the name in the registration records for more than a dozen domains tied to Toha&#8217;s toschka2003@yandex.ru email address: Anton Medvedovskiy.
Constella Intelligence finds there is an Anton Gannadievich Medvedovskiy living in Kiev who will be 38 years old in December. This individual owns the email address itsmail@i.ua, as well an an Airbnb account featuring a profile photo of a man with roughly the same hairline as the suspect in the blurred photos released by the Ukrainian police. Mr. Medvedovskiy did not respond to a request for comment.
My take on the takedown is that the Ukrainian authorities likely arrested Medvedovskiy. Toha shared on DaMaGeLab in 2005 that he had recently finished the 11th grade and was studying at a university &#8212; a time when Mevedovskiy would have been around 18 years old. On Dec. 11, 2006, fellow Exploit members wished Toha a happy birthday. Records exposed in a 2022 hack at the Ukrainian public services portal diia.gov.ua show that Mr. Medvedovskiy&#8217;s birthday is Dec. 11, 1987.
The law enforcement action and resulting confusion about the identity of the detained has thrown the Russian cybercrime forum scene into disarray in recent weeks, with lengthy and heated arguments about XSS&#8217;s future spooling out across the forums.
XSS relaunched on a new Tor address shortly after the authorities plastered their seizure notice on the forum&#8217;s¬† homepage, but all of the trusted moderators from the old forum were dismissed without explanation. Existing members saw their forum account balances drop to zero, and were asked to plunk down a deposit to register at the new forum. The new XSS &#8220;admin&#8221; said they were in contact with the previous owners and that the changes were to help rebuild security and trust within the community.
However, the new admin&#8217;s assurances appear to have done little to assuage the worst fears of the forum&#8217;s erstwhile members, most of whom seem to be keeping their distance from the relaunched site for now.
Indeed, if there is one common understanding amid all of these discussions about the seizure of XSS, it is that Ukrainian and French authorities now have several years worth of private messages between XSS forum users, as well as contact rosters and other user data linked to the seized Jabber server.
&#8220;The myth of the &#8216;trusted person&#8217; is shattered,&#8221; the user &#8220;GordonBellford&#8221; cautioned on Aug. 3 in an Exploit forum thread about the XSS admin arrest. &#8220;The forum is run by strangers. They got everything. Two years of Jabber server logs. Full backup and forum database.&#8221;
GordonBellford continued:
And the scariest thing is: this data array is not just an archive. It is material for analysis that has ALREADY BEEN DONE . With the help of modern tools, they see everything:
Graphs of your contacts and activity.
Relationships between nicknames, emails, password hashes and Jabber ID.
Timestamps, IP addresses and digital fingerprints.
Your unique writing style, phraseology, punctuation, consistency of grammatical errors, and even typical typos that will link your accounts on different platforms.
They are not looking for a needle in a haystack. They simply sifted the haystack through the AI sieve and got ready-made dossiers.

üéì University AI
No updates.

üè¢ Corporate AI
‚Ä¢ Self-adaptive reasoning for science
  Unlocking self-adaptive cognitive behavior that is more controllable and explainable than reasoning models in challenging scientific domains



Long-running LLM agents equipped with strong reasoning, planning, and execution skills have the potential to transform scientific discovery with high-impact advancements, such as developing new materials or pharmaceuticals. As these agents become more autonomous, ensuring effective human oversight and clear accountability becomes increasingly important, presenting challenges that must be addressed to unlock their full transformative power. Today‚Äôs approaches to long-term reasoning are established during the post-training phase, prior to end-user deployment and typically by the model provider. As a result, the expected actions of these agents are pre-baked by the model developer, offering little to no control from the end user.



At Microsoft, we are pioneering a vision for a continually steerable virtual scientist. In line with this vision, we created the ability to have a non-reasoning model develop thought patterns that allow for control and customizability by scientists. Our approach, a cognitive loop via in-situ optimization (CLIO), does not rely on reinforcement learning post-training to develop reasoning patterns yet still yields equivalent performance as demonstrated through our evaluation on Humanity‚Äôs Last Exam (HLE). Notably, we increased OpenAI GPT-4.1‚Äôs base model accuracy on text-only biology and medicine from 8.55% to 22.37%, an absolute increase of 13.82% (161.64% relative), surpassing o3 (high). This demonstrates that an optimization-based, self-adaptive AI system developed without further post-training can rival post-trained models in domains where adaptability, explainability, and control matter most.



Figure 1. Head-to-head comparison of OpenAI‚Äôs GPT-4.1 with CLIO, o3, and GPT-4.1 with no tools on HLE biology and medicine questions



In-situ optimization with internal self-reflection to enable self-adaptive reasoning



Model development has advanced from using reinforcement learning human feedback (RLHF) for answer alignment to external grading in reinforcement learning (RLVR). Recent approaches show promise in the utilization of intrinsic rewards for training reasoning models (RLIR). Traditionally, these reasoning processes are learned during the post-training process before any user interaction. While today‚Äôs reasoning models require additional data in the training phase and limit user control during the reasoning generation process, CLIO‚Äôs approach enables users to steer reasoning from scratch without additional data. Rather, CLIO generates its own necessary data by creating reflection loops at runtime. These reflection loops are utilized for a wide array of activities that CLIO self-defines, encompassing idea exploration, memory management, and behavior control. Most interesting is CLIO‚Äôs ability to leverage prior inferences to adjust future behaviors, handling uncertainties and raising flags for correction when necessary. Through this open architecture approach to reasoning, we alleviate the necessity for further model post-training to achieve desired reasoning behavior. Performing novel scientific discoveries often has no prior established patterns for reasoning, much less a large enough corpus of high-quality data to train on.&nbsp;



CLIO reasons by continuously reflecting on progress, generating hypotheses, and evaluating multiple discovery strategies. For the HLE test, CLIO was specifically steered to follow the scientific method as a guiding framework. Our research shows that equipping language models with self-adapting reasoning enhances their problem-solving ability. It provides a net benefit in quality for science questions, as well as providing exposure and control to the end user.



Figure 2. CLIO can raise key areas of uncertainty within its self-formulated reasoning process, balancing multiple different viewpoints using graph structures.



Control over uncertainty: Building trust in AI&nbsp;



Orchestrated reasoning systems like CLIO are valuable for scientific discovery, as they provide features beyond accuracy alone. Capabilities such as explaining the outcomes of internal reasoning are standard in the scientific field and are present in current reasoning model approaches. However, elements like displaying complete work, including final outcomes, internal thought processes, and uncertainty thresholds to support reproducibility or correction, as well as indicating uncertainty, are not yet universally implemented. Current models and systems do not have this same innate humility.&nbsp; Rather, we are left with models that produce confident results, whether correct or incorrect. When correct, it is valuable. When incorrect, it is dangerous to the scientific process. Hence, understanding a model or system‚Äôs uncertainty is a crucial aspect that we have developed natively into CLIO.



On the other end of the spectrum, orchestrated reasoning systems tend to oversaturate the user by raising too many flags. We enable prompt-free control knobs within CLIO to set thresholds for raising uncertainty flags. This allows CLIO to flag uncertainty for itself and the end user at the proper point in time. This also enables scientists to revisit CLIO‚Äôs reasoning path with critiques, edit beliefs during the reasoning process, and re-execute them from the desired point in time. Ultimately, this builds a foundational level of trust with scientists to use them in a scientifically defensible and rigorous way.&nbsp;



How does&nbsp;CLIO&nbsp;perform?&nbsp;



We evaluate CLIO against text-based biology and medicine questions from HLE. For this domain, we demonstrate a 61.98% relative increase or an 8.56% net increase in accuracy over OpenAI‚Äôs o3 and substantially outperform base completion models like OpenAI‚Äôs GPT-4.1, while enabling the requisite explainability and control. This technique applies to all models, showing similar increases in OpenAI‚Äôs GPT-4o model, which we observe performs poorly on HLE-level questions. On average, GPT-4.1 is not considered competent for HLE scale questions (GraphRAG. This extension of the cognition pattern provides a further 7.90% over a non-ensembled approach. &nbsp;



Figure 3. The impact of thinking effort on CLIO‚Äôs effectiveness.



Furthermore, CLIO‚Äôs design offers different knobs of control, for example, how much time to think and which technique to utilize for a given problem. In Figure 3, we demonstrate these knobs of control and their increase on GPT-4.1 and GPT-4o&#8217;s performance. In this case, we analyze performance for a subset of biomedical questions, those focused on immunology. CLIO increases GPT-4o&#8217;s base performance to be at par with the best reasoning models for immunology questions. We observe a 13.60% improvement over the base model, GPT-4o. This result shows CLIO to be model agnostic, similar to Microsoft AI Diagnostic Orchestrator&#8217;s (MAI-DxO) (opens in new tab)&#8216;s approach and corresponding performance boost.&nbsp;



Implications for science and trustworthy discovery



The future of scientific discovery demands more than reasoning over knowledge and raw computational power alone. Here, we demonstrate how CLIO not only increases model performance but establishes new layers of control for scientists. In our upcoming work, we will demonstrate how CLIO increases tool utility for highly valuable scientific questions in the drug discovery space which requires precise tools designed for the language of science. While our experiments focus on scientific discovery, we believe CLIO can apply in a domain-agnostic fashion. Experts tackling problems in domains such as financial analysis, engineering, and legal services could potentially benefit from AI systems with a transparent, steerable reasoning approach. Ultimately, we envision CLIO as an enduring control-layer in hybrid AI stacks that combine traditional completion and reasoning models, with external memory systems, and advanced tool calling. These continuous checks and balances that CLIO enables will continue to remain valuable even as components within the AI stacks evolve. This combination of intelligent and steerable scientific decision making and tool optimization is the basis of the recently announced Microsoft Discovery platform (opens in new tab).



At Microsoft, we‚Äôre committed to advancing AI research that earns the trust of scientists, empowering them to discover new frontiers of knowledge. Our work is a testament to what‚Äôs possible when we blend innovation with trustworthiness and a human-centered vision for the future of AI-assisted scientific discovery. We invite the research and scientific community to join us in shaping that future.



Further information:



To learn more details about our approach, please read our pre-print paper published alongside this blog. We are in the process of submitting this work for external peer review and encourage partners to explore the utilization of CLIO in Microsoft Discovery. To learn more about Microsoft‚Äôs research on this or contact our team, please reach out to discoverylabs@microsoft.com.¬†



Acknowledgements



We are grateful for Jason Zander and Nadia Karim‚Äôs support. We extend our thanks to colleagues both inside and outside Microsoft Discovery and Quantum for sharing their insights and feedback, including Allen Stewart, Yasser Asmi, David Marvin, Harsha Nori, Scott Lundberg, and Phil Waymouth.&nbsp;
Opens in a new tabThe post Self-adaptive reasoning for science appeared first on Microsoft Research.
‚Ä¢ Project Ire autonomously identifies malware at scale
  Today, we are excited to introduce an autonomous AI agent that can analyze and classify software without assistance, a step forward in cybersecurity and malware detection. The prototype, Project Ire, automates what is considered the gold standard in malware classification: fully reverse engineering a software file without any clues about its origin or purpose. It uses decompilers and other tools, reviews their output, and determines whether the software is malicious or benign.



Project Ire&nbsp;emerged&nbsp;from a collaboration&nbsp;between&nbsp;Microsoft Research, Microsoft Defender Research, and Microsoft Discovery & Quantum, bringing together security&nbsp;expertise, operational knowledge, data from global malware telemetry, and AI research. It is built on the same collaborative and agentic foundation behind&nbsp;GraphRAG (opens in new tab)&nbsp;and&nbsp;Microsoft Discovery (opens in new tab).&nbsp;The system&nbsp;uses advanced language models and a suite of callable reverse engineering and binary analysis tools to drive investigation and adjudication.



As of this writing, Project Ire has achieved a precision (opens in new tab) of 0.98 and a recall (opens in new tab) of 0.83 using public datasets of Windows drivers. It was the first reverse engineer at Microsoft, human or machine, to author a conviction case‚Äîa detection strong enough to justify automatic blocking‚Äîfor a specific advanced persistent threat (APT) malware sample, which has since been identified and blocked by Microsoft Defender.&nbsp;



Malware classification at a global scale



Microsoft‚Äôs Defender platform scans more than one billion monthly (opens in new tab) active devices through the company‚Äôs Defender suite of products, which routinely require manual review of software by experts.



This kind of work is challenging. Analysts often face error and alert fatigue, and there‚Äôs no easy way to compare and standardize how different people review and classify threats over time. For both of these reasons, today&#8217;s overloaded experts are vulnerable to burnout, a well-documented issue in the field.



Unlike other AI applications in security, malware classification lacks a computable validator (opens in new tab). The AI must make judgment calls without definitive validation beyond expert review. Many behaviors found in software, like reverse engineering protections, don‚Äôt clearly indicate whether a sample is malicious or benign.&nbsp;



This ambiguity requires analysts to investigate each sample incrementally, building enough evidence to determine whether it‚Äôs malicious or benign despite opposition from adaptive, active adversaries. This&nbsp;has long made it difficult to automate and scale what is inherently a complex and expensive process.



Technical foundation



Project Ire attempts to address these challenges by acting as an autonomous system that uses specialized tools to reverse engineer software. The system‚Äôs architecture allows for reasoning at multiple levels, from low-level binary analysis to control flow reconstruction and high-level interpretation of code behavior.



Its tool-use API enables the system to update its understanding of a file using a wide range of reverse engineering tools, including Microsoft memory analysis sandboxes based on Project Freta (opens in new tab), custom and open-source tools, documentation search, and multiple decompilers.&nbsp;&nbsp;



Reaching a verdict&nbsp;



The evaluation process begins with a triage, where automated reverse engineering tools identify the file type, its structure, and potential areas of interest. From there, the system reconstructs the software‚Äôs control flow graph using frameworks such as angr (opens in new tab) and Ghidra (opens in new tab), building a graph that forms the backbone of Project Ire‚Äôs memory model and guides the rest of the analysis.&nbsp;&nbsp;



Through iterative function analysis, the LLM calls specialized tools through an API to identify and summarize key functions. Each result feeds into a ‚Äúchain of evidence,‚Äù a detailed, auditable trail that shows how the system reached its conclusion. This traceable evidence log supports secondary review by security teams and helps refine the system in cases of misclassification.&nbsp;&nbsp;



To verify its findings, Project Ire can invoke a validator tool that cross-checks claims in the report against the chain of evidence. This tool draws on expert statements from malware reverse engineers on the Project Ire team. Drawing on this evidence and its internal model, the system creates a final report and classifies the sample as malicious or benign.



	
		

		
		Spotlight: AI-POWERED EXPERIENCE
	
	
	
						
				
					
				
			
			
			

									Microsoft research copilot experience
				
								Discover more about research at Microsoft through our AI-powered experience
				
								
					
						
							Start now						
					
				
							
	
Opens in a new tab	
	


Preliminary testing shows promise&nbsp;



Two early evaluations tested Project Ire‚Äôs effectiveness as an autonomous malware classifier. In the first, we assessed Project Ire on a dataset of publicly accessible Windows drivers, some known to be malicious, others benign. Malicious samples came from the Living off the Land Drivers (opens in new tab) database, which includes a collection of Windows drivers used by attackers to bypass security controls, while known benign drivers were sourced from Windows Update.&nbsp;



This classifier performed well, correctly identifying 90% of all files and flagging only 2% of benign files as threats. It achieved a precision of 0.98 and a recall of 0.83. This low false-positive rate suggests clear potential for deployment in security operations, alongside expert reverse engineering reviews.&nbsp;



For each file it analyzes, Project Ire generates a report that includes an evidence section, summaries of all examined code functions, and other technical artifacts.&nbsp;&nbsp;



Figures 1 and 2 present reports for two successful malware classification cases generated during testing. The first involves a kernel-level rootkit, Trojan:Win64/Rootkit.EH!MTB (opens in new tab). The system identified several key features, including jump-hooking, process termination, and web-based command and control. It then correctly flagged the sample as malicious.






  
  Figure 1 Analysis
  


  
    The binary contains a function named &#8216;MonitorAndTerminateExplorerThread_16f64&#8217; that runs an infinite loop waiting on synchronization objects and terminates system threads upon certain conditions. It queries system or process information, iterates over processes comparing their names case-insensitively to &#8216;Explorer.exe&#8217;, and manipulates registry values related to &#8216;Explorer.exe&#8217;. This function appears to monitor and potentially terminate or manipulate the &#8216;Explorer.exe&#8217; process, a critical Windows shell process. Such behavior is suspicious and consistent with malware that aims to disrupt or control system processes.
    Another function, &#8216;HttpGetRequestAndResponse_174a4&#8217;, performs HTTP GET requests by parsing URLs, resolving hostnames, opening sockets, sending requests, and reading responses. This network communication capability could be leveraged for command and control or data exfiltration, common in malware.
    The binary also includes a function &#8216;PatchProcessEntryPointWithHook_12b5c&#8217; that patches the entry point of a process by writing a hook or trampoline that redirects execution to a specified address. This technique is commonly used for process injection or hooking, allowing malware to alter process behavior or inject malicious code.
    Other functions related to sending IOCTL requests to device drivers were identified, but their maliciousness could not be conclusively determined without additional context.
    Overall, the binary exhibits multiple indicators of malicious behavior, including process manipulation, network communication, and code injection techniques, suggesting it is likely malware designed to interfere with system processes and communicate with remote servers.
  


Figure 1. Project Ire report, sample with SHA256: 86047bb1969d1db455493955fd450d18c62a3f36294d0a6c3732c88dfbcc4f62 (opens in new tab)



The second sample, HackTool:Win64/KillAV!MTB (opens in new tab), was designed to disable antivirus software. Project Ire correctly identified the code that locates and disables antivirus programs, providing evidence that the file was malicious.&nbsp;&nbsp;



In one section of the code, however, the system misidentified a function as anti-debugging behavior. To maintain accuracy, the system used the validator tool to flag the claim as unsupported. The issue was later resolved by updating decompiler rules, but this example illustrates how Project Ire navigates uncertainty during analysis. Figure 2 shows the corresponding report.&nbsp;






  
  Figure 2 Analysis
  


  
    The binary contains several functions indicative of malicious intent. The function register_and_log_known_processes_140001000 logs and registers process names associated with antivirus and security software, such as &#8216;avp.exe&#8217;, &#8216;avpui.exe&#8217;, and &#8216;360Tray.exe&#8217;. It calls another function, TerminateProcessesByNameSubstring_1400010f4, which enumerates system processes and terminates those whose names contain specified substrings. This behavior is typical of malware attempting to disable or evade security software by killing their processes.
    Another function, check_and_handle_special_state_14000502c, performs checks on a global variable and triggers software interrupts if certain conditions are not met. While the exact purpose of these interrupts (int 0x29 and int 0x3) is unclear, they could represent an anti-debug or anti-analysis mechanism to detect or interfere with debugging or tampering attempts. However, this assumption could not be fully validated against expert statements.
    Other functions include initialization routines and simple logging wrappers, but the core malicious behavior centers on process termination targeting security software. This indicates the binary is designed to compromise system security by disabling protective processes, a hallmark of malware such as trojans or rootkits.
  


Figure 2. Project Ire report, sample with SHA256: b6cb163089f665c05d607a465f1b6272cdd5c949772ab9ce7227120cf61f971a (opens in new tab)



Real-world evaluation with Microsoft Defender&nbsp;



The more demanding test involved nearly 4,000 ‚Äúhard-target‚Äù files not classified by automated systems and slated for manual review by expert reverse engineers.



In this real-world scenario, Project Ire operated fully autonomously on files created after the language models‚Äô training cutoff, files that no other automated tools at Microsoft could classify at the time.



The system achieved a high precision score of 0.89, meaning nearly 9 out of 10 files flagged malicious were correctly identified as malicious. Recall was 0.26, indicating that under these challenging conditions, the system detected roughly a quarter of all actual malware.



The system correctly identified many of the malicious files, with few false alarms, just a 4% false positive rate. While overall performance was moderate, this combination of accuracy and a low error rate suggests real potential for future deployment.



Looking ahead&nbsp;



Based on these early successes, the Project Ire prototype will be leveraged inside Microsoft‚Äôs Defender organization as Binary Analyzer for threat detection and software classification.



Our goal is to scale the system‚Äôs speed and accuracy so that it can correctly classify files from any source, even on first encounter. Ultimately, our vision is to detect novel malware directly in memory, at scale.



Acknowledgements&nbsp;



Project Ire acknowledges the following additional developers that contributed to the results in this publication: Dayenne de Souza, Raghav Pande, Ryan Terry, Shauharda Khadka, and Bob Fleck for their independent review of the system.



The system incorporates multiple tools, including the&nbsp;angr&nbsp;framework developed by&nbsp;Emotion Labs (opens in new tab). Microsoft has collaborated extensively with Emotion Labs, a pioneer in cyber autonomy, throughout the development of Project Ire, and thanks them for the innovations and insights that contributed to the successes reported here.&nbsp;
Opens in a new tabThe post Project Ire autonomously identifies malware at scale appeared first on Microsoft Research.
‚Ä¢ VeriTrail: Detecting hallucination and tracing provenance in multi-step AI workflows
  Watch VeriTrail Explainer




Many applications of language models (LMs) involve generating content based on source material, such as answering questions, summarizing information, and drafting documents. A critical challenge for these applications is that LMs may produce content that is not supported by the source text ‚Äì a phenomenon known as ‚Äúclosed-domain hallucination.‚Äù1



Existing methods for detecting closed-domain hallucination typically compare a given LM output to the source text, implicitly assuming that there is only a single output to evaluate. However, applications of LMs increasingly involve processes with multiple generative steps: LMs generate intermediate outputs that serve as inputs to subsequent steps and culminate in a final output. Many agentic workflows follow this paradigm (e.g., each agent is responsible for a specific document or sub-task, and their outputs are synthesized into a final response).‚ÄØ&nbsp;



In our paper ‚ÄúVeriTrail: Closed-Domain Hallucination Detection with Traceability,‚Äù we argue that, given the complexity of processes with multiple generative steps, detecting hallucination in the final output is necessary but not sufficient. We also need traceability, which has two components:&nbsp;




Provenance: if the final output is supported by the source text, we should be able to trace its path through the intermediate outputs to the source.&nbsp;



Error Localization: if the final output is not supported by the source text, we should be able to trace where the error was likely introduced.




Our paper presents VeriTrail, the first closed-domain hallucination detection method designed to provide traceability for processes with any number of generative steps. We also demonstrate that VeriTrail outperforms baseline methods commonly used for hallucination detection. In this blog post, we provide an overview of VeriTrail‚Äôs design and performance.2



VeriTrail‚Äôs hallucination detection process



A key idea leveraged by VeriTrail is that a wide range of generative processes can be represented as a directed acyclic graph (DAG). Each node in the DAG represents a piece of text (i.e., source material, an intermediate output, or the final output) and each edge from node A to node B indicates that A was used as an input to produce B. Each node is assigned a unique ID, as well as a stage reflecting its position in the generative process.‚ÄØ&nbsp;



An example of a process with multiple generative steps is GraphRAG. A DAG representing a GraphRAG run is illustrated in Figure 1, where the boxes and arrows correspond to nodes and edges, respectively.3



Figure 1: GraphRAG splits the source text into chunks (Stage 1). For each chunk, an LM extracts entities and relationships (the latter are denoted by ‚Äú‚≠§ ‚Äú), along with short descriptions (Stage 2). If an entity or a relationship was extracted from multiple chunks, an LM summarizes the descriptions (Stage 3). A knowledge graph is constructed from the final set of entities and relationships, and a community detection algorithm, such as Leiden clustering, groups entities into communities. For each community, an LM generates a ‚Äúcommunity report‚Äù that summarizes the entities and relationships (Stage 4). To answer a user‚Äôs question, an LM generates ‚Äúmap-level answers‚Äù based on groups of community reports (Stage 5), then synthesizes them into a final answer (Stage 6).



VeriTrail takes as input a DAG representing a completed generative process and aims to determine whether the final output is fully supported by the source text. It begins by extracting claims (i.e., self-contained, verifiable statements) from the final output using Claimify. VeriTrail verifies claims in the reverse order of the generative process: it starts from the final output and moves toward the source text. Each claim is verified separately. Below, we include two case studies that illustrate how VeriTrail works, using the DAG from Figure 1.‚ÄØ



Case study 1: A ‚ÄúFully Supported‚Äù claim



Figure 2: Left: GraphRAG as a DAG. Right: VeriTrail‚Äôs hallucination detection process for a ‚ÄúFully Supported‚Äù claim.



Figure 2 shows an example of a claim that VeriTrail determined was not hallucinated:‚ÄØ




In Iteration 1, VeriTrail identified the nodes that were used as inputs for the final answer: Nodes 15 and 16. Each identified node was split into sentences, and each sentence was programmatically assigned a unique ID.

An LM then performed Evidence Selection, selecting all sentence IDs that strongly implied the truth or falsehood of the claim. The LM also generated a summary of the selected sentences (not shown in Figure 2). In this example, a sentence was selected from Node 15.



Next, an LM performed Verdict Generation. If no sentences had been selected in the Evidence Selection step, the claim would have been assigned a ‚ÄúNot Fully Supported‚Äù verdict. Instead, an LM was prompted to classify the claim as ‚ÄúFully Supported,‚Äù ‚ÄúNot Fully Supported,‚Äù or ‚ÄúInconclusive‚Äù based on the evidence. In this case, the verdict was ‚ÄúFully Supported.‚Äù





Since the verdict in Iteration 1 was ‚ÄúFully Supported,‚Äù VeriTrail proceeded to Iteration 2. It considered the nodes from which at least one sentence was selected in the latest Evidence Selection step (Node 15) and identified their input nodes (Nodes 12 and 13). VeriTrail repeated Evidence Selection and Verdict Generation for the identified nodes. Once again, the verdict was ‚ÄúFully Supported.‚Äù This process ‚Äì identifying candidate nodes, performing Evidence Selection and Verdict Generation ‚Äì was repeated in Iteration 3, where the verdict was still ‚ÄúFully Supported,‚Äù and likewise in Iteration 4.‚ÄØ



In Iteration 4, a single source text chunk was verified. Since the source text, by definition, does not have any inputs, verification terminated and the verdict was deemed final.




Case study 2: A ‚ÄúNot Fully Supported‚Äù claim



Figure 3: Left: GraphRAG as a DAG. Right: VeriTrail‚Äôs hallucination detection‚ÄØprocess for a ‚ÄúNot Fully Supported‚Äù claim, where the maximum number of consecutive ‚ÄúNot Fully Supported‚Äù verdicts was set to 2. 



Figure 3 provides an example of a claim where VeriTrail identified hallucination:




In Iteration 1, VeriTrail identified the nodes used as inputs for the final answer: Nodes 15 and 16. After Evidence Selection and Verdict Generation, the verdict was ‚ÄúNot Fully Supported.‚Äù Users can configure the maximum number of consecutive ‚ÄúNot Fully Supported‚Äù verdicts permitted. If the maximum had been set to 1, verification would have terminated here, and the verdict would have been deemed final. Let‚Äôs assume the maximum was set to 2, meaning that VeriTrail had to perform at least one more iteration.



Even though evidence was selected only from Node 15 in Iteration 1, VeriTrail checked the input nodes for both Node 15 and Node 16 (i.e., Nodes 12, 13, and 14) in Iteration 2. Recall that in Case Study 1 where the verdict was ‚ÄúFully Supported,‚Äù VeriTrail only checked the input nodes for Node 15. Why was the ‚ÄúNot Fully Supported‚Äù claim handled differently? If the Evidence Selection step overlooked relevant evidence, the ‚ÄúNot Fully Supported‚Äù verdict might be incorrect. In this case, continuing verification based solely on the selected evidence (i.e., Node 15) would propagate the mistake, defeating the purpose of repeated verification.



In Iteration 2, Evidence Selection and Verdict Generation were repeated for Nodes 12, 13, and 14. Once again, the verdict was ‚ÄúNot Fully Supported.‚Äù Since this was the second consecutive ‚ÄúNot Fully Supported‚Äù verdict, verification terminated and the verdict was deemed final.




	
		

	
	
						
				
					
				
			
			
			

									Azure AI Foundry Labs
				
								Get a glimpse of potential future directions for AI, with these experimental technologies from Microsoft Research.
				
								
					
						
							Azure AI Foundry						
					
				
							
	
Opens in a new tab	
	


Providing traceability



In addition to assigning a final ‚ÄúFully Supported,‚Äù ‚ÄúNot Fully Supported,‚Äù or ‚ÄúInconclusive‚Äù verdict to each claim, VeriTrail returns (a) all Verdict Generation results and (b) an evidence trail composed of all Evidence Selection results: the selected sentences, their corresponding node IDs, and the generated summaries. Collectively, these outputs provide traceability:&nbsp;




Provenance: For ‚ÄúFully Supported‚Äù and ‚ÄúInconclusive‚Äù claims, the evidence trail traces a path from the source material to the final output, helping users understand how the output may have been derived. For example, in Case Study 1, the evidence trail consists of Sentence 8 from Node 15, Sentence 11 from Node 13, Sentence 26 from Node 4, and Sentence 79 from Node 1.



Error Localization: For ‚ÄúNot Fully Supported‚Äù claims, VeriTrail uses the Verdict Generation results to identify the stage(s) of the process where the unsupported content was likely introduced. For instance, in Case Study 2, where none of the verified intermediate outputs supported the claim, VeriTrail would indicate that the hallucination occurred in the final answer (Stage 6). Error stage identification helps users address hallucinations and understand where in the process they are most likely to occur.&nbsp;




The evidence trail also helps users verify the verdict: instead of reading through all nodes ‚Äì which may be infeasible for processes that generate large amounts of text ‚Äì users can simply review the evidence sentences and summaries.‚ÄØ



Key design features



VeriTrail‚Äôs design prioritizes reliability, efficiency, scalability, and user agency. Notable features include:‚ÄØ




During Evidence Selection (introduced in Case Study 1), the sentence IDs returned by the LM are checked against the programmatically assigned IDs. If a returned ID does not match an assigned ID, it is discarded; otherwise, it is mapped to its corresponding sentence. This approach guarantees that the sentences included in the evidence trail are not hallucinated.



After a claim is assigned an interim ‚ÄúFully Supported‚Äù or ‚ÄúInconclusive‚Äù verdict (as in Case Study 1), VeriTrail verifies the input nodes of only the nodes from which evidence was previously selected ‚Äì not all possible input nodes. By progressively narrowing the search space, VeriTrail limits the number of nodes the LM must evaluate. In particular, since VeriTrail starts from the final output and moves toward the source text, it tends to verify a smaller proportion of nodes as it approaches the source text. Nodes closer to the source text tend to be larger (e.g., a book chapter should be larger than its summary), so verifying fewer of them helps reduce computational cost.



VeriTrail is designed to handle input graphs with any number of nodes, regardless of whether they fit in a single prompt. Users can specify an input size limit per prompt. For Evidence Selection, inputs that exceed the limit are split across multiple prompts. If the resulting evidence exceeds the input size limit for Verdict Generation, VeriTrail reruns Evidence Selection to compress the evidence further. Users can configure the maximum number of Evidence Selection reruns.‚ÄØ‚ÄØ



The configurable maximum number of consecutive ‚ÄúNot Fully Supported‚Äù verdicts (introduced in Case Study 2) allows the user to find their desired balance between computational cost and how conservative VeriTrail is in flagging hallucinations. A lower maximum reduces cost by limiting the number of checks. A higher maximum increases confidence that a flagged claim is truly hallucinated since it requires repeated confirmation of the ‚ÄúNot Fully Supported‚Äù verdict.‚ÄØ




Evaluating VeriTrail‚Äôs performance



We tested VeriTrail on two datasets covering distinct generative processes (hierarchical summarization4 and GraphRAG), tasks (summarization and question-answering), and types of source material (fiction novels and news articles). For the source material, we focused on long documents and large collections of documents (i.e., >100K tokens), where hallucination detection is especially challenging and processes with multiple generative steps are typically most valuable. The resulting DAGs were much more complex than the examples provided above (e.g., in one of the datasets, the average number of nodes was 114,368).



We compared VeriTrail to three types of baseline methods commonly used for closed-domain hallucination detection: Natural Language Inference models (AlignScore and INFUSE); Retrieval-Augmented Generation; and long-context models (Gemini 1.5 Pro and GPT-4.1 mini). Across both datasets and all language models tested, VeriTrail outperformed the baseline methods in detecting hallucination.5



Most importantly, VeriTrail traces claims through intermediate outputs ‚Äì unlike the baseline methods, which directly compare the final output to the source material. As a result, it can identify where hallucinated content was likely introduced and how faithful content may have been derived from the source. By providing traceability, VeriTrail brings transparency to generative processes, helping users understand, verify, debug, and, ultimately, trust their outputs.‚ÄØ&nbsp;



For an in-depth discussion of VeriTrail, please see our paper ‚ÄúVeriTrail: Closed-Domain Hallucination Detection with Traceability.‚Äù







1 (opens in new tab) The term ‚Äúclosed-domain hallucination‚Äù was introduced by OpenAI in the GPT-4 Technical Report (opens in new tab).



2 VeriTrail is currently used for research purposes only and is not available commercially.



3 We focus on GraphRAG‚Äôs global search method.



4 (opens in new tab) In hierarchical summarization, an LM summarizes each source text chunk individually, then the resulting summaries are repeatedly grouped and summarized until a final summary is produced (Wu et al., 2021 (opens in new tab); Chang et al., 2023 (opens in new tab)).



5 The only exception was the mistral-large-2411 model, where VeriTrail had the highest balanced accuracy, but not the highest macro F1 score.
Opens in a new tabThe post VeriTrail: Detecting hallucination and tracing provenance in multi-step AI workflows appeared first on Microsoft Research.
‚Ä¢ Pioneering AI workflows at scale: A deep dive into Asana AI Studio and Amazon Q index collaboration
  Organizations today face a critical challenge: managing an ever-increasing volume of tasks and information across multiple systems. Although traditional task management tools help organize work, they often fall short in delivering the intelligence needed for truly efficient operations. 
Today, we‚Äôre excited to announce the integration of Asana AI Studio with Amazon Q index, bringing generative AI directly into your daily workflows. This dynamic combination helps teams work smarter by powering AI workflows at scale with data from everyday applications. The result is seamless automation for key use cases like project intake, campaign management, and product launches, transforming how teams work and deliver results. 
In this post, we explore how Asana AI Studio and Amazon Q index transform enterprise efficiency through intelligent workflow automation and enhanced data accessibility. We start by examining the core capabilities of this powerful integration. Then we dive deep into the technical implementation and a step-by-step process of how to set up the Amazon Q Business data accessor and configure the Asana admin console. Throughout the post, we cover essential topics including security considerations, access controls, and best practices for maximizing the value of this integration. 
Whether you‚Äôre looking to streamline operations, improve decision-making, or break down data silos, this comprehensive guide will show you how to harness the full potential of Asana AI Studio and Amazon Q index in your enterprise environment. 
The power of integrating Asana AI Studio with Amazon Q index 
Asana AI Studio represents a breakthrough in no-code automation, helping teams create and deploy AI-powered workflows that streamline operations and minimize routine busywork. As the leading work management service for human and AI coordination, Asana offers enterprise-grade solutions that break down traditional data silos and enhance collaboration through AI-driven automation and insights. 
Amazon Q index for independent software vendors (ISVs) provides seamless integration of generative AI applications with enterprise data and metadata through an Amazon Q index, so you can search across your application data alongside other enterprise content. This integration capability makes sure ISVs can provide their customers with a unified search experience while maintaining strict security, access controls, and ownership over their data. 
With the refined indexing of Amazon Q combined with Asana‚Äôs comprehensive work management data, organizations can transform how they harness the power of their data. 
Through the Amazon Q Business data accessor capability, Asana can securely tap into and analyze information from diverse business applications, converting previously isolated data into meaningful business intelligence. This unlocks key use cases like project intake, campaign management, and product launches, all while maintaining data security. 
The following video demonstrates this solution in action, as a team member uses Asana to quickly access project information, create automated workflows, and receive AI-powered recommendations for task optimization across connected services. 

 
  
 
 
How the integration benefits enterprises 
Organizations can transform scattered information into real business outcomes with the combined power of Asana AI Studio and Amazon Q index: 
 
 Connect applications in minutes ‚Äì Use cross-application data quickly and confidently with one-time setup and secure permissions 
 Build integrated workflows ‚Äì Design AI-powered workflows that unify teams and applications, with no code required 
 Scale AI company-wide safely and securely ‚Äì Access real-time data and insights where your teams already work while the system intelligently maintains strict security protocols and existing access control list (ACL) permissions to help keep sensitive information protected 
 
Spencer Herrick, Principal AI Product Manager at Asana, shares: 

 ‚ÄúAt Asana, we‚Äôre committed to helping teams move faster with clarity and confidence. Integrating Amazon Q index into our AI infrastructure is a powerful step forward‚Äîit allows us to unify knowledge scattered across enterprise systems and surface highly relevant insights directly in workflows. This not only accelerates decision-making but also improves the quality of execution across teams. AWS has been an exceptional partner throughout this process, and we‚Äôre thrilled by the early results and the opportunity to continue innovating on top of this foundation.‚Äù
 
Solution overview 
The Amazon Q Business data accessor functions as a secure gateway, linking enterprise tools to the Amazon Q index. This component offers organizations a protected channel through which Asana AI Studio can retrieve information from their Amazon Q index. This enables Asana‚Äôs AI features to provide relevant answers to user queries by incorporating data from various connected systems. 
The result is an AI-powered experience that combines Asana‚Äôs work management capabilities with your organization‚Äôs broader information landscape that uses enterprise data across multiple systems. When a user asks a question through smart chat or triggers an AI Studio workflow, Asana‚Äôs AI orchestrator processes the query, checking both Asana‚Äôs native data for relevant information as well as the Amazon Q Business data accessor to search the Amazon Q index, hosted across the customer‚Äôs AWS environment. The index can aggregate data from various enterprise systems such as Google Workspace, Microsoft 365, and Salesforce using built-in Amazon Q Business connectors. The orchestrator then uses generative AI to provide users with contextual, actionable insights directly within the Asana system. With this integration, teams can quickly access project documentation, communication history, and other critical information, enhancing productivity and decision-making efficiency across the aspects of work management. 
The following diagram illustrates the overall solution integrating Asana AI Studio and Amazon Q index in the customer‚Äôs environment. 
 
Prerequisites 
To use the Amazon Q index integration with Asana‚Äôs AI features, you must have the following in place: 
 
 An AWS account with the necessary permissions and service access 
 An active Amazon Q Business setup, configured with AWS IAM Identity Center for user authentication 
 Asana‚Äôs AI features enabled 
 Super admin rights within your Asana workspace 
 
The Amazon Q Business data accessor facilitates a smooth connection between Asana‚Äôs AI tools and your Amazon Q index. To implement this integration, you must perform some initial setup in both the Amazon Q Business environment and your Asana admin console. This typically involves configuring authentication methods, specifying which data sources to include, and setting appropriate access controls. 
For a comprehensive guide on setting up Amazon Q Business, refer to Innovate on enterprise data with generative AI &amp; Amazon Q Business application. For Asana-specific steps, you can find detailed instructions in the Asana Help Center for Amazon Q index, but we also cover key steps in this post. 
Add Asana as a data accessor in Amazon Q Business 
Complete the following steps to add Asana as an Amazon Q Business data accessor: 
 
 On the Amazon Q Business console, choose Applications in the navigation pane. 
 Open the application you want to add a data accessor to. 
 Choose Data accessors in the navigation pane. 
 
 
 
 Choose Add data accessor. 
 For Data accessors, choose Asana. 
 
 
 
 For External ID, enter the external ID provided to you by Asana. 
 
In Asana, the tenant ID (or external ID) in the Amazon Q Business data accessor is called the domain ID. To retrieve the Asana domain ID, choose your account profile in Asana and choose Admin Console. Choose Settings, and scroll to Domain settings to retrieve the ID. 
 
 Choose Create trusted token issuer. 
 
 
 
 For Trusted token issuer name, enter the name of the trusted token issuer, then choose Create trusted token issuer. 
 
 
 
 For Data source access, select Allow access to all data sources. 
 For User access, select All users with application access, or select Define access based on user and groups for more granular access control. 
 Choose Add data accessor. 
 
 
After you add Asana as a data accessor successfully, a pop-up window will appear with data accessor configuration details. Share the data accessor configuration details with Asana, because it uses them to make a secure connection from the Asana AI Studio system through the Amazon Q index SearchRelevantContent API to retrieve content from your Amazon Q index. 
 
Connect an Amazon Q index to the Asana admin console 
Connecting an Amazon Q index to Asana requires super admin permission. Complete the following steps to connect: 
 
 Log in to the Asana admin console 
 Choose Settings, Asana AI, and Data connectors. 
 Choose Link account. 
 Fill in the required fields with information gathered from the Amazon Q Business console. 
 Choose Verify connection. 
 
After an Amazon Q index is connected to Asana, the option Data to use from connected apps will appear in the rule builder settings within AI Studio. You can now start running workflow rules with enriched context from third-party data sources like Google Drive, Microsoft Outlook, and Salesforce, as shown in the following screenshot. 
 
You can also use Asana smart chat in your Asana workspace and start asking questions. While interacting, smart chat will automatically search for relevant information across Asana and the connected third-party data sources like Google Drive using Amazon Q index to provide you with instant answers, as shown in the following screenshot. 
 
Clean up 
When you‚Äôre done using this solution, clean up the resources you created: 
 
 On your Asana admin page, navigate to Settings, Asana AI, Data connectors, and disconnect Amazon Q. 
 
 
 
 On Amazon Q Business console, delete the Asana data accessor from the Data accessors page. Deleting this data accessor will delete permissions and access to the data accessor for users. 
 Delete the Amazon Q Business application that you created as a prerequisite on the Applications page. Deleting the Amazon Q Business application will remove the associated index and data source connectors and avoid incurring additional costs. 
 
Conclusion 
This new Asana and Amazon Q index integration marks a significant advancement in intelligent work management. By merging Asana‚Äôs robust work management system with Amazon Q index‚Äôs comprehensive search capabilities, organizations can now effortlessly access and use information previously scattered across multiple systems. AI Studio users can craft more intelligent workflows by using external data through the new Data to use from connected apps option in the rule builder, and smart chat users can benefit from automatically cited responses that draw from both Asana and connected third-party sources. All of this is achieved while maintaining security and data ownership, effectively breaking down information silos that often hinder collaboration and efficiency. 
Whether you‚Äôre aiming to increase efficiency, optimize workflows, or make more informed decisions, the Asana and Amazon Q index integration provides the necessary tools to transform how your organization works. We encourage you to explore the Amazon Q Business console and Asana‚Äôs AI features documentation to unlock the full potential of your connected workplace. By embracing this powerful integration, you‚Äôre not just improving your work management, you‚Äôre setting the stage for a more intelligent and efficient future for your team. 
 
About the Authors 
Spencer Herrick is Principal AI Product Manager at Asana. He leads the development of AI-first products that help organizations seamlessly integrate new AI capabilities into collaborative work management. His journey into AI began not in Silicon Valley, but at the Large Hadron Collider in Switzerland, where he worked as an experimental physicist. Since then, Spencer has built and led innovative product teams at Grammarly, Uber, and Postmates, consistently pushing the boundaries of what‚Äôs possible at the intersection of AI, personalization, and user experience. 
Sourabh Banerjee is a Senior WW GenAI specialist at AWS with over 10 years of experience in leading product and GTM strategy. Sourabh leads global strategy and execution for helping customers and partners unlock business productivity through Amazon Q Business across industries. 
Chinmayee Rane is a Generative AI Specialist Solutions Architect at AWS, with a core focus on generative AI. She helps ISVs accelerate the adoption of generative AI by designing scalable and impactful solutions. With a strong background in applied mathematics and machine learning, she specializes in intelligent document processing and AI-driven innovation. Outside of work, she enjoys salsa and bachata dancing. 
Bobby Williams is a Senior Solutions Architect at AWS. He has decades of experience designing, building, and supporting enterprise software solutions that scale globally. He works on solutions across industry verticals and horizontals and is driven to create a delightful experience for every customer. 
Akhilesh Amara is a Software Development Engineer on the Amazon Q team based in Seattle, WA. He is contributing to the development and enhancement of intelligent and innovative AI tools. 
Chanki Nathani is a Sr. Partner Solutions Architect at AWS, focusing on business applications. With over 10 years of experience, he helps ISVs build and scale their SaaS solutions on AWS, specializing in serverless architectures and generative AI implementations. Chanki helps organizations use AWS AI/ML services, including Amazon Bedrock and Amazon Q Business, to transform their applications and enhance user experiences.
‚Ä¢ Responsible AI for the payments industry ‚Äì Part 1
  The payments industry stands at the forefront of digital transformation, with artificial intelligence (AI) rapidly becoming a cornerstone technology that powers a variety of solutions, from fraud detection to customer service. According to the following Number Analytics report, digital payment transactions are projected to exceed $15 trillion globally by 2027. Generative AI has expanded the scope and urgency of responsible AI in payments, introducing new considerations around content generation, conversational interfaces, and other complex dimensions. As financial institutions and payment solutions providers increasingly adopt AI solutions to enhance efficiency, improve security, and deliver personalized experiences, the responsible implementation of these technologies becomes paramount. According to the following McKinsey report, AI could add an estimated $13 trillion to the global economy by 2030, representing about a 16% increase in cumulative GDP compared with today. This translates to approximately 1.2% additional GDP growth per year through 2030. 
AI in payments helps drive technological advancement and strengthens building trust. When customers entrust their financial data and transactions to payment systems, they expect convenience and security, additionally fairness, transparency, and respect for their privacy. AWS recognizes the critical demands facing payment services and solution providers, offering frameworks that can help executives and AI practitioners transform responsible AI into a potential competitive advantage. The following Accenture report has additional statistics and data about responsible AI. 
This post explores the unique challenges facing the payments industry in scaling AI adoption, the regulatory considerations that shape implementation decisions, and practical approaches to applying responsible AI principles. In Part 2, we provide practical implementation strategies to operationalize responsible AI within your payment systems. 
Payment industry challenges 
The payments industry presents a unique landscape for AI implementation, where the stakes are high and the potential impact on individuals is significant. Payment technologies directly impact consumers‚Äô financial transactions and merchant options, making responsible AI practices an important consideration and a critical necessity. 
The payments landscape‚Äîencompassing consumers, merchants, payment networks, issuers, banks, and payment processors‚Äîfaces several challenges when implementing AI solutions: 
 
 Data classification and privacy ‚Äì Payment data is among the most sensitive information. In addition to financial details, it also includes patterns that can reveal personal behaviors, preferences, and life circumstances. Due to various regulations, AI systems that process these data systems are required to maintain the highest standards of privacy protection and data security. 
 Real-time processing requirements ‚Äì Payment systems often require split-second decisions, such as approving a transaction, flagging potential fraud, or routing payments. Production AI systems seek to deliver high standards for accuracy, latency, and cost while maintaining security and minimizing friction. This is important because failed transactions or incorrect decisions might result in poor customer experience or other financial loss. 
 Global operational context ‚Äì Payment providers often operate across jurisdictions with varying regulatory frameworks and standards. These include India‚Äôs Unified Payments Interface (UPI), Brazil‚Äôs PIX instant payment system, the United States‚Äô FedNow and Real-Time Payments (RTP) networks, and the European Union‚Äôs Payment Services Directive (PSD2) and Single Euro Payments Area (SEPA) regulations. AI systems should be adaptable enough to function appropriately across these diverse contexts while adhering to consistent responsible standards. 
 Financial inclusion imperatives ‚Äì The payment industry seeks to expand access to financial services for their customers. It‚Äôs important to design AI systems that promote inclusive financial access by mitigating bias and discriminatory outcomes. Responsible AI considerations can help create equitable opportunities while delivering frictionless experiences for diverse communities. 
 Regulatory landscape ‚Äì The payments industry navigates one of the economy‚Äôs most stringent regulatory environments, with AI implementation adding new layers of compliance requirements: 
   
   Global regulatory frameworks ‚Äì From the EU‚Äôs General Data Protection Regulation (GDPR) and the upcoming EU AI Act to the Consumer Financial Protection Bureau (CFPB) guidelines in the US, payment solution providers navigate disparate global requirements, presenting a unique challenge for scaling AI usage across the globe. 
   Explainability requirements ‚Äì Regulators increasingly demand that financial institutions be able to explain AI-driven decisions, especially those that impact consumers directly, like multimodal AI for combining biometric, behavioral, and contextual authentication. 
   Anti-discrimination mandates ‚Äì Financial regulations in many jurisdictions explicitly prohibit discriminatory practices. AI systems should be designed and monitored to help prevent inadvertent bias in decisions related to payment approvals and comply with fair lending laws. 
   Model risk management ‚Äì Regulatory frameworks like Regulation E in the US require financial institutions to validate models, including AI systems, and maintain robust governance processes around their development, implementation, and ongoing monitoring. 
    
 
The regulatory landscape for AI in financial services continues to evolve rapidly. Payment providers strive to stay abreast of changes and maintain flexible systems that can adapt to new requirements. 
Core principles of responsible AI 
In the following sections, we review how responsible AI considerations can be applied in the payment industry. The core principles include controllability, privacy and security, safety, fairness, veracity and robustness, explainability, transparency, and governance, as illustrated in the following figure. 
 
Controllability 
Controllability refers to the extent to which an AI system behaves as designed, without deviating from its functional objectives and constraints. Controllability promotes practices that keep AI systems within designed limits while maintaining human control. This principle requires robust human oversight mechanisms, allowing for intervention, modification, and fine-grained control over AI-driven financial processes. In practice, this means creating sophisticated review workflows, establishing clear human-in-the-loop protocols for high-stakes financial decisions, and maintaining the ability to override or modify AI recommendations when necessary. 
In the payment industry, you can apply controllability in the following ways: 
 
 Create human review workflows for high-value or unusual transactions using Amazon Augmented AI (Amazon A2I). For more details, see Automate digitization of transactional documents with human oversight using Amazon Textract and Amazon A2I. 
 Develop override mechanisms for AI-generated fraud alerts. One possible approach could be implementing a human-in-the-loop system. For an example implementation, refer to Implement human-in-the-loop confirmation with Amazon Bedrock Agents. 
 Establish clear protocols to flag and escalate AI-related decisions that impact customer financial health. This can help establish a defined path to take in the case of any discrepancy or anomalies. 
 Implement configurable AI systems that can be adjusted based on specific institutional policies. This can help make sure the AI systems are agile and flexible with ever-evolving changes, which can be configurable to steer model behavior accordingly. 
 Design user interfaces (UIs) in which users can provide context or challenge AI-driven decisions. 
 
Privacy and security: Protecting consumer information 
Given the sensitive nature of financial data, privacy and security represent a critical consideration in AI-driven payment systems. A multi-layered protection strategy might include advanced encryption protocols, rigorous data minimization techniques, and comprehensive safeguards for personally identifiable information (PII). Compliance with global data protection regulations represents a legal requirement and is also a fundamental commitment to responsibly protecting individuals‚Äô most sensitive financial information. 
In the payment industry, you can maintain privacy and security with the following methods: 
 
 Implement advanced encryption for all transaction data. Use AWS Key Management Service (AWS KMS) for encrypting data at rest and Transport Layer Security (TLS) for encrypting data in transit. 
 Implement environment segmentation for a multi-layered protection strategy. 
 Apply differential privacy techniques. For more details, see How differential privacy helps unlock insights without revealing data at the individual-level. 
 Create anonymized datasets for AI training that can‚Äôt be traced back to individual customers. 
 Develop secure multi-factor authentication systems powered by AI. This can be done using multi-factor authentication for IAM. 
 
Safety: Mitigating potential risks 
Safety in AI-driven payment systems focuses on proactively identifying and mitigating potential risks. This involves developing comprehensive risk assessment frameworks (such as NIST AI Risk Management Framework, which provides structured approaches to govern, map, measure, and manage AI risks), implementing advanced guardrails to help prevent unintended system behaviors, and creating fail-safe mechanisms that protect both payment solutions providers and users from potential AI-related vulnerabilities. The goal is to create AI systems that work well and are fundamentally reliable and trustworthy. 
In the payment industry, you can implement safety measures as follows: 
 
 Develop guardrails to help prevent unauthorized transaction patterns. One possible way is using Amazon Bedrock Guardrails. For an example solution, see Implement model-independent safety measures with Amazon Bedrock Guardrails. 
 Create AI systems that can detect and help prevent potential financial fraud in real-time. 
 Implement multi-layered risk assessment models for complex financial products. One possible method is using an Amazon SageMaker inference pipeline. 
 Design fail-safe mechanisms that can halt AI decision-making during anomalous conditions. This can be done by architecting the system to determine anomalous behavior, flagging it, and possibly adding a human in the loop for those transactions. 
 Implement red teaming and perform penetration testing to identify potential system vulnerabilities before they can be exploited. 
 
Fairness: Detect and mitigate bias 
To create a more inclusive financial landscape and promote demographic parity, fairness should be a key consideration in payments. Financial institutions are required to rigorously examine their AI systems to mitigate potential bias or discriminatory outcomes across demographic groups. This means algorithms and training data for applications such as credit scoring, loan approval, or fraud detection should be carefully calibrated and meticulously assessed for biases. 
In the payment industry, you can implement fairness in the following ways: 
 
 Assess models and data for the presence and utilization of attributes such as gender, race, or socioeconomic background to promote demographic parity. Tools such as Amazon Bedrock Evaluations or Amazon SageMaker Clarify can help evaluate and assess the application‚Äôs bias in data and model output. 
 Implement observability, monitoring, and alerts using AWS services like Amazon CloudWatch to support regulatory compliance and provide non-discriminatory opportunities across customer demographics. 
 Evaluate data used for model training for biases using tools like SageMaker Clarify to correct and mitigate disparities. 
 
These guidelines can be applied for various payment applications and processes, including fraud detection, loan approval, financial risk assessment, credit scoring, and more. 
Veracity and robustness: Promoting accuracy and reliability 
Truthful and accurate system output is an important consideration for AI in payment systems. By continuously validating AI models, organizations can make sure that financial predictions, risk assessments, and transaction analyses maintain consistent accuracy over time. To achieve robustness, AI systems must maintain performance across diverse scenarios, handle unexpected inputs, and adapt to changing financial landscapes without compromising accuracy or reliability. 
In the payment industry, you can apply robustness through the following methods: 
 
 Create AI models that maintain accuracy across diverse economic conditions. 
 Implement rigorous testing protocols that simulate various financial scenarios. For example test tools, refer to Test automation. 
 Create cross-validation mechanisms to verify AI model predictions. SageMaker provides built-in cross-validation capabilities, experiment tracking, and continuous model monitoring, and AWS Step Functions orchestrates complex validation workflows across multiple methods. For critical predictions, Amazon A2I enables human-in-the-loop validation. 
 Use Retrieval Augmented Generation (RAG) and Amazon Bedrock Knowledge Bases to improve accuracy of AI-powered payment decision systems, reducing the risk of hallucinations. 
 
Explainability: Making complex decisions understandable 
Explainability bridges the gap between complex AI algorithms and human understanding. In payments, this means developing AI systems can articulate the reasoning behind its decisions in clear, understandable terms. AI should provide insights that are meaningful and accessible to users and financial professionals explaining a risk calculation, fraud detection flag, or transaction recommendation depending on the business use case. 
In the payment industry, you can implement explainability as follows: 
 
 Generate consumer-friendly reports that break down complex financial algorithms. 
 Create interactive tools so users can explore the factors behind their financial assessments. 
 Develop visualization tools that demonstrate how AI arrives at specific financial recommendations. 
 Provide regulatory compliance-aligned documentation that explains AI model methodologies. 
 Design multilevel explanation systems that cater to both technical and non-technical audiences. 
 
Transparency: Articulate the decision-making process 
Transparency refers to providing clear, accessible, and meaningful information that helps stakeholders understand the system‚Äôs capabilities, limitations, and potential impacts. Transparency transforms AI from an opaque black box into a human understandable, communicative system. In the payments sector, this principle demands that AI-powered financial decisions be both accurate and explicable. Financial institutions should be able to evidence how credit limits are determined, why a transaction might be flagged, or how a financial risk assessment is calculated. 
In the payment industry, you can promote transparency in the following ways: 
 
 Create interactive dashboards that break down how AI calculates transaction risks. You can use services like Amazon QuickSight to build interactive dashboards and data stories. You can use SageMaker for feature importance summary or SHAP (SHapley Additive exPlanations) reports that quantify how much each input feature contributes to a model‚Äôs prediction for a specific instance. 
 Offer real-time notifications that explain why a transaction was flagged or declined. You can send notifications using Amazon Simple Notification Service (Amazon SNS). 
 Develop customer-facing tools that help users understand the factors influencing their credit scores. AI agents can provide interactive feedback about the factors involved and deliver more details to users. You can build these AI agents using Amazon Bedrock. 
 
Governance: Promoting accuracy and reliability 
Governance establishes the framework for responsible AI implementation and ongoing monitoring and management. In payments, this means creating clear structures for AI oversight, defining roles and responsibilities, and establishing processes for regular review and intervention when necessary. Effective governance makes sure AI systems operate within established responsible AI boundaries while maintaining alignment with organizational values and regulatory requirements. 
In the payment industry, you can apply governance as follows: 
 
 Implement cross-functional AI review boards with representation from legal, compliance, and ethics teams. 
 Establish clear escalation paths for AI-related decisions that require human judgment. 
 Develop comprehensive documentation of AI system capabilities, limitations, and risk profiles. 
 Create regular audit schedules to evaluate AI performance against responsible AI dimensions. 
 Design feedback mechanisms that incorporate stakeholder input into AI governance processes. 
 Maintain version control and change management protocols for AI model updates. 
 
Conclusion 
As we‚Äôve explored throughout this guide, responsible AI in the payments industry represents both a strategic imperative and competitive advantage. By embracing the core principles of controllability, privacy, safety, fairness, veracity, explainability, transparency, and governance, payment providers can build AI systems that enhance efficiency and security, and additionally foster trust with customers and regulators. In an industry where financial data sensitivity and real-time decision-making intersect with global regulatory frameworks, those who prioritize responsible AI practices will be better positioned to navigate challenges while delivering innovative solutions. We invite you to assess your organization‚Äôs current AI implementation against these principles and refer to Part 2 of this series, where we provide practical implementation strategies to operationalize responsible AI within your payment systems. 
As the payments landscape continues to evolve, organizations that establish responsible AI as a core competency will mitigate risks and build stronger customer relationships based on trust and transparency. In an industry where trust is the ultimate currency, responsible AI is a responsible choice and an important business imperative. 
To learn more about responsible AI, refer to the AWS Responsible Use of AI Guide. 
 
About the authors 
Neelam Koshiya Neelam Koshiya is principal Applied AI Architect (GenAI specialist) at AWS. With a background in software engineering, she moved organically into an architecture role. Her current focus is to help enterprise customers with their ML/ genAI journeys for strategic business outcomes. She likes to build content/mechanisms to scale to larger audience. She is passionate about innovation and inclusion. In her spare time, she enjoys reading and being outdoors. 
 Ana Gosseen Ana is a Solutions Architect at AWS who partners with independent software vendors in the public sector space. She leverages her background in data management and information sciences to guide organizations through technology modernization journeys, with particular focus on generative AI implementation. She is passionate about driving innovation in the public sector while championing responsible AI adoption. She spends her free time exploring the outdoors with her family and dog, and pursuing her passion for reading.

‚∏ª