âœ… Morning News Briefing â€“ September 25, 2025 10:44

ğŸ“… Date: 2025-09-25 10:44
ğŸ·ï¸ Tags: #briefing #ai #publichealth #digitalgov

â¸»

ğŸ§¾ Weather
â€¢ No watches or warnings in effect, Pembroke
  No watches or warnings in effect. No warnings or watches or watches in effect . Watch or warnings are no longer in effect in the U.S. No watches, warnings are in effect for the rest of the day . No watches and warnings are still in effect, but no watches are in place for the day's events . The weather is not expected to be affected by the weather .
â€¢ Current Conditions:  11.8Â°C
  Temperature: 11.8&deg;C Pressure / Tendency: 101.4 kPa falling Humidity: 98 % Humidity : 98 % Dewpoint: 11 .5&deg:C Wind: W calm km/h . Air Quality Health Index: n/a . Pembroke 6:00 AM EDT Thursday 25 September 2025 . Weather forecast: 10/11
â€¢ Thursday: Chance of showers. High 20. POP 60%
  Cloudy with 60 percent chance of showers. High 20. Humidex 25. UV index 3 or moderate. Chance of rain in the morning . Showery cloudy with low-level rainfallfallfall forecast for the day of the day . Forecast issued 5:00 AM EDT Thursday 25 September 2025 (10:00am GMT) Forecast: "Showery

ğŸŒ International News
No updates.

ğŸ Canadian News
No updates.

ğŸ‡ºğŸ‡¸ U.S. Top Stories
â€¢ Danish officials believe drone flyovers at 4 airports meant to sow fear
  Defense Minister Troels Lund Poulsen said it appeared a "professional actor" was behind the "systematic" flights . Defense Minister said a professional actor appeared to be behind the flights, without providing additional details during a news conference Thursday morning . The incident occurred in Denmark's capital, Copenhagen, on the island of Copenhagen . The U.S. State Department is investigating the incident .
â€¢ A judge ruled their firings were illegal. The government got to do it anyway
  A judge ruled the firing of thousands of federal employees was illegal . But he stopped short of ordering the government to reinstate them . The judge predicts the Supreme Court would overturn the ruling, which he predicts will be overturned . The firing was illegal, but a judge ruled it was illegal by firing thousands of government employees, but he did not order them to be re-hired . The
â€¢ Tensions are high in a Utah redistricting fight that didn't start with Trump
  Utah Republicans are redrawing congressional maps mid-decade . Unlike Texas, Utah's new maps could give Democrats more of a shot at winning a seat . Utah's maps could help Democrats win a seat in the U.S. House of Representatives in the state of Utah, which is a Republican state . The maps could also give Democrats a chance at winning more seats in Utah .
â€¢ 8 walking and biking safety tips that just might save your life
  In 2024, 7,100 pedestrians were killed on the road, and in recent years, more than 1,000 cyclists have been hit and killed annually . DO make yourself visible with bike lights. DO look up from your phone when walking. DO make sure that you don't use your phone while walking. Do make sure you're not using it when you're on the street. Do
â€¢ Strict rules can foster calm classrooms. But some students pay the price
  Indiana charter school network has won praise for its strong academics . But some students with disabilities struggle to follow the school system's discipline policies . Students with disabilities often struggle to comply with discipline policies, school officials say . The school system has been criticized for its policies on discipline and discipline for some students in the past . It has been praised for its focus on academics and social justice for students with

ğŸ§  Artificial Intelligence
No updates.

ğŸ’» Digital Strategy
â€¢ SAP's 'simplified' licensing leaves users more confused
  Business Suite nostalgia unlikely to ease customers' public cloud journey . SAP experts are doubting the enterprise software giant's message that it is simplifying licensing after the changes were discussed at the German-speaking user group conference . The changes are expected to be discussed at an SAP conference in Germany this week . The company is set to simplify licensing for the public cloud . It is not expected to ease
â€¢ EU starting registration of fingerprints and faces for short-stay foreigners
  Biometric Entry/Exit System phased in from October to 29 Schengen countries . Travelers including Britons and Americans visiting most European countries will have to register their fingerprints and faces under a system that goes live next month . The system will be phased in to 29 countries, including Britain and the U.S. It will go live in October and goes live in 29 countries across Europe .
â€¢ Empty shelves, empty coffers: Co-op pegs cyber hit at Â£80m
  Supermarket says the hack that shut down systems and emptied shelves has turned profits into losses . The Co-operative Group has revealed the cyberattack that knocked its systems offline earlier this year will leave it nursing an Â£80 million hangover . Supermarket say the hack will leave them nursing anÂ Â£80 millionÂ hangoverÂ andÂ receivingÂ a loss of profits . Super
â€¢ Check your own databases before asking to see our passport photos, Home Office tells UK cops
  Guidance follows privacy complaints over sharp increase in police searches of travel doc and visa pic libraries . Home Office has told police forces to check their own photo databases before asking it to search its libraries of passport and visa facial images . Avoid urgent requests "unless it is absolutely necessary" for police to search libraries of passports and visa images, as well as avoiding urgent requests unless it is 'absolutely
â€¢ The sweetest slice of Pi: Raspberry Pi 500+ sports mechanical keys, 16GB, and built-in SSD
  Raspberry Pi has unveiled a fully loaded version of its computer-in-a-keyboard . Features oodles of RAM, an SSD, and a clicky, mechanical keyboard . Big performance on offer, but be prepared to spend $200 HANDS ON the $200 version of the Raspberry Pi . The Raspberry Pi is a Raspberry Pi mini-computer with a touch-screen

ğŸ¥ Public Health
No updates.

ğŸ”¬ Science
â€¢ Ophthalmic care at an academic medical centre for patients who were incarcerated or in immigration detention
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
â€¢ Quality safety and disparity of an AI chatbot in managing chronic diseases: simulated patient experiments
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
â€¢ Health impacts of wildfires travel far and wide
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
â€¢ Correction: Maternity care practices supportive of breastfeeding in U.S. advanced neonatal care units, United States, 2022
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
â€¢ Content validity of the Mental Health Literacy Scale for perinatal use based on expert and patient input
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations

ğŸ§¾ Government & Policy
No updates.

ğŸ›ï¸ Enterprise Architecture & IT Governance
No updates.

ğŸ¤– AI & Emerging Tech
â€¢ Fusion power plants donâ€™t exist yet, but theyâ€™re making money anyway
  This week, Commonwealth Fusion Systems announced it has another customer for its first commercial fusion power plant, in Virginia. Eni, one of the worldâ€™s largest oil and gas companies, signed a billion-dollar deal to buy electricity from the facility.





One small detail? That reactor doesnâ€™t exist yet. Neither does the smaller reactor Commonwealth is building first to demonstrate that its tokamak design will work as intended.



This is a weird moment in fusion. Investors are pouring billions into the field to build power plants, and some companies are even signing huge agreements to purchase power from those still-nonexistent plants. All this comes before companies have actually completed a working reactor that can produce electricity. It takes money to develop a new technology, but all this funding could lead to some twisted expectations.&nbsp;



Nearly three years ago, the National Ignition Facility at Lawrence Livermore National Laboratory hit a major milestone for fusion power. With the help of the worldâ€™s most powerful lasers, scientists heated a pellet of fuel to 100 million Â°C. Hydrogen atoms in that fuel fused together, releasing more energy than the lasers put in.



It was a game changer for the vibes in fusion. The NIF experiment finally showed that a fusion reactor could yield net energy. Plasma physicistsâ€™ models had certainly suggested that it should be true, but it was another thing to see it demonstrated in real life.



But in some ways, the NIF results didnâ€™t really change much for commercial fusion. That siteâ€™s lasers used a bonkers amount of energy, the setup was wildly complicated, and the whole thing lasted a fraction of a second. To operate a fusion power plant, not only do you have to achieve net energy, but you also need to do that on a somewhat constant basis andâ€”cruciallyâ€”do it economically.



So in the wake of the NIF news, all eyes went to companies like Commonwealth, Helion, and Zap Energy. Who would be the first to demonstrate this milestone in a more commercially feasible reactor? Or better yet, who would be the first to get a power plant up and running?



So far, the answer is none of them.





To be fair, many fusion companies have made technical progress. Commonwealth has built and tested its high-temperature superconducting magnets and published research about that work. Zap Energy demonstrated three hours of continuous operation in its test system, a milestone validated by the US Department of Energy. Helion started construction of its power plant in Washington in July. (And thatâ€™s not to mention a thriving, publicly funded fusion industry in China.)&nbsp;&nbsp;



These are all important milestones, and these and other companies have seen many more. But as Ed Morse, a professor of nuclear engineering at Berkeley, summed it up to me: â€œThey donâ€™t have a reactor.â€ (He was speaking specifically about Commonwealth, but really, the same goes for the others.)



And yet, the money pours in. Commonwealth raised over $800 million in funding earlier this year. And now itâ€™s got two big customers signed on to buy electricity from this future power plant.



Why buy electricity from a reactor thatâ€™s currently little more than ideas on paper? From the perspective of these particular potential buyers, such agreements can be something of a win-win, says Adam Stein, director of nuclear energy innovation at the Breakthrough Institute.



By putting a vote of confidence behind Commonwealth, Eni could help the fusion startup get the capital it needs to actually build its plant. The company also directly invests in Commonwealth, so it stands to benefit from success. Getting a good rate on the capital needed to build the plant could also mean the electricity is ultimately cheaper for Eni, Stein says.&nbsp;



Ultimately, fusion needs a lot of money. If fossil-fuel companies and tech giants want to provide it, all the better. One concern I have, though, is how outside observers are interpreting these big commitments.&nbsp;



US Energy Secretary Chris Wright has been loud about his support for fusion and his expectations of the technology. Earlier this month, he told the BBC that it will soon power the world.



Heâ€™s certainly not the first to have big dreams for fusion, and it is an exciting technology. But despite the jaw-dropping financial milestones, this industry is still very much in development.&nbsp;



And while Wright praises fusion, the Trump administration is slashing support for other energy technologies, including wind and solar power, and spreading disinformation about their safety, cost, and effectiveness.&nbsp;



To meet the growing electricity demand and cut emissions from the power sector, weâ€™ll need a whole range of technologies. Itâ€™s a risk and a distraction to put all our hopes on an unproven energy tech when there are plenty of options that actually exist.&nbsp;



This article is from The Spark, MIT Technology Reviewâ€™s weekly climate newsletter. To receive it in your inbox every Wednesday, sign up here.
â€¢ How AI and Wikipedia have sent vulnerable languages into a doom spiral
  When Kenneth Wehr started managing the Greenlandic-language version of Wikipedia four years ago, his first act was to delete almost everything. It had to go, he thought, if it had any chance of surviving.



Wehr, whoâ€™s 26, isnâ€™t from Greenlandâ€”he grew up in Germanyâ€”but he had become obsessed with the island, an autonomous Danish territory, after visiting as a teenager. Heâ€™d spent years writing obscure Wikipedia articles in his native tongue on virtually everything to do with it. He even ended up moving to Copenhagen to study Greenlandic, a language spoken by some 57,000 mostly Indigenous Inuit people scattered across dozens of far-flung Arctic villages.&nbsp;




The Greenlandic-language edition was added to Wikipedia around 2003, just a few years after the site launched in English. By the time Wehr took its helm nearly 20 years later, hundreds of Wikipedians had contributed to it and had collectively written some 1,500 articles totaling over tens of thousands of words. It seemed to be an impressive vindication of the crowdsourcing approach that has made Wikipedia the go-to source for information online, demonstrating that it could work even in the unlikeliest places.&nbsp;



There was only one problem: The Greenlandic Wikipedia was a mirage.&nbsp;






Virtually every single article had been published by people who did not actually speak the language. Wehr, who now teaches Greenlandic in Denmark, speculates that perhaps only one or two Greenlanders had ever contributed. But what worried him most was something else: Over time, he had noticed that a growing number of articles appeared to be copy-pasted into Wikipedia by people using machine translators. They were riddled with elementary mistakesâ€”from grammatical blunders to meaningless words to more significant inaccuracies, like an entry that claimed Canada had only 41 inhabitants. Other pages sometimes contained random strings of letters spat out by machines that were unable to find suitable Greenlandic words to express themselves.Â 



â€œIt might have looked Greenlandic to [the authors], but they had no way of knowing,â€ complains Wehr. 



â€œSentences wouldnâ€™t make sense at all, or they would have obvious errors,â€ he adds. â€œAI translators are really bad at Greenlandic.â€Â Â 



What Wehr describes is not unique to the Greenlandic edition.&nbsp;



Wikipedia is the most ambitious multilingual project after the Bible: There are editions in over 340 languages, and a further 400 even more obscure ones are being developed and tested. Many of these smaller editions have been swamped with automatically translated content as AI has become increasingly accessible. Volunteers working on four African languages, for instance, estimated to MIT Technology Review that between 40% and 60% of articles in their Wikipedia editions were uncorrected machine translations. And after auditing the Wikipedia edition in Inuktitut, an Indigenous language close to Greenlandic thatâ€™s spoken in Canada, MIT Technology Review estimates that more than two-thirds of pages containing more than several sentences feature portions created this way.&nbsp;



This is beginning to cause a wicked problem. AI systems, from Google Translate to ChatGPT, learn to â€œspeakâ€ new languages by scraping huge quantities of text from the internet. Wikipedia is sometimes the largest source of online linguistic data for languages with few speakersâ€”so any errors on those pages, grammatical or otherwise, can poison the wells that AI is expected to draw from. That can make the modelsâ€™ translation of these languages particularly error-prone, which creates a sort of linguistic doom loop as people continue to add more and more poorly translated Wikipedia pages using those tools, and AI models continue to train from poorly translated pages. Itâ€™s a complicated problem, but it boils down to a simple concept: Garbage in, garbage out.&nbsp;



â€œThese models are built on raw data,â€ says Kevin Scannell, a former professor of computer science at Saint Louis University who now builds computer software tailored for endangered languages. â€œThey will try and learn everything about a language from scratch. There is no other input. There are no grammar books. There are no dictionaries. There is nothing other than the text that is inputted.â€



There isnâ€™t perfect data on the scale of this problem, particularly because a lot of AI training data is kept confidential and the field continues to evolve rapidly. But back in 2020, Wikipedia was estimated to make up more than half the training data that was fed into AI models translating some languages spoken by millions across Africa, including Malagasy, Yoruba, and Shona. In 2022, a research team from Germany that looked into what data could be obtained by online scraping even found that Wikipedia was the sole easily accessible source of online linguistic data for 27 under-resourced languages.&nbsp;



This could have significant repercussions in cases where Wikipedia is poorly writtenâ€”potentially pushing the most vulnerable languages on Earth toward the precipice as future generations begin to turn away from them.&nbsp;



â€œWikipedia will be reflected in the AI models for these languages,â€ says Trond Trosterud, a computational linguist at the University of TromsÃ¸ in Norway, who has been raising the alarm about the potentially harmful outcomes of badly run Wikipedia editions for years. â€œI find it hard to imagine it will not have consequences. And, of course, the more dominant position that Wikipedia has, the worse it will be.â€&nbsp;



Use responsibly



Automation has been built into Wikipedia since the very earliest days. Bots keep the platform operational: They repair broken links, fix bad formatting, and even correct spelling mistakes. These repetitive and mundane tasks can be automated away with little problem. There is even an army of bots that scurry around generating short articles about rivers, cities, or animals by slotting their names into formulaic phrases. They have generally made the platform better.&nbsp;




But AI is different. Anybody can use it to cause massive damage with a few clicks.&nbsp;



Wikipedia has managed the onset of the AI era better than many other websites. It has not been flooded with AI bots or disinformation, as social media has been. It largely retains the innocence that characterized the earlier internet age. Wikipedia is open and free for anyone to use, edit, and pull from, and itâ€™s run by the very same community it serves. It is transparent and easy to use. But community-run platforms live and die on the size of their communities. English has triumphed, while Greenlandic has sunk.&nbsp;




â€œWe need good Wikipedians. This is something that people take for granted. It is not magic,â€ says Amir Aharoni, a member of the volunteer Language Committee, which oversees requests to open or close Wikipedia editions. â€œIf you use machine translation responsibly, it can be efficient and useful. Unfortunately, you cannot trust all people to use it responsibly.â€&nbsp;



Trosterud has studied the behavior of users on small Wikipedia editions and says AI has empowered a subset that he terms â€œWikipedia hijackers.â€ These users can range widelyâ€”from naive teenagers creating pages about their hometowns or their favorite YouTubers to well-meaning Wikipedians who think that by creating articles in minority languages they are in some way â€œhelpingâ€ those communities.&nbsp;



â€œThe problem with them nowadays is that they are armed with Google Translate,â€ Trosterud says, adding that this is allowing them to produce much longer and more plausible-looking content than they ever could before: â€œEarlier they were armed only with dictionaries.â€&nbsp;



This has effectively industrialized the acts of destructionâ€”which affect vulnerable languages most, since AI translations are typically far less reliable for them. There can be lots of different reasons for this, but a meaningful part of the issue is the relatively small amount of source text that is available online. And sometimes models struggle to identify a language because it is similar to others, or because some, including Greenlandic and most Native American languages, have structures that make them badly suited to the way most machine translation systems work. (Wehr notes that in Greenlandic most words are agglutinative, meaning they are built by attaching prefixes and suffixes to stems. As a result, many words are extremely context specific and can express ideas that in other languages would take a full sentence.)&nbsp;



Research produced by Google before a major expansion of Google Translate rolled out three years ago found that translation systems for lower-resourced languages were generally of a lower quality than those for better-resourced ones. Researchers found, for example, that their model would often mistranslate basic nouns across languages, including the names of animals and colors. (In a statement to MIT Technology Review, Google wrote that it is â€œcommitted to meeting a high standard of quality for all 249 languagesâ€ it supports â€œby rigorously testing and improving [its] systems, particularly for languages that may have limited public text resources on the web.â€)Â 



Wikipedia itself offers a built-in editing tool called Content Translate, which allows users to automatically translate articles from one language to anotherâ€”the idea being that this will save time by preserving the references and fiddly formatting of the originals. But it piggybacks on external machine translation systems, so itâ€™s largely plagued by the same weaknesses as other machine translatorsâ€”a problem that the Wikimedia Foundation says is hard to solve. Itâ€™s up to each editionâ€™s community to decide whether this tool is allowed, and some have decided against it. (Notably, English-language Wikipedia has largely banned its use, claiming that some 95% of articles created using Content Translate failed to meet an acceptable standard without significant additional work.) But itâ€™s at least easy to tell when the program has been used; Content Translate adds a tag on the Wikipedia back end.&nbsp;



Other AI programs can be harder to monitor. Still, many Wikipedia editors I spoke with said that once their languages were added to major online translation tools, they noticed a corresponding spike in the frequency with which poor, likely machine-translated pages were created.&nbsp;



Some Wikipedians using AI to translate content do occasionally admit that they do not speak the target languages. They may see themselves as providing smaller communities with rough-cut articles that speakers can then fixâ€”essentially following the same model that has worked well for more active Wikipedia editions.&nbsp;&nbsp;




Google Translate, for instance, says the Fulfulde word for January means June, while ChatGPT says itâ€™s August or September. The programs also suggest the Fulfulde word for â€œharvestâ€ means â€œfeverâ€ or â€œwell-being,â€ among other possibilities.&nbsp;&nbsp;




But once error-filled pages are produced in small languages, there is usually not an army of knowledgeable people who speak those languages standing ready to improve them. There are few readers of these editions, and sometimes not a single regular editor.Â 



Yuet Man Lee, a Canadian teacher in his 20s, says that he used a mix of Google Translate and ChatGPT to translate a handful of articles that he had written for the English Wikipedia into Inuktitut, thinking itâ€™d be nice to pitch in and help a smaller Wikipedia community. He says he added a note to one saying that it was only a rough translation. â€œI did not think that anybody would noticeâ€ the article, he explains. â€œIf you put something out there on the smaller Wikipediasâ€”most of the time nobody does.â€Â 



But at the same time, he says, he still thought â€œsomeone might see it and fix it upâ€â€”adding that he had wondered whether the Inuktitut translation that the AI systems generated was grammatically correct. Nobody has touched the article since he created it.



Lee, who teaches social sciences in Vancouver and first started editing entries in the English Wikipedia a decade ago, says that users familiar with more active Wikipedias can fall victim to this mindset, which he terms a â€œbigger-Wikipedia arroganceâ€: When they try to contribute to smaller Wikipedia editions, they assume that others will come along to fix their mistakes. It can sometimes work. Lee says he had previously contributed several articles to Wikipedia in Tatar, a language spoken by several million people mainly in Russia, and at least one of those was eventually corrected. But the Inuktitut Wikipedia is, by comparison, a â€œbarren wasteland.â€&nbsp;



He emphasizes that his intentions had been good: He wanted to add more articles to an Indigenous Canadian Wikipedia. â€œI am now thinking that it may have been a bad idea. I did not consider that I could be contributing to a recursive loop,â€ he says. â€œIt was about trying to get content out there, out of curiosity and for fun, without properly thinking about the consequences.â€&nbsp;



&nbsp;â€œTotally, completely no futureâ€



Wikipedia is a project that is driven by wide-eyed optimism. Editing can be a thankless task, involving weeks spent bickering with faceless, pseudonymous people, but devotees put in hours of unpaid labor because of a commitment to a higher cause. It is this commitment that drives many of the regular small-language editors I spoke with. They all feared what would happen if garbage continued to appear on their pages.



Abdulkadir Abdulkadir, a 26-year-old agricultural planner who spoke with me over a crackling phone call from a busy roadside in northern Nigeria, said that he spends three hours every day fiddling with entries in his native Fulfulde, a language used mainly by pastoralists and farmers across the Sahel. â€œBut the work is too much,â€ he said.&nbsp;



Abdulkadir sees an urgent need for the Fufulde Wikipedia to work properly. He has been suggesting it as one of the few online resources for farmers in remote villages, potentially offering information on which seeds or crops might work best for their fields in a language they can understand. If you give them a machine-translated article, Abdulkadir told me, then it could â€œeasily harm them,â€ as the information will probably not be translated correctly into Fulfulde.&nbsp;



Google Translate, for instance, says the Fulfulde word for January means June, while ChatGPT says itâ€™s August or September. The programs also suggest the Fulfulde word for â€œharvestâ€ means â€œfeverâ€ or â€œwell-being,â€ among other possibilities.Â Â 



Abdulkadir said he had recently been forced to correct an article about cowpeas, a foundational cash crop across much of Africa, after discovering that it was largely illegible.&nbsp;



If someone wants to create pages on the Fulfulde Wikipedia, Abdulkadir said, they should be translated manually. Otherwise, â€œwhoever will read your articles will [not] be able to get even basic knowledge,â€ he tells these Wikipedians. Nevertheless, he estimates that some 60% of articles are still uncorrected machine translations. Abdulkadir told me that unless something important changes with how AI systems learn and are deployed, then the outlook for Fulfulde looks bleak. â€œIt is going to be terrible, honestly,â€ he said. â€œTotally, completely no future.â€Â 



Across the country from Abdulkadir, Lucy Iwuala contributes to Wikipedia in Igbo, a language spoken by several million people in southeastern Nigeria. â€œThe harm has already been done,â€ she told me, opening the two most recently created articles. Both had been automatically translated via Wikipediaâ€™s Content Translate and contained so many mistakes that she said it would have given her a headache to continue reading them. â€œThere are some terms that have not even been translated. They are still in English,â€ she pointed out. She recognized the username that had created the pages as a serial offender. â€œThis one even includes letters that are not used in the Igbo language,â€ she said.&nbsp;



Iwuala began regularly contributing to Wikipedia three years ago out of concern that Igbo was being displaced by English. It is a worry that is common to many who are active on smaller Wikipedia editions. â€œThis is my culture. This is who I am,â€ she told me. â€œThat is the essence of it all: to ensure that you are not erased.â€&nbsp;



Iwuala, who now works as a professional translator between English and Igbo, said the users doing the most damage are inexperienced and see AI translations as a way to quickly increase the profile of the Igbo Wikipedia. She often finds herself having to explain at online edit-a-thons she organizes, or over email to various error-prone editors, that the results can be the exact opposite, pushing users away: â€œYou will be discouraged and you will no longer want to visit this place. You will just abandon it and go back to the English Wikipedia.â€Â Â 



These fears are echoed by Noah Haâ€˜alilio Solomon, an assistant professor of Hawaiian language at the University of Hawaiâ€˜i. He reports that some 35% of words on some pages in the Hawaiian Wikipedia are incomprehensible. â€œIf this is the Hawaiian that is going to exist online, then it will do more harm than anything else,â€ he says.&nbsp;



Hawaiian, which was teetering on the verge of extinction several decades ago, has been undergoing a recovery effort led by Indigenous activists and academics. Seeing such poor Hawaiian on such a widely used platform as Wikipedia is upsetting to Haâ€˜alilio Solomon.&nbsp;



â€œIt is painful, because it reminds us of all the times that our culture and language has been appropriated,â€ he says. â€œWe have been fighting tooth and nail in an uphill climb for language revitalization. There is nothing easy about that, and this can add extra impediments. People are going to think that this is an accurate representation of the Hawaiian language.â€&nbsp;



The consequences of all these Wikipedia errors can quickly become clear. AI translators that have undoubtedly ingested these pages in their training data are now assisting in the production, for instance, of error-strewn AI-generated books aimed at learners of languages as diverse as Inuktitut and Cree, Indigenous languages spoken in Canada, and Manx, a small Celtic language spoken on the Isle of Man. Many of these have been popping up for sale on Amazon. â€œIt was just complete nonsense,â€ says Richard Compton, a linguist at the University of Quebec in Montreal, of a volume he reviewed that had purported to be an introductory phrasebook for Inuktitut.&nbsp;



Rather than making minority languages more accessible, AI is now creating an ever expanding minefield for students and speakers of those languages to navigate. â€œIt is a slap in the face,â€ Compton says. He worries that younger generations in Canada, hoping to learn languages in communities that have fought uphill battles against discrimination to pass on their heritage, might turn to online tools such as ChatGPT or phrasebooks on Amazon and simply make matters worse. â€œIt is fraud,â€ he says.



A race against time



According to UNESCO, a language is declared extinct every two weeks. But whether the Wikimedia Foundation, which runs Wikipedia, has an obligation to the languages used on its platform is an open question. When I spoke to Runa Bhattacharjee, a senior director at the foundation, she said that it was up to the individual communities to make decisions about what content they wanted to exist on their Wikipedia. â€œUltimately, the responsibility really lies with the community to see that there is no vandalism or unwanted activity, whether through machine translation or other means,â€ she said. Usually, Bhattacharjee added, editions were considered for closure only if a specific complaint was raised about them.&nbsp;



But if there is no active community, how can an edition be fixed or even have a complaint raised?&nbsp;



Bhattacharjee explained that the Wikimedia Foundation sees its role in such cases as about maintaining the Wikipedia platform in case someone comes along to revive it: â€œIt is the space that we provide for them to grow and develop. That is where we are at.â€&nbsp;&nbsp;&nbsp;



Inari Saami, spoken in a single remote community in northern Finland, is a poster child for how people can take good advantage of Wikipedia. The language was headed toward extinction four decades ago; there were only four children who spoke it. Their parents created the Inari Saami Language Association in a last-ditch bid to keep it going. The efforts worked. There are now several hundred speakers, schools that use Inari Saami as a medium of instruction, and 6,400 Wikipedia articles in the language, each one copy-edited by a fluent speaker.&nbsp;



This success highlights how Wikipedia can indeed provide small and determined communities with a unique vehicle to promote their languagesâ€™ preservation. â€œWe donâ€™t care about quantity. We care about quality,â€ says Fabrizio Brecciaroli, a member of the Inari Saami Language Association. â€œWe are planning to use Wikipedia as a repository for the written language. We need to provide tools that can be used by the younger generations. It is important for them to be able to use Inari Saami digitally.â€&nbsp;





This has been such a success that Wikipedia has been integrated into the curriculum at the Inari Saamiâ€“speaking schools, Brecciaroli adds. He fields phone calls from teachers asking him to write up simple pages on topics from tornadoes to Saami folklore. Wikipedia has even offered a way to introduce words into Inari Saami. â€œWe have to make up new words all the time,â€ Brecciaroli says. â€œYoung people need them to speak about sports, politics, and video games. If they are unsure how to say something, they now check Wikipedia.â€



Wikipedia is a monumental intellectual experiment. Whatâ€™s happening with Inari Saami suggests that with maximum care, it can work in smaller languages. â€œThe ultimate goal is to make sure that Inari Saami survives,â€ Brecciaroli says. â€œIt might be a good thing that there isnâ€™t a Google Translate in Inari Saami.â€&nbsp;



That may be trueâ€”though large language models like ChatGPT can be made to translate phrases into languages that more traditional machine translation tools do not offer. Brecciaroli told me that ChatGPT isnâ€™t great in Inari Saami but that the quality varies significantly depending on what you ask it to do; if you ask it a question in the language, then the answer will be filled with words from Finnish and even words it invents. But if you ask it something in English, Finnish, or Italian and then ask it to reply in Inari Saami, it will perform better.Â 



In light of all this, creating as much high-quality content online as can possibly be written becomes a race against time. â€œChatGPT only needs a lot of words,â€ Brecciaroli says. â€œIf we keep putting good material in, then sooner or later, we will get something out. That is the hope.â€ This is an idea supported by multiple linguists I spoke withâ€”that it may be possible to end the â€œgarbage in, garbage outâ€ cycle. (OpenAI, which operates ChatGPT, did not respond to a request for comment.)



Still, the overall problem is likely to grow and grow, since many languages are not as lucky as Inari Saamiâ€”and their AI translators will most likely be trained on more and more AI slop. Wehr, unfortunately, seems far less optimistic about the future of his beloved Greenlandic.&nbsp;



Since deleting much of the Greenlandic-language Wikipedia, he has spent years trying to recruit speakers to help him revive it. He has appeared in Greenlandic media and made social media appeals. But he hasnâ€™t gotten much of a response; he says it has been demoralizing.&nbsp;



â€œThere is nobody in Greenland who is interested in this, or who wants to contribute,â€ he says. â€œThere is completely no point in it, and that is why it should be closed.â€&nbsp;



Late last year, he began a process requesting that the Wikipedia Language Committee shut down the Greenlandic-language edition. Months of bitter debate followed between dozens of Wikipedia bureaucrats; some seemed to be surprised that a superficially healthy-seeming edition could be gripped by so many problems.&nbsp;



Then, earlier this month, Wehrâ€™s proposal was accepted: Greenlandic Wikipedia is set to be shuttered, and any articles that remain will be moved into the Wikipedia Incubator, where new language editions are tested and built. Among the reasons cited by the Language Committee is the use of AI tools, which have â€œfrequently produced nonsense that could misrepresent the language.â€&nbsp;&nbsp;&nbsp;



Nevertheless, it may be too lateâ€”mistakes in Greenlandic already seem to have become embedded in machine translators. If you prompt either Google Translate or ChatGPT to do something as simple as count to 10 in proper Greenlandic, neither program can deliver.&nbsp;



Jacob Judah is an investigative journalist based in London.&nbsp;
â€¢ Roundtables: The Future of Birth Control
  Kevin Eisenfrats is one of the MIT Technology Review 2025 Innovators Under 35 . His company, Contraline, is working toward testing new birth control options for men . He is co-founder and CEO of the company, and MIT Tech Review's executive editor, Amy Nordrum . The list of the top 25 Innovator of the Year is published on September 24, 2025 .
â€¢ The Download: accidental AI relationships, and the future of contraception
  This is today&#8217;s edition of&nbsp;The Download,&nbsp;our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.







Itâ€™s surprisingly easy to stumble into a relationship with an AI chatbot



The news: The first large-scale computational analysis of the Reddit community r/MyBoyfriendIsAI, which is dedicated to discussing AI relationships, found that many people formed those relationships unintentionally while using AI for other purposes. In fact, only 6.5% of them said theyâ€™d deliberately sought out an AI companion.&nbsp;



Why it matters: The study found that AI companionship provides vital support for some but exacerbates underlying problems for others. This means itâ€™s hard to take a one-size-fits-all approach to user safety. Read the full story.



â€”Rhiannon Williams







Join us at 1.30pm ET today to learn about the future of birth control&nbsp;



Conversations around birth control usually focus on women, but Kevin Eisenfrats, one of the MIT Technology Review 2025 Innovators Under 35, is working to change that. His company, Contraline, is working toward testing new birth control options for men. Join us for an exclusive subscribers-only Roundtable interview to hear Kevin in conversation with our executive editor Amy Nordrum at 1.30 ET today.&nbsp;







MIT Technology Review Narrated: Whatâ€™s next for AI and math



The last year has seen rapid progress in the ability of large language models to tackle math at high school level and beyond. Is AI closing in on human mathematicians?Â 



This story is the latest to be turned into a MIT Technology Review Narrated podcast, which we publish every week on Spotify and Apple Podcasts. Just navigate to MIT Technology Review Narrated on either platform, and follow us to listen to all our new episodes as theyâ€™re released.







The must-reads



Iâ€™ve combed the internet to find you todayâ€™s most fun/important/scary/fascinating stories about technology.



1 Secret Service agents dismantled a giant operation to cripple cell networksThey say itâ€™s likely it was intended to be used for scams. (Wired $)



2 Welcome to the new era of fragmented US vaccine policiesThe federal government is abdicating responsibility for public health. Who will fill the void? (New Yorker $)+ Why US federal health agencies are abandoning mRNA vaccines. (MIT Technology Review)3 European defense leaders are discussing building a â€˜drone wallâ€™&nbsp;Theyâ€™re scrambling to catch up as Russian incursions into their territory increase. (ABC)4 How will we know if weâ€™ve reached artificial general intelligence?Thatâ€™s the multi-billion dollar questionâ€”but thereâ€™s no clear answer. (IEEE Spectrum)+ Experts donâ€™t even agree on what AI is to begin with, never mind AGI. (MIT Technology Review)



5 Robot umpires are coming to baseballâ€™s major leagues next year&nbsp;Humans will still be in charge of calling balls and strikes, but tech will help to judge appealed decisions. (AP)6 AIâ€™s energy needs are being overstatedAnd that could lock us into unnecessary, costly fossil fuel projects. (The Verge)+ Four reasons to be optimistic about AIâ€™s energy usage. (MIT Technology Review)7 Extreme drought is set to become a lot more commonplaceGovernments need to do a lot more to prepare. (Gizmodo)&nbsp;



8 AI is coming for subtitle writersâ€™ jobs&nbsp;But their work is harder to replace than you might think. (The Guardian)+ â€˜Workslopâ€™ is slowing everything down. (Harvard Business Review)+ And, to add to the problem, AI systems may never be secure. (The Economist $)9 How epigenetics could help save wildlife from extinctionIt could allow scientists to detect accelerated aging before an animal population starts to visibly collapse. (Knowable)+ Aging clocks aim to predict how long youâ€™ll live. (MIT Technology Review)10 TikTok is getting introduced to the concept of the rapture&nbsp;Which is due today, according to some. If so, itâ€™s been great knowing you. Good luck! (The Guardian)







Quote of the day



â€œEverybody has a backup.â€



â€”Stella Li, executive vice president at BYD, tells CNBC the company has contingency plans in case Beijing orders it to stop using Nvidia chips.







One more thing



GETTY IMAGES




This app is helping workers reclaim millions in lost wages



Reclamo, a new web app, helps immigrant workers who have experienced wage theft. It guides them through assembling case details, and ultimately produces finished legal claims that can be filed instantly. A process that would otherwise take multiple meetings with an attorney can now be done within an hour.A significant amount of wage theft targets immigrants, both legal and undocumented, in part because of communication barriers and their perceived lack of power or legal recourse. But the app is already making a differenceâ€”helping workers to reclaim $1 million in lost wages since it started beta testing in October 2022. Read the full story.



â€”Patrick Sisson







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)



+ Itâ€™s Fat Bear Week! Who gets your vote this year?+ Learn about Lord Woodbine, the forgotten sixth Beatle.Â + There are some truly wild and wacky recipes in this Medieval Cookery collection. Venison porridge, anyone?Â + Pessimism about technology is as old as technology itself, as this archive shows.
â€¢ Trump is pushing leucovorin as a new treatment for autism. What is it?
  MIT Technology Review Explains: Let our writers untangle the complex, messy world of technology to help you understand whatâ€™s coming next. You can read more from the series here.



At a press conference on Monday, President Trump announced that his administration was taking action to address â€œthe meteoric rise in autism.â€ He suggested that childhood vaccines and acetaminophen, the active ingredient in Tylenol, are to blame for the increasing prevalence and advised pregnant women against taking the medicine. â€œDonâ€™t take Tylenol,â€ he said. â€œFight like hell not to take it.â€Â 



The presidentâ€™s&nbsp; assertions left many scientists and health officials perplexed and dismayed. The notion that childhood vaccines cause autism has been thoroughly debunked.&nbsp;



â€œThere have been many, many studies across many, many children that have led science to rule out vaccines as a significant causal factor in autism,â€ says James McPartland, a child psychologist and director of the Yale Center for Brain and Mind Health in New Haven, Connecticut. 



And although some studies suggest a link between Tylenol and autism, the most rigorous have failed to find a connection.&nbsp;



The administration also announced that the Food and Drug Administration would work to make a medication called leucovorin available as a treatment for children with autism. Some small studies do suggest the drug has promise, but â€œthose are some of the most preliminary treatment studies that we have,â€ says Matthew Lerner, a psychologist at Drexel Universityâ€™s A.J. Drexel Autism Institute in Philadelphia. â€œThis is not one I would say that the research suggests is ready for fast-tracking.â€Â 



The press conference â€œalarms us researchers who committed our entire careers to better understanding autism,â€ said the Coalition for Autism Researchers, a group of more than 250 scientists, in a statement.



â€œThe data cited do not support the claim that Tylenol causes autism and leucovorin is a cure, and only stoke fear and falsely suggest hope when there is no simple answer.â€



Thereâ€™s a lot to unpack here. Letâ€™s begin.&nbsp;



Has there been a â€œmeteoric riseâ€ in autism?



Not in the way the president meant. Sure, the prevalence of autism has grown, from about 1 in 500 children in 1995 to 1 in 31 today. But thatâ€™s due, in large part, to diagnostic changes. The latest iteration of the Diagnostic and Statistical Manual of Mental Illnesses, published in 2013, grouped five previously separate diagnoses into a single diagnosis of autism spectrum disorder (ASD).



That meant that more people met the criteria for an autism diagnosis. Lerner points out that there is also far more awareness of the condition today than there was several decades ago. â€œThereâ€™s autism representation in the media,â€ he says. â€œThere are plenty of famous people in the news and finance and in business and in Hollywood who are publicly, openly autistic.â€



Is Tylenol a contributor to autism?Â 



Some studies have found an association between the use of acetaminophen in pregnancy and autism in children. In these studies, researchers asked women about past acetaminophen use during pregnancy and then assessed whether children of the women who took the medicine were more likely to develop autism than children of women who didnâ€™t take it.&nbsp;



These kinds of epidemiological studies are tricky to interpret because theyâ€™re prone to bias. For example, women who take acetaminophen during pregnancy may do so because they have an infection, a fever, or an autoimmune disease. 



â€œMany of these underlying reasons could themselves be causes of autism,â€ says Ian Douglas, an epidemiologist at the London School of Hygiene and Tropical Medicine. Itâ€™s also possible women with a higher genetic predisposition for autism have other medical conditions that make them more likely to take acetaminophen.&nbsp;



Two studies attempted to account for these potential biases by looking at siblings whose mothers had used acetaminophen during only one of the pregnancies. The largest is a 2024 study that looked at nearly 2.5 million children born between 1915 and 2019 in Sweden. The researchers initially found a slightly increased risk of autism and ADHD in children of the women who took acetaminophen, but when they conducted a sibling analysis, the association disappeared.&nbsp;&nbsp;



Rather, scientists have long known that autism is largely genetic. Twin studies suggest that 60% to 90% of autism risk can be attributed to your genes. However, environmental factors appear to play a role too. That â€œdoesnâ€™t necessarily mean toxins in the environment,â€ Lerner says. In fact, one of the strongest environmental predictors of autism is paternal age. Autism rates seem to be higher when a childâ€™s father is older than 40.



So should someone who is pregnant&nbsp; avoid Tylenol just to be safe?



No. Acetaminophen is the only over-the-counter pain reliever that is deemed safe to take during pregnancy, and women should take it if they need it. The American College of Obstetricians and Gynecologists (ACOG) supports the use of acetaminophen in pregnancy â€œwhen taken as needed, in moderation, and after consultation with a doctor.â€&nbsp;



â€œThereâ€™s no downside in not taking it,â€ Trump said at the press conference. But high fevers during pregnancy can be dangerous. â€œThe conditions people use acetaminophen to treat during pregnancy are far more dangerous than any theoretical risks and can create severe morbidity and mortality for the pregnant person and the fetus,â€ ACOG president Steven Fleischman said in a statement.



What about this new treatment for autism? Does it work?&nbsp;



The medication is called leucovorin. Itâ€™s also known as folinic acid; like folic acid, itâ€™s a form of folate, a B vitamin found in leafy greens and legumes. The drug has been used for years to counteract the side effects of some cancer medications and as a treatment for anemia.Â 



Researchers have known for decades that folate plays a key role in the fetal development of the brain and spine. Women who donâ€™t get enough folate during pregnancy have a greater risk of having babies with neural tube defects like spina bifida. Because of this, many foods are fortified with folic acid, and the CDC recommends that women take folic acid supplements during pregnancy. â€œIf you are pregnant and youâ€™re taking maternal prenatal vitamins, thereâ€™s a good chance it has folate already,â€ Lerner says.



â€œThe idea that a significant proportion of autistic people have autism because of folate-related difficulties is not a well established or widely accepted premise,â€ says McPartland.



However, in the early 2000s, researchers in Germany identified a small group of children who developed neurodevelopmental symptoms because of a folate deficiency. â€œThese kids are born pretty normal at birth,â€ says Edward Quadros, a biologist at SUNY Downstate Health Sciences University in Brooklyn, New York. But after a year or two, â€œthey start developing a neurologic presentation very similar to autism,â€ he says. When the researchers gave these children folinic acid, some of their symptoms improved, especially in children younger than six.Â 



Because the children had low levels of folate in the fluid that surrounds the spine and brain but normal folate levels in the blood, the researchers posited that the problem was the transport of folate from the blood to that fluid. Research by Quadros and other scientists suggested that the deficiency was the result of an autoimmune response. Children develop antibodies against the receptors that help transport folate, and those antibodies block folate from crossing the blood-brain barrier. High doses of folinic acid, however, activate a second transporter that allows folate in, Quadros says.Â 



There are also plenty of individual anecdotes suggesting that leucovorin works. But the medicine has only been tested as a treatment for autism in four small trials that used different doses and measured different outcomes. The evidence that it can improve symptoms of autism is â€œweak,â€ according to the Coalition of Autism Scientists. â€œA much higher standard of science would be needed to determine if leucovorin is an effective and safe treatment for autism,â€ the researchers said in a statement.

ğŸ”’ Cybersecurity & Privacy
â€¢ Feds Tie â€˜Scattered Spiderâ€™ Duo to $115M in Ransoms
  U.S. prosecutors last week levied criminal hacking charges against 19-year-old U.K. national Thalha Jubair for allegedly being a core member of Scattered Spider, a prolific cybercrime group blamed for extorting at least $115 million in ransom payments from victims. The charges came as Jubair and an alleged co-conspirator appeared in a London court to face accusations of hacking into and extorting several large U.K. retailers, the London transit system, and healthcare providers in the United States.
At a court hearing last week, U.K. prosecutors laid out a litany of charges against Jubair and 18-year-old Owen Flowers, accusing the teens of involvement in an August 2024 cyberattack that crippled Transport for London, the entity responsible for the public transport network in the Greater London area.
A court artist sketch of Owen Flowers (left) and Thalha Jubair appearing at Westminster Magistrates&#8217; Court last week. Credit: Elizabeth Cook, PA Wire.
On July 10, 2025, KrebsOnSecurity reported that Flowers and Jubair had been arrested in the United Kingdom in connection with recent Scattered Spider ransom attacks against the retailers Marks &amp; Spencer and Harrods, and the British food retailer Co-op Group.
That story cited sources close to the investigation saying Flowers was the Scattered Spider member who anonymously gave interviews to the media in the days after the group&#8217;s September 2023 ransomware attacks disrupted operations at Las Vegas casinos operated by MGM Resorts and Caesars Entertainment.
The story also noted that Jubair&#8217;s alleged handles on cybercrime-focused Telegram channels had far lengthier rap sheets involving some of the more consequential and headline-grabbing data breaches over the past four years. What follows is an account of cybercrime activities that prosecutors have attributed to Jubair&#8217;s alleged hacker handles, as told by those accounts in posts to public Telegram channels that are closely monitored by multiple cyber intelligence firms.
EARLY DAYS (2021-2022)
Jubair is alleged to have been a core member of the LAPSUS$Â cybercrime group thatÂ broke into dozens of technology companies beginning in late 2021, stealing source code and other internal data from tech giants includingÂ Microsoft,Â Nvidia,Â Okta,Â Rockstar Games,Â Samsung,Â T-Mobile, andÂ Uber.
That is, according to the former leader of the now-defunct LAPSUS$. In April 2022, KrebsOnSecurity published internal chat records taken from a server that LAPSUS$ used, and those chats indicate Jubair was working with the group using the nicknames AmtrakÂ andÂ Asyntax. In the middle of the gang&#8217;s cybercrime spree, Asyntax told the LAPSUS$ leader not to share T-Mobileâ€™s logo in images sent to the group because heâ€™d been previously busted for SIM-swapping and his parents would suspect he was back at it again.
The leader of LAPSUS$ responded by gleefully posting Asyntax&#8217;s real name, phone number, and other hacker handles into a public chat room on Telegram:

In March 2022, the leader of the LAPSUS$ data extortion group exposed Thalha Jubairâ€™s name and hacker handles in a public chat room on Telegram.

That story about the leaked LAPSUS$ chats also connected Amtrak/Asyntax to several previous hacker identities, including â€œEverlynn,â€ who in April 2021 began offering a cybercriminal service that sold fraudulent â€œemergency data requestsâ€Â targeting the major social media and email providers.
In these so-called &#8220;fake EDR&#8221; schemes, the hackers compromise email accounts tied to police departments and government agencies, and then send unauthorized demands for subscriber data (e.g. username, IP/email address), while claiming the information being requested canâ€™t wait for a court order because it relates to an urgent matter of life and death.
The roster of the now-defunct &#8220;Infinity Recursion&#8221; hacking team, which sold fake EDRs between 2021 and 2022. The founder &#8220;Everlynn&#8221; has been tied to Jubair. The member listed as â€œPeterâ€ became the leader of LAPSUS$ who would later post Jubairâ€™s name, phone number and hacker handles into LAPSUS$â€™s chat channel.

EARTHTOSTAR
Prosecutors in New Jersey last week alleged Jubair was part of a threat group variously known as Scattered Spider, 0ktapus, and UNC3944, and that he used the nicknames EarthtoStar, Brad, Austin, and Austistic.
Beginning in 2022, EarthtoStar co-ran a bustling Telegram channel called Star Chat, which was home to a prolific SIM-swapping group that relentlessly used voice- and SMS-based phishing attacks to steal credentials from employees at the major wireless providers in the U.S. and U.K.
Jubair allegedly used the handle &#8220;Earth2Star,&#8221; a core member of a prolific SIM-swapping group operating in 2022. This ad produced by the group lists various prices for SIM swaps.
The group would then use that access to sell a SIM-swapping service that could redirect a target&#8217;s phone number to a device the attackers controlled, allowing them to intercept the victim&#8217;s phone calls and text messages (including one-time codes). Members of Star Chat targeted multiple wireless carriers with SIM-swapping attacks, but they focused mainly on phishing T-Mobile employees.
In February 2023, KrebsOnSecurity scrutinized more than seven months of these SIM-swapping solicitations on Star Chat, which almost daily peppered the public channel with &#8220;Tmo up!&#8221; and &#8220;Tmo down!&#8221; notices indicating periods wherein the group claimed to have active access to T-Mobile&#8217;s network.
A redacted receipt from Star Chat&#8217;s SIM-swapping service targeting a T-Mobile customer after the group gained access to internal T-Mobile employee tools.
The data showed that Star Chat &#8212; along with two other SIM-swapping groups operating at the same time &#8212; collectively broke into T-Mobile over a hundred times in the last seven months of 2022. However, Star Chat was by far the most prolific of the three, responsible for at least 70 of those incidents.
The 104 days in the latter half of 2022 in which different known SIM-swapping groups claimed access to T-Mobile employee tools. Star Chat was responsible for a majority of these incidents. Image: krebsonsecurity.com.
A review of EarthtoStar&#8217;s messages on Star Chat as indexed by the threat intelligence firm Flashpoint shows this person also sold &#8220;AT&amp;T email resets&#8221; and AT&amp;T call forwarding services for up to $1,200 per line. EarthtoStar explained the purpose of this service in post on Telegram:
&#8220;Ok people are confused, so you know when u login to chase and it says &#8216;2fa required&#8217; or whatever the fuck, well it gives you two options, SMS or Call. If you press call, and I forward the line to you then who do you think will get said call?&#8221;
New Jersey prosecutors allege Jubair also was involved in a mass SMS phishing campaign during the summer of 2022 that stole single sign-on credentials from employees at hundreds of companies. The text messages asked users to click a link and log in at a phishing page that mimicked their employerâ€™s Okta authentication page, saying recipients needed to review pending changes to their upcoming work schedules.
The phishing websites used a Telegram instant message bot to forward any submitted credentials in real-time, allowing the attackers to use the phished username, password and one-time code to log in as that employee at the real employer website.
That weeks-long SMS phishing campaign led to intrusions and data thefts at more than 130 organizations, including LastPass, DoorDash, Mailchimp, Plex and Signal.
A visual depiction of the attacks by the SMS phishing group known as 0ktapus, ScatterSwine, and Scattered Spider. Image: Amitai Cohen twitter.com/amitaico.
DA, COMRADE
EarthtoStar&#8217;s group Star Chat specialized in phishing their way into business process outsourcing (BPO) companies that provide customer support for a range of multinational companies, including a number of the world&#8217;s largest telecommunications providers. In May 2022, EarthtoStar posted to the Telegram channel &#8220;Frauwudchat&#8221;:
&#8220;Hi, I am looking for partners in order to exfiltrate data from large telecommunications companies/call centers/alike, I have major experience in this field, [including] a massive call center which houses 200,000+ employees where I have dumped all user credentials and gained access to the [domain controller] + obtained global administrator I also have experience with REST API&#8217;s and programming. I have extensive experience with VPN, Citrix, cisco anyconnect, social engineering + privilege escalation. If you have any Citrix/Cisco VPN or any other useful things please message me and lets work.&#8221;
At around the same time in the Summer of 2022, at least two different accounts tied to Star Chat &#8212; &#8220;RocketAce&#8221; and &#8220;Lopiu&#8221; &#8212; introduced the group&#8217;s services to denizens of the Russian-language cybercrime forum Exploit, including:
-SIM-swapping services targeting Verizon and T-Mobile customers;
-Dynamic phishing pages targeting customers of single sign-on providers like Okta;
-Malware development services;
-The sale of extended validation (EV) code signing certificates.
The user &#8220;Lopiu&#8221; on the Russian cybercrime forum Exploit advertised many of the same unique services offered by EarthtoStar and other Star Chat members. Image source: ke-la.com.
These two accounts on Exploit created multiple sales threads in which they claimed administrative access to U.S. telecommunications providers and asked other Exploit members for help in monetizing that access. In June 2022, RocketAce, which appears to have been just one of EarthtoStar&#8217;s many aliases, posted to Exploit:
Hello. I have access to a telecommunications company&#8217;s citrix and vpn. I would like someone to help me break out of the system and potentially attack the domain controller so all logins can be extracted we can discuss payment and things leave your telegram in the comments or private message me ! Looking for someone with knowledge in citrix/privilege escalation
On Nov. 15, 2022, EarthtoStar posted to their Star Sanctuary Telegram channel that they were hiring malware developers with a minimum of three years of experience and the ability to develop rootkits, backdoors and malware loaders.
&#8220;Optional: Endorsed by advanced APT Groups (e.g. Conti, Ryuk),&#8221; the ad concluded, referencing two of Russia&#8217;s most rapacious and destructive ransomware affiliate operations. &#8220;Part of a nation-state / ex-3l (3 letter-agency).&#8221;
2023-PRESENT DAY
The Telegram and Discord chat channels wherein Flowers and Jubair allegedly planned and executed their extortion attacks are part of a loose-knit network known as the Com, an English-speaking cybercrime community consisting mostly of individuals living in the United States, the United Kingdom, Canada and Australia.
Many of these Com chat servers have hundreds to thousands of members each, and some of the more interesting solicitations on these communities are job offers for in-person assignments and tasks that can be found if one searches for posts titled, â€œIf you live near,â€ or â€œIRL jobâ€ â€” short for â€œin real lifeâ€ job.
These &#8220;violence-as-a-service&#8221; solicitations typically involve â€œbrickings,â€ where someone is hired to toss a brick through the window at a specified address. Other IRL jobs for hire include tire-stabbings, molotov cocktail hurlings, drive-by shootings, and even home invasions. The people targeted by these services are typically other criminals within the community, but it&#8217;s not unusual to see Com members asking others for help in harassing or intimidating security researchers and even the very law enforcement officers who are investigating their alleged crimes.
It remains unclear what precipitated this incident or what followed directly after, but on January 13, 2023, a Star Sanctuary account used by EarthtoStar solicited the home invasion of a sitting U.S. federal prosecutor from New York. That post included a photo of the prosecutor taken from the Justice Department&#8217;s website, along with the message:
&#8220;Need irl niggas, in home hostage shit no fucking pussies no skinny glock holding 100 pound niggas either&#8221;
Throughout late 2022 and early 2023, EarthtoStar&#8217;s alias &#8220;Brad&#8221; (a.k.a. &#8220;Brad_banned&#8221;) frequently advertised Star Chat&#8217;s malware development services, including custom malicious software designed to hide the attacker&#8217;s presence on a victim machine:
We can develop KERNEL malware which will achieve persistence for a long time,
bypass firewalls and have reverse shell access.
This shit is literally like STAGE 4 CANCER FOR COMPUTERS!!!
Kernel meaning the highest level of authority on a machine.
This can range to simple shells to Bootkits.
Bypass all major EDR&#8217;s (SentinelOne, CrowdStrike, etc)
Patch EDR&#8217;s scanning functionality so it&#8217;s rendered useless!
Once implanted, extremely difficult to remove (basically impossible to even find)
Development Experience of several years and in multiple APT Groups.
Be one step ahead of the game. Prices start from $5,000+. Message @brad_banned to get a quote
In September 2023 , both MGM Resorts and Caesars Entertainment suffered ransomware attacks at the hands of a Russian ransomware affiliate program known as ALPHV and BlackCat. Caesars reportedly paid a $15 million ransom in that incident.
Within hours of MGM publicly acknowledging the 2023 breach, members of Scattered Spider were claiming credit and telling reporters they&#8217;d broken in by social engineering a third-party IT vendor. At a hearing in London last week, U.K. prosecutors told the court Jubair was found in possession of more than $50 million in ill-gotten cryptocurrency, including funds that were linked to the Las Vegas casino hacks.
The Star Chat channel was finally banned by Telegram on March 9, 2025. But U.S. prosecutors say Jubair and fellow Scattered Spider members continued their hacking, phishing and extortion activities up until September 2025.
In April 2025, the Com was buzzing about the publication of &#8220;The Com Cast,&#8221; a lengthy screed detailing Jubair&#8217;s alleged cybercriminal activities and nicknames over the years. This account included photos and voice recordings allegedly of Jubair, and asserted that in his early days on the Com Jubair used the nicknames Clark and Miku (these are both aliases used by Everlynn in connection with their fake EDR services).
Thalha Jubair (right), without his large-rimmed glasses, in an undated photo posted in The Com Cast.
More recently, the anonymous Com Cast author(s) claimed, Jubair had used the nickname &#8220;Operator,&#8221; which corresponds to a Com member who ran an automated Telegram-based doxing service that pulled consumer records from hacked data broker accounts. That public outing came after Operator allegedly seized control over the Doxbin, a long-running and highly toxic community that is used to &#8220;dox&#8221; or post deeply personal information on people.
&#8220;Operator/Clark/Miku: A key member of the ransomware group Scattered Spider, which consists of a diverse mix of individuals involved in SIM swapping and phishing,&#8221; the Com Cast account stated. &#8220;The group is an amalgamation of several key organizations, including Infinity Recursion (owned by Operator), True Alcorians (owned by earth2star), and Lapsus, which have come together to form a single collective.&#8221;
The New Jersey complaint (PDF) alleges Jubair and other Scattered Spider members committed computer fraud, wire fraud, and money laundering in relation to at least 120 computer network intrusions involving 47 U.S. entities between May 2022 and September 2025. The complaint alleges the group&#8217;s victims paid at least $115 million in ransom payments.
U.S. authorities say they traced some of those payments to Scattered Spider to an Internet server controlled by Jubair. The complaint states that a cryptocurrency wallet discovered on that server was used to purchase several gift cards, one of which was used at a food delivery company to send food to his apartment. Another gift card purchased with cryptocurrency from the same server was allegedly used to fund online gaming accounts under Jubair&#8217;s name. U.S. prosecutors said that when they seized that server they also seized $36 million in cryptocurrency.
The complaint also charges Jubair with involvement in a hacking incident in January 2025 against the U.S. courts system that targeted a U.S. magistrate judge overseeing a related Scattered Spider investigation. That other investigation appears to have been the prosecution of Noah Michael Urban, a 20-year-old Florida man charged in November 2024 by prosecutors in Los Angeles as one of five alleged Scattered Spider members.
Urban pleaded guilty in April 2025 to wire fraud and conspiracy charges, and in August he was sentenced to 10 years in federal prison. Speaking with KrebsOnSecurity from jail after his sentencing, Urban asserted that the judge case gave him more time than prosecutors requested because he was mad that Scattered Spider hacked his email account.
Noah &#8220;Kingbob&#8221; Urban, posting to Twitter/X around the time of his sentencing on Aug. 20.
AÂ court transcript (PDF) from a status hearing in February 2025 shows Urban was telling the truth about the hacking incident that happened while he was in federal custody. The judge told attorneys for both sides that a co-defendant in the California case was trying to find out about Mr. Urbanâ€™s activity in the Florida case, and that the hacker accessed the account by impersonating a judge over the phone and requesting a password reset.
Allison Nixon is chief research officer at the New York based security firm Unit 221B, and easily one of the world&#8217;s leading experts on Com-based cybercrime activity. Nixon said the core problem with legally prosecuting well-known cybercriminals from the Com has traditionally been that the top offenders tend to be under the age of 18, and thus difficult to charge under federal hacking statutes.
In the United States, prosecutors typically wait until an underage cybercrime suspect becomes an adult to charge them. But until that day comes, she said, Com actors often feel emboldened to continue committing &#8212; and very often bragging about &#8212; serious cybercrime offenses.
&#8220;Here we have a special category of Com offenders that effectively enjoy legal immunity,&#8221; Nixon told KrebsOnSecurity. &#8220;Most get recruited to Com groups when they are older, but of those that join very young, such as 12 or 13, they seem to be the most dangerous because at that age they have no grounding in reality and so much longevity before they exit their legal immunity.â€
Nixon said U.K. authorities face the same challenge when they briefly detain and search the homes of underage Com suspects: Namely, the teen suspects simply go right back to their respective cliques in the Com and start robbing and hurting people again the minute they&#8217;re released.
Indeed, the U.K. court heard from prosecutors last week that both Scattered Spider suspects were detained and/or searched by local law enforcement on multiple occasions, only to return to the Com less than 24 hours after being released each time.
&#8220;What we see is these young Com members become vectors for perpetrators to commit enormously harmful acts and even child abuse,&#8221; Nixon said. &#8220;The members of this special category of people who enjoy legal immunity are meeting up with foreign nationals and conducting these sometimes heinous acts at their behest.&#8221;
Nixon said many of these individuals have few friends in real life because they spend virtually all of their waking hours on Com channels, and so their entire sense of identity, community and self-worth gets wrapped up in their involvement with these online gangs. She said ifÂ the law was such that prosecutors could treat these people commensurate with the amount of harm they cause society, that would probably clear up a lot of this problem.
&#8220;If law enforcement was allowed to keep them in jail, they would quit reoffending,&#8221; she said.
The Times of London reports that Flowers is facing three charges under the Computer Misuse Act: two of conspiracy to commit an unauthorized act in relation to a computer causing/creating risk of serious damage to human welfare/national security and one of attempting to commit the same act. Maximum sentences for these offenses can range from 14 years to life in prison, depending on the impact of the crime.
Jubair is reportedly facing two charges in the U.K.: One of conspiracy to commit an unauthorized act in relation to a computer causing/creating risk of serious damage to human welfare/national security and one of failing to comply with a section 49 notice to disclose the key to protected information.
In the United States, Jubair is charged with computer fraud conspiracy, two counts of computer fraud, wire fraud conspiracy, two counts of wire fraud, and money laundering conspiracy. If extradited to the U.S., tried and convicted on all charges, he faces a maximum penalty of 95 years in prison.
In July 2025, the United Kingdom followed Australia&#8217;s example in banning victims of hacking from paying ransoms to cybercriminal groups unless approved by officials. U.K. organizations that are considered part of critical infrastructure reportedly will face a complete ban, as will the entire public sector. U.K. victims of a hack are now required to notify officials to better inform policymakers on the scale of Britain&#8217;s ransomware problem.
For further reading (bless you), check out Bloomberg&#8217;s poignant story last week based on a year&#8217;s worth of jailhouse interviews with convicted Scattered Spider member Noah Urban.

ğŸ“ University AI
No updates.

ğŸ¢ Corporate AI
â€¢ Using AI to assist in rare disease diagnosis
  In the promising and rapidly evolving field of genetic analysis, the ability to accurately interpret whole genome sequencing data is crucial for diagnosing and improving outcomes for people with rare genetic diseases. Yet despite technological advancements, genetic professionals face steep challenges in managing and synthesizing the vast amounts of data required for these analyses. Fewer than 50% of&nbsp;initial&nbsp;cases yield a diagnosis, and while reanalysis can lead to new findings, the process remains&nbsp;time-consuming and complex.&nbsp;



To better understand and address these challenges, Microsoft Researchâ€”in collaboration with Drexel University and the Broad Instituteâ€‹â€‹â€”conducted a comprehensive study titledÂ AI-Enhanced Sensemaking: Exploring the Design of a Generative AI-Based Assistant to Support Genetic Professionals (opens in new tab).Â The study was recently published in a special edition ofÂ ACM Transactions on Interactive Intelligent SystemsÂ journal focused on generative AI.Â Â 



The study focused on integrating generative AI to support the complex, time-intensive, and information-dense sensemaking tasks inherent in whole genome sequencing analysis. Through detailed empirical research and collaborative design sessions with experts in the field, we identified key obstacles genetic professionals face and proposed AI-driven solutions to enhance their workflows.&nbsp;â€‹&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;â€‹We&nbsp;developed strategies for how generative AI can help synthesize biomedical data, enabling AI-expert collaboration to increase the diagnoses of previously unsolved rare diseasesâ€”ultimately aiming to improve patientsâ€™ quality of life and life expectancy.



Whole genome sequencing in rare disease diagnosis



Rare diseases affect up to half a billion people globally and obtaining a diagnosis can take multiple years. These diagnoses often involve specialist consultations, laboratory tests, imaging studies, and invasive procedures. Whole genome sequencing is used to identify genetic variants responsible for these diseases by comparing a patientâ€™s DNA sequence to reference genomes.&nbsp;â€‹â€‹Genetic professionals use bioinformatics tools such as&nbsp;seqr,&nbsp;an open-source, web-based tool for rare disease case analysis and project management to assist them in filtering and prioritizing&nbsp; > 1 million variants to determine their potential role in disease.&nbsp;A critical component of&nbsp;their&nbsp;work is sensemaking: the process of searching, filtering, and synthesizing data to build, refine, and present models from complex sets of gene and variant information.&nbsp;&nbsp;



â€‹â€‹The multi-step sequencing processâ€‹â€‹â€‹&nbsp;typically takes three to 12 weeks and requires extensive amounts of evidence and time to synthesize and aggregate information&nbsp;â€‹â€‹to understand the gene and variant effects for the patient.&nbsp;If a patient&#8217;s case goes unsolved, their whole genome sequencing data is set aside until enough time has passed to warrant a reanalysisâ€‹â€‹. This creates a backlog of patient casesâ€‹â€‹. The ability to easily&nbsp;identify&nbsp;when new scientific evidence&nbsp;emerges&nbsp;and when to reanalyze an unsolved patient case is key to shortening the time patients suffer with an unknown rare disease diagnosis.&nbsp;



The promise of AI systems to assist with complex human tasks



Approximately 87% of AI systems never reach deployment&nbsp;â€‹simply because they solveâ€‹â€‹â€‹&nbsp;the wrong problems.&nbsp;â€‹â€‹Understanding the AI support desired by different types of professionals, their current workflows, and AI capabilities is critical to successful AI system deployment and use. Matching technology capabilities with user tasks is particularly challenging in AI design because AI models can generate numerous outputs, and their capabilities can be unclear.&nbsp;â€‹To design an effectiveâ€‹â€‹â€‹&nbsp;AI-based systemâ€‹, one needs to identifyâ€‹&nbsp;â€‹â€‹tasks AI can support,&nbsp;â€‹â€‹determineâ€‹â€‹â€‹â€‹â€‹â€‹&nbsp;the appropriate level of AI involvement, and&nbsp;â€‹â€‹designâ€‹â€‹â€‹â€‹â€‹â€‹&nbsp;user-AI interactions. This necessitates considering how humans interact with technology and how&nbsp;â€‹â€‹AI&nbsp;can best be incorporated into workflows and tools.



	
		

		
		PODCAST SERIES
	
	
	
						
				
					
				
			
			
			

									The AI Revolution in Medicine, Revisited
				
								Join Microsoftâ€™s Peter Lee on a journey to discover how AI is impacting healthcare and what it means for the future of medicine.
				
								
					
						
							Listen now						
					
				
							
	
Opens in a new tab	
	


Study objectives and co-designing a genetic AI assistant



Our study aimed to understand the current challenges and needs of genetic professionals performing whole genome sequencing analyses and explore the tasks where they want an AI assistant to support them in their work. The first phase of our study involved interviews with 17 genetics professionals to better understand their workflows, tools, and challenges. They included genetic analysts directly involved in interpreting data, as well as other roles participating in whole genome sequencing. In the second phase of our study, we conducted co-design sessions with study participants on how an AI assistant could support their workflows. We then developed a prototype of an AI assistant, which was further tested and refined with study participants in follow-up design walk-through sessions.



Identifying challenges in whole genome sequencing analysis



Through our in-depth interviews with genetic professionals, our study uncovered three critical challenges in whole genome sequencing analysis:




Information Overload: Genetic analysts need to gather and synthesize vast amounts of data from multiple sources. This task is incredibly time-consuming and prone to human error.



Collaborative Sharing: Sharing findings with others in the field can be cumbersome and inefficient, often relying on outdated methods that slow the collaborative analysis process.



Prioritizing Reanalysis: Given the continuous influx of new scientific discoveries, prioritizing unsolved cases to reanalyze is a daunting challenge. Analysts need a systematic approach to identify cases that might benefit most from reanalysis.




Genetic professionals highlighted the time-consuming nature of gathering and synthesizing information about genes and variants from different data sources. Other genetic professionals may have insights into certain genes and variants, but sharing and interpreting information with others for collaborative sensemaking requires significant time and effort. Although new scientific findings could affect unsolved cases through reanalysis, prioritizing cases based on new findings was challenging given the number of unsolved cases and limited time of genetic professionals.



Co-designing with experts and AI-human sensemaking tasks



Our study participants prioritized two potential tasks of an AI assistant. The first task was flagging cases for reanalysis based on new scientific findings. The assistant would alert analysts to unsolved cases that could benefit from new research, providing relevant updates drawn from recent publications. The second task focused on aggregating and synthesizing information about genes and variants from the scientific literature. This feature would compile essential information from numerous scientific papers about genes and variants, presenting it in a user-friendly format and saving analysts significant time and effort. Participants emphasized the need to balance selectivity with comprehensiveness in the evidence they review. They also envisioned collaborating with other genetic professionals to interpret, edit, and verify artifacts generated by the AI assistant.



Genetic professionals require both broad and focused evidence at different stages of their workflow. The AI assistant prototypes were designed to allow flexible filtering and thorough evidence aggregation, ensuring users can delve into comprehensive data or selectively focus on pertinent details. The prototypes included features for collaborative sensemaking, enabling users to interpret, edit, and verify AI-generated information collectively. This&nbsp;â€‹â€‹approach not only&nbsp;â€‹underscoresâ€‹â€‹â€‹&nbsp;the trustworthiness of AI outputs, but also facilitates shared understanding and decision-making among genetic professionals.



Design implications for expert-AI sensemaking



In the&nbsp;shifting frontiers of genome sequence analysis,&nbsp;leveraging generative AI to enhance sensemaking offers intriguing possibilitiesâ€‹â€‹. The task of staying&nbsp;â€‹â€‹currentâ€‹â€‹â€‹â€‹â€‹â€‹, synthesizing information from diverse sources, and making informed decisions&nbsp;â€‹â€‹is challengingâ€‹â€‹â€‹â€‹â€‹â€‹.&nbsp;&nbsp;



Our study participants emphasized the hurdles in integrating data from multiple sources without losing critical components, documenting decision rationales, and fostering collaborative environments. Generative AI models, with their advanced capabilities, have started to address these challenges by automatically generating interactive artifacts to support sensemaking. However, the effectiveness of such systems hinges on careful design considerations,&nbsp;â€‹â€‹particularly in how they facilitate distributed sensemaking, support both initial and ongoing sensemaking, and combine evidence from multiple modalities. We next discuss three design considerations for using generative AI models to support sensemaking.



Distributed expert-AI sensemaking design



Generative AI models can create artifacts that aid an individual user&#8217;s sensemaking process; however, the true potential lies in sharing these artifacts among users to foster collective understanding and efficiency. Participants in our study emphasized the importance of explainability, feedback, and trust when interacting with AI-generated content.&nbsp;â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹Trust is gained byâ€‹â€‹â€‹â€‹â€‹â€‹&nbsp;viewing portions of artifacts marked as correct by other users, or observing edits made to AI-generated informationâ€‹â€‹.&nbsp;â€‹â€‹Someâ€‹â€‹â€‹â€‹â€‹â€‹&nbsp;usersâ€‹, however,â€‹&nbsp;cautioned against over-reliance on AI, which could obscure underlying inaccuracies. Thus, design strategies should ensure that any corrections are clearly marked&nbsp;â€‹â€‹and annotatedâ€‹â€‹â€‹â€‹â€‹â€‹. Furthermore, to enhance distributed sensemaking, visibility of others&#8217; notes and context-specific synthesis through AI can streamline the processâ€‹â€‹.&nbsp;



Initial expert-AI sensemaking and re-sensemaking design



In our fast-paced, information-driven world,&nbsp;â€‹â€‹it is essential to understand a situation both&nbsp;initially&nbsp;and again when new information arises.â€‹â€‹&nbsp;â€‹â€‹Sensemaking is inherently temporal, reflecting and shaping our understanding of time as we revisit tasks to reevaluate past decisions or incorporate new information. Generative AI plays a pivotal role here by transforming static data into dynamic artifacts that evolve, offering a comprehensive view of past rationales. Such AI-generated artifacts provide continuity, allowing usersâ€”both&nbsp;original decision-makers or new individualsâ€”to access the rationale behind decisions made in earlier task instances. By continuously editing and updating these artifacts, generative AI highlights new information since the last review, supporting ongoing understanding and decision-making.&nbsp;Moreover, AI systems enhance&nbsp;â€‹â€‹transparencyâ€‹â€‹â€‹â€‹â€‹â€‹&nbsp;by summarizing previous notes and questions, offering insights into earlier thought processes and facilitating a deeper understanding of how conclusions were drawn. This reflective capability not only can reinforce initial sensemaking efforts but also equips users with the clarity needed for informed re-sensemaking as new data emerges.&nbsp;



Combining evidence from multiple modalities to enhance AI-expert sensemaking



â€‹â€‹â€‹Theâ€‹â€‹â€‹â€‹â€‹â€‹&nbsp;ability to combine evidence from multiple modalities is essential for effective sensemaking. Users often need to integrate diverse types of dataâ€”text, images, spatial coordinates, and moreâ€”into a coherent narrative to make informed decisions. Consider the case of search and rescue operations, where workers must rapidly synthesize information from texts, photographs, and GPS data to strategize their efforts. Recent advancements in multimodal generative AI models have empowered users by incorporating and synthesizing these varied inputs into a unified, comprehensive view. For instance, a participant in our study illustrated this capability by using a generative AI model to merge text from scientific publications with a visual gene structure depiction. This integration&nbsp;â€‹â€‹could createâ€‹â€‹â€‹â€‹â€‹â€‹&nbsp;an image that contextualizes an individual&#8217;s genetic variant within the&nbsp;â€‹â€‹contextâ€‹â€‹â€‹â€‹â€‹â€‹&nbsp;of documented variants. Such advanced synthesis enables users to capture complex relationships and insights briefly, streamlining decision-making and expanding the potential for innovative solutions across diverse fields.&nbsp;



Sensemaking Process with AI Assistant



Figure: Sensemaking process when interpreting variants with the introduction of prototype AI assistant. Gray boxes represent sensemaking activities which are currently performed by an analyst but are human-in-the-loop processes with involvement of our prototype AI assistant. Non-gray boxes represent activities reserved for analyst completion without assistance by our AI assistant prototype. Within the foraging searching and synthesizing processes, examples of data sources and data types for each, respectively, are connected by dotted lines.



Conclusion



We explored the potential of generative AI&nbsp;to supportâ€‹â€‹ genetic professionalsâ€‹&nbsp;â€‹in diagnosing rare diseasesâ€‹â€‹. By designing an AI-based assistant, we aim to streamline whole genome sequencing analysis, helping professionals diagnose rare genetic diseases more efficiently. Our study unfolded in two key phases:&nbsp;â€‹pinpointingâ€‹â€‹â€‹&nbsp;existing challenges in analysis, and design ideation, where we crafted a prototype AI assistant. This tool is designed to boost diagnostic yield and cut down diagnosis time by flagging cases for reanalysis and synthesizing crucial gene and variant data. Despite valuable findings, more research is neededâ€‹â€‹. Future research will involve testing the AI assistant in real-time, task-based user testing with genetic professionals to assess the AI&#8217;s impact on their workflow. The promise of AI advancements lies in solving the right user problems and building the appropriate solutions, achieved through collaboration among model developers, domain experts, system designers, and HCI researchers. By fostering these collaborations, we aim to develop robust, personalized AI assistants tailored to specific domains.&nbsp;



Join the conversation



Join us as we continue to explore the transformative potential of generative AI in genetic analysis, and please read the full text publication&nbsp;here (opens in new tab). Follow us on social media, share this post with your network, and let us know your thoughts on how AI can transform genetic research. If interested in our other related research work, check out&nbsp;Evidence Aggregator: AI reasoning applied to rare disease diagnosis. (opens in new tab)&nbsp;&nbsp;




Opens in a new tabThe post Using AI to assist in rare disease diagnosis appeared first on Microsoft Research.
â€¢ Running deep research AI agents on Amazon Bedrock AgentCore
  AI agents are evolving beyond basic single-task helpers into more powerful systems that can plan, critique, and collaborate with other agents to solve complex problems. Deep Agentsâ€”a recently introduced framework built on LangGraphâ€”bring these capabilities to life, enabling multi-agent workflows that mirror real-world team dynamics. The challenge, however, is not just building such agents but also running them reliably and securely in production. This is where Amazon Bedrock AgentCore Runtime comes in. By providing a secure, serverless environment purpose-built for AI agents and tools, Runtime makes it possible to deploy Deep Agents at enterprise scale without the heavy lifting of managing infrastructure. 
In this post, we demonstrate how to deploy Deep Agents on AgentCore Runtime. As shown in the following figure, AgentCore Runtime scales any agent and provides session isolation by allocating a new microVM for each new session. 
 
What is Amazon Bedrock AgentCore? 
Amazon Bedrock AgentCore is both framework-agnostic and model-agnostic, giving you the flexibility to deploy and operate advanced AI agents securely and at scale. Whether youâ€™re building with Strands Agents, CrewAI, LangGraph, LlamaIndex, or another frameworkâ€”and running them on a large language model (LLM)â€”AgentCore provides the infrastructure to support them. Its modular services are purpose-built for dynamic agent workloads, with tools to extend agent capabilities and controls required for production use. By alleviating the undifferentiated heavy lifting of building and managing specialized agent infrastructure, AgentCore lets you bring your preferred framework and model and deploy without rewriting code. 
Amazon Bedrock AgentCore offers a comprehensive suite of capabilities designed to transform local agent prototypes into production-ready systems. These include persistent memory for maintaining context in and across conversations, access to existing APIs using Model Context Protocol (MCP), seamless integration with corporate authentication systems, specialized tools for web browsing and code execution, and deep observability into agent reasoning processes. In this post, we focus specifically on the AgentCore Runtime component. 
Core capabilities of AgentCore Runtime 
AgentCore Runtime provides a serverless, secure hosting environment specifically designed for agentic workloads. It packages code into a lightweight container with a simple, consistent interface, making it equally well-suited for running agents, tools, MCP servers, or other workloads that benefit from seamless scaling and integrated identity management.AgentCore Runtime offers extended execution times up to 8 hours for complex reasoning tasks, handles large payloads for multimodal content, and implements consumption-based pricing that charges only during active processingâ€”not while waiting for LLM or tool responses. Each user session runs in complete isolation within dedicated micro virtual machines (microVMs), maintaining security and helping to prevent cross-session contamination between agent interactions. The runtime works with many frameworks (for example: LangGraph, CrewAI, Strands, and so on) and many foundation model providers, while providing built-in corporate authentication, specialized agent observability, and unified access to the broader AgentCore environment through a single SDK. 
Real-world example: Deep Agents integration 
In this post weâ€™re going to deploy the recently released Deep Agents implementation example on AgentCore Runtimeâ€”showing just how little effort it takes to get the latest agent innovations up and running. 
 
The sample implementation in the preceding diagram includes: 
 
 A research agent that conducts deep internet searches using the Tavily API 
 A critique agent that reviews and provides feedback on generated reports 
 A main orchestrator that manages the workflow and handles file operations 
 
Deep Agents uses LangGraphâ€™s state management to create a multi-agent system with: 
 
 Built-in task planning through a write_todos tool that helps agents break down complex requests 
 Virtual file system where agents can read/write files to maintain context across interactions 
 Sub-agent architecture allowing specialized agents to be invoked for specific tasks while maintaining context isolation 
 Recursive reasoning with high recursion limits (more than 1,000) to handle complex, multi-step workflows 
 
This architecture enables Deep Agents to handle research tasks that require multiple rounds of information gathering, synthesis, and refinement.The key integration points in our code showcase how agents work with AgentCore. The beauty is in its simplicityâ€”we only need to add a couple of lines of code to make an agent AgentCore-compatible: 
 
 # 1. Import the AgentCore runtime
from bedrock_agentcore.runtime import BedrockAgentCoreApp
app = BedrockAgentCoreApp()

# 2. Decorate your agent function with @app.entrypoint
@app.entrypoint
async def langgraph_bedrock(payload):
    # Your existing agent logic remains unchanged
    user_input = payload.get("prompt")
    
    # Call your agent as before
    stream = agent.astream(
        {"messages": [HumanMessage(content=user_input)]},
        stream_mode="values"
    )
    
    # Stream responses back
    async for chunk in stream:
        yield(chunk)

# 3. Add the runtime starter at the bottom
if __name__ == "__main__":
    app.run() 
 
Thatâ€™s it! The rest of the codeâ€”model initialization, API integrations, and agent logicâ€”remains exactly as it was. AgentCore handles the infrastructure while your agent handles the intelligence. This integration pattern works for most Python agent frameworks, making AgentCore truly framework-agnostic. 
Deploying to AgentCore Runtime: Step-by-step 
Letâ€™s walk through the actual deployment process using the AgentCore Starter ToolKit, which dramatically simplifies the deployment workflow. 
Prerequisites 
Before you begin, make sure you have: 
 
 Python 3.10 or higher 
 AWS credentials configured 
 Amazon Bedrock AgentCore SDK installed 
 
Step 1: IAM permissions 
There are two different AWS Identity and Access Management (IAM) permissions you need to consider when deploying an agent in an AgentCore Runtimeâ€”the role you, as a developer use to create AgentCore resources and the execution role that an agent needs to run in an AgentCore Runtime. While the latter role can now be auto-created by the AgentCore Starter Toolkit (auto_create_execution_role=True), the former must be defined as described in IAM Permissions for AgentCore Runtime. 
Step 2: Add a wrapper to your agent 
As shown in the preceding Deep Agents example, add the AgentCore imports and decorator to your existing agent code. 
Step 3: Deploy using the AgentCore starter toolkit 
The starter toolkit provides a three-step deployment process: 
 
 from bedrock_agentcore_starter_toolkit import Runtime

# Step 1: Configure
agentcore_runtime = Runtime()
config_response = agentcore_runtime.configure(
    entrypoint="hello.py", # contains the code we showed earlier in the post
    execution_role=role_arn, # or auto-create
    auto_create_ecr=True,
    requirements_file="requirements.txt",
    region="us-west-2",
    agent_name="deepagents-research"
)

# Step 2: Launch
launch_result = agentcore_runtime.launch()
print(f"Agent deployed! ARN: {launch_result['agent_arn']}")

# Step 3: Invoke
response = agentcore_runtime.invoke({
    "prompt": "Research the latest developments in quantum computing"
}) 
 
Step 4: What happens behind the scenes 
When you run the deployment, the starter kit automatically: 
 
 Generates an optimized Docker file with Python 3.13-slim base image and OpenTelemetry instrumentation 
 Builds your container with the dependencies from requirements.txt 
 Creates an Amazon Elastic Container Registry (Amazon ECR) repository (if auto_create_ecr=True) and pushes your image 
 Deploys to AgentCore Runtime and monitors the deployment status 
 Configures networking and observability with Amazon CloudWatch and AWS X-Ray integration 
 
The entire process typically takes 2â€“3 minutes, after which your agent is ready to handle requests at scale. Each new session is launched in its own fresh AgentCore Runtime microVM, maintaining complete environment isolation. 
The starter kit generates a configuration file (.bedrock_agentcore.yaml) that captures your deployment settings, making it straightforward to redeploy or update your agent later. 
Invoking your deployed agent 
After deployment, you have two options for invoking your agent: 
Option 1: Using the start kit (shown in Step 3) 
 
 response = agentcore_runtime.invoke({
    "prompt": "Research the latest developments in quantum computing"
})
 
 
Option 2: Using boto3 SDK directly 
 
 import boto3
import json

agentcore_client = boto3.client('bedrock-agentcore', region_name='us-west-2')
response = agentcore_client.invoke_agent_runtime(
    agentRuntimeArn=agent_arn,
    qualifier="DEFAULT",
    payload=json.dumps({
        "prompt": "Analyze the impact of AI on healthcare in 2024"
    })
)

# Handle streaming response
for event in response['completion']:
    if 'chunk' in event:
        print(event['chunk']['bytes'].decode('utf-8')) 
 
Deep Agents in action 
As the code executes in Bedrock AgentCore Runtime, the primary agent orchestrates specialized sub-agentsâ€”each with its own purpose, prompt, and tool accessâ€”to solve complex tasks more effectively. In this case, the orchestrator prompt (research_instructions) sets the plan: 
 
 Write the question to question.txt 
 Fan out to one or more research-agent calls (each on a single sub-topic) using the internet_search tool 
 Synthesize findings into final_report.md 
 Call critique-agent to evaluate gaps and structure 
 Optionally loop back to more research/edits until quality is met 
 
Here it is in action: 

 
  
 
 
Clean up 
When finished, donâ€™t forget to de-allocate provisioned AgentCore Runtime in addition to the container repository that was created during the process: 
 
 agentcore_control_client = boto3.client(
    'bedrock-agentcore-control', region_name=region )
ecr_client = boto3.client('ecr',region_name=region )
runtime_delete_response = agentcore_control_client.delete_agent_runtime(    agentRuntimeId=launch_result.agent_id,)
response = ecr_client.delete_repository(
    repositoryName=launch_result.ecr_uri.split('/')[1],force=True)
 
 
Conclusion 
Amazon Bedrock AgentCore represents a paradigm shift in how we deploy AI agents. By abstracting away infrastructure complexity while maintaining framework and model flexibility, AgentCore enables developers to focus on building sophisticated agent logic rather than managing deployment pipelines. Our Deep Agents deployment demonstrates that even complex, multi-agent systems with external API integrations can be deployed with minimal code changes. The combination of enterprise-grade security, built-in observability, and serverless scaling makes AgentCore the best choice for production AI agent deployments. Specifically for deep research agents, AgentCore offers the following unique capabilities that you can explore: 
 
 AgentCore Runtime can handle asynchronous processing and long running (up to 8 hours) agents. Asynchronous tasks allow your agent to continue processing after responding to the client and handle long-running operations without blocking responses. Your background research sub-agent could be asynchronously researching for hours. 
 AgentCore Runtime works with AgentCore Memory, enabling capabilities such as building upon previous findings, remembering research preferences, and maintaining complex investigation context without losing progress between sessions. 
 You can use AgentCore Gateway to extend your deep research to include proprietary insights from enterprise services and data sources. By exposing these differentiated resources as MCP tools, your agents can quickly take advantage and combine that with publicly available knowledge. 
 
Ready to deploy your agents to production? Hereâ€™s how to get started: 
 
 Install the AgentCore starter kit: pip install bedrock-agentcore-starter-toolkit 
 Experiment: Deploy your code by following this step by step guide. 
 
The era of production-ready AI agents is here. With AgentCore, the journey from prototype to production has never been shorter. 
 
About the authors 
Vadim Omeltchenko is a Sr. AI/ML Solutions Architect who is passionate about helping AWS customers innovate in the cloud. His prior IT experience was predominantly on the ground. 
Eashan Kaushik is a Specialist Solutions Architect AI/ML at Amazon Web Services. He is driven by creating cutting-edge generative AI solutions while prioritizing a customer-centric approach to his work. Before this role, he obtained an MS in Computer Science from NYU Tandon School of Engineering. Outside of work, he enjoys sports, lifting, and running marathons. 
Shreyas Subramanian is a Principal data scientist and helps customers by using Machine Learning to solve their business challenges using the AWS platform. Shreyas has a background in large scale optimization and Machine Learning, and in use of Machine Learning and Reinforcement Learning for accelerating optimization tasks. 
Mark Roy is a Principal Machine Learning Architect for AWS, helping customers design and build generative AI solutions. His focus since early 2023 has been leading solution architecture efforts for the launch of Amazon Bedrock, the flagship generative AI offering from AWS for builders. Markâ€™s work covers a wide range of use cases, with a primary interest in generative AI, agents, and scaling ML across the enterprise. He has helped companies in insurance, financial services, media and entertainment, healthcare, utilities, and manufacturing. Prior to joining AWS, Mark was an architect, developer, and technology leader for over 25 years, including 19 years in financial services. Mark holds six AWS Certifications, including the ML Specialty Certification.
â€¢ Integrate tokenization with Amazon Bedrock Guardrails for secure data handling
  This post is co-written by Mark Warner, Principal Solutions Architect for Thales, Cyber Security Products. 
As generative AI applications make their way into production environments, they integrate with a wider range of business systems that process sensitive customer data. This integration introduces new challenges around protecting personally identifiable information (PII) while maintaining the ability to recover original data when legitimately needed by downstream applications. Consider a financial services company implementing generative AI across different departments. The customer service team needs an AI assistant that can access customer profiles and provide personalized responses that include contact information, for example: â€œWeâ€™ll send your new card to your address at 123 Main Street.â€ Meanwhile, the fraud analysis team requires the same customer data but must analyze patterns without exposing actual PII, working only with protected representations of sensitive information. 
Amazon Bedrock Guardrails helps detect sensitive information, such as PII, in standard format in input prompts or model responses. Sensitive information filters give organizations control over how sensitive data is handled, with options to block requests containing PII or mask the sensitive information with generic placeholders like {NAME} or {EMAIL}. This capability helps organizations comply with data protection regulations while still using the power of large language models (LLMs). 
Although masking effectively protects sensitive information, it creates a new challenge: the loss of data reversibility. When guardrails replace sensitive data with generic masks, the original information becomes inaccessible to downstream applications that might need it for legitimate business processes. This limitation can impact workflows where both security and functional data are required. 
Tokenization offers a complementary approach to this challenge. Unlike masking, tokenization replaces sensitive data with format-preserving tokens that are mathematically unrelated to the original information but maintain its structure and usability. These tokens can be securely reversed back to their original values when needed by authorized systems, creating a path for secure data flows throughout an organizationâ€™s environment. 
In this post, we show you how to integrate Amazon Bedrock Guardrails with third-party tokenization services to protect sensitive data while maintaining data reversibility. By combining these technologies, organizations can implement stronger privacy controls while preserving the functionality of their generative AI applications and related systems. The solution described in this post demonstrates how to combine Amazon Bedrock Guardrails with tokenization services from Thales CipherTrust Data Security Platform to create an architecture that protects sensitive data without sacrificing the ability to process that data securely when needed. This approach is particularly valuable for organizations in highly regulated industries that need to balance innovation with compliance requirements. 
Amazon Bedrock Guardrails APIs 
This section describes the key components and workflow for the integration between Amazon Bedrock Guardrails and a third-party tokenization service. 
Amazon Bedrock Guardrails provides two distinct approaches for implementing content safety controls: 
 
 Direct integration with model invocation through APIs like InvokeModel and Converse, where guardrails automatically evaluate inputs and outputs as part of the model inference process. 
 Standalone evaluation through the ApplyGuardrail API, which decouples guardrails assessment from model invocation, allowing evaluation of text against defined policies. 
 
This post uses the ApplyGuardrail API for tokenization integration because it separates content assessment from model invocation, allowing for the insertion of tokenization processing between these steps. This separation creates the necessary space in the workflow to replace guardrail masks with format-preserving tokens before model invocation, or after the model response is handed over to the target application downstream in the process. 
The solution extends the typical ApplyGuardrail API implementation by inserting tokenization processing between guardrail evaluation and model invocation, as follows: 
 
 The application calls the ApplyGuardrail API to assess the user input for sensitive information. 
 If no sensitive information is detected (action = "NONE"), the application proceeds to model invocation via the InvokeModel API. 
 If sensitive information is detected (action = "ANONYMIZED"): 
   
   The application captures the detected PII and its positions. 
   It calls a tokenization service to convert these entities into format-preserving tokens. 
   It replaces the generic guardrail masks with these tokens. 
   The application then invokes the foundation model with the tokenized content. 
    
 For model responses: 
   
   The application applies guardrails to check the output from the model for sensitive information. 
   It tokenizes detected PII before passing the response to downstream systems. 
    
 
Solution overview 
To illustrate how this workflow delivers value in practice, consider a financial advisory application that helps customers understand their spending patterns and receive personalized financial recommendations. In this example, three distinct application components work together to provide secure, AI-powered financial insights: 
 
 Customer gateway service â€“ This trusted frontend orchestrator receives customer queries that often contain sensitive information. For example, a customer might ask: â€œHi, this is j.smith@example.com. Based on my last five transactions on acme.com, and my current balance of $2,342.18, should I consider their new credit card offer?â€ 
 Financial analysis engine â€“ This AI-powered component analyzes financial patterns and generates recommendations but doesnâ€™t need access to actual customer PII. It works with anonymized or tokenized information. 
 Response processing service â€“ This trusted service handles the final customer communication, including detokenizing sensitive information before presenting results to the customer. 
 
The following diagram illustrates the workflow for integrating Amazon Bedrock Guardrails with tokenization services in this financial advisory application. AWS Step Functions orchestrates the sequential process of PII detection, tokenization, AI model invocation, and detokenization across the three key components (customer gateway service, financial analysis engine, and response processing service) using AWS Lambda functions. 
 
The workflow operates as follows: 
 
 The customer gateway service (for this example, through Amazon API Gateway) receives the user input containing sensitive information. 
 It calls the ApplyGuardrail API to identify PII or other sensitive information that should be anonymized or blocked. 
 For detected sensitive elements (such as user names or merchant names), it calls the tokenization service to generate format-preserving tokens. 
 The input with tokenized values is passed to the financial analysis engine for processing. (For example, â€œHi, this is [[TOKEN_123]]. Based on my last five transactions on [[TOKEN_456]] and my current balance of $2,342.18, should I consider their new credit card offer?â€) 
 The financial analysis engine invokes an LLM on Amazon Bedrock to generate financial advice using the tokenized data. 
 The model response, potentially containing tokenized values, is sent to the response processing service. 
 This service calls the tokenization service to detokenize the tokens, restoring the original sensitive values. 
 The final, detokenized response is delivered to the customer. 
 
This architecture maintains data confidentiality throughout the processing flow while preserving the informationâ€™s utility. The financial analysis engine works with structurally valid but cryptographically protected data, allowing it to generate meaningful recommendations without exposing sensitive customer information. Meanwhile, the trusted components at the entry and exit points of the workflow can access the actual data when necessary, creating a secure end-to-end solution. 
In the following sections, we provide a detailed walkthrough of implementing the integration between Amazon Bedrock Guardrails and tokenization services. 
Prerequisites 
To implement the solution described in this post, you must have the following components configured in your environment: 
 
 An AWS account with Amazon Bedrock enabled in your target AWS Region. 
 Appropriate AWS Identity and Access Management (IAM) permissions configured following least privilege principles with specific actions enabled: bedrock:CreateGuardrail, bedrock:ApplyGuardrail, and bedrock-runtime:InvokeModel. 
 For AWS Organizations, verify Amazon Bedrock access is permitted by service control policies. 
 A Python 3.7+ environment with the boto3 library installed. For information about installing the boto3 library, refer to AWS SDK for Python (Boto3). 
 AWS credentials configured for programmatic access using the AWS Command Line Interface (AWS CLI). For more details, refer to Configuring settings for the AWS CLI. 
 This implementation requires a deployed tokenization service accessible through REST API endpoints. Although this walkthrough demonstrates integration with Thales CipherTrust, the pattern adapts to tokenization providers offering protect and unprotect API operations. Make sure network connectivity exists between your application environment and both AWS APIs and your tokenization service endpoints, along with valid authentication credentials for accessing your chosen tokenization service. For information about setting up Thales CipherTrust specifically, refer to How Thales Enables PCI DSS Compliance with a Tokenization Solution on AWS. 
 
Configure Amazon Bedrock Guardrails 
Configure Amazon Bedrock Guardrails for PII detection and masking through the Amazon Bedrock console or programmatically using the AWS SDK. Sensitive information filter policies can anonymize or redact information from model requests or responses: 
 
 import boto3
def create_bedrock_guardrail():
    """
    Create a guardrail in Amazon Bedrock for financial applications with PII protection.
    """
    bedrock = boto3.client('bedrock')
    
    response = bedrock.create_guardrail(
        name="FinancialServiceGuardrail",
        description="Guardrail for financial applications with PII protection",
        sensitiveInformationPolicyConfig={
            'piiEntitiesConfig': [
                {
                    'type': 'URL',
                    'action': 'ANONYMIZE',
                    'inputAction': 'ANONYMIZE',
                    'outputAction': 'ANONYMIZE',
                    'inputEnabled': True,
                    'outputEnabled': True
                },
                {
                    'type': 'EMAIL',
                    'action': 'ANONYMIZE',
                    'inputAction': 'ANONYMIZE',
                    'outputAction': 'ANONYMIZE',
                    'inputEnabled': True,
                    'outputEnabled': True
                },
                {
                    'type': 'NAME',
                    'action': 'ANONYMIZE',
                    'inputAction': 'ANONYMIZE',
                    'outputAction': 'ANONYMIZE',
                    'inputEnabled': True,
                    'outputEnabled': True
                }
            ]
        },
        blockedInputMessaging="I can't provide information with PII data.",
        blockedOutputsMessaging="I can't generate content with PII data."
    )
    
    return response 
 
Integrate the tokenization workflow 
This section implements the tokenization workflow by first detecting PII entities with the ApplyGuardrail API, then replacing the generic masks with format-preserving tokens from your tokenization service. 
Apply guardrails to detect PII entities 
Use the ApplyGuardrail API to validate input text from the user and detect PII entities: 
 
 import boto3
from botocore.exceptions import ClientError
def invoke_guardrail(user_query):
    """
    Apply Amazon Bedrock Guardrails to validate input text and detect PII entities.
    
    Args:
        user_query (str): The user's input text to be checked.
    
    Returns:
        dict: The response from the ApplyGuardrail API.
    
    Raises:
        ClientError: If there's an error applying the guardrail.
    """
    try:
        bedrock_runtime = boto3.client('bedrock-runtime')
        
        response = bedrock_runtime.apply_guardrail(
            guardrailIdentifier='your-guardrail-id', # Replace with your actual guardrail ID
            guardrailVersion='your-guardrail-version', # Replace with your actual version
            source="INPUT",
            content=[{"text": {"text": user_query}}]
        )
        
        return response
    except ClientError as e:
        print(f"Error applying guardrail: {e}")
        raise 
 
Invoke tokenization service 
The response from the ApplyGuadrail API includes the list of PII entities matching the sensitive information policy. Parse those entities and invoke the tokenization service to generate the tokens. 
The following example code uses the Thales CipherTrust tokenization service: 
 
 import json
import requests
from botocore.exceptions import ClientError
def thales_ciphertrust_tokenizer(guardrail_response):
  """
  Process PII entities detected by the guardrail and tokenize them using Thales CipherTrust
    
  Args:
      guardrail_response (dict): The response from the ApplyGuardrail API
    
  Returns:
      list: List of dictionaries containing original values, types, and tokenized responses
    
  Raises:
      ClientError: If there's an error invoking Thales CipherTrust.
  """
  try:
    protected_results = []
      
    for assessment in guardrail_response.get("assessments", []):
      pii_entities = assessment.get("sensitiveInformationPolicy", {}).get("piiEntities", [])
            
      for entity in pii_entities:
          sensitive_value = entity.get("match")
          entity_type = entity.get("type")
                
          if sensitive_value:
              # Prepare payload for Thales CipherTrust tokenization service
              crdp_payload = {
                  "protection_policy_name": "plain-alpha-internal",
                  "DATA_KEY": sensitive_value,
              }
                    
              url_str = "http://your-ciphertrust-cname:8090/v1/protect"  # Replace with your actual CipherTrust URL
              headers = {"Content-Type": "application/json"}
                    
              # Invoke the Thales CipherTrust tokenization service
              response = requests.post(url_str, headers=headers, data=json.dumps(crdp_payload))
              response.raise_for_status()
              response_json = response.json()
                    
              protected_results.append({
                   "original_value": sensitive_value,
                   "type": entity_type,
                   "protection_response": response_json
              })
        
    return protected_results
  except requests.RequestException as e:
    print(f"Error invoking Thales CipherTrust: {e}")
    raise ClientError(f"Error invoking Thales CipherTrust: {e}", "TokenizationError") 
 
Replace guardrail masks with tokens 
Next, substitute the generic guardrail masks with the tokens generated by the Thales CipherTrust tokenization service. This enables downstream applications to work with structurally valid data while maintaining security and reversibility. 
 
 def process_guardrail_output(protected_results, guardrail_response):
    """
    Process guardrail output by replacing placeholders with protected values.
    
    Args:
        protected_results (list): List of protected data tokenized by Thales CipherTrust.
        guardrail_response (dict): Guardrail response dictionary.
        
    Returns:
        list: List of modified output items with placeholders replaced by tokens.
    
    Raises:
        ValueError: If input parameters are invalid.
        Exception: For any unexpected errors during processing.
    """
    try:
        # Validate input types
        if not isinstance(protected_results, list) or not isinstance(guardrail_response, dict):
            raise ValueError("Invalid input parameters")
            
        # Extract protection map
        protection_map = {res['type'].upper(): res['protection_response']['protected_data'] 
                          for res in protected_results}
        # Process outputs 
        modified_outputs = []
        for output_item in guardrail_response.get('outputs', []):
            if 'text' in output_item:
                modified_text = output_item['text']
                
                # Replace all placeholders in one pass
                for pii_type, protected_value in protection_map.items():
                    modified_text = modified_text.replace(f"{{{pii_type}}}", protected_value)
                
                modified_outputs.append({"text": modified_text})
        return modified_outputs
    except (ValueError, KeyError) as e:
        print(f"Error processing guardrail output: {e}")
        raise
    except Exception as e:
        print(f"Unexpected error while processing guardrail output: {e}")
        raise 
 
The result of this process transforms user inputs containing information that match the sensitive information policy applied using Amazon Bedrock Guardrails into unique and reversible tokenized versions. 
The following example input contains PII elements: 
 
 "Hi, this is john.smith@example.com. Based on my last five transactions on acme.com, and my current balance of $2,342.18, should I consider their new credit card offer?" 
 
The following is an example of the sanitized user input: 
 
 "Hi, this is 1001000GC5gDh1.D8eK71@EjaWV.lhC. Based on my last five transactions on 1001000WcFzawG.Jc9Tfc, and my current balance of $2,342.18, should I consider their new credit card offer?" 
 
Downstream application processing 
The sanitized input is ready to be used by generative AI applications, including model invocations on Amazon Bedrock. In response to the tokenized input, an LLM invoked by the financial analysis engine would produce a relevant analysis that maintains the secure token format: 
 
 "Based on your recent transactions at 1001000WcFzawG.Jc9Tfc and your current account status, I can confirm that the new credit card offer would provide approximately $33 in monthly rewards based on your spending patterns. With annual benefits of around $394 against the $55 annual fee, this card would be beneficial for your profile, 1001000GC5gDh1.D8eK71@EjaWV.lhC." 
 
When authorized systems need to recover original values, tokens are detokenized. With Thales CipherTrust, this is accomplished using the Detokenize API, which requires the same parameters as in the previous tokenize action. This completes the secure data flow while preserving the ability to recover original information when needed. 
Clean up 
As you follow the approach described in this post, you will create new AWS resources in your account. To avoid incurring additional charges, delete these resources when you no longer need them. 
To clean up your resources, complete the following steps: 
 
 Delete the guardrails you created. For instructions, refer to Delete your guardrail. 
 If you implemented the tokenization workflow using Lambda, API Gateway, or Step Functions as described in this post, remove the resources you created. 
 This post assumes a tokenization solution is already available in your account. If you deployed a third-party tokenization solution (such as Thales CipherTrust) to test this implementation, refer to that solutionâ€™s documentation for instructions to properly decommission these resources and stop incurring charges. 
 
Conclusion 
This post demonstrated how to combine Amazon Bedrock Guardrails with tokenization to enhance handling of sensitive information in generative AI workflows. By integrating these technologies, organizations can protect PII during processing while maintaining data utility and reversibility for authorized downstream applications. 
The implementation illustrated uses Thales CipherTrust Data Security Platform for tokenization, but the architecture supports many tokenization solutions. To learn more about a serverless approach to building custom tokenization capabilities, refer to Building a serverless tokenization solution to mask sensitive data. 
This solution provides a practical framework for builders to use the full potential of generative AI with appropriate safeguards. By combining the content safety mechanisms of Amazon Bedrock Guardrails with the data reversibility of tokenization, you can implement responsible AI workflows that align with your application requirements and organizational policies while preserving the functionality needed for downstream systems. 
To learn more about implementing responsible AI practices on AWS, see Transform responsible AI from theory into practice. 
 
About the Authors 
Nizar Kheir is a Senior Solutions Architect at AWS with more than 15 years of experience spanning various industry segments. He currently works with public sector customers in France and across EMEA to help them modernize their IT infrastructure and foster innovation by harnessing the power of the AWS Cloud. 
Mark Warner is a Principal Solutions Architect for Thales, Cyber Security Products division. He works with companies in various industries such as finance, healthcare, and insurance to improve their security architectures. His focus is assisting organizations with reducing risk, increasing compliance, and streamlining data security operations to reduce the probability of a breach.
â€¢ Rapid ML experimentation for enterprises with Amazon SageMaker AI and Comet
  This post was written with Sarah Ostermeier from Comet. 
As enterprise organizations scale their machine learning (ML) initiatives from proof of concept to production, the complexity of managing experiments, tracking model lineage, and managing reproducibility grows exponentially. This is primarily because data scientists and ML engineers constantly explore different combinations of hyperparameters, model architectures, and dataset versions, generating massive amounts of metadata that must be tracked for reproducibility and compliance. As the ML model development scales across multiple teams and regulatory requirements intensify, tracking experiments becomes even more complex. With increasing AI regulations, particularly in the EU, organizations now require detailed audit trails of model training data, performance expectations, and development processes, making experiment tracking a business necessity and not just a best practice. 
Amazon SageMaker AI provides the managed infrastructure enterprises need to scale ML workloads, handling compute provisioning, distributed training, and deployment without infrastructure overhead. However, teams still need robust experiment tracking, model comparison, and collaboration capabilities that go beyond basic logging. 
Comet is a comprehensive ML experiment management platform that automatically tracks, compares, and optimizes ML experiments across the entire model lifecycle. It provides data scientists and ML engineers with powerful tools for experiment tracking, model monitoring, hyperparameter optimization, and collaborative model development. It also offers Opik, Cometâ€™s open source platform for LLM observability and development. 
Comet is available in SageMaker AI as a Partner AI App, as a fully managed experiment management capability, with enterprise-grade security, seamless workflow integration, and a straightforward procurement process through AWS Marketplace. 
The combination addresses the needs of an enterprise ML workflow end-to-end, where SageMaker AI handles infrastructure and compute, and Comet provides the experiment management, model registry, and production monitoring capabilities that teams require for regulatory compliance and operational efficiency. In this post, we demonstrate a complete fraud detection workflow using SageMaker AI with Comet, showcasing reproducibility and audit-ready logging needed by enterprises today. 
Enterprise-ready Comet on SageMaker AI 
Before proceeding to setup instructions, organizations must identify their operating model and based on that, decide how Comet is going to be set up. We recommend implementing Comet using a federated operating model. In this architecture, Comet is centrally managed and hosted in a shared services account, and each data science team maintains fully autonomous environments. Each operating model comes with their own sets of benefits and limitations. For more information, refer to SageMaker Studio Administration Best Practices. 
Letâ€™s dive into the setup of Comet in SageMaker AI. Large enterprise generally have the following personas: 
 
 Administrators â€“ Responsible for setting up the common infrastructure services and environment for use case teams 
 Users â€“ ML practitioners from use case teams who use the environments set up by platform team to solve their business problems 
 
In the following sections, we go through each personaâ€™s journey. 
Comet works well with both SageMaker AI and Amazon SageMaker. SageMaker AI provides the Amazon SageMaker Studio integrated development environment (IDE), and SageMaker provides the Amazon SageMaker Unified Studio IDE. For this post, we use SageMaker Studio. 
Administrator journey 
In this scenario, the administrator receives a request from a team working on a fraud detection use case to provision an ML environment with a fully managed training and experimentation setup. The administratorâ€™s journey includes the following steps: 
 
 Follow the prerequisites to set up Partner AI Apps. This sets up permissions for administrators, allowing Comet to assume a SageMaker AI execution role on behalf of the users and additional privileges for managing the Comet subscription through AWS Marketplace. 
 On the SageMaker AI console, under Applications and IDEs in the navigation pane, choose Partner AI Apps, then choose View details for Comet. 
 
 
The details are shown, including the contract pricing model for Comet and infrastructure tier estimated costs. 
 
Comet provides different subscription options ranging from a 1-month to 36-month contract. With this contract, users can access Comet in SageMaker. Based on the number of users, the admin can review and analyze the appropriate instance size for the Comet dashboard server. Comet supports 5â€“500 users running more than 100 experiment jobs.. 
 
 Choose Go to Marketplace to subscribe to be redirected to the Comet listing on AWS Marketplace. 
 Choose View purchase options. 
 
 
 
 In the subscription form, provide the required details. 
 
 
When the subscription is complete, the admin can start configuring Comet. 
 
 
 While deploying Comet, add the project lead of the fraud detection use case team as an admin to manage the admin operations for the Comet dashboard. 
 
It takes a few minutes for the Comet server to be deployed. For more details on this step, refer to Partner AI App provisioning. 
 
 Set up a SageMaker AI domain following the steps in Use custom setup for Amazon SageMaker AI. As a best practice, provide a pre-signed domain URL for the use case team member to directly access the Comet UI without logging in to the SageMaker console. 
 Add the team members to this domain and enable access to Comet while configuring the domain. 
 
Now the SageMaker AI domain is ready for users to log in to and start working on the fraud detection use case. 
User journey 
Now letâ€™s explore the journey of an ML practitioner from the fraud detection use case. The user completes the following steps: 
 
 Log in to the SageMaker AI domain through the pre-signed URL. 
 
You will be redirected to the SageMaker Studio IDE. Your user name and AWS Identity and Access Management (IAM) execution role are preconfigured by the admin. 
 
 Create a JupyterLab Space following the JupyterLab user guide. 
 You can start working on the fraud detection use case by spinning up a Jupyter notebook. 
 
The admin has also set up required access to the data through an Amazon Simple Storage Service (Amazon S3) bucket. 
 
 To access Comet APIs, install the comet_ml library and configure the required environment variables as described in Set up the Amazon SageMaker Partner AI Apps SDKs. 
 To access the Comet UI, choose Partner AI Apps in the SageMaker Studio navigation pane and choose Open for Comet. 
 
 
Now, letâ€™s walk through the use case implementation. 
Solution overview 
This use case highlights common enterprise challenges: working with imbalanced datasets (in this example, only 0.17% of transactions are fraudulent), requiring multiple experiment iterations, and maintaining full reproducibility for regulatory compliance. To follow along, refer to the Comet documentation and Quickstart guide for additional setup and API details. 
For this use case, we use the Credit Card Fraud Detection dataset. The dataset contains credit card transactions with binary labels representing fraudulent (1) or legitimate (0) transactions. In the following sections, we walk through some of the important sections of the implementation. The entire code of the implementation is available in the GitHub repository. 
Prerequisites 
As a prerequisite, configure the necessary imports and environment variables for the Comet and SageMaker integration: 
 
 # Comet ML for experiment tracking
import comet_ml
from comet_ml import Experiment, API, Artifact
from comet_ml.integration.sagemaker import log_sagemaker_training_job_v1
AWS_PARTNER_APP_AUTH=true
AWS_PARTNER_APP_ARN=&lt;Your_AWS_PARTNER_APP_ARN&gt;
COMET_API_KEY=&lt;Your_Comet_API_Key&gt; 	
# From Details Page, click Open Comet. In the top #right corner, click on user -&gt; API # Key
# Comet ML configuration
COMET_WORKSPACE = '&lt;your-comet-workspace-name&gt;'
COMET_PROJECT_NAME = '&lt;your-comet-project-name&gt;' 
 
Prepare the dataset 
One of Cometâ€™s key enterprise features is automatic dataset versioning and lineage tracking. This capability provides full auditability of what data was used to train each model, which is critical for regulatory compliance and reproducibility. Start by loading the dataset: 
 
 # Create a Comet Artifact to track our raw dataset
dataset_artifact = Artifact(
    name="fraud-dataset",
    artifact_type="dataset",
    aliases=["raw"]
)
# Add the raw dataset file to the artifact
dataset_artifact.add_remote(s3_data_path, metadata={
    "dataset_stage": "raw", 
    "dataset_split": "not_split", 
    "preprocessing": "none"
}) 
 
Start a Comet experiment 
With the dataset artifact created, you can now start tracking the ML workflow. Creating a Comet experiment automatically begins capturing code, installed libraries, system metadata, and other contextual information in the background. You can log the dataset artifact created earlier in the experiment. See the following code: 
 
 # Create a new Comet experiment
experiment_1 = comet_ml.Experiment(
    project_name=COMET_PROJECT_NAME,
    workspace=COMET_WORKSPACE,
)
# Log the dataset artifact to this experiment for lineage tracking
experiment_1.log_artifact(dataset_artifact) 
 
Preprocess the data 
The next steps are standard preprocessing steps, including removing duplicates, dropping unneeded columns, splitting into train/validation/test sets, and standardizing features using scikit-learnâ€™s StandardScaler. We wrap the processing code in preprocess.py and run it as a SageMaker Processing job. See the following code: 
 
 # Run SageMaker processing job
processor = SKLearnProcessor(
    framework_version='1.0-1',
    role=sagemaker.get_execution_role(),
    instance_count=1,
    instance_type='ml.t3.medium'
)
processor.run(
    code='preprocess.py',
    inputs=[ProcessingInput(source=s3_data_path, destination='/opt/ml/processing/input')],
    outputs=[ProcessingOutput(source='/opt/ml/processing/output', destination=f's3://{bucket_name}/{processed_data_prefix}')]
) 
 
After you submit the processing job, SageMaker AI launches the compute instances, processes and analyzes the input data, and releases the resources upon completion. The output of the processing job is stored in the S3 bucket specified. 
Next, create a new version of the dataset artifact to track the processed data. Comet automatically versions artifacts with the same name, maintaining complete lineage from raw to processed data. 
 
 # Create an updated version of the 'fraud-dataset' Artifact for the preprocessed data
preprocessed_dataset_artifact = Artifact(
    name="fraud-dataset",
    artifact_type="dataset", 
    aliases=["preprocessed"],
    metadata={
        "description": "Credit card fraud detection dataset",
        "fraud_percentage": f"{fraud_percentage:.3f}%",
        "dataset_stage": "preprocessed",
        "preprocessing": "StandardScaler + train/val/test split",
    }
)
# Add our train, validation, and test dataset files as remote assets 
preprocessed_dataset_artifact.add_remote(
    uri=f's3://{bucket_name}/{processed_data_prefix}',
    logical_path='split_data'
)
# Log the updated dataset to the experiment to track the updates
experiment_1.log_artifact(preprocessed_dataset_artifact) 
 
The Comet and SageMaker AI experiment workflow 
Data scientists prefer rapid experimentation; therefore, we organized the workflow into reusable utility functions that can be called multiple times with different hyperparameters while maintaining consistent logging and evaluation across all runs. In this section, we showcase the utility functions along with a brief snippet of the code inside the function: 
 
 train() â€“ Spins up a SageMaker model training job using the SageMaker built-in XGBoost algorithm: 
 
 
     # Create SageMaker estimator
    estimator = Estimator(
        image_uri=xgboost_image,
        role=execution_role,
        instance_count=1,
        instance_type='ml.m5.large',
        output_path=model_output_path,
        sagemaker_session=sagemaker_session_obj,
        hyperparameters=hyperparameters_dict,
        max_run=1800  # Maximum training time in seconds
    )
    # Start training
    estimator.fit({
        'train': train_channel,
        'validation': val_channel
    }) 
 
 
 log_training_job() â€“ Captures the training metadata and metrics and links the model asset to the experiment for complete traceability: 
 
 
 # Log SageMaker training job to Comet 
    log_sagemaker_training_job_v1(
        estimator=training_estimator,
        experiment=api_experiment
    ) 
 
 
 log_model_to_comet() â€“ Links model artifacts to Comet, captures the training metadata, and links the model asset to the experiment for complete traceability: 
 
 
 experiment.log_remote_model(
        model_name=model_name,
        uri=model_artifact_path,
        metadata=metadata
    ) 
 
 
 deploy_and_evaluate_model() â€“ Performs model deployment and evaluation, and metric logging: 
 
 
 # Deploy to endpoint
predictor = estimator.deploy(
initial_instance_count=1,
       instance_type="ml.m5.xlarge")
# Log metrics and visualizations to Comet 
experiment.log_metrics(metrics) experiment.log_confusion_matrix(matrix=cm,labels=['Normal', 'Fraud']) 
# Log ROC curve 
fpr, tpr, _ = roc_curve(y_test, y_pred_prob_as_np_array) experiment.log_curve("roc_curve", x=fpr, y=tpr) 
 
The complete prediction and evaluation code is available in the GitHub repository. 
Run the experiments 
Now you can run multiple experiments by calling the utility functions with different configurations and compare experiments to find the most optimal settings for the fraud detection use case. 
For the first experiment, we establish a baseline using standard XGBoost hyperparameters: 
 
 # Define hyperparameters for first experiment
hyperparameters_v1 = {
    'objective': 'binary:logistic',	# Binary classification
    'num_round': 100,                   # Number of boosting rounds
    'eval_metric': 'auc',               # Evaluation metric
    'learning_rate': 0.15,              # Learning rate
    'booster': 'gbtree'                 # Booster algorithm
}
# Train the model
estimator_1 = train(
    model_output_path=f"s3://{bucket_name}/{model_output_prefix}/1",
    execution_role=role,
    sagemaker_session_obj=sagemaker_session,
    hyperparameters_dict=hyperparameters_v1,
    train_channel_loc=train_channel_location,
    val_channel_loc=validation_channel_location
)
# log the training job and model artifact
log_training_job(experiment_key = experiment_1.get_key(), training_estimator=estimator_1)
log_model_to_comet(experiment = experiment_1,
                   model_name="fraud-detection-xgb-v1", 
                   model_artifact_path=estimator_1.model_data, 
                   metadata=metadata)
# Deploy and evaluate
deploy_and_evaluate_model(experiment=experiment_1,
                          estimator=estimator_1,
                          X_test_scaled=X_test_scaled,
                          y_test=y_test
                          ) 
 
While running a Comet experiment from a Jupyter notebook, we need to end the experiment to make sure everything is captured and persisted in the Comet server. See the following code: experiment_1.end() 
When the baseline experiment is complete, you can run additional experiments with different hyperparameters. Check out the notebook to see the details of both experiments. 
When the second experiment is complete, navigate to the Comet UI to compare these two experiment runs. 
View Comet experiments in the UI 
To access the UI, you can locate the URL in the SageMaker Studio IDE or by executing the code provided in the notebook: experiment_2.url 
The following screenshot shows the Comet experiments UI. The experiment details are for illustration purposes only and do not represent a real-world fraud detection experiment. 
 
This concludes the fraud detection experiment. 
Clean up 
For the experimentation part, SageMaker processing and training infrastructure is ephemeral in nature and shuts down automatically when the job is complete. However, you must still manually clean up a few resources to avoid unnecessary costs: 
 
 Shut down the SageMaker JupyterLab Space after use. For instructions, refer to Idle shutdown. 
 The Comet subscription renews based on the contract chosen. Cancel the contract when there is no further requirement to renew the Comet subscription. 
 
Advantages of SageMaker and Comet integration 
Having demonstrated the technical workflow, letâ€™s examine the broader advantages this integration provides. 
Streamlined model development 
The Comet and SageMaker combination reduces the manual overhead of running ML experiments. While SageMaker handles infrastructure provisioning and scaling, Cometâ€™s automatic logging captures hyperparameters, metrics, code, installed libraries, and system performance from your training jobs without additional configuration. This helps teams focus on model development rather than experiment bookkeeping.Cometâ€™s visualization capabilities extend beyond basic metric plots. Built-in charts enable rapid experiment comparison, and custom Python panels support domain-specific analysis tools for debugging model behavior, optimizing hyperparameters, or creating specialized visualizations that standard tools canâ€™t provide. 
Enterprise collaboration and governance 
For enterprise teams, the combination creates a mature platform for scaling ML projects across regulated environments. SageMaker provides consistent, secure ML environments, and Comet enables seamless collaboration with complete artifact and model lineage tracking. This helps avoid costly mistakes that occur when teams canâ€™t recreate previous results. 
Complete ML lifecycle integration 
Unlike point solutions that only address training or monitoring, Comet paired with SageMaker supports your complete ML lifecycle. Models can be registered in Cometâ€™s model registry with full version tracking and governance. SageMaker handles model deployment, and Comet maintains the lineage and approval workflows for model promotion. Cometâ€™s production monitoring capabilities track model performance and data drift after deployment, creating a closed loop where production insights inform your next round of SageMaker experiments. 
Conclusion 
In this post, we showed how to use SageMaker and Comet together to spin up fully managed ML environments with reproducibility and experiment tracking capabilities. 
To enhance your SageMaker workflows with comprehensive experiment management, deploy Comet directly in your SageMaker environment through the AWS Marketplace, and share your feedback in the comments. 
For more information about the services and features discussed in this post, refer to the following resources: 
 
 Set up Partner AI Apps 
 Comet Quickstart 
 GitHub notebook 
 Comet Documentation 
 Opik open source platform for LLM observability 
 
 
 
About the authors 
Vikesh Pandey is a Principal GenAI/ML Specialist Solutions Architect at AWS, helping large financial institutions adopt and scale generative AI and ML workloads. He is the author of book â€œGenerative AI for financial services.â€ He carries more than 15 years of experience building enterprise-grade applications on generative AI/ML and related technologies. In his spare time, he plays an unnamed sport with his son that lies somewhere between football and rugby. 
Naufal Mir is a Senior GenAI/ML Specialist Solutions Architect at AWS. He focuses on helping customers build, train, deploy and migrate machine learning workloads to SageMaker. He previously worked at financial services institutes developing and operating systems at scale. Outside of work, he enjoys ultra endurance running and cycling. 
Sarah Ostermeier is a Technical Product Marketing Manager at Comet. She specializes in bringing Cometâ€™s GenAI and ML developer products to the engineers who need them through technical content, educational resources, and product messaging. She has previously worked as an ML engineer, data scientist, and customer success manager, helping customers implement and scale AI solutions. Outside of work she enjoys traveling off the beaten path, writing about AI, and reading science fiction.

â¸»