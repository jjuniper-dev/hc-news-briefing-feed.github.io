‚úÖ Morning News Briefing ‚Äì October 08, 2025 10:44

üìÖ Date: 2025-10-08 10:44
üè∑Ô∏è Tags: #briefing #ai #publichealth #digitalgov

‚∏ª

üßæ Weather
‚Ä¢ Current Conditions: Partly Cloudy, 6.5¬∞C
  Observed at Garrison Petawawa 6:00 AM EDT Wednesday 8 October 2025 . Temperature: 6.5&deg;C Pressure: 101.9 kPa . Visibility: 16 km . Humidity: 95 % Dewpoint: 5.8&deg:C Wind: W 9 km/h . Air Quality Health Index: n/a . Health Index is n/
‚Ä¢ Wednesday: Chance of showers. High 12. POP 30%
  30 percent chance of showers late this morning and this afternoon . Increasing cloudiness. High 12.5 degrees Celsius . UV index 3 or moderate. . High 12 degrees Celsius or higher than normal for most of the day . Forecast issued 5:00 AM EDT Wednesday 8 October 2025. Forecast: "Showery" and "Cloudy" with rain in New York City .
‚Ä¢ Wednesday night: A few clouds. Low minus 2.
  Clear. Clear. Wind up to 15 km/h. Wind chill chill minus 5 overnight . Low minus 2.50¬∞F . Wind chill will be minus 5¬∞F overnight. Clear and wind chill will drop to minus 2¬∞F in the morning . Forecast issued 5:00 AM EDT Wednesday 8 October 2025. Weather forecasters predict temperatures will be below zero in 2080

üåç International News
No updates.

üçÅ Canadian News
No updates.

üá∫üá∏ U.S. Top Stories
‚Ä¢ Why preorders are such a big deal for authors
  Planet Money is available for preorder . The Planet Money book is available to preorder on Amazon.com.com/Planet Money . It is available in print and on sale for $100,000. Special gifts include a copy of the book and a Kindle version of this edition of this week‚Äôs edition of Planet Money . Back to the page you came from, comment on
‚Ä¢ Research on metal-organic frameworks gets the chemistry Nobel Prize
  Susumu Kitagawa, Richard Robson and Omar M. Yaghi will share the prize . Their structures can "capture carbon dioxide, store toxic gases or catalyse chemical reactions," committee says . The structures can also be used to store toxic gas and store carbon dioxide or store gases, the committee said . The winners' structures will be unveiled on Friday at a ceremony
‚Ä¢ Famed polar exploration ship Endurance not as strong as legend held, researcher says
  Ernest Shackleton's crew famously survived after the Endurance became stuck in ice in 1915 . Shackleton was aware the ship was ill-equipped for the voyage . A researcher says Shackleton knew he was going on the voyage and Shackleton would have been aware of the situation . The Endurance is believed to have been ill-prepared for the trip to the Antarctic in 1915. Shackleton died
‚Ä¢ Pumpkin: A favorite sign of fall, with a bit of shady history
  The word "pumpkin" has been around for centuries, but the plant dates back thousands of years . Pumpkins are a harvest symbol and part of our nostalgia for a simpler time . The word 'pumpkins' has been used for centuries and is considered a symbol of the harvest . The plant is thought to have been used in Europe and Asia for more than a century .
‚Ä¢ Hundreds of hikers rescued from Mount Everest after severe snowstorm
  About 900 hikers, guides and other staff stranded by a weekend snowstorm on the Chinese side of Mount Everest have reached safety, state media said . State media said late Tuesday that the stranded hikers and guides had reached safety . The snowstorm hit the Chinese mountain in a weekend storm on the China side of the world's highest mountain . The storm is the worst in the world, killing about

üß† Artificial Intelligence
No updates.

üíª Digital Strategy
‚Ä¢ Telecoms wholesaler ICUK restores services after two-day DDoS pelting
  ICUK is back on track after multi-day DDoS attack on its network and systems . No idea who's behind it, just happy it's over London-based wholesale telecoms biz ICUK . ICUK was attacked by a DDoS-attack on its networks and systems, but no-one knows who was behind it . The attack was the first time ICUK had
‚Ä¢ UK Home Office doubles down on Oracle with ¬£54M cloud contract
  UK's Home Office has signed a new deal with Oracle for around ¬£54 million ($72 million) in cloud infrastructure and platform services . Home Office gears up as the centerpiece of a government shared services strategy based on Big Red . Department also wedded to Big Red in shared service applications . The Home Office is also keen on cloud infrastructure, platform services as it gears up to be the centerpiece
‚Ä¢ Brits sitting on ¬£1.6B gold mine of Windows 10 junk as support ends
  Recyclers reckon millions of obsolete PCs could yield a small fortune in precious metals . Millions of obsolete Windows 10 PCs could be worth millions of pounds of gold, they say . Recycling experts say millions of old PCs could make a fortune from their sale of precious metals in the U.S. They say they could also be worth a fortune in gold, silver and gold .
‚Ä¢ AI companion bots use emotional manipulation to boost usage
  AI companion apps such as Character.ai and Replika commonly try to boost user engagement with emotional manipulation . Researchers argue that this dark pattern poses a legal risk AI apps like Replika pose legal risk . The practice is described as a practice that academics characterize as a dark pattern . The dark pattern is characterized as a pattern that has been characterized by academics as a 'dark pattern' and '
‚Ä¢ Teens arrested in London preschool ransomware attack
  London police arrest two teenagers on suspicion of computer misuse and blackmail following a ransomware attack on a chain of London preschools . Both men, 17, taken into custody . The attack was carried out on the preschools in the capital capital, London, on Tuesday . Both teens, both 17, were arrested and are being held in custody in connection with the attack . They are accused of using

üè• Public Health
No updates.

üî¨ Science
‚Ä¢ A One Health framework for global and local stewardship across the antimicrobial lifecycle
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Rapid reagent free COVID19 detection using MEMS based FTIR spectroscopy and machine learning in NIR and MIR regions
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Evaluation of large language models within GenAI in qualitative research
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Trends in the double burden of malnutrition among Indonesian adults, 2007 to 2023
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Clustering and time series analyses of hybrid immunity to SARS-COV-2 using data from the BQC19 biobank
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations

üßæ Government & Policy
No updates.

üèõÔ∏è Enterprise Architecture & IT Governance
No updates.

ü§ñ AI & Emerging Tech
‚Ä¢ The Trump administration may cut funding for two major direct-air capture plants
  US Department of Energy appears poised to terminate funding for two large carbon-sucking factories . The projects were originally set to receive more than $1 billion in government grants . It's not clear if the termination of the initial grants would mean the full funding would also be canceled . A DOE spokesman stressed that no final decision has been made, saying it's "untrue" to suggest those two projects have been terminated .
‚Ä¢ AI toys are all the rage in China‚Äîand now they‚Äôre appearing on shelves in the US too
  A recent report predicts that the AI toy industry will surpass ¬•100 billion ($14 billion) by 2030 . According to the Chinese corporation registration database Qichamao, there are over 1,500 AI toy companies operating in China . A toy called BubblePal clips onto a child‚Äôs favorite stuffed animal and makes it ‚Äútalk‚Äù It costs $149 and 200,000 units have been sold since it launched last summer . FoloToy allows parents to customize a bear, bunny, or cactus toy by training it to speak with their own voice and speech pattern .
‚Ä¢ The Download: extracting lithium, and what we still don‚Äôt know about Sora
  This is today&#8217;s edition of¬†The Download,¬†our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



This company is planning a lithium empire from the shores of the Great Salt Lake



On a bright afternoon in August, the shore of Utah‚Äôs Great Salt Lake looks like something out of a science fiction film set in a scorching alien world.This otherworldly scene is the test site for a company called Lilac Solutions, which is developing a technology it says will shake up the United States‚Äô efforts to pry control over the global supply of lithium, the so-called ‚Äúwhite gold‚Äù needed for electric vehicles and batteries, away from China.The startup is in a race to commercialize a new, less environmentally-damaging way to extract lithium from rocks. If everything pans out, it could significantly increase domestic supply at a crucial moment for the nation‚Äôs lithium extraction industry. Read the full story.



‚ÄîAlexander C. Kaufman







The three big unanswered questions about Sora



Last week OpenAI released Sora, a TikTok-style app that presents an endless feed of exclusively AI-generated videos, each up to 10 seconds long. The app allows you to create a ‚Äúcameo‚Äù of yourself‚Äîa hyperrealistic avatar that mimics your appearance and voice‚Äîand insert other peoples‚Äô cameos into your own videos (depending on what permissions they set).&nbsp;



In the days since, it soared to the top spot on Apple‚Äôs US App Store. But its explosive growth raises a bunch of questions: can its popularity last? Can OpenAI afford it? And how soon until we start seeing lawsuits over its use of copyrighted content? Here‚Äôs what we‚Äôve learned so far.



This story originally appeared in The Algorithm, our weekly newsletter about the latest in AI. To get stories like this in your inbox first, sign up here.



‚ÄîJames O‚ÄôDonnell







2025 climate tech companies to watch: HiNa Battery Technology and its effort to commercialize salt cells



Over the next few decades the world will need a lot more batteries to power electric cars and keep grids stable. Today most battery cells are made with lithium, so the mineral is expected to be in hyper demand. But a new technology has come on the scene, potentially disrupting the global battery industry.For decades, research of sodium-ion cell technology was abandoned due to the huge commercial success of lithium-ion cells. Now, HiNa Battery Technology is working to bring sodium back to the limelight‚Äîand to the mass market. Read the full story.



‚ÄîYou Xiaoying



HiNa Battery Technology is one of our 10 climate tech companies to watch‚Äîour annual list of some of the most promising climate tech firms on the planet. Check out the rest of the list here.







The must-reads



I‚Äôve combed the internet to find you today‚Äôs most fun/important/scary/fascinating stories about technology.



1 OpenAI has signed a major chip dealIt will collaborate with AMD in a challenge to Nvidia‚Äôs dominance. (WSJ $)+ The multi-billion dollar deal will play out over five years. (FT $)+ Just two weeks ago, OpenAI agreed a deal with Nvidia. (CNN)+ The data center boom in the desert. (MIT Technology Review)



2 Google lost a US Supreme Court bidThe justices denied Google‚Äôs bid to pause changes to its app store. (Bloomberg $)+ It‚Äôs part of the lawsuit Epic Games brought against the tech giant. (Reuters)+ The dispute remains unsolved, so it may be handed back to the justices. (NYT $)



3 You can now use some apps directly within ChatGPTIt‚Äôs all part of OpenAI‚Äôs ambitions to make it a one-stop-shop for all your needs. (The Verge)+ Sam Altman wants it to become your primary digital portal. (The Information $)



4 Deloitte used AI to generate a report for the Australian governmentUnfortunately, it was littered with hallucinated mistakes. (Ars Technica)5 The Nobel prize for medicine has been awarded to three immunity researchersThe trio discovered an immune cell that helps stop the immune system attacking itself. (New Scientist $)6 Russians are using AI to create video memorials of their war deadA burgeoning industry has sprung up, and practitioners will generate clips for $30. (WP $)+ Deepfakes of your dead loved ones are a booming Chinese business. (MIT Technology Review)



7 The dream of greener air travel is starting to die Hydrogen-powered planes are years away. So what now? (FT $)+ How new technologies could clean up air travel. (MIT Technology Review)



8 How job hunters are trying to trick AI r√©sum√©-checkersInserting sneaky hidden prompts is becoming commonplace. (NYT $)



9 The creator of the Friend AI pendant doesn‚Äôt care if you hate itThe backlash to its provocative ads is all part of the plan, apparently. (The Atlantic $)



10 Taylor Swift‚Äôs fans really don‚Äôt like AIThey‚Äôve accused the singer‚Äôs new videos, which appear to be AI-generated, of looking cheap and sloppy. (NY Mag $)+ AI text is out, moving pictures are in. (Economist $)







Quote of the day



‚ÄúWhen AI videos are just as good as normal videos, I wonder what that will do to YouTube and how it will impact the millions of creators currently making content for a living&#8230; scary times.‚Äù



‚ÄîYouTuber Jimmy Donaldson, aka MrBeast, reflects on AI videos infiltrating the internet, TechCrunch reports.







One more thing











The case against humans in spaceElon Musk and Jeff Bezos are bitter rivals in the commercial space race, but they agree on one thing: Settling space is an existential imperative. Space is the place. The final frontier. It is our human destiny to transcend our home world and expand our civilization to extraterrestrial vistas.This belief has been mainstream for decades, but its rise has been positively meteoric in this new gilded age of astropreneurs.But as visions of giant orbital stations and Martian cities dance in our heads, a case against human space colonization has found its footing in a number of recent books, from doubts about the practical feasibility of off-Earth communities, to realism about the harsh environment of space and the enormous tax it would exact on the human body. Read the full story.



‚ÄîBecky Ferreira







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)+ Wow: scientists have successfully reconstructed a million-year old skull + Take a trip back in time with this fun compilation of music from the very first Sims game.+ RIP ‚Äòstomp clap hey‚Äô‚Äîmusic‚Äôs most misunderstood and simultaneously annoying genre.+ How to live a good life in a tough world.
‚Ä¢ The three big unanswered questions about Sora
  Last week OpenAI released Sora, a TikTok-style app that presents an endless feed of exclusively AI-generated videos, each up to 10 seconds long. The app allows you to create a ‚Äúcameo‚Äù of yourself‚Äîa hyperrealistic avatar that mimics your appearance and voice‚Äîand insert other peoples‚Äô cameos into your own videos (depending on what permissions they set).&nbsp;



To some people who believed earnestly in OpenAI‚Äôs promise to build AI that benefits all of humanity, the app is a punchline. A former OpenAI researcher who left to build an AI-for-science startup referred to Sora as an ‚Äúinfinite AI tiktok slop machine.‚Äù&nbsp;



That hasn‚Äôt stopped it from soaring to the top spot on Apple‚Äôs US App Store. After I downloaded the app, I quickly learned what types of videos are, at least currently, performing well: bodycam-style footage of police pulling over pets or various trademarked characters, including SpongeBob and Scooby Doo; deepfake memes of Martin Luther King Jr. talking about Xbox; and endless variations of Jesus Christ navigating our modern world.&nbsp;



Just as quickly, I had a bunch of questions about what‚Äôs coming next for Sora. Here‚Äôs what I‚Äôve learned so far.



Can it last?



OpenAI is betting that a sizable number of people will want to spend time on an app in which you can suspend your concerns about whether what you‚Äôre looking at is fake and indulge in a stream of raw AI. One reviewer put it this way: ‚ÄúIt‚Äôs comforting because you know that everything you‚Äôre scrolling through isn‚Äôt real, where other platforms you sometimes have to guess if it‚Äôs real or fake. Here, there is no guessing, it‚Äôs all AI, all the time.&#8221;



This may sound like hell to some. But judging by Sora‚Äôs popularity, lots of people want it.&nbsp;



So what‚Äôs drawing these people in? There are two explanations. One is that Sora is a flash-in-the-pan gimmick, with people lining up to gawk at what cutting-edge AI can create now (in my experience, this is interesting for about five minutes). The second, which OpenAI is betting on, is that we‚Äôre witnessing a genuine shift in what type of content can draw eyeballs, and that users will stay with Sora because it allows a level of fantastical creativity not possible in any other app.&nbsp;



There are a few decisions down the pike that may shape how many people stick around: how OpenAI decides to implement ads, what limits it sets for copyrighted content (see below), and what algorithms it cooks up to decide who sees what.&nbsp;



Can OpenAI afford it?



OpenAI is not profitable, but that‚Äôs not particularly strange given how Silicon Valley operates. What is peculiar, though, is that the company is investing in a platform for generating video, which is the most energy-intensive (and therefore expensive) form of AI we have. The energy it takes dwarfs the amount required to create images or answer text questions via ChatGPT.



This isn‚Äôt news to OpenAI, which has joined a half-trillion-dollar project to build data centers and new power plants. But Sora‚Äîwhich currently allows you to generate AI videos, for free, without limits‚Äîraises the stakes: How much will it cost the company?&nbsp;





OpenAI is making moves toward monetizing things (you can now buy products directly through ChatGPT, for example). On October 3, its CEO, Sam Altman, wrote in a blog post that ‚Äúwe are going to have to somehow make money for video generation,‚Äù but he didn‚Äôt get into specifics. One can imagine personalized ads and more in-app purchases.&nbsp;



Still, it‚Äôs concerning to imagine the mountain of emissions might result if Sora becomes popular. Altman has accurately described the emissions burden of one query to ChatGPT as impossibly small. What he has not quantified is what that figure is for a 10-second video generated by Sora. It‚Äôs only a matter of time until AI and climate researchers start demanding it.&nbsp;



How many lawsuits are coming?&nbsp;



Sora is awash in copyrighted and trademarked characters. It allows you to easily deepfake deceased celebrities. Its videos use copyrighted music.&nbsp;



Last week, the Wall Street Journal reported that OpenAI has sent letters to copyright holders notifying them that they‚Äôll have to opt out of the Sora platform if they don‚Äôt want their material included, which is not how these things usually work. The law on how AI companies should handle copyrighted material is far from settled, and it‚Äôd be reasonable to expect lawsuits challenging this.&nbsp;



In last week‚Äôs blog post, Altman wrote that OpenAI is ‚Äúhearing from a lot of rightsholders‚Äù who want more control over how their characters are used in Sora. He says that the company plans to give those parties more ‚Äúgranular control‚Äù over their characters. Still, ‚Äúthere may be some edge cases of generations that get through that shouldn‚Äôt,‚Äù he wrote.



But another issue is the ease with which you can use the cameos of real people. People can restrict who can use their cameo, but what limits will there be for what these cameos can be made to do in Sora videos?&nbsp;



This is apparently already an issue OpenAI is being forced to respond to. The head of Sora, Bill Peebles, posted on October 5 that users can now restrict how their cameo can be used‚Äîpreventing it from appearing in political videos or saying certain words, for example. How well will this work? Is it only a matter of time until someone‚Äôs cameo is used for something nefarious, explicit, illegal, or at least creepy, sparking a lawsuit alleging that OpenAI is responsible?¬†



Overall, we haven‚Äôt seen what full-scale Sora looks like yet (OpenAI is still doling out access to the app via invite codes). When we do, I think it will serve as a grim test: Can AI create videos so fine-tuned for endless engagement that they‚Äôll outcompete ‚Äúreal‚Äù videos for our attention? In the end, Sora isn‚Äôt just testing OpenAI‚Äôs technology‚Äîit‚Äôs testing us, and how much of our reality we‚Äôre willing to trade for an infinite scroll of simulation.
‚Ä¢ This company is planning a lithium empire from the shores of the Great Salt Lake
  BOX ELDER COUNTY, Utah ‚Äì On a bright afternoon in August, the shore on the North Arm of the Great Salt Lake looks like something out of a science fiction film set in a scorching alien world. The desert sun is blinding as it reflects off the white salt that gathers and crunches underfoot like snow at the water‚Äôs edge. In a part of the lake too shallow for boats, bacteria have turned the water a Pepto-Bismol pink. The landscape all around is ringed with jagged red mountains and brown brush. The only obvious sign of people is the salt-encrusted hose running from the water‚Äôs edge to a makeshift encampment of shipping containers and trucks a few hundred feet away.&nbsp;



This otherworldly scene is the test site for a company called Lilac Solutions, which is developing a technology it says will shake up the United States‚Äô efforts to pry control over the global supply of lithium, the so-called ‚Äúwhite gold‚Äù needed for electric vehicles and batteries, away from China. Before tearing down its demonstration facility to make way for its first commercial plant, due online next year, the company invited me to be the first journalist to tour its outpost in this remote area, a roughly two-hour drive from Salt Lake City.



The startup is in a race to commercialize a new way to extract lithium from rocks, called direct lithium extraction (DLE). This approach is designed to reduce the environmental damage caused by the two most common traditional methods of mining lithium: hard-rock mining and brining.&nbsp;



Australia, the world‚Äôs top producer of lithium, uses the first approach, scraping rocks laden with lithium out of the earth so they can be chemically processed into industrial-grade versions of the metal. Chile, the second-largest lithium source, uses the second: It floods areas of its sun-soaked Atacama Desert with water. This results in ponds rich in dissolved lithium, which are then allowed to dry off, leaving behind lithium salts that can be harvested and processed elsewhere.&nbsp;



An intake hose, used to pump water to Lilac Solutions‚Äô demonstration site, snakes into the pink-hued Great Salt Lake.ALEXANDER KAUFMAN




The range of methods known as DLE use lithium brine too, but instead of water-intensive evaporation, they all involve advanced chemical or physical filtering processes that selectively separate out lithium ions. While DLE has yet to take off, its reduced need for water and land has made it a prime focus for companies and governments looking to ramp up production to meet the growing demand for lithium as electric vehicles take off and even bigger batteries are increasingly used to back up power grids. China, which processes more than two-thirds of the world‚Äôs mined lithium, is developing its own DLE to increase domestic production of the raw material. New approaches are still being researched, but nearly a dozen companies are actively looking to commercialize DLE technology now, and some industrial giants already offer basic off-the-shelf hardware.&nbsp;



In August, Lilac completed its most advanced test yet of its technology, which the company says doesn‚Äôt just require far less water than traditional lithium extraction‚Äîit uses a fraction of what other DLE approaches demand.&nbsp;





The company uses proprietary beads to draw lithium ions from water and says its process can extract lithium using a tenth as much water as the alumina sorbent technology that dominates the DLE industry. Lilac also highlights its all-American supply chain. Technology originally developed by Koch Industries, for example, uses some Chinese-made components. Lilac‚Äôs beads are manufactured at the company‚Äôs plant in Nevada.&nbsp;



Lilac says the beads are particularly well suited to extracting lithium where concentrations are low. That doesn‚Äôt mean they could be deployed just anywhere‚Äîthere won‚Äôt be lithium extraction on the Hudson River anytime soon. But Lilac‚Äôs tech could offer significant advantages over what‚Äôs currently on the market. And forgoing plans to become a major producer itself could enable the company to seize a decent slice of global production by appealing to lithium miners companies looking for the best equipment, says Milo McBride, a researcher at the Carnegie Endowment for International Peace who authored a recent report on DLE.&nbsp;



If everything pans out, the pilot plant Lilac builds next to prove its technology at commercial scale could significantly increase domestic supply at a moment when the nation‚Äôs largest proposed lithium project, the controversial hard-rock Thacker Pass mine in Nevada, has faced fresh uncertainty. At the beginning of October, the Trump administration renegotiated a federal loan worth more than $2 billion to secure a 5% ownership stake for the US government.&nbsp;



The blue tank on the left filters the brine from the Great Salt Lake to remove large particles before pumping the lithium-rich water into the ion-exchange systems located in the shipping containers.ALEXANDER KAUFMAN




Despite bipartisan government support, the prospect of opening a deep gash in an unspoiled stretch of Nevada landscape has drawn fierce opposition from conservationists and lawsuits from ranchers and Native American tribes who say the Thacker Pass project would destroy the underground freshwater reservoirs on which they depend. Water shortages in the parched West have also made it difficult to plan on using additional evaporation ponds, the other traditional way of extracting lithium.&nbsp;



Lilac is not the only company in the US pushing for DLE. In California‚Äôs Salton Sea, developers such as EnergySource Minerals are looking to build a geothermal power plant to power a DLE facility pulling lithium from the inland desert lake. And energy giants such as Exxon Mobil, Chevron, and Occidental Petroleum are racing to develop an area in southwestern Arkansas called the Smackover region, where researchers with the US Geological Survey have found as much as 19 million metric tons of untapped lithium in salty underground water. In between, both geographically and strategically, is Lilac: It‚Äôs looking to develop new technology like the California companies but sell its hardware to the energy giants in Arkansas.&nbsp;





The Great Salt Lake isn‚Äôt an obvious place to develop a lithium mine. The Salton Sea boasts lithium concentrations of just under 200 parts per million. Argentina, where Lilac has another test facility, has resources of above 700 parts per million.&nbsp;



Here on the Great Salt Lake? ‚ÄúIt‚Äôs 70 parts per million,‚Äù Raef Sully, Lilac‚Äôs Australia-born chief executive, tells me. ‚ÄúSo if you had a football stadium with 45,000 seats, this would be three people.‚Äù



For Lilac, this is actually a feature of the location. ‚ÄúIt‚Äôs a very, very good demonstration of the capability of our technology,‚Äù Sully says. Showing that Lilac‚Äôs hardware can extract lithium at high purity levels from a brine with low concentration, he says, proves its versatility. That wasn‚Äôt the reason Lilac selected the site, though. ‚ÄúUtah is a mining friendly state,‚Äù says Elizabeth Pond, the vice president of communications. And though the lake water has low concentrations of lithium, extracting the brine simply calls for running a hose into the water, whereas other locations would require digging a well at great cost.&nbsp;



When I accompanied Sully to the test site during my tour, our route following unpaved county roads lined with fields of wild sunflowers. The facility itself is little more than an assortment of converted shipping containers and two mobile trailers, one to serve as the main office and the other as a field laboratory to test samples. It‚Äôs off the grid, relying on diesel generators that the company says will be replaced with propane units once this location is converted to a permanent facility but could eventually be swapped for geothermal technology tapping into a hot rock resource located nearby. (Solar panels, Sully clarifies, couldn‚Äôt supply the 24-7 power supply the facility will need.) But it depends on its connection to the Great Salt Lake via that lengthy hose.&nbsp;



Hardened salt and impurities are encrusted on metal mesh that keeps larger materials out of Lilac‚Äôs water intake system.ALEXANDER KAUFMAN




Pumped uphill, the lake water passes through a series of filters to remove solids until it ends up in a vessel filled with the company‚Äôs specially designed ceramic beads, made from a patented material that attracts lithium ions from the water. Once saturated, the beads are put through an acid wash to remove the lithium. The remaining brine is then repeatedly tested and, once deemed safe to release back into the lake, pumped back down to the shore through an outgoing tube in the hose. The lithium solution, meanwhile, is stockpiled in tanks on site before shipping off to a processing plant to be turned into battery-grade lithium carbonate, which is a white powder.&nbsp;



‚ÄúAs a technology provider in the long term, if we‚Äôre going to have decades of lithium demand, they want to position their technology as something that can tap a bunch of markets,‚Äù McBride says. ‚ÄúTo have a technology that can potentially economically recover different types of resources in different types of environments is an enticing proposition.‚Äù&nbsp;



This testing ground won‚Äôt stay this way for long. During my visit, Lilac‚Äôs crew was starting to pack up the location after completing its demonstration testing. The results the company shared exclusively with me suggest a smashing success, particularly for such low-grade brine with numerous impurities: Lilac‚Äôs equipment recovered 87% of the available lithium, on average, with a purity rate of 99.97%.





The next step will be to clear the area to make way for construction of Lilac‚Äôs first permanent commercial facility at the same site. To meet the stipulations of Utah state permits for the new plant, the company had to cease all operations at the demonstration project. If everything goes according to plan, Lilac‚Äôs first US facility will begin commercial production in the second half of 2027. The company has lined up about two-thirds of its funding for the project. That could make the plant the first new commercial source of lithium in the US to come online in years, and the first DLE facility ever.&nbsp;



Once it‚Äôs fully online, the project should produce 5,000 tons per year‚Äîdoubling annual US production of lithium. But a full-scale plant using Lilac‚Äôs technology would produce between three and five times that amount.&nbsp;



There are some potential snags. Utah regulators this year started cracking down on mineral companies pumping water from the Great Salt Lake, which is shrinking amid worsening droughts. (Lilac says it‚Äôs largely immune to the restrictions since it returns the water to the lake.) While the relatively low concentrations of lithium in the water make for a good test case, full-scale commercial production would likely prove far more economical in a place with more of the metal.&nbsp;



Wild sunflowers line the unpaved county roads that cut through ranching land en route to Lilac Solutions‚Äô remote demonstration site.ALEXANDER KAUFMAN




‚ÄúThe Great Salt Lake is probably the worst possible place to be doing this, because there are real challenges around pulling water from the lake,‚Äù says Ashley Zumwalt-Forbes, a mining engineer who previously served as the deputy director of battery minerals at the Department of Energy. ‚ÄúBut if it‚Äôs just being used as a trial for the technology, that makes sense.‚Äù&nbsp;



What makes Lilac stand out among its peers is that it has no plans to design and manufacture its own DLE equipment and produce actual lithium. Lilac wants instead to sell its technology to others. The pilot plant is just intended to test and debut its hardware. Sully tells me it‚Äôs being built under a separate limited-liability corporation to make a potential sale easier if it‚Äôs successful.&nbsp;



It‚Äôs an unusual play in the lithium industry. Once most companies see success with their technology, ‚Äúthey go crazy and think they can vertically integrate and at the same time be a miner and an energy producer,‚Äù Kwasi Ampofo, the head of minerals and metals at the energy consultancy BloombergNEF, tells me.&nbsp;



‚ÄúLilac is trying to be a technology vendor,‚Äù he says. ‚ÄúI wonder why a lot more people aren‚Äôt choosing that route.‚Äù&nbsp;



If things work out the right way, Sully says, Lilac could become the vendor of choice to projects like the oil-backed sites in the Smackover and beyond.&nbsp;



‚ÄúWe think our technology is the next generation,‚Äù he says. ‚ÄúAnd if we end up working with an Exxon or a Chevron or a Rio Tinto, we want to be the DLE technology provider in their lithium project.‚Äù

üîí Cybersecurity & Privacy
‚Ä¢ ShinyHunters Wage Broad Corporate Extortion Spree
  A cybercriminal group that used voice phishing attacks to siphon more than a billion records from Salesforce customers earlier this year has launched a website that threatens to publish data stolen from dozens of Fortune 500 firms if they refuse to pay a ransom. The group also claimed responsibility for a recent breach involving Discord user data, and for stealing terabytes of sensitive files from thousands of customers of the enterprise software maker Red Hat.
The new extortion website tied to ShinyHunters (UNC6040), which threatens to publish stolen data unless Salesforce or individual victim companies agree to pay a ransom.
In May 2025, a prolific and amorphous English-speaking cybercrime group known as ShinyHunters launched a social engineering campaign that used voice phishing to trick targets into connecting a malicious app to their organization&#8217;s Salesforce portal.
The first real details about the incident came in early June, when the Google Threat Intelligence Group (GTIG)¬†warned that ShinyHunters &#8212; tracked by Google as UNC6040 &#8212;¬†was extorting victims over their stolen Salesforce data, and that the group was poised to launch a data leak site to publicly shame victim companies into paying a ransom to keep their records private. A month later, Google acknowledged that one of its own corporate Salesforce instances was impacted in the voice phishing campaign.
Last week, a new victim shaming blog dubbed &#8220;Scattered LAPSUS$ Hunters&#8221; began publishing the names of companies that had customer Salesforce data stolen as a result of the May voice phishing campaign.
&#8220;Contact us to negotiate this ransom or all your customers data will be leaked,&#8221; the website stated in a message to Salesforce. &#8220;If we come to a resolution all individual extortions against your customers will be withdrawn from. Nobody else will have to pay us, if you pay, Salesforce, Inc.&#8221;
Below that message were more than three dozen entries for companies that allegedly had Salesforce data stolen, including Toyota, FedEx, Disney/Hulu, and UPS. The entries for each company specified the volume of stolen data available, as well as the date that the information was retrieved (the stated breach dates range between May and September 2025).
Image: Mandiant.
On October 5, the Scattered LAPSUS$ Hunters victim shaming and extortion blog announced that the group was responsible for a breach in September involving a GitLab server used by Red Hat that contained more than 28,000 Git code repositories, including more than 5,000 Customer Engagement Reports (CERs).
&#8220;Alot of folders have their client&#8217;s secrets such as artifactory access tokens, git tokens, azure, docker (redhat docker, azure containers, dockerhub), their client&#8217;s infrastructure details in the CERs like the audits that were done for them, and a whole LOT more, etc.,&#8221; the hackers claimed.
Their claims came several days after a previously unknown hacker group calling itself the Crimson Collective took credit for the Red Hat intrusion on Telegram.
Red Hat disclosed on October 2 that attackers had compromised a company GitLab server, and said it was in the process of notifying affected customers.
&#8220;The compromised GitLab instance housed consulting engagement data, which may include, for example, Red Hat‚Äôs project specifications, example code snippets, internal communications about consulting services, and limited forms of business contact information,&#8221; Red Hat wrote.
Separately, Discord has started emailing users affected by another breach claimed by ShinyHunters. Discord said an incident on September 20 at a &#8220;third-party customer service provider&#8221; impacted a &#8220;limited number of users&#8221; who communicated with Discord customer support or Trust &amp; Safety teams. The information included Discord usernames, emails, IP address, the last four digits of any stored payment cards, and government ID images submitted during age verification appeals.
The Scattered Lapsus$ Hunters claim they will publish data stolen from Salesforce and its customers if ransom demands aren&#8217;t paid by October 10. The group also claims it will soon begin extorting hundreds more organizations that lost data in August after a cybercrime group stole vast amounts of authentication tokens from Salesloft, whose AI chatbot is used by many corporate websites to convert customer interaction into Salesforce leads.
In a communication sent to customers today, Salesforce emphasized that the theft of any third-party Salesloft data allegedly stolen by ShinyHunters did not originate from a vulnerability within the core Salesforce platform. The company also stressed that it has no plans to meet any extortion demands.
&#8220;Salesforce will not engage, negotiate with, or pay any extortion demand,&#8221; the message to customers read. &#8220;Our focus is, and remains, on defending our environment, conducting thorough forensic analysis, supporting our customers, and working with law enforcement and regulatory authorities.&#8221;
The GTIG tracked the group behind the Salesloft data thefts as UNC6395, and says the group has been observed harvesting the data for authentication tokens tied to a range of cloud services like Snowflake and Amazon&#8217;s AWS.
Google catalogs Scattered Lapsus$ Hunters by so many UNC names (throw in UNC6240 for good measure) because it is thought to be an amalgamation of three hacking groups &#8212; Scattered Spider, Lapsus$ and ShinyHunters. The members of these groups hail from many of the same chat channels on the Com, a mostly English-language cybercriminal community that operates across an ocean of Telegram and Discord servers.
The Scattered Lapsus$ Hunters darknet blog is currently offline. The outage appears to have coincided with the disappearance of the group&#8217;s new clearnet blog &#8212; breachforums[.]hn &#8212; which vanished after shifting its Domain Name Service (DNS) servers from DDoS-Guard to Cloudflare.
But before it died, the websites disclosed that hackers were exploiting a critical zero-day vulnerability in Oracle&#8217;s E-Business Suite software. Oracle has since confirmed that a security flaw tracked as CVE-2025-61882 allows attackers to perform unauthenticated remote code execution, and is urging customers to apply an emergency update to address the weakness.
Mandiant&#8217;s Charles Carmichael shared on LinkedIn that CVE-2025-61882 was initially exploited in August 2025 by the Clop ransomware gang to steal data from Oracle E-Business Suite servers. Bleeping Computer writes that news of the Oracle zero-day first surfaced on the Scattered Lapsus$ Hunters blog, which published a pair of scripts that were used to exploit vulnerable Oracle E-Business Suite instances.
On Monday evening, KrebsOnSecurity received a malware-laced message from a reader that threatened physical violence unless their unstated demands were met. The missive, titled &#8220;Shiny hunters,&#8221; contained the hashtag $LAPSU$$SCATEREDHUNTER, and urged me to visit a page on limewire[.]com to view their demands.
A screenshot of the phishing message linking to a malicious trojan disguised as a Windows screenshot file.
KrebsOnSecurity did not visit this link, but instead forwarded it to Mandiant, which confirmed that similar menacing missives were sent to employees at Mandiant and other security firms around the same time.
The link in the message fetches a malicious trojan disguised as a Windows screenshot file (Virustotal&#8217;s analysis on this malware is here). Simply viewing the booby-trapped screenshot image on a Windows PC is enough to cause the bundled trojan to launch in the background.
Mandiant&#8217;s Austin Larsen said the trojan is a commercially available backdoor known as ASYNCRAT, which is a .NET-based backdoor that communicates using a custom binary protocol over TCP, and can execute shell commands and download plugins to extend its features.
A scan of the malicious screenshot file at Virustotal.com shows it is detected as bad by nearly a dozen security and antivirus tools.
&#8220;Downloaded plugins may be executed directly in memory or stored in the registry,&#8221; Larsen wrote in an analysis shared via email. &#8220;Capabilities added via plugins include screenshot capture, file transfer, keylogging, video capture, and cryptocurrency mining. ASYNCRAT also supports a plugin that targets credentials stored by Firefox and Chromium-based web browsers.&#8221;
Malware-laced targeted emails are not out of character for certain members of the Scattered Lapsus$ Hunters, who have previously harassed and threatened security researchers and even law enforcement officials who are investigating and warning about the extent of their attacks.
With so many big data breaches and ransom attacks now coming from cybercrime groups operating on the Com, law enforcement agencies on both sides of the pond are under increasing pressure to apprehend the criminal hackers involved. In late September, prosecutors in the U.K. charged two alleged Scattered Spider members aged 18 and 19 with extorting at least $115 million in ransom payments from companies victimized by data theft.
U.S. prosecutors heaped their own charges on the 19 year-old in that duo &#8212; U.K. resident Thalha Jubair &#8212;¬†who is alleged to have been involved in data ransom attacks against Marks &amp; Spencer and Harrods, the British foot retailer Co-op Group, and the 2023 intrusions at MGM Resorts and Caesars Entertainment. Jubair also was allegedly a key member of LAPSUS$, a cybercrime group that broke into dozens of technology companies beginning in late 2021.
A Mastodon post by Kevin Beaumont, lamenting the prevalence of major companies paying millions to extortionist teen hackers, refers derisively to Thalha Jubair as a part of an APT threat known as &#8220;Advanced Persistent Teenagers.&#8221;
In August, convicted Scattered Spider member and 20-year-old Florida man Noah Michael Urban was sentenced to 10 years in federal prison and ordered to pay roughly $13 million in restitution to victims.
In April 2025, a 23-year-old Scottish man thought to be an early Scattered Spider member was extradited from Spain to the U.S., where he is facing charges of wire fraud, conspiracy and identity theft. U.S. prosecutors allege¬†Tyler Robert Buchanan¬†and co-conspirators hacked into dozens of companies in the United States and abroad, and that he personally controlled more than $26 million stolen from victims.

üéì University AI
No updates.

üè¢ Corporate AI
‚Ä¢ Ideas: More AI-resilient biosecurity with the Paraphrase Project
  Behind every emerging technology is a great idea propelling it forward. In the Microsoft Research Podcast series Ideas, members of the research community at Microsoft discuss the beliefs that animate their research, the experiences and thinkers that inform it, and the positive human impact it targets.



AI has been described as a ‚Äúdual use‚Äù technology: the capabilities that can be leveraged for good can also potentially be used to cause harm. In this episode, Microsoft Chief Scientific Officer Eric Horvitz and his guests‚ÄîBruce Wittmann, a senior applied scientist at Microsoft; Tessa Alexanian (opens in new tab), a technical lead at the International Biosecurity and Biosafety Initiative for Science (IBBIS);&nbsp;and James Diggans (opens in new tab), a vice president at Twist Bioscience‚Äîexplore this idea in the context of AI-powered protein design.



With Horvitz at the lead, Alexanian, Diggans, and Wittmann were part of a cross-sector team that demonstrated toxic protein candidates could be designed with help from AI‚Äîand that they could bypass the systems in place to defend against their creation. The project, known as the Paraphrase Project, culminated in a cybersecurity-style response, a more robust protein screening system, and a modified approach to peer review with implications for how we think about and tackle AI risk more broadly. The work was recently published in Science.








Learn more:




Strengthening nucleic acid biosecurity screening against generative protein design toolsPublication | October 2025



Toward AI-Resilient Screening of Nucleic Acid Synthesis Orders: Process, Results, and RecommendationsPreprint | December 2024



The Paraphrase Project: Designing defense for an era of synthetic biologyMicrosoft Research Blog | October 2025



When AI meets biology: Promise, risk, and responsibilityMicrosoft Research Blog | Eric Horvitz | October 2025



Paraphrase ProjectProject homepage










	
		
			Subscribe to the Microsoft Research Podcast:		
		
							
					
						  
						Apple Podcasts
					
				
			
							
					
						
						Email
					
				
			
							
					
						
						Android
					
				
			
							
					
						
						Spotify
					
				
			
							
					
						
						RSS Feed
					
				
					
	




	
		
			
				
					

Transcript



[MUSIC]



ERIC HORVITZ: You‚Äôre&nbsp;listening to&nbsp;Ideas, a Microsoft Research Podcast that dives deep into the world of technology research and the profound questions behind the code.&nbsp;I‚Äôm&nbsp;Eric Horvitz, Microsoft‚Äôs chief scientific officer, and in this series, we explore the technologies shaping our future and the&nbsp;big ideas&nbsp;that propel them forward.



[MUSIC FADES]



Today,&nbsp;I‚Äôm&nbsp;excited to talk about the Paraphrase Project, an effort I co-led exploring how&nbsp;advances in&nbsp;AI tools&nbsp;for protein design&nbsp;might&nbsp;impact&nbsp;biosecurity. The results were reported in our recent paper,&nbsp;‚ÄúStrengthening nucleic acid biosecurity screening against generative protein design tools,‚Äù (opens in new tab)&nbsp;published in&nbsp;Science&nbsp;on Oct. 2.&nbsp;



Joining me are&nbsp;three&nbsp;of the larger set of&nbsp;coauthors on that paper:&nbsp;Bruce Wittmann, senior applied scientist at Microsoft;&nbsp;James&nbsp;Diggans, vice president at Twist Bioscience and chair of the board for the International Gene Synthesis Consortium;&nbsp;and Tessa Alexanian, technical lead at the International Biosecurity and Biosafety Initiative for Science, also known as IBBIS.&nbsp;



				
				
					



Now, let‚Äôs&nbsp;rewind two years.&nbsp;Almost to&nbsp;the day, Bruce and I uncovered a vulnerability. While preparing a case study for a workshop on AI and biosecurity, we discovered that open-source AI protein design tools could be used to redesign toxic proteins in ways that could bypass biosecurity screening systems, systems set up to&nbsp;identify&nbsp;incoming orders of concern.&nbsp;



Now in that work, we&nbsp;created an AI pipeline from open-source tools that could&nbsp;essentially ‚Äúparaphrase‚Äù the amino acid sequences‚Äîreformulating&nbsp;them while&nbsp;working to&nbsp;preserve&nbsp;their structure and potentially their function.&nbsp;



These paraphrased sequences could evade the screening systems used by major DNA&nbsp;synthesis companies, and these are the systems that scientists rely on to safely produce AI-designed proteins.&nbsp;



Now, experts in the field described this finding as the first ‚Äúzero day‚Äù for AI and biosecurity.&nbsp;And this&nbsp;marked the beginning of a deep, two-year collaborative effort to investigate and address this challenge.&nbsp;



With the help of a&nbsp;strong&nbsp;cross-sector team‚Äîincluding James, Tessa, Bruce, and many others‚Äîwe worked behind the scenes to build AI biosecurity&nbsp;red-teaming approaches,&nbsp;probe for vulnerabilities, and to design practical fixes. These ‚Äúpatches,‚Äù akin to those in cybersecurity,&nbsp;have now been shared with&nbsp;organizations&nbsp;globally to strengthen biosecurity screening.&nbsp;



This has been one of the most fascinating projects&nbsp;I‚Äôve&nbsp;had the privilege to work on, for its technical complexity, its ethical and policy dimensions, and the remarkable collaboration across industry, government, and nonprofit sectors.&nbsp;



The project highlights that the&nbsp;same AI tools capable of&nbsp;incredible&nbsp;good can also be misused, requiring us to be vigilant, thoughtful, and creative so we continue to get the most benefit out of AI tools while working to ensure&nbsp;that&nbsp;we avoid costly misuses.&nbsp;



With that, let me officially welcome our guests.



Bruce, James, Tessa, welcome to the podcast.



BRUCE WITTMANN: Thanks, Eric.



JAMES DIGGANS: Thanks for having us.



HORVITZ: It&#8217;s been such a pleasure working closely with each of you, not only for your expertise but also for your deep commitment and passion about public health and global safety.



Before we dive into the technical side of things, I&#8217;d like to ask each of you, how did you get into this field? What inspired you to become biologists and then pursue the implications of advances in AI for biosecurity? Bruce?



WITTMANN:&nbsp;Well, I&#8217;ve always liked building things. That&#8217;s where I would say I come from. You know, my hobbies when I&#8217;m not working on biology or AI things‚Äîas you know, Eric‚Äîis, like, building things around the house, right. Doing construction. That kind of stuff.



But my broader interests have always been biology, chemistry. So I originally got into organic chemistry. I found that was fascinating. From there, went to synthetic biology, particularly metabolic engineering, because that&#8217;s kind of like organic chemistry, but you&#8217;re wiring together different parts of an organism‚Äôs metabolism rather than different chemical reactions. And while I was working in that space, I, kind of, had the thought of there&#8217;s got to be an easier way to do this [LAUGHS] because it is really difficult to do any type of metabolic engineering. And that&#8217;s how I got into the AI space, trying to solve these very complicated biological problems, trying to build things that we don&#8217;t necessarily even understand using our understanding from data or deriving understanding from data.



So, you know, that&#8217;s the roundabout way of how I got to where I am‚Äîthe abstract way of how I got to where I am.



HORVITZ: And, Tessa, what motivated you to jump into this area and zoom into biology and biosciences and helping us to avoid catastrophic outcomes?



ALEXANIAN: Yeah, I mean, probably the origin of me being really excited about biology is actually a book called [The] Lives of [a] Cell (opens in new tab) by Lewis Thomas, which is an extremely beautiful book of essays that made me be like, Oh, wow, life is just incredible. I think I read it when I was, you know, 12 or 13, and I was like, Life is incredible. I want to work on this. This is the most beautiful science, right. And then I, in university, I was studying engineering, and I heard there was this engineering team for engineering biology‚Äîthis iGEM (opens in new tab) team‚Äîand I joined it, and I thought, Oh, this is so cool. I really got to go work in this field of synthetic biology.



And then I also tried doing the wet lab biology, and I was like, Oh, but I don&#8217;t like this part. I don&#8217;t actually, like, like babysitting microbes. [LAUGHTER] I think there&#8217;s a way ‚Ä¶ some people who are great wet lab biologists are made of really stern stuff. And they really enjoy figuring out how to redesign their negative controls so they can figure out whether it was contamination or whether it was, you know, temperature fluctuation. I&#8217;m not that, apparently.



And so I ended up becoming a lab automation engineer because I could help the science happen, but I ‚Ä¶ but my responsibilities were the robots and the computers rather than the microbes, which I find a little bit intransigent.



HORVITZ: Right. I was thinking of those tough souls; they also used their mouths to do pipetting and so on of these contaminated fluids ‚Ä¶



WITTMANN: Not anymore. ALEXANIAN: It&#8217;s true. [LAUGHTER]



DIGGANS: Not anymore. [LAUGHS]



ALEXANIAN: They used to be tougher. They used to be tougher.



HORVITZ: James.



DIGGANS: So I did my undergrad in computer science and microbiology, mostly because at the time, I couldn&#8217;t pick which of the two I liked more. I liked them both. And by the time I graduated, I was lucky enough that I realized that the intersection of the two could be a thing. And so I did a PhD in computational biology, and then I worked for five years at the MITRE Corporation. It‚Äôs a nonprofit. I got the chance to work with the US biodefense community and just found an incredible group of people working to protect forces and the population at large from biological threats and just learned a ton about both biology and also dual-use risk. And then so when Twist called me and asked if I wanted to join Twist and set up their biosecurity program, I leapt at the chance and have done that for the past 10 years.



HORVITZ: Well, thanks everyone.



I believe that AI-powered protein design in particular is one of the most exciting frontiers of modern science. It holds promise for breakthroughs in medicine, public health, even material science. We&#8217;re already seeing it lead to new vaccines, novel therapeutics, and‚Äîon the scientific front‚Äîpowerful insights into the machinery of life.



So there&#8217;s much more ahead, especially in how AI can help us promote wellness, longevity, and the prevention of disease. But before we get too far ahead, while some of our listeners work in bioscience, many may not have a good understanding of some of the foundations.



So, Bruce, can you just give us a high-level overview of proteins? What are they? Why are they important? How do they figure into human-designed applications?



WITTMANN: Sure. Yeah. Fortunately, I used to TA a class on AI for protein design, so it‚Äôs right in my wheelhouse. [LAUGHS]



HORVITZ: Perfect, perfect background. [LAUGHS]



WITTMANN:&nbsp;It&#8217;s perfect. Yeah. I got to go back to all of that. Yeah, so from the very basic level, proteins are the workhorses of life.



Every chemical reaction that happens in our body‚Äîwell, nearly every chemical reaction that happens in our body‚Äîmost of the structure of our cells, you name it. Any life process, proteins are central to it.



Now proteins are encoded by what are known as ‚Ä¶ well, I shouldn&#8217;t say encoded. They are constructed from what are called amino acids‚Äîthere are 20 of them‚Äîand depending on the combination and order in which you string these amino acids together, you get a different protein sequence. So that&#8217;s what we mean when we say protein sequence.



The sequence of a protein then determines what shape that protein folds into in a cell, and that shape determines what the protein does. So we will often say sequence determines structure, which determines function.



Now the challenge that we face in engineering proteins is just how many possibilities there are. For all practical purposes, it&#8217;s infinite. So we have 20 building blocks. There are on average around 300 amino acids in a protein. So that&#8217;s 20 to the power of 300 possible combinations. And a common reference point is that it&#8217;s estimated there are around 10 to the 80 particles in the observable universe. So beyond astronomical numbers of possible combinations that we could have, and the job of a protein engineer is to find that one or a few of the proteins within that space that do what we want it to do.



So when a human has an idea of, OK, here&#8217;s what I want a protein to do, we have various techniques of finding that desired protein, one of which is using artificial intelligence and trying to either sift through that milieu of potential proteins or, as we&#8217;ll talk about more in this podcast, physically generating them. So creating them in a way, sampling them out of some distribution of reasonable proteins.



HORVITZ: Great. So I wanted to throw it to James now to talk about how protein design goes from computer to reality‚Äîfrom in silico to test tubes. What role does Twist Bioscience (opens in new tab) play in transforming digital protein designs into synthesized proteins? And maybe we can talk also about what safeguards are in place at your company and why do we need them.



DIGGANS: So all of these proteins that Bruce has described are encoded in DNA. So the language that our cells use to kind of store the information about how to make these proteins is all encoded in DNA. And so if you as an engineer have designed a protein and you want to test it to see if it does what you think it does, the first step is to have the DNA that encodes that protein manufactured, and companies like Twist carry out that role.



So we are cognizant also, however, that these are what are called dual-use technologies. So you can use DNA and proteins for an incredible variety of amazing applications. So drug development, agricultural improvements, bioindustrial manufacturing, all manner of incredible applications. But you could also potentially use those to cause harm so toxins or other, you know, sort of biological misuse.



And so the industry has since at least 2010 recognized that they have a responsibility to make sure that when we&#8217;re asked to make some sequence of DNA that we understand what that thing is encoding and who we&#8217;re giving it for. So we&#8217;re screening both the customer that&#8217;s coming to us and we&#8217;re screening the sequence that they&#8217;re requesting.



And so Twist has long invested in a very, sort of, complicated system for essentially reverse engineering the constructs that we&#8217;re asked to make so that we understand what they are. And then a system where we engage with our customers and make sure that they&#8217;re going to use those for legitimate purpose and responsibly.



HORVITZ: And how do the emergence of these new generative AI tools influence how you think about risk?



DIGGANS: A lot of the power of these AI tools is they allow us to make proteins or design proteins that have never existed before in nature to carry out functions that don&#8217;t exist in the natural world. That&#8217;s an extremely powerful capability.



But the existing defensive tools that we use at DNA synthesis companies generally rely on what&#8217;s called homology, similarity to known naturally occurring sequences, to determine whether something might pose risk. And so AI tools kind of break the link between those two things.



HORVITZ: Now you also serve as chair of the International Gene Synthesis Consortium (opens in new tab). Can you tell us a little bit more about the IGSC, its mission, how it supports global biosecurity?



DIGGANS: Certainly. So the IGSC was founded in 2010[1] and right now has grown to more than 40 companies and organizations across 10 countries. And the IGSC is essentially a place where companies who might be diehard competitors in the market around nucleic acid synthesis come together and design and develop best practices around biosecurity screening to, kind of, support the shared interest we all have in making sure that these technologies are not subject to misuse.



HORVITZ: Thanks, James. Now, Tessa, your organization, IBBIS (opens in new tab) is focused‚Äîit&#8217;s a beautiful mission‚Äîon advancing science while minimizing catastrophic risk, likelihood of catastrophic risk. When we say catastrophic risk, what do we really mean, Tessa, in the context of biology and AI? And how is that ‚Ä¶ do you view that risk landscape as evolving as AI capabilities are growing?



ALEXANIAN: I think the ‚Ä¶ to be honest, as a person who&#8217;s been in biosecurity for a while, I&#8217;ve been surprised by how much of the conversation about the risks from advances in artificial intelligence has centered on the risk of engineered biological weapons and engineered pandemics.



Even recently, there was a new discussion on introducing redlines for AI that came up at the UN General Assembly. And the very first item they list in their list of risks, if I&#8217;m not mistaken, was engineered pandemics, which is exactly the sort of thing that people fear could be done, could be done with these biological AI tools.



Now, I think that when we talk about catastrophic risk, we talk about, you know, something that has an impact on a large percentage of humanity. And I think the reason that we think that biotechnologies pose a catastrophic risk is that we believe there, as we&#8217;ve seen with many historical pandemics, there&#8217;s a possibility for something to emerge or be created that is beyond our society&#8217;s ability to control.



You know, there were a few countries in COVID that managed to more or less successfully do a zero-COVID policy, but that was not, that was not most countries. That was not any of the countries that I lived in. And, you know, we saw millions of people die. And I think we believe that with something like the 1918 influenza, which had a much higher case fatality rate, you could have far more people die.



Now, why we think about this in the context of AI and where this connects to DNA synthesis is that, you know, there is a ‚Ä¶ these risks of both, sort of, public health risks, pandemic risks, and misuse risks‚Äîpeople deliberately trying to do harm with biology, as we&#8217;ve seen from the long history of biological weapons programs‚Äîyou know, we think that those might be accelerated in a few different ways by AI technology, both the potential ‚Ä¶ and I say potential here because as everyone who has worked in a wet lab‚Äîwhich I think is everyone on this call‚Äîknows, engineering biology is really difficult. So there&#8217;s maybe a potential for it to become easier to develop biological technology for the purposes of doing harm, and there&#8217;s maybe also the potential to create novel threats.



And so I think people talk about both of those, and people have been looking hard for possible safeguards. And I think one safeguard that exists in this biosecurity world that, for example, doesn&#8217;t exist as cleanly in the cybersecurity world is that none of these biological threats can do harm until they are realized in physical reality, until you actually produce the protein or produce the virus or the microorganism that could do harm. And so I think at this point of production, both in DNA synthesis and elsewhere, we have a chance to introduce safeguards that can have a really large impact on the amount of risk that we&#8217;re facing‚Äîas long as we develop those safeguards in a way that keeps pace with AI.



HORVITZ: Well, thanks, Tessa. So, Bruce, our project began when I posed a challenge to you of the form: could current open-source AI tools be tasked with rewriting toxic protein sequences in a way that preserves their native structure, and might they evade today&#8217;s screening systems?



And I was preparing for a global workshop on AI and biosecurity that I&#8217;d been organizing with Frances Arnold, David Baker, and Lynda Stuart, and I wanted a concrete case study to challenge attendees. And what we found was interesting and deeply concerning.



So I wanted to dive in with you, Bruce, on the technical side. Can you describe some about the generative pipeline and how it works and what you did to build what we might call an AI and biosecurity red-teaming pipeline for testing and securing biosecurity screening tools?



WITTMANN: Sure. Yeah. I think the best place to start with this is really by analogy.



An analogy I often use in this case is the type of image generation AI tools we&#8217;re all familiar with now where I can tell the AI model, &#8220;Hey, give me a cartoonish picture of a dog playing fetch.&#8221; And it&#8217;ll do that, and it&#8217;ll give us back something that is likely never been seen before, right. That exact image is new, but the theme is still there. The theme is this dog.



And that&#8217;s kind of the same technology that we&#8217;re using in this red-teaming pipeline. Only rather than using plain language, English, we&#8217;re passing in what we would call conditioning information that is relevant to a protein.



So our AI models aren&#8217;t at the point yet where I can say, &#8220;Give me a protein that does x.&#8221; That would be the dream. We&#8217;re a long way from that. But what instead we do is we pass in things that match that theme that we&#8217;re interested in. So rather than saying, &#8220;Hey, give me back the theme on a dog,&#8221; we pass in information that we know will cause or at least push this generative model to create a protein that has the characteristics that we want.



So in the case of that example you just mentioned, Eric, it would be the protein structure. Like I mentioned earlier, we usually say structure determines function. There&#8217;s obviously a lot of nuance to that, but we can, at a first approximation, say structure determines function. So if I ask an AI model, ‚ÄùHey, here&#8217;s this structure; give me a protein sequence that folds to this structure,‚Äù just like with that analogy with the dog, it&#8217;s going to give me something that matches that structure but that is likely still never been seen before. It&#8217;s going to be a new sequence.



So you can imagine taking this one step further. In the red-teaming pipeline, what we would do is take a protein that should normally be captured by DNA synthesis screening‚Äîthat would be captured by DNA synthesis screening‚Äîfind its structure, pass it through one of these models, and get variants on the theme of that structure so these new sequences, these synthetic homologs that you mentioned, paraphrased, reformulated, whatever phrase we want to use to describe them.



And they have a chance or a greater chance than not of maintaining the structure and so maintaining the function while being sufficiently different that they&#8217;re not detected by these tools anymore.



So that&#8217;s the nuts and bolts of how the red-teaming pipeline comes together. We use more tools than just structure. I think structure is the easiest one to understand. But we have a suite of tools in there, each pass different conditioning information that causes the model to generate sequences that are paraphrased versions of potential proteins of concern.



HORVITZ: But to get down to brass tacks, what Bruce did for the framing study was ‚Ä¶ we took the toxic, well-known toxic protein ricin, as we described in a framing paper that&#8217;s actually part of the appendix now to the Science publication, and we generated through this pipeline, composed of open-source tools, thousands of AI-rewritten versions of ricin.



And this brings us to the next step of our project, way back when, at the early ‚Ä¶ in the early days of this effort, where Twist Bioscience was one of the companies we approached with what must have seemed like an unusual question to your CEO, in fact, James: would you be open to testing whether current screening systems could detect thousands of AI-rewritten versions of ricin, a well-known toxic protein?



And your CEO quickly connected me with you, James. So, James, what were your first thoughts on hearing about this project, and how did you respond to our initial framing study?



DIGGANS: I think my first response was gratitude and excitement. So it was fantastic that Microsoft had really leaned forward on this set of ideas and had produced this dataset. But to have it, you know, show up on our doorstep in a very concrete way with a partner that was ready to, sort of, help us try and address that, I think was a really ‚Ä¶ a valuable opportunity. And so we really leapt at that.



HORVITZ: And the results were that both for you and another company, major producer IDT [Integrated DNA Technologies], those thousands of variants flew through ‚Ä¶ flew under the radar of the biosecurity screening software as we covered in that framing paper.



Now, after our initial findings on this, we quietly shared the paper with a few trusted contacts, including some in government. Through my work with the White House Office of Science and Technology Policy, or OSTP, we connected up with biosecurity leads there, and it was an OSTP biosecurity lead who described our results as the first zero day in AI and biosecurity. And now in cybersecurity, a zero day is a vulnerability unknown to defenders generally, meaning there&#8217;s no time to respond before it could be exploited should it be known.



In that vein, we took a cybersecurity approach. We stood up a CERT‚ÄîC-E-R-T‚Äîa cybersecurity [computer] emergency response team approach used in responding to cybersecurity vulnerabilities, and we implemented this process to address what we saw as a vulnerability with AI-enabled challenges to biosecurity.



At one point down the line, it was so rewarding to hear you say, James, ‚ÄúI&#8217;m really glad Microsoft got here first.‚Äù I&#8217;m curious how you think about this kind of AI-enabled vulnerability compared to other ones, biosecurity threats, you&#8217;ve encountered, and I&#8217;d love to hear your perspective on how we handled the situation from the early discovery to the coordination and outreach.



DIGGANS: Yeah, I think in terms of comparison known threats, the challenge here is really there is no good basis on which we can just, sort of, say, Oh, I&#8217;ll build a new tool to detect this concrete universe of things, right. This was more a pattern of I&#8217;m going to use tools‚Äîand I love the name ‚ÄúParaphrase‚Äù; it&#8217;s a fantastic name‚ÄîI can paraphrase anything that I would normally think of as biological ‚Ä¶ as posing biological risk, and now that thing is harder to detect for existing tools. And so that really was a very eye-opening experience, and I think the practice of forming this CERT response, putting together a group of people who were well versed not just in the threat landscape but also in the defensive technologies, and then figuring out how to mitigate that risk and broaden that study, I think, was a really incredibly valuable response to the entire synthesis industry.



HORVITZ: Yeah, and, Bruce, can you describe a little bit about the process by which we expanded the effort beyond our initial framing study to more toxins and then to a larger challenge set and then the results that we pursued and achieved?



WITTMANN:&nbsp;Yeah, of course. So, you know, using machine learning lingo, you don&#8217;t want to overfit to a single example. So early on with this, as part of the framing study, we were able to show or I should say James and coworkers across the screening field were able to show that this could be patched, right. We needed to just make some changes to the tools,&nbsp;and we could at the very least detect ricin or reformulated versions of ricin.



So the next step of course was then, OK, how generalizable are these patches? Can they detect other reformulated sequences, as well? So we had to expand the set of proteins that we had reformulated. We couldn&#8217;t just do 10s of thousands of ricins. We had to do 10s of thousands of name your other potentially hazardous ‚Ä¶



HORVITZ: I think we had 72, was it?



WITTMANN:&nbsp;It was 72 in the end that we ended up at. I believe, James, it was you and maybe Jake, another one of the authors on the list ‚Ä¶ on the paper, who primarily put that list together &#8230;



HORVITZ: This is Jacob Beal ‚Ä¶ Jacob Beal at Raytheon BBN.



WITTMANN: I think James actually might be the better one to answer how this list was expanded.



DIGGANS:&nbsp;Initially the focus [was] on ricin as a toxin so that list expanded to 62 sort of commonly controlled toxins that are subject to an export control restriction or other concern. And then on top of that, we added 10 viral proteins. So we didn&#8217;t really just want to look at toxins. We also wanted to look at viral proteins, largely because those proteins tend to have multiple functions. They have highly constrained structures. And so if we could work in a toxin context, could Paraphrase also do the same for viral proteins, as well.



HORVITZ: And, Bruce, can you describe some about how we characterize the updates and the, we&#8217;ll say, the boost in capabilities of the patched screening tools?



WITTMANN: So we had, like you said, Eric, 72 base proteins or template proteins. And for each of those, we had generated a few 100 to a couple thousand reformulated variants of them. The only way to really get any sense of validity of those sequences was to predict their structures. So we predicted protein structures for I think it was 70ish thousand protein structures in the end that we had to predict and score them using in silico metrics. So things like, how similar is this to that template, wild-type protein structure that we used as our conditioning information?



We put them on a big grid. So we have two axes. We have on the x-axis‚Äîand this is a figure in our paper‚Äîthe quality of the prediction. It&#8217;s essentially a confidence metric: how realistic is this protein sequence? And on the other axis is, how similar is the predicted structure of this variant to the original? And ultimately, what we were wanting to see was the proteins that scored well in both of those metrics, so that showed up in the top right of that diagram, were caught primarily, because these are again the ones that are most likely, having to say most likely, to retain function of the original.



So when you compare the original tools‚ÄîTool Series A, right, the unpatched tools‚Äîwhat you&#8217;ll find is varying degrees of success in the top right. It varied by tool. But in some cases, barely anything being flagged as potentially hazardous. And so improvement is then in the next series‚ÄîSeries B, the patched version of tools‚Äîwe have more flagged in that upper-right corner.



HORVITZ: And we felt confident that we had a more AI-resilient screening solution across the companies, and, James, at this point, the whole team decided it was time to disclose the vulnerability as well as the patch details and pointers to where to go for the updated screening software and to communicate this to synthesis companies worldwide via the IGSC. This was probably July, I think, of 2024. What was that process like, and how did members respond?



DIGGANS: I think members were really grateful and excited. To present to that group, to say, hey, this activity (a) has gone on, (b) was successful, and (c) was kept close hold until we knew how to mitigate this, I think everyone was really gratified by that and comforted by the fact that now they had kind of off-the-shelf solutions that they could use to improve their resilience against any incoming heavily engineered protein designs.



HORVITZ: Thanks, James.



Now, I know that we all understand this particular effort to be important but a piece of the biosecurity and AI problem. I&#8217;m just curious to ‚Ä¶ I‚Äôll ask all three of you to just share some brief reflections.



I know, Bruce, you&#8217;ve been on ‚Ä¶ you‚Äôve stayed on this, and we‚Äôve‚Äîall of us on the original team‚Äîhave other projects going on that are pushing on the frontiers ahead of where we were with this paper when we published it.



Let me start with Tessa in terms of, like, what new risks do you see emerging as AI accelerates and maybe couple that with thoughts about how do we proactively get ahead of them.



ALEXANIAN: Yeah, I think with the Paraphrase‚Äôs work, as Bruce explained so well, you know, I sometimes use the metaphor of the previous response that the IGSC had to do, the synthesis screening community, where it used to be you could look for similarities to DNA sequences, and then everyone started doing synthetic biology where they were doing codon optimization so that proteins could express more efficiently in different host organisms, and now all of a sudden, well, you&#8217;ve scrambled your DNA sequence and it doesn&#8217;t look very similar even though your protein sequence actually still looks, you know, very similar or often the same once it&#8217;s been translated from DNA to protein, and so that was a, you know, many, many in the industry were already screening both DNA and protein, but they had to start screening ‚Ä¶ everybody had to start screening protein sequences even just to do the similarity testing as these codon optimization tools became universal.



I feel like we&#8217;re, kind of, in a similar transition phase with protein-design, protein-rephrasing, tools where, you know, these tools are still in many cases drawing from the natural distribution of proteins. You know, I think some of the work we saw in, you know, designing novel CRISPR enzymes, you go, OK, yeah, it is novel; it&#8217;s very unlike any one CRISPR enzyme. But if you do a massive multiple sequence alignment of every CRISPR enzyme that we know about, you&#8217;re like, OK, this fits in the distribution of those enzymes. And so, you know, I think we&#8217;re not ‚Ä¶ we&#8217;re having to do a more flexible form of screening, where we look for things that are kind of within distribution of natural proteins.



But I feel like broadly, all of the screening tools were able to respond by doing something like that. And I think &#8230; I still feel like the clock is ticking down on that and that as the AI tools get better at predicting function and designing, sort of, novel sequences to pursue a particular function, you know‚Äîyou have tools now that can go from Gene Ontology terms to a potential structure or potential sequence that may again be much farther out of the distribution of natural protein‚ÄîI think all of us on the screening side are going to have to be responding to that, as well.



So I think I see this as a necessary ongoing engagement between people at the frontier of designing novel biology and people at the frontier of producing all of the materials that allow that novel biology to be tested in the lab. You know, I think this feels like the first, you know, detailed, comprehensive zero day disclosure and response. But I think that&#8217;s ‚Ä¶ I think we&#8217;re going to see more of those. And I think what I&#8217;m excited about doing at IBBIS is trying to encourage and set up more infrastructure so that you can, as an AI developer, disclose these new discoveries to the people who need to respond before the publication comes out.



HORVITZ: Thank you, Tessa.



The, the ‚Ä¶ Bruce, I mean, you and I are working on all sorts of dimensions. You&#8217;re leading up some efforts at Microsoft, for example, on the foundation model front and so on, among other directions. We&#8217;ve talked about new kinds of embedding models that might go beyond sequence and structure. Can you talk a little bit about just a few of the directions that just paint the larger constellation of the kinds of things that we talk about when we put our worry hats on?



WITTMANN:&nbsp;I feel like that could have its own dedicated podcast, as well. There&#8217;s a lot ‚Ä¶ [LAUGHTER] there&#8217;s a lot to talk about.



HORVITZ: Yeah. We want to make sure that we don&#8217;t tell the world that the whole problem is solved here.



WITTMANN:&nbsp;Right, right, right. I think Tessa said it really, really well in that most of what we&#8217;re doing right now, it&#8217;s a variant on a known theme. I have to know the structure that does something bad to be able to pass it in as context. I have to know some existing sequence that does something bad to pass it in.



And obviously the goal is to move away from that in benign applications, where when I&#8217;m designing something, I often want to design it because nothing exists [LAUGHS] that already does it. So we are going to be heading to this space where we don&#8217;t know what this protein does. It&#8217;s kind of a circular problem, right, where we&#8217;re going to need to be able to predict what some obscure protein sequence does in order to be able to still do our screening.



Now, the way that I think about this, I often think about it beyond just DNA synthesis screening. It&#8217;s one line of defense, and there needs to be many lines of defense that come into play here that go beyond just relying on this one roadblock. It&#8217;s a very powerful roadblock. It&#8217;s a very powerful barrier. But we need to be proactively thinking about how we broaden the scope of defenses. And there are lots of conversations that are ongoing. I won&#8217;t go into the details of them. Again, that would be its own podcast.



But primarily my big push‚Äîand I think this is emerging consensus in the field, though I don&#8217;t want to speak for everybody‚Äîis it needs to ‚Ä¶ any interventions we have need to come more at the systems level and less at the model level, primarily because this is such dual-use technology. If it can be used for good biological design, it can be used for bad biological design. Biology has no sense of morality. There is no bad protein. It&#8217;s just a protein.



So we need to think about this differently than how we would maybe think about looking at the outputs of that image generator model that I spoke about earlier, where I can physically look at an image and say, don&#8217;t want my model producing that, do want my model producing that. I don&#8217;t have that luxury in this space. So it&#8217;s a totally different problem. It&#8217;s an evolving problem. Conversations are happening about it, but the work is very much not done.



HORVITZ: And, James, I want to give you the same open question, but I&#8217;d like to apply what Bruce just said on system level and so on and in the spirit of the kind of things that you&#8217;re very much involved with internationally to also add to it, just get some comments on programs and policies that move beyond technical solutions for governance mechanisms‚Äîlogging, auditing nucleic acid orders, transparency, various kinds‚Äîthat might complement technical approaches like Paraphrase and their status today.



DIGGANS: Yeah, I&#8217;m very gratified that Bruce said that we, the synthesis industry, should not be the sole bulwark against misuse. That is very comforting and correct.



Yeah, so the US government published a guidance document in 2023 that essentially said you, the entire biotech supply chain, have a responsibility to make sure that you&#8217;re evaluating your customers. You should know your customer; you know that they&#8217;re legitimate. I think that&#8217;s an important practice.



Export controls are designed to minimize the movement of equipment and materials that can be used in support of these kinds of misuse activities. And then governments have really been quite active in trying to incentivize, you know, sort of what we would think of as positive behavior, so screening, for example, in DNA synthesis companies. The US government created a framework in 2024, and it&#8217;s under a rewrite now to basically say US research dollars will only go to companies who make nucleic acid who do these good things. And so that is using, kind of, the government-funding carrot to, kind of, continue to build these layers of defense against potential misuse.



HORVITZ: Thanks. Now, discussing risk, especially when it involves AI and biosecurity, isn&#8217;t always easy. As we&#8217;ve all been suggesting, some worry about alarming the public or arming bad actors. Others advocate for openness as a principle of doing science with integrity.



A phase of our work as we prepared our paper was giving serious thought to both the benefits and the risks of transparency about what it was that we were doing. Some experts encouraged full disclosure as important for enhancing the science of biosecurity. Other experts, all experts, cautioned against what are called information hazards, the risk of sharing the details to enable malevolent actions with our findings or our approach.



So we faced a real question: how can we support open science while minimizing the risk of misuse? And we took all the input we got, even if it was contradictory, very seriously. We carefully deliberated about a good balance, and even then, once we chose our balance and submitted our manuscript to Science, the peer reviewers came back and said they wanted some of the more sensitive details that we withheld with explanations as to why.



So this provoked some thinking out of the box about a novel approach, and we came up with a perpetual gatekeeping strategy where requests for access to sensitive methods and data and even the software across different risk categories would be carefully reviewed by a committee and a process for access that would continue in perpetuity.



Now, we brought the proposal to Tessa and her team at IBBIS‚Äîthis is a great nonprofit group; look at their mission‚Äîand we worked with Tessa and her colleagues to refine a workable solution that was accepted by Science magazine as a new approach to handling information hazards as first demonstrated by our paper.



So, Tessa, thank you again for helping us to navigate such a complex challenge. Can you share your perspective on information hazards? And then walk us through how our proposed system ensures responsible data and software sharing.



ALEXANIAN: Yeah. And thanks, Eric.



It&#8217;s all of the long discussions we had among the group of people on this podcast and the other authors on the paper and many people we engaged, you know, technical experts, people in various governments, you know, we heard a lot of contradictory advice.



And I think it showed us that there isn&#8217;t a consensus right now on how to handle information hazards in biotechnology. You know, I think ‚Ä¶ I don&#8217;t want to overstate how much of a consensus there is in cybersecurity either. If you go to DEF CON, you&#8217;ll hear people about how they&#8217;ve been mistreated in their attempts to do responsible disclosure for pacemakers and whatnot. But I think we&#8217;re ‚Ä¶ we have even less of a consensus when it comes to handling biological information.



You know, you have some people who say, oh, because the size of the consequences could be so catastrophic if someone, you know, releases an engineered flu or something, you know, we should just never share information about this. And then you have other people who say there&#8217;s no possibility of building defenses unless we share information about this. And we heard very strong voices with both of those perspectives in the process of conducting this study.



And I think what we landed on that I&#8217;m really excited about and really excited to get feedback on now that the paper is out, you know, if you go and compare our preprint, which came out in December of 2024, and this paper in October 2025, you&#8217;ll see a lot of information got added back in.



And I&#8217;m excited to see people&#8217;s reaction to that because even back in January 2025, talking with people who were signatories to the responsible biodesign commitments, they were really excited that this was such an empirically concrete paper because they&#8217;d maybe read a number of papers talking about biosecurity risks from AI that didn&#8217;t include a whole lot of data, you know, often, I think, because of concerns about information hazards. And they found the arguments in this paper are much more convincing because we are able to share data.



So the process we underwent that I felt good about was trying to really clearly articulate, when we talk about an information hazard, what are we worried about being done with this data? And if we put this data in public, completely open source, does it shift the risk at all? You know, I think doing that kind of marginal contribution comparison is really important because it also let us make more things available publicly.



But there were a few tiers of data that after a lot of discussion amongst the authors of the paper, we thought, OK, potentially someone who wanted to do harm, if they got access to this data, it might make it easier for them. Again, not necessarily saying it, you know, it opens the floodgates, but it might make it easier for them. And when we thought about that, we thought, OK, you know, giving all of those paraphrased protein sequences, maybe, maybe that, you know, compared to having to set up the whole pipeline with the open-source tools yourself, just giving you those protein sequences, maybe that makes your life a bit easier if you&#8217;re trying to do harm.



And then we thought, OK, giving you those protein sequences plus whether or not they were successfully flagged, maybe that makes your life, you know, quite a bit easier. And then finally, we thought, OK, the code that we want to share with some people who might try to reproduce these results or might try to build new screening systems that are more robust, we want to share the code with them. But again, if you have that whole code pipeline just prepared for you, it might really help make your life easier if you&#8217;re trying to do harm.



And so we, sort of, sorted the data into these three tiers and then went through a process actually very inspired by the existing customer screening processes in nucleic acid synthesis about how to determine, you know, we tried to take an approach not of what gets you in but what gets you out. You know, for the most part, we think it should be possible to access this data.



You know, if you have an affiliation with a recognizable institution or some good explanation of why you don&#8217;t have one right now, you know, if you have a reason for accessing this data, it shouldn&#8217;t be too hard to meet those requirements, but we wanted to have some in place. And we wanted it to be possible to rule out some people from getting access to this data. And so we&#8217;ve tried to be extremely transparent about what those are. If you go through our data access process and for some reason you get rejected, you&#8217;ll get a list of, &#8220;Here&#8217;s the reasons we rejected you. If you don&#8217;t think that&#8217;s right, get back to us.&#8221;



So I&#8217;m really excited to pilot this in part because I think, you know, we&#8217;re already in conversations with some other people handling potential bio-AI information hazard about doing a similar process for their data of, you know, tiering it, determining which gates to put in which tiers, but I really hope a number of people do get access through the process or if they try and they fail, they tell us why. Because I think as we move toward this world of potentially, you know, biology that is much easier to engineer, partly due to dual-use tools, you know, my dream is it&#8217;s, like, still hard to engineer harm with biology, even if it&#8217;s really easy to engineer biology. And I think these, kind of, new processes for managing access to things, this sort of like, you know, open but not completely public, I think those can be a big part of that layered defense.



HORVITZ: Thanks, Tessa. So we&#8217;re getting close to closing, and I just thought I would ask each of you to just share some reflections on what we&#8217;ve learned, the process we&#8217;ve demonstrated, the tools, the policy work that we did, this idea of facing the dual-use dilemma with ‚Ä¶ even at the information hazard level, with sharing information versus withholding it. What do you think about how our whole end to end of the study, now reaching the two-year point, can help other fields facing dual-use dilemmas?



Tessa, Bruce, James ‚Ä¶ James, have you ever thought about that? And we&#8217;ll go to Bruce and then Tessa.



DIGGANS:&nbsp;Yeah, I think it was an excellent model. I would like to see a study like this repeated on a schedule, you know, every six months because from where I sit, you know, the tools that we used for this project are now two years old. And so capabilities have moved on. Is the picture the same in terms of defensive capability? And so using that model over time, I think, would be incredibly valuable. And then using the findings to chart, you know, how much should we be investing in alternative strategies for this kind of risk mitigation for AI tool ‚Ä¶ the products of AI tools?



HORVITZ: Bruce.



WITTMANN:&nbsp;Yeah, I think I would extend on what James said. The anecdote I like to point out about this project is, kind of, our schedule. We found the vulnerability and it was patched within a week, two weeks, on all major synthesis screening platforms. We wrote the paper within a month. We expanded on the paper within two months, and then we spent a year and a half to nearly two years [LAUGHS] trying to figure out what goes into the paper; how do we release this information; you know, how do we do this responsibly?



And my hope is similar to what James said. We&#8217;ve made it easier for others to do this type of work. Not this exact work; it doesn&#8217;t have to necessarily do with proteins. But to do this type of work where you are dealing with potential hazards but there is also value in sharing and that hopefully that year and a half we spent figuring out how to appropriately share and what to share will not be a year and a half for other teams because these systems are in place or at least there is an example to follow up from. So that&#8217;s my takeaway.



HORVITZ: Tessa, bring us home‚Äîbring us home! [LAUGHS]



ALEXANIAN: Bring us home! Let&#8217;s do it faster next time. [LAUGHTER] Come talk to any of us if you&#8217;re dealing with this kind of stuff. You know, I think IBBIS, especially, we want to be a partner for building those layers of defense and, you know, having ripped out our hair as a collective over the past year and a half about the right process to follow here, I think we all really hope it&#8217;ll be faster next time.



And I think, you know, the other thing I would encourage is if you&#8217;re an AI developer, I would encourage you to think about how your tool can strengthen screening and strengthen recognition of threats.



I know James and I have talked before about how, you know, our Google search alerts each week send us dozens of cool AI bio papers, and it&#8217;s more like once a year or maybe once every six months, if we&#8217;re lucky, that we get something that&#8217;s like applying AI bio to biosecurity. So, you know, if you&#8217;re interested in these threats, I think we&#8217;d love to see more work that&#8217;s directly applied to facing these threats using the most modern technology.



HORVITZ: Well said.



Well, Bruce, James, Tessa, thank you so much for joining me today and for representing the many collaborators, both coauthors and beyond, who made this project possible.



It&#8217;s been a true pleasure to work with you. I&#8217;m so excited about what we&#8217;ve accomplished, the processes and the models that we&#8217;re now sharing with the world. And I&#8217;m deeply grateful for the collective intelligence and dedication that really powered the effort from the very beginning. So thanks again.



[MUSIC]



WITTMANN: Thanks, Eric.



DIGGANS: Thank you.



ALEXANIAN: Thank you.



[MUSIC FADES]

				
			
			
				Show more			
		
	








[1] The original organization was founded in 2009 and became the International Gene Synthesis Consortium in 2010.
Opens in a new tabThe post Ideas: More AI-resilient biosecurity with the Paraphrase Project appeared first on Microsoft Research.
‚Ä¢ When AI Meets Biology: Promise, Risk, and Responsibility
  Advances in AI are opening extraordinary frontiers in biology. AI-assisted protein engineering holds the promise of new medicines, materials, and breakthroughs in scientific understandings. Yet these same technologies also introduce biosecurity risks and may lower barriers to designing harmful toxins or pathogens. This ‚Äúdual-use‚Äù potential, where the same knowledge can be harnessed for good or misuse to cause harm, poses a critical dilemma for modern science.



Great Promise‚Äîand Potential Threat



I‚Äôm excited about the potential for AI-assisted protein design to drive breakthroughs in biology and medicine. At the same time, I‚Äôve also studied how these tools could be misused. In computer-based studies, we found that AI protein design (AIPD) tools could generate modified versions of proteins of concern, such as ricin. Alarmingly, these reformulated proteins were able to evade the biosecurity screening systems used by DNA synthesis companies, which scientists rely on to synthesize AI-generated sequences for experimental use. 



In our paper published in Science on October 2, ‚ÄúStrengthening nucleic acid biosecurity screening against generative protein design tools (opens in new tab),‚Äù we describe a two-year confidential project we began in late 2023 while preparing a case study for a workshop on AI and biosecurity.



We worked confidentially with partners across organizations and sectors for 10 months to develop AI biosecurity ‚Äúred-teaming‚Äù methods that allowed us to better understand vulnerabilities and craft practical solutions‚Äî&#8221;patches‚Äù that have now been adopted globally, making screening systems significantly more AI-resilient.



Summary of AIPD red-teaming workflow.



For structuring, methods, and process in our study, we took inspiration from the cybersecurity community, where ‚Äúzero-day‚Äù vulnerabilities are kept confidential until a protective patch is developed and deployed. Following the acknowledgment by a small group of workshop attendees of a zero-day for AI in biology, we worked closely with stakeholders‚Äîincluding synthesis companies, biosecurity organizations, and policymakers‚Äîto rapidly create and distribute patches that improved detection of AI-redesigned protein sequences. We delayed public disclosure until protective measures were in place and widely adopted.



Dilemma of Disclosure



The dual use dilemma also complicates how we share information about vulnerabilities and safeguards. Across AI and other fields, researchers face a core question: 




How can scientists share potentially risk-revealing methods and results in ways that enable progress without offering a roadmap for misuse?




We recognized that our work itself‚Äîdetailing methods and failure modes‚Äîcould be exploited by malicious actors if published openly. To guide decisions about what to share, we held a multi-stakeholder deliberation involving government agencies, international biosecurity organizations, and policy experts. Opinions varied: some urged full transparency to maximize reproducibility‚Äîand to help others to build on our work; others stressed restraint to minimize risk. It was clear that a new model of scientific communication was needed, one that could balance openness and security.



The Novel Framework



The risk of sharing dangerous information through biological research has become a growing concern. We have participated in community-wide discussion on the challenges, including a recent National Academies of Science, Engineering, and Medicine workshop and study.&nbsp;



In preparing our manuscript for publication, we worked on designing a process to limit the spread of dangerous information while still enabling scientific progress.&nbsp;



To address the dual challenges, we devised a tiered access system for data and methods, implemented in partnership with the International Biosecurity and Biosafety Initiative for Science (IBBIS) (opens in new tab), a nonprofit dedicated to advancing science while reducing catastrophic risks. The system works as follows:




Controlled access: Researchers can request access through IBBIS, providing their identity, affiliation, and intended use. Requests are reviewed by an expert biosecurity committee, ensuring that only legitimate scientists conducting relevant research gain access.



Stratified tiers of information: Data and code are classified into several tiers according to their potential hazard, from low-risk summaries through sensitive technical data to critical software pipelines.



Safeguards and agreements: Approved users sign tailored usage agreements, including non-disclosure terms, before receiving data.



Resilience and longevity: Provisions are built in for declassification when risks subside, and for succession of stewardship to trusted organizations should IBBIS be unable to continue its operation.




This framework allows replication and extension of our work while guarding against misuse. Rather than relying on secrecy, it provides a durable system of responsible access.



To ensure continued funding for the storage and responsible distribution of sensitive data and software, and for the operation of the sharing program, we provided an endowment to IBBIS to support the program in perpetuity. This approach was modeled after the One Hundred Year Study on AI at Stanford, which is endowed to continue for the life of the university.



An Important Step in Scientific Publishing



We are pleased that the leadership at Science accepted our approach to handling information hazards. To our knowledge, this is the first time a leading scientific journal has formally endorsed a tiered-access approach to manage an information hazard. This recognition validates the idea that rigorous science and responsible risk management can coexist‚Äîand that journals, too, can play a role in shaping how sensitive knowledge is shared. We acknowledge the visionary leadership at Science, including editors, Michael Funk and Valda Vinson, and Editor-in-Chief, Holden Thorp.



Beyond Biology: A Model for Sensitive Research



While developed for AI-powered protein design, our approach offers a generalizable model for dual-use research of concern (DURC) across disciplines. Whether in biology, chemistry, or emerging technologies, scientists will increasingly confront situations where openness and security pull in opposite directions. Our experience shows that these values can be balanced: with creativity, coordination, and new institutional mechanisms, science can uphold both reproducibility and responsibility.



We hope this framework becomes a template for future projects, offering a way forward for researchers who wish to share their insights without amplifying risks. By embedding resilience into how knowledge is communicated‚Äînot just what is communicated‚Äîwe can ensure that scientific progress continues to serve humanity safely.



The responsible management of information hazards is no longer a peripheral concern: it is central to how science will advance in the age of powerful technologies like AI. This approach to managing information hazards demonstrates a path forward, where novel frameworks for access and stewardship allow sensitive but vital research to be shared, scrutinized, and extended responsibly. Approaches like this will be critical to ensuring that scientific openness and societal safety advance hand-in-hand.







Additional reading



Strengthening nucleic acid biosecurity screening against generative protein design tools.



The Age of AI in the Life Sciences: Benefits and Biosecurity Considerations, National Academies of Science, Engineering, and Medicine, 2025. (opens in new tab)



Disseminating In Silico and Computational Biological Research: Navigating Benefits and Risks: Proceedings of a Workshop, National Academies of Science, Engineering, and Medicine, 2025. (opens in new tab)



Protecting scientific integrity in an age of generative AI, Proceedings of the National Academy of Science, 2024. (opens in new tab)
Opens in a new tabThe post When AI Meets Biology: Promise, Risk, and Responsibility appeared first on Microsoft Research.
‚Ä¢ Automate Amazon QuickSight data stories creation with agentic AI using Amazon Nova Act
  Amazon QuickSight data stories support global customers by transforming complex data into interactive narratives for faster decisions. However, manual creation of multiple daily data stories consumes significant time and resources, delaying critical decisions and preventing teams from focusing on valuable analysis. 
Each organization has multiple business units, and each business unit creates and operates multiple dashboards based on specific reporting requirements. Users create various data stories from these dashboards according to their needs. Currently, data story creation is a manual process that consumes significant time because users need to develop multiple narratives. By automating this process, organizations can dramatically improve productivity, so users can redirect their time toward making data-driven decisions. 
In this post, we demonstrate how Amazon Nova Act automates QuickSight data story creation, saving time so you can focus on making critical, data-driven business decisions. 
Amazon Nova Act modernizes web browser automation, which helps in performing complex, real-world tasks through web interfaces. Unlike traditional large language models (LLMs) focused on conversation, Amazon Nova Act emphasizes action-oriented capabilities by breaking down complex tasks into reliable atomic commands. This transformative technology advances autonomous automation with minimal human supervision, making it particularly valuable for business productivity and IT operations. 
QuickSight data stories transform complex data into interactive presentations that guide viewers through insights. It automatically combines visualizations, text, and images to bridge the gap between analysts and stakeholders, helping organizations communicate data effectively and make faster decisions while maintaining professional standards. 
With the automation capabilities of Amazon Nova Act, you can automatically generate data stories, reducing time-consuming manual efforts. Using browser automation, Amazon Nova Act seamlessly interacts with QuickSight to create customized data narratives. By combining the automation of Amazon Nova Act with the robust visualization capabilities of QuickSight, you can minimize repetitive tasks and accelerate data-driven decision-making across teams. 
Solution overview 
In our solution, QuickSight transforms complex data into interactive narratives through data stories, enabling faster decisions. Amazon Nova Act transforms web browser automation by enabling AI agents to execute complex tasks autonomously, streamlining operations for enhanced business productivity. 
Prompt best practices 
Amazon Nova Act achieves optimal results by breaking down prompts into distinct act() calls, similar to providing step-by-step instructions. At the time of writing, this is the recommended approach for building repeatable, reliable, simple-to-maintain workflows. In this section, we discuss some prompt best practices. 
First, be prescriptive and succinct in what the agent should do. For example, don‚Äôt use the following code: 
nova.act("Select the SaaS-Sales dataset") 
We recommend the following prompt instead: 
nova.act("Click on Datasets option on the left-hand side and then select SaaS-Sales dataset ") 
Additionally, we recommend breaking up large actions into smaller ones. For example, don‚Äôt use the following code: 
nova.act("Publish dashboard as ‚Äòtest-dashboard‚Äô") 
The following prompt is broken up into separate actions: 
nova.act("select Analyses on the left-hand side‚Äù) 
nova.act("select the ‚ÄòSaaS-Sales analysis‚Äô ") 
nova.act("select ‚ÄòPUBLISH‚Äô from the top right-hand corner") 
nova.act("In the 'Publish dashboard' dialog box, locate the input field labeled 'Dashboard name'. Enter 'test_dashboard' into this field‚Äù) 
nova.act(‚ÄúSelect PUBLISH DASHBOARD‚Äù) 
Prerequisites 
The following prerequisites are needed to create and publish a QuickSight data story using Amazon Nova Act: 
 
 An API key for authentication. To generate an API key, refer to Amazon Nova Act. 
 For Amazon Nova Act prerequisites and installation instructions, refer to the GitHub repo. 
 A Pro user (author or reader) to create QuickSight data stories. 
 A published QuickSight dashboard containing the visuals required for your QuickSight data story. 
 
For Windows users, complete the following setup and installation steps in Windows PowerShell: 
 
 Create a virtual environment: python -m venv venv. 
 Activate the virtual environment: venv\Scripts\activate 
 Set your API key as an environment variable: $Env:NOVA_ACT_API_KEY="your_api_key" 
 Install Amazon Nova Act: pip install nova-act 
 To run a script (Python file), use the following command, and specify the script name you want to run: python &lt;script_name&gt;.py 
 
To keep it simple, we have hardcoded some of the values. You can implement programming logic using Python features to accept these values as input parameters. 
There are multiple ways to write prompts. In the following sections, we provide examples demonstrating how to automate QuickSight data story creation and distribution. 
Setup 
Run the following code to import the NovaAct class from the nova_act module, create an Amazon Nova instance beginning at the QuickSight login page, and initiate an automated browser session: 
 
 from nova_act import NovaAct

nova = NovaAct(starting_page="https://quicksight.aws.amazon.com/")

nova.start()
 
 
Sign in with credentials After you have opened the QuickSight login page, complete the following steps to log in with your credentials: 
 
 Enter your QuickSight account name and choose Next. (Specify the QuickSight account name in the following code, or implement programming logic to handle it as an input parameter.) nova.act("enter QuickSight account name &lt;Account Name&gt; and select Next") 
 Enter your user name and move to the password field. (You can configure the user name as an input parameter using programming logic.) nova.act("Enter username and click on the password field") 
 Collect the password from the command line and enter it using Playwright: nova.page.keyboard.type(getpass()) 
 Now that user name and password are filled in, choose Sign in. nova.act("Click Sign in") 
 
If the agent is unable to focus on the page element (in this case, the password field), you can use the following code: 
nova.act("enter '' in the password field") 
nova.page.keyboard.type(getpass()) 
Create a new data story On the QuickSight console, choose Data stories in the navigation pane: 
nova.act("Select Data stories on the left side menu") 
nova.act("Select NEW DATA STORY"). 
 
To build the data story, you must complete the following steps: 
 
 Describe the data story 
 Select visuals from the dashboard 
 Build the data story 
 
nova.act("Please enter ‚ÄòCountry wide sales data story‚Äô into the 'Describe your data story' field and Click on + ADD") 
nova.act("select all the visuals and select BUILD") 
time.sleep(300) 
 
In this example, the script defaults to a single dashboard (Demo Dashboard). For multiple dashboards, include a prompt to select the specific dashboard and its visuals for the data story. Additionally, you can describe the data story according to your requirements. If there are multiple visuals, you can select the ones you want to include as part of the data story. Adjust the time.sleep duration based on dashboard data volume and the number of visuals being compiled. 
To view your data story, choose Data stories in the navigation pane and choose your data story. 
 
Clean up 
Complete the following steps to delete the data story you created: 
 
 Sign in to the QuickSight console. 
 Choose Data stories in the navigation pane. 
 Find the data story you want to delete. 
 Choose the options menu icon (three dots) next to the story. 
 Choose Delete from the dropdown menu. 
 
Conclusion 
In this post, we demonstrated how to create a QuickSight data story using Amazon Nova Act prompts. This solution showcases how Amazon Nova Act simplifies task automation, significantly boosting productivity and saving valuable time. 
To learn more about Amazon Nova Act and QuickSight data stories, check out the following resources: 
 
 Amazon Nova Act GitHub repo 
 Introducing Amazon Nova Act 
 Working with data stories in Amazon QuickSight 
 
 
About the author 
Satish Bhonsle is a Senior Technical Account Manager at AWS. He is passionate about customer success and technology. He loves working backwards by quickly understanding strategic customer objectives, aligning them to software capabilities and effectively driving customer success.
‚Ä¢ Implement automated monitoring for Amazon Bedrock batch inference
  Amazon Bedrock is a fully managed service that offers a choice of high-performing foundation models (FMs) from leading AI companies through a single API, along with capabilities to build generative AI applications with security, privacy, and responsible AI. 
Batch inference in Amazon Bedrock is for larger workloads where immediate responses aren‚Äôt critical. With a batch processing approach, organizations can analyze substantial datasets efficiently, with significant cost advantages: you can benefit from a 50% reduction in pricing compared to the on-demand option. This makes batch inference particularly valuable for handling extensive data to get inference from Amazon Bedrock FMs. 
As organizations scale their use of Amazon Bedrock FMs for large-volume data processing, implementing effective monitoring and management practices for batch inference jobs becomes an important focus area for optimization. This solution demonstrates how to implement automated monitoring for Amazon Bedrock batch inference jobs using AWS serverless services such as AWS Lambda, Amazon DynamoDB, and Amazon EventBridge, reducing operational overhead while maintaining reliable processing of large-scale batch inference workloads. Through a practical example in the financial services sector, we show how to build a production-ready system that automatically tracks job status, provides real-time notifications, and maintains audit records of processing activities. 
Solution overview 
Consider a scenario where a financial services company manages millions of customer interactions and data points, including credit histories, spending patterns, and financial preferences. This company recognized the potential of using advanced AI capabilities to deliver personalized product recommendations at scale. However, processing such massive datasets in real time isn‚Äôt always necessary or cost-effective. 
The solution presented in this post uses batch inference in Amazon Bedrock with automated monitoring to process large volumes of customer data efficiently using the following architecture. 
 
This architecture workflow includes the following steps: 
 
 The financial services company uploads customer credit data and product data to be processed to an Amazon Simple Storage Service (Amazon S3) bucket. 
 The first Lambda function reads the prompt template and data from the S3 bucket, and creates a JSONL file with prompts for the customers along with their credit data and available financial products. 
 The same Lambda function triggers a new Amazon Bedrock batch inference job using this JSONL file. 
 In the prompt template, the FM is given a role of expert in recommendation systems within the financial services industry. This way, the model understands the customer and their credit information to intelligently recommend most suitable products. 
 An EventBridge rule monitors the state changes of the batch inference job. When the job completes or fails, the rule triggers a second Lambda function. 
 The second Lambda function creates an entry for the job with its status in a DynamoDB table. 
 After a batch job is complete, its output files (containing personalized product recommendations) will be available in the S3 bucket‚Äôs inference_results folder. 
 
This automated monitoring solution for Amazon Bedrock batch inference offers several key benefits: 
 
 Real-time visibility ‚Äì Integration of DynamoDB and EventBridge provides real-time visibility into the status of batch inference jobs, enabling proactive monitoring and timely decision-making 
 Streamlined operations ‚Äì Automated job monitoring and management minimizes manual overhead, reducing operational complexities so teams can focus on higher-value tasks like analyzing recommendation results 
 Optimized resource allocation ‚Äì Metrics and insights about token count and latency stored in DynamoDB help organizations optimize resource allocation, facilitating efficient utilization of batch inference capabilities and cost-effectiveness 
 
Prerequisites 
To implement this solution, you must have the following: 
 
 An active AWS account with appropriate permissions to create resources, including S3 buckets, Lambda functions, and Amazon Bedrock resources. 
 Access to your selected models hosted on Amazon Bedrock. Make sure the selected model has been enabled in Amazon Bedrock. 
 
Additionally, make sure to deploy the solution in an AWS Region that supports batch inference. 
Deploy solution 
For this solution, we provide an AWS CloudFormation template that sets up the services included in the architecture, to enable repeatable deployments. This template creates the following resources: 
 
 An S3 bucket to store the input and output 
 AWS Identity and Access Management (IAM) roles for Lambda functions, EventBridge rule, and Amazon Bedrock batch inference job 
 Amazon Bedrock Prompt Management template 
 EventBridge rule to trigger the Lambda function 
 DynamoDB table to store the job execution details 
 
To deploy the CloudFormation template, complete the following steps: 
 
 Sign in to the AWS Management Console. 
 Open the template directly on the Create stack page of the CloudFormation console. 
 Choose Next and provide the following details: 
   
   For Stack name, enter a unique name. 
   For ModelId, enter the model ID that you need your batch job to run with. Only Anthropic Claude family models can be used with the CloudFormation template provided in this post. 
    
 Add optional tags, permissions, and other advanced settings if needed. 
 Review the stack details, select I acknowledge that AWS CloudFormation might create AWS IAM resources, and choose Next. 
 Choose Submit to initiate the deployment in your AWS account. The stack might take several minutes to complete. 
 
 
 Choose the Resources tab to find the newly created S3 bucket after the deployment succeeds. 
 Open the S3 bucket and confirm that there are two CSV files in your data folder.  
 
 
 On the Amazon S3 console, go to the data folder and create two more folders manually. This will prepare your S3 bucket to store the prompts and batch inference job results.  
 
 
 On the Lambda console, choose Functions in the navigation pane. 
 Choose the function that has create-jsonl-file in its name.  
 
 
 On the Test tab, choose Test to run the Lambda function.  The function reads the CSV files from the S3 bucket and the prompt template, and creates a JSONL file with prompts for the customers under the prompts folder of your S3 bucket. The JSONL file has 100 prompts using the customers and products data. Lastly, the function submits a batch inference job with the CreateModelInvocationJob API call using the JSONL file. 
 On the Amazon Bedrock console, choose Prompt Management under Builder tools in the navigation pane. 
 Choose the finance-product-recommender-v1 prompt to see the prompt template input for the FM. 
 Choose Batch inference in the navigation pane under Inference and Assessment to find the submitted job.  
 
The job progresses through different statuses: Submitted, Validating, In Progress, and lastly Completed, or Failed. You can leave this page and check the status after a few hours. 
The EventBridge rule will automatically trigger the second Lambda function with event-bridge-trigger in its name on completion of the job. This function will add an entry in the DynamoDB table named bedrock_batch_job_status with details of the execution, as shown in the following screenshot. 
 
This DynamoDB table functions as a state manager for Amazon Bedrock batch inference jobs, tracking the lifecycle of each request. The columns of the table are logically divided into the following categories: 
 
 Job identification and core attributes (job_arn, job_name) ‚Äì These columns provide the unique identifier and a human-readable name for each batch inference request, serving as the primary keys or core attributes for tracking. 
 Execution and lifecycle management (StartTime, EndTime, last_processed_timestamp, TotalDuration) ‚Äì This category captures the temporal aspects and the overall progression of the job, allowing for monitoring of its current state, start/end times, and total processing duration. last_processed_timestamp is crucial for understanding the most recent activity or checkpoint. 
 Processing statistics and performance (TotalRecordCount, ProcessedRecordCount, SuccessRecordCount, ErrorRecordCount) ‚Äì These metrics provide granular insights into the processing efficiency and outcome of the batch job, highlighting data volume, successful processing rates, and error occurrences. 
 Cost and resource utilization metrics (InputTokenCount, OutputTokenCount) ‚Äì Specifically designed for cost analysis, these columns track the consumption of tokens, which is a direct factor in Amazon Bedrock pricing, enabling accurate resource usage assessment. 
 Data and location management (InputLocation, OutputLocation) ‚Äì These columns link the inference job to its source and destination data within Amazon S3, maintaining traceability of the data involved in the batch processing. 
 
View product recommendations 
Complete the following steps to open the output file and view the recommendations for each customer generated by the FM: 
 
 On the Amazon Bedrock console, open the completed batch inference job. 
 Find the job Amazon Resource Name (ARN) and copy the text after model-invocation-job/, as illustrated in the following screenshot.  
 
 
 Choose the link for S3 location under Output data. A new tab opens with the inference_results folder of the S3 bucket. 
 
 
 Search for the job results folder using the text copied from the previous step. 
 Open the folder to find two output files: 
   
   The file named manifest contains information like number of tokens, number of successful records, and number of errors. 
   The second output file contains the recommendations. 
    
 Download the second output file and open it in a text editor like Visual Studio Code to find the recommendations against each customer. 
 
The example in the following screenshot shows several recommended products and why the FM chose this product for the specific customer. 
 
Best practices 
To optimize or enhance your monitoring solution, consider the following best practices: 
 
 Set up Amazon CloudWatch alarms for failed jobs to facilitate prompt attention to issues. For more details, see Amazon CloudWatch alarms. 
 Use appropriate DynamoDB capacity modes based on your workload patterns. 
 Configure relevant metrics and logging of batch job performance for operational visibility. Refer to Publish custom metrics for more details. The following are some useful metrics: 
   
   Average job duration 
   Token throughput rate (inputTokenCount + outputTokenCount) / jobDuration) 
   Error rates and types 
    
 
Estimated costs 
The cost estimate of running this solution one time is less than $1. The estimate for batch inference jobs considers Anthropic‚Äôs Claude 3.5 sonnet V2 model. Refer to Model pricing details for batch job pricing of other models on Amazon Bedrock. 
Clean up 
If you no longer need this automated monitoring solution, follow these steps to delete the resources it created to avoid additional costs: 
 
 On the Amazon S3 console, choose Buckets in the navigation pane. 
 Select the bucket you created and choose Empty to delete its contents. 
 On the AWS CloudFormation console, choose Stacks in the navigation pane. 
 Select the created stack and choose Delete. 
 
This automatically deletes the deployed stack and the resources created. 
Conclusion 
In this post, we demonstrated how a financial services company can use an FM to process large volumes of customer records and get specific data-driven product recommendations. We also showed how to implement an automated monitoring solution for Amazon Bedrock batch inference jobs. By using EventBridge, Lambda, and DynamoDB, you can gain real-time visibility into batch processing operations, so you can efficiently generate personalized product recommendations based on customer credit data. The solution addresses key challenges in managing batch inference operations: 
 
 Alleviates the need for manual status checking or continuous polling 
 Provides immediate notifications when jobs complete or fail 
 Maintains a centralized record of job statuses 
 
This automated monitoring approach significantly enhances the ability to process large amounts of financial data using batch inference for Amazon Bedrock. This solution offers a scalable, efficient, and cost-effective approach to do batch inference for a variety of use cases, such as generating product recommendations, identifying fraud patterns, or analyzing financial trends in bulk, with the added benefit of real-time operational visibility. 
 
About the authors 
Durga Prasad&nbsp;is a Senior Consultant at AWS, specializing in the Data and AI/ML. He has over 17 years of industry experience and is passionate about helping customers design, prototype, and scale Big Data and Generative AI applications using AWS native and open-source tech stacks. 
Chanpreet Singh is a Senior Consultant at AWS with 18+ years of industry experience, specializing in Data Analytics and AI/ML solutions. He partners with enterprise customers to architect and implement cutting-edge solutions in Big Data, Machine Learning, and Generative AI using AWS native services, partner solutions and open-source technologies. A passionate technologist and problem solver, he balances his professional life with nature exploration, reading, and quality family time.
‚Ä¢ Responsible AI: How PowerSchool safeguards millions of students with AI-powered content filtering using Amazon SageMaker AI
  This post is cowritten with Gayathri Rengarajan and Harshit Kumar Nyati from PowerSchool. 
PowerSchool is a leading provider of cloud-based software for K-12 education, serving over 60 million students in more than 90 countries and over 18,000 customers, including more than 90 of the top 100 districts by student enrollment in the United States. When we launched PowerBuddy, our AI assistant integrated across our multiple educational platforms, we faced a critical challenge: implementing content filtering sophisticated enough to distinguish between legitimate academic discussions and harmful content in educational contexts. 
In this post, we demonstrate how we built and deployed a custom content filtering solution using Amazon SageMaker AI that achieved better accuracy while maintaining low false positive rates. We walk through our technical approach to fine tuning Llama 3.1 8B, our deployment architecture, and the performance results from internal validations. 
PowerSchool‚Äôs PowerBuddy 
PowerBuddy is an AI assistant that provides personalized insights, fosters engagement, and provides support throughout the educational journey. Educational leaders benefit from PowerBuddy being brought to their data and their users‚Äô most common workflows within the PowerSchool ecosystem ‚Äì such as Schoology Learning, Naviance CCLR, PowerSchool SIS, Performance Matters, and more ‚Äì to ensure a consistent experience for students and their network of support providers at school and at home. 
The PowerBuddy suite includes several AI solutions: PowerBuddy for Learning functions as a virtual tutor; PowerBuddy for College and Career provides insights for career exploration; PowerBuddy for Community simplifies access to district and school information, and others. The solution includes built-in accessibility features such as speech-to-text and text-to-speech functionality. 
Content filtering for PowerBuddy 
As an education technology provider serving millions of students‚Äîmany of whom are minors‚Äîstudent safety is our highest priority. National data shows that approximately 20% of students ages 12‚Äì17 experience bullying, and 16% of high school students have reported seriously considering suicide. With PowerBuddy‚Äôs widespread adoption across K-12 schools, we needed robust guardrails specifically calibrated for educational environments. 
The out-of-the-box content filtering and safety guardrails solutions available on the market didn‚Äôt fully meet PowerBuddy‚Äôs requirements, primarily because of the need for domain-specific awareness and fine-tuning within the education context. For example, when a high school student is learning about sensitive historical topics such as World War II or the Holocaust, it‚Äôs important that educational discussions aren‚Äôt mistakenly flagged for violent content. At the same time, the system must be able to detect and immediately alert school administrators to indications of potential harm or threats. Achieving this nuanced balance requires deep contextual understanding, which can only be enabled through targeted fine-tuning. 
We needed to implement a sophisticated content filtering system that could intelligently differentiate between legitimate academic inquiries and truly harmful content‚Äîdetecting and blocking prompts indicating bullying, self-harm, hate speech, inappropriate sexual content, violence, or harmful material not suitable for educational settings. Our challenge was finding a cloud solution to train and host a custom model that could reliably protect students while maintaining the educational functionality of PowerBuddy. 
After evaluating multiple AI providers and cloud services that allow model customization and fine-tuning, we selected Amazon SageMaker AI as the most suitable platform based on these critical requirements: 
 
 Platform stability: As a mission-critical service supporting millions of students daily, we require an enterprise-grade infrastructure with high availability and reliability. 
 Autoscaling capabilities: Student usage patterns in education are highly cyclical, with significant traffic spikes during school hours. Our solution needed to handle these fluctuations without degrading performance. 
 Control of model weights after fine-tuning: We needed control over our fine-tuned models to enable continuous refinement of our safety guardrails, enabling us to quickly respond to new types of harmful content that might emerge in educational settings. 
 Incremental training capability: The ability to continually improve our content filtering model with new examples of problematic content was essential. 
 Cost-effectiveness: We needed a solution that would allow us to protect students without creating prohibitive costs that would limit schools‚Äô access to our educational tools. 
 Granular control and transparency: Student safety demands visibility into how our filtering decisions are made, requiring a solution that isn‚Äôt a black box but provides transparency into model behavior and performance. 
 Mature managed service: Our team needed to focus on educational applications rather than infrastructure management, making a comprehensive managed service with production-ready capabilities essential. 
 
Solution overview 
 
Our content filtering system architecture, shown in the preceding figure, consists of several key components: 
 
 Data preparation pipeline: 
   
   Curated datasets of safe and unsafe content examples specific to educational contexts 
   Data preprocessing and augmentation to ensure robust model training 
   Secure storage in Amazon S3 buckets with appropriate encryption and access controls Note: All training data was fully anonymized and did not include personally identifiable student information 
    
 
 
 Model training infrastructure: 
   
   SageMaker training jobs for fine-tuning Llama 3.1 8B 
    
 
 
 Inference architecture: 
   
   Deployment on SageMaker managed endpoints with auto-scaling configured 
   Integration with PowerBuddy through Amazon API Gateway for real-time content filtering 
   Monitoring and logging through Amazon CloudWatch for continuous quality assessment 
    
 
 
 Continuous improvement loop: 
   
   Feedback collection mechanism for false positives/negatives 
   Scheduled retraining cycles to incorporate new data and improve performance 
   A/B testing framework to evaluate model improvements before full deployment 
    
 
Development process 
After exploring multiple approaches to content filtering, we decided to fine-tune Llama 3.1 8B using Amazon SageMaker JumpStart. This decision followed our initial attempts to develop a content filtering model from scratch, which proved challenging to optimize for consistency across various types of harmful content. 
SageMaker JumpStart significantly accelerated our development process by providing pre-configured environments and optimized hyperparameters for fine-tuning foundation models. The platform‚Äôs streamlined workflow allowed our team to focus on curating high-quality training data specific to educational safety concerns rather than spending time on infrastructure setup and hyperparameter tuning. 
We fine-tuned Llama 3.1 8B model using Low Rank Adaptation (LoRA) technique on Amazon SageMaker AI training jobs, which allowed us to maintain full control over the training process. 
After the fine-tuning was done, we deployed the model on SageMaker AI managed endpoint and integrated it as a critical safety component within our PowerBuddy architecture. 
For our production deployment, we selected NVIDIA A10G GPUs available through ml.g5.12xlarge instances, which offered the ideal balance of performance and cost-effectiveness for our model size. The AWS team provided crucial guidance on selecting optimal model serving configuration for our use case. This advice helped us optimize both performance and cost by ensuring we weren‚Äôt over-provisioning resources. 
Technical implementation 
Below is the code snippet to fine-tune the model on the pre-processed dataset. Instruction tuning dataset is first converted into domain adaptation dataset format and scripts utilize Fully Sharded Data Parallel (FSDP) as well as Low Rank Adaptation (LoRA) method for fine-tuning the model. 
We define an estimator object first. By default, these models train via domain adaptation, so you must indicate instruction tuning by setting the instruction_tuned hyperparameter to True. 
 
 estimator = JumpStartEstimator(
    model_id=model_id,
    environment={"accept_eula": "true"},  
    disable_output_compression=True,
    hyperparameters={
        "instruction_tuned": "True",
        "epoch": "5",
        "max_input_length": "1024",
        "chat_dataset": "False"
    },
    sagemaker_session=session,
    base_job_name = "CF-M-0219251"
) 
 
After we define the estimator, we are ready to start training: 
estimator.fit({"training": train_data_location}) 
After training, we created a model using the artifacts stored in S3 and deployed the model to a real-time endpoint for evaluation. We tested the model using our test dataset that covers key scenarios to validate performance and behavior. We calculated recall, F1, confusion matrix and inspected misclassifications. If needed, adjust hyperparameters/prompt template and retrain; otherwise proceed with production deployment. 
You can also check out the sample notebook for fine tuning Llama 3 models on SageMaker JumpStart in SageMaker examples. 
We used the Faster autoscaling on Amazon SageMaker realtime endpoints notebook to set up autoscaling on SageMaker AI endpoints. 
Validation of solution 
To validate our content filtering solution, we conducted extensive testing across multiple dimensions: 
 
 Accuracy testing: In our internal validation testing, the model achieved ~93% accuracy in identifying harmful content across a diverse test set representing various forms of inappropriate material. 
 False positive analysis: We worked to minimize instances where legitimate educational content was incorrectly flagged as harmful, achieving a false positive rate of less than 3.75% in test environments; results may vary by school context. 
 Performance testing: Our solution maintained response times averaging 1.5 seconds. Even during peak usage periods simulating real classroom environments, the system consistently delivered seamless user experience with no failed transactions. 
 Scalability and reliability validation: 
   
   Comprehensive load testing achieved 100% transaction success rate with consistent performance distribution, validating system reliability under sustained educational workload conditions. 
   Transactions completed successfully without degradation in performance or accuracy, demonstrating the system‚Äôs ability to scale effectively for classroom-sized concurrent usage scenarios. 
    
 Production deployment: Initial rollout to a select group of schools showed consistent performance in real-world educational environments. 
 Student safety outcomes: Schools reported a significant reduction in reported incidents of AI-enabled bullying or inappropriate content generation compared to other AI systems without specialized content filtering. 
 
Fine-tuned model metrics compared to out-of-the-box content filtering solutions 
The fine-tuned content filtering model demonstrated higher performance than generic, out-of-the-box filtering solutions in key safety metrics. It achieved a higher accuracy (0.93 compared to 0.89), and better F1-scores for both the safe (0.95 compared to 0.91) and unsafe (0.90 compared to 0.87) classes. The fine-tuned model also demonstrated a more balanced trade-off between precision and recall, indicating more consistent performance across classes. Importantly, it makes fewer false positive errors by misclassifying only 6 safe cases as unsafe, compared to 19 original responses in a test set of 160‚Äî a significant advantage in safety-sensitive applications. Overall, our fine-tuned content filtering model proved to be more reliable and effective. 
Future plans 
As the PowerBuddy suite evolves and is integrated into other PowerSchool products and agent flows, the content filter model will be continuously adapted and improved with fine tuning for other products with specific needs. 
We plan to implement additional specialized adapters using the SageMaker AI multi-adapter inference feature alongside our content filtering model subject to feasibility and compliance consideration. The idea is to deploy fine-tuned small language models (SLMs) for specific problem solving in cases where large language models (LLMs) are huge and generic and don‚Äôt meet the need for narrower problem domains. For example: 
 
 Decision making agents specific to the Education domain 
 Data domain identification in cases of text to SQL queries 
 
This approach will deliver significant cost savings by eliminating the need for separate model deployments while maintaining the specialized performance of each adapter. 
The goal is to create an AI learning environment that is not only safe but also inclusive and responsive to diverse student needs across our global implementations, ultimately empowering students to learn effectively while being protected from harmful content. 
Conclusion 
The implementation of our specialized content filtering system on Amazon SageMaker AI has been transformative for PowerSchool‚Äôs ability to deliver safe AI experiences in educational settings. By building robust guardrails, we‚Äôve addressed one of the primary concerns educators and parents have about introducing AI into classrooms‚Äîhelping to ensure student safety. 
As Shivani Stumpf, our Chief Product Officer, explains: ‚ÄúWe‚Äôre now tracking around 500 school districts who‚Äôve either purchased PowerBuddy or activated included features, reaching over 4.2 million students approximately. Our content filtering technology ensures students can benefit from AI-powered learning support without exposure to harmful content, creating a safe space for academic growth and exploration.‚Äù 
The impact extends beyond just blocking harmful content. By establishing trust in our AI systems, we‚Äôve enabled schools to embrace PowerBuddy as a valuable educational tool. Teachers report spending less time monitoring student interactions with technology and more time on personalized instruction. Students benefit from 24/7 learning support without the risks that might otherwise come with AI access. 
For organizations requiring domain-specific safety guardrails, consider how the fine-tuning capabilities and managed endpoints of SageMaker AI can be adapted to your use case. 
As we continue to expand PowerBuddy‚Äôs capabilities with the multi-adapter inference of SageMaker, we remain committed to maintaining the perfect balance between educational innovation and student safety‚Äîhelping to ensure that AI becomes a positive force in education that parents, teachers, and students can trust. 
 
About the authors 
Gayathri Rengarajan is the Associate Director of Data Science at PowerSchool, leading the PowerBuddy initiative. Known for bridging deep technical expertise with strategic business needs, Gayathri has a proven track record of delivering enterprise-grade generative AI solutions from concept to production. 
 Harshit Kumar Nyati is a Lead Software Engineer at PowerSchool with 10+ years of experience in software engineering and analytics. He specializes in building enterprise-grade Generative AI applications using Amazon SageMaker AI, Amazon Bedrock, and other cloud services. His expertise includes fine-tuning LLMs, training ML models, hosting them in production, and designing MLOps pipelines to support the full lifecycle of AI applications. 
Anjali Vijayakumar is a Senior Solutions Architect at AWS with over 9 years of experience helping customers build reliable and scalable cloud solutions. Based in Seattle, she specializes in architectural guidance for EdTech solutions, working closely with Education Technology companies to transform learning experiences through cloud innovation. Outside of work, Anjali enjoys exploring the Pacific Northwest through hiking. 
Dmitry Soldatkin&nbsp;is a Senior AI/ML Solutions Architect at Amazon Web Services (AWS), helping customers design and build AI/ML solutions. Dmitry‚Äôs work covers a wide range of ML use cases, with a primary interest in Generative AI, deep learning, and scaling ML across the enterprise. He has helped companies in many industries, including insurance, financial services, utilities, and telecommunications. You can connect with Dmitry on&nbsp;LinkedIn. 
Karan Jain is a Senior Machine Learning Specialist at AWS, where he leads the worldwide Go-To-Market strategy for Amazon SageMaker Inference. He helps customers accelerate their generative AI and ML journey on AWS by providing guidance on deployment, cost-optimization, and GTM strategy. He has led product, marketing, and business development efforts across industries for over 10 years, and is passionate about mapping complex service features to customer solutions.

‚∏ª