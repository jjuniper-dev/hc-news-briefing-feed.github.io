‚úÖ Morning News Briefing ‚Äì October 16, 2025 10:45

üìÖ Date: 2025-10-16 10:45
üè∑Ô∏è Tags: #briefing #ai #publichealth #digitalgov

‚∏ª

üßæ Weather
‚Ä¢ Current Conditions:  1.9¬∞C
  Temperature: 1.9&deg;C Pressure / Tendency: 102.2 kPa rising Humidity: 78 % Dewpoint: -1.5&deg:C Wind: W 10 km/h Air Quality Health Index: n/a . Observed at: Pembroke 6:00 AM EDT Thursday 16 October 2025 Temperature: . 1.3¬∞C
‚Ä¢ Thursday: Sunny. High 13.
  Sunny. Sunny. Wind becoming northwest 20 km/h late this morning . High 13.6 degrees Celsius . UV index 4 or moderate for the area, with highs of 13 degrees Celsius or 14 degrees Celsius in the area . Forecast issued 5:00 AM EDT Thursday 16 October 2025 . For the rest of the day, see www.cnn.com/tomorroworrow for
‚Ä¢ Thursday night: Clear. Low plus 1.
  Clear. Clear. Wind north 20 km/h becoming light this evening . Low plus 1.50C is expected to be the coldest night of the year . Forecast issued 5:00 AM EDT Thursday 16 October 2025 . Forecasts issued for October 16, 2025 . Weather forecast: Clear, cold, sunny, cloudy, sunny and breezy, cold. Low minus 1C

üåç International News
No updates.

üçÅ Canadian News
No updates.

üá∫üá∏ U.S. Top Stories
‚Ä¢ Targets of Trump's Justice Department must pay up, even if they win
  For defendants facing criminal charges from the Justice Department, the costs of fighting are enormous and can reverberate for years, veteran attorneys say . The cost of fighting is enormous, according to veteran attorneys, who say it can take years to recoup for years . The costs of defending against criminal charges are enormous, they say, and they can be devastating for many years to the government . The
‚Ä¢ 80-year-old grandmother becomes oldest woman to finish the Ironman World Championship
  Natalie Grabow won her age group at the famously grueling triathlon competition in Hawaii . Grabow was just 20 years old when she was learning to swim . Now Grabow is hailed as an inspiration after winning her age age group in the Hawaii triathlon in her 20th year of age at age of 36 . She won the race in her first triathlon race in 20 years .
‚Ä¢ 'Death fold' proteins can make cells self-destruct. Scientists want to control them
  Scientists are hoping to treat diseases including cancer and Alzheimer's by influencing how cells make life-or-death decisions . Scientists hope to use this technique to help treat diseases such as cancer, Alzheimer's . Scientists are also hoping to influence how cells decide when to make a certain or certain decisions, such as whether to live in a certain place or die, by influencing the cell's decisions .
‚Ä¢ The two wildcards shaping the tail end of the Virginia race for governor
  Virginia is just weeks away from electing a new governor, but the government shutdown and an explosive text message scandal could reshape the election . The governor's race for attorney general is the focus of the race for Virginia's attorney general . Virginia's governor and attorney general race could be the subject of a major political scandal in the state's upcoming governor race . The scandal could change the course of
‚Ä¢ Should the bus be free? Transit advocates are divided
  "Free buses" is one of the big ideas that helped Zohran Mamdani win the Democratic mayoral primary in New York City . But the track record in cities that have stopped collecting fares is mixed . Free buses are a big part of the idea that helped Mammani win New York's mayoral primary . The track record is mixed for cities that stop collecting fares in favor of

üß† Artificial Intelligence
No updates.

üíª Digital Strategy
‚Ä¢ Firefox 144 brings fixes, features, and farewells for 32-bit Linux die-hards
  Mozilla hardens its browser and toys with AI search while closing the door on legacy systems . New versions of both Mozilla's browser and its subsidiary MZLA's messaging client are here ‚Äì with some bad news for users of older kit . Users of Mozilla's new version of both the browser and the messaging client will be released this month . Mozilla's version of the browser is the latest in
‚Ä¢ The $100B memory war: Inside the battle for AI's future
  The AI gold rush is so large that even third place is lucrative . In sprawling AI datacenters housing thousands of GPUs, the real chokepoint isn't processing speed ‚Äì it's memory bandwidth . Even third place in the world is lucrative for even third-place AI . The AI revolution has exposed a brutal truth: raw computing power means nothing if you can't feed the beast . In
‚Ä¢ SpaceX's Starship: Two down, Mons Huygens to climb
  SpaceX is celebrating two consecutive launches without unplanned explosions . But it faces a daunting path forward before the spacecraft can deliver astronauts to the lunar surface . Musk's moonshot still missing orbit, refueling, landing, landing and orbit orbit . SpaceX is still working on a mission to the moon that will take astronauts to orbit and land on the surface of the moon, but it's still missing
‚Ä¢ Larry Ellison's latest craze: Vectorizing all the customers
  Oracle slurps your data whether you like it or not... for the good and bad of the planet . Larry Ellison, co-founder and CTO, lobbed about the terminology in this week's conference keynote as if it conferred some sort of mystical technological incantation .‚Ä¶‚Ä¶‚Ä¶ Or, more accurately, it has vectorized your data, according to Larry Ellison . Oracle has vectored
‚Ä¢ End of support for older Office and Windows Server versions pile on the pain for admins
  Windows 10's free support has shuffled off this mortal coil for most customers . Older versions of Office and Windows Server have also been shown the door . Windows 10 is the least of some people's problems, but that's merely the headline act in Microsoft's October support massacre . Microsoft has also shown off older versions of Windows Server and Office software as support for Windows 10 has been cut .

üè• Public Health
No updates.

üî¨ Science
‚Ä¢ Cardiovascular disease prevention in China: challenges and opportunities in the artificial intelligence-enabled digital health era
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Bacterial sexually transmitted infections and related antibiotic use among individuals eligible for doxycycline post-exposure prophylaxis in the United States
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Exploring sleep and vision impairment through a Patient and Public Involvement and Engagement (PPIE) lens: insights from multiple stakeholders
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Germany‚Äôs national genomDE strategy
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Gender-specific biomechanical effects of smartphone use with backpack load during standing and walking
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations

üßæ Government & Policy
No updates.

üèõÔ∏è Enterprise Architecture & IT Governance
No updates.

ü§ñ AI & Emerging Tech
‚Ä¢ Meet the man building a starter kit for civilization
  You live in a house you designed and built yourself. You rely on the sun for power, heat your home with a woodstove, and farm your own fish and vegetables. The year is 2025.&nbsp;



This is the life of Marcin Jakubowski, the 53-year-old founder of Open Source Ecology, an open collaborative of engineers, producers, and builders developing what they call the Global Village Construction Set (GVCS). It‚Äôs a set of 50 machines‚Äîeverything from a tractor to an oven to a circuit maker‚Äîthat are capable of building civilization from scratch and can be reconfigured however you see fit.&nbsp;



Jakubowski immigrated to the US from Slupca, Poland, as a child. His first encounter with what he describes as the ‚Äúprosperity of technology‚Äù was the vastness of the American grocery store. Seeing the sheer quantity and variety of perfectly ripe produce cemented his belief that abundant, sustainable living was within reach in the United States.&nbsp;





With a bachelor‚Äôs degree from Princeton and a doctorate in physics from the University of Wisconsin, Jakubowski had spent most of his life in school. While his peers kick-started their shiny new corporate careers, he followed a different path after he finished his degree in 2003: He bought a tractor to start a farm in Maysville, Missouri, eager to prove his ideas about abundance. ‚ÄúIt was a clear decision to give up the office cubicle or high-level research job, which is so focused on tiny issues that one never gets to work on the big picture,‚Äù he says. But in just a short few months, his tractor broke down‚Äîand he soon went broke.&nbsp;



Every time his tractor malfunctioned, he had no choice but to pay John Deere for repairs‚Äîeven if he knew how to fix the problem on his own. John Deere, the world‚Äôs largest manufacturer of agricultural equipment, continues to prohibit farmers from repairing their own tractors (except in Colorado, where farmers were granted a right to repair by state law in 2023). Fixing your own tractor voids any insurance or warranty, much like jailbreaking your iPhone.&nbsp;



Today, large agricultural manufacturers have centralized control over the market, and most commercial tractors are built with proprietary parts. Every year, farmers pay $1.2 billion in repair costs and lose an estimated $3 billion whenever their tractors break down, entirely because large agricultural manufacturers have lobbied against the right to repair since the ‚Äô90s. Currently there are class action lawsuits involving hundreds of farmers fighting for their right to do so.



‚ÄúThe machines own farmers. The farmers don‚Äôt own [the machines],‚Äù Jakubowski says. He grew certain that self-sufficiency relied on agricultural autonomy, which could be achieved only through free access to technology. So he set out to apply the principles of open-source software to hardware. He figured that if farmers could have access to the instructions and materials required to build their own tractors, not only would they be able to repair them, but they‚Äôd also be able to customize the vehicles for their needs. Life-changing technology should be available to all, he thought, not controlled by a select few. So, with an understanding of mechanical engineering, Jakubowski built his own tractor and put all his schematics online on his platform Open Source Ecology.&nbsp;&nbsp;



That tractor Jakubowski built is designed to be taken apart. It‚Äôs a critical part of the GVCS, a collection of plug-and-play machines that can ‚Äúbuild a thriving economy anywhere in the world ‚Ä¶ from scratch.‚Äù The GVCS includes a 3D printer, a self-contained hydraulic power unit called the Power Cube, and more, each designed to be reconfigured for multiple purposes. There‚Äôs even a GVCS micro-home. You can use the Power Cube to power a brick press, a sawmill, a car, a CNC mill, or a bioplastic extruder, and you can build wind turbines with the frames that are used in the home.&nbsp;



Jakubowski compares the GVCS to Lego blocks and cites the Linux ecosystem as his inspiration. In the same way that Linux‚Äôs source code is free to inspect, modify, and redistribute, all the instructions you need to build and repurpose a GVCS machine are freely accessible online. Jakubowski envisions a future in which the GVCS parallels the Linux infrastructure, with custom tools built to optimize agriculture, construction, and material fabrication in localized contexts. ‚ÄúThe [final form of the GVCS] must be proven to allow efficient production of food, shelter, consumer goods, cars, fuel, and other goods‚Äîexcept for exotic imports (coffee, bananas, advanced semiconductors),‚Äù he wrote on his Open Source Ecology wiki.&nbsp;



The ethos of GVCS is reminiscent of the Whole Earth Catalog, a countercultural publication that offered a combination of reviews, DIY manuals, and survival guides between 1968 and 1972. Founded by Stewart Brand, the publication had the slogan ‚ÄúAccess to tools‚Äù and was famous for promoting self-sufficiency. It heavily featured the work of R. Buckminster Fuller, an American architect known for his geodesic domes (lightweight structures that can be built using recycled materials) and for coining the term ‚Äúephemeralization,‚Äù which refers to the ability of technology to let us do more with less material, energy, and effort.&nbsp;



The schematics for Marcin Jakubowski&#8217;s designs are all available online.COURTESY OF OPEN SOURCE ECOLOGY




Jakubowski owns the publication‚Äôs entire printed output, but he offers a sharp critique of its legacy in our current culture of tech utopianism. ‚ÄúThe first structures we built were domes. Good ideas. But the open-source part of that was not really there yet‚ÄîFuller patented his stuff,‚Äù he says. Fuller and the Whole Earth Catalog may have popularized an important philosophy of self-reliance, but to Jakubowski, their failure to advocate for open collaboration stopped the ultimate vision of sustainability from coming to fruition. ‚ÄúThe failure of the techno-utopians to organize into a larger movement of collaborative, open, distributed production resulted in a miscarriage of techno-utopia,‚Äù he says.&nbsp;



With a background in physics and an understanding of mechanical engineering, Marcin Jakubowski built his own tractor.COURTESY OF OPEN SOURCE ECOLOGY




Unlike software, hardware can‚Äôt be infinitely reproduced or instantly tested. It requires manufacturing infrastructure and specific materials, not to mention exhaustive documentation. There are physical constraints‚Äîdifferent port standards, fluctuations in availability of materials, and more. And now that production chains are so globalized that manufacturing a hot tub can require parts from seven different countries and 14 states, how can we expect anything to be replicable in our backyard? The solution, according to Jakubowski, is to make technology ‚Äúappropriate.‚Äù&nbsp;



Appropriate technology is technology that‚Äôs designed to be affordable and sustainable for a specific local context. The idea comes from Gandhi‚Äôs philosophy of swadeshi (self-reliance) and sarvodaya (upliftment of all) and was popularized by the economist Ernst Friedrich ‚ÄúFritz‚Äù Schumacher‚Äôs book Small Is Beautiful, which discussed the concept of ‚Äúintermediate technology‚Äù: ‚ÄúAny intelligent fool can make things bigger, more complex, and more violent. It takes a touch of genius‚Äîand a lot of courage‚Äîto move in the opposite direction.‚Äù Because different environments operate at different scales and with different resources, it only makes sense to tailor technology for those conditions. Solar lamps, bikes, hand-¬≠powered water pumps‚Äîanything that can be built using local materials and maintained by the local community‚Äîare among the most widely cited examples of appropriate technology.&nbsp;





This concept has historically been discussed in the context of facilitating economic growth in developing nations and adapting capital-intensive technology to their needs. But Jakubowski hopes to make it universal. He believes technology needs to be appropriate even in suburban and urban places with access to supermarkets, hardware stores, Amazon deliveries, and other forms of infrastructure. If technology is designed specifically for these contexts, he says, end-to-end reproduction will be possible, making more space for collaboration and innovation.&nbsp;



What makes Jakubowski‚Äôs technology ‚Äúappropriate‚Äù is his use of reclaimed materials and off-the-shelf parts to build his machines. By using local materials and widely available components, he‚Äôs able to bypass the complex global supply chains that proprietary technology often requires. He also structures his schematics around concepts already familiar to most people who are interested in hardware, making his building instructions easier to follow.



Everything you need to build Jakubowski‚Äôs machines should be available around you, just as everything you need to know about how to repair or operate the machine is online‚Äîfrom blueprints to lists of materials to assembly instructions and testing protocols. ‚ÄúIf you‚Äôve got a wrench, you‚Äôve got a tractor,‚Äù his manual reads.&nbsp;&nbsp;



This spirit dates back to the ‚Äô70s, when the idea of building things ‚Äúmoved out of the retired person‚Äôs garage and into the young person‚Äôs relationship with the Volkswagen,‚Äù says Brand. He references John Muir‚Äôs 1969 book How to Keep Your Volkswagen Alive: A Manual of Step-by-Step Procedures for the Compleat Idiot and fondly recalls how the Beetle‚Äôs simple design and easily swapped parts made it common for owners to rebody their cars, combining the chassis of one with the body of another. He also mentions the impact of the Ford Model T cars that, with a few extra parts, were made into tractors during the Great Depression.&nbsp;



For Brand, the focus on repairability is critical in the modern context. There was a time when John Deere tractors were ‚Äúappropriate‚Äù in Jakubowski‚Äôs terms, Brand says: ‚ÄúA century earlier, John Deere took great care to make sure that his plowshares could be taken apart and bolted together, that you can undo and redo them, replace parts, and so on.‚Äù The company ‚Äúattracted insanely loyal customers because they looked out for the farmers so much,‚Äù Brand says, but ‚Äúthey‚Äôve really reversed the orientation.‚Äù Echoing Jakubowski‚Äôs initial motivation for starting OSE, Brand insists that technology is appropriate to the extent that it is repairable.&nbsp;



Even if you can find all the parts you need from Lowe‚Äôs, building your own tractor is still intimidating. But for some, the staggering price advantage is reason enough to take on the challenge: A GVCS tractor costs $12,000 to build, whereas a commercial tractor averages around $120,000 to buy, not including the individual repairs that might be necessary over its lifetime at a cost of $500 to $20,000 each. And gargantuan though it may seem, the task of building a GVCS tractor or other machine is doable: Just a few years after the project launched in 2008, more than 110 machines had been built by enthusiasts from Chile, Nicaragua, Guatemala, China, India, Italy, and Turkey, just to name a few places.&nbsp;



Of the many machines developed, what‚Äôs drawn the most interest from GVCS enthusiasts is the one nicknamed ‚ÄúThe Liberator,‚Äù which presses local soil into compressed earth blocks, or CEBs‚Äîa type of cost- and energy-¬≠efficient brick that can withstand extreme weather conditions. It‚Äôs been especially popular among those looking to build their own homes: A man named Aur√©lien Bielsa replicated the brick press in a small village in the south of France to build a house for his family in 2018, and in 2020 a group of volunteers helped a member of the Open Source Ecology community build a tiny home using blocks from one of these presses in a fishing village near northern Belize.&nbsp;



The CEB press, nicknamed ‚ÄúThe Liberator,‚Äù turns local soil into energy-efficient compressed earth blocks.COURTESY OF OPEN SOURCE ECOLOGY




Jakubowski recalls receiving an email about one of the first complete reproductions of the CEB press, built by a Texan named James Slate, who ended up starting a business selling the bricks: ‚ÄúWhen [James] sent me a picture [of our brick press], I thought it was a Photoshopped copy of our machine, but it was his. He just downloaded the plans off the internet. I knew nothing about it.‚Äù Slate described having a very limited background in engineering before building the brick press. ‚ÄúI had taken some mechanics classes back in high school. I mostly come from an IT computer world,‚Äù he said in an interview with Open Source Ecology. ‚ÄúPretty much anyone can build one, if they put in the effort.‚Äù&nbsp;





Andrew Spina, an early GVCS enthusiast, agrees. Spina spent five years building versions of the GVCS tractor and Power Cube, eager to create means of self-¬≠sufficiency at an individual scale. ‚ÄúI‚Äôm building my own tractor because I want to understand it and be able to maintain it,‚Äù he wrote in his blog, Machining Independence. Spina‚Äôs curiosity gestures toward the broader issue of technological literacy: The more we outsource to proprietary tech, the less we understand how things work‚Äîfurther entrenching our need for that proprietary tech. Transparency is critical to the open-source philosophy precisely because it helps us become self-sufficient.&nbsp;



Since starting Open Source Ecology, Jakubowski has been the main architect behind the dozens of machines available on his platform, testing and refining his designs on a plot of land he calls the Factor¬†e Farm in Maysville. Most GVCS enthusiasts reproduce Jakubowski‚Äôs machines for personal use; only a few have contributed to the set themselves. Of those select few, many made dedicated visits to the farm for weeks at a time to learn how to build Jakubowski‚Äôs GVCS collection. James Wise, one of the earliest and longest-term GVCS contributors, recalls setting up tents and camping out in his car to attend sessions at Jakubowski‚Äôs workshop, where visiting enthusiasts would gather to iterate on designs: ‚ÄúWe‚Äôd have a screen on the wall of our current best idea. Then we‚Äôd talk about it.‚Äù Wise doesn‚Äôt consider himself particularly experienced on the engineering front, but after working with other visiting participants, he felt more emboldened to contribute. ‚ÄúMost of [my] knowledge came from [my] peers,‚Äù he says.¬†



Jakubowski‚Äôs goal of bolstering collaboration hinges on a degree of collective proficiency. Without a community skilled with hardware, the organic innovation that the open-source approach promises will struggle to bear fruit, even if Jakubowski‚Äôs designs are perfectly appropriate and thoroughly documented.



‚ÄúThat‚Äôs why we‚Äôre starting a school!‚Äù said Jakubowski, when asked about his plan to build hardware literacy. Earlier this year, he announced the Future Builders Academy, an apprenticeship program where participants will be taught all the necessary skills to develop and build the affordable, self-sustaining homes that are his newest venture. Seed Eco Homes, as Jakubowski calls them, are ‚Äúhuman-sized, panelized‚Äù modular houses complete with a biodigester, a thermal battery, a geothermal cooling system, and solar electricity. Each house is entirely energy independent and can be built in five days, at a cost of around $40,000. Over eight of these houses have been built across the country, and Jakubowski himself lives in the earliest version of the design. Seed Eco Homes are the culmination of his work on the GVCS: The structure of each house combines parts from the collection and embodies its modular philosophy. The venture represents Jakubowski‚Äôs larger goal of making everyday technology accessible. ‚ÄúHousing [is the] single largest cost in one‚Äôs life‚Äîand a key to so much more,‚Äù he says.



The final goal of Open Source Ecology is a ‚Äúzero marginal cost‚Äù society, where producing an additional unit of a good or service costs little to nothing. Jakubowski‚Äôs interpretation of the concept (popularized by the American economist and social theorist Jeremy Rifkin) assumes that by eradicating licensing fees, decentralizing manufacturing, and fostering collaboration through education, we can develop truly equitable technology that allows us to be self-sufficient. Open-source hardware isn‚Äôt just about helping farmers build their own tractors; in Jakubowski‚Äôs view, it‚Äôs a complete reorientation of our relationship to technology.&nbsp;



In the first issue of the Whole Earth Catalog, a key piece of inspiration for Jakubowski‚Äôs project, Brand wrote: ‚ÄúWe are as gods and we might as well get good at it.‚Äù In 2007, in a book Brand wrote about the publication, he corrected himself: ‚ÄúWe are as gods and have to get good at it.‚Äù Today, Jakubowski elaborates: ‚ÄúWe‚Äôre becoming gods with technology. Yet technology has badly failed us. We‚Äôve seen great progress with civilization. But how free are people today compared to other times?‚Äù Cautioning against our reliance on the proprietary technology we use daily, he offers a new approach: Progress should mean not just achieving technological breakthroughs but also making everyday technology equitable.&nbsp;



‚ÄúWe don‚Äôt need more technology,‚Äù he says. ‚ÄúWe just need to collaborate with what we have now.‚Äù



Tiffany Ng is a freelance writer exploring the relationship between art, tech, and culture. She writes Cyber Celibate, a neo-Luddite newsletter on Substack.&nbsp;
‚Ä¢ The race to make the perfect baby is creating an ethical mess
  Consider, if you will, the translucent blob in the eye of a microscope: a human blastocyst, the biological specimen that emerges just five days or so after a fateful encounter between egg and sperm. This bundle of cells, about the size of a grain of sand pulled from a powdery white Caribbean beach, contains the coiled potential of a future life: 46 chromosomes, thousands of genes, and roughly six billion base pairs of DNA‚Äîan instruction manual to assemble a one-of-a-kind human.



Now imagine a laser pulse snipping a hole in the blastocyst‚Äôs outermost shell so a handful of cells can be suctioned up by a microscopic pipette. This is the moment, thanks to advances in genetic sequencing technology, when it becomes possible to read virtually that entire instruction manual.



An emerging field of science seeks to use the analysis pulled from that procedure to predict what kind of a person that embryo might become. Some parents turn to these tests to avoid passing on devastating genetic disorders that run in their families. A much smaller group, driven by dreams of Ivy League diplomas or attractive, well-behaved offspring, are willing to pay tens of thousands of dollars to optimize for intelligence, appearance, and personality. Some of the most eager early boosters of this technology are members of the Silicon Valley elite, including tech billionaires like Elon Musk, Peter Thiel, and Coinbase CEO Brian Armstrong.&nbsp;




Embryo selection is less like a build-a-baby workshop and more akin to a store where parents can shop for their future children from several available models‚Äîcomplete with stat cards.




But customers of the companies emerging to provide it to the public may not be getting what they‚Äôre paying for. Genetics experts have been highlighting the potential deficiencies of this testing for years. A 2021 paper by members of the European Society of Human Genetics said, ‚ÄúNo clinical research has been performed to assess its diagnostic effectiveness in embryos. Patients need to be properly informed on the limitations of this use.‚Äù And a paper published this May in the Journal of Clinical Medicine echoed this concern and expressed particular reservations about screening for psychiatric disorders and non-¬≠disease-related traits: ‚ÄúUnfortunately, no clinical research has to date been published comprehensively evaluating the effectiveness of this strategy [of predictive testing]. Patient awareness regarding the limitations of this procedure is paramount.‚Äù &nbsp;&nbsp;&nbsp;



Moreover, the assumptions underlying some of this work‚Äîthat how a person turns out is the product not of privilege or circumstance but of innate biology‚Äîhave made these companies a political lightning rod.&nbsp;



SELMAN DESIGN




As this niche technology begins to make its way toward the mainstream, scientists and ethicists are racing to confront the implications‚Äîfor our social contract, for future generations, and for our very understanding of what it means to be human.







Preimplantation genetic testing (PGT), while still relatively rare, is not new. Since the 1990s, parents undergoing in vitro fertilization have been able to access a number of genetic tests before choosing which embryo to use. A type known as PGT-M can detect single-gene disorders like cystic fibrosis, sickle cell anemia, and Huntington‚Äôs disease. PGT-A can ascertain the sex of an embryo and identify chromosomal abnormalities that can lead to conditions like Down syndrome or reduce the chances that an embryo will implant successfully in the uterus. PGT-SR helps parents avoid embryos with issues such as duplicated or missing segments of the chromosome.



Those tests all identify clear-cut genetic problems that are relatively easy to detect, but most of the genetic instruction manual included in an embryo is written in far more nuanced code. In recent years, a fledgling market has sprung up around a new, more advanced version of the testing process called PGT-P: preimplantation genetic testing for polygenic disorders (and, some claim, traits)‚Äîthat is, outcomes determined by the elaborate interaction of hundreds or thousands of genetic variants.



In 2020, the first baby selected using PGT-P was born. While the exact figure is unknown, estimates put the number of children who have now been born with the aid of this technology in the hundreds. As the technology is commercialized, that number is likely to grow.





Embryo selection is less like a build-a-baby workshop and more akin to a store where parents can shop for their future children from several available models‚Äîcomplete with stat cards indicating their predispositions.



A handful of startups, armed with tens of millions of dollars of Silicon Valley cash, have developed proprietary algorithms to compute these stats‚Äîanalyzing vast numbers of genetic variants and producing a ‚Äúpolygenic risk score‚Äù that shows the probability of an embryo developing a variety of complex traits.&nbsp;&nbsp;



For the last five years or so, two companies‚ÄîGenomic Prediction and Orchid‚Äîhave dominated this small landscape, focusing their efforts on disease prevention. But more recently, two splashy new competitors have emerged: Nucleus Genomics and Herasight, which have rejected the more cautious approach of their predecessors and waded into the controversial territory of genetic testing for intelligence. (Nucleus also offers tests for a wide variety of other behavioral and appearance-related traits.)&nbsp;



The practical limitations of polygenic risk scores are substantial. For starters, there is still a lot we don‚Äôt understand about the complex gene interactions driving polygenic traits and disorders. And the biobank data sets they are based on tend to overwhelmingly represent individuals with Western European ancestry, making it more difficult to generate reliable scores for patients from other backgrounds. These scores also lack the full context of environment, lifestyle, and the myriad other factors that can influence a person‚Äôs characteristics. And while polygenic risk scores can be effective at detecting large, population-level trends, their predictive abilities drop significantly when the sample size is as tiny as a single batch of embryos that share much of the same DNA.



The medical community‚Äîincluding organizations like the American Society of Human Genetics, the American College of Medical Genetics and Genomics, and the American Society for Reproductive Medicine‚Äîis generally wary of using polygenic risk scores for embryo selection. ‚ÄúThe practice has moved too fast with too little evidence,‚Äù the American College of Medical Genetics and Genomics wrote in an official statement in 2024.



But beyond questions of whether evidence supports the technology‚Äôs effectiveness, critics of the companies selling it accuse them of reviving a disturbing ideology: eugenics, or the belief that selective breeding can be used to improve humanity. Indeed, some of the voices who have been most confident that these methods can successfully predict nondisease traits have made startling claims about natural genetic hierarchies and innate racial differences.



What everyone can agree on, though, is that this new wave of technology is helping to inflame a centuries-old debate over nature versus nurture.







The term ‚Äúeugenics‚Äù was coined in 1883 by a British anthropologist and statistician named Sir Francis Galton, inspired in part by the work of his cousin Charles Darwin. He derived it from a Greek word meaning ‚Äúgood in stock, hereditarily endowed with noble qualities.‚Äù



Some of modern history‚Äôs darkest chapters have been built on Galton‚Äôs legacy, from the Holocaust to the forced sterilization laws that affected certain groups in the United States well into the 20th century. Modern science has demonstrated the many logical and empirical problems with Galton‚Äôs methodology. (For starters, he counted vague concepts like ‚Äúeminence‚Äù‚Äîas well as infections like syphilis and tuberculosis‚Äîas heritable phenotypes, meaning characteristics that result from the interaction of genes and environment.)



Yet even today, Galton‚Äôs influence lives on in the field of behavioral genetics, which investigates the genetic roots of psychological traits. Starting in the 1960s, researchers in the US began to revisit one of Galton‚Äôs favorite methods: twin studies. Many of these studies, which analyzed pairs of identical and fraternal twins to try to determine which traits were heritable and which resulted from socialization, were funded by the US government. The most well-known of these, the Minnesota Twin Study, also accepted grants from the Pioneer Fund, a now defunct nonprofit that had promoted eugenics and ‚Äúrace betterment‚Äù since its founding in 1937.&nbsp;



The nature-versus-nurture debate hit a major inflection point in 2003, when the Human Genome Project was declared complete. After 13 years and at a cost of nearly $3 billion, an international consortium of thousands of researchers had sequenced 92% of the human genome for the first time.



Today, the cost of sequencing a genome can be as low as $600, and one company says it will soon drop even further. This dramatic reduction has made it possible to build massive DNA databases like the UK Biobank and the National Institutes of Health‚Äôs All of Us, each containing genetic data from more than half a million volunteers. Resources like these have enabled researchers to conduct genome-wide association studies, or GWASs, which identify correlations between genetic variants and human traits by analyzing single-nucleotide polymorphisms (SNPs)‚Äîthe most common form of genetic variation between individuals. The findings from these studies serve as a reference point for developing polygenic risk scores.



Most GWASs have focused on disease prevention and personalized medicine. But in 2011, a group of medical researchers, social scientists, and economists launched the Social Science Genetic Association Consortium (SSGAC) to investigate the genetic basis of complex social and behavioral outcomes. One of the phenotypes they focused on was the level of education people reached.



‚ÄúIt was a bit of a phenotype of convenience,‚Äù explains Patrick Turley, an economist and member of the steering committee at SSGAC, given that educational attainment is routinely recorded in surveys when genetic data is collected. Still, it was ‚Äúclear that genes play some role,‚Äù he says. ‚ÄúAnd trying to understand what that role is, I think, is really interesting.‚Äù He adds that social scientists can also use genetic data to try to better ‚Äúunderstand the role that is due to nongenetic pathways.‚Äù




Many on the left are generally willing to allow that any number of traits, from addiction to obesity, are genetically influenced. Yet heritable cognitive ability seems to be ‚Äúbeyond the pale for us to integrate as a source of difference.‚Äù




The work immediately stirred feelings of discomfort‚Äînot least among the consortium‚Äôs own members, who feared that they might unintentionally help reinforce racism, inequality, and genetic determinism.&nbsp;



It‚Äôs also created quite a bit of discomfort in some political circles, says Kathryn Paige Harden, a psychologist and behavioral geneticist at the University of Texas in Austin, who says she has spent much of her career making the unpopular argument to fellow liberals that genes are relevant predictors of social outcomes.&nbsp;



Harden thinks a strength of those on the left is their ability to recognize ‚Äúthat bodies are different from each other in a way that matters.‚Äù Many are generally willing to allow that any number of traits, from addiction to obesity, are genetically influenced. Yet, she says, heritable cognitive ability seems to be ‚Äúbeyond the pale for us to integrate as a source of difference that impacts our life.‚Äù&nbsp;



Harden believes that genes matter for our understanding of traits like intelligence, and that this should help shape progressive policymaking. She gives the example of an education department seeking policy interventions to improve math scores in a given school district. If a polygenic risk score is ‚Äúas strongly correlated with their school grades‚Äù as family income is, she says of the students in such a district, then ‚Äúdoes deliberately not collecting that [genetic] information, or not knowing about it, make your research harder [and] your inferences worse?‚Äù



To Harden, persisting with this strategy of avoidance for fear of encouraging eugenicists is a mistake. If ‚Äúinsisting that IQ is a myth and genes have nothing to do with it was going to be successful at neutralizing eugenics,‚Äù she says, ‚Äúit would‚Äôve won by now.‚Äù



Part of the reason these ideas are so taboo in many circles is that today‚Äôs debate around genetic determinism is still deeply infused with Galton‚Äôs ideas‚Äîand has become a particular fixation among the online right.&nbsp;



SELMAN DESIGN




After Elon Musk took over Twitter (now X) in 2022 and loosened its restrictions on hate speech, a flood of accounts started sharing racist posts, some speculating about the genetic origins of inequality while arguing against immigration and racial integration. Musk himself frequently reposts and engages with accounts like Cr√©mieux Recueil, the pen name of independent researcher Jordan Lasker, who has written about the ‚ÄúBlack-White IQ gap,‚Äù and i/o, an anonymous account that once praised Musk for ‚Äúacknowledging data on race and crime,‚Äù saying it ‚Äúhas done more to raise awareness of the disproportionalities observed in these data than anything I can remember.‚Äù (In response to allegations that his research encourages eugenics, Lasker wrote to MIT Technology Review, ‚ÄúThe popular understanding of eugenics is about coercion and cutting people cast as ‚Äòundesirable‚Äô out of the breeding pool. This is nothing like that, so it doesn‚Äôt qualify as eugenics by that popular understanding of the term.‚Äù After going to print, i/o wrote&nbsp;in an email,&nbsp;‚ÄúJust because differences in intelligence at the&nbsp;individual&nbsp;level are largely heritable, it does not mean that&nbsp;group&nbsp;differences in measured intelligence &#8230; are due to genetic differences between groups,‚Äù&nbsp;but that the latter is not ‚Äúscientifically settled‚Äù and&nbsp;‚Äúan extremely important (and necessary) research area that should be funded rather than made taboo.‚Äù&nbsp;He added,&nbsp;‚ÄúI‚Äôve never made any argument against racial integration or intermarriage or whatever.‚Äù&nbsp;X and Musk did not respond to requests for comment.)



Harden, though, warns against discounting the work of an entire field because of a few noisy neoreactionaries. ‚ÄúI think there can be this idea that technology is giving rise to the terrible racism,‚Äù she says. The truth, she believes, is that ‚Äúthe racism has preexisted any of this technology.‚Äù







In 2019, a company called Genomic Prediction began to offer the first preimplantation polygenic testing that had ever been made commercially available. With its LifeView Embryo Health Score, prospective parents are able to assess their embryos‚Äô predisposition to genetically complex health problems like cancer, diabetes, and heart disease. Pricing for the service starts at $3,500. Genomic Prediction uses a technique called an SNP array, which targets specific sites in the genome where common variants occur. The results are then cross-checked against GWASs that show correlations between genetic variants and certain diseases.





Four years later, a company named Orchid began offering a competing test. Orchid‚Äôs Whole Genome Embryo Report distinguished itself by claiming to sequence more than 99% of an embryo‚Äôs genome, allowing it to detect novel mutations and, the company says, diagnose rare diseases more accurately. For $2,500 per embryo, parents can access polygenic risk scores for 12 disorders, including schizophrenia, breast cancer, and hypothyroidism.&nbsp;



Orchid was founded by a woman named Noor Siddiqui. Before getting undergraduate and graduate degrees from Stanford, she was awarded the Thiel fellowship‚Äîa $200,000 grant given to young entrepreneurs willing to work on their ideas instead of going to college‚Äîback when she was a teenager, in 2012. This set her up to attract attention from members of the tech elite as both customers and financial backers. Her company has raised $16.5 million to date from investors like Ethereum founder Vitalik Buterin, former Coinbase CTO Balaji Srinivasan, and Armstrong, the Coinbase CEO.



In August Siddiqui made the controversial suggestion that parents who choose not to use genetic testing might be considered irresponsible. ‚ÄúJust be honest: you‚Äôre okay with your kid potentially suffering for life so you can feel morally superior ‚Ä¶‚Äù she wrote on X.



Americans have varied opinions on the emerging technology. In 2024, a group of bioethicists surveyed 1,627 US adults to determine attitudes toward a variety of polygenic testing criteria. A large majority approved of testing for physical health conditions like cancer, heart disease, and diabetes. Screening for mental health disorders, like depression, OCD, and ADHD, drew a more mixed‚Äîbut still positive‚Äîresponse. Appearance-related traits, like skin color, baldness, and height, received less approval as something to test for.



Intelligence was among the most contentious traits‚Äîunsurprising given the way it has been weaponized throughout history and the lack of cultural consensus on how it should even be defined. (In many countries, intelligence testing for embryos is heavily regulated; in the UK, the practice is banned outright.) In the 2024 survey, 36.9% of respondents approved of preimplantation genetic testing for intelligence, 40.5% disapproved, and 22.6% said they were uncertain.







Despite the disagreement, intelligence has been among the traits most talked about as targets for testing. From early on, Genomic Prediction says, it began receiving inquiries ‚Äúfrom all over the world‚Äù about testing for intelligence, according to Diego Marin, the company‚Äôs head of global business development and scientific affairs.



At one time, the company offered a predictor for what it called ‚Äúintellectual disability.‚Äù After some backlash questioning both the predictive capacity and the ethics of these scores, the company discontinued the feature. ‚ÄúOur mission and vision of this company is not to improve [a baby], but to reduce risk for disease,‚Äù Marin told me. ‚ÄúWhen it comes to traits about IQ or skin color or height or something that‚Äôs cosmetic and doesn‚Äôt really have a connotation of a disease, then we just don‚Äôt invest in it.‚Äù



Orchid, on the other hand, does test for genetic markers associated with intellectual disability and developmental delay. But that may not be all. According to one employee of the company, who spoke on the condition of anonymity, intelligence testing is also offered to ‚Äúhigh-roller‚Äù clients. According to this employee, another source close to the company, and reporting in the Washington Post, Musk used Orchid‚Äôs services in the conception of at least one of the children he shares with the tech executive Shivon Zilis. (Orchid, Musk, and Zilis did not respond to requests for comment.)







I met Kian Sadeghi, the 25-year-old founder of New York‚Äìbased Nucleus Genomics, on a sweltering July afternoon in his SoHo office. Slight and kinetic, Sadeghi spoke at a machine-gun pace, pausing only occasionally to ask if I was keeping up.&nbsp;



Sadeghi had modified his first organism‚Äîa sample of brewer‚Äôs yeast‚Äîat the age of 16. As a high schooler in 2016, he was taking a course on CRISPR-Cas9 at a Brooklyn laboratory when he fell in love with the ‚Äúbeautiful depth‚Äù of genetics. Just a few years later, he dropped out of college to build ‚Äúa better 23andMe.‚Äù&nbsp;



His company targets what you might call the application layer of PGT-P, accepting data from IVF clinics‚Äîand even from the competitors mentioned in this story‚Äîand running its own computational analysis.



‚ÄúUnlike a lot of the other testing companies, we‚Äôre software first, and we‚Äôre consumer first,‚Äù Sadeghi told me. ‚ÄúIt‚Äôs not enough to give someone a polygenic score. What does that mean? How do you compare them? There‚Äôs so many really hard design problems.‚Äù



Like its competitors, Nucleus calculates its polygenic risk scores by comparing an individual‚Äôs genetic data with trait-associated variants identified in large GWASs, providing statistically informed predictions.&nbsp;



Nucleus provides two displays of a patient‚Äôs results: a Z-score, plotted from ‚Äì4 to 4, which explains the risk of a certain trait relative to a population with similar genetic ancestry (for example, if Embryo #3 has a 2.1 Z-score for breast cancer, its risk is higher than average), and an absolute risk score, which includes relevant clinical factors (Embryo #3 has a minuscule actual risk of breast cancer, given that it is male).



The real difference between Nucleus and its competitors lies in the breadth of what it claims to offer clients. On its sleek website, prospective parents can sort through more than 2,000 possible diseases, as well as traits from eye color to IQ. Access to the Nucleus Embryo platform costs $8,999, while the company‚Äôs new IVF+ offering‚Äîwhich includes one IVF cycle with a partner clinic, embryo screening for up to 20 embryos, and concierge services throughout the process‚Äîstarts at $24,999.




‚ÄúMaybe you want your baby to have blue eyes versus green eyes,‚Äù Nucleus founder Kian Sadeghi said at a June event. ‚ÄúThat is up to the liberty of the parents.‚Äù




Its promises are remarkably bold. The company claims to be able to forecast a propensity for anxiety, ADHD, insomnia, and other mental issues. It says you can see which of your embryos are more likely to have alcohol dependence, which are more likely to be left-handed, and which might end up with severe acne or seasonal allergies. (Nevertheless, at the time of writing, the embryo-screening platform provided this disclaimer: ‚ÄúDNA is not destiny. Genetics can be a helpful tool for choosing an embryo, but it‚Äôs not a guarantee. Genetic research is still in it‚Äôs [sic] infancy, and there‚Äôs still a lot we don‚Äôt know about how DNA shapes who we are.‚Äù)



To people accustomed to sleep trackers, biohacking supplements, and glucose monitoring, taking advantage of Nucleus‚Äôs options might seem like a no-brainer. To anyone who welcomes a bit of serendipity in their life, this level of perceived control may be disconcerting to say the least.



Sadeghi likes to frame his arguments in terms of personal choice. ‚ÄúMaybe you want your baby to have blue eyes versus green eyes,‚Äù he told a small audience at Nucleus Embryo‚Äôs June launch event. ‚ÄúThat is up to the liberty of the parents.‚Äù



On the official launch day, Sadeghi spent hours gleefully sparring with X users who accused him of practicing eugenics. He rejects the term, favoring instead ‚Äúgenetic optimization‚Äù‚Äîthough it seems he wasn‚Äôt too upset about the free viral marketing. ‚ÄúThis week we got five million impressions on Twitter,‚Äù he told a crowd at the launch event, to a smattering of applause. (In an email to MIT Technology Review, Sadeghi wrote, ‚ÄúThe history of eugenics is one of coercion and discrimination by states and institutions; what Nucleus does is the opposite‚Äîgenetic forecasting that empowers individuals to make informed decisions.‚Äù)



Nucleus has raised more than $36 million from investors like Srinivasan, Alexis Ohanian‚Äôs venture capital firm Seven Seven Six, and Thiel‚Äôs Founders Fund. (Like Siddiqui, Sadeghi was a recipient of a Thiel fellowship when he dropped out of college; a representative for Thiel did not respond to a request for comment for this story.) Sadeghi has even poached Genomic Prediction‚Äôs cofounder Nathan Treff, who is now Nucleus‚Äôs chief clinical officer.



Sadeghi‚Äôs real goal is to build a one-stop shop for every possible application of genetic sequencing technology, from genealogy to precision medicine to genetic engineering. He names a handful of companies providing these services, with a combined market cap in the billions. ‚ÄúNucleus is collapsing all five of these companies into one,‚Äù he says. ‚ÄúWe are not an IVF testing company. We are a genetic stack.‚Äù







This spring, I elbowed my way into a packed hotel bar in the Flatiron district, where over a hundred people had gathered to hear a talk called ‚ÄúHow to create SUPERBABIES.‚Äù The event was part of New York‚Äôs Deep Tech Week, so I expected to meet a smattering of biotech professionals and investors. Instead, I was surprised to encounter a diverse and curious group of creatives, software engineers, students, and prospective parents‚Äîmany of whom had come with no previous knowledge of the subject.



The speaker that evening was Jonathan Anomaly, a soft-spoken political philosopher whose didactic tone betrays his years as a university professor.



Some of Anomaly‚Äôs academic work has focused on developing theories of rational behavior. At Duke and the University of Pennsylvania, he led introductory courses on game theory, ethics, and collective action problems as well as bioethics, digging into thorny questions about abortion, vaccines, and euthanasia. But perhaps no topic has interested him so much as the emerging field of genetic enhancement.&nbsp;





In 2018, in a bioethics journal, Anomaly published a paper with the intentionally provocative title ‚ÄúDefending Eugenics.‚Äù He sought to distinguish what he called ‚Äúpositive eugenics‚Äù‚Äînoncoercive methods aimed at increasing traits that ‚Äúpromote individual and social welfare‚Äù‚Äîfrom the so-called ‚Äúnegative eugenics‚Äù we know from our history books.



Anomaly likes to argue that embryo selection isn‚Äôt all that different from practices we already take for granted. Don‚Äôt believe two cousins should be allowed to have children? Perhaps you‚Äôre a eugenicist, he contends. Your friend who picked out a six-foot-two Harvard grad from a binder of potential sperm donors? Same logic.



His hiring at the University of Pennsylvania in 2019 caused outrage among some students, who accused him of ‚Äúracial essentialism.‚Äù In 2020, Anomaly left academia, lamenting that ‚ÄúAmerican universities had become an intellectual prison.‚Äù



A few years later, Anomaly joined a nascent PGT-P company named Herasight, which was promising to screen for IQ.



At the end of July, the company officially emerged from stealth mode. A representative told me that most of the money raised so far is from angel investors, including Srinivasan, who also invested in Orchid and Nucleus. According to the launch announcement on X, Herasight has screened ‚Äúhundreds of embryos‚Äù for private customers and is beginning to offer its first publicly available consumer product, a polygenic assessment that claims to detect an embryo‚Äôs likelihood of developing 17 diseases.



Their marketing materials boast predictive abilities 122% better than Orchid‚Äôs and 193% better than Genomic Prediction‚Äôs for this set of diseases. (‚ÄúHerasight is comparing their current predictor to models we published over five years ago,‚Äù Genomic Prediction responded in a statement. ‚ÄúOur team is confident our predictors are world-class and are not exceeded in quality by any other lab.‚Äù)&nbsp;



The company did not include comparisons with Nucleus, pointing to the ‚Äúabsence of published performance validations‚Äù by that company and claiming it represented a case where ‚Äúmarketing outpaces science.‚Äù (‚ÄúNucleus is known for world-class science and marketing, and we understand why that‚Äôs frustrating to our competitors,‚Äù a representative from the company responded in a comment.)&nbsp;



Herasight also emphasized new advances in ‚Äúwithin-family validation‚Äù (making sure that the scores are not merely picking up shared environmental factors by comparing their performance between unrelated people to their performance between siblings) and ‚Äúcross-¬≠ancestry accuracy‚Äù (improving the accuracy of scores for people outside the European ancestry groups where most of the biobank data is concentrated). The representative explained that pricing varies by customer and the number of embryos tested, but it can reach $50,000.




When it comes to traits that Jonathan Anomaly believes are genetically encoded, intelligence is just the tip of the iceberg. He has also spoken about the heritability of empathy, violence, religiosity, and political leanings.




Herasight tests for just one non-disease-related trait: intelligence. For a couple who produce 10 embryos, it claims it can detect an IQ spread of about 15 points, from the lowest-scoring embryo to the highest. The representative says the company plans to release a detailed white paper on its IQ predictor in the future.



The day of Herasight‚Äôs launch, Musk responded to the company announcement: ‚ÄúCool.‚Äù Meanwhile, a Danish researcher named Emil Kirkegaard, whose research has largely focused on IQ differences between racial groups, boosted the company to his nearly 45,000 followers on X (as well as in a Substack blog), writing, ‚ÄúProper embryo selection just landed.‚Äù Kirkegaard has in fact supported Anomaly‚Äôs work for years; he‚Äôs posted about him on X and recommended his 2020 book Creating Future People, which he called a ‚Äúbiotech eugenics advocacy book,‚Äù adding: ‚ÄúNaturally, I agree with this stuff!‚Äù



When it comes to traits that Anomaly believes are genetically encoded, intelligence‚Äîwhich he claimed in his talk is about 75% heritable‚Äîis just the tip of the iceberg. He has also spoken about the heritability of empathy, impulse control, violence, passivity, religiosity, and political leanings.



Anomaly concedes there are limitations to the kinds of relative predictions that can be made from a small batch of embryos. But he believes we‚Äôre only at the dawn of what he likes to call the ‚Äúreproductive revolution.‚Äù At his talk, he pointed to a technology currently in development at a handful of startups: in vitro gametogenesis. IVG aims to create sperm or egg cells in a laboratory using adult stem cells, genetically reprogrammed from cells found in a sample of skin or blood. In theory, this process could allow a couple to quickly produce a practically unlimited number of embryos to analyze for preferred traits. Anomaly predicted this technology could be ready to use on humans within eight years.



SELMAN DESIGN




‚ÄúI doubt the FDA will allow it immediately. That‚Äôs what places like Pr√≥spera are for,‚Äù he said, referring to the so-called ‚Äústartup city‚Äù in Honduras, where scientists and entrepreneurs can conduct medical experiments free from the kinds of regulatory oversight they‚Äôd encounter in the US.



‚ÄúYou might have a moral intuition that this is wrong,‚Äù said Anomaly, ‚Äúbut when it‚Äôs discovered that elites are doing it privately ‚Ä¶ the dominoes are going to fall very, very quickly.‚Äù The coming ‚Äúevolutionary arms race,‚Äù he claimed, will ‚Äúchange the moral landscape.‚Äù



He added that some of those elites are his own customers: ‚ÄúI could already name names, but I won‚Äôt do it.‚Äù



After Anomaly‚Äôs talk was over, I spoke with a young photographer who told me he was hoping to pursue a master‚Äôs degree in theology. He came to the event, he told me, to reckon with the ethical implications of playing God. ‚ÄúTechnology is sending us toward an Old-to-New-Testament transition moment, where we have to decide what parts of religion still serve us,‚Äù he said soberly.







Criticisms of polygenic testing tend to fall into two camps: skepticism about the tests‚Äô effectiveness and concerns about their ethics. ‚ÄúOn one hand,‚Äù says Turley from the Social Science Genetic Association Consortium, ‚Äúyou have arguments saying ‚ÄòThis isn‚Äôt going to work anyway, and the reason it‚Äôs bad is because we‚Äôre tricking parents, which would be a problem.‚Äô And on the other hand, they say, ‚ÄòOh, this is going to work so well that it‚Äôs going to lead to enormous inequalities in society.‚Äô It‚Äôs just funny to see. Sometimes these arguments are being made by the same people.‚Äù



One of those people is Sasha Gusev, who runs a quantitative genetics lab at the Dana-Farber Cancer Institute. A vocal critic of PGT-P for embryo selection, he also often engages in online debates with the far-right accounts promoting race science on X.





Gusev is one of many professionals in his field who believe that because of numerous confounding socioeconomic factors‚Äîfor example, childhood nutrition, geography, personal networks, and parenting styles‚Äîthere isn‚Äôt much point in trying to trace outcomes like educational attainment back to genetics, particularly not as a way to prove that there‚Äôs a genetic basis for IQ.



He adds, ‚ÄúI think there‚Äôs a real risk in moving toward a society where you see genetics and ‚Äògenetic endowments‚Äô as the drivers of people‚Äôs behavior and as a ceiling on their outcomes and their capabilities.‚Äù



Gusev thinks there is real promise for this technology in clinical settings among specific adult populations. For adults identified as having high polygenic risk scores for cancer and cardiovascular disease, he argues, a combination of early screening and intervention could be lifesaving. But when it comes to the preimplantation testing currently on the market, he thinks there are significant limitations‚Äîand few regulatory measures or long-term validation methods to check the promises companies are making. He fears that giving these services too much attention could backfire.



‚ÄúThese reckless, overpromised, and oftentimes just straight-up manipulative embryo selection applications are a risk for the credibility and the utility of these clinical tools,‚Äù he says.



Many IVF patients have also had strong reactions to publicity around PGT-P. When the New York Times published an opinion piece about Orchid in the spring, angry parents took to Reddit to rant. One user posted, ‚ÄúFor people who dont [sic] know why other types of testing are necessary or needed this just makes IVF people sound like we want to create ‚Äòperfect‚Äô babies, while we just want (our) healthy babies.‚Äù



Still, others defended the need for a conversation. ‚ÄúWhen could technologies like this change the mission from helping infertile people have healthy babies to eugenics?‚Äù one Redditor posted. ‚ÄúIt‚Äôs a fine line to walk and an important discussion to have.‚Äù



Some PGT-P proponents, like Kirkegaard and Anomaly, have argued that policy decisions should more explicitly account for genetic differences. In a series of blog posts following the 2024 presidential election, under the header ‚ÄúMake science great again,‚Äù Kirkegaard called for ending affirmative action laws, legalizing race-based hiring discrimination, and removing restrictions on data sets like the NIH‚Äôs All of Us biobank that prevent researchers like him from using the data for race science. Anomaly has criticized social welfare policies for putting a finger on the scale to ‚Äúpunish the high-IQ people.‚Äù



Indeed, the notion of genetic determinism has gained some traction among loyalists to President Donald Trump.&nbsp;



In October 2024, Trump himself made a campaign stop on the conservative radio program The Hugh Hewitt Show. He began a rambling answer about immigration and homicide statistics. ‚ÄúA murderer, I believe this, it‚Äôs in their genes. And we got a lot of bad genes in our country right now,‚Äù he told the host.



Gusev believes that while embryo selection won‚Äôt have much impact on individual outcomes, the intellectual framework endorsed by many PGT-P advocates could have dire social consequences.



‚ÄúIf you just think of the differences that we observe in society as being cultural, then you help people out. You give them better schooling, you give them better nutrition and education, and they‚Äôre able to excel,‚Äù he says. ‚ÄúIf you think of these differences as being strongly innate, then you can fool yourself into thinking that there‚Äôs nothing that can be done and people just are what they are at birth.‚Äù



For the time being, there are no plans for longitudinal studies to track actual outcomes for the humans these companies have helped bring into the world. Harden, the behavioral geneticist from UT Austin, suspects that 25 years down the line, adults who were once embryos selected on the basis of polygenic risk scores are ‚Äúgoing to end up with the same question that we all have.‚Äù They will look at their life and wonder, ‚ÄúWhat would‚Äôve had to change for it to be different?‚Äù



Julia Black is a Brooklyn-based features writer and a reporter in residence at Omidyar Network. She has previously worked for Business Insider, Vox, The Information, and Esquire.
‚Ä¢ The problem with Big Tech‚Äôs favorite carbon removal tech
  Sucking carbon pollution out of the atmosphere is becoming a big business‚Äîcompanies are paying top dollar for technologies that can cancel out their own emissions.





Today, nearly 70% of announced carbon removal contracts are for one technology: bioenergy with carbon capture and storage (BECCS). Basically, the idea is to use trees or some other types of biomass for energy, and then capture the emissions when you burn it.



While corporations, including tech giants like Microsoft, are betting big on this technology, there are a few potential problems with BECCS, as my colleague James Temple laid out in a new story. And some of the concerns echo similar problems with other climate technologies we cover, like carbon offsets and alternative jet fuels.



Carbon math can be complicated.



To illustrate one of the biggest issues with BECCS, we need to run through the logic on its carbon accounting. (And while this tech can use many different forms of biomass, let‚Äôs assume we‚Äôre talking about trees.)



When trees grow, they suck up carbon dioxide from the atmosphere. Those trees can be harvested and used for some intended purpose, like making paper. The leftover material, which might otherwise be waste, is then processed and burned for energy.



This cycle is, in theory, carbon neutral. The emissions from burning the biomass are canceled out by what was removed from the atmosphere during plants‚Äô growth. (Assuming those trees are replaced after they‚Äôre harvested.)



So now imagine that carbon-scrubbing equipment is added to the facility that burns the biomass, capturing emissions. If the cycle was logically carbon neutral before, now it‚Äôs carbon negative: On net, emissions are removed from the atmosphere. Sounds great, no notes.&nbsp;



There are a few problems with this math, though. For one, it leaves out the emissions that might be produced while harvesting, transporting, and processing wood. And if projects require clearing land to plant trees or grow crops, that transformation can wind up releasing emissions too.



Issues with carbon math might sound a little familiar if you‚Äôve read any of James‚Äôs reporting on carbon offsets, programs where people pay for others to avoid emissions. In particular, his 2021 investigation with ProPublica‚Äôs Lisa Song laid out how this so-called solution was actually adding millions of tons of carbon dioxide into the atmosphere.



Carbon capture may entrench polluting facilities.



One of the big benefits of BECCS is that it can be added to existing facilities. There‚Äôs less building involved than there might be in something like a facility that vacuums carbon directly out of air. That helps keep costs down, so BECCS is currently much cheaper than direct air capture and other forms of carbon removal.



But keeping legacy equipment running might not be a great thing for emissions or local communities in the long run.



Carbon dioxide is far from the only pollutant spewing out of these facilities. Burning biomass or biofuels can release emissions that harm human health, like particulate matter, sulfur dioxide, and carbon monoxide. Carbon capture equipment might trap some of these pollutants, like sulfur dioxide, but not all.



Assuming that waste material wouldn‚Äôt be used for something else might not be right.



It sounds great to use waste, but there‚Äôs a major asterisk lurking here, as James lays out in the story:



But the critical question that emerges with waste is: Would it otherwise have been burned or allowed to decompose, or might some of it have been used in some other way that kept the carbon out of the atmosphere?&nbsp;



Biomass can be used for other things, like making plastic, building material, or even soil additives that can help crops get more nutrients. So the assumption that it‚Äôs BECCS or nothing is flawed.





Moreover, a weird thing happens when you start making waste valuable: There‚Äôs an incentive to produce more of it. Some experts are concerned that companies could wind up trimming more trees or clearing more forests than what‚Äôs needed to make more material for BECCS.



These waste issues remind me of conversations around sustainable aviation fuels. These alternative fuels can be made from a huge range of materials, including crop waste or even used cooking oil. But as demand for these clean fuels has ballooned, things have gotten a little wonky‚Äîthere are even some reports of fraud, where scammers try to pass off newly made oil from crops as used cooking oil.



BECCS is a potentially useful technology, but like many things in climate tech, it can quickly get complicated.&nbsp;



James has been reporting on carbon offsets and carbon removal for years. As he put it to me this week when we were chatting about this story: ‚ÄúJust cut emissions and stop messing around.‚Äù



This article is from The Spark, MIT Technology Review‚Äôs weekly climate newsletter. To receive it in your inbox every Wednesday, sign up here.
‚Ä¢ The Download: Big Tech‚Äôs carbon removals plans, and the next wave of nuclear reactors
  This is today&#8217;s edition of¬†The Download,¬†our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



Big Tech‚Äôs big bet on a controversial carbon removal tactic



Microsoft, JP MorganChase, and a tech company consortium that includes Alphabet, Meta, Shopify, and Stripe have all recently struck multimillion-dollar deals to pay paper mill owners to capture at least hundreds of thousands of tons of this greenhouse gas by installing carbon scrubbing equipment in their facilities.The captured carbon dioxide will then be piped down into saline aquifers more than a mile underground, where it should be sequestered permanently.Big Tech is suddenly betting big on this form of carbon removal, known as bioenergy with carbon capture and storage, or BECCS. But experts have raised a number of concerns. Read the full story.



‚ÄîJames Temple







2025 climate tech companies to watch: Kairos Power and its next-generation nuclear reactors



Like many new nuclear startups, Kairos promises a path to reliable, 24/7 decarbonized power. Unlike most, it already has prototypes under construction and permits for several reactors.The company uses molten salt to cool its reactions and transfer heat, rather than the high-pressure water that‚Äôs used in existing fission reactors. It hopes its technology will enable commercial reactors that are cost-competitive with natural gas plants and boast safer operation than conventional reactors, even in the event of complete power loss. Read the full story.



‚ÄîMark Harris



Kairos Power is one of our 10 climate tech companies to watch‚Äîour annual list of some of the most promising climate tech firms on the planet. Check out the rest of the list here.







MIT Technology Review Narrated: Inside the strange limbo facing millions of IVF embryos



Millions of embryos created through IVF sit frozen in time, stored in tanks around the world. The number is only growing thanks to advances in technology, the rising popularity of IVF, and improvements in its success rates.At a basic level, an embryo is simply a tiny ball of a hundred or so cells. But unlike other types of body tissue, it holds the potential for life. Many argue that this endows embryos with a special moral status, one that requires special protections.The problem is that no one can really agree on what that status is. While these embryos persist in suspended animation, patients, clinicians, embryologists, and legislators must grapple with the essential question of what we should do with them. What do these embryos mean to us? Who should be responsible for them?



This is our latest story to be turned into a MIT Technology Review Narrated podcast, which we‚Äôre publishing each week on Spotify and Apple Podcasts. Just navigate to MIT Technology Review Narrated on either platform, and follow us to get all our new content as it‚Äôs released.







The must-reads



I‚Äôve combed the internet to find you today‚Äôs most fun/important/scary/fascinating stories about technology.



1 ChatGPT will start talking dirty to verified adults¬†The chatbot is getting a new erotica function as part of OpenAI‚Äôs bid to ‚Äúsafely relax‚Äù its restrictions. (The Verge)+ The company has created its own wellness council to inform its decisions. (Ars Technica)+ It‚Äôs surprisingly easy to stumble into a relationship with an AI chatbot. (MIT Technology Review)



2 A secret surveillance empire tracked thousands of people across the worldThe European-led First Wap has operated covertly for more than two decades. (Mother Jones)+ The group ran at least 10 scam compounds across the country. (Wired $)+ Inside a romance scam compound‚Äîand how people get tricked into being there. (MIT Technology Review)



3 YouTube ran Israel-funded ads claiming there was food in famine-struck GazaAnd allowed them to remain online even after complaints from multiple government authorities. (WP $)+ Companies have denied they‚Äôre involved in rebuilding Gaza. (Wired $)



4 Instagram wants to become a more teen-friendly spaceIt‚Äôs bringing in new age-gating measures inspired by the PG-13 movie rating. (NBC News)+ The policy will also extend to its chatbots. (NYT $)



5 A massive Cambodia-based pig butchering scheme has been foiledIt‚Äôs the biggest forfeiture action the US Department of Justice has ever pursued. (CNBC)



6 Waymo‚Äôs driverless taxis are coming to LondonFrom next year, it says pedestrians will be able to hail its robotaxis. (WSJ $)



7 Black patients were failed by a race-based medical calculationIt delayed their access to life-saving kidney transplants. (The Markup)+ A woman in the US is the third person to receive a gene-edited pig kidney. (MIT Technology Review)8 AI flood forecasting is helping farmers across the worldNonprofits are using it to deliver early aid. (Rest of World)



9 A man with paralysis can feel objects through another person&#8217;s handThanks to a new brain implant. (New Scientist $)+ Meet the other companies developing brain-computer interfaces. (MIT Technology Review)



10 Tech internships are alive and well¬†Despite all the AI angst. (Insider $)







Quote of the day



‚ÄúYou made ChatGPT ‚Äúpretty restrictive‚Äù? Really. Is that why it has been recommending kids harm and kill themselves?‚Äù



‚ÄîJosh Hawley, US Senator for Missouri, reacts to the news OpenAI is planning to loosen its restrictions in a post on X.







One more thing







Why we should thank pigeons for our AI breakthroughsPeople looking for precursors to artificial intelligence often point to science fiction by authors like Isaac Asimov or thought experiments like the Turing test. But an equally important, if surprising and less appreciated, forerunner is American psychologist B.F. Skinner‚Äôs research with pigeons in the middle of the 20th century.Skinner believed that association‚Äîlearning, through trial and error, to link an action with a punishment or reward‚Äîwas the building block of every behavior, not just in pigeons but in all living organisms, including human beings.His ‚Äúbehaviorist‚Äù theories fell out of favor with psychologists and animal researchers in the 1960s but were taken up by computer scientists who eventually provided the foundation for many of the artificial-intelligence tools from leading firms like Google and OpenAI. Read the full story.



‚ÄîBen Crair







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)



+ I love the sound of Grateful Fishing TV‚Äîstarring two fishermen who just love hanging out and frying some fish. Truly wholesome stuff (thanks to Chino Moreno via Perfectly Imperfect for the recommendation!)+ Rest in power D‚ÄôAngelo, your timeless tunes will live on.+ If you‚Äôre into stress-watches, this list is full of anxiety-inducing classics.+ One of the world‚Äôs longest dinosaur superhighways has been uncovered in a sleepy part of England.
‚Ä¢ Future-proofing business capabilities with AI technologies
  From oil and gas to retail, logistics to law, AI is being deployed in critical workflows . Future of enterprise AI will be defined by how effectively organizations can marry innovation with scale, security, and strategy . Concerns about privacy, security and the accuracy of LLMs remain pressing . Long gone are the days of incremental steps forward, the impact of AI expertise come together, and the impact is transformative .

üîí Cybersecurity & Privacy
‚Ä¢ Patch Tuesday, October 2025 ‚ÄòEnd of 10‚Äô Edition
  Microsoft today released software updates to plug a whopping 172 security holes in its Windows operating systems, including at least two vulnerabilities that are already being actively exploited. October&#8217;s Patch Tuesday also marks the final month that Microsoft will ship security updates for Windows 10 systems. If you&#8217;re running a Windows 10 PC and you&#8217;re unable or unwilling to migrate to Windows 11, read on for other options.

The first zero-day bug addressed this month (CVE-2025-24990) involves a third-party modem driver called Agere Modem that&#8217;s been bundled with Windows for the past two decades. Microsoft responded to active attacks on this flaw by completely removing the vulnerable driver from Windows.
The other zero-day is CVE-2025-59230, an elevation of privilege vulnerability in Windows Remote Access Connection Manager (also known as RasMan), a service used to manage remote network connections through virtual private networks (VPNs) and dial-up networks.
&#8220;While RasMan is a frequent flyer on Patch Tuesday, appearing more than 20 times since January 2022, this is the first time we&#8217;ve seen it exploited in the wild as a zero day,&#8221; said Satnam Narang, senior staff research engineer at Tenable.
Narang notes that Microsoft Office users should also take note of CVE-2025-59227 and CVE-2025-59234, a pair of remote code execution bugs that take advantage of &#8220;Preview Pane,‚Äù meaning that the target doesn‚Äôt even need to open the file for exploitation to occur. To execute these flaws, an attacker would social engineer a target into previewing an email with a malicious Microsoft Office document.
Speaking of Office, Microsoft quietly announced this week that Microsoft Word will now automatically save documents to OneDrive, Microsoft&#8217;s cloud platform. Users who are uncomfortable saving all of their documents to Microsoft&#8217;s cloud can change this in Word&#8217;s settings; ZDNet has a useful how-to on disabling this feature.
Kev Breen, senior director of threat research at Immersive, called attention to CVE-2025-59287, a critical remote code execution bug in the Windows Server Update Service¬† (WSUS) &#8212; the very same Windows service responsible for downloading security patches for Windows Server versions. Microsoft says there are no signs this weakness is being exploited yet. But with a threat score of 9.8 out of possible 10 and marked &#8220;exploitation more likely,&#8221; CVE-2025-59287 can be exploited without authentication and is an easy &#8220;patch now&#8221; candidate.
&#8220;Microsoft provides limited information, stating that an unauthenticated attacker with network access can send untrusted data to the WSUS server, resulting in deserialization and code execution,&#8221; Breen wrote. &#8220;As WSUS is a trusted Windows service that is designed to update privileged files across the file system, an attacker would have free rein over the operating system and could potentially bypass some EDR detections that ignore or exclude the WSUS service.&#8221;
For more on other fixes from Redmond today, check out the SANS Internet Storm Center monthly roundup, which indexes all of the updates by severity and urgency.
Windows 10 isn&#8217;t the only Microsoft OS that is reaching end-of-life today;¬†Exchange Server 2016, Exchange Server 2019, Skype for Business 2016, Windows 11 IoT Enterprise Version 22H2, and Outlook 2016 are some of the other products that Microsoft is sunsetting today.

If you&#8217;re running any Windows 10 systems, you&#8217;ve probably already determined whether your PC meets the technical hardware specs recommended for the Windows 11 OS. If you&#8217;re reluctant or unable to migrate a Windows 10 system to Windows 11, there are alternatives to simply continuing to use Windows 10 without ongoing security updates.
One option is to pay for another year&#8217;s worth of security updates through Microsoft&#8217;s Extended Security Updates (ESU) program. The cost is just $30 if you don&#8217;t have a Microsoft account, and apparently free if you register the PC to a Microsoft account. This video breakdown from Ask Your Computer Guy does a good job of walking Windows 10 users through this process. Microsoft emphasizes that ESU enrollment does not provide other types of fixes, feature improvements or product enhancements. It also does not come with technical support.
If your Windows 10 system is associated with a Microsoft account and signed in when you visit Windows Update, you should see an option to enroll in extended updates. Image: https://www.youtube.com/watch?v=SZH7MlvOoPM
Windows 10 users also have the option of installing some flavor of Linux instead. Anyone seriously considering this option should check out the website endof10.org, which includes a plethora of tips and a DIY installation guide.
Linux Mint is a great option for Linux newbies. Like most modern Linux versions, Mint will run on anything with a 64-bit CPU that has at least 2GB of memory, although 4GB is recommended. In other words, it will run on almost any computer produced in the last decade.
Linux Mint also is likely to be the most intuitive interface for regular Windows users, and it is largely configurable without any fuss at the text-only command-line prompt. Mint and other flavors of Linux come with LibreOffice, which is an open source suite of tools that includes applications similar to Microsoft Office, and it can open, edit and save documents as Microsoft Office files.
If you‚Äôd prefer to give Linux a test drive before installing it on a Windows PC, you can always just download it to a removable USB drive. From there, reboot the computer (with the removable drive plugged in) and select the option at startup to run the operating system from the external USB drive. If you don‚Äôt see an option for that after restarting, try restarting again and hitting the F8 button, which should open a list of bootable drives.¬†Here‚Äôs a fairly thorough tutorial¬†that walks through exactly how to do all this.
And if this is your first time trying out Linux, relax and have fun: The nice thing about a ‚Äúlive‚Äù version of Linux (as it‚Äôs called when the operating system is run from a removable drive such as a CD or a USB stick) is that none of your changes persist after a reboot. Even if you somehow manage to break something, a restart will return the system back to its original state.
As ever, if you experience any difficulties during or after applying this month&#8217;s batch of patches, please leave a note about it in the comments below.

üéì University AI
No updates.

üè¢ Corporate AI
‚Ä¢ Transforming enterprise operations: Four high-impact use cases with Amazon Nova
  Since the launch of Amazon Nova at AWS re:Invent 2024, we have seen adoption trends across industries, with notable gains in operational efficiency, compliance, and customer satisfaction. With its capabilities in secure, multimodal AI and domain customization, Nova is enhancing workflows and enabling cost efficiencies across core use cases. 
In this post, we share four high-impact, widely adopted use cases built with Nova in Amazon Bedrock, supported by real-world customers deployments, offerings available from AWS partners, and experiences. These examples are ideal for organizations researching their own AI adoption strategies and use cases across industries. 
Customer service 
Traditional chatbots often frustrate users with scripted, inflexible responses that fail to understand context or intent. For enterprises, these are missed opportunities to resolve issues quickly, lower support costs, and drive customer loyalty. AI-powered applications can understand natural language, adapt to individual customer needs, and integrate with backend systems in real time. Organizations are transforming support from a cost center into a strategic driver of satisfaction and retention. These are often high-volume and interactive scenarios, so the balance of cost, speed, and intelligence is critical. 
Customer service applications built with Nova in Amazon Bedrock can seamlessly integrate with business data stored with AWS, and offer the security, privacy, and reliability for production use in enterprise environments. 
 
 Infosys, a leading global IT services and consulting organization, developed Infosys Event AI for real-time transcription, multilingual translation, and intelligent summarization of live event content. Infosys Event AI is built with Amazon Nova Pro in Amazon Bedrock. During a recent event in Bangalore, the AI assistant handled around 230 users per minute and was queried an average of 57 times per minute, generating more than 9,000 session summaries. This solution enhanced knowledge retention, engagement, and inclusivity by making event insights instantly accessible in multiple languages and formats for hearing-impaired and remote participants. By transforming event content into a persistent, searchable multilingual knowledge asset, Infosys Event AI accelerates learning and collaboration. 
 Fortinet, an AWS Partner and cybersecurity company, uses Amazon Nova Micro to power its AI support assistant, delivering significant performance improvements at a fraction of the cost. By switching to Nova Micro in Amazon Bedrock, Fortinet achieved an 85 times reduction in inference costs, dramatically lowering TCO while maintaining rapid response times. The assistant now helps users quickly navigate complex documentation across more than 60 products, improving support efficiency and elevating customer satisfaction. 
 Amazon Customer Service uses Nova with its AI-driven issue resolution system. The system is a two-step approach combining intent detection and issue resolution. Amazon Customer Service customized Nova Micro, resulting in 76.9% accuracy for in-domain issues and 69.2% in generalization testing, surpassing current baselines by 5.4% and 7.3%, respectively. Additionally, Nova Lite is used for tool selection, achieving 86.1% accuracy and 4.8% improvement over existing systems. 
 AWS Summit New York City 2025 was attended by 18,000 participants, featuring the AI assistant Diana for customer service developed with Nova Sonic. By dialing a phone number, the Sonic-powered voice assistant answered hundreds of queries about the event, including session details, location, and FAQs. 
 
Search 
Large enterprises face slow, siloed, and inefficient search across vast stores of structured and unstructured data, costing time, productivity, and customer responsiveness. By adopting AI-powered, multimodal search that understands natural language and enforces secure access, organizations can deliver instant, relevant answers from documents, images, and technical files. This accelerates decision-making, shortens deal cycles, improves customer satisfaction, and reduces the cost of knowledge discovery at scale. Search applications increasingly rely on a mix of information across modalities, including text, documents, images, and video. 
Nova is among the fastest and most cost-effective multimodal models, offering vision fine-tuning capabilities. Nova also integrates with broader Amazon models including Amazon Titan Multimodal Embeddings and data services including Amazon OpenSearch Service for more robust search capabilities and performance. 
 
 Siemens faced growing performance bottlenecks as its massive datasets strained traditional search systems, slowing retrieval speeds and impacting productivity across its global operations. To address this, Siemens integrated Amazon Nova, achieving a threefold boost in search performance that dramatically accelerated data retrieval and improved workflow efficiency. Amazon Nova delivers high-speed, scalable search capabilities, and Siemens‚Äôs implementation facilitates seamless integration with existing systems, maintaining business continuity with minimal disruption. This enhanced user experience and positioned Siemens to handle future data growth with ease, supported by continuous performance monitoring and tight infrastructure alignment. 
 CBRE&nbsp;Global Pulse System (GPS)‚Äîbuilt with Amazon Nova Pro in Amazon Bedrock and OpenSearch Service‚Äîtransforms property search across thousands of users worldwide. Built in partnership with AWS Professional Services and GenAI Specialists, GPS replaces slow, fragmented legacy systems with an AI-driven, multimodal search platform capable of handling complex queries, massive PDFs, and strict permission controls. Key results include 75% faster document ingestion, 70% lower database latency, 87% faster keyword searches, and 51% faster natural language queries. When fully deployed to over 6,000 users later in 2025, GPS is projected to save over 98,000 employee workdays annually, unlocking $320,000 ARR and significant operational efficiency. By shifting from Anthropic‚Äôs Claude Sonnet to Nova Pro and Anthropic‚Äôs Claude Haiku 3, CBRE also cut AI inference costs by 3.5 times and 12 times, respectively, without sacrificing accuracy. 
 
Video understanding and analysis 
Organizations are adopting video understanding applications to drive business value across multiple fronts, including customer behavior analysis, traffic patterns, and manufacturing quality control. Security and safety benefits are realized through real-time threat detection and workplace safety monitoring, and customer experience is enhanced through personalized content recommendations and improved content searchability. Organizations gain competitive advantage through data-driven decision-making and innovation in service delivery, while reducing costs by minimizing manual review processes and decreasing security incidents. This comprehensive approach to video analysis helps companies extract insights from their video data, ultimately leading to improved operations, better decision-making, and enhanced customer experiences. As developers build, iterate, and evolve these applications, there is a growing demand to natively understand video as opposed to dealing with the overhead of frames, time stamps, and synchronization. 
Amazon Nova models can analyze, classify, and summarize information in the video based on provided instructions. Applications built with Nova understanding models in Amazon Bedrock offer comprehensive analysis of multiple video formats through flexible input methods, with the ability to analyze, classify, and summarize video content while handling files up to 1 GB through Amazon Simple Storage Service (Amazon S3) integration. 
 
 Bitcentral partnered with Caylent to transform how archived content is discovered, accessed, and reused. Using Nova Pro in Amazon Bedrock, Caylent deployed a solution that aligned with the needs of journalists, producers, and broadcasters across more than 1,600 client sites. By embedding semantic video search, contextual metadata generation, and AI-powered content analysis into its workflows, Bitcentral redefined how archived footage is indexed, discovered, and reused. Journalists and producers can now surface high-value content in real time and unlock new revenue streams. 
 Loka, an AWS Premier Partner, built a video surveillance offering to automatically identify and classify millions of visual events in video footage. This system effectively distinguishes between routine events and critical incidents, helping filter out non-essential activities and alerts. The solution proved highly successful, reducing irrelevant alerts by 55% while maintaining a threat detection rate above 97%. By implementing this automated filtering system, Loka doubled video monitoring efficiency for their client. The tool, built on Amazon Bedrock using Amazon Nova Pro, significantly reduced the workload for human operators while improving overall threat detection capabilities. 
 Accenture Spotlight can analyze long-form videos and automatically generate personalized short-form clips and highlights, which are particularly useful for sports content like soccer, Formula 1, and rugby. Spotlight is capable of matching content to specific audience demographics and can process real-time CCTV footage in retail settings to create personalized offers. The system is built with Amazon Nova in Amazon Bedrock and operates through three specialized super agents working under a central orchestrator. Spotlight can process videos in minutes rather than the traditional hours or days, while achieving cost savings that are 10 times better than conventional methods. The solution is versatile enough to be used across different industries, from media and entertainment to retail, while maintaining high quality standards and brand alignment through its human-in-the-loop quality assurance option. 
 
Creative content generation 
Organizations are seeking ways to revolutionize creative content generation including stock imagery, marketing campaign assets, and product visualizations. It is often slowed down by fragmented workflows, high production costs, and the need to continuously balance scale with personalization. Marketing teams struggle to keep up with the demand for fresh, high-quality assets across multiple channels, while creative fatigue and long lead times limit their agility. 
Amazon Nova addresses these challenges with Nova Canvas and Nova Reel: high-quality creative models that transform text and image inputs into professional-grade images and videos. Nova creative models are designed to deliver customizable visual content with control features, making creative content generation accessible and efficient for media, entertainment, retail, marketing, and advertising industries. 
 
 Dentsu is reimagining how ads come to life with Amazon Nova creative generation models. What used to take weeks of brainstorming, filming, and editing now happens in days. Their creative teams can sketch out an idea in plain language and watch it turn into polished videos and custom images, ready for markets across the globe in over 200 languages. Built-in safeguards like moderation, watermarking, and IP indemnity mean every piece stays brand safe. For Co-op, Dentsu went a step further‚Äîpairing Nova with Amazon Ads to design custom audience profiles that delivered a +4-point lift in brand preference among 25‚Äì34-year-olds and a +5-point lift in favorability among affluent shoppers. 
 Quantiphi, an AWS Premier Global Consulting Partner, developed Qreator, a generative AI-powered marketing content creation service built on AWS. Their service helps marketers create content through natural language prompts while maintaining brand consistency and cross-channel adaptability. With Qreator, business can achieve an approximate 30% reduction in content creation time and get to market approximately 40% faster, automating what was a manual process, and improving consistency across formats and channels. 
 The Fragrance Lab is a unique AWS activation that was showcased at the Cannes Lions International Festival of Creativity. It demonstrates how to build personalized products and campaign assets using Amazon Nova foundation models in Amazon Bedrock. Although our activation at Cannes Lions focused on personalized fragrance development and ad campaign creation, the underlying architecture and methodology can be adapted across diverse categories, such as fashion, food, and beverage, opening endless possibilities for customized customer experiences. The Fragrance Lab activation won two International Business Awards: Gold for Exhibition Event Experience and Silver for Experiential Event. 
 
Conclusion 
The four use cases presented in this post demonstrate the utility of Amazon Nova across industries and applications. From Infosys‚Äôs Event AI improving accessibility and engagement, to CBRE‚Äôs revolutionary property search system, to Loka‚Äôs intelligent video surveillance, and Dentsu‚Äôs creative content generation, each implementation showcases significant, measurable improvements in efficiency, cost reduction, and customer satisfaction. 
Organizations using Amazon Nova are achieving tangible business outcomes through evidence-based adoption strategies. By partnering with Amazon and AWS Partners, organizations are accelerating their AI transformation while maintaining strong foundations in security, compliance, and privacy-by-design principles. 
To get started building with Nova, visit the Amazon Nova user guide or visit the AWS console. 
 
About the Authors 
Abhinav Bhargava is a Sr Product Marketing Manager at AWS on the Amazon Nova team, where he focuses on scaling generative AI adoption through customer-centric solutions. With a background in design and sustainability, he brings a unique perspective to connecting technology and creativity to drive enterprise innovation. Based in Seattle, Abhinav enjoys playing volleyball, traveling, and learning about new cultures. 
Raechel Frick is a Sr Product Marketing Manager at AWS. With over 20 years of experience in the tech industry, she brings a customer-first approach and growth mindset to building integrated marketing programs. Based in the greater Seattle area, Raechel balances her professional life with being a soccer mom and after-school carpool manager, demonstrating her ability to excel both in the corporate world and family life.
‚Ä¢ Building smarter AI agents: AgentCore long-term memory deep dive
  Building AI agents that remember user interactions requires more than just storing raw conversations. While Amazon Bedrock AgentCore short-term memory captures immediate context, the real challenge lies in transforming these interactions into persistent, actionable knowledge that spans across sessions. This is the information that transforms fleeting interactions into meaningful, continuous relationships between users and AI agents. In this post, we‚Äôre pulling back the curtain on how the Amazon Bedrock AgentCore Memory long-term memory system works. 
If you‚Äôre new to AgentCore Memory, we recommend reading our introductory blog post first:&nbsp;Amazon Bedrock AgentCore Memory: Building context-aware agents. In brief, AgentCore Memory is a fully managed service that enables developers to build context-aware AI agents by providing both short-term working memory and long-term intelligent memory capabilities. 
The challenge of persistent memory 
When humans interact, we don‚Äôt just remember exact conversations‚Äîwe extract meaning, identify patterns, and build understanding over time. Teaching AI agents to respond the same requires solving several complex challenges: 
 
 Agent memory systems must distinguish between meaningful insights and routine chatter, determining which utterances deserve long-term storage versus temporary processing. A user saying ‚ÄúI‚Äôm vegetarian‚Äù should be remembered, but ‚Äúhmm, let me think‚Äù should not. 
 Memory systems need to recognize related information across time and merge it without creating duplicates or contradictions. When a user mentions they‚Äôre allergic to shellfish in January and mentions ‚Äúcan‚Äôt eat shrimp‚Äù in March, these needs to be recognized as related facts and consolidated with existing knowledge without creating duplicates or contradictions. 
 Memories must be processed in order of temporal context. Preferences that change over time (for example, the user loved spicy chicken in a restaurant last year, but today, they prefer mild flavors) require careful handling to make sure the most recent preference is respected while maintaining historical context. 
 As memory stores grow to contain thousands or millions of records, finding relevant memories quickly becomes a significant challenge. The system must balance comprehensive memory retention with efficient retrieval. 
 
Solving these problems requires sophisticated extraction, consolidation, and retrieval mechanisms that go beyond simple storage.&nbsp;Amazon Bedrock AgentCore Memory tackles these complexities by implementing a research-backed long-term memory pipeline that mirrors human cognitive processes while maintaining the precision and scale required for enterprise applications. 
How AgentCore long-term memory works 
When the agentic application sends conversational events to AgentCore Memory, it initiates a pipeline to transform raw conversational data into structured, searchable knowledge through a multi-stage process. Let‚Äôs explore each component of this system.&nbsp; 
1. Memory extraction: From conversation to insights 
When new events are stored in short-term memory, an asynchronous extraction process analyzes the conversational content to identify meaningful information. This process leverages large language models (LLMs) to understand context and extract relevant details that should be preserved in long-term memory.&nbsp;The extraction engine processes incoming messages alongside prior context to generate memory records in a predefined schema. As a developer, you can configure one or more Memory strategies to extract only the information types relevant to your application needs. The extraction process supports three built-in memory strategies: 
 
 Semantic memory: Extracts facts and knowledge. Example: 
   
   "The customer's company has 500 employees across Seattle, Austin, and Boston" 
    
 User preferences: Captures explicit and implicit preferences given context. Example: 
   
   {‚Äúpreference‚Äù: "Prefers Python for development work", ‚Äúcategories‚Äù: [‚Äúprogramming‚Äù, ‚Äùcode-style‚Äù], ‚Äúcontext‚Äù: ‚ÄúUser wants to write a student enrollment website‚Äù} 
    
 Summary memory: Creates running narratives of conversations under different topics scoped to sessions and preserves the key information in a structured XML format. Example: 
   
   &lt;topic=‚ÄúMaterial-UI TextareaAutosize inputRef Warning Fix Implementation‚Äù&gt; A developer successfully implemented a fix for the issue in Material-UI where the TextareaAutosize component gives a "Does not recognize the 'inputRef' prop" warning when provided to OutlinedInput through the 'inputComponent' prop. &lt;/topic&gt; 
    
 
For each strategy, the system processes events with timestamps for&nbsp;maintaining the continuity of context and conflict resolution. Multiple memories can be extracted from a single event, and each memory strategy operates independently, allowing parallel processing. 
2. Memory consolidation 
Rather than simply adding new memories to existing storage, the system performs intelligent consolidation to merge related information, resolve conflicts, and minimize redundancies. This consolidation makes sure the agent‚Äôs memory remains coherent and up to date as new information arrives. 
The consolidation process works as follows: 
 
 Retrieval: For each newly extracted memory, the system retrieves the top most semantically similar existing memories from the same namespace and strategy. 
 Intelligent processing: The new memory and retrieved memories are sent to the LLM with a consolidation prompt. The prompt preserves the semantic context, thus avoiding unnecessary updates (for example, ‚Äúloves pizza‚Äù and ‚Äúlikes pizza‚Äù are considered essentially the same information). Preserving these core principles, the prompt is designed to handle various scenarios: 
   
   You are an expert in managing data. Your job is to manage memory store.&nbsp;
Whenever a new input is given, your job is to decide which operation to perform.

Here is the new input text.
TEXT: {query}

Here is the relevant and existing memories
MEMORY: {memory}

You can call multiple tools to manage the memory stores... 
   Based on this prompt, the LLM determines the appropriate action: 
   
   ADD: When the new information is distinct from existing memories 
   UPDATE: Enhance existing memories when the new knowledge complements or updates the existing memories 
   NO-OP: When the information is redundant 
    
 Vector store updates: The system applies the determined actions, maintaining an immutable audit trail by marking the outdated memories as INVALID instead of instantly deleting them. 
 
This approach makes sure that contradictory information is resolved (prioritizing recent information), duplicates are minimized, and related memories are appropriately merged. 
Handling edge cases 
The consolidation process gracefully handles several challenging scenarios: 
 
 Out-of-order events: Although the system processes events in temporal order within sessions, it can handle late-arriving events through careful timestamp tracking and consolidation logic. 
 Conflicting information: When new information contradicts existing memories, the system prioritizes recency while maintaining a record of previous states: 
   
   Existing: "Customer budget is \$500"
New: "Customer mentioned budget increased to \$750"
Result: New active memory with \$750, previous memory marked inactive 
    
 Memory failures: If consolidation fails for one memory, it doesn‚Äôt impact others. The system uses exponential backoff and retry mechanisms to handle transient failures. If consolidation ultimately fails, the memory is added to the system to help prevent potential loss of information. 
 
Advanced custom memory strategy configurations 
While built-in memory strategies cover common use cases, AgentCore Memory recognizes that different domains require tailored approaches for memory extraction and consolidation. The system supports built-in strategies with overrides for custom prompts that extend the built-in extraction and consolidation logic, letting teams adapt memory handling to their specific requirements. To maintain system compatibility and focus on criteria and logic rather than output formats, custom prompts help developers customize what information gets extracted or filtered out, how memories should be consolidated, and how to resolve conflicts between contradictory information. 
AgentCore Memory also supports custom model selection for memory extraction and consolidation. This flexibility helps developers balance accuracy and latency based on their specific needs. You can define them via the APIs when you create the memory_resource as a strategy override or via the console (as shown below in the console screenshot). 
 
Apart from override functionality, we also offer self-managed strategies that provide complete control over your memory processing pipeline. With self-managed strategies, you can implement custom extraction and consolidation algorithms using any models or prompts while leveraging AgentCore Memory for storage and retrieval. Also, using the Batch APIs, you can directly ingest extracted records into AgentCore Memory while maintaining full ownership of the processing logic. 
Performance characteristics 
We evaluated our built-in memory strategy across three public benchmarking datasets to assess different aspects of long-term conversational memory: 
 
 LoCoMo: Multi-session conversations generated through a machine-human pipeline with persona-based interactions and temporal event graphs. Tests long-term memory capabilities across realistic conversation patterns. 
 LongMemEval: Evaluates memory retention in long conversations across multiple sessions and extended time periods. We randomly sampled 200 QA pairs for evaluation efficiency. 
 PrefEval: Tests preference memory across 20 topics using 21-session instances to evaluate the system‚Äôs ability to remember and consistently apply user preferences over time. 
 PolyBench-QA: A question-answering dataset containing 807 Question Answer (QA) pairs across 80 trajectories, collected from a coding agent solving tasks in PolyBench. 
 
We use two standard metrics: correctness and compression rate. LLM-based correctness evaluates whether the system can correctly recall and use stored information when needed. Compression rate is defined as output memory token count / full context token count, and evaluates how effectively the memory system stores information. Higher compression rates indicate the system maintains essential information while reducing storage overhead. This compression rate directly translates to faster inference speeds and lower token consumption‚Äìthe most critical consideration for deploying agents at scale because it enables more efficient processing of large conversational histories and reduces operational costs. 
 
  
   
   Memory Type 
   Dataset 
   Correctness 
   Compression Rate 
   
   
   RAG baseline (full conversation history) 
   LoCoMo 
   77.73% 
   0% 
   
   
   LongMemEval-S 
   75.2% 
   0% 
   
   
   PrefEval 
   51% 
   0% 
   
   
   Semantic Memory 
   LoCoMo 
   70.58% 
   89% 
   
   
   LongMemEval-S 
   73.60% 
   94% 
   
   
   Preference Memory 
   PrefEval 
   79% 
   68% 
   
   
   Summarization 
   PolyBench-QA 
   83.02% 
   95% 
   
  
 
The retrieval-augmented-generation (RAG) baseline performs well on factual QA tasks due to complete conversation history access, but struggles with preference inference. The memory system achieves strong practical trade-offs: though information compression leads to slightly lower correctness on some factual tasks, it provides 89-95% compression rates for scalable deployment, maintaining bounded context sizes, and performs effectively at their specialized use cases. 
For more complex tasks requiring inference (understanding user preferences or behavioral patterns), memory demonstrates clear advantages in both performance accuracy and storage efficiency‚Äîthe extracted insights are more valuable than raw conversational data for these use cases. 
Beyond accuracy metrics, AgentCore Memory delivers the performance characteristics necessary for production deployment. 
 
 Extraction and consolidation operations complete within 20-40 seconds for standard conversations after the extraction is triggered. 
 Semantic search retrieval (retrieve_memory_records API) returns results in approximately 200 milliseconds. 
 Parallel processing architecture enables multiple memory strategies to process independently; thus, different memory types can be processed simultaneously without blocking each other. 
 
These latency characteristics, combined with the high compression rates, enable the system to maintain responsive user experiences while managing extensive conversational histories efficiently across large-scale deployments. 
Best practices for long-term memory 
To maximize the effectiveness of long-term memory in your agents: 
 
 Choose the right memory strategies: Select built-in strategies that align with your use case or create custom strategies for domain-specific needs.&nbsp;Semantic memory captures factual knowledge, preference memory tailors towards&nbsp;individual preference, and&nbsp;summarization memory distills complex information for better context management. For example, a customer support agent might use semantic memory to capture customer transaction history and past issues, while summarization memory creates short narratives of current support conversations and troubleshooting workflows across different topics. 
 Design meaningful namespaces: Structure your namespaces to reflect your application‚Äôs hierarchy. This also enables precise memory isolation and efficient retrieval. For example, use customer-support/user/john-doe for individual agent memories and customer-support/shared/product-knowledge for team-wide information. 
 Monitor consolidation patterns: Regularly review what memories are being created (using list_memories or retrieve_memory_records API), updated, or skipped. This helps refine your extraction strategies and helps the system capture relevant information that‚Äôs better fitted to your use case. 
 Plan for async processing: Remember that long-term memory extraction is asynchronous. Design your application to handle the delay between event ingestion and memory availability. Consider using short-term memory for immediate retrieval needs while long-term memories are being processed and consolidated in the background. You might also want to implement fallback mechanisms or loading states to manage user expectations during processing delays. 
 
Conclusion 
The Amazon Bedrock AgentCore Memory long-term memory system represents a significant advancement in building AI agents. By combining sophisticated extraction algorithms, intelligent consolidation processes, and immutable storage designs, it provides a robust foundation for agents that learn, adapt, and improve over time. 
The science behind this system, from research-backed prompts to innovative consolidation workflow, makes sure that your agents don‚Äôt just remember, but understand. This transforms one-time interactions into continuous learning experiences, creating AI agents that become more helpful and personalized with every conversation. 
Resources: ‚Äì AgentCore Memory Docs ‚Äì AgentCore Memory code samples ‚Äì Getting started with AgentCore ‚Äì Workshop 
 
About the authors 
Akarsha Sehwag is a Generative AI Data Scientist for Amazon Bedrock AgentCore GTM team. With over six years of expertise in AI/ML, she has built production-ready enterprise solutions across diverse customer segments in Generative AI, Deep Learning and Computer Vision domains. Outside of work, she likes to hike, bike or play Badminton. 
Jiarong Jiang is a Principal Applied Scientist at AWS, driving innovations in Retrieval-Augmented Generation (RAG) and agent memory systems to improve the accuracy and intelligence of enterprise AI. She‚Äôs passionate about enabling customers to build context-aware, reasoning-driven applications that leverage their own data effectively. 
Jay Lopez-Braus is a Senior Technical Product Manager at AWS. He has over ten years of product management experience. In his free time, he enjoys all things outdoors. 
Dani Mitchell is a Generative AI Specialist Solutions Architect at Amazon Web Services (AWS). He is focused on helping accelerate enterprises across the world on their generative AI journeys with Amazon Bedrock and Bedrock AgentCore. 
Peng Shi is a Senior Applied Scientist at AWS, where he leads advancements in agent memory systems to enhance the accuracy, adaptability, and reasoning capabilities of AI. His work focuses on creating more intelligent and context-aware applications that bridge cutting-edge research with real-world impact.
‚Ä¢ Configure and verify a distributed training cluster with AWS Deep Learning Containers on Amazon EKS
  Training state-of-the-art large language models (LLMs) demands massive, distributed compute infrastructure. Meta‚Äôs Llama 3, for instance, ran on 16,000 NVIDIA H100 GPUs for over 30.84 million GPU hours. Amazon Elastic Kubernetes Service (Amazon EKS) is a managed service that simplifies the deployment, management, and scaling of Kubernetes clusters that can scale up to the ranges needed to train LLMs. To facilitate the configuration of such large, distributed workloads, AWS Deep Learning Containers (DLCs) provide pre-built, performance-optimized images for popular frameworks like PyTorch, so teams can launch jobs faster and with fewer compatibility issues. However, even with Amazon EKS and DLCs, configuring clusters for large training workloads is not a trivial task. 
A source of complexity for the configuration of the training cluster is the configuration of the GPUs in the GPU-powered instances used in distributed training. GPU-powered Amazon Elastic Compute Cloud (Amazon EC2) instances come in two families: the G family (for example, G6 with NVIDIA L4 Tensor Core GPUs) for cost-efficient inference and lighter training, and the P family (for example, P6 with NVIDIA GB200 NVL72) for massive, distributed jobs. A single P5 has 8 H100 GPUs with 640 GB HBM3 and delivers 3,200 Gbps EFA networking, ideal for multi-billion-parameter model training. Although G instances are more affordable, they lack the high-bandwidth, low-latency fabric, and memory throughput needed for extreme scale. P instances, though fast, require precise configuration of networking, storage, and GPU topologies, making them powerful but operationally complex and a potential source of misconfigurations or errors for the distributed job. 
Misconfiguration issues in distributed training with Amazon EKS can be prevented following a systematic approach to launch required components and verify their proper configuration. This post walks through the steps to set up and verify an EKS cluster for training large models using DLCs. 
Solution overview 
The solution consists of the following high-level steps: 
 
 Build a Docker image with the required dependencies using a PyTorch Framework DLC. 
 Launch the required infrastructure in a stable, GPU-ready cluster with Amazon EKS. 
 Install task-specific plugins required for GPU device plugins, Elastic Fabric Adapter (EFA) support, distributed training frameworks, and persistent file storage. 
 Run health checks to verify node readiness and the correct configuration of NVIDIA and EFA plugins. 
 Launch a small training job to verify the whole system. 
 
We walk through these steps using a fleet of two p4d.24xlarge instances that we are consuming from a capacity reservation. The scripts used in this post are available in GitHub. Similar scripts for other GPU-powered instances are available in the following GitHub repository. The overall component setup, including worker nodes with persistent storage, plugins, and drivers, is shown in the following diagram. 
 
Prerequisites 
To deploy this solution, you need to have these prerequisites: 
 
 An AWS account with billing enabled 
 Sufficient service quotas for on-demand G instances, or access to a capacity reservation 
 Hugging Face token with access to Meta Llama 2 7B 
 
Build Docker image from AWS DLC 
DLCs are pre-built, performance-optimized Docker images that make it straightforward to run popular frameworks like PyTorch and TensorFlow on AWS. Each DLC ships with a fully integrated stack that includes compatible versions of CUDA, cuDNN, and NCCL, plus optional EFA support for high-throughput, low-latency distributed training. These containers are validated across Amazon EC2, Amazon Elastic Container Service (Amazon ECS), and Amazon EKS, providing consistent performance on G- and P-family GPU instances. This uniform environment is critical for distributed workloads, where even minor version mismatches can trigger throughput degradation, stalled all-reduce operations, or CUDA/NCCL errors. Although it‚Äôs possible to build training containers from scratch, doing so at production scale is tedious: GPU drivers, CUDA, NCCL, and networking libraries must be aligned with strict version and hardware requirements. DLCs simplify this by providing secure, regularly updated images that are already optimized for AWS infrastructure. 
Most distributed training jobs need additional libraries, launch utilities, or orchestration scripts that the base DLCs don‚Äôt include. As a result, teams typically use DLCs as a foundation and extend them with the dependencies required for their workloads. This approach preserves the reliability of AWS optimized images while providing the flexibility to customize for large-scale training. 
In this post, we show the process of building a custom Docker container by adding custom Python libraries to the PyTorch 2.7.1 Training DLC to launch a training job with Meta Llama 2 7B. For more details, refer to AWS Deep Learning Containers for PyTorch 2.7 Training on EC2, ECS and EKS. To prevent mismatches with the NVIDIA drivers and CUDA versions, we recommend using an EC2 instance powered by a Deep Learning AMI (DLAMI) to build the image. The DLAMI is used only for building a container image used by the training job referenced in this post. It‚Äôs different from an Amazon EKS optimized AMI, which is used to run worker nodes in an EKS cluster to run that training job. 
Complete the following steps to build a Docker image: 
 
 Launch an EC2 instance using the ‚ÄúDeep Learning Base OSS Nvidia Driver GPU AMI (Ubuntu 24.04)‚Äù for 64-bit (x86) architecture. Use at least a c5.4xlarge instance or larger, and enable HTTP/HTTPS traffic from the internet. 
 
 
 
 Allocate at least 100 GiB for storage. 
 
 
 
 Connect to the EC2 instance using an SSH client and your private key for authentication. 
 Clone the GitHub repository to access the scripts for this post: 
 
 
 git clone https://github.com/aws-samples/sample-aws-deep-learning-containers.git
cd training/eks 
 
 
 Install the AWS CLI, kubectl, and eksctl to manage the training clusters from the command line of the EC2 instance: 
 
 
 source ./setup_ec2.sh 
 
 
 Run the following script to authenticate into the DLC registry, build the custom image with the dependencies specified in the Dockerfile, and push the custom image to a private repository: 
 
 
 bash ./build.sh 
 
Launch EKS cluster 
In this step, we use a YAML file to launch an EKS cluster that contains the required infrastructure for the distributed training job. We launch two managed node groups in an existing virtual private cloud (VPC) and subnets: 
 
 A system node group (c5.2xlarge) for running cluster system pods and auto scaling components 
 A GPU node group (p4d.24xlarge) with EFA enabled networking and RAID0 local storage, designed for distributed training 
 
The script also installs several Amazon EKS add-ons (for example, an EBS CSI driver, Amazon CloudWatch observability, or a node monitoring agent) for storage provisioning and cluster observability. 
 
 apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: eks-p4d
  region: PLACEHOLDER_AWS_REGION
  version: "1.33"

# List availability zones where cluster subnets will be created
availabilityZones:
  - PLACEHOLDER_AZ1
  - PLACEHOLDER_AZ2

# Substitute vpc and subnet ids below
# if you want a VPC to be created, comment out vpc related lines
vpc:
  id: PLACEHOLDER_VPC_ID
  subnets:
    private:
      private-one:
        id: PLACEHOLDER_SUBNET_PRIVATE_1
      private-two:
        id: PLACEHOLDER_SUBNET_PRIVATE_2
    public:
      public-one:
        id: PLACEHOLDER_SUBNET_PUBLIC_1
      public-two:
        id: PLACEHOLDER_SUBNET_PUBLIC_2

iam:
  withOIDC: true

# EKS-managed node group(s)
managedNodeGroups:
  # Nodegroup for system pods
  - name: sys
    instanceType: c5.2xlarge
    desiredCapacity: 1
    iam:
      withAddonPolicies:
        autoScaler: true
        cloudWatch: true
    nodeRepairConfig:
      enabled: true

  # GPU nodegroup
  # List availability zones where instances in from this nodegroup will be launched
  # Update capacityReservationID with your own if you have a capacity reservation
  # Update desiredCapacity to the number of instances you want to launch
  - name: p4d
    instanceType: p4d.24xlarge
    instancePrefix: p4d
    privateNetworking: true
    efaEnabled: true
    minSize: 0
    desiredCapacity: 2
    maxSize: 4
    volumeSize: 500
# if you have Capacity Reservation the AZ has to be same
# if you don‚Äôt have CR nodes will be assigned per availability
    availabilityZones: ["PLACEHOLDER_AZ"]
    capacityReservation:
      capacityReservationTarget:
        capacityReservationID: "cr-xxxxxxxxxx"
    # Utilize the local instance store volume(s)
    overrideBootstrapCommand: |
      apiVersion: node.eks.aws/v1alpha1
      kind: NodeConfig
      spec:
        instance:
          localStorage:
            strategy: RAID0

   iam:
      withAddonPolicies:
        autoScaler: true
        cloudWatch: true
        ebs: true
        fsx: true
    nodeRepairConfig:
      enabled: true

addons:
# vpc-cni, coredns, and kube-proxy addons are installed by default by EKS
# we setup additional drivers as addon including storage plugins
- name: aws-ebs-csi-driver
  wellKnownPolicies:      # add IAM and service account
    ebsCSIController: true
- name: aws-fsx-csi-driver
  attachPolicyARNs:
  - arn:aws:iam::aws:policy/AmazonFSxFullAccess
- name: eks-node-monitoring-agent
  resolveConflicts: overwrite
- name: amazon-cloudwatch-observability
  resolveConflicts: overwrite
  attachPolicyARNs:
    - arn:aws:iam::aws:policy/CloudWatchFullAccess 
 
Other sample configurations for training clusters are available in the GitHub repo: 
 
 eks-g4dn-vpc.yaml ‚Äì G4dn with EFA 
 eks-p4de-odcr.yaml ‚Äì P4de with capacity reservation 
 eks-p5-odcr.yaml ‚Äì P5 with capacity reservation 
 
You can modify the chosen YAML file with your AWS Region, Kubernetes version, VPC and subnets, and optional capacity reservation details. Managed node groups are recommended because they handle node lifecycle, software, and cluster integration automatically, reducing operational overhead compared to self-managed nodes. 
After the YAML file has been updated, launch your cluster: 
 
 eksctl create cluster -f ./eks-p4d-odcr.yaml 
 
Provisioning takes 15‚Äì30 minutes. You can verify the status of your nodes with the following command: 
 
 kubectl get nodes 
 
With a successful deployment, you should see all nodes in Ready status. 
Use the following command to see all pods created by installed add-ons in Running status: 
 
  
  kubectl get pods ‚ÄìA 
  
 
Install training-specific plugins 
After you set up a basic EKS cluster, you must install additional plugins to enable critical functionalities for distributed training workloads. These plugins make sure GPUs, high-speed networking, distributed training frameworks, and persistent storage are available and correctly integrated into the cluster: 
 
 NVIDIA GPU plugin ‚Äì The NVIDIA device plugin exposes GPU resources to Kubernetes, enabling pods to request and use GPUs 
 EFA plugin ‚Äì The EFA device plugin provides high-performance networking for EFA enabled instances (for example P4 and P5), which is essential for multi-node training 
 Distributed training plugins ‚Äì These plugins include services like etcd‚Äîfor rendezvous in PyTorch‚Äîand the Kubeflow Training Operator (with the MPI Operator) to enable large-scale job orchestration 
 Persistent file storage ‚Äì The FSx CSI driver and EBS CSI driver enable scalable, high-throughput storage for datasets, model checkpoints, monitoring, and logs in Amazon FSx for Lustre and Amazon Elastic Block Store (Amazon EBS), respectively 
 
By enabling these plugins, the cluster becomes production-ready for large-scale training workloads. 
Install the NVIDIA device plugin 
Because we‚Äôre using an Amazon EKS optimized AMI with GPU support, the NVIDIA device plugin is already included. Verify that the plugin pods are running with the following command: 
 
 kubectl get pods -n kube-system | grep nvidia 
 
The expected output is as follows: 
 
 nvidia-device-plugin-daemonset-xxxxx   1/1   Running   0   3m48s
nvidia-device-plugin-daemonset-yyyyy   1/1   Running   0   3m48s 
 
If the plugin is missing, install it manually with the following command: 
 
 kubectl create -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v0.17.3/deployments/static/nvidia-device-plugin.yml 
 
Verify the availability of GPUs in your nodes with the following command: 
 
 kubectl get nodes -o json | jq '.items[].status.capacity."nvidia.com/gpu"' 
 
The expected output for nodes with 8 GPUs is as follows: 
 
 "8"
"8" 
 
Install the EFA plugin 
If you are using EFA enabled instances (such as P4d, P4de, or P5), verify that EFA resources are advertised: 
 
 kubectl get nodes -o=custom-columns=NAME:.metadata.name,EFA:.status.allocatable.vpc\\.amazonaws\\.com/efa 
 
The expected values will depend on your instance type: 
 
 P4d or p4de: 4 
 P5: 32 
 
If EFA is not visible, use the following command to install the plugin: 
 
 kubectl apply -f https://raw.githubusercontent.com/aws-samples/aws-do-eks/main/Container-Root/eks/deployment/efa-device-plugin/efa-k8s-device-plugin.yaml 
 
Install distributed training plugins: etcd and Kubeflow Training Operator 
In distributed PyTorch training workloads on Kubernetes, etcd serves as an elegant coordination mechanism that enables seamless worker orchestration. This powerful backend service built for Kubernetes acts as a central meeting point where training workers can perform three critical functions: register their presence in the cluster, discover their peer workers, and achieve synchronized startup across the distributed training job. This coordination pattern is particularly valuable when running large-scale machine learning (ML) workloads on Amazon EKS to enable efficient distributed training. 
Create an etcd store with the following command: 
 
 kubectl apply -f etcd.yaml 
 
Verify its deployment: 
 
 kubectl get pods 
 
The output should look like the following code: 
 
 NAME		READY	STATUS	RESTARTS	AGE
etcd-xxxxx-xxx	1/1	Running	0	10s 
 
The Kubeflow Training Operator simplifies distributed PyTorch training on Amazon EKS by providing custom resources (such as PyTorchJob) that automate the complex orchestration of multi-node training deployments, including worker pod lifecycle management and fault handling. By using the built-in MPI Operator, it enables efficient inter-node communication patterns critical for distributed deep learning workloads, handling the intricacies of MPI process placement, rank assignment, and network configuration that would otherwise require significant manual setup and expertise. 
Deploy Kubeflow Training Operator: 
 
 kubectl apply --server-side -k "github.com/kubeflow/training-operator.git/manifests/overlays/standalone?ref=v1.9.3" 
 
Kubeflow Training Operator (v1) is a legacy project to Kubeflow Trainer (v2), which is currently in alpha status, and APIs may change. 
Install storage plugins: FSx for Luster and Amazon EBS 
For latency-sensitive and high-bandwidth throughput dynamic workloads, such as distributed training and model serving across multiple GPU compute instances, we recommend FSx for Lustre. It provides a fully managed, high-performance parallel file system that is designed for compute-intensive workloads like high-performance computing (HPC) and ML. 
We installed the FSx for Lustre file system CSI driver using the Amazon EKS add-on while creating the cluster to mount FSx for Lustre file systems on Amazon EKS as a persistent volume (PV). In this step, you deploy an FSx for Lustre file system as a standalone high-performance cache or as an Amazon Simple Storage Service (Amazon S3) linked file system to act as a high-performance cache for Amazon S3 data, providing fast I/O and high throughput for data access across your GPU compute instances. 
Create the FSx for Lustre file system with the following command: 
 
 bash ./fsx_create.sh 
 
Create a PVC object to allow Kubernetes pods to claim storage on the FSx for Lustre file system: 
 
 kubectl apply -f ./fsx-pvc-static.yaml 
 
In FSx for Lustre, throughput scales with storage type and provisioned capacity. Optimize your deployment based on your dataset size and checkpointing needs. 
The EBS CSI driver gives Amazon EKS the ability to dynamically create and attach block volumes (using Amazon EBS) to pods. When creating node groups, EBS root volumes can be preconfigured (size, type: gp2/gp3/io1/io2). We have already installed the EBS CSI driver through the EKS cluster setup. Verify that the instance role includes the policy arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy, because without it, EBS PVC provisioning will fail. 
In summary, by layering these plugins on top of a baseline EKS cluster, you can unlock the following: 
 
 GPUs for compute 
 High-performance networking 
 Orchestration for distributed training 
 Persistent storage 
 
Together, these plugins create an environment capable of supporting large-scale, fault-tolerant, high-performance deep learning workloads on Amazon EKS. 
Verify plugins for distributed training 
When you first launch a distributed GPU training cluster on Amazon EKS (with AWS DLCs), it‚Äôs critical to validate that the environment is healthy before starting large-scale jobs. This prevents wasted time and cost due to misconfigurations or hardware issues. The checks discussed in this section cover the most important areas. 
GPU driver and NVIDIA-SMI validation 
Each GPU node must have a valid driver installation that matches the CUDA version in your AWS DLC. You can verify this either by running a script inside a GPU-enabled pod or by connecting with AWS Systems Manager. 
Regardless of the option you chose, confirm the following as part of your validation: 
 
 The driver version matches the CUDA version in your DLC 
 The GPU model, temperature, and utilization look correct 
 No errors are reported 
 
Option 1: Run inside a GPU-enabled debug pod 
The NVIDIA System Management Interface (nvidia-smi) is a command line utility intended to aid in the management and monitoring of NVIDIA GPU devices. This utility makes it possible for administrators to query GPU device state. 
Apply an nvidia-smi job manifest using the following code: 
 
 kubectl apply -f nvidia_smi.yaml
kubectl logs nvidia-smi 
 
Option 2: Connect directly using Systems Manager 
Find the instance ID of your node: 
 
 aws ec2 describe-instances \
    --filters "Name=tag:eks:nodegroup-name,Values=eks-p4d" \
    --query "Reservations[].Instances[].InstanceId" \
    --output text 
 
Start a Systems Manager session: 
 
 aws ssm start-session --target &lt;instance-id&gt; 
 
Run the nvidia-smi check to query the state of your GPUs: 
 
 nvidia-smi 
 
NCCL and multi-node communication 
Distributed training depends on fast GPU-to-GPU communication, often using the NVIDIA Collective Communications Library (NCCL). 
Deploy NCCL tests with the following script: 
 
 kubectl apply -f ./nccl-tests.yaml 
 
Verify that the NCCL worker pods are up and running: 
 
 kubectl get pods | grep nccl 
 
The results should look like the following code: 
 
 nccl-tests-launcher    1/1     Running     0          12s
nccl-tests-worker-0    1/1     Running     0          13s
nccl-tests-worker-1    1/1     Running     0          12s 
 
Validate the following: 
 
 All-reduce and communication operations complete without errors 
 Bandwidth and latency values are within expected ranges 
 If using EFA, confirm that the NCCL is using AWS_OFI_NCCL as the transport layer (optimal for HPC networking) 
 
Validate training environment with sample workload 
Finally, validate that your framework (PyTorch), GPUs, and networking all integrate properly by running a small training workload. In this case, we demonstrate this by running supervised fine-tuning on a Meta Llama 2 model. 
 
 Get a Hugging Face token. Llama 2 7B is a gated model, so you must request access to the model and then pass your Hugging Face token to the FSDP script. To register and obtain a token here, see User access tokens. Then insert the token into your conf file. 
 Run the validation script to load the environment variables and generate a job YAML manifest from the template: 
 
 
 bash ./fsdp.sh 
 
 
 Start a PyTorch distributed job: 
 
 
 kubectl apply -f ./fsdp.yaml 
 
The expected output is as follows: 
 
 pytorchjob.kubeflow.org/fsdp created 
 
 
 Check that the worker pods have been created: 
 
 
 kubectl get pods | grep fsdp 
 
The output should show both FSDP worker pods as Running: 
 
 fsdp-worker-0          1/1     Running     0          7m11s
fsdp-worker-1          1/1     Running     0          7m11s 
 
 
 Inspect the job: 
 
 
 kubectl describe -f ./fsdp.yaml 
 
You should see pod events like those in the following screenshot. 
 
 
 After the pod is created, review the logs for errors or failures: 
 
 
 kubectl logs -f fsdp-worker-0 
 
When the job is complete, the pods should move to a Completed state: 
 
 fsdp-worker-0          0/1     Completed   0          9m32s
fsdp-worker-1          0/1     Completed   0          9m32s 
 
If the job starts properly, you can stop the job with the following commands: 
 
 kubectl delete -f ./fsdp.yaml
kubectl delete -f ./etcd.yaml 
 
Both the worker pods and the etcd pod must be deleted and recreated before launching a new job, otherwise you might encounter RendezvousClosedError. 
These initial health checks help validate the following: 
 
 The cluster and nodes are ready 
 GPUs are installed, visible, and healthy 
 Multi-node communication is optimized 
 The AWS DLC environment can run ML workloads 
 
After these checks pass, you can scale up to large-scale distributed training jobs. 
Clean up 
Delete the cluster using the following command when it‚Äôs no longer needed to prevent incurring cost: 
 
 eksctl delete cluster -f ./eks-p4d-odcr.yaml 
 
Conclusion 
Distributed training requires an infrastructure foundation that delivers both computing power and predictability. When you integrate the Amazon EKS optimized AMI together with AWS DLCs, the result is a GPU-enabled cluster offering a consistent, validated runtime environment that spans all nodes. The implementation of high-bandwidth, low-latency networking capabilities enhanced with EFA helps distributed workloads execute at maximum efficiency. The addition of GPU plugins, coupled with storage integration and distributed training frameworks, creates a streamlined approach to scaling and orchestration. The final step of executing targeted initial health checks, which include NCCL connectivity testing, confirms the cluster is fully prepared for long-duration training operations. After these components are properly configured, teams can redirect their energy from infrastructure maintenance to achieving breakthrough advances in model performance. 
For scripts for running FSDP distributed training on Amazon EKS, refer to the following GitHub repo. For distributed training reference architectures, and tests, refer to the following GitHub repo. For a list of available DLC images, refer to the following GitHub repo. For an alternative implementation for running ML training and inference on Amazon EKS using a JARK stack, refer to Deploy Generative AI Models on Amazon EKS. 
 
About the authors 
Meryem Ozcelik is a GenAI/ML Specialist Solution Architect at Amazon Web Services. Her work focuses on designing and implementing generative AI and machine learning solutions, specializing in Amazon Bedrock, SageMaker, and AI/ML workload optimization on AWS. She helps accelerating AI adoption through architectural guidance, best practices, and scalable ML infrastructure design. Meryem holds a Master‚Äôs Degree in Computer Science from Georgia Institute of Technology. 
Pratik Yeole is a solutions architect specializing in container services at AWS. He helps customers adopt modern cloud-native architectures and best practices. He is a tenured Amazonian with expertise in containers and AI/ML. For leisure, he plays cricket, chess and enjoys game nights/hikes/restaurants with family and friends. 
Felipe Lopez is a Senior AI/ML Specialist Solutions Architect at AWS. Prior to joining AWS, Felipe worked with GE Digital and SLB, where he focused on modeling and optimization products for industrial applications. 
Jinyan Li is a Software Development Engineer at Amazon Web Services. Her work focuses on building and improving containerized environments for machine learning workloads on AWS. She holds a Master‚Äôs degree in Computer Science from Northeastern University. 
Sirut ‚ÄúG‚Äù Buasai is a Software Development Engineer at Amazon Web Services, working within the SageMaker AI organization. He specializes in optimizing deep learning containers and developing cloud-native solutions for machine learning workloads. His expertise includes container optimization, Kubernetes development, and ML model performance benchmarking.
‚Ä¢ Scala development in Amazon SageMaker Studio with Almond kernel
  Scala stands out as a versatile programming language that combines object-oriented and functional programming approaches. By running on the Java Virtual Machine (JVM), it maintains seamless compatibility with Java libraries while offering a concise and scalable development experience. The language has distinguished itself in the realm of distributed computing and big data processing, with the Apache Spark framework, built using Scala, serving as a prime example of its capabilities. Though Amazon SageMaker Studio provides comprehensive support for Python-based data science and machine learning (ML) workflows, it doesn‚Äôt include built-in support for Scala development. 
This integration is particularly valuable for those working with Spark or engaged in complex data processing tasks, because it supports seamless Scala-based exploratory analysis and development alongside Python-centric tools in Amazon SageMaker. The addition of the Almond kernel expands the versatility of SageMaker Studio, so teams can maintain their preferred Scala workflows while taking advantage of the service‚Äôs ML and cloud computing capabilities. 
Organizations and teams working in mixed-language environments, particularly those heavily invested in Scala and Spark-based data processing workflows, face challenges when using SageMaker Studio because it doesn‚Äôt have built-in Scala support. The current process requires developers to maintain separate environments or use workarounds, disrupting workflows and reducing productivity. Data scientists and engineers who prefer Scala‚Äôs strong typing and functional programming must adapt to Python or switch platforms, increasing development overhead and risking inconsistencies in production pipelines. Furthermore, teams that have built extensive Scala code bases for their big data processing face additional complexity when trying to integrate their existing work with ML capabilities of SageMaker, which slow down the adoption of advanced ML features or require additional engineering effort to align their Scala-based data processing and Python-based ML workflows. 
This post provides a comprehensive guide on integrating the Almond kernel into SageMaker Studio, offering a solution for Scala development within the platform. 
Solution overview 
The Almond kernel is an open source project that brings Scala support to Jupyter notebooks, effectively integrating Scala and interactive data analysis environments. The installation of the Almond kernel uses Coursier, a widely recommended Scala application installer and artifact manager. Coursier simplifies and automates the process of downloading, managing, and installing Scala libraries and dependencies. Its dependency resolution mechanism makes sure users have consistent and compatible library versions, significantly reducing potential conflicts and installation complexities. The installation steps using Coursier are executed within a custom Conda environment, maintaining a clear separation from the base SageMaker Studio setup. 
By walking through the installation and configuration process, developers and data scientists can use Scala‚Äôs robust features directly within SageMaker Studio. The following sections provide a step-by-step process to set up Scala development in SageMaker Studio using the Almond kernel. 
Prerequisites 
To begin working with this project, you must have access to JupyterLab (version 2.4.1 or later) in SageMaker Studio. This requires an active AWS account with a SageMaker Studio domain configured and a user profile set up. To set up your domain, refer to Guide to getting set up with Amazon SageMaker AI. Familiarity with the Jupyter notebooks environment is beneficial, because you will be working extensively within this interface. 
By default, SageMaker Studio provides a network interface that allows communication with the internet through a virtual private cloud (VPC) managed by Amazon SageMaker AI. Egress to the internet from SageMaker Studio is necessary for downloading necessary packages and accessing various resources. Be aware that corporate firewalls or network restrictions might interfere with your ability to download required packages. If you have deployed SageMaker Studio in a private subnet, refer to Connect Amazon SageMaker Studio in a VPC to External Resources for instructions on enabling egress access to internet. 
Appropriate AWS Identity and Access Management (IAM) permissions are essential for launching and modifying SageMaker Studio environments. For SageMaker Studio setup, admin access is initially required to configure the environment; however, in production scenarios, it‚Äôs crucial to follow the principle of least privilege by granting only the minimum necessary IAM permissions to users and roles for their specific tasks within SageMaker Studio. 
Create Jupyter Lab space in SageMaker Studio 
For instructions to create a JupyterLab space, refer to Create a space. You can choose your preferred supported version of SageMaker distribution for the Jupyter Lab space. Run and open the Jupyter lab space after you create it. 
Create and activate custom Conda environment 
With a custom Conda environment, you can maintain an isolated, reproducible development environment with specific package versions. Open the terminal in the Jupyter Lab space and run the following commands to create and activate the Conda environment: 
 
 conda create -n myenv python=3.10 -y
conda init bash
source ~/.bashrc
conda activate myenv 
 
Install OpenJDK 
Java must be installed inside the Conda environment because Scala needs it. Check if Java is already installed by running the following command: 
 
 java --version 
 
If Java is not found, install OpenJDK 11, which is compatible with Spark 3.3.2: 
 
 conda install -c conda-forge openjdk=11 -y 
 
Verify that the Java installation is successful: 
 
 java --version 
 
Set JAVA_HOME 
Validate that JAVA_HOME is updated with the configuration from Conda by running the following command: 
 
 which java
export JAVA_HOME=/home/sagemaker-user/.conda/envs/myenv 
 
Download and set up Coursier 
Install the Coursier artifact manager using the following command: 
 
 curl -Lo coursier https://git.io/coursier-cli
chmod +x coursier 
 
Install the Scala (Almond) kernel 
Run the following command to install the Almond kernel: 
 
 ./coursier launch almond -- --install 
 
Fix kernel Java path 
JupyterLab‚Äôs Scala kernel has difficulty locating the correct Java installation by default. This occurs because the kernel specification file (kernel.json) initially uses a generic Java path reference, which might not point to the actual Java installation on your SageMaker Studio instance. You must modify the Scala kernel‚Äôs configuration file (kernel.json) to explicitly specify the correct Java installation path. 
Edit the kernel configuration located at the following location: 
 
 ~/.local/share/jupyter/kernels/scala/kernel.json 
 
Update the java path to the absolute path returned by which java: 
 
 {
&nbsp;&nbsp;"argv": [
&nbsp;&nbsp; &nbsp;"/home/sagemaker-user/.conda/envs/myenv/bin/java",
&nbsp;&nbsp; &nbsp;"-jar",
&nbsp;&nbsp; &nbsp;"/home/sagemaker-user/.local/share/jupyter/kernels/scala/launcher.jar",
&nbsp;&nbsp; &nbsp;"--connection-file",
&nbsp;&nbsp; &nbsp;"{connection_file}"
&nbsp;&nbsp;],
&nbsp;&nbsp;"display_name": "Scala",
&nbsp;&nbsp;"language": "scala"
} 
 
Launch the kernel 
From the JupyterLab space launcher in SageMaker Studio, open a new notebook using the Scala kernel (see the following screenshot). 
 
Test Spark integration 
Use the following sample code to verify if the Scala kernel is functioning: 
 
 println(s"Scala: ${scala.util.Properties.versionNumberString}")
println(s"Java : ${System.getProperty("java.version")}") 
 
The following is an example of the expected output: 
 
 Scala: 2.13.x
Java : 11.0.x 
 
 
Technical considerations during and after deployment 
In this section, we discuss some of the key considerations when working with the Scala kernel on SageMaker Studio: 
 
 Verifying the correct version of the JVM is critical, particularly when using Scala for Spark-based data processing. Spark has specific JVM version requirements, and incompatibilities can lead to performance degradation or runtime failures. 
 It is essential to identify Spark compatibility requirements, typically documented in the official Spark compatibility guide. 
 Users should carefully select and manage JVM versions using tools such as Conda to maintain isolated environments with compatible JDK versions. Regular verification of the JVM version within the Scala kernel environment helps facilitate smooth operations and prevents unexpected issues. 
 Maintaining isolation from the base Conda environments provided by SageMaker Studio is crucial for the stability and reliability of the overall environment. Custom kernels should always reside within isolated Conda environments to prevent dependency conflicts and package corruption. Such isolation facilitates smoother upgrades and maintenance without impacting other user workflows. Additionally, it simplifies troubleshooting and debugging processes by clearly distinguishing between custom and default packages. 
 Because custom kernels are user-managed, the responsibility for maintaining the health and security of these environments lies with the user. Regular maintenance involves keeping all Scala libraries and dependencies up-to-date to mitigate vulnerabilities. Users should actively track updates to Spark and associated Scala libraries to maintain compatibility and optimize performance. Implementing routine monitoring and scheduling periodic reviews of installed packages are essential steps to promptly apply security patches and necessary updates. 
 
By proactively addressing these technical considerations, teams can effectively integrate Scala workflows within SageMaker Studio, creating robust and reliable data science environments that complement the existing Python-centric tools. These settings remain intact even when SageMaker Studio is restarted. 
Cost considerations 
This solution uses open source tools and doesn‚Äôt incur additional AWS charges beyond the use of the underlying SageMaker Studio environment. Review the SageMaker Pricing to get additional pricing information. 
Clean up 
After successfully setting up and using your new Scala environment in SageMaker Studio, it‚Äôs important to clean up to maintain efficiency and cost-effectiveness. This step not only frees up space but also keeps your SageMaker Studio environment tidy and organized. You can always recreate the environment later if needed, following the steps outlined in this post. By maintaining good housekeeping practices, your SageMaker Studio remains optimized for your current projects and ready for future explorations. 
 
 When you‚Äôre finished with your Scala work, shut down the SageMaker Studio kernel. This helps prevent unnecessary resource usage and potential charges. 
 Additionally, if you no longer need the custom Conda environment you created for Scala, you can delete it entirely: 
 
 
 conda deactivate
conda remove -n myenv --all -y 
 
 
 Clean up associated SageMaker resources: 
   
   Stop and delete running applications such as SageMaker Studio applications and notebooks within user profiles. 
   Delete user profiles and shared spaces within the domain. 
    
 Delete the SageMaker domain: 
   
   On the SageMaker console, choose Domains in the navigation pane. 
   Select the domain you want to delete. 
   On the Actions menu, choose Delete. 
    
 
Conclusion 
By following this post, you can use Scala within SageMaker Studio, taking advantage of the powerful capabilities of Spark and Scala-based data engineering and analytics workflows. This setup is ideal for data scientists and engineers who rely on Scala‚Äôs concise syntax and functional programming constructs, especially when working with Spark-based pipelines.The following are additional resources that can help you further explore the Almond kernel, Coursier, and Scala: 
 
 Almond Kernel GitHub repo 
 Coursier CLI documentation 
 Apache Spark compatibility guide 
 Scala language tour 
 
Start exploring these resources today to enhance your Scala development journey and streamline your ML workflows on SageMaker. 
 
About the authors 
Varun Rajan is a Senior Solutions Architect supporting Strategic Industries at Amazon Web Services. Varun has over two decades of experience in designing, building, and optimizing cloud-based solutions for a diverse range of clients and specializes in translating complex business challenges into scalable, secure solutions that deliver measurable business value. 
Aakash Aggarwal is a Technical Account Manager at Amazon Web Services (AWS), based in the San Francisco Bay Area. He has over a decade of experience in the development and management of cloud-based workloads, and specializes in helping strategic AWS customers accelerate their cloud adoption. His focus areas include AI/ML, containerization, and observability on AWS.
‚Ä¢ Build a device management agent with Amazon Bedrock AgentCore
  The proliferation of Internet of Things (IoT) devices has transformed how we interact with our environments, from homes to industrial settings. However, as the number of connected devices grows, so does the complexity of managing them. Traditional device management interfaces often require navigating through multiple applications, each with its own UI and learning curve. This fragmentation creates friction for users trying to monitor and control their IoT environment. 
In this post, we explore how to build a conversational device management system using Amazon Bedrock AgentCore. With this solution, users can manage their IoT devices through natural language, using a UI for tasks like checking device status, configuring WiFi networks, and monitoring user activity. To learn more about how Amazon Bedrock AgentCore enables deploying and operating highly effective agents securely at scale using a variety of frameworks and models, refer to Enabling customers to deliver production-ready AI agents at scale. 
The challenge of device management 
Managing a modern IoT environment involves navigating numerous challenges that can hinder user experience and technology adoption. Interface fragmentation forces users to juggle multiple applications and management tools for different devices, and technical complexity can make even basic configuration tasks intimidating for non-specialists. Adding to these difficulties are visibility limitations that prevent comprehensive monitoring of device status, and inadequate user management capabilities that make it difficult to track device usage patterns. 
Together, these pain points create significant friction for users trying to implement and maintain IoT solutions effectively. 
Solution overview 
The conversational AI solution using agents offers a comprehensive approach to IoT complexity through its unified conversational interface that consolidates device management tasks into a single access point. Users can perform sophisticated operations through natural language interaction instead of navigating technical menus, while gaining comprehensive visibility across connected devices and transforming complex configuration tasks into straightforward conversations. The system delivers essential capabilities, including device management for inventory control and status monitoring, WiFi network management for simplified network configuration, user management for access control, and activity tracking for temporal analysis of user interactions. This seamless management experience minimizes monitoring vulnerabilities and provides valuable insights into usage patterns and potential security concerns, effectively removing the typical barriers to successful IoT implementation while maintaining appropriate system authorization throughout the network. 
Architecture overview 
 
The device management system follows a modular architecture that uses several AWS services. The architecture consists of the following components: 
 
 User and application interface ‚Äì Users interact with the system through a web application that serves as the frontend interface. 
 Foundation models ‚Äì This system uses various foundation models (FMs) in Amazon Bedrock to power natural language understanding and generation capabilities. 
 Amazon Bedrock AgentCore Gateway ‚Äì This feature acts as the secure entry point for authenticated requests, validating bearer tokens before routing requests to the appropriate target. 
 Amazon Bedrock AgentCore Identity ‚Äì This feature manages agent identity and permissions, controlling what actions the agent can perform on behalf of users. 
 Amazon Bedrock AgentCore Memory ‚Äì This feature supports both short-term and long-term memory, maintaining immediate conversation context within a session and storing persistent insights and preferences across sessions. This enables agents to provide consistent, context-aware responses without developers needing to manage complex memory infrastructure. 
 Amazon Bedrock AgentCore Observability ‚Äì This feature monitors agent performance, tracks metrics, and provides insights into system usage and behavior for debugging and optimization. 
 Amazon Bedrock AgentCore Runtime ‚Äì This secure, serverless environment supports AI agents built with open source frameworks. It maintains complete session isolation by dedicating isolated containers per user session, enabling scalable and secure management of long-running, stateful interactions. 
 Amazon Cognito ‚Äì Amazon Cognito handles user authentication through bearer token generation and validation, facilitating secure access to the system. 
 Amazon DynamoDB ‚Äì Amazon DynamoDB stores system data across five tables. 
 AWS Lambda ‚Äì The solution connects the gateway to AWS Lambda functions that execute specific device management operations. Lambda contains the business logic for device management, implementing seven core tools. 
 
This architecture enables a seamless flow from user query to response: the user submits a natural language request through the application, which is authenticated through Amazon Cognito and processed by Amazon Bedrock AgentCore Runtime. The runtime determines the appropriate tool to invoke and sends the request through the gateway to the Lambda function, which queries or updates DynamoDB as needed. The result flows back through the same path, with the runtime generating a natural language response based on the data retrieved. 
Refer to the GitHub repository for detailed deployment instructions. 
Key functionalities of the device management agent 
The device management system uses Lambda to implement seven essential tools for device management, including listing devices, retrieving settings, managing WiFi networks, and monitoring user activity, all invoked by the agent as needed. This functionality is supported by our flexible NoSQL database architecture in DynamoDB, which comprises five distinct tables‚ÄîDevices, DeviceSettings, WifiNetworks, Users, and UserActivities‚Äîstoring specialized data to maintain comprehensive system records. Together, these components create a robust foundation that enables efficient device management while maintaining detailed audit trails of system activities. 
Key features showcase 

 
  
 
 
Performance and security considerations 
The solution balances robust concurrent processing capabilities with comprehensive protection measures. The device management system efficiently handles multiple simultaneous requests through automatically scaling Lambda functions, consistent DynamoDB performance regardless of data volume, and intelligent retry logic with exponential backoff when encountering rate limitations. To scale across hundreds of tools, the semantic search capability in Amazon Bedrock AgentCore Gateway enables efficient and relevant discovery of tools by meaning, facilitating quick and accurate responses even at large scale. 
The system implements industry-leading security practices, including Amazon Cognito authentication, Amazon Bedrock AgentCore Identity, layered access control through gateway and Lambda level permission verification, comprehensive data encryption at rest and in transit, and Amazon Bedrock Guardrails to help prevent prompt injection attacks while maintaining interaction safety. 
Conclusion 
The device management system presented in this post uses Amazon Bedrock AgentCore to transform IoT management through conversational AI, creating an intuitive interface where complex device operations become simple dialogue. Its composable, reusable, and decoupled agentic architecture alleviates undifferentiated heavy lifting by providing built-in features for secure, scalable deployment and seamless integration. By combining large language models with an AWS infrastructure, the solution provides enterprise-grade capabilities without burdening developers with infrastructure management. Key benefits include simplified user experiences through natural language interaction, operational efficiency with unified interfaces, comprehensive device visibility, and future-proof architecture that evolves with AI advancements. The system‚Äôs model-agnostic approach supports continuous improvement as new FMs emerge, and robust security and observability features help organizations confidently deploy scalable, next-generation device management solutions tailored to their specific IoT environments. 
To implement this solution, refer to the GitHub repository. 
 
About the Author 
Godwin Sahayaraj Vincent is an Enterprise Solutions Architect at AWS who is passionate about Machine Learning and providing guidance to customers to design, deploy and manage their AWS workloads and architectures. In his spare time, he loves to play cricket with his friends and tennis with his three kids. 
Ramesh Kumar Venkatraman is a Senior Solutions Architect at AWS who is passionate about Generative AI, Containers and Databases. He works with AWS customers to design, deploy and manage their AWS workloads and architectures. In his spare time, he loves to play with his two kids and follows cricket. 
Chhavi Kaushik is an AWS Solutions Architect specializing in cloud-native architectures and digital transformation. She is passionate about helping customers harness the power of Generative AI, designing and implementing enterprise-scale solutions that combine AWS‚Äôs cutting-edge AI/ML services. Outside of her professional life, Chhavi enjoys exploring the California outdoors, making the most of the Bay Area‚Äôs beautiful weather and lifestyle.

‚∏ª