‚úÖ Morning News Briefing ‚Äì October 27, 2025 10:47

üìÖ Date: 2025-10-27 10:47
üè∑Ô∏è Tags: #briefing #ai #publichealth #digitalgov

‚∏ª

üßæ Weather
‚Ä¢ Current Conditions:  2.9¬∞C
  Temperature: 2.9&deg;C Pressure / Tendency: 103.3 kPa rising Humidity: 81 % Dewpoint: 0.0&deg:C Wind: WNW 5 km/h . Air Quality Health Index: n/a . Pembroke 6:00 AM EDT Monday 27 October 2025 . Weather forecast: 0/2.9/deg
‚Ä¢ Monday: A mix of sun and cloud. High 8.
  Fog patches dissipating this morning. Cloudy. Becoming a mix of sun and cloud this morning . High 8. UV index 3 or moderate. High 8 or moderate . Forecast issued 5:00 AM EDT Monday 27 October 2025. Weather will be mostly sunny and breezy in the morning of October 27, 2025. Forecast also expected to be sunny and sunny in the afternoon
‚Ä¢ Monday night: Clear. Low minus 5.
  Fog patches developing after midnight . Clear. Clear. Low minus 5.50 degrees Fahrenheit . Clear skies expected to be clear . Fog patches expected to develop after midnight; low minus 5 degrees Fahrenheit Fahrenheit . Forecast issued 5:00 AM EDT Monday 27 October 2025 . Weather forecasters predict temperatures will drop to minus 6 degrees Fahrenheit over the next few days . Forecasters predict snowfall in

üåç International News
No updates.

üçÅ Canadian News
No updates.

üá∫üá∏ U.S. Top Stories
‚Ä¢ Why public media giants NPR and CPB are fighting in court this week
  NPR is accusing the Corporation for Public Broadcasting in federal court of reneging on a contract to appease the White House . NPR is accused of violating a contract with the federal government . The White House is also suing NPR for violating its contract . NPR says it is suing the government for $1 billion in damages and millions of dollars in legal fees . The lawsuit is expected to reach a total
‚Ä¢ Sheep, soldiers, and grains: Studying the physics of crowds
  In Pamplona, where the bulls run, a scientist studies the physics of crowds . The bull run is a popular event in Spain, where it takes place in the middle of the year's biggest festival in the country's fiestiest city . The bulls run is one of the biggest events in the world to take place in Spain and the world's most famous cities, Pampl
‚Ä¢ Out-of-network and on your own? What to do if your insurer drops your doctors
  Patients sometimes find themselves scrambling for affordable care when their insurer and hospital get into a contract dispute . Here are six things to know if that happens to you if you're in a dispute with your insurer or hospital . The dispute is often a dispute between insurers and hospitals over who pays for what they want to pay for their medical bills and what they think they are owed . For confidential support call
‚Ä¢ Melissa strengthens to a Category 5 hurricane as it nears Jamaica
  Melissa is forecast to make landfall on the island of Jamaica Tuesday, with up to 30 inches of rain and a life-threatening storm surge . The storm is expected to be a Category 5 hurricane, the highest on the Saffir-Simpson scale . Melissa is the highest-level Category 5 storm in the world, with a storm surge of up to a storm-force force .
‚Ä¢ How one teacher's kindness changed a grieving teenager's life
  Eric Schwartz was 15 when his mother died . The first day back at school after the loss was difficult, but one teacher's kindness changed his experience of the day and the rest of his life . Eric Schwartz's mother died when he was 15, but he says she was a good teacher . Schwartz says he was shocked by the kindness of a teacher who helped him cope with the loss of

üß† Artificial Intelligence
No updates.

üíª Digital Strategy
‚Ä¢ Everything you know about last week's AWS outage is wrong
  AWS put out a hefty analysis of its October 20 outage, and it's apparently written in a continuing stream of consciousness before the Red Bull wore off and the author passed out after 36 straight hours of writing . AI wasn't the cause, and multi-cloud is for rubes Column Columnists say it's a good readout of the outage, but it's not the cause of it
‚Ä¢ Machine learning saves ¬£4.4M in UK.gov work and pensions fraud detection
  National Audit Office says DWP saved ¬£4.4 million over three years by using machine learning to tackle fraud . Poor data standards across government hamper scaling, says spending watchdog . DWP's ability to expand work is limited by fragmented IT systems and poor cross-government data standards, says NAO . Department for Work and Pensions is using machine-learning to combat fraud, says
‚Ä¢ The Chinese Box and Turing Test: Is AI really intelligent?
  ChatGPT is just good at mass-production copy and paste . Chatbots don't think ‚Äì they've just gotten exponentially better at pretending . The 1966 chatbot from MIT's AI Lab convinced countless people it was intelligent using nothing but simple pattern matching and canned responses. Nearly 60 years later, ChatGpt has people making the same mistake. ChatGTT is just a mass-
‚Ä¢ The perfect AWS storm has blown over, but the climate is only getting worse
  When it rains, it pours ‚Äì and nobody packed an umbrella . Amazon's cloud infrastructure has been a disaster for some of the world's biggest tech companies . When it comes to a cab driver, you're asked: "What's this AWS thing, then?" Amazon has been accused of being a disaster in the world of cloud computing, writes Andrew Hammond . He says it's a
‚Ä¢ Frustrated consultant 'went full Hulk' and started smashing hardware
  Who, Me? is The Register's weekly reader-contributed column that tells tales of your greatest misses, and how you rebuilt a career afterward . For the uninitiitiated, it's The Register‚Äôs weekly readers' weekly column that tell tales of their greatest misses and how they rebuilt their careers .‚Ä¶‚Ä¶‚Ä¶ Four back-to-back weekends of work ‚Äì and

üè• Public Health
No updates.

üî¨ Science
‚Ä¢ Identifying missed prevention opportunities: maternal and congenital syphilis in hospital records and birth certificates in California from 2011 to 2021
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Alarming rates of malnutrition among preschoolers in Gaza
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Growing burden of early-onset pancreatic cancer without increasing risk: what is the trick
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Pre-existing comorbidities and hospitalization for COVID-19 are associated with post-COVID conditions in the U.S. veteran population
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Reducing annotation burden in physical activity research using vision language models
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations

üßæ Government & Policy
No updates.

üèõÔ∏è Enterprise Architecture & IT Governance
No updates.

ü§ñ AI & Emerging Tech
‚Ä¢ I tried OpenAI‚Äôs new Atlas browser but I still don‚Äôt know what it‚Äôs for
  OpenAI rolled out a new web browser last week called Atlas . It comes with ChatGPT built in, along with an agent, so that you can browse, get direct answers, and have automated tasks performed on your behalf all at the same time . In some cases, the built-in chatbot was worse and dumber . The real user is the company collecting data about what Atlas is browsing websites, it's collecting data .
‚Ä¢ Stand Up for Research, Innovation, and Education
  The MIT community is standing up for MIT and its mission to serve the nation and the world . Right now, MIT alumni and friends are voicing their support for:¬†America‚Äôs scientific and technological leadership¬†- and affordable education . We need you to join us at this critical moment. The community needs to voice support for the MIT mission and the future of the university . The community
‚Ä¢ The Download: carbon removal‚Äôs future, and measuring pain using an app
  This is today&#8217;s edition of¬†The Download,¬†our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



What‚Äôs next for carbon removal?



After years of growth that spawned hundreds of startups, the nascent carbon removal sector appears to be facing a reckoning.Running Tide, a promising aquaculture company, shut down its operations last summer, and a handful of other companies have shuttered, downsized, or pivoted in recent months as well. Venture investments have flagged. And the collective industry hasn‚Äôt made a whole lot more progress toward Running Tide‚Äôs ambitious plans to sequester a billion tons of carbon dioxide by this year.



The hype phase is over and the sector is sliding into the turbulent business trough that follows, experts warn.¬†



And the open question is: If the carbon removal sector is heading into a painful if inevitable clearing-out cycle, where will it go from there? Read the full story.



‚ÄîJames Temple



This story is part of MIT Technology Review‚Äôs What‚Äôs Next series, which looks across industries, trends, and technologies to give you a first look at the future. You can read the rest of them here.







An AI app to measure pain is here



This week I‚Äôve also been wondering how science and technology can help answer that question‚Äîespecially when it comes to pain.&nbsp;



In the latest issue of MIT Technology Review‚Äôs print magazine, Deena Mousa describes how an AI-powered smartphone app is being used to assess how much pain a person is in.



The app, and other tools like it, could help doctors and caregivers. They could be especially useful in the care of people who aren‚Äôt able to tell others how they are feeling.



But they are far from perfect. And they open up all kinds of thorny questions about how we experience, communicate, and even treat pain. Read the full story.



‚ÄîJessica Hamzelou



This article first appeared in The Checkup, MIT Technology Review‚Äôs weekly biotech newsletter. To receive it in your inbox every Thursday, and read articles like this first, sign up here.







The must-reads



I‚Äôve combed the internet to find you today‚Äôs most fun/important/scary/fascinating stories about technology.



1 Meta‚Äôs lawyers advised workers to remove parts of its teen mental health researchIts counsel told researchers to block or update their work to reduce legal liability. (Bloomberg $)+ Meta recently laid off more than 100 staff tasked with monitoring risks to user privacy. (NYT $)&nbsp;



2 Donald Trump has pardoned the convicted Binance founderChangpeng Zhao pleaded guilty to violating US money laundering laws in 2023. (WSJ $)+ The move is likely to enable Binance to resume operating in the US. (CNN)+ Trump has vowed to be more crypto-friendly than the Biden administration. (Axios)



3 Anthropic and Google Cloud have signed a major chips dealThe agreement is worth tens of billions of dollars. (FT $)



4 Microsoft doesn‚Äôt want you to talk dirty to its AIIt‚Äôll leave that kind of thing to OpenAI, thank you very much. (CNBC)+ Copilot now has its own version of Clippy‚Äîjust don‚Äôt try to get erotic with it. (The Verge)+ It‚Äôs pretty easy to get DeepSeek to talk dirty, however. (MIT Technology Review)5 Big Tech is footing the bill for Trump‚Äôs White House ballroomStand up Amazon, Apple, Google, Meta, and Microsoft. (TechCrunch)+ Crypto twins Tyler and Cameron Winklevoss are also among the donors. (CNN)



6 US investigators have busted a series of high-tech gambling schemesInvolving specially-designed contact lenses and x-ray tables. (NYT $)+ The case follows insider bets on basketball and poker games rigged by the mafia. (BBC)+ Automatic card shufflers can be compromised, too. (Wired $)



7 Deepfake harassment tools are easily accessible on social mediaAnd simple web searches. (404 Media)+ Bans on deepfakes take us only so far‚Äîhere‚Äôs what we really need. (MIT Technology Review)



8 How algorithms can drive up prices onlineEven benign algorithms can sometimes yield bad outcomes for buyers. (Quanta Magazine)+ When AIs bargain, a less advanced agent could cost you. (MIT Technology Review)



9 How to give an LLM brain rotTrain it on short ‚Äúsuperficial‚Äù posts from X, for a start. (Ars Technica)+ AI trained on AI garbage spits out AI garbage. (MIT Technology Review)



10 Meet the tech workers using AI as little as possibleIn a bid to keep their skills sharp. (WP $)+ This professor thinks there are other ways to teach people how to learn. (The Atlantic $)







Quote of the day



‚ÄúHe was convicted. He‚Äôs not innocent.‚Äù



‚ÄîRepublican Senator Thom Tillis criticises Donald Trump‚Äôs decision to pardon convicted cryptocurrency mogul Changpeng Zhao, Politico reports.







One more thing







We‚Äôve never understood how hunger works. That might be about to change.



When you‚Äôre starving, hunger is like a demon. It awakens the most ancient and primitive parts of the brain, then commandeers other neural machinery to do its bidding until it gets what it wants.Although scientists have had some success in stimulating hunger in mice, we still don‚Äôt really understand how the impulse to eat works. Now, some experts are following known parts of the neural hunger circuits into uncharted parts of the brain to try and find out.Their work could shed new light on the factors that have caused the number of overweight adults worldwide to skyrocket in recent years. And it could also help solve the mysteries around how and why a new class of weight-loss drugs seems to work so well. Read the full story.



‚ÄîAdam Piore







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)+¬† Middle aged men are getting into cliff-jumping. Should you?+ Pumpkin spice chocolate chip cookies sounds like a great idea to me.+ Christmas Island‚Äôs crabs are on the move! + Watch out if you‚Äôre taking the NY subway today: you might bump into these terrifying witches.
‚Ä¢ An AI app to measure pain is here
  How are you feeling?



I‚Äôm genuinely interested in the well-being of all my treasured Checkup readers, of course. But this week I‚Äôve also been wondering how science and technology can help answer that question‚Äîespecially when it comes to pain.¬†In the latest issue of MIT Technology Review magazine, Deena Mousa describes how an AI-powered smartphone app is being used to assess how much pain a person is in.



The app, and other tools like it, could help doctors and caregivers. They could be especially useful in the care of people who aren‚Äôt able to tell others how they are feeling.



But they are far from perfect. And they open up all kinds of thorny questions about how we experience, communicate, and even treat pain.





Pain can be notoriously difficult to describe, as almost everyone who has ever been asked to will know. At a recent medical visit, my doctor asked me to rank my pain on a scale from 1 to 10. I found it incredibly difficult to do. A 10, she said, meant ‚Äúthe worst pain imaginable,‚Äù which brought back unpleasant memories of having appendicitis.



A short while before the problem that brought me in, I‚Äôd broken my toe in two places, which had hurt like a mother‚Äîbut less than appendicitis. If appendicitis was a 10, breaking a toe was an 8, I figured. If that was the case, maybe my current pain was a 6. As a pain score, it didn‚Äôt sound as bad as I actually felt. I couldn‚Äôt help wondering if I might have given a higher score if my appendix were still intact. I wondered, too, how someone else with my medical issue might score their pain.



In truth, we all experience pain in our own unique ways. Pain is subjective, and it is influenced by our past experiences, our¬†moods, and our expectations. The way people describe their pain can vary tremendously, too.



We‚Äôve known this for ages. In the 1940s, the anesthesiologist Henry Beecher noted that¬†wounded soldiers were much less likely to ask for pain relief than similarly injured people in civilian hospitals. Perhaps they were putting on a brave face, or maybe they just felt lucky to be alive, given their circumstances. We have no way of knowing how much pain they were really feeling.



Given this messy picture, I can see the appeal of a simple test that can score pain and help medical professionals understand how best to treat their patients. That‚Äôs what is being offered by PainChek, the smartphone app Deena wrote about. The app works by assessing small facial movements, such as lip raises or brow pinches. A user is then required to fill a separate checklist to identify other signs of pain the patient might be displaying. It seems to work well, and it is already being used in hospitals and care settings.





But the app is judged against subjective reports of pain. It might be useful for assessing the pain of people who can‚Äôt describe it themselves‚Äîperhaps because they have dementia, for example‚Äîbut it won‚Äôt add much to assessments from people who can already communicate their pain levels.



There are other complications. Say a test could spot that a person was experiencing pain. What can a doctor do with that information? Perhaps prescribe pain relief‚Äîbut most of the pain-relieving drugs we have were designed to treat acute, short-term pain. If a person is grimacing from a chronic pain condition, the treatment options are more limited, says Stuart Derbyshire, a pain neuroscientist at the National University of Singapore.



The last time I spoke to Derbyshire was back in 2010, when I covered¬†work by researchers in London who were using brain scans to measure pain. That was 15 years ago. But pain-measuring brain scanners are yet to become a routine part of clinical care.



That scoring system was also built on subjective pain reports. Those reports are, as Derbyshire puts it, ‚Äúbaked into the system.‚Äù It‚Äôs not ideal, but when it comes down to it, we must rely on these wobbly, malleable, and sometimes incoherent self-descriptions of pain. It‚Äôs the best we have.



Derbyshire says he doesn‚Äôt think we‚Äôll ever have a ‚Äúpain-o-meter‚Äù that can tell you what a person is truly experiencing. ‚ÄúSubjective report is the gold standard, and I think it always will be,‚Äù he says.



This article first appeared in The Checkup,¬†MIT Technology Review‚Äôs¬†weekly biotech newsletter. To receive it in your inbox every Thursday, and read articles like this first,¬†sign up here.
‚Ä¢ What‚Äôs next for carbon removal?
  MIT Technology Review‚Äôs What‚Äôs Next series looks across industries, trends, and technologies to give you a first look at the future. You can read the rest of them here.In the early 2020s, a little-known aquaculture company in Portland, Maine, snagged more than $50 million by pitching a plan to harness nature to fight back against climate change. The company, Running Tide, said it could sink enough kelp to the seafloor to sequester a billion tons of carbon dioxide by this year, according to one of its early customers.



Instead, the business shut down its operations last summer, marking the biggest bust to date in the nascent carbon removal sector.



Its demise was the most obvious sign of growing troubles and dimming expectations for a space that has spawned hundreds of startups over the last few years. A handful of other companies have shuttered, downsized, or pivoted in recent months as well. Venture investments have flagged. And the collective industry hasn‚Äôt made a whole lot more progress toward that billion-ton benchmark.The hype phase is over and the sector is sliding into the turbulent business trough that follows, warns Robert H√∂glund, cofounder of CDR.fyi, a public-benefit corporation that provides data and analysis on the carbon removal industry.



‚ÄúWe‚Äôre past the peak of expectations,‚Äù he says. ‚ÄúAnd with that, we could see a lot of companies go out of business, which is natural for any industry.‚Äù



The open question is: If the carbon removal sector is heading into a painful if inevitable clearing-out cycle, where will it go from there?&nbsp;



The odd quirk of carbon removal is that it never made a lot of sense as a business proposition: It‚Äôs an atmospheric cleanup job, necessary for the collective societal good of curbing climate change. But it doesn‚Äôt produce a service or product that any individual or organization strictly needs‚Äîor is especially eager to pay for.



To date, a number of businesses have voluntarily agreed to buy tons of carbon dioxide that companies intend to eventually suck out of the air. But whether they‚Äôre motivated by sincere climate concerns or pressures from investors, employees, or customers, corporate do-goodism will only scale any industry so far.&nbsp;



Most observers argue that whether carbon removal continues to bobble along or transforms into something big enough to make a dent in climate change will depend largely on whether governments around the world decide to pay for a whole, whole lot of it‚Äîor force polluters to.&nbsp;



‚ÄúPrivate-sector purchases will never get us there,‚Äù says Erin Burns, executive director of Carbon180, a nonprofit that advocates for the removal and reuse of carbon dioxide. ‚ÄúWe need policy; it has to be policy.‚Äù



What‚Äôs the problem?



The carbon removal sector began to scale up in the early part of this decade, as increasingly grave climate studies revealed the need to dramatically cut emissions and suck down vast amounts of carbon dioxide to keep global warming in check.



Specifically, nations may have to continually remove as much as 11 billion tons of carbon dioxide per year by around midcentury to have a solid chance of keeping the planet from warming past 2 ¬∞C over preindustrial levels, according to a UN climate panel report in 2022.



A number of startups sprang up to begin developing the technology and building the infrastructure that would be needed, trying out a variety of approaches like sinking seaweed or building carbon-dioxide-sucking factories.





And they soon attracted customers. Companies including Stripe, Google, Shopify, Microsoft, and others began agreeing to pre-purchase tons of carbon removal, hoping to stand up the nascent industry and help offset their own climate emissions. Venture investments also flooded into the space, peaking in 2023 at nearly $1 billion, according to data provided by PitchBook.



From early on, players in the emerging sector sought to draw a sharp distinction between conventional carbon offset projects, which studies have shown frequently exaggerate climate benefits, and ‚Äúdurable‚Äù carbon removal that could be relied upon to suck down and store away the greenhouse gas for decades to centuries. There‚Äôs certainly a big difference in the price: While buying carbon offsets through projects that promise to preserve forests or plant trees might cost a few dollars per ton, a ton of carbon removal can run hundreds to thousands of dollars, depending on the approach.&nbsp;



That high price, however, brings big challenges. Removing 10 billion tons of carbon dioxide a year at, say, $300 a ton adds up to a global price tag of $3 trillion‚Äîa year.&nbsp;



Which brings us back to the fundamental question: Who should or would foot the bill to develop and operate all the factories, pipelines, and wells needed to capture, move, and bury billions upon billions of tons of carbon dioxide?



The state of the market



The market is still growing, as companies voluntarily purchase tons of carbon removal to make strides toward their climate goals. In fact, sales reached an all-time high in the second quarter of this year, mostly thanks to several massive purchases by Microsoft.





But industry sources fear that demand isn‚Äôt growing fast enough to support a significant share of the startups that have formed or even the projects being built, undermining the momentum required to scale the sector up to the size needed by midcentury.



To date, all those hundreds of companies that have spun up in recent years have disclosed deals to sell some 38 million tons of carbon dioxide pulled from the air, according to CDR.fyi. That‚Äôs roughly the amount the US pumps out in energy-related emissions every three days.&nbsp;



And they‚Äôve only delivered around 940,000 tons of carbon removal. The US emits that much carbon dioxide in less than two hours. (Not every transaction is publicly announced or revealed to CDR.fyi, so the actual figures could run a bit higher.)



Another concern is that the same handful of big players continue to account for the vast majority of the overall purchases, leaving the health and direction of the market dependent on their whims and fortunes.&nbsp;



Most glaringly, Microsoft has agreed to buy 80% of all the carbon removal purchased to date, according to&nbsp; CDR.fyi. The second-biggest buyer is Frontier, a coalition of companies that includes Google, Meta, Stripe, and Shopify, which has committed to spend $1 billion.



If you strip out those two buyers, the market shrinks from 16 million tons under contract during the first half of this year to just 1.2 million, according to data provided to MIT Technology Review by CDR.fyi.&nbsp;



Signs of trouble



Meanwhile, the investor appetite for carbon removal is cooling. For the 12-month period ending in the second quarter of 2025, venture capital investments in the sector fell more than 13% from the same period last year, according to data provided by PitchBook. That tightening funding will make it harder and harder for companies that aren‚Äôt bringing in revenue to stay afloat.



Other companies that have already shut down include the carbon removal marketplace Nori, the direct air capture company Noya and Alkali Earth, which was attempting to use industrial by-products to tie up carbon dioxide.



Still other businesses are struggling. Climeworks, one of the first companies to build direct-air-capture (DAC) factories, announced it was laying off 10% of its staff in May, as it grapples with challenges on several fronts.



The company‚Äôs plans to collaborate on the development of a major facility in the US have been at least delayed as the Trump administration has held back tens of millions of dollars in funding granted in 2023 under the Department of Energy‚Äôs Regional Direct Air Capture Hubs program. It now appears the government could terminate the funding altogether, along with perhaps tens of billions of dollars‚Äô worth of additional grants previously awarded for a variety of other US carbon removal and climate tech projects.



‚ÄúMarket rumors have surfaced, and Climeworks is prepared for all scenarios,‚Äù Christoph Gebald, one of the company‚Äôs co-CEOs, said in a previous statement to MIT Technology Review. ‚ÄúThe need for DAC is growing as the world falls short of its climate goals and we‚Äôre working to achieve the gigaton capacity that will be needed.‚Äù



But purchases from direct-air-capture projects fell nearly 16% last year and account for just 8% of all carbon removal transactions to date. Buyers are increasingly looking to categories that promise to deliver tons faster and for less money, notably including burying biochar or installing carbon capture equipment on bioenergy plants. (Read more in my recent story on that method of carbon removal, known as BECCS, here.)



CDR.fyi recently described the climate for direct air capture in grim terms: ‚ÄúThe sector has grown rapidly, but the honeymoon is over: Investment and sales are falling, while deployments are delayed across almost every company.‚Äù‚ÄúMost DAC companies,‚Äù the organization added, ‚Äúwill fold or be acquired.‚Äù



What‚Äôs next?



In the end, most observers believe carbon removal isn‚Äôt really going to take off unless governments bring their resources and regulations to bear. That could mean making direct purchases, subsidizing these sectors, or getting polluters to pay the costs to do so‚Äîfor instance, by folding carbon removal into market-based emissions reductions mechanisms like cap-and-trade systems.&nbsp;



More government support does appear to be on the way. Notably, the European Commission recently proposed allowing ‚Äúdomestic carbon removal‚Äù within its EU Emissions Trading System after 2030, integrating the sector into one of the largest cap-and-trade programs. The system forces power plants and other polluters in member countries to increasingly cut their emissions or pay for them over time, as the cap on pollution tightens and the price on carbon rises.&nbsp;



That could create incentives for more European companies to pay direct-air-capture or bioenergy facilities to draw down carbon dioxide as a means of helping them meet their climate obligations.



There are also indications that the International Civil Aviation Organization, a UN organization that establishes standards for the aviation industry, is considering incorporating carbon removal into its market-based mechanism for reducing the sector‚Äôs emissions. That might take several forms, including allowing airlines to purchase carbon removal to offset their use of traditional jet fuel or requiring the use of carbon dioxide obtained through direct air capture in some share of sustainable aviation fuels.



Meanwhile, Canada has committed to spend $10 million on carbon removal and is developing a protocol to allow direct air capture in its national offsets program. And Japan will begin accepting several categories of carbon removal in its emissions trading system.&nbsp;



Despite the Trump administration‚Äôs efforts to claw back funding for the development of carbon-sucking projects, the US does continue to subsidize storage of carbon dioxide, whether it comes from power plants, ethanol refineries, direct-air-capture plants, or other facilities. The so-called 45Q tax credit, which is worth up to $180 a ton, was among the few forms of government support for climate-tech-related sectors that survived in the 2025 budget reconciliation bill. In fact, the subsidies for putting carbon dioxide to other uses increased.



Even in the current US political climate, Burns is hopeful that local or federal legislators will continue to enact policies that support specific categories of carbon removal in the regions where they make the most sense, because the projects can provide economic growth and jobs as well as climate benefits.



‚ÄúI actually think there are lots of models for what carbon removal policy can look like that aren‚Äôt just things like tax incentives,‚Äù she says. ‚ÄúAnd I think that this particular political moment gives us the opportunity in a unique way to start to look at what those regionally specific and pathway specific policies look like.‚Äù



The dangers ahead



But even if more nations do provide the money or enact the laws necessary to drive the business of durable carbon renewal forward, there are mounting concerns that a sector conceived as an alternative to dubious offset markets could increasingly come to replicate their problems.



Various incentives are pulling in that direction.



Financial pressures are building on suppliers to deliver tons of carbon removal. Corporate buyers are looking for the fastest and most affordable way of hitting their climate goals. And the organizations that set standards and accredit carbon removal projects often earn more money as the volume of purchases rises, creating clear conflicts of interest.



Some of the same carbon registries that have long signed off on carbon offset projects have begun creating standards or issuing credits for various forms of carbon removal, including Verra and Gold Standard.





‚ÄúReliable assurance that a project‚Äôs declared ton of carbon savings equates to a real ton of emissions removed, reduced, or avoided is crucial,‚Äù Cynthia Giles, a senior EPA advisor under President Biden, and Cary Coglianese, a law professor at the University of Pennsylvania, wrote in a recent editorial in Science. ‚ÄúYet extensive research from many contexts shows that auditors selected and paid by audited organizations often produce results skewed toward those entities‚Äô interests.‚Äù



Noah McQueen, the director of science and innovation at Carbon180, has stressed that the industry must strive to counter the mounting credibility risks, noting in a recent LinkedIn post: ‚ÄúGrowth matters, but growth without integrity isn‚Äôt growth at all.‚Äù



In an interview, McQueen said that heading off the problem will require developing and enforcing standards to truly ensure that carbon removal projects deliver the climate benefits promised. McQueen added that to gain trust, the industry needs to earn buy-in from the communities in which these projects are built and avoid the environmental and health impacts that power plants and heavy industry have historically inflicted on disadvantaged communities.



Getting it right will require governments to take a larger role in the sector than just subsidizing it, argues David Ho, a professor at the University of Hawai ªi at MƒÅnoa who focuses&nbsp; on ocean-based carbon removal.



He says there should be a massive, multinational research drive to determine the most effective ways of mopping up the atmosphere with minimal environmental or social harm, likening it to a Manhattan Project (minus the whole nuclear bomb bit).



‚ÄúIf we‚Äôre serious about doing this, then let‚Äôs make it a government effort,‚Äù he says, ‚Äúso that you can try out all the things, determine what works and what doesn‚Äôt, and you don‚Äôt have to please your VCs or concentrate on developing [intellectual property] so you can sell yourself to a fossil-fuel company.‚Äù



Ho adds that there‚Äôs a moral imperative for the world‚Äôs historically biggest climate polluters to build and pay for the carbon-sucking and storage infrastructure required to draw down billions of tons of greenhouse gas. That‚Äôs because the world‚Äôs poorest, hottest nations, which have contributed the least to climate change, will nevertheless face the greatest dangers from intensifying heat waves, droughts, famines, and sea-level rise.



‚ÄúIt should be seen as waste management for the waste we‚Äôre going to dump on the Global South,‚Äù he says, ‚Äúbecause they‚Äôre the people who will suffer the most from climate change.‚ÄùCorrection (October 24): An earlier version of this article referred to Noya as a carbon removal marketplace. It was a direct air capture company.

üîí Cybersecurity & Privacy
‚Ä¢ Canada Fines Cybercrime Friendly Cryptomus $176M
  Financial regulators in Canada this week levied $176 million in fines against Cryptomus, a digital payments platform that supports dozens of Russian cryptocurrency exchanges and websites hawking cybercrime services . The penalties for violating Canada&#8217;s anti money-laundering laws come ten months after KrebsOnSecurity noted the Vancouver address was home to dozens of foreign currency dealers, money transfer businesses, and cryptocurrency exchanges .

üéì University AI
No updates.

üè¢ Corporate AI
‚Ä¢ Responsible AI design in healthcare and life sciences
  Generative AI has emerged as a transformative technology in healthcare, driving digital transformation in essential areas such as patient engagement and care management. It has shown potential to revolutionize how clinicians provide improved care through automated systems with diagnostic support tools that provide timely, personalized suggestions, ultimately leading to better health outcomes. For example, a study reported in BMC Medical Education that medical students who received large language model (LLM)-generated feedback during simulated patient interactions significantly improved their clinical decision-making compared to those who did not. 
At the center of most generative AI systems are LLMs capable of generating remarkably natural conversations, enabling healthcare customers to build products across billing, diagnosis, treatment, and research that can perform tasks and operate independently with human oversight. However, the utility of generative AI requires an understanding of the potential risks and impacts on healthcare service delivery, which necessitates the need for careful planning, definition, and execution of a system-level approach to building safe and responsible generative AI-infused applications. 
In this post, we focus on the design phase of building healthcare generative AI applications, including defining system-level policies that determine the inputs and outputs. These policies can be thought of as guidelines that, when followed, help build a responsible AI system. 
Designing responsibly 
LLMs can transform healthcare by reducing the cost and time required for considerations such as quality and reliability. As shown in the following diagram, responsible AI considerations can be successfully integrated into an LLM-powered healthcare application by considering quality, reliability, trust, and fairness for everyone. The goal is to promote and encourage certain responsible AI functionalities of AI systems. Examples include the following: 
 
 Each component‚Äôs input and output is aligned with clinical priorities to maintain alignment and promote controllability 
 Safeguards, such as guardrails, are implemented to enhance the safety and reliability of your AI system 
 Comprehensive AI red-teaming and evaluations are applied to the entire end-to-end system to assess safety and privacy-impacting inputs and outputs 
 
Conceptual architecture 
The following diagram shows a conceptual architecture of a generative AI application with an LLM. The inputs (directly from an end-user) are mediated through input guardrails. After the input has been accepted, the LLM can process the user‚Äôs request using internal data sources. The output of the LLM is again mediated through guardrails and can be shared with end-users. 
 
Establish governance mechanisms 
When building generative AI applications in healthcare, it‚Äôs essential to consider the various risks at the individual model or system level, as well as at the application or implementation level. The risks associated with generative AI can differ from or even amplify existing AI risks. Two of the most important risks are confabulation and bias: 
 
 Confabulation ‚Äî The model generates confident but erroneous outputs, sometimes referred to as hallucinations. This could mislead patients or clinicians. 
 Bias ‚Äî This refers to the risk of exacerbating historical societal biases among different subgroups, which can result from non-representative training data. 
 
To mitigate these risks, consider establishing content policies that clearly define the types of content your applications should avoid generating. These policies should also guide how to fine-tune models and which appropriate guardrails to implement. It is crucial that the policies and guidelines are tailored and specific to the intended use case. For instance, a generative AI application designed for clinical documentation should have a policy that prohibits it from diagnosing diseases or offering personalized treatment plans. 
Additionally, defining clear and detailed policies that are specific to your use case is fundamental to building responsibly. This approach fosters trust and helps developers and healthcare organizations carefully consider the risks, benefits, limitations, and societal implications associated with each LLM in a particular application. 
The following are some example policies you might consider using for your healthcare-specific applications. The first table summarizes the roles and responsibilities for human-AI configurations. 
 
  
   
   Action ID 
   Suggested Action 
   Generative AI Risks 
   
   
   GV-3.2-001 
   Policies are in place to bolster oversight of generative AI systems with independent evaluations or assessments of generative AI models or systems where the type and robustness of evaluations are proportional to the identified risks. 
   CBRN Information or Capabilities; Harmful Bias and Homogenization 
   
   
   GV-3.2-002 
   Consider adjustment of organizational roles and components across lifecycle stages of large or complex generative AI systems, including: test and evaluation, validation, and red-teaming of generative AI systems; generative AI content moderation; generative AI system development and engineering; increased accessibility of generative AI tools, interfaces, and systems; and incident response and containment. 
   Human-AI Configuration; Information Security; Harmful Bias and Homogenization 
   
   
   GV-3.2-003 
   Define acceptable use policies for generative AI interfaces, modalities, and human-AI configurations (for example, for AI assistants and decision-making tasks), including criteria for the kinds of queries generative AI applications should refuse to respond to. 
   Human-AI Configuration 
   
   
   GV-3.2-004 
   Establish policies for user feedback mechanisms for generative AI systems that include thorough instructions and any mechanisms for recourse. 
   Human-AI Configuration 
   
   
   GV-3.2-005 
   Engage in threat modeling to anticipate potential risks from generative AI systems. 
   CBRN Information or Capabilities; Information Security 
   
  
 
The following table summarizes policies for risk management in AI system design. 
 
  
   
   Action ID 
   Suggested Action 
   Generative AI Risks 
   
   
   GV-4.1-001 
   Establish policies and procedures that address continual improvement processes for generative AI risk measurement. Address general risks associated with a lack of explainability and transparency in generative AI systems by using ample documentation and techniques such as application of gradient-based attributions, occlusion or term reduction, counterfactual prompts and prompt engineering, and analysis of embeddings. Assess and update risk measurement approaches at regular cadences. 
   Confabulation 
   
   
   GV-4.1-002 
   Establish policies, procedures, and processes detailing risk measurement in context of use with standardized measurement protocols and structured public feedback exercises such as AI red-teaming or independent external evaluations. 
   CBRN Information and Capability; Value Chain and Component Integration 
   
  
 
Transparency artifacts 
Promoting transparency and accountability throughout the AI lifecycle can foster trust, facilitate debugging and monitoring, and enable audits. This involves documenting data sources, design decisions, and limitations through tools like model cards and offering clear communication about experimental features. Incorporating user feedback mechanisms further supports continuous improvement and fosters greater confidence in AI-driven healthcare solutions. 
AI developers and DevOps engineers should be transparent about the evidence and reasons behind all outputs by providing clear documentation of the underlying data sources and design decisions so that end-users can make informed decisions about the use of the system. Transparency enables the tracking of potential problems and facilitates the evaluation of AI systems by both internal and external teams. Transparency artifacts guide AI researchers and developers on the responsible use of the model, promote trust, and help end-users make informed decisions about the use of the system. 
The following are some implementation suggestions: 
 
 When building AI features with experimental models or services, it‚Äôs essential to highlight the possibility of unexpected model behavior so healthcare professionals can accurately assess whether to use the AI system. 
 Consider publishing artifacts such as Amazon SageMaker model cards or AWS system cards. Also, at AWS we provide detailed information about our AI systems through AWS AI Service Cards, which list intended use cases and limitations, responsible AI design choices, and deployment and performance optimization best practices for some of our AI services. AWS also recommends establishing transparency policies and processes for documenting the origin and history of training data while balancing the proprietary nature of training approaches. Consider creating a hybrid document that combines elements of both model cards and service cards, because your application likely uses foundation models (FMs) but provides a specific service. 
 Offer a feedback user mechanism. Gathering regular and scheduled feedback from healthcare professionals can help developers make necessary refinements to improve system performance. Also consider establishing policies to help developers allow for user feedback mechanisms for AI systems. These should include thorough instructions and consider establishing policies for any mechanisms for recourse. 
 
Security by design 
When developing AI systems, consider security best practices at each layer of the application. Generative AI systems might be vulnerable to adversarial attacks suck as prompt injection, which exploits the vulnerability of LLMs by manipulating their inputs or prompt. These types of attacks can result in data leakage, unauthorized access, or other security breaches. To address these concerns, it can be helpful to perform a risk assessment and implement guardrails for both the input and output layers of the application. As a general rule, your operating model should be designed to perform the following actions: 
 
 Safeguard patient privacy and data security by implementing personally identifiable information (PII) detection, configuring guardrails that check for prompt attacks 
 Continually assess the benefits and risks of all generative AI features and tools and regularly monitor their performance through Amazon CloudWatch or other alerts 
 Thoroughly evaluate all AI-based tools for quality, safety, and equity before deploying 
 
Developer resources 
The following resources are useful when architecting and building generative AI applications: 
 
 Amazon Bedrock Guardrails helps you implement safeguards for your generative AI applications based on your use cases and responsible AI policies. You can create multiple guardrails tailored to different use cases and apply them across multiple FMs, providing a consistent user experience and standardizing safety and privacy controls across your generative AI applications. 
 The AWS responsible AI whitepaper serves as an invaluable resource for healthcare professionals and other developers that are developing AI applications in critical care environments where errors could have life-threatening consequences. 
 AWS AI Service Cards explains the use cases for which the service is intended, how machine learning (ML) is used by the service, and key considerations in the responsible design and use of the service. 
 
Conclusion 
Generative AI has the potential to improve nearly every aspect of healthcare by enhancing care quality, patient experience, clinical safety, and administrative safety through responsible implementation. When designing, developing, or operating an AI application, try to systematically consider potential limitations by establishing a governance and evaluation framework grounded by the need to maintain the safety, privacy, and trust that your users expect. 
For more information about responsible AI, refer to the following resources: 
 
 NIST Trustworthy and Responsible AI 
 OWASP Top 10 for Large Language Model applications 
 
 
 
About the authors 
Tonny Ouma is an Applied AI Specialist at AWS, specializing in generative AI and machine learning. As part of the Applied AI team, Tonny helps internal teams and AWS customers incorporate leading-edge AI systems into their products. In his spare time, Tonny enjoys riding sports bikes, golfing, and entertaining family and friends with his mixology skills. 
Simon Handley, PhD, is a Senior AI/ML Solutions Architect in the Global Healthcare and Life Sciences team at Amazon Web Services. He has more than 25 years‚Äô experience in biotechnology and machine learning and is passionate about helping customers solve their machine learning and life sciences challenges. In his spare time, he enjoys horseback riding and playing ice hockey.
‚Ä¢ Beyond pilots: A proven framework for scaling AI to production
  The era of perpetual AI pilots is over. This year, 65% of AWS Generative AI Innovation Center customer projects moved from concept to production‚Äîsome launching in just 45 days, as AWS VP Swami Sivasubramanian shared on LinkedIn. These results come from insights gained across more than one thousand customer implementations. 
The Generative AI Innovation Center pairs organizations across industries with AWS scientists, strategists, and engineers to implement practical AI solutions that drive measurable outcomes. These initiatives transform diverse sectors worldwide. For example, through a cross-functional AWS collaboration, we supported the National Football League (NFL) to create a generative AI-powered solution that obtains statistical game insights within 30 seconds. This helps their media and production teams locate video content six times faster. Similarly, we helped Druva‚Äôs DruAI system streamline customer support and data protection through natural language processing, reducing investigation time from hours to minutes. 
These achievements reflect a broader pattern of success, driven by a powerful methodology: The Five V‚Äôs Framework for AI Implementation. 
 
This framework takes projects from initial testing to full deployment by focusing on concrete business outcomes and operational excellence. It‚Äôs grounded in two of Amazon‚Äôs Leadership Principles, Customer Obsession and Deliver Results. By starting with what customers actually need and working backwards, we‚Äôve helped companies across industries modernize their operations and better serve their customers. 
The Five V‚Äôs Framework: A foundation for success 
Every successful AI deployment begins with groundwork. In our experience, projects thrive when organizations first identify specific challenges they need to solve, align key stakeholders around these goals, and establish clear accountability for results. The Five V‚Äôs Framework helps guide organizations through a structured process: 
 
 Value: Target high-impact opportunities aligned with your strategic priorities 
 Visualize: Define clear success metrics that link directly to business outcomes 
 Validate: Test solutions against real-world requirements and constraints 
 Verify: Create a scalable path to production that delivers sustainable results 
 Venture: Secure the resources and support needed for long-term success 
 
Value: The critical first step 
The Value phase emphasizes working backwards from your most pressing business challenges. By starting with existing pain points and collaborating across technical and business teams, organizations can develop solutions that deliver meaningful return on investment (ROI). This focused approach helps direct resources where they‚Äôll have the greatest impact. 
Visualize: Defining success through measurement 
The next step requires translating the potential benefits‚Äîcost reduction, revenue growth, risk mitigation, improved customer experience, and competitive advantage‚Äîinto clear, measurable performance indicators. A comprehensive measurement framework starts with baseline metrics using historical data where available. These metrics should address both technical aspects like accuracy and response time, as well as business outcomes such as productivity gains and customer satisfaction. 
The Visualize phase examines data availability and quality to support proper measurement while working with stakeholders to define success criteria that align with strategic objectives. This dual focus helps organizations track not just the performance of the AI solution, but its actual impact on business goals. 
Validate: Where ambition meets reality 
The Validate phase focuses on testing solutions against real-world conditions and constraints. Our approach integrates strategic vision with implementation expertise from day one. As Sri Elaprolu, Director of the Generative AI Innovation Center, explains: ‚ÄúEffective validation creates alignment between vision and execution. We unite diverse perspectives‚Äîfrom scientists to business leaders‚Äîso that solutions deliver both technical excellence and measurable business impact.‚Äù 
This process involves systematic integration testing, stress testing for expected loads, verifying compliance requirements, and gathering end-user feedback. Security specialists shape the core architecture. Industry subject matter experts define the operational processes and decision logic that guide prompt design and model refinement. Change management strategies are integrated early to ensure alignment and adoption. 
The Generative AI Innovation Center partnered with SparkXGlobal, an AI-driven marketing-technology company, to validate their new solution through comprehensive testing. Their platform, Xnurta, provides business analytics and reporting for Amazon merchants, demonstrating impressive results: report processing time dropped from 6-8 hours to just 8 minutes while maintaining 95% accuracy. This successful validation established a foundation for SparkXGlobal‚Äôs continued innovation and enhanced AI capabilities. 
Working with the Generative AI Innovation Center, the U.S. Environmental Protection Agency (EPA) created an intelligent document processing solution powered by Anthropic models on Amazon Bedrock. This solution helped EPA scientists accelerate chemical risk assessments and pesticide reviews through transparent, verifiable, and human-controlled AI practices. The impact has been substantial: document processing time decreased by 85%, evaluation costs dropped by 99%, and more than 10,000 regulatory applications have advanced faster to protect public health. 
Verify: The path to production 
Moving from pilot to production requires more than proof of concept‚Äîit demands scalable solutions that integrate with existing systems and deliver consistent value. While demos can seem compelling, verification reveals the true complexity of enterprise-wide deployment. This critical stage maps the journey from prototype to production, establishing a foundation for sustainable success. 
Building production-ready AI solutions brings together several key elements. Robust governance structures must facilitate responsible AI deployment and oversight, managing risk and compliance in an evolving regulatory landscape. Change management prepares teams and processes for new ways of working, driving organization-wide adoption. Operational readiness assessments evaluate existing workflows, integration points, and team capabilities to facilitate smooth implementation. 
Architectural decisions in the verification phase balance scale, reliability, and operability, with security and compliance woven into the solution‚Äôs fabric. This often involves practical trade-offs based on real-world constraints. A simpler solution aligned to existing team capabilities may prove more valuable than a complex one requiring specialized expertise. Similarly, meeting strict latency requirements might necessitate choosing a streamlined model over a more sophisticated one, as model selection requires a balance of performance, accuracy, and computational costs based on the use case. 
Generative AI Innovation Center Principal Data Scientist, Isaac Privitera, captures this philosophy: ‚ÄúWhen building a generative AI solution, we focus primarily on three things: measurable business impact, production readiness from day one, and sustained operational excellence. This trinity drives solutions that thrive in real-world conditions.‚Äù 
Effective verification demands both technical expertise and practical wisdom from real-world deployments. It requires proving not just that a solution works in principle, but that it can operate at scale within existing systems and team capabilities. By systematically addressing these factors, we help make sure deployments deliver sustainable, long-term value. 
Venture: Securing long-term success 
Long-term success in AI also requires mindful resource planning across people, processes, and funding. The Venture phase maps the full journey from implementation through sustained organizational adoption. 
Financial viability starts with understanding the total cost of ownership, from initial development through deployment, integration, training, and ongoing operations. Promising projects can stall mid-implementation due to insufficient resource planning. Success requires strategic budget allocation across all phases, with clear ROI milestones and the flexibility to scale. 
Successful ventures demand organizational commitment through executive sponsorship, stakeholder alignment, and dedicated teams for ongoing optimization and maintenance. Organizations must also account for both direct and indirect costs‚Äîfrom infrastructure and development, to team training, process adaptation, and change management. A blend of sound financial planning and flexible resource strategies allows teams to accelerate and adjust as opportunities and challenges arise. 
From there, the solution must integrate seamlessly into daily operations with clear ownership and widespread adoption. This transforms AI from a project into a core organizational capability. 
Adopting the Five V‚Äôs Framework in your enterprise 
The Five V‚Äôs Framework shifts AI focus from technical capabilities to business results, replacing ‚ÄòWhat can AI do?‚Äô with ‚ÄòWhat do we need AI to do?‚Äô. Successful implementation requires both an innovative culture and access to specialized expertise. 
 
AWS resources to support your journey 
AWS offers a variety of resources to help you scale your AI to production. 
Expert guidance 
The AWS Partnership Network (APN) offers multiple pathways to access specialized expertise, while AWS Professional Services brings proven methodologies from its own successful AI implementations. Certified partners, including Generative AI Partner Innovation Alliance members who receive direct enablement training from the Generative AI Innovation Center team, extend this expertise across industries. AWS Generative AI Competency Partners bring use case-specific success, while specialized partners focus on model customization and evaluation. 
Self-service learning 
For teams building internal capabilities, AWS provides technical blogs with implementation guides based on real-world experience, GitHub repositories with production-ready code, and AWS Workshop Studio for hands-on learning that bridges theory and practice. 
Balancing learning and innovation 
Even with the right framework and resources, not every AI project will reach production. These initiatives still provide valuable lessons that strengthen your overall program. Organizations can build lasting AI capabilities through three key principles: 
 
 Embracing a portfolio approach: Treat AI initiatives as an investment portfolio where diversification drives risk management and value creation. Balance quick wins (delivering value within months), strategic initiatives (driving longer-term transformation), and moonshot projects (potentially revolutionizing your business). 
 Creating a culture of safe experimentation: Organizations thrive with AI when teams can innovate boldly. In rapidly evolving fields, the cost of inaction often exceeds the risk of calculated experiments. 
 Learning from ‚Äúproductive failures‚Äù: Capture insights systematically across projects. Technical challenges reveal capability gaps, data issues expose information needs, and organizational readiness concerns illuminate broader transformation requirements ‚Äì all shaping future initiatives. 
 
The path forward 
The next 12-18 months present a pivotal opportunity for organizations to harness generative AI and agentic AI to solve previously intractable problems, establish competitive advantages, and explore entirely new frontiers of business possibility. Those who successfully move from pilot to production will help define what‚Äôs possible within their industries and beyond. 
Are you ready to move your AI initiatives into production? 
 
 Learn more about the AWS Generative AI Innovation Center and contact your AWS Account Manager to be connected to our expert guidance and support. 
 Join our AWS Builder community to connect with others on a similar AI journey. 
 
 
 
About the authors 
Sri Elaprolu serves as Director of the AWS Generative AI Innovation Center, where he leverages nearly three decades of technology leadership experience to drive artificial intelligence and machine learning innovation. In this role, he leads a global team of machine learning scientists and engineers who develop and deploy advanced generative and agentic AI solutions for enterprise and government organizations facing complex business challenges. Throughout his nearly 13-year tenure at AWS, Sri has held progressively senior positions, including leadership of ML science teams that partnered with high-profile organizations such as the NFL, Cerner, and NASA. These collaborations enabled AWS customers to harness AI and ML technologies for transformative business and operational outcomes. Prior to joining AWS, he spent 14 years at Northrop Grumman, where he successfully managed product development and software engineering teams. Sri holds a Master‚Äôs degree in Engineering Science and an MBA with a concentration in general management, providing him with both the technical depth and business acumen essential for his current leadership role. 
 Dr. Diego Socolinsky is currently the North America Head of the Generative AI Innovation Center at Amazon Web Services (AWS). With over 25 years of experience at the intersection of technology, machine learning, and computer vision, he has built a career driving innovation from cutting-edge research to production-ready solutions. Dr. Socolinsky holds a Ph.D. in Mathematics from The Johns Hopkins University and has been a pioneer in various fields including thermal imaging biometrics, augmented/mixed reality, and generative AI initiatives. His technical expertise spans from optimizing low-level embedded systems to architecting complex real-time deep learning solutions, with particular focus on generative AI platforms, large-scale unstructured data classification, and advanced computer vision applications. He is known for his ability to bridge the gap between technical innovation and strategic business objectives, consistently delivering transformative technology that solves complex real-world problems. 
Sabine Khan is a Strategic Initiatives Leader with the AWS Generative AI Innovation Center, where she implements delivery and strategy initiatives focused on scaling enterprise-grade Generative AI solutions. She specializes in production-ready AI systems and drives agentic AI projects from concept to deployment. With over twenty years of experience in software delivery and a strong focus on AI/ML during her tenure at AWS, she has established a track record of successful enterprise implementations. Prior to AWS, she led digital transformation initiatives and held product development and software engineering leadership roles in Houston‚Äôs energy sector. Sabine holds a Master‚Äôs degree in GeoScience and an MBA. 
Andrea Jimenez is a dual master‚Äôs candidate at the Massachusetts Institute of Technology, pursuing an M.S. in Computer Science from the School of Engineering and an MBA from the Sloan School of Management. As a GenAI Lead Graduate Fellow at the MIT GenAI Innovation Center, she researches agentic AI systems and the economic implications of generative AI technologies, while leveraging her background in artificial intelligence, product development, and startup innovation to lead teams at the intersection of technology and business strategy. Her work focuses on advancing human-AI collaboration and translating cutting-edge research into scalable, high-impact solutions. Prior to AWS and MIT, she led product and engineering teams in the tech industry and founded and sold a startup that helped early-stage companies build and launch SaaS products. 
Randi Larson connects AI innovation with executive strategy for the AWS Generative AI Innovation Center, shaping how organizations understand and translate technical breakthroughs into business value. She combines strategic storytelling with data-driven insight through global keynotes, Amazon‚Äôs first tech-for-good podcast, and conversations with industry and Amazon leaders on AI transformation. Before Amazon, Randi refined her analytical precision as a Bloomberg journalist and advisor to economic institutions, think tanks, and family offices on technology initiatives. Randi holds an MBA from Duke University‚Äôs Fuqua School of Business and a B.S. in Journalism and Spanish from Boston University.
‚Ä¢ Generate Gremlin queries using Amazon Bedrock models
  Graph databases have revolutionized how organizations manage complex, interconnected data. However, specialized query languages such as Gremlin often create a barrier for teams looking to extract insights efficiently. Unlike traditional relational databases with well-defined schemas, graph databases lack a centralized schema, requiring deep technical expertise for effective querying. 
To address this challenge, we explore an approach that converts natural language to Gremlin queries, using Amazon Bedrock models such as Amazon Nova Pro. This approach helps business analysts, data scientists, and other non-technical users access and interact with graph databases seamlessly. 
In this post, we outline our methodology for generating Gremlin queries from natural language, comparing different techniques and demonstrating how to evaluate the effectiveness of these generated queries using large language models (LLMs) as judges. 
Solution overview 
Transforming natural language queries into Gremlin queries requires a deep understanding of graph structures and the domain-specific knowledge encapsulated within the graph database. To achieve this, we divided our approach into three key steps: 
 
 Understanding and extracting graph knowledge 
 Structuring the graph similar to text-to-SQL processing 
 Generating and executing Gremlin queries 
 
The following diagram illustrates this workflow. 
 
Step 1: Extract graph knowledge 
A successful query generation framework must integrate both graph knowledge and domain knowledge to accurately translate natural language queries. Graph knowledge encompasses structural and semantic information extracted directly from the graph database. Specifically, it includes: 
 
 Vertex labels and properties ‚Äì A listing of vertex types, names, and their associated attributes 
 Edge labels and properties ‚Äì Information about edge types and their attributes 
 One-hop neighbors for each vertex ‚Äì Capturing local connectivity information, such as direct relationships between vertices 
 
With this graph-specific knowledge, the framework can effectively reason about the heterogeneous properties and complex connections inherent to graph databases. 
Domain knowledge captures additional context that augments the graph knowledge and is tailored specifically to the application domain. It is sourced in two ways: 
 
 Customer-provided domain knowledge ‚Äì For example, the customer kscope.ai helped specify those vertices that represent metadata and should never be queried. Such constraints are encoded to guide the query generation process. 
 LLM-generated descriptions ‚Äì To enhance the system‚Äôs understanding of vertex labels and their relevance to specific questions, we use an LLM to generate detailed semantic descriptions of vertex names, properties, and edges. These descriptions are stored within the domain knowledge repository and provide additional context to improve the relevance of the generated queries. 
 
Step 2: Structure the graph as a text-to-SQL schema 
To improve the model‚Äôs comprehension of graph structures, we adopt an approach similar to text-to-SQL processing, where we construct a schema representing vertex types, edges, and properties. This structured representation enhances the model‚Äôs ability to interpret and generate meaningful queries. 
The question processing component transforms natural language input into structured elements for query generation. It operates in three stages: 
 
 Entity recognition and classification ‚Äì Identifies key database elements in the input question (such as vertices, edges, and properties) and categorizes the question based on its intent 
 Context enhancement ‚Äì Enriches the question with relevant information from the knowledge component, so both graph-specific and domain-specific context is properly captured 
 Query planning ‚Äì Maps the enhanced question to specific database elements needed for query execution 
 
The context generation component makes sure the generated queries accurately reflect the underlying graph structure by assembling the following: 
 
 Element properties ‚Äì Retrieves attributes of vertices and edges along with their data types 
 Graph structure ‚Äì Facilitates alignment with the database‚Äôs topology 
 Domain rules ‚Äì Applies business constraints and logic 
 
Step 3: Generate and execute Gremlin queries 
The final step is query generation, where the LLM constructs a Gremlin query based on the extracted context. The process follows these steps: 
 
 The LLM generates an initial Gremlin query. 
 The query is executed within a Gremlin engine. 
 If the execution is successful, results are returned. 
 If execution fails, an error message parsing mechanism analyzes the returned errors and refines the query using LLM-based feedback. 
 
This iterative refinement makes sure the generated queries align with the database‚Äôs structure and constraints, improving overall accuracy and usability. 
Prompt template 
Our final prompt template is as follows: 
 
 ## Request
Please write a gremlin query to answer the given question:
{{question}}
You will be provided with couple relevant vertices, together with their 
schema and other information.
Please choose the most relevant vertex according to its schema and other 
information to make the gremlin query correct.


## Instructions
1. Here are related vertices and their details:
{{schema}}
2. Don't rename properties.
3. Don't change lines (using slash n) in the generated query.


## IMPORTANT
Return the results in the following XML format:

&lt;Results&gt;
    &lt;Query&gt;INSERT YOUR QUERY HERE&lt;/Query&gt;
    &lt;Explanation&gt;
        PROVIDE YOUR EXPLANATION ON HOW THIS QUERY WAS GENERATED 
        AND HOW THE PROVIDED SCHEMA WAS LEVERAGED
    &lt;/Explanation&gt;
&lt;/Results&gt; 
 
Comparing LLM-generated queries to ground truth 
We implemented an LLM-based evaluation system using Anthropic‚Äôs Claude 3.5 Sonnet on Amazon Bedrock as a judge to assess both query generation and execution results for Amazon Nova Pro and a benchmark model. The system operates in two key areas: 
 
 Query evaluation ‚Äì Assesses correctness, efficiency, and similarity to ground-truth queries; calculates exact matching component percentages; and provides an overall rating based on predefined rules developed with domain experts 
 Execution evaluation ‚Äì Initially used a single-stage approach to compare generated results with ground truth, then enhanced to a two-stage evaluation process: 
   
   Item-by-item verification against ground truth 
   Calculation of overall match percentage 
    
 
Testing across 120 questions demonstrated the framework‚Äôs ability to effectively distinguish correct from incorrect queries. The two-stage approach particularly improved the reliability of execution result evaluation by conducting thorough comparison before scoring. 
Experiments and results 
In this section, we discuss the experiments we conducted and their results. 
Query similarity 
In the query evaluation case, we propose two metrics: query exact match and query overall rating. An exact match score is calculated by identifying matching vs. non-matching components between generated and ground truth queries. The following table summarizes the scores for query exact match. 
 
  
   
    
   Easy 
   Medium 
   Hard 
   Overall 
   
   
   Amazon Nova Pro 
   82.70% 
   61% 
   46.60% 
   70.36% 
   
   
   Benchmark Model  
   92.60% 
   68.70% 
   56.20% 
   78.93% 
   
  
 
An overall rating is provided after considering factors including query correctness, efficiency, and completeness as instructed in the prompt. The overall rating is on scale 1‚Äì10. The following table summarizes the scores for query overall rating. 
 
  
   
    
   Easy 
   Medium 
   Hard 
   Overall 
   
   
   Amazon Nova Pro 
   8.7 
   7 
   5.3 
   7.6 
   
   
   Benchmark Model 
   9.7 
   8 
   6.1 
   8.5 
   
  
 
One limitation in the current query evaluation setup is that we rely solely on the LLM‚Äôs ability to compare ground truth against LLM-generated queries and arrive at the final scores. As a result, the LLM can fail to align with human preferences and under- or over-penalize the generated query. To address this, we recommend working with a subject matter expert to include domain-specific rules in the evaluation prompt. 
Execution accuracy 
To calculate accuracy, we compare the results of the LLM-generated Gremlin queries against the results of ground truth queries. If the results from both queries match exactly, we count the instance as correct; otherwise, it is considered incorrect. Accuracy is then computed as the ratio of correct query executions to the total number of queries tested. This metric provides a straightforward evaluation of how well the model-generated queries retrieve the expected information from the graph database, facilitating alignment with the intended query logic. 
The following table summarizes the scores for execution results count match. 
 
  
   
    
   Easy 
   Medium 
   Hard 
   Overall 
   
   
   Amazon Nova Pro 
   80% 
   50% 
   10% 
   60.42% 
   
   
   Benchmark Model 
   90% 
   70% 
   30% 
   74.83% 
   
  
 
Query execution latency 
In addition to accuracy, we evaluate the efficiency of generated queries by measuring their runtime and comparing it with the ground truth queries. For each query, we record the runtime in milliseconds and analyze the difference between the generated query and the corresponding ground truth query. A lower runtime indicates a more optimized query, whereas significant deviations might suggest inefficiencies in query structure or execution planning. By considering both accuracy and runtime, we gain a more comprehensive assessment of query quality, making sure the generated queries are correct and performant within the graph database. The following box plot showcases query execution latency with respect to time for the ground truth query and the query generated by Amazon Nova Pro. As illustrated, all three types of queries exhibit comparable runtimes, with similar median latencies and overlapping interquartile ranges. Although the ground truth queries display a slightly wider range and a higher outlier, the median values across all three groups remain close. This suggests that the model-generated queries are at the same level as human-written ones in terms of execution efficiency, supporting the claim that AI-generated queries are of similar quality and don‚Äôt incur additional latency overhead. 
 
Query generation latency and cost 
Finally, we compare the time taken to generate each query and calculate the cost based on token consumption. More specifically, we measure the query generation time and track the number of tokens used, because most LLM-based APIs charge based on token usage. By analyzing both the generation speed and token cost, we can determine whether the model is efficient and cost-effective. These results provide insights in selecting the optimal model that balances query accuracy, execution efficiency, and economic feasibility. 
As shown in the following plots, Amazon Nova Pro consistently outperforms the benchmark model in both generation latency and cost. In the left plot, which depicts query generation latency, Amazon Nova Pro demonstrates a significantly lower median generation time, with most values clustered between 1.8‚Äì4 seconds, compared to the benchmark model‚Äôs broader range from around 5‚Äì11 seconds. The right plot, illustrating query generation cost, shows that Amazon Nova Pro maintains a much smaller cost per query‚Äîcentered well below $0.005‚Äîwhereas the benchmark model incurs higher and more variable costs, reaching up to $0.025 in some cases. These results highlight Amazon Nova Pro‚Äôs advantage in terms of both speed and affordability, making it a strong candidate for deployment in time-sensitive or large-scale systems. 
 
Conclusion 
We experimented with all 120 ground truth queries provided to us by kscope.ai and achieved an overall accuracy of 74.17% in generating correct results. The proposed framework demonstrates its potential by effectively addressing the unique challenges of graph query generation, including handling heterogeneous vertex and edge properties, reasoning over complex graph structures, and incorporating domain knowledge. Key components of the framework, such as the integration of graph and domain knowledge, the use of Retrieval Augmented Generation (RAG) for query plan creation, and the iterative error-handling mechanism for query refinement, have been instrumental in achieving this performance. 
In addition to improving accuracy, we are actively working on several enhancements. These include refining the evaluation methodology to handle deeply nested query results more effectively and further optimizing the use of LLMs for query generation. Moreover, we are using the RAGAS-faithfulness metric to improve the automated evaluation of query results, resulting in greater reliability and consistency in assessing the framework‚Äôs outputs. 
 
About the authors 
Mengdie (Flora) Wang is a Data Scientist at AWS Generative AI Innovation Center, where she works with customers to architect and implement scalable Generative AI solutions that address their unique business challenges. She specializes in model customization techniques and agent-based AI systems, helping organizations harness the full potential of generative AI technology. Prior to AWS, Flora earned her Master‚Äôs degree in Computer Science from the University of Minnesota, where she developed her expertise in machine learning and artificial intelligence. 
Jason Zhang&nbsp;has expertise in machine learning, reinforcement learning, and generative AI. He earned his Ph.D. in Mechanical Engineering in 2014, where his research focused on applying reinforcement learning to real-time optimal control problems. He began his career at Tesla, applying machine learning to vehicle diagnostics, then advanced NLP research at Apple and Amazon Alexa. At AWS, he worked as a Senior Data Scientist on generative AI solutions for customers. 
Rachel Hanspal is a Deep Learning Architect at AWS Generative AI Innovation Center, specializing in end-to-end GenAI solutions with a focus on frontend architecture and LLM integration. She excels in translating complex business requirements into innovative applications, leveraging expertise in natural language processing, automated visualization, and secure cloud architectures. 
Zubair Nabi is the CTO and Co-Founder of Kscope, an Integrated Security Posture Management (ISPM) platform. His expertise lies at the intersection of Big Data, Machine Learning, and Distributed Systems, with over a decade of experience building software, data, and AI platforms. Zubair is also an adjunct faculty member at George Washington University and the author of Pro Spark Streaming: The Zen of Real-Time Analytics Using Apache Spark. He holds an MPhil from the University of Cambridge. 
Suparna Pal  ‚Äì CEO &amp; Co-Founder of kscope.ai ‚Äì 20+ years of journey of building innovative platforms &amp; solutions for Industrial, Health Care and IT operations at PTC, GE, and Cisco. 
Wan Chen is an Applied Science Manager at AWS Generative AI Innovation Center. As a ML/AI veteran in tech industry, she has wide range of expertise on traditional machine learning, recommender system, deep learning and Generative AI. She is a stronger believer of Superintelligence and is very passionate to push the boundary of AI research and application to enhance human life and drive business growth. She holds Ph.D in Applied Mathematics from University of British Columbia and had worked as postdoctoral fellow in Oxford University. 
Mu Li is a Principal Solutions Architect with AWS Energy. He‚Äôs also the Worldwide Tech Leader for the AWS Energy &amp; Utilities Technical Field Community (TFC), a community of 300+ industry and technical experts. Li is passionate about working with customers to achieve business outcomes using technology. Li has worked with customers to migrate all-in to AWS from on-prem and Azure, launch the Production Monitoring and Surveillance industry solution, deploy ION/OpenLink Endur on AWS, and implement AWS-based IoT and machine learning workloads. Outside of work, Li enjoys spending time with his family, investing, following Houston sports teams, and catching up on business and technology.
‚Ä¢ Incorporating responsible AI into generative AI project prioritization
  Over the past two years, companies have seen an increasing need to develop a project prioritization methodology for generative AI. There is no shortage of generative AI use cases to consider. Rather, companies want to evaluate the business value against the cost, level of effort, and other concerns, for a large number of potential generative AI projects. One new concern for generative AI compared to other domains is considering issues like hallucination, generative AI agents making incorrect decisions and then acting on those decisions through tool calls to downstream systems, and dealing with the rapidly changing regulatory landscape. In this post we describe how to incorporate responsible AI practices into a prioritization method to systematically address these types of concerns. 
Responsible AI overview 
The AWS Well-Architected Framework defines responsible AI as ‚Äúthe practice of designing, developing, and using AI technology with the goal of maximizing benefits and minimizing risks.‚Äù The AWS responsible AI framework begins by defining eight dimensions of responsible AI: fairness, explainability, privacy and security, safety, controllability, veracity and robustness, governance, and transparency. At key points in the development lifecycle, a generative AI team should consider the possible harms or risks for each dimension (inherent and residual risks), implements risk mitigations, and monitors risk on an ongoing basis. Responsible AI applies across the entire development lifecycle and should be considered during initial project prioritization. That‚Äôs especially true for generative AI projects, where there are novel types of risks to consider, and mitigations might not be as well understood or researched. Considering responsible AI up front gives a more accurate picture of project risk and mitigation level of effort and reduces the chance of costly rework if risks are uncovered later in the development lifecycle. In addition to potentially delayed projects due to rework, unmitigated concerns might also harm customer trust, result in representational harm, or fail to meet regulatory requirements. 
Generative AI prioritization 
While most companies have their own prioritization methods, here we‚Äôll demonstrate how to use the weighted shortest job first (WSJF) method from the Scaled Agile system. WSJF assigns a priority using this formula: 
Priority = (cost of delay) / (job size) 
The cost of delay is a measure of business value. It includes the direct value (for example, additional revenue or cost savings), the timeliness (such as, is shipping this project worth a lot more today than a year from now), and the adjacent opportunities (such as, would delivering this project open up other opportunities down the road). 
The job size is where you consider the level of effort to deliver the project. That normally includes direct development costs and paying for any infrastructure or software you need. The job size is where you can include the results of the initial responsible AI risk assessment and expected mitigations. For example, if the initial assessment uncovers three risks that require mitigation, you include the development cost for those mitigations in the job size. You can also qualitatively assess that a project with ten high-priority risks is more complex than a project with only two high-priority risks. 
Example scenario 
Now, let‚Äôs walk through a prioritization exercise that compares two generative AI projects. The first project uses a large language model (LLM) to generate product descriptions. A marketing team will use this application to automatically create production descriptions that go into the online product catalog website. The second project uses a text-to-image model to generate new visuals for advertising campaigns and the product catalog. The marketing team will use this application to more quickly create customized brand assets. 
First pass prioritization 
First, we‚Äôll go through the prioritization method without considering responsible AI, assigning a score of 1‚Äì5 for each part of the WSJF formula. The specific scores vary by organization. Some companies prefer to use t-shirt sizing (S, M, L, and XL), others prefer a score of 1‚Äì5, and others will use a more granular score. A score of 1‚Äì5 is a common and straightforward way to start. For example, the direct value scores can be calculated as: 
1 = no direct value 
2 = 20% improvement in KPI (time to create high-quality descriptions) 
3 = 40% improvement in KPI 
4 = 80% improvement in KPI 
5 = 100% or more improvement in KPI 
 
  
   
    
   Project 1: Automated product descriptions (scored from 1‚Äì5) 
   Project 2: Creating visual brand assets (scored from 1‚Äì5) 
   
   
   Direct value 
   3: Helps marketing team create higher quality descriptions more quickly 
   3: Helps marketing team create higher quality assets more quickly 
   
   
   Timeliness 
   2: Not particularly urgent 
   4: New ad campaign planned this quarter; without this project, cannot create enough brand assets without hiring a new agency to supplement the team 
   
   
   Adjacent opportunities 
   2: Might be able to reuse for similar scenarios) 
   3: Experience gained in image generation will build competence for future projects 
   
   
   Job size 
   2: Basic, well-known pattern 
   2: Basic, well-known pattern 
   
   
   Score 
   (3+2+2)/2 = 3.5 
   (3+4+3)/2 = 5 
   
  
 
At first glance, it looks like Project 2 is more compelling. Intuitively that makes sense‚Äîit takes people a lot longer to make high-quality visuals than to create textual product descriptions. 
Risk assessment 
Now let‚Äôs go through a risk assessment for each project. The following table lists a brief overview of the outcome of a risk assessment along each of the AWS responsible AI dimensions, along with a t-shirt size (S, M, L, and XL) severity level. The table also includes suggested mitigations. 
 
  
   
    
   Project 1: Automated product descriptions 
   Project 2: Creating visual brand assets 
   
   
   Fairness 
   L: Are descriptions appropriate in terms of gender and demographics? Mitigate using guardrails. 
   L: Images must not portray particular demographics in a biased way. Mitigate using human and automated checks. 
   
   
   Explainability 
   No risks identified. 
   No risks identified. 
   
   
   Privacy and security 
   L: Some product information is proprietary and cannot be listed on a public site. Mitigate using data governance. 
   L: Model must not be trained on any images that contain proprietary information. Mitigate using data governance. 
   
   
   Safety 
   M: Language must be age-appropriate and not cover offensive topics. Mitigate using guardrails. 
   L: Images must not contain adult content or images of drugs, alcohol, or weapons. Mitigate using guardrails. 
   
   
   Controllability 
   S: Need to track customer feedback on the descriptions. Mitigate using customer feedback collection. 
   L: Do images align to our brand guidelines? Mitigate using human and automated checks. 
   
   
   Veracity and robustness 
   M: Will the system hallucinate and imply product capabilities that aren‚Äôt real? Mitigate using guardrails. 
   L: Are images realistic enough to avoid uncanny valley effects? Mitigate using human and automated checks. 
   
   
   Governance 
   M: Prefer LLM providers that offer copyright indemnification. Mitigate using LLM provider selection. 
   L: Require copyright indemnification and image source attribution. Mitigate using model provider selection. 
   
   
   Transparency 
   S: Disclose that descriptions are AI generated. 
   S: Disclose that descriptions are AI generated. 
   
  
 
The risks and mitigations are use-case specific. The preceding table is for illustrative purposes only. 
Second pass prioritization 
How does the risk assessment affect the prioritization? 
 
  
   
    
   Project 1: Automated product descriptions (scored from 1‚Äì5) 
   Project 2: Creating visual brand assets (scored from 1‚Äì5) 
   
   
   Job size 
   3: Basic, well-known pattern; requires fairly standard guardrails, governance, and feedback collection. 
   5: Basic, well-known pattern. Requires advanced image guardrails with human oversight, and a more expensive commercial model. Research spike needed. 
   
   
   Score 
   (3+2+2)/3 = 2.3 
   (3+4+3)/5 = 2 
   
  
 
Now it looks like Project 1 is a better one to start with. Intuitively, after you consider responsible AI, that makes sense. Poorly crafted or offensive images are more noticeable and have a larger impact than a poorly phrased product description. And the guardrails you can use for maintaining image safety are less mature than the equivalent guardrails for text, particularly in ambiguous cases like adhering to brand guidelines. In fact, an image guardrail system might require training a monitoring model or using people to spot-check some percentage of the output. You might need to dedicate a small science team to study this problem first. 
Conclusion 
In this post, you saw how to include responsible AI considerations in a generative AI project prioritization method. You saw how conducting a responsible AI risk assessment in the initial prioritization phase can change the outcome by uncovering a substantial amount of mitigation work. Moving forward, you should develop your own responsible AI policy and start adopting responsible AI practices for generative AI projects. You can find additional details and resources at Transform responsible AI from theory into practice. 
 
About the author 
Randy DeFauw is a Sr. Principal Solutions Architect at AWS. He has over 20 years of experience in technology, starting with his university work on autonomous vehicles. He has worked with and for customers ranging from startups to Fortune 50 companies, launching Big Data and Machine Learning applications. He holds an MSEE and an MBA, serves as a board advisor to K-12 STEM education initiatives, and has spoken at leading conferences including Strata and GlueCon. He is the co-author of the books SageMaker Best Practices and Generative AI Cloud Solutions. Randy currently acts as a technical advisor to AWS‚Äô director of technology in North America.
‚Ä¢ Build scalable creative solutions for product teams with Amazon Bedrock
  Creative teams and product developers are constantly seeking ways to streamline their workflows and reduce time to market while maintaining quality and brand consistency. This post demonstrates how to use AWS services, particularly Amazon Bedrock, to transform your creative processes through generative AI. You can implement a secure, scalable solution that accelerates your creative workflow, such as managing product launches, creating marketing campaigns, or developing multimedia content. 
This post examines how product teams can deploy a generative AI application that enables rapid content iteration across formats. The solution addresses comprehensive needs‚Äîfrom product descriptions and marketing copy to visual concepts and video content for social media. By integrating with brand guidelines and compliance requirements, teams can significantly reduce time to market while maintaining creative quality and consistency. 
Solution overview 
Consider a product development team at an ecommerce company creating multimedia marketing campaigns for their seasonal product launches. Their traditional workflow has bottlenecks due to lengthy revisions, manual compliance reviews, and complex coordination across creative teams. The team is exploring solutions to rapidly iterate through creative concepts, generate multiple variations of marketing materials. 
By using Amazon Bedrock and Amazon Nova models, the team can transform its creative process. Amazon Nova models enable the generation of product descriptions and marketing copy. The team creates concept visuals and product mockups with Amazon Nova Canvas, and uses Amazon Nova Reel to produce engaging video content for social media presence. Amazon Bedrock Guardrails can help the team maintain consistent brand guidelines with configurable safeguards and governance for its generative AI applications at scale. 
The team can further enhance its brand consistency with Amazon Bedrock Knowledge Bases, which can serve as a centralized repository for brand style guides, visual identity documentation, and successful campaign materials. This comprehensive knowledge base makes sure generated content is informed by the organization‚Äôs historical success and established brand standards. Product specifications, market research, and approved messaging are seamlessly integrated into the creative process, enabling more relevant and effective content generation. 
With this solution, the team can simultaneously develop materials for multiple channels while maintaining consistent brand voice across their content. Creative professionals can now focus their energy on strategic decisions rather than repetitive tasks, leading to higher-quality outputs and improved team satisfaction. 
The following sample application creates a scalable environment that streamlines the creative workflow. It helps product teams move seamlessly from initial concept to market-ready materials with automated systems handling compliance and consistency checks throughout the journey. 
 
The solution‚Äôs workflow begins with the application engineer‚Äôs setup: 
 
 Creative assets and brand guidelines are securely stored in encrypted Amazon Simple Storage Service (Amazon S3) buckets. This content is then indexed in Amazon OpenSearch Service to create a comprehensive knowledge base. 
 Guardrails are configured to enforce brand standards and compliance requirements. 
 
The user experience flows from authentication to content delivery: 
 
 Creative team members access the interface through a secure portal hosted in Amazon S3. 
 Authentication is managed through Amazon Cognito. 
 Team members‚Äô submitted creative briefs or requirements are routed to Amazon API Gateway. 
 An AWS Lambda function queries relevant brand guidelines and assets from the knowledge base. 
 The Lambda function sends the contextual information from the knowledge base to Amazon Bedrock, along with the user‚Äôs creative briefs. 
 The prompt and generated response are filtered through Amazon Bedrock Guardrails. 
 Amazon Polly converts text into lifelike speech, generating audio streams that can be played immediately and stored in S3 buckets for later use. 
 The models‚Äô generated content is delivered to the user. 
 Chat history stored in Amazon DynamoDB. 
 
Prerequisites 
The following prerequisites are required before continuing: 
 
 An AWS account 
 An AWS Identity and Access Management (IAM) role with permission to manage AWS Marketplace subscriptions and AWS services 
 AWS services: 
   
   AWS CloudFormation 
   Amazon API Gateway 
   AWS CloudFormation 
   Amazon Cognito 
   Amazon DynamoDB 
   Amazon Polly 
   Amazon S3 
   Amazon Virtual Private Cloud (Amazon VPC) with two public subnets 
    
 Amazon Bedrock models enabled: 
   
   Amazon Nova Canvas 
   Amazon Nova Reels 
   Amazon Nova Pro 
   Amazon Nova Lite 
    
 Anthropic models (optional): 
   
   Anthropic‚Äôs Claude 3 Sonnet 
    
 
Select the Models to Use in Amazon Bedrock 
When working with Amazon Bedrock for generative AI applications, one of the Ô¨Årst steps is selecting which foundation models you want to access. Amazon Bedrock oÔ¨Äers a variety of models from diÔ¨Äerent providers, and you‚Äôll need to explicitly enable the ones we plan to use in this blog. 
 
 In the Amazon Bedrock console, find and select Model access from the navigation menu on the left. 
 Click the Modify model access button to begin selecting your models. 
 Select the following Amazon models: 
   
   Nova Canvas 
   Nova Premier Cross-region inference Nova Pro 
   Titan Embeddings G1 ‚Äì Text 
   Titan Text Embeddings V2 
    
 Select the Anthropic Claude 3.7 Sonnet model. 
 Choose Next. 
 Review your selections carefully on the summary page, then choose&nbsp;Submit to confirm your choices. 
 
Set up the CloudFormation template 
We use a use a CloudFormation template to deploy all necessary solution resources. Follow these steps to prepare your installation files: 
 
 Clone the GitHub repository: 
   
   git clone https://github.com/aws-samples/aws-service-catalog-reference-architectures.git
 
    
 Navigate to the solution directory: 
   
   cd aws-service-catalog-reference-architectures/blog_content/bedrock_genai
 
   (Make note of this location as you‚Äôll need it in the following steps) 
 Sign in to your AWS account with administrator privileges to ensure you can create all required AWS resources. 
 Create an S3 bucket in the AWS Region where you plan to deploy this solution. Remember the bucket name for later steps. 
 Upload the entire content folder to your newly created S3 bucket. 
 Navigate to the content/genairacer/src folder in your S3 bucket. 
 Copy the URL for the content/genairacer/src/genairacer_setup.json file. You‚Äôll need this URL for the deployment phase. 
 
Deploy the CloudFormation template 
Complete the following steps to use the provided CloudFormation template to automatically create and configure the application components within your AWS account: 
 
 On the CloudFormation console, choose Stacks in navigation pane. 
 Choose Create stack and select with new resources (standard). 
 On the Create stack page, under Specify template, for Object URL, enter the URL copied from the previous step, then choose Next. 
 On the Specify stack details page, enter a stack name. 
 Under Parameters, choose Next. 
 On the Configure stack options page, choose Next. 
 On the Review page, select the acknowledgement check boxes and choose Submit. 
 
Sign in to the Amazon Bedrock generative AI application 
Accessing your newly deployed application is simple and straightforward. Follow these steps to log in for the Ô¨Årst time and start exploring the Amazon Bedrock generative AI interface. 
 
 On the CloudFormation console, select the stack you deployed and select the Outputs tab. 
 Find the FrontendURL value and open the provided link. 
 When the sign-in screen displays, enter the username you speciÔ¨Åed during the CloudFormation deployment process. 
 Enter the temporary password that was sent to the email address you provided during setup. 
 After you sign in, follow the prompts to change your password. 
 Choose Send to conÔ¨Årm your new credentials. 
 
Once authenticated, you‚Äôll be directed to the main Amazon Bedrock generative AI dashboard, where you can begin exploring all the features and capabilities of your new application. 
Using the application 
Now that the application has been deployed, you can use it for text, image, and audio management. In the following sections, we explore some sample use cases. 
Text generation 
The creative team at the ecommerce company wants to draft compelling product descriptions. By inputting the basic product features and desired tone, the LLM generates engaging and persuasive text that highlights the unique selling points of each item, making sure the online store‚Äôs product pages are both informative and captivating for potential customers. 
To use the text generation feature and perform actions with the supported text models using Amazon Bedrock, follow these steps: 
 
 On the AWS CloudFormation console, go to the stack you created. 
 Choose the Outputs tab. 
 Choose the link for FrontendURL. 
 Log in using the credentials sent to the email you provided during the stack deployment process. 
 On the Text tab, enter your desired prompt in the input field. 
 Choose the specific model ID you want Amazon Bedrock to use from the available options. 
 Choose Run. 
 
Repeat this process for any additional prompts you want to process. 
 
Image generation 
The creative team can now conceptualize and produce stunning product images. By describing the desired scene, style, and product placement, they can enhance the online shopping experience and increase the likelihood of customer engagement and purchase.To use the image generation feature, follow these steps: 
 
 In the UI, choose the Images tab. 
 Enter your desired text-to-image prompt in the input field. 
 Choose the specific model ID you want Amazon Bedrock to utilize from the available options. 
 Optionally, choose the desired style of the image from the provided style options. 
 Choose Generate Image. 
 
Repeat this process for any additional prompts you want to process. 
  
Audio generation 
The ecommerce company‚Äôs creative team wants to develop audio content for marketing campaigns. By specifying the message, brand voice, target demographic, and audio components, they can compose scripts and generate voiceovers for promotional videos and audio ads, resulting in consistent and professional audio materials that effectively convey the brand‚Äôs message and values.To use the audio generation feature, follow these steps: 
 
 In the UI, choose the Audio tab. 
 Enter your desired prompt in the input field. 
 Choose Run. An audio file will appear and start to play. 
 Choose the file (right-click) and choose Save Audio As to save the file. 
 
 
Amazon Bedrock Knowledge Bases 
With Amazon Bedrock Knowledge Bases, you can provide foundation models (FMs) and agents with contextual information from your organization‚Äôs private data sources, to deliver more relevant, accurate, and tailored responses. It is a powerful and user-friendly implementation of the Retrieval Augmented Generation (RAG) approach. The application showcased in this post uses the Amazon Bedrock components in the backend, simplifying the process to merely uploading a document using the application‚Äôs GUI, and then entering a prompt that will query the documents you upload. 
For our example use case, the creative team now needs to research information about internal processes and customer data, which are typically stored in documentation. When this documentation is stored in the knowledge base, they can query it on the KnowledgeBase tab. The queries executed on this tab will search the documents for the specific information they are looking for. 
Manage documents 
The documents you have uploaded will be listed on the KnowledgeBase tab. To add more, complete the following steps: 
 
 In the UI, choose the KnowledgeBase tab. 
 Choose Manage Document. 
 Choose Browse, then choose a file. 
 Choose Upload. 
 
You will see a message confirming that the file was uploaded successfully.The Amazon Bedrock Knowledge Bases syncing process is triggered when the file is uploaded. The application will be ready for queries against the new document within a minute. 
Query the knowledge base 
To query the knowledge base, complete the following steps: 
 
 In the UI, choose the KnowledgeBase tab. 
 Enter your query in the input field. 
 For Model, choose the model you want Amazon Bedrock to use for performing the query. 
 Choose Run. 
 
The generated text response from Amazon Bedrock will appear. 
Amazon Bedrock guardrails 
You can use the Guardrails tab to manage your guardrails, and create and remove guardrails as needed. Guardrails are used on the Text tab when performing queries. 
Create a guardrail 
Complete the following steps to create a new guardrail: 
 
 In the UI, choose the Guardrails tab. 
 Enter the required fields or choose the appropriate options. 
 Choose the type of guardrail under Content Filter Type. 
 Choose Create Guardrail. 
 
The newly created guardrail will appear in the right pane. 
Delete a guardrail 
Complete the following steps to delete a guardrail: 
 
 In the UI, choose the Guardrails tab. 
 Choose the guardrail you want to delete in the right pane. 
 Choose the X icon next to the guardrail. 
 
By following these steps, you can effectively manage your guardrails, for a seamless and controlled experience when performing queries in the Text tab. 
Use guardrails 
The creative team requires access to information about internal processes and customer data, which are securely stored in documentation within the knowledge base. To enforce compliance with personally identifiable information (PII) guardrails, queries executed using the Text tab are designed to search documents for specific, non-sensitive information while preventing the exposure or inclusion of PII in both prompts and answers. This approach helps the team retrieve necessary data without compromising privacy or security standards. 
To use the guardrails feature, complete the following steps: 
 
 In the UI, choose the Text tab. 
 Enter your prompt in the input field. 
 For Model ID, choose the specific model ID you want Amazon Bedrock to use. 
 Turn on Guardrails. 
 For Select Filter, choose the guardrail you want to use. 
 Choose Run. 
 
The generated text from Amazon Bedrock will appear within a few seconds. Repeat this process for any additional prompts you want to process. 
 
Clean up 
To avoid incurring costs, delete resources that are no longer needed. If you no longer need the solution, complete the following steps to delete all resources you created from your AWS account: 
 
 On the AWS CloudFormation console, choose Stacks in the navigation pane. 
 Select the stack you deployed and choose Delete. 
 
Conclusion 
By combining Amazon Bedrock, Knowledge Bases, and Guardrails with Cognito, API Gateway, and Lambda, organizations can give employees powerful AI tools for text, image, and data work. This serverless approach integrates generative AI into daily workflows securely and scalably, boosting productivity and innovation across teams.. 
For more information about generative AI and Amazon Bedrock, refer to the Amazon Bedrock category in the AWS News Blog. 
 
About the authors 
 Kenneth Walsh is a Senior AI Acceleration Architect based in New York who transforms AWS builder productivity through innovative generative AI automation tools. With a strategic focus on standardized frameworks, Kenneth accelerates partner adoption of generative AI technologies at scale. As a trusted advisor, he guides customers through their GenAI journeys with both technical expertise and genuine passion. Outside the world of artiÔ¨Åcial intelligence, Kenneth enjoys crafting culinary creations, immersing himself in audiobooks, and cherishing quality time with his family and dog. 
Wanjiko Kahara is a New York‚Äìbased Solutions Architect with a interest area in generative AI. Wanjiko is excited about learning new technology to help her customers be successful. Outside of work, Wanjiko loves to travel, explore the outdoors, and read. 
Greg Medard is a Solutions Architect with AWS. Greg guides clients in architecting, designing, and developing cloud-optimized infrastructure solutions. His drive lies in fostering cultural shifts by embracing DevOps principles that overcome organizational hurdles. Beyond work, he cherishes quality time with loved ones, tinkering with the latest tech gadgets, or embarking on adventures to discover new destinations and culinary delights. 
Bezuayehu Wate is a Specialist Solutions Architect at AWS, with a focus on big data analytics. Passionate about helping customers design, build, and modernize their cloud-based analytics solutions, she Ô¨Ånds joy in learning and exploring new technologies. Outside of work, Bezuayehu enjoys quality time with family and traveling. 
Nicole Murray is a generative AI Senior Solutions Architect at AWS, specializing in MLOps and Cloud Operations for AI startups. With 17 years of experience‚Äîincluding helping government agencies design secure, compliant applications on AWS‚Äîshe now partners with startup founders to build and scale innovative AI/ML solutions. Nicole helps teams navigate secure cloud management, technical strategy, and regulatory best practices in the generative AI space, and is also a passionate speaker and educator known for making complex cloud and AI topics accessible.

‚∏ª