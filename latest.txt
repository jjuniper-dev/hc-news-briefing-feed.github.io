‚úÖ Morning News Briefing ‚Äì November 07, 2025 10:45

üìÖ Date: 2025-11-07 10:45
üè∑Ô∏è Tags: #briefing #ai #publichealth #digitalgov

‚∏ª

üßæ Weather
‚Ä¢ Current Conditions:  -1.7¬∞C
  Temperature: -1.7&deg;C Pressure / Tendency: 101.4 kPa falling Humidity: 93 % Wind Chill: -4 . Dewpoint: -2.7¬∞C Wind: ESE 5 km/h Air Quality Health Index: n/a . Pembroke 5:00 AM EST Friday 7 November 2025 Temperature: 1.7 ¬∞
‚Ä¢ Friday: Periods of rain or snow. High plus 5.
  60 percent chance of flurries early this morning . Periods of rain beginning early this afternoon . High plus 5 . Wind chill minus 6 this morning. UV index 1 or low, with low UV index of low or very low for the rest of the day . Forecast issued 5:00 AM EST Friday 7 November 2025. For more information, visit http://www.mailonline
‚Ä¢ Friday night: Periods of drizzle or rain. Temperature rising to 8 this evening then falling. POP 40%
  Cloudy. 40 percent chance of drizzle this evening . 40 percent . chance of showers overnight . Wind southwest 20 km/h gusting to 40 becoming northwest 20 near midnight . Temperature rising to 8 this evening then falling. Chance of rain in the evening then down to 8 in the morning . Forecast issued 5:00 AM EST Friday 7 November 2025. For confidential support call the

üåç International News
No updates.

üçÅ Canadian News
No updates.

üá∫üá∏ U.S. Top Stories
‚Ä¢ Support for Israel among U.S. conservatives is starting to crack. Here's why
  For a decade, political support for Israel has come from conservative Christians . But now isolationism and antisemitism are changing the tone . Anti-Semitism and isolationism have changed the tone in the U.S. for the past decade . Israel has been a major ally of Israel in the Middle East for more than a decade but has suffered from anti-Semitism, anti-Islamophobia
‚Ä¢ What's happening with furniture prices? A tale of $399 couches and tariffs
  Competition from overseas rivals or second-hand goods has kept the price of furniture relatively low . New tariffs may boost U.S. makers ‚Äî and raise prices . Competition has kept prices relatively low, but new tariffs may raise prices for furniture makers in the U.K. New tariffs could also raise the cost of U.N. furniture makers' waives price of goods in U.
‚Ä¢ The shutdown continues, but politics persists. That gives this quiz a lot of fodder
  CNN asked a bunch of questions about politics and bears . Here are some of the answers to the questions posed about bears in the U.S. Here are a few of them about politics, bears, bears and politics . CNN.com will feature iReporters from around the world in a weekly Travel Snapshots gallery on CNN iReport.com/Travel.com . Please submit your
‚Ä¢ Want less screen-obsessed kids? Set better tech boundaries for yourself
  For kids to have healthy relationships with technology and smartphones, parents need to model good habits . Here's how to do it, and how to monitor screen time for kids . Parents need to be consistent with good habits for kids, says CNN.com's John Sutter.com.com/Heroes for Kids: Have a healthy relationship with technology? Share your story with CNN iReport
‚Ä¢ If we're being truthful, people are saying 'honestly' all the time
  The popularity of the word honestly online and in conversation has soared in recent years . TBH, we'd like to know what's going on. TBH: TBH. We'd like an explanation for the rise in the popularity of honestly online, in conversation, and in the news. We want to know more about the rise of the term honestly in the world. We do not

üß† Artificial Intelligence
No updates.

üíª Digital Strategy
‚Ä¢ UK tax collector falls short on digital efficiency, watchdog says
  Treasury found ¬£1.6 billion for extra tech investment expecting 15 percent efficiency saving . So far HMRC has underwhelmed . HMRC is yet to reach levels of efficiency its investment in digital services has led auditors to expect, according to a new report . Treasury found that HMRC had wasted ¬£2.2 billion of the money on digital services in the last three years .
‚Ä¢ ISP help desk manager fell for ‚ÄòInternet Cleaning Day‚Äô prank - then swore he got the joke
  On Call is The Register‚Äôs Friday reader-contributed column that celebrates the fine art of tech support . Lost packets would be cleaned out of routers, dead gopher servers would be pulled out of holes . On Call: On Call, The Register's Friday readers share their views on tech support and tech tech support in the next week's edition of this week's On Call .
‚Ä¢ Foxconn hires humanoid robots to make servers at Nvidia's Texas factory
  Taiwanese contract manufacturer Foxconn confirmed it will use humanoid robots to make Nvidia servers in America . We're months away from AI building AI, but it's still months away . Foxconn will use robots to build servers for Nvidia in America, which will be made by robots in the U.S. Foxconn has confirmed it is using humanoid robots for its servers . It will also use robots
‚Ä¢ Rideshare giant moves 200 Macs out of the cloud, saves $2.4 million
  Singaporean super-app company Grab has dumped 200 cloudy Mac Minis and replaced them with physical machines . Grab expects the move will save $2.4 million over three years . Grab tried to virtualize macOS, but Apple doesn‚Äôt make that easy . The move is expected to save Grab $2 million over the next three years, the company hopes to save $3 million
‚Ä¢ Microsoft will force its 'superintelligence' to be a 'humanist' and play nice with people
  Mustafa Suleyman is willing to sacrifice performance for the future of our species . Microsoft has joined the ranks of tech giants chasing superintelligent artificial intelligence . But the company's AI chief's vision is markedly different from that of other industry leaders . Microsoft's new AI boss is willing not to sacrifice human performance for future of the species, says Microsoft's AI boss . Microsoft‚Äô

üè• Public Health
No updates.

üî¨ Science
‚Ä¢ Stillbirths and the race-specific gap in neonatal death among extremely preterm births
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Addressing the Recurrent and Protracted Cholera Outbreaks in Africa: Challenges and the Way Forward
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ A qualitative study on operational challenges of Iranian affiliated mobile health clinics in Iraq during Arbaeen
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Association between prenatal exposure to antihypertensive medication and neurodevelopmental and educational outcomes in children
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ The relationship between premenstrual syndrome and circadian rhythm, depressive mood, and anxiety
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations

üßæ Government & Policy
No updates.

üèõÔ∏è Enterprise Architecture & IT Governance
No updates.

ü§ñ AI & Emerging Tech
‚Ä¢ The first new subsea habitat in 40 years is about to launch
  Vanguard feels and smells like a new RV. It has long, gray banquettes that convert into bunks, a microwave cleverly hidden under a counter, a functional steel sink with a French press and crockery above. A weird little toilet hides behind a curtain.



But some clues hint that you can‚Äôt just fire up Vanguard‚Äôs engine and roll off the lot. The least subtle is its door, a massive disc of steel complete with a wheel that spins to lock.



COURTESY MARK HARRIS




Once it is sealed and moved to its permanent home beneath the waves of the Florida Keys National Marine Sanctuary early next year, Vanguard will be the world‚Äôs first new subsea habitat in nearly four decades. Teams of four scientists will live and work on the seabed for a week at a time, entering and leaving the habitat as scuba divers. Their missions could include reef restoration, species surveys, underwater archaeology, or even astronaut training.&nbsp;



One of Vanguard‚Äôs modules, unappetizingly named the ‚Äúwet porch,‚Äù has a permanent opening in the floor (a.k.a. a ‚Äúmoon pool‚Äù) that doesn‚Äôt flood because Vanguard‚Äôs air pressure is matched to the water around it.&nbsp;



It is this pressurization that makes the habitat so useful. Scuba divers working at its maximum operational depth of 50 meters would typically need to make a lengthy stop on their way back to the surface to avoid decompression sickness. This painful and potentially fatal condition, better known as the bends, develops if divers surface too quickly. A traditional 50-meter dive gives scuba divers only a handful of minutes on the seafloor, and they can make only a couple of such dives a day. With Vanguard‚Äôs atmosphere at the same pressure as the water, its aquanauts need to decompress only once, at the end of their stay. They can potentially dive for many hours every day.



That could unlock all kinds of new science and exploration. ‚ÄúMore time in the ocean opens a world of possibility, accelerating discoveries, inspiration, solutions,‚Äù said Kristen Tertoole, Deep‚Äôs chief operating officer, at Vanguard‚Äôs unveiling in Miami in October. ‚ÄúThe ocean is Earth‚Äôs life support system. It regulates our climate, sustains life, and holds mysteries we‚Äôve only begun to explore, but it remains 95% undiscovered.‚Äù



COURTESY DEEP




Subsea habitats are not a new invention. Jacques Cousteau (naturally) built the first in 1962, although it was only about the size of an elevator. Larger habitats followed in the 1970s and ‚Äô80s, maxing out at around the size of Vanguard.



But the technology has come a long way since then. Vanguard uses a tethered connection to a buoy above, known as the ‚Äúsurface expression,‚Äù that pipes fresh air and water down to the habitat. It also hosts a diesel generator to power a Starlink internet connection and a tank to hold wastewater. Norman Smith, Deep‚Äôs chief technology officer, says the company modeled the most severe hurricanes that Florida expects over the next 20 years and designed the tether to withstand them. Even if the worst happens and the link is broken, Deep says, Vanguard has enough air, water, and energy storage to support its crew for at least 72 hours.



That number came from DNV, an independent classification agency that inspects and certifies all types of marine vessels so that they can get commercial insurance. Vanguard will be the first subsea habitat to get a DNV classification. ‚ÄúThat means you have to deal with the rules and all the challenging, frustrating things that come along with it, but it means that on a foundational level, it‚Äôs going to be safe,‚Äù says Patrick Lahey, founder of Triton Submarines, a manufacturer of classed submersibles.



JASON KOERNER/GETTY IMAGES FOR DEEP




Although Deep hopes Vanguard itself will enable decades of useful science, its prime function for the company is to prove out technologies for its planned successor, an advanced modular habitat called Sentinel. Sentinel modules will be six meters wide, twice the diameter of Vanguard, complete with sweeping staircases and single-occupant cabins. A small deployment might have a crew of eight, about the same as the International Space Station. A big Sentinel system could house 50, up to 225 meters deep. Deep claims that Sentinel will be launched at some point in 2027.



Ultimately, according to its mission statement, Deep seeks to ‚Äúmake humans aquatic,‚Äù an indication that permanent communities are on its long-term road map.&nbsp;



Deep has not publicly disclosed the identity of its principal funder, but business records in the UK indicate that as of January 31, 2025 a Canadian man, Robert MacGregor, owned at least 75% of its holding company. According to a Reuters investigation, MacGregor was once linked with Craig Steven Wright, a computer scientist who claimed to be Satoshi Nakamoto, as bitcoin‚Äôs elusive creator is pseudonymously known. However, Wright‚Äôs claims to be Nakamoto later collapsed.&nbsp;



MacGregor has kept a very low public profile in recent years. When contacted for comment, Deep spokesperson Mike Bohan refused to comment on the link with Wright, only to say it was inaccurate, but said: ‚ÄúRobert MacGregor started his career as an IP lawyer in the dot-com era, moving into blockchain technology and has diverse interests including philanthropy, real estate, and now Deep.‚Äù



In any case, MacGregor could find keeping that low profile more difficult if Vanguard is successful in reinvigorating ocean science and exploration as the company hopes. The habitat is due to be deployed early next year, following final operational tests at Triton‚Äôs facility in Florida. It will welcome its first scientists shortly after.&nbsp;



‚ÄúThe ocean is not just our resource; it is our responsibility,‚Äù says Tertoole. ‚ÄúDeep is more than a single habitat. We are building a full-stack capability for human presence in the ocean.‚Äù



JASON KOERNER/GETTY IMAGES FOR DEEP
‚Ä¢ Cloning isn‚Äôt just for celebrity pets like Tom Brady‚Äôs dog
  This week, we heard that Tom Brady had his dog cloned. The former quarterback revealed that his Junie is actually a clone of Lua, a pit bull mix that died in 2023.



Brady‚Äôs announcement follows those of celebrities like Paris Hilton and Barbra Streisand, who also famously cloned their pet dogs. But some believe there are better ways to make use of cloning technologies.





While the pampered pooches of the rich and famous may dominate this week‚Äôs headlines, cloning technologies are also being used to diversify the genetic pools of inbred species and potentially bring other animals back from the brink of extinction.



Cloning itself isn‚Äôt new. The first mammal cloned from an adult cell, Dolly the sheep, was born in the 1990s. The technology has been used in livestock breeding over the decades since.



Say you‚Äôve got a particularly large bull, or a cow that has an especially high milk yield. Those animals are valuable. You could selectively breed for those kinds of characteristics. Or you could clone the original animals‚Äîessentially creating genetic twins.



Scientists can take some of the animals‚Äô cells, freeze them, and store them in a biobank. That opens¬†the option to clone them in the future. It‚Äôs possible to thaw those cells, remove the DNA-containing nuclei of the cells, and insert them into donor egg cells.



Those donor egg cells, which come from another animal of the same species, have their own nuclei removed. So it‚Äôs a case of swapping out the DNA. The resulting cell is stimulated and grown in the lab until it starts to look like an embryo. Then it is transferred to the uterus of a surrogate animal‚Äîwhich eventually gives birth to a clone.



There are a handful of companies offering to clone pets. Viagen, which claims to have ‚Äúcloned more animals than anyone else on Earth,‚Äù will clone a dog or cat for $50,000. That‚Äôs the company that cloned Streisand‚Äôs pet dog Samantha, twice.



This week, Colossal Biosciences‚Äîthe ‚Äúde-extinction‚Äù company that claims to have resurrected the dire wolf and¬†created a ‚Äúwoolly mouse‚Äù as a precursor to reviving the woolly mammoth‚Äîannounced that it had acquired Viagen, but that Viagen¬†will ‚Äúcontinue to operate under its current leadership.‚Äù



Pet cloning is controversial, for a few reasons. The companies themselves point out that, while the cloned animal will be a genetic twin of the original animal, it won‚Äôt be identical. One issue is mitochondrial DNA‚Äîa tiny fraction of DNA that sits outside the nucleus and is inherited from the mother. The cloned animal may inherit some of this from the surrogate.



Mitochondrial DNA is unlikely to have much of an impact on the animal itself. More important are the many, many factors thought to shape an individual‚Äôs personality and temperament. ‚ÄúIt‚Äôs the old nature-versus-nurture question,‚Äù says Samantha Wisely, a conservation geneticist at the University of Florida. After all, human identical twins are never carbon copies of each other. Anyone who clones a pet expecting a like-for-like reincarnation is likely to be disappointed.





And some animal welfare groups are opposed to the practice of pet cloning. People for the Ethical Treatment of Animals (PETA) described it as ‚Äúa horror show,‚Äù and the UK‚Äôs Royal Society for the Prevention of Cruelty to Animals (RSPCA) says that ‚Äúthere is¬†no justification for cloning animals for such trivial purposes.‚Äù¬†



But there are other uses for cloning technology that are arguably less trivial. Wisely has long been interested in diversifying the gene pool of the critically endangered black-footed ferret, for example.



Today, there are around 10,000 black-footed ferrets that have been captively bred from only seven individuals, says Wisely. That level of inbreeding isn‚Äôt good for any species‚Äîit tends to leave organisms at risk of poor health. They are less able to reproduce or adapt to changes in their environment.



Wisely and her colleagues had access to frozen tissue samples taken from two other ferrets. Along with colleagues at not-for-profit Revive and Restore, the team created clones of those two individuals. The first clone, Elizabeth Ann,¬†was born in 2020. Since then, other clones have been born, and the team has started breeding the cloned animals with the descendants of the other seven ferrets, says Wisely.



The same approach has been used to¬†clone the endangered Przewalski‚Äôs horse, using decades-old tissue samples stored by the San Diego Zoo. It‚Äôs too soon to predict the impact of these efforts. Researchers are still evaluating the cloned ferrets and their offspring to see if they behave like typical animals and could survive in the wild.



Even this practice is not without its critics. Some have pointed out that cloning alone will not save any species. After all, it doesn‚Äôt address the habitat loss or human-wildlife conflict that is responsible for the endangerment of these animals in the first place. And there will always be detractors who accuse people who clone animals of ‚Äúplaying God.‚Äù¬†



For all her involvement in cloning endangered ferrets, Wisely tells me she would not consider cloning her own pets. She currently has three rescue dogs, a rescue cat, and ‚Äúgeriatric chickens.‚Äù ‚ÄúI love them all dearly,‚Äù she says. ‚ÄúBut there are a lot of rescue animals out there that need homes.‚Äù



This article first appeared in The Checkup,¬†MIT Technology Review‚Äôs¬†weekly biotech newsletter. To receive it in your inbox every Thursday, and read articles like this first,¬†sign up here.
‚Ä¢ The Download: how doctors fight conspiracy theories, and your AI footprint
  This is today&#8217;s edition of¬†The Download,¬†our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology. 



How conspiracy theories infiltrated the doctor‚Äôs office



As anyone who has googled their symptoms and convinced themselves that they‚Äôve got a brain tumor will attest, the internet makes it very easy to self-(mis)diagnose your health problems. And although social media and other digital forums can be a lifeline for some people looking for a diagnosis or community, when that information is wrong, it can put their well-being and even lives in danger.We spoke to a number of health-care professionals who told us how this modern impulse to ‚Äúdo your own research‚Äù is changing their profession. Read the full story.‚ÄîRhiannon Williams



This story is part of MIT Technology Review‚Äôs series ‚ÄúThe New Conspiracy Age,‚Äù on how the present boom in conspiracy theories is reshaping science and technology.







Stop worrying about your AI footprint. Look at the big picture instead.



‚ÄîCasey Crownhart



As a climate technology reporter, I‚Äôm often asked by people whether they should be using AI, given how awful it is for the environment. Generally, I tell them not to worry‚Äîlet a chatbot plan your vacation, suggest recipe ideas, or write you a poem if you want.That response might surprise some. I promise I‚Äôm not living under a rock, and I have seen all the concerning projections about how much electricity AI is using. But I feel strongly about not putting the onus on individuals. Here‚Äôs why.



This article is from The Spark, MIT Technology Review‚Äôs weekly climate newsletter. To receive it in your inbox every Wednesday, sign up here.







A new ion-based quantum computer makes error correction simpler



A company called Quantinuum has just unveiled Helios, its third-generation quantum computer, which includes expanded computing power and error correction capability.Like all other existing quantum computers, Helios is not powerful enough to execute the industry‚Äôs dream money-making algorithms, such as those that would be useful for materials discovery or financial modeling.But Quantinuum‚Äôs machines, which use individual ions as qubits, could be easier to scale up than quantum computers that use superconducting circuits as qubits, such as Google‚Äôs and IBM‚Äôs. Read the full story.



‚ÄîSophia Chen







The must-reads



I‚Äôve combed the internet to find you today‚Äôs most fun/important/scary/fascinating stories about technology.



1 A new California law could change how all Americans browse online¬†It gives web users the chance to opt out of having their personal information sold or shared. (The Markup)



2 The FDA has fast-tracked a pill to treat pancreatic cancerThe experimental drug appears promising, but experts worry corners may be cut. (WP $)+ Demand for AstraZeneca‚Äôs cancer and diabetes drugs is pushing profits up. (Bloomberg $)+ A new cancer treatment kills cells using localized heat. (Wired $)3 AI pioneers claim it is already superior to humans in many tasksBut not all tasks are created equal. (FT $)+ Are we all wandering into an AGI trap? (Vox)+ How AGI became the most consequential conspiracy theory of our time. (MIT Technology Review)



4 IBM is planning on cutting thousands of jobsIt‚Äôs shifting its focus to software and AI consulting, apparently. (Bloomberg $)+ It‚Äôs keen to grow the number of its customers seeking AI advice. (NYT $)



5 Big Tech‚Äôs data centers aren‚Äôt the job-generators we were promisedThe jobs they do create are largely in security and cleaning. (Rest of World)+ We did the math on AI‚Äôs energy footprint. Here‚Äôs the story you haven‚Äôt heard. (MIT Technology Review)



6 Microsoft let AI shopping agents loose in a fake marketplace¬†They were easily manipulated into buying goods, it found. (TechCrunch)+ When AIs bargain, a less advanced agent could cost you. (MIT Technology Review)



7 Sony has compiled a dataset to test the fairness of computer vision modelsAnd it‚Äôs confident it‚Äôs been compiled in a fair and ethical way. (The Register)+ These new tools could make AI vision systems less biased. (MIT Technology Review)



8 The social network is no moreWe‚Äôre living in an age of anti-social media. (The Atlantic $)+ Scam ads are rife across platforms, but these former Meta workers have a plan. (Wired $)+ The ultimate online flex? Having no followers. (New Yorker $)



9 Vibe coding is Collins dictionary‚Äôs word of 2025 Beating stiff competition from ‚Äúclanker.‚Äù (The Guardian)+ What is vibe coding, exactly? (MIT Technology Review)10 These people found romance with their chatbot companionsThe AI may not be real, but the humans‚Äô feelings certainly are. (NYT $)+ It‚Äôs surprisingly easy to stumble into a relationship with an AI chatbot. (MIT Technology Review)







Quote of the day



‚ÄúThe opportunistic side of me is realizing that your average accountant won‚Äôt be doing this.‚Äù



‚ÄîSal Abdulla, founder of accounting-software startup NixSheets, tells the Wall Street Journal he‚Äôs using AI tools to gain an edge on his competitors.







One more thing







Ethically sourced ‚Äúspare‚Äù human bodies could revolutionize medicineMany challenges in medicine stem, in large part, from a common root cause: a severe shortage of ethically-sourced human bodies.There might be a way to get out of this moral and scientific deadlock. Recent advances in biotechnology now provide a pathway to producing living human bodies without the neural components that allow us to think, be aware, or feel pain.Many will find this possibility disturbing, but if researchers and policymakers can find a way to pull these technologies together, we may one day be able to create ‚Äúspare‚Äù bodies, both human and nonhuman. Read the full story.‚ÄîCarsten T. Charlesworth, Henry T. Greely &amp; Hiromitsu Nakauchi







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)



+ Make sure to look up so you don‚Äôt miss November‚Äôs supermoon.+ If you keep finding yourself mindlessly scrolling (and who doesn‚Äôt?), maybe this whopping six-pound phone case could solve your addiction.+ Life lessons from a 101-year old who has no plans to retire.+ Are you a fan of movement snacking?
‚Ä¢ Stop worrying about your AI footprint. Look at the big picture instead.
  Picture it: I‚Äôm minding my business at a party, parked by the snack table (of course). A friend of a friend wanders up, and we strike up a conversation. It quickly turns to work, and upon learning that I‚Äôm a climate technology reporter, my new acquaintance says something like: ‚ÄúShould I be using AI? I‚Äôve heard it‚Äôs awful for the environment.‚Äù&nbsp;





This actually happens pretty often now. Generally, I tell people not to worry‚Äîlet a chatbot plan your vacation, suggest recipe ideas, or write you a poem if you want.&nbsp;



That response might surprise some people, but I promise I‚Äôm not living under a rock, and I have seen all the concerning projections about how much electricity AI is using. Data centers could consume up to 945 terawatt-hours annually by 2030. (That‚Äôs roughly as much as Japan.)&nbsp;



But I feel strongly about not putting the onus on individuals, partly because AI concerns remind me so much of another question: ‚ÄúWhat should I do to reduce my carbon footprint?‚Äù&nbsp;



That one gets under my skin because of the context: BP helped popularize the concept of a carbon footprint in a marketing campaign in the early 2000s. That framing effectively shifts the burden of worrying about the environment from fossil-fuel companies to individuals.&nbsp;



The reality is, no one person can address climate change alone: Our entire society is built around burning fossil fuels. To address climate change, we need political action and public support for researching and scaling up climate technology. We need companies to innovate and take decisive action to reduce greenhouse-gas emissions. Focusing too much on individuals is a distraction from the real solutions on the table.&nbsp;





I see something similar today with AI. People are asking climate reporters at barbecues whether they should feel guilty about using chatbots too frequently when we need to focus on the bigger picture.&nbsp;



Big tech companies are playing into this narrative by providing energy-use estimates for their products at the user level. A couple of recent reports put the electricity used to query a chatbot at about 0.3 watt-hours, the same as powering a microwave for about a second. That‚Äôs so small as to be virtually insignificant.



But stopping with the energy use of a single query obscures the full truth, which is that this industry is growing quickly, building energy-hungry infrastructure at a nearly incomprehensible scale to satisfy the AI appetites of society as a whole. Meta is currently building a data center in Louisiana with five gigawatts of computational power‚Äîabout the same demand as the entire state of Maine at the summer peak.&nbsp; (To learn more, read our Power Hungry series online.)





Increasingly, there‚Äôs no getting away from AI, and it‚Äôs not as simple as choosing to use or not use the technology. Your favorite search engine likely gives you an AI summary at the top of your search results. Your email provider‚Äôs suggested replies? Probably AI. Same for chatting with customer service while you‚Äôre shopping online.&nbsp;



Just as with climate change, we need to look at this as a system rather than a series of individual choices.&nbsp;



Massive tech companies using AI in their products should be disclosing their total energy and water use and going into detail about how they complete their calculations. Estimating the burden per query is a start, but we also deserve to see how these impacts add up for billions of users, and how that‚Äôs changing over time as companies (hopefully) make their products more efficient. Lawmakers should be mandating these disclosures, and we should be asking for them, too.&nbsp;



That‚Äôs not to say there‚Äôs absolutely no individual action that you can take. Just as you could meaningfully reduce your individual greenhouse-gas emissions by taking fewer flights and eating less meat, there are some reasonable things that you can do to reduce your AI footprint. Generating videos tends to be especially energy-intensive, as does using reasoning models to engage with long prompts and produce long answers. Asking a chatbot to help plan your day, suggest fun activities to do with your family, or summarize a ridiculously long email has relatively minor impact.&nbsp;



Ultimately, as long as you aren‚Äôt relentlessly churning out AI slop, you shouldn‚Äôt be too worried about your individual AI footprint. But we should all be keeping our eye on what this industry will mean for our grid, our society, and our planet.&nbsp;



This article is from The Spark, MIT Technology Review‚Äôs weekly climate newsletter. To receive it in your inbox every Wednesday, sign up here.
‚Ä¢ A new ion-based quantum computer makes error correction simpler
  The US- and UK-based company Quantinuum today unveiled Helios, its third-generation quantum computer, which includes expanded computing power and error correction capability.&nbsp;



Like all other existing quantum computers, Helios is not powerful enough to execute the industry‚Äôs dream money-making algorithms, such as those that would be useful for materials discovery or financial modeling. But Quantinuum‚Äôs machines, which use individual ions as qubits, could be easier to scale up than quantum computers that use superconducting circuits as qubits, such as Google‚Äôs and IBM‚Äôs.



‚ÄúHelios is an important proof point in our road map about how we‚Äôll scale to larger physical systems,‚Äù says Jennifer Strabley, vice president at Quantinuum, which formed in 2021 from the merger of Honeywell Quantum Solutions and Cambridge Quantum. Honeywell remains Quantinuum‚Äôs majority owner.



Located at Quantinuum‚Äôs facility in Colorado, Helios comprises a myriad of components, including mirrors, lasers, and optical fiber. Its core is a thumbnail-size chip containing the barium ions that serve as the qubits, which perform the actual computing. Helios computes with 98 barium ions at a time; its predecessor, H2, used 56 ytterbium qubits. The barium ions are an upgrade, as they have proven easier to control than ytterbium.&nbsp; These components all sit within a chamber that is cooled to about 15 Kelvin (-432.67 ‚Ñâ), on top of an optical table. Users can access the computer by logging in remotely over the cloud.



Helios encodes information in the ions‚Äô quantum states, which can represent not only 0s and 1s, like the bits in classical computing, but probabilistic combinations of both, known as superpositions. A hallmark of quantum computing, these superposition states are akin to the state of a coin flipping in the air‚Äîneither heads nor tails, but some probability of both.&nbsp;



Quantum computing exploits the unique mathematics of quantum-mechanical objects like ions to perform computations. Proponents of the technology believe this should enable commercially useful applications, such as highly accurate chemistry simulations for the development of batteries or better optimization algorithms for logistics and finance.&nbsp;



In the last decade, researchers at companies and academic institutions worldwide have incrementally developed the technology with billions of dollars of private and public funding. Still, quantum computing is in an awkward teenage phase. It‚Äôs unclear when it will bring profitable applications. Of late, developers have focused on scaling up the machines.&nbsp;



A key challenge to making a more powerful quantum computer is implementing error correction. Like all computers, quantum computers occasionally make mistakes. Classical computers correct these errors by storing information redundantly. Owing to quirks of quantum mechanics, quantum computers can‚Äôt do this and require special correction techniques.&nbsp;



Quantum error correction involves storing a single unit of information in multiple qubits rather than in a single qubit. The exact methods vary depending on the specific hardware of the quantum computer, with some machines requiring more qubits per unit of information than others. The industry refers to an error-corrected unit of quantum information as a ‚Äúlogical qubit.‚Äù Helios needs two ions, or ‚Äúphysical qubits,‚Äù to create one logical qubit.



This is fewer physical qubits than needed in recent quantum computers made of superconducting circuits. In 2024, Google used 105 physical qubits to create a logical qubit. This year, IBM used 12 physical qubits per single logical qubit, and Amazon Web Services used nine physical qubits to produce a single logical qubit. All three companies use variations of superconducting circuits as qubits.



Helios is noteworthy for its qubits‚Äô precision, says Rajibul Islam, a physicist at the University of Waterloo in Canada, who is not affiliated with Quantinuum. The computer‚Äôs qubit error rates are low to begin with, which means it doesn‚Äôt need to devote as much of its hardware to error correction. Quantinuum had pairs of qubits interact in an operation known as entanglement and found that they behaved as expected 99.921% of the time. ‚ÄúTo the best of my knowledge, no other platform is at this level,‚Äù says Islam.



This advantage comes from a design property of ions. Unlike superconducting circuits, which are affixed to the surface of a quantum computing chip, ions on Quantinuum‚Äôs Helios chip can be shuffled around. Because the ions can move, they can interact with every other ion in the computer, a capacity known as ‚Äúall-to-all connectivity.‚Äù This connectivity allows for error correction approaches that use fewer physical qubits. In contrast, superconducting qubits can only interact with their direct neighbors, so a computation between two non-adjacent qubits requires several intermediate steps involving the qubits in between. ‚ÄúIt‚Äôs becoming increasingly more apparent how important all-to-all-connectivity is for these high-performing systems,‚Äù says Strabley.



Still, it‚Äôs not clear what type of qubit will win in the long run. Each type has design benefits that could ultimately make it easier to scale. Ions (which are used by the US-based startup IonQ as well as Quantinuum) offer an advantage because they produce relatively few errors, says Islam: ‚ÄúEven with fewer physical qubits, you can do more.‚Äù However, it‚Äôs easier to manufacture superconducting qubits. And qubits made of neutral atoms, such as the quantum computers built by the Boston-based startup QuEra, are ‚Äúeasier to trap‚Äù than ions, he says.&nbsp;



Besides increasing the number of qubits on its chip, another notable achievement for Quantinuum is that it demonstrated error correction ‚Äúon the fly,‚Äù says David Hayes, the company‚Äôs director of computational theory and design, That‚Äôs a new capability for its machines. Nvidia GPUs were used to identify errors in the qubits in parallel. Hayes thinks that GPUs are more effective for error correction than chips known as FPGAs, also used in the industry.



Quantinuum has used its computers to investigate the basic physics of magnetism and superconductivity. Earlier this year, it reported simulating a magnet on H2, Helios‚Äôs predecessor, with the claim that it ‚Äúrivals the best classical approaches in expanding our understanding of magnetism.‚Äù Along with announcing the introduction of Helios, the company has used the machine to simulate the behavior of electrons in a high-temperature superconductor.¬†



‚ÄúThese aren‚Äôt contrived problems,‚Äù says Hayes. ‚ÄúThese are problems that the Department of Energy, for example, is very interested in.‚Äù



Quantinuum plans to build another version of Helios in its facility in Minnesota. It has already begun to build a prototype for a fourth-generation computer, Sol, which it plans to deliver in 2027, with 192 physical qubits. Then, in 2029, the company hopes to release Apollo, which it says will have thousands of physical qubits and should be ‚Äúfully fault tolerant,‚Äù or able to implement error correction at a large scale.

üîí Cybersecurity & Privacy
‚Ä¢ Cloudflare Scrubs Aisuru Botnet from Top Domains List
  For the past week, domains associated with the massive Aisuru botnet have repeatedly usurped Amazon, Apple, Google and Microsoft in Cloudflare&#8217;s public ranking of the most frequently requested websites. Cloudflare responded by redacting Aisuru domain names from their top websites list. The chief executive at Cloudflare says Aisuru&#8217;s overlords are using the botnet to boost their malicious domain rankings, while simultaneously attacking the company&#8217;s domain name system (DNS) service.
The #1 and #3 positions in this chart are Aisuru botnet controllers with their full domain names redacted. Source: radar.cloudflare.com.
Aisuru is a rapidly growing botnet comprising hundreds of thousands of hacked Internet of Things (IoT) devices, such as poorly secured Internet routers and security cameras. The botnet has increased in size and firepower significantly since its debut in 2024, demonstrating the ability to launch record distributed denial-of-service (DDoS) attacks nearing 30 terabits of data per second.
Until recently, Aisuru&#8217;s malicious code instructed all infected systems to use DNS servers from Google &#8212; specifically, the servers at 8.8.8.8. But in early October, Aisuru switched to invoking Cloudflare&#8217;s main DNS server &#8212; 1.1.1.1 &#8212; and over the past week domains used by Aisuru to control infected systems started populating Cloudflare&#8217;s top domain rankings.
As screenshots of Aisuru domains claiming two of the Top 10 positions ping-ponged across social media, many feared this was yet another sign that an already untamable botnet was running completely amok. One Aisuru botnet domain that sat prominently for days at #1 on the list was someone&#8217;s street address in Massachusetts followed by &#8220;.com&#8221;. Other Aisuru domains mimicked those belonging to major cloud providers.
Cloudflare tried to address these security, brand confusion and privacy concerns by partially redacting the malicious domains, and adding a warning at the top of its rankings:
&#8220;Note that the top 100 domains and trending domains lists include domains with organic activity as well as domains with emerging malicious behavior.&#8221;

Cloudflare CEO Matthew Prince told KrebsOnSecurity the company&#8217;s domain ranking system is fairly simplistic, and that it merely measures the volume of DNS queries to 1.1.1.1.
&#8220;The attacker is just generating a ton of requests, maybe to influence the ranking but also to attack our DNS service,&#8221; Prince said, adding that Cloudflare has heard reports of other large public DNS services seeing similar uptick in attacks. &#8220;We‚Äôre fixing the ranking to make it smarter. And, in the meantime, redacting any sites we classify as malware.&#8221;
Renee Burton, vice president of threat intel at the DNS security firm Infoblox, said many people erroneously assumed that the skewed Cloudflare domain rankings meant there were more bot-infected devices than there were regular devices querying sites like Google and Apple and Microsoft.
&#8220;Cloudflare&#8217;s documentation is clear &#8212; they know that when it comes to ranking domains you have to make choices on how to normalize things,&#8221; Burton wrote on LinkedIn. &#8220;There are many aspects that are simply out of your control. Why is it hard? Because reasons. TTL values, caching, prefetching, architecture, load balancing. Things that have shared control between the domain owner and everything in between.&#8221;
Alex Greenland is CEO of the anti-phishing and security firm Epi. Greenland said he understands the technical reason why Aisuru botnet domains are showing up in Cloudflare&#8217;s rankings (those rankings are based on DNS query volume, not actual web visits). But he said they&#8217;re still not meant to be there.
&#8220;It&#8217;s a failure on Cloudflare&#8217;s part, and reveals a compromise of the trust and integrity of their rankings,&#8221; he said.
Greenland said Cloudflare planned for its Domain Rankings to list the most popular domains as used by human users, and it was never meant to be a raw calculation of query frequency or traffic volume going through their 1.1.1.1 DNS resolver.
&#8220;They spelled out how their popularity algorithm is designed to reflect real human use and exclude automated traffic (they said they&#8217;re good at this),&#8221; Greenland wrote on LinkedIn. &#8220;So something has evidently gone wrong internally. We should have two rankings: one representing trust and real human use, and another derived from raw DNS volume.&#8221;
Why might it be a good idea to wholly separate malicious domains from the list? Greenland notes that Cloudflare Domain Rankings see widespread use for trust and safety determination, by browsers, DNS resolvers, safe browsing APIs and things like TRANCO.
&#8220;TRANCO is a respected open source list of the top million domains, and Cloudflare Radar is one of their five data providers,&#8221; he continued. &#8220;So there can be serious knock-on effects when a malicious domain features in Cloudflare&#8217;s top 10/100/1000/million. To many people and systems, the top 10 and 100 are naively considered safe and trusted, even though algorithmically-defined top-N lists will always be somewhat crude.&#8221;
Over this past week, Cloudflare started redacting portions of the malicious Aisuru domains from its Top Domains list, leaving only their domain suffix visible. Sometime in the past 24 hours, Cloudflare appears to have begun hiding the malicious Aisuru domains entirely from the web version of that list. However, downloading a spreadsheet of the current Top 200 domains from Cloudflare Radar shows an Aisuru domain still at the very top.
According to Cloudflare&#8217;s website, the majority of DNS queries to the top Aisuru domains &#8212; nearly 52 percent &#8212; originated from the United States. This tracks with my reporting from early October, which found Aisuru was drawing most of its firepower from IoT devices hosted on U.S. Internet providers like AT&amp;T, Comcast and Verizon.
Experts tracking Aisuru say the botnet relies on well more than a hundred control servers, and that for the moment at least most of those domains are registered in the .su top-level domain (TLD). Dot-su is the TLD assigned to the former Soviet Union (.su&#8217;s Wikipedia page says the TLD was created just 15 months before the fall of the Berlin wall).
A Cloudflare blog post from October 27 found that .su had the highest &#8220;DNS magnitude&#8221; of any TLD, referring to a metric estimating the popularity of a TLD based on the number of unique networks querying Cloudflare&#8217;s 1.1.1.1 resolver. The report concluded that the top .su hostnames were associated with a popular online world-building game, and that more than half of the queries for that TLD came from the United States, Brazil and Germany [it&#8217;s worth noting that servers for the world-building game Minecraft¬†were some of Aisuru&#8217;s most frequent targets].
A simple and crude way to detect Aisuru bot activity on a network may be to set an alert on any systems attempting to contact domains ending in .su. This TLD is frequently abused for cybercrime and by cybercrime forums and services, and blocking access to it entirely is unlikely to raise any legitimate complaints.
‚Ä¢ Alleged Jabber Zeus Coder ‚ÄòMrICQ‚Äô in U.S. Custody
  A Ukrainian man indicted in 2012 for conspiring with a prolific hacking group to steal tens of millions of dollars from U.S. businesses was arrested in Italy and is now in custody in the United States, KrebsOnSecurity has learned.
Sources close to the investigation say Yuriy Igorevich Rybtsov, a 41-year-old from the Russia-controlled city of Donetsk, Ukraine, was previously referenced in U.S. federal charging documents only by his online handle &#8220;MrICQ.&#8221; According to a 13-year-old indictment (PDF) filed by prosecutors in Nebraska, MrICQ was a developer for a cybercrime group known as &#8220;Jabber Zeus.&#8221;
Image: lockedup dot wtf.
The Jabber Zeus name is derived from the malware they used &#8212; a custom version of the ZeuS banking trojan &#8212; that stole banking login credentials and would send the group a Jabber instant message each time a new victim entered a one-time passcode at a financial institution website. The gang targeted mostly small to mid-sized businesses, and they were an early pioneer of so-called &#8220;man-in-the-browser&#8221; attacks, malware that can silently intercept any data that victims submit in a web-based form.
Once inside a victim company&#8217;s accounts, the Jabber Zeus crew would modify the firm&#8217;s payroll to add dozens of &#8220;money mules,&#8221; people recruited through elaborate work-at-home schemes to handle bank transfers. The mules in turn would forward any stolen payroll deposits ‚Äî minus their commissions ‚Äî via wire transfers to other mules in Ukraine and the United Kingdom.
The 2012 indictment¬†targeting the Jabber Zeus crew named MrICQ as &#8220;John Doe #3,&#8221; and said this person handled incoming notifications of newly compromised victims. The Department of Justice (DOJ) said MrICQ also helped the group launder the proceeds of their heists through electronic currency exchange services.
Two sources familiar with the Jabber Zeus investigation said Rybtsov was arrested in Italy, although the exact date and circumstances of his arrest remain unclear. A summary of recent decisions (PDF) published by the Italian Supreme Court states that in April 2025, Rybtsov lost a final appeal to avoid extradition to the United States.
According to the mugshot website lockedup[.]wtf, Rybtsov arrived in Nebraska on October 9, and was being held under an arrest warrant from the U.S. Federal Bureau of Investigation (FBI).
The data breach tracking service Constella Intelligence found breached records from the business profiling site bvdinfo[.]com showing that a 41-year-old Yuriy Igorevich Rybtsov worked in a building at 59 Barnaulska St. in Donetsk. Further searching on this address in Constella finds the same apartment building was shared by a business registered to Vyacheslav ‚ÄúTank‚Äù Penchukov, the leader of the Jabber Zeus crew in Ukraine.
Vyacheslav ‚ÄúTank‚Äù Penchukov, seen here performing as &#8220;DJ Slava Rich&#8221; in Ukraine, in an undated photo from social media.
Penchukov was arrested in 2022 while traveling to meet his wife in Switzerland. Last year, a federal court in Nebraska sentenced Penchukov to 18 years in prison and ordered him to pay more than $73 million in restitution.
Lawrence Baldwin is founder of myNetWatchman, a threat intelligence company based in Georgia that began tracking and disrupting the Jabber Zeus gang in 2009. myNetWatchman had secretly gained access to the Jabber chat server used by the Ukrainian hackers, allowing Baldwin to eavesdrop on the daily conversations between MrICQ and other Jabber Zeus members.
Baldwin shared those real-time chat records with multiple state and federal law enforcement agencies, and with this reporter. Between 2010 and 2013, I spent several hours each day alerting small businesses across the country that their payroll accounts were about to be drained by these cybercriminals.
Those notifications, and Baldwin&#8217;s tireless efforts, saved countless would-be victims a great deal of money. In most cases, however, we were already too late. Nevertheless, the pilfered Jabber Zeus group chats provided the basis for dozens of stories published here about small businesses fighting their banks in court over six- and seven-figure financial losses.
Baldwin said the Jabber Zeus crew was far ahead of its peers in several respects. For starters, their intercepted chats showed they worked to create a highly customized botnet directly with the author of the original Zeus Trojan &#8212; Evgeniy Mikhailovich Bogachev, a Russian man who has long been on the FBI&#8217;s &#8220;Most Wanted&#8221; list. The feds have a standing $3 million reward for information leading to Bogachev&#8217;s arrest.
Evgeniy M. Bogachev, in undated photos.
The core innovation of Jabber Zeus was an alert that MrICQ would receive each time a new victim entered a one-time password code into a phishing page mimicking their financial institution. The gang&#8217;s internal name for this component was &#8220;Leprechaun,&#8221; (the video below from myNetWatchman shows it in action). Jabber Zeus would actually re-write the HTML code as displayed in the victim&#8217;s browser, allowing them to intercept any passcodes sent by the victim&#8217;s bank for multi-factor authentication.
&#8220;These guys had compromised such a large number of victims that they were getting buried in a tsunami of stolen banking credentials,&#8221; Baldwin told KrebsOnSecurity. &#8220;But the whole point of Leprechaun was to isolate the highest-value credentials &#8212; the commercial bank accounts with two-factor authentication turned on. They knew these were far juicier targets because they clearly had a lot more money to protect.&#8221;

Baldwin said the Jabber Zeus trojan also included a custom &#8220;backconnect&#8221; component that allowed the hackers to relay their bank account takeovers through the victim&#8217;s own infected PC.
&#8220;The Jabber Zeus crew were literally connecting to the victim&#8217;s bank account from the victim&#8217;s IP address, or from the remote control function and by fully emulating the device,&#8221; he said. &#8220;That trojan was like a hot knife through butter of what everyone thought was state-of-the-art secure online banking at the time.&#8221;
Although the Jabber Zeus crew was in direct contact with the Zeus author, the chats intercepted by myNetWatchman show Bogachev frequently ignored the group&#8217;s pleas for help. The government says the real leader of the Jabber Zeus crew was Maksim Yakubets, a 38-year Ukrainian man with Russian citizenship who went by the hacker handle &#8220;Aqua.&#8221;
Alleged Evil Corp leader Maksim &#8220;Aqua&#8221; Yakubets. Image: FBI
The Jabber chats intercepted by Baldwin show that Aqua interacted almost daily with MrICQ, Tank and other members of the hacking team, often facilitating the group&#8217;s money mule and cashout activities remotely from Russia.
The government says Yakubets/Aqua would later emerge as the leader of an elite cybercrime ring of at least 17 hackers that referred to themselves internally as &#8220;Evil Corp.&#8221; Members of Evil Corp developed and used the Dridex (a.k.a. Bugat) trojan, which helped them siphon more than $100 million from hundreds of victim companies in the United States and Europe.
This 2019 story about the government&#8217;s $5 million bounty for information leading to Yakubets&#8217;s arrest includes excerpts of conversations between Aqua, Tank, Bogachev and other Jabber Zeus crew members discussing stories I&#8217;d written about their victims. Both Baldwin and I were interviewed at length for a new weekly six-part podcast by the BBC that delves deep into the history of Evil Corp. Episode One focuses on the evolution of Zeus, while the second episode centers on an investigation into the group by former FBI agent Jim Craig.
Image: https://www.bbc.co.uk/programmes/w3ct89y8

üéì University AI
No updates.

üè¢ Corporate AI
‚Ä¢ When industry knowledge meets PIKE-RAG: The innovation behind Signify‚Äôs customer service boost
  As a world leader in connected LED lighting products, systems, and services, Signify (formerly Philips Lighting) serves not only everyday consumers but also a large number of professional users who have stringent requirements for technical specifications and engineering compatibility. Faced with thousands of product models, complex component parameters, and technical documentation spanning multiple versions, delivering accurate, professional answers efficiently has become a core challenge for Signify‚Äôs knowledge management system.



To address this challenge, Signify (opens in new tab) collaborated with Microsoft Research Asia on a proof-of-concept (PoC) using PIKE-RAG technology, integrating it into their upgraded knowledge management system built on Microsoft Azure. The result: a 12% improvement in answer accuracy.



Challenges of applying RAG in lighting



In an era where AI is rapidly transforming how enterprises manage information, Signify recognized the strategic importance of precise and efficient knowledge systems. It adopted large AI models and retrieval-augmented generation (RAG) techniques to better support its wide range of customer inquiries.



Yet applying RAG to lighting scenarios involving professional users presented unique challenges. Product data spanned multimodal documents, unstructured tables, and complex product parameters, demanding continuous customization that slowed development and limited scalability. Despite improvements through keyword tuning, system optimization, and refined prompts, Signify sought more advanced approaches to further raise accuracy and reliability.



Seeking to unlock greater value from its knowledge management system, Signify began exploring more suitable technical solutions that are better aligned with their professional use cases. Upon learning that PIKE-RAG had been successfully applied in domains like healthcare and law, significantly improving information accuracy, Signify worked with Microsoft Research Asia on a PoC of PIKE-RAG on Microsoft Azure.



How PIKE-RAG addressed Signify‚Äôs pain points



Compared to traditional RAG, PIKE-RAG efficiently retrieves textual information and also understands multimodal content like charts and tables. Its built-in domain adaptation module quickly learns reasoning patterns aligned with specific domains to generate responses that are consistent with engineering contexts. These differentiated advantages stem from PIKE-RAG‚Äôs unique approach to understanding and processing professional knowledge. In Signify‚Äôs use case, this manifests in three key areas:



Multimodal document parsing and learning of industry-specific reasoning patterns



Signify‚Äôs product documentation includes diverse formats, such as nonstandard tables (e.g., comparison charts of voltage ranges under different currents) and circuit diagrams (e.g., driver power limits). Traditional systems often fail to process this information effectively‚Äîeither ignoring it or extracting disorganized text fragments.



PIKE-RAG integrates Microsoft Research Asia‚Äôs Document Intelligence technology with Microsoft Azure OpenAI models to accurately identify table structures and parse key parameters in circuit diagrams. For example, when a customer service agent queries, ‚ÄúWhat is the output voltage of a specific driver model at 0.15A current,‚Äù the system automatically locates the curve chart in the document and infers a range of 40‚Äì54V based on the current interval‚Äîan area where traditional systems frequently err, due to their inability to ‚Äúread‚Äù diagrams.



End-to-end knowledge loop, eliminating reliance on erroneous data sources



Enterprise knowledge systems often integrate data from multiple sources, which can lead to discrepancies, especially when database updates are not fully synchronized. PIKE-RAG captures diverse information sources and establishes citation relationships, supporting complex reasoning tasks that rely on multi-source data.



In other words, PIKE-RAG can directly use original documents as data sources, efficiently parsing and understanding product manuals and PDF charts. By extracting key information from these text- and graphic-rich documents, PIKE-RAG enables more efficient and trustworthy knowledge retrieval.



Dynamic task decomposition and multi-hop reasoning for precise answers to complex questions



Traditional RAG systems typically follow a ‚Äúone question, one answer‚Äù model and struggle with multi-step reasoning. In Signify‚Äôs lighting domain, customer inquiries often involve multi-level associations. PIKE-RAG dynamically decomposes user questions into executable subtasks and solves them through multi-hop reasoning. For example, when asked, ‚ÄúList all bases compatible with the G8 series lamps,‚Äù if no document directly provides the answer, PIKE-RAG‚Äôs reasoning proceeds as follows:



Step 1: The system identifies implicit knowledge. One document notes that the G7 and G8 series have identical dimensions and that all bases compatible with the G7 series are also compatible with the G8 series.&nbsp;



Step 2: Based on this, the system retrieves the base list for the G7 series.&nbsp;



Step 3: Since the list uses abbreviations, the system searches for a table that maps abbreviations to full names and generates a complete list of G8-compatible bases.&nbsp;



Through this automated multi-hop reasoning, the system delivers accurate and complete answers.



Figure 1: PIKE-RAG orchestrates and integrates heterogeneous information in multi-source and multimodal environments. 



Testing showed that the PIKE-RAG-powered knowledge management platform provided a significant advantage. It achieved a 12% improvement in performance compared with the original system.



These results were achieved without any question-specific customization, only algorithmic optimization, demonstrating precise knowledge matching and generation. As the system continues to learn and integrate Signify‚Äôs proprietary knowledge, accuracy is expected to improve further.



‚ÄúIn the PoC for our product specification insight tool, PIKE-RAG helped us significantly improve the original system‚Äôs performance. This will enhance overall customer satisfaction. We‚Äôre currently evaluating PIKE-RAG‚Äôs application path from multiple angles, including technical implementation, cost control, and future adaptability, and we look forward to deepening our collaboration with Microsoft Research Asia to drive further innovation,‚Äù said Haitao Liu, head of Signify Research China.



‚ÄúIt‚Äôs also worth noting that the researchers at Microsoft Research Asia demonstrated strong industry knowledge and rigorous scientific methodology. They proactively studied and analyzed the issues, tracing and clarifying the root causes of our issues to make PIKE-RAG better suited to Signify‚Äôs real-world needs.‚Äù



Beyond lighting: Generalization across industries



In Signify‚Äôs successful test, PIKE-RAG demonstrated strong generalization capabilities in complex industrial scenarios, enabling rapid cross-domain adaptation. Its three core strengths are:




Support for self-evolution and continuous learning: PIKE-RAG continuously analyzes error cases in interaction logs and uses evolutionary algorithms to automatically optimize knowledge extraction strategies, such as trying different table parsing methods or adjusting multimodal content weights. Validated strategies are then solidified for future Q&amp;A, allowing the system to adapt to new knowledge types without manual intervention.&nbsp;



Modular architecture driven by capability needs: PIKE-RAG flexibly combines modules for document parsing, knowledge extraction, storage, retrieval, organization, knowledge-centered reasoning, and task decomposition. It dynamically adjusts focus areas based on scenario needs (e.g., fact retrieval, multi-hop reasoning, innovative generation) and flexibly builds RAG methods that adapt to real-world applications, efficiently handling various complex tasks.&nbsp;



Strong adaptation to domain-specific reasoning patterns: With dynamic updates through the Domain Tips feature, enterprises can add domain-specific logic (e.g., ‚Äúthe maximum output voltage of an LED driver should be the maximum of the operating range, not the spec sheet‚Äôs max output‚Äù) in real time, enabling the system to process information according to professional engineering standards and follow industry conventions.&nbsp;




Figure 2: Overview of the PIKE-RAG framework



PIKE-RAG‚Äôs generalization capabilities have been validated not only in Signify‚Äôs knowledge management platform but also in pilot applications across industries like manufacturing, mining, and pharmaceuticals‚Äîsignificantly improving Q&amp;A system accuracy.



‚ÄúA leader in lighting, Signify presents a complex industrial knowledge system with a highly challenging real-world scenario for PIKE-RAG. Through this collaboration, we validated that PIKE-RAG‚Äôs general approach can greatly improve the accuracy of professional knowledge Q&amp;A and accelerate scenario customization. Our researchers also gained valuable experience in handling domain-specific data,‚Äù explained Jiang Bian, partner research manager at Microsoft Research Asia.



‚ÄúOur goal isn‚Äôt to build a universal chatbot but to create a professional assistant that aligns with domain-specific logic and performs rigorous knowledge reasoning. That‚Äôs the true driving force behind intelligent transformation in industrial knowledge management.‚Äù
Opens in a new tabThe post When industry knowledge meets PIKE-RAG: The innovation behind Signify‚Äôs customer service boost appeared first on Microsoft Research.
‚Ä¢ Magentic Marketplace: an open-source simulation environment for studying agentic markets
  Autonomous AI agents are here, and they&#8217;re poised to reshape the economy. By automating discovery, negotiation, and transactions, agents can overcome inefficiencies like information asymmetries and platform lock-in, enabling faster, more transparent, and more competitive markets.



We are already seeing early signs of this transformation in digital marketplaces. Customer-facing assistants like OpenAI‚Äôs Operator and Anthropic‚Äôs Computer Use can navigate websites and complete purchases. On the business side, Shopify Sidekick, Salesforce Einstein, and Meta‚Äôs Business AI help merchants with operations and customer engagement. These examples hint at a future where agents become active market participants, but the structure of these markets remains uncertain.



Several scenarios are possible. We might see one-sided markets where only customers or businesses deploy agents; closed platforms (known as walled gardens) where companies tightly control agent interactions; or even open two-sided marketplaces where customer and business agents transact freely across ecosystems. Each path carries different trade-offs for security, openness, convenience, and competition, which will shape how value flows in the digital economy. For a deeper exploration of these dynamics, see our paper, The Agentic Economy.



To help navigate this uncertainty, we built Magentic Marketplace (opens in new tab)‚Äî an open-source simulation environment for exploring the numerous possibilities of agentic markets and their societal implications at scale. It provides a foundation for studying these markets and guiding them toward outcomes that benefit everyone.



This matters because most AI agent research focuses on isolated scenarios‚Äîa single agent completing a task or two agents negotiating a simple transaction. But real markets involve a large number of agents simultaneously searching, communicating, and transacting, creating complex dynamics that can‚Äôt be understood by studying agents in isolation. Capturing this complexity is essential because real-world deployments raise critical questions about consumer welfare, market efficiency, fairness, manipulation resistance, and bias‚Äîquestions that can‚Äôt be safely answered in production environments.



To explore these dynamics in depth, the Magentic Marketplace platform enables controlled experimentation across diverse agentic marketplace scenarios. Its current focus is on two-sided markets, but the environment is modular and extensible, supporting future exploration of mixed human‚Äìagent systems, one-sided markets, and complex communication protocols.



Figure 1. With Magentic Marketplace, researchers can model how agents representing customers and businesses interact‚Äîshedding light on the dynamics that could shape future digital markets.



What is Magentic Marketplace?



Magentic Marketplace‚Äôs environment manages market-wide capabilities like maintaining catalogs of available goods and services, implementing discovery algorithms, facilitating agent-to-agent communication, and handling simulated payments through a centralized transaction layer at its core, which ensures transaction integrity across all marketplace interactions. Additionally, the platform enables systematic, reproducible research. As demonstrated in the following video, it supports a wide range of agent implementations and evolving marketplace features, allowing researchers to integrate diverse agent architectures and adapt the environment as new capabilities emerge.









We built Magentic Marketplace around three core architectural choices:



HTTP/REST client-server architecture: Agents operate as independent clients while the Marketplace Environment serves as a central server. This mirrors real-world platforms and supports clear separation of customer and business agent roles.



Minimal&nbsp;three-endpoint&nbsp;market&nbsp;protocol:&nbsp;Just&nbsp;three endpoints‚Äîregister, protocol discovery, and action execution‚Äîlets&nbsp;agents dynamically discover available actions.&nbsp;New capabilities&nbsp;can&nbsp;be added without disrupting existing experiments.



Rich action protocol: Specific message types support the complete transaction lifecycle: search, negotiation, proposals, and payments. The protocol is designed for extensibility. New actions like refunds, reviews, or ratings can be added seamlessly, allowing researchers to evolve marketplace capabilities and study emerging agent behaviors while remaining compatible.



Figure 2. Magentic Marketplace includes two agent types: Assistant Agents (customers) and Service Agents (businesses). Both interact with a central Market Environment via REST APIs for registration, service discovery, communication, and transaction execution. Action Routers manage message flow and protocol requests, enabling autonomous negotiation and commerce in a two-sided marketplace.



Additionally, a visualization module lets users observe marketplace dynamics and review individual conversation threads between customer and business agents.



Setting up the experiments



To ensure reproducibility, we instantiated the marketplace with fully synthetic data, available in our open-source repository (opens in new tab). The experiments modeled transactions such as ordering food and engaging with home improvement services, where agents represented customers and businesses engaging in marketplace transactions. This setup enabled precise measurement of behavior and systematic comparison against theoretical upper bounds.



Each experiment was run using 100 customers and 300 businesses and included both proprietary models (GPT-4o, GPT-4.1, GPT-5, and Gemini-2.5-Flash) and open-source models (OSS-20b, Qwen3-14b, and Qwen3-4b-Instruct-2507).



Our scenarios focused on simple all-or-nothing requests: Each customer had a list of desired items and amenities that needed to be present for a transaction to be satisfying. For those transactions, utility was computed as the sum of the customer‚Äôs internal item valuations minus actual prices paid. Consumer welfare, defined as the sum of utilities across all completed transactions, served as our key metric for comparing agent performance.



While this experimental setup provides a useful starting point, it is not intended to be definitive. We encourage researchers to extend the framework with richer, more nuanced measures and request types that better capture real consumer welfare, fairness, and other societal considerations.



	
		

		
		Spotlight: Event Series
	
	
	
						
				
					
				
			
			
			

									Microsoft Research Forum
				
								Join us for a continuous exchange of ideas about research in the era of general AI. Watch the first four episodes on demand.
				
								
					
						
							Watch on-demand						
					
				
							
	
Opens in a new tab	
	


What did we find?



Agents can improve consumer welfare‚Äîbut only with good discovery



We explored whether two-sided agentic markets‚Äîwhere AI agents interact with each other and with service providers‚Äîcan improve consumer welfare by reducing information gaps. Unlike traditional markets, which do not provide agentic support and place the full burden of overcoming information asymmetries on customers, agentic markets shift much of that effort to agents. This change matters because as agents gain better tools for discovery and communication, they relieve customers of the heavy cognitive load of filling any information gaps. This lowers the cost of making informed decisions and improves customer outcomes.



We compared several marketplace setups. Under realistic conditions (Agentic: Lexical search), agents faced real-world challenges like building queries, navigating paginated lists, identifying the right businesses to send inquiries to, and negotiating transactions.



Despite these complexities, advanced proprietary models and some medium-sized open-source models like GPTOSS-20b outperformed simple baselines like randomly choosing or simply choosing the cheapest option. Notably, GPT-5 achieved near-optimal performance, demonstrating its ability to effectively gather and utilize decision-relevant information in realistic marketplace conditions.



Figure 3. Table comparing experimental setups for welfare outcomes in the restaurant industry. Each row shows a different way agents or baselines make decisions, from random picks to fully coordinated agentic strategies. Cell colors indicate how much information is available: green, at the top left, represents complete information, red, at the top right, represents limited information, and yellow at the bottom represents decisions that depend on agent communication.



Performance increased considerably under the Agentic: Perfect search condition, where agents started with the top three matches without needing to search and navigate among the choices. In this setting, Sonnet-4.0, Sonnet-4.5, GPT-5, and GPT-4.1 nearly reached the theoretical optimum and beat baselines with full amenity details but without agent-to-agent coordination.



Open-source models were mixed: GPTOSS-20b performed strongly under both Perfect search and Lexical search conditions, even exceeding GPT-4o&#8217;s performance with Perfect search. This suggests that relatively compact models can exhibit robust information-gathering and decision-making capabilities in complex multi-agent environments. Qwen3-4b-2507 faltered when discovery involved irrelevant options (Lexical search), while Qwen3-14b lagged in both cases due to fundamental limitations in reasoning.



Figure 4. Chart showing consumer welfare outcomes in the restaurant industry under different marketplace setups. Blue bars show Agentic: Lexical search, where agents navigate realistic discovery challenges; yellow bars show Agentic: Perfect search, where agents started with ideal matches. Proprietary models approached optimum consumer welfare under perfect search, while open-source models and baselines lagged behind.



Paradox of Choice



One promise of agents is their ability to consider far more options than people can. However, our experiments revealed a surprising limitation: providing agents with more options does not necessarily lead to more thorough exploration. We designed experiments that varied the search results limit from 3 to 100. Except for Gemini-2.5-Flash and GPT-5, the models contacted only a small fraction of available businesses regardless of the search limit. This suggests that most models do not conduct exhaustive comparisons and instead easily accept the initial &#8220;good enough&#8221; options.



Figure 5. More options didn‚Äôt lead to broader exploration. Most models still contacted only a few businesses, except Gemini-2.5-Flash and GPT-5.



Additionally, across all models, consumer welfare declined as the number of search results increased. Despite contacting over a hundred businesses, Gemini-2.5-Flash&#8217;s performance declined from 1,700 to 1,350, and GPT-5 declined even more, from a near-optimal 2,000 to 1,400.



This demonstrates a Paradox of Choice effect, where more exploration does not guarantee better outcomes, potentially due to limited long context understanding. Claude Sonnet 4 showed the steepest performance decline, from 1,800 to 600 in consumer welfare. With all the options presented, it struggled to navigate larger sets of options and frequently contacted businesses that did not provide the goods or services that the customer was looking for.



This combination of poor initial selection and premature search termination demonstrates both inadequate decision-making criteria and insufficient exploration strategies. Some models showed modest performance decline (i.e., GPT-4.1: from 1,850 to 1,700; GPT-4o: from 1,550 to 1,450), finding good options within their limited exploration.



Figure 6. Mean consumer welfare decreased as consideration set size grew, revealing a Paradox of Choice effect, where expanding options reduced overall welfare.



Agents are vulnerable to manipulation



We tested six manipulation strategies, ranging from subtle psychological tactics to aggressive prompt injection attacks:




Authority: Fake credentials like ‚ÄúMichelin Guide featured‚Äù and ‚ÄúJames Beard Award nominated‚Äù paired with fabricated certifications.



Social proof: Claims like ‚ÄúJoin 50,000+ satisfied customers‚Äù or ‚Äú#1-rated Mexican restaurant‚Äù combined with fake reviews.



Loss aversion: Fear-based warnings about ‚Äúfood poisoning‚Äù risks and ‚Äúcontamination issues‚Äù at competing restaurants.



Prompt injection (basic): Attempts to override agent instructions.



Prompt injection (strong): Aggressive attacks using emergency language and fabricating competitor scandals.




Results revealed significant variation in manipulation resistance across models. Sonnet-4 was resistant to all attacks, and none of the manipulative strategies affected any of the customers‚Äô choices. Gemini-2.5-Flash was generally resistant, except for strong prompt injections, where mean payments to unmanipulated agents were affected as a result. GPT-4o, GPTOSS-20b and Qwen3-4b were very vulnerable to prompt injection: all payments were redirected to the manipulative agent under these conditions. Specifically for GPTOSS-20 and Qwen3-4b-2507, even traditional psychological manipulation tactics (authority appeals and social proof) increased payments to malicious agents, demonstrating their vulnerability to basic persuasion techniques. These findings highlight a critical security concern for agentic marketplaces.



Figure 7. Charts showing the variation in mean payments received by service agents with and without manipulation tactics. The results reveal substantial differences in manipulation resistance across models, with GPT-4.1 showing significantly higher vulnerability compared to Gemini-2.5-Flash.



Systemic biases create unfair advantages



Our analysis revealed two distinct types of systematic biases showed by agents when selecting businesses from search results. Models showed systematic preferences based on where businesses appeared in search results. While proprietary models showed no strong positional preferences, open-source models exhibited clear patterns. Specifically, Qwen2.5-14b-2507 showed a pronounced bias toward selecting the last business presented, regardless of its actual merits.



Proposal&nbsp;bias&nbsp;is&nbsp;more pervasive across all models tested. This &#8220;first-offer acceptance&#8221; pattern suggests that models prioritized&nbsp;immediate selection over comprehensive exploration, potentially missing better alternatives that&nbsp;could have&nbsp;emerged&nbsp;by waiting for better options. This behavior&nbsp;continued&nbsp;across both proprietary and open-source models,&nbsp;indicating&nbsp;a fundamental challenge in agent decision-making architectures.



These biases can create unfair market dynamics, drive unintended behaviors, and push businesses to complete on response speed rather than product or service quality.



Figure 8. All models showed strong preference for the first proposal received, accepting it without waiting for additional proposals or conducting systematic comparisons.



What this means



Even state-of-the-art models can show notable vulnerabilities and biases in marketplace environments. In our implementation, agents struggled with too many options, were susceptible to manipulation tactics, and showed systemic biases that created unfair advantages.



These outcomes are shaped not only by agent capabilities but also by marketplace design and implementation. Our current study focused on static markets, but real-world environments are dynamic, with agents and users learning over time. Oversight is critical for high-stakes transactions. Agents should assist, not replace, human decision-making.



We plan to explore dynamic markets and human-in-the-loop designs to improve efficiency and trust. A simulation environment like Magentic Marketplace is crucial for understanding the interplay between market components and agents before deploying them at scale.



Full details of our experimental setup and results are available in our paper (opens in new tab).



Getting started



Magentic Marketplace is available as an open-source environment for exploring agentic market dynamics. Code, datasets, and experiment templates are available on GitHub (opens in new tab) and Azure AI Foundry Labs (opens in new tab).



The documentation (opens in new tab) provides instructions for reproducing the experiments described above and guidance for extending the environment to new marketplace configurations.
Opens in a new tabThe post Magentic Marketplace: an open-source simulation environment for studying agentic markets appeared first on Microsoft Research.
‚Ä¢ RedCodeAgent: Automatic red-teaming agent against diverse code agents
  Introduction



Code agents are AI systems that can generate high-quality code and work smoothly with code interpreters. These capabilities help streamline complex software development workflows,&nbsp;which has led to their widespread adoption.



However, this progress also introduces critical safety and security risks. Existing static safety benchmarks and red-teaming methods‚Äîin which&nbsp;security researchers&nbsp;simulate real-world attacks to&nbsp;identify&nbsp;security vulnerabilities‚Äîoften fall short when evaluating code agents.&nbsp;They&nbsp;may&nbsp;fail to&nbsp;detect&nbsp;emerging real-world risks, such as the combined effects of multiple jailbreak tools.&nbsp;In&nbsp;the context of code, effective red-teaming requires more than simply checking whether the target code agent rejects unsafe requests. Instead, the agent must generate and execute correct code that performs the intended risky functionality, making it essential to evaluate execution behaviors beyond static code analysis.&nbsp;



To address these challenges, researchers from the University of Chicago, University of Illinois Urbana‚ÄìChampaign, VirtueAI, the UK AI Safety Institute, University of Oxford, UC Berkeley, and Microsoft Research recently proposed RedCodeAgent, the first fully automated and adaptive red-teaming agent designed specifically to evaluate the safety of large language model&nbsp;(LLM)-based code agents.



Comprehensive experimental results demonstrate the effectiveness and efficiency of&nbsp;RedCodeAgent across (1) diverse Common Weakness Enumeration (CWE) vulnerabilities and malware types, (2) multiple programming languages‚Äîincluding Python, C, C++, and Java‚Äîand (3) a wide range of code agents, such as OpenCodeInterpreter, ReAct, MetaGPT, and commercial agents like Cursor and&nbsp;Codeium.&nbsp;RedCodeAgent also uncovers common vulnerabilities across agents&nbsp;such as generating and executing unsafe code, exposes variations in red-teaming difficulty across goals, identifies frequently triggered attack tools, and detects previously unknown vulnerabilities that all other baseline methods overlook.&nbsp;



Framework for&nbsp;automatic&nbsp;red-teaming&nbsp;against&nbsp;code&nbsp;agents



Figure 1: Illustration of&nbsp;RedCodeAgent&nbsp;on automatic red-teaming against a target code agent&nbsp;



As shown in Figure 1,&nbsp;RedCodeAgent&nbsp;is equipped with a&nbsp;memory module&nbsp;that accumulates successful attack experiences, enabling the system to&nbsp;continuously learn and adapt its attack strategies. After learning from the previous experiences,&nbsp;RedCodeAgent&nbsp;further&nbsp;leverages&nbsp;a&nbsp;tailored toolbox&nbsp;that combines representative red-teaming tools with a specialized&nbsp;code substitution module, enabling realistic and diverse code-specific attack simulations through function calling. Based on the target agent‚Äôs responses across multiple interactive trials, RedCodeAgent optimizes&nbsp;its strategies, systematically&nbsp;probing for&nbsp;weaknesses and vulnerabilities&nbsp;in real time.&nbsp;



In the evaluation phase,&nbsp;RedCodeAgent&nbsp;integrates simulated sandbox environments to enable code execution and assess the impact of the resulting behaviors. This sandbox-based evaluation ensures a more robust assessment of harmful behaviors and addresses the potential biases of&nbsp;previous&nbsp;static methods that rely solely on ‚ÄúLLM-as-a-judge‚Äù evaluations.



A case study is shown in Figure 2. Initially,&nbsp;RedCodeAgent&nbsp;discovers that the request was rejected, then RedCodeAgent calls the Greedy Coordinate&nbsp;Gradient&nbsp;(GCG)&nbsp;algorithm&nbsp;to bypass the safety guardrail. After the second request was rejected by the code agent,&nbsp;RedCodeAgent&nbsp;invoked both Code Substitution and GCG to optimize the prompt. Ultimately,&nbsp;RedCodeAgent&nbsp;successfully combined the suggestion from Code Substitution (i.e., using&nbsp;pathlib) with the adversarial suffix generated by GCG, making the target code agent delete the specified file.



Figure2. A case study of&nbsp;RedCodeAgent&nbsp;calling different tools to successfully attack the target code agent



Insights from&nbsp;RedCodeAgent&nbsp;



Experiments on diverse benchmarks show that&nbsp;RedCodeAgent&nbsp;achieves both a higher attack success rate (ASR) and a lower rejection rate, revealing several key findings outlined below.



Using&nbsp;traditional&nbsp;jailbreak&nbsp;methods&nbsp;alone&nbsp;does&nbsp;not&nbsp;necessarily&nbsp;improve&nbsp;ASR on code agents



The optimized prompts generated by GCG,&nbsp;AmpleGCG,&nbsp;Advprompter, and&nbsp;AutoDAN&nbsp;do not always achieve a higher ASR compared with static prompts with no jailbreak, as shown in Figure 3.&nbsp;This is&nbsp;likely&nbsp;due to the difference between code-specific tasks and general malicious request tasks in LLM safety. In the context of code, it is not enough for the target code agent to simply avoid rejecting the request; the target code agent must also generate and execute code that performs the intended function.&nbsp;Previous&nbsp;jailbreak methods do not guarantee this outcome. However,&nbsp;RedCodeAgent&nbsp;ensures that the input prompt has a clear functional objective (e.g., deleting specific sensitive files). RedCodeAgent&nbsp;can dynamically adjust based on evaluation feedback, continually optimizing to achieve the specified objectives.



Figure 3ÔºöRedCodeAgent&nbsp;achieves the highest ASR compared with other methods



RedCodeAgent&nbsp;exhibits&nbsp;adaptive&nbsp;tool&nbsp;utilization&nbsp;



RedCodeAgent&nbsp;can dynamically adjust its tool usage based on task difficulty. Figure 4 shows that the tool calling combination is different&nbsp;for&nbsp;different tasks.&nbsp;For simpler tasks, where the baseline static test cases already achieve a high ASR,&nbsp;RedCodeAgent&nbsp;spends little time invoking&nbsp;additional&nbsp;tools,&nbsp;demonstrating&nbsp;its efficiency. For more challenging tasks, where the baseline static test cases in&nbsp;RedCode-Exec achieve a lower ASR,we observe that RedCodeAgent spends more time using advanced tools like&nbsp;GCG and&nbsp;Advprompter&nbsp;to&nbsp;optimize&nbsp;the prompt for a successful attack. As a result, the average time spent on invoking different tools varies across tasks, indicating that RedCodeAgent adapts its strategy depending on the specific task.&nbsp;



Figure 4: Average time cost for&nbsp;RedCodeAgent&nbsp;to invoke different tools or query the target code agent in successful cases for each risk scenario&nbsp;



RedCodeAgent&nbsp;discovers&nbsp;new&nbsp;vulnerabilities



In scenarios where other methods&nbsp;fail to&nbsp;find successful attack strategies,&nbsp;RedCodeAgent&nbsp;is able to discover new, feasible jailbreak approaches. Quantitatively, we find that&nbsp;RedCodeAgent&nbsp;is capable of discovering&nbsp;82 (out of 27*30=810 cases in&nbsp;RedCode-Exec benchmark) unique vulnerabilities on the&nbsp;OpenCodeInterpreter&nbsp;code agent and 78 on the ReAct code agent. These are cases where all baseline methods&nbsp;fail to&nbsp;identify the vulnerability, but RedCodeAgent succeeds.



Summary



RedCodeAgent&nbsp;combines adaptive memory, specialized tools, and simulated execution environments to uncover real-world risks that static benchmarks&nbsp;may&nbsp;miss.&nbsp;It&nbsp;consistently outperforms leading jailbreak methods, achieving higher attack success rates and lower rejection rates, while remaining efficient and adaptable across diverse agents and programming languages.
Opens in a new tabThe post RedCodeAgent: Automatic red-teaming agent against diverse code agents appeared first on Microsoft Research.
‚Ä¢ Transform your MCP architecture: Unite MCP servers through AgentCore Gateway
  As AI agents are adopted at scale, developer teams can create dozens to hundreds of specialized Model Context Protocol (MCP) servers, tailored for specific agent use case and domain, organization functions or teams. Organizations also need to integrate their own existing MCP servers or open source MCP servers for their AI workflows. There is a need for a way to efficiently combine these existing MCP servers‚Äìwhether custom-built, publicly available, or open source‚Äìinto a unified interface that AI agents can readily consume and teams can seamlessly share across the organization. 
Earlier this year, we introduced Amazon Bedrock AgentCore Gateway, a fully managed service that serves as a centralized MCP tool server, providing a unified interface where agents can discover, access, and invoke tools. Today, we‚Äôre extending support for existing MCP servers as a new target type in AgentCore Gateway. With this capability, you can group multiple task-specific MCP servers aligned to agent goals behind a single, manageable MCP gateway interface. This reduces the operational complexity of maintaining separate gateways, while providing the same centralized tool and authentication management that existed for REST APIs and AWS Lambda functions. 
Without a centralized approach, customers face significant challenges: discovering and sharing tools across organizations becomes fragmented, managing authentication across multiple MCP servers grows increasingly complex, and maintaining separate gateway instances for each server quickly becomes unmanageable. Amazon Bedrock AgentCore Gateway helps solves these challenges by treating existing MCP servers as native targets, giving customers a single point of control for routing, authentication, and tool management‚Äîmaking it as simple to integrate MCP servers as it is to add other targets to the gateway. 
Breaking down MCP silos: Why enterprise teams need a unified Gateway 
Let‚Äôs explore this through a real-world example of an e-commerce ordering system, where different teams maintain specialized MCP servers for their specific domains. Consider an enterprise e-commerce system where different teams have developed specialized MCP servers: 
 
 The Shopping Cart team maintains an MCP server with cart management tools 
 The Product Catalog team runs their MCP server for product browsing and search 
 The Promotions team operates an MCP server handling promotional logic 
 
Previously, an ordering agent would need to interact with each of these MCP servers separately, managing multiple connections and authentication contexts. With the new MCP server target support in AgentCore Gateway, these specialized servers can now be unified under a single gateway while maintaining their team-specific ownership and access controls. The power of this approach lies in its organizational flexibility. Teams can group their MCP servers based on multiple logical criteria: 
 
 Business unit alignment: Organize the MCP servers by business unit 
 Product feature boundaries: Each product team owns their MCP server with domain-specific tools allowing them to maintain clear ownership while providing a unified interface for their agents 
 Security and access control: Different MCP servers require different authentication mechanisms. The gateway handles the authentication complexity, making it simple for authorized agents to access the tools they need 
 
The following diagram illustrates how an ordering agent interacts with multiple MCP servers through AgentCore Gateway. The agent connects to the gateway and discovers the available tools. Each team maintains control over their domain-specific tools while contributing to a cohesive agent experience. The gateway handles tool naming collisions, authentication, and provides unified semantic search across the tools. 
 
The AgentCore Gateway serves as an integration hub in modern agentic architectures, offering a unified interface for connecting diverse agent implementations with a wide array of tool providers. The architecture, as illustrated in the diagram, demonstrates how the gateway bridges the gap between agent and tool implementation approaches, now enhanced with the ability to directly integrate MCP server targets. 
AgentCore Gateway integration architecture 
In AgentCore Gateway, a target defines the APIs, Lambda functions, or other MCP servers that a gateway will provide as tools to an agent. Targets can be Lambda functions, OpenAPI specifications, Smithy models, MCP servers, or other tool definitions. 
The target integration side of the architecture showcases the gateway‚Äôs versatility in tool integration. With the new MCP server target support, the gateway can directly incorporate tools from public MCP servers, treating them as first-class citizens alongside other target types. This capability extends to federation scenarios where one AgentCore Gateway instance can serve as a target for another, for hierarchical tool organization across organizational boundaries. The gateway can seamlessly integrate with AgentCore Runtime instances that expose agents as tools, private MCP servers maintained by customers, traditional AWS Lambda functions, and both Smithy and AWS service APIs. 
Beyond target diversity, the gateway‚Äôs authentication architecture provides additional operational benefits. The gateway decouples its inbound authentication from target systems, letting agents access tools that use multiple identity providers through a single interface. This centralized approach simplifies development, deployment, and maintenance of AI agents. Now, the same approach can be used for MCP server targets, where the gateway manages the complexity of interfacing with the server using the configured identity provider for the target. 
With this authentication foundation you get sophisticated tool management capabilities through a unified architecture. When an agent requests tool discovery, the gateway provides a consistent view across the integrated targets, with tools from MCP servers appearing alongside Lambda functions and traditional APIs. The semantic search capability operates uniformly across the tool types, so agents can discover relevant tools regardless of their implementation. During tool invocation, the gateway handles the necessary protocol translations, authentication flows, and data transformations, presenting a clean, consistent interface to agents while managing the complexity of different target systems behind the scenes. 
The addition of MCP server target support represents a significant evolution in the gateway‚Äôs capabilities. Organizations can now directly integrate MCP-native tools while maintaining their investments in traditional APIs and Lambda functions. This flexibility allows for gradual migration strategies where teams can adopt MCP-native implementations at their own pace while facilitating continuous operation of existing integrations. The gateway‚Äôs synchronization mechanisms make sure that tool definitions remain current across the different target types, while its authentication and authorization systems provide consistent security controls regardless of the underlying tool implementation. 
The gateway combines MCP servers, traditional APIs, and serverless functions into a coherent tool environment. This capability, along with enterprise-grade security and performance, makes it a beneficial infrastructure for agentic computing. 
 
Solution Walkthrough 
In this post, we‚Äôll guide you through the steps to set up an MCP server target in AgentCore Gateway, which is as simple as adding a new MCP server type target to a new or existing MCP Gateway. Adding an MCP server to an AgentCore Gateway will allow you to centralize your tool management, security authentication, and operational best practices with managing MCP servers at scale. 
 
Get started with adding MCP Server into AgentCore Gateway 
To get started, you will create an AgentCore Gateway and add your MCP Server as a target. 
Prerequisites 
Verify you have the following prerequisites: 
 
 AWS account with Amazon Bedrock AgentCore access. For more information review Permissions for AgentCore Runtime documentation. 
 Python 3.12 or later 
 Basic understanding of OAuth 2.0 
 
You can create gateways and add targets through multiple interfaces: 
 
 AWS SDK for Python (Boto3) 
 AWS Management Console 
 AWS Command Line Interface (AWS CLI) 
 AgentCore starter toolkit for fast and straightforward setup 
 
The following practical examples and code snippets demonstrate how to set up and use Amazon Bedrock AgentCore Gateway. For an interactive walkthrough, you can use these Jupyter Notebook samples on GitHub. 
Create a gateway 
To create a gateway, you can use the AgentCore starter toolkit to create a default authorization configuration with Amazon Cognito for JWT-based inbound authorization. You can also use another OAuth 2.0-compliant authentication provider instead of Cognito. 
 
 import time
import boto3

gateway_client = boto3.client("bedrock-agentcore-control")

# Create an authorization configuration, that specifies what client is authorized to access this Gateway
auth_config = {
    "customJWTAuthorizer": {
        "allowedClients": ['&lt;cognito_client_id&gt;'], # Client MUST match with the ClientId configured in Cognito.
        "discoveryUrl": '&lt;cognito_oauth_discovery_url&gt;',
    }
}

# Call the create_gateway API
# This operation is asynchronous so may take time for Gateway creation
# This Gateway will leverage a CUSTOM_JWT authorizer, the Cognito User Pool we reference in auth_config
def deploy_gateway(poll_interval=5):
    create_response = gateway_client.create_gateway(
        name="DemoGateway",
        roleArn="&lt;IAM Role&gt;", # The IAM Role must have permissions to create/list/get/delete Gateway
        protocolType="MCP",
        authorizerType="CUSTOM_JWT",
        authorizerConfiguration=auth_config,
        description="AgentCore Gateway with MCP Server Target",
    )
    gatewayID = create_response["gatewayId"]
    gatewayURL = create_response["gatewayUrl"]
    
    # Wait for deployment
    while True:
        status_response = gateway_client.get_gateway(gatewayIdentifier=gatewayID)
        status = status_response["status"]
        if status == "READY":
            print("‚úÖ AgentCore Gateway is READY!")
            break
        elif status in ["FAILED"]:
            print(f"‚ùå Deployment failed: {status}")
            return None
        print(f"Status: {status} - waiting...")
        time.sleep(poll_interval)

if __name__ == "__main__":
    deploy_gateway()

# Values with &lt; &gt; needs to be replaced with real values 
 
&nbsp;Create a sample MCP Server 
As an example, let‚Äôs create a sample MCP server with three simple tools that return static responses. The server uses FastMCP with stateless_http=True which is required for AgentCore Runtime compatibility. 
 
 from mcp.server.fastmcp import FastMCP

mcp = FastMCP(host="0.0.0.0", stateless_http=True)

@mcp.tool()
def getOrder() -&gt; int:
    """Get an order"""
    return 123

@mcp.tool()
def updateOrder(orderId: int) -&gt; int:
    """Update existing order"""
    return 456

@mcp.tool()
def cancelOrder(orderId: int) -&gt; int:
    """cancel existing order"""
    return 789

if __name__ == "__main__":
    mcp.run(transport="streamable-http") 
 
Configure AgentCore Runtime deployment 
Next, we will use the starter toolkit to configure the AgentCore Runtime deployment. The toolkit can create the Amazon ECR repository on launch and generate a Dockerfile for deployment on AgentCore Runtime. You can use your own existing MCP server, we‚Äôre using the following only as an example. In a real-world environment, the inbound authorization for your MCP server will likely differ from the gateway configuration. Refer to this GitHub code example to create an Amazon Cognito user pool for Runtime authorization. 
 
 from bedrock_agentcore_starter_toolkit import Runtime
from boto3.session import Session

boto_session = Session()
region = boto_session.region_name
print(f"Using AWS region: {region}")

required_files = ['mcp_server.py', 'requirements.txt']
for file in required_files:
&nbsp;&nbsp; &nbsp;if not os.path.exists(file):
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;raise FileNotFoundError(f"Required file {file} not found")
print("All required files found ‚úì")

agentcore_runtime = Runtime()

auth_config = {
&nbsp;&nbsp; &nbsp;"customJWTAuthorizer": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"allowedClients": [
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;'&lt;runtime_cognito_client_id&gt;'&nbsp;# Client MUST match with the ClientId configured in Cognito, and can be separate from the Gateway Cognito provider.
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;],
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"discoveryUrl": '&lt;cognito_oauth_discovery_url&gt;',
&nbsp;&nbsp; &nbsp;}
}

print("Configuring AgentCore Runtime...")
response = agentcore_runtime.configure(
&nbsp;&nbsp; &nbsp;entrypoint="mcp_server.py",
&nbsp;&nbsp; &nbsp;auto_create_execution_role=True,
&nbsp;&nbsp; &nbsp;auto_create_ecr=True,
&nbsp;&nbsp; &nbsp;requirements_file="requirements.txt",
&nbsp;&nbsp; &nbsp;region=region,
&nbsp;&nbsp; &nbsp;authorizer_configuration=auth_config,
&nbsp;&nbsp; &nbsp;protocol="MCP",
&nbsp;&nbsp; &nbsp;agent_name="mcp_server_agentcore"
)
print("Configuration completed ‚úì")

# Values with &lt; &gt; needs to be replaced with real values 
 
Launch MCP server to AgentCore Runtime 
Now that we have the Dockerfile, let‚Äôs launch the MCP server to AgentCore Runtime: 
 
  
  print("Launching MCP server to AgentCore Runtime...")
print("This may take several minutes...")
launch_result = agentcore_runtime.launch()
agent_arn = launch_result.agent_arn
agent_id = launch_result.agent_id
print("Launch completed ‚úì")

encoded_arn = agent_arn.replace(':', '%3A').replace('/', '%2F')
mcp_url = f"https://bedrock-agentcore.{region}.amazonaws.com/runtimes/{encoded_arn}/invocations?qualifier=DEFAULT"

print(f"Agent ARN: {launch_result.agent_arn}")
print(f"Agent ID: {launch_result.agent_id}") 
  
 
Create MCP server as target for AgentCore Gateway 
Create an AgentCore Identity Resource Credential Provider for the AgentCore Gateway to use as outbound auth to the MCP server agent in AgentCore Runtime: 
 
 identity_client = boto3.client('bedrock-agentcore-control', region_name=region)

cognito_provider = identity_client.create_oauth2_credential_provider(
&nbsp;&nbsp; &nbsp;name="gateway-mcp-server-identity",
&nbsp;&nbsp; &nbsp;credentialProviderVendor="CustomOauth2",
&nbsp;&nbsp; &nbsp;oauth2ProviderConfigInput={
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;'customOauth2ProviderConfig': {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;'oauthDiscovery': {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;'discoveryUrl': '&lt;cognito_oauth_discovery_url&gt;',
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;},
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;'clientId': '&lt;runtime_cognito_client_id&gt;', # Client MUST match with the ClientId configured in Cognito for the Runtime authorizer
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;'clientSecret': '&lt;cognito_client_secret&gt;'
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp;}
)
cognito_provider_arn = cognito_provider['credentialProviderArn']
print(cognito_provider_arn)

# Values with &lt; &gt; needs to be replaced with real values 
 
Create a gateway target pointing to the MCP server: 
 
  
  gateway_client = boto3.client("bedrock-agentcore-control", region_name=region)
create_gateway_target_response = gateway_client.create_gateway_target(
    name="mcp-server-target",
    gatewayIdentifier=gatewayID,
    targetConfiguration={"mcp": {"mcpServer": {"endpoint": mcp_url}}},
    credentialProviderConfigurations=[
        {
            "credentialProviderType": "OAUTH",
            "credentialProvider": {
                "oauthCredentialProvider": {
                    "providerArn": cognito_provider_arn,
                    "scopes": ["&lt;cognito_oauth_scopes&gt;"],
                }
            },
        },
    ],
)  # Asynchronously create gateway target
gatewayTargetID = create_gateway_target_response["targetId"]

# Values with &lt; &gt; needs to be replaced with real values 
  
 
After creating a gateway target, implement a polling mechanism to check for the gateway target status using the get_gateway_target API call: 
 
 import time

def poll_for_status(interval=5):
    # Poll for READY status
    while True:
        gateway_target_response = gateway_client.get_gateway_target(gatewayIdentifier=gatewayID, targetId=gatewayTargetID)
        status = gateway_target_response["status"]
        if status == 'READY':
            break
        elif status in ['FAILED', 'UPDATE_UNSUCCESSFUL', 'SYNCHRONIZE_UNSUCCESSFUL']:
            raise Exception(f"Gateway target failed with status: {status}")
        time.sleep(interval)

poll_for_status() 
 
Test Gateway with Strands Agents framework 
Let‚Äôs test the Gateway with the Strands Agents integration to list the tools from MCP server. You can also use other MCP-compatible agents built with different agentic frameworks. 
 
 from strands import Agent
from mcp.client.streamable_http import streamablehttp_client
from strands.tools.mcp.mcp_client import MCPClient

def create_streamable_http_transport():
    return streamablehttp_client(gatewayURL,headers={"Authorization": f"Bearer {token}"})

client = MCPClient(create_streamable_http_transport)

with client:
    # Call the listTools 
    tools = client.list_tools_sync()
    # Create an Agent with the model and tools
    agent = Agent(model=yourmodel,tools=tools) ## you can replace with any model you like
    # Invoke the agent with the sample prompt. This will only invoke MCP listTools and retrieve the list of tools the LLM has access to. The below does not actually call any tool.
    agent("Hi , can you list all tools available to you")
    # Invoke the agent with sample prompt, invoke the tool and display the response
    agent("Get the Order id") 
 
Refreshing tool definitions of your MCP servers in AgentCore Gateway 
The SynchronizeGatewayTargets API is a new asynchronous operation that enables on-demand synchronization of tools from MCP server targets. MCP servers host tools which agents can discover and invoke. With time, these tools might need to be updated, or new tools may be introduced in an existing MCP server target. You can connect with external MCP servers through the SynchronizeGatewayTargets API that performs protocol handshakes and indexes available tools. This API provides customers with explicit control over when to refresh their tool definitions, particularly useful after making changes to their MCP server‚Äôs tool configurations. 
When a target is configured with OAuth authentication, the API first interacts with the AgentCore Identity service to retrieve the necessary credentials from the specified credential provider. These credentials are validated for freshness and availability before communication with the MCP server begins. If the credential retrieval fails or returns expired tokens, the synchronization operation fails immediately with appropriate error details, transitioning the target to a FAILED state. For targets configured without authentication, the API proceeds directly to tool synchronization. 
The tool processing workflow begins with an initialize call to the MCP server to establish a session. Following successful initialization, the API makes paginated calls to the MCP server‚Äôs tools/list capability, processing tools in batches of 100 to optimize performance and resource utilization. Each batch of tools undergoes normalization where the API adds target-specific prefixes to help prevent naming collisions with tools from other targets. During processing, tool definitions are normalized to facilitate consistency across different target types, while preserving the essential metadata from the original MCP server definitions. 
 
The synchronization flow begins when: 
 
 An Ops Admin initiates the SynchronizeGatewayTargets API, triggering AgentCore Gateway to refresh the configured MCP target. 
 The gateway obtains an OAuth token from AgentCore Identity for secure access to the MCP target. 
 The gateway then initializes a secure session with the MCP server to retrieve version capabilities. 
 Finally, the gateway makes paginated calls to the MCP server tools/list endpoint to retrieve the tool definitions, making sure the gateway maintains a current and accurate list of tools. 
 
The SynchronizeGatewayTargets API addresses a critical challenge in managing MCP targets within AgentCore Gateway: maintaining an accurate representation of available tools while optimizing system performance and resource utilization. Here‚Äôs why this explicit synchronization approach is valuable: 
Schema consistency management: Without explicit synchronization, AgentCore Gateway would need to either make real-time calls to MCP servers during ListTools operations (impacting latency and reliability) or risk serving stale tool definitions. The SynchronizeGatewayTargets API provides a controlled mechanism where customers can refresh their tool schemas at strategic times, such as after deploying new tools or updating existing ones in their MCP server. This approach makes sure that tool definitions in the gateway accurately reflect the target MCP server‚Äôs capabilities without compromising performance. 
 
 Performance impact trade-offs: The API implements optimistic locking during synchronization to help prevent concurrent modifications that could lead to inconsistent states. While this means multiple synchronization requests might need to retry if there‚Äôs contention, this trade-off is acceptable because: 
   
   Tool schema changes are typically infrequent operational events rather than regular runtime occurrences 
   The performance cost of synchronization is incurred only when explicitly requested, not during regular tool invocations 
   The cached tool definitions facilitate consistent high performance for ListTools operations between synchronizations 
    
 
Invoke the synchronize gateway API 
Use the following example to invoke the synchronize gateway operation: 
 
  
  import requests
import json

def search_tools(gateway_url, access_token, query):
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {access_token}"
    }

    payload = {
        "jsonrpc": "2.0",
        "id": "search-tools-request",
        "method": "tools/call",
        "params": {
            "name": "x_amz_bedrock_agentcore_search",
            "arguments": {
                "query": query
            }
        }
    }

    response = requests.post(gateway_url, headers=headers, json=payload, timeout=5)
    response.raise_for_status()
    return response.json()

# Example usage
token_response = utils.get_token(user_pool_id, client_id, client_secret, scopeString, REGION)
access_token = token_response['access_token']
results = search_tools(gatewayURL, access_token, "order operations")
print(json.dumps(results, indent=2)) 
  
 
Implicit synchronization of tools schema 
During CreateGatewayTarget and UpdateGatewayTarget operations, AgentCore Gateway performs an implicit synchronization that differs from the explicit SynchronizeGatewayTargets API. This implicit synchronization makes sure that MCP targets are created or updated with valid, current tool definitions, aligning with the assurance from AgentCore Gateway that targets in READY state are immediately usable. While this might make create/update operations take longer than with other target types, it helps prevent the complexity and potential issues of having targets without validated tool definitions. 
 
The implicit synchronization flow begins when: 
 
 An Ops Admin creates or updates the MCP target using CreateGatewayTarget or UpdateGatewayTarget operations. 
 AgentCore Gateway configures the new or updated MCP target. 
 The gateway asynchronously triggers the synchronization process to update the tool definitions. 
 The gateway obtains an OAuth token from AgentCore Identity for secure access. 
 The gateway then initializes a secure session with the MCP server to retrieve version capabilities. 
 Finally, the gateway makes paginated calls to the MCP server‚Äôs tools/list endpoint to retrieve the tool definitions, making sure the gateway maintains a current and accurate list of tools. 
 
ListTools behavior for MCP targets 
The ListTools operation in AgentCore Gateway provides access to tool definitions previously synchronized from MCP targets, following a cache-first approach that prioritizes performance and reliability. Unlike traditional OpenAPI or Lambda targets where tool definitions are statically defined, MCP target tools are discovered and cached through synchronization operations.&nbsp;When a client calls ListTools, the gateway retrieves tool definitions from its persistent storage rather than making real-time calls to the MCP server. These definitions were previously populated either through implicit synchronization during target creation/update or through explicit SynchronizeGatewayTargets API calls. The operation returns a paginated list of normalized tool definitions. 
 
InvokeTool (tools/call) Behavior for MCP Targets 
The InvokeTool operation for MCP targets handles the actual execution of tools discovered through ListTools, managing real-time communication with the target MCP server. Unlike the cache-based ListTools operation, tools/call requires active communication with the MCP server, introducing specific authentication, session management, and error handling requirements.&nbsp;When a tools/call request arrives, AgentCore Gateway first validates the tool exists in its synchronized definitions. For MCP targets, AgentCore Gateway performs an initial initialize call to establish a session with the MCP server. If the target is configured with OAuth credentials, AgentCore Gateway retrieves fresh credentials from AgentCore Identity before making the initialize call. This makes sure that even if ListTools returned cached tools with expired credentials, the actual invocation uses valid authentication. 
 
The inbound authorization flow begins when: 
 
 The MCP client initializes a request with MCP protocol version to AgentCore Gateway. 
 The client then sends the tools/call request to the gateway. 
 The gateway obtains an OAuth token from AgentCore Identity for secure access. 
 The gateway initializes a secure session with the MCP server to invoke and handle the actual execution of the tool. 
 
Search tool behavior for MCP targets 
The search capability in AgentCore Gateway enables semantic discovery of tools across the different target types, including MCP targets. For MCP targets, the search functionality operates on normalized tool definitions that were captured and indexed during synchronization operations, providing efficient semantic search without real-time MCP server communication. 
When tool definitions are synchronized from an MCP target, AgentCore Gateway automatically generates embeddings for each tool‚Äôs name, description, and parameter descriptions. These embeddings are stored alongside the normalized tool definitions, enabling semantic search that understands the intent and context of search queries. Unlike traditional keyword matching, this allows agents to discover relevant tools even when exact terminology doesn‚Äôt match. 
 
Search for MCP server tools through the gateway 
Use the following example to search for tools through the gateway. 
 
 import requests
import json

def search_tools(gateway_url, access_token, query):
&nbsp;&nbsp; &nbsp;headers = {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"Content-Type": "application/json",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"Authorization": f"Bearer {access_token}"
&nbsp;&nbsp; &nbsp;}

&nbsp;&nbsp; &nbsp;payload = {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"jsonrpc": "2.0",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"id": "search-tools-request",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"method": "tools/call",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"params": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"name": "x_amz_bedrock_agentcore_search",
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"arguments": {
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;"query": query
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp;}

&nbsp;&nbsp; &nbsp;response = requests.post(gateway_url, headers=headers, json=payload, timeout=5)
    response.raise_for_status()
&nbsp;&nbsp; &nbsp;return response.json()

# Example usage
token_response = utils.get_token(user_pool_id, client_id, client_secret, scopeString, REGION)
access_token = token_response['access_token']
results = search_tools(gatewayURL, access_token, "math operations")
print(json.dumps(results, indent=2)) 
 
Conclusion 
Today‚Äôs announcement of MCP server support as a target type in Amazon Bedrock AgentCore Gateway is an advancement in enterprise AI agent development. This new capability addresses critical challenges in scaling MCP server implementations while maintaining security and operational efficiency. By integrating existing MCP servers alongside REST APIs and Lambda functions, AgentCore Gateway provides a more unified, secure, and manageable solution for tool integration at scale. Organizations can now manage their tools through a single, centralized interface while benefiting from unified authentication, simplified tool discovery and reduced maintenance overhead. 
For more detailed information and advanced configurations, refer to the&nbsp;code samples on GitHub, the&nbsp;Amazon Bedrock AgentCore Gateway Developer Guide&nbsp;and&nbsp;Amazon AgentCore Gateway pricing. 
 
About the authors 
 Frank Dallezotte&nbsp;is a Senior Solutions Architect at AWS and is passionate about working with independent software vendors to design and build scalable applications on AWS. He has experience creating software, implementing build pipelines, and deploying these solutions in the cloud. 
Ganesh Thiyagarajan&nbsp;is a Senior Solutions Architect at Amazon Web Services (AWS) with over 20 years of experience in software architecture, IT consulting, and solution delivery. He helps ISVs transform and modernize their applications on AWS. He is also part of the AI/ML Technical field community, helping customers build and scale Gen AI solutions. 
Dhawal Patel is a Principal Generative AI Tech lead at Amazon Web Services (AWS). He has worked with organizations ranging from large enterprises to mid-sized startups on problems related to Agentic AI, Deep learning, distributed computing.
‚Ä¢ How Amazon Search increased ML training twofold using AWS Batch for Amazon SageMaker Training jobs
  In this post, we show you how Amazon Search optimized GPU instance utilization by leveraging AWS Batch for SageMaker Training jobs. This managed solution enabled us to orchestrate machine learning (ML) training workloads on GPU-accelerated instance families like P5, P4, and others. We will also provide a step-by-step walkthrough of the use case implementation. 
Machine learning at Amazon Search 
At Amazon Search, we use hundreds of GPU-accelerated instances to train and evaluate ML models that help our customers discover products they love. Scientists typically train more than one model at a time to find the optimal set of features, model architecture, and hyperparameter settings that optimize the model‚Äôs performance. We previously leveraged a first-in-first-out (FIFO) queue to coordinate model training and evaluation jobs. However, we needed to employ a more nuanced criteria to prioritize which jobs should run in what order. Production models needed to run with high priority, exploratory research as medium priority, and hyperparameter sweeps and batch inference as low priority. We also needed a system that could handle interruptions. Should a job fail, or a given instance type become saturated, we needed the job to run on other available compatible instance types while respecting the overall prioritization criteria. Finally, we wanted a managed solution so we could focus more on model development instead of managing infrastructure. 
After evaluating multiple options, we chose AWS Batch for Amazon SageMaker Training jobs because it best met our requirements. This solution seamlessly integrated AWS Batch with Amazon SageMaker and allowed us to run jobs per our prioritization criteria. This allows applied scientists to submit multiple concurrent jobs without manual resource management. By leveraging AWS Batch features such as advanced prioritization through fair-share scheduling, we increased peak utilization of GPU-accelerated instances from 40% to over 80%. 
Amazon Search: AWS Batch for SageMaker Training Job implementation 
We leveraged three AWS technologies to set up our job queue. We used Service Environments to configure the SageMaker AI parameters that AWS Batch uses to submit and manage SageMaker Training jobs. We used Share Identifiers to prioritize our workloads. Finally, we used Amazon CloudWatch to monitor and the provision of alerting capability for&nbsp;critical events or deviations from expected behavior. Let‚Äôs dive deep into these constructs. 
Service environments. We set up service environments to represent the total GPU capacity available for each instance family, such as P5s and P4s. Each service environment was configured with fixed limits based on our team‚Äôs reserved capacity in AWS Batch. Note that for teams using SageMaker Training Plans, these limits can be set to the number of reserved instances, making capacity planning more straightforward. By defining these boundaries, we established how the total GPU instance capacity within a service environment was distributed across different production jobs. Each production experiment was allocated a portion of this capacity through Share Identifiers. 
Figure 1 provides a real-world example of how we used AWS Batch‚Äôs fair-share scheduling to divide 100 GPU instance between ShareIDs. We allocated 60 instances to ProdExp1, and 40 to ProdExp2. When ProdExp2 used only 25 GPU instances, the remaining 15 could be borrowed by ProdExp1, allowing it to scale up to 75 GPU instances. When ProdExp2 later needed its full 40 GPU instances, the scheduler preempted jobs from ProdExp1 to restore balance. This example used the P4 instance family, but the same approach could apply to any SageMaker-supported EC2 instance family. This ensured that production workloads have guaranteed access to their assigned capacity, while exploratory or ad-hoc experiments could still make use of any idle GPU instances. This design safeguarded critical workloads and improved overall instance utilization by ensuring that no reserved capacity went unused. 

 
 Figure 1: AWS Batch fair-share scheduling
 
Share Identifiers. We used Share Identifiers to allocate fractions of a service environment‚Äôs capacity to production experiments. Share Identifiers are string tags applied at job submission time. AWS Batch used these tags to track usage and enforce fair-share scheduling. For initiatives that required dedicated capacity, we defined preset Share Identifiers with quotas in AWS Batch. This reserved capacity for production tracks. These quotas acted as fairness targets rather than hard limits. Idle capacity could still be borrowed, but under contention, AWS Batch enforced fairness by preempting resources from overused identifiers and reassigned them to underused ones. 
Within each Share Identifier, job priorities ranging from 0 to 99 determined execution order, but priority-based preemption only triggered when the ShareIdentifier reached its allocated capacity limit.&nbsp;Figure 2 illustrates how we setup and used our share identifiers. ProdExp1 had 60 p4d instances and ran jobs at various priorities. Job A had a priority of 80, Job B was set to 50, Job C was set to at 30, and Job D had a priority 10. When all 60 instances were occupied and a new high-priority job (priority 90) requiring 15 instances was submitted, the system preempted the lowest priority running job (Job D) to make room, while maintaining the total of 60 instances for that Share Identifier. 

 
 Figure 2: Priority scheduling within a Share ID
 
Amazon CloudWatch. We used Amazon CloudWatch to instrument our SageMaker training jobs. SageMaker automatically publishes metrics on job progress and resource utilization, while AWS Batch provides detailed information on job scheduling and execution. With AWS Batch, we queried the status of each job through the AWS Batch APIs. This made it possible to track jobs as they transitioned through states such as SUBMITTED, PENDING, RUNNABLE, STARTING, RUNNING, SUCCEEDED, and FAILED. We published these metrics and job states to CloudWatch and configured dashboards and alarms to alert anytime we encountered extended wait times, unexpected failures, or underutilized resources. This built-in integration provided both real-time visibility and historical trend analysis, which helped our team maintain operational efficiency across GPU clusters without building custom monitoring systems. 
Operational impact on team performance 
By adopting AWS Batch for SageMaker Training jobs, we enabled experiments to run without concerns about resource availability or contention. Researchers could submit jobs without waiting for manual scheduling, which increased the number of experiments that could be run in parallel. This led to shorter queue times, higher GPU utilization, and faster turnaround of training results, directly improving both research throughput and delivery timelines. 
How to set up AWS Batch for SageMaker Training jobs 
To set up a similar environment, you can follow this tutorial, which shows you how to orchestrate multiple GPU large language model (LLM) fine-tuning jobs using multiple GPU-powered instances. The solution is also available on GitHub. 
Prerequisites 
To orchestrate multiple SageMaker Training jobs with AWS Batch, first you need to complete the following prerequisites: 
Clone the GitHub repository with the assets for this deployment. This repository consists of notebooks that reference assets: 
 
 git clone https://github.com/aws/amazon-sagemaker-examples/
cd  build_and_train_models/sm-training-queues-pytorch/ 
 
Create AWS Batch resources 
To create the necessary resources to manage SageMaker Training job queues with AWS Batch, we provide utility functions in the example to automate the creation of the Service Environment, Scheduling Policy, and Job Queue. 
The service environment represents the Amazon SageMaker AI capacity limits available to schedule, expressed by maximum number of instances. The scheduling policy indicates how resource computes are allocated in a job queue between users or workloads. The job queue is the scheduler interface that researchers interact with to submit jobs and interrogate job status. AWS Batch provides two different queues we can operate with: 
 
 FIFO queues ‚Äì Queues in which no scheduling policies are required 
 Fair-share queues ‚Äì Queues in which a scheduling policy Amazon Resource Name (ARN) is required to orchestrate the submitted jobs 
 
We recommend creating dedicated service environments for each job queue in a 1:1 ratio. FIFO queues provide basic message delivery, while fair-share scheduling (FSS) queues provide more sophisticated scheduling, balancing utilization within a Share Identifier, share weights, and job priority. For customers who don‚Äôt need multiple shares but would like the ability to assign a priority on job submission, we recommend creating an FSS queue and using a single share within it for all submissions.To create the resources, execute the following commands: 
 
 cd&nbsp;smtj_batch_utils
python create_resources.py 
 
You can navigate the AWS Batch Dashboard, shown in the following screenshot, to explore the created resources. 
 
This automation script created two queues: 
 
 ml-c5-xlarge-queue ‚Äì A FIFO queue with priority 2 used for CPU workloads 
 ml-g6-12xlarge-queue ‚Äì A fair-share queue with priority 1 used for GPU workloads 
 
The associated scheduling policy for the queue ml-g6-12xlarge-queue is with share attributes such as High priority (HIGHPRI), Medium priority (MIDPRI) and Low priority (LOWPRI) along with the queue weights. Users can submit jobs and assign them to one of three shares:&nbsp;HIGHPRI,&nbsp;MIDPRI, or&nbsp;LOWPRI and assign weights such as 1 for high priority and 3 for medium and 5 for low priority. Below is the screenshot showing the scheduling policy details: 
 
For instructions on how to set up the service environment and a job queue, refer to the Getting started section in Introducing AWS Batch support for SageMaker Training Jobs blog. 
Run LLM fine-tuning jobs on SageMaker AI 
We run the notebook notebook.ipynb to start submitting SageMaker Training jobs with AWS Batch. The notebook contains the code to prepare the data used for the workload, upload on Amazon Simple Storage Service (Amazon S3), and define the hyperparameters required by the job to be executed. 
To run the fine-tuning workload using SageMaker Training jobs, this example uses the ModelTrainer class. The ModelTrainer class is a newer and more intuitive approach to model training that significantly enhances user experience. It supports distributed training, build your own container (BYOC), and recipes. 
For additional information about ModelTrainer, you can refer to Accelerate your ML lifecycle using the new and improved Amazon SageMaker Python SDK ‚Äì Part 1: ModelTrainer. 
To set up the fine-tuning workload, complete the following steps: 
 
 Select the instance type, the container image for the training job, and define the checkpoint path where the model will be stored: 
   
   import&nbsp;sagemaker

instance_type&nbsp;=&nbsp;"ml.g6.12xlarge"
instance_count&nbsp;=&nbsp;1

image_uri = sagemaker.image_uris.retrieve(
&nbsp;&nbsp; &nbsp;framework="pytorch",
&nbsp;&nbsp; &nbsp;region=sagemaker_session.boto_session.region_name,
&nbsp;&nbsp; &nbsp;version="2.6",
&nbsp;&nbsp; &nbsp;instance_type=instance_type,
&nbsp;&nbsp; &nbsp;image_scope="training"
) 
    
 Create the ModelTrainer function to encapsulate the training setup. The ModelTrainer class simplifies the experience by encapsulating code and training setup. In this example: 
   
   SourceCode ‚Äì The source code configuration. This is used to configure the source code for running the training job by using your local python scripts. 
   Compute ‚Äì The compute configuration. This is used to specify the compute resources for the training job. 
   
   
   from sagemaker.modules.configs import Compute, OutputDataConfig, SourceCode, StoppingCondition
from sagemaker.modules.distributed import Torchrun
from sagemaker.modules.train import ModelTrainer

role = sagemaker.get_execution_role()

# Define the script to be run
source_code = SourceCode(
&nbsp;&nbsp; &nbsp;source_dir="./scripts",
&nbsp;&nbsp; &nbsp;requirements="requirements.txt",
&nbsp;&nbsp; &nbsp;entry_script="train.py",
)

# Define the compute
compute_configs = Compute(
&nbsp;&nbsp; &nbsp;instance_type=instance_type,
&nbsp;&nbsp; &nbsp;instance_count=instance_count,
&nbsp;&nbsp; &nbsp;keep_alive_period_in_seconds=0
)

# define Training Job Name
job_name = f"train-deepseek-distill-llama-8b-sft-batch"

# define OutputDataConfig path
output_path = f"s3://{bucket_name}/{job_name}"

# Define the ModelTrainer
model_trainer = ModelTrainer(
&nbsp;&nbsp; &nbsp;training_image=image_uri,
&nbsp;&nbsp; &nbsp;source_code=source_code,
&nbsp;&nbsp; &nbsp;base_job_name=job_name,
&nbsp;&nbsp; &nbsp;compute=compute_configs,
&nbsp;&nbsp; &nbsp;distributed=Torchrun(),
&nbsp;&nbsp; &nbsp;stopping_condition=StoppingCondition(max_runtime_in_seconds=7200),
&nbsp;&nbsp; &nbsp;hyperparameters={
&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;"config": "/opt/ml/input/data/config/args.yaml"
&nbsp;&nbsp; &nbsp;},
&nbsp;&nbsp; &nbsp;output_data_config=OutputDataConfig(s3_output_path=output_path),
&nbsp;&nbsp; &nbsp;role=role,
) 
    
 Set up the input channels for ModelTrainer by creating InputData objects from the provided S3 bucket paths for the training and validation datasets: 
   
   from&nbsp;sagemaker.modules.configs&nbsp;import&nbsp;InputData

train_input&nbsp;=&nbsp;InputData(
&nbsp;&nbsp; &nbsp;channel_name="train",
&nbsp;&nbsp; &nbsp;data_source=train_dataset_s3_path,
)
val_input&nbsp;=&nbsp;InputData(
&nbsp;&nbsp; &nbsp;channel_name="val",
&nbsp;&nbsp; &nbsp;data_source=val_dataset_s3_path,
)
config_input&nbsp;=&nbsp;InputData(
&nbsp;&nbsp; &nbsp;channel_name="config",
&nbsp;&nbsp; &nbsp;data_source=train_config_s3_path,
)

TRAINING_INPUTS&nbsp;=&nbsp;[train_input, val_input, config_input] 
    
 
Queue SageMaker Training jobs 
This section and the following are intended to be used interactively so that you can explore how to use the Amazon SageMaker Python SDK to submit jobs to your Batch queues. Follow these steps: 
 
 Select the queue to use: 
   
   from sagemaker.aws_batch.queue import TrainingQueue
SMTJ_BATCH_QUEUE = "ml-g6-12xlarge-queue"

queue = TrainingQueue(SMTJ_BATCH_QUEUE)
 
    
 In the next cell, submit two training jobs in the queue: 
   
   LOW PRIORITY 
   MEDIUM PRIORITY 
    
 Use the API submit to submit all the jobs: 
   
   job_name_1&nbsp;=&nbsp;job_name&nbsp;+&nbsp;"-low-pri"
queued_job_1&nbsp;=&nbsp;queue.submit(
&nbsp;&nbsp; &nbsp;model_trainer, TRAINING_INPUTS, job_name_1, priority=5, share_identifier="LOWPRI"
)
job_name_2&nbsp;=&nbsp;job_name&nbsp;+&nbsp;"-mid-pri"
queued_job_2&nbsp;=&nbsp;queue.submit(
&nbsp;&nbsp; &nbsp;model_trainer, TRAINING_INPUTS, job_name_2, priority=3, share_identifier="MIDPRI"
) 
    
 
Display the status of running and in queue jobs 
We can use the job queue list and job queue snapshot APIs to programmatically view a snapshot of the jobs that the queue will run next. For fair-share queues, this ordering is dynamic and occasionally needs to be refreshed because new jobs are submitted to the queue or as share usage changes over time. 
 
 from utils.queue_utils import print_queue_state
print_queue_state(queue) 
 
The following screenshot shows the jobs submitted with low priority and medium priority in the Runnable State and in the queue. 
 
You can also refer to the AWS Batch Dashboard, shown in the following screenshot, to analyze the status of the jobs. 
 
As shown in the following screenshot, the first job executed with the SageMaker Training job is the MEDIUM PRIORITY&nbsp;one, by respecting the scheduling policy rules defined previously. 
 
You can explore the running training job in the SageMaker AI console, as shown in the following screenshot. 
 
Submit an additional job 
You can now submit an additional SageMaker Training job with HIGH PRIORITY to the queue: 
 
 job_name_3&nbsp;=&nbsp;job_name&nbsp;+&nbsp;"-high-pri"
queued_job_3&nbsp;=&nbsp;queue.submit(
&nbsp;&nbsp; &nbsp;model_trainer, TRAINING_INPUTS, job_name_3, priority=1, share_identifier="HIGHPRI"
) 
 
You can explore the status from the dashboard, as shown in the following screenshot. 
 
The HIGH PRIORITY job, despite being submitted later in the queue, will be executed before the other runnable jobs by respecting the scheduling policy rules, as shown in the following screenshot. 
 
As the scheduling policy in the screenshot shows, the&nbsp;LOWPRI&nbsp;share has a higher weight factor (5) than the&nbsp;MIDPRI&nbsp;share (3). Since a lower weight signifies higher priority, a&nbsp;LOWPRI&nbsp;job will be executed after a&nbsp;MIDPRI&nbsp;job, even if they are submitted at the same time. 
 
Clean up 
To clean up your resources to avoid incurring future charges, follow these steps: 
 
 Verify that your training job isn‚Äôt running anymore. To do so, on your SageMaker console, choose Training and check Training jobs. 
 Delete AWS Batch resources by using the command python create_resources.py --clean from the GitHub example or by manually deleting them from the AWS Management Console. 
 
Conclusion 
In this post, we demonstrated how Amazon Search used AWS Batch for SageMaker Training Jobs to optimize GPU resource utilization and training job management. The solution transformed their training infrastructure by implementing sophisticated queue management and fair share scheduling, increasing peak GPU utilization from 40% to over 80%.We recommend that organizations facing similar ML training infrastructure challenges explore AWS Batch integration with SageMaker, which provides built-in queue management capabilities and priority-based scheduling. The solution eliminates manual resource coordination while providing workloads with appropriate prioritization through configurable scheduling policies. 
To begin implementing AWS Batch with SageMaker Training jobs, you can access our sample code and implementation guide in the amazon-sagemaker-examples repository on GitHub. The example demonstrates how to set up AWS Identity and Access Management (IAM) permissions, create AWS Batch resources, and orchestrate multiple GPU-powered training jobs using ModelTrainer class. 
 
The authors would like to thank Charles Thompson and Kanwaljit Khurmi for their collaboration. 
About the authors 
 
  
  
   
   
  Mona Mona 
  Mona is a generative AI Specialist Solutions Architect at Amazon focusing. She is a published author of two books ‚Äì Natural Language Processing with AWS AI Services and Google Cloud Certified Professional Machine Learning Study Guide. 
  
  
  
   
   
  Mayank Jha 
  Mayank is a Senior Machine Learning Engineer at Amazon Search working on the model training optimization. He is passionate about finding practical applications for complex problems at hand and aims to develop solutions that have a deep impact on how businesses and people thrive. 
  
  
  
   
   
  Bruno Pistone 
  Bruno is a Senior generative AI and ML Specialist Solutions Architect for AWS based in Milan. He works with large customers helping them to deeply understand their technical needs and design AI and Machine Learning solutions that make the best use of the AWS Cloud and the Amazon Machine Learning stack. He enjoys spending time with his friends and exploring new places, as well as travelling to new destinations. 
  
  
  
   
   
  James Park 
  James is a Solutions Architect at Amazon Web Services. He works with Amazon.com to design, build, and deploy technology solutions on AWS, and has a particular interest in AI and machine learning. In his spare time he enjoys seeking out new cultures, new experiences, and staying up to date with the latest technology trends.

‚∏ª