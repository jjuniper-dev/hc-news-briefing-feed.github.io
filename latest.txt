‚úÖ Morning News Briefing ‚Äì October 20, 2025 10:47

üìÖ Date: 2025-10-20 10:47
üè∑Ô∏è Tags: #briefing #ai #publichealth #digitalgov

‚∏ª

üßæ Weather
‚Ä¢ Current Conditions:  14.0¬∞C
  Temperature: 14.0&deg;C Pressure / Tendency: 100.1 kPa rising Humidity: 95 % Dewpoint: 13.3&deg:C Wind: S 2 km/h Air Quality Health Index: n/a . Observed at: Pembroke 6:00 AM EDT Monday 20 October 2025 . Weather: 14-14.0/
‚Ä¢ Monday: Showers. High 12.
  Showers ending this afternoon then cloudy then cloudy . Wind becoming northwest 20 km/h gusting to 40 near noon . High 12.5 degrees Celsius . UV index 1 or low, with low-visibility factor of 1 in the UV index of 1 or 0 . Forecast issued 5:00 AM EDT Monday 20 October 2025 . Weather forecasters predict temperatures will reach 12 degrees Celsius
‚Ä¢ Monday night: Mainly cloudy. Low plus 4.
  Cloudy. Becoming partly cloudy this evening. Wind northwest 20 km/h becoming light late this evening . Low plus 4.50C is expected to be cloudy in the early hours of tomorrow morning . Forecast issued 5:00 AM EDT Monday 20 October 2025 . Forecasts issued 5,000C to 5,500C for Monday 20 Oct. 20 Oct 2025. Forecast

üåç International News
No updates.

üçÅ Canadian News
No updates.

üá∫üá∏ U.S. Top Stories
‚Ä¢ As deadline for Trump's colleges compact looms, schools signal dissent
  Of the original nine schools that received the Trump administration's Compact for Academic Excellence in Higher Education, the majority have indicated they are not planning on signing . The majority of those schools have indicated that they will not sign the Compact . The Compact is part of a larger effort to improve the quality of higher education in America's higher education . The compact was created by the Obama administration in 2009 .
‚Ä¢ Can collagen supplements improve your skin? Here's what the research shows
  In wellness circles, the buzz is that collagen supplements can help with wrinkles, joint pain and joint pain . But are these claims something you should swallow? Are you ready to swallow the collagen supplements? Share your thoughts with CNN iReport.com . Back to the page you came from: http://www.dailymailonline.com/news/newsnet/newsport/com/
‚Ä¢ Centrist Rodrigo Paz wins Bolivia's presidential runoff, topping right-wing rival
  Centrist senator Rodrigo Paz won Bolivia's presidency with 54% of the vote . Paz ended 20 years of rule by the Movement Toward Socialism party amid economic turmoil . Centrist Paz ends 20 years in power of the party's former government amid economic woes . Rodrigo won the presidency in Bolivia's first presidential election since 1992 . Paza's victory ends 20-year rule by
‚Ä¢ Trump announces tariffs and an end to U.S. aid to Colombia amid clash over drug trade
  The U.S. will slash assistance to Colombia and enact tariffs on its exports because the country's leader, Gustavo Petro, "does nothing to stop" drug production, President Donald Trump said Sunday . President Trump: Colombia's leader does nothing to "stop" production of drugs . Colombia's drug production has been a major source of cocaine production in the country, the president said .
‚Ä¢ Sam Rivers, bassist and founding member of Limp Bizkit, dies aged 48
  Limp Bizkit: "Sam Rivers wasn't just our bass player ‚Äî he was pure magic" "The pulse beneath every song, the calm in the chaos, the soul in the sound," the band said in a social media post Saturday . Limp said Rivers was "pure magic" and "purely magic" Rivers died at the age of 50 on Saturday at age of

üß† Artificial Intelligence
No updates.

üíª Digital Strategy
‚Ä¢ The real insight behind measuring Copilot usage is Microsoft's desperation
  The quantum theory of management includes an analogy for the physical law of the observer effect, where observing a system changes its state . When you make a metric a target, it is not useful as a metric . Instead of reflecting what underlying behavior it was intended to measure, the metric becomes a measure of how well the benchmark is being gamed . Citizen! You are falling short in your AI
‚Ä¢ Major AWS outage across US-East region breaks half the internet
  Amazon reports DNS issues hitting DynamoDB, leaving services from Roblox to McDonald's struggling . Even Amazon's own web page reported to be offline . Dozens of other online services and websites affected, including disruption in the UK . Amazon Web Services (AWS) reported to have been affected by the issue, with even the company's web page offline . Amazon reports problems with DynamoDB and
‚Ä¢ A simple AI prompt saved a developer from this job interview scam
  Engineer David Dodda says he was just "30 seconds away" from running malware on his own computer . Dodda nearly fell victim to a North Korea-style job interview scam with a "legitimate" blockchain company . Plus: Ransomware posing as Teams installer, Cisco 0-day exploit to drop rootkit, and European cops bust SIM-box service INFOSEC in Europe
‚Ä¢ Tribunal wonders if Microsoft has found a legal hero after pivot to copyright gambit
  ValueLicensing dispute probes whether Office counts as a creative work . Microsoft's attempt to make the case about copyright has taken another turn . Microsoft and Value Licensing are locked in a long-running legal battle over the resale of software licenses . Microsoft is trying to argue that Office is not a work that should be licensed to use in a commercial way . Value licensing and Microsoft are
‚Ä¢ UK calls up Armed Forces veterans for digital ID soft launch
  Armed Forces veterans tasked with proving the government can successfully roll out a digital ID card scheme . The UK's armed forces veterans are being tasked with one last mission ‚Äì proving the scheme can be successfully rolled out . The scheme will be rolled out across the UK for the first time in five years . It will be the first attempt to roll out digital ID cards in the UK's Armed Forces .

üè• Public Health
No updates.

üî¨ Science
‚Ä¢ RNA replicon vaccination confers long-lasting protection against H5N1 avian influenza in 23 zoo bird species
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Factors associated with willingness to use ecological sanitation toilets in Katine sub county Soroti district Uganda: a cross sectional study
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Perceptions of targeted sexual healthcare among French MSM based on HPV vaccination policy in France
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Global landscape of kidney health across Indigenous populations
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations
‚Ä¢ Cardiovascular disease prevention in China: challenges and opportunities in the artificial intelligence-enabled digital health era
  CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery . Please submit your best shots of our featured destinations for next week . Visit CNN iReport.com/Travel next Wednesday for a new gallery of snapshots . Visit www.dailyimpact.com for a gallery next week for snapshots of places to go next week in the gallery . Submit photos of your favorite destinations

üßæ Government & Policy
No updates.

üèõÔ∏è Enterprise Architecture & IT Governance
No updates.

ü§ñ AI & Emerging Tech
‚Ä¢ AI could predict who will have a heart attack
  For all the modern marvels of cardiology, we struggle to predict who will have a heart attack. Many people never get screened at all. Now, startups like Bunkerhill Health, Nanox.AI, and HeartLung Technologies are applying AI algorithms to screen millions of CT scans for early signs of heart disease. This technology could be a breakthrough for public health, applying an old tool to uncover patients whose high risk for a heart attack is hiding in plain sight. But it remains unproven at scale while raising thorny questions about implementation and even how we define disease.¬†



Last year, an estimated 20 million Americans had chest CT scans done, after an event like a car accident or to screen for lung cancer. Frequently, they show evidence of coronary artery calcium (CAC), a marker for heart attack risk, that is buried or not mentioned in a radiology report focusing on ruling out bony injuries, life-threatening internal trauma, or cancer.





Dedicated testing for CAC remains an underutilized method of predicting heart attack risk. Over decades, plaque in heart arteries moves through its own life cycle, hardening from lipid-rich residue into calcium. Heart attacks themselves typically occur when younger, lipid-rich plaque unpredictably ruptures, kicking off a clotting cascade of inflammation that ultimately blocks the heart‚Äôs blood supply. Calcified plaque is generally stable, but finding CAC suggests that younger, more rupture-prone plaque is likely present too.&nbsp;



Coronary artery calcium can often be spotted on chest CTs, and its concentration can be subjectively described. Normally, quantifying a person‚Äôs CAC score involves obtaining a heart-specific CT scan. Algorithms that calculate CAC scores from routine chest CTs, however, could massively expand access to this metric. In practice, these algorithms could then be deployed to alert patients and their doctors about abnormally high scores, encouraging them to seek further care. Today, the footprint of the startups offering AI-derived CAC scores is not large, but it is growing quickly. As their use grows, these algorithms may identify high-risk patients who are traditionally missed or who are on the margins of care.&nbsp;



Historically, CAC scans were believed to have marginal benefit and were marketed to the worried well. Even today, most insurers won‚Äôt cover them. Attitudes, though, may be shifting. More expert groups are endorsing CAC scores as a way to refine cardiovascular risk estimates and persuade skeptical patients to start taking statins.&nbsp;



The promise of AI-derived CAC scores is part of a broader trend toward mining troves of medical data to spot otherwise undetected disease. But while it seems promising, the practice raises plenty of questions. For example, CAC scores ¬≠haven‚Äôt proved useful as a blunt instrument for universal screening. A 2022 Danish study evaluating a population-based program, for example, showed no benefit in mortality rates for patients who had undergone CAC screening tests. If AI delivered this information automatically, would the calculus really shift?&nbsp;



And with widespread adoption, abnormal CAC scores will become common. Who follows up on these findings? ‚ÄúMany health systems aren‚Äôt yet set up to act on incidental calcium findings at scale,‚Äù says Nishith Khandwala, the cofounder of Bunkerhill Health. Without a standard procedure for doing so, he says, ‚Äúyou risk creating more work than value.‚Äù&nbsp;





There‚Äôs also the question of whether these AI-generated scores would actually improve patient care. For a symptomatic patient, a CAC score of zero may offer false reassurance. For the asymptomatic patient with a high CAC score, the next steps remain uncertain. Beyond statins, it isn‚Äôt clear if these patients would benefit from starting costly cholesterol-lowering drugs such as Repatha or other PCSK9-inhibitors. It may encourage some to pursue unnecessary but costly downstream procedures that could even end up doing harm. Currently, AI-derived CAC scoring is not reimbursed as a separate service by Medicare or most insurers. The business case for this technology today, effectively, lies in these potentially perverse incentives.&nbsp;



At a fundamental level, this approach could actually change how we define disease. Adam Rodman, a hospitalist and AI expert at Beth Israel Deaconess Medical Center in Boston, has observed that AI-derived CAC scores share similarities with the ‚Äúincidentaloma,‚Äù a term coined in the 1980s to describe unexpected findings on CT scans. In both cases, the normal pattern of diagnosis‚Äîin which doctors and patients deliberately embark on testing to figure out what‚Äôs causing a specific problem‚Äîwere fundamentally disrupted. But, as Rodman notes, incidentalomas were still found by humans reviewing the scans.&nbsp;



Now, he says, we are entering an era of ‚Äúmachine-based nosology,‚Äù where algorithms define diseases on their own terms. As machines make more diagnoses, they may catch things we miss. But Rodman and I began to wonder if a two-tiered diagnostic future may emerge, where ‚Äúhaves‚Äù pay for brand-name algorithms while ‚Äúhave-nots‚Äù settle for lesser alternatives.&nbsp;



For patients who have no risk factors or are detached from regular medical care, an AI-derived CAC score could potentially catch problems earlier and rewrite the script. But how these scores reach people, what is done about them, and whether they can ultimately improve patient outcomes at scale remain open questions. For now‚Äîholding the pen as they toggle between patients and algorithmic outputs‚Äîclinicians still matter.&nbsp;



Vishal Khetpal is a fellow in cardiovascular disease. The views expressed in this article do not represent those of his employers.&nbsp;
‚Ä¢ Flowers of the future
  Flowers play a key role in most landscapes, from urban to rural areas. There might be dandelions poking through the cracks in the pavement, wildflowers on the highway median, or poppies covering a hillside. We might notice the time of year they bloom and connect that to our changing climate. Perhaps we are familiar with their cycles: bud, bloom, wilt, seed. Yet flowers have much more to tell in their bright blooms: The very shape they take is formed by local and global climate conditions.&nbsp;



The form of a flower is a visual display of its climate, if you know what to look for. In a dry year, its petals‚Äô pigmentation may change. In a warm year, the flower might grow bigger. The flower‚Äôs ultraviolet-absorbing pigment increases with higher ozone levels. As the climate changes in the future, how might flowers change?&nbsp;



Anthocyanins are red or indigo pigments that supply antioxidants and photoprotectants, which help a plant tolerate climate-related stresses such as droughts.¬© 2021 SULLIVAN CN, KOSKI MH




An artistic research project called Plant Futures imagines how a single species of flower might evolve in response to climate change between 2023 and 2100‚Äîand invites us to reflect on the complex, long-term impacts of our warming world. The project has created one flower for every year from 2023 to 2100. The form of each one is data-driven, based on climate projections and research into how climate influences flowers‚Äô visual attributes.&nbsp;



More ultraviolet pigment protects flowers‚Äô pollen against increasing ozone levels.MARCO TODESCO




Under unpredictable weather conditions, the speculative flowers grow a second layer of petals. In botany, a second layer is called a ‚Äúdouble bloom‚Äù and arises from random mutations.COURTESY OF ANNELIE BERNER




Plant Futures began during an artist residency in Helsinki, where I worked closely with the biologist Aku Korhonen to understand how climate change affected the local ecosystem. While exploring the primeval Haltiala forest, I learned of the Circaea alpina, a tiny flower that was once rare in that area but has become more common as temperatures have risen in recent years. Yet its habitat is delicate: The plant requires shade and a moist environment, and the spruce population that provides those conditions is declining in the face of new forest pathogens. I wondered: What if the Circaea alpina could survive in spite of climate uncertainty? If the dark, shaded bogs turn into bright meadows and the wet ground dries out, how might the flower adapt in order to survive? This flower‚Äôs potential became the project‚Äôs grounding point.&nbsp;



The author studying historical Circaea samples in the Luomus Botanical Collections.COURTESY OF ANNELIE BERNER




Outside the forest, I worked with botanical experts in the Luomus Botanical Collections. I studied samples of Circaea flowers from as far back as 1906, and I researched historical climate conditions in an attempt to understand how flower size and color related to a year‚Äôs temperature and precipitation patterns.&nbsp;



I researched how other flowering plants respond to changes to their climate conditions and wondered how the Circaea would need to adapt to thrive in a future world. If such changes happened, what would the Circaea look like in 2100?&nbsp;



We designed the future flowers through a combination of data-driven algorithmic mapping and artistic control. I worked with the data artist Marcin Ignac from Variable Studio to create 3D flowers whose appearance was connected to climate data. Using Nodes.io, we made a 3D model of the Circaea alpina based on its current morphology and then mapped how those physical parameters might shift as the climate changes. For example, as the temperature rises and precipitation decreases in the data set, the petal color shifts toward red, reflecting how flowers protect themselves with an increase in anthocyanins. Changes in temperature, carbon dioxide levels, and precipitation rates combine to affect the flowers‚Äô size, density of veins, UV pigments, color, and tendency toward double bloom.



2025: Circaea alpina is ever so slightly larger than usual owing to a warmer summer, but it is otherwise close to the typical Circaea flower in size, color, and other attributes.



2064: We see a bigger flower with more petals, given an increase in carbon dioxide levels and temperature. The bull‚Äôs-eye pattern, composed of UV pigment, is bigger and messier because of an increase in ozone and solar radiation. A second tier of petals reflects uncertainty in the climate model.



2074: The flower becomes pinker, an antioxidative response to the stress of consecutive dry days and higher temperatures. Its size increases, primarily because of higher levels of carbon dioxide. The double bloom of petals persists as the climate model‚Äôs projections increase in uncertainty.



2100: The flower‚Äôs veins are densely packed, which could signal appropriation of a technique leaves use to improve water transport during droughts. It could also be part of a strategy to attract pollinators in the face of worsening air quality that degrades the transmission of scents.



2023‚Äî2100: Each year, the speculative flower changes. Size, color, and form shift in accordance with the increased temperature and carbon dioxide levels and the changes in precipitation patterns.



In this 10-centimeter cube of plexiglass, the future flowers are ‚Äúpreserved,‚Äù allowing the viewer to see them in a comparative, layered view.COURTESY OF ANNELIE BERNER




Based in Copenhagen, Annelie Berner is a designer, researcher, teacher, and artist specializing in data visualization.
‚Ä¢ The Download: the rehabilitation of AI art, and the scary truth about antimicrobial resistance
  This is today&#8217;s edition of¬†The Download,¬†our weekday newsletter that provides a daily dose of what&#8217;s going on in the world of technology.



From slop to Sotheby‚Äôs? AI art enters a new phase



In this era of AI slop, the idea that generative AI tools like Midjourney and Runway could be used to make art can seem absurd.&nbsp;



But amid all the muck, there are people using AI tools with real consideration and intent. Some of them are finding notable success as AI artists: They are gaining huge online followings, selling their work at auction, and even having it exhibited in galleries and museums. Read the full story.



‚ÄîGrace Huckins



This story is from our forthcoming print issue, which is all about the body. If you haven‚Äôt already, subscribe now to receive future issues once they land. Plus, you&#8217;ll also receive a free digital report on nuclear power.







Take our quiz: How much do you know about antimicrobial resistance?



This week we had some terrifying news from the World Health Organization: Antibiotics are failing us. A growing number of bacterial infections aren‚Äôt responding to these medicines‚Äîincluding common ones that affect the blood, gut, and urinary tract. Get infected with one of these bugs, and there‚Äôs a fair chance antibiotics won‚Äôt help.You‚Äôve probably heard about antimicrobial resistance before, but how much do you know about it? Here‚Äôs our attempt to put the ‚Äúfun‚Äù in ‚Äúfundamental threat to modern medicine.‚Äù Test your knowledge here!



‚ÄîJessica Hamzelou



This article appeared in The Checkup, MIT Technology Review‚Äôs weekly biotech newsletter. To receive it in your inbox every Thursday, sign up here.







2025 climate tech companies to watch: Envision Energy and its ‚Äúsmart‚Äù wind turbines



Envision Energy, one of China‚Äôs biggest wind turbine makers, has expanded into batteries, green hydrogen, and industrial parks designed to run heavy industry on clean power.With flagship projects in Inner Mongolia and new ventures planned abroad, the company is testing whether renewables can decarbonize sectors that electricity alone can‚Äôt reach. Read the full story.Envision Energy is one of our 10 climate tech companies to watch‚Äîour annual list of some of the most promising climate tech firms on the planet. Check out the rest of the list here.







The must-reads



I‚Äôve combed the internet to find you today‚Äôs most fun/important/scary/fascinating stories about technology.



1 ICE is beefing up its surveillance capabilities¬†It‚Äôs recently bought iris-scanning technology, spyware and location tracking software. (WP $)+ Viral ICE videos are shaping how Americans feel about the agency. (Vox)+ Protestors in Chicago are fighting back after mass arrests in the city. (New Yorker $)



2 OpenAI has stopped people from generating videos of MLK JrAfter some people used Sora to create ‚Äúdisrespectful depictions‚Äù of the civil rights activist. (TechCrunch)+ It‚Äôs not the first time AI‚Äôs depiction of public figures has been criticized. (The Information $)3 A teenager is suing the owners of ‚Äúnudifying‚Äù app ClothOffA classmate used an image of the New Jersey student to generate fake nudes. (WSJ $)+ Meet the 15-year-old deepfake victim pushing Congress into action. (MIT Technology Review)



4 Amazon‚Äôs Ring camera arm is signing deals with law enforcementIt‚Äôs working with Flock Safety and Axon to share footage with criminal investigations. (CNBC)+ A division of ICE has used Flock‚Äôs AI-powered surveillance network. (404 Media)+ How Amazon Ring uses domestic violence to market doorbell cameras. (MIT Technology Review)



5 Plug-in hybrids pollute almost as much as diesel carsA new report has found that pollution levels are well above official estimates. (The Guardian)+ What to expect if you‚Äôre expecting a plug-in hybrid. (MIT Technology Review)



6 South Korea is prohibiting its citizens from travelling to CambodiaIt says hundreds of its nationals have been kidnapped and forced into scam complexes. (FT $)+ Inside a romance scam compound‚Äîand how people get tricked into being there. (MIT Technology Review)



7 What it‚Äôs like to be trans online in 2025The internet once helped trans people to connect‚Äînow it‚Äôs being weaponized against them. (The Verge)



8 Generative AI will make you the star of adsCompanies have to make returns on all that AI investment somehow. (NY Mag $)



9 San Francisco‚Äôs AI companies are pushing up housing pricesRents are rising in a city already renowned for a staggeringly high cost of living. (NYT $)



10 Samsung is making a tri-folding phoneBut attendees at the event it‚Äôs being shown off at won‚Äôt be allowed to touch it. (Bloomberg $)







Quote of the day



‚ÄúGrandma will be thrown off the Internet because Junior illegally downloaded a few songs on a visit.‚Äù



‚ÄîUS broadband provider Cox Communications details a potential scenario in a legal case filed by major record labels, which have accused Cox of failing to disconnect people who are illegally downloading music, Ars Technica reports.&nbsp;







One more thing







An AI startup made a hyperrealistic deepfake of me that‚Äôs so good it‚Äôs scaryUntil now, AI-generated videos of people have tended to have some stiffness, glitchiness, or other unnatural elements that make them pretty easy to differentiate from reality.For the past several years, AI video startup Synthesia has produced these kinds of AI-generated avatars. But back in April 2024, it launched a new generation, its first to take advantage of the latest advancements in generative AI, and they are more realistic and expressive than anything we&#8217;ve seen before.¬†



We tested it out by making an AI clone of Melissa Heikkil√§, our former senior AI reporter. Read the full story and check out the synthetic version of Melissa.







We can still have nice things



A place for comfort, fun and distraction to brighten up your day. (Got any ideas? Drop me a line or skeet &#8217;em at me.)



+ As support winds down for Windows 10 this week, did you know its blue Windows icon desktop image was taken from a real photograph? Take a look behind the scenes.+ Rest in power Ace Frehley, Kiss cofounder and undisputed guitar hero.+ A week spent eating along France‚Äôs 385-mile food trail? Yes please.+ As we get into the Halloween spirit, dare you tour America‚Äôs spookiest cities?
‚Ä¢ From slop to Sotheby‚Äôs? AI art enters a new phase
  In this era of AI slop, the idea that generative AI tools like Midjourney and Runway could be used to make art can seem absurd: What possible artistic value is there to be found in the likes of Shrimp Jesus and Ballerina Cappuccina? But amid all the muck, there are people using AI tools with real consideration and intent. Some of them are finding notable success as AI artists: They are gaining huge online followings, selling their work at auction, and even having it exhibited in galleries and museums.&nbsp;





‚ÄúSometimes you need a camera, sometimes AI, and sometimes paint or pencil or any other medium,‚Äù says Jacob Adler, a musician and composer who won the top prize at the generative video company Runway‚Äôs third annual AI Film Festival for his work Total Pixel Space. ‚ÄúIt‚Äôs just one tool that is added to the creator‚Äôs toolbox.‚Äù¬†



One of the most conspicuous features of generative AI tools is their accessibility. With no training and in very little time, you can create an image of whatever you can imagine in whatever style you desire. That‚Äôs a key reason AI art has attracted so much criticism: It‚Äôs now trivially easy to clog sites like Instagram and TikTok with vapid nonsense, and companies can generate images and video themselves instead of hiring trained artists.



Henry Daubrez created these visuals for a bitcoin NFT titled The Order of Satoshi, which sold at Sotheby‚Äôs for $24,000.COURTESY OF THE ARTIST




Henry Daubrez, an artist and designer who created the AI-generated visuals for a bitcoin NFT that sold for $24,000 at Sotheby‚Äôs and is now Google‚Äôs first filmmaker in residence, sees that accessibility as one of generative AI‚Äôs most positive attributes. People who had long since given up on creative expression, or who simply never had the time to master a medium, are now creating and sharing art, he says.¬†



But that doesn‚Äôt mean the first AI-generated masterpiece could come from just anyone. ‚ÄúI don‚Äôt think [generative AI] is going to create an entire generation of geniuses,‚Äù says Daubrez, who has described himself as an ‚ÄúAI-assisted artist.‚Äù Prompting tools like DALL-E and Midjourney might not require technical finesse, but getting those tools to create something interesting, and then evaluating whether the results are any good, takes both imagination and artistic sensibility, he says: ‚ÄúI think we‚Äôre getting into a new generation which is going to be driven by taste.‚Äù&nbsp;



Kira Xonorika‚Äôs Trickster is the first piece to use generative AI in the Denver Art Museum‚Äôs permanent collection.COURTESY OF THE ARTIST




Even for artists who do have experience with other media, AI can be more than just a shortcut. Beth Frey, a trained fine artist who shares her AI art on an Instagram account with over 100,000 followers, was drawn to early generative AI tools because of the uncanniness of their creations‚Äîshe relished the deformed hands and haunting depictions of eating. Over time, the models‚Äô errors have been ironed out, which is part of the reason she hasn‚Äôt posted an AI-generated piece on Instagram in over a year. ‚ÄúThe better it gets, the less interesting it is for me,‚Äù she says. ‚ÄúYou have to work harder to get the glitch now.‚Äù



Beth Frey‚Äôs Instagram account @sentientmuppetfactory features uncanny AI creations.COURTESY OF THE ARTIST




Making art with AI can require relinquishing control‚Äîto the companies that update the tools, and to the tools themselves. For Kira Xonorika, a self-described ‚ÄúAI-collaborative artist‚Äù whose short film Trickster is the first generative AI piece in the Denver Art Museum‚Äôs permanent collection, that lack of control is part of the appeal. ‚Äú[What] I really like about AI is the element of unpredictability,‚Äù says Xonorika, whose work explores themes such as indigeneity and nonhuman intelligence. ‚ÄúIf you‚Äôre open to that, it really enhances and expands ideas that you might have.‚Äù



But the idea of AI as a co-creator‚Äîor even simply as an artistic medium‚Äîis still a long way from widespread acceptance. To many people, ‚ÄúAI art‚Äù and ‚ÄúAI slop‚Äù remain synonymous. And so, as grateful as Daubrez is for the recognition he has received so far, he‚Äôs found that pioneering a new form of art in the face of such strong opposition is an emotional mixed bag. ‚ÄúAs long as it‚Äôs not really accepted that AI is just a tool like any other tool and people will do whatever they want with it‚Äîand some of it might be great, some might not be‚Äîit‚Äôs still going to be sweet [and] sour,‚Äù he says.
‚Ä¢ This startup thinks slime mold can help us design better cities
  It is a yellow blob with no brain, yet some researchers believe a curious organism known as slime mold could help us build more resilient cities.



Humans have been building cities for 6,000 years, but slime mold has been around for 600 million. The team behind a new startup called Mireta wants to translate the organism‚Äôs biological superpowers into algorithms that might help improve transit times, alleviate congestion, and minimize climate-related disruptions in cities worldwide.



Mireta‚Äôs algorithm mimics how slime mold efficiently distributes resources through branching networks. The startup‚Äôs founders think this approach could help connect subway stations, design bike lanes, or optimize factory assembly lines. They claim its software can factor in flood zones, traffic patterns, budget constraints, and more.



‚ÄúIt‚Äôs very rational to think that some [natural] systems or organisms have actually come up with clever solutions to problems we share,‚Äù says Raphael Kay, Mireta‚Äôs cofounder and head of design, who has a background in architecture and mechanical engineering and is currently a PhD candidate in materials science and mechanical engineering at Harvard University.



As urbanization continues‚Äîabout 60% of the global population will live in metropolises by 2030‚Äîcities must provide critical services while facing population growth, aging infrastructure, and extreme weather caused by climate change. Kay, who has also studied how microscopic sea creatures could help researchers design zero-energy buildings, believes nature‚Äôs time-tested solutions may offer a path toward more adaptive urban systems.





Officially known as Physarum polycephalum, slime mold is neither plant, animal, nor fungus but a single-¬≠celled organism older than dinosaurs. When searching for food, it extends tentacle-like projections in multiple directions simultaneously. It then doubles down on the most efficient paths that lead to food while abandoning less productive routes. This process creates optimized networks that balance efficiency with resilience‚Äîa sought-after quality in transportation and infrastructure systems.



The organism‚Äôs ability to find the shortest path between multiple points while maintaining backup connections has made it a favorite among researchers studying network design. Most famously, in 2010 researchers at Hokkaido University reported results from an experiment in which they dumped a blob of slime mold onto a detailed map of Tokyo‚Äôs railway system, marking major stations with oat flakes. At first the brainless organism engulfed the entire map. Days later, it had pruned itself back, leaving behind only the most efficient pathways. The result closely mirrored Tokyo‚Äôs actual rail network.



Since then, researchers worldwide have used slime mold to solve mazes and even map the dark matter holding the universe together. Experts across Mexico, Great Britain, and the Iberian peninsula have tasked the organism with redesigning their roadways‚Äîthough few of these experiments have translated into real-world upgrades.



Historically, researchers working with the organism would print a physical map and add slime mold onto it. But Kay believes that Mireta‚Äôs approach, which replicates slime mold‚Äôs pathway-building without requiring actual organisms, could help solve more complex problems. Slime mold is visible to the naked eye, so Kay‚Äôs team studied how the blobs behave in the lab, focusing on the key behaviors that make these organisms so good at creating efficient networks. Then they translated these behaviors into a set of rules that became an algorithm.



Some experts aren‚Äôt convinced. According to Geoff Boeing, an associate professor at the University of Southern California‚Äôs Department of Urban Planning and Spatial Analysis, such algorithms don‚Äôt address ‚Äúthe messy realities of entering a room with a group of stakeholders and co-visioning a future for their community.‚Äù Modern urban planning problems, he says, aren‚Äôt solely technical issues: ‚ÄúIt‚Äôs not that we don‚Äôt know how to make infrastructure networks efficient, resilient, connected‚Äîit‚Äôs that it‚Äôs politically challenging to do so.‚Äù



Michael Batty, a professor emeritus at University College London‚Äôs Centre for Advanced Spatial Analysis, finds the concept more promising. ‚ÄúThere is certainly potential for exploration,‚Äù he says, noting that humans have long drawn parallels between biological systems and cities. For decades now, designers have looked to nature for ideas‚Äîthink ventilation systems inspired by termite mounds or bullet trains modeled after the kingfisher‚Äôs beak.&nbsp;



Like Boeing, Batty worries that such algorithms could reinforce top-down planning when most cities grow from the bottom up. But for Kay, the algorithm‚Äôs beauty lies in how it mimics bottom-up biological growth‚Äîlike the way slime mold starts from multiple points and connects organically rather than following predetermined paths.&nbsp;



Since launching earlier this year, Mireta, which is based in Cambridge, Massachusetts, has worked on about five projects. And slime mold is just the beginning. The team is also looking at algorithms inspired by ants, which leave chemical trails that strengthen with use and have their own decentralized solutions for network optimization. ‚ÄúBiology has solved just about every network problem you can imagine,‚Äù says Kay.



Elissaveta M. Brandon is an independent journalist interested in how design, culture, and technology shape the way we live.

üîí Cybersecurity & Privacy
‚Ä¢ Email Bombs Exploit Lax Authentication in Zendesk
  Cybercriminals are abusing a widespread lack of authentication in the customer service platform Zendesk to flood targeted email inboxes with menacing messages . The Washington Post, Tinder, CapCom, CompTIA, Discord, GMAC, NordVPN and NordVPN are among the victims of the abuse . The abusive missives can include any subject line chosen by the abusers, such as a supposed law enforcement investigation involving KrebsOnSecurity.com .

üéì University AI
No updates.

üè¢ Corporate AI
‚Ä¢ How TP ICAP transformed CRM data into real-time insights with Amazon Bedrock
  This post is co-written with Ross Ashworth at TP ICAP. 
The ability to quickly extract insights from customer relationship management systems (CRMs) and vast amounts of meeting notes can mean the difference between seizing opportunities and missing them entirely. TP ICAP faced this challenge, having thousands of vendor meeting records stored in their CRM. Using Amazon Bedrock, their Innovation Lab built a production-ready solution that transforms hours of manual analysis into seconds by providing AI-powered insights, using a combination of Retrieval Augmented Generation (RAG) and text-to-SQL approaches. 
This post shows how TP ICAP used Amazon Bedrock Knowledge Bases and Amazon Bedrock Evaluations to build ClientIQ, an enterprise-grade solution with enhanced security features for extracting CRM insights using AI, delivering immediate business value. 
The challenge 
TP ICAP had accumulated tens of thousands of vendor meeting notes in their CRM system over many years. These notes contained rich, qualitative information and details about product offerings, integration discussions, relationship insights, and strategic direction. However, this data was being underutilized and business users were spending hours manually searching through records, knowing the information existed but unable to efficiently locate it. The TP ICAP Innovation Lab set out to make the information more accessible, actionable, and quickly summarized for their internal stakeholders. Their solution needed to surface relevant information quickly, be accurate, and maintain proper context. 
ClientIQ: TP ICAP‚Äôs custom CRM assistant 
With ClientIQ, users can interact with their Salesforce meeting data through natural language queries. For example: 
 
 Ask questions about meeting data in plain English, such as ‚ÄúHow can we improve our relationship with customers?‚Äù, ‚ÄúWhat do our clients think about our solution?‚Äù, or ‚ÄúHow were our clients impacted by Brexit?‚Äù 
 Refine their queries through follow-up questions. 
 Apply filters to restrict model answers to a particular time period. 
 Access source documents directly through links to specific Salesforce records. 
 
ClientIQ provides comprehensive responses while maintaining full traceability by including references to the source data and direct links to the original Salesforce records. The conversational interface supports natural dialogue flow, so users can refine and explore their queries without starting over. The following screenshot shows an example interaction (examples in this post use fictitious data and AnyCompany, a fictitious company, for demonstration purposes). 
 
ClientIQ performs multiple tasks to fulfill a user‚Äôs request: 
 
 It uses a large language model (LLM) to analyze each user query to determine the optimal processing path. 
 It routes requests to one of two workflows: 
   
   The RAG workflow for getting insights from unstructured meeting notes. For example, ‚ÄúWas topic A discussed with AnyCompany the last 14 days?‚Äù 
   The SQL generation workflow for answering analytical queries by querying structured data. For example, ‚ÄúGet me a report on meeting count per region for last 4 weeks.‚Äù 
    
 It then generates the responses in natural language. 
 ClientIQ respects existing permission boundaries and access controls, helping verify users only access the data they‚Äôre authorized to. For example, if a user only has access to their regional accounts in the CRM system, ClientIQ only returns information from these accounts. 
 
Solution overview 
Although the team considered using their CRM‚Äôs built-in AI assistant, they opted to develop a more customized, cost-effective solution that would precisely match their requirements. They partnered with AWS and built an enterprise-grade solution powered by Amazon Bedrock. With Amazon Bedrock, TP ICAP evaluated and selected the best models for their use case and built a production-ready RAG solution in weeks rather than months, without having to manage the underlying infrastructure. They specifically used the following Amazon Bedrock managed capabilities: 
 
 Amazon Bedrock foundation models ‚Äì Amazon Bedrock provides a range of foundation models (FMs) from providers, including Anthropic, Meta, Mistral AI, and Amazon, accessible through a single API. TP ICAP experimented with different models for various tasks and selected the best model for each task, balancing latency, performance, and cost. For instance, they used Anthropic‚Äôs Claude 3.5 Sonnet for classification tasks and Amazon Nova Pro for text-to-SQL generation. Because Amazon Bedrock is fully managed, they didn‚Äôt need to spend time setting up infrastructure for hosting these models, reducing the time to delivery. 
 Amazon Bedrock Knowledge Bases ‚Äì The FMs needed access to the information in TP ICAP‚Äôs Salesforce system to provide accurate, relevant responses. TP ICAP used Amazon Bedrock Knowledge Bases to implement RAG, a technique that enhances generative AI responses by incorporating relevant data from your organization‚Äôs knowledge sources. Amazon Bedrock Knowledge Bases is a fully managed RAG capability with built-in session context management and source attribution. The final implementation delivers precise, contextually relevant responses while maintaining traceability to source documents. 
 Amazon Bedrock Evaluations ‚Äì For consistent quality and performance, the team wanted to implement automated evaluations. By using Amazon Bedrock Evaluations and the RAG evaluation tool for Amazon Bedrock Knowledge Bases in their development environment and CI/CD pipeline, they were able to evaluate and compare FMs with human-like quality. They evaluated different dimensions, including response accuracy, relevance, and completeness, and quality of RAG retrieval. 
 
Since launch, their approach scales efficiently to analyze thousands of responses and facilitates data-driven decision-making about model and inference parameter selection, and RAG configuration.The following diagram showcases the architecture of the solution. 
 
The user query workflow consists of the following steps: 
 
 The user logs in through a frontend React application, hosted in an Amazon Simple Storage Service (Amazon S3) bucket and accessible only within the organization‚Äôs network through an internal-only Application Load Balancer. 
 After logging in, a WebSocket connection is opened between the client and Amazon API Gateway to enable real-time, bi-directional communication. 
 After the connection is established, an AWS Lambda function (connection handler) is invoked, which process the payload, logs tracking data to Amazon DynamoDB, and publishes request data to an Amazon Simple Notification Service (Amazon SNS) topic for downstream processing. 
 Lambda functions for different types of tasks consume messages from Amazon Simple Queue Service (Amazon SQS) for scalable and event-driven processing. 
 The Lambda functions use Amazon Bedrock FMs to determine whether a question is best answered by querying structured data in Amazon Athena or by retrieving information from an Amazon Bedrock knowledge base. 
 After processing, the answer is returned to the user in real time using the existing WebSocket connection through API Gateway. 
 
Data ingestion 
ClientIQ needs to be regularly updated with the latest Salesforce data. Rather than using an off-the-shelf option, TP ICAP developed a custom connector to interface with their highly tailored Salesforce implementation and ingest the latest data to Amazon S3. This bespoke approach provided the flexibility needed to handle their specific data structures while remaining simple to configure and maintain. The connector, which employs Salesforce Object Query Language (SOQL) queries to retrieve the data, runs daily and has proven to be fast and reliable. To optimize the quality of the results during the RAG retrieval workflow, TP ICAP opted for a custom chunking approach in their Amazon Bedrock knowledge base. The custom chunking happens as part of the ingestion process, where the connector splits the data into individual CSV files, one per meeting. These files are also automatically tagged with relevant topics from a predefined list, using Amazon Nova Pro, to further increase the quality of the retrieval results. The final outputs in Amazon S3 contain a CSV file per meeting and a matching JSON metadata file containing tags such as date, division, brand, and region. The following is an example of the associated metadata file: 
 
 {
"metadataAttributes": {
   "Tier": "Bronze",
   "Number_Date_of_Visit": 20171130,
   "Author_Region_C": "AMER",
   "Brand_C": "Credit",
   "Division_C": "Credit",
   "Visiting_City_C": "Chicago",
   "Client_Name": "AnyCompany‚Äù
   }
} 
 
As soon as the data is available in Amazon S3, an AWS Glue job is triggered to populate the AWS Glue Data Catalog. This is later used by Athena when querying the Amazon S3 data. 
The Amazon Bedrock knowledge base is also synced with Amazon S3. As part of this process, each CSV file is converted into embeddings using Amazon Titan v1 and indexed in the vector store, Amazon OpenSearch Serverless. The metadata is also ingested and available for filtering the vector store results during retrieval, as described in the following section. 
Boosting RAG retrieval quality 
In a RAG query workflow, the first step is to retrieve the documents that are relevant to the user‚Äôs query from the vector store and append them to the query as context. Common ways to find the relevant documents include semantic search, keyword search, or a combination of both, referred to as hybrid search. ClientIQ uses hybrid search to first filter documents based on their metadata and then perform semantic search within the filtered results. This pre-filtering provides more control over the retrieved documents and helps disambiguate queries. For example, a question such as ‚Äúfind notes from executive meetings with AnyCompany in Chicago‚Äù can mean meetings with any AnyCompany division that took place in Chicago or meetings with AnyCompany‚Äôs division headquartered in Chicago. 
TP ICAP used the manual metadata filtering capability in Amazon Bedrock Knowledge Bases to implement hybrid search in their vector store, OpenSearch Serverless. With this approach, in the preceding example, the documents are first pre-filtered for ‚ÄúChicago‚Äù as Visiting_City_C. After that, a semantic search is performed to find the documents that contain executive meeting notes for AnyCompany. The final output contains notes from meetings in Chicago, which is what is expected in this case. The team enhanced this functionality further by using the implicit metadata filtering of Amazon Bedrock Knowledge Bases. This capability relies on Amazon Bedrock FMs to automatically analyze the query, understand which values can be mapped to metadata fields, and rewrite the query accordingly before performing the retrieval. 
Finally, for additional precision, users can manually specify filters through the application UI, giving them greater control over their search results. This multi-layered filtering approach significantly improves context and final response accuracy while maintaining fast retrieval speeds. 
Security and access control 
To maintain Salesforce‚Äôs granular permissions model in the ClientIQ solution, TP ICAP implemented a security framework using Okta group claims mapped to specific divisions and regions. When a user signs in, their group claims are attached to their session. When the user asks a question, these claims are automatically matched against metadata fields in Athena or OpenSearch Serverless, depending on the path followed. 
For example, if a user has access to see information for EMEA only, then the documents are automatically filtered by the EMEA region. In Athena, this is done by automatically adjusting the query to include this filter. In Amazon Bedrock Knowledge Bases, this is done by introducing an additional metadata field filter for region=EMEA in the hybrid search. This is highlighted in the following diagram. 
 
Results that don‚Äôt match the user‚Äôs permission tags are filtered out, so that users can only access data they‚Äôre authorized to see. This unified security model maintains consistency between Salesforce permissions and ClientIQ access controls, preserving data governance across solutions. 
The team also developed a custom administrative interface for admins that manage permission in Salesforce to add or remove users from groups using Okta‚Äôs APIs. 
Automated evaluation 
The Innovation Lab team faced a common challenge in building their RAG application: how to scientifically measure and improve its performance. To address that, they developed an evaluation strategy using Amazon Bedrock Evaluations that involves three phrases: 
 
 Ground truth creation ‚Äì They worked closely with stakeholders and testing teams to develop a comprehensive set of 100 representative question answers pairs that mirrored real-world interactions. 
 RAG evaluation ‚Äì In their development environment, they programmatically triggered RAG evaluations in Amazon Bedrock Evaluations to process the ground truth data in Amazon S3 and run comprehensive assessments. They evaluated different chunking strategies, including default and custom chunking, tested different embedding models for retrieval, and compared FMs for generation using a range of inference parameters. 
 Metric-driven optimization ‚Äì Amazon Bedrock generates evaluation reports containing metrics, scores, and insights upon completion of an evaluation job. The team tracked content relevance and content coverage for retrieval and quality, and responsible AI metrics such as response relevance, factual accuracy, retrieval precision, and contextual comprehension for generation. They used the evaluation reports to make optimizations until they reached their performance goals. 
 
The following diagram illustrates this approach. 
 
In addition, they integrated RAG evaluation directly into their continuous integration and continuous delivery (CI/CD) pipeline, so every deployment automatically validates that changes don‚Äôt degrade response quality. The automated testing approach gives the team confidence to iterate quickly while maintaining consistently high standards for the production solution. 
Business outcomes 
ClientIQ has transformed how TP ICAP extracts value from their CRM data. Following the initial launch with 20 users, the results showed that the solution has driven a 75% reduction in time spent on research tasks. Stakeholders also reported an improvement in insight quality, with more comprehensive and contextual information being surfaced. Building on this success, the TP ICAP Innovation Lab plans to evolve ClientIQ into a more intelligent virtual assistant capable of handling broader, more complex tasks across multiple enterprise systems. Their mission remains consistent: to help technical and non-technical teams across the business to unlock business benefits with generative AI. 
Conclusion 
In this post, we explored how the TP ICAP Innovation Lab team used Amazon Bedrock FMs, Amazon Bedrock Knowledge Bases, and Amazon Bedrock Evaluations to transform thousands of meeting records from an underutilized resource into a valuable asset and accelerate time to insights while maintaining enterprise-grade security and governance. Their success demonstrates that with the right approach, businesses can implement production-ready AI solutions and deliver business value in weeks. To learn more about building similar solutions with Amazon Bedrock, visit the&nbsp;Amazon Bedrock documentation or discover real-world success stories and implementations on the&nbsp;AWS Financial Services Blog. 
 
About the authors 
Ross Ashworth works in TP ICAP‚Äôs AI Innovation Lab, where he focuses on enabling the business to harness Generative AI across a range of projects. With over a decade of experience working with AWS technologies, Ross brings deep technical expertise to designing and delivering innovative, practical solutions that drive business value. Outside of work, Ross is a keen cricket fan and former amateur player. He is now a member at The Oval, where he enjoys attending matches with his family, who also share his passion for the sport. 
Anastasia Tzeveleka is a Senior Generative AI/ML Specialist Solutions Architect at AWS. Her experience spans the entire AI lifecycle, from collaborating with organizations training cutting-edge Large Language Models (LLMs) to guiding enterprises in deploying and scaling these models for real-world applications. In her spare time, she explores new worlds through fiction.
‚Ä¢ Principal Financial Group accelerates build, test, and deployment of Amazon Lex V2 bots through automation
  This guest post was written by Mulay Ahmed and Caroline Lima-Lane of Principal Financial Group. The content and opinions in this post are those of the third-party authors and AWS is not responsible for the content or accuracy of this post. 
With US contact centers that handle millions of customer calls annually, Principal Financial Group¬Æ wanted to modernize their customer call experience. In the post Principal Financial Group increases Voice Virtual Assistant performance using Genesys, Amazon Lex, and Amazon QuickSight, we discussed the overall Principal Virtual Assistant solution using Genesys Cloud, Amazon Lex V2, multiple AWS services, and a custom reporting and analytics solution using Amazon QuickSight. 
This post focuses on the acceleration of the Virtual Assistant (VA) platform delivery processes through automated build, testing, and deployment of an Amazon Lex V2 bot (including other database and analytics resources described later in this post) using a GitHub continuous integration and delivery (CI/CD) pipeline with automated execution of the Amazon Lex V2 Test Workbench for quality assurance. This solution helps Principal¬Æ scale and maintain VA implementations with confidence and speed using infrastructure as code (IaC), configuration as code (CaC,) and an automated CI/CD approach instead of testing and deploying the Amazon Lex V2 bot on the AWS Management Console. 
Principal is a global financial company with nearly 20,000 employees passionate about improving the wealth and well-being of people and businesses. In business for 145 years, Principal is helping approximately 70 million customers (as of Q4 2024) plan, protect, invest, and retire, while working to support the communities where it does business.The enterprise virtual assistant engineering team at Principal, in collaboration with AWS, used Amazon Lex V2 to implement a voice virtual assistant to provide self-service and routing capabilities for contact center customers. The following engineering opportunities were recognized and prioritized: 
 
 Elimination of console-driven configuration, testing, and deployment of an Amazon Lex V2 bot 
 Collaboration through structured version control and parallel development workflows for multiple team members 
 Acceleration of development cycles with automated build, test, and deployment processes for Amazon Lex bot creation and optimization 
 Enhanced quality assurance controls through automated testing gates and coding standard validation for reliable releases 
 
With the automation solutions described in the post, as of September 2024, Principal has accelerated development efforts by 50% across all environments (development, pilot, and production) through streamlined implementation and deployment processes. This solution also enhances deployment reliability through automated workflows, providing consistent updates while minimizing errors across development, pilot, and production environments, and maximizes development efficiency by integrating the Test Workbench with GitHub, enabling version control and automated testing.With the automation of the Test Workbench and its integration with GitHub, the solution strengthens the CI/CD pipeline by maintaining alignment between test files and bot versions, creating a more agile and reliable development process. 
Solution overview 
The solution uses the services described in Principal Financial Group increases Voice Virtual Assistant performance using Genesys, Amazon Lex, and Amazon QuickSight. The following services/APIs are also used as part of this solution: 
 
 AWS Step Functions to orchestrate the deployment workflow 
 The Test Workbench APIs, which are invoked within the Step Functions state machine as a sequence of tasks 
 AWS Lambda to process data to support some of the Test Workbench APIs inputs 
 
VA code organization and management 
The Principal VA implementation uses Genesys Cloud as the contact center application and the following AWS services organized as different stacks: 
 
 Bot stack: 
   
   The Amazon Lex V2 CDK is used for defining and deploying the bot infrastructure 
   Lambda functions handle the bot logic and manage routing logic (for Amazon Lex and Genesys Cloud) 
   AWS Secrets Manager stores secrets for calling downstream systems endpoints 
    
 Testing stack: 
   
   Step Functions orchestrates the testing workflow 
   Lambda functions are used in the testing process 
   Test files contains test cases and scenarios in Test Workbench format 
   Simulated data is used to simulate various scenarios for testing without connecting to downstream systems or APIs 
    
 Data stack: 
   
   Amazon Dynamo DB manages and stores bot prompts 
   Amazon Simple Storage Service (Amazon S3) stores testing data 
    
 Analytics stack: 
   
   Amazon S3 stores logs and processed data 
   Amazon Data Firehose streams logs to Amazon S3 
   Lambda orchestrates extract, transform, and load (ETL) operations 
   AWS Glue manages the Data Catalog and ETL jobs 
   Amazon Athena is used for querying and analyzing analytics data in Amazon S3 
   Amazon QuickSight is used for data visualization and business intelligence 
    
 CI/CD pipeline: 
   
   GitHub serves as the source code repository 
   A GitHub workflow automates the CI/CD pipeline 
    
 
Amazon Lex V2 configuration as code and CI/CD workflow 
The following diagram illustrates how multiple developers can work on changes to the bot stack and test in parallel by deploying changes locally or using a GitHub workflow. 
 
The process consists of the following steps: 
 
 A developer clones the repository and creates a new branch for changes. 
 Developer A or B makes changes to the bot configuration or Lambda functions using code. 
 The developer creates a pull request. 
 The developer deploys the Amazon Lex V2 CDK stack through one of the following methods: 
   
   Create a pull request and ensure all code quality and standards checks are passing. 
   Merge it with the main branch. 
   Deploy the Amazon Lex V2 CDK stack from their local environment. 
    
 The developer runs the Test Workbench as part of the CI/CD pipeline or from their local environment using the automation scripts. 
   
   Tests results are displayed in GitHub Actions and the terminal (if run locally). 
   The pipeline succeeds only if defined checks such as linting, unit testing, infrastructure testing and integration, and Test Workbench functional testing pass. 
    
 After all tests and checks pass, a new pre-release can be drafted to deploy to the staging environment. After staging deployment and testing (automated and UAT) is successful, a new release can be created for production deployment (after manual review and approval). 
 
Amazon Lex Test Workbench automation 
The solution uses GitHub and AWS services, such as Step Functions state machines and Lambda functions, to orchestrate the entire Amazon Lex V2 Bot testing process (instead of using the existing manual testing process for Amazon Lex). The pipeline triggers the upload of test sets, Lambda functions to interact with the Amazon Lex V2 bot and Test Workbench, then another Lambda function to read the tests results and provide results in the pipeline. 
To maintain consistent, repeatable evaluations of your Amazon Lex V2 bots, it‚Äôs essential to manage and organize your test datasets effectively. The following key practices help keep test sets up-to-date: 
 
 Test set files are version-controlled and linked to each bot and its version 
 Separate golden test sets are created for each intent and updated on a regular basis to include production customer utterances, increasing intent recognition rates 
 The versioned test data is deployed as part of each bot deployment in non-production environments 
 
The following diagram illustrates the end-to-end automated process for testing Amazon Lex V2 bots after each deployment. 
 
The post-deployment workflow consists of the following steps: 
 
 The developer checks the test file into the GitHub repository (or deploys directly from local). After each bot deployment, GitHub triggers the test script using the GitHub workflow. 
 The test scripts upload the test files to an S3 bucket. 
 The test script invokes a Step Functions state machine, using a bot name and list of file keys as inputs. 
 Amazon Lex Model API calls are invoked to get the bot ID (ListBots) and alias (ListBotAliases). 
 Each test file key is iterated within a Map state, where the following tasks are executed: 
   
   Call Amazon Lex APIs to start import jobs: 
     
     StartImport ‚Äì Creates a test set ID and stores it under an S3 bucket specified location. 
     DescribeImport ‚Äì Checks if the status of StartImport is complete. 
      
   Run the test set: 
     
     StartTestExecution ‚Äì Creates a test execution ID and executes the test. 
     ListTestExecutions ‚Äì Gathers all test executions. A Lambda function filters out the current test execution id and its status. 
      
   Get test results. 
    
 When the test is complete: 
   
   The ListTestExecutionResultItems API is invoked to gather overall test results. 
   The ListTestExecutionResultItems API is invoked to fetch test failure details at the utterance level if present. 
    
 A Lambda function orchestrates the final cleanup and reporting: 
   
   DeleteTestSet cleans up test sets that are no longer needed from an S3 bucket. 
   The pipeline outputs the results and if there are test failures, these are listed in the GitHub action or local terminal job report. 
    
 Developers conduct the manual process of reviewing the test result files from the Test Workbench console. 
 
Conclusion 
In this post, we presented how Principal accelerated the development, testing, and deployment of Amazon Lex V2 bots and supporting AWS services using code. In addition to the reporting and analytics solution, this provides a robust solution for the continued enhancement and maintenance of the Virtual Assistant ecosystem. 
By automating Test Workbench processes and integrating them with version control and CI/CD processes, Principal was able to decrease testing and deployment time, increase test coverage, streamline their development workflows, and deliver quality conversational experience to customers. For a deeper dive into other relevant services, refer to Evaluating Lex V2 bot performance with the Test Workbench. 
AWS and Amazon are not affiliates of any company of the Principal Financial Group. This communication is intended to be educational in nature and is not intended to be taken as a recommendation. Insurance products issued by Principal National Life Insurance Co (except in NY) and Principal Life Insurance Company. Plan administrative services offered by Principal Life. Principal Funds, Inc. is distributed by Principal Funds Distributor, Inc. Securities offered through Principal Securities, Inc., member SIPC and/or independent broker/dealers. Referenced companies are members of the Principal Financial Group, Des Moines, IA 50392. ¬©2025 Principal Financial Services, Inc. 4373397-042025 
 
About the authors 
Mulay Ahmed is a Solutions Architect at Principal with expertise in architecting complex enterprise-grade solutions, including AWS Cloud implementations. 
Caroline Lima-Lane is a Software Engineer at Principal with a vast background in the AWS Cloud space.
‚Ä¢ Beyond vibes: How to properly select the right LLM for the right task
  Choosing the right large language model (LLM) for your use case is becoming both increasingly challenging and essential. Many teams rely on one-time (ad hoc) evaluations based on limited samples from trending models, essentially judging quality on ‚Äúvibes‚Äù alone. 
This approach involves experimenting with a model‚Äôs responses and forming subjective opinions about its performance. However, relying on these informal tests of model output is risky and unscalable, often misses subtle errors, overlooks unsafe behavior, and provides no clear criteria for improvement. 
A more holistic approach entails evaluating the model based on metrics around qualitative and quantitative aspects, such as quality of response, cost, and performance. This also requires the evaluation system to compare models based on these predefined metrics and give a comprehensive output comparing models across all these areas. However, these evaluations don‚Äôt scale effectively enough to help organizations take full advantage of the model choices available. 
In this post, we discuss an approach that can guide you to build comprehensive and empirically driven evaluations that can help you make better decisions when selecting the right model for your task. 
From vibes to metrics and why it matters 
Human brains excel at pattern-matching, and models are designed to be convincing. Although a vibes-based approach can serve as a starting point, without systematic evaluation, we lack the evidence needed to trust a model in production. This limitation makes it difficult to compare models fairly or identify specific areas for improvement. 
The limitations of ‚Äújust trying it out‚Äù include: 
 
 Subjective bias ‚Äì Human testers might favor responses based on style or tone rather than factual accuracy. Users can be swayed by ‚Äúexotic words‚Äù or formatting. A model whose writing sounds confident might win on vibes while actually introducing inaccuracies. 
 Lack of coverage ‚Äì A few interactive prompts won‚Äôt cover the breadth of real-world inputs, often missing edge cases that reveal model weaknesses. 
 Inconsistency ‚Äì Without defined metrics, evaluators might disagree on why one model is better based on different priorities (brevity vs. factual detail), making it difficult to align model choice with business goals. 
 No trackable benchmarks ‚Äì Without quantitative metrics, it‚Äôs impossible to track accuracy degradation during prompt optimization or model changes. 
 
Established benchmarks like MMLU, HellaSwag, and HELM offer valuable standardized assessments across reasoning, knowledge retrieval, and factuality dimensions, efficiently helping narrow down candidate models without extensive internal resources. 
However, exclusive reliance on these benchmarks is problematic: they measure generalized rather than domain-specific performance, prioritize easily quantifiable metrics over business-critical capabilities, and can‚Äôt account for your organization‚Äôs unique constraints around latency, costs, and safety requirements. A high-ranking model might excel at trivia while failing with your industry terminology or producing responses too verbose or costly for your specific implementation. 
A robust evaluation framework is vital for building trust, which is why no single metric can capture what makes an LLM response ‚Äúgood.‚Äù Instead, you must evaluate across multiple dimensions: 
 
 Accuracy ‚Äì Does the model produce accurate information? Does it fully answer the question or cover required points? Is the response on-topic, contextually relevant, well-structured, and logically coherent? 
 Latency ‚Äì How fast does the model produce a response? For interactive applications, response time directly impacts user experience. 
 Cost-efficiency ‚Äì What is the monetary cost per API call or token? Different models have varying pricing structures and infrastructure costs. 
 
By evaluating along these facets, you can make informed decisions aligned with product requirements. For example, if robustness under adversarial inputs is crucial, a slightly slower but more aligned model might be preferable. For simple internal tasks, trading some accuracy for cost-efficiency might make sense. 
Although many metrics require qualitative judgment, you can structure and quantify these with careful evaluation methods. Industry best practices combine quantitative metrics with human or AI raters for subjective criteria, moving from ‚ÄúI like this answer more‚Äù to ‚ÄúModel A scored 4/5 on correctness and 5/5 on completeness.‚Äù This detail enables meaningful discussion and improvement, and technical managers should demand such accuracy measurements before deploying any model. 
Unique evaluation dimensions for LLM performance 
In this post, we make the case for structured, multi-metric assessment of foundation models (FMs) and discuss the importance of creating ground truth as a prerequisite to model evaluation. We use the open source 360-Eval framework as a practical, code-first tool to orchestrate rigorous evaluations across multiple models and cloud providers. 
We show the approach by comparing four LLMs within Amazon Bedrock, across a spectrum of correctness, completeness, relevance, format, coherence, and instruction following, to understand how each model responds matches our ground truth dataset. Our evaluation measures the accuracy, latency, and cost for each model, painting a 360¬∞ picture of their strengths and weaknesses. 
To evaluate FMs, it‚Äôs highly recommended that you break up model performance into distinct dimensions. The following is a sample set of criteria and what each one measures: 
 
 Correctness (accuracy) ‚Äì The factual accuracy of the model‚Äôs output. For tasks with a known answer, you can measure this using exact match or cosine similarity; for open-ended responses, you might rely on human or LLM judgment of factual consistency. 
 Completeness ‚Äì The extent to which the model‚Äôs response addresses all parts of the query or problem. In human/LLM evaluations, completeness is often scored on a scale (did the answer partly address or fully address the query). 
 Relevance ‚Äì Measures if the content of the response is on-topic and pertinent to the user‚Äôs request. Relevance scoring looks at how well the response stays within scope. High relevance means the model understood the query and stayed focused on it. 
 Coherence ‚Äì The logical flow and clarity of the response. Coherence can be judged by human or LLM evaluators, or approximated with metrics like coherence scores or by checking discourse structure. 
 Following instructions ‚Äì How well the model obeys explicit instructions in the prompt (formatting, style, length, and so on). For example, if asked ‚ÄúList three bullet-point advantages,‚Äù does the model produce a three-item bullet list? If the system or user prompt sets a role or tone, does the model adhere to it? Instruction-following can be evaluated by programmatically checking if the output meets the specified criteria (for example, contains the required sections) or using evaluator ratings. 
 
Performing such comprehensive evaluations manually can be extremely time-consuming. Each model needs to be run on many if not hundreds of prompts, and each output must be checked for across all metrics. Doing this by hand or writing one-off scripts is error-prone and doesn‚Äôt scale. In practice, these can be evaluated automatically using LLM-as-a-judge or human feedback. This is where evaluation frameworks come into play. 
After you‚Äôve chosen an evaluation philosophy, it‚Äôs wise to invest in tooling to support it. Instead of combining ad hoc evaluation scripts, you can use dedicated frameworks to streamline the process of testing LLMs across many metrics and models. 
Automating 360¬∞ model evaluation with 360-Eval 
360-Eval is a lightweight solution that captures the depth and breadth of model evaluation. You can use it as an evaluation orchestrator to define the following: 
 
 Your dataset of test prompts and respective golden answers (expected answers or reference outputs) 
 Models you want to evaluate 
 The metrics and tasks framework evaluating the models against 
 
The tool is designed to capture relevant and user-defined dimensions of model performance in one workflow, supporting multi-model comparisons out of the box. You can evaluate models hosted in Amazon Bedrock or Amazon SageMaker, or call external APIs‚Äîthe framework is flexible in integrating different model endpoints. This is ideal for a scenario where you might want to use the full power of Amazon Bedrock models without having to sacrifice performance. 
The framework consists of the following key components: 
 
 Data configuration ‚Äì You specify your evaluation dataset; for example, a JSONL file of prompts with optional expected outputs, the task, and a description. The framework can also work with a custom prompt CSV dataset you provide. 
 API gateway ‚Äì Using the versatile LiteLLM framework, it abstracts the API differences so the evaluation loop can treat all models uniformly. Inference metadata such as time-to-first-token (TTFT), time-to-last-token (TTLT), total token output, API errors count, and pricing is also captured. 
 Evaluation architecture ‚Äì 360-Eval uses LLM-as-a-judge to score and calculate the weight of model outputs on qualities like correctness or relevance. You can provide all the metrics you care about into one pipeline. Each evaluation algorithm will produce a score and verdict per test case per model. 
 
Choosing the right model: A real-world example 
For our example use case, AnyCompany is developing an innovative software as a service (SaaS) solution that streamlines database architecture for developers and businesses. Their platform accepts natural language requirements as input and uses LLMs to automatically generate PostgreSQL-specific data models. Users can describe their requirements in plain English‚Äîfor example, ‚ÄúI need a cloud-based order management platform designed to streamline operations for small to medium businesses‚Äù‚Äîand the tool intelligently extracts the entity and attribute information and creates an optimized table structure specifically for PostgreSQL. This solution avoids hours of manual entity and database design work, reduces the expertise barrier for database modeling, and supports PostgreSQL best practices even for teams without dedicated database specialists. 
In our example, we provide our model a set of requirements (as prompts) relevant to the task and ask it to extract the dominant entity and its attributes (a data extraction task) and also produce a relevant create table statement using PostgreSQL (a text-to-SQL task). 
Example prompt: 
 
 Given the following requirement, extract the data model and attributes that you will 
recommend. I need the output in a single line. You can provide the attributes separated 
by comma: "A global manufacturing company uses a web-based supply chain management 
system to track inventory across 50 locations, manage relationships with over 200 
suppliers, forecast material needs, and automatically trigger purchase orders when stock 
levels reach predefined thresholds......" 
 
The following table shows our task types, criteria, and golden answers for this example prompt. We have shortened the prompt for brevity. In a real-world use case, your requirements might span multiple paragraphs. 
 
  
   
   task_type 
   task_criteria 
   golden_answer 
   
   
   DATA EXTRACTION 
   Check if the extracted entity and attributes matches the requirements 
    
     
     Supply Chain Inventory: inventory_id, product_sku, 
location_id, quantity_on_hand, reorder_threshold, 
supplier_id, last_order_date, forecasted_demand, 
cost_per_unit, status, last_updated 
      
   
   
   TEXT-TO-SQL 
   Given the requirements check if the generated create table matches the requirements 
    
     
     CREATE TABLE supply_chain_inventory (
    inventory_id SERIAL PRIMARY KEY,
    product_sku VARCHAR(50) NOT NULL,
    location_id INTEGER NOT NULL,
    quantity_on_hand INTEGER NOT NULL,
    reorder_threshold INTEGER NOT NULL,
    supplier_id INTEGER,
    last_order_date TIMESTAMP,
    forecasted_demand NUMERIC(10,2),
    cost_per_unit NUMERIC(10,2),
    status VARCHAR(20),
    last_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP
); 
      
   
  
 
AnyCompany wants to find a model that will solve the task in the fastest and most cost-effective way, without compromising on quality. 
360-Eval UI 
To reduce the complexity of the process, we have built a UI on top of the evaluation engine. 
The UI_README.md file has instructions to launch and run the evaluation using the UI. You must also follow the instructions in the README.md to install the Python packages as prerequisites and enable Amazon Bedrock model access. 
Let‚Äôs explore the different pages in the UI in more detail. 
Setup page 
As you launch the UI, you land on the initial Setup page, where you select your evaluation data, define your label, define your task as discreetly as possible, and set the temperature the models will have when being evaluated. Then you select the models you want to evaluate against your dataset, the judges that will evaluate the models‚Äô accuracy (using custom metrics and the standard quality and relevance metrics), configure pricing and AWS Region options, and finally configure how you want the evaluation to take place, such as concurrency, request per minute, and experiment counts (unique runs). 
 
This is where you specify the CSV file with sample prompts, task type, and task criteria according to your needs. 
Monitor page 
After the evaluation criteria and parameters are defined, they are displayed on the Monitor page, which you can navigate to by choosing Monitor in the Navigation section. On this page, you can monitor all your evaluations, including those currently running, those queued, and those not yet scheduled to run. You can choose the evaluation you want to run, and if any evaluation is no longer relevant, you can remove it here as well. 
The workflow is as follows: 
 
 Execute the prompts in the input file against the models selected. 
 Capture the metrics such as input token count, output token count, and TTFT. 
 Use the input and output tokens to calculate the cost of running each prompt against the models. 
 Use an LLM-as-a-judge to evaluate the accuracy against predefined metrics (correctness, completeness, relevance, format, coherence, following instructions) and any user-defined metrics. 
 
 
Evaluations page 
Detailed information of the evaluations, such as the evaluation configuration, the judge models used to evaluate, the Regions where the models are hosted, the input and output cost, and the task and its criteria the model was evaluated with, are displayed on the Evaluations page. 
 
Reports page 
Lastly, the Reports page is where you can select the completed evaluations to generate a report in HTML format. You can also delete old and irrelevant reports. 
 
Understanding the evaluation report 
The tool output is an HTML file that shows the results of the evaluation. It includes the following sections: 
 
 Executive Summary ‚Äì This section provides an overall summary of the results. It provides a quick summary of which model was most accurate, which model was the fastest overall, and which model provided the best success-to-cost ratio. 
 Recommendations ‚Äì This section contains more details and a breakdown of what you see in the executive summary, in a tabular format. 
 Latency Metrics ‚Äì In this section, you can review the performance aspect of your evaluation. We use the TTFT and output tokens per second as a measure for performance. 
 Cost Metrics ‚Äì This section shows the overall cost of running the evaluation, which indicates what you can expect in your AWS billing. 
 Task Analysis ‚Äì The tool further breaks down the performance and cost metrics by task type. In our case, there will be a section for the text-to-SQL task and one for data extraction. 
 Judge Scores Analysis ‚Äì In this section, you can review the quality of each model based on the various metrics. You can also explore prompt optimizations to improve your model. In our case, our prompts were more biased towards the Anthropic family, but if you use the Amazon Bedrock prompt optimization feature, you might be able to address this bias. 
 
Interpreting the evaluation results 
By using the 360-Eval UI, AnyCompany ran the evaluation with their own dataset and got the following results. They chose four different LLMs in Amazon Bedrock to conduct the evaluation. For this post, the exact models used aren‚Äôt relevant. We call these models Model-A, Model-B, Model-C, and Model-D. 
These results will vary in your case depending on the dataset and prompts. The results here are a reflection of our own example within a test account. As shown in the following figures, Model-A was the fastest, followed by Model-B. Model-C was 3‚Äì4 times slower than Model-A. Model-D was the slowest. 
  
As shown in the following figure, Model B was the cheapest. Model A was three times more expensive than Model-B. Model-C and Model-D were both very expensive. 
 
The next focus was the quality of the evaluation. The two most important metrics to were the correctness and completeness of the response. In the following evaluation, only Model-D scored more than 3 for both task types. 
 
Model-C was the next closest contender. 
 
Model-B scored lowest in the correctness and completeness metrics. 
 
Model-A missed slightly on the completeness for the text-to-SQL use case. 
 
Evaluation summary 
Let‚Äôs revisit AnyCompany‚Äôs criteria, which was to find a model that will solve the task in the fastest and most cost-effective way, without compromising on quality. There was no obvious winner. 
AnyCompany then considered providing a tiered pricing model to their customers. Premium-tier customers will receive the most accurate model at a premium price, and basic-tier customers will get the model with the best price-performance. 
Although for this use case, Model-D was the slowest and more expensive, it scored highest on the most crucial metrics: correctness and completeness of responses. For a database modeling tool, accuracy is far more important than speed or cost, because incorrect database schemas might lead to significant downstream issues in application development. AnyCompany chose Model-D for premium-tier customers. 
Cost is a major constraint for the basic-tier, so AnyCompany chose Model-A, because it scored reasonably well on correctness for both tasks and only slightly missed on completeness for one task type, while being faster and less expensive than the top performers. 
AnyCompany also considered Model-B as a viable option for free-tier customers. 
Conclusion 
As FMs become more reliant, they can also become more complex. Because their strengths and weaknesses more difficult to detect, evaluating them requires a systematic approach. By using a data-driven, multi-metric evaluation, technical leaders can make informed decisions rooted in the model‚Äôs actual performance, including factual accuracy, user experience, compliance, and cost. 
Adopting frameworks like 360-Eval can operationalize this approach. You can encode your evaluation philosophy into a standardized procedure, making sure every new model or version is judged the same, and enabling side-by-side comparisons. 
The framework handles the heavy lifting of running models on test cases and computing metrics, so your team can focus on interpreting results and making decisions. As the field of generative AI continues to evolve rapidly, having this evaluation infrastructure can help you find the right model for your use case. Furthermore, this approach can enable faster iteration on prompts and policies, and ultimately help you develop more reliable and effective AI systems in production. 
 
About the authors 
Claudio Mazzoni is a Sr Specialist Solutions Architect on the Amazon Bedrock GTM team. Claudio exceeds at guiding costumers through their Gen AI journey. Outside of work, Claudio enjoys spending time with family, working in his garden, and cooking Uruguayan food. 
Anubhav Sharma is a Principal Solutions Architect at AWS with over 2 decades of experience in coding and architecting business-critical applications. Known for his strong desire to learn and innovate, Anubhav has spent the past 6 years at AWS working closely with multiple independent software vendors (ISVs) and enterprises. He specializes in guiding these companies through their journey of building, deploying, and operating SaaS solutions on AWS.
‚Ä¢ Splash Music transforms music generation using AWS Trainium and Amazon SageMaker HyperPod
  Generative AI is rapidly reshaping the music industry, empowering creators‚Äîregardless of skill‚Äîto create studio-quality tracks with foundation models (FMs) that personalize compositions in real time. As demand for unique, instantly generated content grows and creators seek smarter, faster tools, Splash Music collaborated with AWS to develop and scale music generation FMs, making professional music creation accessible to millions. 
In this post, we show how Splash Music is setting a new standard for AI-powered music creation by using its advanced HummingLM model with AWS Trainium on Amazon SageMaker HyperPod. As a selected startup in the 2024 AWS Generative AI Accelerator, Splash Music collaborated closely with AWS Startups and the AWS Generative AI Innovation Center (GenAIIC) to fast-track innovation and accelerate their music generation FM development lifecycle. 
Challenge: Scaling music generation 
Splash Music has empowered a new generation of creators to make music, and has already driven over 600 million streams worldwide. By giving users tools that adapt to their evolving tastes and styles, the service makes music production accessible, fun, and relevant to how fans actually want to create. However, building the technology to unlock this creative freedom, especially the models that power it, meant overcoming several key challenges: 
 
 Model complexity and scale ‚Äì Splash Music developed HummingLM‚Äîa cutting-edge, multi-billion-parameter model tailored for generative music to deliver its mission of making music creation truly accessible. HummingLM is engineered to capture the subtlety of human humming, converting creative ideas into music tracks. Meeting these high standards of fidelity meant Splash had to scale up computing power and storage significantly, so the model could deliver studio-quality music. 
 Rapid pace of change ‚Äì&nbsp;The pace of industry and technological change, driven by rapid AI advancement, means Splash Music must continually adapt, train, fine-tune, and deploy new models to meet user expectations for fresh, relevant features. 
 Infrastructure scaling ‚Äì&nbsp;Managing and scaling large clusters in the generative AI model development lifecycle brought unpredictable costs, frequent interruptions, and time-consuming manual management. Prior to AWS, Splash Music relied on externally managed GPU clusters, which involved unpredictable latency, additional troubleshooting, and management complexity that hindered their ability to experiment and scale as quickly as needed. 
 
The service needed a scalable, automated, and cost-effective infrastructure. 
Overview of HummingLM: Splash Music‚Äôs foundation model 
HummingLM is Splash Music‚Äôs proprietary, multi-modal generative model, developed in close collaboration with the GenAIIC. It represents an improvement in how AI can interpret and generate music. The model‚Äôs architecture is built around a transformer-based large language model (LLM) coupled with a specialized music encoder upsampler: 
 
 HummingLM uses Descript-Audio-Codec (DAC) audio encoding to obtain compressed audio representations that capture both frequency and timbre characteristics 
 The system transforms hummed melodies into professional instrumental performances without explicit timbre representation learning 
 
The innovation lies in how HummingLM fuses these token streams. Using a transformer-based backbone, the model learns to blend the melodic intent from humming with the stylistic and structural cues from instrument sound (for example, to make the humming sound like a guitar, piano, flute, or different synthesized sound). Users can hum a tune, add an instrument control signal, and receive a fully arranged, high-fidelity track in return. HummingLM‚Äôs architecture is designed for both efficiency and expressiveness. By using discrete token representations, the model achieves faster convergence and reduced computational overhead compared to traditional waveform-based approaches. This makes it possible to train on diverse, large-scale datasets and adapt quickly to new genres or user preferences. 
The following diagram illustrates how HummingLM is trained and the inference process to generate high-quality music: 
 
Solution overview: Accelerating model development with AWS Trainium on Amazon SageMaker HyperPod 
Splash Music collaborated with the GenAIIC to advance its HummingLM foundation model, using the combined capabilities of Amazon SageMaker HyperPod and AWS Trainium chips for model training. 
Splash Music‚Äôs architecture follows SageMaker HyperPod best-practices using Amazon Elastic Kubernetes Service (EKS) as the orchestrator, FSx for Lustre for storage to store over 2 PB of data, and AWS Trainium EC2 instances for acceleration. The following diagram illustrates the solution architecture. 
 
In the following sections, we walk through each step of the model development lifecycle, from dataset preparation to compilation for optimized inference. 
Dataset preparation 
Efficient preparation and processing of large-scale audio datasets is critical for developing controllable music generation models: 
 
 Feature extraction pipeline ‚Äì Splash Music built a feature extraction pipeline for efficient, scalable processing of large volumes of audio data, producing high-quality features for model training. It starts by retrieving audio in batches from a centralized database, minimizing I/O overhead and supporting large-scale operations. 
 Audio processing ‚Äì Each audio file is resampled from 44,100 Hz to 22,050 Hz to standardize inputs and reduce computational load. A mono reference signal is also created by averaging the stereo channels from a reference audio file, serving as a consistent benchmark for analysis. In parallel, a Basic Pitch Extractor generates a synthetic, MIDI-like version of the audio, providing a symbolic representation of pitch and rhythm that enhances the richness of extracted features. 
 Descript Audio Codec (DAC) extractor ‚Äì&nbsp;The pipeline processes three audio streams: the stereo channels from the original audio, the mono reference, and the synthetic MIDI signal. This multi-stream approach captures diverse aspects of the audio signal, producing a robust set of features. Extracted data is organized into two main sets: audio-feature, which includes features from the original stereo channels, and sine-audio-feature, which contains features from the MIDI and mono reference audio. This structure streamlines downstream model training. 
 Parallel processing: To maximize performance, the pipeline uses parallel processing for concurrent feature extraction and data uploading. This significantly boosts efficiency, making sure the system handles large datasets with speed and consistency. 
 
In addition, the solution uses an advanced stem separation system that isolates songs into six distinct audio stems: drums, bass, vocals, lead, chordal, and other instruments: 
 
 Stem Preparation: Splash Music creates high-quality training data by preparing separate stems for each musical element. Lead and chordal stems are generated using a synthesizer tool and a diverse dataset of music tracks. This rich dataset covers multiple genres and styles. This provides a strong foundation for the model to learn precise component separation. 
 
By streamlining data handling from the outset, we make sure that the subsequent model training stages have access to clean, well-structured features. 
Model architecture and optimization 
HummingLM employs a dual-component architecture: 
 
 LLM for coarse token generation ‚Äì A 385 M parameter transformer-based language model (24 layers, 1024 embedding dimension, 16 attention heads) that generates foundational musical structure 
 Upsampling component ‚Äì A specialized component that expands the coarse representation into complete, high-fidelity audio. 
 
This division of labor is key to HummingLM‚Äôs effectiveness: the LLM captures high-level musical intent, and the upsampling component handles acoustic details. Together with the GenAIIC, Splash collaborated on research to optimize the HummingLM model to facilitate optimal performance: 
 
 Flexible control signal design ‚Äì&nbsp;The model accepts control signals of varying durations (1-5 seconds), a significant improvement over fixed-window approaches. 
 Zero-shot capability ‚Äì&nbsp;Unlike systems requiring explicit timbre embedding learning, HummingLM can generalize to unseen instrument presets without additional training. 
 Non-autoregressive generation ‚Äì&nbsp;The upsampling component uses parallel token prediction for significantly faster inference compared to traditional autoregressive approaches. 
 
Our evaluation demonstrated HummingLM‚Äôs superior first codebook prediction capabilities ‚Äì a critical factor in residual quantization systems where the first codebook contains most acoustic information. The model consistently outperformed baseline approaches like VALL-E across multiple quality metrics. The evaluation revealed several important findings: 
 
 HummingLM demonstrates significant improvements over baseline approaches in signal fidelity (57.93% better SI-SDR) 
 The model maintains robust performance across diverse musical conditions, with particular strength in the Aeolian mode 
 Zero-shot performance on unseen instrument presets is comparable to seen presets, confirming strong generalization capabilities 
 Data augmentation strategies provide substantial benefits (27.70% improvement in SI-SDR) 
 
Overall, HummingLM achieves state-of-the-art controllable music generation by significantly improving signal fidelity, generalizing well to unseen instruments, and delivering strong performance across diverse musical styles, boosted further by effective data augmentation strategies. 
Efficient distributed training through parallelism, memory, and AWS Neuron optimization 
Splash Music compiled and optimized its model for AWS Neuron, accelerating its model development lifecycle and deployment on AWS Trainium chips. The team considered scalability, parallelization, and memory efficiency and designed a system for supporting models scaling from 2B to over 10B parameters. This includes: 
 
 Enable distributed training with sequence parallelism (SP), tensor parallelism (TP), and data parallelism (DP), scaling up to 64 trn1.32xlarge instances 
 Implement ZeRO-1 memory optimization with selective checkpoint re-computation 
 Integrate Neuron Kernel Interface (NKI) to deploy Flash Attention, accelerating dense attention layers and streamlining causal mask management 
 Decompose the model into core subcomponents (token processors, transformer layers, MLPs) and optimize each for Neuron execution 
 Implement mixed-precision training (bfloat16 and float32) 
 
When optimizations at the Neuron level were complete, optimizing the orchestration layer was important as well. Orchestrated by SageMaker HyperPod, Splash Music developed a robust, Slurm-integrated pipeline that streamlines multi-node training, balances parallelism, and uses activation checkpointing for superior memory efficiency. The pipeline processes data through several critical stages: 
 
 Tokenization ‚Äì Audio inputs are processed through a Descript Audio Codec (DAC) encoder to generate multiple codebook representations 
 Conditional generation ‚Äì&nbsp;The model learns to predict codebooks given hummed melodies and timbre control signals 
 Loss functions ‚Äì&nbsp; The solution uses a specialized cross-entropy loss function to optimize both token prediction and audio reconstruction quality 
 
Model Inference on AWS Inferentia on Amazon Elastic Container Service (ECS) 
After training, the model is deployed on an Amazon Elastic Container Service (Amazon ECS) cluster with AWS Inferentia instances. The audio is uploaded to Amazon Simple Storage Service (Amazon S3) to handle large volumes of user-submitted recordings, which often vary in quality. Each upload triggers an AWS Lambda function, which queues the file in Amazon Simple Queue Service (Amazon SQS) for delivery to the ECS cluster where inference runs. On the cluster, HummingLM performs two key steps: stem separation to isolate and clean vocals, and audio-to-melody conversion to extract musical structure. Finally, the pipeline recombines the cleaned vocals through a post-processing step with backing tracks, producing the fully processed remixed audio. 
Results and impact 
Splash Music‚Äôs research and development teams now rely on a unified infrastructure built on Amazon SageMaker HyperPod and AWS Trainium chips. The solution has yielded the following benefits: 
 
 Automated, resilient and scalable training ‚Äì SageMaker HyperPod provisions clusters of AWS Trainium EC2 instances at scale, managing orchestration, resource allocation, and fault recovery automatically. This removes weeks of manual setup and facilitates reliable, repeatable training runs. SageMaker HyperPod continuously monitors cluster health, automatically rerouting jobs and repairing failed nodes, minimizing downtime and maximizing resource utilization. With SageMaker HyperPod, Splash Music cut operational downtime to near zero, enabling weekly model refreshes and faster deployment of new features. 
 AWS Trainium reduced Splash‚Äôs training costs by over 54% ‚Äì&nbsp;Splash Music realized over twofold gains in training speed and cut training costs by 54% using AWS Trainium based instances over traditional GPU-based solutions used with their previous cloud provider. With this leap in efficiency, Splash Music can train larger models, release updates more frequently, and accelerate innovation across their generative music service. The acceleration also delivers faster model iteration, with 8% improvement in throughput, and increased its maximum batch size from 70 to 512 for a more efficient use of compute resources and higher throughput per training run. 
 
Splash achieved significant throughput improvements over conventional architectures, to process expansive datasets, supporting the model‚Äôs complex multimodal nature. The solution provides a robust foundation for future growth as data and models continue to scale. 

 ‚ÄúAWS Trainium and SageMaker HyperPod took the friction out of our workflow at Splash Music.‚Äù&nbsp;says Daniel Hatadi, Software Engineer, Splash Music. ‚ÄúWe replaced brittle GPU clusters with automated, self-healing distributed training that scales seamlessly. Training times are nearly 50% faster, and training costs have dropped by 54%. By relying on AWS AI chips and SageMaker HyperPod and collaborating with the AWS Generative AI Innovation Center, we were able to focus on model design and music-specific research, instead of cluster maintenance. This collaboration has made it easier for us to iterate quickly, run more experiments, train larger models, and keep shipping improvements without needing a bigger team.‚Äù
 
Splash Music also featured in the AWS Summit Sydney 2025 keynote: 

 
  
 
 
Conclusion and Next Steps 
Splash Music is redefining how creators bring their musical ideas to life, making it possible for anyone to generate fresh, personalized tracks that resonate with millions of listeners worldwide. To support this vision at scale, Splash built its HummingLM FM in close collaboration with AWS Startups and the GenAIIC, using services such as SageMaker HyperPod and AWS Trainium. These solutions provide the infrastructure and performance needed to keep pace, helping Splash to create even more intuitive and inspiring experiences for creators. 

 ‚ÄúWith SageMaker HyperPod and Trainium, our researchers experiment as fast as our community creates.‚Äù says Randeep Bhatia, Chief Technology Officer, Splash Music. ‚ÄúWe‚Äôre not just keeping up with music trends‚Äîwe‚Äôre setting them.‚Äù
 
Looking forward, Splash Music plans to expand its training datasets tenfold, explore multimodal audio/video generation, and additionally collaborate with the GenAIIC on additional R&amp;D and its next version of HummingLM FM. 
Try creating your own music using Splash Music, and learn more about Amazon SageMaker HyperPod and AWS Trainium. 
 
About the authors 
Sheldon Liu is an Senior Applied Scientist, ANZ Tech Lead at the AWS Generative AI Innovation Center. He partners with AWS customers across diverse industries to develop and implement innovative generative AI solutions, accelerating their AI adoption journey while driving significant business outcomes. 
Mahsa Paknezhad is a Deep Learning Architect and a key member of the AWS Generative AI Innovation Center. She works closely with enterprise clients to design, implement, and optimize cutting-edge generative AI solutions. With a focus on scalability and production readiness, Mahsa helps organizations across diverse industries harness advanced Generative AI models to achieve meaningful business outcomes. 
Xiaoning Wang is a machine learning engineer at the AWS Generative AI Innovation Center. He specializes in large language model training and optimization on AWS Trainium and Inferentia, with experience in distributed training, RAG, and low-latency inference. He works with enterprise customers to build scalable generative AI solutions that drive real business impact. 
Tianyu Liu is an applied scientist at the AWS Generative AI Innovation Center. He partners with enterprise customers to design, implement, and optimize cutting-edge generative AI models, advancing innovation and helping organizations achieve transformative results with scalable, production-ready AI solutions. 
Xuefeng Liu leads a science team at the AWS Generative AI Innovation Center in the Asia Pacific regions. His team partners with AWS customers on generative AI projects, with the goal of accelerating customers‚Äô adoption of generative AI. 
Daniel Wirjo is a Solutions Architect at AWS, focused on AI and SaaS startups. As a former startup CTO, he enjoys collaborating with founders and engineering leaders to drive growth and innovation on AWS. Outside of work, Daniel enjoys taking walks with a coffee in hand, appreciating nature, and learning new ideas.
‚Ä¢ Iterative fine-tuning on Amazon Bedrock for strategic model improvement
  Organizations often face challenges when implementing single-shot fine-tuning approaches for their generative AI models. The single-shot fine-tuning method involves selecting training data, configuring hyperparameters, and hoping the results meet expectations without the ability to make incremental adjustments. Single-shot fine-tuning frequently leads to suboptimal results and requires starting the entire process from scratch when improvements are needed. 
Amazon Bedrock now supports iterative fine-tuning, enabling systematic model refinement through controlled, incremental training rounds. With this capability you can build upon previously customized models, whether they were created through fine-tuning or distillation, providing a foundation for continuous improvement without the risks associated with complete retraining. 
In this post, we will explore how to implement the iterative fine-tuning capability of Amazon Bedrock to systematically improve your AI models. We‚Äôll cover the key advantages over single-shot approaches, walk through practical implementation using both the console and SDK, discuss deployment options, and share best practices for maximizing your iterative fine-tuning results. 
When to use iterative fine-tuning 
Iterative fine-tuning provides several advantages over single-shot approaches that make it valuable for production environments. Risk mitigation becomes possible through incremental improvements, so you can test and validate changes before committing to larger modifications. With this approach, you can make data-driven optimization based on real performance feedback rather than theoretical assumptions about what might work. The methodology also helps developers to apply different training techniques sequentially to refine model behavior. Most importantly, iterative fine-tuning accommodates evolving business requirements driven by continuous live data traffic. As user patterns change over time and new use cases emerge that weren‚Äôt present in initial training, you can leverage this fresh data to refine your model‚Äôs performance without starting from scratch. 
How to implement iterative fine-tuning on Amazon Bedrock 
Setting up iterative fine-tuning involves preparing your environment and creating training jobs that build upon your existing custom models, whether through the console interface or programmatically using the SDK. 
Prerequisites 
Before beginning iterative fine-tuning, you need a previously customized model as your starting point. This base model can originate from either fine-tuning or distillation processes and supports customizable models and variants available on Amazon Bedrock. You‚Äôll also need: 
 
 Standard IAM permissions for Amazon Bedrock model customization 
 Incremental training data focused on addressing specific performance gaps 
 S3 bucket for training data and job outputs 
 
Your incremental training data should target the specific areas where your current model needs improvement rather than attempting to retrain on all possible scenarios. 
Using the AWS Management Console 
The Amazon Bedrock console provides a straightforward interface for creating iterative fine-tuning jobs. 
Navigate to the Custom Models section and select Create fine-tuning job. The key difference in iterative fine-tuning lies in the base model selection, where you choose your previously customized model instead of a foundation model.  
During training, you can visit the Custom models page in the Amazon Bedrock console to track the job status.  
Once complete, you can monitor your jobs performance metrics on console through multiple metric charts, on the Training metrics and Validation metrics tabs.  
Using the SDK 
Programmatic implementation of iterative fine-tuning follows similar patterns to standard fine-tuning with one critical difference: specifying your previously customized model as the base model identifier. Here‚Äôs an example implementation: 
 
 import boto3
from datetime import datetime
import uuid

# Initialize Bedrock client
bedrock = boto3.client('bedrock')

# Define job parameters
job_name = f"iterative-finetuning-{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}"
custom_model_name = f"iterative-model-{str(uuid.uuid4())[:8]}"

# Key difference: Use your previously customized model ARN as base
# This could be from previous fine-tuning or distillation
base_model_id = "arn:aws:bedrock:&lt;Region&gt;:&lt;AccountID&gt;:custom-model/&lt;your-previous-custom-model-id&gt;"

# S3 paths for training data and outputs
training_data_uri = "s3://&lt;your-bucket&gt;/&lt;iterative-training-data&gt;"
output_path = "s3://&lt;your-bucket&gt;/&lt;iterative-output-folder&gt;/"

# Hyperparameters adjusted based on previous iteration learnings
hyperparameters = {
    "epochCount": "3" # Example
}

# Create the iterative fine-tuning job
response = bedrock.create_model_customization_job(
    customizationType="FINE_TUNING",
    jobName=job_name,
    customModelName=custom_model_name,
    roleArn=role_arn,
    baseModelIdentifier=base_model_id,  # Your previously customized model
    hyperParameters=hyperparameters,
    trainingDataConfig={
        "s3Uri": training_data_uri
    },
    outputDataConfig={
        "s3Uri": output_path
    }
)

job_arn = response.get('jobArn')
print(f"Iterative fine-tuning job created with ARN: {job_arn}")
 
 
Setting up inference for your iteratively fine-tuned model 
Once your iterative fine-tuning job completes, you have two primary options for deploying your model for inference, provisioned throughput and on-demand inference, each suited to different usage patterns and requirements. 
Provisioned Throughput 
Provisioned Throughput offers stable performance for predictable workloads where consistent throughput requirements exist. This option provides dedicated capacity so that the iteratively fine-tuned model maintains performance standards during peak usage periods. Setup involves purchasing model units based on expected traffic patterns and performance requirements. 
On-demand inference 
On-demand inference provides flexibility for variable workloads and experimentation scenarios. Amazon Bedrock now supports Amazon Nova Micro, Lite, and Pro models as well as Llama 3.3 models for on-demand inference with pay-per-token pricing. This option avoids the need for capacity planning so you can test your iteratively fine-tuned model without upfront commitments. The pricing model scales automatically with usage, making it cost-effective for applications with unpredictable or low-volume inference patterns. 
Best practices 
Successful iterative fine-tuning requires attention to several key areas. Most importantly, your data strategy should emphasize quality over quantity in incremental datasets. Rather than adding large volumes of new training examples, focus on high-quality data that addresses specific performance gaps identified in previous iterations. 
To track progress effectively, evaluation consistency across iterations allows meaningful comparison of improvements. Establish baseline metrics during your first iteration and maintain the same evaluation framework throughout the process. You can use Amazon Bedrock Evaluations to help you systematically identify where gaps exist in your model performance after each customization run. This consistency helps you understand whether changes are producing meaningful improvements. 
Finally, recognizing when to stop the iterative process helps to prevent diminishing returns on your investment. Monitor performance improvements between iterations and consider concluding the process when gains become marginal relative to the effort required. 
Conclusion 
Iterative fine-tuning on Amazon Bedrock provides a systematic approach to model improvement that reduces risks while enabling continuous refinement. With the iterative fine-tuning methodology organizations can build upon existing investments in custom models rather than starting from scratch when adjustments are needed. 
To get started with iterative fine-tuning, access the Amazon Bedrock console and navigate to the Custom models section. For detailed implementation guidance, refer to the Amazon Bedrock documentation. 
 
About the authors 
Yanyan Zhang is a Senior Generative AI Data Scientist at Amazon Web Services, where she has been working on cutting-edge AI/ML technologies as a Generative AI Specialist, helping customers use generative AI to achieve their desired outcomes. Yanyan graduated from Texas A&amp;M University with a PhD in Electrical Engineering. Outside of work, she loves traveling, working out, and exploring new things. 
Gautam Kumar is an Engineering Manager at AWS AI Bedrock, leading model customization initiatives across large-scale foundation models. He specializes in distributed training and fine-tuning. Outside work, he enjoys reading and traveling. 
Jesse Manders is a Senior Product Manager on Amazon Bedrock, the AWS Generative AI developer service. He works at the intersection of AI and human interaction with the goal of creating and improving generative AI products and services to meet our needs. Previously, Jesse held engineering team leadership roles at Apple and Lumileds, and was a senior scientist in a Silicon Valley startup. He has an M.S. and Ph.D. from the University of Florida, and an MBA from the University of California, Berkeley, Haas School of Business.

‚∏ª